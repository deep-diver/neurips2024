[{"type": "text", "text": "Jihao Qiu1\u2217 Yuan Zhang1\u2217 Xi Tang1\u2217 Lingxi Xie Tianren Ma1 ", "page_idx": 0}, {"type": "text", "text": "Pengyu Yan2 David Doermann2 Qixiang Ye1 Yunjie Tian1,2\u2020 ", "page_idx": 0}, {"type": "text", "text": "1University of Chinese Academy of Sciences 2University at Buffalo ", "page_idx": 0}, {"type": "text", "text": "{qiujiahao19, zhangyuan192, tangxi19, matianren18, tianyunjie19} $@$ mails.ucas.ac.cn pyan4@buffalo.edu doermann@buffalo.edu 198808xc@gmail.com qxye $@$ ucas.ac.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Videos carry rich visual information including object description, action, interaction, etc., but the existing multimodal large language models (MLLMs) fell short in referential understanding scenarios such as video-based referring. In this paper, we present Artemis, an MLLM that pushes video-based referential understanding to a finer level. Given a video, Artemis receives a natural-language question with a bounding box in any video frame and describes the referred target in the entire video. The key to achieving this goal lies in extracting compact, targetspecific video features, where we set a solid baseline by tracking and selecting spatiotemporal features from the video. We train Artemis on the newly established VideoRef45K dataset with 45K video-QA pairs and design a computationally efficient, three-stage training procedure. Results are promising both quantitatively and qualitatively. Additionally, we show that Artemis can be integrated with video grounding and text summarization tools to understand more complex scenarios. Code and data are available at https://github.com/qiujihao19/Artemis. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The past year has witnessed rapid progress of multimodal large language models (MLLMs) [33, 68, 40], offering abundant abilities of open-world image understanding with language-based dialogues. In comparison, there are fewer studies on training MLLMs for video understanding, albeit videos are much more informative than still images. Existing video-based MLLMs [29, 61, 37, 31, 38] mostly focus on superficial dialogues in which the video is encoded holistically, inevitably lacking the ability to understand fine-level video contents, e.g., describing a user-specific target in the video. ", "page_idx": 0}, {"type": "text", "text": "We are considering a new task called video-based referential understanding to compensate for the limitation. Specifically, we are interested in complex videos that span 20\u201330 seconds and the target performs multiple actions during this period. Given a video, the MLLM tries to answer a question like \u2018What is the target <region> doing in this video?\u2019 where <region> refers to a bounding box in any video frame. We argue that the task is not only challenging as it requires feature extraction, tracking, summarization, etc., but also important because it lays the foundation of finer-level video understanding. However, as shown in Figure 1, existing MLLMs often fell short in ", "page_idx": 0}, {"type": "image", "img_path": "FaNhyXY6Y1/tmp/59279c81ebd8ed1eb330a9be6f8badc3dccf0115710ffffbc196fd3b728ff90a.jpg", "img_caption": ["Figure 1: Artemis\u2019 ability in video-based dialogue. Notably, Artemis excels particularly in videobased referring, outperforming the existing MLLMs including Merlin [57] and Video-LLaVA [31] lacking comprehensiveness and Osprey [59] suffering hallucination. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "this seemingly easy task because they were mostly trained for image-based referential understanding;   \nas a result, they can only perceive the action in a single moment rather than that in an entire video2. ", "page_idx": 1}, {"type": "text", "text": "This paper presents Artemis3 as a solid baseline for the above task. Artemis follows the generic design of modern MLLMs (i.e., visual instruction tuning [33]), but encounters a challenge in finding sparse, target-related information from dense video data. A preliminary study shows that feeding raw video features into the MLLM results in computational inefficiency and training instability. To extract target-specific video features, we propose a simple yet effective solution that involves (i) tracking the target over time and (ii) selecting informative features from a long list of regions-of-interest (RoIs). The compactness of features makes it easier to train the MLLM. We design a three-stage training schedule where the MLLM gradually learns video-text alignment from coarse to fine. This efficient design requires only 28 hours (3 hours for the final stage) on $8\\times$ NVIDIA-A800 GPUs. ", "page_idx": 1}, {"type": "text", "text": "To train and evaluate Artemis, we organize 7 existing video understanding datasets into the VideoRef45K benchmark comprising 45K video question-answer pairs. To our knowledge, this is the first benchmark with box-level prompts and answers spanning complex videos. Experiments show the promising results of Artemis in a wide range of quantitative metrics including the BERT score, BLEU, etc.. Qualitatively, Artemis also shows a clear advantage in the comprehensiveness of description meanwhile avoiding hallucination (see Figure 1 for examples). Beyond the ability of video-based referring, Artemis serves as an important building block for complex video understanding, where we integrate Artemis with off-the-shelf video grounding and text summarization tools for interactive video-based dialogue and long video understanding, respectively. We expect our work to shed light on upgrading MLLMs for fine-level and interactive video understanding. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Large language models (LLMs) and multimodal LLMs (MLLMs). LLMs [15, 5, 13, 45, 12, 64, 49, 60, 11] have opened a new era of AI, demonstrating the potential to deal with various languagebased understanding and generation tasks. To unleash the power of LLMs for visual understanding, the computer vision community has been working on aligning language and vision data in the same feature space [41]. There are mainly two lines of research, where the internal adaptation methods [1] integrated cross-attention within an LLM for visual-language alignment, and the external adaptation methods [27, 14, 33] trained extra modules for this purpose. As a result, the vision foundation models, especially vision transformers [17, 35, 41, 48, 47, 66, 25], have been upgraded into MLLMs [33, 46, 63, 26] which gain the ability of language-guided visual understanding. ", "page_idx": 2}, {"type": "text", "text": "MLLMs for referring and grounding. MLLMs can be integrated with instance-level visual understanding tasks, allowing the models to (i) respond to questions targeted at specific regions of the image and (ii) identify regions corresponding to the contents in the dialogue \u2013 these functions are referred to as visual referring [63, 7] and grounding [40, 34], respectively. There are two main ways to integrate these functions into MLLMs, differing from each other in how the positional information is processed. The explicit methods [40, 52] introduced extra tokens to encode positions, while the implicit methods [9, 51, 54] used natural language to represent positions. Recently, there are also efforts [46] that used LLMs to call external vision modules for more flexible instance-level understanding quests. ", "page_idx": 2}, {"type": "text", "text": "Video-based MLLMs. Compared to the large corpus of image-based MLLMs, there are fewer video-based MLLMs for at least two reasons. First, there are fewer paired video-text data, especially for instance-level video understanding. Second, the higher dimensionality of video data poses a greater challenge to efficiently encode videos into visual features and find useful features to answer the questions. Existing efforts include VideoChat [29], Video-ChatGPT [37], Video-LLaMA [61], Video-LLaVA [31], LanguageBind [67], Valley [36], etc.; most of them followed the paradigm of image-based MLLMs and some of them [37] proposed a more efficient video feature. Recently, there have been some preliminary studies for instance-level video understanding, e.g., LEGO [30] studied moment retrieval with the assistance of LLMs, and PG-Video-LLaVA [38] performed video grounding by employing off-the-shelf tracking and grounding modules. Merlin [57] studied videobased referring, but it was built upon three manually specified frames as visual input, incurring extra burden for users and also limiting the model\u2019s ability to understand long and complex videos. This paper aims to address the above two challenges, for which we set up a new formulation, establish a new benchmark named VideoRef45K, and present a solid baseline named Artemis. ", "page_idx": 2}, {"type": "text", "text": "3 Artemis: A Baseline for Video-based Referential Understanding ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Problem Formulation and Data Preparation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "A video can be represented in the raw form of $\\mathbf{V}\\in\\mathbb{R}^{T\\times W\\times H\\times C}$ , where $T$ , W, $H$ , and $C$ stand for the number of frames, width, height, and the number of channels, respectively. In the task of video-based referential understanding (a.k.a. video-based referring), the model receives a question in the form of \u2018What is the <region> doing in this video?\u2019, where the concrete class of the referred object (like man or dog) is not provided, and the <region> is supplemented by a bounding box $\\mathbf{B}=(t;x_{1},y_{1},x_{2},y_{2})$ in a frame $t\\in\\{1,2,\\ldots,T\\}$ . The expected output is a sentence describing the target\u2019s action in the full video as detailed as possible (see Figure 1 for examples). Note that the proposed task requires a stronger ability beyond image-based referring and video captioning, mainly in the coverage and granularity of visual understanding. Specifically, the model is expected to produce complex action descriptions for different target <region> specified. ", "page_idx": 2}, {"type": "image", "img_path": "FaNhyXY6Y1/tmp/3e93f0e119d3ee28f9d1d6005eaaeccdf9ea511be8999e5b7e5a8dd2981158dd.jpg", "img_caption": ["Figure 2: Left: the overall framework of Artemis, where an MLLM receives a text prompt together with spatial, temporal, and target-specific video features, and produces the answer. Right: the RoI tracking and selection mechanism to generate target-specific features. We use different IDs to show the clustering result. This figure is best viewed in color. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "We collect video data for referential understanding from 7 datasets, including HC-STVG [44], VIDSentence [10], A2D Sentences [20], LaSOT [18], MeViS [16], GOT10K [24], and MGIT [23]. In total, there are 45K video QA pairs. We perform dataset-specific operations, including re-tracking (for HC-STVG and A2D-Sentences), clip cropping (for LaSOT and MGIT), and caption summary (for GOT10K), to convert them into the required form. Please refer to Appendix A for further details. ", "page_idx": 3}, {"type": "text", "text": "3.2 Overall Framework and Visual Features ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The overall framework of Artemis, as illustrated in Figure 2, follows the pipeline of visual instruction tuning [33, 68] where a multimodal large language model (MLLM) receives video features with a text prompt and produces the desired output. We denote the function as $\\mathbf{T}_{\\mathrm{out}}=f(\\mathbf{F}_{\\mathbf{V}},\\mathbf{T}_{\\mathrm{in}})$ , where ${\\bf{T}}_{\\mathrm{{in}}}$ and $\\mathbf{T_{\\mathrm{out}}}$ are input and output texts (in tokens) and $\\mathbf{F}_{\\mathbf{V}}$ is the set of features extracted from $\\mathbf{V}$ . ", "page_idx": 3}, {"type": "text", "text": "Compared to image-based referring, a clear difficulty of video-based referring arises from the high dimensionality of video data. Specifically, if we define $\\mathbf{F}_{\\mathbf{V}}$ as the set of dense video features (e.g., using a pre-trained visual encoder such as the CLIP ViT-L model [41] to extract frame-wise visual features for $\\mathbf{V}$ ), the features often contain highly redundant information due to the similarity of neighboring frames. This brings two-fold drawbacks: (i) extra complexity for the MLLM to deal with these vision tokens, and (ii) extra difficulty for the MLLM to locate useful information, which leads to a slower convergence. To overcome this issue, we decrease the input feature dimensionality by using various slices to replace $\\mathbf{F}_{\\mathbf{V}}$ , where each slice captures important yet complementary properties of the input video. Throughout this paper, we investigate three slices: the spatial, temporal, and target-specific video features. ", "page_idx": 3}, {"type": "text", "text": "Spatial and temporal features. The extraction of spatial and temporal video features follows the design of Video-ChatGPT [37]. Given a video clip $\\stackrel{\\triangledown}{V}\\in\\mathbb{R}^{T\\times W\\times H^{\\dagger}\\times C}$ , we use the CLIP ViT-L/14 visual encoder to cast it into frame-wise features, denoted as $F_{\\mathrm{frame}}\\in\\mathbb{R}^{T\\times W^{\\prime}\\times H^{\\prime}\\times D}$ , where the number of frames remains unchanged, $W^{\\prime}=W/s$ and $H^{\\prime}=H/s$ are the down-sampled resolution (e.g., $s=14$ for ViT-L/14) of the visual features, and $D$ is the feature dimensionality.) Then, these features are fed into average pooling along the $T$ axis (into the spatial features $\\mathbf{F}_{\\mathbf{V}}^{\\mathrm{S}}\\in\\mathbb{R}^{W^{\\prime}\\times H^{\\prime}\\times D})$ and along the $W^{\\prime}\\times H^{\\prime}$ plane (into the temporal features $\\mathbf{F}_{\\mathbf{V}}^{\\mathrm{T}}\\in\\mathbb{R}^{T\\times D})$ , respectively. ", "page_idx": 3}, {"type": "text", "text": "Target-specific features. $\\mathbf{F_{V}^{S}}$ and ${\\bf F}_{\\bf V}^{\\mathrm{T}}$ have focused on the spatial and temporal features but ignored the referred target which may move or change during the video. To offer a compromise feature that captures spatiotemporal features, we propose an RoI (region-of-interest) tracking and selection mechanism (detailed in Section 3.3) and obtain a list of RoIs (represented as bounding boxes) $\\boldsymbol{B}=\\left(\\mathbf{B}_{1},\\ldots,\\mathbf{B}_{M}\\right)$ , where $M$ is the number of RoIs that are recognized by the algorithm to be important for referential understanding. We use the RoIAlign method [21] to extract visual features from each RoI, producing a set of target-specific features, $\\mathcal{\\breve{F}}_{\\mathbf{V}}^{\\mathrm{R}}=(\\mathbf{F}_{\\mathbf{V},\\mathbf{B}_{1}}^{\\mathrm{R}},\\cdot\\cdot\\cdot,\\mathbf{F}_{\\mathbf{V},\\mathbf{B}_{M}}^{\\mathrm{R}})$ . ", "page_idx": 4}, {"type": "text", "text": "Instruction fine-tuning. When the video features are ready, we feed them with the text tokens into Artemis. The MLLM follows instruction fine-tuning through three steps, gradually acquiring the ability of video-based referring. The details are described in Section 3.4. ", "page_idx": 4}, {"type": "text", "text": "3.3 RoI Tracking and Selection ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Our goal is to extract compact features for video-based referring. The key lies in two factors, (i) completeness \u2013 locating the referred target in every video frame, and (ii) avoiding redundancy \u2013 not preserving too many features in the frames with similar semantics. We propose a simple solution upon RoI tracking and selection. As we shall see later, it offers a solid baseline for future work. ", "page_idx": 4}, {"type": "text", "text": "Step 1: RoI tracking. We apply HQTrack [69], an off-the-shelf tracking algorithm, to localize the RoI in each input frame. The pre-trained tracking model is not fine-tuned in the training phase. Given a RoI (a bounding box) in any video frame, the tracking algorithm outputs either a bounding box or nothing (e.g., if the target is occluded) in each of the remaining frames. This step outputs a raw list of RoIs denoted as $B^{\\prime}=\\mathbf{\\bar{\\alpha}}(\\mathbf{B}_{1}^{\\prime},\\ldots,\\mathbf{B}_{M^{\\prime}}^{\\prime})$ where $M^{\\prime}$ can be close to the number of frames. ", "page_idx": 4}, {"type": "text", "text": "Step 2: RoI selection. Feeding all tracked frames into the MLLM often incurs computational inefficiency and extra difficulties in model training. To avoid this, we select a subset from $B^{\\prime}$ containing $M<M^{\\prime}$ RoIs, with the goal being to preserve diverse visual features using a limited number of RoIs. In practice, we pre-defined the target number, $M$ , and adopt the K-means algorithm to form $M$ clusters from the original set of $M^{\\prime}$ RoIs. The final RoI list, $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ , consists of a randomly chosen RoI from each cluster. ", "page_idx": 4}, {"type": "text", "text": "Discussions. Finding representative RoIs belongs to a generic topic of feature selection. On one hand, one can set a simple baseline by performing random or uniform sampling from the original set $B^{\\prime}$ . On the other hand, the information theory offers a general principle, i.e., maximize the diversity of RoIs throughout the selection procedure. As demonstrated in Section 4.1, random and uniform sampling algorithms frequently fail to capture semantic changes throughout complex videos. By contrast, the simple K-means clustering used in Artemis significantly increases the diversity (see Appendix D), ensuring representative video features. We conjecture that the effectiveness of feature selection is related to the quality of video features; with stronger video foundation models, more sophisticated feature selection algorithms can make a larger difference. We leave this topic to future research. ", "page_idx": 4}, {"type": "text", "text": "3.4 Model Architecture and Training ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The MLLM is built upon Vicuna-7B v1.5 [11], an open-sourced $\\mathrm{LLM^{4}}$ . We use CLIP ViT-L/14 [41] to extract visual features. To feed these 1024-dimensional visual tokens into the LLM, we use a learnable, two-layer MLP (1024-4096-4096) to project the visual features into the 4096-dimensional language space. We always use the auto-regressive framework to train the MLLM. ", "page_idx": 4}, {"type": "text", "text": "The training procedure of Artemis comprises three steps, (1) video-text pre-training, (2) video-based instruction tuning, and (3) video-based referring. The first two stages are similar to Video-LLaVA [31] but different training data are used. We set a unified template, ", "page_idx": 4}, {"type": "text", "text": "guiding the model to output the desired answer. Here, <video-tokens> contains the spatial and temporal video features $(\\mathbf{\\dot{F}}_{\\mathbf{V}}^{\\mathrm{S}}$ and ${\\bf F}_{\\bf V}^{\\mathrm{T}}$ , projected by MLP), and <instruction> contains the language tokens of the task description (see below). ", "page_idx": 4}, {"type": "text", "text": "In the first stage, <instruction $_{.}>$ has the form of \u2018Write a terse but informative summary of the following video clip.\u2019 and the model outputs the overall description of the video. ", "page_idx": 4}, {"type": "text", "text": "The training data includes image-text and video-text pairs, using images as still videos. We use a subset of 558K LAION-CCSBU image-text pairs with BLIP [28] captions, sourced from CC3M [42] and refined by LLaVA [33]. Additionally, we use the $702\\mathrm{K}$ video-text pairs provided by VideoLLaVA [31], derived from the 703K pairs constructed by Valley [36] using WebVid [3]. Only the MLP is trained (from scratch) in this stage, initializing the alignment of vision and language. The training elapses one epoch with a learning rate of $1\\times10^{-3}$ , taking about 5 hours on $8\\!\\times\\!\\mathrm{A800}$ GPUs. ", "page_idx": 5}, {"type": "text", "text": "In the second stage, <instruction> contains specific task descriptions like \u2018Where is the person in the image?\u2019 and \u2018What is the person doing in the video?\u2019, and the model follows the instruction to produce the answer. The training data comprises the 665K imagetext instruction dataset from LLaVA-1.5 [33] and the 100K video-text instruction set from VideoChatGPT [37]. Both the LLM and MLP are fine-tuned in this stage. The training elapses one epoch with a learning rate of $2\\times10^{-5}$ , taking about 20 hours on $8\\!\\times\\!\\mathrm{A800}$ GPUs. ", "page_idx": 5}, {"type": "text", "text": "In the third stage, we use the curated VideoRef45K dataset to endow the model with the ability of video-based referring. The template is modified as follows, ", "page_idx": 5}, {"type": "text", "text": "User: <video-tokens> <refer-instruction> <track-instruction> Assistant: Here, <refer-instruction> is formulated as \u2018What is the <region> doing during this video?\u2019 where the <region> token is replaced by the visual features extracted from the bounding box in the specified input frame, and <track-instruction> contains additional information, \u2018arTeh ithse  itsar gteht-es pterciafcick ifenagt ulreiss $(\\mathbf{F}_{\\mathbf{V},\\mathbf{B}_{1}}^{\\mathrm{R}},\\bar{.}\\bar{.}\\cdot.\\,,\\mathbf{F}_{\\mathbf{V},\\mathbf{B}_{M}}^{\\mathrm{R}}$ ,. ,p r<ojreecgteido nb>y\u2019  a wLhienreea tr)h ee x<trraecgtieod nf>r otomk ethnes selected , and the number of $<\\mathtt{r e g i o n}>$ token is . In this stage, we fine-tune the LLM (with LoRA [22]), MLP and the RoI Align module. The training procedure elapses 3 epochs with a learning rate of $4\\times10^{-5}$ , taking about 3 hours on $8\\!\\times\\!\\mathrm{A800}$ GPUs. ", "page_idx": 5}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "4.1 Artemis Is a Strong Baseline for Video-based Referential Understanding ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Setting and metrics. We evaluate the ability of Artemis in video-based referring on the test set of HC-STVG [44]. The video and text data are pre-processed using the same method as in the training set. The test procedure uses the same instruction as in the third training stage and applies HQTrack [69] to localize the RoIs in video frames. We use the standard evaluation metrics including BERTScore [65], BLEU $@4$ [39], METEOR [4], ROUGE_L [32], CIDEr [50], and SPICE [2]. ", "page_idx": 5}, {"type": "table", "img_path": "FaNhyXY6Y1/tmp/edbd85d2a3e9651993be35313be4a4e64348f2a94027a41d029216d71b7d7c34.jpg", "table_caption": ["Table 1: A comparison of video-based referring metrics on the HC-STVG test set. \u2020: We use 5 key frames while using 8 frames leads to worse results. "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "Adapting existing MLLMs for video-based referring. Due to the limited availability of research for video-based referring, we compare our model to a few recent MLLMs trained for image-based or multi-frame based referring6. The image-based referring models include Osprey [59], Ferret [56], and Shikra [9]. For each video, we extract 5 key frames with RoIs produced by HQTrack and ask the trained model \u201cWhat is the target <region> doing?\u201d in the way the models are familiar with. Finally, we use GPT-3.5-Turbo to summarize the 5 answers into the overall description of the target. The multi-frame based reference model is Merlin [57] which receives 5 key video frames and RoIs and produces the overall description. The selection of key frames is consistent with Artemis. To compare with MLLMs that are trained for video understanding, such as Video-ChatGPT [37] and Video-LLaVA [31], we follow [43] to draw a red rectangle to mark the referred object in each key frame of the video. Then, we feed the rendered video to the models and ask the question \u201cWhat is the target indicated by the red rectangle doing?\u201d. ", "page_idx": 5}, {"type": "image", "img_path": "FaNhyXY6Y1/tmp/b2fb98d2e910db78b29e60bec67bb5ddd9bf488ff85a989117d3ae0847828953.jpg", "img_caption": ["Figure 3: Artemis and Merlin for video-based referring. Note that Merlin needs the semantic class of <region> to be provided while Artemis does not. In each case, the orange rectangle indicates the input <region>, blue rectangles are the tracked RoIs, and yellow stars label the selected RoIs. Red and green texts indicate incorrect and correct answers, respectively. This figure is best viewed in color. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Quantitative results, and necessity of native video-based referring. The numbers are summarized in Table 1. Artemis outperforms other MLLMs in each single evaluation metric. Note that the advantage is significant for some metrics, e.g., BLEU4. Please refer to Figure 1 for representative examples. In Figure 9 (see Appendix C), we show the behavior of the methodology using a standalone LLM (e.g., GPT-3.5- Turbo) upon image-based referring outputs. The image-based models tend to describe individual moments rather than an entire video; based on these inputs, the LLM cannot realize video descriptions and is sometimes confused to hallucinate what never happens in the video. The comparison validates the necessity of training a native model (i.e., directly on the instruction data for video-based referring) like what Artemis has done. Equipping with such a fundamental ability of video understanding at a finer level, Artemis can perform even more complex video understanding tasks, as shown in Section 4.2. ", "page_idx": 6}, {"type": "image", "img_path": "FaNhyXY6Y1/tmp/72b0663f678ed4f10c4f029754c80c6f9cff3f436eadc17aed4b6559b11eec6f.jpg", "img_caption": ["Figure 4: RoI manipulation increases the informativeness and diversity of RoIs. See Appendix D for details. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Qualitative results. We display several representative examples of video-based referring in Figure 3. The output of Artemis is comprehensive (especially compared to other MLLMs, see Figure 1), often containing fine-grained actions of the target. This mainly concerns the compact video features extracted by the RoI tracking and selection algorithm that extracts key features for understanding. ", "page_idx": 6}, {"type": "text", "text": "Ablative studies for target-specific video features. The key to extracting compact target-specific features lies in RoI tracking and selection. To validate this, we ablate two key parameters: the strategy of RoI selection and the number of preserved RoIs. In Table 2, we define a baseline for region of interest. For each frame of a video containing an object of interest, we enclose the object\u2019s location with a red rectangle and encode the video using Video-ChatGPT. The system is then queried with \u201cWhat is the object in the red rectangle doing in this video?\u201d. As illustrated in Table 2, K-means clustering emerges as a simple yet effective approach for RoI selection, whereas random or uniform sampling fails to consistently capture representative RoIs. To validate the effectiveness of RoI features in representing the object, we replace the <region> features with visual features (the [CLS] token from CLIP-ViT-L/14) in key frames, a variation we refer to as \u201cw/<track-instruction>\u201d. The performance decline compared to Artemis indicates that RoI features are of higher quality, as whole-frame features are more susceptible to background noise. Additionally, Table 3 demonstrates the importance of using multiple RoIs for understanding the full video content. While retaining more keyframes slows down both training and inference, it also introduces redundant information, leanding to a slight drop in performance metrics. Empirically, using 4 RoIs provides the optimal balance on the HC-STVG test set, although increasing the number of RoIs may be beneficial for more complex videos. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "From the information theory, we show that RoI tracking and selection improve informativeness (in terms of entropy) and diversity (in terms of frame-level difference) of the target-specific features in Figure 4. As shown in Figure 5, RoI tracking and selection gradually improve the comprehensiveness of the referring results. ", "page_idx": 7}, {"type": "table", "img_path": "FaNhyXY6Y1/tmp/4bddf61fe4b49f76cf3af3c75d9b54dd182d57a5183cb0da31fa0d4306aa910c.jpg", "table_caption": ["Table 2: Ablation on different RoI selection methods. Results are reported on HC-STVG. "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "FaNhyXY6Y1/tmp/637c2581b733bf77916839be6c613eee7b7774928d5452b3af208e5ee09a8a2c.jpg", "table_caption": ["Table 3: Ablation on the number of selected RoIs. Results are reported on HC-STVG. "], "table_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "FaNhyXY6Y1/tmp/0911b27287dd31f183d782f2de3cec9d4d249985c532b34f5d0d0df1d4451111.jpg", "img_caption": ["Figure 5: How RoI tracking and selection gradually improves the quality of video-based referring. In each example, the orange rectangle indicates the input $<\\mathtt{r e g i o n}>$ , blue rectangles are the tracked RoIs, and green and yellow stars label the uniformly sampled and K-means selected RoIs, respectively. Red and green texts highlight the incorrect and correct outputs. This figure is best viewed in color. "], "img_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "FaNhyXY6Y1/tmp/61df3dfbefdd5ebdcb1781370b7f67af855fb0e5ba8d76d5aaddcf8432f39676.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 7: Example of long video understanding. We apply Artemis to output descriptions for segmented video clips and integrate them using an LLM (GPT-3.5-Turbo in this example). ", "page_idx": 8}, {"type": "text", "text": "4.2 Artemis Is a Building Block for Complex Video Understanding ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "With a strong ability of video-based referring, Artemis serves as a building block that strengthens the existing video-based MLLMs in complex video understanding. ", "page_idx": 8}, {"type": "text", "text": "Multi-round video understanding with grounding. Multi-round dialogues, especially answering logically related chain-of-questions [46], is an important yet challenging topic for MLLMs. In Figure 6 and Figure 14 in Appendix E, we show that Artemis\u2019s referential understanding ability can be combined with image-based grounding models (e.g., GroundingDINO [34]) to answer multi-round chain-of-questions, where the entities mentioned in the video-based referring result is located and fed into the next round of video-based referring quest, allowing for more complex interactions. ", "page_idx": 8}, {"type": "text", "text": "Long video understanding with text summarization. Target-centric understanding of long videos is a major challenge for existing video-based MLLMs. The difficulty mainly lies in extracting compact video features (to feed into the MLLM) and tracking the target throughout the long video. We offer a simple solution that first segments the video into shorter clips, applies Artemis for understanding these clips, and applies an off-the-shelf LLM (e.g., GPT-3.5-Turbo) for summarization. As shown in Figure 7 and Figure 12 in Appendix E, the final output offers a comprehensive understanding. To our knowledge, this function was not achieved by existing MLLMs. ", "page_idx": 8}, {"type": "image", "img_path": "FaNhyXY6Y1/tmp/f1515ddc6454555f471936600e886ebdbd7998a5d2e83b3d994fda8b838013b1.jpg", "img_caption": ["Figure 6: An example of multi-round, video-based referring by integrating Artemis with GroundingDINO [34]. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Video question answering. Lastly, we show that Artemis can perform general video question answering. We test the trained model on the Video-ChatGPT test set [37] and the other three benchmarks (i.e., MSVD-QA [8], MSRVTT-QA [53], and ActivityNet-QA [6, 58]) where their training sets was not used to train Artemis. Results are summarized in Tables 4. Artemis shows competitive performance among a few recent MLLMs. These results inspire us that (i) an MLLM trained for finer-level video understanding can seamlessly transfer to coarser-level tasks, and (ii) extracting compact video features also benefits video question answering. ", "page_idx": 8}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "This paper proposes a challenging setting for video-based referring and establishes an effective MLLM named Artemis. Compared to existing methods, Artemis can understand human intention from simpler inputs (a text prompt and a single-frame bounding box) and comprehensively describe the target\u2019s action in a complex video. At the core of Artemis is an RoI tracking and selection mechanism to extract compact video features. Artemis shows advantages in video-based referring in VideoRef45K and transfers the ability to general video understanding, including being integrated with other modules for more complex tasks. We hope that Artemis can serve as a solid baseline to facilitate the research in fine-level video understanding. ", "page_idx": 8}, {"type": "table", "img_path": "FaNhyXY6Y1/tmp/f822676e4d114f296f9ed830726c98849a515ea5a990754d685c37b17184ff74.jpg", "table_caption": ["Table 4: Left: Video QA on Video-ChatGPT. Metrics: correctness (CN), detail orientation (DO), contextual understanding (CU), temporal understanding (TU), consistency (CC). Right: Zero-shot video QA on MSVD-QA, MSRVTT-QA, and ActivityNet-QA. Metrics: accuracy (Acc.), score (Sc.). "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Limitations. First, Artemis relies on a tracking algorithm to generate the RoIs; however, the tracking algorithm may produce inaccurate results and can confuse Artemis\u2013 see an example in Figure 13 (top) in Appendix E. Second, Artemis also suffers from the issues of general video-based understanding, such as the spatial-temporal aliasing problem, which can affect the model\u2019s ability to describe the visual content accurately \u2013 see an example in Figure 13 (bottom) where Artemis accurately predicts the movement of the target but reverses the temporal order. ", "page_idx": 9}, {"type": "text", "text": "6 Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was supported by National Natural Science Foundation of China (NSFC) under Grant No.62225208 and CAS Project for Young Scientists in Basic Research under Grant No.YSBR-117. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. Advances in Neural Information Processing Systems, 35:23716\u201323736, 2022.   \n[2] Peter Anderson, Basura Fernando, Mark Johnson, and Stephen Gould. Spice: Semantic propositional image caption evaluation. In Computer Vision\u2013ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part V 14, pages 382\u2013398. Springer, 2016.   \n[3] Max Bain, Arsha Nagrani, G\u00fcl Varol, and Andrew Zisserman. Frozen in time: A joint video and image encoder for end-to-end retrieval. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1728\u20131738, 2021.   \n[4] Satanjeev Banerjee and Alon Lavie. Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. In Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization, pages 65\u201372, 2005.   \n[5] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.   \n[6] Fabian Caba Heilbron, Victor Escorcia, Bernard Ghanem, and Juan Carlos Niebles. Activitynet: A large-scale video benchmark for human activity understanding. In Proceedings of the ieee conference on computer vision and pattern recognition, pages 961\u2013970, 2015.   \n[7] Chi Chen, Ruoyu Qin, Fuwen Luo, Xiaoyue Mi, Peng Li, Maosong Sun, and Yang Liu. Position-Enhanced Visual Instruction Tuning for Multimodal Large Language Models. arXiv preprint arXiv:2308.13437, August 2023.   \n[8] David Chen and William B Dolan. Collecting highly parallel data for paraphrase evaluation. In Proceedings of the 49th annual meeting of the association for computational linguistics: human language technologies, pages 190\u2013200, 2011.   \n[9] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao. Shikra: Unleashing Multimodal LLM\u2019s Referential Dialogue Magic. arXiv preprint arXiv:2306.15195, July 2023.   \n[10] Zhenfang Chen, Lin Ma, Wenhan Luo, and Kwan-Yee K Wong. Weakly-supervised spatio-temporally grounding natural sentence in video. arXiv preprint arXiv:1906.02549, 2019.   \n[11] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al. Vicuna: An open-source chatbot impressing gpt-4 with chatgpt quality. See https://vicuna. lmsys. org (accessed 14 April 2023), 2023.   \n[12] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.   \n[13] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416, 2022.   \n[14] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven C. H. Hoi. Instructblip: Towards general-purpose vision-language models with instruction tuning. arXiv preprint arXiv:2305.06500, 2023.   \n[15] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.   \n[16] Henghui Ding, Chang Liu, Shuting He, Xudong Jiang, and Chen Change Loy. Mevis: A large-scale benchmark for video segmentation with motion expressions. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2694\u20132703, 2023.   \n[17] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.   \n[18] Heng Fan, Liting Lin, Fan Yang, Peng Chu, Ge Deng, Sijia Yu, Hexin Bai, Yong Xu, Chunyuan Liao, and Haibin Ling. Lasot: A high-quality benchmark for large-scale single object tracking. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 5374\u20135383, 2019.   \n[19] Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xiangyu Yue, et al. Llama-adapter v2: Parameter-efficient visual instruction model. arXiv preprint arXiv:2304.15010, 2023.   \n[20] Kirill Gavrilyuk, Amir Ghodrati, Zhenyang Li, and Cees GM Snoek. Actor and action video segmentation from a sentence. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5958\u20135966, 2018.   \n[21] Kaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, and Ross Girshick. Mask r-cnn. In Proceedings of the IEEE international conference on computer vision, pages 2961\u20132969, 2017.   \n[22] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021.   \n[23] Shiyu Hu, Dailing Zhang, Xiaokun Feng, Xuchen Li, Xin Zhao, Kaiqi Huang, et al. A multi-modal global instance tracking benchmark (mgit): Better locating target in complex spatio-temporal and causal relationship. Advances in Neural Information Processing Systems, 36, 2024.   \n[24] Lianghua Huang, Xin Zhao, and Kaiqi Huang. Got-10k: A large high-diversity benchmark for generic object tracking in the wild. IEEE transactions on pattern analysis and machine intelligence, 43(5):1562\u2013 1577, 2019.   \n[25] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Doll\u00e1r, and Ross Girshick. Segment Anything. arXiv preprint arXiv:2304.02643, April 2023.   \n[26] Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, and Jiaya Jia. LISA: Reasoning Segmentation via Large Language Model. arXiv preprint arXiv:2308.00692, August 2023.   \n[27] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. BLIP-2: Bootstrapping Language-Image Pretraining with Frozen Image Encoders and Large Language Models. arXiv preprint arXiv:2301.12597, May 2023.   \n[28] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In International Conference on Machine Learning, pages 12888\u201312900. PMLR, 2022.   \n[29] KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao. Videochat: Chat-centric video understanding, 2024.   \n[30] Zhaowei Li, Qi Xu, Dong Zhang, Hang Song, Yiqing Cai, Qi Qi, Ran Zhou, Junting Pan, Zefeng Li, Van Tu Vu, Zhida Huang, and Tao Wang. Groundinggpt:language enhanced multi-modal grounding model, 2024.   \n[31] Bin Lin, Yang Ye, Bin Zhu, Jiaxi Cui, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning united visual representation by alignment before projection, 2023.   \n[32] Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out, pages 74\u201381, 2004.   \n[33] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual Instruction Tuning. arXiv preprint arXiv:2304.08485, April 2023.   \n[34] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. arXiv preprint arXiv:2303.05499, 2023.   \n[35] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF international conference on computer vision, pages 10012\u201310022, 2021.   \n[36] Ruipu Luo, Ziwang Zhao, Min Yang, Junwei Dong, Da Li, Pengcheng Lu, Tao Wang, Linmei Hu, Minghui Qiu, and Zhongyu Wei. Valley: Video assistant with large language model enhanced ability, 2023.   \n[37] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-chatgpt: Towards detailed video understanding via large vision and language models, 2023.   \n[38] Shehan Munasinghe, Rusiru Thushara, Muhammad Maaz, Hanoona Abdul Rasheed, Salman Khan, Mubarak Shah, and Fahad Khan. Pg-video-llava: Pixel grounding large video-language models, 2023.   \n[39] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311\u2013318, 2002.   \n[40] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei. Kosmos-2: Grounding Multimodal Large Language Models to the World. arXiv preprint arXiv:2306.14824, July 2023.   \n[41] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748\u20138763. PMLR, 2021.   \n[42] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2556\u20132565, 2018.   \n[43] Aleksandar Shtedritski, Christian Rupprecht, and Andrea Vedaldi. What does clip know about a red circle? visual prompt engineering for vlms. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 11987\u201311997, 2023.   \n[44] Zongheng Tang, Yue Liao, Si Liu, Guanbin Li, Xiaojie Jin, Hongxu Jiang, Qian Yu, and Dong Xu. Humancentric spatio-temporal video grounding with visual transformers. IEEE Transactions on Circuits and Systems for Video Technology, 32(12):8238\u20138249, 2021.   \n[45] Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. Lamda: Language models for dialog applications. arXiv preprint arXiv:2201.08239, 2022.   \n[46] Yunjie Tian, Tianren Ma, Lingxi Xie, Jihao Qiu, Xi Tang, Yuan Zhang, Jianbin Jiao, Qi Tian, and Qixiang Ye. Chatterbox: Multi-round multimodal referring and grounding. arXiv preprint arXiv:2401.13307, 2024.   \n[47] Yunjie Tian, Lingxi Xie, Jihao Qiu, Jianbin Jiao, Yaowei Wang, Qi Tian, and Qixiang Ye. Fast-itpn: Integrally pre-trained transformer pyramid network with token migration. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024.   \n[48] Yunjie Tian, Lingxi Xie, Zhaozhi Wang, Longhui Wei, Xiaopeng Zhang, Jianbin Jiao, Yaowei Wang, Qi Tian, and Qixiang Ye. Integrally Pre-Trained Transformer Pyramid Networks. In 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18610\u201318620. IEEE, June 2023.   \n[49] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.   \n[50] Ramakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image description evaluation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4566\u20134575, 2015.   \n[51] Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, et al. Cogvlm: Visual expert for pretrained language models. arXiv preprint arXiv:2311.03079, 2023.   \n[52] Wenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu, Xizhou Zhu, Gang Zeng, Ping Luo, Tong Lu, Jie Zhou, Yu Qiao, et al. Visionllm: Large language model is also an open-ended decoder for vision-centric tasks. arXiv preprint arXiv:2305.11175, 2023.   \n[53] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: A large video description dataset for bridging video and language. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5288\u20135296, 2016.   \n[54] Shiyu Xuan, Qingpei Guo, Ming Yang, and Shiliang Zhang. Pink: Unveiling the Power of Referential Comprehension for Multi-modal LLMs. arXiv preprint arXiv:2310.00582, October 2023.   \n[55] Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, and Cordelia Schmid. Zero-shot video question answering via frozen bidirectional language models. Advances in Neural Information Processing Systems, 35:124\u2013141, 2022.   \n[56] Haoxuan You, Haotian Zhang, Zhe Gan, Xianzhi Du, Bowen Zhang, Zirui Wang, Liangliang Cao, Shih-Fu Chang, and Yinfei Yang. Ferret: Refer and Ground Anything Anywhere at Any Granularity. arXiv preprint arXiv:2310.07704, October 2023.   \n[57] En Yu, Liang Zhao, Yana Wei, Jinrong Yang, Dongming Wu, Lingyu Kong, Haoran Wei, Tiancai Wang, Zheng Ge, Xiangyu Zhang, and Wenbing Tao. Merlin:empowering multimodal llms with foresight minds, 2023.   \n[58] Zhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao, Yueting Zhuang, and Dacheng Tao. Activitynet-qa: A dataset for understanding complex web videos via question answering, 2019.   \n[59] Yuqian Yuan, Wentong Li, Jian Liu, Dongqi Tang, Xinjie Luo, Chi Qin, Lei Zhang, and Jianke Zhu. Osprey: Pixel understanding with visual instruction tuning, 2024.   \n[60] Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, et al. Glm-130b: An open bilingual pre-trained model. arXiv preprint arXiv:2210.02414, 2022.   \n[61] Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language model for video understanding, 2023.   \n[62] Renrui Zhang, Jiaming Han, Chris Liu, Peng Gao, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, and Yu Qiao. Llama-adapter: Efficient fine-tuning of language models with zero-init attention. arXiv preprint arXiv:2303.16199, 2023.   \n[63] Shilong Zhang, Peize Sun, Shoufa Chen, Min Xiao, Wenqi Shao, Wenwei Zhang, Kai Chen, and Ping Luo. GPT4RoI: Instruction Tuning Large Language Model on Region-of-Interest. arXiv preprint arXiv:2307.03601, July 2023.   \n[64] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022.   \n[65] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. Bertscore: Evaluating text generation with bert. arXiv preprint arXiv:1904.09675, 2019.   \n[66] Xiaosong Zhang, Yunjie Tian, Lingxi Xie, Wei Huang, Qi Dai, Qixiang Ye, and Qi Tian. Hivit: A simpler and more efficient design of hierarchical vision transformer. In The Eleventh International Conference on Learning Representations, 2022.   \n[67] Bin Zhu, Bin Lin, Munan Ning, Yang Yan, Jiaxi Cui, HongFa Wang, Yatian Pang, Wenhao Jiang, Junwu Zhang, Zongwei Li, Wancai Zhang, Zhifeng Li, Wei Liu, and Li Yuan. Languagebind: Extending video-language pretraining to n-modality by language-based semantic alignment, 2024.   \n[68] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023.   \n[69] Jiawen Zhu, Zhenyu Chen, Zeqi Hao, Shijie Chang, Lu Zhang, Dong Wang, Huchuan Lu, Bin Luo, Jun-Yan He, Jin-Peng Lan, et al. Tracking anything in high quality. arXiv preprint arXiv:2307.13974, 2023. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Data Curation ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We organized 7 existing video datasets and performed a careful data curation procedure, resulting in the VideoRef45K benchmark, which comprises 45K video question-answer pairs. The 7 datasets include HC-STVG [44], VID-Sentence [10], A2D Sentences [20], LaSOT [18], MeViS [16], GOT10K [24], and MGIT [23]. ", "page_idx": 14}, {"type": "text", "text": "HC-STVG is a movie clip dataset that provides tracking sequences and textual annotations describing the person\u2019s actions during a certain period. We use the training portion, which contains approximately 10K video clips as our training data. The validation portion, containing 3,400 video clips, evaluates Artemis\u2019s ability. The original tracking sequences in HC-STVG are of poor quality, so we use the off-the-shelf tracking model HQTrack [69] to regenerate the tracking sequences and remove some low-quality bounding boxes. To prevent tracking target deviation caused by cross-frame tracking, we select the first and middle frames with ground truth bounding boxes annotated by the HC-STVG dataset as the referring frames for HQTrack to generate tracking lists for the whole video. We then compare these two generated tracking lists with the HC-STVG annotations and exclude the frames with low IoU between the generated and annotated bounding boxes from the tracking lists. ", "page_idx": 14}, {"type": "text", "text": "A2D Sentences provides tracking sequences and captions for different objects, but the tracking sequences are only 3 frames long. To address this, we use HQTrack to regenerate the sequences and obtain longer tracking frames, extending them to 20 frames. ", "page_idx": 14}, {"type": "text", "text": "LaSOT provides a caption for an object along with its tracking sequence. However, LaSOT videos are usually long, and the captions for the entire video are generic. To address this, we extract three segments of 10 seconds each from the entire video for our training data. ", "page_idx": 14}, {"type": "text", "text": "GOT-10K is a tracking dataset that provides tracking sequences of objects and their categories, actions, and adverbs describing the actions. We concatenate these elements to describe the object\u2019s action in the video, e.g., \u201cbear is slowly walking.\u201d ", "page_idx": 14}, {"type": "text", "text": "MGIT videos are typically long, with annotations indicating the object\u2019s actions at different time intervals. We extract these segments as our training data. ", "page_idx": 14}, {"type": "text", "text": "For MeViS and VID-Sentence, we did not perform any special processing. We converted the mask annotations of MeViS into bounding boxes. ", "page_idx": 14}, {"type": "text", "text": "Figure 8 shows some examples of VideoRef45K. ", "page_idx": 14}, {"type": "image", "img_path": "FaNhyXY6Y1/tmp/a4ad3b706cdad879b62e36ceea3b2b0671c7ec29cd90b8dc614b5a74713cc795.jpg", "img_caption": ["Figure 8: Some examples of VideoRef45K. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "The total dataset composition is summarized in Table 5. ", "page_idx": 14}, {"type": "table", "img_path": "FaNhyXY6Y1/tmp/e89551aaf9628765b943868028c7b49b339d02679ab769e9afc94e196f4c9257.jpg", "table_caption": ["Table 5: Curated datasets for Video-based Referring. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "Through the above processing steps, we obtained the VideoRef45K dataset. This dataset includes captions describing the actions of objects in videos, along with their corresponding tracking sequences. To utilize VideoRef45K, we created a template of questions to prompt the language model (LLM) to answer what the referred object did in a video. The template consists of \u2018<refer-instruction>\u2019 and \u2018<track-instruction>\u2019. The \u2018<refer-instruction>\u2019 is formulated as \u2018What is the <region> doing during this video?\u2019, and we utilized GPT-3.5-Turbo to generate the refer instruction template. The \u2018<track-instruction>\u2019 contains additional tracking lists to help the LLM perceive the referred object, such as \u2018This is the region\u2019s tracking list: <region> ... <region>\u2019. We created several \u2018<track-instruction>\u2019 options as the track instruction template. During training, we randomly sampled a \u2018<refer-instruction>\u2019 from the refer instruction template and a \u2018<track-instruction>\u2019 from the track instruction template. The \u2018<refer-instruction>\u2019 and the \u2018<track-instruction>\u2019 are then concatenated as \u2018<refer-instruction><track-instruction>\u2019 to formulate the text prompt. ", "page_idx": 15}, {"type": "text", "text": "B Implementation Details ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We report the detailed training hyper-parameters of Artemis in Table 6. ", "page_idx": 15}, {"type": "table", "img_path": "FaNhyXY6Y1/tmp/9375ddb41f5699d20811eea95f9b081aefaf3f7060181b88a1d2299d59f6f1ff.jpg", "table_caption": ["Table 6: Training hyper-parameters of Artemis. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "C Image-based MLLMs for Video-based Referring ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Existing multimodal language models (MLLMs) for video understanding rarely possess video-based referring capabilities. To address this limitation, we leverage image-based MLLMs (such as Osprey [59] and Ferret [56]) to perform image referring on 5 keyframes within the video independently. Subsequently, we integrate these outputs using GPT-3.5-Turbo to achieve video-based referring, as depicted in Figure 9. However, these models struggle to perceive the action and behavior of the given region of interest (RoI) as effectively as Artemis. ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "A woman is standing in the image, positioned towards the right side. She is wearing a black shirt and appears to be looking at a man. ", "page_idx": 16}, {"type": "text", "text": "A woman is standing in the image, positioned towards the right side. She is wearing a black shirt and appears to be looking at a man. ", "page_idx": 16}, {"type": "text", "text": "A woman,   \npossibly a mother, is standing next to a man. She is   \nwearing a black   \nshirt and appears to be looking at   \nthe man. ", "page_idx": 16}, {"type": "text", "text": "A woman is standing next to a man. They appear to be in a pleasant and relaxed state, possibly enjoying a special occasion or a casual conversation. ", "page_idx": 16}, {"type": "text", "text": "A woman,   \npossibly a   \ngrandmother, is   \nstanding next to a man. She is   \nwearing a black shirt and appears to be looking at the man. ", "page_idx": 16}, {"type": "text", "text": "Throughout the video, a woman, possibly a mother or grandmother, wearing a black shirt, stands next to a man, smiling and engaging in a pleasant interaction, potentially enjoying a special occasion. ", "page_idx": 16}, {"type": "text", "text": "The region is a woman dressed in a black evening gown. Her attire suggests that she might be attending a formal event or a gathering. The woman's presence contributes to the elegance of the scene. ", "page_idx": 16}, {"type": "text", "text": "The region is a woman. She stands in a doorway. The overall image appears to be set in a luxurious room with a man in a suit and a woman in a black dress. The image shows a formal event or gathering. ", "page_idx": 16}, {"type": "text", "text": "The region is a woman. Her attire suggests that she is attending a formal event, perhaps a dinner or a party. The woman's dress, which is worn at such events, contributes to the elegance of the occasion. ", "page_idx": 16}, {"type": "text", "text": "The region is a woman who seems to be looking away. It appears that she is involved in a conversation that doesn't require her to face the camera. Her body language and suggest she might be listening something. ", "page_idx": 16}, {"type": "text", "text": "The region is a woman who seems to be looking away. It appears that she is involved in a conversation that doesn't require her to face the camera. Her body language and suggest she might be listening something. ", "page_idx": 16}, {"type": "text", "text": "Throughout the video, a woman dressed in a black evening gown attends a formal event or gathering, standing in a doorway and adding elegance to the scene. She is seen listening or observing, contributing an element of mystery and intrigue to the overall image. ", "page_idx": 16}, {"type": "text", "text": "The woman in black clothes walks to the man in the vest and stops. ", "page_idx": 16}, {"type": "image", "img_path": "FaNhyXY6Y1/tmp/563b7d43e8737e2e3794d2e5029561ca25e5bc3787609edd7f75addc2f6c39e3.jpg", "img_caption": [], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "Figure 9: A comparison of video-based referring between image-based MLLMs and Artemis. GPT3.5-Turbo is used to integrate the 5 independent outputs from the image-based MLLMs. ", "page_idx": 16}, {"type": "text", "text": "D How RoI Tracking and Selection Improve the Results ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "To better demonstrate the effectiveness of our method, we conducted the following experiments. Firstly, we computed the attention values between different RoI tokens and temporal tokens. We observed that the tracking tokens added through tracking compensated for the weak perception aspect of the initial video tokens, as illustrated in Figure 10. Subsequently, as depicted in Figure 4 (top), we calculated the information entropy before and after adding the tracking list. Upon adding the tracking list, the overall amount of RoI information fed into the MLLM increased by $20.3\\%$ . Additionally, we computed the inter-frame differences of the boxes chosen from the tracking list. As shown in Figure 4 (bottom), the K-means clustering method selects RoIs with greater differences than random and average selection. This enables the MLLM to better perceive the action changes of RoIs throughout the entire video. ", "page_idx": 16}, {"type": "text", "text": "E More Qualitative Examples ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Figure 11 shows more examples of Artemis\u2019s outputs of video-based referring. Figure 12 shows more examples of target-centric video understanding with text summarization. Figure 13 shows ", "page_idx": 16}, {"type": "image", "img_path": "FaNhyXY6Y1/tmp/d8e98140cd02a13c5151a008658456f064ce87315352661251947127ad06fc8a.jpg", "img_caption": ["Figure 10: Attention map between RoI tokens and temporal tokens. ", "Temporal Tokens "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "FaNhyXY6Y1/tmp/093f167187c9028224be736338a4ddbad58f7d0d6fde3ce5134f16c45e991e6b.jpg", "img_caption": ["Figure 11: Examples of video-based referring generated by Artemis. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "Artemis\u2019s failure cases, revealing the limitation of Artemis. Figure 14 shows mores example of combining Artemis with off-the-shelf grounding model, e.g. GroundingDINO to answer multi-round chain-of-questions conveniently. ", "page_idx": 17}, {"type": "image", "img_path": "FaNhyXY6Y1/tmp/a0de0e4aa5af5a90934fd30042815d2a8bed8d23c6bfb44e6c71385ae2eba9b3.jpg", "img_caption": [], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "Prompt: I divided a video into 4 segments and described a specific object in each of these segments. Please connect these 4 descriptions in chronological order to form a complete sentence describing what the object did in the video. These are the 4 descriptions:[1] [2] [3] [4] ", "page_idx": 18}, {"type": "text", "text": "The man in the red tie turns around and walks to the woman in the hat, then he turns and walks up the stairs, followed by turning again and walking to the door, and finally, the man in the vest goes to the table and sits down. ", "page_idx": 18}, {"type": "text", "text": "Figure 12: The example of long video understanding generated by Artemis. ", "page_idx": 18}, {"type": "image", "img_path": "FaNhyXY6Y1/tmp/bcb22738833c10b2a1548fd1550231aeba7e034dea7c6775d9fd79f6f47c64bb.jpg", "img_caption": ["Figure 13: Failure cases of Artemis. Top: The tracking module generates inaccurate RoI list, misleading Artemis\u2019s understanding. Bottom: Spatial-temporal aliasing in video hinders Artemis to perceive objects. "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "FaNhyXY6Y1/tmp/df1642817acaf891fa5c23ffd2f9eda8bc941db9d8c88271388e820cb414b9e6.jpg", "img_caption": ["Figure 14: Examples of multi-round video understanding with grounding generated by Artemis. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: The abstract and introduction provide a comprehensive overview of the background and motivation of this study, effectively outlining its main contributions, thus accurately reflecting the paper\u2019s scope and significance. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 20}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We primarily focused on discussing the limitations associated with this study in Sec. 5. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 20}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: The paper includes the theoretical result. Notably, it covers the results of information entropy and inter-frame RoI difference, ensuring completeness and accuracy in theoretical presentation. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 21}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: All information regarding the key contribution of this paper including architectural, data, and experimental configurations, have be fully disclosed. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 21}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: The supplementary material submitted with the manuscript includes scripts necessary to faithfully reproduce the main experimental results and the anonymous link includes the code and data. Instructions for running the code are also provided within the anonymous link. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 22}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: The paper specifies detailed experimental configurations in Section 4.1 in main manuscript and Section A and Section B in Appendix, providing readers with essential information to comprehend the results. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 22}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [No] ", "page_idx": 22}, {"type": "text", "text": "Justification: Generally, papers about MLLMs do not include error bars, and we have found that the MLLM\u2019s training is quite stable with little variation across multiple runs. However, we have provided the code, hyperparameters, and random seeds used in our experiments to facilitate the reproducibility of our findings. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 23}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: All experiments were carried out on an $8\\!\\times\\!\\mathrm{A800}$ GPU server, as detailed at Section 3.4. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 23}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: After carefully reviewing the referenced document, we certify that the research conducted in the paper conforms, in every respect, with the NeurIPS Code of Ethics. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 23}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: The paper primarily focuses on multimodal large language models about video understanding using publicly available datasets that have undergone thorough validation. This study only serves as a foundation model for video-based referring, which is not directly applicable for practical scenarios until now. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 24}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: The proposed models are trained on benchmark datasets such as HC-STVG, VID-Sentence, and S2D Sentences, etc. These datasets have been extensively used in the computer vision community and have undergone comprehensive safety risk assessments. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 24}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: In the paper, we clearly specified the datasets and code sources used, and provided appropriate citations in the reference section. Additionally, we ensured transparency by including the original sources of any modified code files, making the changes traceable. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 25}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We have included the code, data, along with detailed usage instructions, in a anonymous link. After the review process is completed, we will make the code and data publicly available to the community. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 25}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: This study does not involve any crowdsourcing experiments or research with human subjects. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 25}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: No crowdsourcing experiments or research with human subjects were involved in this study. All experiments were conducted using code and GPU servers. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 26}]