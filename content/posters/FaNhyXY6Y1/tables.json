[{"figure_path": "FaNhyXY6Y1/tables/tables_5_1.jpg", "caption": "Table 1: A comparison of video-based referring metrics on the HC-STVG test set. \u2020: We use 5 key frames while using 8 frames leads to worse results.", "description": "This table presents a comparison of the performance of different video-based referring models on the HC-STVG test set.  The metrics used are BERTScore, BLEU@4, METEOR, ROUGE-L, CIDEr, and SPICE, which are common evaluation metrics for assessing the quality of generated text descriptions.  The table highlights Artemis's superior performance compared to other models, achieving significantly higher scores across all metrics.  The \u2020 symbol indicates that the Merlin model uses 5 key frames for evaluation, as using 8 frames led to decreased performance.", "section": "4.1 Artemis Is a Strong Baseline for Video-based Referential Understanding"}, {"figure_path": "FaNhyXY6Y1/tables/tables_7_1.jpg", "caption": "Table 2: Ablation on different RoI selection methods. Results are reported on HC-STVG.", "description": "This table presents the results of an ablation study conducted on the HC-STVG dataset to evaluate the effectiveness of different RoI (Region of Interest) selection methods for video-based referring.  The methods compared include a baseline (no RoI selection), removing tracking instructions, uniformly sampling RoIs, randomly sampling RoIs, and using K-means clustering for RoI selection. The table shows the performance of each method across various metrics, including BLEU@4, METEOR, ROUGE_L, CIDEr, and SPICE. The results demonstrate the superiority of K-means clustering in selecting representative RoIs that effectively capture the semantic changes throughout complex videos.", "section": "4.1 Artemis Is a Strong Baseline for Video-based Referential Understanding"}, {"figure_path": "FaNhyXY6Y1/tables/tables_7_2.jpg", "caption": "Table 3: Ablation on the number of selected RoIs. Results are reported on HC-STVG.", "description": "This table presents the results of an ablation study conducted on the HC-STVG dataset to determine the optimal number of selected Regions of Interest (RoIs) for video-based referring.  The study varied the number of RoIs (1, 2, 4, 6, and 8) and measured the performance using several metrics: BERT Score, BLEU@4, METEOR, ROUGE_L, CIDEr, and SPICE. The results show that using 4 RoIs provides the best balance between performance and efficiency.", "section": "4.1 Artemis Is a Strong Baseline for Video-based Referential Understanding"}, {"figure_path": "FaNhyXY6Y1/tables/tables_8_1.jpg", "caption": "Table 1: A comparison of video-based referring metrics on the HC-STVG test set. \u2020: We use 5 key frames while using 8 frames leads to worse results.", "description": "This table compares the performance of different methods on the video-based referring task using the HC-STVG test dataset.  The metrics used are BERTScore, BLEU@4, METEOR, ROUGE-L, CIDEr, and SPICE, which are common evaluation metrics for evaluating the quality of generated text.  The table shows that the proposed Artemis model outperforms other existing methods on all metrics. The \u2020 symbol indicates that a different number of frames were used for the compared method. ", "section": "4.1 Artemis Is a Strong Baseline for Video-based Referential Understanding"}, {"figure_path": "FaNhyXY6Y1/tables/tables_9_1.jpg", "caption": "Table 1: A comparison of video-based referring metrics on the HC-STVG test set. \u2020: We use 5 key frames while using 8 frames leads to worse results.", "description": "This table compares the performance of various models on the HC-STVG video-based referring task, using metrics such as BERTScore, BLEU@4, METEOR, ROUGE-L, CIDEr, and SPICE.  It highlights the superior performance of the Artemis model compared to existing models like Osprey, Ferret, Shikra, Video-ChatGPT, Video-LLaVA, and Merlin.  The use of 5 keyframes versus 8 is noted as affecting Merlin's results. ", "section": "4.1 Artemis Is a Strong Baseline for Video-based Referential Understanding"}, {"figure_path": "FaNhyXY6Y1/tables/tables_15_1.jpg", "caption": "Table 5: Curated datasets for Video-based Referring.", "description": "This table presents the composition of the VideoRef45K benchmark dataset, which is created by curating seven existing video datasets.  It shows the number of video clips and question-answer pairs from each original dataset (HC-STVG, MeViS, A2D Sentences, LaSOT, VID Sentence, GOT-10K, MGIT),  as well as the combined totals for the VideoRef45K benchmark.", "section": "3.1 Problem Formulation and Data Preparation"}, {"figure_path": "FaNhyXY6Y1/tables/tables_15_2.jpg", "caption": "Table 6: Training hyper-parameters of Artemis.", "description": "This table details the hyperparameters used for training the Artemis model. It's divided into three sections representing the different training stages: pre-training, instruction tuning, and referring instruction tuning.  For each stage, it lists the ViT initialization, LLM initialization, projection initialization, image resolution, video feature length, LLM sequence length, optimizer, peak learning rate, minimum learning rate, learning rate schedule, weight decay, LoRA rank, number of input trackboxes, number of chosen bounding boxes, training steps, global batch size, and numerical precision used.", "section": "3.4 Model Architecture and Training"}]