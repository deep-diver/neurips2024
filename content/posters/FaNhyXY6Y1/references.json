{"references": [{"fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-07-01", "reason": "This paper introduces CLIP, a foundational model for aligning vision and language, which is directly used in Artemis for visual feature extraction."}, {"fullname_first_author": "Haotian Liu", "paper_title": "Visual Instruction Tuning", "publication_date": "2023-04-01", "reason": "This paper introduces the concept of Visual Instruction Tuning, a crucial training paradigm for multimodal LLMs which is central to Artemis' training process."}, {"fullname_first_author": "Junnan Li", "paper_title": "Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation", "publication_date": "2022-07-01", "reason": "This paper introduces BLIP, a powerful vision-language model whose architecture is partially adopted in Artemis for its multimodal understanding capabilities."}, {"fullname_first_author": "Bin Lin", "paper_title": "Video-llava: Learning united visual representation by alignment before projection", "publication_date": "2023-04-01", "reason": "This paper introduces Video-LLaVA, a strong baseline for video-based understanding which is compared against in the Artemis experiments."}, {"fullname_first_author": "Zhiliang Peng", "paper_title": "Kosmos-2: Grounding Multimodal Large Language Models to the World", "publication_date": "2023-06-01", "reason": "This paper introduces Kosmos-2, a multimodal LLM that is relevant to Artemis' design and is referenced in the discussion of related work."}]}