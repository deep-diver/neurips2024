[{"type": "text", "text": "Unleashing the power of novel conditional generative approaches for new materials discovery ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAfilfiation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "For a very long time, computational approaches to the design of new ma  \n2 terials have relied on an iterative process of finding a candidate material   \n3 and modeling its properties. AI has played a crucial role in this regard,   \n4 helping to accelerate the discovery and optimization of crystal properties   \n5 and structures through advanced computational methodologies and data  \n6 driven approaches. To address the problem of new materials design and   \n7 fasten the process of new materials search, we have applied latest generative   \n8 approaches to the problem of crystal structure design, trying to solve the   \n9 inverse problem: by given properties generate a structure that satisfies them   \n10 without utilizing supercomputer powers. In our work we propose two ap  \n11 proaches: 1) conditional structure modification: optimization of the stability   \n12 of an arbitrary atomic configuration, using the energy difference between the   \n13 most energetically favorable structure and all its less stable polymorphs and   \n14 2) conditional structure generation. We used a representation for materials   \n15 that includes the following information: lattice, atom coordinates, atom   \n16 types, chemical features, space group and formation energy of the structure.   \n17 The loss function was optimized to take into account the periodic boundary   \n18 conditions of crystal structures. We have applied Diffusion models approach,   \n19 Flow matching, usual Autoencoder (AE) and compared the results of the   \n20 models and approaches. As a metric for the study, physical pymatgen   \n21 matcher was employed: we compare target structure with generated one   \n22 using default tolerances. So far, our modifier and generator produce struc  \n23 tures with needed properties with accuracy $41\\%$ and $82\\%$ respectively. To   \n24 prove the offered methodology efifciency, inference have been carried out,   \n25 resulting in several potentially new structures with formation energy below   \n26 the AFLOW-derived convex hulls. ", "page_idx": 0}, {"type": "text", "text": "27 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "28 The search for novel materials with specified properties has been a cornerstone of scientific   \n29 exploration for decades. From the discovery of semiconductors revolutionizing electronics   \n30 to the development of superalloys enhancing aerospace technologies, the synthesis of new   \n31 materials has continually propelled technological advancements.   \n32 However, traditional methods for material discovery often employ exhaustive trial and error   \n33 experimental approaches. In turn, computational efforts, relying on density functional theory   \n34 (DFT)[1] approaches, usually require huge amounts of computing power. In this regard,   \n35 automatic descriptor generators[2], GNNs[3][4] and transferable GNN models [5] fueled   \n36 combination of these methods and machine learning (ML) approaches. In particular, the   \n37 utilization of generative machine learning models, such as Variational Autoencoder[6] and   \n38 GANs[7], presents a paradigm shift in how crystal structures are generated and optimized.   \n39 By harnessing the power of data-driven approaches, we can navigate the vast landscape of   \n40 possible crystal structures with unprecedented efifciency and precision.   \n41 Recent advancements in the field of materials discovery have yielded promising results   \n42 through various innovative approaches. For instance, FTCP[6] utilizes Autoencoders for   \n43 uncovering new materials, while CubicGAN[7] leverages GANs for the discovery of cubic   \n44 crystal materials. Additionally, Physics Guided Crystal Generative Model (PGCGM)[8] has   \n45 introduced a method for generating crystal structures based on specific space groups encoding.   \n46 DP-CDVAE[9] is a model, that combines VAE and diffusion approaches. MatterGen[10]   \n47 employed equivariant GNNs as score matching function in diffusion processes for crystal   \n48 structure generation.   \n49 One of the most discussed frameworks is GNoME[11] that has made most recent and large   \n50 advancements in the field of the new materials discovery employs a sophisticated pipeline   \nto discover new materials, particularly focusing on inorganic crystals. This allows for the   \n52 discovery of innovative materials beyond known structures.   \n53 After generating candidate structures through both pipelines, GNoME evaluates their stability   \nby predicting their formation energies. Based on the comparison of the obtained formation   \nenergy with those of the known competing phases (i.e. stability assessment), the model selects   \nthe most promising candidates for further evaluation using known theoretical frameworks.   \nThe question of the completeness of chemical space arises due to two main concerns with   \n58 GNoME-derived stable structures. Firstly, they mostly contain three or more unique elements,   \n59 while ternary and quaternary structures are less explored than binary compounds. Secondly,   \n60 the comparison of GNoME-discovered structures to the Materials Project, which has 154,718   \n61 materials, is flawed since larger databases like AFLOW, NOMAD, and the Open Quantum   \n62 Materials Database contain millions of entries. This raises questions about the novelty of   \nthe discovered materials.   \n64 In this study, we present an end-to-end framework for the generation of crystal structures with   \nspecified properties using advanced generative AI techniques. The basis architecture of the   \n66 models is Autoencoder, enabling encoding and decoding structural representations. Then, the   \n67 most commonly used generative approaches in image generation were utilized to model prob  \n68 ability distribution transformations, and to capture complex underlying structure-property   \n69 relationships within our dataset: Flow Matching[12], Denoising Diffusion Probabilistic   \n70 Models(DDPM)[13], and Denoising Diffusion Implicit Models(DDIM)[14]. Through the   \n71 integration of these techniques, we aim to transcend conventional limitations in materials   \n72 discovery, paving the way for accelerated predictions of materials with desired properties.   \nTo employ model architectures often used for image/video generation, a matrix represen  \n74 tation of crystal structures was developed, containing crucial information such as chemical   \n75 composition, atomic coordinates, symmetries (space group), and formation energies. Within   \n76 the approach proposed, it has become important to develop a novel metric for assessing   \n77 the similarity between generated structures and target configurations. This metric obviates   \n78 the need for computationally expensive DFT calculations, allowing for rapid validation and   \n79 refinement of generated structures. Furthermore, we introduce a loss function that accounts   \n80 for the periodic boundary conditions inherent in crystal lattices, ensuring the fidelity of the   \n81 generated structures.   \nOur study explores two distinct approaches for crystal structure prediction: 1) conditional   \nstructure modification and 2) conditional structure generation. The former involves optimiz  \n84 ing the stability of existing structures by generating more stable polymorphs, while the latter   \n85 entails the generation of entirely new structures based on user-defined criteria. Through   \n86 rigorous analysis, we demonstrate the efifcacy of our approach in discovering novel materials   \n87 with desired properties.   \n88 Importantly, to validate the utility of our framework, we conducted a series of generation   \n89 experiments using the Vienna Ab initio simulation package(VASP)[15] as a tool for inference   \n90 validation. Remarkably, our method facilitated the discovery of 6 structures below the   \n91 corresponding convex hull. This significant outcome underscores the remarkable potential of   \n92 our framework in uncovering thermodynamically stable materials, thereby offering promising   \n93 avenues for advanced materials discovery and design. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "94 2 Data, Dataset ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "95 2.1 Data overview ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "96 In this study, the AFLOW database[16] was utilized as a source of data on the structures   \n97 and properties of materials. AFLOW is an extensive and comprehensive database that   \n98 consolidates a vast array of materials-related information, offering an expansive repository   \n99 for crystallographic data, computed properties, and various other materials-science-related   \n100 datasets. AFLOW database contains more than 3.5 million structures.   \n101 From the extensive collection housed within AFLOW, the focus was narrowed to select only   \n102 polymorphs, because models are trained to distinguish composition-property and structure  \n103 property relations with numerous structures of the same chemical composition. Specifically,   \n104 the selection process targeted polymorphic structures with 4 to 60 atoms within their unit   \n105 cells. This criterion aimed to encompass a diverse yet manageable subset of structures,   \n106 balancing complexity with computational feasibility. By filtering polymorphs based on their   \n107 atom count, the dataset was balanced.   \n108 Moreover, in order to decrease the complexity of the data, we have removed all structures   \n109 containing elements and space groups found in less than $1\\%$ of all structures. The entire   \n110 dataset consisted of more than 85000 polymorph groups including more than 2.1 million   \n111 structures. The minimum size of group of polymorphs was 7 samples and the maximum one   \n112 was 71 samples. The total number of space groups was 19 and the total number of chemical   \n113 species over the dataset was 55. Each structure $S$ in the dataset is described by the following   \n114 features: ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "\u2022 Fractional coordinates of atoms in the lattice basis $\\mathrm{X}_{\\mathrm{coord}}$ (has 60 rows with 3 coordinates $x,y,z$ each) and Xlattice (matrix 3 by 3 constructed of 3 base vectors). Overall matrix X of structure is constructed as ", "page_idx": 2}, {"type": "text", "text": "117 ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathrm{\\Sigma_{64\\times3}^{X}}=c o n c a t a t e n a t i o n(\\mathrm{X_{coord},p a d d i n g,\\mathrm{X_{lattice}}})\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "\u2022 Chemical elements which are presented as a one-hot matrix $e l e m e n t s_{i j}$ of size $64\\times103$ (including padding), where ones are positioned at the indices corresponding to the position of a certain chemical element in the periodic table. ", "page_idx": 2}, {"type": "equation", "text": "$$\ne l e m e n t s_{i j}=\\left\\{\\begin{array}{l l}{{1}}&{{\\mathrm{if~}i{\\mathrm{~}i\\mathrm{~}i\\mathrm{~}a\\mathrm{tom}^{\\prime}\\mathrm{s~element~number~from~the~periodic~table}=j}}}\\\\ {{0}}&{{\\mathrm{otherwise}}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "118 \u2022 Elemental property matrix elementalProperties containing 22 chemical features   \n119 encoding chemical elements obtained from [8]. The properties of each element were   \n120 calculated using Mendeleev package[17].   \n121 \u2022 Space group spg of a structure. We use the space group encoding method presented in   \n122 [8], when each space group is represented by a $192\\times4\\times4$ matrix, which corresponds   \n123 to 192 possible symmetry operations.   \n124 \u2022 Structure formation energy $E$   \n125 \u2022 Nsites - number of atoms in a crystal lattice. ", "page_idx": 2}, {"type": "text", "text": "126 2.2 Data representation. Modification task ", "page_idx": 2}, {"type": "text", "text": "127 The crystal pair sampling strategy involves handling a potential data leakage: possible   \n128 inclusion of structures from the same polymorph group but with different energies into training   \n129 and validation subsets. To mitigate this issue, the polymorph group formulas were initially   \n130 divided into distinct training and validation sets, ensuring a relatively balanced distribution   \n131 of chemical elements across these subsets. Subsequently, the pairs were categorized into two   \n132 groups: those with low-energy (lowest energy in polymorph group) targets designated as   \n133 lowestEner $\\jmath y P a i r s=(S_{i},S_{0})\\forall i\\in[1,...,s t r u c t u r e s N u m]$ and those with non-low-energy   \n134 targets, all structures except the most optimal one, formed as nonLowestEnergyPairs =   \n135 $(S_{i},S_{j})|\\,\\,i>j>0$ . The validation set was constructed as a subset of lowestEnergyPairs.   \n136 The training set was dynamically constructed every epoch from lowestEnergyPairs and   \n137 nonlowestEnergyPairs, preserving equal numbers of pairs sampled and maintaining a   \n138 limited count per polymorph group. This strategy ensured a robust separation between   \n139 training and validation sets, thus preventing data leakage and improving model performance.   \n140 Each pair sample $\\{S_{i n i t},S_{t a r g e t}\\}\\in p a i r D a t a s e t$ consisted of the information about each   \n141 structure (hereinafter, we will call them initial and target structures). The following data   \n142 was used:   \n143 \u2022 Coordinates and lattice information of initial and target structures $X_{i n i t},X_{t a r g e t}$   \n144 \u2022 Difference in formation energies between initial and target structures $E_{d i f f}\\;=$   \n145 $E_{t a r g e t}-E_{i n i t}$   \n146 \u2022 Space group of target structure spgtarget   \n147 \u2022 Elements matrix elemetsMatrix, elemental property matrix elementalProperties   \n148 and number of sites numSites, which are the same for initial and target structure   \n149 because of identical chemical composition.   \n150 The modification task involved transforming the input structure $X_{i n i t}$ into the target structure   \n151 Xtarget. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "152 2.3 Data representation. Generation task ", "page_idx": 3}, {"type": "text", "text": "153 In its tern the generation task receives normal or uniform (depends on a model) noise as   \n154 input from which the structure is generated, which is akin to the image generation processes   \n155 in computer vision tasks.   \n156 For the generation task, an additional dataset was constructed. Data for the generation task   \n157 is slightly simpler, while it considers only $\\left\\{S_{t a r g e t}\\right\\}$ . Therefore, the models can be trained on   \n158 all structures available, rather than pairs. The following data is used:   \n159 \u2022 Coordinates and lattice information of target structure $X_{t a r g e t}$   \n160 \u2022 Formation energy of target structure $E_{t a r g e t}$   \n161 \u2022 Space group of target structure spgtarget   \n162 \u2022 Elements matrix elemetsMatrix, elemental property matrix elementalProperties   \n163 and number of sites numSites of target structure. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "164 3 Loss and metrics ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "165 3.1 Atomic coordinates ", "page_idx": 3}, {"type": "text", "text": "166 The atomic coordinates are represented as a $60\\times3$ matrix, where each row corresponds   \n167 to the coordinates of an atom. The L1 loss was utilized during the training of a model for   \n168 predicting atomic coordinates.   \n169 $\\begin{array}{r}{L_{1}(p r e d s,t a r g e t)_{i}=||p r e d s_{i}-t a r g e t_{i}||_{1}=\\sum_{j=1}^{3}|p r e d s_{i j}-t a r g e t_{i j}|}\\end{array}$ , where target and pred   \n170 are target and predicted atomic coordinate matrices. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "171 3.2 Lattice ", "page_idx": 3}, {"type": "text", "text": "172 The lattice itself is represented as a 3x3 matrix, where each row signifies a directing basis   \n173 vector. In this case, we have also used the L1 norm as a loss function. ", "page_idx": 3}, {"type": "text", "text": "174 3.3 Periodic boundary condition loss ", "page_idx": 3}, {"type": "text", "text": "175 This section presents an enhanced loss function, designed for the regression model (see   \n176 Section 5.1), that addresses this challenge by integrating periodic boundary conditions into   \n177 the loss calculation, outperforming the conventional L1 loss function. In the field of ML   \n178 applied to atomic structures, even slight displacement of atomic coordinates is crucial and   \n179 employing appropriate loss functions that consider the periodic nature of atomic structures   \n180 increases the flexibility of model predictions.   \n181 In the dataset representing atomic structures, it is crucial to acknowledge the presence   \n182 of atoms residing at various positions within the lattice framework. Certain atoms are   \n183 positioned at the vertices, edges, or faces of the lattice. According to periodic boundary   \n184 conditions (PBC), identical atoms in the vicinity of vertices, edges, or faces but also exist in   \n185 analogous positions across the lattice. Implementation of such an invariance within the loss   \n86 function helps in effectively capturing periodic pattern of crystals, enhancing the model\u2019s   \n187 capability to learn and predict atomic structures more comprehensively.   \n188 The loss function is being calculated as minimum of distances from predicted point to the   \n189 target one taking into account 26 its periodic images (according to PBC) A.4.   \n190 The empirical validation of this enhanced loss function showcases its superiority(Figure4) in   \n191 capturing discrepancies within atomic structures, thus indicating its potential as a robust   \n192 tool for improving the accuracy of ML models in materials science applications. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "image", "img_path": "I3kIEjoON6/tmp/0c3d6e461dda8bf64836e19c65d0ebe7fff55edfc0ebd70bb2c3457cee6b86da.jpg", "img_caption": ["Figure 1: Illustration of atoms at a)vertices, b)edges, and c)faces of lattice under periodic boundary conditions "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "193 3.4 Metric ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "194 As a metric, we have chosen an analogue of accuracy: the generated structures are compared   \n195 to the target structures using a specialized matcher, yielding the proportion of structures that   \n196 successfully pass the matching process. For metric calculation, we employed the Pymatgen   \n197 StructureMatcher with the default set of parameters ( $l t o l=0.2,s t o l=0.3,a n g l e\\_t o l=5$ ).   \n198 Although this approach is less accurate than structure relaxation using ab initio calculations   \n199 and comparing the structure formation energy with the energy above the hull, it enables   \n200 model validation to be performed orders of magnitude faster than the traditional method. ", "page_idx": 4}, {"type": "text", "text": "201 4 Model ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "202 For experiments, a 1d UNet model (see Figure2 (b)) architecture similar to the 2d UNet model   \n203 described in [18] was utilized along with 2D and 1D convolutional neural networks (CNNs)   \n204 for the space group and element matrix embeddings, respectively. Based on this model, 3   \n205 different training processes have been developed: ordinary regression model, Conditional   \n206 Flow Matching (CFM)[19] model, and diffusion model.   \n207 The model was conditioned (see Figure2 (a)) on the following data: time condition $(t)$ , the   \n208 same as in [18], element condition ( $e l$ ), formation energy difference condition $(E_{d i f f})$ , and   \n209 desirable space group (spg). $e l_{e m b}$ , $s p g_{e m b}$ and $E_{d i f f}$ are concatenated into one embedding   \n210 $C_{e m b}$ . $t$ is fed into the Transformer Positional Encoding Layer and transformed into an   \n211 embedding $T_{e m b}$ . The two embeddings: $C_{e m b}$ and $T_{e m b}$ are then applied into one condition. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "image", "img_path": "I3kIEjoON6/tmp/f02f4d51136e3e86d0280af83fa3f3feab0d4c00ec6d282d07c0281038196dec.jpg", "img_caption": ["Figure 2: a)Formation of conditions using formation energy, space group, and elemental representation, and b)Schematic depiction of the model architecture "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "212 5 Methodology ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "213 In this work, two approaches are proposed: crystal structure generation and crystal structure   \n214 modification. For the generation approach, crystal structures are generated from normal   \n215 or uniform noise and conditioned to $t$ , el, $E$ , spg. Within the generation, we employed   \n216 three algorithms: DDPM, DDIM, and CFM models. For the modification approach, crystal   \n217 structures are generated by modifying other structures, while conditioning to el, $E_{d i f f}$ , spg   \n218 (and optionally $t$ , not used in ordinary regression UNet). For the modification task, we have   \n219 employed three algorithms: UNet Regression model, diffusion model, based on Palette[20]   \n220 approach, and CFM model. For the generation task, we have employed four algorithms:   \n221 diffusion models with DDPM and DDIM samplers, and CFM models on Uniform and Normal   \n222 noise. ", "page_idx": 5}, {"type": "text", "text": "223 5.1 Regression model ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "224 During the training stage, the structure coordinates and lattice $x_{0}$ , elements features $e l$ , space   \n225 group spg and $E_{d i f f}$ are used as conditions. The model is trained to return $x_{1}$ structure   \n226 coordinates and lattice (Algorithm 1). As for the inference process, one can see the details   \n227 in the Algorithm 2 ", "page_idx": 5}, {"type": "text", "text": "228 5.2 Conditional Flow Matching models ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "229 CFM is a fast method for training Continuous Normalizing Flows (CNF)[21] models without   \n230 the need for simulations. It offers a training objective that enables conditional generative   \n231 modeling and accelerates both training and inference.   \n232 The basic way of training CFM model (Algorithm 3) organized as follows: during the   \n233 training stage, $x_{0}$ and $x_{1}$ are sampled from the source distribution and the target distribution   \n234 respectively, then a linear interpolation $x_{t}$ is calculated as $x_{t}=t x_{1}+(1-t)x_{0}$ (exponential   \n235 moving average between distributions $x_{0}$ and $x_{1}$ ; $t$ is sampled from a uniform distribution   \n236 $\\mathcal{U}(0,1).$ ), and afterwards pass the $x_{t}$ and $t$ as inputs to our model $f_{\\theta}$ , forcing the model to   \n237 predict a velocity from the distribution $x_{0}$ to $x_{1}$ . Therefore, the loss for CFM model is   \n238 the following: $L_{C F M}=E_{t,x_{1},x_{0}}[||f_{\\theta}(x_{t},t)-(x_{1}-x_{0})||^{2}]=E_{t,x_{1},x_{0}}[||f_{\\theta}(t x_{1}+(1-t)x_{0},t)-(x_{1}-x_{0})||^{2}]$   \n239 $(x_{1}-x_{0})||^{2}]$   \n240 For the modification approach, $x_{0}$ and $x_{1}$ are both sampled from our dataset distribution   \n241 according to the sampling strategy for modification mentioned in 2.2. Also, the model is   \n242 conditioned to $e l,s p g_{1},E_{d i f f}$ , besides $t$ (see Algorithm 4)   \n243 For the generation approach, we tested two noise distributions for the $x_{0}$ : normal distribu  \n244 tion $\\mathcal{N}(0,1)$ and uniform noise distribution $\\mathcal{U}(0,1)$ , which resulted in significantly better   \n245 performance. The intuition for using uniform distribution instead of normal one was inspired   \n246 by the diagram of x, y, z coordinate distribution (Figure 3). The model is also conditioned   \n247 to $e l,s p g_{1},E$ , and $t$ (see Algorithm 5)   \n248 During the sampling stage, we generate $X_{1}$ structure by the given $X_{0}$ by solving the following   \n249 ordinary differential equation (ODE): $d x_{t}=f_{\\theta}(x_{t},t,e l,s p g_{1},E)d t$ , beginning with $x_{0}$ . In   \n250 order to solve the ODE, the Euler method was employed: $x_{t+h}=x_{t}+h f_{\\theta}(x_{t},t,e l,s p g_{1},E)$   \n251 (Algorithm 6) ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "252 5.3 Diffusion models ", "page_idx": 6}, {"type": "text", "text": "253 In our work, we observe diffusion models. Diffusion models generate samples from a target   \n254 distribution $x_{1}$ , starting from a source distribution $x_{0}\\sim\\mathcal{N}(0,I)$ .   \n255 During training, these models are trained to reverse a Markovian forward process, which   \n256 adds noise $x_{0}$ to the data step by step. Meaning, diffusion models are trained to predict the   \n257 noise added to the data samples $x_{1}$ . In order to train a model in this setup, the following loss   \n258 function is used, $L_{s i m p l e}=E_{t,x_{1},x_{0}}[||x_{0}-f_{\\theta}(\\sqrt{\\bar{\\alpha_{t}}}x_{1}+\\sqrt{1-\\bar{\\alpha_{t}}}x_{0},t)||^{2}]$ where $\\begin{array}{r}{\\bar{\\alpha_{t}}=\\prod_{s=1}^{t}\\alpha_{s}}\\end{array}$   \n259 and $\\alpha_{t}=1-\\beta_{t}$ ( $\\beta_{t}$ is the variance by which added noise is being scheduled on each step $t$ ).   \n260 Our modification approach is based on Palette, which enables sample-to-sample generation   \n261 (from noise $\\mathbf{\\boldsymbol{\\epsilon}}\\sim\\mathcal{N}(\\mathbf{\\boldsymbol{0}},\\mathbf{\\boldsymbol{1}}))$ using $x_{0}$ structure coordinates and lattice, $e l$ , $s p g_{1}$ , $E_{d i f f}$ and $t$ as   \n262 conditions for generation of $x_{1}$ using the DDPM algorithm. Sampling stage is performed by   \n263 a backward diffusion process with linear scheduler (see Algorithms 7, 8).   \n264 For the generation approach ((Algorithm 9), $x_{0}$ is sampled from a normal distribution and   \n265 el, $s p g_{1}$ , $E$ , $t$ are fed into the model as conditions. During our experiments, we tested   \n266 2 approaches: DDPM(Algorithm 10) classic approach and DDIM(Algorithm 11) which   \n267 results in usage of smaller number of sampling steps in order to speed up the generation   \n268 process. Moreover, DDIM enables the process of generating samples from random noise to   \n269 be deterministic. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "270 6 Experiment Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "271 All the models presented in tables (Table 1 and Table 2) have been trained with the same   \n272 hyperparameters and architectures. The metric used is described in Section 3.4. We also   \n273 provide all experiment details in A.3. ", "page_idx": 6}, {"type": "table", "img_path": "I3kIEjoON6/tmp/b0f35693ad3a277feeca1d443eaf10e83d644b48fff858445b30e63d77fa24f4.jpg", "table_caption": ["Table 1: Validation metrics on generation task "], "table_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "I3kIEjoON6/tmp/e8580e3b867a5886665b7cbf150010fc35a9b38c4b5b05980f0744d08372ef73.jpg", "table_caption": ["Table 2: Validation metrics on modification task "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "274 7 Inference ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "275 In order to demonstrate a potential of the proposed approaches, we have chosen a chemical   \n276 composition, containing numerous variations and phases of structures composed of [W, B,   \n277 Ta] with well-explored convex hull. Structures that lie on the convex hull are considered to   \n278 be thermodynamically stable, and the ones above it are either metastable or unstable.   \n280 The proposed testing procedure involves generating test conditions for structures, passing   \n281 them to the trained generative models, pre-optimizing the generated structures to accelerate   \n282 the following ab initio calculations, and final relaxation and formation energy calculating using   \n283 VASP. Although in this work two approaches were proposed: Generation and Modification,   \n284 the following pipeline has only been applied to generation models, due to the fact, that   \n285 modification approach is based on structure-polymorphs, which leads to the necessity to   \n286 have at least one structure with needed composition, which is not always so. That fact   \n287 makes generation models much more flexible in generation structures not only with needed   \n288 properties, but also with needed composition. Another advantage of the generation models is   \n289 value of metric that is two times bigger than in modification tasks. The inference algorithm   \n290 is as follows:   \n292 \u2022 The chosen chemical formulas were utilized for feature extraction of $e l$ . Three   \n293 chemical compositions have been used: 1) Ta1W1B6, 2) Ta1W2B $^{5}$ and 3)   \n294 $\\mathrm{Ta_{2}W_{1}B_{5}}$ .   \n295 \u2022 We have taken spg presented in the dataset as an additional condition, obtaining   \n296 19 space groups.   \n297 \u2022 Finally, a set of target formation energies $E$ has been formed. We have carried   \n298 out three experiments: 1) starting from the energy of the convex hull and   \n299 decreasing with a step of 0.01 eV/atom, 2) starting from the energy of the   \n300 convex hull and decreasing with a step of 0.1 eV/atom, and 3) starting from the   \n301 energy 1 eV/atom less than the energy of the convex hull and decreasing with a   \n302 step of 0.01 eV/atom. In total, 21 energy values were used for every inference   \n303 run.   \n304 \u2022 Final inference conditions were obtained by making all possible combinations of   \n305 spg and $E$ for a certain composition el   \n306 2. Model Inference: The conditions from the step 1 have been put to one of the trained   \n307 models, resulting in the generation of structures. Two models have been employed:   \n308 Diffusion approach and Flow matching   \n309 3. Pre-Optimization: Following the generation of all structures, each structure has been   \n310 pre-optimized using the PyMatGen structure relaxation method. The method used   \n311 m3gnet [22] model with default parameters. PyMatGen pre-optimization contributed   \n312 to overall speedup of further VASP relaxation.   \n313 4. Structure relaxation: Pre-optimized structures were relaxed using VASP (the rec  \n314 ommended pseudopotentials, plane wave energy cutoff of 500 eV, Ediif and Ediffg   \n315 convergence criteria of $10^{-5}$ and $-10^{-2}$ were used). ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "316 7.2 Inference results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "317 To summarize, 6 experiments have been carried out for two different models and for three   \n318 formation energy conditionings. Every experiment includes 3\\*380 structures, per 380   \n319 structures for every single chemical composition. The results of experiments can be seen in   \n320 Table 3   \n321 As can be seen, 4 structures were obtained with formation energies significantly lower than   \n322 those obtained from the AFLOW-derived convex hull. Thus, it can be concluded that   \n323 this observation indicates the potential stability of the generated structures rather than   \n324 differences in the computational methods used in this work and during AFLOW generation.   \n325 Another four structures also have energies below the convex hull, but in the vicinity of it.   \n326 Thus, their potential stability should be interpreted with caution. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "327 8 Data availability ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "328 The raw crystal dataset is downloaded from   \n329 https://aflowlib.org ", "page_idx": 7}, {"type": "table", "img_path": "I3kIEjoON6/tmp/4a87f6c2675af401e04acfc1f2b81c3b25f159832a364bf937ccf3de5e40cfb6.jpg", "table_caption": ["Table 3: Inference results. Each matrix element corresponds to either the minimal energy above the hull achieved in an experiment or the energy above the hull of structures with energies below the hull. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "330 9 Code availability ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "331 The source code for training and inferencing our models can be obtained from GitHub at   \n332 https://github.com/AIRI-Institute/conditional-crystal-generation ", "page_idx": 8}, {"type": "text", "text": "333 10 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "334 In this article, we have offered two approaches to generate crystal structures: conditional   \n335 generation and conditional modification. The first approach is significantly more flexible   \n336 as it does not require structure-polymorphs, enabling the generation of structures without   \n337 restrictions on chemical composition, which can be crucial in certain scenarios. Another   \n338 advantage of the first approach is the simplicity of data preprocessing; it only requires the   \n339 chemical composition, space group, atom coordinates, and formation energies.   \n340 Our methodology has experimentally proven its effectiveness, resulting in four confident   \n341 potentially new crystal structures with the following energies above the hull: $\\{$ {-1.409, -5.497,   \n342 -5.426, and -4.852} meV/atom, and four uncertain candidates with energies of $\\{$ -0.483, -0.466,   \n343 -0.387, and -0.042} meV/atom. We have demonstrated that conditional generation approaches,   \n344 commonly used in image generation, are also fruitful in the design of new materials.   \n345 Although the proposed methodology demonstrates its efifciency in generating potentially   \n346 new crystal structures, it has certain limitations. Firstly, the data is represented in a matrix   \n347 form, which does not account for all possible symmetries of the crystal structures. Secondly,   \n348 the structures in the dataset range from 4 to 60 atoms per unit cell, with most structures   \n349 containing fewer than 8 atoms per unit cell. However, to perform well on structures with a   \n350 large number of atoms per unit cell, the models just should be pretrained on a dataset that   \n351 includes larger structures.   \n352 Furthermore, despite the limited number of experiments(6) and structures generated (7182),   \n353 we succeeded in identifying hypothetically new structures. We hope that our article will   \n354 help to reveal the potential of generative AI in design of new materials with targeted   \n355 thermodynamic properties and inspire other researchers to be part of this innovative journey   \n356 in materials design. We believe that rapid and efifcient generation of novel materials can lead   \n357 to breakthroughs in various fields such as electronics, pharmaceuticals, and energy storage.   \n358 This can accelerate technological advancements and make cutting-edge technologies more   \n359 accessible and affordable.   \n360 References   \n361 [1] Diola Bagayoko. Understanding density functional theory (dft) and completing it in   \n362 practice. AIP Advances, 4(12), 2014.   \n363 [2] Alexander Dunn, Qi Wang, Alex Ganose, Daniel Dopp, and Anubhav Jain. Benchmark  \n364 ing materials property prediction methods: the matbench test set and automatminer   \n365 reference algorithm. npj Computational Materials, 6(1):138, 2020.   \n366 [3] Roman A Eremin, Innokentiy S Humonen, Alexey A Kazakov, Vladimir D Lazarev,   \n367 Anatoly P Pushkarev, and Semen A Budennyy. Graph neural networks for predicting   \n368 structural stability of cd-and zn-doped $\\gamma$ -cspbi3. Computational Materials Science,   \n369 232:112672, 2024.   \n370 [4] Alexey N Korovin, Innokentiy S Humonen, Artem I Samtsevich, Roman A Eremin,   \n371 AI Vasilev, Vladimir D Lazarev, and Semen A Budennyy. Boosting heterogeneous   \n372 catalyst discovery by structurally constrained deep learning models. Materials Today   \n373 Chemistry, 30:101541, 2023.   \n374 [5] Alexandre Duval, Simon V Mathis, Chaitanya K Joshi, Victor Schmidt, Santiago   \n375 Miret, Fragkiskos D Malliaros, Taco Cohen, Pietro Li\\`o, Yoshua Bengio, and Michael   \n376 Bronstein. A hitchhiker\u2019s guide to geometric gnns for 3d atomic systems. arXiv preprint   \n377 arXiv:2312.07511, 2023.   \n378 [6] Zekun Ren, Siyu Isaac Parker Tian, Juhwan Noh, Felipe Oviedo, Guangzong Xing, Jiali   \n379 Li, Qiaohao Liang, Ruiming Zhu, Armin G Aberle, Shijing Sun, et al. An invertible   \n380 crystallographic representation for general inverse design of inorganic crystals with   \n381 targeted properties. Matter, 5(1):314\u2013335, 2022.   \n382 [7] Yong Zhao, Mohammed Al-Fahdi, Ming Hu, Edirisuriya MD Siriwardane, Yuqi Song,   \n383 Alireza Nasiri, and Jianjun Hu. High-throughput discovery of novel cubic crystal   \n384 materials using deep generative neural networks. Advanced Science, 8(20):2100566,   \n385 2021.   \n386 [8] Yong Zhao, Edirisuriya M Dilanga Siriwardane, Zhenyao Wu, Nihang Fu, Mohammed   \n387 Al-Fahdi, Ming Hu, and Jianjun Hu. Physics guided deep learning for generative design   \n388 of crystal materials with symmetry constraints. npj Computational Materials, 9(1):38,   \n389 2023.   \n390 [9] Teerachote Pakornchote, Natthaphon Choomphon-Anomakhun, Sorrjit Arrerut,   \n391 Chayanon Atthapak, Sakarn Khamkaeo, Thiparat Chotibut, and Thiti Bovornratanaraks.   \n392 Diffusion probabilistic models enhance variational autoencoder for crystal structure   \n393 generative modeling. Scientific Reports, 14(1):1275, 2024.   \n394 [10] Claudio Zeni, Robert Pinsler, Daniel Zu\u00a8gner, Andrew Fowler, Matthew Horton, Xiang   \n395 Fu, Sasha Shysheya, Jonathan Crabb\u00b4e, Lixin Sun, Jake Smith, et al. Mattergen: a   \n396 generative model for inorganic materials design. arXiv preprint arXiv:2312.03687, 2023.   \n397 [11] Amil Merchant, Simon Batzner, Samuel S Schoenholz, Muratahan Aykol, Gowoon   \n398 Cheon, and Ekin Dogus Cubuk. Scaling deep learning for materials discovery. Nature,   \n399 624(7990):80\u201385, 2023.   \n400 [12] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow   \n401 matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022.   \n402 [13] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In   \n403 H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in   \n404 Neural Information Processing Systems, volume 33, pages 6840\u20136851. Curran Associates,   \n405 Inc., 2020.   \n406 [14] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models.   \n407 In International Conference on Learning Representations, 2021.   \n408 [15] Guangyu Sun, Jen\u00a8o K\u00a8urti, P\u00b4eter Rajczy, Miklos Kertesz, J\u00a8urgen Hafner, and Georg   \n409 Kresse. Performance of the vienna ab initio simulation package (vasp) in chemical   \n410 applications. Journal of Molecular Structure: THEOCHEM, 624(1-3):37\u201345, 2003.   \n411 [16] Stefano Curtarolo, Wahyu Setyawan, Gus LW Hart, Michal Jahnatek, Roman V Chep  \n412 ulskii, Richard H Taylor, Shidong Wang, Junkai Xue, Kesong Yang, Ohad Levy, et al.   \n413 Aflow: An automatic framework for high-throughput materials discovery. Computational   \n414 Materials Science, 58:218\u2013226, 2012.   \n415 [17] \u0141ukasz Mentel. mendeleev \u2013 a python resource for properties of chemical elements, ions   \n416 and isotopes.   \n417 [18] Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion proba  \n418 bilistic models. In International Conference on Machine Learning, pages 8162\u20138171.   \n419 PMLR, 2021.   \n420 [19] Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le.   \n421 Flow matching for generative modeling. In The Eleventh International Conference on   \n422 Learning Representations, 2023.   \n423 [20] Chitwan Saharia, William Chan, Huiwen Chang, Chris A. Lee, Jonathan Ho, Tim   \n424 Salimans, David J. Fleet, and Mohammad Norouzi. Palette: Image-to-image diffusion   \n425 models, 2022.   \n426 [21] Ricky T. Q. Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural   \n427 ordinary differential equations. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman,   \n428 N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing   \n429 Systems, volume 31. Curran Associates, Inc., 2018.   \n430 [22] Chi Chen and Shyue Ong. A universal graph deep learning interatomic potential for   \n431 the periodic table. Nature Computational Science, 2:718\u2013728, 11 2022. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "432 A Appendix section 1 ", "page_idx": 11}, {"type": "text", "text": "433 A.1 Distribution of atomic coordinates ", "page_idx": 11}, {"type": "image", "img_path": "I3kIEjoON6/tmp/117fb7a9e6cc3bdbbd7c40bf51f5cd6add3d56e4402a79b1edb3b90808874e56.jpg", "img_caption": ["Figure 3: Distribution of the components of fractional atomic coordinates (X, Y, Z) "], "img_footnote": [], "page_idx": 11}, {"type": "text", "text": "434 A.2 Pseudocode ", "page_idx": 11}, {"type": "text", "text": "Algorithm 1 Training Regression Modification Model ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "1: repeat   \n2: $x_{0}\\sim q(x_{0});x_{1}\\sim q(x_{1});e l\\sim q(e l);s p g_{1}\\sim q(s p g_{1});E\\sim q(E)$   \n3: $\\mathcal{L}\\gets||x_{1}-f_{\\theta}(x_{0},t,e l,s p g_{1},E)||$   \n4: $\\theta\\gets U p d a t e(\\theta,\\nabla_{\\theta}\\mathcal{L}(\\theta))$   \n5: until converge ", "page_idx": 11}, {"type": "text", "text": "Algorithm 2 Inferencing Regression Modification Model ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "1: $x_{0}\\sim q(x_{0});e l\\sim q(e l);s p g_{1}\\sim q(s p g_{1});E\\sim q(E)$   \n2: $x_{1}=f_{\\theta}(x_{0},t,e l,s p g_{1},E)$   \n3: return $x_{1}$ ", "page_idx": 11}, {"type": "text", "text": "Algorithm 3 CFM Training ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "1: repeat   \n2: $x_{0}\\sim q(x_{0});x_{1}\\sim q(x_{1})$   \n3: $t\\sim\\mathcal{U}(0,1)$   \n4: $x_{t}=t x_{1}+(1-t)x_{0}$   \n5: $\\mathcal{L}_{C F M}\\gets||f_{\\theta}(x_{t},t)-(x_{1}-x_{0})||$   \n6: $\\theta\\leftarrow U p d a t e(\\theta,\\nabla_{\\theta}\\mathcal{L}_{C F M}(\\theta))$   \n7: until converge ", "page_idx": 11}, {"type": "text", "text": "Algorithm 4 Training CFM for Modification ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "1: repeat   \n2: $\\begin{array}{l}{{x_{0}\\sim q(x_{0});x_{1}\\sim q(x_{1});e l\\sim q(e l);s p g_{1}\\sim q(s p g_{1});E\\sim q(E)}}\\\\ {{t\\sim\\mathcal{U}(0,1)}}\\\\ {{x_{t}=t x_{1}+(1-t)x_{0}}}\\\\ {{\\mathcal{L}_{C F M}\\leftarrow||f_{\\theta}(x_{t},t,e l,s p g_{1},E)-(x_{1}-x_{0})||}}\\\\ {{\\theta\\leftarrow U p d a t e(\\theta,\\nabla_{\\theta}\\mathcal{L}_{C F M}(\\theta))}}\\end{array}$   \n3:   \n4:   \n5:   \n6:   \n7: until converge ", "page_idx": 11}, {"type": "text", "text": "Algorithm 5 Training CFM for Generation ", "page_idx": 12}, {"type": "text", "text": "1: repeat   \n2: $x_{0}\\sim\\mathcal{N}(0,1)$ or $x_{0}\\sim\\mathcal{U}(0,1)$   \n3: $x_{1}\\sim q(x_{1});e l\\sim q(e l);s p g_{1}\\sim q(s p g_{1});E\\sim q(E)$   \n4: $t\\sim\\mathcal{U}(0,1)$   \n5: $x_{t}=t x_{1}+(1-t)x_{0}$   \n6: $\\mathcal{L}_{C F M}\\gets||f_{\\theta}(x_{t},t,e l,s p g_{1},E)-(x_{1}-x_{0})||$   \n7: $\\theta\\leftarrow U p d a t e(\\theta,\\nabla_{\\theta}\\mathcal{L}_{C F M}(\\theta))$   \n8: until converge ", "page_idx": 12}, {"type": "text", "text": "Algorithm 6 Sampling with CFM for Modification or Generation ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "1: $\\begin{array}{r}{h={\\frac{1}{T}}}\\end{array}$   \n2: $x_{0}\\sim q(x_{0})$ or $x_{0}\\sim\\mathcal{N}(0,1)$ or $x_{0}\\sim\\mathcal{U}(0,1)$   \n3: $e l\\sim q(e l);s p g_{1}\\sim q(s p g_{1});E\\sim q(E)$   \n4: for dot = 1, . . . , $T$ do   \n5: $x_{t+1}=x_{t}+h f_{\\theta}(x_{t},t,e l,s p g_{1},E)$   \n6: end for   \n7: return $x_{1}$ ", "page_idx": 12}, {"type": "text", "text": "Algorithm 7 Training DM for Modification ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "1: repeat   \n2: $x_{0}\\sim q(x_{0});x_{1}\\sim q(x_{1});e l\\sim q(e l);s p g_{1}\\sim q(s p g_{1});E\\sim q(E)$   \n3: $t\\sim\\mathcal{U}(\\{1,\\cdot\\cdot\\cdot,T\\})$   \n4: $\\epsilon\\sim\\mathcal{N}(0,I)$   \n5: LD $-\\ ||\\epsilon-f_{\\theta}(\\sqrt{\\bar{\\alpha_{t}}}x_{1}+\\sqrt{1-\\bar{\\alpha_{t}}}\\epsilon,x_{0},t,e l,s p g_{1},E)|$   \n6: $\\theta\\gets U p d a t e(\\theta,\\nabla_{\\theta}\\mathcal{L}_{D}(\\theta))$   \n7: until converge ", "page_idx": 12}, {"type": "text", "text": "Algorithm 8 Sampling with DM for Modification ", "page_idx": 12}, {"type": "text", "text": "1: $x_{T}\\sim\\mathcal{N}(0,I)$   \n2: for $\\mathrm{do}t=T,\\dots1$ do   \n3: $x_{0}\\sim q(x_{0});x_{1}\\sim q(x_{1});e l\\sim q(e l);s p g_{1}\\sim q(s p g_{1});E\\sim q(E)$   \n4: $z\\sim\\mathcal{N}(0,I)$ if $t>1$ else $\\mathbf{Z}\\,=\\,0$   \n5: $\\begin{array}{r}{x_{t-1}=\\frac{1}{\\sqrt{\\alpha_{t}}}\\big(x_{t}-\\frac{1-\\alpha_{t}}{\\sqrt{1-\\alpha_{t}}}f_{\\theta}\\big(x_{t},x_{0},t,e l,s p g_{1},E\\big)\\big)+\\sqrt{1-\\alpha_{t}}z}\\end{array}$   \n6: end for   \n7: return $x_{1}$ ", "page_idx": 12}, {"type": "text", "text": "Algorithm 9 Training DM for Generation ", "page_idx": 12}, {"type": "text", "text": "1: repeat   \n2: $x_{1}\\sim q(x_{1});e l\\sim q(e l);s p g_{1}\\sim q(s p g_{1});E\\sim q(E)$   \n3: $t\\sim\\mathcal{U}(\\{1,\\cdot\\cdot\\cdot,T\\})$   \n4: $\\epsilon\\sim\\mathcal{N}(0,I)$   \n5: $\\mathcal{L}_{D}\\gets\\lvert\\lvert\\epsilon-f_{\\theta}(\\sqrt{\\bar{\\alpha_{t}}}x_{1}+\\sqrt{1-\\bar{\\alpha_{t}}}\\epsilon,t,e l,s p g_{1},E)$ ||   \n6: $\\theta\\leftarrow U p d a t e(\\theta,\\nabla_{\\theta}\\mathcal{L}_{D}(\\theta))$   \n7: until converge ", "page_idx": 12}, {"type": "text", "text": "Algorithm 10 DDPM Sampling ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "1: $x_{T}\\sim\\mathcal{N}(0,I)$   \n2: for $\\mathrm{do}t=T,\\dots1$ do   \n3: $x_{1}\\sim q(x_{1});e l\\sim q(e l);s p g_{1}\\sim q(s p g_{1});E\\sim q(E)$   \n45:: $z\\sim\\mathcal{N}(0,I)$ $t>1$ $\\mathbf{Z}\\,=\\,0$   \n$\\begin{array}{r}{x_{t-1}=\\frac{1}{\\sqrt{\\alpha_{t}}}\\big(x_{t}-\\frac{1-\\alpha_{t}}{\\sqrt{1-\\bar{\\alpha_{t}}}}f_{\\theta}\\big(x_{t},t,e l,s p g_{1},E\\big)\\big)+\\sqrt{1-\\alpha_{t}}z}\\end{array}$   \n6: end for   \n7: return $x_{1}$   \n1: $x_{T}\\sim\\mathcal{N}(0,I)$   \n2: for $\\mathrm{do}t=T,\\dots1$ with step C do   \n3: $x_{0}\\sim q(x_{0});x_{1}\\sim q(x_{1});e l\\sim q(e l);s p g_{1}\\sim q(s p g_{1});E\\sim q(E)$   \n4: $z\\sim\\mathcal{N}(0,I)$ if $t>1$ else $\\mathbf{Z}\\,=\\,0$   \n5: $x_{\\theta}=f_{\\theta}(x_{t},t,e l,s p g_{1},E)$   \n6: $\\begin{array}{r}{x_{t-1}=\\sqrt{\\alpha_{t-1}}\\big(\\frac{x_{t}-\\sqrt{1-\\alpha_{t}}x_{\\theta}}{\\sqrt{\\alpha_{t}}}\\big)+\\sqrt{1-\\alpha_{t-1}-\\sigma_{t}^{2}}x_{\\theta}+\\sigma_{t}z}\\end{array}$   \n7:   \n8: end for   \n9: return $x_{1}$ ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "435 A.3 Experiment Details ", "page_idx": 13}, {"type": "text", "text": "436 All the experiments use the same hyperparameters for the model: ", "page_idx": 13}, {"type": "text", "text": "437 $\\begin{array}{l}{{\\bf\\bullet}\\ \\mathrm{num\\_res}_{-}\\mathrm{blocks}=7}\\\\ {{\\bf\\bullet}\\ \\mathrm{attention}_{-}\\mathrm{resolution}=(1,\\,2,\\,4,\\,8)}\\\\ {{\\bf\\bullet}\\ \\mathrm{model}_{-}\\mathrm{channels}=128}\\end{array}$   \n438   \n439 ", "page_idx": 13}, {"type": "text", "text": "440 In all the experiments models are trained with the same training parameters: ", "page_idx": 13}, {"type": "text", "text": "441 ${\\begin{array}{r l}&{\\bullet\\ \\mathrm{optimizer}=\\mathrm{Adam}}\\\\ &{\\qquad-\\ \\mathrm{betas}=(0.9,0.999)}\\\\ &{\\qquad-\\ \\mathrm{eps}=\\mathrm{le}\\cdot\\mathrm{d}8}\\\\ &{\\qquad-\\ \\mathrm{weight}_{-}\\mathrm{decay}=0}\\\\ &{\\bullet\\ \\mathrm{batch}_{-}\\mathrm{size}=256}\\\\ &{\\bullet\\ \\mathrm{epochs}=400}\\\\ &{\\bullet\\ \\mathrm{learning}_{-}\\mathrm{rate}=1\\mathrm{e}\\cdot4}\\\\ &{\\bullet\\ \\mathrm{lr}_{-}\\mathrm{warning}_{-}\\mathrm{siteps}=500}\\\\ &{\\bullet\\ \\mathrm{random}_{\\ s i t e r}=42}\\end{array}}$   \n442   \n443   \n444   \n445   \n446   \n447   \n448   \n449   \n450 An important note, that all our experiments have been conducted in mixed precision in fp16.   \n451 Generation task: Diffusion Model (DDPM):   \n452 $\\begin{array}{l}{{\\bf{\\Psi}^{\\bullet}\\ n u m\\_t r a i n\\_t i m e s t e p s=1000\\ (d i f f u s i o n\\ p r o c e s s\\ d i s c r e t i n}}\\\\ {{\\bf{\\Psi}^{\\bullet}\\ b e t a\\_s t a r t=0.0001}}\\\\ {{\\bf{\\Psi}^{\\bullet}\\ b e t a\\_e n d=0.02}}\\\\ {{\\bf{\\Psi}^{\\bullet}\\ n u m\\_l n f e r e n c e\\_s t e p s=100}}\\\\ {{\\bf{\\Psi}^{\\bullet}\\ b e t a\\_s c h e d u l e=\\Psi^{\\vee}s q u a r e d c o s\\_c a p\\_v2\"\\ (\\ c o s i n e)}}\\end{array}$ zation)   \n453   \n454   \n455   \n456 ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "457 Diffusion Model (DDIM): ", "page_idx": 13}, {"type": "text", "text": "458 ${\\begin{array}{r l}&{\\bullet\\ \\operatorname*{num}_{-}\\operatorname{train}_{-}\\operatorname{timesteps}=1000\\ \\mathrm{(diffusion~process~of~}}\\\\ &{\\bullet\\ \\operatorname{beta}_{-}\\operatorname{start}=0.0001}\\\\ &{\\bullet\\ \\operatorname{beta}_{-}\\operatorname{end}=0.02}\\\\ &{\\bullet\\ \\operatorname*{num}_{-}\\operatorname{inference}_{-}\\operatorname{steps}=100}\\\\ &{\\bullet\\ \\operatorname{beta}_{-}\\operatorname{schedule}=\"*\\operatorname{squaredcos}_{-}\\operatorname{cap}_{-}\\operatorname{v2}\"\\ \\mathrm{(cosine)}}\\end{array}}$ iscretization)   \n459   \n460   \n461   \n462 ", "page_idx": 13}, {"type": "text", "text": "463 Flow Matching $x_{0}\\sim\\mathcal{N}(0,1)$ : ", "page_idx": 13}, {"type": "text", "text": "464 $\\mathrm{\\bullet\\num\\_inference\\_steps=100}$ ", "page_idx": 13}, {"type": "text", "text": "465 Flow Matching $x_{0}\\sim\\mathcal{U}(0,1)$ : ", "page_idx": 13}, {"type": "text", "text": "466 $\\mathrm{\\Delta\\num\\_inference\\_steps=100}$ ", "page_idx": 14}, {"type": "equation", "text": "", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "467 Modification task: ", "page_idx": 14}, {"type": "text", "text": "468 Regression UNet: ", "page_idx": 14}, {"type": "text", "text": "469 $\\mathbf{\\nabla}\\cdot\\mathrm{num}_{\\_}\\mathrm{inference}_{\\_}\\mathrm{steps}=1$ ", "page_idx": 14}, {"type": "equation", "text": "", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "470 Diffusion Model: ", "page_idx": 14}, {"type": "text", "text": "471 ${\\begin{array}{r l}&{{\\mathrm{~\\bf~\\lambda~num\\_train\\_timesteps}}=1000\\;({\\mathrm{diffusion~process}}\\;.}\\\\ &{{\\mathrm{~\\bf~beta\\_start}}=0.0001}\\\\ &{{\\mathrm{~\\bf~beta\\_end=0.02}}}\\\\ &{{\\mathrm{~\\bf~\\lambda~num\\_inference\\_steps}}=100}\\\\ &{{\\mathrm{~\\bf~beta\\_schedule=\\lambda^{\\nu}s q u a r e d c o s\\_c a p\\_v2^{\\nu}~(c o s i n e)}}}\\end{array}}$ discretization)   \n472   \n473   \n474   \n475 ", "page_idx": 14}, {"type": "text", "text": "476 Flow Matching: ", "page_idx": 14}, {"type": "text", "text": "477 $\\mathrm{\\Delta\\num\\_inference\\_steps=100}$ ", "page_idx": 14}, {"type": "equation", "text": "", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "478 A.4 PBC Loss details ", "page_idx": 14}, {"type": "text", "text": "479 The PBC loss function operates through several steps: ", "page_idx": 14}, {"type": "text", "text": "1. Vertices evaluation: If the target coordinate of the atom is lattice vertex (all 3 coordinates $x,y$ , zare equal to $1$ or $0$ ), then loss between prediction point $p r e d s_{i}$ and target point targe $t_{i}$ is being calculated using following formula: ", "page_idx": 14}, {"type": "equation", "text": "$$\nL_{v e r t e x}(p r e d s_{i},t a r g e t_{i})=\\operatorname*{min}_{v\\in v e r t i c e s}||p r e d s_{i}-v||.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "480   \n481   \n482   \n483   \n484   \n485   \n486   \n487 ", "page_idx": 14}, {"type": "text", "text": "where vertices is a set of 8 possible positions according to PBC $(\\{0,0,0\\},\\,\\{0,0,1\\}$ , ..., $\\{1,1,1\\}$ ). ", "page_idx": 14}, {"type": "text", "text": "2. Edges evaluation: If the target coordinate of the atom is located on lattice edge (two coordinates are equal to $^{1}$ or 0 and one is not). For example, a lattice edge atom at point $\\{0,1,0.3\\}$ has identical atoms at points $\\{0,0,0.3\\},\\{1,0,0.3\\},\\{1,1,0.3\\}$ . As we can see, in this example $z$ -coordinate is fixed but $x$ and $_y$ are exchangeable. Therefore, if the target point is represented as $\\{x,y,z\\}$ , we can use the following formula: ", "page_idx": 14}, {"type": "equation", "text": "$$\nL_{e d g e}(p r e d s_{i},t a r g e t_{i})=\\operatorname*{min}_{e\\in e d g e P o i n t s}||p r e d s_{i}-e||,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where edgePoints is a set of 4 possible positions according to PBC. ", "page_idx": 14}, {"type": "text", "text": "489   \n490   \n491   \n492   \n493   \n494   \n495 ", "page_idx": 14}, {"type": "text", "text": "3. Sides evaluation: If the target coordinate of the atom is located on lattice side (one coordinate is equal to $^1$ or 0 and two are not). For example, lattice side atom at point $\\{0,0.5,0.3\\}$ has identical atom at point $\\{1,0.5,0.3\\}$ . In this example $_y$ and $z$ coordinates are fixed but $x$ is exchangeable. Therefore, if the target point is represented as $\\{x,y,z\\}$ , we can use the following formula: ", "page_idx": 14}, {"type": "equation", "text": "$$\nL_{s i z e}(p r e d s_{i},t a r g e t_{i})=\\operatorname*{min}_{s\\in s i d e P o i n t s}||p r e d s_{i}-s||,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where sidePoints is a set of 2 possible positions according to PBC. ", "page_idx": 14}, {"type": "text", "text": "497   \n498   \n499   \n500 ", "page_idx": 14}, {"type": "text", "text": "\u2022 Case of exchangeable point $x$ : $s i d e P o i n t s=\\{\\{0,y,z\\},\\{1,y,z\\}\\}$ \u2022 Case of exchangeable point $_y$ : $s i d e P o i n t s=\\{\\{x,0,z\\},\\{x,1,z\\}\\}$ \u2022 Case of exchangeable point $z:\\,s i d e P o i n t s=\\{\\{x,y,0\\},\\{x,y,1\\}\\}$ ", "page_idx": 14}, {"type": "image", "img_path": "I3kIEjoON6/tmp/e68f3e42a12ea28dffc927358417512254182dfb75573314d3a77cd2067c7c93.jpg", "img_caption": ["(a) Target structure "], "img_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "I3kIEjoON6/tmp/6fe63f27ac5be368e70386cde3173a804a9a311a4bb912d2bcc12accdac1cc2e.jpg", "img_caption": ["(b) Prediction "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "Figure 4: Example of using PBC-aware loss. The depicted structures (Mo2Nb2Ta2W2) are visually different, but in fact they are exact the same. It is confirmed by insignificant value of PBC-aware loss ", "page_idx": 15}, {"type": "text", "text": "501 4. Points, which don\u2019t belong to the groups above, are processed using the default loss   \n502 function. ", "page_idx": 15}, {"type": "text", "text": "Since the $\\operatorname*{min}(x_{1},x_{2},...,x_{n})$ function is undifferentiable at multiple points ( $x_{i}=x_{j}\\;\\forall i\\neq j$ ), it makes a loss function to have a more complicated surface. Therefore, we used a norm function with order $k\\rightarrow-\\infty$ which is differentiable at all points as a replacement. ", "page_idx": 15}, {"type": "equation", "text": "$$\nm i n_{d i f f}(x_{1},x_{2},...,x_{n})=(\\sum_{i=1}^{n}|x_{i}|^{k})^{\\frac{1}{k}}\\;\\;\\;k\\longrightarrow-\\infty\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "503 Therefore, overall PBC-aware loss for a structure is represented as: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{L_{P B C}(p r e d s,t a r g e t)=\\displaystyle\\sum_{i=1}^{n}\\mathbb{I}(t a r g e t_{i}~\\mathrm{is~vertex~point})L_{v e r t e x}(p r e d s_{i},t a r g e t_{i})}\\\\ &{}&{+\\mathbb{I}(t a r g e t_{i}~\\mathrm{is~edge~point})L_{e d g e}(p r e d s_{i},t a r g e t_{i})}\\\\ &{}&{+\\mathbb{I}(t a r g e t_{i}~\\mathrm{is~side~point})L_{s i d e}(p r e d s_{i},t a r g e t_{i})}\\\\ &{}&{+\\mathbb{I}(t a r g e t_{i}~\\mathrm{is~usual~point})L_{2}(p r e d s_{i},t a r g e t_{i})}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "504 As the count of atoms varies across different structures, the $L_{P B C}$ metric tends to yield   \n505 higher values for structures featuring a larger number of atoms. Thus, it is important to   \n506 normalize the loss function with the number of atoms in the structure if it would be used in   \n507 batches with structures with different number of atoms. Therefore, a PBC-aware loss for a   \n508 batch of structures is formulated as: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{b a t c h S i z e}\\frac{L_{b a t c h P B C}(b a t c h P r e d s,b a t c h T a r g e t s)=}{\\displaystyle\\sum_{n u m S i t e s_{i}}^{1}L_{P B C}(b a t c h P r e d s_{i},b a t c h T a r g e t s_{i})}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "509 Compute resources ", "page_idx": 15}, {"type": "text", "text": "510 For our computational needs in model training and inference, we deployed a total of three   \n511 GPU servers with the following configurations: ", "page_idx": 15}, {"type": "text", "text": "512 Server 1: ", "page_idx": 15}, {"type": "text", "text": "513 \u2022 GPU: NVIDIA A100/80G  \n514 \u2022 CPU: 8vCPU of Intel(R) Xeon(R) Gold 6248R @ 3.00 GHz  \n515 \u2022 RAM: 64Gb", "page_idx": 15}, {"type": "text", "text": "516 Server 2: ", "page_idx": 15}, {"type": "text", "text": "517 \u2022 GPU: NVIDIA V100 (32GB)   \n518 \u2022 CPU: 8vCPU of Intel(R) Xeon(R) Gold 6278C $@$ 2.60 GHz   \n519 \u2022 RAM: 64Gb ", "page_idx": 16}, {"type": "text", "text": "520 Server 3: ", "page_idx": 16}, {"type": "text", "text": "521 \u2022 GPU: NVIDIA V100 (32GB)   \n522 \u2022 CPU: 8vCPU of Intel(R) Xeon(R) Gold 6278C $@$ 2.60 GHz   \n523 \u2022 RAM: 64Gb   \n524 Every model training time consumed up to 2 weeks employing computing power of one GPU   \n525 server.   \n526 For the ab-initio calculations implemented in VASP, we deployed a total of 5 identical CPU   \n527 servers with the following configurations:   \n528 \u2022 CPU: 64vCPU of Intel(R) Xeon(R) Gold 6278C CPU @ 2.60GHz   \n529 \u2022 RAM: 256Gb   \n530 Structure relaxation with VASP for all six experiments mentioned in Table 3 took more that   \n531 180 thousand CPU hours. Computing power of all CPU servers was employed.   \n533 The checklist is designed to encourage best practices for responsible machine learning research,   \n534 addressing issues of reproducibility, transparency, research ethics, and societal impact. Do   \n535 not remove the checklist: The papers not including the checklist will be desk rejected. The   \n536 checklist should follow the references and follow the (optional) supplemental material. The   \n537 checklist does NOT count towards the page limit.   \n538 Please read the checklist guidelines carefully for information on how to answer these questions.   \n539 For each question in the checklist:   \n540 \u2022 You should answer [Yes] , [No] , or $\\mathrm{[NA]}$ .   \n541 \u2022 [NA] means either that the question is Not Applicable for that particular paper or   \n542 the relevant information is Not Available.   \n543 \u2022 Please provide a short (1\u20132 sentence) justification right after your answer (even for   \n544 NA).   \n545 The checklist answers are an integral part of your paper submission. They are visible to the   \n546 reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also   \n547 include it (after eventual revisions) with the final version of your paper, and its final version   \n548 will be published with the paper.   \n549 The reviewers of your paper will be asked to use the checklist as one of the factors in their   \n550 evaluation. While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to   \n551 answer \"[No] \" provided a proper justification is given (e.g., \"error bars are not reported   \n552 because it would be too computationally expensive\" or \"we were unable to find the license for   \n553 the dataset we used\"). In general, answering \"[No] \" or \"[NA] \" is not grounds for rejection.   \n554 While the questions are phrased in a binary way, we acknowledge that the true answer   \n555 is often more nuanced, so please just use your best judgment and write a justification to   \n556 elaborate. All supporting evidence can appear either in the main paper or the supplemental   \n557 material, provided in appendix. If you answer [Yes] to a question, in the justification please   \n558 point to the section(s) where related material for the question can be found. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "559 IMPORTANT, please: ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "560 \u2022 Delete this instruction block, but keep the section heading \u201cNeurIPS paper checklist\",   \n561 \u2022 Keep the checklist subsection headings, questions/answers and guidelines below.   \n562 \u2022 Do not modify the questions and only use the provided macros for your answers.   \n563 1. Claims   \n564 Question: Do the main claims made in the abstract and introduction accurately   \n565 reflect the paper\u2019s contributions and scope?   \n566 Answer: [Yes] ,   \n567 Justification: The main claims made in the abstract and introduction do accurately   \n568 reflect the paper\u2019s contributions and scope. Every aspects mentioned in the abstract   \n569 and introduction are further revealed in the main paper.   \n570 Guidelines:   \n571 \u2022 The answer NA means that the abstract and introduction do not include the   \n572 claims made in the paper.   \n573 \u2022 The abstract and/or introduction should clearly state the claims made, including   \n574 the contributions made in the paper and important assumptions and limitations.   \n575 A No or NA answer to this question will not be perceived well by the reviewers.   \n576 \u2022 The claims made should match theoretical and experimental results, and reflect   \n577 how much the results can be expected to generalize to other settings.   \n578 \u2022 It is fine to include aspirational goals as motivation as long as it is clear that   \n579 these goals are not attained by the paper.   \n580 2. Limitations   \n581 Question: Does the paper discuss the limitations of the work performed by the   \n582 authors?   \n583 Answer: [Yes]   \n584 Justification: All the limitations are discussed in the conclusion section. More   \n585 specifically, limitations are 1) in number of atoms per unit cell of crystal structure   \n586 that model can work with and 2) symmetries that our structure representation is   \n587 able to encode. For example, crystals have infinite periodic structure. There is no   \n588 possible way to   \n589 Guidelines:   \n590 \u2022 The answer NA means that the paper has no limitation while the answer No   \n591 means that the paper has limitations, but those are not discussed in the paper.   \n592 \u2022 The authors are encouraged to create a separate \"Limitations\" section in their   \n593 paper.   \n594 \u2022 The paper should point out any strong assumptions and how robust the results   \n595 are to violations of these assumptions (e.g., independence assumptions, noiseless   \n596 settings, model well-specification, asymptotic approximations only holding   \n597 locally). The authors should reflect on how these assumptions might be violated   \n598 in practice and what the implications would be.   \n599 \u2022 The authors should reflect on the scope of the claims made, e.g., if the approach   \n600 was only tested on a few datasets or with a few runs. In general, empirical   \n601 results often depend on implicit assumptions, which should be articulated.   \n602 \u2022 The authors should reflect on the factors that influence the performance of the   \n603 approach. For example, a facial recognition algorithm may perform poorly when   \n604 image resolution is low or images are taken in low lighting. Or a speech-to-text   \n605 system might not be used reliably to provide closed captions for online lectures   \n606 because it fails to handle technical jargon.   \n607 \u2022 The authors should discuss the computational efifciency of the proposed algo  \n608 rithms and how they scale with dataset size.   \n609 \u2022 If applicable, the authors should discuss possible limitations of their approach   \n610 to address problems of privacy and fairness.   \n611 \u2022 While the authors might fear that complete honesty about limitations might   \n612 be used by reviewers as grounds for rejection, a worse outcome might be that   \n613 reviewers discover limitations that aren\u2019t acknowledged in the paper. The   \n614 authors should use their best judgment and recognize that individual actions in   \n615 favor of transparency play an important role in developing norms that preserve   \n616 the integrity of the community. Reviewers will be specifically instructed to not   \n617 penalize honesty concerning limitations.   \n618 3. Theory Assumptions and Proofs   \n619 Question: For each theoretical result, does the paper provide the full set of assump  \n620 tions and a complete (and correct) proof?   \n621 Answer: [NA] .   \n622 Justification: Our work does not include significant theoretical results due to the   \n623 fact that it is mainly focused on experiments.   \n624 Guidelines:   \n625 \u2022 The answer NA means that the paper does not include theoretical results.   \n626 \u2022 All the theorems, formulas, and proofs in the paper should be numbered and   \n627 cross-referenced.   \n628 \u2022 All assumptions should be clearly stated or referenced in the statement of any   \n629 theorems.   \n630 \u2022 The proofs can either appear in the main paper or the supplemental material,   \n631 but if they appear in the supplemental material, the authors are encouraged to   \n632 provide a short proof sketch to provide intuition.   \n633 \u2022 Inversely, any informal proof provided in the core of the paper should be   \n634 complemented by formal proofs provided in appendix or supplemental material.   \n635 \u2022 Theorems and Lemmas that the proof relies upon should be properly referenced.   \n636 4. Experimental Result Reproducibility   \n637 Question: Does the paper fully disclose all the information needed to reproduce   \n638 the main experimental results of the paper to the extent that it affects the main   \n639 claims and/or conclusions of the paper (regardless of whether the code and data are   \n640 provided or not)?   \n641 Answer: [Yes]   \n642 Justification: The paper fully discloses all the information needed to reproduce the   \n643 main experimental results. The information includes: 1) all hyperparameters for   \n644 the models, including the random seed they were performed on 2) crystal structures   \n645 used as a data, 3) VASP settings.   \n646 Guidelines:   \n647 \u2022 The answer NA means that the paper does not include experiments.   \n648 \u2022 If the paper includes experiments, a No answer to this question will not be   \n649 perceived well by the reviewers: Making the paper reproducible is important,   \n650 regardless of whether the code and data are provided or not.   \n651 \u2022 If the contribution is a dataset and/or model, the authors should describe the   \n652 steps taken to make their results reproducible or verifiable.   \n653 \u2022 Depending on the contribution, reproducibility can be accomplished in various   \n654 ways. For example, if the contribution is a novel architecture, describing the   \n655 architecture fully might sufifce, or if the contribution is a specific model and   \n656 empirical evaluation, it may be necessary to either make it possible for others to   \n657 replicate the model with the same dataset, or provide access to the model. In   \n658 general. releasing code and data is often one good way to accomplish this, but   \n659 reproducibility can also be provided via detailed instructions for how to replicate   \n660 the results, access to a hosted model (e.g., in the case of a large language model),   \n661 releasing of a model checkpoint, or other means that are appropriate to the   \n662 research performed.   \n663 \u2022 While NeurIPS does not require releasing code, the conference does require all   \n664 submissions to provide some reasonable avenue for reproducibility, which may   \n665 depend on the nature of the contribution. For example   \n666 (a) If the contribution is primarily a new algorithm, the paper should make it   \n667 clear how to reproduce that algorithm.   \n668 (b) If the contribution is primarily a new model architecture, the paper should   \n669 describe the architecture clearly and fully.   \n670 (c) If the contribution is a new model (e.g., a large language model), then there   \n671 should either be a way to access this model for reproducing the results or a   \n672 way to reproduce the model (e.g., with an open-source dataset or instructions   \n673 for how to construct the dataset).   \n674 (d) We recognize that reproducibility may be tricky in some cases, in which   \n675 case authors are welcome to describe the particular way they provide for   \n676 reproducibility. In the case of closed-source models, it may be that access to   \n677 the model is limited in some way (e.g., to registered users), but it should be   \n678 possible for other researchers to have some path to reproducing or verifying   \n679 the results.   \n680 5. Open access to data and code   \n681 Question: Does the paper provide open access to the data and code, with sufifcient   \n682 instructions to faithfully reproduce the main experimental results, as described in   \n683 supplemental material?   \n684 Answer: [Yes]   \n685 Justification: The raw crystal dataset can be downloaded from https://aflowlib.org.   \n686 The source code for training and inferencing our models can be obtained from   \n687 GitHub at https://github.com/AIRI-Institute/conditional-crystal-generation. Both   \n88 data and code are also referenced in the main paper\u2019s Section 9 and Section 9 .   \n689 Guidelines:   \n690 \u2022 The answer NA means that paper does not include experiments requiring code.   \n691 \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.   \n692 cc/public/guides/CodeSubmissionPolicy) for more details.   \n693 \u2022 While we encourage the release of code and data, we understand that this might   \n694 not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected   \n695 simply for not including code, unless this is central to the contribution (e.g., for   \n696 a new open-source benchmark).   \n697 \u2022 The instructions should contain the exact command and environment needed   \n698 to run to reproduce the results. See the NeurIPS code and data submis  \n699 sion guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy)   \n700 for more details.   \n701 \u2022 The authors should provide instructions on data access and preparation, in  \n702 cluding how to access the raw data, preprocessed data, intermediate data, and   \n703 generated data, etc.   \n704 \u2022 The authors should provide scripts to reproduce all experimental results for   \n705 the new proposed method and baselines. If only a subset of experiments are   \n706 reproducible, they should state which ones are omitted from the script and why.   \n707 \u2022 At submission time, to preserve anonymity, the authors should release   \n708 anonymized versions (if applicable).   \n709 \u2022 Providing as much information as possible in supplemental material (appended   \n710 to the paper) is recommended, but including URLs to data and code is permitted.   \n711 6. Experimental Setting/Details   \n712 Question: Does the paper specify all the training and test details (e.g., data splits,   \n713 hyperparameters, how they were chosen, type of optimizer, etc.) necessary to   \n714 understand the results?   \n715 Answer: [Yes]   \n716 Justification: All the training and testing details and all the hyperparameters for   \n717 the experiments are mentioned in the paper.   \n718 Guidelines:   \n719 \u2022 The answer NA means that the paper does not include experiments.   \n720 \u2022 The experimental setting should be presented in the core of the paper to a level   \n721 of detail that is necessary to appreciate the results and make sense of them.   \n722 \u2022 The full details can be provided either with the code, in appendix, or as   \n723 supplemental material.   \n724 7. Experiment Statistical Significance   \n725 Question: Does the paper report error bars suitably and correctly defined or other   \n726 appropriate information about the statistical significance of the experiments?   \n727 Answer: [No]   \n728 Justification: Firstly, calculating statistical significance for all our experiments   \n729 is computationally expensive. Secondly, experimental results(crystal structures   \n730 obtained from model inference) have been approved by further ab-initio calculations   \n731 implemented in VASP.   \n732 Guidelines:   \n733 \u2022 The answer NA means that the paper does not include experiments.   \n734 \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars,   \n735 confidence intervals, or statistical significance tests, at least for the experiments   \n736 that support the main claims of the paper.   \n737 \u2022 The factors of variability that the error bars are capturing should be clearly   \n738 stated (for example, train/test split, initialization, random drawing of some   \n739 parameter, or overall run with given experimental conditions).   \n740 \u2022 The method for calculating the error bars should be explained (closed form   \n741 formula, call to a library function, bootstrap, etc.)   \n742 \u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n743 \u2022 It should be clear whether the error bar is the standard deviation or the standard   \n744 error of the mean.   \n745 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors   \n746 should preferably report a 2-sigma error bar than state that they have a $96\\%$   \n747 CI, if the hypothesis of Normality of errors is not verified.   \n748 \u2022 For asymmetric distributions, the authors should be careful not to show in   \n749 tables or figures symmetric error bars that would yield results that are out of   \n750 range (e.g. negative error rates).   \n751 \u2022 If error bars are reported in tables or plots, The authors should explain in the   \n752 text how they were calculated and reference the corresponding figures or tables   \n753 in the text.   \n754 8. Experiments Compute Resources   \n755 Question: For each experiment, does the paper provide sufifcient information on the   \n756 computer resources (type of compute workers, memory, time of execution) needed   \n757 to reproduce the experiments?   \n758 Answer: [Yes]   \n759 Justification: In the paper, we state all the compute powers that we have employed   \n760 for AI model training and ab-initio calculations. One can observe it in Section A.4.   \n761 Guidelines:   \n762 \u2022 The answer NA means that the paper does not include experiments.   \n763 \u2022 The paper should indicate the type of compute workers CPU or GPU, internal   \n764 cluster, or cloud provider, including relevant memory and storage.   \n765 \u2022 The paper should provide the amount of compute required for each of the   \n766 individual experimental runs as well as estimate the total compute.   \n767 \u2022 The paper should disclose whether the full research project required more   \n768 compute than the experiments reported in the paper (e.g., preliminary or failed   \n769 experiments that didn\u2019t make it into the paper).   \n770 9. Code Of Ethics   \n771 Question: Does the research conducted in the paper conform, in every respect, with   \n772 the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n773 Answer: [Yes]   \n774 Justification: Our research, working process, data and code correspond with NeurIPS   \n775 Code of Ethics in https://neurips.cc/public/EthicsGuidelines   \n776 Guidelines:   \n777 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code   \n778 of Ethics.   \n779 \u2022 If the authors answer No, they should explain the special circumstances that   \n780 require a deviation from the Code of Ethics.   \n781 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special   \n782 consideration due to laws or regulations in their jurisdiction).   \n783 10. Broader Impacts   \n784 Question: Does the paper discuss both potential positive societal impacts and   \n785 negative societal impacts of the work performed?   \n786 Answer: [Yes]   \n787 Justification: The broad societal impacts are discussed int the conclusion section.   \n788 Guidelines:   \n789 \u2022 The answer NA means that there is no societal impact of the work performed.   \n790 \u2022 If the authors answer NA or No, they should explain why their work has no   \n791 societal impact or why the paper does not address societal impact.   \n792 \u2022 Examples of negative societal impacts include potential malicious or unintended   \n793 uses (e.g., disinformation, generating fake proflies, surveillance), fairness consid  \n794 erations (e.g., deployment of technologies that could make decisions that unfairly   \n795 impact specific groups), privacy considerations, and security considerations.   \n796 \u2022 The conference expects that many papers will be foundational research and   \n797 not tied to particular applications, let alone deployments. However, if there   \n798 is a direct path to any negative applications, the authors should point it out.   \n799 For example, it is legitimate to point out that an improvement in the quality   \n800 of generative models could be used to generate deepfakes for disinformation.   \n801 On the other hand, it is not needed to point out that a generic algorithm for   \n802 optimizing neural networks could enable people to train models that generate   \n803 Deepfakes faster.   \n804 \u2022 The authors should consider possible harms that could arise when the technology   \n805 is being used as intended and functioning correctly, harms that could arise when   \n806 the technology is being used as intended but gives incorrect results, and harms   \n807 following from (intentional or unintentional) misuse of the technology.   \n808 \u2022 If there are negative societal impacts, the authors could also discuss possible   \n809 mitigation strategies (e.g., gated release of models, providing defenses in addition   \n810 to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a   \n811 system learns from feedback over time, improving the efifciency and accessibility   \n812 of ML).   \n813 11. Safeguards   \n814 Question: Does the paper describe safeguards that have been put in place for   \n815 responsible release of data or models that have a high risk for misuse (e.g., pretrained   \n816 language models, image generators, or scraped datasets)?   \n817 Answer: [NA]   \n818 Justification: The paper poses no such risks.   \n819 Guidelines:   \n820 \u2022 The answer NA means that the paper poses no such risks.   \n821 \u2022 Released models that have a high risk for misuse or dual-use should be released   \n822 with necessary safeguards to allow for controlled use of the model, for example   \n823 by requiring that users adhere to usage guidelines or restrictions to access the   \n824 model or implementing safety filters.   \n825 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The   \n826 authors should describe how they avoided releasing unsafe images.   \n827 \u2022 We recognize that providing effective safeguards is challenging, and many papers   \n828 do not require this, but we encourage authors to take this into account and   \n829 make a best faith effort.   \n830 12. Licenses for existing assets   \n831 Question: Are the creators or original owners of assets (e.g., code, data, models),   \n832 used in the paper, properly credited and are the license and terms of use explicitly   \n833 mentioned and properly respected?   \n834 Answer: [Yes]   \n835 Justification: The models utilized in this research are appropriately cited within   \n836 the text, including references to the AFLOW database. We have no commercial   \n837 interests related to the use of these models and the crystal structure database. The   \n838 code used in this study was entirely developed by our team.   \n839 Guidelines:   \n840 \u2022 The answer NA means that the paper does not use existing assets.   \n841 \u2022 The authors should cite the original paper that produced the code package or   \n842 dataset.   \n843 \u2022 The authors should state which version of the asset is used and, if possible,   \n844 include a URL.   \n845 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n846 \u2022 For scraped data from a particular source (e.g., website), the copyright and   \n847 terms of service of that source should be provided.   \n848 \u2022 If assets are released, the license, copyright information, and terms of use in   \n849 the package should be provided. For popular datasets, paperswithcode.com/   \n850 datasets has curated licenses for some datasets. Their licensing guide can help   \n851 determine the license of a dataset.   \n852 \u2022 For existing datasets that are re-packaged, both the original license and the   \n853 license of the derived asset (if it has changed) should be provided.   \n854 \u2022 If this information is not available online, the authors are encouraged to reach   \n855 out to the asset\u2019s creators.   \n856 13. New Assets   \n857 Question: Are new assets introduced in the paper well documented and is the   \n858 documentation provided alongside the assets?   \n859 Answer: [Yes]   \n860 Justification: The primary assets of our research are: 1) the methodology, and 2) the   \n861 code for training models and performing inference. Both are thoroughly documented.   \n862 Guidelines:   \n863 \u2022 The answer NA means that the paper does not release new assets.   \n864 \u2022 Researchers should communicate the details of the dataset/code/model as part   \n865 of their submissions via structured templates. This includes details about   \n866 training, license, limitations, etc.   \n867 \u2022 The paper should discuss whether and how consent was obtained from people   \n868 whose asset is used.   \n869 \u2022 At submission time, remember to anonymize your assets (if applicable). You   \n870 can either create an anonymized URL or include an anonymized zip file.   \n871 14. Crowdsourcing and Research with Human Subjects   \n872 Question: For crowdsourcing experiments and research with human subjects, does   \n873 the paper include the full text of instructions given to participants and screenshots,   \n874 if applicable, as well as details about compensation (if any)?   \n875 Answer: [NA]   \n876 Justification: The paper does not involve crowdsourcing nor research with human   \n877 subjects.   \n878 Guidelines:   \n879 \u2022 The answer NA means that the paper does not involve crowdsourcing nor   \n880 research with human subjects.   \n881 \u2022 Including this information in the supplemental material is fine, but if the main   \n882 contribution of the paper involves human subjects, then as much detail as   \n883 possible should be included in the main paper.   \n884 \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection,   \n885 curation, or other labor should be paid at least the minimum wage in the   \n886 country of the data collector.   \n887 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human   \n888 Subjects   \n889 Question: Does the paper describe potential risks incurred by study participants,   \n890 whether such risks were disclosed to the subjects, and whether Institutional Review   \n891 Board (IRB) approvals (or an equivalent approval/review based on the requirements   \n892 of your country or institution) were obtained?   \n893 Answer: [NA]   \n894 Justification: The paper does not involve crowdsourcing nor research with human   \n895 subjects   \n896 Guidelines:   \n897   \n898   \n899   \n900   \n901   \n902   \n903   \n904   \n905   \n906 ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 24}]