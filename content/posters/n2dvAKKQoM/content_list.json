[{"type": "text", "text": "Task-oriented Time Series Imputation Evaluation via Generalized Representers ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Zhixian Wang1,2, Linxiao Yang2, Liang $\\mathbf{Sun^{2}}$ , Qingsong Wen2, Yi Wang1 ", "page_idx": 0}, {"type": "text", "text": "1The University of Hong Kong, 2DAMO Academy, Alibaba Group zxwang@eee.hku.hk, linxiao.ylx@alibaba-inc.com, liang.sun@alibaba-inc.com, qingsongedu@gmail.com, yiwang@eee.hku.hk ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Time series analysis is widely used in many fields such as power energy, economics, and transportation, including different tasks such as forecasting, anomaly detection, classification, etc. Missing values are widely observed in these tasks, and often leading to unpredictable negative effects on existing methods, hindering their further application. In response to this situation, existing time series imputation methods mainly focus on restoring sequences based on their data characteristics, while ignoring the performance of the restored sequences in downstream tasks. Considering different requirements of downstream tasks (e.g., forecasting), this paper proposes an efficient downstream task-oriented time series imputation evaluation approach. By combining time series imputation with neural network models used for downstream tasks, the gain of different imputation strategies on downstream tasks is estimated without retraining, and the most favorable imputation value for downstream tasks is given by combining different imputation strategies according to the estimated gain. The corresponding code can be found in the repository https://github.com/hkuedl/Task-Oriented-Imputation. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Time series analysis plays a crucial role in many real-world applications, such as energy, finance, healthcare, and other fields [1, 2, 3]. For example, forecasting load series forms the basis for further decision-making in power dispatch in the power grid system, thereby generating a significant amount of economic benefits [4, 5, 6]. However, collecting time series data, especially high-quality ones, is challenging. Due to the instability of the external environment, sensor failures, and even ethical and legal privacy issues, missing values are prevalent in time series data [7]. For instance, in the BDG2 load series dataset [8], widely used in building energy analysis, the ratio of complete time series data is less than $10\\%$ . ", "page_idx": 0}, {"type": "text", "text": "To handle missing values in time series data, numerous methods have been proposed for time series imputation in the literature. Based on the features of the imputation methods, these approaches can be divided into statistical and machine learning methods, such as ARIMA and KNN [9, 10], as well as deep learning-based methods [11, 12, 13, 14]. Both types of methods generally use reconstruction errors of missing values to guide learning and perform evaluation. Recently, some researchers have turned their attention to evaluation strategies based on downstream task performance [15]. However, in most cases, downstream tasks are classification tasks [13], while forecasting tasks, as another important branch of time series-related tasks, have not been fully considered. The main challenge for time series forecasting is that the time series serves as both input and label (output) for the model during training, whereas in classification tasks, it only serves as input for the model. ", "page_idx": 0}, {"type": "text", "text": "In supervised learning, training labels influence the calculation of the loss function, which in turn affects the optimization of model parameters and, ultimately, the performance of the model on the test set. [16] indicates that noise in input data (missing data can be considered a type of noise) often has a limited impact on forecasting results. In contrast, label noise can significantly affect the model and, consequently, the final test results from the beginning to the end of the time series. Therefore, when evaluating the impact of different time imputation methods on downstream forecasting tasks, it is essential to focus on assessing the quality of training labels constructed through various imputation methods. ", "page_idx": 1}, {"type": "text", "text": "To evaluate how the quality of the imputation labels affects downstream forecasting tasks, it is important to clarify that an excellent imputation strategy does not necessarily mean that the imputed value at each time step is superior to any other method. [15] provides a benchmark for various methods in time series imputation tasks. Although SAITS [13], as one of the latest SOTA methods, has achieved remarkable results, there are still methods that surpass SAITS in some cases. This demonstrates that time series imputation is a complex task, making it difficult to find a universal method capable of handling all situations, let alone considering the performance of downstream forecasting tasks. A more realistic scenario is that while one method may perform better overall, it may not outperform other methods locally. In time series, this means that one method may excel in some time steps, while others do better in different time steps. However, to examine the impact of each time step on forecasting, retraining the forecasting model multiple times is necessary, which is impractical due to time and computational costs. Therefore, an efficient estimation method is needed to examine the impact of each time step with different imputation methods. Additionally, since finding a universal method is difficult, it is natural to shift focus toward combining the advantages of current methods to obtain a better imputation strategy. Consequently, determining how to combine different strategies becomes a challenge. ", "page_idx": 1}, {"type": "text", "text": "Based on the above situation, we have developed a task-oriented time series imputation evaluation strategy. Specifically, we summarize our contributions into the following points. ", "page_idx": 1}, {"type": "text", "text": "1. We propose a strategy that evaluates each time series imputation method by estimating the impact of missing (imputed) labels at each time step on downstream tasks without requiring multiple retraining, which significantly reduces time and computational consumption. To the best of our knowledge, we are the first to consider the impact of missing values in time series as labels on downstream forecasting tasks.   \n2. We introduce a simple and effective similarity calculation method based on the characteristics of long time series to estimate the impact of imputed values more quickly, striking a balance between performance and computational cost.   \n3. We develop a time series imputation framework guided by maximizing the gains of downstream tasks, enabling the combination of advantages from different time series imputation strategies to achieve a better one. This results in improved performance in the downstream forecasting task. ", "page_idx": 1}, {"type": "text", "text": "1.1 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Time Series Imputation Time series imputation can be primarily classified into two categories: traditional techniques and neural network-based techniques. Traditional methods replace missing values with statistics, such as the mean value or the last observed value [17]. They include simple statistical models like ARIMA [18], ARFIMA, SARIMA [19], and machine learning techniques such as KNNI [20], TIDER [21], MICE [22], BayOTIDE [23]. In recent years, deep learning imputation methods have demonstrated remarkable capabilities in capturing intricate temporal relationships and complex variation patterns inherent in time series data. These methods employ deep learning models like Transformers [24, 13], generative neural networks such as VAEs [12, 25], GANs [26, 27], and diffusion models [28] to capture complex dynamic relationships within time series data. Although different methods exhibit various advantages, no universal method currently outperforms others in all scenarios and datasets. This observation inspires us to consider combining existing advanced methods in this work to achieve better time series imputation strategies. ", "page_idx": 1}, {"type": "text", "text": "Sample-based Explaination Sample-based explainable methods can be divided into two categories [29]. One is based on retraining, which evaluates the importance of corresponding data by comparing the impact of removing data points on the model and even the final test results [30, 31, 32]. ", "page_idx": 1}, {"type": "text", "text": "Among them, the introduction of shapley value by [30] naturally ensures the fairness of data attribution. The other type is based on gradient methods, which directly estimate the influence of data points without the need for retraining. This type of method can be subdivided into three main categories, which are based on representative theories, Influence Function, and training loss trajectory. [33] is a representative of the method of the first type, whose core idea is to fix the other layers and only focus on the last layer of the neural network, so that the influence of each sample can be explicitly calculated. On the other hand, the Influence Function [34] is based on the assumption of convergence and uses the Hessian matrix to estimate the influence of samples. The last type of method takes into account the entire training process of the neural network, continuously tracking the impact of samples on each parameter update [35]. In addition, [36] summarizes the gradient-based method and unifies them as generalized representers. ", "page_idx": 2}, {"type": "text", "text": "2 Methodology ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 Problem Statement ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Consider a multivariate time series dataset represented by $\\{(\\boldsymbol{X}_{i},\\boldsymbol{y}_{i})\\}_{i=1}^{n}$ , incorporating $n$ samples. In this dataset, $\\pmb{X}_{i}\\,\\in\\,\\mathbb{R}^{D\\times L_{1}}$ corresponds to a feature matrix containing $D$ distinctive features over $L_{1}$ temporal intervals, whereas $\\mathbf{\\boldsymbol{y}}_{i}\\in\\mathbb{R}^{L_{2}}$ signifies the target time series, which spans $L_{2}$ temporal intervals. It is crucial to recognize that $\\pmb{y}_{i}$ may include several missing entries, a common complication within real-world datasets. For example, in the context of electrical load forecasting, $X_{i}$ encompasses daily weather-related time series data, comprising variables such as temperature, humidity, and wind speed, while $\\pmb{y}_{i}$ represents the electrical load time series of a given day, possibly containing missing entries due to issues in data collection or transmission. ", "page_idx": 2}, {"type": "text", "text": "Addressing missing values in $\\{{\\pmb y}_{i}\\}$ through imputation is a fundamental preprocessing step for machine learning tasks involving this data, underscoring the necessity to assess the effectiveness of various imputation methods. Consider $\\{\\pmb{y}_{i}^{(1)}\\}$ and $\\{y_{i}^{(2)}\\}$ as two time series resulting from the imputation of $\\{y\\}$ via two different methods. The goal is to ascertain whether the imputation performed on $\\{y_{i}^{(2)}\\}$ is superior to that on $\\{\\pmb{y}_{i}^{(1)}\\}$ . Moreover, we seek to evaluate the quality of imputation at each temporal interval, determining if the imputation of the $l$ -th interval in $y_{i}^{(2)}$ is more accurate than that in $\\pmb{y}_{i}^{(1)}$ ", "page_idx": 2}, {"type": "text", "text": "Conventionally, the quality of imputation is quantified by measuring the discrepancy between the imputed values and the actual data, favoring methods that minimize this deviation. In this study, however, we propose to assess imputation quality based on the performance of subsequent tasks. ", "page_idx": 2}, {"type": "text", "text": "One step further, we evaluate the quality of imputation on a timestep basis, examining if the imputation for the $l$ -th interval in $y_{i}^{(2)}$ exhibits improved efficacy over $\\pmb{y}_{i}^{(1)}$ , thereby offering a more nuanced and comprehensive evaluation of imputation methodologies. ", "page_idx": 2}, {"type": "text", "text": "Let us define the loss function for the downstream task as $\\mathcal{L}(f(\\boldsymbol{X},\\boldsymbol{\\theta}),\\boldsymbol{y})$ , where $f(\\cdot,\\pmb\\theta)$ denotes the model used in the downstream task parametered by $\\pmb{\\theta}$ . And let $\\{(\\bar{X}_{i}^{v},\\bar{{\\pmb y}}_{i}^{v})\\}_{i=1}^{m}$ constitute a test dataset that will be used to gauge model performance. We denote $y_{i,l}^{(1)}$ and $y_{i,l}^{(2)}$ as the $l$ -th entries of $\\pmb{y}_{i}^{(1)}$ and $y_{i}^{(2)}$ , respectively. According to our intuition, if $y_{i,l}^{(2)}$ is superior to $y_{i,l}^{(1)}$ , swapping $y_{i,l}^{(1)}$ for $y_{i,l}^{(2)}$ should result in a decrease in the test set\u2019s loss. Guided by this rationale, we define the indicator function $I(i,l)$ , which discerns whether $y_{i,l}^{(2)}$ is preferable over $y_{i,l}^{(1)}$ as follows: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{I(i,l)=\\displaystyle\\sum_{k=1}^{m}I(i,l,X_{k}^{v})=\\displaystyle\\sum_{k=1}^{m}\\big(\\mathcal{L}(f(X_{k}^{v},\\theta_{1}),y_{k}^{v})-\\mathcal{L}(f(X_{k}^{v},\\theta_{2}),y_{k}^{v})\\big)}\\\\ &{\\mathrm{~s.t.~}\\theta_{1}=\\arg\\displaystyle\\operatorname*{min}_{\\theta}\\sum_{k=1}^{n}\\mathcal{L}(f(X_{k},\\theta),y_{k}^{(1)})}\\\\ &{\\qquad\\theta_{2}=\\arg\\displaystyle\\operatorname*{min}_{\\theta}\\mathcal{L}(f(X_{i},\\theta),\\overline{{y_{i}}}^{(2,l)})+\\displaystyle\\sum_{k\\neq i}^{n}\\mathcal{L}(f(X_{k},\\theta),y_{k}^{(1)}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Here, $\\overline{{\\pmb{y}_{i}}}^{(2,l)}$ denotes a vector identical to $\\pmb{y}_{i}^{(1)}$ , except at the $l$ -th entry, which matches that of $\\pmb{y}_{i}^{(2)}$ . Clearly, $I(i,l)\\ge0$ implies that the substitution of $y_{i,l}^{(1)}$ with leads to a decreased test set loss, suggesting that $y_{i,l}^{(2)}$ is superior. Conversely, if $I(i,l)<0$ , it suggests that $y_{i,l}^{(1)}$ is preferable to $y_{i,l}^{(2)}$ . Despite the effectiveness of the definition provided by Equation (1), computing $I(i,l)$ for every missing value in the dataset is impractical due to the extensive model retraining required, which can be prohibitive in terms of time. To overcome this challenge, in the next section, we put forth an efficient methodology for estimating $I(i,l)$ without retraining the model. ", "page_idx": 3}, {"type": "text", "text": "2.2 Approximation Model Construction ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "To compute $I(i,l)$ efficiently, we propose a retrain-free method in this subsection. As both $\\pmb{y}^{(1)}$ and $\\pmb{y}^{(2)}$ are imputation of $\\textit{\\textbf{y}}$ , then we assume that $y_{i,l}^{(1)}$ is close to $y_{i,l}^{(2)}$ , with which we approximate $I(i,l)$ using the first order Tolyer expansion as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{I(i,l)\\approx\\!\\displaystyle\\sum_{k=1}^{m}\\frac{\\partial\\mathcal{L}(f(\\boldsymbol{X}_{k}^{v},\\boldsymbol{\\theta}),\\boldsymbol{y}_{k}^{v})}{\\partial y_{i,l}}\\bigg\\vert_{y_{i,l}=y_{i,l}^{(1)}}(y_{i,l}^{(1)}-y_{i,l}^{(2)})}\\\\ &{\\qquad=\\displaystyle\\sum_{k=1}^{m}\\frac{\\partial\\mathcal{L}(f(\\boldsymbol{X}_{k}^{v},\\boldsymbol{\\theta}),\\boldsymbol{y}_{k}^{v})}{\\partial f(\\boldsymbol{X}_{k}^{v},\\boldsymbol{\\theta})}^{T}\\frac{\\partial f(\\boldsymbol{X}_{k}^{v},\\boldsymbol{\\theta})}{\\partial y_{i,l}}\\bigg\\vert_{y_{i,l}=y_{i,l}^{(1)}}(y_{i,l}^{(1)}-y_{i,l}^{(2)}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Equation (2) provides an approximation for computing $I(i,l)$ , where $\\frac{\\partial f(\\pmb{X}_{k}^{v},\\pmb{\\theta})}{\\partial y_{i,l}}$ measures how the training target yi,l affect the prediction of the test data, and \u2202L(\u2202ff((XXvkv,,\u03b8\u03b8),)yvk) computes how the changing of the prediction of $X_{k}^{v}$ affect the final loss. Note that the symbolic expression $\\frac{\\partial f(\\pmb{X}_{k}^{v},\\pmb{\\theta})}{\\partial y_{i,l}}$ can be conceptually broken down into $\\frac{\\partial f(\\pmb{X}_{k}^{v},\\pmb{\\theta})}{\\partial\\pmb{\\theta}}\\cdot\\frac{\\partial\\pmb{\\theta}}{\\partial y_{i,l}}$ , elucidating the role of the label $y_{i,l}$ in shaping the model parameters $\\pmb{\\theta}$ throughout the training process. This, in turn, has repercussions on the model\u2019s prediction when evaluated on unseen data from the test set, i.e. $f\\left(\\mathbf{{X}}_{k}^{v},\\theta\\right)$ . By focusing on the derivative $\\frac{\\partial f(\\pmb{X}_{k}^{v},\\pmb{\\theta})}{\\partial y_{i,l}}$ , our goal is to assess the extent to which changes in label values $y_{i,l}$ influence the model\u2019s predictions on the test set, thereby affecting overall model efficacy. ", "page_idx": 3}, {"type": "text", "text": "When it comes back to the estimation, dispit e \u2202L(\u2202ff((XXvkvk,,\u03b8\u03b8),)yvk)and yi(,1l) \u2212yi(,2l) are easy to compute, estimating i,l \u2202f(\u2202yXvk,\u03b8) is difficult. The difficulty comes from two aspects. Firstly, for the complex $f(\\cdot,\\pmb\\theta)$ , the final parameter is not only affected by the training data, some other factors, such as the structure of the network and learning rate during the learning process. Secondly, all of the $n$ training samples affect the parameters of the model, leading to the mixture of the effect of data points on the final model. Thus isolating the effect of a single data point is difficult. ", "page_idx": 3}, {"type": "text", "text": "To overcome these two difficulties, we propose to approximate $\\frac{\\partial f(\\pmb{X}_{k}^{v},\\pmb{\\theta})}{\\partial y_{i,l}}$ using a white-box model, where how each training datapoint affects the final prediction is clear from the design of the model. To this end, we propose to approximate $\\frac{\\partial f(\\pmb{X}_{k}^{v},\\pmb{\\theta})}{\\partial y_{i,l}}$ using a kernel machine, i.e. $\\pmb{\\alpha}_{i,l}^{T}K\\left(\\pmb{X}_{i},\\pmb{X}_{k}^{v}\\right)$ , where $K\\left(\\mathbf{{X}}_{i},\\cdot\\right)$ is a kenerl between the training sample $X_{i}$ and test samples measuring the similarity between the $X_{i}$ and $X_{k}^{v}$ , and $_{\\alpha}$ is a learnerable hyperparameter. It can be proven that the indicator function based on this definition satisfies many desirable properties (please see Appendix for details) to construct an axiomatic attribution. Formally, the coefficient $\\alpha_{i,l}^{\\mathrm{~\\,~}}\\in\\mathbb{R}^{L_{2}}$ can be computed by solving the following optimization problem: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\hat{\\alpha}=\\operatorname*{argmin}_{\\alpha\\in\\mathbb{R}^{n}\\times\\mathbb{R}^{L_{2}}\\times\\mathbb{R}^{L_{2}}}\\left\\{\\sum_{i=1}^{n}\\sum_{l=1}^{L_{2}}\\sum_{j=1}^{n}\\mathcal{L}\\left(\\alpha_{i,l}^{T}K\\left(\\boldsymbol{X}_{i},\\boldsymbol{X}_{j}\\right),\\frac{\\partial f(\\boldsymbol{X}_{j},\\boldsymbol{\\theta})}{\\partial y_{i,l}}\\right)\\right\\}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "To solve this problem, $\\frac{f(\\pmb{X}_{j},\\pmb{\\theta}_{1})-f(\\pmb{X}_{j},\\pmb{\\theta}_{2})}{y_{i,l}^{(1)}-y_{i,l}^{(2)}}$ can act as a substitute of \u2202f(\u2202yXj,\u03b8)since there is no ground truth. However, the problem is still not practical to solve because we can not obtain $f(\\mathbf{{X}}_{j},\\pmb{{\\theta}}_{2})$ without retraining the model. Even though it isn\u2019t necessary to traverse all $i,\\,j$ , and $l$ to obtain the complete data, it still goes against our original intention to calculate $I(i,l)$ efficiently. Furthermore, it is also difficult for us to determine how much data is sufficient to ensure the accuracy of the solution. Fortunately, with the help of Remark 1, we can bypass the process of finding enough f(Xj,\u03b8(11))\u2212f((2X)j,\u03b82)and directly approximate $\\frac{\\partial f(\\pmb{X}_{j},\\pmb{\\theta})}{\\partial y_{i,l}}$ ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Remark 1. Given two infinitely differentiable functions $f({\\boldsymbol{x}})$ and $g(x)$ in a bounded domain $D\\in R^{n}$ , domain D, the measure of the region I that satisfyin $||f({\\boldsymbol{\\mathbf{x}}})-g({\\boldsymbol{\\mathbf{\\mathit{x}}}})||$ is always less than \u03f5. For any given g || \u2202f\u2202(xx)\u2212\u2202g\u2202(xx )|| > \u03b4 is not greater than \u03f52, $\\delta$ and $\\epsilon_{2}$ , there exists an \u03f5 such that, in the i.e, m(I) \u2264\u03f52. ", "page_idx": 4}, {"type": "text", "text": "Remark 1 gives us the intuition that we can avoid retraining our downstream models by first-order approximation. However, some issues still need to be clarified. First, we consider neural networks used in the downstream task as infinitely differentiable functions since in practical applications, it is unlikely for computed floating-point numbers to precisely equal non-differentiable points. Second, Remark 1 limits the definition domain to a bounded region $D$ . Time series data is usually bounded (for example, the renewable generation sequence cannot be greater than the installed capacity), making this assumption reasonable. Finally, Remark 1 can be rephrased as the better the approximation of the original function, the better the approximation of its derivative, that is, we can have a $\\frac{m(D)-\\epsilon_{2}}{m(D)}$ probability of ftiting the derivative well. Therefore, the optimization problem (3) can be transformed into an easier one. (Note that we indicate the existence of $\\epsilon$ that meets the conditions in this remark, but make no restrictions on $\\epsilon$ , while it is often difficult for us to make the approximation error of the original function sufficiently small. If the $\\epsilon$ in the remark is infinitely close to 0, practical applications will encounter difficulties. However, in gradient descent-based neural network training, such situations often do not hinder our practical applications. Due to the space limit, the full theoretical discussion is provided in the Appendix.) ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\hat{\\pmb{\\alpha}^{\\prime}}=\\operatorname*{argmin}_{\\alpha^{\\prime}\\in\\mathbb{R}^{n}\\times\\mathbb{R}^{L_{2}}}\\left\\{\\sum_{i=1}^{n}\\mathcal{L}\\left(\\sum_{j=1}^{n}\\alpha_{j}^{\\prime T}K\\left(\\pmb{X}_{i},\\pmb{X}_{j}\\right),f(\\pmb{X}_{i},\\pmb{\\theta})\\right)\\right\\},}\\\\ {\\displaystyle\\hat{\\pmb{\\alpha_{i,l}}}=\\frac{\\partial\\hat{\\pmb{\\alpha^{\\prime}}_{i}}}{\\partial y_{i,l}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Now the problem is converted to solving the problem (4). Intuitively, we solve it by projecting it onto the RKHS subspace spanned by the kernels, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\hat{\\alpha}^{\\prime}=\\operatorname*{argmin}_{\\alpha^{\\prime}\\in\\mathbb{R}^{n}\\times\\mathbb{R}^{L_{2}}}\\left\\{\\sum_{i=1}^{n}\\mathcal{L}\\left(\\underbrace{\\sum_{j=1}^{n}\\alpha_{j}^{\\prime T}K\\left(X_{i},X_{j}\\right)}_{f_{K}\\left(X_{i}\\right)},f\\left(X_{i},\\theta\\right)\\right)+\\frac{1}{2}\\underbrace{\\sum_{l=1}^{L_{2}}\\alpha_{\\cdot,l}^{\\prime\\intercal}K_{l}\\alpha_{\\cdot,l}^{\\prime}}_{\\left\\Vert f_{K}\\left\\Vert_{\\mu_{K}}^{2}\\right\\Vert}\\right\\},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\kappa_{l}$ is the kernel gram matrix defined as $\\pmb{K}_{l,i j}=\\pmb{K}\\left(\\pmb{X}_{i},\\pmb{X}_{j}\\right)_{l}$ . Considering the first-order optimality condition, \u03b1\u02c6\u2032i,l = \u2212n1\u2202L( f\u02c6K\u2202(f\u02c6Xi()X,f()Xi,\u03b8)) [36, 37]. Recalling our goal of estimating the relationship between $\\hat{\\alpha^{\\prime}}$ and $y_{i,l}$ in (5), their relationship is still unclear. This is because the objective of the optimization problem is to construct an approximation model without considering the role of label value $y_{i,l}$ . To clarify the effect of $y_{i,l}$ , we introduce an approximation by trigonometric inequality that $\\mathcal{L}\\left(f_{K}\\left(X_{i}\\right),f\\left(X_{i},\\theta\\right)\\right)\\leq\\mathcal{L}\\left(y_{i},f\\left(X_{i},\\theta\\right)\\right)+\\mathcal{L}\\left(f_{K}\\left(X_{i}\\right),y_{i}\\right)$ . The second term on the right side is the loss function corresponding to the downstream model, and in the case of training convergence, this should be close to a constant. Therefore, the optimization problem (6) can be rewritten as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\hat{\\alpha}^{\\prime}=\\operatorname*{argmin}_{\\alpha^{\\prime}\\in\\mathbb{R}^{n}\\times\\mathbb{R}^{L_{2}}}\\left\\{\\sum_{i=1}^{n}\\left(\\mathcal{L}\\left(y_{i},f\\left(X_{i},\\boldsymbol{\\theta}\\right)\\right)+\\mathcal{L}\\left(f_{K}\\left(X_{i}\\right),y_{i}\\right)\\right)+\\frac{1}{2}\\underbrace{\\sum_{l=1}^{L_{2}}\\alpha_{\\cdot,l}^{\\prime\\intercal}K_{l}\\alpha_{\\cdot,l}^{\\prime}}_{\\parallel f_{K}\\parallel_{\\mu_{K}}^{2}}\\right\\}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Formally, the solution are \u02c6\u03b1\u2032i = \u2212n1\u2202L( f\u02c6\u02c6K(Xi),yi) and $\\begin{array}{r}{\\hat{\\pmb{\\alpha}}_{i,l}\\,=\\,-\\frac{1}{n}\\frac{\\partial^{2}\\mathcal{L}\\left(\\hat{f}_{K}\\left(\\pmb{X}_{i}\\right),\\pmb{y}_{i}\\right)}{\\partial\\hat{f}_{K}\\left(\\pmb{X}_{i}\\right)\\partial\\pmb{y}_{i,l}}}\\end{array}$ . Furthermore, since $f_{K}(\\cdot)$ is used to approximate $f(\\cdot,\\pmb\\theta)$ , we use $f(\\cdot,\\theta)$ to replace $f_{K}(\\cdot)$ to simplify the calculation. With the above preparation, the $I(i,l)$ can be represented as follow with NTK kernel [38], ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{m}-\\frac{1}{n}\\frac{\\partial\\mathcal{L}\\big(f(X_{k}^{v},\\theta),y_{k}^{v}\\big)}{\\partial f(X_{k}^{v},\\theta)}\\underbrace{\\frac{\\partial^{2}\\mathcal{L}\\left(f\\left(X_{i},\\theta\\right),y_{i}\\right)^{T}}{\\partial f\\left(X_{i},\\theta_{t}\\right)\\partial y_{i,l}}}_{a_{i,l}}\\underbrace{\\frac{\\partial f\\left(X_{i},\\theta\\right)}{\\partial\\theta}\\frac{\\partial f\\left(X_{k}^{v},\\theta\\right)^{T}}{\\partial\\theta}}_{N T K k e r n e l}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "2.3 Similarity Calculation Acceleration ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In the previous section, to gauge the effects of substituting $y_{i,l}^{(1)}$ with $y_{i,l}^{(2)}$ on the downstream task, we utilized the Neural Tangent Kernel to assess the similarity between the model outputs for inputs $X_{i}$ and $X_{k}^{v}$ . Given that the model\u2019s output length is $L_{2}$ , the computational complexity of calculating $I(i,l)$ for all time steps in $\\textit{\\textbf{y}}$ scales as $\\mathcal{O}(m L_{2}P)$ , where $P$ denotes the total number of parameters in the model $f(\\cdot,\\pmb\\theta)$ , i.e., $|\\theta|$ . In numerous time series applications, such as forecasting, $L_{2}$ can be substantially large (e.g., 128), rendering the evaluation process for all imputations timeconsuming. To mitigate this challenge, we propose a method to compress the size of \u2202f(\u2202X\u03b8i,\u03b8). Our approach is inspired by the observation that in time series forecasting, the output of $\\scriptstyle{\\check{f}}(\\cdot,\\,\\theta)$ typically exhibits smooth variability across different $l$ values. Therefore, we posit that the model output $f(\\boldsymbol{X}_{i},\\boldsymbol{\\theta})$ resides in a low-dimensional space spanned by a limited number of smooth basis functions. In mathematical terms, $f(X_{i},\\pmb\\theta)\\approx\\pmb{A}^{\\dagger}\\pmb{A}f(\\pmb X_{i},\\pmb\\theta)$ where $A\\in\\mathbb{R}^{r\\times L_{2}}$ consists of rows each representing a predefined smooth vector, $\\pmb{A}^{\\dagger}\\in\\mathbb{R}^{L_{2}\\times r}$ is the pseudo-inverse of $\\pmb{A}$ , and $r$ , which is significantly smaller than $L_{2}$ , denotes the number of basis functions employed to approximate $f(\\boldsymbol{X}_{i},\\boldsymbol{\\theta})$ . Consequently, we can approximat e \u2202f(Xi,\u03b8)as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\frac{\\partial f\\left(\\mathbf{X}_{i},\\pmb{\\theta}\\right)}{\\partial\\pmb{\\theta}}\\approx\\mathbf{A}^{\\dagger}\\frac{\\partial\\mathbf{A}f\\left(\\mathbf{X}_{i},\\pmb{\\theta}\\right)}{\\partial\\pmb{\\theta}}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "This approximation allows for the compression of the model output, thereby reducing the number of gradients that require computation. Through this simplification, the computational complexity for calculating $I(i,l)$ decreases to $\\mathcal{O}(m r P)$ , substantially less than the original complexity. ", "page_idx": 5}, {"type": "text", "text": "In our experiments, we further simplify by assuming $\\pmb{A}$ to be a block diagonal matrix, defined as blkdiag $\\left(\\mathbf{1}_{1},\\mathbf{1}_{2},\\ldots,\\mathbf{1}_{c}\\right)$ , where $\\mathbf{1}_{1}=\\cdots=\\mathbf{1}_{c-1}$ are vectors of length $\\lfloor\\bar{L}_{2}/c\\rfloor$ with all elements equal to 1, and $\\mathbf{1}_{c}\\in\\mathbb{R}^{L_{2}-(c-1)\\lfloor L_{2}/c\\rfloor}$ is a vector with all elements equal to 1. ", "page_idx": 5}, {"type": "text", "text": "2.4 Task-oriented Imputation Evaluation ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We have introduced a method for computing the indicator I(i, l), which assesses if replacing yi(,1l) with $y_{i,l}^{(2)}$ results in a reduced loss for the downstream task. Given two sets of imputation results, $\\{y_{i}^{(1)}\\}_{i=1}^{n}$ sa nadn $\\{y_{i}^{(2)}\\}_{i=1}^{n}$ ,p sd, earinvde di dferontmif yd itsthiantc poutuattpieornf otercmhsn s,a tw tei mcae ns teevpa aift $I(i,l)$ garceraotsesr $y_{i}^{(2)}$ $\\pmb{y}_{i}^{(1)}$ $l$ $I(i,l)$ than zero and vice versa if the value is lesser. In contrast to conventional evaluation strategies, our proposed method does not necessitate the availability of the ground truth $\\textit{\\textbf{y}}$ , thereby enhancing its practical utility in myriad real-world scenarios where actual values remain unobtainable. This feature renders our approach significantly more adaptable to situations where empirical truths are elusive. ", "page_idx": 5}, {"type": "text", "text": "2.5 Task-oriented Imputation Emsemble ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Given that our proposed method can evaluate the quality of two imputations at the time step level, a natural extension is to combine these two sets of imputations to derive an improved result. Specifically, we can generate a new set of imputations $\\pmb{y}_{i}^{\\prime}$ for the $i^{\\th}$ -th sample, where the $l$ -th entry is $y_{i,l}^{(2)}$ if $I(i,l)\\;>\\;0$ , and $y_{i,l}^{(1)}$ otherwise. Based on the definition of $I(i,l)$ , we anticipate that a model trained using $\\pmb{y}_{i}^{\\prime}$ will incur a lower loss compared to one trained with $\\pmb{y}_{i}^{(1)}$ , thereby yielding better imputation results. It is important to note, however, that the calculation of $I(i,l)$ is predicated on the scenario where only the lth timestep in the $i$ th sample from $\\{y^{(1)}\\}_{i=1}^{n}$ is substituted with $y_{i,l}^{(2)}$ . This consideration omits the potential interactions among samples. Consequently, in practical implementations, we opt to substitute only the timesteps that rank within the top $c\\%$ of $I(i,l)$ values. Our experiments, detailed in 3.3.2, confirm the efficacy of our proposed task-oriented imputation ensemble method. Our proposed ensemble process is summarized in Algorithm 1. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "image", "img_path": "n2dvAKKQoM/tmp/ba69d900f454f717288fa08d7bb534ffb900efc175d99e66f77edc30cc589f8e.jpg", "img_caption": [], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "3 Experiment ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "3.1 Datasets and Experiment Setup ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "To validate our method, we conduct experiments on six datasets: the GEF load forecasting competition dataset with the corresponding temperature [39], the UCI dataset (electricity load and air quality) [40], the Traffic dataset containing road occupancy rates2, and two transformer datasets, ETTH1 and ETTH2 [41]. Note that we use the hourly resolution version of the UCI electricity dataset from [42] in the main experiment. In our main experiment, we set the downstream task as univariate time series forecasting, with both input sequence and prediction lengths set to 24. In addition to the GEF dataset, we implement our method on the \u2019OT\u2019 time series in ETH1 and ETH2, the mean value of the road occupancy rate in Traffic, the temperature in the UCI air quality dataset, and the total electricity consumption of 321 users in the UCI electricity dataset. It is important to note that there are no original missing values in these datasets. To simulate the missing values situation, we randomly set masks with lengths in [2, 4, 6, 12, 24, 48, 96, 120], and replace the original values with the average value at the corresponding positions as the baseline. For the missing rate setting, if the missing rate is too low, the difference between different imputation methods may be small, while if the missing rate is too high, the even best imputation method will also be difficult to obtain reasonable results. Based on the above considerations, we mainly consider $40\\%$ missing rates as our main experimental setup. Meanwhile, we will provide experimental results under other missing rate settings in $[30\\%$ , $50\\%$ , $60\\%]$ in the Appendix. ", "page_idx": 6}, {"type": "text", "text": "3.2 Time Series Imputation Methods ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "To verify the performance of our strategy in evaluating different time imputation methods, we introduce multiple advanced time imputation methods. Firstly, as mentioned in the last subsection, we use the basic mean value imputation as the baseline. Secondly, we consider several time series imputation methods based on deep neural networks. They are GPVAE [25] and USGAN [27] based on generative neural network, mRNN [43] and BRITS [11] based on RNN structure, SAITS [13], and ImputeFormer [44] based on attention mechanism. All the implementations of the above models are with the help of the toolkit called PyPOTS [45]. In addition, we also implement the network structure of a spatiotemporal graphs-assisted method called SPIN [46] for time series imputation. All the details of the neural network setting will be described in the Appendix. ", "page_idx": 6}, {"type": "text", "text": "3.3 Experimental Results ", "text_level": 1, "page_idx": 7}, {"type": "image", "img_path": "n2dvAKKQoM/tmp/53f24daea328fa5be2e5d474ee8e0c84c9bf69705b968a89f5ae09b556105446.jpg", "img_caption": ["3.3.1 Application 1: Estimate the Gain ", "Figure 1: The correlation and accuracy comparison between the estimation of imputation value gain and actual gain (MSE\u2193), where INF (section D.5) represents our modified Influence Function, Seq-sim represents our original method, and Seg-N represents the acceleration method divided by $_\\mathrm{N}$ segments. The horizontal axis here represents selecting the sample with the highest $x\\%$ influence based on the absolute value of the estimation. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "In this section, we examine the estimated gains of imputation for each time step. We divide the GEF dataset into a training set and a test set, where the training set includes load data from 2011 and the test set includes load data from 2012. We replace $40\\%$ of the load data in the training set with the average value at the corresponding position as the baseline, apply the linear interpolation method to replace the corresponding baseline, and use them as training labels separately. In the training set, we have obtained a total of 8760 training samples. Therefore, we replace the labels of each sample one by one to construct new 8760 sets of samples, retrain 8760 forecasting models, test their performance on the test set, and compare the performance of the new model with the model trained on the mean value-based samples. Note that although our estimation is done at each time step of each sample, the time consumption of replacing each time step and retraining is too high. Therefore, we replace all the time steps in each sample and sum up the benefti estimates for each time step as the benefti estimate for the entire sample after replacement. We adopt the 3-layer MLP structure used in [35] and extend it to be used for time series. In addition, we have also added a simple and widely used forecasting model, DLinear [47], as our forecasting backbone model. ", "page_idx": 7}, {"type": "table", "img_path": "n2dvAKKQoM/tmp/a92d65bc990ce0e007c2f67f025dad3cc72d41ccc80790192630af1e39ee9400.jpg", "table_caption": ["Table 1: Time comparison between different methods. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "As shown in Figure 1, we use two metrics to compare the performance of different methods, where Corr represents the correlation between estimated gain and actual gain, and accuracy represents the percentage of the same sign between actual gain and estimated gain. In the MLP model, the Influence Function achieves a good correlation. However, in terms of accuracy, the accuracy of the Influence Function rapidly decreases as the percentage of estimated data to total data increases. On the contrary, our method exhibits good characteristics at different percentages. In addition, the segmented acceleration method generally shows that the more segments there are, the better the performance. Although it performs poorly in small percentages, it exhibits good performance in a wide range of data. In the DLiner model, both our method and the segmented acceleration method present similar situations to those in the MLP model. However, the Influence Function exhibits poor performance, with estimation accuracy and correlation significantly lagging behind our method. In addition, Table 1 reports the total time required for different methods to run on the GTX4090, indicating that retraining requires a significant amount of time while our method achieves a good balance between performance and time consumption. It is worth mentioning that when we focus not only on the small portion of time steps with the greatest impact but also examine the impact of relatively large time steps on the forecasting results, our segmented acceleration method achieves very good performance while reducing the required time. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "3.3.2 Application 2: Combine Different Time Series Imputation ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Similar to the previous section, we use mean value as a baseline to estimate the benefits on the validation set (not test set) obtained by imputation at each time step and replace the $10\\%$ time step with the highest benefits to train the forecasting model. In addition, the missing values in the time series damage the original characteristic of the time series. Although imputation values can repair it to some extent, they may still have adverse effects on downstream tasks. Therefore, we also introduce the Influence Function as a comparison, using it to estimate the $10\\%$ points with the worst impact on the forecasting results after imputation and remove them, which is often the case in the application of Influence Function [34]. ", "page_idx": 8}, {"type": "text", "text": "Table 2 reports the comparison of MSE for downstream forecasting tasks (here we use the DLinear as the backbone model), and we will analyze from two aspects. ", "page_idx": 8}, {"type": "text", "text": "I. Comparison between Original and Gain Estimation. Part I and II in Table 2 demonstrate that combining different imputations can enhance the performance of downstream tasks on all datasets. The improvements in the GEF dataset, Electricity dataset, and Traffic dataset are significant, while the enhancement in others is relatively less noticeable. The primary reason is that, in those datasets, the performance of other imputation methods considerably lags behind the original one, making the replacement of the original label with a newly imputed label seem less impactful. However, after incorporating our estimation, there is still a slight improvement in forecasting performance. On the other hand, when considering combining two imputation methods with similar original performance, incorporating our method will bring significant gains (as seen in the GEF datasets). It\u2019s worth noting that the mean value used as a baseline outperforms other imputation methods in most cases, primarily because the training data input is also based on mean value imputation, facilitating unified and convenient comparisons. In practical applications, we can also apply other advanced imputation methods to the input data and modify the labels based on the estimated benefits. ", "page_idx": 8}, {"type": "text", "text": "II. Comparison between Gain Estimation and Influence Function. Part I, II, and III Table 2 indicate that discarding a certain number of samples according to the Influence Function can indeed improve the performance of forecasting; however, such improvement is not universal. In the AIR dataset, discarding some data can negatively impact the performance of most methods. This may be due to the small amount of data contained in the AIR dataset, resulting in a greater adverse effect when discarding data. The operation of discarding data can only consider one imputation method, while our method can combine any two imputation methods to achieve better results. Furthermore, the results of repeated experiments show that the strategy of modifying values at specific time steps can make performance more stable, as its variance is significantly smaller than that of discarding data. ", "page_idx": 8}, {"type": "text", "text": "In addition to the univariate input results displayed in Table 2, we also include the results of multivariate inputs, which are common in practical applications. For instance, when predicting power loads, temperature is a crucial external variable. A large amount of research has focused on studying the relationship between load and temperature [48, 49]. In this experimental setting, unlike multivariate forecasting, temperature plays an auxiliary role in load forecasting while there is no need to forecast temperature itself. Consequently, we conduct experiments on the GEF dataset, inputting temperature as an external variable into the model to forecast loads. Table 3 presents the performance of our model under multivariate input, which is consistent with the univariate input scenario, and incorporating our method proves to be beneficial. ", "page_idx": 8}, {"type": "table", "img_path": "n2dvAKKQoM/tmp/4efe09c674c2c0c8f1b6fbe37c2ca00cb1a9315d5cb440d832b4dec3835118c4.jpg", "table_caption": ["Table 2: $\\mathrm{MSE}\\downarrow$ in the downstream forecasting task with univariate input, every experiment is done 3 times. "], "table_footnote": [], "page_idx": 9}, {"type": "table", "img_path": "n2dvAKKQoM/tmp/ce2215f1544aca7718dde30456eb674343343d51ecd9bd7220a356ff566529c0.jpg", "table_caption": ["Table 3: $\\mathrm{MSE.}$ in the downstream forecasting task with multivariate input, every experiment is done 3 times. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "4 Conclusion and Future Work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we propose to evaluate the imputation values at each time step for the impact on downstream forecasting tasks. On the one hand, our method can accurately estimate the gain of each imputation value without retraining. On the other hand, our method can also combine different time series imputation strategies based on the estimation of gain to obtain better imputation for downstream tasks. To ensure the applicability of this method in practical scenarios, we also provide an accelerated calculation method. In the future, we will focus on further downstream tasks, such as optimization tasks based on prediction values, and build an end-to-end evaluation strategy. ", "page_idx": 9}, {"type": "text", "text": "5 Acknowledgement ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The work was supported in part by the National Key R&D Program of China (2022YFE0141200), in part by the Research Grants Council of the Hong Kong SAR (HKU 27203723), and in part by the Alibaba Group through Alibaba Research Intern Program. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] M. Jin, H. Y. Koh, Q. Wen, D. Zambon, C. Alippi, G. I. Webb, I. King, and S. Pan, \u201cA survey on graph neural networks for time series: Forecasting, classification, imputation, and anomaly detection,\u201d IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024.   \n[2] Y. Liang, H. Wen, Y. Nie, Y. Jiang, M. Jin, D. Song, S. Pan, and Q. Wen, \u201cFoundation models for time series analysis: A tutorial and survey,\u201d in Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, 2024, pp. 6555\u20136565. [3] J. D. Hamilton, Time series analysis. Princeton university press, 2020.   \n[4] D. Qin, C. Wang, Q. Wen, W. Chen, L. Sun, and Y. Wang, \u201cPersonalized federated darts for electricity load forecasting of individual buildings,\u201d IEEE Transactions on Smart Grid, vol. 14, no. 6, pp. 4888\u20134901, 2023. [5] J. Wang, L. Han, X. Zhang, Y. Wang, and S. Zhang, \u201cElectrical load forecasting based on variable t-distribution and dual attention mechanism,\u201d Energy, vol. 283, p. 128569, 2023. [6] M. Grabner, Y. Wang, Q. Wen, B. Bla\u017ei\u02c7c, and V. \u0160truc, \u201cA global modeling framework for load forecasting in distribution networks,\u201d IEEE Transactions on Smart Grid, vol. 14, no. 6, pp. 4927\u20134941, 2023. [7] W. Du, J. Wang, L. Qian, Y. Yang, F. Liu, Z. Wang, Z. Ibrahim, H. Liu, Z. Zhao, Y. Zhou et al., \u201cTsi-bench: Benchmarking time series imputation,\u201d arXiv preprint arXiv:2406.12747, 2024. [8] C. Miller, A. Kathirgamanathan, B. Picchetti, P. Arjunan, J. Y. Park, Z. Nagy, P. Raftery, B. W. Hobson, Z. Shi, and F. Meggers, \u201cThe building data genome project 2, energy meter data from the ASHRAE great energy predictor III competition,\u201d Scientific Data, vol. 7, p. 368, Oct. 2020. [9] M. Steinbach and P.-N. Tan, \u201cknn: k-nearest neighbors,\u201d in The top ten algorithms in data mining. Chapman and Hall/CRC, 2009, pp. 165\u2013176.   \n[10] R. H. Shumway, D. S. Stoffer, R. H. Shumway, and D. S. Stoffer, \u201cArima models,\u201d Time series analysis and its applications: with R examples, pp. 75\u2013163, 2017.   \n[11] W. Cao, D. Wang, J. Li, H. Zhou, L. Li, and Y. Li, \u201cBrits: Bidirectional recurrent imputation for time series,\u201d Advances in neural information processing systems, vol. 31, 2018.   \n[12] L. Pinheiro Cinelli, M. Ara\u00fajo Marins, E. A. Barros da Silva, and S. Lima Netto, \u201cVariational autoencoder,\u201d in Variational Methods for Machine Learning with Applications to Deep Networks. Springer, 2021, pp. 111\u2013149.   \n[13] W. Du, D. C\u00f4t\u00e9, and Y. Liu, \u201cSaits: Self-attention-based imputation for time series,\u201d Expert Systems with Applications, vol. 219, p. 119619, 2023.   \n[14] J. Cheng, C. Yang, W. Cai, Y. Liang, Q. Wen, and Y. Wu, \u201cNuwaTS: Mending every incomplete time series,\u201d arXiv preprint arXiv:2405.15317, 2024.   \n[15] J. Wang, W. Du, W. Cao, K. Zhang, W. Wang, Y. Liang, and Q. Wen, \u201cDeep learning for multivariate time series imputation: A survey,\u201d arXiv preprint arXiv:2402.04059, 2024.   \n[16] H. Cheng, Q. Wen, Y. Liu, and L. Sun, \u201cRobustTSF: Towards theory and design of robust time series forecasting with anomalies,\u201d in The Twelfth International Conference on Learning Representations (ICLR), 2024.   \n[17] M. Amiri and R. Jensen, \u201cMissing data imputation using fuzzy-rough methods,\u201d Neurocomputing, vol. 205, pp. 152\u2013164, 2016.   \n[18] G. E. Box, G. M. Jenkins, G. C. Reinsel, and G. M. Ljung, Time series analysis: forecasting and control. John Wiley & Sons, 2015.   \n[19] C. Hamza\u00e7ebi, \u201cImproving artificial neural networks\u2019 performance in seasonal time series forecasting,\u201d Information Sciences, vol. 178, no. 23, pp. 4550\u20134559, 2008.   \n[20] N. S. Altman, \u201cAn introduction to kernel and nearest-neighbor nonparametric regression,\u201d The American Statistician, vol. 46, no. 3, pp. 175\u2013185, 1992.   \n[21] S. Liu, X. Li, G. Cong, Y. Chen, and Y. Jiang, \u201cMultivariate time-series imputation with disentangled temporal representations,\u201d in The Eleventh International Conference on Learning Representations, 2022.   \n[22] S. Van Buuren and K. Groothuis-Oudshoorn, \u201cmice: Multivariate imputation by chained equations in r,\u201d Journal of statistical software, vol. 45, pp. 1\u201367, 2011.   \n[23] S. Fang, Q. Wen, Y. Luo, S. Zhe, and L. Sun, \u201cBayotide: Bayesian online multivariate time series imputation with functional decomposition,\u201d in Forty-first International Conference on Machine Learning (ICML), 2024.   \n[24] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, \u0141. Kaiser, and I. Polosukhin, \u201cAttention is all you need,\u201d Advances in neural information processing systems, vol. 30, 2017.   \n[25] V. Fortuin, D. Baranchuk, G. R\u00e4tsch, and S. Mandt, \u201cGp-vae: Deep probabilistic time series imputation,\u201d in International conference on artificial intelligence and statistics. PMLR, 2020, pp. 1651\u20131661.   \n[26] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio, \u201cGenerative adversarial networks,\u201d Communications of the ACM, vol. 63, no. 11, pp. 139\u2013144, 2020.   \n[27] X. Miao, Y. Wu, J. Wang, Y. Gao, X. Mao, and J. Yin, \u201cGenerative semi-supervised learning for multivariate time series imputation,\u201d in Proceedings of the AAAI conference on artificial intelligence, vol. 35, no. 10, 2021, pp. 8983\u20138991.   \n[28] J. Ho, A. Jain, and P. Abbeel, \u201cDenoising diffusion probabilistic models,\u201d Advances in Neural Information Processing Systems, vol. 33, pp. 6840\u20136851, 2020.   \n[29] Z. Hammoudeh and D. Lowd, \u201cTraining data influence analysis and estimation: A survey,\u201d Machine Learning, pp. 1\u201353, 2024.   \n[30] A. Ghorbani and J. Zou, \u201cData shapley: Equitable valuation of data for machine learning,\u201d in International conference on machine learning. PMLR, 2019, pp. 2242\u20132251.   \n[31] T. Wang and R. Jia, \u201cData banzhaf: A data valuation framework with maximal robustness to learning stochasticity,\u201d arXiv preprint arXiv:2205.15466, 2022.   \n[32] Y. Kwon and J. Zou, \u201cBeta shapley: a unified and noise-reduced data valuation framework for machine learning,\u201d arXiv preprint arXiv:2110.14049, 2021.   \n[33] C.-K. Yeh, J. Kim, I. E.-H. Yen, and P. K. Ravikumar, \u201cRepresenter point selection for explaining deep neural networks,\u201d Advances in neural information processing systems, vol. 31, 2018.   \n[34] P. W. Koh and P. Liang, \u201cUnderstanding black-box predictions via influence functions,\u201d in International conference on machine learning. PMLR, 2017, pp. 1885\u20131894.   \n[35] G. Pruthi, F. Liu, S. Kale, and M. Sundararajan, \u201cEstimating training data influence by tracing gradient descent,\u201d Advances in Neural Information Processing Systems, vol. 33, pp. 19 920\u2013 19 930, 2020.   \n[36] C.-P. Tsai, C.-K. Yeh, and P. Ravikumar, \u201cSample based explanations via generalized representers,\u201d Advances in Neural Information Processing Systems, vol. 36, 2024.   \n[37] B. Sch\u00f6lkopf, R. Herbrich, and A. J. Smola, \u201cA generalized representer theorem,\u201d in International conference on computational learning theory. Springer, 2001, pp. 416\u2013426.   \n[38] A. Jacot, F. Gabriel, and C. Hongler, \u201cNeural tangent kernel: Convergence and generalization in neural networks,\u201d Advances in neural information processing systems, vol. 31, 2018.   \n[39] T. Hong, P. Pinson, and S. Fan, \u201cGlobal energy forecasting competition 2012,\u201d pp. 357\u2013363, 2014.   \n[40] D. Dua, C. Graff et al., \u201cUci machine learning repository,\u201d 2017.   \n[41] H. Zhou, S. Zhang, J. Peng, S. Zhang, J. Li, H. Xiong, and W. Zhang, \u201cInformer: Beyond efficient transformer for long sequence time-series forecasting,\u201d in The Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Virtual Conference, vol. 35, no. 12. AAAI Press, 2021, pp. 11 106\u201311 115.   \n[42] G. Lai, W.-C. Chang, Y. Yang, and H. Liu, \u201cModeling long-and short-term temporal patterns with deep neural networks,\u201d in The 41st international ACM SIGIR conference on research & development in information retrieval, 2018, pp. 95\u2013104.   \n[43] J. Yoon, W. R. Zame, and M. van der Schaar, \u201cEstimating missing data in temporal data streams using multi-directional recurrent neural networks,\u201d IEEE Transactions on Biomedical Engineering, vol. 66, no. 5, pp. 1477\u20131490, 2018.   \n[44] T. Nie, G. Qin, W. Ma, Y. Mei, and J. Sun, \u201cImputeformer: Low rankness-induced transformers for generalizable spatiotemporal imputation,\u201d in Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, 2024, pp. 2260\u20132271.   \n[45] W. Du, \u201cPyPOTS: a Python toolbox for data mining on Partially-Observed Time Series,\u201d 2023. [Online]. Available: https://arxiv.org/abs/2305.18811   \n[46] I. Marisca, A. Cini, and C. Alippi, \u201cLearning to reconstruct missing data from spatiotemporal graphs with sparse observations,\u201d Advances in Neural Information Processing Systems, vol. 35, pp. 32 069\u201332 082, 2022.   \n[47] A. Zeng, M. Chen, L. Zhang, and Q. Xu, \u201cAre transformers effective for time series forecasting?\u201d in Proceedings of the AAAI conference on artificial intelligence, vol. 37, no. 9, 2023, pp. 11 121\u201311 128.   \n[48] L. Yang, R. Ren, X. Gu, and L. Sun, \u201cInteractive generalized additive model and its applications in electric load forecasting,\u201d in Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, 2023, pp. 5393\u20135403.   \n[49] J. Xie and T. Hong, \u201cTemperature scenario generation for probabilistic load forecasting,\u201d IEEE Transactions on Smart Grid, vol. 9, no. 3, pp. 1680\u20131687, 2016.   \n[50] E. Winter, \u201cThe shapley value,\u201d Handbook of game theory with economic applications, vol. 3, pp. 2025\u20132054, 2002.   \n[51] L. Bottou, \u201cLarge-scale machine learning with stochastic gradient descent,\u201d in Proceedings of COMPSTAT\u20192010: 19th International Conference on Computational StatisticsParis France, August 22-27, 2010 Keynote, Invited and Contributed Papers. Springer, 2010, pp. 177\u2013186.   \n[52] D. Kingma, \u201cAdam: a method for stochastic optimization,\u201d in Int Conf Learn Represent, 2014.   \n[53] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga et al., \u201cPytorch: An imperative style, high-performance deep learning library,\u201d Advances in neural information processing systems, vol. 32, 2019. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Why We Need Task-oriented Imputation ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Here we use a toy example to illustrate that, in some cases, we can not directly use the accuracy of imputation instead of downstream tasks to evaluate the imputation method. We want to point out that better imputation accuracy does not always mean better forecasting performance, and we simulate a dataset based on the GEF dataset to illustrate this viewpoint, experimenting with a predicted length of 24. Suppose that we only observed the value at the time step nk $(\\mathtt{k}{\\ge}0)$ and $\\mathrm{nk}{+}1$ $(\\mathtt{k}{\\ge}1)$ , just for the convenience of linear interpolation. In the first case(represented by I), we set $\\mathtt{n}=4$ , fill the missing value with linear interpolation, and uniformly add Gaussian noise $\\mathcal{N}(0.05,0.3)$ . In the second case(represented by II), we set $\\mathfrak{n}=6$ and only do the linear interpolation (shown in Figure 2). We put two data sets into MLP and calculated the forecasting error as shown in the following Table 4. ", "page_idx": 13}, {"type": "image", "img_path": "n2dvAKKQoM/tmp/e62c409f503f18fc9c5d81f93e0fc91058d28cf8ba1ff1e6bfddf7000afa11a8.jpg", "img_caption": ["Figure 2: Visualization of simulated data "], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "Table 4: Imputation and Forecasting accuracy on simulated dataset ", "page_idx": 13}, {"type": "table", "img_path": "n2dvAKKQoM/tmp/e02382905e6416ddb4665a0bcbfcbee63e620eab5fcd4819262526da05e41882.jpg", "table_caption": [], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "B Axioms ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In [36], there are several axioms desirable for a fair attribution. In this section, we modify them to a version suitable for our task and demonstrate that our method satisfies such properties. Here we use $I(i,l,X_{k})$ to represent the impact of the perturbation of the l-th step of the i-th sample on sample Xk. ", "page_idx": 13}, {"type": "text", "text": "Definition 1 (Efficiency Axiom). For any model $f(\\cdot,\\pmb\\theta)$ , and test point $X_{k}^{v}$ , an indication function $I(\\cdot,\\cdot,\\cdot)$ satisfies the efficiency axiom iff: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{i}^{n}\\sum_{l}^{L_{2}}I(i,l)=\\displaystyle\\sum_{k=1}^{m}\\left(\\mathcal{L}\\left(f\\left(X_{k}^{v},\\theta_{1}\\right),y_{k}^{v}\\right)-\\mathcal{L}\\left(f\\left(X_{k}^{v},\\theta_{2}\\right),y_{k}^{v}\\right)\\right)}\\\\ &{\\mathrm{s.t.}\\ \\theta_{1}=\\arg\\operatorname*{min}_{\\theta}\\displaystyle\\sum_{k=1}^{n}\\mathcal{L}\\left(f\\left(X_{k},\\theta\\right),y_{k}^{(1)}\\right)}\\\\ &{\\theta_{2}=\\arg\\operatorname*{min}_{\\theta}\\displaystyle\\sum_{k=1}^{n}\\mathcal{L}\\left(f\\left(X_{k},\\theta\\right),y_{k}^{(2)}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "This is a counterpart of the efficiency axioms in Shapley values [50], and our method is naturally satisfying. ", "page_idx": 14}, {"type": "text", "text": "Definition 2 (Self-Explanation Axiom). An indication function $I(\\cdot,\\cdot,\\cdot)$ satisfies the self-explanation axiom iff there exists any training point $X_{i}$ having no effect on itself, i.e. $I(i,l,\\pmb{X}_{i})=0$ , the training point should not impact any other points, i.e. $I(\\bar{i},l,X)=0$ for all $\\mathbf{\\deltaX}$ . ", "page_idx": 14}, {"type": "text", "text": "Similar to the dummy axiom in the Shapley values, our method naturally satisfies. ", "page_idx": 14}, {"type": "text", "text": "Definition 3 (Symmetric Zero Axiom). An indication function $I(\\cdot,\\cdot,\\cdot)$ satisfies the symmetric zero axiom iff any two training points $X_{i},X_{j}$ such that if $I(i,\\cdot,{\\pmb X}_{i})\\not=0$ and $I(j,\\cdot,{\\pmb X}_{j})\\sp{\\prime}\\ne0$ , then ", "page_idx": 14}, {"type": "equation", "text": "$$\nI(j,\\cdot,X_{i})=0\\Longrightarrow I(i,\\cdot,X_{j})=0.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "This situation holds if and only if $K(X_{i},X_{j})=0$ , therefore, our method satisfies. ", "page_idx": 14}, {"type": "text", "text": "Definition 4 (Symmetric Cycle Axiom). An indication function $I(\\cdot,\\cdot,\\cdot)$ satisfies the symmetric cycle axiom iff for any set of training points $\\pmb{X}_{t_{1}},\\pmb{\\mathscr{\\tau}}.\\cdot\\pmb{X}_{t_{k}}$ , with possible duplicates, and $\\pmb{X}_{t_{k+1}}=\\pmb{X}_{t_{1}}$ , it holds that: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\prod_{i=1}^{k}I\\left(t_{i},\\cdot,{\\pmb X}_{t_{i+1}}\\right)=\\prod_{i=1}^{k}I\\left(t_{i+1},\\cdot,{\\pmb X}_{t_{i}}\\right)\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The original definition of $I(\\cdot,\\cdot,\\cdot)$ does not satisfy the above properties, as our goal is to estimate the impact of label perturbations on the loss. However, when removing \u2202L(f(Xk,\u03b8),yk) from the definition, our method satisfies this property. ", "page_idx": 14}, {"type": "text", "text": "Definition 5 (Continuity Axiom). An indication function $I(\\cdot,\\cdot,\\cdot)$ satisfies the continuity axiom iff it is continuous wrt the test data point $\\mathbf{\\deltaX}$ , for any fixed training point $X_{i}$ : ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{{\\pmb X}^{\\prime}\\to{\\pmb X}}I(j,\\cdot,{\\pmb X}^{\\prime})=I(j,\\cdot,{\\pmb X})\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Definition 6 (Irreducibility Axiom). An indication function $I(\\cdot,\\cdot,\\cdot)$ satisfies the irreducibility axiom iff for any number of training points $X_{1},\\ldots,X_{k}$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname*{det}\\left(\\begin{array}{c c c c}{I(1,\\cdot,X_{1})}&{I(1,\\cdot,X_{2})}&{\\ldots}&{I(1,\\cdot,X_{k})}\\\\ {I(2,\\cdot,X_{1})}&{I(2,\\cdot,X_{2})}&{\\ldots}&{I(2,\\cdot,X_{k})}\\\\ {\\ldots}&{\\ldots}&{\\ldots}&{\\ldots}\\\\ {I(k,\\cdot,X_{1})}&{I(k,\\cdot,X_{2})}&{\\ldots}&{I(k,\\cdot,X_{k})}\\end{array}\\right)\\geq0.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "A sufficient condition for an attribution $A(\\cdot)$ to satisfy the irreducibility axiom is for ", "page_idx": 14}, {"type": "equation", "text": "$$\n|I(i,\\cdot,{\\pmb X}_{i})|>\\sum_{j\\neq i}|I(i,\\cdot,{\\pmb X}_{j})|\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "When selecting the NTK kernel, this property naturally satisfies. ", "page_idx": 14}, {"type": "text", "text": "C Discussion on Remark 1 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Remark 1. Given two infinitely differentiable functions $f({\\boldsymbol{x}})$ and $g(x)$ in a bounded domain $D\\in R^{n}$ , $||f({\\boldsymbol{\\mathbf{x}}})-g({\\boldsymbol{\\mathbf{\\mathit{x}}}})||$ is always less than \u03f5. For any given $\\delta$ and $\\epsilon_{2}$ , there exists an $\\epsilon$ such that, in the id.eo, $D$ . measure of the region $I$ that satisfying || \u2202f\u2202(xx)\u2212\u2202g\u2202(xx )|| > \u03b4 is not greater than \u03f52, $m(I)\\leq\\epsilon_{2}$ ", "page_idx": 14}, {"type": "text", "text": "Correctness. Firstly, we can relax the restrictions on the function by requiring that each dimension of the function on $R^{n}$ be continuous. Then, we can simplify the problem into a one-dimensional case on $R$ . Secondly, we can let $h(x)=f(x)-g(x)$ (note that $|h(x)|<\\epsilon)$ , and then our problem can be transformed into proving that for any given $\\delta$ and $\\epsilon_{2}$ , there exists an $\\epsilon$ such that, in the interval $[a,b]$ , the measure of the region satisfying $\\vert h^{\\prime}(x)\\vert>\\delta$ is less than $\\epsilon_{2}$ . ", "page_idx": 14}, {"type": "text", "text": "Let the domain $I$ represent the set of all $x$ that satisfy $\\vert h^{\\prime}(x)\\vert>\\delta$ . We first need to prove that $I$ can be rewritten as the union of several disjoint intervals $I_{i}$ that satisfy ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\forall I_{i},\\forall x\\in I_{i},h(x)>\\delta\\quad o r\\quad h(x)<-\\delta.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Since $h^{\\prime}(x)$ itself is a continuous function, the division is obvious. The problem is that we need to prove the number of $I_{i}$ is countable. ", "page_idx": 15}, {"type": "text", "text": "Assuming $I_{i}$ is uncountable, we perform n bisection on $[a,b]$ to obtain numbers of smaller intervals like $\\textstyle{\\bar{[}a+{\\bar{(}{\\frac{1}{2}})}^{n},a+{\\bar{(}{\\frac{1}{2}})}^{n+1}{\\]}}$ . If $I_{i}$ is not countable, then no matter how large $n$ is, we can always find two points $x$ and $y$ in the interval that satisfy $h^{\\prime}(x)>\\delta,h^{\\prime}(y)>\\delta$ (here we mainly consider greater cases, the smaller case is the same), then ", "page_idx": 15}, {"type": "equation", "text": "$$\n|x-y|\\delta\\leq|h(x)-h(y)|\\leq|h(x)|+|h(y)|\\leq2\\epsilon,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and this equation does not hold as $\\epsilon\\rightarrow0$ , which makes contradiction. Therefore, we can rewrite $I$ as $\\textstyle\\bigcup_{i=1}^{\\infty}I_{i}$ . To simplify, assume that $I$ can be written in the form of a finite number of unions $\\begin{array}{r}{I=\\bigcup_{i=1}^{N}I_{i}}\\end{array}$ . If the remark does not hold, we have max $m(I_{i})\\,=\\,k$ , then $N k\\ge\\epsilon_{2}$ . Obviously, $|\\delta k|\\leq2\\epsilon$ , then $k\\leq|\\frac{2\\epsilon}{\\delta}|$ . Finally, we have ", "page_idx": 15}, {"type": "equation", "text": "$$\nN|\\frac{2\\epsilon}{\\delta}|\\geq N k\\geq\\epsilon_{2},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "which makes contradiction as $\\epsilon\\to0$ . If the number of $I_{i}$ is infinite, we can always find $N$ that is big enough to satisfy $N k\\geq\\epsilon_{2}$ and the rest is the same. ", "page_idx": 15}, {"type": "text", "text": "Discussion on Application. As shown in the analysis above, for some given $\\delta$ and $\\epsilon_{2}$ , we only require that $|\\epsilon|<|x-y|\\delta$ and $\\begin{array}{r}{|\\epsilon|<|\\frac{\\epsilon_{2}\\delta}{2N}|}\\end{array}$ . Even though the real boundary is highly related to the specific scenarios and is difficult to tell in real applications, we can have a look at some widely used examples. Here, we mainly discuss two kinds of optimizers that are widely used in neural network training and they are SGD [51] and Adam [52]. ", "page_idx": 15}, {"type": "text", "text": "For the loss function $\\mathcal{L}$ to be set as MSE. Note that we use $X$ and $X_{t e s t}$ to represent the input data in the training set and test set, separately. $y$ is the training label and $y_{i,l}$ is the value of a time step in it. We use t to represent the training epoch and there will be T epochs total. Our goal is to use $g(X_{t e s t},y_{i,l})$ to approximate $f(X_{t e s t},\\theta_{T},y_{i,l})$ first and then use \u2202g(Xtest,yi,l) to approximate $\\frac{\\partial f(X_{t e s t},\\theta_{T},y_{i,l})}{\\partial y_{i,l}}$ ", "page_idx": 15}, {"type": "text", "text": "During the training process of a forecasting model, we let $h_{t}(y_{i,l})$ represent \u2202L(f(X,\u03b8t,yi,l),y) and $\\frac{\\partial^{2}h_{t}\\big(y_{i,l}\\big)}{\\partial y_{i,l}^{2}}$ will be zero. Recalling that our goal is to approximate the gradient $\\frac{\\partial f(X_{t e s t},\\theta_{T},y_{i,l})}{\\partial y_{i,l}}$ by approximating $f(X_{t e s t},\\theta_{T},y_{i,l})$ first and then take the gradient. However, the function that we really want to approximate is \u2202\u2202y\u03b8iT,l since $\\begin{array}{r}{\\frac{\\partial f(X_{t e s t},\\theta_{T},y_{i,l})}{\\partial y_{i,l}}=\\frac{\\partial f(X_{t e s t},\\theta_{T},y_{i,l})}{\\partial\\theta_{T}}\\frac{\\partial\\theta_{T}}{\\partial y_{i,l}}}\\end{array}$ and $\\frac{\\partial f(X_{t e s t},\\theta_{T},y_{i,l})}{\\partial\\theta_{T}}$ is a constant for given $X_{t e s t}$ . ", "page_idx": 15}, {"type": "text", "text": "For SGD (as well as its variants SGD with momentum), $\\begin{array}{r}{\\theta_{T}=\\theta_{0}-\\sum_{t=1}^{T}{\\eta h_{t}\\left(y_{i,l}\\right)}}\\end{array}$ . Therefore, the \u2202\u22022y\u03b82T = 0, which means that the gradient of the function we want to approximate is constant. In this case, our approximation will be pretty good. ", "page_idx": 15}, {"type": "text", "text": "For Adam, on the one hand, [35] has claimed that the first-order approximation in the SGD situation remains valid, as long as a small step-size $\\eta$ is used in the update. On the other hand, let $\\theta_{T}\\,=$ $\\begin{array}{r}{\\theta_{0}-\\sum_{t=1}^{T}\\eta v_{t}(h_{t}\\Bar{(y_{i,l})})}\\end{array}$ , where $v_{t}$ represent the terms in Adam. In this situation, \u2202\u03b8T will be an algebraic function with only a finite number of monotonic intervals. Therefore, for any given $\\delta$ and $\\epsilon$ , the $\\epsilon_{2}$ will not be really high since the $y_{i,l}$ that makes $\\begin{array}{r}{||\\frac{\\partial f(X_{t e s t},\\theta_{T},y_{i,l})}{\\partial y_{i,l}}-\\frac{\\partial g(X_{t e s t},\\bar{y_{i,l}})}{\\partial y_{i,l}}||>\\delta}\\end{array}$ can only appear near the local extreme point, whose number is finite. ", "page_idx": 15}, {"type": "text", "text": "D Implementation Details ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Table 5 summarizes the dataset partitioning we used. Except for the GEF data, the rest are multivariate datasets. We forecast the \u2018OT\u2019 sequences in ETH1 and ETH2, as well as the combined electricity ", "page_idx": 15}, {"type": "text", "text": "consumption of all users in ELECTRITY, the average occupancy rate of all roads in TRAFFIC, and the temperature series in the AIR dataset. ", "page_idx": 16}, {"type": "text", "text": "D.1 Datasets for forecasting ", "text_level": 1, "page_idx": 16}, {"type": "table", "img_path": "n2dvAKKQoM/tmp/2256eae41ee45812da0d3538cbd1e5b48950627914eededf011b1549c1be2169.jpg", "table_caption": ["Table 5: Datasets used in the forecasting task. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "D.2 Implementation of time series forecasting model ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We include two models in our experiment. The first one is a 3-layer MLP in which the input size and output size are both 24 while the hidden size is 128. In addition, we mainly apply the simple and high-performance DLiner with default setting in [47] as our forecasting model backbone. In addition, to adapt to situations where the input and output dimensions are different, we constructed an output layer at the end of the DLiner model, mapping the output of multiple variables to the output of a specified dimension. We use the torch.SGD optimizer [53] to optimize the parameters of the model, where the learning rate is set to 0.1. The maximum epochs for each training are 300, and the patience is set to 10. ", "page_idx": 16}, {"type": "text", "text": "D.3 Implementation of time series imputation model ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We have introduced a total of five imputation methods for comparison, and all experiments were based on pyPOTS [45] except for SPIN. The hyperparameters for each method are set as shown in Table 6. In addition, referring to [13], we set the total training epoch to 100 and the patience to 10, while other hyperparameters are the default setting. ", "page_idx": 16}, {"type": "table", "img_path": "n2dvAKKQoM/tmp/390c25dd74d72b5111a6bc2786a6165985a2a52e434cecfeb1a61f7b4364f9b0.jpg", "table_caption": ["Table 6: Hyperparameters of the time series imputation. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "D.4 Implementation of our estimation ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In section 2.2, we gives the estimation of $I(i,l)$ as follows, ", "page_idx": 16}, {"type": "equation", "text": "$$\n-\\frac{1}{n}\\frac{\\partial^{2}\\mathcal{L}\\left(f\\left(X_{i},\\theta\\right),y_{i}\\right)^{T}}{\\partial f\\left(X_{i},\\theta_{t}\\right)\\partial y_{i,l}}\\frac{\\partial\\mathcal{L}\\left(f\\left(X_{k}^{v},\\theta\\right),y_{k}^{v}\\right)}{\\partial f\\left(X_{k}^{v},\\theta\\right)}\\underbrace{\\frac{\\partial f\\left(X_{i},\\theta\\right)}{\\partial\\theta}\\frac{\\partial f\\left(X_{k}^{v},\\theta\\right)^{T}}{\\partial\\theta}}_{N T K k e r n e l}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "However, depending on the solution to the optimization problem (7), we may have different forms of estimation for $I(i,l)$ . Referring to [36, 35], when we no longer only consider the downstream model parameters $\\pmb{\\theta}$ at the moment of training convergence but also consider the entire training process, we can obtain another form of solution to the optimization problem that $\\begin{array}{r}{\\hat{\\alpha_{i,l}}\\,=\\,-\\sum_{t\\in[T]:i\\in B^{(t)}}\\frac{\\eta^{(t)}}{\\left|B^{(t)}\\right|}\\frac{\\partial^{2}\\mathcal{L}\\left(f^{(t-1)}(\\mathbf{X}_{i},\\theta),y_{i}\\right)}{\\partial f^{(t-1)}(\\mathbf{X}_{i},\\theta)\\partial y_{i,l}}}\\end{array}$ , where $t$ , $B^{(t)}$ , and $\\eta^{(t)}$ here represent the $t$ th epoch, the batch size, and the learning rate, respectively. And the $I(i,l)$ in this situation will be ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\,-\\,\\displaystyle\\sum_{t\\in[T]:i\\in B^{(t)}}\\frac{\\eta^{(t)}}{|B^{(t)}|}\\frac{\\partial^{2}\\mathcal{L}\\left(f^{(t-1)}\\left(X_{i},\\theta\\right),y_{i}\\right)^{T}}{\\partial f^{(t-1)}\\left(X_{i},\\theta_{t}\\right)\\partial y_{i,l}}\\frac{\\partial\\mathcal{L}\\left(f(X_{k}^{v},\\theta),y_{k}^{v}\\right)}{\\partial f(X_{k}^{v},\\theta)}\\,\\frac{\\partial f^{(t-1)}\\left(X_{i},\\theta\\right)^{\\top}}{\\partial\\theta}\\frac{\\partial f(X_{k}^{v},\\theta)}{\\partial\\theta}}\\\\ &{\\,=\\displaystyle-\\sum_{t\\in[T]:i\\in B^{(t)}}\\frac{\\eta^{(t)}}{|B^{(t)}|}\\frac{\\partial^{2}\\mathcal{L}\\left(f^{(t-1)}\\left(X_{i},\\theta\\right),y_{i}\\right)}{\\partial f^{(t-1)}\\left(X_{i},\\theta_{t}\\right)\\partial y_{i,l}}\\,\\frac{\\partial f^{(t-1)}\\left(X_{i},\\theta\\right)^{\\top}}{\\partial\\theta}\\frac{\\partial\\mathcal{L}\\left(f\\left(X_{k}^{v},\\theta\\right),y_{k}^{v}\\right)}{\\partial\\theta}\\Bigg\\vert_{\\theta=\\theta^{(t)}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Compared to the original estimation, repeated calculations will bring a significant computational burden. However, based on our acceleration method, the time required for such multiple calculations is still controlled within a reasonable range. In application, we calculate for each parameter updates in each epoch. ", "page_idx": 17}, {"type": "text", "text": "D.5 Implementation of Influence Function ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In section 3.3.1, we compared the performance of our method with the Influence Function that we modified. Below, we will describe how to modify the Influence Function to fit our task. ", "page_idx": 17}, {"type": "text", "text": "For a training point $(X,y)$ , define $\\begin{array}{r l r}{\\pmb{y}_{l,\\delta}}&{{}\\stackrel{\\mathrm{def}}{=}}&{[y_{1},\\cdots,y_{l}\\,+\\,\\delta,\\cdots\\,,y_{L_{2}}]}\\end{array}$ . Consider the perturbation $\\textit{\\textbf{y}}\\mapsto\\textit{\\textbf{y}}_{\\!l,\\delta}$ and let $\\begin{array}{r l}{\\pmb{\\theta}_{\\epsilon,\\delta}}&{\\stackrel{\\mathrm{def}}{=}}&{\\arg\\operatorname*{min}_{\\theta\\in\\Theta}\\frac{1}{n}\\sum_{i=1}^{n}L\\left(f(\\pmb{X}_{i},\\pmb{\\theta}),\\pmb{y}_{i}\\right)+\\epsilon L\\left(f(\\pmb{X},\\pmb{\\theta}),\\pmb{y}_{l,\\delta}\\right)\\,-}\\end{array}$ $\\epsilon L\\left(f(\\pmb{X},\\pmb{\\theta}),\\pmb{y}_{l,\\delta}\\right)$ . Then we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{d\\hat{\\pmb{\\theta}}_{\\epsilon,\\delta}}{d\\epsilon}\\bigg|_{\\epsilon=0}=-H_{\\pmb{\\theta}}^{-1}\\left(\\nabla_{\\pmb{\\theta}}L(f(\\pmb{X},\\pmb{\\theta}),\\pmb{y})\\right)-\\nabla_{\\pmb{\\theta}}L(f(\\pmb{X},\\pmb{\\theta}),\\pmb{y}))}\\\\ &{\\qquad\\qquad\\approx-H_{\\pmb{\\theta}}^{-1}\\left[\\nabla_{y_{l}}\\nabla_{\\pmb{\\theta}}L(f(\\pmb{X},\\pmb{\\theta}),\\pmb{y})\\right]\\delta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Therefore, $I(l,X_{t e s t})\\,=\\,-\\nabla_{\\theta}L\\left(f(X_{t e s t},\\theta),y_{t e s t}\\right)^{\\top}H_{\\hat{\\theta}}^{-1}\\left[\\nabla_{y_{l}}\\nabla_{\\theta}L(f(X,\\hat{\\theta}),y)\\right]\\delta$ . Note that we applied the Conjugate gradients mentioned in [34] to accelerate its computation and compare it with our methods. ", "page_idx": 17}, {"type": "text", "text": "D.6 Hareware usage ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We use 1 NVIDIA GTX 4090 GPU with 24GB of memory for all our experiments. ", "page_idx": 17}, {"type": "text", "text": "E Potential Social Impact ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Our estimation may not be $100\\%$ accurate compared to the actual situation, so it is possible to introduce bias in the evaluation among different imputation strategies, which may further have adverse effects on downstream tasks. ", "page_idx": 17}, {"type": "text", "text": "F Supplementary Experimental Results ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "F.1 Acceleration method ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "F.1.1 Performance of the acceleration method ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In our practice, we mainly examine the benefits of modifying each time step on downstream tasks. Therefore, we mainly focus on whether the gain estimation is positive or negative without providing precise values. Based on this idea, we provide methods for accelerating calculations in Section 2.3. Here, we present a comparison between the accelerated estimate and the original estimate. Note that we conduct this experiment on three datasets and they are GEF, ELECTRICITY, and a generated time series, denoted by Brown, based on the following Python code. ", "page_idx": 17}, {"type": "text", "text": "from fbm import FBM ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "$\\texttt{f}=$ FBM $\\scriptstyle\\mathbf{n}=2281$ , hurst $=\\!0.75$ , length $=\\!1$ , method $\\equiv\\cdot$ \u2019daviesharte\u2019) ", "page_idx": 18}, {"type": "text", "text": "The result is shown in the Figure 3. Note that here we also use correlation and accuracy mentioned in the main text. ", "page_idx": 18}, {"type": "image", "img_path": "n2dvAKKQoM/tmp/54b36fe834cac911f33a10896210d008345cdf9ef8c88fc41194d9c356ab65a5.jpg", "img_caption": [], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "Figure 3: The correlation and accuracy comparison between the estimation of our original method and the acceleration method. ", "page_idx": 18}, {"type": "table", "img_path": "n2dvAKKQoM/tmp/30665e8b2cc2ebd0bb18bde814f49c07d45ef4e5acb8c6ad99bc5b93ef00d896.jpg", "table_caption": ["F.1.2 MSE comparison on downstream forecasting task ", "Table 7: MSE\u2193in the downstream forecasting task. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "Table 7 summarizes the performance of our acceleration methods Seg-4 and Seg-2 in downstream forecasting tasks. Overall, performance gradually improves as the number of segmented segments ", "page_idx": 18}, {"type": "text", "text": "gradually increases. In addition, even if it is only divided into two segments, our method can still bring some gain with minimal additional computational burden. ", "page_idx": 19}, {"type": "text", "text": "F.1.3 A larger dataset ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "For large-scale data, we applied our method to a 15-minute resolution UCI electricity dataset (with approximately 100000 training points) and we adjusted our experimental setup to input 96 points and output 96 points, and here is the result. ", "page_idx": 19}, {"type": "table", "img_path": "n2dvAKKQoM/tmp/1a61d63b48b752c8cd144b48c9de2ce2338fabdeece52565d756776dacffe046.jpg", "table_caption": ["Table 8: MSE \u2193comparison on larger dataset. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "F.2 Additional missing rate ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In addition to the $40\\%$ missing rate in the main experiment, we also conduct several experiments in the ELECTRICITY dataset with missing rates in $\\bar{[30\\%}$ , $50\\%$ , $60\\%]$ . ", "page_idx": 19}, {"type": "table", "img_path": "n2dvAKKQoM/tmp/d1218f0b6427e423d885519d8c7b58407dba971138b833c7921c6156568735b4.jpg", "table_caption": ["Table 9: MSE \u2193comparison on different missing rate. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "F.3 Combination with robust time series forecasting ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In addition to our solution, some kinds of methods, such as robust time series forecasting, to deal with missing (anomaly) values have been proposed these days. Here we combine our method with [16], which is one of the SOTA of such kind of method to illustrate that this kind of method is not contradictory to our approach but can be combined. Note that the hyperparameters are the same as the original paper in [16] and we replace the dataset with ours. ", "page_idx": 19}, {"type": "text", "text": "Table 10: MSE \u2193comparison on the ELECTRICITY dataset combining our method and RobustTSF. ", "page_idx": 19}, {"type": "table", "img_path": "n2dvAKKQoM/tmp/197202b5976b8dc777bb21c2d2ca0e506cb37b676534bf19ecce1abbed763377.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "F.4 Illustration on multivariate forecasting ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We conduct our method on multivariate forecasting tasks and give a small example of the ELECTRICITY dataset. Note that we apply our method to the first three users (columns) in the dataset. ", "page_idx": 19}, {"type": "table", "img_path": "n2dvAKKQoM/tmp/18e6cceef8db3f0c510b4be0b1e7253c3305410bb4fffbb582be7bfea5a221cb.jpg", "table_caption": ["Table 11: MSE \u2193comparison on multivariate forecasting. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "F.5 Forecasting results ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Figures 4, 5, 6, 7, 8, and 9, show the visualization of the forecasting performance of the model. We mainly compared the combination of the SAITS method and the baseline model with the original SATIS method, and it can be seen that combining the two imputations will bring benefits to the forecasting. ", "page_idx": 19}, {"type": "image", "img_path": "n2dvAKKQoM/tmp/bd3d045a6b94dd8c796f642cf61550096d750d1685057cf27870e83f2eea1b08.jpg", "img_caption": ["Figure 4: Visualization of forecasting result on AIR "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "n2dvAKKQoM/tmp/23045426d9271d7159801254598d1cae0788203ec47ec79cc78a113e1afc72d4.jpg", "img_caption": ["Figure 5: Visualization of forecasting result on ELECTRICITY "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "n2dvAKKQoM/tmp/35e29acd16072a084b5ed8aaede63af39f4ddf3e6c0378f3414cb508827f3dba.jpg", "img_caption": ["Figure 6: Visualization of forecasting result on Traffic "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "n2dvAKKQoM/tmp/d5e6f3009e16fd4a2887b424023351ef5b16721f7b079d9eb0f7aaa92c3bc893.jpg", "img_caption": ["Figure 7: Visualization of forecasting result on GEF "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "n2dvAKKQoM/tmp/acd65e0d11a0f8d7f67158748fe4cf25bcfb447e36ebedc2c61830f7be5f1e1e.jpg", "img_caption": ["Figure 8: Visualization of forecasting result on ETTH1 "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "n2dvAKKQoM/tmp/55f64ad4ed1dbc6c96bd85f9fb1909280f84e2fa24984d1151f835c63191823f.jpg", "img_caption": ["Figure 9: Visualization of forecasting result on ETTH2 "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We have summarized our contributions both in the abstract section and at the end of the introduction. The claims made are supported by the results in the paper\u2019s experiment section. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 22}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We cover some limitations of this work in our conclusion section (the part left for future work). ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 22}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: See Section 2 and Appendix C. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 23}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We provide details about our experiment settings in the experiment section to ensure reproducibility as well as the link of the code. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 23}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We have provide the public accessible link to both the code and dataset we used for experiment. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 24}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We cover all these information in our experiment section. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 24}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] , ", "page_idx": 24}, {"type": "text", "text": "Justification: Every experiment is done 3 times. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] , ", "page_idx": 25}, {"type": "text", "text": "Justification: We cover these information in our appendix sections. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 25}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We confirm that our research adheres to the NeurIPS Code of Ethics. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 25}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: The societal impacts of time series forecasting are provided in the Introduction section. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 25}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 26}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 26}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We have properly cited all the models used for the experiment and ensured the license and terms of use are explicitly mentioned and properly respected. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: This paper does not release new assets.. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 27}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 27}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}]