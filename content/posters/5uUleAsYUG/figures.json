[{"figure_path": "5uUleAsYUG/figures/figures_1_1.jpg", "caption": "Figure 1: Our two-stage method for co-speech gesture generation with selective state space models. In the first stage, we construct discrete motion spaces to learn specific motion codes. In the second stage, we develop a speech-driven model of the latent space using selective scanning mechanisms.", "description": "This figure illustrates the two-stage process of the proposed MambaTalk model. Stage 1 involves learning discrete motion priors using VQ-VAEs to generate a codebook of holistic gestures. Stage 2 uses a speech-driven state space model with selective scanning (local and global) to refine the latent space representation and generate final co-speech gestures.  The figure visually shows the input and output of each stage using stylized figures.", "section": "1 Introduction"}, {"figure_path": "5uUleAsYUG/figures/figures_3_1.jpg", "caption": "Figure 2: We propose a two-stage method for co-speech gesture generation. We first train multiple VQ-VAEs for face and different parts of body reconstruction. This step learns discrete motion priors through multiple codebooks. In the second stage, we train a speech-driven gesture generation model in the latent motion space with local and global scan modules.", "description": "This figure illustrates the proposed two-stage method for co-speech gesture generation.  The first stage uses multiple Vector Quantized Variational Autoencoders (VQ-VAEs) to learn discrete motion priors for different body parts (face, upper body, lower body, hands). These priors are represented as codebooks. The second stage employs a speech-driven model using selective state space models (SSMs). This stage incorporates local and global scan modules to refine latent space representations of the movements, improving gesture quality and synchronicity with speech. The model receives input from audio, text, and speaker embeddings, and generates 3D co-speech gestures.", "section": "3 Method"}, {"figure_path": "5uUleAsYUG/figures/figures_14_1.jpg", "caption": "Figure 4: Visualization of the facial motions generated by CaMN, EMAGE and our method. Unreasonable results are indicated by red and gray boxes and reasonable ones by green boxes.", "description": "This figure compares the facial motion generation results of three methods: CaMN, EMAGE, and the proposed MambaTalk.  It visually demonstrates the alignment between generated facial expressions and phonetic articulation in speech.  The results show that MambaTalk's approach better captures the subtle movements and nuances of mouth closure and opening required for different sounds, aligning closely with the ground truth. In contrast, the other two methods show less accuracy in aligning facial movements with the phonetic content and often lack variations in facial expressions, especially in silent segments.", "section": "A.1 More visualization results"}, {"figure_path": "5uUleAsYUG/figures/figures_15_1.jpg", "caption": "Figure 5: Visualization of the gestures generated by CaMN, EMAGE and our method. Unreasonable results are indicated by red boxes and reasonable ones by green boxes.", "description": "This figure compares the gesture generation results of three different methods: CaMN, EMAGE, and the proposed method (MambaTalk). It visually demonstrates how each method generates gestures for specific phrases, highlighting the differences in realism and accuracy.  The red boxes indicate instances where the generated gestures deviate significantly from the expected natural movements, showing limitations in the other two methods.  The green boxes highlight instances where the generated gestures closely align with the ground truth. This figure is crucial to illustrate the quality and naturalness of the proposed MambaTalk method compared to existing state-of-the-art approaches.", "section": "4.5 Qualitative Analysis"}, {"figure_path": "5uUleAsYUG/figures/figures_19_1.jpg", "caption": "Figure 2: We propose a two-stage method for co-speech gesture generation. We first train multiple VQ-VAEs for face and different parts of body reconstruction. This step learns discrete motion priors through multiple codebooks. In the second stage, we train a speech-driven gesture generation model in the latent motion space with local and global scan modules.", "description": "This figure illustrates the two-stage process of the proposed MambaTalk model.  The first stage involves training multiple Vector Quantized-Variational Autoencoders (VQ-VAEs) to learn discrete motion priors for different body parts (face, hands, upper body, lower body).  The second stage uses these learned priors in a speech-driven model, incorporating local and global scan modules to refine latent space representations for more natural and diverse gesture synthesis. The figure shows the data flow in each stage, highlighting the various components and their interconnections.", "section": "3.1 Preliminaries"}]