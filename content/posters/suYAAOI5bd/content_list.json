[{"type": "text", "text": "On the Expressive Power of Tree-Structured Probabilistic Circuits ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Han Zhao ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Department of Computer Science University of Illinois Urbana-Champaign langyin2@illinois.edu ", "page_idx": 0}, {"type": "text", "text": "Department of Computer Science University of Illinois Urbana-Champaign hanzhao@illinois.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Probabilistic circuits (PCs) have emerged as a powerful framework to compactly represent probability distributions for efficient and exact probabilistic inference. It has been shown that PCs with a general directed acyclic graph (DAG) structure can be understood as a mixture of exponentially (in its height) many components, each of which is a product distribution over univariate marginals. However, existing structure learning algorithms for PCs often generate tree-structured circuits or use tree-structured circuits as intermediate steps to compress them into DAGstructured circuits. This leads to the intriguing question of whether there exists an exponential gap between DAGs and trees for the PC structure. In this paper, we provide a negative answer to this conjecture by proving that, for $n$ variables, there exists a quasi-polynomial upper bound $n^{O(\\log n)}$ on the size of an equivalent tree computing the same probability distribution. On the other hand, we also show that given a depth restriction on the tree, there is a super-polynomial separation between tree and DAG-structured PCs. Our work takes an important step towards understanding the expressive power of tree-structured PCs, and our techniques may be of independent interest in the study of structure learning algorithms for PCs. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Probabilistic circuits (PCs) [7, 29], also commonly known as sum product networks (SPNs) [24], are a type of deep graphical model that allow exact probabilistic inference efficiently in linear time with respect to the size of the circuit. Like other deep models, the parameters of a PC can be learned from data samples [38]. Because of these desirable properties, they have been increasingly applied in various contexts, including generative modeling [35], image processing [2, 34], robotics [30], planning [25] and sequential data including both textual and audio signals [6, 23]. Compared with deep neural networks, the sum and product nodes in PCs admit clear probabilistic interpretation [36] of marginalization and context-specific statistical independence [3], which opens the venue of designing efficient parameter learning algorithms for PCs, including the expectation-maximization (EM) algorithm [12], the convex-concave procedure (CCCP) [38], and the variational EM algorithm [37]. ", "page_idx": 0}, {"type": "text", "text": "Perhaps one of the most important properties of PCs is that they can be understood as a mixture of exponentially (in its height) many components, each of which is a product distribution over univariate marginals [38]. Intuitively, each sum node in PC can be viewed as a hidden variable that encodes a mixture model [36] and thus a hierarchy of sum nodes corresponds to an exponential number of components. This probabilistic interpretation of PCs has led to a number of interesting structure learning algorithms [1, 9, 13, 17, 22, 27]. However, almost all of the existing structure learning algorithms for PCs output tree-structured circuits, or use tree-structured circuits as intermediates to compress them into DAG-structured circuits. Because of the restricted structure of trees, such algorithms do not fully exploit the expressive power of PCs with general DAG structures, and often output tree-structured PCs with exceedingly large sizes [13]. Yet, from a theoretical perspective, it remains open whether there truly exists an exponential gap between DAGs and trees for the PC structure. Being able to answer this question is important for understanding the expressive power of tree-structured PCs, and may also lead to new insights in structure learning algorithms for PCs. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "1.1 Our Contributions ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In this work we attempt to answer the question above by leveraging recent results in complexity theory [11, 26, 33]. Our contributions are two-folds: an upper and lower bound for the gap between tree and DAG-structured PCs. In what follows we will first briefly state our main results and then introduce the necessary concepts and tools to tackle this problem. ", "page_idx": 1}, {"type": "text", "text": "An Upper Bound In Section 3, inspired by earlier works in Valiant et al. [33] and Raz and Yehudayoff [26], we show that, for a network polynomial that can be computed efficiently with a DAG-structured PC, there always exists a tree-structured PC of a quasi-polynomial size to represent it. An informal version of our main result for this part is stated below. ", "page_idx": 1}, {"type": "text", "text": "Theorem 1.1 (Informal). Given a network polynomial of n variables, if this polynomial can be computed efficiently by a PC of size $\\mathrm{poly}(n)$ , then there exists an equivalent tree-structured PC of depth $O(\\log n)$ and of size $n^{O(\\log n)}$ that computes the same network polynomial. ", "page_idx": 1}, {"type": "text", "text": "We prove this result by adapting the proof in Raz and Yehudayoff [26]. Our construction involves two phases: Phase one applies the notion of partial derivatives for general arithmetic circuits to represent intermediate network polynomials alternatively, and construct another DAG-structured PC using those alternative representations; we will provide fine-grained analysis on the new DAG, such that its depth is $O(\\log n)$ and its size is still $\\mathrm{poly}(n)$ . Phase two applies the standard duplicating strategy for all nodes with more than one parent to convert the new DAG into a tree. This strategy will lead to an exponential blowup for an arbitrary DAG with depth $D$ , since the size of the constructed tree will be $\\bar{n^{O(D)}}$ . However, note that the DAG constructed in the first phase has depth $O(\\log n)$ . Combining it with the duplicating strategy, we will be able to construct an equivalent tree-structured PC of size upper bounded by $\\bar{n}^{O(\\log{n})}$ , as desired. ", "page_idx": 1}, {"type": "text", "text": "The original algorithm in Raz and Yehudayoff [26] only reduces the depth to $O(\\log^{2}n)$ due to their restriction on the graph of using nodes with at most two children. This restriction is not necessary for PCs, and by avoiding it, we show that the depth can be further reduced to ${\\cal O}(\\log n)$ with a slight modification of the original proof in Raz and Yehudayoff [26]. ", "page_idx": 1}, {"type": "text", "text": "A Lower Bound In Section 4, we show that under a restriction on the depth of the trees, there exists a network polynomial that can be computed efficiently with a DAG-structured PC, but if a tree-structured PC computes it, then the tree must have a super-polynomial size. The following informal theorem states our main result for this part, which will be formally addressed in Section 4. ", "page_idx": 1}, {"type": "text", "text": "Theorem 1.2 (Informal). Given n random variables, there exists a network polynomial on those $n$ variables, such that it can be efficiently computed by a PC of size $O(n\\log n)$ and depth $O(\\log n)$ , but any tree-structured PC with depth $o(\\log n)$ computing this polynomial must have size at least $\\boldsymbol{n}^{\\omega(1)}$ . ", "page_idx": 1}, {"type": "text", "text": "Our result is obtained by finding a reduction to Fournier et al. [11]. We first fix an integer $k$ and a network polynomial of degree $\\bar{n}=2^{2k}$ . To show that the polynomial is not intrinsically difficult to represent, i.e., the minimum circuit representing it shall be efficient, we explicitly construct a PC of depth $O(\\log n)$ and size $O(n\\log{n})$ . Next, suppose via a black box, we have a minimum tree-structured PC of depth $o(\\log n)$ computing this polynomial. After removing some leaves from that minimum tree but maintaining its depth, we recover a regular arithmetic tree that computes a network polynomial of degree ${\\sqrt{n}}\\,{\\stackrel{\\_}{=}{2}}^{k}$ . Moreover, as shown in [11], if an arithmetic tree with depth $o(\\log n)$ computes this low-degree polynomial, then the size of the tree must be at least $n^{\\omega(1)}$ . Our operations on the minimum tree PC must reduce its size; therefore, the original tree must have a larger size than $n^{\\omega(1)}$ , and this fact concludes our proof. ", "page_idx": 1}, {"type": "text", "text": "1.2 More Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "There is an extensive literature on expressive efficiency of network structures for PCs and probabilistic generating circuits (PGCs) [35], another probabilistic graphic model. Very recently, it was shown in ", "page_idx": 1}, {"type": "text", "text": "Broadrick et al. [4] that PCs with negative weights are as expressive as PGCs. The investigation on PCs has started as early as in Delalleau and Bengio [8] and later in Martens and Medabalimi [19]. In neural networks and variants, this topic, along with the relationship between expressive efficiency and depth/width, has attracted many interests as well [10, 16, 18, 20, 21, 28, 31, 32]. In particular, Martens and Medabalimi [19, Theorem 34] has shown that there exists a network polynomial with a super-polynomial minimum tree expression, but it is unclear whether the same polynomial can be computed by a polynomial-sized DAG. Our work provides a positive answer to this question. For arbitrary network polynomials, finding a minimum DAG-structured PC is reducible to a special case of the minimum circuit size problem for arithmetic circuits, which remains to be a longstanding open problem in circuit complexity. ", "page_idx": 2}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We first introduce the setup of probabilistic circuits and relevant notation used in this paper. ", "page_idx": 2}, {"type": "text", "text": "Notation A rooted directed acyclic (DAG) graph consists a set of nodes and directed edges. For such an edge $u\\rightarrow v$ , we say that $u$ is a parent of $v$ , and $v$ is a child of $u$ . We use $\\operatorname{Ch}(u)$ to denote the set of children of the node $u$ . We say there is a directed path from $a$ to $b$ if there is an edge $a\\rightarrow b$ or there are nodes $u_{1},\\cdots,u_{k}$ and edges $a\\rightarrow u_{1}\\rightarrow\\cdot\\cdot\\cdot\\rightarrow u_{k}\\rightarrow b$ ; in this case, we say that $a$ is an ancestor of $b$ and that $b$ is a descendant of $a$ . If two vertices $v$ and $w$ are connected via a directed path, we call the number of edges in a shortest path between them as the distance between them, denoted by $\\mathrm{dist}(v,w)$ . A directed graph is rooted if one and only one of its nodes has no incoming edges. A leaf in a DAG is a node without outgoing edges. A cycle in a directed graph is a directed path from a node to itself, and a directed graph without directed cycles is a DAG. For two disjoint sets $A$ and $B$ , we will denote their disjoint union by $A\\sqcup B$ to emphasize their disjointedness. ", "page_idx": 2}, {"type": "text", "text": "Clearly, each directed graph has an underlying undirected graph obtained by removing arrows on all edges. Although by definition, a DAG cannot have a directed cycle, but its underlying undirected graph may have an undirected cycle. If the underlying undirected graph of a DAG is also acyclic, then that DAG is called a directed tree. Every node in a directed tree has at most one parent. If two nodes share a parent, one is said to be a sibling of the other. ", "page_idx": 2}, {"type": "text", "text": "Complexity Classes In what follows we introduce the necessary complexity classes that will be used throughout the paper. Let $f(n)$ be the runtime of an algorithm with input size $n$ . ", "page_idx": 2}, {"type": "text", "text": "\u2022 A function $f(n)$ is in the polynomial class $\\mathrm{poly}(n)$ if $f(n)\\in O(n^{k})$ for a constant $k\\in\\mathbb{N}$ .   \n\u2022 A function $f(n)$ is in the super-polynomial class if $f(n)$ is not asymptotically bounded above by any polynomial. Formally, this requires $f(n)\\in\\omega(n^{c})$ for any constant $c>0$ , i.e. $\\begin{array}{r}{\\operatorname*{lim}_{n\\to\\infty}\\frac{f(n)}{n^{c}}=\\infty}\\end{array}$ for any $c>0$ .   \n\u2022 A function $f(n)$ is in the quasi-polynomial class if it can be expressed in the form $2^{\\mathrm{poly}(\\log n)}$ .   \n\u2022 A function $f(n)$ is in the exponential class if it can be expressed in the form $2^{O(\\mathrm{poly}(n))}$ . ", "page_idx": 2}, {"type": "text", "text": "Probabilistic Circuits A probabilistic circuit (PC) is a probabilistic model based on a rooted DAG. Without loss of generality, in our work, we focus on PCs over Boolean random variables. We first introduce the notion of network polynomial. For each Boolean variable $X$ , we use the corresponding lower case alphabet $x$ to denote the indicator of $X$ , which is either 0 or 1; for the same variable, $\\textstyle{\\bar{x}}$ represents the negation. In many cases, we use $1:N$ to denote the index set $[N]$ . A PC over Boolean variables $\\{X_{1},\\cdot\\cdot\\cdot,X_{n}\\}$ computes a polynomial over the set of indicators $\\{{\\dot{x}}_{1},\\cdot\\cdot\\cdot x_{n},{\\bar{x}}_{1},\\cdot\\cdot\\cdot\\ ,{\\bar{x}}_{n}\\}$ ; we will refer this polynomial as the network polynomial. In the network, the leaves are indicators of variables, and all other nodes are either sum or product nodes; a node that is not a leaf may also be called an internal node. Each internal node computes a polynomial already: a sum node computes a weighted sum of the polynomials computed by its children, and a product node computes the product of the polynomials computed by its children. A PC is said to be normalized if the weights of the outgoing edges from a sum node sum to one. It was shown in Zhao et al. [36] that, every unnormalized PC can be transformed into an equivalent normalized PC within linear time. ", "page_idx": 2}, {"type": "text", "text": "To represent valid probability distributions, a PC must satisfy two structural properties: decomposability and smoothness. To define them, we need to define the scope of a node, which is the set of variables whose indicators are descendants of that node. For a node $v$ , if the indicator $x$ of the variable $X$ is one of descendants of $v$ , then $X\\in\\operatorname{scope}(v)$ ; more generally, $\\operatorname{scope}(v)=\\cup_{v^{\\prime}\\in\\operatorname{Ch}(v)}\\operatorname{scope}(v^{\\prime})$ . ", "page_idx": 2}, {"type": "image", "img_path": "suYAAOI5bd/tmp/db85513977c888943c35615e6da8c6daf9695afbd1d20fadb25399de87427018.jpg", "img_caption": ["Figure 1: Partial derivatives of sum nodes. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Definition 2.1 (Decomposability and Smoothness). A PC is decomposable if and only if for every product node v and any pair of its children $v_{1}$ and $v_{2}$ , we have $\\mathrm{scope}(v_{1})\\cap\\mathrm{scope}(v_{2})=\\emptyset$ . A PC is smooth if and only if for each sum node, all of its children have the same scope. ", "page_idx": 3}, {"type": "text", "text": "In this paper we restrict our attention to PCs that are both decomposable and smooth, since otherwise we can always transform a PC into an equivalent one that is both decomposable and smooth in quadratic time [36]. The degree of a monomial is the sum of the exponents of all its variables, and the degree of a polynomial $f$ , denoted by $\\deg(f)$ is the highest degree among its constituent monomials. A polynomial is said to be homogeneous if all of its monomials have the same degree. A PC is said to be homogeneous if all of its sum and product nodes compute a homogeneous polynomial. Later, we will show that decomposability and smoothness imply homogeneity, and vice versa with mild conditions. For a node $v$ in a PC, we use $\\deg(v)$ to denote $\\operatorname{\\bar{d}e g}(f_{v})$ . As emphasized earlier, this paper investigates the quantitative relationship between a DAG and a tree, which are both PCs and represent the same probability distribution. To make the terminology uncluttered, we will call the former a $D A G$ -structured $P C$ , and the latter a tree $P C$ . If a tree PC computes the same network polynomial as a DAG-structured PC, then the tree PC is said to be an equivalent tree $P C$ with respect to that DAG-structured PC. ", "page_idx": 3}, {"type": "text", "text": "Unless specified otherwise, we will use $\\Phi$ to denote the entire PC in consideration and $f$ the network polynomial computed by the root. For each node $v$ , the sub-network rooted at $v$ is denoted by $\\Phi_{v}$ and the polynomial computed by $v$ is $f_{v}$ . The set of variables in $f_{v}$ is $X_{v}$ , which is a subset of $\\{X_{1},\\cdot\\cdot\\cdot,\\dot{X}_{n}\\}$ . The size of the network $\\Phi$ , denoted by $|\\Phi|$ , is the number of nodes and edges in the network. The depth of $\\Phi$ , denoted by $D(\\Phi)$ , is its maximum length of a directed path. ", "page_idx": 3}, {"type": "text", "text": "Partial Derivatives In the process of proving the upper bound, a key notion named partial derivative is frequently used and formally defined below. ", "page_idx": 3}, {"type": "text", "text": "Definition 2.2 (Partial Derivative). For two nodes v and w in a network $\\Phi$ , the partial derivative of the polynomial $f_{v}$ with respect to the node $w$ , denoted by $\\partial_{w}f_{v}$ , is constructed by the following steps: ", "page_idx": 3}, {"type": "text", "text": "1. Substitute the polynomial $f_{w}$ by a new variable $y$ .   \n2. Compute the polynomial computed by $v$ with the variable $y$ ; denote the new polynomial by $\\bar{f}_{v}$ . Due to decomposability, $\\bar{f}_{v}$ is linear in $y$ .   \n3. Define the partial derivative $\\begin{array}{r}{\\dot{\\partial}_{w}f_{v}=\\frac{\\partial\\bar{f}_{v}}{\\partial y}}\\end{array}$ ", "page_idx": 3}, {"type": "text", "text": "Observe that the chain rule in calculus also holds for our notion here, and therefore leads to the two following facts. ", "page_idx": 3}, {"type": "text", "text": "\u2022 Let $v$ be a sum node with children $v_{1}$ and $v_{2}$ , and the edges have weight $a_{1}$ and $a_{2}$ , respectively, then by definition $f_{v}\\,=\\,a_{1}f_{v_{1}}\\,+a_{2}f_{v_{2}}$ . For any other node $w$ , the partial derivative is $\\partial_{w}f_{v}=a_{1}\\cdot\\partial_{w}f_{v_{1}}+a_{2}\\cdot\\partial_{w}f_{v_{2}}$ .   \n\u2022 Similarly, let $v$ be a product node with children $v_{1}$ and $v_{2}$ , then $f_{v}=f_{v_{1}}\\cdot f_{v_{2}}$ . For any other node $w$ , we have $\\partial_{w}f_{v}=f_{v_{1}}\\cdot\\partial_{w}f_{v_{2}}+f_{v_{2}}\\cdot\\partial_{w}f_{v_{1}}$ . ", "page_idx": 3}, {"type": "text", "text": "The partial derivative has been proven to be a powerful tool in the field of complexity theory and combinatorial geometry [14, 15]. Readers are welcome to refer Chen et al. [5] for more details and extensive background. An illustration is provided in Figure 1. ", "page_idx": 4}, {"type": "text", "text": "Arithmetic Circuits An arithmetic circuit, aka algebraic circuit, is a generalization of a probabilistic circuit. Such a circuit shares the same structure as a PC and also computes a network polynomial. If an arithmetic/algebraic circuit is a directed tree, then we call it an arithmetic/algebraic formula. In the proof of the lower bound, the notion of monotonicity of a formula is essential, whose definition relies on the concept parse tree. ", "page_idx": 4}, {"type": "text", "text": "Definition 2.3 (Parse Tree). A parse tree of a formula $\\Phi$ is a sub-formula of $\\Phi$ which corresponds to a monomial of $f$ , the network polynomial computed by $\\Phi$ . Parse trees of $\\Phi$ is defined inductively by the following process: ", "page_idx": 4}, {"type": "text", "text": "\u2022 If the root of $\\Phi$ is a sum node, a parse tree of $\\Phi$ is obtained by taking a parse tree of one of its children together with the edge between the root and that child. \u2022 If the root of $\\Phi$ is a product node, a parse tree of $\\Phi$ is obtained by taking a parse tree of each of its children together with all outgoing edges from the root. \u2022 The only parse tree of a leaf is itself. ", "page_idx": 4}, {"type": "text", "text": "Definition 2.4 (Monotonicity). An algebraic formula is monotone if the monomial computed by any of its parse trees has a non-negative coefficient in the network polynomial. ", "page_idx": 4}, {"type": "text", "text": "3 A Universal Upper Bound ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we present our first main result, which provides a universal upper bound on the size of an equivalent tree versus a DAG-structured PC. ", "page_idx": 4}, {"type": "text", "text": "Theorem 3.1. For any given $D A G$ -structured PC over n variables and of size $\\mathrm{poly}(n)$ , there exists an equivalent tree-structured $P C$ of size $n^{O(\\log n)}$ nodes and of depth ${\\cal O}(\\log n)$ , computing the same polynomial. ", "page_idx": 4}, {"type": "text", "text": "As discussed earlier, our constructive proof heavily relies on deeper properties of partial derivatives, and applying them to represent sub-network polynomials. Our strategy, inspired by Raz and Yehudayoff [26], will be efficient if the circuit being considered is a binary circuit, i.e. every node has at most two children. While such structure is rare for natural networks, we make the following observation, that an arbitrary PC can always be transformed to a binary one with a polynomial increase in size and depth. The proof and an illustrating figure will appear in Appendix A. ", "page_idx": 4}, {"type": "text", "text": "Lemma 3.2. Given a DAG-structured $P C\\,\\Phi$ , we may transform it into another $D A G\\;\\Phi^{\\prime}$ that computes the same network polynomial and every node in $\\Phi^{\\prime}$ has at most two children. Moreover, the differences between the sizes and depths of $\\Phi^{\\prime}$ and $\\Phi$ are only in polynomial size. ", "page_idx": 4}, {"type": "text", "text": "Therefore, for the remaining discussion in this section, we will assume without loss of generality, that a given PC is binary. During the conversion process towards a binary circuit, some intermediate nodes may be created to ensure no sum node is connected to another sum node and no product node is connected to another product node. The set of those intermediate nodes is denoted by $\\Phi_{1}$ , and will be present in our later discussions. Next, we present a key property of partial derivatives, which holds for any (including non-binary) PC. ", "page_idx": 4}, {"type": "text", "text": "Lemma 3.3. Given a $P C\\ \\Phi$ , if v and w are two nodes in $\\Phi$ such that $\\partial_{w}f_{v}\\neq0_{:}$ , then $\\partial_{w}f_{v}$ is $a$ homogeneous polynomial over the set of variables $X_{v}\\setminus X_{w}$ of degree $\\deg(v)-\\deg(w)$ . ", "page_idx": 4}, {"type": "text", "text": "The next lemma tells that, given a product node, its partial derivative with another node with a restriction on degree can be expressed using its children. ", "page_idx": 4}, {"type": "text", "text": "Lemma 3.4. Let v be a product node and w be any other node in a $P C\\;\\Phi$ , and $\\deg(v)<2\\deg(w)$ .   \nThe children of $v$ are $v_{1}$ and $v_{2}$ such that $\\deg(v_{1})\\geq\\deg(v_{2})$ . Then $\\partial_{w}f_{v}=f_{v_{2}}\\cdot\\partial_{w}f_{v_{1}}$ . ", "page_idx": 4}, {"type": "text", "text": "To construct the quasi-polynomial tree, the key is to compress many nodes with partial derivatives. Fundamentally, we will use the following results to show that such compression works because each node, and each partial derivative of any node with any other, can be more concisely represented using partial derivatives. The key question is to find eligible nodes, so that taking partial derivatives with respect to them will lead to compact expressions. Inspired by the observation in Raz and Yehudayoff [26], we define the following set ${\\bf G}_{m}$ , which will be critical in our later process. ", "page_idx": 4}, {"type": "text", "text": "Data: The original DAG-structured PC $\\Phi$ with $n$ variables of size $\\mathrm{poly}(n)$ , and the set of its nodes $\\nu$ ", "page_idx": 5}, {"type": "text", "text": "Result: Another DAG-structured PC $\\Psi$ of size poly $(n)$ and depth $O(\\log n)$   \n$i\\gets0$ ; $\\mathcal{V}\\gets\\emptyset$ ; $\\mathcal{P}\\leftarrow\\emptyset$ .   \nfor $i=0,1$ , $\\lceil\\log n\\rceil-1$ do Fix $m_{1}\\gets2^{i}$ ; Find all nodes $v$ such that $2^{i}<\\deg(v)\\leq2^{i+1}$ , and place them in $\\nu$ ; Find all pairs of nodes $(u,w)$ such that $2^{i}<\\deg(u)-\\deg(w)\\leq2^{i+1}$ and $\\deg(u)^{-}<2\\deg(w)$ , and place them in $\\mathcal{P}$ ; for every $v\\in\\mathcal{V}$ do Find all nodes in ${\\bf G}_{m_{1}}$ and compute $f_{v}$ using Equation 15; end for every pair of nodes $(u,w)\\in\\mathcal{P}$ do Fix $m_{2,(u,w)}\\gets2^{i}+\\deg(w)$ ; Find all nodes in ${\\bf G}_{m_{2,(u,w)}}$ and compute $\\partial_{w}f_{v}$ using Equation 17; end $\\mathcal{V}\\leftarrow\\emptyset;\\mathcal{P}\\leftarrow\\emptyset.$   \nend ", "page_idx": 5}, {"type": "text", "text": "Definition 3.5. Given a $P C\\Phi$ and an integer $m\\in\\mathbb{N}$ , the set $\\mathbf{G}_{m}$ is the collection of product nodes $t$ in $\\Phi$ with children $t_{1}$ and $t_{2}$ such that $m<\\deg(t)$ and max $\\{\\mathrm{deg}(t_{1}),\\mathrm{deg}(t_{2})\\}\\leq m$ . ", "page_idx": 5}, {"type": "text", "text": "With this set, we may choose a set of nodes as variables for partial derivatives for any node in a PC, and the following two lemmas respectively illustrate: 1) the compact expression of the sub-network polynomial $f_{v}$ for any node $v$ in a PC; 2) the compact expression of $\\partial_{w}f_{v}$ given two nodes $v$ and $w$ with a degree restriction. It is easy to observe that $\\Phi_{1}\\cap\\mathbf{G}_{m}=\\emptyset$ . ", "page_idx": 5}, {"type": "text", "text": "We now present two key lemmas that will be central to the proof for the upper bound. Specifically, they spell out the alternative representations for the network polynomial of any node, and the partial derivative of any pair of nodes. ", "page_idx": 5}, {"type": "text", "text": "Lemma 3.6 ([26]). Let $m\\in\\mathbb{N}$ and a node v such that $m<\\deg(v)\\leq2m,$ , then $\\begin{array}{r}{f_{v}=\\sum_{t\\in{\\bf G}_{m}}f_{t}}\\end{array}$ \u00b7 $\\partial_{t}f_{v}$ . ", "page_idx": 5}, {"type": "text", "text": "Lemma 3.7 ([26]). Let $m\\in\\mathbb{N},$ , and $v$ and w be two nodes such that $\\deg(w)\\,\\leq\\,m\\,<\\,\\deg(v)\\,<$ $2\\deg(w)$ , then $\\begin{array}{r}{\\partial_{w}f_{v}=\\sum_{t\\in{\\bf G}_{m}}\\partial_{w}f_{t}\\cdot\\partial_{t}f_{v}}\\end{array}$ . ", "page_idx": 5}, {"type": "text", "text": "3.1 Construction of $\\Psi$ , another DAG-structured PC with restriction on depth ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Given a binary DAG-structured PC $\\Phi$ with $n$ variables and $\\mathrm{poly}(n)$ nodes, we explicitly construct a tree PC with size $n^{O(\\log n)}$ and depth $O(\\log n)$ . Specifically, the construction takes two main steps: ", "page_idx": 5}, {"type": "text", "text": "1. Transform $\\Phi$ to another DAG-structured PC $\\Psi$ with size $\\mathrm{poly}(n)$ and depth $O(\\log n)$ . 2. Apply a simple duplicating strategy to further convert $\\Psi$ to a tree with size $n^{O(\\log n)}$ and the same depth of $\\Psi$ . ", "page_idx": 5}, {"type": "text", "text": "We will later show that step two can be simply done. Step one, however, needs much more careful operations. Each iteration, starting from $i=0$ , again needs two steps: ", "page_idx": 5}, {"type": "text", "text": "1. Compute $f_{v}$ for each node $v$ such that $2^{i-1}<\\deg(v)\\leq2^{i}$ using the compact expression illustrated earlier. We will show that, computing one such polynomial adds $\\mathrm{poly}(n)$ nodes and increases the depth by at most two on $\\Psi$ . This new node representing $f_{v}$ will be a node in $\\Psi$ , denoted by $v^{\\prime}$ .   \n2. Compute all partial derivatives $\\partial_{w}f_{u}$ for two non-variable nodes $u$ and $w$ in $\\Phi$ , such that $u$ is an ancestor of $w$ and $2^{i-1}<\\deg(u)-\\deg(w)\\leq2^{i}$ and $\\deg(u)<2\\deg(w)$ . Like those new nodes representing sub-network polynomials from $\\Phi$ , this new node representing a partial derivative will also be a node in $\\Psi$ , denoted by $(u,w)$ . We will show that computing a partial derivative with respect to each pair adds poly $(n)$ nodes and increases the depth by at most two on $\\Psi$ .   \nData: A rooted DAG of size $S$ and depth $D$ , and the set of its nodes $\\nu$   \nResult: A tree of size $O(S^{D})$ and depth $D$   \nfor every node v in $\\mathcal{V}$ do if $\\mathrm{InDeg}(v)>1$ then Duplicate the tree rooted at $v$ for $\\mathrm{InDeg}(v)-1$ times; Construct an outgoing edge from each parent of $v$ to itself. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "The process is summarized in Algorithm 1. Before presenting the construction, we first confirm the quantitative information of $\\Psi$ , the output of the algorithm. The first observation is the number of iterations: The degree of the root of $\\Phi$ is $n$ , so at most $\\log n$ iterations are needed for the entire process. Each iteration only increases the size of the updated circuit by $\\mathrm{poly}(n)$ and the depth by a constant number. Consequently, the final form of $\\Psi$ has size $\\mathrm{poly}(n)$ and depth ${\\cal O}(\\log n)$ . ", "page_idx": 6}, {"type": "text", "text": "We now provide an inductive construction of $\\Psi$ starting from $i=0$ . After each step, it is necessary to verify the validity of the updated $\\Psi$ . Although decomposability is easy to verify, smoothness is less straightforward. To tackle this, we argue that the final state of $\\Psi$ is homogeneous, i.e. every node in $\\Psi$ computes a homogeneous polynomial, and consequently $\\Psi$ is smooth due to the following lemma. ", "page_idx": 6}, {"type": "text", "text": "Lemma 3.8. If a decomposable PC contains n variables and computes a polynomial of degree $n$ , then it is homogeneous if and only if it is smooth. ", "page_idx": 6}, {"type": "text", "text": "Iteration zero ${\\mathit{\\omega}}^{\\prime}i=0$ ): During this iteration, for the first step, we only need to consider nodes $v$ such that $0.5<\\deg(v)\\leq1$ ; the degree of any node must be an integer, so we must have $\\deg(v)=1$ , i.e. $v$ represents an affine polynomial. Without loss of generality, we may assume all such affine nodes are sum nodes with strictly more than one child. Indeed, if a product node represents an affine polynomial, then it must only have exactly one child, which must be a leaf node; in this case, we may remove this product node and connect that leaf to the parents of the original product node. Similarly, if a sum node represents an affine polynomial and has exactly one child, then that child must also be a leaf node, hence we may again remove the sum node and connect that leaf to the parents of the original sum node. Due to smoothness, such an affine node $v$ must represent a polynomial in the form $a x+(1-a){\\bar{x}}$ , where $x$ is the indicator of a variable, and $0<a<1$ . Therefore, the depth of each sub-network $\\Phi_{v}$ is only one. By duplicating all such affine nodes onto $\\Psi$ , we add at most poly $(n)$ nodes and increase the depth by one only. ", "page_idx": 6}, {"type": "text", "text": "Next, for step two, we only need to consider pairs of nodes $(u,w)$ such that $\\deg(u)-\\deg(w)\\leq1$ . Thanks to Lemma 3.3, we know that $\\partial_{w}f_{u}$ is affine. For each pair satisfying the restriction, we create a sum node $(u,w)$ whose sub-network $\\Phi_{(u,w)}$ has size three and depth one. By moving all such sub-networks to $\\Psi$ for each eligible pair, we again add at most $\\mathrm{poly}(n)$ nodes and increase the depth by one to $\\Psi$ . ", "page_idx": 6}, {"type": "text", "text": "Iteration $i+1$ : Suppose, after all previous iterations, we have already computed all sub-network polynomials $f_{v}$ for nodes $v$ such that $\\deg(v)\\leq2^{i}$ , and all partial derivatives $\\partial_{w}f_{u}$ for pairs of nodes $(u,w)$ such that $\\deg(u)-\\deg(w)\\,\\leq\\,2^{i}$ and $\\deg(u)\\,\\leq\\,2\\deg(w)$ . Like the base case, step $i+1$ takes two steps: The first step computes $f_{v}$ for eligible nodes, and the second step concerns partial derivatives for eligible pairs of nodes. Because the analysis of the two steps during this iteration is highly involved, we will discussion the construction in details in Appendix A.7. ", "page_idx": 6}, {"type": "text", "text": "3.2 Construction of the Quasi-polynomial Tree ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We conclude the proof of Theorem 3.1 in this section by transforming the newly constructed $\\Psi$ into a quasi-polynomial tree. The transformation is a simple application of the naive duplication strategy, which will be illustrated below. In summary, given a $\\mathrm{poly}(n)$ -sized DAG, the size of the transformed tree directly depends on the depth of the original DAG. The process of the duplication is briefly summarized in Algorithm 2, and the detailed process of the entire transformation from the original $\\Phi$ to the final tree is described in Algorithm 5. ", "page_idx": 6}, {"type": "text", "text": "Duplication Strategy Given a DAG-structured PC of size $V$ and depth $D$ , a natural algorithm to a tree is that, if a node $v$ has $k>1$ parents, then duplicate the sub-tree rooted at $v$ for $k-1$ times, and connect each duplicated sub-tree to a parent of $v$ . Indeed this algorithm generates a tree computing the same function as the original DAG does, but in the worst case we have to duplicate the entire graph $O(V)$ times and such iterative duplication may be executed for every layer from the first to layer $D$ . Therefore, in the worst case, the final tree has size $O(V^{D})$ . ", "page_idx": 7}, {"type": "text", "text": "The construction of $\\Psi$ shows that its size is $O(n^{3})$ and depth is ${\\cal O}(\\log n)$ . Using the naive duplication, we obtain that the size of the final tree is $n^{O(\\log n)}$ . ", "page_idx": 7}, {"type": "text", "text": "4 A Conditional Lower Bound ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we present our second main result, which provides a lower bound on the tree complexity of a network polynomial given a restriction on the depth of the tree. Obtaining a lower bound for the problem of circuit complexity is in general a more difficult problem than obtaining an upper bound because one cannot achieve this goal by showing the failure of a single algorithm. Instead, one must construct a specific polynomial, and confirm that no algorithm can produce an equivalent tree of size lower than the desired lower bound. However, thanks to some recent results in circuit complexity theory, such a separation is ensured if the tree PC has a bounded depth. The main result in this section is presented below, stating that, there is a concrete network polynomial that cannot be represented by a polynomial-sized tree-structured PC if the depth of the tree is restricted. ", "page_idx": 7}, {"type": "text", "text": "Theorem 4.1. Given an integer $k\\,\\geq\\,1$ and $\\textit{n}=\\,2^{2k}$ , there exists a network polynomial $P\\ \\in$ $\\mathbb{R}[x_{1},\\cdot\\cdot\\cdot\\,,x_{n},\\bar{x}_{1},\\cdot\\cdot\\cdot\\,,\\bar{x}_{n}]$ of degree $n=2^{2k}$ , such that any probabilistic tree of depth $o(\\log n)=$ $o(k)$ computing $P$ must have size $n^{\\omega(1)}$ . ", "page_idx": 7}, {"type": "text", "text": "Note that if the polynomial $P$ is innately difficult to be represented by PCs, i.e., if it cannot even be represented efficiently by DAG-structured PCs, then separation is not shown. To show separation, $P$ should be efficiently computed by a DAG-structured PC, but any tree-structured PC representing $P$ must have a strictly larger size. Our next construction, described with more details in Algorithm 3, confirms a separation by constructing an efficient DAG-structured PC $P^{*}$ that computes $P$ . This PC has size $O(n\\,\\mathrm{{log}}\\,n)$ and depth $2k=2\\log n$ , where $k$ is the integer given in Theorem 4.1. The next proposition confirms the validity of $P^{*}$ , and the proof is in Appendix B. ", "page_idx": 7}, {"type": "text", "text": "Proposition 4.2. The tree $P C\\;P^{*}$ is decomposable and smooth. ", "page_idx": 7}, {"type": "text", "text": "It is easy to check that $P^{*}$ has the correct size and depth as described earlier. Before adding leaf nodes, the algorithm in total constructs $\\begin{array}{r}{\\sum_{r=0}^{2k}2^{r}=2^{2k+1}-1=2n-1}\\end{array}$ nodes. Finally, observe that during the construction of leaf nodes, each negation indicator is added exactly $k$ times: At a layer containing only product nodes, if a negation indicator is added to a product node $v$ at this layer, then it will next be added to the sibling of the grandparent of $v$ . Because each product node has exactly one sibling, the negation indicator for a random variable is duplicated exactly $k$ times, and finally the total size is $2\\bar{n^{-}}-1+k n=O(k n)=O(n\\log n)$ . The depth $O(k)$ is also apparent from the algorithm. We therefore conclude that $P$ can be efficiently computed by a polynomial sized tree PC for an unrestricted depth. ", "page_idx": 7}, {"type": "text", "text": "However, the efficiency would be less optimal if we restrict the depth to $o(k)$ . To show this, we design a reduction from our problem for PCs to a well-studied problem on arithmetic circuits. Our proof essentially argues that, for any minimum-sized tree-structured PC that computes $P$ , we can obtain its sub-tree that computes a polynomial, and that polynomial has been recently proven to not be able to be represented by a polynomial-sized tree-structured PC. This recent result is stated below. ", "page_idx": 7}, {"type": "text", "text": "Theorem 4.3 ([11]). Let n and $d=d(n)$ be growing parameters such that $d(n)\\leq{\\sqrt{n}}$ . Then there is a monotone algebraic formula $F$ of size at most n and depth $O(\\log d)$ computing a polynomial $Q\\ \\in\\ \\mathbb{F}[x_{1},\\cdot\\cdot\\cdot\\ ,\\bar{x}_{n}]$ of degree at most $d$ such that any monotone formula $F^{\\prime}$ of depth $o(\\log d)$ computing $Q$ must have size $n^{\\omega(1)}$ . ", "page_idx": 7}, {"type": "text", "text": "The proof of the lower bound for PCs in Theorem 4.1 is to show that, for any $\\Pi$ , a minimum treestructured PC with depth $o(k)$ that computes $P$ , the polynomial in the statement of Theorem 4.1, we can always obtain a smaller-sized arithmetic formula $\\Pi^{\\prime}$ with the same depth that computes the polynomial $Q$ in the statement of Theorem 4.3. The size of $\\Pi^{\\prime}$ is super-polynomial due to ", "page_idx": 7}, {"type": "text", "text": "Data: A positive integer $k$ , the number $2^{2k}$ , a set $\\{x_{1},\\cdot\\cdot\\cdot,x_{2^{2k}},\\bar{x}_{1},\\cdot\\cdot\\cdot,\\bar{x}_{2^{2k}}\\}$ of $2^{2k+1}$ ", "page_idx": 8}, {"type": "image", "img_path": "suYAAOI5bd/tmp/536dbace06110f049c0bcd75d240a7bf5a0b587253d5cb225eae84bb26c20ea2.jpg", "img_caption": [], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Theorem 4.3, and as a result, the size of $\\Pi$ cannot be smaller. In other words, our proof involves a reduction from the PC problem to the AC problem. Before introducing the reduction, we first present the polynomial $Q$ in the statement of Theorem 4.3. The original construction in Fournier et al. [11] is for the general class, but over here, we only present a specific case with $r=2$ , which is sufficient for our purpose. ", "page_idx": 8}, {"type": "text", "text": "The Construction of the Polynomial $Q$ We denote the polynomial $Q$ by $H^{(k,2)}$ , which is defined over $2^{2k}$ variables ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\left\\{x_{\\sigma,\\tau}:\\sigma,\\tau\\in[2]^{k}\\right\\}.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "The polynomial $H^{(k,2)}$ is recursively defined over intermediate polynomials $\\boldsymbol{H}_{u,v}$ for all $(u,v)\\in$ [2] ${\\leq}^{k}\\times[2]{\\leq}^{k}$ and $|u|=|v|$ . Specifically, if $|u|=|v|=k$ , then $H_{u,v}=x_{u,v}$ ; otherwise, $H_{u,v}=$ $\\begin{array}{r}{\\sum_{a=1}^{r}H_{u1,v a}H_{u2,v a}}\\end{array}$ . The final polynomial $H^{(k,2)}$ is defined to be $H_{\\varnothing,\\varnothing}$ . Observe that the degree of $H^{(k,2)}$ is $2^{k}$ , and it contains $2^{2^{k}-1}$ monomials. ", "page_idx": 8}, {"type": "text", "text": "Given a minimum tree-structured PC \u03a0, which computes $P$ and is of depth $o(k)$ , we remove all of its leaves that represent negation variables and call this pruned network $\\Pi^{\\prime}$ ; without leaves representing negation variables, $\\Pi^{\\prime}$ is just a standard arithmetic formula. Clearly, $|\\Pi^{\\prime}|\\ \\leq\\ |\\Pi|$ , and the next proposition reveals the polynomial computed by $\\Pi^{\\prime}$ , and its proof is in Appendix B. ", "page_idx": 8}, {"type": "text", "text": "Proposition 4.4. The arithmetic formula $\\Pi^{\\prime}$ computes $H^{(k,2)}$ . ", "page_idx": 8}, {"type": "text", "text": "Having all the necessary ingredients, we are now ready to conclude this section by proving Theorem 4.1, the main result of this section. ", "page_idx": 8}, {"type": "text", "text": "Proof of Theorem 4.1. The proof of Theorem 4.3 in Fournier et al. [11] uses the polynomial class $H^{(k,r)}$ as the hard polynomial $Q$ in the statement, in particular, with $r=2$ , $n=2^{2k}$ and $d(n)=$ ${\\sqrt{n}}=2^{k}$ . Note that the depth of $\\Pi^{\\prime}$ is $o(\\log d)=o(k)$ , and the degree of $H^{(k,2)}$ is $d=2^{k}$ , so the conditions in the statement of Theorem 4.1 are indeed satisfied. Since $\\Pi^{\\prime}$ is obtained from $\\Pi$ by removing leaves, we obtain the following inequality that concludes the proof: ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "equation", "text": "$$\n|\\Pi|\\geq|\\Pi^{\\prime}|\\geq n^{\\omega(1)}.\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper we have shown that given a network polynomial with $n$ variables that can be efficiently computed by a DAG-structured PC, we can construct a tree PC with at most quasi-polynomial size and is no deeper than $O(\\log n)$ . On the flip side, we have also shown that there indeed exists a polynomial that can be efficiently computed by a $\\mathrm{poly}(n)$ -sized PC without a depth restriction, but there is a super-polynomial separation if we restrict the depth of the tree to be $o(\\log n)$ . Our results make an important step towards understanding the expressive power of tree-structured PCs and show that a quasi-polynomial upper bound is possible. However, the lower bound is still largely open, and we have only shown a separation under a specific depth restriction. One potential direction for the future work are discussed below: although the upper bound $n^{O(\\log n)}$ is quasi-polynomial, it is still prohibitively large as $n$ grows. The construction outputs a tree of depth $O(\\log n)$ , which would be considered as a shallow tree. Is it possible to further reduce the size of the tree, possibly in the cost of a larger depth? ", "page_idx": 9}, {"type": "text", "text": "Acknowledgement ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "HZ would like to thank the support from a Google Research Scholar Award. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Tameem Adel, David Balduzzi, and Ali Ghodsi. Learning the structure of sum-product networks via an svd-based algorithm. In Conference on Uncertainty in Artificial Intelligence, 2015. URL https://api.semanticscholar.org/CorpusID:15429402.   \n[2] Mohamed R Amer and Sinisa Todorovic. Sum product networks for activity recognition. IEEE transactions on pattern analysis and machine intelligence, 38(4):800\u2013813, 2015.   \n[3] Craig Boutilier, Nir Friedman, Moises Goldszmidt, and Daphne Koller. Context-specific independence in bayesian networks. arXiv preprint arXiv:1302.3562, 2013.   \n[4] Oliver Broadrick, Honghua Zhang, and Guy Van den Broeck. Polynomial semantics of tractable probabilistic circuits. In Conference on Uncertainty in Artificial Intelligence, 2024. URL https://starai.cs.ucla.edu/papers/BroadrickUAI24.pdf.   \n[5] Xi Chen, Neeraj Kayal, and Avi Wigderson. Partial derivatives in arithmetic complexity and beyond. Found. Trends Theor. Comput. Sci., 6(1-2):1\u2013138, 2011. doi: 10.1561/0400000043. URL https://doi.org/10.1561/0400000043.   \n[6] Wei-Chen Cheng, Stanley Kok, Hoai Vu Pham, Hai Leong Chieu, and Kian Ming A Chai. Language modeling with sum-product networks. In Fifteenth Annual Conference of the International Speech Communication Association, 2014.   \n[7] Y Choi, Antonio Vergari, and Guy Van den Broeck. Probabilistic circuits: A unifying framework for tractable probabilistic models. UCLA. URL: http://starai. cs. ucla. edu/papers/ProbCirc20. pdf, 2020.   \n[8] Olivier Delalleau and Yoshua Bengio. Shallow vs. deep sum-product networks. In Proceedings of the 24th International Conference on Neural Information Processing Systems, NIPS\u201911, page 666\u2013674, Red Hook, NY, USA, 2011. Curran Associates Inc. ISBN 9781618395993.   \n[9] Aaron Dennis and Dan Ventura. Learning the architecture of sum-product networks using clustering on variables. In F. Pereira, C.J. Burges, L. Bottou, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems, volume 25. Curran Associates, ", "page_idx": 9}, {"type": "text", "text": "Inc., 2012. URL https://proceedings.neurips.cc/paper_files/paper/2012/file/ ", "page_idx": 10}, {"type": "text", "text": "f33ba15effa5c10e873bf3842afb46a6-Paper.pdf.   \n[10] Ronen Eldan and Ohad Shamir. The power of depth for feedforward neural networks. In Conference on learning theory, pages 907\u2013940. PMLR, 2016.   \n[11] Herv\u00e9 Fournier, Nutan Limaye, Guillaume Malod, Srikanth Srinivasan, and S\u00e9bastien Tavenas. Towards optimal depth-reductions for algebraic formulas. In Proceedings of the Conference on Proceedings of the 38th Computational Complexity Conference, CCC \u201923, Dagstuhl, DEU, 2023. Schloss Dagstuhl\u2013Leibniz-Zentrum fuer Informatik. ISBN 9783959772822. doi: 10. 4230/LIPIcs.CCC.2023.28. URL https://doi.org/10.4230/LIPIcs.CCC.2023.28.   \n[12] Robert Gens and Pedro Domingos. Discriminative learning of sum-product networks. Advances in Neural Information Processing Systems, 25, 2012.   \n[13] Robert Gens and Domingos Pedro. Learning the structure of sum-product networks. In International conference on machine learning, pages 873\u2013880. PMLR, 2013.   \n[14] Larry Guth and Nets Hawk Katz. Algebraic methods in discrete analogs of the kakeya problem. Advances in Mathematics, 225(5):2828\u20132839, 2010. ISSN 0001-8708. doi: https://doi.org/ 10.1016/j.aim.2010.05.015. URL https://www.sciencedirect.com/science/article/ pii/S0001870810002094.   \n[15] Haim Kaplan, Micha Sharir, and Eugenii Shustin. On lines and joints. Discrete and Computational Geometry, 44(4):838\u2013843, 2010. ISSN 0179-5376. doi: 10.1007/s00454-010-9246-3.   \n[16] Joe Kileel, Matthew Trager, and Joan Bruna. On the expressive power of deep polynomial neural networks. Curran Associates Inc., Red Hook, NY, USA, 2019.   \n[17] Sang-Woo Lee, Min-Oh Heo, and Byoung-Tak Zhang. Online incremental structure learning of sum\u2013product networks. volume 8227, pages 220\u2013227, 11 2013. ISBN 978-3-642-42041-2. doi: 10.1007/978-3-642-42042-9_28.   \n[18] Eran Malach and Shai Shalev-Shwartz. Is deeper better only when shallow is good? Curran Associates Inc., Red Hook, NY, USA, 2019.   \n[19] James Martens and Venkatesh Medabalimi. On the expressive efficiency of sum product networks. CoRR, abs/1411.7717, 2014. URL http://arxiv.org/abs/1411.7717.   \n[20] Hrushikesh Mhaskar, Qianli Liao, and Tomaso Poggio. When and why are deep networks better than shallow ones? In Proceedings of the AAAI conference on artificial intelligence, volume 31, 2017.   \n[21] Quynh Nguyen, Mahesh Chandra Mukkamala, and Matthias Hein. Neural networks should be wide enough to learn disconnected decision regions. In Jennifer Dy and Andreas Krause, editors, Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 3740\u20133749. PMLR, 10\u201315 Jul 2018. URL https://proceedings.mlr.press/v80/nguyen18b.html.   \n[22] Robert Peharz, Bernhard Geiger, and Franz Pernkopf. Greedy part-wise learning of sum-product networks. volume 8189, 09 2013. ISBN 978-3-642-38708-1. doi: 10.1007/978-3-642-40991-2_ 39.   \n[23] Robert Peharz, Georg Kapeller, Pejman Mowlaee, and Franz Pernkopf. Modeling speech with sum-product networks: Application to bandwidth extension. In 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 3699\u20133703. IEEE, 2014.   \n[24] Hoifung Poon and Pedro Domingos. Sum-product networks: A new deep architecture. In 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), pages 689\u2013690, 2011. doi: 10.1109/ICCVW.2011.6130310.   \n[25] Andrzej Pronobis, Francesco Riccio, Rajesh PN Rao, et al. Deep spatial affordance hierarchy: Spatial knowledge representation for planning in large-scale environments. In ICAPS 2017 Workshop on Planning and Robotics, pages 1\u20139, 2017.   \n[26] Ran Raz and Amir Yehudayoff. Balancing syntactically multilinear arithmetic circuits. Computational Complexity, 17:515\u2013535, 2008.   \n[27] Amirmohammad Rooshenas and Daniel Lowd. Learning sum-product networks with direct and indirect variable interactions. In Proceedings of the 31st International Conference on International Conference on Machine Learning - Volume 32, ICML\u201914, page I\u2013710\u2013I\u2013718. JMLR.org, 2014.   \n[28] Itay Safran and Ohad Shamir. Depth-width tradeoffs in approximating natural functions with neural networks. In International conference on machine learning, pages 2979\u20132987. PMLR, 2017.   \n[29] Raquel S\u00e1nchez-Cauce, Iago Par\u00eds, and Francisco Javier D\u00edez. Sum-product networks: A survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(7):3821\u20133839, 2021.   \n[30] Bruno Massoni Sguerra and Fabio G Cozman. Image classification using sum-product networks for autonomous filght of micro aerial vehicles. In 2016 5th Brazilian Conference on Intelligent Systems (BRACIS), pages 139\u2013144. IEEE, 2016.   \n[31] Shizhao Sun, Wei Chen, Liwei Wang, Xiaoguang Liu, and Tie-Yan Liu. On the depth of deep neural networks: A theoretical view. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 30, 2016.   \n[32] Matus Telgarsky. beneftis of depth in neural networks. In Vitaly Feldman, Alexander Rakhlin, and Ohad Shamir, editors, 29th Annual Conference on Learning Theory, volume 49 of Proceedings of Machine Learning Research, pages 1517\u20131539, Columbia University, New York, New York, USA, 23\u201326 Jun 2016. PMLR. URL https://proceedings.mlr.press/v49/ telgarsky16.html.   \n[33] Leslie G. Valiant, Sven Skyum, Stuart J. Berkowitz, and Charles Rackoff. Fast parallel computation of polynomials using few processors. SIAM J. Comput., 12:641\u2013644, 1983. URL https://api.semanticscholar.org/CorpusID:10197224.   \n[34] Jinghua Wang and Gang Wang. Hierarchical spatial sum\u2013product networks for action recognition in still images. IEEE Transactions on Circuits and Systems for Video Technology, 28(1): 90\u2013100, 2016.   \n[35] Honghua Zhang, Brendan Juba, and Guy Van den Broeck. Probabilistic generating circuits. In International Conference on Machine Learning, pages 12447\u201312457. PMLR, 2021.   \n[36] Han Zhao, Mazen Melibari, and Pascal Poupart. On the relationship between sum-product networks and bayesian networks. In International Conference on Machine Learning, pages 116\u2013124. PMLR, 2015.   \n[37] Han Zhao, Tameem Adel, Geoff Gordon, and Brandon Amos. Collapsed variational inference for sum-product networks. In International conference on machine learning, pages 1310\u20131318. PMLR, 2016.   \n[38] Han Zhao, Pascal Poupart, and Geoffrey J Gordon. A unified approach for learning the parameters of sum-product networks. Advances in neural information processing systems, 29, 2016. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "image", "img_path": "suYAAOI5bd/tmp/ff614721ad704a09f76e2bc1d4b7c2e92a1732410421bedc97ce1e720684a55c.jpg", "img_caption": [], "img_footnote": [], "page_idx": 12}, {"type": "text", "text": "Figure 2: The process of transforming a non-binary DAG-structured PC to a binary one that computes the identical network polynomial. We omit the edge weights for simplicity. ", "page_idx": 12}, {"type": "text", "text": "A Missing proofs in Section 3 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "In this section we provide the proofs of the lemmas and theorems that are not included in the main text. For better readability, we first restate the statements and then provide the proofs. ", "page_idx": 12}, {"type": "text", "text": "A.1 Proof of Lemma 3.2 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Given a depth- $D$ network with $V$ nodes and $E$ edges, we scan over all its nodes. If a sum node has more than two children, say $M_{1},\\cdot\\cdot\\cdot,M_{k}$ , then keep $M_{1}$ and create a product node, whose only child is an intermediate sum node. The intermediate sum node has two children: $M_{2}$ and another just created intermediate sum node. Keep going and until an intermediate sum node has $M_{k}$ as the only child. ", "page_idx": 12}, {"type": "text", "text": "The operation is the same if a product node has more than two children by just exchanging sum and product. Note that for one operation for a node with $k$ children, the depth increases by $2(k-1)$ , and $\\bar{2}(k-1)$ nodes and edges are added. Once we do the same for all nodes, the number of increased depth, nodes, and edges are upper bounded by ", "page_idx": 12}, {"type": "equation", "text": "$$\n2\\times\\left(\\sum_{N\\in V}\\mathrm{out-degree~of~node~}N\\mathrm{~if~}N\\mathrm{~has~more~than~two~children}\\right)-2V\\leq2E-2V\\in O(E).\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "In fact, for depth, this upper bound is very conservative because, for example, if a node has three children, one of its children again has three children. After we operate on both of them, the depth increases by four only. A better upper bound is $O(M)\\leq O(V)$ , where $M$ is the maximum out-degree in the original network. It is easy to check that each child of the root computes the same polynomial as before, and so does the new network. Clearly, the new network is still decomposable and smooth if the original network is. ", "page_idx": 12}, {"type": "text", "text": "A.2 Proof of Lemma 3.3 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Lemma 3.3. Given a PC $\\Phi$ , if v and w are two nodes in $\\Phi$ such that $\\partial_{w}f_{v}\\,\\ne\\,0,$ , then $\\partial_{w}f_{v}$ is $a$ homogeneous polynomial over the set of variables $X_{v}\\setminus X_{w}$ of degree $\\deg(v)-\\deg(w)$ . ", "page_idx": 12}, {"type": "text", "text": "Proof. Clearly, $\\partial_{w}f_{v}\\neq0$ implies that $w$ is a descendant of $v$ . We prove the statement by induction on $L$ , the length of the longest directed path from $v$ to $w$ . If $L=0$ , i.e. $w=v$ , then $\\partial_{w}f_{v}=1$ and the statement trivially holds. Suppose the statement is true for all $L$ and now the longest distance from $v$ to $w$ is $L+1$ . We prove the statement by discussing two cases, whether $w$ is a sum or product node. ", "page_idx": 13}, {"type": "text", "text": "Case I: $w$ is a sum node. We first assume $w$ is a sum node, and its parent inside this particular path $v\\,\\gg\\,w$ is $u$ , whose children are $w$ and $w^{\\prime}$ . We write $\\overline{{f}}_{v}$ as the polynomial if we substitute $w$ with $y$ , and $\\widehat{f_{v}}$ as the polynomial if we substitute $u$ with $y$ . Note that if we write them as functions with respect to $y$ , then $\\overline{{f}}_{v}^{\\prime}(y)=\\widehat{f}_{v}(y\\cdot f_{w^{\\prime}})$ , and hence ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\partial_{w}f_{v}=\\frac{\\partial\\overline{{f}}_{v}(y)}{\\partial y}=\\frac{\\partial\\widehat{f}_{v}(y\\cdot f_{w^{\\prime}})}{\\partial y}=\\frac{\\partial\\widehat{f}_{v}(y\\cdot f_{w^{\\prime}})}{\\partial(y\\cdot f_{w^{\\prime}})}\\cdot f_{w^{\\prime}}=\\partial_{u}f_{v}\\cdot f_{w^{\\prime}}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "By the inductive hypothesis, $\\partial_{u}f_{v}$ is a homogeneous polynomial over the set of variables $X_{v}\\setminus X_{u}$ of total degree $\\deg(v)-\\deg(u)$ , so $\\partial_{w}f_{v}$ must also be homogeneous, and its degree is $\\deg(\\partial_{u}f_{v})+$ d $\\mathrm{eg}(w^{\\prime})=\\deg(v)-\\deg(u)+\\deg(w^{\\prime})=\\deg(v)-\\deg(w)-\\deg(w^{\\prime})+\\deg(w^{\\prime})=\\deg(v)-\\deg(w),$ and it is over variables $;\\left(X_{v}\\setminus X_{u}\\right)\\cup X_{w^{\\prime}}=\\left(X_{v}\\setminus\\left(X_{w}\\cup X_{w^{\\prime}}\\right)\\right)\\cup X_{w^{\\prime}}=X_{v}\\setminus X_{w}.$ ", "page_idx": 13}, {"type": "text", "text": "Case II: $w$ is a product node. Next, assume $w$ is a product node. In this case, $u$ is a sum node and $\\deg(u)=\\deg(w)=\\deg(w^{\\prime})$ , and $X_{u}=X_{w}=X_{w^{\\prime}}$ . Let the weight of the edge $u\\rightarrow w$ be $a$ , and the weight for $u\\rightarrow w^{\\prime}$ be $b$ . Then, $\\overline{{f}}_{v}(y)=\\widehat{f}_{v}(a y+b f_{w^{\\prime}})$ , and ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\partial_{w}f_{v}=\\frac{\\partial\\overline{{f}}_{v}(y)}{\\partial y}=\\frac{\\partial\\widehat{f}_{v}(a y+b f_{w^{\\prime}})}{\\partial y}=a\\cdot\\frac{\\partial\\widehat{f}_{v}(a y+b f_{w^{\\prime}})}{\\partial(a y+b f_{w^{\\prime}}))}=a\\cdot\\partial_{u}f_{v}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Clearly, by the inductive hypothesis, both $\\partial_{u}f_{v}$ and $\\partial_{w}f_{v}$ are homogeneous, and they have the same degree and set of variables. Specifically, ${\\mathrm{deg}}(\\partial_{w}\\,f_{v})\\,=\\,{\\mathrm{deg}}(\\partial_{u}\\,f_{v})\\,=\\,{\\mathrm{deg}}(v)-{\\mathrm{deg}}(u)\\,=$ $\\deg(v)-\\deg(w)$ , and $X_{w,v}=X_{u,v}=X_{v}\\setminus X_{u}=X_{v}\\setminus X_{w}$ . \u25a0 ", "page_idx": 13}, {"type": "text", "text": "A.3 Proof of Lemma 3.4 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Lemma 3.4. Let v be a product node and w be any other node in a $P C\\;\\Phi$ , and $\\deg(v)<2\\deg(w)$ .   \nThe children of $v$ are $v_{1}$ and $v_{2}$ such that $\\deg(v_{1})\\geq\\deg(v_{2})$ . Then $\\partial_{w}f_{v}=f_{v_{2}}\\cdot\\partial_{w}f_{v_{1}}$ . ", "page_idx": 13}, {"type": "text", "text": "Proof. Clearly, $\\deg(v)\\;=\\;\\deg(v_{1})\\,+\\,\\deg(v_{2})$ . Therefore, since $\\deg(v)\\ <\\ 2\\deg(w)$ , we have $\\deg(v_{2})<\\deg(w)$ ; by Lemma 3.3, we have $\\partial_{w}f_{v_{2}}=0$ , and the conclusion follows directly because of the chain rule. ", "page_idx": 13}, {"type": "text", "text": "A.4 Proof of Lemma 3.6 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "First, observe that with such choice of $m$ , we have $\\mathbf{G}_{m}\\cap\\Phi_{v}\\neq\\emptyset$ . Write $v_{1}$ and $v_{2}$ as the children of $v$ . If $\\deg(v_{1})\\leq m$ and $\\deg(v_{2})\\leq m$ , then $v\\in\\mathbf{G}_{m}$ . Otherwise, assume without loss of generality that $\\deg(v_{1})\\geq\\deg(v_{2})$ and $\\deg(v_{1})>m$ . Keep reducing and there will be a position such that the condition of being a member in ${\\bf G}_{m}$ holds. ", "page_idx": 13}, {"type": "text", "text": "We now prove the statement by induction on $L$ , the length of the longest directed path from $v$ to $\\mathbf{G}_{m}$ , i.e. $L=\\mathbf{\\dot{\\max}}_{v^{\\prime}\\in\\mathbf{G}_{m}}\\operatorname{dist}(v,v^{\\prime})$ . If $L=0$ , then $v\\in\\mathbf{G}_{m}$ and all other nodes in $\\mathbf{G}_{m}$ (if any) are not descendants of $v$ . Therefore, if $t\\in\\mathbf{G}_{m}$ and $t\\neq v$ , we have $\\partial_{t}f_{v}=0$ . Clearly, $\\partial_{v}f_{v}=1$ , so ", "page_idx": 13}, {"type": "equation", "text": "$$\nf_{v}=f_{v}\\cdot\\underbrace{\\partial_{v}f_{v}}_{=1}+\\sum_{t\\in\\mathbf{G}_{m}:t\\neq v}f_{t}\\cdot\\underbrace{\\partial_{t}f_{v}}_{=0}=\\sum_{t\\in\\mathbf{G}_{m}}f_{t}\\cdot\\partial_{t}f_{v}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Now suppose the statement is true for all $L$ , and now the longest directed path from $v$ to ${\\bf G}_{m}$ has length $L+1$ . ", "page_idx": 13}, {"type": "text", "text": "Case I: $v$ is a sum node. First, assume $v$ is a sum node and $f_{v}=a_{1}f_{v_{1}}+a_{2}f_{v_{2}}$ . Recall that, since $v$ is a sum node, we have $m<\\deg(v_{1})=\\deg(v_{2})=\\deg(v)\\leq2m$ , so we may apply the inductive hypothesis on $v_{1}$ and $v_{2}$ . Therefore, ", "page_idx": 13}, {"type": "equation", "text": "$$\nf_{v_{1}}=\\sum_{t\\in{\\bf G}_{m}}f_{t}\\cdot\\partial_{t}f_{v_{1}};\\quad f_{v_{2}}=\\sum_{t\\in{\\bf G}_{m}}f_{t}\\cdot\\partial_{t}f_{v_{2}}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Hence, using the chain rule of the partial derivative, we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle f_{v}=a_{1}f_{v_{1}}+a_{2}f_{v_{2}}=\\sum_{t\\in{\\bf G}_{m}}a_{1}\\cdot f_{t}\\cdot\\partial_{t}f_{v_{1}}+\\sum_{t\\in{\\bf G}_{m}}a_{2}\\cdot f_{t}\\cdot\\partial_{t}f_{v_{2}}}}\\\\ {{\\displaystyle\\qquad\\qquad\\qquad=\\sum_{t\\in{\\bf G}_{m}}f_{t}\\cdot\\left(a_{1}\\cdot\\partial_{t}f_{v_{1}}+a_{2}\\cdot\\partial_{t}f_{v_{2}}\\right)=\\sum_{t\\in{\\bf G}_{m}}f_{t}\\cdot\\partial_{t}f_{v}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Case II: $v$ is a product node. Next, assume $v$ is a product node and $\\deg(v_{1})\\,\\geq\\,\\deg(v_{2})$ . If $v\\in\\mathbf{G}_{m}$ , then the statement trivially holds like the base case, so we assume $v\\not\\in{\\bf G}_{m}$ , and therefore $m<\\deg(v_{1})\\leq2m$ and the longest directed path from $v_{1}$ to ${\\bf G}_{m}$ has length $L$ , while such a path does not exist from $v_{2}$ to $\\mathbf{G}_{m}$ . So, by the inductive hypothesis, ", "page_idx": 14}, {"type": "equation", "text": "$$\nf_{v_{1}}=\\sum_{t\\in\\mathbf{G}_{m}}f_{t}\\cdot\\partial_{t}f_{v_{1}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "By definition, if $t\\in\\mathbf{G}_{m}$ , then we must have $2\\deg(t)>2m\\geq\\deg(v)$ , and by Lemma 3.4, ", "page_idx": 14}, {"type": "equation", "text": "$$\nf_{v}=f_{v_{1}}\\cdot f_{v_{2}}=\\sum_{t\\in{\\bf G}_{m}}f_{t}\\cdot(f_{v_{2}}\\cdot\\partial_{t}f_{v_{1}})=\\sum_{t\\in{\\bf G}_{m}}f_{t}\\cdot\\partial_{t}f_{v}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "A.5 Proof of Lemma 3.7 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We again write $v_{1}$ and $v_{2}$ as the children of $v$ , and again induct on $L$ , the length of the longest directed path from $v$ to ${\\bf G}_{m}$ in the network. If $L=0$ , then $v\\in\\mathbf{G}_{m}$ , and same as the previous case, every other node $t$ in ${\\bf G}_{m}$ is not a descendant of $v$ , which implies $\\partial_{t}f_{v}=0$ . So, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\partial_{w}f_{v}=\\partial_{w}f_{v}\\cdot\\underbrace{\\partial_{v}f_{v}}_{=1}+\\sum_{t\\in{\\bf G}_{m}:t\\neq v}\\partial_{w}f_{v}\\cdot\\underbrace{\\partial_{t}f_{v}}_{=0}=\\sum_{t\\in{\\bf G}_{m}}\\partial_{w}f_{t}\\cdot\\partial_{t}f_{v}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Suppose the statement is true for all $L$ , and now the longest directed path from $v$ to ${\\bf G}_{m}$ has length $L+1$ . ", "page_idx": 14}, {"type": "text", "text": "Case I: $v$ is a sum node. First, assume $v$ is a sum node and $f_{v}=a_{1}f_{v_{1}}+a_{2}f_{v_{2}}$ . Again, since $v$ is a sum node we may apply the inductive hypothesis on $v_{1}$ and $v_{2}$ : ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\partial_{w}f_{v_{1}}=\\sum_{t\\in{\\bf G}_{m}}\\partial_{w}f_{t}\\cdot\\partial_{t}f_{v_{1}};\\quad\\partial_{w}f_{v_{2}}=\\sum_{t\\in{\\bf G}_{m}}\\partial_{w}f_{t}\\cdot\\partial_{t}f_{v_{2}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Again, by the chain rule, we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\partial_{w}f_{v}=a_{1}\\partial_{w}f_{v_{1}}+a_{2}\\partial_{w}f_{v_{2}}=\\sum_{t\\in{\\bf G}_{m}}\\partial_{w}f_{t}\\cdot\\left(a_{1}\\partial_{t}f_{v_{1}}+a_{2}\\partial_{t}f_{v_{2}}\\right)=\\sum_{t\\in{\\bf G}_{m}}\\partial_{w}f_{t}\\cdot\\partial_{t}f_{v}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Case II: $v$ is a product node. Now assume $v$ is a product node and $\\deg(v_{1})\\,\\geq\\,\\deg(v_{2})$ . If $v\\in\\mathbf{G}_{m}$ , then the statement trivially holds like the base case, so we assume $v\\not\\in{\\bf G}_{m}$ , and therefore $m<\\deg(v_{1})<2\\deg(w)$ and the longest directed path from $v_{1}$ to ${\\bf G}_{m}$ has length $L$ , while such a path does not exist from $v_{2}$ to ${\\bf G}_{m}$ . So, by the inductive hypothesis, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\partial_{w}f_{v_{1}}=\\sum_{t\\in{\\bf G}_{m}}\\partial_{w}f_{t}\\cdot\\partial_{t}f_{v_{1}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Since $\\deg(v)\\,<\\,2\\deg(w)$ , and for all nodes $t\\,\\in\\,\\mathbf{G}_{m}$ , we have $2\\deg(t)>2m>\\deg(v)$ , so by applying Lemma 3.4 twice, we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\partial_{w}f_{v}=f_{v_{2}}\\cdot\\partial_{w}f_{v_{1}}=\\sum_{t\\in\\mathbf{G}_{m}}\\partial_{w}f_{t}\\cdot\\left(f_{v_{2}}\\cdot\\partial_{t}f_{v_{1}}\\right)=\\sum_{t\\in\\mathbf{G}_{m}}\\partial_{w}f_{t}\\cdot\\partial_{t}f_{v}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "image", "img_path": "suYAAOI5bd/tmp/9b9f49b1d2081b3029ac5d9b8b35f37294bddf1600e261f04db02b5c3c603862.jpg", "img_caption": ["Figure 3: The process of converting an arbitrary DAG to a DAG with depth restriction. The red nodes are those in $\\mathbf{G}_{2}$ and their relationships imply the computational procedure. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "A.6 Proof of Lemma 3.8 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Suppose the network is smooth. Recall that if the root of a probabilistic circuit contains $n$ variables, then the network computes a multi-linear polynomial of degree $n$ . If the root is a sum node, then its children must be homogeneous with degree $n$ . If the root is a product node, then its children must also be homogeneous, otherwise the product node will not be homogeneous. ", "page_idx": 15}, {"type": "text", "text": "Conversely, suppose such network is homogeneous. We prove by induction on the depth $d$ of the network. If $d=1$ and the root is a sum node, then the polynomial must be linear and therefore there can only be one variable $x$ and $\\textstyle{\\bar{x}}$ ; as a result, this simple network is smooth. Now suppose the statement is true for any $d$ , and we have a probabilistic circuit with depth $d+1$ . If the root is a product node, we are done because if any sum node had two children with different scopes, the inductive hypothesis would be violated. If the root is a sum node, then every sum node other than the root cannot have two children with different scopes, because each sum node is in the induced sub-network rooted at a grandchild of the root of depth at most $d-1$ so the inductive hypothesis must hold. So, we only need to show $X_{R}=X_{R_{1}}=\\cdot\\cdot\\cdot=X_{R_{k}}$ , where $R_{1},\\ldots,R_{k}$ are children of $R$ . Since the sub-networks rooted at $R_{1},\\cdot\\cdot\\cdot,R_{k}$ are decomposable and homogeneous, those sub-networks must be smooth by the inductive hypothesis. Hence, each $R_{i}$ computes a polynomial of degree $|X_{R_{i}}|$ . If $|X_{R_{i}}|<n$ , then the polynomial computed by $R$ is not homogeneous of degree $n$ and we obtain a contradiction. Therefore, each $R_{i}$ must contain all of $n$ variables to compute a polynomial of degree $n$ and as a result, we prove $X_{R}=X_{R_{1}}=\\cdot\\cdot=X_{R_{k}}$ and the smoothness of the entire network. ", "page_idx": 15}, {"type": "text", "text": "A.7 Construction of $\\Psi$ ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A.7.1 Step one: computing $f_{v}$ for eligible nodes ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "During iteration $i+1$ , a polynomial $f_{v}$ is in consideration if and only if $2^{i}\\,<\\,\\deg(v)\\,\\leq\\,2^{i+1}$ . Naturally we shall apply Lemma 3.6, and therefore choosing an appropriate $m$ and the corresponding ${\\bf G}_{m}$ is essential. Here we choose $m=2^{i}$ . Moreover, we define a set $T=\\mathbf{G}_{m}\\cap\\Phi_{v}$ for each $v$ being considered; for every $t\\in T$ , we use $t_{1}$ and $t_{2}$ to denote its children. By Lemma 3.6 and the definition that all nodes in ${\\bf G}_{m}$ are product nodes, we have ", "page_idx": 15}, {"type": "equation", "text": "$$\nf_{v}=\\sum_{t\\in T}f_{t}\\cdot\\partial_{t}f_{v}=\\sum_{t\\in T}f_{t_{1}}\\cdot f_{t_{2}}\\cdot\\partial_{t}f_{v}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Since $t\\in T$ , we must have max $\\{\\deg(t_{1}),\\deg(t_{2})\\}\\leq m=2^{i}$ , and therefore ", "page_idx": 15}, {"type": "equation", "text": "$$\n2^{i}=m<\\deg(t)=\\deg(t_{1})+\\deg(t_{2})\\leq2m=2^{i+1}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Therefore, $\\deg(v)-\\deg(t)<2^{i+1}-2^{i}=2^{i}$ and $\\deg(v)<2^{i}+\\deg(t)<2\\deg(t)$ . Hence, $f_{t_{1}}$ , $f_{t_{2}}$ and $\\partial_{t}f_{v}$ have all been computed already during earlier iterations. If $\\deg(v)=\\deg(t)$ , then $t$ is a child of $v$ and $\\partial_{t}f_{v}$ is the weight of the edge $v\\rightarrow t$ . Therefore, to compute such a $f_{v}$ , we need to add $|T|$ product nodes and one sum node, whose children are those $|T|$ product nodes; apparently, the depth increases by two. If a subset of the three terms $\\{f_{t_{1}},f_{t_{2}},\\partial_{t}\\boldsymbol{\\dot{f}}_{v}\\}$ is a constant, then their product will be the weight of the edge connecting the product node $f_{t_{1}}\\cdot f_{t_{2}}$ and the new sum node. ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "We now verify the validity of this updated circuit. Because $\\Phi$ is decomposable and $t$ is a product node, we conclude $X_{t_{1}}\\cap\\dot{X}_{t_{2}}=\\emptyset$ and $X_{t}=X_{t_{1}}\\sqcup X_{t_{2}}$ . By Lemma 3.3, we have $X_{t,v}=X_{v}^{\\phantom{\\mathsf{\\bar{\\alpha}}}}\\backslash X_{t}=$ $X_{v}\\setminus\\left(X_{t_{1}}\\sqcup X_{t_{2}}\\right)$ . Therefore, every summand in Equation (15) is a product node whose children are sum nodes with pairwise disjoint scopes, and thus, the updated circuit must be decomposable as well. Also, since $f_{v}$ is a homogeneous polynomial, so must be every summand for each $t$ . As a result, the updated circuit is also homogeneous. Thanks to Lemma 3.8, the updated circuit is valid. ", "page_idx": 16}, {"type": "text", "text": "A.7.2 Step two: computing $\\partial_{w}f_{u}$ for eligible pairs of nodes ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "As discussed earlier, during iteration $i+1$ , a pair of nodes $u$ and $w$ are chosen if and only if $2^{i}<\\deg(u)-\\deg(w)\\leq2^{\\bar{i}+1}$ and $\\deg(u)<2\\deg(w)$ . In this case, we fix $m=2^{i}+\\deg(w)$ , and define $T=\\mathbf{G}_{m}\\cap\\Phi_{u}$ . Clearly, $\\deg(w)<m<\\deg(u)<2\\deg(w)$ , so by Lemma 3.7, we have $\\begin{array}{r}{\\partial_{w}f_{u}=\\sum_{t\\in T}\\partial_{w}f_{t}\\cdot\\partial_{t}f_{u}}\\end{array}$ . For each $t\\in T$ , by definition $t$ must be a product node, and since $t\\in\\Phi_{u}$ , we have $\\deg(w)<\\deg(t)\\leq\\deg(u)<2\\deg(w)$ . Recall that the children of $t$ are denoted by $t_{1}$ and $t_{2}$ , and we may assume without loss of generality that $\\deg(t_{1})\\geq\\deg(t_{2})$ . Hence, by Lemma 3.4, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\partial_{w}f_{u}=\\sum_{t\\in T}f_{t_{2}}\\cdot\\partial_{w}f_{t_{1}}\\cdot\\partial_{t}f_{u}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Furthermore, we may safely assume $\\deg(w)\\leq\\deg(t_{1})$ , otherwise $w$ is not a descendant of $t_{1}$ nor $t$ and therefore $\\partial_{w}f_{t_{1}}=\\partial_{w}f_{t}=0$ . Next, by analyzing their degrees and differences in degrees, we show that for each $t$ , the terms $f_{t_{2}},\\,\\partial_{w}f_{t_{1}}$ , and $\\partial_{t}f_{u}$ in that summand have all been computed by earlier iterations or the step one during this iteration $i+1$ . ", "page_idx": 16}, {"type": "text", "text": "Term $f_{t_{2}}$ : Since ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\deg(u)\\leq\\deg(w)+2^{i+1}\\leq2^{i+1}+\\deg(t_{1})=2^{i+1}+\\deg(t)-\\deg(t_{2}),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\deg(t_{2})\\leq2^{i+1}+\\deg(t)-\\deg(u)\\leq2^{i+1}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Hence, $f_{t_{2}}$ has already been computed during the first step of this current iteration or even earlier. ", "page_idx": 16}, {"type": "text", "text": "Term $\\partial_{w}f_{t_{1}}$ : Recall that $\\deg(t_{1})\\leq m=2^{i}+\\deg(w)$ , so $\\deg(t_{1})-\\deg(w)\\,\\leq\\,2^{i}$ . Moreover, $\\deg(t_{1})\\leq\\deg(t)\\leq\\deg(u)<2\\deg(w)$ . Therefore, the pair $(t_{1},w)$ satisfies both requirements to be computed during iteration $i$ or earlier. ", "page_idx": 16}, {"type": "text", "text": "Term $\\partial_{t}f_{u}$ : Recall that $\\deg(t)>m=2^{i}+\\deg(w)$ , so ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\deg(u)-\\deg(t)<\\deg(u)-\\deg(w)-2^{i}\\leq2^{i+1}-2^{i}=2^{i},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the second inequality follows from $\\deg(u)-\\deg(w)\\leq2^{i+1}$ , the requirement of choosing $u$ and $w$ for iteration $i+1$ . Finally, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\deg(u)\\leq2^{i+1}+\\deg(w)<2\\cdot\\underbrace{(2^{i}+\\deg(w))}_{=m<\\deg(t)}<2\\deg(t).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "These two facts together ensure that $\\partial_{t}f_{u}$ must have been computed during iteration $i$ or earlier. ", "page_idx": 16}, {"type": "text", "text": "Finally, we verify the validity of the updated circuit after this step. The new objects introduced in this step are only $|T|$ product nodes whose children are $f_{t_{2}},\\partial_{w}f_{t_{1}}$ , and $\\partial_{t}f_{u}$ for each $t\\in T$ , and one sum node whose children are those $|T|$ product nodes. It is easy to see that the sets $X_{t_{2}}$ , $X_{t_{1}}\\setminus X_{w}$ and $X_{u}\\setminus X_{t}$ are pairwise disjoint since $X_{w}\\subseteq X_{t_{1}}$ and $X_{t_{1}}\\cap X_{t_{2}}=\\emptyset$ ; therefore, the updated circuit is indeed decomposable. By Lemma 3.3, all three terms in each summand are homogeneous, and therefore the new circuit is also homogeneous, and consequently, it is valid again by Lemma 3.8. ", "page_idx": 16}, {"type": "text", "text": "B Missing proofs in Section 4 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "B.1 Proof of Proposition 4.2 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Before writing the rigorous proof, we first fix some terminologies. In this proof, we refer the layer of all indicators constructed in step one as layer zero, and each set of nodes constructed in one of steps three, four, and five as one layer above. A negation indicator added in step six does not belong to any layer. Therefore, when we consider a layer of sum nodes, the negation indicator leaves whose parents are product nodes on the next layer are not in consideration. Step six augments the scope of every product node; for any product node $v$ , we use $v^{\\prime}$ to denote the same vertex before being augmented during step six. To prove this statement, we first prove an equivalent condition for $P^{*}$ to be valid, and then show $P^{*}$ satisfies the equivalent property. ", "page_idx": 17}, {"type": "text", "text": "Lemma B.1. Validity of $P^{*}$ is equivalent with the following statement: ", "page_idx": 17}, {"type": "text", "text": "In $P^{*}$ , every product node and its sibling have the same scope. If two product nodes are on the same layer but not siblings, then they have disjoint scopes. ", "page_idx": 17}, {"type": "text", "text": "Proof. Suppose the statement holds, then for any sum node, its two children are product nodes and siblings, so they have the same scope; for any product node $v$ , denote its children by $w$ and $w^{\\prime}$ , and their children by $\\{w_{1},w_{2}\\}$ and $\\{\\bar{w_{1}^{\\prime}},w_{2}^{\\prime}\\}$ , respectively. Clearly, $w_{i}$ and $w_{i}^{\\prime}$ are siblings and have the same scope for any $i\\in\\{1,2\\}$ , but if $j\\neq i$ , then $w_{i}$ and $w_{j}^{\\prime}$ have disjoint scopes. Therefore, $\\operatorname{scope}(w)=\\operatorname{scope}(w_{1})=\\operatorname{scope}(w_{2})$ and $\\mathrm{scope}(w^{\\prime})=\\mathrm{scope}(\\dot{w}_{1}^{\\prime})=\\mathrm{scope}(w_{2}^{\\prime})$ are disjoint, as desired. ", "page_idx": 17}, {"type": "text", "text": "Conversely, suppose $P^{*}$ is decomposable and smooth. For any pair of product nodes which are siblings, they share a unique parent and thus have the same scope due to smoothness. Now suppose we have two product nodes $v$ and $w$ , which are on the same layer but not siblings. We prove a useful fact: If two product nodes are on the same layer $2j+1$ for some $1\\,\\le\\,j\\,\\le\\,k$ , then $\\deg(v)=\\deg(w)=2^{\\frac{1}{2}j+2}$ . When $j=1$ , we know that initially every product node on layer one has two leaf children, so adding two negation indicators enforce that every product node on that layer has degree four. Assume the statement is true for all $j$ , and we now consider those product nodes on layer $2(j+1)-1=2j+1$ . By the inductive hypothesis, every product node on layer $2j-1$ has degree $2^{2j}$ , and therefore every sum node on layer $2j$ also has degree $2^{2j}$ . If $u$ is a product node on layer $2j+1$ with the sibling $\\boldsymbol{u}^{*}$ , we have $\\deg(\\dot{u}^{\\prime})=\\deg((u^{*})^{\\prime})\\stackrel{}{=}2^{2j+1}$ . Step six ensures that $\\deg(u)=\\deg(u^{*})=\\deg(u^{\\prime})+\\deg((u^{*})^{\\prime})=2^{2j+2}$ . ", "page_idx": 17}, {"type": "text", "text": "If they share an ancestor that is a product node, then their scopes are disjoint due to decomposability. On the other hand, suppose their only common ancestor is the root, whose children are denoted by $a_{1}$ and $a_{2}$ , then without loss of generality, we may assume that $v$ is a descendant of $a_{1}$ and $w$ is a descendant of $a_{2}$ . Because $P^{*}$ is valid, it must be homogeneous and we have $\\deg(a_{1})=\\deg(a_{2})$ . The fact we proved in the previous paragraph implies that $\\deg(a_{1}^{\\prime})=\\deg(a_{2}^{\\prime})=2^{2k-3+2}=2^{2k-1}$ . In other words, step six increases the degree of $a_{1}^{\\prime}$ and $a_{2}^{\\prime}$ by $2^{2k-1}$ each. Because the whole tree $P^{*}$ is decomposable, the increase in $\\deg(a_{1}^{\\prime})$ is exactly $2^{2k-1}=|\\operatorname{scope}(a_{2}^{\\prime})|$ , and vice versa. Due to smoothness, $\\{X_{1},\\cdot\\cdot\\cdot,X_{2^{2k}}\\}\\,=\\,\\mathrm{scope}(\\dot{a}_{1}^{\\prime})\\cup\\mathrm{scope}(a_{2}^{\\prime})$ , and thus $\\mathrm{scope}(a_{1}^{\\prime})\\cap\\mathrm{scope}(a_{2}^{\\prime})\\,=$ $\\varnothing$ . Finally, since $\\mathrm{scope}(v)\\,\\subseteq\\,\\mathrm{scope}(a_{1}^{\\prime})$ and $\\mathrm{scope}(w)\\ \\subseteq\\mathrm{scope}(a_{2}^{\\prime})$ , we must have $\\operatorname{scope}(v)\\cap$ $\\operatorname{scope}(w)=\\emptyset$ . \u25a0 ", "page_idx": 17}, {"type": "text", "text": "Now we prove Proposition 4.2 by showing that $P^{*}$ indeed satisfies the equivalent property.   \nProposition 4.2. The tree PC $P^{*}$ is decomposable and smooth. ", "page_idx": 17}, {"type": "text", "text": "Proof. Now we prove that $P^{*}$ satisfies the equivalent statement by induction on the index of the layer containing product nodes only. Using the index above, only the layers with odd indices from $\\{\\dot{2}i-1\\}_{i=1}^{k}$ are concerned. For the base case, consider those $2^{\\bar{2}k-1}$ product nodes constructed in step two, denoted by $v_{1},\\cdots,v_{2^{2k-1}}$ . For each $1\\leq j\\leq2^{2k-1}$ , if $j$ is odd, then following steps two and six, the children of $v_{j}$ are $\\{x_{2j-1},x_{2j},\\bar{x}_{2j+1},\\bar{x}_{2j+2}\\}$ . Its only sibling is $v_{j+1}$ , whose children are $\\{x_{2j+2},x_{2j+1},\\bar{x}_{2j},\\bar{x_{2j-1}}\\}$ . Thus, $\\mathrm{scope}(\\tilde{v}_{j})=\\mathrm{{scope}}(v_{j+1})=\\{X_{2j-1},\\tilde{X}_{2j},X_{2j+1},X_{2j+2}\\}$ The argument is identical if $j$ is even. ", "page_idx": 17}, {"type": "text", "text": "On the other hand, suppose $1\\leq r<s\\leq2^{2k-1}$ and two product nodes $v_{r}$ and $v_{s}$ are not siblings, i.e.   \neither $s-r>1$ , or $s-r=1$ and $r$ is even. ", "page_idx": 17}, {"type": "text", "text": "Case I: $s\\mathrm{~-~}r\\,>\\,1$ . In this case, the set $\\mathrm{scope}(v_{r})$ is $\\{X_{2r-1},X_{2r},X_{2r+1},X_{2r+2}\\}$ if $r$ is odd, $\\{X_{2r-3},X_{2r-2},X_{2r-1},X_{2r}\\}$ if it is even; similarly, the set $\\mathrm{scope}(v_{s})$ depends on the parity of $s$ . If $s\\,-\\,r\\,=\\,2$ , then they have an identical parity. If they are both odd, then $\\operatorname{scope}(v_{s})\\;=\\;$ $\\left\\{X_{2(r+2)-1},X_{2(r+2)},X_{2(r+2)+1},X_{2(r+2)+2}\\right\\}=\\left\\{X_{2r+3},X_{2r+4},X_{2r+5},X_{2r+6}\\right\\}$ , and is disjoint with $\\operatorname{scope}(v_{r})$ . The argument is identical if they are both even. If $s-r>2$ , then the largest index among the elements in $\\operatorname{scope}(v_{r})$ is $2r+2$ , and the smallest index among the elements in $\\mathrm{scope}(v_{s})$ is $2s-3\\geq2(r+3)-3=2r+3$ ; hence, $\\mathrm{scope}(v_{r})\\cap\\mathrm{scope}(v_{s})=\\emptyset$ . ", "page_idx": 18}, {"type": "text", "text": "Case II: $s-r=1$ and $r$ is even. In this case, $\\mathrm{scope}(v_{r})=\\{X_{2r-3},X_{2r-2},X_{2r-1},X_{2r}\\}$ and $\\mathrm{scope}(v_{s})=\\{X_{2s-1},X_{2s},X_{2s+1},X_{2s+2}\\}=\\{X_{2r+1},X_{2r+2},\\dot{X}_{2r+3},X_{2r+4}\\}$ because $s=r+1$ is odd. Clearly, $\\mathrm{scope}(v_{r})\\cap\\mathrm{scope}(v_{s})=\\emptyset.$ . ", "page_idx": 18}, {"type": "text", "text": "The argument above proves the base case. Suppose the statement holds until the layer $2i-1$ for some $i<k$ , and we now consider layer $2(i+1)-1=2i+1$ , which contains $2^{2k-2i-1}$ product nodes, denoted by $v_{1},\\cdot\\cdot\\cdot\\ ,v_{2^{2k-2i-1}}$ . They must have non-leaf children, and we denote these nodes without their leaf nodes by v\u20321, \u00b7 \u00b7 \u00b7 , $v_{1}^{\\prime},\\cdot\\cdot\\cdot\\ ,v_{2^{2k}-2i-1}^{\\prime}$ v\u203222k\u22122i\u22121. By construction, the layer 2i below contains 22k\u22122i sum nodes, denoted by $w_{1},\\cdot\\cdot\\cdot\\,,w_{2^{2k-2i}}$ ; and the layer $2i-1$ contains $2^{2k-2i+1}$ product nodes, denoted by $z_{1},\\cdot\\cdot\\cdot\\ ,z_{2^{2k-2i+1}}$ .  \u00b7 F\u00b7or each $1\\leq r\\leq2^{2k-2i-1}$ ,  th\u2212e product node $v_{r}$ has children $w_{2r-1}$ and $w_{2r}$ , and is their unique parent. Similarly, $w_{2r}$ has children $z_{4r-1}$ and $z_{4r}$ and is their unique parent; $w_{2r-1}$ has children $z_{4r-3}$ and $z_{4r-2}$ , and is their unique parent. ", "page_idx": 18}, {"type": "text", "text": "We prove a simple fact that will simplify the induction step. We claim that, given two integers $r,s\\in$ $\\{1,\\dot{\\cdot}\\cdot\\cdot,2^{2k-2i^{2}-1}\\}$ and $r\\neq s$ , the scopes $\\mathrm{scope}(v_{r}^{\\prime})$ and scope $\\left(v_{s}^{\\prime}\\right)$ are disjoint. Without loss of generality, we assume $r<s$ . By construction, $\\mathrm{Ch}(v_{r}^{\\prime})=\\{w_{2r-1},w_{2r}\\}$ and $\\mathrm{Ch}(v_{s}^{\\prime})=\\{w_{2s-1},w_{2s}\\}$ ; furthermore, $\\mathrm{Ch}(w_{2r-1})=\\{z_{4r-3},z_{4r-2}\\}$ , $\\mathrm{Ch}(w_{2r})=\\{z_{4r-1},z_{4r}\\}$ , $\\operatorname{Ch}(w_{2s-1})=\\{z_{4s-3},z_{4s-2}\\}$ , $\\mathrm{Ch}(w_{2s})~=~\\{z_{4s-1},z_{4s}\\}$ . Observe that, if a pair of product nodes belong to one of the four sets above, then they are siblings and have the same scope; if they belong to distinct sets, then they are not siblings and have disjoint scopes. We know that the scope of a node is the union of the scopes of their children, so the four scopes scop $\\mathrm{e}(w_{2r-1}),\\,\\mathrm{scope}(w_{2r}),\\,\\mathrm{scope}(w_{2s-1})$ , and $\\mathrm{scope}(w_{2s})$ are pairwise disjoint. As a result, the scopes $\\mathrm{scope}(v_{r}^{\\prime})=\\mathrm{scope}(w_{2r-1})\\sqcup\\mathrm{scope}(w_{2r})$ and $\\mathrm{scope}(v_{s}^{\\prime})=\\mathrm{scope}(w_{2s-1})\\sqcup\\mathrm{scope}(w_{2s})$ are disjoint. ", "page_idx": 18}, {"type": "text", "text": "Now we prove the induction step. In the first case, suppose $v_{r}$ and $v_{r+1}$ are sibling, i.e. $r$ is odd so $v_{r+1}$ is the only sibling of $v_{r}$ . We have shown that $\\mathrm{scope}(v_{r}^{\\prime})\\cap\\mathrm{scope}(v_{r+1}^{\\prime})=\\emptyset$ . However, step six enforces that $\\operatorname{scope}(v_{r})=\\operatorname{scope}(v_{r}^{\\prime})\\sqcup\\operatorname{scope}(v_{r+1}^{\\prime})=\\operatorname{scope}(v_{r+1})$ , as desired. ", "page_idx": 18}, {"type": "text", "text": "Next, suppose $1\\leq r<s\\leq2^{2k-2i-1}$ and $v_{r}$ and $v_{s}$ are not siblings. Denote the siblings of $v_{r}$ and $v_{s}$ by $v_{r^{\\prime}}$ and $v_{s^{\\prime}}$ , respectively; by definition, $r^{\\prime}\\in\\{r-1,r+1\\}$ and $s^{\\prime}\\in\\{s-1,s+1\\}$ , depending on the parity of $r$ and $s$ . Clearly, the four nodes $v_{r},v_{r^{\\prime}},v_{s},v_{s^{\\prime}}$ are distinct, and consequently the four sets $\\operatorname{scope}(v_{r})$ , $\\mathrm{scope}(v_{r^{\\prime}})$ , $\\mathrm{scope}(v_{s})$ , and $\\mathrm{scope}(v_{s^{\\prime}})$ are pairwise disjoint. Step six enforces that $\\mathrm{scope}(v_{r})=\\mathrm{scope}(v_{r}^{\\prime})\\sqcup\\mathrm{scope}(v_{r^{\\prime}}^{\\prime})$ and $\\mathrm{scope}(\\dot{v_{s}})=\\mathrm{scope}(v_{s}^{\\prime})\\sqcup\\mathrm{scope}(v_{s^{\\prime}}^{\\prime})$ , which are disjoint as desired. \u25a0 ", "page_idx": 18}, {"type": "text", "text": "B.2 Proof of Proposition 4.4 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We first realize the polynomial $P$ returned by Algorithm 3 without adding those leaves representing negation variables. Recall that layer one contains $2^{2k-1}$ product nodes, and before adding negation variables, the bottom layer (layer zero) contains $2^{2\\bar{k}}$ leaves. If for every odd integer $i\\;\\;\\stackrel{{\\displaystyle\\bullet}}{\\in}\\;\\{1,3,5,\\ldots,2^{2k}\\;-\\;1\\}$ , we denote the monomial by $f_{i,i+1}\\,=\\,x_{i}x_{i+1}$ , then without adding negation variables, the polynomial can be constructed by the following recursion with $2k+1$ steps: ", "page_idx": 18}, {"type": "text", "text": "\u2022 Construct $2^{2k-1}$ monomials $x_{1}x_{2},\\ldots,x_{2^{2k}-1}x_{2^{2k}}.$ .   \n\u2022 Sum up $2^{2k-2}$ pairs of consecutive monomials, and return $2^{2k-2}$ polynomials with two monomials $x_{1}x_{2}+x_{3}x_{4},\\ldots,x_{2^{2k}-3}x_{2^{2k}-2}+x_{2^{2k}-1}x_{2^{2k}}$ .   \n\u2022 Multiply $2^{2k-3}$ pairs of consecutive polynomials, and return $2^{2k-3}$ polynomials.   \n\u2022 Repeat the operation until only one polynomial is returned. ", "page_idx": 18}, {"type": "text", "text": "Observe that this polynomial is exactly $H^{(k,2)}$ defined in Section 4 with an alternative set of indices for the variables $([2]^{\\tilde{k}}\\times[2]^{k}$ versus $[\\dot{2}^{2k}],$ ). To prove Proposition 4.4, it is sufficient to show that for every minimum tree-structured probabilistic circuit $\\Pi$ of depth $o(k)$ that computes $P$ , the removal of those leaves representing negation variables returns an arithmetic formula that has the same depth and computes $\\bar{H}^{(k,2)}$ . To show this, we need the following lemma. ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "Lemma B.2. In any tree-structured probabilistic circuit $\\Pi$ that computes $P$ , no sum node has $a$ negation indicator as a child, and no product node has only negation indicators as its children. ", "page_idx": 19}, {"type": "text", "text": "The proof of Lemma B.2 relies on the following lemma on monotone arithmetic formulas. ", "page_idx": 19}, {"type": "text", "text": "Lemma B.3. A monotone arithmetic formula computing a homogeneous polynomial must be homogeneous. ", "page_idx": 19}, {"type": "text", "text": "Proof. If a formula is rooted at a sum node, then clearly every child of its must be homogeneous. If the root is a product node with $k$ children, then denote the polynomials computed by them as $f_{1},\\cdot\\cdot\\cdot,f_{k}$ and write $\\textstyle f=\\prod_{i=1}^{k}f_{i}$ . Furthermore, for each $i\\in[k]$ , further assume $f_{i}$ contains $q_{i}$ monomials, and write it as ", "page_idx": 19}, {"type": "equation", "text": "$$\nf_{i}=f_{i,1}+\\cdots+f_{i,q_{i}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Without loss of generality, assume $f_{1}$ is not homogeneous and $\\deg(f_{1,1})\\neq\\deg(f_{1,2})$ . Then at least two of the monomials of $f$ , namely $\\textstyle f_{1,1}\\times\\prod_{j=2}^{k}f_{j,1}$ and $\\textstyle f_{1,2}\\times\\prod_{j=2}^{k}f_{j,1}$ , must have distinct degrees and therefore destroy the homogeneity of the root. ", "page_idx": 19}, {"type": "text", "text": "Proof of Lemma B.2. First, observe that in a PC, if a sum node has a leaf as a child, then due to smoothness, it can only have two children, which are negation and non-negation indicators for a same variable. ", "page_idx": 19}, {"type": "text", "text": "Suppose $\\Pi$ does have a sum node $u$ that has a negation indicator $\\bar{x}_{i}$ as a child. Observe that, if we replace all negation indicators with the constant one, then the resulting tree is still monotone and computes $F$ , which is a homogeneous polynomial. The replacement will cause that sum node to compute exactly $x_{i}+1$ , which is not homogeneous. ", "page_idx": 19}, {"type": "text", "text": "Similarly, if a product node $v$ has only negation indicators as its children, then the replacements above force $v$ to compute one. Smoothness enforces that its siblings have the same scope as $v$ does, and without loss of generality we may assume none of its siblings computes the same polynomial as $v$ does, so their degrees are higher than one. As a result, the replacements of all negation indicators to one will force the parent of $v$ to compute a non-homogeneous polynomial, which contradicts Lemma B.3. \u25a0 ", "page_idx": 19}, {"type": "text", "text": "Now Proposition 4.4 can be confirmed, because the removal strategy will indeed produce a tree that computes $H^{(k,2)}$ , and no internal nodes will be affected, because Lemma B.2 ensures that, no internal node in $\\Pi^{\\prime}$ computes a constant one and can be removed. ", "page_idx": 19}, {"type": "text", "text": "C Pseudocodes ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Algorithm 4: Construction of $\\mathbf{G}$ ", "text_level": 1, "page_idx": 20}, {"type": "image", "img_path": "suYAAOI5bd/tmp/60147fb00fcdad9816e2c3db0ee7d9868c0a049b002591f103c0affc54be2651.jpg", "img_caption": [], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "Algorithm 5: Construction of the tree ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Data: a binary DAG-structured PC $\\Phi$ with $V$ nodes and $n$ variables   \nResult: a tree-structurd PC with size $2^{O(\\log^{2}n)}$   \n$T\\gets\\emptyset$ ; $P_{i}\\gets\\emptyset,Q_{i}\\gets\\emptyset$ , $\\mathbf{G}_{2^{i}}\\gets\\emptyset$ , $\\mathbf{G}_{2^{i},w}\\gets\\emptyset$ for $i\\in\\{1,\\cdot\\cdot\\cdot,\\log n\\}$ and $w\\in\\Phi$ ; $\\Phi_{v}\\gets\\emptyset$ for $v\\in\\Phi$ ; $m\\in\\mathbb{N}$ is not defined yet   \nOperate Algorithm 4 and return ${\\bf G}_{2^{i}}$ and $\\mathbf{G}_{2^{i},w}$ for all $i\\in\\{1,\\cdot\\cdot\\cdot,\\log n\\}$ and those $w\\in\\Phi$ that were selected for computing partial derivatives.   \nfor $i=1$ to $\\log n$ do $m\\leftarrow2^{i}$ . for $v\\in P_{i}$ do $T\\gets\\mathbf{G}_{2^{i}}\\cap\\Phi_{v}$ ; for $t\\in T$ do Create a product node $\\otimes_{t}$ computing $f_{t_{1}}\\cdot f_{t_{2}}\\cdot\\partial_{t}f_{v}$ . end Create a sum node $\\oplus_{v}$ that sums over all $\\otimes_{t}$ ; for $t\\in T$ such that $\\partial_{t}f_{v}$ is a non-zero-or-one constant, the edge $\\Phi_{v}\\rightarrow\\otimes_{t}$ has weight $\\partial_{t}f_{v}$ . end for $(v,w)\\in Q_{i}$ do $m\\gets2^{i}+\\deg(w),T\\gets\\mathbf{G}_{2^{i},w}\\cap\\Phi_{v};$ for $t\\in T$ do Create a product node $\\otimes_{t}$ computing $f_{t_{2}}\\cdot\\partial_{w}f_{t_{1}}\\cdot\\partial_{t}f_{v}$ . end Create a sum node $\\oplus_{(v,w)}$ that sums over all $\\otimes_{t}$ ; for $t\\in T$ such that $\\otimes_{t}$ contains a constant multiplier, the edge $\\oplus_{(v,w)}\\rightarrow\\otimes_{t}$ has weight of that constant. end   \nend ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Apply the naive duplication to convert the DAG into a tree. Apply Algorithm 2 in [36] to normalize the tree. ", "page_idx": 21}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 22}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 22}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 23}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: Our paper does not include experiments. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 23}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: Our paper does not include experiments. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 24}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: Our paper does not include experiments. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 24}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: Our paper does not include experiments. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: Our paper does not include experiments. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 25}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 25}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: Our paper studies a long-standing theoretical conjecture, and therefore shall not have direct societal impact. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: Our paper does not include experiments and is not relevant with any data or models. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 26}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: The license is CC-BY 4.0. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 26}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 27}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: Our paper does not release new assets. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 27}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: Our paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 27}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: Our paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 27}]