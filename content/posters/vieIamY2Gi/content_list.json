[{"type": "text", "text": "Improved off-policy training of diffusion samplers ", "text_level": 1, "page_idx": 0}, {"type": "image", "img_path": "vieIamY2Gi/tmp/e64cb5b406aaecd61b85bccf98a876b530591bc6533f63a854c2ca9738c31a1a.jpg", "img_caption": [], "img_footnote": [], "page_idx": 0}, {"type": "text", "text": "{marcin.sendera,...,nikolay.malkin}@mila.quebec ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We study the problem of training diffusion models to sample from a distribution with a given unnormalized density or energy function. We benchmark several diffusion-structured inference methods, including simulation-based variational approaches and off-policy methods (continuous generative flow networks). Our results shed light on the relative advantages of existing algorithms while bringing into question some claims from past work. We also propose a novel exploration strategy for off-policy methods, based on local search in the target space with the use of a replay buffer, and show that it improves the quality of samples on a variety of target distributions. Our code for the sampling methods and benchmarks studied is made public at (link) as a base for future work on diffusion models for amortized inference. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Approximating and sampling from complex multivariate distributions is a fundamental problem in probabilistic deep learning [e.g., 26, 34, 25, 47, 56] and in scientific applications [3, 51, 37, 1, 31]. The problem of drawing samples from a distribution given only an unnormalized probability density or energy is particularly challenging in high-dimensional spaces and when the distribution of interest has many separated modes [5]. Sampling methods based on Markov chain Monte Carlo (MCMC) \u2013 such as Metropolis-adjusted Langevin [MALA; 23, 64, 63] and Hamiltonian MC [HMC; 19, 30] \u2013 may be slow to mix between modes and have a high cost per sample. While variants such as sequential MC [SMC; 24, 12, 15] and nested sampling [68, 9, 42] have better mode coverage, their cost may grow prohibitively with the dimensionality of the problem. This motivates the use of amortized variational inference, i.e., fitting parametric models that sample the target distribution. ", "page_idx": 0}, {"type": "text", "text": "Diffusion models, continuous-time stochastic processes that gradually evolve a simple distribution to a complex target, are powerful density estimators with proven mode-mixing properties [14]; as such, they have been widely used in the setting of generative models learned from data [69, 71, 27, 49, 65]. However, the problem of training diffusion models to sample from a distribution with a given blackbox density or energy function has attracted less attention. Recent work has drawn connections between diffusion (learning the denoising process) and stochastic control (learning the F\u00f6llmer drift [20]), leading to approaches such as the path integral sampler [PIS; 87], denoising diffusion sampler [DDS; 77], and time-reversed diffusion sampler [DIS; 8]; such approaches were recently unified by [62] and [78]. Another line of work [41, 85] is based on continuous generative flow networks (GFlowNets), which are deep reinforcement learning algorithms adapted to variational inference that offer stable off-policy training and thus flexible exploration [45]. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Despite the advances in sampling methods and attempts to unify them theoretically [62, 78], the field suffers from some failures in benchmarking and reproducibility, with the works differing in the choice of model architectures, using unstated hyperparameters, and even disagreeing in their definitions of the same target densities (see $\\S B.1$ ). The first main contribution of this paper is a unified library for diffusion-structured samplers. The library has a focus on off-policy methods (continuous GFlowNets) but also includes simulation-based variational objectives such as PIS. Using this codebase, we are able to benchmark methods from past work under comparable conditions and confirm claims about exploration strategies and desirable inductive biases, while calling into question other claims on robustness and sample efficiency. Our library also includes several new modeling and training techniques, and we provide preliminary evidence of their utility in possible future work (\u00a75.3). ", "page_idx": 1}, {"type": "text", "text": "Our second contribution is a study of methods for improving exploration and credit assignment \u2013 the propagation of learning signals from the target density to the parameters of earlier sampling steps \u2013 in diffusion-structured samplers (\u00a74). First, our results (\u00a75.2) suggest that the technique of utilizing partial trajectory information [43, 54], as done in the diffusion setting by [85], offers little benefti, and a higher training cost, over on-policy [87] or off-policy [41] trajectory-based optimization. Second, we examine the utility of a gradient-based variant which parametrizes the denoising distribution as a correction to a Langevin process [87]. We show that this inductive bias is also beneficial in the offpolicy (GFlowNet) setting despite higher computational cost. Finally, motivated by recent approaches in discrete sampling, we propose an efficient exploration technique based on local search in the target space with the use of a replay buffer, which improves sample quality across various target distributions. ", "page_idx": 1}, {"type": "text", "text": "2 Prior work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Amortized variational inference approaches use a parametric model $q_{\\theta}$ to approximate a given target density $p_{\\mathrm{target}}$ , typically through stochastic optimization [29, 57, 2]. Notably, explicit density models like autoregressive models and normalizing flows have been extensively utilized in density estimation [59, 18, 80, 21, 50]. However, these models impose structural constraints, thereby limiting their expressive power [13, 22, 86]. The adoption of diffusion processes in generative models has stimulated a renewed interest in hierarchical models as density estimators [79, 27, 75]. Approaches like PIS [87] leverage stochastic optimal control for sampling from unnormalized densities, albeit still struggling with scalability in high-dimensional spaces. ", "page_idx": 1}, {"type": "text", "text": "Generative flow networks, originally defined in the discrete case by [6, 7], view hierarchical sampling (i.e., stepwise generation) as a sequential decision-making process and represent a synthesis of reinforcement learning and variational inference approaches [45, 89, 72, 17], expanding from specific scientific domains [e.g., 35, 4, 88] to amortized inference over a broader array of latent structures [e.g., 76, 33]. Their ability to efficiently navigate trajectory spaces via off-policy exploration has been crucial, yet they encounter challenges in training dynamics, such as credit assignment and exploration efficiency [44, 43, 54, 58, 67, 38, 36]. These challenges have repercussions in the scalability of these methods in more complex scenarios, which this paper addresses in the continuous case. ", "page_idx": 1}, {"type": "text", "text": "3 Setting: Diffusion-structured sampling ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Let $\\mathcal{E}:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ be a differentiable energy function and define $R(\\mathbf{x})=\\exp(-\\mathcal{E}(\\mathbf{x}))$ , the reward or unnormalized target density. Assuming the integral $\\begin{array}{r}{Z:=\\int_{\\mathbb{R}^{d}}R(\\mathbf{x})\\,d\\mathbf{x}}\\end{array}$ x exists, $\\varepsilon$ defines a Boltzmann density $p_{\\mathrm{target}}(\\mathbf{x})=R(\\mathbf{x})/Z$ on $\\mathbb{R}^{d}$ . We are interested in the problems of sampling from $p_{\\mathrm{target}}$ and approximating the partition function $Z$ given access only to $\\varepsilon$ and possibly to its gradient $\\nabla\\delta$ . ", "page_idx": 1}, {"type": "text", "text": "We describe two closely related perspectives on this problem: via neural SDEs and stochastic control (\u00a73.1) and via continuous generative flow networks (\u00a73.2). ", "page_idx": 1}, {"type": "text", "text": "3.1 Euler-Maruyama hierarchical samplers ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Generative modeling with SDEs. Diffusion models assume a continuous-time generative process given by a neural stochastic differential equation [SDE; 74, 53, 66]: ", "page_idx": 2}, {"type": "equation", "text": "$$\nd\\mathbf{x}_{t}=u(\\mathbf{x}_{t},t;\\theta)\\,d t+g(\\mathbf{x}_{t},t;\\theta)\\,d\\mathbf{w}_{t},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\mathbf{X}_{0}$ follows a fixed tractable distribution $\\mu_{0}$ (such as a Gaussian or a point mass). The initial distribution $\\mu_{0}$ and the stochastic dynamics specified by (1) induce marginal densities $p_{t}$ on $\\mathbb{R}^{d}$ for each $t>0$ . The functions $u$ and $g$ have learnable parameters that we wish to optimize, using some objective, so as to make the terminal density $p_{1}$ close to $p_{\\mathrm{target}}$ . Samples can be drawn from $p_{1}$ by sampling $\\mathbf{x}_{0}\\sim\\mu_{0}$ and simulating the SDE (1) to time $t=1$ . ", "page_idx": 2}, {"type": "text", "text": "The SDE driving $\\mu_{0}$ to $p_{\\mathrm{target}}$ is not unique. However, if one fixes a reverse-time SDE, or noising process, that pushes $p_{\\mathrm{target}}$ at $t=1$ to $\\mu_{0}$ at $t=0$ , then its reverse, the forward SDE (1), is uniquely determined under mild conditions and is called the denoising process. For usual choices of the noising process, there are stochastic regression objectives for learning the drift $u$ of the denoising process given samples from $p_{\\mathrm{target}}$ , and the diffusion rate $g$ is available in closed form [27, 71]. ", "page_idx": 2}, {"type": "text", "text": "Time discretization. In practice, the integration of the SDE (1) is approximated by a discrete-time scheme, the simplest of which is Euler-Maruyama integration. The process (1) is replaced by a discrete-time Markov chain $\\mathbf{x}_{0}\\rightarrow\\mathbf{x}_{\\Delta t}\\rightarrow\\mathbf{x}_{2\\Delta t}\\rightarrow\\dots\\rightarrow\\mathbf{x}_{1}$ , where $\\begin{array}{r}{\\Delta t=\\frac{1}{T}}\\end{array}$ is the time increment and and $T$ is the number of steps: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbf{x}_{0}\\sim\\mu_{0},\\quad\\mathbf{x}_{t+\\Delta t}=\\mathbf{x}_{t}+u(\\mathbf{x}_{t},t;\\theta)\\Delta t+g(\\mathbf{x}_{t},t;\\theta)\\sqrt{\\Delta t}\\,\\mathbf{z}_{t}\\quad\\mathbf{z}_{t}\\sim{\\cal N}(\\mathbf{0},\\mathbf{I}_{d}).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The density of the transition kernel from $\\mathbf{X}_{t}$ to $\\mathbf{X}_{t+\\Delta t}$ can explicitly be written as ", "page_idx": 2}, {"type": "equation", "text": "$$\np_{F}(\\mathbf{x}_{t+\\Delta t}\\mid\\mathbf{x}_{t})=\\mathcal{N}(\\mathbf{x}_{t+\\Delta t};\\mathbf{x}_{t}+u(\\mathbf{x}_{t},t;\\theta)\\Delta t,g(\\mathbf{x}_{t},t;\\theta)^{2}\\Delta t\\mathbf{I}_{d}),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $p_{F}$ denotes the transition density of the discretized forward SDE. This density defines a joint distribution over trajectories starting at $\\mathbf{X}_{0}$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\np_{F}(\\mathbf{x}_{\\Delta t},\\mathbf{\\mathbf{\\xi}}...\\,,\\mathbf{x}_{1}\\mid\\mathbf{x}_{0})=\\prod_{i=0}^{T-1}p_{F}(\\mathbf{x}_{(i+1)\\Delta t}\\mid\\mathbf{x}_{i\\Delta t}).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Similarly, a discrete-time reverse process $\\mathbf{x}_{\\mathrm{1}}\\rightarrow\\mathbf{x}_{\\mathrm{1-}\\Delta t}\\rightarrow\\mathbf{x}_{\\mathrm{1-}2\\Delta t}\\rightarrow\\cdots\\rightarrow\\mathbf{x}_{\\mathrm{0}}$ with transition densities $p_{B}(\\mathbf{x}_{t-\\Delta t}\\mid\\mathbf{x}_{t})$ defines a joint distribution1 via ", "page_idx": 2}, {"type": "equation", "text": "$$\np_{B}(\\mathbf{x}_{0},\\ldots,\\mathbf{x}_{1-\\Delta t}\\mid\\mathbf{x}_{1})=\\prod_{t=1}^{T}p_{B}(\\mathbf{x}_{(i-1)\\Delta t}\\mid\\mathbf{x}_{i\\Delta t}).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "If the forward and backward processes (starting from $\\mu_{0}$ and $p_{\\mathrm{target}}$ , respectively) are reverses of each other, then they define the same distribution over trajectories, $i.e.$ , for all $\\mathbf{x}_{0}\\rightarrow\\mathbf{x}_{\\Delta t}\\rightarrow\\dots\\rightarrow\\mathbf{x}_{1}$ , ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mu_{0}(\\mathbf{x}_{0})p_{F}(\\mathbf{x}_{\\Delta t},\\dotsc,\\mathbf{x}_{1}\\mid\\mathbf{x}_{0})=p_{\\mathrm{target}}(\\mathbf{x}_{1})p_{B}(\\mathbf{x}_{0},\\dotsc,\\mathbf{x}_{1-\\Delta t}\\mid\\mathbf{x}_{1}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "In particular, the marginal densities of $\\mathbf{X}_{1}$ under the forward and backward processes are then equal to $p_{\\mathrm{target}}$ , and the forward process can be used to sample the target distribution. ", "page_idx": 2}, {"type": "text", "text": "Because the reverse of a process with Gaussian increments is, in general, not itself Gaussian, (6) can be enforced only approximately, but the discrepancy vanishes as $\\Delta t\\to0$ (i.e., increments are infinitesimally Gaussian), an application of the central limit theorem that is key to stochastic calculus [53]. ", "page_idx": 2}, {"type": "text", "text": "SDE learning as hierarchical variational inference. The problem of learning the parameters $\\theta$ of the forward process so as to enforce (6) is one of hierarchical variational inference. The backward process transforms $\\mathbf{X}_{1}$ into $\\mathbf{X}_{0}$ via a sequence of latent variables $\\mathbf{x}_{1-\\Delta t},\\ldots,\\mathbf{x}_{0}$ , and the forward process aims to match the posterior distribution over these variables and thus to approximately enforce (6). ", "page_idx": 2}, {"type": "text", "text": "In the setting of diffusion models learned from data, where one has samples from $p_{\\mathrm{target}}$ , one can optimize the forward process by minimizing the KL divergence $D_{\\mathrm{KL}}(p_{\\mathrm{target}}\\cdot p_{B}\\|\\mu_{0}\\cdot\\bar{p_{F}})$ between the distribution over trajectories given by the reverse process and that given by the forward process. ", "page_idx": 2}, {"type": "text", "text": "This is equivalent to the typical training of diffusion models, which optimizes a variational bound on the data log-likelihood (see [70]). However, in the setting of an intractable density $p_{\\mathrm{target}}$ , unbiased estimators of this divergence are not available. Instead, one can optimize the reverse KL:2 ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{D_{\\mathrm{KL}}(\\mu_{0}\\cdot p_{F}\\|p_{\\mathrm{target}}\\cdot p_{B})}\\\\ &{=\\!\\int\\log\\frac{\\mu_{0}(\\mathbf{x}_{0})p_{F}\\left(\\mathbf{x}_{\\Delta t},\\,\\dots,\\,,\\mathbf{x}_{1}\\mid\\mathbf{x}_{0}\\right)}{p_{\\mathrm{target}}(\\mathbf{x}_{1})p_{B}\\left(\\mathbf{x}_{0},\\,\\dots,\\,,\\mathbf{x}_{1-\\Delta t}\\mid\\mathbf{x}_{1}\\right)}d\\mu_{0}(\\mathbf{x}_{0})p_{F}(\\mathbf{x}_{\\Delta t},\\,,\\dots,\\mathbf{x}_{1}\\mid\\mathbf{x}_{0})\\,\\,d\\mathbf{x}_{\\Delta t}\\,\\,\\dots\\,d\\mathbf{x}_{1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Various estimators of this objective are available. For instance, the path integral sampler objective [PIS; 87] uses the reparametrization trick to express (7) as an expectation over noise variables $\\mathbf{Z}_{t}$ that participate in the hierarchical sampling of $\\mathbf{x}_{\\Delta t},\\ldots,\\mathbf{x}_{1}$ , yielding an unbiased gradient estimator, but one that requires backpropagation into the simulation of the forward process. The related denoising diffusion sampler [DDS; 77] applies the same principle in a different integration scheme. ", "page_idx": 3}, {"type": "text", "text": "3.2 Euler-Maruyama samplers as GFlowNets ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Continuous generative flow networks (GFlowNets) [41] express the problem of enforcing (6) as a reinforcement learning task. In this section, we summarize this interpretation, its connection to neural SDEs, the associated learning objectives, and their relative advantages and disadvantages. ", "page_idx": 3}, {"type": "text", "text": "The connection between generative flow networks and diffusion models or SDEs was first made informally by [45] in the distribution-matching setting and by [83] in the maximum-likelihood setting, while the theoretical foundations for continuous GFlowNets were later laid down by [41]. ", "page_idx": 3}, {"type": "text", "text": "State and action space. To formulate sampling as a sequential decision-making problem, one must define the spaces of states and actions. In the case of sampling by $T.$ -step Euler-Maruyama integration, assuming $\\mu_{0}$ is a point mass at 0, the state space is ", "page_idx": 3}, {"type": "equation", "text": "$$\nS=\\left\\{(\\mathbf{0},0)\\cup\\left\\{(\\mathbf{x},t):\\mathbf{x}\\in\\mathbb{R}^{d},t\\in\\left\\{\\Delta t,2\\Delta t,\\dots,1\\right\\}\\right\\},\\right.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "with the point $\\left({{\\bf{x}},t}\\right)$ representing that the sampling agent is at position $\\mathbf{X}$ at time $t$ . ", "page_idx": 3}, {"type": "text", "text": "Sampling begins with the initial state $\\mathbf{x}_{0}:=(\\mathbf{0},0)$ , proceeds through a sequence of states $(\\mathbf{x}_{\\Delta t},\\Delta t)$ , $(\\mathbf{x}_{2\\Delta t},2\\Delta t)$ , . . . , and ends at a state $(\\mathbf{x}_{1},1)$ ; states $\\left({{\\bf{x}},t}\\right)$ with $t=1$ are called terminal states and their collection is denoted $\\chi$ . From now on, we will often write $\\mathbf{X}_{t}$ in place of the state $\\left(\\mathbf{x}_{t},t\\right)$ when the time $t$ is clear from context. The sequence of states $\\mathbf{x}_{0}\\rightarrow\\mathbf{x}_{\\Delta t}\\rightarrow\\dots\\rightarrow\\mathbf{x}_{1}$ is called a complete trajectory. ", "page_idx": 3}, {"type": "text", "text": "The actions from a nonterminal state $\\left(\\mathbf{x}_{t},t\\right)$ correspond to the possible next states $(\\mathbf{x}_{t+\\Delta t},t+\\Delta t)$ that can be reached from $\\left(\\mathbf{x}_{t},t\\right)$ by a single step of the Euler-Maruyama integrator. ", "page_idx": 3}, {"type": "text", "text": "Forward policy and learning problem. A (forward) policy is a collection of continuous distributions over the successor states \u2013 states reachable by a single action \u2013 of every nonterminal state $\\left({{\\bf{x}},t}\\right)$ . In our context, this amounts to a collection of conditional probability densities $p_{F}(\\mathbf{x}_{t+\\Delta t}\\mid\\mathbf{x}_{t};\\theta)$ , representing the density of the transition kernel from $\\mathbf{X}_{t}$ to $\\mathbf{X}_{t+\\Delta t}$ . GFlowNet training optimizes the parameters $\\theta$ , which may be the weights of a neural network specifying a density over $\\mathbf{X}_{t+\\Delta t}$ conditioned on $\\mathbf{X}_{\\Delta t}$ ", "page_idx": 3}, {"type": "text", "text": "A policy $p_{F}$ induces a distribution over complete trajectories $\\tau=(\\mathbf{x}_{0}\\rightarrow\\mathbf{x}_{\\Delta t}\\rightarrow\\cdots\\rightarrow\\mathbf{x}_{1})$ ) via ", "page_idx": 3}, {"type": "equation", "text": "$$\np_{F}(\\tau;\\boldsymbol{\\theta})=\\prod_{i=0}^{T-1}p_{F}(\\mathbf{x}_{(i+1)\\Delta t}\\mid\\mathbf{x}_{i\\Delta t};\\boldsymbol{\\theta}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "In particular, we get a marginal density over terminal states: ", "page_idx": 3}, {"type": "equation", "text": "$$\np_{F}^{\\top}({\\bf x}_{1};\\theta){=}\\!\\int\\,p_{F}({\\bf x}_{0}\\rightarrow{\\bf x}_{\\Delta t}\\rightarrow\\cdot\\cdot\\cdot\\rightarrow{\\bf x}_{1};\\theta)\\,d{\\bf x}_{\\Delta t}\\,.\\,.\\,d{\\bf x}_{1-\\Delta t}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The learning problem solved by GFlowNets is to find the parameters $\\theta$ of a policy $p_{F}$ whose terminating density $p_{F}^{\\top}$ is equal to $p_{\\mathrm{target}}$ , i.e., ", "page_idx": 3}, {"type": "equation", "text": "$$\np_{F}^{\\top}(\\mathbf{x}_{1};\\theta)=\\frac{R(\\mathbf{x}_{1})}{Z}\\quad\\forall\\mathbf{x}_{1}\\in\\mathbb{R}^{d}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "However, because the integral (8) is intractable and $Z$ is unknown, auxiliary objects must be introduced into optimization objectives to enforce (9), as discussed below. ", "page_idx": 4}, {"type": "text", "text": "Notably, if the policy is a Gaussian with mean and variance given by neural networks taking $\\mathbf{X}_{t}$ and $t$ as input, then learning the policy amounts to learning the drift $u(\\mathbf{x}_{t},t;\\theta)$ and diffusion $g(\\mathbf{x}_{t},t;\\theta)$ of a SDE (1), i.e., fitting a neural SDE. The SDE learning problem in $\\S3.1$ is thus the same as that of fitting a GFlowNet with Gaussian policies. ", "page_idx": 4}, {"type": "text", "text": "Backward policy and trajectory balance. A backward policy is a collection of conditional probability densities $p_{B}(\\mathbf{x}_{t-\\Delta t}\\mid\\mathbf{x}_{t};\\boldsymbol{\\psi})$ , representing a probability density of transitioning from $\\mathbf{X}_{t}$ to an ancestor state $\\mathbf{X}_{t-\\Delta t}$ . The backward policy induces a distribution over complete trajectories $\\tau$ conditioned on their terminal state (cf. (5)): ", "page_idx": 4}, {"type": "equation", "text": "$$\np_{B}(\\tau\\mid\\mathbf{x}_{1};\\boldsymbol{\\psi})=\\prod_{i=1}^{T}p_{B}(\\mathbf{x}_{(i-1)\\Delta t}\\mid\\mathbf{x}_{i\\Delta t};\\boldsymbol{\\psi}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where exceptionally $p_{B}(\\mathbf{x}_{0}\\mid\\mathbf{x}_{\\Delta t})=1$ as $\\mu_{0}$ is a point mass. ", "page_idx": 4}, {"type": "text", "text": "Generalizing a result in the discrete-space setting [44], [41] show that $p_{F}$ samples from the target distribution (i.e., satisfies (9)) if and only if there exists a backward policy $p_{B}$ and a scalar $Z_{\\theta}$ such that the trajectory balance conditions are fulfliled for every complete trajectory $\\tau=(\\mathbf{x}_{0}\\rightarrow\\mathbf{x}_{\\Delta t}\\rightarrow\\cdots\\rightarrow\\mathbf{x}_{1})$ ): ", "page_idx": 4}, {"type": "equation", "text": "$$\nZ_{\\theta}p_{F}(\\tau;\\theta)=R({\\bf x}_{1})p_{B}(\\tau\\mid{\\bf x}_{1};\\psi).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "If these conditions hold, then $Z_{\\theta}$ equals the true partition function $\\begin{array}{r}{Z=\\int_{\\mathbf{x}}R(\\mathbf{x})\\,d\\mathbf{x}}\\end{array}$ . The trajectory balance objective for a trajectory $\\tau$ is the squared log-ratio of the two sides of (10), that is: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{TB}}(\\tau;\\theta,\\psi)=\\left(\\log\\frac{Z_{\\theta}p_{F}(\\tau;\\theta)}{R(\\mathbf{x}_{1})p_{B}(\\tau\\mid\\mathbf{x}_{1};\\psi)}\\right)^{2}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "One can thus achieve (9) by minimizing to zero the loss $\\mathcal{L}_{\\mathrm{TB}}(\\tau;\\theta,\\psi)$ with respect to the parameters $\\theta$ and $\\psi$ , where the trajectories $\\tau$ used for training are sampled from some training policy $\\pi(\\tau)$ . While it is possible to optimize (11) with respect to the parameters of both the forward and backward policies, in some learning problems, one fixes the backward policy and only optimizes the parameters of $p_{F}$ and the estimate of the partition function $Z_{\\theta}$ . For example, for most experiments in $\\S5$ , we fix the backward policy to a discretized Brownian bridge, following past work. ", "page_idx": 4}, {"type": "text", "text": "Off-policy optimization. Unlike the KL objective (7), whose gradient involves an expectation over the distribution of trajectories under the current forward process, (11) can be optimized off-policy, i.e., using trajectories sampled from an arbitrary distribution $\\pi$ . Because minimizing $\\mathcal{L}_{\\mathrm{TB}}(\\tau;\\theta,\\psi)$ to 0 for all $\\tau$ in the support of $\\pi$ will achieve (9), $\\pi$ can be taken be any distribution with full support, so as to promote discovery of modes of the target distribution. Various choices motivated by reinforcement learning techniques have been proposed, including noisy exploration or tempering [6], replay buffers [16], Thompson sampling [58], and backward traces from terminal states obtained by MCMC [42]. In the continuous case, [45, 41] proposed to simply add a small constant to the policy variance when sampling trajectories for training. Off-policy optimization is a key advantage of GFlowNets over variational methods such as PIS, which require on-policy optimization [45]. ", "page_idx": 4}, {"type": "text", "text": "However, when $\\mathcal{L}_{\\mathrm{TB}}$ happens to be optimized on-policy, i.e., using trajectories sampled from the policy $p_{F}$ itself, we get an unbiased estimator of the gradient of the KL divergence (7) with respect to $p_{F}$ \u2019s parameters up to a constant [61, 45, 89], that is: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\tau\\sim p_{F}(\\tau)}\\left[\\nabla_{\\theta^{\\prime}}\\mathcal{L}_{\\mathrm{TB}}(\\tau;\\theta,\\psi)\\right]=2\\nabla_{\\theta^{\\prime}}D_{\\mathrm{KL}}(p_{F}(\\tau;\\theta)\\|p_{\\mathrm{target}}(\\mathbf{x}_{1})p_{B}(\\tau\\mid\\mathbf{x}_{1};\\psi)),}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\nabla_{\\theta^{\\prime}}$ denotes the gradient with respect to the parameters of $p_{F}$ , but not $Z_{\\theta}$ . This unbiased estimator tends to have higher variance than the reparametrization-based estimator used by PIS. On the other hand, it does not require backpropagation through the simulation of the forward process and can be used to optimize the parameters of both the forward and backward policies. ", "page_idx": 4}, {"type": "text", "text": "Other objectives. The trajectory balance objective (11) is not the only possible objective that can be used to enforce (9). A notable generalization is subtrajectory balance [SubTB; 43], which involves modeling a scalar state flow $f(\\mathbf{x}_{t};\\theta)$ associated with each state $\\mathbf{X}_{t}$ \u2013 intended to model the marginal density of the forward process at $\\mathbf{X}_{t}$ \u2013 and enforcing subtrajectory balance conditions for all partial trajectories $\\mathbf{x}_{m\\Delta t}\\rightarrow\\mathbf{x}_{(m+1)\\Delta t}\\rightarrow\\cdots\\rightarrow\\mathbf{x}_{n\\Delta t}$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\nf(\\mathbf{x}_{m\\Delta t};\\theta)\\prod_{i=m}^{n-1}p_{F}(\\mathbf{x}_{(i+1)\\Delta t}\\mid\\mathbf{x}_{i\\Delta t};\\theta)=f(\\mathbf{x}_{n\\Delta t};\\theta)\\prod_{i=m+1}^{n}p_{B}(\\mathbf{x}_{(i-1)\\Delta t}\\mid\\mathbf{x}_{i\\Delta t};\\psi),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where for terminal states $f(\\mathbf{x}_{1})=R(\\mathbf{x}_{1})$ . This approach has some computational overhead associated with training the state flow, but has been shown to be effective in discrete-space settings, especially when combined with the forward-looking reward shaping scheme proposed by [54]. It has also been tested in the continuous case, but our experimental results suggest that it offers little benefit over the TB objective in the diffusion setting (see $\\S4.1$ and $\\S B.1$ ). ", "page_idx": 5}, {"type": "text", "text": "It is also worth noting the off-policy VarGrad estimator [52, 61], rediscovered for GFlowNets by [84]. Like TB, VarGrad can be optimized over trajectories drawn off-policy. Rather than enforcing (10) for every trajectory, VarGrad optimizes the empirical variance (over a minibatch) of the log-ratio of the two sides of (10). As noted by [45], this is equivalent to minimizing $\\mathcal{L}_{\\mathrm{TB}}$ first with respect to $\\log{Z_{\\theta}}$ to optimality over the batch, then with respect to the parameters of $p_{F}$ . ", "page_idx": 5}, {"type": "text", "text": "4 Exploration and credit assignment in continuous GFlowNets ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The main challenges in training off-policy sampling models are exploration efficiency (discovery of high-reward states) and credit assignment (propagation of reward signals to the actions that led to them). We describe several new and existing methods for addressing these challenges in the context of diffusion-structured GFlowNets. These techniques will be empirically studied and compared in $\\S5$ . ", "page_idx": 5}, {"type": "text", "text": "4.1 Credit assignment methods ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Partial energies and subtrajectory-based learning. [85] studied the diffusion sampler learning problem introduced by [41], but replaced the TB learning objective with the SubTB objective.4 In addition, an inductive bias resembling the geometric interpolation in [46] was used for the state flow function: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\log f(\\mathbf{x}_{t};\\theta)=(1-t)\\log p_{t}^{\\mathrm{ref}}(\\mathbf{x}_{t})+t\\log R(\\mathbf{x}_{t})+\\mathrm{NN}(\\mathbf{x}_{t},t;\\theta),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where NN is a neural network and $p_{t}^{\\mathrm{ref}}(\\mathbf{x}_{t})=N(\\mathbf{x}_{t};0,\\sigma^{2}t I_{d})$ is the marginal density of a Brownian motion with rate $\\sigma$ at $\\mathbf{X}_{t}$ . The use of the target density $\\log R(\\mathbf{x}_{t})=-\\mathcal{E}(\\mathbf{x}_{t})$ in the state flow function was hypothesized to provide an effective signal driving the sampler to high-density states at early steps in the trajectory. Such an inductive bias on the state flow was called forward-looking (FL) by [54], and we will refer to this method as FL-SubTB in $\\S5$ . ", "page_idx": 5}, {"type": "text", "text": "Langevin dynamics inductive bias. [87] proposed an inductive bias on the architecture of the drift of the neural SDE $u(\\mathbf{x}_{t},t;\\theta)$ (in GFlowNet terms, the mean of the Gaussian density $p_{F}\\big(\\mathbf{x}_{t+\\Delta t}\\mid\\mathbf{x}_{t};\\theta\\big)\\big)$ that resembles a Langevin process on the target distribution. One writes ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{u(\\mathbf{x}_{t},t;\\theta)=\\mathbf{N}\\mathbf{N}_{1}(\\mathbf{x}_{t},t;\\theta)+\\mathbf{N}\\mathbf{N}_{2}(t;\\theta)\\nabla\\mathcal{E}(\\mathbf{x}_{t}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\mathrm{NN}_{1}$ and $\\mathrm{NN}_{2}$ are neural networks outputting a vector and a scalar, respectively. The second term in (14) is a scaled gradient of the target energy \u2013 the drift of a Langevin SDE \u2013 and the first term is a learned correction. This inductive bias, which we name the Langevin parametrization (LP), was shown to improve the efficiency of PIS. We will study its effect on continuous GFlowNets in $\\S5$ . ", "page_idx": 5}, {"type": "text", "text": "The inductive bias (14) placed on policies represents a different way of incorporating the reward signal at intermediate steps in the trajectory and can steer the sampler towards low-energy regions. It contrasts with (13) in that it provides the gradient of the energy directly to the policy, rather than just using the energy to provide a learning signal to policies via the parametrization of the log-state flow (13). ", "page_idx": 5}, {"type": "text", "text": "Considerations of the continuous-time limit lead us to conjecture that the Langevin parametrization (14) with $\\mathrm{NN}_{1}$ independent of $\\mathbf{X}_{t}$ is equivalent to the forward-looking flow (13) in the limit of small time increments $\\Delta t\\rightarrow0$ , i.e., they induce the same asymptotics of the discrepancy in the SubTB constraints (12) over short partial trajectories. Such theoretical analysis can be the subject of future work. ", "page_idx": 5}, {"type": "text", "text": "4.2 A new method for off-policy exploration with local search and replay buffer ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Local search with parallel MALA. The FL and LP inductive biases both induce computational overhead: either in the evaluation and optimization of a state flow or in the need to evaluate the energy gradient at every step of sampling (see $\\S C.3\\$ ). We present an alternative technique that does not induce additional computation cost per training trajectory. ", "page_idx": 5}, {"type": "table", "img_path": "vieIamY2Gi/tmp/200a172e9eec11036d565312a438769ed5320081eecc9b7b542bcf5257d0eafa.jpg", "table_caption": ["Table 1: Log-partition function estimation errors for unconditional modeling tasks (mean and standard deviation over 5 runs). The four groups of models are: MCMC-based samplers, simulation-driven variational methods, baseline GFlowNet methods with different learning objectives, and methods augmented with Langevin parametrization and local search. See $\\mathrm{\\SC.1}$ for additional metrics. "], "table_footnote": ["Highlight $\\boldsymbol{:}$ mean indistinguishable from best in column with $p<0.05$ under one-sided Welch unpaired $t$ -test. "], "page_idx": 6}, {"type": "text", "text": "To enhance the quality of samples during training, we incorporate local search into the exploration process, motivated by the success of local exploration [82, 32, 39] and replay buffer [e.g., 16] methods for GFlowNets in discrete spaces. Unlike these methods, which define MCMC kernels via the GFlowNet policies, our method leverages parallel Metropolis-adjusted Langevin (MALA) directly in the target space. ", "page_idx": 6}, {"type": "image", "img_path": "vieIamY2Gi/tmp/2bcf6e6045a044de20a513ef5f3705b61ff8347ebb62ba877f1145af6ddb73a1.jpg", "img_caption": ["PIS +LP TB + Expl.TB + Expl. + LS True samples ", "Figure 1: Two-dimensional projections of Manywell samples from models trained by different algorithms. Our proposed replay buffer with local search is capable of preventing mode collapse. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "In detail, we initially sample $M$ candidates from the sampler: $\\{\\mathbf{x}^{(1)},\\hdots,\\mathbf{x}^{(M)}\\}\\,\\sim\\,p_{F}^{\\top}(\\cdot)$ . Subsequently, we run parallel MALA across $M$ chains over $K$ transitions , with the initial states of the Markov chain being $\\{\\mathbf{x}^{(1)},\\ldots.,\\mathbf{x}^{(M)}\\}$ . After the $K_{\\mathrm{burn-in}}$ burn-in transitions, the accepted samples are stored in a local search buffer $\\mathcal{D}_{\\mathrm{LS}}$ . We occasionally update the buffer using MALA steps and replay samples from it to minimize the computational demands of iterative local search. MALA steps are far more parallelizable than sampler training and need to be made only rarely (as the buffer is much larger than the training batch size), so the overhead of local search is small. ", "page_idx": 6}, {"type": "text", "text": "Training with local search and replay buffer. To train samplers with the aid of the buffer, we draw a sample $\\mathbf{X}$ from $\\mathcal{D}_{\\mathrm{LS}}$ (uniformly or using a prioritization scheme, $\\S E,$ , sample a trajectory $\\tau$ leading to $\\mathbf{X}$ from the backward process, and make a gradient update on the objective (e.g., TB) associated with $\\tau$ . ", "page_idx": 6}, {"type": "text", "text": "When training with local search guidance, we alternate two steps, inspired by [42], who alternate training on forward trajectories and backward trajectories initialized at a fixed set of MCMC samples. Step A involves training with on-policy or exploratory forward sampling while Step B uses samples drawn from the local search buffer described above. This allows the sampler to explore both diversified samples (Step A) and low-energy samples (Step B). See $\\S E$ for detailed pseudocode of adaptive-step parallel MALA and local search-guided GFlowNet training. ", "page_idx": 6}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We conduct comprehensive benchmarks of various diffusion-structured samplers, encompassing both GFlowNet samplers and methods such as PIS. For the GFlowNet samplers, we investigate a range of techniques, including different exploration strategies and loss functions. Additionally, we examine the efficacy of the Langevin parametrization and the newly proposed local search with buffer. ", "page_idx": 6}, {"type": "text", "text": "5.1 Tasks and baselines ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We explore two types of tasks, with more details provided in $\\S B$ : sampling from energy distributions \u2013 a 2-dimensional mixture of Gaussians with 25 modes (25GMM), the 10-dimensional Funnel, the 32-dimensional Manywell distribution, and the 1600-dimensional Log-Gaussian Cox process \u2013 ", "page_idx": 6}, {"type": "text", "text": "and conditional sampling from the latent posterior of a variational autoencoder (VAE; [40, 60]).   \nThis allows us to investigate both unconditional and conditional generative modeling techniques. ", "page_idx": 7}, {"type": "text", "text": "We evaluate three algorithm categories: ", "page_idx": 7}, {"type": "text", "text": "(1) Traditional sampling methods: We consider a standard Sequential Monte Carlo (SMC) implementation and a state-of-the-art nested sampling method (GGNS, [42]).   \n(2) Simulation-driven variational approaches: DIS [8], DDS [77], and PIS [87].   \n(3) Diffusion-based GFlowNet samplers: Our evaluation focuses on TB-based training and the enhancements described in $\\S4$ : the VarGrad estimator (VarGrad), off-policy exploration (Expl.), Langevin parametrization $(\\mathbf{L}\\mathbf{P})$ , and local search (LS). Additionally, we assess the FL-SubTBbased continuous GFlowNet as studied by [85] for a comprehensive comparison. ", "page_idx": 7}, {"type": "text", "text": "For (2) and (3), we employ a consistent neural architecture across methods (details in $\\S D$ ). ", "page_idx": 7}, {"type": "text", "text": "Learning problem and fixed backward process. In our main experiments, we borrow the modeling setting from [87]. We aim to learn a Gaussian forward policy $p_{F}$ that samples from the target distribution in $T=100$ steps $\\Delta t=0.01)$ ). Just as in past work [87, 41, 85], the backward process is fixed to a discretized Brownian bridge with a noise rate $\\sigma$ that depends on the domain; explicitly, ", "page_idx": 7}, {"type": "equation", "text": "$$\np_{B}(\\mathbf{x}_{t-\\Delta t}\\mid\\mathbf{x}_{t})\\!=\\!\\mathcal{N}\\left(\\mathbf{x}_{t-\\Delta t};\\frac{t-\\Delta t}{t}\\mathbf{x}_{t},\\frac{t-\\Delta t}{t}\\boldsymbol{\\sigma}^{2}\\Delta t\\mathbf{I}_{d}\\right),\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "understood to be a point mass at 0 when $t=\\Delta t$ . To keep the learning problem consistent with past work, we fix the variance of the forward policy $p_{F}$ to $\\sigma^{2}$ . This simplification is justified in continuous time, when the forward and reverse SDEs have the same diffusion rate. However, in $\\S5.3$ , we will provide evidence that learning the forward policy\u2019s variance is quite beneficial for shorter trajectories. ", "page_idx": 7}, {"type": "text", "text": "Benchmarking metrics. To evaluate diffusion-based samplers, we use two metrics from past work [87, 41], which we restate in our notation. Given any forward policy $p_{F}$ , we have a variational lower bound on the log-partition function $\\begin{array}{r}{\\log Z=\\int_{\\mathbb{R}^{d}}R(\\mathbf{\\bar{x}})\\,d\\mathbf{x};}\\end{array}$ : ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathrm{og}\\int_{\\mathbb{R}^{d}}R(\\mathbf{x})\\,d\\mathbf{x}=\\log\\underset{\\tau=(\\cdots\\rightarrow\\mathbf{x}_{1})\\sim p_{F}(\\tau)}{\\mathbb{E}}\\left[\\frac{R(\\mathbf{x}_{1})p_{B}(\\tau\\mid\\mathbf{x}_{1})}{p_{F}(\\tau)}\\right]\\geq\\underset{\\tau=(\\cdots\\rightarrow\\mathbf{x}_{1})\\sim p_{F}(\\tau)}{\\mathbb{E}}\\left[\\log\\frac{R(\\mathbf{x}_{1})p_{B}(\\tau\\mid\\mathbf{x}_{1})}{p_{F}(\\tau)}\\right].\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "We use a $K$ -sample $\\langle K\\ =\\ 2000\\rangle$ Monte Carlo estimate of this expectation, $\\log{\\hat{Z}}$ , as a metric, which equals the true $\\log{Z}$ if $p_{F}$ and $p_{B}$ jointly satisfy (10) and thus $p_{F}$ samples from the target distribution. We also employ an importance-weighted variant, which emphasizes mode coverage over accurate local modeling: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\log\\hat{Z}^{\\mathrm{RW}}:=\\log\\sum_{i=1}^{K}\\left[\\frac{R(\\mathbf{x}_{1}^{(i)})p_{B}(\\tau^{(i)}\\mid\\mathbf{x}_{1}^{(i)})}{p_{F}(\\tau^{(i)})}\\right],\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\tau^{(1)},\\dots,\\tau^{(K)}$ are trajectories sampled from $p_{F}$ and leading to terminal states $\\mathbf{x}_{1}^{(1)},\\ldots,\\mathbf{x}_{1}^{(K)}$ The estimator $\\log{\\hat{Z}^{\\mathrm{RW}}}$ is also a lower bound on $\\log{Z}$ and approaches it as $K\\rightarrow\\infty$ [10]. In the unconditional modeling benchmarks, we compare both estimators to the true log-partition function, which is known analytically for all tasks except LGCP (leading to discrepancies in past work; see $\\S B.1]$ ). ", "page_idx": 7}, {"type": "text", "text": "In addition, we include a sample-based metric (2-Wasserstein distance); see $\\mathrm{\\SC.1}$ . ", "page_idx": 7}, {"type": "text", "text": "5.2 Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Unconditional sampling. We report the metrics for all algorithms and energies in Table 1. ", "page_idx": 7}, {"type": "text", "text": "We observe that TB\u2019s performance is generally modest without additional exploration and credit assignment mechanisms, except on the Funnel task, where variations in performance across methods are negligible. This confirms hypotheses from past work about the importance of offpolicy exploration [45, 41] and the importance of improved credit assignment [85]. On the other hand, our results do not show a consistent ", "page_idx": 7}, {"type": "image", "img_path": "vieIamY2Gi/tmp/42a6cce5a6b1a359214a17107c600f444e42a4922ca7add5d335f0af16f56e02.jpg", "img_caption": ["Figure 2: Effect of exploration variance on models trained with TB on the 25GMM energy. Exploration promotes mode discovery, but should be decayed over time to optimally allocate the modeling power to high-likelihood trajectories. "], "img_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "vieIamY2Gi/tmp/2c463e76d25c6b2a449a641147cce732d2ea99aa70e60952596769abc61baf26.jpg", "img_caption": ["Figure 3: Left: Distribution of $\\mathbf{x}_{0},\\mathbf{x}_{0.1},\\ldots,\\mathbf{x}_{1}$ learned by 10-step samplers with fixed $(t o p)$ and learned (middle) forward policy variance on the 25GMM energy. The last step of sampling the fixed-variance model adds Gaussian noise of a variance close to that of the components of the target distribution, preventing the the sampler from sharply capturing the modes. The last row shows the policy variance learned as a function of $\\mathbf{X}_{t}$ at various time steps $t$ (white is high variance, blue is low), showing that less noise is added around the peaks near $t=1$ . The two models\u2019 log-partition function estimates are $-1.67$ and $-0.62$ , respectively. Right: For varying number of steps $T$ , we plot the $\\log{\\hat{Z}}$ obtained by models with fixed and learned variance. Learning policy variances gives similar samplers with fewer steps. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "and significant improvement of the FL-SubTB objective used by [85] over TB. Replacing TB with the VarGrad objective yields similar results. ", "page_idx": 8}, {"type": "text", "text": "The simple off-policy exploration method of adding variance to the policy notably enhances performance on the 25GMM task. We investigate this phenomenon in more detail in Fig. 2, finding that exploration that slowly decreases over the course of training is the best strategy. ", "page_idx": 8}, {"type": "text", "text": "On the other hand, our local search-guided exploration with a replay buffer (LS) leads to a substantial improvement in performance, surpassing or competing with GFlowNet baselines, non-GFlowNet baselines, and non-amortized sampling methods in most tasks and metrics. This advantage is attributed to efficient exploration and the ability to replay past low-energy regions, thus preventing mode collapse during training (Fig. 1). Further details on LS enhancements are discussed in $\\mathrm{\\SE}$ with ablation studies in $\\S E.2$ . ", "page_idx": 8}, {"type": "text", "text": "Incorporating Langevin parametrization (LP) into TB or FL-SubTB results in notable performance improvements (despite being $2{-}3\\times$ slower per iteration), indicating that previous observations [87] transfer to off-policy algorithms. Compared to FL-SubTB, which aims for enhanced credit assignment through partial energy, LP achieves superior credit assignment leveraging gradient information, akin to partial energy in continuous time. LP is either superior or competitive across most tasks and metrics. ", "page_idx": 8}, {"type": "text", "text": "In $\\mathrm{\\&C}.3$ , we study the scaling of the algorithms with dimension, showing efficiency of the proposed LS. ", "page_idx": 8}, {"type": "text", "text": "Conditional sampling. For the VAE task, we observe that the performance of the baseline GFlowNet-based samplers is generally worse ", "page_idx": 8}, {"type": "table", "img_path": "vieIamY2Gi/tmp/c11df09be7f015b31cede6a8be6955af248320c2ae4c4cb76cf0cd05daf59320.jpg", "table_caption": ["Table 2: Log-likelihood estimates on a test set for a pretrained VAE decoder on MNIST. The latent being sampled is 20-dimensional. The VAE\u2019s training ELBO (Gaussian encoder) was $\\approx-101$ . "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "than that of the simulation-based PIS (Table 2). While LP and LS improve the performance of TB, they do not close the gap in likelihood estimation; however, with the VarGrad objective, the performance is competitive with or superior to PIS. We hypothesize that this discrepancy is due to the difficulty of fitting the conditional log-partition function estimator, which is required for the TB objective but not for VarGrad, which only learns the policy. (In Fig. D.1 we show decoded samples encoded using the best-performing diffusion encoder.) ", "page_idx": 8}, {"type": "text", "text": "5.3 Extensions to general SDE learning problems ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Our implementation of diffusion-structured generative flow networks includes several additional options that diverge from the modeling assumptions made in most past work in the field. Notably, it features the ability to: ", "page_idx": 9}, {"type": "text", "text": "\u2022 optimize the backward (noising) process \u2013 not only the denoising process \u2013 as was done for related learning problems in [11, 62, 78];   \n\u2022 learn the forward process\u2019s diffusion rate $g(\\mathbf{x}_{t},t;\\theta)$ , not only the mean $u(\\mathbf{x}_{t},t;\\theta)$ ;   \n\u2022 assume a varying noise schedule for the backward process, making it possible to train models with standard noising SDEs used for diffusion models for images. ", "page_idx": 9}, {"type": "text", "text": "These extensions will allow others to build on our implementation and apply it to problems such as finetuning diffusion models trained on images with a GFlowNet objective. ", "page_idx": 9}, {"type": "text", "text": "As noted in $\\S5.1$ , in the main experiments we fixed the diffusion rate of the learned forward process, an assumption inherited from all past work and justified in the continuous-time limit. However, we perform an experiment to show the importance of extensions such as learning the forward variance in discrete time. Fig. 3 shows the samples of models on the 25GMM energy following the experimental setup of [42]. We see that when the forward policy\u2019s variance is learned, the model can better capture the details of the target distributions, choosing a low variance in the vicinity of the peaks to avoid \u2018blurring\u2019 them through the noise added in the last step of sampling. ", "page_idx": 9}, {"type": "text", "text": "In $\\mathrm{\\SC}.2$ , we include preliminary results using a variance-preserving backward process, as commonly used in diffusion models, in place of the reversed Brownian motion used in the main experiments. ", "page_idx": 9}, {"type": "text", "text": "The ability to model distributions accurately in fewer steps is important for computational efficiency. Future work can consider ways to improve performance in coarse time discretizations, such as nonGaussian transitions, whose utility in diffusion models trained from data has been demonstrated [81]. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We have presented a study of diffusion-structured samplers for amortized inference over continuous variables. Our results suggest promising techniques for improving the mode coverage and efficiency of these models. Future work on applications can consider inference of high-dimensional parameters of dynamical systems and inverse problems. In probabilistic machine learning, extensions of this work should study integration of our amortized sequential samplers as variational posteriors in an expectation-maximization loop for training latent variable models, as was recently done for discrete compositional latents by [32], and for sampling Bayesian posteriors over high-dimensional model parameters. The most important direction of theoretical work is understanding the continuous-time limit $T\\to\\infty)$ ) of all the algorithms we have studied. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We thank Cheng-Hao Liu for assistance with methods from prior work, as well as Julius Berner, V\u00edctor Elvira, Lorenz Richter, Alexander Tong, and Siddarth Venkatraman for helpful discussions and suggestions. ", "page_idx": 9}, {"type": "text", "text": "The authors acknowledge funding from UNIQUE, CIFAR, NSERC, Intel, Recursion Pharmaceuticals, and Samsung. The research was enabled in part by computational resources provided by the Digital Research Alliance of Canada (https://alliancecan.ca), Mila (https://mila.quebec), and NVIDIA. The research of M.S. was in part funded by National Science Centre, Poland, 2022/45/N/ST6/03374. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Adam, A., Coogan, A., Malkin, N., Legin, R., Perreault-Levasseur, L., Hezaveh, Y., and Bengio, Y. Posterior samples of source galaxies in strong gravitational lenses with score-based priors. arXiv preprint arXiv:2211.03812, 2022. ", "page_idx": 9}, {"type": "text", "text": "[2] Agrawal, A. and Domke, J. Amortized variational inference for simple hierarchical models. Neural Information Processing Systems (NeurIPS), 2021. ", "page_idx": 9}, {"type": "text", "text": "[3] Albergo, M. S., Kanwar, G., and Shanahan, P. E. Flow-based generative models for Markov chain Monte Carlo in lattice field theory. Physical Review D, 100(3):034515, 2019. [4] Atanackovic, L., Tong, A., Wang, B., Lee, L. J., Bengio, Y., and Hartford, J. DynGFN: Towards bayesian inference of gene regulatory networks with GFlowNets. Neural Information Processing Systems (NeurIPS), 2023.   \n[5] Bandeira, A. S., Maillard, A., Nickl, R., and Wang, S. On free energy barriers in Gaussian priors and failure of cold start MCMC for high-dimensional unimodal distributions. Philosophical transactions. Series A, Mathematical, physical, and engineering sciences, 381, 2022. [6] Bengio, E., Jain, M., Korablyov, M., Precup, D., and Bengio, Y. Flow network based generative models for non-iterative diverse candidate generation. Neural Information Processing Systems (NeurIPS), 2021. [7] Bengio, Y., Lahlou, S., Deleu, T., Hu, E. J., Tiwari, M., and Bengio, E. GFlowNet foundations. Journal of Machine Learning Research, 24(210):1\u201355, 2023.   \n[8] Berner, J., Richter, L., and Ullrich, K. An optimal control perspective on diffusion-based generative modeling. arXiv preprint arXiv:2211.01364, 2022. [9] Buchner, J. Nested sampling methods. arXiv preprint arXiv:2101.09675, 2021.   \n[10] Burda, Y., Grosse, R. B., and Salakhutdinov, R. Importance weighted autoencoders. International Conference on Learning Representations (ICLR), 2016.   \n[11] Chen, T., Liu, G.-H., and Theodorou, E. A. Likelihood training of Schr\u00f6dinger bridge using forward-backward SDEs theory. International Conference on Learning Representations (ICLR), 2022.   \n[12] Chopin, N. A sequential particle filter method for static models. Biometrika, 89(3):539\u2013552, 2002.   \n[13] Cornish, R., Caterini, A., Deligiannidis, G., and Doucet, A. Relaxing bijectivity constraints with continuously indexed normalising flows. International Conference on Machine Learning (ICML), 2020.   \n[14] De Bortoli, V. Convergence of denoising diffusion models under the manifold hypothesis. Transactions on Machine Learning Research (TMLR), 2022.   \n[15] Del Moral, P., Doucet, A., and Jasra, A. Sequential Monte Carlo samplers. Journal of the Royal Statistical Society Series B: Statistical Methodology, 68(3):411\u2013436, 2006.   \n[16] Deleu, T., G\u00f3is, A., Emezue, C., Rankawat, M., Lacoste-Julien, S., Bauer, S., and Bengio, Y. Bayesian structure learning with generative flow networks. Uncertainty in Artificial Intelligence (UAI), 2022.   \n[17] Deleu, T., Nouri, P., Malkin, N., Precup, D., and Bengio, Y. Discrete probabilistic inference as control in multi-path environments. Uncertainty in Artificial Intelligence (UAI), 2024.   \n[18] Dinh, L., Sohl-Dickstein, J., and Bengio, S. Density estimation using Real NVP. International Conference on Learning Representations (ICLR), 2017.   \n[19] Duane, S., Kennedy, A., Pendleton, B. J., and Roweth, D. Hybrid Monte Carlo. Physics Letters B, 195(2):216\u2013222, 1987.   \n[20] F\u00f6llmer, H. An entropy approach to the time reversal of diffusion processes. pp. 156\u2013163, 1985.   \n[21] Gao, C., Isaacson, J., and Krause, C. i-flow: High-dimensional integration and sampling with normalizing flows. Machine Learning: Science and Technology, 1(4):045023, 2020.   \n[22] Grathwohl, W., Chen, R. T., Bettencourt, J., Sutskever, I., and Duvenaud, D. FFJORD: Freeform continuous dynamics for scalable reversible generative models. International Conference on Learning Representations (ICLR), 2019.   \n[23] Grenander, U. and Miller, M. I. Representations of knowledge in complex systems. Journal of the Royal Statistical Society: Series B (Methodological), 56(4):549\u2013581, 1994.   \n[24] Halton, J. H. Sequential Monte Carlo. In Mathematical Proceedings of the Cambridge Philosophical Society, volume 58, pp. 57\u201378. Cambridge University Press, 1962.   \n[25] Harrison, J., Willes, J., and Snoek, J. Variational Bayesian last layers. International Conference on Learning Representations (ICLR), 2024.   \n[26] Hern\u00e1ndez-Lobato, J. M. and Adams, R. Probabilistic backpropagation for scalable learning of Bayesian neural networks. International Conference on Machine Learning (ICML), 2015.   \n[27] Ho, J., Jain, A., and Abbeel, P. Denoising diffusion probabilistic models. Neural Information Processing Systems (NeurIPS), 2020.   \n[28] Hoffman, M., Sountsov, P., Dillon, J. V., Langmore, I., Tran, D., and Vasudevan, S. NeuTralizing bad geometry in Hamiltonian Monte Carlo using neural transport. arXiv preprint arXiv:1903.03704, 2019.   \n[29] Hoffman, M. D., Blei, D. M., Wang, C., and Paisley, J. W. Stochastic variational inference. Journal of Machine Learning Research (JMLR), 14:1303\u20131347, 2013.   \n[30] Hoffman, M. D., Gelman, A., et al. The No-U-Turn sampler: adaptively setting path lengths in Hamiltonian Monte Carlo. Journal of Machine Learning Research (JMLR), 15(1):1593\u20131623, 2014.   \n[31] Holdijk, L., Du, Y., Hooft, F., Jaini, P., Ensing, B., and Welling, M. Stochastic optimal control for collective variable free sampling of molecular transition paths. Neural Information Processing Systems (NeurIPS), 2023.   \n[32] Hu, E. J., Malkin, N., Jain, M., Everett, K., Graikos, A., and Bengio, Y. GFlowNet-EM for learning compositional latent variable models. International Conference on Machine Learning (ICML), 2023.   \n[33] Hu, E. J., Jain, M., Elmoznino, E., Kaddar, Y., Lajoie, G., Bengio, Y., and Malkin, N. Amortizing intractable inference in large language models. International Conference on Learning Representations (ICLR), 2024.   \n[34] Izmailov, P., Vikram, S., Hoffman, M. D., and Wilson, A. G. What are Bayesian neural network posteriors really like? International Conference on Machine Learning (ICML), 2021.   \n[35] Jain, M., Bengio, E., Hernandez-Garcia, A., Rector-Brooks, J., Dossou, B. F., Ekbote, C. A., Fu, J., Zhang, T., Kilgour, M., Zhang, D., et al. Biological sequence design with gflownets. International Conference on Machine Learning (ICML), 2022.   \n[36] Jang, H., Kim, M., and Ahn, S. Learning energy decompositions for partial inference of GFlowNets. International Conference on Learning Representations (ICLR), 2024.   \n[37] Jing, B., Corso, G., Chang, J., Barzilay, R., and Jaakkola, T. Torsional diffusion for molecular conformer generation. Neural Information Processing Systems (NeurIPS), 2022.   \n[38] Kim, M., Ko, J., Zhang, D., Pan, L., Yun, T., Kim, W., Park, J., and Bengio, Y. Learning to scale logits for temperature-conditional GFlowNets. arXiv preprint arXiv:2310.02823, 2023.   \n[39] Kim, M., Yun, T., Bengio, E., Zhang, D., Bengio, Y., Ahn, S., and Park, J. Local search GFlowNets. International Conference on Learning Representations (ICLR), 2024.   \n[40] Kingma, D. P. and Welling, M. Auto-encoding variational Bayes. International Conference on Learning Representations (ICLR), 2014.   \n[41] Lahlou, S., Deleu, T., Lemos, P., Zhang, D., Volokhova, A., Hern\u00e1ndez-Garc\u0131a, A., Ezzine, L. N., Bengio, Y., and Malkin, N. A theory of continuous generative flow networks. International Conference on Machine Learning (ICML), 2023.   \n[42] Lemos, P., Malkin, N., Handley, W., Bengio, Y., Hezaveh, Y., and Perreault-Levasseur, L. Improving gradient-guided nested sampling for posterior inference. arXiv preprint arXiv:2312.03911, 2023.   \n[43] Madan, K., Rector-Brooks, J., Korablyov, M., Bengio, E., Jain, M., Nica, A., Bosc, T., Bengio, Y., and Malkin, N. Learning GFlowNets from partial episodes for improved convergence and stability. International Conference on Machine Learning (ICML), 2022.   \n[44] Malkin, N., Jain, M., Bengio, E., Sun, C., and Bengio, Y. Trajectory balance: Improved credit assignment in gflownets. Neural Information Processing Systems (NeurIPS), 2022.   \n[45] Malkin, N., Lahlou, S., Deleu, T., Ji, X., Hu, E., Everett, K., Zhang, D., and Bengio, Y. GFlowNets and variational inference. International Conference on Learning Representations (ICLR), 2023.   \n[46] M\u00e1t\u00e9, B. and Fleuret, F. Learning interpolations between Boltzmann densities. Transactions on Machine Learning Research (TMLR), 2023.   \n[47] Mittal, S., Bracher, N. L., Lajoie, G., Jaini, P., and Brubaker, M. A. Exploring exchangeable dataset amortization for bayesian posterior inference. In ICML 2023 Workshop on Structured Probabilistic Inference {\\&} Generative Modeling, 2023.   \n[48] M\u00f8ller, J., Syversveen, A., and Waagepetersen, R. Log Gaussian Cox processes. Scandinavian Journal of Statistics, 25(3):451\u2013482, 1998. ISSN 0303-6898.   \n[49] Nichol, A. and Dhariwal, P. Improved denoising diffusion probabili1stic models. International Conference on Machine Learning (ICML), 2021.   \n[50] Nicoli, K. A., Nakajima, S., Strodthoff, N., Samek, W., M\u00fcller, K.-R., and Kessel, P. Asymptotically unbiased estimation of physical observables with neural samplers. Physical Review E, 101(2):023304, 2020.   \n[51] No\u00e9, F., Olsson, S., K\u00f6hler, J., and Wu, H. Boltzmann generators: Sampling equilibrium states of many-body systems with deep learning. Science, 365(6457):eaaw1147, 2019.   \n[52] N\u00fcsken, N. and Richter, L. Solving high-dimensional Hamilton\u2013Jacobi\u2013Bellman PDEs using neural networks: perspectives from the theory of controlled diffusions and measures on path space. Partial Differential Equations and Applications, 2(4):48, 2021.   \n[53] \u00d8ksendal, B. Stochastic Differential Equations: An Introduction with Applications. Springer, 2003.   \n[54] Pan, L., Malkin, N., Zhang, D., and Bengio, Y. Better training of GFlowNets with local credit and incomplete trajectories. International Conference on Machine Learning (ICML), 2023.   \n[55] Pillai, N. S., Stuart, A. M., and Thi\u00e9ry, A. H. Optimal scaling and diffusion limits for the langevin algorithm in high dimensions. The Annals of Applied Probability, 22(6), December 2012.   \n[56] Radev, S. T., Mertens, U. K., Voss, A., Ardizzone, L., and K\u00f6the, U. Bayesflow: Learning complex stochastic models with invertible neural networks. IEEE transactions on neural networks and learning systems, 33(4):1452\u20131466, 2020.   \n[57] Ranganath, R., Gerrish, S., and Blei, D. Black box variational inference. Artificial Intelligence and Statistics (AISTATS), 2014.   \n[58] Rector-Brooks, J., Madan, K., Jain, M., Korablyov, M., Liu, C.-H., Chandar, S., Malkin, N., and Bengio, Y. Thompson sampling for improved exploration in GFlowNets. arXiv preprint arXiv:2306.17693, 2023.   \n[59] Rezende, D. and Mohamed, S. Variational inference with normalizing flows. International Conference on Machine Learning (ICML), 2015.   \n[60] Rezende, D. J., Mohamed, S., and Wierstra, D. Stochastic backpropagation and approximate inference in deep generative models. International Conference on Machine Learning (ICML), 2014.   \n[61] Richter, L., Boustati, A., N\u00fcsken, N., Ruiz, F. J. R., and \u00d6mer Deniz Akyildiz. VarGrad: A low-variance gradient estimator for variational inference. Neural Information Processing Systems (NeurIPS), 2020.   \n[62] Richter, L., Berner, J., and Liu, G.-H. Improved sampling via learned diffusions. International Conference on Learning Representations (ICLR), 2023.   \n[63] Roberts, G. O. and Rosenthal, J. S. Optimal scaling of discrete approximations to langevin diffusions. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 60(1): 255\u2013268, 1998.   \n[64] Roberts, G. O. and Tweedie, R. L. Exponential convergence of Langevin distributions and their discrete approximations. Bernoulli, pp. 341\u2013363, 1996.   \n[65] Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. High-resolution image synthesis with latent diffusion models. Conference on Computer Vision and Pattern Recognition (CVPR), 2021.   \n[66] S\u00e4rkk\u00e4, S. and Solin, A. Applied stochastic differential equations. Cambridge University Press, 2019.   \n[67] Shen, M. W., Bengio, E., Hajiramezanali, E., Loukas, A., Cho, K., and Biancalani, T. Towards understanding and improving GFlowNet training. International Conference on Machine Learning (ICML), 2023.   \n[68] Skilling, J. Nested sampling for general Bayesian computation. Bayesian Analysis, 1(4):833 \u2013 859, 2006. doi: 10.1214/06-BA127. URL https://doi.org/10.1214/06-BA127.   \n[69] Sohl-Dickstein, J., Weiss, E. A., Maheswaranathan, N., and Ganguli, S. Deep unsupervised learning using nonequilibrium thermodynamics. International Conference on Machine Learning (ICML), 2015.   \n[70] Song, Y., Durkan, C., Murray, I., and Ermon, S. Maximum likelihood training of score-based diffusion models. Neural Information Processing Systems (NeurIPS), 2021.   \n[71] Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., and Poole, B. Score-based generative modeling through stochastic differential equations. International Conference on Learning Representations (ICLR), 2021.   \n[72] Tiapkin, D., Morozov, N., Naumov, A., and Vetrov, D. Generative flow networks as entropyregularized RL. arXiv preprint arXiv:2310.12934, 2023.   \n[73] Tripp, A., Daxberger, E., and Hern\u00e1ndez-Lobato, J. M. Sample-efficient optimization in the latent space of deep generative models via weighted retraining. Neural Information Processing Systems (NeurIPS), 2020.   \n[74] Tzen, B. and Raginsky, M. Neural stochastic differential equations: Deep latent Gaussian models in the diffusion limit. arXiv preprint arXiv:1905.09883, 2019.   \n[75] Tzen, B. and Raginsky, M. Theoretical guarantees for sampling and inference in generative models with latent diffusions. Conference on Learning Theory (CoLT), 2019.   \n[76] van Krieken, E., Thanapalasingam, T., Tomczak, J., van Harmelen, F., and ten Teije, A. A-NeSI: A scalable approximate method for probabilistic neurosymbolic inference. Neural Information Processing Systems (NeurIPS), 2023.   \n[77] Vargas, F., Grathwohl, W., and Doucet, A. Denoising diffusion samplers. International Conference on Learning Representations (ICLR), 2023.   \n[78] Vargas, F., Padhy, S., Blessing, D., and N\u00fcsken, N. Transport meets variational inference: Controlled Monte Carlo diffusions. International Conference on Learning Representations (ICLR), 2024.   \n[79] Vincent, P. A connection between score matching and denoising autoencoders. Neural computation, 23(7):1661\u20131674, 2011.   \n[80] Wu, H., K\u00f6hler, J., and No\u00e9, F. Stochastic normalizing flows. Neural Information Processing Systems (NeurIPS), 2020.   \n[81] Xiao, Z., Kreis, K., and Vahdat, A. Tackling the generative learning trilemma with denoising diffusion GANs. International Conference on Leraning Representations (ICLR), 2022.   \n[82] Zhang, D., Malkin, N., Liu, Z., Volokhova, A., Courville, A., and Bengio, Y. Generative flow networks for discrete probabilistic modeling. International Conference on Machine Learning (ICML), 2022.   \n[83] Zhang, D., Chen, R. T. Q., Malkin, N., and Bengio, Y. Unifying generative models with GFlowNets and beyond. arXiv preprint arXiv:2209.02606, 2023.   \n[84] Zhang, D., Rainone, C., Peschl, M., and Bondesan, R. Robust scheduling with GFlowNets. International Conference on Learning Representations (ICLR), 2023.   \n[85] Zhang, D., Chen, R. T. Q., Liu, C.-H., Courville, A., and Bengio, Y. Diffusion generative flow samplers: Improving learning signals through partial trajectory optimization. International Conference on Learning Representations (ICLR), 2024.   \n[86] Zhang, Q. and Chen, Y. Diffusion normalizing flow. Neural Information Processing Systems (NeurIPS), 2021.   \n[87] Zhang, Q. and Chen, Y. Path integral sampler: a stochastic control approach for sampling. International Conference on Learning Representations (ICLR), 2022.   \n[88] Zhu, Y., Wu, J., Hu, C., Yan, J., Hsieh, C.-Y., Hou, T., and Wu, J. Sample-efficient multiobjective molecular optimization with GFlowNets. Neural Information Processing Systems (NeurIPS), 2023.   \n[89] Zimmermann, H., Lindsten, F., van de Meent, J.-W., and Naesseth, C. A. A variational perspective on generative flow networks. Transactions on Machine Learning Research (TMLR), 2023. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "A Code and hyperparameters ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Code is available at https://github.com/GFNOrg/gfn-diffusion and will continue to be maintained and extended. ", "page_idx": 15}, {"type": "text", "text": "Below are commands to reproduce some of the results on Manywell and VAE with PIS and GFlowNet models as an example, showing the hyperparameters: ", "page_idx": 15}, {"type": "text", "text": "PIS: ", "page_idx": 15}, {"type": "text", "text": "--mode_fwd pis --lr_policy 1e-3", "page_idx": 15}, {"type": "text", "text": "PIS $^+$ Langevin: ", "page_idx": 15}, {"type": "text", "text": "--mode_fwd pis --lr_policy 1e-3 --langevin ", "page_idx": 15}, {"type": "text", "text": "GFlowNet TB: ", "page_idx": 15}, {"type": "text", "text": "python train.py  \n--t_scale 1. --energy many_well --pis_architectures --zero_init --clipping  \n--mode_fwd tb --lr_policy 1e-3 --lr_flow 1e-1", "page_idx": 15}, {"type": "text", "text": "GFlowNet TB $^+$ Expl.: ", "page_idx": 15}, {"type": "text", "text": "python train.py  \n--t_scale 1. --energy many_well --pis_architectures --zero_init --clipping  \n--mode_fwd tb --lr_policy 1e-3 --lr_flow 1e-1  \n--exploratory --exploration_wd --exploration_factor 0.2", "page_idx": 15}, {"type": "text", "text": "GFlowNet VarGrad $^+$ Expl.: ", "page_idx": 15}, {"type": "text", "text": "python train.py  \n--t_scale 1. --energy many_well --pis_architectures --zero_init --clipping  \n--mode_fwd tb-avg --lr_policy 1e-3 --lr_flow 1e-1  \n--exploratory --exploration_wd --exploration_factor 0.2", "page_idx": 15}, {"type": "text", "text": "GFlowNet FL-SubTB: ", "page_idx": 15}, {"type": "text", "text": "python train.py  \n--t_scale 1. --energy many_well --pis_architectures --zero_init --clipping  \n--mode_fwd subtb --lr_policy 1e-3 --lr_flow 1e-2  \n--partial_energy --conditional_flow_model", "page_idx": 15}, {"type": "text", "text": "GFlowNet FL-SubTB $^+$ LP: ", "page_idx": 15}, {"type": "text", "text": "python train.py  \n--t_scale 1. --energy many_well --pis_architectures --zero_init --clipping  \n--mode_fwd subtb --lr_policy 1e-3 --lr_flow 1e-2  \n--partial_energy --conditional_flow_model  \n--langevin --epochs 10000", "page_idx": 15}, {"type": "text", "text": "GFlowNet TB $^+$ Expl. $+\\,\\mathrm{L}S$ : ", "page_idx": 15}, {"type": "text", "text": "python train.py  \n--t_scale 1. --energy many_well --pis_architectures --zero_init --clipping  \n--mode_fwd tb --lr_policy 1e-3 --lr_back 1e-3 --lr_flow 1e-1  \n--exploratory --exploration_wd --exploration_factor 0.1  \n--both_ways --local_search  \n--buffer_size 600000 --prioritized rank --rank_weight 0.01  \n--ld_step 0.1 --ld_schedule --target_acceptance_rate 0.574", "page_idx": 15}, {"type": "text", "text": "GFlowNet TB $^+$ Expl. $+\\,\\mathrm{LP}$ : ", "page_idx": 15}, {"type": "text", "text": "python train.py  \n--t_scale 1. --energy many_well --pis_architectures --zero_init --clipping  \n--mode_fwd tb --lr_policy 1e-3 --lr_flow 1e-1  \n--exploratory --exploration_wd --exploration_factor 0.2  \n--langevin --epochs 10000", "page_idx": 16}, {"type": "text", "text": "GFlowNet TB $^+$ Expl. $^+$ LS (VAE):", "page_idx": 16}, {"type": "text", "text": "python train.py  \n--energy vae --pis_architectures --zero_init --clipping  \n--mode_fwd cond-tb-avg --mode_bwd cond-tb-avg --repeats 5  \n--lr_policy 1e-3 --lr_flow 1e-1 --lr_back 1e-3  \n--exploratory --exploration_wd --exploration_factor 0.1  \n--both_ways --local_search  \n--max_iter_ls 500 --burn_in 200  \n--buffer_size 90000 --prioritized rank --rank_weight 0.01  \n--ld_step 0.001 --ld_schedule --target_acceptance_rate 0.574", "page_idx": 16}, {"type": "text", "text": "GFlowNet TB $^+$ Expl. $+\\,\\mathrm{LP}+\\mathrm{LS}$ (VAE): ", "page_idx": 16}, {"type": "text", "text": "python train.py  \n--energy vae --pis_architectures --zero_init --clipping  \n--mode_fwd cond-tb-avg --mode_bwd cond-tb-avg --repeats 5  \n--lr_policy 1e-3 --lr_flow 1e-1  \n--lgv_clip 1e2 --gfn_clip 1e4 --epochs 10000  \n--exploratory --exploration_wd --exploration_factor 0.1  \n--both_ways --local_search  \n--lr_back 1e-3 --max_iter_ls 500 --burn_in 200  \n--buffer_size 90000 --prioritized rank --rank_weight 0.01  \n--langevin  \n--ld_step 0.001 --ld_schedule --target_acceptance_rate 0.574", "page_idx": 16}, {"type": "text", "text": "B Target densities ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Gaussian Mixture Model with 25 modes (25GMM). The model, termed as 25GMM, consists of a two-dimensional Gaussian mixture model with 25 distinct modes. Each mode exhibits an identical variance of 0.3. The centers of these modes are strategically positioned on a grid formed by the Cartesian product $\\{-10,-5,0,5,10\\}\\times\\{-10,-5,0,5,\\bar{10}\\}$ , effectively distributing them across the coordinate space. ", "page_idx": 17}, {"type": "text", "text": "Funnel [28]. The funnel represents a classical benchmark in sampling techniques, characterized by a ten-dimensional distribution defined as follows: The first dimension, $x_{0}$ , follows a normal distribution with mean 0 and variance 9, denoted as $x_{0}\\sim{\\cal N}(0,9)$ . Conditional on $x_{0}$ , the remaining dimensions, $x_{1:9}$ , are distributed according to a multivariate normal distribution with mean vector 0 and a covariance matrix $\\exp(x_{0})\\mathbf{I}$ , where I is the identity matrix. This is succinctly represented as $x_{1:9}\\mid x_{0}\\sim N\\left(\\mathbf{0},\\exp\\left(x_{0}\\right)\\mathbf{I}\\right)$ . ", "page_idx": 17}, {"type": "text", "text": "Manywell [51]. The manywell is characterized by a 32-dimensional distribution, which is constructed as the product of 16 identical two-dimensional double well distributions. Each of these two-dimensional components is defined by a potential function, $\\mu(x_{1},x_{2})$ , expressed as $\\mu(x_{1},x_{2})=\\exp\\left(-x_{1}^{4}+6x_{1}^{2}+\\mathbf{\\bar{0}}.5x_{1}-0.5x_{2}^{2}\\right)$ . ", "page_idx": 17}, {"type": "text", "text": "VAE [40]. This task involves sampling from a 20-dimensional latent posterior $p(z|x)\\propto p(z)p(x|z)$ , where $p(z)$ is a fixed prior and $p(x|z)$ is a pretrained VAE decoder, using a conditional sampler $q(z|x)$ dependent on input data (image) $x$ . ", "page_idx": 17}, {"type": "text", "text": "LGCP [48]. This density over a 1600-dimensional variable is a Log-Gaussian Cox process fit to a distribution of pine saplings in Finland. ", "page_idx": 17}, {"type": "text", "text": "B.1 Discrepancies in past work ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Wrong definitions of the Funnel density. As already noted by [77], [87] uses a different variance of the first component in the Funnel density, 1 instead of 9. This apparent bug in the task definition has been propagated to subsequent work, including [41]. ", "page_idx": 17}, {"type": "text", "text": "Evaluation on LGCP. The LGCP benchmark suffers from the lack of a consistent ground truth $\\log{Z}$ to compare against. Previous work has compared the value of the partition function $\\log{Z}$ against a \u201clong run of Sequential Monte Carlo\u201d [87]. We note that this approach produces noisy estimates of the partition function, especially in high-dimensional problems (indeed, SMC has rarely been used in problems with over a thousand dimensions); therefore, it is unclear how long the SMC needs to be run to produce an accurate estimate. We found that two different values are being used in the literature: $\\log Z=512.6$ in one repository and $\\log Z=501.8$ in another. ", "page_idx": 17}, {"type": "text", "text": "On FL-SubTB as used in [85]. We make two observations calling into question the main results of [85]. ", "page_idx": 17}, {"type": "text", "text": "First, the only substantial difference between the algorithm used by [85] and the one from the past work [41] \u2013 which first proposed the use of GFlowNet objectives to train diffusion samplers \u2013 is the substitution of the FL-SubTB objective [54, 43] for TB [44]. However, [85] elects to compare FL-SubTB with the Langevin parameterization to TB without the Langevin parameterization. Our results in Table 1 show that while the Langevin parameterization is crucial for the performance of all objectives; FL-SubTB does not provide any consistent benefit over TB or VarGrad. ", "page_idx": 17}, {"type": "text", "text": "Second, the results are not reproducible, neither with the published code from [85] run \u2018out of the box\u2019, nor with our reimplementation. In particular, on the LGCP density, the training did not converge within the allotted training time. We have contacted the authors of [85], who confirmed that running their published code does not reproduce the results in the paper but could not provide any further explanation or a working implementation. ", "page_idx": 17}, {"type": "text", "text": "C Additional results ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "C.1 Expanded unconditional sampling results ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Table C.1 is an expanded version of Table 1, showing Wasserstein distances between sets of $K$ samples from the true distribution and generated by a trained sampler. (Note that ground truth for LGCP is not available.) ", "page_idx": 17}, {"type": "table", "img_path": "vieIamY2Gi/tmp/c3d16e1662e185a03079f52d05cdb81a18b7013cc41018e849f1f54c5d9daacc.jpg", "table_caption": ["Table C.1: Log-partition function estimation errors and 2-Wasserstein distances for unconditional modeling tasks (mean and standard deviation over 5 runs). The four groups of models are: MCMCbased samplers, simulation-driven variational methods, baseline GFlowNet methods with different learning objectives, and methods augmented with Langevin parametrization and local search. "], "table_footnote": ["Highlight $\\boldsymbol{:}$ mean indistinguishable from minimum in column with $p<0.05$ under one-sided Welch unpaired \ud835\udc61-test. "], "page_idx": 18}, {"type": "text", "text": "Table C.2: Log-partition function estimation errors and empirical 2-Wasserstein distances on the 32-dimensional Manywell with Brownian and variance-preserving noising processes. ", "page_idx": 18}, {"type": "table", "img_path": "vieIamY2Gi/tmp/df8b98a094819e40fad69492172d46aaed859f172d7e2e95eea3ffe9e1a9757a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "C.2 Variance-preserving noising process ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Following the recent results by [8, 62, 77], we perform an additional set of experiments with a different successful noise schedule. We replace the Brownian motion by the variance-preserving SDEs from Song et al. [71], given by an Ornstein-Uhlenbeck process: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\sigma(t):=\\nu\\sqrt{2\\beta(t)}\\mathbf{I}\\quad\\mathrm{and}\\quad\\mu(x,t):=-\\beta(t)x\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "with $\\nu\\in(0,\\infty)$ . ", "page_idx": 18}, {"type": "text", "text": "In particular, we follow the common procedure - use $\\nu:=1$ and ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\beta(t):=(1-t)\\beta_{m i n}+t\\beta_{m a x},\\quad t\\in[0,1],\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "with $\\beta_{m i n}=0.01$ and $\\beta_{m a x}=4.0$ . ", "page_idx": 18}, {"type": "text", "text": "We evaluate three representative methods using this variance-preserving backward process. The results, in Table C.2, are similar to those using the Brownian bridge process. We expect that the choice of noising process gains importance in challenging high-dimensional problems. ", "page_idx": 18}, {"type": "text", "text": "C.3 Scalability study ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "The Manywell energy $(\\S\\mathrm{B})$ is defined in any even number of dimensions and thus allows to study the scaling of the methods with dimension. We evaluate several representative methods in dimension 8, 128, and 512 (in addition to the 32 studied in the main text). All experimental settings are kept the same as as for $d=32$ . Due to the large runtime, some runs in dimensions 128 and 512 had to be limited at 12 hours, while in dimensions 8 and 32 all run in under 3 hours on a RTX8000 GPU. ", "page_idx": 18}, {"type": "text", "text": "These results are shown in Table C.3. We observe: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The overhead of the Langevin parametrization grows with dimension, but is critical to performance.   \n\u2022 The even higher overhead of FL-SubTB as used by [85].   \n\u2022 The relatively high efficiency and low overhead of our newly proposed local search. ", "page_idx": 18}, {"type": "table", "img_path": "vieIamY2Gi/tmp/33bae7cfc19d30d3fbd7cf1b607a1335238d962aa5075251c3052989dcc3b80c.jpg", "table_caption": ["Table C.3: Scaling with dimension on Manywell: log-partition function estimation errors and time per training iteration on a RTX8000 GPU. "], "table_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "", "img_caption": [], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "D Experiment details ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Sampling energies. In this section, we detail the hyperparameters used for our experiments. An important parameter is the diffusion coefficient of the forward policy, which is denoted by $\\sigma$ and also used in the definition of the fixed backward process. The base diffusion rate $\\sigma^{2}$ (parameter t_scale) is set to 5 for 25GMM and 1 for Funnel and Manywell, consistent with past work. ", "page_idx": 19}, {"type": "text", "text": "For LGCP, we found that using too small diffusion rate $\\sigma^{2}$ (e.g., $\\sigma^{2}=1$ ) prevents the methods from achieving reasonable results. We tested different values of $\\sigma^{2}=\\{1,3,5\\}$ , and selected $\\sigma^{2}=5$ , which gives the best results, which follows the findings in Zhang & Chen [87]. ", "page_idx": 19}, {"type": "text", "text": "For all our experiments, we used a learning rate of $10^{-3}$ . Additionally, we used a higher learning rate for learning the flow parameterization, which is set as $10^{-1}$ when using the TB loss and $10^{-2}$ with the SubTB loss. These settings were found to be consistently stable (unlike those with higher learning rates) and converge within the allotted number of steps (unlike those with lower learning rates). ", "page_idx": 19}, {"type": "text", "text": "For the SubTB loss, we experimented with the settings of $10\\times$ lower learning rates for both flow and policy models communicated by the authors of [85], but found the results to be inferior both using their published code (and other unstated hyperparameters communicated by the authors) and using our reimplementation. ", "page_idx": 19}, {"type": "text", "text": "For models with exploration, we use an exploration factor of 0.2 (that is, noise with a variance of 0.2 is added to the policy when sampling trajectories for training), which decays linearly over the first half of training, consistent with [41]. ", "page_idx": 19}, {"type": "text", "text": "We train all our models for 25, 000 iterations except those using Langevin dynamics, which are trained for 10, 000 iterations. This results in approximately equal computation time owing to the overhead from computation of the score at each sampling step. ", "page_idx": 19}, {"type": "text", "text": "We use the same neural network architecture for the GFlowNet as one of our baselines [87]. Similar to [87], we also use an initialization scheme with last-layer weights set to 0 at the start of training. Since the SubTB requires the flow function to be conditioned on the current state $\\mathbf{X}_{t}$ and time $t$ , we follow [85] and parametrize the flow model with the same architecture as the Langevin scaling model $\\mathrm{NN}_{2}$ in [87]. Additionally, we perform clipping on the output of the network as well as the score obtained from the energy function, typically setting the clipping parameter of Langevin scaling model to $10^{2}$ and policy network to $10^{4}$ , similarly to [77]: ", "page_idx": 19}, {"type": "equation", "text": "$$\nf_{\\theta}(k,x)\\!=\\!\\!\\operatorname{clip}\\!\\Big(\\!\\operatorname{NN}_{1}(k,x;\\theta)+\\operatorname{NN}_{2}(k;\\theta)\\odot\\operatorname{clip}\\!\\big(\\nabla\\ln\\pi(x),-10^{2},10^{2}\\big),-10^{4},10^{4}\\Big).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "All models were trained with a batch size of 300. In each experiment, we train models on a single NVIDIA A100-Large GPU, if not stated explicitly otherwise. ", "page_idx": 19}, {"type": "text", "text": "VAE experiment. In the VAE experiment, we used a standard VAE model pretrained for 100 epochs on the MNIST dataset. The encoder $q(z|x)$ contains an input linear layer (784 neurons) followed by hidden linear layer (400 neurons), ReLU activation function, and two linear heads (20 neurons each) whose outputs were reparametrized to be means and scales of multivariate Normal distribution. The decoder consists of 20-dimensional input, one hidden layer (400 neurons), followed by the ReLU activation, and 784-dimensional output. The output is processed by the sigmoid function to be scaled properly into [0, 1]. ", "page_idx": 19}, {"type": "text", "text": "The goal is to sample conditionally on $x$ the latent $z$ from the unnormalized density $p(z,x)\\;=\\;$ $p(z)\\bar{p}(x\\mid z)$ (where $p(z)$ is the prior and $p(x|z)$ is the likelihood computed from the decoder), which is proportional to the posterior $p(z\\mid x)$ . We reuse the model architectures from the unconditional sampling experiments, but also provide $x$ as an input to the first layer of the models expressing the policy drift (as well as the flow, for FL-SubTB) and add one hidden layer to process high-dimensional conditions. For models trained with TB, $\\log{Z_{\\theta}}$ also becomes a MLP taking $x$ as input. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "The VarGrad and LS techniques require adaptations in the conditional setting. For LS, buffers ${\\mathcal{D}}_{\\mathrm{buffer}}$ and $\\mathcal{D}_{\\mathrm{LS}}$ ) must store the associated conditions $x$ together with the samples $z$ and the corresponding unnormalized density $R(z;x)$ , i.e., a tuple of $(x,z,R(z;x))$ . For VarGrad, because the partition function depends on the conditioning information $x$ , it is necessary to compute variance over many trajectories sharing the same condition. We choose to sample 10 trajectories for each condition occurring in a minibatch and compute the VarGrad loss for each such set of 10 trajectories. ", "page_idx": 20}, {"type": "text", "text": "The VAE model was trained on the entire MNIST training set and never updated on the test part of MNIST. In order to evaluate samplers (with respect to the variational lower bound) on a unique set of examples, we chose the first 100 elements of MNIST test data. All of the samplers were trained having access to the MNIST training data and the frozen VAE decoder. For a fair comparison, samplers utilizing the LP were trained for 10, 000, whereas the remaining for 25, 000 iterations. In each iteration, a batch of 300 examples from MNIST was given as conditions. In each experiment, we train models on a single NVIDIA A100-Large GPU, if not stated explicitly otherwise. ", "page_idx": 20}, {"type": "image", "img_path": "vieIamY2Gi/tmp/c7eb24ead3e6281f4cb3a380ae30d1b611e5499be7392b56756a5d8e5c762231.jpg", "img_caption": [], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "(a) Conditioning data (MNIST test(b) VarGrad $^+$ Expl. $+\\,\\mathrm{LP}$ samples (c) VAE reconstruction   \nset) decoded ", "page_idx": 20}, {"type": "text", "text": "Figure D.1: Our sampler $\\mathrm{(VarGrad+Expl.\\+LP)}$ is conditioned by a subset of never-seen data coming from the ground truth distribution (left). The conditional samples were then decoded by the the fixed VAE (middle). For the comparison, we show the reconstruction of the real data by VAE (right). We observed that the decoded samples are visually very similar to the reconstructions making these two pictures almost indistinguishable. Both, decoded samples and reconstruction, are more blurry than the ground truth data, which is caused by a limited capacity of the VAE\u2019s latent space. ", "page_idx": 20}, {"type": "text", "text": "E Local search-guided GFlowNet ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Prioritized sampling scheme. We can use uniform or prioritized sampling to draw samples from the buffer for training. We found prioritized sampling to work slightly better in our experiments (see ablation study in $\\S E.2\\$ ), although the choice should be investigated more thoroughly in future work. ", "page_idx": 21}, {"type": "text", "text": "We use rank-based prioritization [73], which follows a probabilistic approach defined as: ", "page_idx": 21}, {"type": "equation", "text": "$$\np(\\mathbf{x};\\mathcal{D}_{\\mathrm{buffer}})\\propto\\left(k|\\mathcal{D}_{\\mathrm{buffer}}|+\\mathrm{rank}_{\\mathcal{D}_{\\mathrm{buffer}}}(\\mathbf{x})\\right)^{-1},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where rank $\\mathcal{D}_{\\mathrm{buffer}}(\\mathbf{x})$ represents the relative rank of a sample $x$ based on a ranking function $R(\\mathbf{x})$ (in our case, the unnormalized target density at sample $\\mathbf{x}$ ). The parameter $k$ is a hyperparameter for prioritization, where a lower value of $k$ assigns a higher probability to samples with higher ranks, thereby introducing a more greedy selection approach. We set $k=0.01$ for every task. Given that the sampling is proportional to the size of ${\\mathcal{D}}_{\\mathrm{buffer}}$ , we impose a constraint on the maximum size of the buffer: $\\lvert\\mathcal{D}_{\\mathrm{buffer}}\\rvert=600,000$ with first-in first out (FIFO) data structure for every task, except we use $\\lvert\\mathcal{D}_{\\mathrm{buffer}}\\rvert=90,00$ 00 for VAE task. See the algorithm below for a detailed pseudocode. ", "page_idx": 21}, {"type": "table", "img_path": "vieIamY2Gi/tmp/d9e97019cfa7593481b8efe86bbe5145864a731252a3bbddcb68e19beacb0e4d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "We use the number of total iterations $I=25,000$ for every task as default. Note as local search is performed to update $\\mathcal{D}_{\\mathrm{LS}}$ occasionally that per 100 iterations, the number of local search updates is done $25,000/100=250$ . ", "page_idx": 21}, {"type": "text", "text": "E.1 Local search algorithm ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "This section describes a detailed algorithm for local search, which provides an updated buffer $\\mathcal{D}_{\\mathrm{LS}}$ , which contains low-energy samples. ", "page_idx": 22}, {"type": "text", "text": "Dynamic adjustment of step size $\\eta$ . To enhance local search using parallel MALA, we dynamically select the Langevin step size $(\\eta)$ , which governs the MH acceptance rate. Our objective is to attain an average acceptance rate of 0.574, which is theoretically optimal for high-dimensional MALA\u2019s efficiency [55]. While the user can customize the target acceptance rate, the adaptive approach eliminates the need for manual tuning. ", "page_idx": 22}, {"type": "text", "text": "Computational cost of local search. The computational cost of local search is not significant. Local search for iteration of $K=200$ requires 6.04 seconds (averaged with five trials in Manywell), where we only occasionally (every 100 iterations) update $\\mathcal{D}_{\\mathrm{LS}}$ with MALA. The speed is evaluated using the computational resources of the Intel Xeon Scalable Gold 6338 CPU (2.00GHz) and the NVIDIA RTX 4090 GPU. ", "page_idx": 22}, {"type": "text", "text": "Algorithm 2 Local search (Parallel MALA) ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "inp\ud835\udf02ut Initial states $\\{x_{1}^{(0)},\\ldots,x_{M}^{(0)}\\}$ , current buf f\ud835\udc53er $\\mathcal{D}_{\\mathrm{LS}}$ , total steps $K$ , burn in steps $K_{\\mathrm{burn-in}}$ , initial step size , amplifying factor $f_{\\mathrm{increase}}$ , damping factor $f_{\\mathrm{decrease}}$ , unnormalized target density $R$   \noutput Updated buffer $\\mathcal{D}_{\\mathrm{LS}}$ Initialize acceptance counter $a=0$ Set $\\eta\\leftarrow\\eta_{0}$ for $k=1:K$ do Initialize step acceptance count $a_{k}=0$ for $m=1:M$ do Sample $\\sigma\\sim{\\cal N}(0,I)$ Propose $\\boldsymbol{x}_{m}^{*}\\gets\\boldsymbol{x}_{m}^{(k-1)}+\\eta\\nabla\\log R(\\boldsymbol{x}_{m}^{(k-1)})+\\sqrt{2\\eta}\\sigma$ Compute acceptance ratio $\\begin{array}{r l}&{r\\gets\\operatorname*{min}\\left(1,\\frac{R(x_{m}^{*})\\exp\\left(-\\frac{1}{4\\eta}\\|x_{m}^{(k-1)}-x_{m}^{*}-\\eta\\nabla\\log R(x_{m}^{*})\\|^{2}\\right)}{R(x_{m}^{(k-1)})\\exp\\left(-\\frac{1}{4\\eta}\\|x_{m}^{*}-x_{m}^{(k-1)}-\\eta\\nabla\\log R(x_{m}^{(k-1)})\\|^{2}\\right)}\\right)}\\end{array}$ With probability $r$ , accept the proposal: \ud835\udc65(\ud835\udc5a\ud835\udc58)\u2190\ud835\udc65\u2217\ud835\udc5aand increment \ud835\udc4e\ud835\udc58\u2190\ud835\udc4e\ud835\udc58+ 1 if $k>K_{\\mathrm{burn-in}}$ then Update buffer: $\\mathcal{D}_{\\mathrm{LS}}\\leftarrow\\mathcal{D}_{\\mathrm{LS}}\\cup\\{x_{m}^{*}\\}$ end if end for Compute step acceptance rate $\\alpha_{k}=a_{k}/M$ if $\\alpha_{k}>\\alpha_{\\mathrm{target}}$ then $\\eta\\leftarrow\\eta\\times\\bar{f}$ increase else if $\\alpha_{k}<\\alpha$ target then $\\eta\\leftarrow\\eta\\times f_{0}$ decrease end if end for ", "page_idx": 22}, {"type": "text", "text": "We adopt default parameters: $f_{\\mathrm{increase}}=1.1$ , $f_{\\mathrm{decrease}}=0.9$ , $\\eta_{0}=0.01$ , $K=200$ , $K_{\\mathrm{burn-in}}=100$ , and $\\alpha_{\\mathrm{target}}=0.574$ for three unconditional tasks. For conditional tasks of VAE, we give more iterations of local search: $K=500$ , $K_{\\mathrm{burn-in}}=200$ . ", "page_idx": 22}, {"type": "text", "text": "It is noteworthy that by adjusting the inverse temperature $\\beta$ into $R^{\\beta}$ during the computation of the Metropolis-Hastings acceptance ratio $r$ , we can facilitate a greedier local search strategy aimed at exploring samples with lower energy (i.e., higher density $p_{\\mathrm{target}})$ . This approach proves advantageous for navigating high-dimensional and steep landscapes, which are typically challenging for locating low-energy samples. For unconditional tasks, we set $\\beta=1$ . ", "page_idx": 22}, {"type": "text", "text": "In the context of the VAE task (Table 2), we utilize two GFlowNet loss functions: TB and VarGrad. For local search within TB, we set $\\beta=1$ , while for VarGrad, we employ $\\beta=5$ . As illustrated in Table 2, employing a local search with $\\beta=1$ fails to enhance the performance of the TB method. Conversely, a local search with $\\beta=5$ results in improvements at the $\\log{\\hat{Z}^{\\mathrm{RW}}}$ metric over the VarGrad $+\\;\\mathrm{Expl.}+\\mathrm{LP},$ even though the performance of $\\mathrm{VarGrad}+\\mathrm{Expl.}+\\mathrm{LP}$ surpasses that of TB substantially. This underscores the importance of selecting an appropriate $_\\beta$ value, which is critical for optimizing the exploration-exploitation balance depending on the target objectives. ", "page_idx": 22}, {"type": "text", "text": "E.2 Ablation study for local search-guided GFlowNets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Increasing capacity of buffer. The capacity of the replay buffer influences the duration for which it retains past experiences, enabling it to replay these experiences to the policy. This mechanism helps in preventing mode collapse during training. Table E.1 demonstrates that enhancing the buffer\u2019s capacity leads to improved sampling quality. Furthermore, Figure 1 illustrates that increasing the buffer\u2019s capacity\u2014thereby encouraging the model to recall past low-energy experiences\u2014enhances its mode-seeking capability. ", "page_idx": 23}, {"type": "text", "text": "Table E.1: Comparison of the sampling quality of each sampler trained with varying replay buffer capacities in Manywell. Five independent runs have been conducted, with both the mean and standard deviation reported. ", "page_idx": 23}, {"type": "table", "img_path": "vieIamY2Gi/tmp/c97c47efa0179519cb5ac0e4b5d297fc9dc2635e48adf922e1860ba8c091cd87.jpg", "table_caption": [], "table_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "vieIamY2Gi/tmp/f9c5f282681d14e93471c2a0ca78408b96c99369d18a14f3241ac1a36671c2fb.jpg", "img_caption": ["Figure E.1: Illustration of each sampler trained with varying capacities of replay buffers, depicting 2,000 samples. As the capacity of the buffer increases, the number of modes captured by the sampler also increases. "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "Benefit of prioritization. ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Rank-prioritized sampling gives faster convergence compared with no prioritization (uniform sampling), as shown in Fig. E.2a. ", "page_idx": 23}, {"type": "text", "text": "Dynamic adjustment of $\\eta$ vs. fixed $\\eta\\,=\\,0.01$ . As shown in Fig. E.2b, dynamic adjustment to target acceptance rate $\\alpha_{\\mathrm{target}}=0.574$ gives better performances than fixed Langevin step size of $\\eta$ showcasing the effectiveness of the dynamic adjustment. ", "page_idx": 23}, {"type": "image", "img_path": "vieIamY2Gi/tmp/e8c56393a064f84d86a21adf733cae29f99960e4c446b39b1114ed69b37c600f.jpg", "img_caption": ["(a) Benefit of prioritization in replay buffer sampling.. "], "img_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "vieIamY2Gi/tmp/0e8afc794c14e2dde2c79c22712d205661ad48c4991a28ca4f4e4c9d265ea5b8.jpg", "img_caption": ["(b) Benefit of scheduling $\\eta$ dynamically. "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "Figure E.2: Ablation study for prioritized replay buffer and step size $\\eta$ scheduling of local search.   \nMean and standard deviation are plotted based on five independent runs. ", "page_idx": 23}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: See theory and experiment sections. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 24}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: Yes, see section 5.3 and conclusion, as well as references to appendix material where relevant. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 24}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: No new theoretical results. For exposition of the mathematical basis for our algorithms, we state the assumptions. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 25}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: See experiment sections and references to appendix material. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 25}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We provide code to reproduce nearly all of our experimental results. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 26}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: See the experiment sections and references to appendix material. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 26}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: All results tables and plots show standard deviation and indicate significance of the best metric. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: See experiment sections and references to appendix material. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 27}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We believe there are no violations of the CoE. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 27}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: The paper studies a ML problem with no immediate societal impacts. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: The paper studies a ML problem with no immediate application to generation of new image or text content, nor other functions that have the potential for misuse, to the best of our knowledge. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 28}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: We cite the works introducing all datasets we study. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA]   \nJustification: No new assets. Guidelines:   \n\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] Justification: No human studies. ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 29}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] Justification: No human studies. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 29}]