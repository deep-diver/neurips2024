[{"type": "text", "text": "Non-geodesically-convex optimization in the Wasserstein space ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Hoang Phuc Hau Luu Hanlin Yu Bernardo Williams Petrus Mikkola Marcelo Hartmann Kai Puolam\u00e4ki Arto Klami ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Department of Computer Science, University of Helsinki ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We study a class of optimization problems in the Wasserstein space (the space of probability measures) where the objective function is nonconvex along generalized geodesics. Specifically, the objective exhibits some difference-of-convex structure along these geodesics. The setting also encompasses sampling problems where the logarithm of the target distribution is difference-of-convex. We derive multiple convergence insights for a novel semi Forward-Backward Euler scheme under several nonconvex (and possibly nonsmooth) regimes. Notably, the semi ForwardBackward Euler is just a slight modification of the Forward-Backward Euler whose convergence is\u2014to our knowledge\u2014still unknown in our very general non-geodesically-convex setting. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Sampling and optimization are intertwined. For example, the (overdamped) Langevin dynamics, typically considered a sampling algorithm, can be considered as gradient descent optimization where a suitable amount of Gaussian noise is injected at each step. There are also deeper connections. At the limit of infinitesimal stepsize, the law of the Langevin dynamics is governed by the Fokker-Planck equation describing a diffusion over time of probability measures. In the seminal paper [39], Jordan, Kinderlehrer, and Otto reinterpreted the Fokker-Planck equation as the gradient flow of the functional relative entropy, a.k.a. Kullback-Leibler (KL) divergence, in the (Wasserstein) space of finite secondmoment probability measures equipped with the Wasserstein metric. The discovery connects the two fields and encourages optimization in the Wasserstein space, even conceptually, as it directly gives insight into the sampling context. Studies in continuous-time dynamics [21, 12, 66, 30] seem natural and enjoy nice theoretical properties without discretization errors. Another line of research studies discretization of Wasserstein gradient flow by either quantifying the discretization error between the continuous-time flow and the discrete-time flow [39, 67, 27, 23, 28] or viewing discrete-time flows as iterative optimization schemes in the Wasserstein space [65, 26, 70, 11] where the primary focus is on (geodesically) convex optimization problems. ", "page_idx": 0}, {"type": "text", "text": "Nonconvex, nonsmooth optimization is challenging, even in Euclidean space, quoting Rockafellar [63]: \u201cIn fact the great watershed in optimization isn\u2019t between linearity and nonlinearity, but convexity and nonconvexity.\u201d The landscape of nonconvex problems is mostly underexplored in the Wasserstein space. In the sampling language, it amounts to sampling from a non-log-concave and possibly non-log-Lipschitz-smooth target distribution. Recently, Balasubramanian et al. [9] advocated the need for a sound theory for non-log-concave sampling and provided some guarantees for the unadjusted Langevin algorithm (ULA) in sampling from log-smooth (Lipschitz/H\u00f6lder smooth) densities. These results are preliminary for the ULA (and its variants) with a specific class of densities (smooth). Theoretical understandings of other classes of algorithms and densities are needed. ", "page_idx": 0}, {"type": "text", "text": "We approach the subject through the lens of nonconvex optimization in the space of probability distributions and pose discretized Wasserstein gradient flows as iterative minimization algorithms. This allows us to, on the one hand, use and extend tools from classical nonconvex optimization and, on the other hand, derive more connections between sampling and optimization. ", "page_idx": 1}, {"type": "text", "text": "We study the following non-geodesically-convex optimization problem defined over the space ${\\mathcal{P}}_{2}(X)$ of probability measures $\\mu$ over $X=\\ensuremath{\\mathbb{R}}^{d}$ with finite second moment, i.e., $\\int\\|x\\|^{2}d\\mu(x)<+\\infty$ , ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mu\\in\\mathcal{P}_{2}(X)}\\mathcal{F}(\\mu):=\\mathcal{E}_{F}(\\mu)+\\mathcal{H}(\\mu):=\\mathcal{E}_{G-H}(\\mu)+\\mathcal{H}(\\mu)\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $F:X\\to\\mathbb{R}$ is a nonconvex function which can be represented as a difference of two convex functions $G$ and $H$ $\\textstyle\\cdot,\\mathcal{E}_{F}(\\mu):=\\int F(x)d\\mu(x)$ is the potential energy, and ${\\mathcal{H}}:{\\mathcal{P}}_{2}(X)\\to\\mathbb{R}\\cup\\{+\\infty\\}$ plays a role as the regularizer which is assumed to be a convex function along generalized geodesics. ", "page_idx": 1}, {"type": "text", "text": "Why difference-of-convex structure? Nonconvexity lies at the difference-of-convex (DC) structure $F=G-H$ , where $G$ and $H$ are called the first and second DC components, respectively. $F$ being nonconvex implies $\\mathcal{E}_{F}$ being non-geodesically-convex in general. First, the class of DC functions is very rich, and DC structures are present everywhere in real-world applications [60, 46, 47, 1, 22, 56, 58]. Weakly convex and Lipschitz smooth ( $L$ -smooth or simply smooth) functions are two subclasses of DC functions. Furthermore, any continuous function can be approximated by a sequence of DC functions over a compact, convex domain [8]. We also remark that many nonconvex functions admit quite natural DC decompositions, for example, an $L$ -smooth function $F$ has the following splittings: $\\bar{F}(x)\\,=\\,\\alpha\\|x\\|^{2}-(\\alpha\\|\\bar{x}\\|^{2}-F(x))$ and $\\bar{F}(x)\\,=\\,(F(x)+\\alpha\\|x\\|^{2})-\\alpha\\|x\\|^{2}$ whenever $\\alpha\\geq L/2$ . Second, DC functions preserve enough structure to extend convex analysis. Such structure is key in classic DC programming [60] and in our Wasserstein space analysis with optimal transport tools. ", "page_idx": 1}, {"type": "text", "text": "Context Many problems in machine learning and sampling fall into the spectrum of problem (1). For example, refer to a discussion in [65] that inspired our work. The regularizer $\\mathcal{H}$ can be the internal energy [4, Sect. 10.4.3]. Under McCann\u2019s condition, the internal energy is convex along generalized geodesics [4, Prop. 9.3.9]. In particular, the negative entropy, $\\begin{array}{r}{\\mathcal{H}(\\bar{\\mu})\\,=\\,\\int\\log(\\mu(x))\\bar{d}\\mu(x)}\\end{array}$ if $\\mu$ is absolutely continuous w.r.t. Lebesgue measure, $+\\infty$ otherwise, is a special case of internal energy satisfying McCann\u2019s condition. In the latter case, ${\\mathcal F}(\\mu)\\,=\\,\\mathrm{D}_{\\mathrm{KL}}\\bar{(}\\mu\\|\\mu^{*})\\,+$ const where $\\mu^{*}(x)\\;\\propto\\;\\exp(-F(x))$ , the optimization problem reduces to a sampling problem with log- $.D C$ target distribution. In Bayesian inference, the posterior structure depends on both the prior and likelihood. If the likelihood is log-smooth, it exhibits the aforementioned DC splittings. Logpriors, often nonsmooth to capture sparsity or low rank, typically also have explicit DC structures [48, 32, 25]. In the context of infinitely wide one-layer neural networks and Maximum Mean Discrepancy [51, 7, 21], let $\\mu^{*}$ be the optimal distribution over a network\u2019s parameters, $k$ be a given kernel, the regularizer is then the interaction energy $\\begin{array}{r}{\\mathcal{H}(\\mu)\\,=\\,\\int\\!\\!\\int k(x,\\dot{y})d\\mu(x)d\\mu(y)}\\end{array}$ and $\\begin{array}{r}{F(x)\\,=\\,-2\\int k(x,y)d\\mu^{*}(y)}\\end{array}$ . In general, $\\mathcal{H}$ is not convex along generalized geodesics and $F$ is nonconvex but not necessarily DC. When the kernel has Lipschitz gradient, we can adjust both $\\mathcal{H}$ and $F$ as $\\begin{array}{r}{\\mathcal{H}(\\mu)=\\iint k(x,y)\\dot{+}\\dot{\\alpha}\\|x\\|^{2}\\!+\\!\\alpha\\|y\\|^{2}d\\mu(x)d\\mu(y)}\\end{array}$ and $\\begin{array}{r}{F(\\bar{x^{\\prime}})=-2\\int k(x,y)\\bar{d}\\mu^{*}(y)\\!-\\!2\\alpha\\|x\\|^{2}}\\end{array}$ for some $\\alpha>0$ making $\\mathcal{H}$ generalized geodesically convex and $F$ concave (hence DC); Appx. A.2. ", "page_idx": 1}, {"type": "text", "text": "Our idea is to minimize (1) in the space of probability distributions by discretization of the gradient flow of $\\mathcal{F}$ , leveraging on the JKO (Jordan, Kinderlehrer, and Otto) operator (2). In the previous work [70], this has been done with the Forward-Backward (FB) Euler discretization, but it lacks convergence analysis. Recently, Salim et al. [65] did some study on FB Euler, but their results do not apply here because $F$ is nonconvex and possibly nonsmooth. Further leveraging on the DC structure of $F$ and inspired by classical DC programming literature [60], we subtly modify the FB Euler to give rise to a scheme named semi FB Euler that enjoys major theoretical advantages as we can provide a wide range of convergence analysis. Regarding the name, \"semi\" addresses the splitting of the potential energy. The scheme can be nevertheless reinterpreted as FB Euler. ", "page_idx": 1}, {"type": "text", "text": "Contributions To our knowledge, no prior work studies problem (1) when $F$ is DC. Therefore, most of the derived results in this paper are novel. We propose and analyze the semi FB Euler (4) leveraging on the classic DC optimization proof template [60, 45, 46] with a substantial accommodation of Wasserstein geometry and derive the following set of new insights: ", "page_idx": 1}, {"type": "text", "text": "Thm. 1 We show that if the $H$ is continuously differentiable, every cluster point of the sequence of distributions $\\{\\mu_{n}\\}_{n\\in\\mathbb{N}}$ generated by semi FB Euler is a critical point to $\\mathcal{F}$ . Note that criticality is a notion from the DC programming literature [60] and it is a necessary condition for local optimality; See Sect. 3.3. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Thm. 2 We provide convergence rate of $O(N^{-1})$ in terms of Wasserstein (sub)gradient mapping in the general nonsmooth setting. The notion of gradient mapping [33, 38, 55] is from the context of proximal algorithms in Euclidean space that is applicable to nonconvex programs where the notion of distance to global solution is\u2014in general\u2014not possible to work out. ", "page_idx": 2}, {"type": "text", "text": "Thm. 3 Under the extra assumption that $H$ is continuously twice differentiable and has bounded Hessian, we provide a convergence rate of $O(N^{-\\frac{1}{2}})$ in terms of distance of 0 to the Fr\u00e9chet subdifferential of $\\mathcal{F}$ . One can think of this as convergence rate to Fr\u00e9chet stationarity, i.e., if $\\mu^{*}$ is a Fr\u00e9chet stationary point of $\\mathcal{F}$ , then, by definition, 0 is in the Fr\u00e9chet subdifferential of $\\mathcal{F}$ at $\\mu^{*}$ . Fr\u00e9chet stationarity is a relatively sharp necessary condition for local optimality. ", "page_idx": 2}, {"type": "text", "text": "hm. 4, 5 Under the assumptions of Thm. 3 and additionally $\\mathcal{F}$ satisfying the \u0141ojasciewicz-type inequality for some \u0141ojasciewicz exponent of $\\theta\\in[0,1)$ , we show that $\\{\\mu_{n}\\}_{n\\in\\mathbb{N}}$ is a Cauchy sequence under Wasserstein topology, and thanks to the completeness of the Wasserstein space, the whole sequence $\\{\\mu_{n}\\}_{n\\in\\mathbb{N}}$ converges to some $\\mu^{*}$ . We show that $\\mu^{*}$ is in fact a global minimizer to $\\mathcal{F}$ . Furthermore, we provide convergence rate of $\\mu_{n}\\to\\mu^{*}$ in three different regimes $W_{2}$ denotes the Wasserstein metric): (1) if $\\theta=0$ , $W_{2}(\\mu_{n},\\mu^{*})$ converges to 0 after a finite number of steps; (2) if $\\theta\\in(0,1/2]$ , both $\\mathcal{F}(\\mu_{n})-\\mathcal{F}(\\mu^{*})$ and $W_{2}(\\mu_{n},\\mu^{*})$ converges to 0 exponentially fast; (3) if $\\theta\\in(1/2,1)$ , both $\\mathcal{F}(\\mu_{n})-\\mathcal{F}(\\mu^{*})$ and $W_{2}(\\mu_{n},\\mu^{*})$ converges sublinearly to 0 with rates $O\\left(n^{-{\\frac{1}{2\\theta-1}}}\\right)$ and $O\\left(n^{-{\\frac{1-\\theta}{2\\theta-1}}}\\right)$ , respectively. When $\\mathcal{H}$ is the negative entropy, $\\mathcal{F}(\\mu_{n})-\\mathcal{F}(\\dot{\\mu}^{*})=\\dot{\\mathrm{D}_{\\mathrm{KL}}}(\\mu_{n}\\|\\mu^{*})$ ; Therefore, in the sampling context, we provide convergence guarantees in both Wasserstein and KL distances. See Sect. 4.3 for additional observations and implications. ", "page_idx": 2}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 Notations and basic results in measure theory and functional analysis ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We denote by $X=\\ensuremath{\\mathbb{R}}^{d}$ , $B(X)$ the Borel $\\sigma$ -algebra over $X$ , and $\\mathcal{L}^{d}$ the Lebesgue measure on $X$ . ${\\mathcal{P}}(X)$ is the set of Borel probability measures on $X$ . For $\\mu\\in{\\mathcal{P}}(X)$ , we denote its second-order moment by $\\begin{array}{r}{\\mathfrak{m}_{2}(\\mu):=\\int_{X}\\dot{\\|}x\\|^{2}d\\mu(x)}\\end{array}$ , where ${\\mathfrak{m}}_{2}(\\mu)$ can be infinity. ${\\mathcal{P}}_{2}(X)\\subset{\\mathcal{P}}(X)$ denotes a set of finite second-order moment probability measures. $\\mathcal{P}_{2,\\mathrm{abs}}(X)\\subset\\mathcal{P}_{2}(X)$ is the set of measures that are absolutely continuous w.r.t. $\\mathcal{L}^{d}$ . Here $\\mu$ -a.e. stands for almost everywhere w.r.t. $\\mu$ . ", "page_idx": 2}, {"type": "text", "text": "Let $C^{p}(X),C_{c}^{\\infty}(X),C_{b}(X)$ be the classes of $p$ -time continuously differentiable functions, infinitely differentiable functions with compact support, bounded and continuous functions, respectively. ", "page_idx": 2}, {"type": "text", "text": "From functional analysis [20], for each $p\\,\\geq\\,1$ , $L^{p}(X,\\mu)$ denotes the Banach space of measurable (where measurable is understood as Borel measurable from now on) functions $f$ such that $\\int_{X}|f(x)|^{p}d\\mu(x)\\;<\\;+\\infty$ . We shall consider an element of $L^{p}(X,\\mu)$ as an equivalent class of functions that agree $\\mu$ -a.e. on $X$ rather than a sole function. The norm of $f~\\in~L^{p}(X,\\mu)$ is $\\begin{array}{r}{\\|f\\|_{L^{p}(X,\\mu)}=(\\int_{X}|f(x)|^{p}d\\mu(x))^{1/p}}\\end{array}$ . When $p=2$ , $L^{2}(X,\\mu)$ is actually a Hilbert space with the inner product $\\begin{array}{r}{\\langle f,g\\rangle_{L^{2}(X,\\mu)}=\\int_{X}f(x)g(x)d\\mu(x)}\\end{array}$ which induces the mentioned norm. These results can be extended to vector-valued functions. In particular, we denote by $L^{2}(X,X,\\mu)$ the Hilbert space of $\\xi:X\\rightarrow X$ in which $\\|\\xi\\|\\in L^{2}(X,\\mu)$ . The norm $\\begin{array}{r}{\\|\\xi\\|_{L^{2}(X,X,\\mu)}:=(\\int_{X}\\|\\xi(x)\\|^{2}d\\mu(x))^{1/2}}\\end{array}$ . ", "page_idx": 2}, {"type": "text", "text": "We say that $f:X\\to\\mathbb{R}$ has quadratic growth if there exists $a>0$ such that $|f(x)|\\leq a(\\|x\\|^{2}+1)$ for all $x\\in X$ . It is clear that if $f$ has quadratic growth and $\\mu\\in{\\mathcal{P}}_{2}(X)$ , then $f\\in L^{1}(X,\\mu)$ . ", "page_idx": 2}, {"type": "text", "text": "The pushforward of a measure $\\mu\\in{\\mathcal{P}}(X)$ through a Borel map $T:X\\rightarrow\\mathbb{R}^{m}$ , denoted by $T_{\\#}\\mu$ is defined by $(T_{\\#}\\mu)(A):=\\mu(T^{-1}(A))$ for every Borel sets $A\\subset\\mathbb{R}^{m}$ . ", "page_idx": 2}, {"type": "text", "text": "2.2 Optimal transport [4, 5, 69, 68] ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Given $\\mu,\\nu\\in{\\mathcal{P}}(X)$ , the principal problem in optimal transport is to find a transport map $T$ pushing $\\mu$ to $\\nu$ , i.e., $T_{\\#}\\mu=\\nu$ , in the most cost-efficient way, i.e., minimizing $\\|x-T\\bar{(}x)\\|^{2}$ on $\\mu$ -average. Monge\u2019s formulation for this problem is $\\begin{array}{r}{\\operatorname*{inf}_{T:T_{\\#}\\mu=\\nu}\\int_{X}\\|x-T(x)\\|^{2}d\\mu(x)}\\end{array}$ , where the optimal solution, if exists, is denoted by $T_{\\mu}^{\\nu}$ and called the optimal (Monge) map. Monge\u2019s problem can be ill-posed, e.g., no such $T_{\\mu}^{\\nu}$ exists when $\\mu$ is a Dirac mass and $\\nu$ is absolutely continuous [5]. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "By relaxing Monge\u2019s formulation, Kantorovich considers $\\begin{array}{r}{\\operatorname*{min}_{\\gamma\\in\\Gamma(\\mu,\\nu)}\\int_{X\\times X}\\|x-y\\|^{2}d\\gamma(x,y),}\\end{array}$ where $\\Gamma(\\mu,\\nu)$ denotes the set of probabilities over $X\\times X$ whose marginals are $\\mu$ and $\\nu$ , i.e, $\\gamma\\in\\Gamma(\\mu,\\nu)$ iff ${\\mathrm{\\Delta)roj_{1\\#}}}\\gamma=\\mu,{\\mathrm{proj_{2\\#}}}\\gamma=\\nu$ wher $\\mathrm{:proj_{1},p r o j_{2}}$ are the projections onto the first $X$ space and the second $X$ space, respectively. Such $\\gamma$ is called a plan. Kantorovich\u2019s formulation is well-posed because $\\Gamma(\\mu,\\nu)$ is non-empty (at least $\\mu\\times\\nu\\in\\Gamma(\\mu,\\nu))$ and the arg min element actually exists (see [5, Sect. 2.2]). The set of optimal plans between $\\mu$ and $\\nu$ is denoted by $\\Gamma_{o}(\\mu,\\nu)$ . In terms of random variables, any pairs $(X,Y)$ where $X\\sim\\mu,Y\\sim\\nu$ is called a coupling of $\\mu$ and $\\nu$ while it is called an optimal coupling if the joint law of $X$ and $Y$ is in $\\Gamma_{o}(\\mu,\\nu)$ . ", "page_idx": 3}, {"type": "text", "text": "In ${\\mathcal{P}}_{2}(X)$ , the min value in Kantorovich\u2019s problem specifies a valid metric referred to as Wasserstein distance, $\\begin{array}{r}{W_{2}(\\mu,\\nu)=(\\int_{X\\times X}\\|x-y\\|^{2}d\\gamma(x,y))^{1/2}}\\end{array}$ for some, and thus all, $\\gamma\\in\\Gamma_{o}(\\mu,\\nu)$ . The metric space $(\\mathcal{P}_{2}(X),W_{2})$ is then called the Wasserstein space. In ${\\mathcal{P}}_{2}(X)$ , beside the convergence notion induced by the Wasserstein metric, there is a weaker notion of convergence called narrow convergence: we say a sequence $\\{\\mu_{n}\\}_{n\\in\\mathbb{N}}\\,\\subset\\,{\\mathcal{P}}_{2}(X)$ converges narrowly to $\\bar{\\mu\\bar{\\mathrm{~\\in~}}}\\bar{\\mathcal{P}}_{2}(X)$ if $\\begin{array}{r l}{\\int_{X}\\phi(x)d\\mu_{n}(\\bar{x})\\,\\to\\,}\\end{array}$ $\\textstyle\\int_{X}\\phi(x)d\\mu(x)$ for all $\\phi\\in C_{b}(X)$ . Convergence in the Wasserstein metric implies narrow convergence but the converse is not necessarily true. The extra condition to make it true is ${\\mathfrak{m}}_{2}(\\mu_{n})\\to{\\mathfrak{m}}_{2}(\\mu)$ . We denote Wasserstein and narrow convergence b y\u2212\u2212\u2192and\u2212\u2212\u2212\u2192 , respectively. ", "page_idx": 3}, {"type": "text", "text": "If $\\mu\\in{\\mathcal{P}}_{2,\\mathrm{abs}}(X),\\nu\\in{\\mathcal{P}}_{2}(X)$ , Monge\u2019s formulation is well-posed and the unique ( $\\mu$ -a.e.) solution exists, and in this case, it is safe to talk about (and use) the optimal transport map $T_{\\mu}^{\\nu}$ . Moreover, there exists some convex function $f$ such that $T_{\\mu}^{\\nu}=\\nabla f$ $\\mu$ -a.e. Kantorovich\u2019s problem also has a unique solution $\\gamma$ and it is given by $\\gamma=(I,T_{\\mu}^{\\nu})_{\\#}\\mu$ where $I$ is the identity map. This is known as Brenier theorem or polar factorization theorem [18]. ", "page_idx": 3}, {"type": "text", "text": "2.3 Subdifferential calculus in the Wasserstein space ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Apart from being a metric space, $(\\mathcal{P}_{2}(X),W_{2})$ also enjoys some pre-Riemannian structure making subdifferential calculus on it possible. Let us have a picture of a manifold in mind. Firstly, the tangent space [4] of ${\\mathcal{P}}_{2}(X)$ at $\\mu$ is $\\mathrm{Tan}_{\\mu}\\,\\mathcal{P}_{2}(X):=\\overline{{\\{\\nabla\\psi:\\psi\\in C_{c}^{\\infty}(X)\\}}}^{L^{2}(X,X,\\mu)}$ , where the closure is w.r.t. the $L^{2}(X,X,\\mu)$ -topology. Intuitively, for $\\psi\\in C_{c}^{\\infty}(X),I+\\epsilon\\nabla\\psi$ is an optimal transport map if $\\epsilon>0$ is small enough [43], so $\\nabla\\psi$ plays a role as \"tangent vector\". ", "page_idx": 3}, {"type": "text", "text": "Let $\\phi:{\\mathcal{P}}_{2}(X)\\to\\mathbb{R}\\cup\\{+\\infty\\}$ , we denote $\\operatorname{dom}(\\phi)=\\{\\mu\\in{\\mathcal{P}}_{2}(X):\\phi(\\mu)<+\\infty\\}$ . Let $\\mu\\in\\mathrm{dom}(\\phi)$ , we say that a map $\\xi\\;\\in\\;L^{2}(X,X,\\mu)$ belongs to the Fr\u00e9chet subdifferential [15, 43] $\\partial_{F}^{-}\\phi(\\mu)$ if $\\begin{array}{r}{\\phi(\\nu)-\\phi(\\mu)\\geq\\operatorname*{sup}_{\\gamma\\in\\Gamma_{o}(\\mu,\\nu)}\\int_{X\\times X}\\left\\langle\\xi(x),y-x\\right\\rangle\\!d\\gamma(x,y)+o(W_{2}(\\mu,\\nu))}\\end{array}$ for all $\\nu\\in{\\mathcal{P}}_{2}(X)$ , where the little-o notation means $\\textstyle\\operatorname*{lim}_{s\\to0}o(s)/s=0$ . If $\\partial_{F}^{-}\\phi(\\mu)\\neq\\emptyset$ , we say $\\phi$ is Fr\u00e9chet subdifferentiable at $\\mu$ . We also denote $\\mathrm{dom}(\\partial_{F}^{-}\\phi)=\\{\\mu\\in\\mathcal{P}_{2}(X):\\partial_{F}^{-}\\phi(\\mu)\\neq\\emptyset\\}$ . ", "page_idx": 3}, {"type": "text", "text": "Similarly, we say that $\\xi\\in L^{2}(X,X,\\mu)$ belongs to the (Fr\u00e9chet) superdifferential $\\partial_{F}^{+}\\phi(\\mu)$ of $\\phi$ at $\\mu$ if $-\\xi\\in\\partial_{F}^{-}(-\\phi)(\\mu)$ . In other words, $\\partial_{F}^{-}(-\\phi)(\\mu)=-\\partial_{F}^{+}\\phi(\\mu)$ . ", "page_idx": 3}, {"type": "text", "text": "We say $\\phi$ is Wassertein differentiable [15, 43] at $\\mu\\,\\in\\,\\mathrm{dom}(\\phi)$ if $\\partial_{F}^{-}\\phi(\\mu)\\cap\\partial_{F}^{+}\\phi(\\mu)\\ \\neq\\ \\emptyset$ . We call an element of the intersection, denoted by $\\nabla_{W}\\phi(\\mu)$ , a Wasserstein gradient of $\\phi$ at $\\mu$ , and it holds $\\begin{array}{r}{\\phi(\\nu)-\\phi(\\mu)=\\int_{X\\times X}\\left\\langle\\nabla_{W}\\phi(\\mu)(x),y\\stackrel{!}{-}x\\right\\rangle d\\dot{\\gamma}(\\dot{x},y)+o(W_{2}(\\mu,\\nu)\\overline{{\\gamma}})}\\end{array}$ , for all $\\nu\\in{\\mathcal{P}}_{2}(X)$ and any $\\gamma\\in\\Gamma_{o}(\\mu,\\nu)$ . The Wasserstein gradient is not unique in general, but its parallel component in $\\operatorname{Tan}_{\\mu}\\mathcal{P}_{2}(X)$ is unique, and this parallel component is again a valid Wasserstein gradient as the orthogonal component plays no role in the above definitions, i.e., if $\\xi^{\\perp}\\in\\operatorname{Tan}_{\\mu}{\\mathcal{P}}_{2}(X)^{\\perp}$ , it holds $\\begin{array}{r}{\\int_{X\\times X}\\langle\\xi^{\\perp}(x),y-x\\rangle d\\gamma(x,y)=0}\\end{array}$ for any $\\nu\\in{\\mathcal{P}}_{2}(X)$ and $\\gamma\\in\\Gamma_{o}(\\mu,\\nu)$ [43, Prop. 2.5]. We may refer to this parallel component as the unique Wasserstein gradient of $\\phi$ at $\\mu$ . ", "page_idx": 3}, {"type": "text", "text": "2.4 Optimization in the Wasserstein space ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "A function $\\phi:{\\mathcal{P}}_{2}(X)\\ \\to\\mathbb{R}\\cup\\{+\\infty\\}$ is called proper if $\\mathrm{dom}(\\phi)\\,\\neq\\,\\emptyset$ , while it is called lower semicontinuous (l.s.c) if for any sequence $\\mu_{n}\\xrightarrow[]{\\mathrm{Wass}}\\mu$ , it holds $\\operatorname*{lim}\\operatorname*{inf}_{n}\\phi(\\mu_{n})\\geq\\phi(\\mu)$ . ", "page_idx": 3}, {"type": "text", "text": "We next recall (a simplified version of) generalized geodesic convexity. ", "page_idx": 4}, {"type": "text", "text": "Definition 1. [65] Let $\\phi:{\\mathcal{P}}_{2}(X)\\to\\mathbb{R}\\cup\\{+\\infty\\}$ . We say $\\phi$ is convex along generalized geodesics if $\\forall\\mu,\\pi\\in\\mathcal{P}_{2}(X)$ , $\\forall\\nu\\in\\mathcal{P}_{2,\\mathrm{abs}}(X)$ , $\\phi((t T_{\\nu}^{\\mu}+(1\\stackrel{\\cdot}{-}t)T_{\\nu}^{\\pi})_{\\#}\\nu)\\leq t\\phi(\\mu)+(1-t)\\phi(\\pi),\\forall t\\in[0,1}$ . ", "page_idx": 4}, {"type": "text", "text": "The curve $t\\mapsto(t T_{\\nu}^{\\mu}+(1-t)T_{\\nu}^{\\pi})_{\\#}\\nu$ (called a generalized geodesic) interpolates from $\\pi$ to $\\mu$ as $t$ runs from 0 to 1. The definition says that $\\phi$ is convex along these curves. If $\\mu\\in\\mathcal{P}_{2,\\mathrm{abs}}(X)$ and $\\nu=\\mu$ , the curve is a geodesic in $(\\mathcal{P}_{2}(X),W_{2})$ . If the definition is relaxed to the class of geodesics only, we say that $\\phi$ is convex along geodesics. ", "page_idx": 4}, {"type": "text", "text": "An important characterization of Fr\u00e9chet subdifferential of a geodesically convex function is that we can drop the little-o notation in its definition in Sect. 2.3 [4, Sect 10.1.1]. As a convention, for a geodesically convex function $\\phi$ , the Fr\u00e9chet subdifferential $\\partial_{F}^{-}$ will be simply written as $\\partial$ . ", "page_idx": 4}, {"type": "text", "text": "First-order optimality conditions Let $\\phi:{\\mathcal{P}}_{2}(X)\\to\\mathbb{R}\\cup\\{+\\infty\\}$ be a proper function. $\\mu^{*}\\in\\mathcal{P}_{2}(X)$ is a global minimizer of $\\phi$ if $\\phi(\\mu^{*})\\,\\leq\\,\\phi(\\mu),\\forall\\mu\\,\\in\\,\\mathcal{P}_{2}(X)$ . For local optimality, we shall use the Wasserstein metric to define neighborhoods. $\\mu^{*}\\in\\mathcal{P}_{2}(X)$ is a local minimizer if there exists $r>0$ such that $\\phi(\\mu^{*})\\,\\leq\\,\\phi(\\mu)$ for all $\\mu:W_{2}(\\mu,\\mu^{*})\\,<\\,r$ . We shall denote $B(\\mu^{*},r):=\\{\\mu\\in\\mathcal{P}_{2}(X):$ $W_{2}(\\mu,\\mu^{*})<r\\}$ the (open) Wasserstein ball centered at $\\mu^{*}$ with radius $r$ . If we replace $<$ by $\\leq$ we obtain the notion of a closed Wasserstein ball. ", "page_idx": 4}, {"type": "text", "text": "We call $\\mu^{*}$ a Fr\u00e9chet stationary point of $\\phi$ if $0\\;\\in\\;\\partial_{F}^{-}\\phi(\\mu^{*})$ . Fr\u00e9chet stationarity is a necessary condition for local optimality. In other words, if $\\mu^{*}$ is a local minimizer, it is a Fr\u00e9chet stationary point (Lem. 5 in Appendix). In addition, if $\\phi$ is Wasserstein differentiable at $\\mu^{*}$ , $\\nabla_{W}\\phi(\\mu^{*})(x)=0$ $\\mu^{*}$ -a.e. [43]. When $\\phi$ is geodesically convex, Fr\u00e9chet stationarity is a sufficient condition for global optimality (Lem. 6 in Appendix). ", "page_idx": 4}, {"type": "text", "text": "3 Semi Forward-Backward Euler for difference-of-convex structures ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "3.1 Wasserstein gradient flows: different types of discretizations ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "To neatly present the idea of minimizing $\\mathcal{F}$ via discretized gradient flow, we first assume for a moment that $F$ is infinitely differentiable and $\\mathcal{H}$ is the negative entropy. See also a discussion in [65]. ", "page_idx": 4}, {"type": "text", "text": "We wish to minimize (1) in the space of probability distributions. A natural idea is to apply discretizations of the gradient flow of $\\mathcal{F}$ , where the gradient flow is defined (under some technical assumptions [39]) as the limit $\\eta\\rightarrow0^{+}$ of the following scheme with some simple time-interpolation ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mu_{n+1}\\in\\mathrm{JKO}_{\\eta\\mathcal{F}}(\\mu_{n}),\\ \\mathrm{where}\\ \\mathrm{JKO}_{\\eta\\mathcal{F}}(\\mu):=\\underset{\\nu\\in\\mathcal{P}_{2}(X)}{\\arg\\operatorname*{min}}\\mathcal{F}(\\mu)+\\frac{1}{2\\eta}W_{2}^{2}(\\mu,\\nu).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Straightforwardly, given a fixed $\\eta\\:>\\:0$ , (2) gives back a discretization for this flow known as Backward Euler. On the other hand, if $\\mathcal{F}$ is Wasserstein differentiable (Sect. 2.2), the Forward Euler discretization reads [70] $\\mu_{n+1}\\,=\\,(I\\,-\\,\\eta\\nabla_{W}{\\mathcal{F}}(\\mu_{n}))_{\\#}\\mu_{n}$ , which is reinterpreted as doing gradient descent in the space of probability distributions. These are optimization methods that work directly on the objective function $\\mathcal{F}$ itself. However, the composite structure of $\\mathcal{F}$ (a sum of several terms) can also be exploited. One such scheme is the unadjusted Langevin algorithm (ULA), where it first takes a gradient step w.r.t. the potential part, then follows the heat flow corresponding to the entropy part [70]: $\\nu_{n+1}\\,=\\,(I\\,-\\,\\eta\\nabla F)_{\\#}\\mu_{n}$ , and $\\mu_{n+1}\\,=\\,\\mathcal{N}(0,2\\eta I)\\ast\\nu_{n+1}$ , where $^*$ is the convolution. This ULA is \"viewed\" in the space of distributions (Eulerian approach), a more familiar and equivalent form of the \u221aULA from the particle perspective (Lagrangian approach) goes like $x_{n+1}\\stackrel{*}{=}x_{n}-\\eta\\nabla F(x_{n})+\\sqrt{2\\eta}z_{k}$ where $z_{k}\\sim\\mathcal{N}(0,I)$ . The ULA is known to be asymptotically biased even for Gaussian target measure (Ornstein-Uhlenbeck process). To correct this bias, the Metropolis-Hasting accept-reject step [62] is sometimes introduced. Metropolis-Hasting algorithm [52, 36] is a much more general framework that works with quite any proposal (e.g., a random walk) whose convergence analysis is based on the Markov kernel satisfying the detailed balance condition. This convergence framework is different from what is considered in this work: we are more interested in the underlying dynamics of the chain. Metropolis-Hasting algorithm is indeed another story. ", "page_idx": 4}, {"type": "text", "text": "In optimization, for composite structure, Forward-Backward (FB) Euler and its variants are methods of choice [59, 10]. The corresponding FB Euler for $\\mathcal{F}$ will take the gradient step (forward) according to the potential, and JKO step (backward) w.r.t. the negative entropy ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\nu_{n+1}=(I-\\eta\\nabla F)_{\\#}\\mu_{n},\\ \\mathrm{and}\\ \\mu_{n+1}\\in\\mathrm{JKO}_{\\eta}{\\mathcal H}(\\nu_{n+1}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "This scheme appears in [70] without convergence analysis, and later on [65] derives non-asymptotic convergence guarantees under the assumption $F$ being convex and Lipschitz smooth. ", "page_idx": 5}, {"type": "text", "text": "In this work, as $F$ is nonconvex and nonsmooth, the theory in [65] does not apply, and the convergence (if any) of (3) remains mysterious. The DC structure of $F$ can be further exploited. In DC programming [60], the forward step should be applied to the concave part, while the backward step should be applied to the convex part. We hence propose the following semi FB Euler ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\nu_{n+1}=(I+\\eta\\nabla H)_{\\#}\\mu_{n},\\ \\mathrm{and}\\ \\mu_{n+1}\\in\\mathrm{JKO}_{\\eta(\\mathcal{H}+\\mathcal{E}_{G})}(\\nu_{n+1})\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "for which we can provide convergence guarantees. Apparently, the difference between semi FB Euler and FB Euler is subtle: while FB Euler does forward on $\\mathcal{E}_{G-H}=\\mathcal{E}_{G}-\\mathcal{E}_{H}$ and backward on $\\mathcal{H}$ , semi FB Euler does forward on $-\\mathcal{E}_{H}$ and backward on $\\mathcal{H}+\\mathcal{E}_{G}$ ; recall that $\\mathcal{F}=\\mathcal{E}_{G}-\\mathcal{E}_{H}+\\mathcal{H}$ . ", "page_idx": 5}, {"type": "text", "text": "Theoretically, semi FB Euler enjoys some advantages compared to FB Euler. Thanks to Brenier theorem (Sect. 2.2), the pushing step in semi FB Euler is optimal since $H$ is convex; Meanwhile, the pushing in FB Euler is non-optimal whose optimal Monge map is not identifiable in general. The convergence of FB Euler is still an open question, even when $F$ is (DC) differentiable. In contrast, we can provide a solid theoretical guarantee for semi FB Euler, especially when $H$ is differentiable. Additionally, we also offer convergence guarantees when $H$ is nonsmooth. ", "page_idx": 5}, {"type": "text", "text": "3.2 Problem setting ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Our goal is to minimize the non-geodesically-convex functional $\\mathcal{F}(\\mu)=\\mathcal{E}_{F}(\\mu)+\\mathcal{H}(\\mu)$ over ${\\mathcal{P}}_{2}(X)$ , where $F=G-H$ is a DC function. We make Assumption 1 throughout the paper: ", "page_idx": 5}, {"type": "text", "text": "Assumption 1. (i) The objective function $\\mathcal{F}$ is bounded below. ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\forall\\eta\\in(0,\\eta_{0}),\\,\\mathrm{JKO}_{\\eta(\\mathcal{E}_{G}+\\mathcal{H})}(\\mu)\\neq\\emptyset\\,f o r\\,e\\nu e r y\\,\\mu\\in\\mathcal{P}_{2}(X)\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Note that Assumption 1(iv) is a commonly-used assumption to simplify technical complication when working with the JKO operator [4, 15, 65]. Assumption 1(ii) implies $\\mathcal{E}_{G}$ and $\\mathcal{E}_{H}$ are continuous w.r.t. Wasserstein topology [3, Prop. 2.4] $(G,H$ are continuous [54, Cor. 2.27] and have quadratic growth). ", "page_idx": 5}, {"type": "text", "text": "3.3 Optimality charactizations ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "First, it follows from Assumption 1(iii), $\\mathrm{dom}(\\mathcal{F})\\subset\\mathcal{P}_{\\mathrm{2,abs}}(X)$ . By analogy to DC programming in Euclidean space, we call $\\mu^{*}\\in\\mathrm{dom}(\\mathcal{F})$ a critical point of $\\mathcal{F}=\\mathcal{H}+\\mathcal{E}_{G}-\\mathcal{E}_{H}$ if $\\partial(\\bar{\\mathcal{H}}+\\mathcal{E}_{G})(\\mu^{*})\\cap$ $\\partial\\mathcal{E}_{H}(\\mu^{*})\\neq\\emptyset$ . Criticality is a necessary condition for local optimality (Lem. 7). Moreover, if either $\\mathcal{H}+\\mathcal{E}_{G}$ or $\\mathcal{E}_{H}$ is Wasserstein differentiable at $\\mu^{*}$ , criticality becomes Fr\u00e9chet stationarity (Lem. 8). ", "page_idx": 5}, {"type": "text", "text": "3.4 Semi FB Euler: a general setting ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We allow $H$ to be non-differentiable in some derivations, meaning that $\\partial H$ (convex subdifferential [54]) contains multiple elements in general. We first pick a selector $S$ of $\\partial H$ , i.e., $S:X\\to X$ , such that $S(x)\\in\\partial H(x)$ . By the axiom of choice (Zermelo, 1904, see, e.g., [37]), such selection always exists. However, an arbitrary selector can behave badly, e.g., not measurable. We shall first restrict ourselves to the class of Borel measurable selectors (see Appx. A.1 for an existence discussion). ", "page_idx": 5}, {"type": "text", "text": "Assumption 2 (Measurability). The selector $S$ is Borel measurable. ", "page_idx": 5}, {"type": "text", "text": "We recall the semi FB scheme (4) but for nonsmooth $F$ as follows: start with an initial distribution $\\mu_{0}\\in\\mathcal{P}_{2,\\mathrm{abs}}(X)$ , given a discretization stepsize $0<\\eta<\\eta_{0}$ , we repeat the following two steps: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\nu_{n+1}=(I+\\eta S)_{\\#}\\mu_{n}\\quad\\mathrm{~<~push~forward~step;}\\quad\\mu_{n+1}=\\operatorname{JKO}_{\\eta(\\mathcal{E}_{G}+\\mathcal{H})}(\\nu_{n+1})\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Well-definiteness and properties: Given $\\mu_{n}\\in\\mathcal P_{2}(X)$ , it follows from Lem. (4) that $\\nu_{n+1}\\in{\\mathcal{P}}_{2}(X)$ . The two generated sequences are then in ${\\mathcal{P}}_{2}(X)$ . Moreover, it follows from Assumption 1 that $\\{\\mu_{n}\\}_{n\\in\\mathbb{N}}$ are in $\\mathcal{P}_{2,\\mathrm{abs}}(X)$ , so are $\\{\\nu_{n}\\}_{n\\in\\mathbb{N}}$ using Lem. 9 by noting that $I+\\eta S$ is subgradient of a strongly convex function $x\\mapsto(1/2)\\|x\\|^{2}+\\eta H(x)$ . ", "page_idx": 5}, {"type": "text", "text": "4 Convergence analysis ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "4.1 Asymptotic analysis ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Lemma 1 (Descent lemma). Under Assumptions 1 and 2, let $\\{\\mu_{n}\\}_{n\\in\\mathbb{N}}$ be the sequence of distributions produced by semi FB Euler starting from some $\\mu_{0}\\in\\mathcal{P}_{2,\\mathrm{abs}}(X)$ with $0<\\eta<\\eta_{0}$ . Then it holds $\\begin{array}{r}{\\mathcal{F}(\\mu_{n+1})\\leq\\mathcal{F}(\\mu_{n})-\\frac{1}{\\eta}\\int_{X}\\|T_{\\nu_{n+1}}^{\\mu_{n}}(x)-T_{\\nu_{n+1}}^{\\mu_{n+1}}(x)\\|^{2}d\\nu_{n+1}(x),\\quad\\forall\\mu_{n+1}=\\nu_{n+1}.}\\end{array}$ $\\forall n\\in\\mathbb{N}$ . ", "page_idx": 6}, {"type": "text", "text": "Lem. 1 shows that the objective does not increase along semi FB Euler\u2019s iterates. Proof of Lem. 1 is in Appx. A.3. By using Lem. 1, we establish asymptotic convergence for semi FB Euler as follows. ", "page_idx": 6}, {"type": "text", "text": "For the asymptotic convergence analysis, we need the following assumption on $H$ . ", "page_idx": 6}, {"type": "text", "text": "Assumption 3. $H$ is continuously differentiable. ", "page_idx": 6}, {"type": "text", "text": "Theorem 1 (Asymptotic convergence). Under Assumptions $I,\\;3,$ , let $\\{\\mu_{n}\\}_{n\\in\\mathbb{N}}$ and $\\{\\nu_{n}\\}_{n\\in\\mathbb{N}}$ be sequences produced by semi $F B$ Euler starting from some $\\mu_{0}\\,\\in\\,\\mathcal{P}_{2,\\mathrm{abs}}(X)$ with $0\\,<\\,\\eta\\,<\\,\\eta_{0}$ . If $\\{\\mu_{n}\\}_{n\\in\\mathbb{N}}$ is relatively compact with respect to the Wasserstein topology and $\\operatorname*{sup}_{n\\in\\mathbb{N}}\\mathcal{H}(\\nu_{n})<+\\infty$ , then every cluster point of $\\{\\mu_{n}\\}_{n\\in\\mathbb{N}}$ is a critical point of $\\mathcal{F}$ . ", "page_idx": 6}, {"type": "text", "text": "Proof of Thm.1 is in Appx. A.4. Thm. 1 does not ensure convergence of the whole sequence $\\{\\mu_{n}\\}_{n\\in\\mathbb{N}}$ ; Rather, it guarantees subsequential convergence to critical points of $\\mathcal{F}$ . ", "page_idx": 6}, {"type": "text", "text": "Remark 1. In the Euclidean space, the compactness assumption of the generated sequence is usually enforced via the coercivity assumption: $f(x)\\rightarrow+\\infty$ whenever $\\|x\\|\\to+\\infty$ . A striking difference in the Wasserstein space is that closed Wasserstein balls are not compact in the Wasserstein topology [43, Prop. $4.2J,$ making coercivity not sufficient to induce (Wasserstein) compactness. For Thm. 1, we simply assume the sequence $\\{\\mu_{n}\\}_{n\\in\\mathbb{N}}$ to be relatively compact. ", "page_idx": 6}, {"type": "text", "text": "4.2 Non asymptotic analysis ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "To measure how fast the algorithm converges, we need some convergence measurement. First, for proximal-type algorithms in Euclidean space, the notion of gradient mapping $\\mathcal{G}_{\\eta}(x_{n})$ is usually used (see, e.g., [33, 55] and [38, Eq. (5)]) and we measure the rate $\\|\\mathcal{G}_{\\eta}(x_{n})\\|^{2}\\,\\to\\,0$ . In analogy as in Euclidean space, we define the Wasserstein (sub)gradient mapping as follows $\\mathcal{G}_{\\eta}(\\mu)\\;:=\\;$ $\\begin{array}{r}{\\frac{1}{\\eta}\\left(I-T_{\\mu}^{\\mathrm{JKO}_{\\eta(\\varepsilon_{G}+\\mathcal{M})}((I+\\eta S)_{\\#}\\mu)}\\right)}\\end{array}$ , and we measure the rate of $\\|\\mathcal{G}_{\\eta}(\\mu_{n})\\|_{L^{2}(X,X,\\mu_{n})}^{2}\\to0.$ . ", "page_idx": 6}, {"type": "text", "text": "Theorem 2 (Convergence rate: Wasserstein (sub)gradient mapping). Under Assumptions $I,2,$ , let $\\{\\mu_{n}\\}_{n\\in\\mathbb{N}}$ be the sequence of distributions produced by semi $F B$ Euler starting from some $\\mu_{0}~\\in$ $\\mathcal{P}_{2,\\mathrm{abs}}(X)$ with $0<\\eta<\\eta_{0}$ . Then it holds $\\begin{array}{r}{\\operatorname*{min}_{n=1,N}\\|\\dot{\\mathcal{G}}_{\\eta}(\\mu_{n})\\|_{L^{2}(X,X,\\mu_{n})}^{2}=O(\\dot{N}^{-1})}\\end{array}$ . ", "page_idx": 6}, {"type": "text", "text": "Proof of Thm. 2 is in Appx. A.5. This theorem holds without requiring $G$ and $H$ to be differentiable. Next, if $H$ is twice differentiable with uniformly bounded Hessian, we can derive a stronger convergence guarantee based on Fr\u00e9chet stationarity (see Sect. 2.4). In other words, we evaluate the rate of dist $(\\stackrel{\\bullet}{0},\\partial_{F}^{-}\\mathcal{F}(\\mu_{n})):=\\operatorname*{inf}_{\\xi\\in\\partial_{F}^{-}\\mathcal{F}(\\mu_{n})}\\|\\xi\\|_{L^{2}(\\dot{X},X;\\mu_{n})}\\to0$ . ", "page_idx": 6}, {"type": "text", "text": "Assumption 4. $H\\in C^{2}(X)$ whose Hessian is bounded uniformly $\\mathit{\\Delta}H$ is then $L_{H}$ -smooth). ", "page_idx": 6}, {"type": "text", "text": "Theorem 3 (Convergence rate: Fr\u00e9chet subdifferentials). Under Assumptions $^{\\,l}$ , 4, let $\\{\\mu_{n}\\}_{n\\in\\mathbb{N}}\\,b e$ the sequence of distributions produced by semi FB Euler starting from some $\\mu_{0}\\in\\mathcal{P}_{2,\\mathrm{abs}}(X)$ with $<\\eta<\\eta_{0},\\,t h e n\\operatorname*{min}_{n=1,N}\\mathrm{dist}\\left(0,\\partial_{F}^{-}\\mathcal{F}(\\mu_{n})\\right)=O\\left(N^{-\\frac{1}{2}}\\right).$ ", "page_idx": 6}, {"type": "text", "text": "Proof of Thm. 3 is in Appx. A.6. ", "page_idx": 6}, {"type": "text", "text": "4.3 Fast convergence under isoperimetry and beyond ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Fast convergence can be obtained under isoperimetry, e.g., log-Sobolev inequality (LSI). There are certain connections between LSI in sampling and the \u0141ojasiewicz condition in optimization allowing linear convergence. In nonconvex optimization in Euclidean space, analytic and subanalytic functions are a large class satisfying \u0141ojasiewicz condition [44, 14]. Subanalytic DC programs are studied in [45]. In the infinite-dimensional setting of the Wasserstein space, the \u0141ojasiewicz condition should be regarded as functional inequalities [12]. ", "page_idx": 6}, {"type": "text", "text": "Assumption 5 (\u0141ojasiewicz condition in the Wasserstein space). Assume that $\\mathcal{F}^{*}$ is the optimal value of $\\mathcal{F}$ , and assume there exist $r_{0}\\,\\in\\,(\\mathcal{F}^{*},+\\infty]$ , $\\theta\\in[0,1)$ , and $c>0$ such that for all $\\mu\\in{\\mathcal{P}}_{2}(X)$ , $\\begin{array}{r}{\\mathcal{F}(\\mu)-\\mathcal{F}^{*}<r_{0}\\Rightarrow c\\left(\\mathcal{F}(\\mu)-\\mathcal{F}^{*}\\right)^{\\theta}\\leq\\|\\xi\\|_{L^{2}(X,X,\\mu)},}\\end{array}$ $\\forall\\xi\\in\\partial_{F}^{-}\\mathcal{F}(\\mu)$ , where the convention $0^{0}=0$ is used. $\\theta\\in[0,1)$ is called \u0141ojasiewicz exponent of $\\mathcal{F}$ at optimality. ", "page_idx": 7}, {"type": "text", "text": "Remark 2. If $\\mathcal{H}$ is the is negative entropy, $F\\in C^{2}(X)$ whose Hessian is bounded uniformly, then $\\mathcal{F}$ is Wasserstein differentiable at any $\\mu\\in\\mathscr{P}_{2,\\mathrm{abs}}(X)$ and $I^{43}$ , Prop. 2.12, E.g. $\\begin{array}{r}{2.3J\\,\\nabla_{W}\\mathcal{F}(\\mu)=\\frac{\\nabla\\mu}{\\mu}\\;+}\\end{array}$ $\\nabla F$ , where, by abuse of notation, the probability density function of $\\mu$ is still denoted by $\\mu$ . We have $\\begin{array}{r}{\\|\\nabla_{W}\\mathcal{F}(\\mu)\\|_{L^{2}(X,X,\\mu)}^{2}\\,=\\,\\int\\left\\|\\frac{\\nabla\\mu(x)}{\\mu(x)}+\\nabla F(x)\\right\\|^{2}\\!d\\mu(x)\\,=\\,\\int\\mu(x)\\left\\|\\nabla\\log\\frac{\\mu(x)}{\\mu^{*}(x)}\\right\\|^{2}d x,}\\end{array}$ , where $\\mu^{*}\\propto$ $\\exp(-F)$ . On the other hand, $\\mathcal{F}(\\mu)-\\mathcal{F}^{*}=\\mathrm{D}_{\\mathrm{KL}}(\\mu\\|\\mu^{*})$ . The log-Sobolev inequality with parameter $\\alpha>0$ inequality reads $\\begin{array}{r}{[57J\\,\\mathrm{D}_{\\mathrm{KL}}(\\mu||\\mu^{*})\\leq\\frac{1}{2\\alpha}\\,\\mathrm{FI}(\\mu||\\mu^{*}):=\\frac{1}{2\\alpha}\\int\\mu(x)\\left\\Vert\\nabla\\log\\frac{\\mu(x)}{\\mu^{*}(x)}\\right\\Vert^{2}d x}\\end{array}$ , where $\\operatorname{FI}(\\mu\\|\\mu^{*})$ is the relative Fisher information of $\\mu$ with respect to $\\mu^{*}$ . Therefore, log-Sobolev inequality is a special case of \u0141ojasiewicz condition with $r_{0}=+\\infty,c=\\sqrt{2\\alpha},$ , and $\\theta=1/2$ . In another case, when the objective function is the Maximum Mean Discrepancy, under some regularity assumption of the kernel, it holds $I7J$ ", "page_idx": 7}, {"type": "equation", "text": "$$\n2(\\mathcal{F}(\\mu)-\\mathcal{F}(\\mu^{*}))\\leq\\|\\mu^{*}-\\mu\\|_{\\dot{H}^{-1}(\\mu)}\\times\\int\\|\\nabla_{W}\\mathcal{F}(\\mu)\\|^{2}d\\mu(x)\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\|\\mu^{*}-\\mu\\|_{\\dot{H}^{-1}(\\mu)}$ is the weighted negative Sobolev distance. This is \"nearly\" the \u0141ojasiewicz condition, with a caveat that $\\|\\mu^{*}\\mathrm{~-~}\\mu\\|_{\\dot{H}^{-1}(\\mu)}$ may be unbounded. Nevertheless, assuming the boundedness of this term along the algorithm\u2019s iterates is sufficient for convergence. ", "page_idx": 7}, {"type": "text", "text": "Theorem 4. Under Assumptions $^{\\,l}$ , 4 and Assumption 5 with parameters $(r_{0},c,\\theta)$ . Let $\\{\\mu_{n}\\}_{n\\in\\mathbb{N}}\\,b e$ the sequence of distributions produced by semi $F B$ Euler starting from some sufficiently warm-up $\\mu_{0}\\in\\mathcal{P}_{2,\\mathrm{abs}}(X)$ such that $\\mathcal{F}(\\mu_{0})<r_{0}$ and with stepsize $0<\\eta<\\eta_{0}$ , then ", "page_idx": 7}, {"type": "text", "text": "(i) if \u03b8 = 0, $\\mathcal{F}(\\mu_{n})-\\mathcal{F}^{*}$ converges to 0 in a finite number of steps; ", "page_idx": 7}, {"type": "text", "text": "Proof of Thm. 4 is in Appx. A.7. ", "page_idx": 7}, {"type": "text", "text": "Remark 3. In the usual sampling case, i.e., $\\mathcal{H}$ is the negative entropy, and under log-Sobolev condition, $r_{0}=+\\infty$ . Therefore, $\\mu_{0}$ can be arbitrarily in $\\mathcal{P}_{2,\\mathrm{abs}}(X)$ . In the general case, however, a good enough starting point (i.e., $\\mathcal{F}(\\mu_{0})~<~r_{0})$ is needed to guarantee we are in the region where \u0141ojasiewicz condition comes into play. In such a case, $\\mathcal{F}(\\bar{\\mu_{n}})-\\mathcal{F}^{*}=\\mathrm{D}_{\\mathrm{KL}}(\\mu_{n}\\|\\mu^{*})$ where $\\mu^{*}(x)\\propto\\exp(-F(x))$ is the target distribution (see Rmk. 2), so Thm. 4 provides convergence rate of $\\{\\mu_{n}\\}_{n\\in\\mathbb{N}}$ to $\\mu^{*}$ in terms of $K L$ divergence and this convergence is exponentially fast if $\\theta\\in(0,1/2]$ . ", "page_idx": 7}, {"type": "text", "text": "Theorem 5. Under the same set of assumptions as in Thm. 4, the sequence $\\{\\mu_{n}\\}_{n\\in\\mathbb{N}}$ is a Cauchy sequence under Wasserstein topology. Furthermore, as the Wasserstein space $(\\mathcal{P}_{2}(X),W_{2})$ is complete $I5,$ , Thm. 2.2], every Cauchy sequence is convergent, i.e., there exists $\\mu^{*}\\in\\mathcal{P}_{2}(X)$ such that \u2212W\u2212ass\u2192\u00b5\u2217. The limit distribution \u00b5\u2217is indeed the global minimizer of F. In addition: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{^{\\langle i i\\rangle}\\ i f\\theta\\in(0,1/2],\\ W_{2}(\\mu_{n},\\mu^{*})=O\\left(\\left(\\frac{M}{M+1}\\right)^{n}\\right),\\ w h e r e\\ M=1+\\frac{(2(\\eta^{2}L_{H}^{2}+1))^{\\frac{1}{2\\theta}}}{(1-\\theta)\\eta^{\\frac{1-\\theta}{\\theta}}c^{\\frac{1}{\\theta}}};}\\\\ &{\\\"\\ i i)\\ i f\\theta\\in(1/2,1),\\ W_{2}(\\mu_{n},\\mu^{*})=O\\left(n^{-\\frac{1-\\theta}{2\\theta-1}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Proof of Thm. 5 is in Appx. A.8. This theorem provides convergence to optimality in terms of Wasserstein distance. ", "page_idx": 7}, {"type": "text", "text": "Remark 4. If $\\mathcal{H}$ is the negative entropy and $\\theta=1/2$ , under some technical assumptions on $F$ (e.g., continuously twice differentiable), $L S I$ implies Talagrand inequality $I57J$ (in optimization, known as \u0141ojasiewicz implies quadratic growth $I4O J,$ ), meaning that $K L$ divergence controls squared Wasserstein distance, so fast convergence under $K L$ divergence implies fast convergence under Wasserstein distance. ", "page_idx": 7}, {"type": "text", "text": "5 Practical implementations ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "The push-forward step $\\nu_{n+1}=(I+\\eta\\nabla H)_{\\#}\\mu_{n}$ is rather straightforward: if $Z$ are samples from $\\mu_{n}$ then $Z+\\eta\\nabla H(Z)$ are samples from $\\nu_{n+1}$ . On the other hand, to move from $\\nu_{n+1}$ to $\\mu_{n+1}$ we have to work out the JKO operator. Recent advances [53, 2] propose using the gradient of an input-convex neural network (ICNN) [6] to approximate the optimal Monge map pushing $\\nu_{n+1}$ to $\\mu_{n+1}$ , which we briefly describe as follows. This approach is inspired by Brenier theorem asserting that an optimal Monge map has to be the (sub)gradient field of some convex function. Therefore, one can \"parametrize\" $\\mu\\in\\mathscr{P}_{2,\\mathrm{abs}}(X)$ as $\\mu=\\nabla\\psi_{\\#}\\nu_{n+1}$ for some convex function $\\psi$ . We then write the JKO objective as ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\mathcal{H}(\\nabla\\psi_{\\#}\\nu_{n+1})+\\int_{X}G(\\nabla\\psi(x))d\\nu_{n+1}(x)+\\frac{1}{2\\eta}\\int_{X}\\|x-\\nabla\\psi(x)\\|^{2}d\\nu_{n+1}(x).\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "While the two last terms (potential energy and squared Wasserstein distance) in (5) can be handled efficiently by the Monte Carlo method using samples from $\\nu_{n+1}$ , the first term $\\mathcal{H}$ might be complicated as it possibly involves the (unavailable) density of $\\nu_{n+1}$ . We remark that the easy case would be $\\mathcal{H}$ being another potential energy or an interaction energy. In such a case, Monte Carlo approximations are again readily applicable. The tricky case would be $\\mathcal{H}$ being the negative entropy that requires the density of $\\nu_{n+1}$ . Fortunately, we have the following change of entropy formula: for any $T:X\\to X$ diffeomorphic, any $\\rho\\,\\in\\,\\mathcal{P}_{2,\\mathrm{abs}}(X)$ , it holds $\\begin{array}{r}{-\\tilde{\\mathcal{H}}(T_{\\#}\\rho\\stackrel{}{)}=\\,-\\tilde{\\mathcal{H}}(\\stackrel{}{\\rho})+\\int_{X}\\log|\\operatorname*{det}\\overleftarrow{\\nabla}T(x)|d\\rho(x)}\\end{array}$ Therefore, (5) can be written as (up to a constant that does not depend on $\\psi$ ) ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\int_{X}\\left[-\\log\\operatorname*{det}\\nabla^{2}\\psi(x)+G(\\nabla\\psi(x))+\\frac{1}{2\\eta}\\|x-\\nabla\\psi(x)\\|^{2}\\right]d\\nu_{n+1}(x).\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Note that this entropy formula can be extended naturally to the case of general internal energy [2], which means we can also handle this general case. Let us now consider the entropy case for simplicity. We can leverage on a class of input convex neural networks [6] $\\psi_{\\boldsymbol\\theta}(\\boldsymbol x)$ ( $\\boldsymbol{\\theta}$ is the neural network\u2019s parameters, $x$ is the input) in which $x\\mapsto\\psi_{\\theta}(x)$ is convex. Optimizing over $\\theta$ can then be solved effectively by standard deep learning optimizers (e.g. Adam). The complete scheme is given in Alg. 1. We also remark that [2] further proposes fast approximation for log det $\\nabla^{2}\\psi$ and [31] leverages on the variational formula of the KL to propose an even faster scheme. Nevertheless, these schemes are generally expensive. For illustrative purposes, we adopt the vanilla version of [53]. The iteration complexity is thus cubic in $d$ , linear in the size of the ICNN, and linear in the iteration count $k$ [53]. ", "page_idx": 8}, {"type": "text", "text": "Algorithm 1 Semi FB Euler for sampling ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Input: Initial measure $\\mu_{0}\\in\\mathcal{P}_{2,\\mathrm{abs}}(X)$ , discretization step size $\\eta>0$ , number of steps $K>0$ , batch size $B$ . ", "page_idx": 8}, {"type": "text", "text": "for $k=1$ to $K$ do for $i=1,2,\\dots$ do Draw a batch of samples $Z\\sim\\mu_{0}$ of size $B$ ; $\\begin{array}{r l}&{\\Xi\\leftarrow(I+\\eta\\nabla H)\\circ\\overset{1}{\\nabla}_{x}\\psi_{\\theta_{k}}\\circ(\\overset{!}{I}+\\eta\\nabla H)\\circ\\nabla_{x}\\psi_{\\theta_{k-1}}\\circ...\\circ(I+\\eta\\nabla H)(Z);}\\\\ &{\\widehat{W_{2}^{2}}\\leftarrow\\frac{1}{B}\\sum_{\\xi\\in\\Xi}\\|\\nabla_{x}\\psi_{\\theta}(\\xi)-\\xi\\|^{2};}\\\\ &{\\hat{\\mathcal{U}}\\leftarrow\\frac{1}{B}\\sum_{\\xi\\in\\Xi}G(\\nabla_{x}\\psi_{\\theta}(\\xi));}\\\\ &{\\widehat{\\Delta\\mathcal{H}}\\leftarrow-\\frac{1}{B}\\sum_{\\xi\\in\\Xi}\\log\\operatorname*{det}\\nabla_{x}^{2}\\psi_{\\theta}(\\xi).}\\\\ &{\\widehat{\\mathcal{L}}\\leftarrow\\frac{1}{2\\eta}\\widehat{W_{2}^{2}}+\\hat{\\mathcal{U}}+\\widehat{\\Delta\\mathcal{H}}.}\\end{array}$ Apply an optimization step (e.g., Adam) over $\\theta$ using $\\nabla_{\\boldsymbol{\\theta}}\\widehat{\\mathcal{L}}$ . end for $\\theta_{k+1}\\gets\\theta$ . dfo ", "page_idx": 8}, {"type": "text", "text": "6 Numerical illustrations ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We perform numerical sampling experiments from non-log-concave distributions: the Gaussian mixture distribution and the distance-to-set-prior [61] relaxed von Mises\u2013Fisher distribution. Both ", "page_idx": 8}, {"type": "text", "text": "are log-DC and the latter has non-differentiable logarithmic probability density (see Appx. C). Fig. 1 presents the sampling results. Experiment details are in Appx. B and Appx. ${\\dot{\\mathbf{C}}}^{1}$ . ", "page_idx": 9}, {"type": "image", "img_path": "LGG1IQhbOr/tmp/1f36e39a48e9dd6af8c9f2dcde309295b8fabb73fa6277bde4655b88f9e8476f.jpg", "img_caption": ["Figure 1: (a) and (b): Mixture of Gaussians. (a) shows samples obtained from semi FB Euler at iteration 40 and (b) shows KL divergence along the training process: semi FB Euler with sound theory is as fast as FB Euler. We also show the ULA\u2019s final result as a horizontal line for reference; (c) and (d): Relaxed von Mises-Fisher. (c) shows true probability density, and (d) shows the sample histogram obtained from semi FB Euler. In this experiment, FB Euler fails to work, attributed to the high curvature of the relaxed von Mises-Fisher. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "7 Discussion and related work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We first narrow down our discussion on FB Euler and its variants in the Wasserstein space. When $\\mathcal{H}$ is the negative entropy, Wibisono [70] provides some insightful discussion on how FB Euler should be consistent (no asymptotic bias) because the backward step is adjoint to the forward step, hence preserves stationarity. However, no convergence theory is presented for FB Euler in the Wasserstein space in [70]. Recently, Salim et al. [65] provide convergence guarantee for FB Euler within the following setting: $\\mathcal{H}$ is convex along generalized geodesics, $F$ is Lipschitz smooth and convex/strongly convex. This setting remains a \"convex $^+$ convex\" structure, while ours has a \"convex $^+$ concave\" structure. A natural extension of our work would be a full-fledged study of DC programming in the Wasserstein space $\\tilde{\\mathcal{F}}\\;=\\;\\tilde{\\mathcal{G}}\\;-\\;\\tilde{\\mathcal{H}}$ where $\\tilde{\\mathcal{G}}$ and $\\tilde{\\mathcal{H}}$ are convex along generalized geodesics and $\\tilde{\\mathcal{H}}$ is not necessarily potential energy $\\mathcal{E}_{H}$ . This problem possesses another implementation challenge regarding the Wasserstein gradient of $\\tilde{\\mathcal{H}}$ . ", "page_idx": 9}, {"type": "text", "text": "Other works that bear a tangential relation to ours involve the use of forward-only Euler and ULA. Arbel et al. [7] study Wasserstein gradient flows with forward Euler for maximum mean discrepancy. This objective function is also nonconvex (specifically, weakly convex). Durmus et al. [26] analyze the ULA from the convex optimization perspective. Vempala et al. [67] show that LSI and Hessian boundedness suffice for fast convergence of the ULA where \"fast\" is understood as fast to the biased target since ULA is a biased algorithm. Balasubramanian et al. [9] analyze the ULA under quite mild conditions: log-density is Lipschitz/H\u00f6lder smooth. Bernton [11] studies the proximal-ULA also under the convex assumption, where the difference to the ULA is the first step: gradient descent is replaced by the proximal operator. Similar to ULA, proximal-ULA is asymptotically biased. To address nonsmoothness, another line of research utilizes Moreau-Yosida envelopes to create smooth approximations of the ULA dynamics [29, 50]. This approach is also applicable to certain classes of non-log-concave distributions [50] and is more of a flavour of discretization error quantification. ", "page_idx": 9}, {"type": "text", "text": "8 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We propose a new semi FB Euler scheme as a discretization of Wasserstein gradient flow and show that it has favourably theoretical guarantees that the commonly used FB Euler does not yet have if the objective function is not convex along generalized geodesics. Our theoretical analysis opens up interesting avenues for future work. Given the ubiquity of nonconvexity, we hope that the idea can be reused in various contexts, such as with different optimal transport cost functions, different base spaces in the Wasserstein space [42], or submanifolds of the Wasserstein space (e.g., Bures-Wasserstein [42, 24]). ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This work is supported by the Research Council of Finland\u2019s Flagship programme: Finnish Center for Artificial Intelligence (FCAI), and additionally by grants 345811, 348952, and 346376 (VILMA: Virtual Laboratory for Molecular Level Atmospheric Transformations). The authors wish to thank the Finnish Computing Competence Infrastructure (FCCI) for supporting this project with computational and data storage resources. We thank the anonymous reviewers for their insightful comments and suggestions. H.P.H. Luu specifically thanks Michel Ledoux, Luigi Ambrosio, and Alain Durmus for helpful information. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] M. Ahn, J.-S. Pang, and J. Xin. Difference-of-convex learning: directional stationarity, optimality, and sparsity. SIAM Journal on Optimization, 27(3):1637\u20131665, 2017.   \n[2] D. Alvarez-Melis, Y. Schiff, and Y. Mroueh. Optimizing functionals on the space of probabilities with input convex neural networks. Transactions on Machine Learning Research, 2022.   \n[3] L. Ambrosio, A. Bressan, D. Helbing, A. Klar, E. Zuazua, L. Ambrosio, and N. Gigli. A user\u2019s guide to optimal transport. Modelling and Optimisation of Flows on Networks: Cetraro, Italy 2009, Editors: Benedetto Piccoli, Michel Rascle, pages 1\u2013155, 2013.   \n[4] L. Ambrosio, N. Gigli, and G. Savar\u00e9. Gradient flows: in metric spaces and in the space of probability measures. Springer Science & Business Media, 2005.   \n[5] L. Ambrosio and G. Savar\u00e9. Gradient flows of probability measures. Handbook of differential equations: evolutionary equations, 3:1\u2013136, 2006.   \n[6] B. Amos, L. Xu, and J. Z. Kolter. Input convex neural networks. In International Conference on Machine Learning, pages 146\u2013155. PMLR, 2017.   \n[7] M. Arbel, A. Korba, A. Salim, and A. Gretton. Maximum mean discrepancy gradient flow. Advances in Neural Information Processing Systems, 32, 2019.   \n[8] M. Bac\u02c7\u00e1k and J. M. Borwein. On difference convexity of locally Lipschitz functions. Optimization, 60(8-9):961\u2013978, 2011.   \n[9] K. Balasubramanian, S. Chewi, M. A. Erdogdu, A. Salim, and S. Zhang. Towards a theory of non-log-concave sampling: first-order stationarity guarantees for Langevin Monte Carlo. In Conference on Learning Theory, pages 2896\u20132923. PMLR, 2022.   \n[10] A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse problems. SIAM journal on imaging sciences, 2(1):183\u2013202, 2009.   \n[11] E. Bernton. Langevin Monte Carlo and JKO splitting. In Conference on learning theory, pages 1777\u20131798. PMLR, 2018.   \n[12] A. Blanchet and J. Bolte. A family of functional inequalities: \u0141ojasiewicz inequalities and displacement convex functions. Journal of Functional Analysis, 275(7):1650\u20131673, 2018.   \n[13] V. I. Bogachev and M. A. S. Ruas. Measure theory, volume 2. Springer, 2007.   \n[14] J. Bolte, A. Daniilidis, and A. Lewis. The \u0142ojasiewicz inequality for nonsmooth subanalytic functions with applications to subgradient dynamical systems. SIAM Journal on Optimization, 17(4):1205\u20131223, 2007.   \n[15] B. Bonnet. A Pontryagin maximum principle in Wasserstein spaces for constrained optimal control problems. ESAIM: Control, Optimisation and Calculus of Variations, 25:52, 2019.   \n[16] J. Borwein and A. Lewis. Convex Analysis and Nonlinear Optimization: Theory and Examples. Springer, 2006.   \n[17] G. E. Bredon. Topology and geometry, volume 139. Springer Science & Business Media, 2013.   \n[18] Y. Brenier. Polar factorization and monotone rearrangement of vector-valued functions. Communications on pure and applied mathematics, 44(4):375\u2013417, 1991.   \n[19] brett1479 (https://math.stackexchange.com/users/62876/brett1479). Borel sigma algebra of one point compactification. Mathematics Stack Exchange. URL:https://math.stackexchange.com/q/3532983 (version: 2020-02-03).   \n[20] H. Br\u00e9zis. Functional analysis, Sobolev spaces and partial differential equations, volume 2. Springer, 2011.   \n[21] L. Chizat and F. Bach. On the global convergence of gradient descent for over-parameterized models using optimal transport. Advances in neural information processing systems, 31, 2018.   \n[22] Y. Cui, J.-S. Pang, and B. Sen. Composite difference-max programs for modern statistical estimation problems. SIAM Journal on Optimization, 28(4):3344\u20133374, 2018.   \n[23] A. S. Dalalyan. Theoretical guarantees for approximate sampling from smooth and log-concave densities. Journal of the Royal Statistical Society Series B: Statistical Methodology, 79(3):651\u2013 676, 2017.   \n[24] M. Z. Diao, K. Balasubramanian, S. Chewi, and A. Salim. Forward-backward gaussian variational inference via jko in the bures-wasserstein space. In International Conference on Machine Learning, pages 7960\u20137991. PMLR, 2023.   \n[25] X. V. Doan and S. Vavasis. Low-rank matrix recovery with Ky Fan 2-k-norm. Journal of Global Optimization, 82(4):727\u2013751, 2022.   \n[26] A. Durmus, S. Majewski, and B. Miasojedow. Analysis of Langevin Monte Carlo via convex optimization. Journal of Machine Learning Research, 20(73):1\u201346, 2019.   \n[27] A. Durmus and \u00c9. Moulines. Nonasymptotic convergence analysis for the unadjusted Langevin algorithm. The Annals of Applied Probability, 27(3):1551 \u2013 1587, 2017.   \n[28] A. Durmus and \u00c9. Moulines. High-dimensional Bayesian inference via the unadjusted Langevin algorithm. Bernoulli, 25(4A):2854 \u2013 2882, 2019.   \n[29] A. Durmus, E. Moulines, and M. Pereyra. Efficient Bayesian computation by proximal Markov chain Monte Carlo: when Langevin meets Moreau. SIAM Journal on Imaging Sciences, 11(1):473\u2013506, 2018.   \n[30] M. Erbar. The heat equation on manifolds as a gradient flow in the Wasserstein space. In Annales de l\u2019IHP Probabilit\u00e9s et statistiques, volume 46, pages 1\u201323, 2010.   \n[31] J. Fan, Q. Zhang, A. Taghvaei, and Y. Chen. Variational Wasserstein gradient flow. In Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 6185\u20136215. PMLR, 2022.   \n[32] J. Geng, L. Wang, and Y. Wang. A non-convex algorithm framework based on DC programming and DCA for matrix completion. Numerical Algorithms, 68:903\u2013921, 2015.   \n[33] S. Ghadimi, G. Lan, and H. Zhang. Mini-batch stochastic approximation methods for nonconvex stochastic composite optimization. Mathematical Programming, 155(1):267\u2013305, 2016.   \n[34] A. Gretton, K. M. Borgwardt, M. J. Rasch, B. Sch\u00f6lkopf, and A. Smola. A kernel two-sample test. The Journal of Machine Learning Research, 13(1):723\u2013773, 2012.   \n[35] P. Hajlasz. Is there a Borel measurable $f:\\mathbb R^{d}\\,\\rightarrow\\,\\mathbb R^{d}$ such that $f(x)\\,\\in\\,\\partial\\varphi(x)$ for all $x$ ? MathOverflow. URL:https://mathoverflow.net/q/453991 (version: 2023-12-02).   \n[36] W. K. Hastings. Monte Carlo sampling methods using Markov chains and their applications. 1970.   \n[37] H. Herrlich. Axiom of choice, volume 1876. Springer, 2006.   \n[38] S. J Reddi, S. Sra, B. Poczos, and A. J. Smola. Proximal stochastic methods for nonsmooth nonconvex finite-sum optimization. Advances in neural information processing systems, 29, 2016.   \n[39] R. Jordan, D. Kinderlehrer, and F. Otto. The variational formulation of the Fokker\u2013Planck equation. SIAM journal on mathematical analysis, 29(1):1\u201317, 1998.   \n[40] H. Karimi, J. Nutini, and M. Schmidt. Linear convergence of gradient and proximal-gradient methods under the Polyak-\u0142ojasiewicz condition. In Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2016, Riva del Garda, Italy, September 19-23, 2016, Proceedings, Part I 16, pages 795\u2013811. Springer, 2016.   \n[41] A. Korotin, V. Egiazarian, A. Asadulaev, A. Safin, and E. Burnaev. Wasserstein-2 generative networks. In International Conference on Learning Representations, 2021.   \n[42] M. Lambert, S. Chewi, F. Bach, S. Bonnabel, and P. Rigollet. Variational inference via wasserstein gradient flows. Advances in Neural Information Processing Systems, 35:14434\u2013 14447, 2022.   \n[43] N. Lanzetti, S. Bolognani, and F. D\u00f6rfler. First-order conditions for optimization in the Wasserstein space. arXiv preprint arXiv:2209.12197, 2022.   \n[44] S. law \u0141ojasiewicz. Ensembles semi-analytiques. IHES notes, page 220, 1965.   \n[45] H. A. Le Thi, V. N. Huynh, and T. Pham Dinh. Convergence analysis of difference-of-convex algorithm with subanalytic data. Journal of Optimization Theory and Applications, 179(1):103\u2013 126, 2018.   \n[46] H. A. Le Thi, V. N. Huynh, T. Pham Dinh, and H. P. H. Luu. Stochastic difference-ofconvex-functions algorithms for nonconvex programming. SIAM Journal on Optimization, 32(3):2263\u20132293, 2022.   \n[47] H. A. Le Thi and T. Pham Dinh. The DC (difference of convex functions) programming and DCA revisited with DC models of real world nonconvex optimization problems. Annals of operations research, 133:23\u201346, 2005.   \n[48] H. A. Le Thi, T. Pham Dinh, H. M. Le, and X. T. Vo. Dc approximation approaches for sparse optimization. European Journal of Operational Research, 244(1):26\u201346, 2015.   \n[49] J. M. Lee. Smooth Manifolds. In Introduction to Smooth Manifolds, pages 1\u201331. Springer.   \n[50] T. D. Luu, J. Fadili, and C. Chesneau. Sampling from non-smooth distributions through Langevin diffusion. Methodology and Computing in Applied Probability, 23:1173\u20131201, 2021.   \n[51] S. Mei, T. Misiakiewicz, and A. Montanari. Mean-field theory of two-layers neural networks: dimension-free bounds and kernel limit. In Conference on Learning Theory, pages 2388\u20132464. PMLR, 2019.   \n[52] N. Metropolis, A. W. Rosenbluth, M. N. Rosenbluth, A. H. Teller, and E. Teller. Equation of state calculations by fast computing machines. The journal of chemical physics, 21(6):1087\u20131092, 1953.   \n[53] P. Mokrov, A. Korotin, L. Li, A. Genevay, J. M. Solomon, and E. Burnaev. Large-scale Wasserstein gradient flows. Advances in Neural Information Processing Systems, 34:15243\u2013 15256, 2021.   \n[54] B. Mordukhovich and M. N. Nguyen. An easy path to convex analysis and applications. Springer Nature, 2023.   \n[55] Y. Nesterov. Introductory lectures on convex optimization: A basic course, volume 87. Springer Science & Business Media, 2013.   \n[56] M. Nouiehed, J.-S. Pang, and M. Razaviyayn. On the pervasiveness of difference-convexity in optimization and statistics. Mathematical Programming, 174(1):195\u2013222, 2019.   \n[57] F. Otto and C. Villani. Generalization of an inequality by Talagrand and links with the logarithmic Sobolev inequality. Journal of Functional Analysis, 173(2):361\u2013400, 2000.   \n[58] J.-S. Pang, M. Razaviyayn, and A. Alvarado. Computing B-stationary points of nonsmooth DC programs. Mathematics of Operations Research, 42(1):95\u2013118, 2017.   \n[59] N. Parikh, S. Boyd, et al. Proximal algorithms. Foundations and trends\u00ae in Optimization, 1(3):127\u2013239, 2014.   \n[60] T. Pham Dinh and H. A. Le Thi. Convex analysis approach to DC programming: theory, algorithms and applications. Acta mathematica vietnamica, 22(1):289\u2013355, 1997.   \n[61] R. Presman and J. Xu. Distance-to-set priors and constrained Bayesian inference. In International Conference on Artificial Intelligence and Statistics, pages 2310\u20132326. PMLR, 2023.   \n[62] G. O. Roberts and R. L. Tweedie. Exponential convergence of Langevin distributions and their discrete approximations. Bernoulli, pages 341\u2013363, 1996.   \n[63] R. T. Rockafellar. Lagrange multipliers and optimality. SIAM review, 35(2):183\u2013238, 1993.   \n[64] R. T. Rockafellar. Convex analysis, volume 11. Princeton university press, 1997.   \n[65] A. Salim, A. Korba, and G. Luise. The Wasserstein proximal gradient algorithm. Advances in Neural Information Processing Systems, 33:12356\u201312366, 2020.   \n[66] A. Taghvaei and P. Mehta. Accelerated flow for probability distributions. In International conference on machine learning, pages 6076\u20136085. PMLR, 2019.   \n[67] S. Vempala and A. Wibisono. Rapid convergence of the unadjusted Langevin algorithm: Isoperimetry suffices. Advances in neural information processing systems, 32, 2019.   \n[68] C. Villani. Optimal transport: old and new, volume 338. Springer, 2009.   \n[69] C. Villani. Topics in optimal transportation, volume 58. American Mathematical Soc., 2021.   \n[70] A. Wibisono. Sampling as optimization in the space of measures: The Langevin dynamics as a composite optimization problem. In Conference on Learning Theory, pages 2093\u20133027. PMLR, 2018.   \n[71] H. Zhang and Y.-S. Niu. A Boosted-DCA with power-sum-DC decomposition for linearly constrained polynomial programs. Journal of Optimization Theory and Applications, pages 1\u201340, 2024.   \n[72] X. Zhou. On the Fenchel duality between strong convexity and Lipschitz continuous gradient. arXiv preprint arXiv:1803.06573, 2018. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Theory ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Lemma 2 (Transfer lemma). $l3,$ , Sect. 1] Let $T:\\mathbb{R}^{m}\\to\\mathbb{R}^{n}$ be a measurable map, and $\\mu\\in\\mathcal P(\\mathbb{R}^{m})$ , then $T_{\\#}\\mu\\in\\mathcal{P}(\\mathbb{R}^{n})$ and $\\textstyle\\int f(y)d(T_{\\#}\\mu)(y)\\,=\\,\\int{(f\\circ T)(x)d\\mu(x)}$ for every measurable function $f:\\mathbb{R}^{n}\\rightarrow\\mathbb{R},$ , where the above identity has to be understood that: one of the integrals exits (potentially $\\pm\\infty)$ iff the other one exists, and in such a case they are equal. Consequently, for a bounded function $f$ , the above integrals exist as real numbers that are equal. ", "page_idx": 14}, {"type": "text", "text": "Lemma 3. $l^{4};$ , Rmk. 6.2.11] Let $\\mu,\\nu\\in\\mathcal{P}_{2,\\mathrm{abs}}(X),$ , then $T_{\\nu}^{\\mu}\\circ T_{\\mu}^{\\nu}=I\\,\\mu\\cdot$ -a.e. and $T_{\\mu}^{\\nu}\\circ T_{\\nu}^{\\mu}=I\\:\\nu\\cdot$ -a.e. Theorem 6 (Characterization of Fr\u00e9chet subdifferential for geodesically convex functions). $l^{4}$ , Section $I0.I J$ Suppose $\\phi:{\\mathcal{P}}_{2}(X)\\to\\mathbb{R}\\cup\\{+\\infty\\}$ is proper, l.s.c, convex on geodesics. Let $\\mu\\,\\in$ $\\operatorname{dom}(\\partial\\phi)\\cap\\mathcal{P}_{\\mathrm{2,abs}}(X),$ , then a vector $\\xi\\in L^{2}\\dot{(X,X,\\mu)}$ belongs to the Fr\u00e9chet subdifferential of $\\phi$ at $\\mu$ if and only $i f$ ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\phi(\\nu)-\\phi(\\mu)\\geq\\int_{X}\\left\\langle\\xi(x),T_{\\mu}^{\\nu}(x)-x\\right\\rangle\\!d\\mu(x)\\quad\\forall\\nu\\in\\operatorname{dom}(\\phi).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Lemma 4. Let $H:X\\to\\mathbb{R}$ be a convex function having quadratic growth and $\\xi$ be a measurable selector of $\\partial H$ , i.e., $\\xi(x)\\in\\partial H(x)$ for all $x\\in X$ . Then, for all $\\mu\\in{\\mathcal{P}}_{2}(X),$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\int_{X}\\|\\xi(x)\\|^{2}d\\mu(x)<+\\infty.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "In other words, $\\xi\\in L^{2}(X,X,\\mu)$ for all $\\mu\\in{\\mathcal{P}}_{2}(X)$ . ", "page_idx": 14}, {"type": "text", "text": "Proof. Since $\\xi(x)\\in\\partial H(x)$ , by tangent inequality for convex functions, we have ", "page_idx": 14}, {"type": "equation", "text": "$$\nH(y)\\geq H(x)+\\langle\\xi(x),y-x\\rangle,\\quad\\forall y\\in X.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "By picking $y=x+\\epsilon\\xi(x)$ for some $\\epsilon>0$ , we get ", "page_idx": 14}, {"type": "equation", "text": "$$\nH(x+\\epsilon\\xi(x))-H(x)\\geq\\langle\\xi(x),\\epsilon\\xi(x)\\rangle=\\epsilon\\|\\xi(x)\\|^{2}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Since $H$ has quadratic growth, for some $a>0$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{H(x+\\epsilon\\xi(x))-H(x)\\le|H(x+\\epsilon\\xi(x))|+|H(x)|}\\\\ &{\\qquad\\qquad\\qquad\\le a\\left(\\|x+\\epsilon\\xi(x)\\|^{2}+1\\right)+a(\\|x\\|^{2}+1)}\\\\ &{\\qquad\\qquad\\le2a+a\\|x\\|^{2}+a(\\|x\\|+\\epsilon\\|\\xi(x)\\|)^{2}}\\\\ &{\\qquad\\qquad\\qquad\\le2a+a\\|x\\|^{2}+2a(\\|x\\|^{2}+\\epsilon^{2}\\|\\xi(x)\\|^{2})}\\\\ &{\\qquad\\qquad\\qquad=2a+3a\\|x\\|^{2}+2a\\epsilon^{2}\\|\\xi(x)\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Combining with (7), it holds ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\epsilon(1-2a\\epsilon)\\|\\boldsymbol{\\xi}(\\boldsymbol{x})\\|^{2}\\leq2a+3a\\|\\boldsymbol{x}\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "By choosing $0<\\epsilon<1/(2a)$ , we obtain ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\|\\xi(x)\\|^{2}\\leq\\frac{2a}{\\epsilon(1-2a\\epsilon)}+\\frac{3a}{\\epsilon(1-2a\\epsilon)}\\|x\\|^{2}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Therefore, $\\|\\xi(x)\\|^{2}$ has quadratic growth and - as a consequence - (6) holds for any $\\mu\\in{\\mathcal{P}}_{2}(X)$ . ", "page_idx": 14}, {"type": "text", "text": "Lemma 5. Let $\\phi:{\\mathcal{P}}_{2}(X)\\to\\mathbb{R}\\cup\\{+\\infty\\}$ be a proper function. Let $\\mu^{*}$ be a local minimizer of $\\phi$ , then $\\mu^{*}$ is a Fr\u00e9chet stationary point of $\\phi$ . ", "page_idx": 14}, {"type": "text", "text": "Proof. There exists $r>0$ such that $\\phi(\\mu^{*})\\leq\\phi(\\mu)$ for all $\\mu\\in\\mathcal{P}_{2}(X):W_{2}(\\mu,\\mu^{*})<r$ . It follows that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{\\mu\\xrightarrow{\\mathrm{ins}}\\mu^{*}}\\frac{\\phi(\\mu)-\\phi(\\mu^{*})}{W_{2}(\\mu,\\mu^{*})}\\geq0,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "so $0\\in\\partial_{F}^{-}\\phi(\\mu^{*})$ , or $\\mu^{*}$ is a Fr\u00e9chet stationary point of $\\phi$ . ", "page_idx": 14}, {"type": "text", "text": "Lemma 6. Let $\\phi:{\\mathcal{P}}_{2}(X)\\to\\mathbb{R}\\cup\\{+\\infty\\}$ be a proper, l.s.c, geodesically convex function. Suppose that $\\mu^{*}\\in\\mathcal{P}_{2,\\mathrm{abs}}(X)$ is a Fr\u00e9chet stationary point of $\\phi$ . Then, $\\mu^{*}$ is a global minimizer of $\\phi$ . ", "page_idx": 15}, {"type": "text", "text": "Proof. By definition of Fr\u00e9chet stationarity, $0\\in\\partial\\phi(\\mu^{*})$ . By characterization of subdifferential of geodesically convex functions (Thm. 6), it holds $\\phi(\\mu)\\geq\\phi(\\mu^{*})$ for all $\\mu\\in\\mathrm{dom}(\\phi)$ , or $\\mu^{*}$ is a global minimizer of $\\phi$ . \u53e3 ", "page_idx": 15}, {"type": "text", "text": "Lemma 7. Under Assumption $^{\\,l}$ , let $\\mu^{*}\\in\\mathrm{dom}(\\mathcal{F})$ be a local minimizer of $\\mathcal{F}$ , then $\\mu^{*}$ is a critical point of $\\mathcal{F}$ , i.e., $\\partial(\\mathcal{H}+\\bar{\\mathcal{E}}_{G})(\\mu^{*})\\cap\\partial\\dot{\\mathcal{E}}_{H}(\\mu^{*})\\neq\\vec{0}$ . ", "page_idx": 15}, {"type": "text", "text": "Proof. Since $\\mu^{*}$ is a local minimizer of $\\mathcal{F}$ , there exists $r>0$ such that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathcal{F}(\\mu^{*})\\leq\\mathcal{F}(\\mu),\\quad\\forall\\mu\\in\\mathcal{P}_{2}(X):W_{2}(\\mu,\\mu^{*})<r.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Let $\\xi$ be a measurable selector of $\\partial H$ . Thanks to Lemma 4, $\\xi\\,\\in\\,L^{2}(X,X,\\mu^{*})$ . According to [5, Prop. 4.13], $\\xi\\in\\partial\\mathcal{E}_{H}(\\mu^{*})$ . It follows from Thm. 6 that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathcal{E}_{H}(\\mu)\\geq\\mathcal{E}_{H}(\\mu^{*})+\\int_{X}\\langle\\xi(x),T_{\\mu^{*}}^{\\mu}(x)-x\\rangle d\\mu^{*}(x),\\quad\\forall\\mu\\in\\mathcal{P}_{2}(X).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "From (8) and (9), for $\\mu\\in B(\\mu^{*},r)$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathcal H(\\mu)+\\mathcal E_{G}(\\mu)\\geq\\mathcal H(\\mu^{*})+\\mathcal E_{G}(\\mu^{*})+\\int_{X}{\\langle}\\xi(x),T_{\\mu^{*}}^{\\mu}(x)-x{\\rangle}d\\mu^{*}(x).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Therefore, $\\xi\\in\\partial(\\mathcal{H}+\\mathcal{E}_{G})(\\mu^{*})$ since $\\mathcal{H}+\\mathcal{E}_{G}$ is geodesically convex. It follows that $\\mu^{*}$ is a critical point of $\\mathcal{F}$ . ", "page_idx": 15}, {"type": "text", "text": "Lemma 8. Let $\\mathcal{U},\\mathcal{V}:\\mathcal{P}_{2}(X)\\rightarrow\\mathbb{R}\\cup\\{+\\infty\\}$ . The following statements hold ", "page_idx": 15}, {"type": "equation", "text": "$\\partial_{F}^{-}(\\mathcal{U}+\\mathcal{V})(\\mu)\\supset\\partial_{F}^{-}\\mathcal{U}(\\mu)+\\partial_{F}^{-}\\mathcal{V}(\\mu).$ ", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "$^b$ . If $\\nu$ is Wasserstein differentiable of $\\mu_{\\cdot}$ , then ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\partial_{F}^{-}(\\mathcal{U}+\\mathcal{V})(\\mu)=\\partial_{F}^{-}\\mathcal{U}(\\mu)+\\nabla_{W}\\mathcal{V}(\\mu).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof. Item a. is trivial from the definition of Fr\u00e9chet subdifferential. For item b., from item a., we first see that, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\partial_{F}^{-}(\\mathcal{U}+\\mathcal{V})(\\mu)\\supset\\partial_{F}^{-}\\mathcal{U}(\\mu)+\\nabla_{W}\\mathcal{V}(\\mu).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "On the other hand, we apply item a. for $\\mathcal{U}+\\mathcal{V}$ and $-\\mathcal{V}$ to obtain ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\partial_{F}^{-}\\mathcal{U}(\\mu)\\supset\\partial_{F}^{-}(\\mathcal{U}+\\mathcal{V})(\\mu)+\\partial_{F}^{-}(-\\mathcal{V})(\\mu).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Since $-\\nabla_{W}\\mathcal{V}(\\mu)\\in\\partial_{F}^{-}(-\\mathcal{V})(\\mu)$ , it follows that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\partial_{F}^{-}\\mathcal{U}(\\mu)\\supset\\partial_{F}^{-}(\\mathcal{U}+\\mathcal{V})(\\mu)-\\nabla_{W}\\mathcal{V}(\\mu).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "From (11) and (12), we derive (10). ", "page_idx": 15}, {"type": "text", "text": "Lemma 9. Let $\\mu\\ll\\mathcal{L}^{d}$ , and $g$ is a strongly convex function. Then $\\nabla g_{\\#}\\mu\\ll\\mathcal{L}^{d}$ . ", "page_idx": 15}, {"type": "text", "text": "Proof. Let $\\Omega=\\{x:\\nabla^{2}g(x){\\mathrm{~exists}}\\}$ , then $\\mathcal{L}^{d}(\\mathbb{R}^{d}\\backslash\\Omega)=0$ (Aleksandrov, see, e.g., [4, Thm. 5.5.4]). Since $g$ is strongly convex, $\\nabla g$ is injective on $\\Omega$ and $|\\operatorname*{det}\\nabla^{2}g|>0$ on $\\Omega$ . By applying Lemma 5.5.3 [4], $\\nabla g_{\\#}\\mu\\ll\\mathcal{L}^{d}$ . \u53e3 ", "page_idx": 15}, {"type": "text", "text": "Lemma 10. [65] Let $\\mathcal{G}:\\mathcal{P}_{2}(X)\\rightarrow\\mathbb{R}\\cup\\{+\\infty\\}$ be proper and l.s.c. Suppose that $\\mathcal{G}$ is convex along generalized geodesics. Let $\\nu\\in\\mathscr{P}_{2,\\mathrm{abs}}(X)$ , $\\mu,\\pi\\in{\\mathcal{P}}_{2}(X)$ . If $\\xi\\in\\partial\\mathcal{G}(\\mu)$ , then ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\int_{X}{\\langle\\xi\\circ T_{\\nu}^{\\mu}(x),T_{\\nu}^{\\pi}(x)-T_{\\nu}^{\\mu}(x)\\rangle}d\\nu(x)\\leq\\mathcal{G}(\\pi)-\\mathcal{G}(\\mu).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "A.1 Existence of a Borel measurable selector of the subdifferential of a convex function ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Given a convex function $H:X\\,\\rightarrow\\,\\mathbb{R}$ , we prove that there exists a Borel measurable selector $S(x)\\in\\partial H(x)$ . Although this problem is of natural interest, we are not aware of it as well as its proof at least in standard textbooks in convex analysis. Credits go to a quite recent MathOverflow thread [35], from which we give detailed proof as follows. ", "page_idx": 16}, {"type": "text", "text": "Firstly, we recall Alexandroff\u2019s compactification of a topological space $(X,\\tau)$ . From set theory, $X$ is strictly smaller than $2^{X}$ , which is the set of all subsets of $X$ , i.e., there is no bijection from $X$ to $2^{X}$ . So $\\dot{2}^{X}$ cannot be contained in $X$ . So, there is an element named $\\infty$ that is not in $X$ . We denote $X^{\\infty}=X\\cup\\{\\infty\\}$ . One-point Alexandroff compactification states that (1) there exists a topology $\\tau^{\\infty}$ in $X^{\\infty}$ accepting $X$ as a topological subspace, i.e., the original topology $\\tau$ in $X$ is inherited from $\\tau^{\\infty}$ , and (2) $(X^{\\infty},\\tau^{\\infty})$ is compact. ", "page_idx": 16}, {"type": "text", "text": "The topology $\\tau^{\\infty}$ can be specifically described as follows: open sets of $\\tau^{\\infty}$ are either open sets of $\\tau$ or the complements of the form $(X\\setminus S)\\cup\\{\\infty\\}$ where $S$ are closed compact subsets of $X$ . ", "page_idx": 16}, {"type": "text", "text": "In our case, $X=\\ensuremath{\\mathbb{R}}^{d}$ and $\\tau$ is the standard Euclidean topology, the Alexandroff compactification of $X$ is also metrizable [17, Thm. 12.12]. It is, in fact, homeomorphic to the sphere $\\mathbb{S}^{d}$ whose topology is inherited from the ambient space $\\mathbb{R}^{d+1}$ . Moreover, the mentioned metric is the Riemannian metric of $\\mathbb{S}^{d}$ [49, Thm. 13.29]. ", "page_idx": 16}, {"type": "text", "text": "Secondly, since $\\psi(x):=H(x)+(1/2)\\|x\\|^{2}$ is 1-strongly convex, its Fenchel conjugate $y\\mapsto\\psi^{*}(y)$ defined as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\psi^{*}(y)=\\operatorname*{sup}_{x\\in\\mathbb{R}^{d}}\\{\\langle x,y\\rangle-\\psi(x)\\}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "is 1-smooth [72, Thm. 1]. ", "page_idx": 16}, {"type": "text", "text": "By [64, Cor. 23.5.1], $(\\partial\\psi)^{-1}=\\nabla\\psi^{*}$ in the sense that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{y\\in\\partial\\psi(\\nabla\\psi^{*}(y))\\quad\\forall y\\in\\mathbb{R}^{d}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "On the other hand, $\\partial\\psi$ is strongly monotone [54, Ex. 3.9] in the following sense, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\langle x_{2}-x_{1},y_{2}-y_{1}\\rangle\\geq\\|x_{2}-x_{1}\\|^{2}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "for all $x_{1},x_{2}\\in\\mathbb{R}^{d}$ and $y_{1}\\in\\partial\\psi(x_{1}),y_{2}\\in\\partial\\psi(x_{2})$ . ", "page_idx": 16}, {"type": "text", "text": "We see that $\\nabla\\psi^{*}$ is subjective. Indeed we show that for every $x\\in\\mathbb{R}^{d}$ , there exists $y\\in\\mathbb{R}^{d}$ such that $\\nabla\\psi^{*}(y)=x$ . We show that relation holds for any $y\\in\\partial\\psi(x)$ . By contradiction, suppose that $\\nabla\\psi^{*}(y)\\neq x$ . From the strong monotonicity of $\\partial\\psi$ as in (14), $\\partial\\dot{\\psi}(\\nabla\\psi^{*}(y))\\cap\\partial\\psi(x)=\\emptyset$ . However, from (13) and by the choice of $y$ , it holds $y\\in\\partial\\psi(\\nabla\\psi^{*}(y))\\cap\\partial\\psi(x)$ . This is a contradiction. ", "page_idx": 16}, {"type": "text", "text": "Thirdly, we recall a fundamental result on the compactness of the subdifferential of a convex function: if $C$ is compact, then $\\partial\\psi(C)$ is compact [64, Thm. 24.7]. ", "page_idx": 16}, {"type": "text", "text": "Fourthly, we need the Federer-Morse theorem [13] as follows: ", "page_idx": 16}, {"type": "text", "text": "Theorem 7. Let $Z$ be a compact metric space, $Y$ be a Hausdorff topological space and $f:Z\\to Y$ be a continuous mapping. Then, there exists a Borel set $B\\subset Z$ such that $f(B)=f(Z)$ and $f$ is injective on $B$ . Furtheremore, $f^{-1}:f(Z)\\to B$ is Borel. ", "page_idx": 16}, {"type": "text", "text": "Now we observe that $\\nabla\\psi^{*}(x)\\to\\infty$ was $x\\to\\infty$ . Otherwise, by using the compactness of $\\partial\\psi$ and (13), we will get a contradiction immediately. We then can extend $\\nabla\\psi^{*}$ in a continuous way in the Alexandorff compactification space $X^{\\infty}=\\mathbf{\\dot{R}}^{d}\\cup\\{\\infty\\}$ by simply putting $\\nabla\\psi^{*}(\\infty)=\\infty$ . We shall show that this extension of $\\nabla\\psi^{*}$ is continuous from $(X^{\\infty},\\tau^{\\infty})$ to $(X^{\\infty},\\tau^{\\infty})$ , or $(\\nabla\\psi^{*})^{-1}(V)\\in\\tau^{\\infty}$ for all $V\\,\\in\\,\\tau^{\\infty}$ . Recall that, by construction, open sets of $\\tau^{\\infty}$ are either open sets of $\\tau$ or the complements of the form $(X\\setminus{\\dot{S}})\\cup\\{\\infty\\}$ where $S$ are compact subsets of $X$ . The former type of open sets is handled easily since $\\nabla\\psi^{*}$ is already continuous in $(X,\\tau)$ . For the latter type, let $U=(\\nabla\\psi^{*})^{-1}((X\\setminus S)\\cup\\{\\ln\\})=(\\nabla\\psi^{*})^{-1}(X\\setminus S)\\cup\\{\\infty\\}=(\\dot{X}\\setminus(\\vec{\\nabla}\\psi^{*})^{-1}(S))\\cup\\{\\infty\\}$ for a compact set $S$ . Proving $U$ open boils down to proving $(\\nabla\\psi^{*})^{-1}(S)$ compact. Indeed, it is closed since $S$ is closed. It is bounded. Otherwise, it will be contradictory to $\\nabla\\psi^{*}(x)\\to\\infty$ as $x\\to\\infty$ . ", "page_idx": 16}, {"type": "text", "text": "We now can apply the Federer-Morse theorem for $Z=Y=(X^{\\infty},\\tau^{\\infty})$ by noting that $(X^{\\infty},\\tau^{\\infty})$ is metrizable and a metric space is a Hausdorff space, and for $f=\\nabla\\psi^{*}$ : there exists a Borel set $B\\subset$ ", "page_idx": 16}, {"type": "text", "text": "$X^{\\infty}$ such that $\\nabla\\psi^{*}|_{B}:B\\rightarrow X^{\\infty}$ is a bijection and the inverse mapping $(\\nabla\\psi^{*}|_{B})^{-1}:X^{\\infty}\\rightarrow B$ is Borel measurable (here Borel set/measurability are with respect to $\\tau^{\\infty}$ , not yet $\\tau$ ). This is the Borel (w.r.t. $\\tau^{\\infty}$ ) selector of $\\partial\\psi$ . ", "page_idx": 17}, {"type": "text", "text": "Finally, we need to convert Borel measurability w.r.t. $\\tau^{\\infty}$ to Borel measurability w.r.t. $\\tau$ . In terms of mapping, $\\infty$ is mapped to $\\infty$ either way around. So we only need to show: $(\\nabla\\dot{\\psi}^{*}|_{B})^{-1}:X\\to X$ is Borel measurable w.r.t. $\\tau$ . Take any Borel set (w.r.t. $\\tau$ ) $E\\subset X$ , $(\\nabla\\psi^{*}|_{B})(\\bar{B}\\cap\\dot{E})$ is Borel set w.r.t. $\\tau^{\\infty}$ and does not contain $\\infty$ . We shall prove $(\\nabla\\psi^{*}|_{B})(B\\cap E)$ is a Borel set w.r.t. $\\tau$ . This follows directly from the following claim, which is from another Mathematics Stack Exchange thread [19]. Claim [19]: ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "equation", "text": "$$\n\\sigma(\\tau^{\\infty})=\\sigma(\\tau\\cup\\{\\infty\\})=\\sigma(\\tau)\\cup\\{V\\cup\\{\\infty\\}:V\\in\\sigma(\\tau)\\}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "A sketch of the claim proof goes as follows. For the first equality in (15), first we have $\\sigma(\\tau\\cup\\{\\infty\\})\\subset$ $\\sigma(\\tau^{\\infty})$ because (1) $\\tau\\subset\\tau^{\\infty}$ and (2) $\\{\\infty\\}=X^{\\infty}\\setminus X\\in\\sigma(\\tau^{\\infty})$ as $X,X^{\\infty}\\,\\in\\,\\tau^{\\infty}$ . On the other hand, $\\sigma(\\tau^{\\infty})\\subset\\sigma(\\tau\\cup\\{\\infty\\})$ because, again, of the construction of $\\tau^{\\infty}$ : let $U\\in\\tau^{\\infty}$ , if $U\\in\\tau$ then $U\\in\\sigma(\\tau\\cup\\{\\infty\\})$ , otherwise $U=(X\\setminus S)\\cup\\{\\infty\\}$ for some compact set $S\\subset X$ . As $X=\\ensuremath{\\mathbb{R}}^{d}$ , $S$ is closed, so $(X\\setminus S)\\in\\tau$ implying $U\\in\\sigma(\\tau\\cup\\{\\infty\\})$ . For the second equality in (15), it is straight forward to verify that ${\\mathcal{G}}\\,:=\\,\\sigma(\\tau)\\cup\\left\\{V\\cup\\left\\{\\infty\\right\\}\\,:\\,V\\,\\in\\,\\sigma(\\tau)\\right\\}$ is a sigma-algebra in $X^{\\infty}$ . Since $\\tau\\cup\\{\\infty\\}\\subset\\mathcal{G}$ , it holds $\\sigma(\\tau\\cup\\{\\infty\\})\\subset\\mathcal{G}$ . Conversely, as $\\sigma(\\tau)\\overset{\\cdot}{\\subset}\\sigma(\\tau\\cup\\{\\infty\\})$ and - consequently - $V\\cup\\{\\infty\\}\\in\\sigma(\\tau\\cup\\{\\infty\\})$ for all $V\\in\\sigma(\\tau)$ , it holds ${\\mathcal{G}}\\subset\\sigma(\\tau\\cup\\{\\infty\\})$ . ", "page_idx": 17}, {"type": "text", "text": "We conclude that $(\\nabla\\psi^{*}|_{B})^{-1}:X\\to X$ is a Borel (w.r.t. $\\tau$ ) selector of $\\partial\\psi$ . As a consequence, $S:=(\\nabla\\psi^{*}|_{B})^{-1}-I$ is a Borel measurable selector of $\\partial H$ . ", "page_idx": 17}, {"type": "text", "text": "A.2 Maximum Mean Discrepancy ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "A.2.1 Definition ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "The notion of Maximum Mean Discrepancy (MMD) between two distributions is introduced in [34]. Given a kernel $k:X\\times X\\to\\mathbb{R}$ and denote by $\\kappa$ its reproducing kernel Hilbert space. Given two distributions $\\mu$ and $\\nu$ , the maximum mean discrepancy between them are defined as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathrm{D}_{\\mathrm{MML}}\\left(\\mu,\\nu\\right)=\\|f_{\\mu,\\nu}\\|_{K},\\quad\\mathrm{where~}f_{\\mu,\\nu}(z)=\\int k(x,z)d\\nu(x)-\\int k(x,z)d\\mu(x),\\,\\forall z\\in X,\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $f_{\\mu,\\nu}$ is called the witness function. Given some target (or optimal) distribution $\\mu^{*}$ , we seek to optimize $\\begin{array}{r}{\\mathcal{F}(\\mu)=\\frac{1}{2}\\,\\mathrm{D}_{\\mathrm{MML}}\\left(\\mu,\\mu^{*}\\right)^{2}}\\end{array}$ . Note that ${\\mathcal{F}}(\\mu)$ admits the following free-energy expression [7] ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathcal F(\\mu)=\\int F(x)d\\mu(x)+\\frac{1}{2}\\int k(x,x^{\\prime})d\\mu(x)d\\mu(x^{\\prime})+C\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where ", "page_idx": 17}, {"type": "equation", "text": "$$\nF(x)=-\\int k(x,x^{\\prime})d\\mu^{*}(x^{\\prime}),\\quad C=\\frac{1}{2}\\int k(x,x^{\\prime})d\\mu^{*}(x)d\\mu^{*}(x^{\\prime}).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "In general, $\\mathcal{F}$ is not convex along generalized geodesics. Rather, it exhibits some weakly convex structure [7] that falls within the DC spectrum as detailed in Subsection A.2.3. ", "page_idx": 17}, {"type": "text", "text": "A.2.2 Connection with infinite-width one hidden layer neural networks ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "See also [7, 65]. We include the discussion here for completeness. ", "page_idx": 17}, {"type": "text", "text": "Consider a one-hidden-layer neural network ", "page_idx": 17}, {"type": "equation", "text": "$$\nf(x)={\\frac{1}{n}}\\sum_{i=1}^{n}\\psi(x,z_{i})\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\psi(x,z_{i})=w_{i}\\sigma(\\left\\langle x,\\theta_{i}\\right\\rangle)$ , $z_{i}=(\\theta_{i},w_{i})\\in Z$ is the parameters (in and out) associated with $i$ -th hidden neuron, $n$ is the number of hidden neurons, $\\sigma$ is the activation function. ", "page_idx": 17}, {"type": "text", "text": "When the number of hidden neurons $n$ tends to infinity, $f$ can be written as $\\begin{array}{r}{f(x)=\\int\\psi(x,z)d\\mu(z)}\\end{array}$ for some distribution $\\mu$ over the parameter space $Z$ . Given data $(x,y)\\sim\\,p(x,y)$ and assuming quadratic loss, the optimization problem reads ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mu\\in\\mathcal{P}_{2}(Z)}\\mathcal{L}(\\mu):=\\mathbb{E}_{(x,y)\\sim p(x,y)}\\left(y-\\int\\psi(x,z)d\\mu(z)\\right)^{2}\\!.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We assume that the model is well-specified, i.e., there exists $\\mu^{*}\\in\\mathcal{P}_{2}(Z)$ such that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}_{y\\sim p(y|x)}(y)=\\int\\psi(x,z)d\\mu^{*}(z).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Expanding $\\mathcal{L}$ , we get ", "page_idx": 18}, {"type": "equation", "text": "$$\n{\\mathcal L}(\\mu)=-2\\int\\mathbb{E}_{(x,y)\\sim p(x,y)}(y\\psi(x,z))d\\mu(z)+\\iint\\mathbb{E}_{x\\sim p(x)}(\\psi(x,z)\\psi(x,z^{\\prime}))d\\mu(z)d\\mu(z^{\\prime})+C\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $C$ does not depend on $\\mu$ . ", "page_idx": 18}, {"type": "text", "text": "Naturally, we define the kernel ", "page_idx": 18}, {"type": "equation", "text": "$$\nk(z,z^{\\prime})=\\mathbb{E}_{x\\sim p(x)}(\\psi(x,z)\\psi(x,z^{\\prime}))\\quad\\forall z,z^{\\prime}\\in Z.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Using the assumption that the model is well-specified (16), we obtain ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\quad\\displaystyle\\int k(z,z^{\\prime})d\\mu^{*}(z^{\\prime})=\\int\\mathbb{E}_{x\\sim p(x)}(\\psi(x,z)\\psi(x,z^{\\prime}))d\\mu^{*}(z^{\\prime})}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad=\\mathbb{E}_{x\\sim p(x)}\\left(\\int\\psi(x,z)\\psi(x,z^{\\prime})d\\mu^{*}(z^{\\prime})\\right)}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad=\\mathbb{E}_{x\\sim p(x)}\\left(\\psi(x,z)\\int\\psi(x,z^{\\prime})d\\mu^{*}(z^{\\prime})\\right)}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad=\\mathbb{E}_{x\\sim p(x)}\\left(\\psi(x,z)\\mathbb{E}_{y\\sim p(y\\mid x)}(y)\\right)}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad=\\mathbb{E}_{(x,y)\\sim p(x,y)}\\left(y\\psi(x,z)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Therefore, $\\mathcal{L}$ can be written as ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\mu)=-2\\int\\left[\\int k(z,z^{\\prime})d\\mu^{*}(z^{\\prime})\\right]d\\mu(z)+\\iint k(z,z^{\\prime})d\\mu(z)d\\mu(z^{\\prime})+C.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "This is exactly the MMD setting in Sect. A.2.1. ", "page_idx": 18}, {"type": "text", "text": "A.2.3 DC structure ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Let $k$ be a kernel whose gradient is Lipschitz continuous, i.e., for some $L>0$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\nabla k(x,y)-\\nabla k(x^{\\prime},y^{\\prime})\\|\\leq L\\left(\\|x-x^{\\prime}\\|^{2}+\\|y-y^{\\prime}\\|^{2}\\right)^{\\frac{1}{2}},\\quad\\forall x,x^{\\prime},y,y^{\\prime}\\in X,}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "which can be expressed equivalently as ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\nabla_{x}k(x,y)-\\nabla_{x}k(x^{\\prime},y^{\\prime})\\|^{2}+\\|\\nabla_{y}k(x,y)-\\nabla_{y}k(x^{\\prime},y^{\\prime})\\|^{2}\\leq L^{2}\\left(\\|x-x^{\\prime}\\|^{2}+\\|y-y^{\\prime}\\|^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Let $\\mu^{*}$ be some target distribution and consider the free-energy functional ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathcal F(\\mu)=\\iint k(x,y)d\\mu(x)d\\mu(y)-2\\iint k(x,y)d\\mu^{*}(y)d\\mu(x).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Let $\\alpha>0$ , we can rewrite $\\mathcal{F}$ as follows ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\tau(\\mu)=\\iint\\left[\\alpha\\|x\\|^{2}+\\alpha\\|y\\|^{2}+k(x,y)\\right]d\\mu(x)d\\mu(y)-2\\int\\left[\\alpha\\|x\\|^{2}+\\int k(x,y)d\\mu^{*}(y)\\right]d\\mu(x).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "As $k$ is Lipschitz smooth w.r.t. $(x,y),x\\mapsto\\textstyle\\int k(x,y)d\\mu^{*}(y)$ is also Lipschitz smooth. Indeed, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\nabla_{x}\\int k(x,y)d\\mu^{*}(y)-\\nabla_{x}\\int k(x^{\\prime},y)d\\mu^{*}(y)\\right|}\\\\ &{=\\left\\|\\int\\left(\\nabla_{x}k(x,y)-\\nabla_{x}k(x^{\\prime},y))d\\mu^{*}(y)\\right\\|}\\\\ &{\\leq\\left(\\int\\|\\nabla_{x}k(x,y)-\\nabla_{x}k(x^{\\prime},y)\\|^{2}\\,d\\mu^{*}(y)\\right)^{\\frac{1}{2}}}\\\\ &{\\leq L\\left(\\int\\|x-x^{\\prime}\\|^{2}\\,d\\mu^{*}(y)\\right)^{\\frac{1}{2}}}\\\\ &{=L\\|x-x^{\\prime}\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Next, as a standard result, if $f$ is an L-smooth function, $x\\;\\mapsto\\;(\\alpha/2)\\|x\\|^{2}\\pm f(x)$ are convex whenever $\\alpha\\geq L$ . Therefore, for $\\alpha\\geq L$ , $W(x,y):=\\alpha\\|x\\|^{2}+\\dot{\\alpha}\\|\\dot{y}\\|^{2}\\overset{\\cdot}{+}\\overset{\\cdot}{k}(x,\\dot{y})$ is convex and $\\begin{array}{r}{F(x)=-2\\left[\\alpha\\|x\\|^{2}+\\int_{X}k(x,y)d\\mu^{*}(y)\\right]}\\end{array}$ is concave. From [4, Prop. 9.3.5], the interaction energy corresponding to $W$ is generalized geodesically convex. ", "page_idx": 19}, {"type": "text", "text": "A.3 Proof of Lemma 1 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Since $I+\\eta S$ is a subgradient selector of a convex function, the optimal transport between $\\mu_{n}$ and $\\nu_{n+1}$ is given by ", "page_idx": 19}, {"type": "equation", "text": "$$\nT_{\\mu_{n}}^{\\nu_{n+1}}=I+\\eta S.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and between $\\mu_{n+1}$ and $\\nu_{n+1}$ [4, Lem. 10.1.2] ", "page_idx": 19}, {"type": "equation", "text": "$$\nT_{\\mu_{n+1}}^{\\nu_{n+1}}\\in I+\\eta\\partial\\left(\\mathcal{E}_{G}+\\mathcal{H}\\right)(\\mu_{n+1}).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Since $\\mathcal{E}_{H}$ is convex along generalized geodesics [4, Prop. 9.3.2] and $S$ is a subgradient of $\\mathcal{E}_{H}$ at $\\mu_{n}$ [5, Proposition 4.13], by Lem. 10 it holds, for any $\\nu\\in\\mathscr{P}_{2,\\mathrm{abs}}(X)$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathcal{E}_{H}(\\mu_{n+1})\\geq\\mathcal{E}_{H}(\\mu_{n})+\\int_{X}\\langle S\\circ T_{\\nu}^{\\mu_{n}}(x),T_{\\nu}^{\\mu_{n+1}}(x)-T_{\\nu}^{\\mu_{n}}(x)\\rangle d\\nu(x).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "By choosing $\\nu=\\nu_{n+1}$ (note that $\\nu_{n+1}\\in\\mathcal{P}_{2,\\mathrm{abs}}(X))$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l c l}{\\displaystyle\\mathcal E_{H}(\\mu_{n+1})\\geq\\mathcal E_{H}(\\mu_{n})+\\int_{X}\\left<S\\circ T_{\\nu_{n+1}}^{\\mu_{n}}(x),T_{\\nu_{n+1}}^{\\mu_{n+1}}(x)-T_{\\nu_{n+1}}^{\\mu_{n}}(x)\\right>\\!d\\nu_{n+1}(x)}\\\\ {\\displaystyle=\\mathcal E_{H}(\\mu_{n})+\\frac1\\eta\\int_{X}\\left<(T_{\\mu_{n}}^{\\nu_{n+1}}-I)\\circ T_{\\nu_{n+1}}^{\\mu_{n}}(x),T_{\\nu_{n+1}}^{\\mu_{n+1}}(x)-T_{\\nu_{n+1}}^{\\mu_{n}}(x)\\right>\\!d\\nu_{n+1}(x)}\\\\ {\\displaystyle=\\mathcal E_{H}(\\mu_{n})+\\frac1\\eta\\int_{X}\\left<x-T_{\\nu_{n+1}}^{\\mu_{n}}(x),T_{\\nu_{n+1}}^{\\mu_{n+1}}(x)-T_{\\nu_{n+1}}^{\\mu_{n}}(x)\\right>\\!d\\nu_{n+1}(x)}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where the second equality uses (17) and the last one uses Lem. 3. ", "page_idx": 19}, {"type": "text", "text": "On the other hand, since $\\mathcal{E}_{G}+\\mathcal{H}$ is convex along generalized geodesics, by applying Lem. 10 for $\\mathcal{E}_{G}+\\mathcal{H}$ at $\\mu_{n+1}$ with a subgradient $\\eta^{-1}(T_{\\mu_{n+1}}^{\\nu_{n+1}}-\\overline{{I}})\\in\\partial(\\mathcal{E}_{G}+\\mathcal{H})(\\mu_{n+1})$ (from (18)), ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(\\mathcal{E}_{G}+\\mathcal{H})(\\mu_{n})}\\\\ &{\\geq(\\mathcal{E}_{G}+\\mathcal{H})(\\mu_{n+1})+\\displaystyle\\int_{X}\\bigg\\langle\\frac{(T_{\\mu_{n+1}}^{\\nu_{n+1}}-I)}{\\eta}\\circ T_{\\nu_{n+1}}^{\\mu_{n+1}}(x),T_{\\nu_{n+1}}^{\\mu_{n}}(x)-T_{\\nu_{n+1}}^{\\mu_{n+1}}(x)\\bigg\\rangle d\\nu_{n+1}(x)}\\\\ &{=(\\mathcal{E}_{G}+\\mathcal{H})(\\mu_{n+1})+\\displaystyle\\frac{1}{\\eta}\\int_{X}\\bigg\\langle x-T_{\\nu_{n+1}}^{\\mu_{n+1}}(x),T_{\\nu_{n+1}}^{\\mu_{n}}(x)-T_{\\nu_{n+1}}^{\\mu_{n+1}}(x)\\bigg\\rangle d\\nu_{n+1}(x)\\qquad\\qquad\\mathrm{otherwise}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where the last equality uses Lem. 3. ", "page_idx": 19}, {"type": "text", "text": "By adding (20) and (19) side by side, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathcal{F}(\\mu_{n})\\geq\\mathcal{F}(\\mu_{n+1})+\\frac{1}{\\eta}\\int_{X}\\|T_{\\nu_{n+1}}^{\\mu_{n}}(x)-T_{\\nu_{n+1}}^{\\mu_{n+1}}(x)\\|^{2}d\\nu_{n+1}(x).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "A.4 Proof of Theorem 1 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Let $\\mu^{*}\\in\\mathcal{P}_{2}(X)$ be a cluster point of $\\{\\mu_{n}\\}_{n\\in\\mathbb{N}}$ . There exists a subsequence \u2212W\u2212ass\u2192\u00b5\u2217. It holds ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{k\\to\\infty}{\\operatorname*{lim}\\operatorname*{inf}}\\,\\mathcal{F}(\\mu_{n_{k}})=\\underset{k\\to\\infty}{\\operatorname*{lim}\\operatorname*{inf}}\\,(\\mathcal{H}(\\mu_{n_{k}})+\\mathcal{E}_{F}(\\mu_{n_{k}}))}\\\\ &{\\qquad\\qquad\\qquad=\\underset{k\\to\\infty}{\\operatorname*{lim}\\operatorname*{inf}}\\,\\mathcal{H}(\\mu_{n_{k}})+\\mathcal{E}_{F}(\\mu^{*})}\\\\ &{\\qquad\\qquad\\qquad\\geq\\mathcal{H}(\\mu^{*})+\\mathcal{E}_{F}(\\mu^{*}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "since $\\mathcal{H}$ is l.s.c. and $\\mathcal{E}_{F}$ is continuous w.r.t. Wasserstein topology. Therefore, $\\mathcal{H}(\\mu^{*})<+\\infty$ , which further implies that $\\mu^{*}\\in\\mathcal{P}_{2,\\mathrm{abs}}(X)$ . ", "page_idx": 20}, {"type": "text", "text": "We have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\displaystyle\\int_{X}\\|T_{\\nu_{n+1}}^{\\mu_{n}}(x)-T_{\\nu_{n+1}}^{\\mu_{n+1}}(x)\\|^{2}d\\nu_{n+1}(x)=\\displaystyle\\int_{X}\\|T_{\\nu_{n+1}}^{\\mu_{n}}(x)-T_{\\nu_{n+1}}^{\\mu_{n+1}}(x)\\|^{2}d T_{\\mu_{n}}^{\\nu_{n+1}}_{\\#}\\mu_{n}(x)}}\\\\ {{=\\displaystyle\\int_{X}\\|x-T_{\\nu_{n+1}}^{\\mu_{n+1}}\\circ T_{\\mu_{n}}^{\\nu_{n+1}}(x)\\|^{2}d\\mu_{n}(x).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We observe that $T_{\\nu_{n+1}}^{\\mu_{n+1}}\\circ T_{\\mu_{n}}^{\\nu_{n+1}}$ is a (possibly non-optimal) transport pushing $\\mu_{n}$ to $\\mu_{n+1}$ , by the optimality of $T_{\\mu_{n}}^{\\mu_{n}+1}$ , ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\int_{X}\\|x-T_{\\nu_{n}+1}^{\\mu_{n+1}}\\circ T_{\\mu_{n}}^{\\nu_{n+1}}(x)\\|^{2}d\\mu_{n}(x)\\geq\\int_{X}\\|x-T_{\\mu_{n}}^{\\mu_{n+1}}(x)\\|^{2}d\\mu_{n}(x)=W_{2}^{2}(\\mu_{n},\\mu_{n+1}).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "By Lem. 1 and (22), (23), ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathcal{F}(\\mu_{n})\\geq\\mathcal{F}(\\mu_{n+1})+\\frac{1}{\\eta}W_{2}^{2}(\\mu_{n},\\mu_{n+1}).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Note that $\\mathcal{F}$ is bounded below (Assumption 1), telescoping (24) gives us ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\sum_{n=0}^{\\infty}W_{2}^{2}(\\mu_{n},\\mu_{n+1})<+\\infty.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "In particular, $W_{2}(\\mu_{n},\\mu_{n+1})\\to0$ . This together with $\\mu_{n_{k}}\\xrightarrow[]{\\mathrm{Wass}}\\mu^{*}$ implies $\\mu_{n_{k}+1}\\xrightarrow[]{\\mathrm{Wass}}\\mu^{*}$ . ", "page_idx": 20}, {"type": "text", "text": "Under Assumption 3, $S=\\nabla H$ and $S$ is continuous. Next, recall $\\nu_{n_{k}+1}=(I+\\eta S)_{\\#}\\mu_{n_{k}}$ , we show ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\nu_{n_{k}+1}\\xrightarrow{\\mathrm{narrow}}\\nu^{*}:=(I+\\eta S)_{\\#}\\mu^{*}\\;\\mathrm{as}\\;k\\to+\\infty.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Thus, let $f$ be a continuous and bounded test functional in $X$ , by using transfer lemma 2, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\operatorname*{lim}_{k\\to\\infty}\\int_{X}f(x)d\\nu_{n_{k}+1}(x)=\\operatorname*{lim}_{k\\to\\infty}\\int_{X}f(x)d(I+\\eta S)_{\\#\\mu_{n_{k}}}(x)}}\\\\ &{}&{\\quad=\\operatorname*{lim}_{k\\to\\infty}\\int_{X}f(x+\\eta S(x))d\\mu_{n_{k}}(x)}\\\\ &{}&{\\quad=\\displaystyle\\int_{X}f(x+\\eta S(x))d\\mu^{*}(x)}\\\\ &{}&{\\quad=\\int_{X}f(x)d\\nu^{*}(x),}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "since S is continuous. So \u03bdnk+1 $\\nu_{n_{k}+1}\\xrightarrow[]{\\mathrm{nartow}}\\nu^{*}$ . We go one step further and prove that $\\nu_{n_{k}+1}$ actually converges to $\\nu^{*}$ in the Wasserstein metric. This boils down to showing convergence in second-order moments, i.e., ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathfrak{m}_{2}(\\nu_{n_{k}+1})\\to\\mathfrak{m}_{2}(\\nu^{*}),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "which is equivalent to showing that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\int_{X}\\|x+\\eta S(x)\\|^{2}d\\mu_{n_{k}}(x)\\to\\int_{X}\\|x+\\eta S(x)\\|^{2}d\\mu^{*}(x).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "On the other hand, $\\psi(x):=\\|x{+}\\eta S(x)\\|^{2}$ has quadratic growth (follows from Lem. 4) and $\\mu_{n_{k}}\\to\\mu^{*}$ in the Wasserstein metric, so [3, Prop. 2.4] ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{k\\to\\infty}\\int_{X}\\|x+\\eta S(x)\\|^{2}d\\mu_{n_{k}}(x)=\\int_{X}\\|x+\\eta S(x)\\|^{2}d\\mu^{\\ast}(x)\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Therefore, $\\nu_{n_{k}+1}\\to\\nu^{*}$ in Wasserstein metric. ", "page_idx": 21}, {"type": "text", "text": "To proceed further, we need the following theorem stating that the graph of the subdifferential of a geodesically convex function is closed under the product of Wasserstein and weak topologies. ", "page_idx": 21}, {"type": "text", "text": "Theorem 8 (Closedness of subdifferential graph). $l^{4};$ , Lemma 10.1.3] Let $\\phi$ be a geodesically convex functional satisfying $\\mathrm{dom}(\\partial\\phi)\\subset\\mathcal{P}_{2,\\mathrm{abs}}(X)$ . Let $\\{\\mu_{n}\\}_{n\\in\\mathbb{N}}$ be a sequence converging in Wasserstein metric to $\\mu\\in\\mathrm{dom}(\\phi)$ . Let $\\xi_{n}\\in\\partial\\phi(\\mu_{n})$ be satisfying ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{n\\in\\mathbb{N}}\\int_{X}\\|\\xi_{n}(x)\\|^{2}d\\mu_{n}(x)<+\\infty,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and converging weakly to $\\xi\\in L^{2}(X,X,\\mu)$ in the following sense: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{n\\to\\infty}\\int_{X}\\zeta(x)\\xi_{n}(x)d\\mu_{n}(x)=\\int_{X}\\zeta(x)\\xi(x)d\\mu(x),\\quad\\forall\\zeta\\in C_{c}^{\\infty}(X).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Then $\\xi\\in\\partial\\phi(\\mu)$ . ", "page_idx": 21}, {"type": "text", "text": "As a side note, we need the notion of weak convergence in the above theorem because \u2013 unlike subdifferentials in flat Euclidean space \u2013 each $\\xi_{n}$ lives in its own $L^{2}(X,X,\\mu_{n})$ space. ", "page_idx": 21}, {"type": "text", "text": "Back to our proof, for item (29), we show that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{n\\in\\mathbb{N}}\\int_{X}\\big\\|T_{\\mu_{n}}^{\\nu_{n}}(x)-x\\big\\|^{2}d\\mu_{n}(x)<+\\infty.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "We proceed as follows to prove (31). We first show that $\\operatorname*{sup}_{n\\in\\mathbb{N}}\\mathfrak{m}_{2}(\\mu_{n})<+\\infty$ . By contradiction, by assuming $\\operatorname*{sup}_{n\\in\\mathbb{N}}\\mathfrak{m}_{2}(\\mu_{n})=+\\infty$ , we can extract a subsequence $\\{\\mu_{n_{k}}\\}_{k\\in\\mathbb{N}}$ such that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{k\\rightarrow\\infty}{\\mathfrak{m}}_{2}(\\mu_{n_{k}})=+\\infty.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "By compactness assumption, there further exists a subsequence $\\{\\mu_{n_{k_{i}}}\\}_{i\\in\\mathbb{N}}$ such that $\\mu_{n_{k_{i}}}$ converges (in Wasserstein metric) to some $\\mu^{**}\\in\\mathcal{P}_{2}(X)$ , implying that $\\begin{array}{r}{\\operatorname*{lim}_{i\\rightarrow\\infty}\\mathfrak{m}_{2}(\\mu_{n_{k_{i}}})=\\mathfrak{m}_{2}(\\bar{\\mu}^{**})}\\end{array}$ , which contradicts (32). Therefore, $\\operatorname*{sup}_{n\\in\\mathbb{N}}\\mathfrak{m}_{2}(\\mu_{n})<+\\infty$ . We next show that $\\operatorname*{sup}_{n\\in\\mathbb{N}}\\mathfrak{m}_{2}(\\nu_{n})<+\\infty$ . Indeed, as $\\|S\\|^{2}$ has quadratic growth, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\lVert S(x)\\rVert^{2}\\leq c(\\lVert x\\rVert^{2}+1)\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "for some $c>0$ . So ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\operatorname*{m}_{2}(\\nu_{n+1})=\\int_{X}\\|x\\|^{2}d\\nu_{n+1}(x)}}\\\\ &{}&{=\\int_{X}\\|x\\|^{2}d(I+\\eta S)_{\\#}\\mu_{n}(x)}\\\\ &{}&{=\\int_{X}\\|x+\\eta S(x)\\|^{2}d\\mu_{n}(x)}\\\\ &{}&{\\leq2\\int_{X}\\|x\\|^{2}d\\mu_{n}(x)+2\\eta^{2}\\int_{X}\\|S(x)\\|^{2}d\\mu_{n}(x)}\\\\ &{}&{\\leq(2+2c\\eta^{2})\\mathsf{m}_{2}(\\mu_{n})+2\\eta^{2}c,}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "implying that $\\mathrm{sup}_{n\\in\\mathbb{N}}\\,\\mathfrak{m}_{2}(\\nu_{n})\\,<\\,+\\infty$ . This in conjunction with $G$ having quadratic growth implies that $\\operatorname*{sup}_{n\\in\\mathbb{N}}\\left\\vert\\mathcal{E}_{G}(\\nu_{n})\\right\\vert<+\\infty$ . Furthermore, $\\operatorname*{inf}_{n}{\\mathcal{\\left(E_{G}+{\\mathcal{H}}\\right)}(\\mu_{n})}>-\\infty$ otherwise by lower semicontinuity of $\\mathcal{H}$ and compactness of $\\{\\mu_{n}\\}_{n\\in\\mathbb{N}}$ we get a contradiction. ", "page_idx": 21}, {"type": "text", "text": "TNhomw,.  a6s, $\\eta^{-1}(T_{\\mu_{n+1}}^{\\nu_{n+1}}-I)\\in\\partial\\left(\\mathcal{E}_{G}+\\mathcal{H}\\right)\\left(\\mu_{n+1}\\right)$ and $\\mathcal{E}_{G}+\\mathcal{H}$ is geodesically convex, by applying ", "page_idx": 21}, {"type": "equation", "text": "$$\n(\\mathcal{E}_{G}+\\mathcal{H})(\\nu_{n+1})\\geq(\\mathcal{E}_{G}+\\mathcal{H})(\\mu_{n+1})+\\frac{1}{\\eta}\\int_{X}\\|T_{\\mu_{n+1}}^{\\nu_{n+1}}(x)-x\\|^{2}d\\mu_{n+1}(x).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "The finiteness as in (31) then follows from $\\begin{array}{r}{\\operatorname*{sup}_{n\\in\\mathbb{N}}|\\mathcal{E}_{G}(\\nu_{n})|<+\\infty,\\operatorname*{sup}_{n\\in\\mathbb{N}}-(\\mathcal{E}_{G}+\\mathcal{H})(\\mu_{n})<}\\end{array}$ $+\\infty$ as proved and $\\operatorname*{sup}_{n\\in\\mathbb{N}}\\mathcal{H}(\\nu_{n})<+\\infty$ as assumed. ", "page_idx": 22}, {"type": "text", "text": "We next prove that there is a subsequence of $\\{\\mu_{n_{k}}\\}_{k\\in\\mathbb{N}}$ such that ", "page_idx": 22}, {"type": "equation", "text": "$$\nT_{\\mu_{n_{k_{j}}}+1}^{\\nu_{n_{k_{j}}}+1}-I\\rightarrow T_{\\mu^{*}}^{\\nu^{*}}-I\\mathrm{\\,weakly}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We consider the sequence of optimal plans between $\\mu_{n}$ and $\\nu_{n}$ as follows ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\rho_{n}=(I,T_{\\mu_{n}}^{\\nu_{n}})_{\\#}\\mu_{n},\\quad\\forall n\\in\\mathbb{N}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We observe that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathfrak{m}_{2}(\\rho_{n})=\\int_{X\\times X}\\left(\\|x\\|^{2}+\\|y\\|^{2}\\right)d\\rho_{n}(x,y)=\\int_{X}\\|x\\|^{2}d\\mu_{n}(x)+\\int_{X}\\|y\\|^{2}d\\nu_{n}(y)<+\\infty,\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "so $\\rho_{n}\\in{\\mathcal{P}}_{2}(X\\times X)$ for all $n\\in\\mathbb N$ . Since $\\operatorname*{sup}_{n\\in\\mathbb{N}}\\mathfrak{m}_{2}\\big(\\nu_{n}\\big)<+\\infty$ as proved, and as a Wasserstein ball is relatively compact under narrow topology, $\\{\\nu_{n}\\}_{n\\in\\mathbb{N}}$ is relatively compact under narrow topology. The same property holds for $\\{\\mu_{n}\\}_{n\\in\\mathbb{N}}$ . According to Prokhorov [3, Theorem 1.3], $\\{\\mu_{n}\\}_{n\\in\\mathbb{N}}$ and $\\{\\nu_{n}\\}_{n\\in\\mathbb{N}}$ are tight. By [3, Remark 1.4], $\\{\\rho_{n}\\}_{n\\in\\mathbb{N}}$ is also tight, hence relatively compact under narrow topology in $\\mathcal{P}_{2}(X\\times X)$ . Consequently, $\\{\\rho_{n_{k}+1}\\}_{k\\in\\mathbb{N}}$ admits a subsequence converging narrowly to some $\\rho^{*}\\in\\mathcal{P}(X\\times X)$ . Let\u2019s say ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\rho_{n_{k_{i}}+1}\\xrightarrow{\\mathrm{narrow}}\\rho^{*}\\;\\mathrm{as}\\;i\\rightarrow\\infty.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We can see that $\\mathrm{proj}_{1\\#}\\rho^{*}\\,=\\,\\mu^{*},\\mathrm{proj}_{2\\#}\\rho^{*}\\,=\\,\\nu^{*}$ . We further show that $\\rho^{*}\\,\\in\\,\\mathcal P_{2}(X\\,\\times\\,X)$ , or equivalently, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\int_{X\\times X}{\\left(\\|x\\|^{2}+\\|y\\|^{2}\\right)}d\\rho^{*}(x,y)<+\\infty.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Let $C\\in\\mathbb{N}$ , thanks to the narrow convergence in (35), we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\int_{X\\times X}\\operatorname*{min}\\{\\|x\\|^{2}+\\|y\\|^{2},C\\}d\\rho_{n_{k_{i}}+1}(x,y)\\to\\int_{X\\times X}\\operatorname*{min}\\{\\|x\\|^{2}+\\|y\\|^{2},C\\}d\\rho^{*}(x,y).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Furthermore, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\int_{X\\times X}\\operatorname*{min}\\{\\|x\\|^{2}+\\|y\\|^{2},C\\}d\\rho_{n_{k_{i}}+1}(x,y)\\leq\\operatorname*{sup}_{n\\in\\mathbb{N}}\\mathfrak{m}_{2}(\\mu_{n})+\\operatorname*{sup}_{n\\in\\mathbb{N}}\\mathfrak{m}_{2}(\\nu_{n}):=M<+\\infty.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Passing to the limit, we get ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\int_{X\\times X}\\operatorname*{min}\\{\\|x\\|^{2}+\\|y\\|^{2},C\\}d\\rho^{*}(x,y)\\leq M\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "for all $C\\in\\mathbb{N}$ . Sending $C$ to $\\infty$ and applying Monotone Convergence Theorem we derive (36). ", "page_idx": 22}, {"type": "text", "text": "Back to the main proof, since $\\{\\rho_{n_{k_{i}}+1}\\}_{i\\in\\mathbb{N}}$ is a sequence of optimal plans, its limit, $\\rho^{*}$ is also optimal [3, Proposition 2.5]. Therefore, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\rho^{*}=(I,T_{\\mu^{*}}^{\\nu^{*}})_{\\#}\\mu^{*}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Moreover, as ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathfrak{m}_{2}(\\rho_{n_{k_{i}}+1})=\\mathfrak{m}_{2}(\\mu_{n_{k_{i}}+1})+\\mathfrak{m}_{2}(\\nu_{n_{k_{i}}+1})\\to\\mathfrak{m}_{2}(\\mu^{*})+\\mathfrak{m}_{2}(\\nu^{*})=\\mathfrak{m}_{2}(\\rho^{*}),\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\rho_{n_{k_{i}}+1}\\xrightarrow{\\mathrm{Wass}}\\rho^{*}\\;\\mathrm{as}\\;i\\rightarrow\\infty.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Now let\u2019s take any test function $\\zeta\\in C_{c}^{\\infty}(X)$ , we show ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{j\\to\\infty}\\int_{X}\\zeta(x){T^{\\nu_{n_{k_{j}}}+1}}(x)d\\mu_{n_{k_{j}}+1}(x)=\\int_{X}\\zeta(x){T^{\\nu}_{\\mu^{*}}}(x)d\\mu^{*}(x).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Indeed, $(x,y)\\mapsto\\zeta(x)\\operatorname{proj}_{i}(y)$ where $\\mathrm{proj}_{i}$ is the projection into the $i$ -th coordinate is continuous and has quadratic growth since $\\zeta(x)$ is bounded and $\\mathrm{proj}_{i}(y)$ is linear. ", "page_idx": 23}, {"type": "text", "text": "Since \u03c1nk+1 $\\rho_{n_{k_{j}}+1}\\xrightarrow{\\mathrm{Wass}}\\rho^{*}$ \u2212W\u2212ass\u2192\u03c1\u2217, it holds: for each i \u2208[d], ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\operatorname*{lim}_{j\\to\\infty}\\int_{X}\\big\\langle x\\big\\rangle\\operatorname{proj}_{i}\\Big\\langle T_{\\mu_{n_{j}}+1}^{\\nu_{n_{i_{j}}+1}}(x)\\Big\\rangle d\\mu_{n_{k_{j}}+1}(x)}\\\\ {\\displaystyle=\\operatorname*{lim}_{j\\to\\infty}\\int_{X}\\zeta(x)\\operatorname{proj}_{i}(y)d(I,T_{\\mu_{n_{j}}+1}^{\\nu_{n_{i_{j}}+1}})\\#\\mu_{n_{k_{j}}+1}(x,y)}\\\\ {\\displaystyle=\\operatorname*{lim}_{j\\to\\infty}\\int_{X}\\zeta(x)\\operatorname{proj}_{i}(y)d\\rho_{n_{k_{j}}+1}(x,y)}\\\\ {\\displaystyle=\\int_{X}\\zeta(x)\\operatorname{proj}_{i}(y)d\\rho^{\\ast}(x,y)}\\\\ {\\displaystyle=\\int_{X}\\zeta(x)\\operatorname{proj}_{i}(y)d(I,T_{\\mu^{\\ast}}^{\\nu})\\#\\mu^{\\ast}(x,y)}\\\\ {\\displaystyle=\\int_{X}\\zeta(x)\\operatorname{proj}_{i}(T_{\\mu^{\\ast}}^{\\nu}(x))d\\mu^{\\ast}(x),}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "so (37) holds. Consequently, (34) also holds by noticing that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\int_{X}x\\zeta(x)d\\mu_{n_{k_{j}}+1}(x)\\to\\int_{X}x\\zeta(x)d\\mu^{*}(x){\\mathrm{~as~}}j\\to\\infty.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "By the closedness of subdifferential graph of $\\partial(\\mathcal{E}_{G}+\\mathcal{H})$ (Thm. 8), we obtain ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\frac{T_{\\mu^{*}}^{\\nu^{*}}-I}{\\eta}\\in\\partial(\\mathcal{E}_{G}+\\mathcal{H})(\\mu^{*}).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "On the other hand, by the definition of $\\nu^{*}$ in (26), we get $T_{\\mu^{*}}^{\\nu^{*}}\\,=\\,I+\\eta S$ . Together with (38), $S\\in\\partial(\\mathcal{E}_{G}+\\mathcal{H})(\\mu^{*})$ . Noting that $S\\in\\partial\\mathcal{E}_{H}(\\mu^{*})$ , it holds $S\\in\\dot{\\partial}(\\mathcal{E}_{G}+\\mathcal{H})(\\mu^{*})\\cap\\partial\\mathcal{E}_{H}(\\mu^{*})$ and we conclude $\\mu^{*}$ is a critical point of $\\mathcal{F}$ . ", "page_idx": 23}, {"type": "text", "text": "A.5 Proof of Theorem 2 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We see that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Vert\\mathcal{G}_{\\eta}(\\mu_{n})\\Vert_{L^{2}(X,X;\\mu_{n})}^{2}=\\frac{1}{\\eta^{2}}\\displaystyle\\int_{X}\\left\\Vert x-T_{\\mu_{n}}^{\\mathrm{JKO}_{\\eta(\\varepsilon_{G}+\\mathcal{M})}((I+\\eta S)_{\\#}\\mu_{n})}(x)\\right\\Vert^{2}\\!d\\mu_{n}(x)}\\\\ &{\\qquad\\qquad\\quad\\quad=\\frac{1}{\\eta^{2}}W_{2}^{2}(\\mu_{n},\\mathrm{JKO}_{\\eta(\\varepsilon_{G}+\\mathcal{M})}((I+\\eta S)_{\\#}\\mu_{n}))}\\\\ &{\\qquad\\qquad\\quad\\quad=\\frac{1}{\\eta^{2}}W_{2}^{2}(\\mu_{n},\\mu_{n+1}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "On the other hand, it follows from (25) of the proof of Thm. 1 that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{i=1,N}W_{2}^{2}(\\mu_{n},\\mu_{n+1})=O(N^{-1}).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "A.6 Proof of Theorem 3 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "$H$ has uniformly bounded Hessian, by [43, Prop. 2.12], $\\mathcal{E}_{H}$ is Wasserstein differentiable and $\\nabla_{W}\\mathcal{E}_{H}(\\mu)=\\nabla H$ for all $\\mu\\in{\\mathcal{P}}_{2}(X)$ . According to Lem. 8 and (18), ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\partial_{F}^{-}\\mathcal{F}(\\mu_{n+1})=\\partial(\\mathcal{E}_{G}+\\mathcal{H})(\\mu_{n+1})-\\nabla H\\ni\\frac{T_{\\mu_{n+1}}^{\\nu_{n+1}}-I}{\\eta}-\\nabla H.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "We then have the following evaluations: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\operatorname{dist}\\left(0,\\partial_{F}^{-}\\mathcal{F}(\\mu_{n+1})\\right)=\\operatorname*{inf}_{\\xi\\in\\partial_{F}^{\\star}\\mathcal{F}(\\mu_{n+1})}\\left\\|\\xi\\right\\|_{L^{2}(X,X,\\mu_{n+1})}}}\\\\ &{\\leq\\left\\|\\frac{\\overline{{f}}_{\\mu_{n+1}}^{\\nu_{n+1}}-I}{\\eta}-\\nabla H\\right\\|_{L^{2}(X,X,\\mu_{n+1})}}\\\\ &{=\\left(\\int_{X}\\left\\|\\frac{T_{\\mu_{n+1}}^{\\nu_{n+1}}}{\\eta}(x)-x-\\nabla H(x)\\right\\|^{2}d\\mu_{n+1}(x)\\right)^{\\frac{1}{2}}}\\\\ &{=\\frac{1}{\\eta}\\left(\\int_{X}\\|T_{\\mu_{n+1}}^{\\nu_{n+1}}(x)-x-\\eta\\nabla H(x)\\|^{2}d\\mu_{n+1}(x)\\right)^{\\frac{1}{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "By transfer lemma 2, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\int_{X}\\Big\\|T_{\\mu_{n+1}}^{\\nu_{n+1}}(x)-x-\\eta\\nabla H(x)\\Big\\|^{2}d\\mu_{n+1}(x)}\\\\ &{=\\int_{X}\\Big\\|T_{\\mu_{n+1}}^{\\nu_{n+1}}(x)-x-\\eta\\nabla H(x)\\Big\\|^{2}d T_{\\nu_{n+1}}^{\\mu_{n+1}}\\#_{n+1}(x)}\\\\ &{=\\int_{X}\\Big\\|T_{\\mu_{n+1}}^{\\nu_{n+1}}\\circ T_{\\nu_{n+1}}^{\\mu_{n+1}}(x)-(I+\\eta\\nabla H)\\circ T_{\\nu_{n+1}}^{\\mu_{n+1}}(x)\\Big\\|^{2}d\\nu_{n+1}(x)}\\\\ &{=\\int_{X}\\Big\\|x-(I+\\eta\\nabla H)\\circ T_{\\nu_{n+1}}^{\\mu_{n+1}}(x)\\Big\\|^{2}d\\nu_{n+1}(x).}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "On the other hand, by using the trivial identity ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\nabla H=\\frac{(I+\\eta\\nabla H)-I}{\\eta}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "we compute, ", "text_level": 1, "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\int_{X}\\|\\nabla H(T_{\\nu_{n+1}}^{\\mu_{n}}(x))-\\nabla H(T_{\\nu_{n+1}}^{\\mu_{n+1}}(x))\\|^{2}d\\nu_{n+1}(x)}}\\\\ &{=\\int_{X}\\bigg\\|\\frac{(I+\\eta\\nabla H)\\circ T_{\\nu_{n+1}}^{\\mu_{n}}(x)-T_{\\nu_{n+1}}^{\\mu_{n}}(x)}{\\eta}-\\frac{(I+\\eta\\nabla H)\\circ T_{\\nu_{n+1}}^{\\mu_{n+1}}(x)-T_{\\nu_{n+1}}^{\\mu_{n+1}}(x)}{\\eta}\\bigg\\|^{2}d\\nu_{n+1}(x)}\\\\ &{=\\frac{1}{\\eta^{2}}\\int_{X}\\left\\|x-T_{\\nu_{n+1}}^{\\mu_{n}}(x)-(I+\\eta\\nabla H)\\circ T_{\\nu_{n+1}}^{\\mu_{n+1}}(x)+T_{\\nu_{n+1}}^{\\mu_{n+1}}(x)\\right\\|^{2}d\\nu_{n+1}(x),}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where the last equality uses $(I+\\eta\\nabla H)\\circ T_{\\nu_{n+1}}^{\\mu_{n}}=I\\,\\nu_{n+1}{\\circ}.\\mathrm{}$ ", "page_idx": 24}, {"type": "text", "text": "The Hessian of $H$ is bounded uniformly, $\\nabla H$ is Lipschitz, let\u2019s say $\\|\\nabla H(x)-\\nabla H(y)\\|\\leq L_{H}\\|x-y\\|$ for all $x,y\\in X$ . We continue evaluating (41) as follows ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\int_{X}\\|x-(I+\\eta\\nabla H)\\circ T_{\\nu_{n+1}}^{\\mu_{n+1}}(x)\\|^{2}d\\nu_{n+1}(x)}\\\\ &{\\le\\displaystyle\\int_{X}\\Big(\\|x-T_{\\nu_{n+1}}^{\\mu_{n}}(x)-(I+\\eta\\nabla H)\\circ T_{\\nu_{n+1}}^{\\mu_{n+1}}(x)+T_{\\nu_{n+1}}^{\\mu_{n+1}}(x)\\|+\\|T_{\\nu_{n+1}}^{\\mu_{n}}(x)-T_{\\nu_{n+1}}^{\\mu_{n+1}}(x)\\|\\Big)^{2}d\\nu_{n+1}}\\\\ &{\\le2\\displaystyle\\int_{X}\\Big\\|x-T_{\\nu_{n+1}}^{\\mu_{n}}(x)-(I+\\eta\\nabla H)\\circ T_{\\nu_{n+1}}^{\\mu_{n+1}}(x)+T_{\\nu_{n+1}}^{\\mu_{n+1}}(x)\\Big\\|^{2}d\\nu_{n+1}(x)}\\\\ &{\\quad+2\\displaystyle\\int_{X}\\|T_{\\nu_{n+1}}^{\\mu_{n}}(x)-T_{\\nu_{n+1}}^{\\mu_{n+1}}(x)\\|^{2}d\\nu_{n+1}(x)}\\\\ &{=2\\eta^{2}\\displaystyle\\int_{X}\\|\\nabla H(T_{\\nu_{n+1}}^{\\mu_{n}}(x))-\\nabla H(T_{\\nu_{n+1}}^{\\mu_{n+1}}(x))\\|^{2}d\\nu_{n+1}(x)+2\\displaystyle\\int_{X}\\|T_{\\nu_{n+1}}^{\\mu_{n}}(x)-T_{\\nu_{n+1}}^{\\mu_{n+1}}(x)\\|^{2}d\\nu_{n+1}(x)}\\\\ &{\\le2(\\eta^{2}L_{H}^{2}+1)\\displaystyle\\int_{X}\\|T_{\\nu_{n+1}}^{\\mu_{n}}(x)-T_{\\nu_{n+1}}^{\\mu_{n+1}}(x)\\|^{2}d\\nu_{n+1}(x)}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where the third equality uses (42). ", "page_idx": 24}, {"type": "text", "text": "From (40), (41), and (43), we derive ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathrm{dist}\\,(0,\\partial_{F}^{-}\\mathcal{F}(\\mu_{n+1}))\\leq\\frac{\\sqrt{2(\\eta^{2}L_{H}^{2}+1)}}{\\eta}\\left(\\int_{X}\\|T_{\\nu_{n+1}}^{\\mu_{n}}(x)-T_{\\nu_{n+1}}^{\\mu_{n+1}}(x)\\|^{2}d\\nu_{n+1}(x)\\right)^{\\frac{1}{2}}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "On the other hand, by telescoping Lem. 1, we obtain ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\sum_{n=1}^{\\infty}\\int_{X}\\|T_{\\nu_{n+1}}^{\\mu_{n}}(x)-T_{\\nu_{n+1}}^{\\mu_{n+1}}(x)\\|^{2}d\\nu_{n+1}(x)<+\\infty.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Therefore, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{n=0}^{N-1}\\mathrm{dist}\\left(0,\\partial^{F}\\mathcal{F}(\\mu_{n+1})\\right)}\\\\ &{\\leq\\frac{\\sqrt{2(\\eta^{2}L_{H}^{2}+1)}}{\\eta}\\sum_{n=0}^{N-1}\\left(\\int_{X}\\|T_{\\nu_{n+1}}^{\\mu_{n}}(x)-T_{\\nu_{n+1}}^{\\mu_{n+1}}(x)\\|^{2}d\\nu_{n+1}(x)\\right)^{\\frac{1}{2}}}\\\\ &{\\leq\\frac{\\sqrt{2(\\eta^{2}L_{H}^{2}+1)}}{\\eta}\\left(N\\displaystyle\\sum_{n=0}^{N-1}\\int_{X}\\|T_{\\nu_{n+1}}^{\\mu_{n}}(x)-T_{\\nu_{n+1}}^{\\mu_{n+1}}(x)\\|^{2}d\\nu_{n+1}(x)\\right)\\right)^{\\frac{1}{2}}}\\\\ &{\\leq\\frac{\\sqrt{2(\\eta^{2}L_{H}^{2}+1)N}}{\\eta}\\left(\\sum_{n=0}^{+\\infty}\\int_{X}\\|T_{\\nu_{n+1}}^{\\mu_{n}}(x)-T_{\\nu_{n+1}}^{\\mu_{n+1}}(x)\\|^{2}d\\nu_{n+1}(x)\\right)^{\\frac{1}{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "We derive ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{n=1,N}\\mathrm{dist}\\,(0,\\partial^{F}{\\mathcal F}(\\mu_{n}))=O\\left({\\frac{1}{\\sqrt{N}}}\\right).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "A.7 Proof of Theorem 4 ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Convergence in terms of objective values ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Since $H\\in C^{2}(X)$ whose Hessian is uniformly bounded, recall from (39) that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\partial_{F}^{-}\\mathcal{F}(\\mu_{n+1})=\\partial(\\mathcal{E}_{G}+\\mathcal{H})(\\mu_{n+1})-\\nabla H\\ni\\frac{T_{\\mu_{n+1}}^{\\nu_{n+1}}-I}{\\eta}-\\nabla H.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Since $\\mathcal{F}(\\mu_{0})-\\mathcal{F}^{*}<r_{0}$ and the sequence $\\{{\\mathcal{F}}(\\mu_{n})\\}_{n\\in\\mathbb{N}}$ is not increasing (Lem. 1), $\\mathcal{F}(\\mu_{n})\\!-\\!\\mathcal{F}^{*}<r_{0}$ for all $n\\in\\mathbb{N}$ . \u0141ojasiewicz condition implies ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{c(\\mathcal{F}(\\mu_{n+1})-\\mathcal{F}^{*})^{\\theta}\\leq\\left\\|\\frac{T_{\\mu_{n}+1}^{\\nu_{n+1}}-I}{\\eta}-\\nabla H\\right\\|_{L^{2}(X,X,\\mu_{n+1})}}\\\\ &{\\qquad\\qquad=\\left(\\int_{X}\\left\\|\\frac{T_{\\mu_{n+1}}^{\\nu_{n+1}}(x)-x}{\\eta}-\\nabla H(x)\\right\\|^{2}\\!d\\mu_{n+1}(x)\\right)^{\\frac{1}{2}}}\\\\ &{\\qquad\\qquad\\leq\\frac{\\sqrt{2(\\eta^{2}L_{H}^{2}+1)}}{\\eta}\\left(\\int_{X}\\|T_{\\nu_{n+1}}^{\\mu_{n}}(x)-T_{\\nu_{n+1}}^{\\mu_{n+1}}(x)\\|^{2}d\\nu_{n+1}(x)\\right)^{\\frac{1}{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where the last inequality follows from (41) and (43) and $L_{H}$ is the Lipschitz constant of $\\nabla H$ . Combining with Lem. 1, we derive ", "page_idx": 25}, {"type": "equation", "text": "$$\nc\\left(\\mathcal{F}(\\mu_{n+1})-\\mathcal{F}^{*}\\right)^{\\theta}\\leq\\frac{\\sqrt{2\\big(\\eta^{2}L_{H}^{2}+1\\big)}}{\\sqrt{\\eta}}\\left(\\mathcal{F}(\\mu_{n})-\\mathcal{F}(\\mu_{n+1})\\right)^{\\frac{1}{2}}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "or ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\left(\\mathcal{F}(\\mu_{n+1})-\\mathcal{F}^{*}\\right)^{2\\theta}\\leq\\frac{2(\\eta^{2}L_{H}^{2}+1)}{c^{2}\\eta}\\left(\\left(\\mathcal{F}(\\mu_{n})-\\mathcal{F}^{*}\\right)-\\left(\\mathcal{F}(\\mu_{n+1})-\\mathcal{F}^{*}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "We then use the following lemma [71, Lem. 4]. ", "page_idx": 25}, {"type": "text", "text": "Lemma 11. Let $\\{s_{k}\\}_{k\\in\\mathbb{N}}$ be a nonincreasing and nonnegative real sequence. Assume that there exist $\\alpha\\geq0$ and $\\beta>0$ such that for all sufficiently large $k$ , ", "page_idx": 26}, {"type": "equation", "text": "$$\ns_{k+1}^{\\alpha}\\leq\\beta(s_{k}-s_{k+1}).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Then ", "page_idx": 26}, {"type": "text", "text": "(i) if $\\alpha=0$ , the sequence $\\{s_{k}\\}_{k\\in\\mathbb{N}}$ converges to 0 in a finite number of steps; ", "page_idx": 26}, {"type": "text", "text": "(ii) i $\\mathfrak{C}_{\\alpha}\\in(0,1]$ , the sequence $\\{s_{k}\\}_{k\\in\\mathbb{N}}$ converges linearly to 0 with rate $\\displaystyle\\frac{\\beta}{\\beta\\!+\\!1}$ ", "page_idx": 26}, {"type": "text", "text": "(iii) if $\\alpha>1$ , the sequence $\\{s_{k}\\}_{k\\in\\mathbb{N}}$ converges sublinearly to 0, i.e., there exists $\\tau>0$ : ", "page_idx": 26}, {"type": "equation", "text": "$$\ns_{k}\\leq\\tau k^{\\frac{-1}{\\alpha-1}}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "for sufficiently large $k$ . ", "page_idx": 26}, {"type": "text", "text": "Compared to [71, Lem. 4], we have dropped the assumption $s_{k}\\,\\to\\,0$ in Lem. 11 because this assumption is vacuous, i.e., it can be induced by (47) and nonnegativity of $\\{s_{k}\\}_{k\\in\\mathbb{N}}$ . ", "page_idx": 26}, {"type": "text", "text": "We now apply Lem. 11 for $s_{k}=\\mathcal{F}(\\mu_{k})-\\mathcal{F}^{*}$ using (46) to derive the followings ", "page_idx": 26}, {"type": "text", "text": "(i) if $\\theta=0$ , $\\mathcal{F}(\\mu_{n})-\\mathcal{F}^{*}$ converges to 0 in a finite number of steps; (ii) if $\\theta\\in(0,1/2],\\mathcal{F}(\\mu_{n})-\\mathcal{F}^{*}$ converges to 0 linearly (exponentially fast) with rate ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathcal{F}(\\mu_{n})-\\mathcal{F}^{*}=O\\left(\\left(\\frac{M}{M+1}\\right)^{n}\\right)\\mathrm{~where~}M=\\frac{2(\\eta^{2}L_{H}^{2}+1)}{c^{2}\\eta};\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "(iii) if $\\theta\\in(1/2,1),\\mathcal{F}(\\mu_{n})-\\mathcal{F}^{*}$ converges sublinearly to 0, i.e., ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathcal F(\\mu_{n})-\\mathcal F^{*}=O\\left(n^{-\\frac{1}{2\\theta-1}}\\right).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "A.8 Proof of Theorem 5 ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Cauchy sequence. ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "By replacing $n:=n-1$ in (45) and rearranging ", "page_idx": 26}, {"type": "equation", "text": "$$\n1\\leq\\frac{\\sqrt{2(\\eta^{2}L_{H}^{2}+1)}}{c\\eta}\\left(\\int_{X}\\|T_{\\nu_{n}}^{\\mu_{n-1}}(x)-T_{\\nu_{n}}^{\\mu_{n}}(x)\\|^{2}d\\nu_{n}(x)\\right)^{\\frac{1}{2}}\\left(\\mathcal{F}(\\mu_{n})-\\mathcal{F}^{*}\\right)^{-\\theta}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "It follows from Lem. 1 and (48) that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\int_{X}\\|T_{\\nu_{n+1}}^{\\mu_{n}}(x)-T_{\\nu_{n+1}}^{\\mu_{n+1}}(x)\\|^{2}d\\nu_{n+1}(x)\\le\\eta(\\mathcal{F}(\\mu_{n})-\\mathcal{F}(\\mu_{n+1}))}\\\\ &{\\le\\displaystyle\\frac{\\sqrt{2(\\eta^{2}L_{H}^{2}+1)}}{c}\\left(\\int_{X}\\|T_{\\nu_{n}}^{\\mu_{n-1}}(x)-T_{\\nu_{n}}^{\\mu_{n}}(x)\\|^{2}d\\nu_{n}(x)\\right)^{\\frac12}(\\mathcal{F}(\\mu_{n})-\\mathcal{F}^{*})^{-\\theta}\\left(\\mathcal{F}(\\mu_{n})-\\mathcal{F}(\\mu_{n+1})\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Since the function $s:\\mathbb{R}^{+}\\to\\mathbb{R}$ , $s(t)=t^{1-\\theta}$ is concave if $\\theta\\in[0,1)$ , tangent inequality holds ", "page_idx": 26}, {"type": "equation", "text": "$$\ns^{\\prime}(a)(a-b)\\leq s(a)-s(b).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Note that $s^{\\prime}(t)=(1-\\theta)t^{-\\theta}$ , the above inequality further implies ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left(1-\\theta\\right)\\left(\\mathcal{F}(\\mu_{n})-\\mathcal{F}^{*}\\right)^{-\\theta}\\left(\\mathcal{F}(\\mu_{n})-\\mathcal{F}(\\mu_{n+1})\\right)\\leq\\left(\\mathcal{F}(\\mu_{n})-\\mathcal{F}^{*}\\right)^{1-\\theta}-\\left(\\mathcal{F}(\\mu_{n+1})-\\mathcal{F}^{*}\\right)^{1-\\theta}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "From (49) and (50) ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\int_{X}\\|T_{\\nu_{n+1}}^{\\mu_{n}}(x)-T_{\\nu_{n+1}}^{\\mu_{n+1}}(x)\\|^{2}d\\nu_{n+1}(x)\\leq\\frac{\\sqrt{2(\\eta^{2}L_{H}^{2}+1)}}{(1-\\theta)c}\\left(\\int_{X}\\|T_{\\nu_{n}}^{\\mu_{n-1}}(x)-T_{\\nu_{n}}^{\\mu_{n}}(x)\\|^{2}d\\nu_{n}(x)\\right)^{\\frac{1}{2}}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "or equivalently, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{r_{n}}{\\sqrt{r_{n-1}}}:=\\frac{\\int_{X}\\|T_{\\nu_{n+1}}^{\\mu_{n}}(x)-T_{\\nu_{n+1}}^{\\mu_{n+1}}(x)\\|^{2}d\\nu_{n+1}(x)}{\\left(\\int_{X}\\|T_{\\nu_{n}}^{\\mu_{n-1}}(x)-T_{\\nu_{n}}^{\\mu_{n}}(x)\\|^{2}d\\nu_{n}(x)\\right)^{\\frac{1}{2}}}}\\\\ {\\displaystyle\\qquad\\leq\\frac{\\sqrt{2(\\eta^{2}L_{H}^{2}+1)}}{(1-\\theta)c}\\left[(\\mathcal{F}(\\mu_{n})-\\mathcal{F}^{*})^{1-\\theta}-(\\mathcal{F}(\\mu_{n+1})-\\mathcal{F}^{*})^{1-\\theta}\\right]}\\\\ {\\int_{X}\\|T_{\\nu_{n+1}}^{\\mu_{n}}(x)-T_{\\nu_{n+1}}^{\\mu_{n+1}}(x)\\|^{2}d\\nu_{n+1}(x).}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where ", "page_idx": 27}, {"type": "text", "text": "By telescoping (51) from $n=1$ to $+\\infty$ we obtain ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\sum_{n=1}^{+\\infty}\\frac{r_{n}}{\\sqrt{r_{n-1}}}<+\\infty.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "On the other hand ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\frac{r_{n}}{\\sqrt{r_{n-1}}}+\\sqrt{r_{n-1}}\\geq2\\sqrt{r_{n}},\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "we derive $\\begin{array}{r}{\\sum_{n=0}^{+\\infty}\\sqrt{r_{n}}<+\\infty}\\end{array}$ . From (22), (23) of the proof of Thm. 1, $r_{n}\\geq W_{2}^{2}(\\mu_{n},\\mu_{n+1})$ , we obtain ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\sum_{n=0}^{+\\infty}W_{2}(\\mu_{n},\\mu_{n+1})<+\\infty.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "or, in other words, $\\{\\mu_{n}\\}_{n\\in\\mathbb{N}}$ is a Cauchy sequence under Wasserstein topology. The Wasserstein space $(\\mathcal{P}_{2}(X),W_{2})$ is complete [5, Thm. 2.2], every Cauchy sequence is convergent, i.e., there exists $\\mu^{*}\\in\\mathcal{P}_{2}(X)$ such that $\\mu_{n}\\xrightarrow{\\mathrm{Wass}}\\mu^{*}$ ", "page_idx": 27}, {"type": "text", "text": "We prove that $\\mu^{*}$ is actually an optimal solution of $\\mathcal{F}$ by showing $\\mathcal{F}(\\mu^{*})=\\mathcal{F}^{*}$ . Indeed, firstly, as $G$ and $H$ have quadratic growth, it holds ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\xi_{G}(\\mu_{n})\\to\\mathcal{E}_{G}(\\mu^{*}),\\mathcal{E}_{H}(\\mu_{n})\\to\\mathcal{E}_{H}(\\mu^{*}).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "On the other hand, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathcal F^{*}=\\underset{n\\to\\infty}{\\operatorname*{lim}}\\mathcal F(\\mu_{n})=\\underset{n\\to\\infty}{\\operatorname*{lim}}\\mathcal F(\\mu_{n})=\\underset{n\\to\\infty}{\\operatorname*{lim}}\\mathcal H(\\mu_{n})+\\mathcal E_{G}(\\mu^{*})-\\mathcal E_{H}(\\mu^{*})}&{}\\\\ {\\geq\\mathcal H(\\mu^{*})+\\mathcal E_{G}(\\mu^{*})-\\mathcal E_{H}(\\mu^{*})=\\mathcal F(\\mu^{*})}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "since $\\mathcal{H}$ is l.s.c. The equality has to occur, i.e., $\\mathcal{F}^{*}=\\mathcal{F}(\\mu^{*})$ , due to the optimality of ${\\mathcal{F}}^{*}$ . ", "page_idx": 27}, {"type": "text", "text": "Convergence rate of $\\{\\mu_{n}\\}_{n\\in\\mathbb{N}}$ . ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "From item (i) of Thm. 4, there exists $n_{0}\\in\\mathbb{N}$ such that $\\mathcal{F}(\\mu_{n})=\\mathcal{F}^{*}$ for all $n\\geq n_{0}$ . It then follows from (24) that $\\mu_{n_{0}}=\\mu_{n_{0}+1}=\\mu_{n_{0}+2}=\\ldots.$ , which further implies that $\\mu_{n}=\\mu^{*}$ for all $n\\geq n_{0}$ . ", "page_idx": 27}, {"type": "text", "text": "(ii) If $\\mathbf{\\nabla}^{\\prime}\\theta\\in(0,1/2]$ Let $\\textstyle s_{i}=\\sum_{n=i}^{\\infty}{\\sqrt{r_{n}}}$ . We have ", "page_idx": 27}, {"type": "equation", "text": "$$\ns_{i}\\geq\\sum_{n=i}^{\\infty}W_{2}(\\mu_{n},\\mu_{n+1})\\geq W_{2}(\\mu_{i},\\mu^{*})\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where the last inequality uses triangle inequality ", "page_idx": 27}, {"type": "equation", "text": "$$\nW_{2}(\\mu_{i},\\mu^{*})\\leq\\sum_{n=i}^{N-1}W_{2}(\\mu_{n},\\mu_{n+1})+W_{2}(\\mu_{N},\\mu^{*})\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "and lets $N\\rightarrow\\infty$ with a notice that $\\mu_{N}\\xrightarrow{\\mathrm{Wass}}\\mu^{*}$ ", "page_idx": 27}, {"type": "text", "text": "From (51) and (52), ", "page_idx": 27}, {"type": "equation", "text": "$$\n2\\sqrt{r_{n}}\\le\\sqrt{r_{n-1}}+\\frac{\\sqrt{2(\\eta^{2}L_{H}^{2}+1)}}{(1-\\theta)c}\\left[(\\mathcal{F}(\\mu_{n})-\\mathcal{F}^{*})^{1-\\theta}-(\\mathcal{F}(\\mu_{n+1})-\\mathcal{F}^{*})^{1-\\theta}\\right].\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Telescope (54) for $n=i$ to $+\\infty$ , ", "page_idx": 28}, {"type": "equation", "text": "$$\ns_{i}\\leq\\sqrt{r_{i-1}}+\\frac{\\sqrt{2(\\eta^{2}L_{H}^{2}+1)}}{(1-\\theta)c}(\\mathcal{F}(\\mu_{i})-\\mathcal{F}^{*})^{1-\\theta}\\leq\\sqrt{r_{i-1}}+\\frac{(2(\\eta^{2}L_{H}^{2}+1))^{\\frac{1}{2\\theta}}}{(1-\\theta)\\eta^{\\frac{1-\\theta}{\\theta}}c^{\\frac{1}{\\theta}}}r_{i-1}^{\\frac{1-\\theta}{2\\theta}}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where the last inequality uses (45). Since $r_{i}\\to0$ as $i\\rightarrow\\infty$ , $r_{i}<1$ for $i$ sufficiently large. It follows from (55) that: for $i$ sufficiently large ", "page_idx": 28}, {"type": "equation", "text": "$$\ns_{i}\\leq M{\\sqrt{r_{i-1}}}=M(s_{i-1}-s_{i})\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where ", "page_idx": 28}, {"type": "equation", "text": "$$\nM=1+\\frac{(2(\\eta^{2}L_{H}^{2}+1))^{\\frac{1}{2\\theta}}}{(1-\\theta)\\eta^{\\frac{1-\\theta}{\\theta}}c^{\\frac{1}{\\theta}}}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Rewriting as $\\begin{array}{r}{s_{i}\\leq\\frac{M}{M+1}s_{i-1}}\\end{array}$ , we derive $\\begin{array}{r}{W_{2}(\\mu_{i},\\mu^{*})=O\\left(\\left(\\frac{M}{M+1}\\right)^{i}\\right).}\\end{array}$ ", "page_idx": 28}, {"type": "text", "text": "(iii) If $\\theta\\in(1/2,1)$ (55) implies: for all $i$ sufficiently large, ", "page_idx": 28}, {"type": "equation", "text": "$$\ns_{i}\\leq M r_{i-1}^{\\frac{1-\\theta}{2\\theta}}=M\\big(s_{i-1}-s_{i}\\big)^{\\frac{1-\\theta}{\\theta}}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $M$ is the same as in (56). ", "page_idx": 28}, {"type": "text", "text": "B Implementation of FB Euler ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Following a similar approach to the semi FB Euler\u2019s implementation outlined in Alg. 1, we present a practical implementation of FB Euler for the sampling context in Alg. 2. ", "page_idx": 28}, {"type": "text", "text": "Algorithm 2 FB Euler for sampling ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Input: Initial measure $\\mu_{0}\\in\\mathcal{P}_{2,\\mathrm{abs}}(X)$ , discretization stepsize $\\eta>0$ , number of steps $K>0$ ,   \nbatch size $B$ .   \nfor $k=1$ to $K$ do for $i=1,2,\\dots{\\mathfrak{~}}$ do Draw a batch of samples $Z\\sim\\mu_{0}$ of size $B$ ; $\\begin{array}{r l}&{\\Xi\\leftarrow(I-\\eta\\nabla F)\\circ\\dot{\\nabla}_{x}\\psi_{\\theta_{k}}\\circ(\\dot{I}-\\eta\\nabla F)\\circ\\nabla_{x}\\psi_{\\theta_{k-1}}\\circ...\\circ(I-\\eta\\nabla F)(Z);}\\\\ &{\\widehat{W_{2}^{2}}\\leftarrow\\frac{1}{B}\\sum_{\\xi\\in\\Xi}\\|\\nabla_{x}\\psi_{\\theta}(\\xi)-\\xi\\|^{2};}\\\\ &{\\widehat{\\Delta\\mathcal{H}}\\leftarrow-\\frac{1}{B}\\sum_{\\xi\\in\\Xi}\\log\\operatorname*{det}\\nabla_{x}^{2}\\psi_{\\theta}(\\xi).}\\\\ &{\\widehat{\\mathcal{L}}\\leftarrow\\frac{1}{2\\eta}\\widehat{W_{2}^{2}}+\\widehat{\\Delta\\mathcal{H}}.}\\end{array}$ Apply an optimization step (e.g., Adam) over $\\theta$ using $\\nabla_{\\boldsymbol{\\theta}}\\widehat{\\mathcal{L}}$ . end for $\\theta_{k+1}\\gets\\theta$ .   \nend for ", "page_idx": 28}, {"type": "text", "text": "C Numerical illustrations ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "We perform numerical experiments in a high-performance computing cluster with GPU support. We use Python version 3.8.0. We allocate 8G memory for the experiments. The total running time for all experiments is a couple of hours. Our implementation is based on the code of [53] (MIT license) with the DenseICNN architecture [41]. ", "page_idx": 28}, {"type": "text", "text": "C.1 Gaussian mixture ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Consider a target Gaussian mixture of the following form: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\pi(x)\\propto\\exp(-F(x)):=\\sum_{i=1}^{K}\\pi_{i}\\exp{\\left(-\\frac{\\|x-x_{i}\\|^{2}}{\\sigma^{2}}\\right)}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "We write ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle F(x)=-\\log\\left(\\sum_{i=1}^{K}\\pi_{i}\\exp\\left(-\\frac{\\|x-x_{i}\\|^{2}}{\\sigma^{2}}\\right)\\right)}\\\\ {\\displaystyle}&{\\,\\,\\,\\,\\,\\,=-\\log\\left(\\sum_{i=1}^{K}\\pi_{i}\\exp\\left(-\\frac{\\|x\\|^{2}+\\|x_{i}\\|^{2}-2\\left\\langle x,x_{i}\\right\\rangle}{\\sigma^{2}}\\right)\\right)}\\\\ {\\displaystyle}&{\\,\\,\\,\\,\\,\\,=-\\log\\left(\\sum_{i=1}^{K}\\pi_{i}\\exp\\left(-\\frac{\\|x\\|^{2}}{\\sigma^{2}}\\right)\\times\\exp\\left(-\\frac{\\|x_{i}\\|^{2}-2\\left\\langle x,x_{i}\\right\\rangle}{\\sigma^{2}}\\right)\\right)}\\\\ {\\displaystyle}&{\\,\\,\\,\\,\\,\\,=\\frac{\\|x\\|^{2}}{\\sigma^{2}}-\\log\\left(\\sum_{i=1}^{K}\\pi_{i}\\exp\\left(-\\frac{\\|x_{i}\\|^{2}-2\\left\\langle x,x_{i}\\right\\rangle}{\\sigma^{2}}\\right)\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "which is DC. Note that the convexity of the second component is thanks to (a) log-sum-exp is convex and (b) the composite of a convex function and an affine function is convex. ", "page_idx": 29}, {"type": "text", "text": "Experiment details We set $K=5$ and randomly generate $x_{1},x_{2},\\dotsc,x_{5}\\in\\mathbb{R}^{2}$ . We set $\\sigma=1$ . The initial distribution is $\\mu_{0}=\\ensuremath{\\mathcal{N}}(0,16I)$ . We use $\\eta=0.1$ for both FB Euler and semi FB Euler. We train both algorithms for 40 iterations using Adam optimizer with a batch size of 512 in which the first 20 iterations use a learning rate of $5\\times\\breve{1}0^{-3}$ while the latter 20 iterations use $2\\times10^{-3}$ . For the baseline ULA, we run 10000 chains in parallel for 4000 iterations with a learning rate of $10^{-3}$ . ", "page_idx": 29}, {"type": "text", "text": "We run the above experiment 5 times where $x_{1},x_{2},\\ldots,x_{5}$ are randomly generated each time. Fig. 1 (b) reports the KL divergence along the training process where the mean curves are in bold and individual curves are displayed in a faded style. The final KL divergence (averaged across 5 runs) of the ULA is reported as a horizontal line. ", "page_idx": 29}, {"type": "text", "text": "C.2 Distance-to-set prior ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Let $\\pi$ be the original prior, $\\Theta$ be the constraint set that we want to impose, and the distance-to-set prior [61] is defined by, for some $\\rho>0$ , ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\tilde{\\pi}(\\theta)\\propto\\pi(\\theta)\\exp\\left(-\\frac{\\rho}{2}d(\\theta,\\Theta)^{2}\\right),\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "that penalize exponentially $\\theta$ deviating from the constraint set. ", "page_idx": 29}, {"type": "text", "text": "Given data $y$ , using the this distance-to-set prior, the posterior reads ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\bar{\\pi}(\\theta|y)\\propto L(\\theta|y)\\pi(\\theta)\\exp\\left(-\\frac{\\rho}{2}d(\\theta,\\Theta)^{2}\\right).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where $L(\\theta|y)$ is the likelihood. ", "page_idx": 29}, {"type": "text", "text": "The structure of $\\bar{\\pi}(\\theta|y)$ depends on three separate components: the original prior, the likelihood, and the constraint set $\\Theta$ . In the ideal case, $\\pi(\\theta)$ and $L(\\theta|y)$ are given in nice forms (e.g., log-concave), and $\\Theta$ is a convex set. As a fact, if $\\Theta$ is a convex set, $\\theta\\mapsto\\^{\\bullet}d(\\theta,\\Theta)^{2}$ is convex, making the whole posterior log-concave. If $\\Theta$ is additionally closed, $d(\\theta,\\Theta)^{2}$ is L-smooth. ", "page_idx": 29}, {"type": "text", "text": "However, whenever $\\Theta$ is nonconvex, the function $\\theta\\mapsto d(\\theta,\\Theta)^{2}$ is not continuously differentiable. This is induced by the Motzkin-Bunt theorem [16, Thm. 9.2.5] asserting that any Chebyshev set (a set $S\\subset X$ is called Chebyshev if every point in $X$ has a unique nearest point in $S$ ) has to be closed and convex. ", "page_idx": 29}, {"type": "text", "text": "On the other hand, $d(\\theta,\\Theta)^{2}$ is always DC regardless the geometric structure of $\\Theta$ : ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{d(\\theta,\\Theta)^{2}=\\displaystyle\\operatorname*{inf}_{x\\in\\Theta}\\|\\theta-x\\|^{2}\\,}&{}\\\\ {=\\displaystyle\\operatorname*{inf}_{x\\in\\Theta}\\,\\big(\\|\\theta\\|^{2}+\\|x\\|^{2}-2\\langle x,\\theta\\rangle\\big)}&{}\\\\ {=\\|\\theta\\|^{2}+\\displaystyle\\operatorname*{inf}_{x\\in\\Theta}\\,\\big(\\|x\\|^{2}-2\\langle x,\\theta\\rangle\\big)}&{}\\\\ {=\\|\\theta\\|^{2}-\\displaystyle\\operatorname*{sup}_{x\\in\\Theta}\\big(-\\|x\\|^{2}+2\\langle x,\\theta\\rangle\\big)\\,.}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Note that the supremum of an arbitrary family of affine functions is convex. ", "page_idx": 30}, {"type": "text", "text": "Therefore, the log-DC structure of the whole posterior only depends on whether the original prior and the likelihood are log-DC, which is likely to be the case. ", "page_idx": 30}, {"type": "text", "text": "Distance-to-set prior relaxed von Mises-Fisher In directional statistics, the von Mises-Fisher distribution is a distribution over unit-length vectors (unit sphere). It can be described as a restriction of a Gaussian distribution in a sphere. By using the distance-to-set prior, we can relax the spherical constraint as ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\bar{\\pi}(\\theta)\\propto\\exp(-F(\\theta)):=\\exp\\left(-\\kappa\\frac{(\\theta-\\mu)^{\\top}(\\theta-\\mu)}{2}\\right)\\times\\exp\\left(-\\frac{\\rho}{2}\\operatorname{dist}(\\theta,S)^{2}\\right)\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where $S$ denote the unit sphere in some $\\mathbb{R}^{d}$ space. By the DC structure (57) of the distance function, $F$ is DC with the following composition ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{F(\\theta)=\\bigg(\\kappa\\displaystyle\\frac{\\|\\theta-\\mu\\|^{2}}{2}+\\frac{\\rho}{2}\\|\\theta\\|^{2}\\bigg)-\\frac{\\rho}{2}\\operatorname*{sup}_{x\\in S}\\left(-\\|x\\|^{2}+2\\langle x,\\theta\\rangle\\right)}\\\\ &{\\qquad:=G(\\theta)-H(\\theta).}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "We also note that $H$ is not continuously differentiable because $S$ is nonconvex. Furthermore, $\\rho\\,\\mathrm{proj}_{S}(\\theta)\\in\\partial H(\\theta)$ where pro $\\vert_{S}(\\theta)$ is the projection of $\\theta$ onto $S$ , which can be computed explicitly in this case. ", "page_idx": 30}, {"type": "text", "text": "Experiment details We consider a unit circle in $\\mathbb{R}^{2}$ with centre $\\mu=(1,1.5)$ . We set $\\kappa=1$ and $\\rho=100$ . The initial distribution is $\\mu_{0}=\\ensuremath{\\mathcal{N}}(0,16I)$ . We use $\\eta=0.1$ for both FB Euler and semi FB Euler. We train both algorithms for 40 iterations using Adam optimizer with a batch size of 512 in which the first 20 iterations use a learning rate of $5\\times10^{-3}$ while the latter 20 iterations use $2\\times10^{-3}$ . ", "page_idx": 30}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Justification: We briefly mention our contribution in the abstract (propose a new scheme named semi Forward-Backward Euler and derive multiple convergence insights) and provide a detailed contribution paragraph in Sect. 1 where each claim is pointed to the corresponding theorem. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 31}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Justification: We mentioned the expensiveness of the method, especially since the JKO operator is not scalable at least for now. See Sect. 5. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 31}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Justification: We provide a full set of assumptions used in each theorem/claim, see Sect. 3.2 for the problem setting and Sect. 4 for the convergence theorems. Proofs for all theorems are provided in the Appendix A with strong mathematical rigour. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 32}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: We provide a detailed implementation of the proposed method in Sect. 5 and Appendix B, and the experiment setting/parameters in Appendix C. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in ", "page_idx": 32}, {"type": "text", "text": "some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 33}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: We provide our code/results in a GitHub repo mentioned in Sect. 6. We also instruct how to reproduce our experiments in README.md. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 33}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: We specify all the training details in Appendix C: initial distribution, target distribution, discretization stepsize, learning rate, minibatch size, optimizer, etc. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 33}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: We report in the Numerical illustration section 6 the mean curves in bold and individual run curves shown faintly in the background playing the same role as error bars. We provide sufficient information in Appendix C. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 33}, {"type": "text", "text": "\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 34}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: We provide resource information in Appendix C. Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 34}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: This is standard machine learning research without any sensitive or humanrelated data. We do not see any negative impact on the society. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 34}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: This is theoretical research on an optimization problem defined in the Wasserstein space. We do not see any societal impact that our work may bring about. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 35}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: the paper poses no such risks. It is a theoretical paper. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 35}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: We explicitly mention in Sect. 5 that we use the deep learning approach proposed in [53] to compute the JKO. We also acknowledge that we leverage the code from [53] in Section C and in README.md in the GitHub repo; the license information is explicitly mentioned in Section C. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 35}, {"type": "text", "text": "", "page_idx": 36}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: the detailed description of the algorithms in Sect. 5. We also include LICENSE.txt in the GitHub repo. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 36}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: the paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 36}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 36}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: the paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 37}]