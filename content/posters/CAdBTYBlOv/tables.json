[{"figure_path": "CAdBTYBlOv/tables/tables_19_1.jpg", "caption": "Table 1: Test log-likelihoods, total training times, and average speed-up among datasets for CG, AP, and SGD after 100 outer-loop marginal likelihood steps with learning rate of 0.1. We consider five datasets with n < 50k, which allows us to solve to tolerance, and report the mean over 10 data splits.", "description": "This table presents the results of applying three different linear system solvers (Conjugate Gradients, Alternating Projections, and Stochastic Gradient Descent) to the problem of hyperparameter optimization in iterative Gaussian Processes.  The table shows the test log-likelihood, total training time, and speedup achieved by each solver across five different datasets, with and without two proposed optimizations (pathwise estimator and warm start).  The experiment involves 100 outer-loop marginal likelihood optimization steps with a learning rate of 0.1, and only datasets with fewer than 50,000 data points are included to ensure that the solvers can reach the tolerance. The mean performance across 10 data splits is reported.", "section": "4 Warm Starting Linear System Solvers"}, {"figure_path": "CAdBTYBlOv/tables/tables_20_1.jpg", "caption": "Table 1: Test log-likelihoods, total training times, and average speed-up among datasets for CG, AP, and SGD after 100 outer-loop marginal likelihood steps with learning rate of 0.1. We consider five datasets with n < 50k, which allows us to solve to tolerance, and report the mean over 10 data splits.", "description": "This table presents the results of three different linear system solvers (Conjugate Gradients, Alternating Projections, and Stochastic Gradient Descent) on five datasets after 100 iterations of marginal likelihood optimization.  The table shows the test log-likelihood, total training time, solver time, and speed-up achieved by each solver, comparing standard and enhanced methods (pathwise and warm start). The small dataset size (n<50k) allows solving to convergence and the mean over 10 data splits is reported.", "section": "Warm Starting Linear System Solvers"}, {"figure_path": "CAdBTYBlOv/tables/tables_20_2.jpg", "caption": "Table 1: Test log-likelihoods, total training times, and average speed-up among datasets for CG, AP, and SGD after 100 outer-loop marginal likelihood steps with learning rate of 0.1. We consider five datasets with n < 50k, which allows us to solve to tolerance, and report the mean over 10 data splits.", "description": "This table presents the results of three different linear system solvers (Conjugate Gradients, Alternating Projections, and Stochastic Gradient Descent) on five datasets after 100 iterations of marginal likelihood optimization.  The table shows test log-likelihood, total training time, solver time, and speed-up compared to a baseline method.  The datasets used all have fewer than 50,000 data points, allowing the solvers to reach a specified tolerance. Results are averaged over 10 different random data splits.", "section": "Warm Starting Linear System Solvers"}, {"figure_path": "CAdBTYBlOv/tables/tables_21_1.jpg", "caption": "Table 1: Test log-likelihoods, total training times, and average speed-up among datasets for CG, AP, and SGD after 100 outer-loop marginal likelihood steps with learning rate of 0.1. We consider five datasets with n < 50k, which allows us to solve to tolerance, and report the mean over 10 data splits.", "description": "This table presents the results of the experiments conducted on five datasets with fewer than 50,000 data points. The experiments involved using three different linear system solvers (Conjugate Gradients, Alternating Projections, and Stochastic Gradient Descent) for 100 iterations of outer-loop marginal likelihood optimization with a learning rate of 0.1. The table shows the test log-likelihood, total training time, and the average speedup achieved by each solver.", "section": "4 Warm Starting Linear System Solvers"}, {"figure_path": "CAdBTYBlOv/tables/tables_21_2.jpg", "caption": "Table 1: Test log-likelihoods, total training times, and average speed-up among datasets for CG, AP, and SGD after 100 outer-loop marginal likelihood steps with learning rate of 0.1. We consider five datasets with n < 50k, which allows us to solve to tolerance, and report the mean over 10 data splits.", "description": "This table presents the results of three different linear system solvers (Conjugate Gradients, Alternating Projections, and Stochastic Gradient Descent) on five datasets.  The solvers were run for 100 iterations of marginal likelihood optimization, with a learning rate of 0.1.  The table shows the test log-likelihood, total training time, solver time, and the speedup achieved by each method, averaged over 10 separate runs for each dataset.  The table highlights the impact of different techniques like the pathwise estimator and warm starting on improving solver performance.", "section": "Warm Starting Linear System Solvers"}, {"figure_path": "CAdBTYBlOv/tables/tables_21_3.jpg", "caption": "Table 1: Test log-likelihoods, total training times, and average speed-up among datasets for CG, AP, and SGD after 100 outer-loop marginal likelihood steps with learning rate of 0.1. We consider five datasets with n < 50k, which allows us to solve to tolerance, and report the mean over 10 data splits.", "description": "This table presents the results of three linear system solvers (Conjugate Gradients, Alternating Projections, and Stochastic Gradient Descent) on five datasets after 100 iterations of marginal likelihood optimization.  It shows the test log-likelihood, total training time, solver time, and speedup achieved by each solver for various combinations of pathwise estimator usage and warm starting.  The data used has less than 50,000 data points, allowing the solvers to reach convergence.", "section": "Warm Starting Linear System Solvers"}, {"figure_path": "CAdBTYBlOv/tables/tables_25_1.jpg", "caption": "Table 1: Test log-likelihoods, total training times, and average speed-up among datasets for CG, AP, and SGD after 100 outer-loop marginal likelihood steps with learning rate of 0.1. We consider five datasets with n < 50k, which allows us to solve to tolerance, and report the mean over 10 data splits.", "description": "This table presents the results of experiments comparing three linear system solvers (Conjugate Gradients, Alternating Projections, and Stochastic Gradient Descent) on five datasets.  The experiment involved running 100 outer-loop marginal likelihood optimization steps with a learning rate of 0.1. The table shows the mean test log-likelihood, total training time, and average speed-up achieved by each solver across the five datasets and two configurations (with and without warm start).  The small dataset size allowed the solvers to reach the tolerance.", "section": "Warm Starting Linear System Solvers"}, {"figure_path": "CAdBTYBlOv/tables/tables_25_2.jpg", "caption": "Table 1: Test log-likelihoods, total training times, and average speed-up among datasets for CG, AP, and SGD after 100 outer-loop marginal likelihood steps with learning rate of 0.1. We consider five datasets with n < 50k, which allows us to solve to tolerance, and report the mean over 10 data splits.", "description": "This table presents the results of applying three different linear system solvers (Conjugate Gradients, Alternating Projections, and Stochastic Gradient Descent) to the problem of hyperparameter optimization in iterative Gaussian processes.  The experiment involved 100 steps of marginal likelihood optimization with a learning rate of 0.1.  Five datasets with less than 50,000 data points were used, allowing the solvers to reach the tolerance.  The table shows the mean test log-likelihood, total training time, solver time, and speed-up achieved for each solver across all datasets and data splits.", "section": "4 Warm Starting Linear System Solvers"}, {"figure_path": "CAdBTYBlOv/tables/tables_25_3.jpg", "caption": "Table 1: Test log-likelihoods, total training times, and average speed-up among datasets for CG, AP, and SGD after 100 outer-loop marginal likelihood steps with learning rate of 0.1. We consider five datasets with n < 50k, which allows us to solve to tolerance, and report the mean over 10 data splits.", "description": "This table presents the results of three different linear system solvers (Conjugate Gradients, Alternating Projections, and Stochastic Gradient Descent) applied to five datasets after 100 iterations of the marginal likelihood optimization.  The table shows test log-likelihood, total training time, solver time, and speedup compared to the standard approach.  The small dataset size (n < 50k) ensures that solutions converge to the specified tolerance.", "section": "Warm Starting Linear System Solvers"}, {"figure_path": "CAdBTYBlOv/tables/tables_25_4.jpg", "caption": "Table 1: Test log-likelihoods, total training times, and average speed-up among datasets for CG, AP, and SGD after 100 outer-loop marginal likelihood steps with learning rate of 0.1. We consider five datasets with n < 50k, which allows us to solve to tolerance, and report the mean over 10 data splits.", "description": "This table presents a comparison of the performance of three different linear system solvers (Conjugate Gradients, Alternating Projections, and Stochastic Gradient Descent) across five datasets.  The solvers are used within an iterative Gaussian Process framework for hyperparameter optimization. The table shows test log-likelihood, total training time, solver time, and the speedup achieved by different techniques (pathwise estimation and warm start) relative to a standard approach.  The results are averaged over 10 different random data splits for each dataset. The focus is on demonstrating improvements in efficiency while maintaining predictive performance.", "section": "Warm Starting Linear System Solvers"}]