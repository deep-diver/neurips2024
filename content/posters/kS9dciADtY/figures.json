[{"figure_path": "kS9dciADtY/figures/figures_1_1.jpg", "caption": "Figure 1: Top: the structure comparison of (a) the previous foreground-based approach and (b) our cross-modal baseline. Bottom: the example of the common-action bias issue. The yellow section represents ground-truth segments. The blue and red lines denote the classification scores for our cross-modal baseline and Ti-FAD, respectively.", "description": "The figure compares three approaches for temporal action detection.  (a) shows a previous foreground-based approach which only uses visual features from pre-extracted proposals. (b) shows the proposed cross-modal baseline which integrates text and visual features throughout the detection process using cross-attention. The bottom part illustrates the common-action bias: the baseline tends to focus on common sub-actions even if the text input specifies a different action, while the Ti-FAD method addresses this issue.  The yellow sections highlight the ground truth action segments, while the blue and red lines represent the classification scores of the baseline and Ti-FAD respectively.", "section": "1 Introduction"}, {"figure_path": "kS9dciADtY/figures/figures_2_1.jpg", "caption": "Figure 2: (a) Structure of our cross-modal baseline. (b) Comparison with existing methods under 50%-50% and 75%-25% settings on THUMOS14. TPT denotes text prompt tuning.", "description": "Figure 2(a) shows the architecture of the proposed cross-modal baseline, which consists of multiple layers of multi-head self-attention (MHSA) and multi-head cross-attention (X-MHA) modules to effectively integrate text and visual information.  Figure 2(b) presents a comparison of the proposed baseline's performance against existing state-of-the-art methods (STALE, ZEETAD) on the THUMOS14 benchmark dataset, under two different experimental settings (50% seen/50% unseen actions and 75% seen/25% unseen actions). The results highlight the competitive performance of the proposed cross-modal baseline.", "section": "3 Method"}, {"figure_path": "kS9dciADtY/figures/figures_4_1.jpg", "caption": "Figure 3: (a) Overview of our proposed Ti-FAD, which includes a foreground-aware head that suppresses the irrelevant background frames, leading the model to concentrate more on the foreground segments, and (b) TiCA that guides the text to focus on the discriminative visual features by employing SAM.", "description": "This figure shows the architecture of the proposed Ti-FAD model.  (a) illustrates the overall model structure, highlighting the three main components: classification, localization, and foreground-aware head. The foreground-aware head is designed to focus on foreground action segments and suppress background noise.  (b) zooms in on the Text-infused Cross Attention (TiCA) module, which employs a Salient Attentive Mask (SAM) to guide the model to focus on the text-relevant, discriminative parts of the visual features.", "section": "3.3 Method"}, {"figure_path": "kS9dciADtY/figures/figures_9_1.jpg", "caption": "Figure 4: Per-unseen class AP (%) at tIoU threshold 0.5 on THUMOS14.", "description": "This bar chart compares the Average Precision (AP) at a threshold of 0.5 Intersection over Union (IoU) for the baseline model and the proposed Ti-FAD model across various unseen action classes in the THUMOS14 dataset.  It highlights the performance improvement achieved by Ti-FAD, particularly in classes with common sub-actions, indicating its effectiveness in focusing on discriminative sub-actions.  The higher AP values for Ti-FAD suggest its superior ability to accurately classify and localize unseen actions.", "section": "4.3 Further Analysis"}, {"figure_path": "kS9dciADtY/figures/figures_9_2.jpg", "caption": "Figure 1: Top: the structure comparison of (a) the previous foreground-based approach and (b) our cross-modal baseline. Bottom: the example of the common-action bias issue. The yellow section represents ground-truth segments. The blue and red lines denote the classification scores for our cross-modal baseline and Ti-FAD, respectively.", "description": "This figure compares the architecture of previous foreground-based ZSTAD methods with the proposed cross-modal baseline. The top part illustrates how previous methods integrate text and visual features only within foreground proposals, limiting the use of complete video information.  The bottom part shows an example of \"common-action bias,\" where the cross-modal baseline over-focuses on common sub-actions (e.g., \"Running\") rather than distinguishing between discriminative sub-actions relevant to the text description (e.g., \"Swing Up\" in Pole Vault). Ti-FAD's improved ability to focus on the relevant parts is highlighted.", "section": "1 Introduction"}, {"figure_path": "kS9dciADtY/figures/figures_12_1.jpg", "caption": "Figure A: Illustrations of different baseline architectures: (a) ActionFormer [27] w/o cross-modal fusion, (b) Cross-modal baseline (self-attn), and (c) (Our baseline) Cross-modal baseline (cross-attn)", "description": "This figure illustrates three different baseline architectures used in the paper for comparison.  (a) shows a standard ActionFormer without any cross-modal fusion. (b) depicts an ActionFormer enhanced with a self-attention mechanism for cross-modal integration. Finally, (c) presents the authors' proposed cross-modal baseline, utilizing cross-attention for improved text and video feature fusion.  The figure highlights the architectural differences in how text and video features are integrated, showcasing the progression towards the final model.", "section": "A Additional Experiments"}, {"figure_path": "kS9dciADtY/figures/figures_13_1.jpg", "caption": "Figure 4: Per-unseen class AP (%) at tIoU threshold 0.5 on THUMOS14.", "description": "This bar chart compares the Average Precision (AP) at a threshold of 0.5 Intersection over Union (IoU) for each unseen action class in the THUMOS14 dataset, between the baseline model and the proposed Ti-FAD model.  It highlights Ti-FAD's improved performance, particularly for action classes with common sub-actions (e.g., Running).", "section": "4.2 Main Results"}, {"figure_path": "kS9dciADtY/figures/figures_13_2.jpg", "caption": "Figure 5: Visualization of the detection results on THUMOS14 in the 50%-50% setting.", "description": "This figure visually compares the performance of the proposed Ti-FAD model and the baseline model on the THUMOS14 dataset. It specifically focuses on the \"Pole Vault\" action category, highlighting the differences in how each model identifies the action segments. The baseline model, shown in blue, struggles to accurately distinguish the \"Pole Vault\" action from similar actions like \"High Jump\" and \"Long Jump\", focusing on the common sub-action of running.  In contrast, the Ti-FAD model, displayed in red, is able to correctly identify the \"Pole Vault\" action by focusing on the more discriminative sub-action (Swing Up), effectively demonstrating the ability of the text-infused attention mechanism to improve accuracy by highlighting text-relevant visual features.", "section": "4.4 Qualitative Results"}]