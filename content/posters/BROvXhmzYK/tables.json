[{"figure_path": "BROvXhmzYK/tables/tables_5_1.jpg", "caption": "Table 1: Self-Discover significantly improves LLM reasoning across a diverse set of 25 complex tasks: BBH, T4D and MATH. CoT: zero-shot Chain of Thought (Kojima et al., 2022). PS: plan-and-solve prompting (Wang et al., 2023).", "description": "This table presents the performance of different LLMs (PaLM 2-L and GPT-4) on three reasoning benchmarks (BBH, T4D, and MATH) using four different prompting methods: Direct Prompting, Chain-of-Thought (CoT), Plan-and-Solve (PS), and Self-Discover.  It demonstrates that the Self-Discover framework substantially improves the performance of both LLMs across all three benchmarks compared to the other methods.", "section": "4 Results"}, {"figure_path": "BROvXhmzYK/tables/tables_12_1.jpg", "caption": "Table 3: Big Bench-Hard (Suzgun et al., 2022) per-task performance of GPT-4 and PaLM 2-L with SELF-DISCOVER.", "description": "This table presents a detailed breakdown of the performance of GPT-4 and PaLM 2-L language models on the Big Bench-Hard benchmark, categorized by individual tasks.  It shows the performance of each model using three different prompting methods: Direct Answering, Chain-of-Thought (COT), and the proposed Self-Discover method.  The results highlight the improvement achieved by using Self-Discover compared to the baseline methods across various reasoning tasks.", "section": "4.1 Does SELF-DISCOVER Improve LLM Reasoning?"}, {"figure_path": "BROvXhmzYK/tables/tables_13_1.jpg", "caption": "Table 3: Big Bench-Hard (Suzgun et al., 2022) per-task performance of GPT-4 and PaLM 2-L with SELF-DISCOVER.", "description": "This table presents the performance comparison between GPT-4 and PaLM 2-L language models on the Big Bench-Hard benchmark.  The results show the accuracy of each model across 23 different tasks when using different prompting methods: Direct Answering, Chain-of-Thought, and SELF-DISCOVER. The table highlights the improvement in performance achieved by the SELF-DISCOVER framework.", "section": "4.1 Does SELF-DISCOVER Improve LLM Reasoning?"}, {"figure_path": "BROvXhmzYK/tables/tables_13_2.jpg", "caption": "Table 3: Big Bench-Hard (Suzgun et al., 2022) per-task performance of GPT-4 and PaLM 2-L with SELF-DISCOVER.", "description": "This table presents a detailed breakdown of the performance of GPT-4 and PaLM 2-L language models on individual tasks within the Big Bench-Hard benchmark.  The performance is shown for three different prompting methods: direct answering, Chain-of-Thought, and the SELF-DISCOVER method introduced in the paper.  The table allows for a comparison of the effectiveness of SELF-DISCOVER against established baselines on a task-by-task basis.  Human performance (average and maximum) is also included as a reference point.", "section": "4.1 Does SELF-DISCOVER Improve LLM Reasoning?"}, {"figure_path": "BROvXhmzYK/tables/tables_14_1.jpg", "caption": "Table 1: Self-Discover significantly improves LLM reasoning across a diverse set of 25 complex tasks: BBH, T4D and MATH. CoT: zero-shot Chain of Thought (Kojima et al., 2022). PS: plan-and-solve prompting (Wang et al., 2023).", "description": "This table presents the performance comparison of the SELF-DISCOVER framework against several baseline methods on three complex reasoning benchmarks: Big-Bench Hard (BBH), Thinking for Doing (T4D), and MATH.  The methods compared include Direct Prompting, Chain of Thought (CoT), Plan-and-Solve (PS), and SELF-DISCOVER, each applied to both PaLM 2-L and GPT-4 language models.  The results are presented as the percentage accuracy achieved by each method on each benchmark.  The table highlights the significant improvement in accuracy achieved by SELF-DISCOVER compared to the other methods.", "section": "4 Results"}, {"figure_path": "BROvXhmzYK/tables/tables_15_1.jpg", "caption": "Table 5: MMLU (Suzgun et al., 2022) per-task performance of GPT-4 and PaLM 2-L with SELF-DISCOVER. We sampled 10 tasks with 50 examples each. SD (instance) refers to that we run stage one on each question and use the generated structure during solving, to acount for the diversity of questions. [New Table]", "description": "This table presents the per-task performance of GPT-4 and PaLM 2-L language models on 10 randomly sampled tasks from the MMLU benchmark.  The performance is measured using three different prompting methods:\n\n1.  Direct Prompting: The model directly generates the answer.\n2.  Chain-of-Thought (CoT): The model generates a reasoning process before giving the answer.\n3.  Self-Discover (+SD): The model uses the Self-Discover framework to generate a task-specific reasoning structure which guides the reasoning process before providing the answer.  Two variations of Self-Discover are tested:  one where the reasoning structure is created once per task and another where a structure is created for each question instance.\n\nThe table allows for a comparison of the performance of the different prompting methods on multiple tasks and models, highlighting the effectiveness of Self-Discover in improving the reasoning capabilities of LLMs.", "section": "4.1 Does SELF-DISCOVER Improve LLM Reasoning?"}, {"figure_path": "BROvXhmzYK/tables/tables_16_1.jpg", "caption": "Table 3: Big Bench-Hard (Suzgun et al., 2022) per-task performance of GPT-4 and PaLM 2-L with SELF-DISCOVER.", "description": "This table presents a detailed breakdown of the performance of GPT-4 and PaLM 2-L language models on the Big Bench-Hard benchmark, comparing their accuracy across various tasks with and without the SELF-DISCOVER framework.  It shows the model's performance on each individual task, highlighting the impact of SELF-DISCOVER on improving accuracy. The table provides a granular view of the effectiveness of SELF-DISCOVER across diverse reasoning challenges.", "section": "4.1 Does SELF-DISCOVER Improve LLM Reasoning?"}, {"figure_path": "BROvXhmzYK/tables/tables_17_1.jpg", "caption": "Table 3: Big Bench-Hard (Suzgun et al., 2022) per-task performance of GPT-4 and PaLM 2-L with SELF-DISCOVER.", "description": "This table presents a detailed breakdown of the performance of GPT-4 and PaLM 2-L language models on the Big Bench Hard benchmark, comparing their accuracy across 23 diverse reasoning tasks.  The results are shown for three different prompting methods: Direct, Chain-of-Thought (+CoT), and the authors' proposed SELF-DISCOVER approach. The table allows for a granular comparison of the effectiveness of each method across various tasks, highlighting the strengths and weaknesses of each approach in different reasoning scenarios.", "section": "4.1 Does SELF-DISCOVER Improve LLM Reasoning?"}, {"figure_path": "BROvXhmzYK/tables/tables_19_1.jpg", "caption": "Table 3: Big Bench-Hard (Suzgun et al., 2022) per-task performance of GPT-4 and PaLM 2-L with SELF-DISCOVER.", "description": "This table presents a detailed breakdown of the performance of GPT-4 and PaLM 2-L language models on the Big Bench-Hard benchmark, categorized by individual tasks.  The results showcase the models' performance using three different prompting methods: direct prompting, chain-of-thought prompting, and the SELF-DISCOVER framework. The table allows for a direct comparison of the effectiveness of SELF-DISCOVER against standard prompting techniques on various reasoning tasks.", "section": "4.1 Does SELF-DISCOVER Improve LLM Reasoning?"}]