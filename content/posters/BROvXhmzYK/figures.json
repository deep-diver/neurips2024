[{"figure_path": "BROvXhmzYK/figures/figures_0_1.jpg", "caption": "Figure 1: SELF-DISCOVER guides LLMs to self-discover and compose atomic reasoning modules into a reasoning structure to solve challenging tasks. Through testing on challenging reasoning benchmarks including Big Bench-Hard (BBH), agent reasoning (T4D), and MATH, we find that SELF-DISCOVER outperforms Direct Answering on 23/25 and CoT on 21/25 tasks in zero-shot setting using PaLM 2-L. Full BBH results are in Appendix C Table 3.", "description": "This figure illustrates the SELF-DISCOVER framework and its performance compared to Direct Answer and Chain-of-Thought (CoT) methods.  The framework allows large language models (LLMs) to automatically discover and compose multiple reasoning modules into a customized reasoning structure.  The charts show the accuracy improvements achieved by SELF-DISCOVER over both baselines on various challenging reasoning benchmarks (BigBench-Hard, Thinking for Doing, and MATH).", "section": "1 Introduction"}, {"figure_path": "BROvXhmzYK/figures/figures_2_1.jpg", "caption": "Figure 1: SELF-DISCOVER guides LLMs to self-discover and compose atomic reasoning modules into a reasoning structure to solve challenging tasks. Through testing on challenging reasoning benchmarks including Big Bench-Hard (BBH), agent reasoning (T4D), and MATH, we find that SELF-DISCOVER outperforms Direct Answering on 23/25 and CoT on 21/25 tasks in zero-shot setting using PaLM 2-L. Full BBH results are in Appendix C Table 3.", "description": "This figure illustrates the SELF-DISCOVER framework, showing how it helps LLMs to self-discover and combine different reasoning modules into a structured approach for problem-solving.  The charts compare the performance of SELF-DISCOVER against standard methods (Direct Answer and Chain-of-Thought) across several challenging reasoning benchmarks (BigBench-Hard, Thinking for Doing, and MATH). The results demonstrate significant improvements achieved by SELF-DISCOVER, especially on tasks involving complex reasoning, highlighting its effectiveness in improving LLMs' reasoning capabilities.", "section": "1 Introduction"}, {"figure_path": "BROvXhmzYK/figures/figures_3_1.jpg", "caption": "Figure 3: Illustration of three actions of SELF-DISCOVER. We use LMs to compose a coherent reasoning structure by selecting relevant modules, adapting to task-specific descriptions, and implement a reasoning structure in JSON.", "description": "This figure illustrates the three main steps involved in the SELF-DISCOVER framework's first stage.  First, the \"SELECT\" step uses a language model (LM) to choose relevant reasoning modules from a given set based on task examples.  Next, the \"ADAPT\" step uses the LM to refine the selected module descriptions, making them more specific to the current task. Finally, the \"IMPLEMENT\" step employs the LM to transform the adapted descriptions into a structured, actionable reasoning plan, formatted as a JSON object, ready for use in the next stage.", "section": "2 Self-Discovering Reasoning Structures for Problem-Solving"}, {"figure_path": "BROvXhmzYK/figures/figures_5_1.jpg", "caption": "Figure 4: Breakdown of SELF-DISCOVER performance improvement on 4 categories on PaLM 2-L. SELF-DISCOVER performs the best on tasks requiring world knowledge.", "description": "The figure shows a bar chart comparing the average accuracy improvement of the SELF-DISCOVER method over two baseline methods (Direct Answering and Chain-of-Thought) across four categories of reasoning problems from the Big Bench Hard benchmark.  The categories are Multilingual, Algorithmic, Natural Language Understanding (NLU), and World Knowledge.  The chart clearly demonstrates that SELF-DISCOVER provides the most significant improvements in tasks requiring world knowledge, indicating its strength in leveraging external information for reasoning.  Improvements are also observed across other categories, although less pronounced than for World Knowledge tasks.", "section": "4.2 Which Types of Problems Do SELF-DISCOVER Help the Most?"}, {"figure_path": "BROvXhmzYK/figures/figures_6_1.jpg", "caption": "Figure 1: SELF-DISCOVER guides LLMs to self-discover and compose atomic reasoning modules into a reasoning structure to solve challenging tasks. Through testing on challenging reasoning benchmarks including Big Bench-Hard (BBH), agent reasoning (T4D), and MATH, we find that SELF-DISCOVER outperforms Direct Answering on 23/25 and CoT on 21/25 tasks in zero-shot setting using PaLM 2-L. Full BBH results are in Appendix C Table 3.", "description": "This figure showcases the performance improvements achieved by SELF-DISCOVER on three challenging reasoning benchmarks: BigBench-Hard (BBH), Thinking for Doing (T4D), and MATH.  The bar charts compare SELF-DISCOVER's accuracy against two baseline methods: Direct Answering and Chain-of-Thought (CoT). The results demonstrate significant improvements in accuracy for SELF-DISCOVER across all three benchmarks when using PaLM 2-L, highlighting its effectiveness in enhancing large language models' reasoning capabilities.", "section": "1 Introduction"}, {"figure_path": "BROvXhmzYK/figures/figures_7_1.jpg", "caption": "Figure 1: SELF-DISCOVER guides LLMs to self-discover and compose atomic reasoning modules into a reasoning structure to solve challenging tasks. Through testing on challenging reasoning benchmarks including Big Bench-Hard (BBH), agent reasoning (T4D), and MATH, we find that SELF-DISCOVER outperforms Direct Answering on 23/25 and CoT on 21/25 tasks in zero-shot setting using PaLM 2-L. Full BBH results are in Appendix C Table 3.", "description": "This figure shows the performance improvement of SELF-DISCOVER compared to direct answering and Chain-of-Thought (CoT) methods on three challenging reasoning benchmarks: BigBench-Hard (BBH), Thinking for Doing (T4D), and MATH.  The bar charts illustrate the accuracy gains achieved by SELF-DISCOVER across various tasks within each benchmark.  The results demonstrate that SELF-DISCOVER significantly improves the performance of LLMs on complex reasoning problems by enabling them to self-discover and compose task-specific reasoning structures.", "section": "1 Introduction"}, {"figure_path": "BROvXhmzYK/figures/figures_7_2.jpg", "caption": "Figure 1: SELF-DISCOVER guides LLMs to self-discover and compose atomic reasoning modules into a reasoning structure to solve challenging tasks. Through testing on challenging reasoning benchmarks including Big Bench-Hard (BBH), agent reasoning (T4D), and MATH, we find that SELF-DISCOVER outperforms Direct Answering on 23/25 and CoT on 21/25 tasks in zero-shot setting using PaLM 2-L. Full BBH results are in Appendix C Table 3.", "description": "This figure shows the performance of SELF-DISCOVER compared to baseline methods (Direct Answering and Chain-of-Thought) on three challenging reasoning benchmarks: Big Bench Hard (BBH), Thinking for Doing (T4D), and MATH.  The bar charts illustrate the accuracy improvements achieved by SELF-DISCOVER.  The results demonstrate that SELF-DISCOVER significantly improves the performance of LLMs on these complex reasoning tasks.", "section": "1 Introduction"}, {"figure_path": "BROvXhmzYK/figures/figures_7_3.jpg", "caption": "Figure 8: Transferrability tests of optimized prompts (OPRO) and composed structures (SELF-DISCOVER).", "description": "This figure compares the performance of SELF-DISCOVER and OPRO (a prompt optimization method) when transferring reasoning structures/prompts optimized using PaLM 2-L to GPT-4.  It shows the accuracy achieved on four different reasoning tasks (Snarks, Movie, T4D, Geometry).  The results indicate that SELF-DISCOVER maintains relatively high accuracy when transferring to a different model, suggesting its reasoning structures are more robust and generalizable than OPRO-optimized prompts.  The y-axis represents the accuracy, and the x-axis represents the tasks.", "section": "5 Deep Diving Into Self-Discovered Reasoning Structures"}, {"figure_path": "BROvXhmzYK/figures/figures_14_1.jpg", "caption": "Figure 1: SELF-DISCOVER guides LLMs to self-discover and compose atomic reasoning modules into a reasoning structure to solve challenging tasks. Through testing on challenging reasoning benchmarks including Big Bench-Hard (BBH), agent reasoning (T4D), and MATH, we find that SELF-DISCOVER outperforms Direct Answering on 23/25 and CoT on 21/25 tasks in zero-shot setting using PaLM 2-L. Full BBH results are in Appendix C Table 3.", "description": "This figure shows the results of the SELF-DISCOVER framework on three challenging reasoning benchmarks: Big-Bench Hard (BBH), Thinking for Doing (T4D), and MATH.  It compares the performance of SELF-DISCOVER against two baselines: Direct Answering (no reasoning steps) and Chain-of-Thought (CoT). The bar charts illustrate the accuracy improvements achieved by SELF-DISCOVER over both baselines.  The key finding is that SELF-DISCOVER significantly outperforms both baselines on the majority of tasks, demonstrating its effectiveness in improving LLM reasoning abilities.", "section": "1 Introduction"}, {"figure_path": "BROvXhmzYK/figures/figures_15_1.jpg", "caption": "Figure 1: SELF-DISCOVER guides LLMs to self-discover and compose atomic reasoning modules into a reasoning structure to solve challenging tasks. Through testing on challenging reasoning benchmarks including Big Bench-Hard (BBH), agent reasoning (T4D), and MATH, we find that SELF-DISCOVER outperforms Direct Answering on 23/25 and CoT on 21/25 tasks in zero-shot setting using PaLM 2-L. Full BBH results are in Appendix C Table 3.", "description": "This figure shows the performance of SELF-DISCOVER compared to baseline methods (Direct Answering and Chain-of-Thought) across three challenging reasoning benchmarks: BigBench-Hard (BBH), Thinking for Doing (T4D), and MATH.  The bar charts illustrate the improvement in accuracy achieved by SELF-DISCOVER.  The results demonstrate that SELF-DISCOVER substantially improves the performance of LLMs, especially on tasks that require complex reasoning steps.", "section": "1 Introduction"}]