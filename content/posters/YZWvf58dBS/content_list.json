[{"type": "text", "text": "Why Not Transform Chat Large Language Models to Non-English? ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The scarcity of non-English data limits the development of non-English large language models (LLMs). Transforming English-centric LLMs to non-English has been identified as an effective and resource-efficient method. Previous works start from base LLMs and perform knowledge distillation (KD) with data generated by stronger LLMs, e.g. GPT-4. Compared to base LLMs, chat LLMs are further optimized for advanced abilities, e.g. multi-turn conversation and human preference alignment, and thus more powerful in both helpfulness and safety. However, transforming a chat LLM involves two critical issues: (1) How can we effectively transfer advanced abilities without their supervised data? (2) How can we prevent the original knowledge from catastrophic forgetting during transformation? We target these issues by introducing a simple framework called TransLLM. For the first issue, TransLLM divides the transfer problem into some common sub-tasks with the translation chain-of-thought, which uses the translation as the bridge between English and non-English step-by-step. We further enhance the performance of sub-tasks with publicly available data. For the second issue, we propose a method comprising two synergistic components: low-rank adaptation for training to maintain the original LLM parameters, and recovery KD, which utilizes data generated by the chat LLM itself to recover the original knowledge from the frozen parameters. In the experiments, we transform the LLaMA-2-chat-7B to the Thai language. Our method, using only single-turn data, outperforms strong baselines and ChatGPT on multi-turn benchmark MT-bench. Furthermore, our method, without safety data, rejects more harmful queries of safety benchmark AdvBench than both ChatGPT and GPT-4. ", "page_idx": 0}, {"type": "table", "img_path": "YZWvf58dBS/tmp/0fda4e09805b38c89d81151dd7fcd247e982eec6ad76a62ca806cebe61bf8d23.jpg", "table_caption": [], "table_footnote": [], "page_idx": 0}, {"type": "text", "text": "Figure 1: In this example, the attacker jailbreaks ChatGPT in Thai, while our method successfully rejects to response. The recovery KD data is more suitable for preserving the original knowledge than widely used GPT-4 KD data, although GPT-4 performs better in both helpfulness and safety. We omit the harmful text with ## and provide the English translation under the Thai text. ", "page_idx": 0}, {"type": "text", "text": "24 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "25 Recently, significant influence has been demonstrated by chat large language models (LLMs), such   \n26 as ChatGPT (OpenAI, 2022), Palm-2 (Anil et al., 2023), and LLaMA-2-chat (Touvron et al., 2023).   \n27 Their high capabilities rely on massive data and complex training processes. Taking the LLaMA  \n28 2-chat as an example, the training usually includes the following steps: (1) pre-training (PT) on a   \n29 large monolingual corpus to obtain the base LLM; (2) supervised fine-tuning (SFT) on multi-turn   \n30 dialogue datasets; (3) iteratively refining on human preference datasets using reinforcement learning   \n31 with human feedback (RLHF) methodologies (Ouyang et al., 2022). These steps help in creating   \n32 LLMs that not only understand and generate human-like text but also align with human values, and   \n33 therefore provide safe and useful responses.   \n34 Unfortunately, popular unlabeled and labeled training data is English-dominated. Consequently,   \n35 LLMs are less satisfying in terms of both usefulness and safety when being applied to non-English.   \n36 Yong et al. (2023) have shown that even powerful LLMs, such as GPT-4, are vulnerable to safety   \n37 concerns in non-English.   \n38 To improve the non-English performance, recent works attempt to transfer knowledge from English   \n39 to non-English. However, they focus on base LLMs instead of powerful chat LLMs. Basically,   \n40 they start from base LLMs and use knowledge distillation (KD) data generated by the strong LLM,   \n41 like GPT-4, for transfer training and instruction tuning. For example, PolyLM (Wei et al., 2023)   \n42 transfers English knowledge implicitly via multilingual instruction tuning on a multilingual base   \n43 LLM. X-LLaMA (Zhu et al., 2023) supplements the multilingual instruction-following task with the   \n44 translation task to build semantic alignment across languages.   \n45 When transforming the base LLMs, instruct tuning is performed simultaneously with or after transfer   \n46 training. Therefore, the instruction following knowledge, i.e. the basic conversation knowledge,   \n47 will not be overridden by extra knowledge. However, for chat LLMs, the advanced conversation   \n48 knowledge, especially human preference, has been incorporated into the model parameters during   \n49 fine-tuning. As a result, subsequent transfer training in previous works will result in catastrophic   \n50 forgetting of such knowledge. What is worse, the high-quality STF data used for training the chat   \n51 LLM is precious and usually unavailable. Therefore, transforming a chat LLM involves two critical   \n52 issues: (1) How can we transfer advanced abilities with limited available data? (2) How can we   \n53 prevent the original English knowledge from catastrophic forgetting during transfer?   \n54 To build safe non-English LLMs as shown in Figure 1, we target these issues by introducing a simple   \n55 framework called TransLLM. For the first issue, TransLLM utilizes the translation chain-of-thought   \n56 (TCOT) (Zhang et al., 2023), which models the transfer as some common sub-tasks. During TCOT,   \n57 the LLM will handle the non-English query step-by-step in single inference: it first translates the   \n58 query to English; then it responds to the query in English; and finally it, generates the non-English   \n59 answer based on all the above context. We further enhance the the performance of sub-tasks with   \n60 publicly available data thus TCOT can transfer English knowledge effectively. For the second issue,   \n61 we propose a method comprising two synergistic components: (1) We employ the low-rank adaptation   \n62 (LoRA) (Hu et al., 2021) for training to maintain the original LLM parameters. (2) We introduce   \n63 recovery KD, utilizing data generated by the chat LLM itself, to recover the original knowledge from   \n64 the frozen parameters. The recovery KD data can be ftited easily using the original parameters. This   \n65 enables the LLM to learn a \u201cshortcut\u201d that uses the English knowledge from the original parameters.   \n66 As shown in Figure 2, TransLLM organizes all the above ideas into the following steps: (1) Model   \n67 extension: we extend the model with LoRA modules and fine-grained target language vocabulary. (2)   \n68 Target language pre-training: we pre-train the chat LLM on the monolingual target language data   \nso that the LLM can leverage such knowledge to improve translation and target language responses.   \n70 (3) Translation pre-training: we further train the LLM with a bi-directional translation task between   \n71 English and the target language, and we also introduce the English language modeling task to protect   \n72 the English embeddings. (4) Transfer fine-tuning: we fine-tune the LLM on TCOT, recover KD, and   \n73 translation data so that the LLM can respond in English, the target language, and the translation tasks   \n74 automatically.   \n75 We conduct comprehensive experiments for transforming the LLaMA-2-chat-7B from English to   \n76 Thai. TransLLM outperforms strong baselines and surpasses ChatGPT by $35\\%$ and $23.75\\%$ for the   \n77 first and second turns on the MT-bench with statistical significance. More importantly, we attain an   \n78 improvement of $14.8\\%$ and $8.65\\%$ over ChatGPT and GPT-4 respectively on the safety benchmark   \n79 AdvBenchmark with statistical significance. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "80 Our main contributions are summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "81 \u2022 In this paper, we highlight the advantages and challenges of transforming a chat LLM to   \n82 non-English and propose a simple yet effective framework for this end.   \n83 \u2022 The experiments indicate TransLLM successfully transfer advanced abilities, e.g. multi-turn   \n84 conversation and human preference alignment, with limited available data. TransLLM, with   \n85 only 7 billion parameters, outperforms ChatGPT in Thai in both helpfulness and safety.   \n86 \u2022 Analysis shows that recovery KD plus with LoRA successfully preserves the original   \n87 knowledge. The TransLLM model mostly uses the original knowledge for English while   \n88 uses the new knowledge for Thai.   \n89 \u2022 We discuss the limitations of TransLLM, and point out several potential future directions. We   \n90 will make our code and datasets publicly available (please refer to supplementary materials).   \n91 We hope this work can lay a solid foundation for developing safe LLMs in non-English. ", "page_idx": 1}, {"type": "image", "img_path": "YZWvf58dBS/tmp/c9dc69f14ac6ba19437501852409f0281bb527a5624d88968a6e4e8c826717f9.jpg", "img_caption": ["Step1: Model Extension Step2:\u00a0Target Language Pre-Training Step4: Transfer Fine-Tuning "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "92 2 Background ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "93 The language models are trained to predict the next token in a sequence given the previous tokens by   \n94 maximum likelihood estimation (MLE), which can be represented by the following equation: ", "page_idx": 2}, {"type": "equation", "text": "$$\nJ_{\\mathrm{PT}}=\\arg\\operatorname*{max}_{\\theta}\\sum_{i=1}^{|y|}\\log P(y_{i}|y_{<i};\\theta),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "95 where $\\theta$ denotes learnable model parameters, and $y_{<i}$ are the tokens preceding $y_{i}$ in the sequence. ", "page_idx": 2}, {"type": "text", "text": "96 For fine-tuning on a supervised dataset, each instance contains a query $x$ and its corresponding label   \n97 $y$ . The SFT loss is only calculated on the label $y$ , ignoring the query $x$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\nJ_{\\mathrm{SFT}}=\\arg\\operatorname*{max}_{\\theta}\\sum_{i=1}^{|y|}\\log P(y_{i}|x,y_{<i};\\theta).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "98 For both PT and SFT, the special tokens $<\\!\\mathrm{s}\\!>$ and $<\\!/\\mathrm{s}\\!>$ are added at the beginning and the end of the   \n99 training instance respectively. ", "page_idx": 2}, {"type": "text", "text": "100 3 Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "101 In this section, we first describe the model architecture, then we introduce the training and inference   \n102 procedures in detail. ", "page_idx": 2}, {"type": "text", "text": "103 3.1 Model Architecture ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "104 Nowadays, popular LLMs use byte-level byte pair encoding (BBPE) tokenizer (Wang et al., 2020)   \n105 following GPT-2 (Radford et al., 2019). However, the tokenizer is usually developed on the English  \n106 dominated dataset, therefore this tokenizer often tokenizes each non-English character to several   \n107 bytes resulting in a long sequence. Inspired by Cui et al. (2023) and Pipatanakul et al. (2023), we   \n108 extend the vocabulary using monolingual data of the target language to improve the model efficiency.   \n109 LoRA is a parameter-efficient training method, which is another technique that has been widely used   \n110 for transferring the LLM. However, in this work, we use LoRA not only for efficiency but also for   \n111 preserving the original parameters. Considering a weight matrix $W\\,\\doteq\\,\\mathbb{R}^{d\\times k}$ of the target LLM,   \n112 LoRA represents its update $\\Delta W$ using two low rank matrices $B\\in\\mathbb{R}^{d\\times r}$ and $A\\in\\mathbb{R}^{r\\times k}$ as follows: ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "equation", "text": "$$\n\\tilde{h}=W h,~\\mathrm{and}~~\\hat{h}=\\tilde{h}+\\Delta W h=\\tilde{h}+B A h,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "113 where $r$ denotes the pre-determined rank, $h$ denotes the input, \u02dch denotes the output of the original   \n114 module, and $\\hat{h}$ denotes the output of the updated module. During training, the original $W$ is frozen,   \n115 so that original knowledge can be recovered by the recovery KD. ", "page_idx": 3}, {"type": "text", "text": "116 3.2 Training ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "117 3.2.1 Target Language Pre-Training ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "118 The chat LLMs are often insufficient on target language modeling due to the imbalanced training   \n119 corpus. Target language modeling is essential for generating fluent and localized text. Furthermore,   \n120 many works show that the monolingual pre-training can significantly improve the translation qual  \n121 ity (Zheng et al., 2019; Xu et al., 2023). To build a solid foundation for the target language, we   \n122 pre-train the TransLLM model on monolingual data of the target language using Eq. 1.   \n123 We do not introduce any English task in this stage because of the following two reasons: first, the   \n124 pre-training involves quite computational consumption, and it can be unacceptable to find a proper   \n125 mixing ratio between the English and target language data; second, the English embeddings are   \n126 rarely updated on the target language data, therefore all the parameters of original LLM are almost   \n127 unchanged. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "128 3.2.2 Translation Pre-Training ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "129 TCOT relies on translation to bridge the English and the target language. Therefore, we introduce   \n130 translation pre-training to improve the bidirectional translation quality between English and the   \n131 target language. Inspired by mBART (Liu et al., 2020), we use the special language id token to   \n132 denote translation directions. Considering we transform the LLM from language $\\alpha$ to $\\beta$ , where $\\alpha=$   \n133 English in this paper, we formulate the parallel pair $(s^{\\alpha},s^{\\beta})$ as two instances: ca $:(s^{\\alpha},<\\beta>,s^{\\beta})$ and   \n134 $\\operatorname{cat}(s^{\\beta},<\\!\\alpha\\!>,s^{\\alpha})$ , where cat $(\\cdot)$ denotes the concatenate operation.   \n135 The translation training could disturb the original English embeddings. Thus, we introduce English   \n136 monolingual data into the translation pre-training stage. Specifically, we randomly insert the transla  \n137 tion instance between English monolingual data using line break $\\mathbf{\\omega}^{\\bullet}\\backslash\\mathbf{n}^{\\bullet}$ as the separator. Based on the   \n138 first stage, we train the TransLLM model on the mixed data by pre-training objective in Eq. 1. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "139 3.2.3 Transfer Fine-Tuning ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "140 The two-stage pre-training enables the TransLLM in target language modeling and cross-lingual   \n141 translation. However, the TransLLM inevitably forgets the original knowledge. In this stage, we aim   \n142 to recover the original knowledge and teach the TransLLM model how to perform TCOT and when   \n143 to do translation.   \n144 Recovery Knowledge Distillation Data. Previous works focus on transferring knowledge from   \n145 base LLMs. To teach the base model how to follow human instructions, previous works perform   \n146 knowledge distillation with strong chat LLMs as the teacher by using the Alpaca dataset (Taori et al.,   \n147 2023). The Alpaca dataset generates queries using the self-instruct technique (Wang et al., 2022),   \n148 then responds using ChatGPT or GPT-4. Although the vanilla KD works well for base LLMs, we   \n149 argue that it is not helpful for chat LLMs as shown in Sec. 5.2. To address this problem, we introduce   \n150 the recovery KD that uses the target chat LLM to generate the responses. Although the recovery   \n151 KD data are often worse than GPT-4 KD data, it will help the model to recover the knowledge from   \n152 the original LLM parameters. We also introduce a special token ${\\tt{<}}\\mathrm{{RESPONSE>}}$ in recovery KD to   \n153 direct the behavior of the TransLLM model. Considering a KD instance in English with query $q^{\\alpha}$ and   \n154 answer $a^{\\alpha}$ , we formulate the query and label in Eq. 2 as $x=q^{\\alpha}$ and $y=\\operatorname{cat}(<\\scriptstyle\\mathrm{{RESPONSE}}>,a^{\\alpha})$   \n155 respectively.   \n156 TCOT Data. Based on the recovery KD data $(q^{\\alpha},a^{\\alpha})$ , we use machine translation to ob  \n157 tain its translations $(q^{\\beta},a^{\\beta})$ . Finally, we can organize the TCOT data as $x\\ =\\ q^{\\beta}$ and $y=$   \n158 ca $\\mathsf{t}(<\\alpha>,q^{\\alpha}$ $q^{\\alpha},<\\scriptstyle\\mathrm{RESPONSE}>,a^{\\alpha},<\\beta>,a^{\\beta})$ . That means when we input a query in $\\beta$ , the model   \n159 should first translate it into $\\alpha$ as $q^{\\alpha}$ . Then the model should <RESPONSE> the English query as   \n160 $a^{\\alpha}$ using original knowledge as we teach in recovery KD. Finally, the TCOT outputs the response   \n161 in $\\beta$ as $\\bar{a}^{\\beta}$ based on all previous sequences. As discussed in Sec. 5.3, the previous sequences also   \n162 contribute to the final response. Different from Zhang et al. (2023), we use special tokens instead of   \n163 natural language to direct the model\u2019s behavior. This is because the special tokens will not disturb the   \n164 English embeddings and make it convenient to extract results.   \n165 Translation Data. Due to the TCOT data, the model may be confused about the translation   \n166 instruction in $\\beta$ without extra translation SFT. Therefore, we also construct bi-direction translation   \n167 data based on previous parallel pairs $(q^{\\alpha},q^{\\beta})$ and $(a^{\\alpha},a^{\\beta})$ . Taking the parallel pair $(q^{\\alpha},q^{\\beta})$ as an   \n168 example, we first wrap the source sentence using translation prompt templates as prompt $(q^{\\alpha})$ .1 Then   \n169 we can obtain $x=\\mathrm{prompt}(q^{\\alpha})$ and $y=\\operatorname{cat}(<\\!\\beta\\!>,q^{\\beta})$ . ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "170 Finally, we randomly mix all the data mentioned above and fine-tune the TransLLM model by Eq. 2. ", "page_idx": 4}, {"type": "text", "text": "171 3.3 Inference ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "172 The final TransLLM model can respond in both $\\alpha$ and $\\beta$ , including $\\alpha{-}\\beta$ bi-direction translation.   \n173 For a single-turn conversation, the TransLLM model will decide the proper mode by itself given   \n174 only the input query $x$ . To leverage the powerful multi-turn conversation ability of the original   \n175 LLM for $\\beta$ , we follow the original multi-turn format. For the multi-turn task in $\\beta$ , we only take   \n176 the English parts of the previous TCOT output as history. To be specific, we organize the input as   \n177 $x=\\mathbf{c}\\mathbf{\\bar{a}}\\mathbf{\\bar{t}}(q_{1}^{\\alpha},\\stackrel{\\_}{a_{1}^{\\alpha}},\\stackrel{\\_}{\\dots},q_{n}^{\\alpha},\\stackrel{\\_}{a_{n}^{\\alpha}},q_{n+1}^{\\beta})$ , where $n$ is the number of past turns. We do not use any special   \n178 tokens in the history as the original LLM does. Interestingly, even in this unseen setting, the model   \n179 still outputs the TCOT format as $y=\\operatorname{cat}(<\\!\\alpha\\!>,q_{n+1}^{\\alpha}$ , <RESPONSE>, $a_{n+1}^{\\alpha},<\\!\\beta\\!>,a_{n+1}^{\\beta}\\!\\bar{)}$ . We show   \n180 the whole multi-turn template in Appendix A.3. ", "page_idx": 4}, {"type": "text", "text": "181 4 Experiments ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "182 4.1 Settings ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "183 It is extravagant to train and evaluate a chat LLM in non-English. Therefore, during our experiment,   \n184 we mainly transform LLMs from English (EN) to Thai (TH) language, i.e. $\\alpha=\\mathrm{EN}$ and $\\beta=\\mathrm{TH}$ . We   \n185 describe our basic settings as follows.   \n186 Models. We implement our pipeline using Chinese-LLaMA-Alpaca- $\\cdot2^{2}$ project, which is based on   \n187 Transformers3. For the TransLLM model, we use the LLaMA2-Chat-7B as the target chat LLM.   \n188 Following Cui et al. (2023), we use SentencePiece (Kudo and Richardson, 2018) to learn the TH   \n189 vocabulary on the monolingual TH data that we use in target language pre-training. After we merge   \n190 the TH vocabulary with the original vocabulary, the final vocabulary size (including 3 special tokens)   \n191 is 43,012. The new embeddings are randomly initialized. We apply LoRA on the weights of the   \n192 attention module and multi-layer perceptron blocks. The LoRA rank is set as $r=64$ . Overall, there   \n193 are a total of 512.27 million trainable parameters including embeddings and LM heads. After all of   \n194 the training is completed, we merge the LoRA modules into the main backbone, the final model has   \n195 6.83 billion parameters. For a fair comparison, we re-implement most of the baselines by our setting   \n196 following their papers. The details of our model and baselines are in Appendix A.1.   \n197 Training Data. For target language pre-training, we use the monolingual TH data from mC4 (Xue   \n198 et al., 2020). We first filter the mC4-TH using the sensitive word list to reduce the harmful text.   \n199 Then, we use MinHashLSH4 to deduplicate documents in mC4-TH following GPT-3 (Brown et al.,   \n200 2020). Finally, we have about 11 billion tokens of TH data. Compared to the 2 trillion tokens EN   \n201 data used in LLaMA-2, the TH dataset is quite small. For translation pre-training, we collect the   \n202 EN-TH parallel data from CCAligned (Chaudhary et al., 2019), Tatoeba Challenge Data (Tiedemann,   \n203 2020), and OpenSubtitles (Lison et al., 2018). We directly use the EN documents released in the Pile   \n204 dataset which has been pre-processed (Gao et al., 2020). We randomly sample 1 million parallel pairs   \n205 and EN documents respectively for translation pre-training. For the transfer fine-tuning, we use the   \n206 query from the Alpaca dataset and generate the response using the target model LLaMA2-Chat-7B.   \n207 We further use Google Translate to obtain TCOT and translation data based on recovery KD data.   \n208 In our preliminary study, Google Translate may translate the variable in code which is not desirable   \n209 for the chat LLM. Thus, we use GPT-4 to recognize the \u201cdo not translate\u201d part. We use the same   \n210 monolingual and translation data for baselines, while we use the Alpaca-GPT-4 (Peng et al., 2023) as   \n211 the SFT data following their setting. There are a total of 52K queries in the Alpaca dataset, we use   \n212 the first 50K queries as the training set and the rest 2K queries as the validation set in our experiments.   \n213 We provide the training details in A.2.   \n214 Benchmark. For helpfulness, we utilize two widely used LLM benchmarks, MT-bench (Zheng   \n215 et al., 2024) and AlpacaEval (Li et al., 2023). MT-bench has 80 multi-turn questions across 8 domains.   \n216 AlpacaEval consists of 805 single-turn questions collected from 5 different subsets. The original   \n217 benchmarks are in EN. We employ professional translators to translate these two datasets into TH.   \n218 For safety, we use the AdvBench. AdvBench consists of 520 instructions that induce LLMs output   \n219 harmful responses. Following the setting in Yong et al. (2023), we directly use Google Translate to   \n220 translate the AdvBench from EN to TH.   \n221 Evaluation. For helpfulness, we use strong LLMs as judges, which show considerable consistency   \n222 with human evaluators in EN (Zheng et al., 2024). LLM-as-a-Judge is efficient, reproducible, and   \n223 cost-effective. However, it is still unknown whether it will work in the TH language. To obtain a   \n224 reliable result, we first invite professional translators to conduct the human evaluation for some strong   \n225 models on the MT-bench. We test the consistency between human and GPT-4 evaluation as described   \n226 in (Zheng et al., 2024). After we prove that GPT-4 achieves acceptable consistency with human   \n227 evaluators, we evaluate all models with it. Both human annotators and LLMs rate the response on a   \n228 scale from 1 to 10, and we further calculate the win, tie, and loss rate based on the scores. We use   \n229 $\\Delta$ to denote the gap between the win and loss rate calculated with the tie. For safety, we translate   \n230 the TH responses into EN and let EN annotators annotate them into Bypass, Reject, and Unclear.   \n231 Bypass means the attack bypasses the safety mechanism of LLMs. Reject means LLMs refuse to   \n232 output harmful information. Unclear means the responses are safe but unclear due to translation or   \n233 hallucination, etc. The setting follows Yong et al. (2023) strictly. Please refer to this paper for details.   \n234 In Appendix A.4, we describe the evaluation procedure, the instructions for human evaluators, and the   \n235 information of evaluators in detail. We also conduct significant tests for main results as described in   \n236 Appendix C. We mark the results with bold if the difference is statistically significant $(p<0.05)$ . ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "table", "img_path": "YZWvf58dBS/tmp/9ef69c7dfeb27b90ba4db31988c5d5fc429d0d7db3e35aac8e553bb0abac2619.jpg", "table_caption": [], "table_footnote": [], "page_idx": 5}, {"type": "table", "img_path": "YZWvf58dBS/tmp/a76216d9f164bf45d9ed2941111b99b28d9fbcf9ae3ea870b5c3a08a87bf4e08.jpg", "table_caption": ["Table 1: Comparison between our model and strong LLMs on MT-bench under human evaluation. We provide the $95\\%$ confidence interval in brackets. ", "Table 2: Agreement between GPT-4 and humans. ${\\bf R}{=}^{,,}$ denotes the expect agreement between random judges. \u2020 EN results are from Zheng et al. (2024). "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "237 4.2 Main Results ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "238 4.2.1 Human Evaluation Results ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "239 Better performance than ChatGPT on MT-Bench. As shown in Table 1, TransLLM surpasses   \n240 ChatGPT by $35\\%$ and $23.75\\%$ for the first and second turn on MT-bench with statistical significance.   \n241 It is an inspiring result although TransLLM is still behind GPT-4 in TH. Because we only use the   \n242 LLaMA-2 with 7B parameters. As the fine-grained scores in Appendix B.1 shown, the two domains   \n243 with the biggest gaps between ours and GPT-4 are Math and Coding, which are also the weaknesses   \n244 of LLaMA-2 in EN. We leave exploring TransLLM on more powerful open-source LLMs in the   \n245 future.   \n246 High agreement between humans and GPT  \n247 4 in TH. Following Zheng et al. (2024), we   \n248 calculate the average agreements by comparing   \n249 every two models. In Table 2, GPT-4 shows   \n250 high consistency with human annotators. The   \n251 consistency (w/ tie) between GPT-4 and humans   \n252 reaches $75.42\\%$ and $70.42\\%$ in the first and sec  \n253 ond turns, which are much higher than random   \n254 guesses and even higher than the consistency in   \n255 EN. Therefore, we use GPT-4 to evaluate the   \n256 helpfulness in the following experiments.   \n257 Higher safety than ChatGPT and GPT-4. In Table 3, TransLLM has a rejection rate of $94.61\\%$ ,   \n258 close to $99.23\\%$ of the original model. It indicates that we successfully transfer most of the human   \n259 preference about the safety of the original model. TransLLM attains an improvement of $14.8\\%$ and   \n260 $8.65\\%$ over ChatGPT and GPT-4 for rejecting harmful queries with statistical significance. More   \n261 importantly, although GPT-4 is as safe as the original LLM in EN, the performance of our w/ GPT-4   \n262 KD is much below our w/ recovery KD. Later, we will demonstrate that this is because recovery KD   \n263 successfully recovers the original knowledge. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "table", "img_path": "YZWvf58dBS/tmp/236795007571f33d0abccc31028c67cc277e8f37014c68ef0295d9a948edfbc8.jpg", "table_caption": [], "table_footnote": ["Table 3: Result for different models on safety benchmark AdvBenchmark under human evaluation. \u2020 GPT-4 results are from Yong et al. (2023). "], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "table", "img_path": "YZWvf58dBS/tmp/771c929cc28f105e37a83ac6fc331b49350cab139a4e6a63587bac2983d4e973.jpg", "table_caption": [], "table_footnote": ["Table 4: Comparison between our model and different methods on MT-Bench under GPT-4 evaluation. "], "page_idx": 6}, {"type": "text", "text": "264 4.2.2 GPT-4 Evaluation Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "65 Better performance than strong baselines. As   \n66 shown in Table 4, TransLLM significantly out  \n7 performs baselines that are built on open-source   \n68 resources. Notably, we specifically build the   \n69 baseline NLLB-bridge which uses the power  \n70 ful translation model NLLB-3B (Costa-juss\u00e0   \n1 et al., 2022) as the bridge between LLaMA-2-   \n72 chat-7B and the TH language. Using the multi  \n73 turn ability of LLaMA-2-chat-7B, NLLB-bridge   \n74 achieves good performance in the second turn. ", "page_idx": 6}, {"type": "table", "img_path": "YZWvf58dBS/tmp/7a980703f5569885b85205ba416bf7f92999b1f4932a43e29103a95f1303b71f.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Table 5: Comparison between our model and different methods on Alpaca-Eval under GPT-4 evaluation. ", "page_idx": 6}, {"type": "text", "text": "75 Although NLLB-bridge uses more parameters and more translation resources, it still loses to   \n76 TransLLM. We will explain in detail why TransLLM is better than translation-as-a-bridge in the   \n7 analysis. Typhoon with TH pre-training achieves sub-optimal second-turn performance among   \n78 baselines. It is probably because the TH documents teach the LLM how to model long context in TH.   \n79 Under GPT-4 evaluation, we slightly outperform ChatGPT without statistical significance. It seems   \n80 difficult for GPT-4 to compare two strong LLMs on small datasets in TH. We select the baselines that   \n81 perform well on the first turn of the MT-bench, for further evaluation on Alpaca-Eval. On the larger   \n82 dataset, TransLLM outperforms baselines and ChatGPT by a large margin with statistical significance   \n83 as shown in Table 5. ", "page_idx": 6}, {"type": "text", "text": "284 5 Analysis ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "285 5.1 All Components Work Together ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "286 We conduct comprehensive ablation studies   \nvs. Model 1st \u2206(%) 2nd \u2206(%   \n287 on MT-Bench to investigate the impact of   \n288 TransLLM\u2019s components and present results w/o chat model 36.25 67.50   \n289 in Table 6. The results confirm our hypothe- w/o wtr/aon sTlHat iporne -ptrraei-ntrain 481..7255 3253..0705   \n290 sis that transforming chat LLMs could provide w/ GPT-4 KD 17.50 45.00   \n291 better conversational ability than base LLMs. w/o LoRA 62.50 66.25   \n292 Pre-training on TH documents helps TransLLM w/ TH history 23.75   \n293 output fluency in TH response with long con  \n294 text. Thus, TransLLM without TH pre-training Table 6: Comparison between our model and ab   \n295 is less satisfying on both the first and second tion models.   \n296 turn. Since TH pre-training and transfer fine  \n297 tuning also provide some translation knowledge, the improvement of the translation pre-training   \n298 not as significant as other components. Beyond safety, the high-quality GPT-4 KD data also leads   \n299 performance degradation for helpfulness. That is because our goal is not to inject more knowled   \n300 but to preserve the original knowledge. We also examine the contribution of LoRA. Specifica   \n301 we merge the LoRA parameters with full parameters before transfer fine-tuning. We are unable   \n302 conduct full fine-tuning for per-training, but the merged model is a good approximation according   \n303 Eq. 3. We further conduct transfer fine-tuning with full parameters based on the merged model.   \n304 most tasks, full fine-tuning is better or comparable with LoRA. However, in our case, full fine-tuni   \n305 wipes the original knowledge from parameters, and therefore its performance is much lower th   \n306 TransLLM with LoRA. When using the history in TH, TransLLM is also capable of multi-tu   \n307 conversation with small performance degradation. That means TransLLM can handle TH conte   \n308 well, this ability could be further developed for retrieval augmentation in TH. ", "page_idx": 7}, {"type": "text", "text": "309 5.2 TransLLM Recover the Original Knowledge ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "310 Knowledge is forgotten and recovered. To   \n311 measure how much original knowledge is forgot  \n312 ten by the chat LLM, we calculate the generation   \n313 probabilities on the hold-out validation set of re  \n314 covery KD data in EN. We also calculate the   \n315 average difference between the generation prob  \n316 abilities of the target LLM and different models.   \n317 As shown in Table 7, after pre-training, which   \n318 has been proven to be necessary, the LLM sig", "page_idx": 7}, {"type": "table", "img_path": "YZWvf58dBS/tmp/e6dd9a447563237f9ced3ec8afbfc4283852d84b33c6ae9bc440116971b471b4.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Table 7: The difference of generation probabilities. ", "page_idx": 7}, {"type": "text", "text": "319 nificantly forgets the conversation knowledge. GPT-4 KD, which is widely used in previous works,   \n320 can provide high-quality knowledge. However, this kind of knowledge is quite different from and   \n321 competes with the original knowledge. As a result, the LLM still forgets much original knowledge   \n322 using GPT-4 KD. Meanwhile, TransLLM successfully recovers the original knowledge.   \n323 LoRA also helps. LoRA keeps the original parameters unchanged. The LLM can fit the recovery   \n324 KD data easily when using knowledge from these frozen parameters. This easy pattern is a \u201cshortcut\u201d   \n325 that prompts the LLM to learn to use the original knowledge for EN and new knowledge for TH. To   \n326 confirm this assumption, on the TCOT validation data, we calculate the cosine similarity between   \n327 the last layer\u2019s hidden states of the original model $\\tilde{h}$ and LoRA updated model \u02c6h as defined in Eq. 3.   \n328 The average similarity per token for EN responses is much larger than that for TH responses, 0.6191   \n329 vs. 0.2522. That means TransLLM successfully learns the \u201cshortcut\u201d using LoRA and recovery KD   \n330 together. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "331 5.3 Why TransLLM is better than translation-as-a-bridge? ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "332 Comparable translation performance. The   \n333 translation performance is critical for both   \n334 TransLLM and translation-as-a-bridge. There  \n335 fore, we test them on the widely used benchmark   \n336 Flores-200 (Goyal et al., 2022). As shown in   \n337 Table 8, beneftiing from translation and TH pre  \n338 training, TransLLM outperforms ChatGPT and   \n339 NLLB on EN-TH and achieves competitive per  \n340 formance on TH-EN. We also ask the naive TH speaker to provide a fluency score for each model   \n341 on MT-Bench in Table 9. The fluency of NLLB is as poor as its translation performance on EN-TH.   \n342 NLLB usually translates the responses literally. For example, NLLB translates \u201cI see\u201d into \u201cI see   \n343 something\u201d instead of \u201cI understand\u201d in TH. Surprisingly, the response of GPT-4 is not very fluent and   \n344 natural. GPT-4 often uses full-stops and commas which are not used in TH. ChatGPT and TransLLM   \n345 are generally fluent, with translationese to a certain degree. For example, TH speakers do not use   \n346 \u201csure\u201d or \u201cof course\u201d at the beginning of responses, but ChatGPT and TransLLM do.   \n347 TransLLM is more than translation. Translation perfor  \n348 mance is important but not the whole story. TransLLM outputs   \n349 an EN query, EN response, and TH response at once. It means   \n350 that TransLLM can use all previous information for TH re  \n351 sponses and therefore achieve better performance. To verify   \n352 it, we use TransLLM to translate its EN responses in another   \n353 round of inference. The performance is worse than the stan  \n354 dard response with $\\Delta=13.75\\%$ and $\\Delta=18.75\\%$   \n355 second turn. The attention map of TransLLM in Appendix B.2 ", "page_idx": 7}, {"type": "table", "img_path": "YZWvf58dBS/tmp/0911ff96d174b241d2f7f5b361636ce651a908e66b96ca905d29b7bbc2932f2d.jpg", "table_caption": [], "table_footnote": ["Table 8: Translation performance on Flores-200. "], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "table", "img_path": "YZWvf58dBS/tmp/b2c8ad363da3da44be0cae24f7e28d2ff54ecd7302bca0670d348d901a1c2116.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "on first and Table 9: Fluency on MT-Bench. ", "page_idx": 8}, {"type": "text", "text": "356 shows that TransLLM outputs the TH response mostly based on the TH response itself and then the   \n357 EN response. However, the TH response also pays a little attention on the TH query and EN query.   \n358 Besides, translation-as-a-bridge needs to deploy two models, which is costly and inconvenient. ", "page_idx": 8}, {"type": "text", "text": "359 6 Related Works ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "360 Recently, there have been many works that attempt to transfer knowledge from English to non-English   \n361 for LLMs. For example, Chinese LLaMA (Cui et al., 2023) and Typhoon(Pipatanakul et al., 2023)   \n362 directly perform continuous pre-training and instruct tuning with extended vocabulary using LoRA.   \n363 PloyLM (Wei et al., 2023) adopts multilingual pre-training based on the curriculum learning strategy   \n364 that gradually exposes more low-resource corpus. ChatGPT (OpenAI, 2022) and GPT-4 (OpenAI,   \n365 2023) are also well-known multilingual LLMs. Zhu et al. (2023) focus on building semantic alignment   \n366 with cross-lingual instruct tuning and translation training. Bansal et al. (2024) augment LLMs by   \n367 combining the English-dominated LLM with the non-English model. Some other works focus on   \n368 transfer reasoning abilities: Qin et al. (2023) introduce cross-lingual prompting to improve zero-shot   \n369 chain-of-thought reasoning across languages; She et al. (2024) propose multilingual alignment-as  \n370 preference optimization to align reasoning abilities across languages. PLUG (Zhang et al., 2023) only   \n371 uses the TCOT data to train the base LLMs directly. Different from PLUG, we propose a systematic   \n372 framework for transforming chat LLMs. We highlight that the TCOT highly relies on the performance   \n373 of its sub-tasks and introduce how to preserve the knowledge of the chat LLM. ", "page_idx": 8}, {"type": "text", "text": "374 7 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "375 Chat LLMs have been specifically optimized for chat usage and therefore are helpful and safe in the   \n376 dominant language. In this paper, we propose a framework for transforming an off-the-shelf chat   \n377 LLM to other languages. In this framework, we utilize TCOT to transfer chat knowledge and further   \n378 enhance the TCOT\u2019s sub-tasks using publicly available data. To recover the original knowledge, we   \n379 propose the recovery KD method supplemented with LoRA. The experiments in TH show that we   \n380 transfer desired abilities to TH and outperform ChatGPT in both helpfulness and safety. Overall, we   \n381 hope that this work can become the foundation for developing safe LLMs in many languages other   \n382 than English.   \n383 Limitations and future works. Due to limited resources, we only conduct experiments that   \n384 transform LLaMA-2-chat-7B to TH. However, we conduct comprehensive experiments and in-depth   \n385 analysis to reveal the mechanism of the proposed TransLLM. For now, TransLLM is still highly   \n386 dependent on translation. Consequently, TransLLM can not handle the queries related to TH text, e.g.   \n387 word games in TH. A simple solution is to enable TransLLM, through training, to choose whether   \n388 respond to with TH mode or TCOT mode. Due to the TCOT, the inference overhead of TransLLM is   \n389 much longer than other baselines. Recently, Goyal et al. (2023) and Deng et al. (2023) show that   \n390 the implicit chain-of-thought achieves similar performance on reasoning tasks without additional   \n391 inference overhead. We would like to explore TransLLM with implicit TCOT in the future. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "392 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "393 Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos,   \n394 Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. 2023. Palm 2 technical report.   \n395 arXiv preprint arXiv:2305.10403.   \n396 Rachit Bansal, Bidisha Samanta, Siddharth Dalmia, Nitish Gupta, Shikhar Vashishth, Sriram Ganapa  \n397 thy, Abhishek Bapna, Prateek Jain, and Partha Talukdar. 2024. LLM augmented llms: Expanding   \n398 capabilities through composition. CoRR, abs/2401.02412. Version 1.   \n399 Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,   \n400 Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models   \n401 are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901.   \n402 Vishrav Chaudhary, Yuqing Tang, Francisco Guzm\u00c3\u00a1n, Holger Schwenk, and Philipp Koehn. 2019.   \n403 Low-resource corpus filtering using multilingual sentence embeddings. In Proceedings of the   \n404 Fourth Conference on Machine Translation (Volume 3: Shared Task Papers, Day 2), pages 263\u2013268,   \n405 Florence, Italy. Association for Computational Linguistics.   \n406 Marta R Costa-juss\u00e0, James Cross, Onur \u00c7elebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan,   \n407 Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, et al. 2022. No language left behind:   \n408 Scaling human-centered machine translation. arXiv preprint arXiv:2207.04672.   \n409 Yiming Cui, Ziqing Yang, and Xin Yao. 2023. Efficient and effective text encoding for chinese llama   \n410 and alpaca. arXiv preprint arXiv:2304.08177.   \n411 Yuntian Deng, Kiran Prasad, Roland Fernandez, Paul Smolensky, Vishrav Chaudhary, and Stuart   \n412 Shieber. 2023. Implicit chain of thought reasoning via knowledge distillation. arXiv preprint   \n413 arXiv:2311.01460.   \n414 Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang,   \n415 Horace He, Anish Thite, Noa Nabeshima, et al. 2020. The pile: An 800gb dataset of diverse text   \n416 for language modeling. arXiv preprint arXiv:2101.00027.   \n417 Naman Goyal, Cynthia Gao, Vishrav Chaudhary, Peng-Jen Chen, Guillaume Wenzek, Da Ju, Sanjana   \n418 Krishnan, Marc\u2019Aurelio Ranzato, Francisco Guzm\u00e1n, and Angela Fan. 2022. The flores-101   \n419 evaluation benchmark for low-resource and multilingual machine translation. Transactions of the   \n420 Association for Computational Linguistics, 10:522\u2013538.   \n421 Sachin Goyal, Ziwei Ji, Ankit Singh Rawat, Aditya Krishna Menon, Sanjiv Kumar, and Vaishnavh   \n422 Nagarajan. 2023. Think before you speak: Training language models with pause tokens. In The   \n423 Twelfth International Conference on Learning Representations.   \n424 Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,   \n425 and Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models. arXiv preprint   \n426 arXiv:2106.09685.   \n427 Taku Kudo and John Richardson. 2018. SentencePiece: A simple and language independent subword   \n428 tokenizer and detokenizer for neural text processing. In Proceedings of the 2018 Conference   \n429 on Empirical Methods in Natural Language Processing: System Demonstrations, pages 66\u201371,   \n430 Brussels, Belgium. Association for Computational Linguistics.   \n431 Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang,   \n432 and Tatsunori B. Hashimoto. 2023. Alpacaeval: An automatic evaluator of instruction-following   \n433 models. https://github.com/tatsu-lab/alpaca_eval.   \n434 Pierre Lison, J\u00f6rg Tiedemann, and Milen Kouylekov. 2018. OpenSubtitles2018: Statistical rescoring   \n435 of sentence alignments in large, noisy parallel corpora. In Proceedings of the Eleventh International   \n436 Conference on Language Resources and Evaluation (LREC 2018), Miyazaki, Japan. European   \n437 Language Resources Association (ELRA).   \n438 Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis,   \n439 and Luke Zettlemoyer. 2020. Multilingual denoising pre-training for neural machine translation.   \n440 Transactions of the Association for Computational Linguistics, 8:726\u2013742. ", "page_idx": 9}, {"type": "text", "text": "441 OpenAI. 2022. Introducing chatgpt. Blog post https://www.openai.com/blog/chatgpt. ", "page_idx": 10}, {"type": "text", "text": "442 OpenAI. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774. ", "page_idx": 10}, {"type": "text", "text": "443 Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong   \n444 Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to   \n445 follow instructions with human feedback. Advances in neural information processing systems,   \n446 35:27730\u201327744.   \n447 Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic   \n448 evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association   \n449 for Computational Linguistics, pages 311\u2013318.   \n450 Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. 2023. Instruction   \n451 tuning with gpt-4. arXiv preprint arXiv:2304.03277.   \n452 Kunat Pipatanakul, Phatrasek Jirabovonvisut, Potsawee Manakul, Sittipong Sripaisarnmongkol,   \n453 Ruangsak Patomwong, Pathomporn Chokchainant, and Kasima Tharnpipitchai. 2023. Typhoon:   \n454 Thai large language models. arXiv preprint arXiv:2312.13951.   \n455 Libo Qin, Qiguang Chen, Fuxuan Wei, Shijue Huang, and Wanxiang Che. 2023. Cross-lingual   \n456 prompting: Improving zero-shot chain-of-thought reasoning across languages. In Proceedings of   \n457 the 2023 Conference on Empirical Methods in Natural Language Processing, pages 2695\u20132709.   \n458 Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019.   \n459 Language models are unsupervised multitask learners. OpenAI blog, 1(8):9.   \n460 Ricardo Rei, Jos\u00e9 GC De Souza, Duarte Alves, Chrysoula Zerva, Ana C Farinha, Taisiya Glushkova,   \n461 Alon Lavie, Luisa Coheur, and Andr\u00e9 FT Martins. 2022. Comet-22: Unbabel-ist 2022 submission   \n462 for the metrics shared task. In Proceedings of the Seventh Conference on Machine Translation   \n463 (WMT), pages 578\u2013585.   \n464 Shuaijie She, Shujian Huang, Wei Zou, Wenhao Zhu, Xiang Liu, Xiang Geng, and Jiajun Chen.   \n465 2024. Mapo: Advancing multilingual reasoning through multilingual alignment-as-preference   \n466 optimization. arXiv preprint arXiv:2401.06838.   \n467 Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy   \n468 Liang, and Tatsunori B Hashimoto. 2023. Stanford alpaca: an instruction-following llama model   \n469 (2023). URL https://github. com/tatsu-lab/stanford_alpaca.   \n470 J\u00f6rg Tiedemann. 2020. The Tatoeba Translation Challenge \u2013 Realistic data sets for low resource   \n471 and multilingual MT. In Proceedings of the Fifth Conference on Machine Translation, pages   \n472 1174\u20131182, Online. Association for Computational Linguistics.   \n473 Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay   \n474 Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open   \n475 foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288.   \n476 Changhan Wang, Kyunghyun Cho, and Jiatao Gu. 2020. Neural machine translation with byte-level   \n477 subwords. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages   \n478 9154\u20139160.   \n479 Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi,   \n480 and Hannaneh Hajishirzi. 2022. Self-instruct: Aligning language models with self-generated   \n481 instructions. arXiv preprint arXiv:2212.10560.   \n482 Xiangpeng Wei, Haoran Wei, Huan Lin, Tianhao Li, Pei Zhang, Xingzhang Ren, Mei Li, Yu Wan,   \n483 Zhiwei Cao, Binbin Xie, et al. 2023. Polylm: An open source polyglot large language model.   \n484 arXiv preprint arXiv:2307.06018.   \n485 Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi,   \n486 Pierric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick   \n487 von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger,   \n488 Mariama Drame, Quentin Lhoest, and Alexander M. Rush. 2020. Transformers: State-of-the-art   \n489 natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in   \n490 Natural Language Processing: System Demonstrations, pages 38\u201345, Online. Association for   \n491 Computational Linguistics.   \n492 Haoran Xu, Young Jin Kim, Amr Sharaf, and Hany Hassan Awadalla. 2023. A paradigm shift in   \n493 machine translation: Boosting translation performance of large language models. arXiv preprint   \n494 arXiv:2309.11674.   \n495 Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya   \n496 Barua, and Colin Raffel. 2020. mt5: A massively multilingual pre-trained text-to-text transformer.   \n497 arXiv preprint arXiv:2010.11934.   \n498 Zheng-Xin Yong, Cristina Menghini, and Stephen H Bach. 2023. Low-resource languages jailbreak   \n499 gpt-4. arXiv preprint arXiv:2310.02446.   \n500 Zhihan Zhang, Dong-Ho Lee, Yuwei Fang, Wenhao Yu, Mengzhao Jia, Meng Jiang, and Francesco   \n501 Barbieri. 2023. Plug: Leveraging pivot language in cross-lingual instruction tuning. arXiv preprint   \n502 arXiv:2311.08711.   \n503 Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,   \n504 Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. 2024. Judging llm-as-a-judge with mt-bench   \n505 and chatbot arena. Advances in Neural Information Processing Systems, 36.   \n506 Zaixiang Zheng, Hao Zhou, Shujian Huang, Lei Li, Xin-Yu Dai, and Jiajun Chen. 2019. Mirror  \n507 generative neural machine translation. In International Conference on Learning Representations.   \n508 Wenhao Zhu, Yunzhe Lv, Qingxiu Dong, Fei Yuan, Jingjing Xu, Shujian Huang, Lingpeng Kong,   \n509 Jiajun Chen, and Lei Li. 2023. Extrapolating large language models to non-english by aligning   \n510 languages. arXiv preprint arXiv:2308.04948. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "511 A Experiment Details ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "512 A.1 Models ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "513 We list backbone, training data, and model size in Table 10. Due to the huge consumption of   \n514 multilingual (MTL) pre-training, we directly use the model PolyLM-MultiAlpaca-13B released   \n515 in Wei et al. (2023) for PolyLM. PolyLM uses ChatGPT to generate the Alpaca data while other   \n516 baselines use the Alpaca data generated by GPT-4. We use the gpt-3.5-turbo-0125 and gpt-4-0613 for   \n517 ChatGPT and GPT-4 in all experiments (including evaluation) through OpenAI API. We re-implement   \n518 other baselines by strictly following their papers and using the same data as our model. To reduce the   \n519 impact of randomness, we use greedy search for all experiments. We set the temperature as 0 for   \n520 ChatGPT and GPT-4 through API to approximate the greedy search.   \n521 Please refer to Touvron et al. (2023) for model structures of LLaMA-2. We only list the LoRA   \n522 parameters here. We set the rank to 64, alpha to 128, and dropout to 0.05 for LoRA. These parameters   \n523 are applied to the q_proj, v_proj, $k_{\\cdot}$ _proj, o_proj, gate_proj, down_proj, and up_proj modules of the   \n524 original model. Besides, the embed_tokens and lm_head are also trainable. ", "page_idx": 11}, {"type": "table", "img_path": "YZWvf58dBS/tmp/1ac0f0967d6247d193fde0d825ece7f5253fdee0ee7e093e10d6106f1f4a49e3.jpg", "table_caption": ["Table 10: Model details. "], "table_footnote": [], "page_idx": 11}, {"type": "text", "text": "525 A.2 Training ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "526 We train the TransLLM model on 8 A100 GPUs as follows. ", "page_idx": 12}, {"type": "text", "text": "527 TH Pre-Training We train the TransLLM using a warm-up ratio of 0.0005, a maximum sequence   \n528 length of 512 tokens, and a weight decay of 0.01. The training was conducted with each GPU   \n529 managing 128 batches and utilizing a gradient accumulation step of 1. The peak learning rate is set at   \n530 2e-4 with a cosine learning rate decay (max_epoch $\\scriptstyle=100$ ), and training operated under bf16 precision   \n531 facilitated by deepspeed, employing ZeRO stage 2.   \n532 We only run 1 epoch for this stage, which spends $168\\times8$ GPU hours. As shown in Figure 3, the   \n533 initial training loss is approximately 7.8, which converges to below 1.7 after around 0.1 epochs of   \n534 training. The final loss reaches around 1.42.   \n535 Translation Pre-Training According to the data size, we set the warm-up ratio as 0.05, the   \n536 max_epoch $=10$ for the cosine learning rate decay. We use $0.1\\%$ examples as the validation set and   \n537 calculate valid loss every 400 steps. The best model has been trained for about 3 epochs, which   \n538 spends $40\\times8$ GPU hours. The remaining configurations remain consistent with the first stage.   \n539 Transfer Fine-Tuning Our max_seq_length is set to 2048 for fine-tuning, and when batching data,   \n540 we pad sentences with \u201c<PAD>\u201d tokens. The peak learning rate is set to 1e-4, the warmup ratio is   \n541 set to 0.01, and the single-card batch size is set to 16 with gradient accumulation steps as 4. We set   \n542 weight decay as 0. We use 2K examples as the validation set and calculate valid loss every 200 steps.   \n543 The best model has been trained for about 1 epoch, which spends $6\\times8$ GPU hours. The remaining   \n544 configurations remain consistent with the first stage. ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "image", "img_path": "YZWvf58dBS/tmp/3924e30b8d0b304c39b0ffc0b71d6377bb18af11a5d01a89d704e4876b0fc0ee.jpg", "img_caption": ["Figure 3: TH Pre-Training loss. "], "img_footnote": [], "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "545 A.3 Inference ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "546 We provide the whole multi-turn prompt in Table 11, where $^{\\cdot\\cdot}<\\!s\\!><\\!/s\\!>^{\\,\\ast}$ , $\\mathrm{^{\\leftrightarrow}<<S Y S>>}\\,<<\\!S Y S\\!>\\!>^{\\!*}$ , and   \n547 \u201c[INST] [\\INST]\u201d denote the whole instance, system prompt, and instruction respectively. ", "page_idx": 12}, {"type": "image", "img_path": "YZWvf58dBS/tmp/94e92bf94bd5421708dfe08b212e115e4b066729e9417253e481fa422710655f.jpg", "img_caption": ["Table 11: The multi-turn prompt template used in our experiments. "], "img_footnote": [], "page_idx": 12}, {"type": "text", "text": "549 A.4.1 Human Evaluation ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "550 For helpfulness, the results are evaluated by three annotators. Annotator A is a professional translator   \n551 expert in EN and TH. Annotator B is a computer engineer who is an expert in EN, Math, Coding,   \n552 and Extraction. Annotator C is a native TH speaker while also an expert in EN. The three annotators   \n553 cooperate with each other to complete the whole evaluation process as follows. Annotator A is the   \n554 major annotator who is responsible for annotating most of the queries except for the Math, Coding,   \n555 and Extraction domains. For these three domains, annotator A first translates the results from TH to   \n556 EN. Annotator B then annotates these three domains in EN translations. Meanwhile, Annotator C   \n557 helps annotator A evaluate the fluency of all responses. To obtain consistent annotations between   \n558 evaluators and questions, we define comprehensive instructions for annotators in Table 12.   \n559 For safety, the responses are first translated from TH to EN and then evaluated by three professional   \n560 translators who are experts in EN. However, one response is only annotated by one translator due to a   \n561 limited budget. Please refer to the annotation instruction in Yong et al. (2023). ", "page_idx": 13}, {"type": "table", "img_path": "YZWvf58dBS/tmp/627e36405c7c743d6187913639deccbc84e25006435b060ca03459a1695b96fe.jpg", "table_caption": ["Table 12: Rating criterion. "], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "562 All models are anonymous to all annotators in the whole evaluation process! ", "page_idx": 13}, {"type": "text", "text": "563 A.4.2 Automatic Evaluation ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "564 We follow the setting of LLM-as-a-Judge in Zheng et al. (2024). For Reasoning, Math, and Coding   \n565 domains, we provide the EN responses of GPT-4 as references. Note that, these three domains are   \n566 different from human evaluation because annotator A is good at Reasoning instead of Extraction.   \n567 We modify the evaluation prompts provided in Zheng et al. (2024) to inform GPT-4 that the queries   \n568 and responses are in TH. Please refer to Zheng et al. (2024) for the details of how to calculate the   \n569 agreement.   \n570 We use the default wmt22-comet-da model 5 for COMET (Rei et al., 2022). We use   \n571 the BLEU (Papineni et al., 2002) implemented in the scarebleu6, whose signature is   \n572 \"BLEU|nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.4.0\". ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "573 A.5 Licenses ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "574 Our experiments use open-source resources. We list their licenses in Table 13. We have properly   \n575 cited their papers and strictly followed their licenses. ", "page_idx": 13}, {"type": "text", "text": "576 B Other Results ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "577 B.1 Results in Scores ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "578 We provide evaluation scores on different benchmarks in Table 14, 15, 16 , and 17. ", "page_idx": 13}, {"type": "table", "img_path": "YZWvf58dBS/tmp/bebd1d34a6743a34219ebab8e76319131dcfd2534d350d89c423662441098074.jpg", "table_caption": [], "table_footnote": [], "page_idx": 14}, {"type": "table", "img_path": "YZWvf58dBS/tmp/14a7c7676d6220ea71403903837714f849ce86ec71bfd75f10d014942cb622a3.jpg", "table_caption": ["Table 13: Licenses of open source resources. ", "Table 14: Human evaluation scores on MT-Bench for different models. "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "579 B.2 Attention Map of the TransLLM Output ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "580 As shown in Figure 4, the TH response focuses on the TH response, EN response, EN query, and TH   \n581 query, in order from high to low. ", "page_idx": 14}, {"type": "table", "img_path": "YZWvf58dBS/tmp/004329f2a1573e5fd3f11e77ac7b10ff31ce888842374127b05bcfdc177a6b37.jpg", "table_caption": ["Table 15: GPT-4 evaluation scores on MT-Bench for different models. "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "582 C Statistical Methods ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "583 C.1 Confidence Interval ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "584 We first calculate the standard deviation for proportion $p$ on $n$ examples as: ", "page_idx": 14}, {"type": "equation", "text": "$$\ns_{p}={\\sqrt{\\frac{p(1-p)}{n}}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "image", "img_path": "YZWvf58dBS/tmp/151716e23d8a5b60e7a0e30a2e01ccd1a4dc41a2fdd717eb81f9182a441bc170.jpg", "img_caption": ["Figure 4: Attention map of the TransLLM output. We mark the attention scores of TH responses with red rectangles. Rectangles from top to bottom indicate attention scores of TH response for TH query, EN query, EN response, and TH response respectively. "], "img_footnote": [], "page_idx": 15}, {"type": "table", "img_path": "YZWvf58dBS/tmp/b0c8b7a179634f929fe4f41c6324a5ceab8508238ee17d32b303369da0410174.jpg", "table_caption": ["Table 16: GPT-4 evaluation scores on Alpaca-Eval for different models. "], "table_footnote": [], "page_idx": 15}, {"type": "table", "img_path": "YZWvf58dBS/tmp/bc5ffece0e729960cbfcda7a80d0469203a937f118a5ab97789d9beeff5dff89.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "Table 17: GPT-4 evaluation scores for ablation studies on MT-bench. ", "page_idx": 16}, {"type": "table", "img_path": "YZWvf58dBS/tmp/26c12ca63b598bfa7c1850d8bf0c78e2ae295b0e271bee43d223ef2a0e805c20.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "table", "img_path": "YZWvf58dBS/tmp/c55335c174003b46926c9cf1d4ca7c1081505a117737903602922e3f9451fdad.jpg", "table_caption": ["Table 18: Binomial test for Table 1. "], "table_footnote": ["Table 19: Binomial test for Table 4. "], "page_idx": 16}, {"type": "text", "text": "585 Then we use the normal approximation method to calculate the CI for ratio $p$ as ", "page_idx": 16}, {"type": "equation", "text": "$$\n(p-u s_{p},\\;\\;p+u s_{p}),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "586 where $u$ denote the critical value, for the two-tailed $95\\%$ confidence interval used in this paper   \n587 $u=1.96$ . ", "page_idx": 16}, {"type": "text", "text": "588 C.2 Significant Test ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "589 We conduct a two-sided binomial test for the win rate without tie $p_{\\mathrm{win}}=n_{\\mathrm{win}}/(n_{\\mathrm{win}}+n_{\\mathrm{loss}})$ . The   \n590 null hypothesis is that the win rate is not different from the loss rate, i.e. $H_{0}:p_{\\mathrm{win}}=p_{\\mathrm{loss}}=0.5$ ,   \n591 alternative hypothesis $H_{1}:p_{\\mathrm{win}}\\neq0.5$ . For the test results of Table 1 and 4, please see Table 18 and   \n592 19. The difference between TransLLM and others in Table 5 are all significant with $p<0.001$ .   \n593 We conduct the $\\chi^{2}$ test for safety results in Table 3, the difference between TransLLM and others are   \n594 all significant with $p<0.001$ . ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "595 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "598 paper\u2019s contributions and scope?   \n599 Answer: [Yes]   \n600 Justification: We have discussed our contributions and scope in detail in the Abstract and   \n601 Introduction chapters.   \n602 Guidelines:   \n603 \u2022 The answer NA means that the abstract and introduction do not include the claims   \n604 made in the paper.   \n605 \u2022 The abstract and/or introduction should clearly state the claims made, including the   \n606 contributions made in the paper and important assumptions and limitations. A No or   \n607 NA answer to this question will not be perceived well by the reviewers.   \n608 \u2022 The claims made should match theoretical and experimental results, and reflect how   \n609 much the results can be expected to generalize to other settings.   \n610 \u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals   \n611 are not attained by the paper. ", "page_idx": 17}, {"type": "text", "text": "612 2. Limitations ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Guidelines:   \n17 \u2022 The answer NA means that the paper has no limitation while the answer No means that   \n18 the paper has limitations, but those are not discussed in the paper.   \n19 \u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n20 \u2022 The paper should point out any strong assumptions and how robust the results are to   \n21 violations of these assumptions (e.g., independence assumptions, noiseless settings,   \n22 model well-specification, asymptotic approximations only holding locally). The authors   \n23 should reflect on how these assumptions might be violated in practice and what the   \n24 implications would be.   \n25 \u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was   \n26 only tested on a few datasets or with a few runs. In general, empirical results often   \n27 depend on implicit assumptions, which should be articulated.   \n28 \u2022 The authors should reflect on the factors that influence the performance of the approach.   \n29 For example, a facial recognition algorithm may perform poorly when image resolution   \n30 is low or images are taken in low lighting. Or a speech-to-text system might not be   \n31 used reliably to provide closed captions for online lectures because it fails to handle   \n32 technical jargon.   \n33 \u2022 The authors should discuss the computational efficiency of the proposed algorithms   \n34 and how they scale with dataset size.   \n35 \u2022 If applicable, the authors should discuss possible limitations of their approach to   \n36 address problems of privacy and fairness.   \n37 \u2022 While the authors might fear that complete honesty about limitations might be used by   \n38 reviewers as grounds for rejection, a worse outcome might be that reviewers discover   \n39 limitations that aren\u2019t acknowledged in the paper. The authors should use their best   \n40 judgment and recognize that individual actions in favor of transparency play an impor  \n41 tant role in developing norms that preserve the integrity of the community. Reviewers   \n42 will be specifically instructed to not penalize honesty concerning limitations.   \n3. Theory Assumptions and Proofs ", "page_idx": 17}, {"type": "text", "text": "643 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "644 Question: For each theoretical result, does the paper provide the full set of assumptions and   \n645 a complete (and correct) proof? ", "page_idx": 17}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 17}, {"type": "text", "text": "Justification: This paper does not include theoretical results. Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 18}, {"type": "text", "text": "659 4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: We have tried our best to provide the details of our experiments in Sec. 4.1 and Appendix A. We also provided our code and datasets in supplementary materials. Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 18}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 18}, {"type": "text", "text": "2 Answer: [Yes]   \n3 Justification: We have provided open access to the data and code, with sufficient instructions   \n04 provided to faithfully reproduce the main experimental results. Please refer the README   \n5 file in code.   \n6 Guidelines:   \n7 \u2022 The answer NA means that paper does not include experiments requiring code.   \n08 \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/   \n09 public/guides/CodeSubmissionPolicy) for more details.   \n0 \u2022 While we encourage the release of code and data, we understand that this might not be   \npossible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not   \n2 including code, unless this is central to the contribution (e.g., for a new open-source   \n3 benchmark).   \n14 \u2022 The instructions should contain the exact command and environment needed to run to   \n15 reproduce the results. See the NeurIPS code and data submission guidelines (https:   \n6 //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n7 \u2022 The authors should provide instructions on data access and preparation, including how   \n18 to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n19 \u2022 The authors should provide scripts to reproduce all experimental results for the new   \n20 proposed method and baselines. If only a subset of experiments are reproducible, they   \nshould state which ones are omitted from the script and why.   \n2 \u2022 At submission time, to preserve anonymity, the authors should release anonymized   \n23 versions (if applicable).   \n24 \u2022 Providing as much information as possible in supplemental material (appended to the   \n5 paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 19}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 19}, {"type": "text", "text": "Justification: We have tried our best to provide the details of our experiments in Sec. 4.1 and Appendix A. ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 19}, {"type": "text", "text": "739 7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "740 Question: Does the paper report error bars suitably and correctly defined or other appropriate   \n741 information about the statistical significance of the experiments? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "743 Justification: Please refer Appendix C.   \n744 Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) ", "page_idx": 19}, {"type": "text", "text": "754 \u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n755 \u2022 It should be clear whether the error bar is the standard deviation or the standard error   \n756 of the mean.   \n757 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should   \n758 preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis   \n759 of Normality of errors is not verified.   \n760 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or   \n761 figures symmetric error bars that would yield results that are out of range (e.g. negative   \n762 error rates).   \n763 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how   \n764 they were calculated and reference the corresponding figures or tables in the text.   \n765 8. Experiments Compute Resources   \n766 Question: For each experiment, does the paper provide sufficient information on the com  \n767 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n768 the experiments?   \n769 Answer: [Yes]   \n770 Justification: Please refer Appendix A.2.   \n771 Guidelines:   \n772 \u2022 The answer NA means that the paper does not include experiments.   \n773 \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,   \n774 or cloud provider, including relevant memory and storage.   \n775 \u2022 The paper should provide the amount of compute required for each of the individual   \n776 experimental runs as well as estimate the total compute.   \n777 \u2022 The paper should disclose whether the full research project required more compute   \n778 than the experiments reported in the paper (e.g., preliminary or failed experiments that   \n779 didn\u2019t make it into the paper).   \n780 9. Code Of Ethics   \n781 Question: Does the research conducted in the paper conform, in every respect, with the   \n782 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n783 Answer: [Yes]   \n784 Justification: We have carefully checked our paper. Our paper conforms, in every respect,   \n785 with the NeurIPS Code of Ethics.   \n786 Guidelines:   \n787 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n788 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n789 deviation from the Code of Ethics.   \n790 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid  \n791 eration due to laws or regulations in their jurisdiction).   \n792 10. Broader Impacts   \n793 Question: Does the paper discuss both potential positive societal impacts and negative   \n794 societal impacts of the work performed?   \n795 Answer: [Yes]   \n796 Justification: We have discussed the potential positive societal impacts in Sec. 7, and it   \n797 seems that this work does not exert obviously negative societal impacts.   \n798 Guidelines:   \n799 \u2022 The answer NA means that there is no societal impact of the work performed.   \n800 \u2022 If the authors answer NA or No, they should explain why their work has no societal   \n801 impact or why the paper does not address societal impact.   \n802 \u2022 Examples of negative societal impacts include potential malicious or unintended uses   \n803 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n804 (e.g., deployment of technologies that could make decisions that unfairly impact specific   \n805 groups), privacy considerations, and security considerations.   \n806 \u2022 The conference expects that many papers will be foundational research and not tied   \n807 to particular applications, let alone deployments. However, if there is a direct path to   \n808 any negative applications, the authors should point it out. For example, it is legitimate   \n809 to point out that an improvement in the quality of generative models could be used to   \n810 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n811 that a generic algorithm for optimizing neural networks could enable people to train   \n812 models that generate Deepfakes faster.   \n813 \u2022 The authors should consider possible harms that could arise when the technology is   \n814 being used as intended and functioning correctly, harms that could arise when the   \n815 technology is being used as intended but gives incorrect results, and harms following   \n816 from (intentional or unintentional) misuse of the technology.   \n817 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n818 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n819 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n820 feedback over time, improving the efficiency and accessibility of ML).   \n821 11. Safeguards   \n822 Question: Does the paper describe safeguards that have been put in place for responsible   \n823 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n824 image generators, or scraped datasets)?   \n825 Answer: [Yes]   \n826 Justification: Our work aims to transfer the safeguards of chat large language models from   \n827 English to non-English.   \n828 Guidelines:   \n829 \u2022 The answer NA means that the paper poses no such risks.   \n830 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n831 necessary safeguards to allow for controlled use of the model, for example by requiring   \n832 that users adhere to usage guidelines or restrictions to access the model or implementing   \n833 safety filters.   \n834 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n835 should describe how they avoided releasing unsafe images.   \n836 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n837 not require this, but we encourage authors to take this into account and make a best   \n838 faith effort.   \n839 12. Licenses for existing assets   \n840 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n841 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n842 properly respected?   \n843 Answer: [Yes]   \n844 Justification: Please refer Appendix A.5.   \n845 Guidelines:   \n846 \u2022 The answer NA means that the paper does not use existing assets.   \n847 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n848 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n849 URL.   \n850 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n851 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n852 service of that source should be provided.   \n853 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n854 package should be provided. For popular datasets, paperswithcode.com/datasets   \n855 has curated licenses for some datasets. Their licensing guide can help determine the   \n856 license of a dataset.   \n857 \u2022 For existing datasets that are re-packaged, both the original license and the license of   \n858 the derived asset (if it has changed) should be provided.   \n859 \u2022 If this information is not available online, the authors are encouraged to reach out to   \n860 the asset\u2019s creators.   \n861 13. New Assets   \n862 Question: Are new assets introduced in the paper well documented and is the documentation   \n863 provided alongside the assets?   \n864 Answer: [Yes]   \n865 Justification: Please refer the README file in code.   \n866 Guidelines:   \n867 \u2022 The answer NA means that the paper does not release new assets.   \n868 \u2022 Researchers should communicate the details of the dataset/code/model as part of their   \n869 submissions via structured templates. This includes details about training, license,   \n870 limitations, etc.   \n871 \u2022 The paper should discuss whether and how consent was obtained from people whose   \n872 asset is used.   \n873 \u2022 At submission time, remember to anonymize your assets (if applicable). You can either   \n874 create an anonymized URL or include an anonymized zip file.   \n875 14. Crowdsourcing and Research with Human Subjects   \n876 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n877 include the full text of instructions given to participants and screenshots, if applicable, as   \n878 well as details about compensation (if any)?   \n879 Answer: [NA]   \n880 Justification: This paper does not involve crowdsourcing nor research with human subjects.   \n881 Guidelines:   \n882 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n883 human subjects.   \n884 \u2022 Including this information in the supplemental material is fine, but if the main contribu  \n885 tion of the paper involves human subjects, then as much detail as possible should be   \n886 included in the main paper.   \n887 \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation,   \n888 or other labor should be paid at least the minimum wage in the country of the data   \n889 collector.   \n890 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human   \n891 Subjects   \n892 Question: Does the paper describe potential risks incurred by study participants, whether   \n893 such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)   \n894 approvals (or an equivalent approval/review based on the requirements of your country or   \n895 institution) were obtained?   \n896 Answer: [NA]   \n897 Justification: This paper does not involve crowdsourcing nor research with human subjects.   \n898 Guidelines:   \n899 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n900 human subjects.   \n901 \u2022 Depending on the country in which research is conducted, IRB approval (or equivalent)   \n902 may be required for any human subjects research. If you obtained IRB approval, you   \n903 should clearly state this in the paper.   \n904 \u2022 We recognize that the procedures for this may vary significantly between institutions   \n905 and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the   \n906 guidelines for their institution.   \n907 \u2022 For initial submissions, do not include any information that would break anonymity (if   \n908 applicable), such as the institution conducting the review. ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}]