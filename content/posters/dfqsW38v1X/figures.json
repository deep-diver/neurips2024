[{"figure_path": "dfqsW38v1X/figures/figures_0_1.jpg", "caption": "Figure 1: The distributions of activations at the input to the FFN block in LLAMA2-7B model, in the tenth layer. Left: using the default configuration as downloaded from Hugging Face. Right: after processing using QuaRot. The processed distribution has no outliers, leading to superior quantization.", "description": "This figure compares the distribution of activations before and after applying the QuaRot technique. The left side shows the original distribution with many outlier data points. The right side shows the distribution after applying QuaRot, where the outliers have been eliminated, resulting in a more uniform distribution and improved quantization.", "section": "1 Introduction"}, {"figure_path": "dfqsW38v1X/figures/figures_1_1.jpg", "caption": "Figure 1: The distributions of activations at the input to the FFN block in LLAMA2-7B model, in the tenth layer. Left: using the default configuration as downloaded from Hugging Face. Right: after processing using QuaRot. The processed distribution has no outliers, leading to superior quantization.", "description": "This figure shows the distribution of activations before and after applying the QuaRot method. The left panel shows the original distribution from the LLAMA2-7B model downloaded from Hugging Face, which exhibits several outliers (extreme values). The right panel shows the distribution after QuaRot processing, which successfully removes outliers by rotating the input data through Hadamard transformations. The removal of outliers makes quantization easier, leading to better results.", "section": "1 Introduction"}, {"figure_path": "dfqsW38v1X/figures/figures_3_1.jpg", "caption": "Figure 2: The gated feed-forward network used in most LMs, including the pre-positioned RMSNorm. The input signal is divided by its norm, and re-scaled by parameters a. Two linear blocks, Wup and Wgate are applied. The activation function o is applied to the gated signal, and the two signals are element-wise multiplied together. The final linear block Wdown produces the output signal Y. Before quantization, different operations are performed either in single (32 bit) or half (16 bit) precision.", "description": "This figure shows a gated feed-forward network, a common component in large language models (LLMs). It illustrates the flow of data through the network, highlighting the operations performed at each stage including RMSNorm, linear transformations (Wgate, Wup, Wdown), and an activation function (\u03c3).  The caption emphasizes that before quantization, operations are typically performed at higher precision (32-bit or 16-bit).", "section": "3.3 Transformer structures"}, {"figure_path": "dfqsW38v1X/figures/figures_4_1.jpg", "caption": "Figure 3: QuaRot applied to a LLaMa-style FFN. The RMSNorm scaling (a) has been absorbed into the weight matrices ((a) is a diagonal matrix with RMSNorm parameters). The hidden state X has been rotated by Q, which is canceled out by the absorption of QT into the first two weight matrices. All weights are stored in INT4, and all activations immediately before the weights are also quantized to INT4. The result of the matmul between the INT4 weights and activations on a TensorCore is INT32, which we immediately cast (and scale) to FP16 which is the default precision of the model. Whilst the signal is still in FP16, we perform a single on-the-fly Hadamard transform before quantizing and computing a (modified) down-proj, which results in a rotated output YQ.", "description": "This figure illustrates the QuaRot method applied to a feed-forward network (FFN) in a Llama-style large language model.  It shows how the weights and activations are processed to enable 4-bit quantization.  The key elements are the absorption of RMSNorm scaling into the weight matrices, the rotation of the hidden state using Hadamard transformation (Q), the cancellation of this rotation through Q<sup>T</sup>, 4-bit quantization of weights and activations, and a final on-the-fly Hadamard transform before the output is produced. This computational invariance allows for efficient low-bit inference.", "section": "4 Method"}, {"figure_path": "dfqsW38v1X/figures/figures_7_1.jpg", "caption": "Figure 4: Performance of the QuaRot kernel on a single transformer block of LLAMA-2 models using NVIDIA RTX 3090 GPU. Left: For the speedup results, we evaluate using sequence length 2048 with different batch sizes. Right: Peak memory saving during decoding of 50 tokens with different prefill sequence lengths using batch size 16.", "description": "This figure shows the performance gains of QuaRot on LLAMA-2 models. The left panel displays speedups in the prefill stage (processing input prompts) for various batch sizes, showing that QuaRot significantly accelerates the process. The right panel illustrates memory savings during the decoding stage (generating text) for different sequence lengths, highlighting QuaRot's efficiency in reducing memory consumption.  Both panels demonstrate QuaRot's effectiveness in improving both speed and efficiency of LLMs.", "section": "5.2 Performance Analysis"}, {"figure_path": "dfqsW38v1X/figures/figures_12_1.jpg", "caption": "Figure 5: Flow diagram of a self-attention block as used in most LMs, including the pre-positioned RMSNorm. Solid arrows represent flow during training, prefill and inference of each token. Dashed arrows show access to and from the KV cache, used at generation-time. The RoPE block computes relative positional embeddings.", "description": "This figure shows a flow diagram of a self-attention block, a common component in large language models (LLMs).  It details the flow of information during training, pre-filling, and inference.  Key elements highlighted include the RMSNorm, query (Wq), key (Wk), value (Wv) projections, the RoPE (Rotary Positional Embedding) mechanism, the multi-head attention operation itself, and the KV cache. The solid arrows represent the main flow during each stage, whereas the dashed arrows illustrate the interactions with the KV cache, particularly during the generation phase.", "section": "3.3 Transformer structures"}, {"figure_path": "dfqsW38v1X/figures/figures_12_2.jpg", "caption": "Figure 6: QuaRot applied to an attention component. The RMSNorm scaling a is absorbed into the input weight matrices, and the hidden state has been rotated by Q in the same way as for the FFN block (see previous figure). Colored labels show the bit-width of each flow, and dashed lines show the flow to/from the KV cache.", "description": "This figure illustrates the QuaRot method applied to the attention component of a transformer network. It shows how the input hidden state is rotated using a Hadamard matrix (Q), and how this rotation is absorbed into the weight matrices to maintain computational invariance.  The figure highlights the quantization of weights, activations, and the KV cache to 4 bits, showcasing the flow of data through the attention mechanism. The dashed lines indicate the interaction with the KV cache.", "section": "Method"}, {"figure_path": "dfqsW38v1X/figures/figures_17_1.jpg", "caption": "Figure 7: Performance of 16-bit and 4-bit linear layer for 2048 sequence lengths with and without online Hadamard transformation on a NVIDIA RTX 3090 GPU, averaged over 1000 runs. The matrix sizes correspond to the linear layer sizes in LLAMA-2 FFN blocks (i.e. Wdown). Here the batch size is 1, but the performance ratio holds for larger batches (see Table 14).", "description": "This figure compares the performance of 16-bit and 4-bit linear layers with and without online Hadamard transformations for different LLAMA-2 model sizes (7B, 13B, and 70B parameters). The runtime is measured in milliseconds and averaged over 1000 runs with a batch size of 1.  The results show significant speedup with 4-bit quantization, especially for larger models.", "section": "A.10 Performance Analysis"}]