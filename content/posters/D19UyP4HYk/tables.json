[{"figure_path": "D19UyP4HYk/tables/tables_2_1.jpg", "caption": "Table 1: List of Skills for each Dataset This table lists down the skill obtained after the skill clustering phase for each dataset and corresponding topics. Skill names were provided by GPT-4-0613. The skills of the other topics in MATH can be found in Appendix Table 10", "description": "This table shows the list of skills obtained after the skill clustering phase for GSM8K and MATH datasets.  The skills are categorized by topic and were generated by GPT-4.  A more complete list for the MATH dataset is available in the appendix.", "section": "Paper organization and main results"}, {"figure_path": "D19UyP4HYk/tables/tables_4_1.jpg", "caption": "Table 2: Text-based prompt results on the MATH Dataset. Our Skill-Based approach, employing CoT prompting, demonstrates superior performance over all other methods across all topics within the MATH dataset. All experiments were conducted using GPT-4-0613.", "description": "This table presents the results of text-based prompting experiments conducted on the MATH dataset.  It compares the performance of different prompting methods: Chain-of-Thought (CoT), Complex CoT, CoT with topic-based examples, and CoT with skill-based examples. The skill-based approach, using chain-of-thought prompting, shows the best overall performance across all mathematical topics in the dataset.  All experiments utilized GPT-4-0613.", "section": "4 Experiments"}, {"figure_path": "D19UyP4HYk/tables/tables_5_1.jpg", "caption": "Table 3: Text-based prompt results on the GSM8K Dataset. Our Skill-Based approach outperforms various other methods on the GSM8K dataset across two different models: GPT-3.5 Turbo and GPT-4-0613. Refer to text for description of baselines.", "description": "This table presents the results of text-based prompting experiments conducted on the GSM8K dataset using two different language models: GPT-3.5-Turbo and GPT-4-0613.  The table compares the performance of several prompting methods, including Retrieval RSD, CoT (Chain of Thought), CoT + Random, CoT + Skill-Based, and CoT + Skill-Based (maj@5). The CoT + Skill-Based approaches consistently outperform the baselines, demonstrating the effectiveness of incorporating skill-based in-context examples. The (maj@5) variation represents the use of self-consistency.", "section": "4 Experiments"}, {"figure_path": "D19UyP4HYk/tables/tables_6_1.jpg", "caption": "Table 4: Program-aided prompts results on the MATH dataset. This table illustrates the performance achieved by employing the Skill-Based approach to generate code for problem-solving tasks drawn from the MATH dataset using GPT-4-0613. Evidently, supplying pertinent in-context examples grounded in specific skills enhances the program generation performance of GPT-4-0613, leading to a notable improvement across all topics encompassed in the MATH dataset.", "description": "This table presents the results of experiments using program-aided prompting methods on the MATH dataset.  It compares the performance of a standard program-aided approach (PAL) with variations incorporating the Skill-Based method. The Skill-Based method uses skill-specific in-context examples to enhance code generation. The results show improved performance across different math topics when using the Skill-Based approach, demonstrating its effectiveness in enhancing code generation for problem-solving.", "section": "4.2 Program-Aided Prompts"}, {"figure_path": "D19UyP4HYk/tables/tables_6_2.jpg", "caption": "Table 5: Transfer Skill Exemplars to Other Models. All experiments are performed using the MATH dataset on the Mixtral 8 \u00d7 7B model, comparing against standard CoT, CoT with topic-based exemplars, CoT with skill-based exemplars, CoT with self-consistency (maj@4) using both topic and skill-based exemplars. Skill labels and exemplars are obtained from GPT-4-0613. The enhanced performance of Skill-Based indicates effective transferability of skills from GPT-4 to another model.", "description": "This table presents the results of experiments conducted using the Mixtral 8x7B model on the MATH dataset.  It compares the performance of several prompting methods: standard Chain-of-Thought (CoT), CoT with topic-based exemplars, CoT with skill-based exemplars, and CoT with self-consistency (maj@4) using both topic and skill-based exemplars. The skill labels and exemplars are obtained from GPT-4-0613. The results highlight the improved performance and transferability of the skill-based approach.", "section": "4 Experiments"}, {"figure_path": "D19UyP4HYk/tables/tables_7_1.jpg", "caption": "Table 7: Instances Benefiting from Skill-Based Approach This table illustrates instances where our skill-based approach empowers the Language Model (LLM) to apply relevant skills effectively. Red-highlighted text reveals conceptual errors by the Topic-Based baseline, while blue-highlighted text showcases skillful and accurate skill application.", "description": "This table presents two examples where the skill-based approach outperforms the topic-based approach in solving math problems.  The skill-based approach correctly identifies and applies the relevant skills, while the topic-based approach makes errors due to a misunderstanding of core concepts. The table highlights the superiority of the skill-based method for accurate and skillful problem-solving.", "section": "4.4 Analysis"}, {"figure_path": "D19UyP4HYk/tables/tables_7_2.jpg", "caption": "Table 6: Transfer of Skill Exemplars to Other Datasets Investigation of skill transfer from GSM8K to different math word problem datasets using GPT-4-0613.", "description": "This table shows the results of applying the skill exemplars learned from the GSM8K dataset to other math word problem datasets.  It demonstrates the transferability of the skill-based approach across different datasets by comparing the performance of the CoT, PAL, CoT + PAL, and CoT + Skill-Based methods on six different datasets: SVAMP, SingleOP, SingleEQ, AddSub, MultiArith, and ASDIV. The results highlight whether the skills learned from one dataset are applicable and effective in solving problems from other datasets.", "section": "4.3 Transfer of Skill Exemplars"}, {"figure_path": "D19UyP4HYk/tables/tables_9_1.jpg", "caption": "Table 2: Text-based prompt results on the MATH Dataset. Our Skill-Based approach, employing CoT prompting, demonstrates superior performance over all other methods across all topics within the MATH dataset. All experiments were conducted using GPT-4-0613.", "description": "This table presents the results of text-based prompting experiments on the MATH dataset.  The Skill-Based approach, which uses chain-of-thought (CoT) prompting and incorporates skill-based in-context examples, is compared against several baselines (CoT, Complex CoT, CoT + Topic-Based). The results show that the Skill-Based approach achieves superior performance across all mathematical topics in the dataset, using GPT-4-0613.", "section": "4 Experiments"}, {"figure_path": "D19UyP4HYk/tables/tables_9_2.jpg", "caption": "Table 9: Alignment. We apply the proposed Skill-Based approach for task of alignment via in-context learning. We find that proposed Skill-Based approach outperforms the Random approach.", "description": "This table presents the results of applying the Skill-Based approach to the alignment task using the Mistral-7B model.  The performance is measured using six metrics (helpfulness, clarity, factuality, depth, engagement, and safety), each scored from 1 to 5 by GPT-4.  The Skill-Based approach shows improvements across all six metrics compared to a random baseline.", "section": "4.6 Metacognitive Abilities beyond Math"}, {"figure_path": "D19UyP4HYk/tables/tables_13_1.jpg", "caption": "Table 1: List of Skills for each Dataset This table lists down the skill obtained after the skill clustering phase for each dataset and corresponding topics. Skill names were provided by GPT-4-0613. The skills of the other topics in MATH can be found in Appendix Table 10", "description": "This table shows the list of skills obtained after performing skill clustering on GSM8K and MATH datasets.  Each topic within the datasets is associated with a set of skills identified by the GPT-4 language model.  The table helps to understand the granularity and types of skills the model identified for different mathematical problem categories.", "section": "3 Automated Skill Discovery"}, {"figure_path": "D19UyP4HYk/tables/tables_15_1.jpg", "caption": "Table 11: This dataset shows examples from the skill exemplar repository constructed using the GSM8K training dataset.", "description": "This table shows example questions and their corresponding answers from the skill exemplar repository created using the GSM8K training dataset. Each example is labeled with a skill, showcasing how the LLM identifies relevant skills for each problem.", "section": "Skill Discovery"}, {"figure_path": "D19UyP4HYk/tables/tables_15_2.jpg", "caption": "Table 3: Text-based prompt results on the GSM8K Dataset. Our Skill-Based approach outperforms various other methods on the GSM8K dataset across two different models: GPT-3.5 Turbo and GPT-4-0613. Refer to text for description of baselines.", "description": "This table presents the results of text-based prompting experiments on the GSM8K dataset.  It compares the performance of the Skill-Based approach against several baselines (Retrieval RSD, CoT, CoT + Random, CoT + Skill-Based), using two different language models (GPT-3.5-Turbo and GPT-4-0613). The Skill-Based approach consistently achieves higher accuracy, demonstrating the effectiveness of using skill-aligned in-context examples.", "section": "4.1 Text-based Prompts"}, {"figure_path": "D19UyP4HYk/tables/tables_17_1.jpg", "caption": "Table 13: Skill Labels Assigned by Mixtral-8x7B, GPT-3.5, and GPT-4", "description": "This table compares the skill labels assigned by three different language models: Mixtral-8x7B, GPT-3.5, and GPT-4, for three distinct mathematical questions.  It highlights the differences in the granularity and descriptive nature of the skill labels produced by each model, showcasing GPT-4's superior ability to provide more precise and comprehensive skill assignments.", "section": "10.4 Comparing Skill Annotation Models"}, {"figure_path": "D19UyP4HYk/tables/tables_17_2.jpg", "caption": "Table 2: Text-based prompt results on the MATH Dataset. Our Skill-Based approach, employing CoT prompting, demonstrates superior performance over all other methods across all topics within the MATH dataset. All experiments were conducted using GPT-4-0613.", "description": "This table presents the results of text-based prompting experiments on the MATH dataset.  It compares the performance of the Skill-Based approach (using Chain-of-Thought prompting) against several baselines across different mathematical topics within the dataset.  The Skill-Based approach consistently outperforms the others, demonstrating the effectiveness of incorporating skill-based information into prompting strategies.", "section": "4 Experiments"}, {"figure_path": "D19UyP4HYk/tables/tables_18_1.jpg", "caption": "Table 2: Text-based prompt results on the MATH Dataset. Our Skill-Based approach, employing CoT prompting, demonstrates superior performance over all other methods across all topics within the MATH dataset. All experiments were conducted using GPT-4-0613.", "description": "This table presents the results of text-based prompting experiments on the MATH dataset.  It compares the performance of different prompting methods, including Chain-of-Thought (CoT), CoT with topic-based examples, and CoT with skill-based examples. The skill-based approach, which uses examples from the Skill Exemplar Repository created in the study, shows significantly better performance across all math topics.", "section": "4 Experiments"}, {"figure_path": "D19UyP4HYk/tables/tables_20_1.jpg", "caption": "Table 5: Transfer Skill Exemplars to Other Models. All experiments are performed using the MATH dataset on the Mixtral 8 \u00d7 7B model, comparing against standard CoT, CoT with topic-based exemplars, CoT with skill-based exemplars, CoT with self-consistency (maj@4) using both topic and skill-based exemplars. Skill labels and exemplars are obtained from GPT-4-0613. The enhanced performance of Skill-Based indicates effective transferability of skills from GPT-4 to another model.", "description": "This table presents the results of transferring skill exemplars from GPT-4 to the Mixtral 8x7B model for solving math problems from the MATH dataset. It compares the performance of several prompting methods, including standard Chain-of-Thought (CoT), CoT with topic-based exemplars, CoT with skill-based exemplars, and CoT with self-consistency, using both topic-based and skill-based exemplars. The results demonstrate the effectiveness of using skill-based exemplars in enhancing the performance of weaker LLMs.", "section": "4.3 Transfer of Skill Exemplars"}, {"figure_path": "D19UyP4HYk/tables/tables_21_1.jpg", "caption": "Table 2: Text-based prompt results on the MATH Dataset. Our Skill-Based approach, employing CoT prompting, demonstrates superior performance over all other methods across all topics within the MATH dataset. All experiments were conducted using GPT-4-0613.", "description": "This table presents the results of text-based prompting experiments conducted on the MATH dataset.  The Skill-Based approach, which incorporates Chain-of-Thought (CoT) prompting and utilizes skill-aligned examples, is compared against several baselines (CoT, Complex CoT, CoT + Topic-Based). The results show the Skill-Based approach significantly outperforms the baselines across various mathematical topics within the MATH dataset.  All experiments used GPT-4-0613.", "section": "4 Experiments"}, {"figure_path": "D19UyP4HYk/tables/tables_22_1.jpg", "caption": "Table 2: Text-based prompt results on the MATH Dataset. Our Skill-Based approach, employing CoT prompting, demonstrates superior performance over all other methods across all topics within the MATH dataset. All experiments were conducted using GPT-4-0613.", "description": "This table presents the results of text-based prompting experiments on the MATH dataset.  It compares the performance of different prompting methods, including Chain of Thought (CoT), CoT with topic-based examples, and CoT with skill-based examples. The Skill-Based approach, which uses the skill exemplar repository developed earlier in the paper, significantly outperforms other methods across all topics in the MATH dataset.", "section": "4 Experiments"}]