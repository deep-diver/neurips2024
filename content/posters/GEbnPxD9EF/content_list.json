[{"type": "text", "text": "Consistency of Neural Causal Partial Identification ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Jiyuan Tan Jose Blanchet Management Science and Engineering Management Science and Engineering Stanford University Stanford University Stanford, CA 94305 Stanford, CA 94305 jiyuantan@stanford.edu jose.blanchet@stanford.edu ", "page_idx": 0}, {"type": "text", "text": "Vasilis Syrgkanis Management Science and Engineering Stanford University Stanford, CA 94305 vsyrgk@stanford.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Recent progress in Neural Causal Models (NCMs) showcased how identification and partial identification of causal effects can be automatically carried out via training of neural generative models that respect the constraints encoded in a given causal graph [52, 3]. However, formal consistency of these methods has only been proven for the case of discrete variables or only for linear causal models. In this work, we prove the consistency of partial identification via NCMs in a general setting with both continuous and categorical variables. Further, our results highlight the impact of the design of the underlying neural network architecture in terms of depth and connectivity as well as the importance of applying Lipschitz regularization in the training phase. In particular, we provide a counterexample showing that without Lipschitz regularization this method may not be asymptotically consistent. Our results are enabled by new results on the approximability of Structural Causal Models (SCMs) via neural generative models, together with an analysis of the sample complexity of the resulting architectures and how that translates into an error in the constrained optimization problem that defines the partial identification bounds. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Identifying causal quantities from observational data is an important problem in causal inference which has wide applications in economics [1], social science [20], health care [34, 18], and recommendation systems [9]. One common approach is to transform causal quantities into statistical quantities using the ID algorithm [49] and deploy general purpose methods to estimate the statistical quantity. However, in the presence of unobserved confounding, typically the causal quantity of interest will not be point-identified by observational data, unless special mechanisms are present in the data generating process (e.g. instruments, unconfounded mediators, proxy controls). In the presence of unobserved confounding, the ID algorithm will typically fail to return a statistical quantity and declare the causal quantity as non-identifiable. ", "page_idx": 0}, {"type": "text", "text": "One remedy to this problem, which we focus on in this paper, is partial identification, which aims to give informative bounds for causal quantities based on the available data. At a high level, partial identification bounds can be defined as follows: find the maximum and the minimum value that a target causal quantity can take, among all Structural Causal Models (SCMs) that give rise to the same observed data distribution and respect the given causal graph (as well as any other structural constraints that one is willing to impose). Note that in the presence of unobserved confounding, there will typically exist many structural mechanisms that could give rise to the same observational distribution but have vastly different counterfactual distributions. Hence, partial identification can be formulated as solving a max and a min optimization problem [3] ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\mathcal{M}\\in\\mathcal{C}}{\\operatorname*{max}}\\setminus\\underset{\\mathcal{M}\\in\\mathcal{C}}{\\operatorname*{min}}\\;\\theta(\\mathcal{M}),}\\\\ &{\\quad\\mathrm{subject}\\;\\mathrm{to}\\;P^{\\mathcal{M}}(V)=P^{\\mathcal{M}^{*}}(V),\\qquad\\mathrm{and}\\qquad\\mathcal{G}_{\\mathcal{M}}=\\mathcal{G}_{\\mathcal{M}^{*}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $\\theta(\\mathcal{M})$ is the causal quantity of interest, $\\mathcal{M}^{\\ast}$ is the true model, $V$ is the set of observed nodes, $P^{\\mathcal{M}}(V)$ is the distribution of $V$ in SCM $\\mathcal{M}$ , $\\mathcal{C}$ is a collection of causal models and $\\mathcal{G}_{\\mathcal{M}}$ is the causal graph of $\\mathcal{M}$ (see Section 2 for formal definitions). Two recent lines of work explore the optimization approach to partial identification. The first line deals with discrete Structure Causal Models (SCMs), where all observed variables are finitely supported. In this case, (1) becomes a Linear Programming (LP) or polynomial programming problem and tight bounds can be obtained [37, 4, 5, 27, 44, 45, 40, 8, 54, 56, 14]. The second line of work focuses on continuous models and explores ways of solving (1) in continuous settings using various techniques [24, 32, 29, 38, 3, 41]. ", "page_idx": 1}, {"type": "text", "text": "Recently, Xia et al. [51] formalized the connection between SCMs and generative models (see also [33] for an earlier version of a special case of this connection). This work showcased that SCMs can be interpreted as neural generative models, namely Neural Causal Models (NCMs), that follow a particular architecture that respects the constraints encoded in a given causal graph. Hence, counterfactual quantities of SCMs can be learned by optimizing over the parameters of the underlying generative models. However, there could be multiple models that lead to the same observed data distribution, albeit have different counterfactual distributions. Xia et al. [51] first analyze the approximation power of NCMs for discrete SCMs and employ the max/min approach to verify the identifiability of causal quantities, without the need to employ the ID algorithm. Balazadeh et al. [3] and Hui et al. [30], extend the method in [51] and re-purpose it to perform partial identification by solving the min and max problem in the partial identification formulation over neural causal models. ", "page_idx": 1}, {"type": "text", "text": "However, for SCMs with general random variables and functional relationships, the approximation error and consistency of this optimization-based approach to partial identification via NCMs has not been established. In particular, two problems remain open. First, given an arbitrary SCM, it is not yet known if we can find an NCM that produces approximately the same intervention distribution as the original one. Although Xia et al.[51] show it is possible to represent any discrete SCM by an NCM, their construction highly relies on the discrete assumption and cannot be directly generalized to the general case. Moreover, Xia et al. [51] use step functions as the activation function in their construction, which may create difficulties in the training process since step functions are discontinuous with a zero gradient almost everywhere. ", "page_idx": 1}, {"type": "text", "text": "Second, since we only have access to $n$ samples from the true distribution, $P^{\\mathcal{M}}(V)$ , we need to replace the constraints in (1) with their empirical version that uses the empirical distribution of samples $P_{n}^{\\mathcal{M}^{*}}(V)$ in place of the population distribution and looks for NCMs, whose implied distribution lies within a small distance from the empirical distribution. Moreover, even the NCM distribution is typically only accessible through sampling, hence we will need to generate $m_{n}$ samples from the NCM and use the empirical distribution of the $m_{n}$ samples from the NCM in place of the true distribution implied by the NCM. Thus, in practice, we will use a constraint of the form $d(P_{n}^{\\mathcal{M}^{*}}(V),P_{m_{n}}^{\\mathcal{M}}(V))\\leqslant\\alpha_{n}$ , where $d$ is some notion of distribution distance and $\\alpha_{n}$ accounts for the sampling error. It is not clear that this approach is consistent, converges to the correct partial identification bounds, when the sample size $n$ grows to infinity. Balazadeh et al. [3] only show the consistency of this approach for linear SCMs. Consistency results concerning more general SCMs is still lacking in the neural causal literature. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we establish representation and consistency results for general SCMs. Our contributions are summarized as follows. ", "page_idx": 1}, {"type": "text", "text": "\u2022 We show that under suitable regularity assumptions, given any Lipschitz SCM, we can approximate it using an NCM such that the Wasserstein distance between any interventional distribution of the NCM and the original SCM is small. Each random variable of the SCM is allowed to be continuous or categorical. We specify two architectures of the Neural Networks (NNs) that can be trained using common gradient-based optimization algorithms (Theorem 2, Theorem 3 and Corollary 1). ", "page_idx": 1}, {"type": "text", "text": "\u2022 To construct the NCM approximation, we develop a novel representation theorem of probability measures (Proposition 1) that may be of independent interest. Proposition 1 implies that under certain assumptions, probability distributions supported on the unit cube can be simulated by pushing forward a multivariate uniform distribution.   \n\u2022 We discover the importance of Lipschitz regularization by constructing a counterexample where the neural causal approach is not consistent without regularization (Proposition 2).   \n\u2022 Using Lipschitz regularization, we prove the consistency of the neural causal approach (Theorem 4). ", "page_idx": 2}, {"type": "text", "text": "Related Work There exists a rich literature on partial identification of average treatment effects (ATE) [37, 4, 5, 27, 44, 45, 40, 8, 54, 56, 14, 7, 39, 26]. Balke and Pearl [4, 5] first give an algorithm to calculate bounds on the ATE in the Instrumental Variable (IV) setting. They show that regardless of the exact distribution of the latent variables, it is sufficient to consider discrete latent variables as long as all endogenous variables are finitely supported. Moreover, they discover that (1) is an LP problem with a closed-form solution. This LP-based technique was generalized to several special classes of SCMs [44, 8, 27, 46]. For general discrete SCMs, [14, 56, 55] consider transforming the problem (1) into a polynomial programming problem. Xia et al. [51] discover the connection between generative models and SCMs. They show that NCMs are expressive enough to approximate discrete SCMs. By setting $\\mathcal{C}$ in problem (1) to be the collection of NCMs, they apply NCMs for identification and estimation. ", "page_idx": 2}, {"type": "text", "text": "For causal models with continuous random variables, the constraints in (1) become integral equations, which makes the problem more difficult. One approach is to discretize the constraints. [24] uses stochastic process representation of the causal model in the continuous IV setting and transforms the problem into a semi-infinite program. [32] relaxes the constraints to finite moment equations and solves the problem by the Augmented Lagrangian Method (ALM). The other approach is to use generative models to approximate the observational distribution and use some metric to measure the distance between distributions. [33] first propose to use GAN to generate images. Later, [29] uses Wasserstein distance in the constraint and transforms the optimization problem into a min and max problem. Similarly, [3] solves the optimization problem using Sinkhorn distance to avoid instability during training. They propose to estimate the Average Treatment Derivative (ATD) and use ATD to obtain a bound on the ATE. They also prove the consistency of the proposed estimator for linear SCMs. [41] uses a linear combination of basis functions to approximate response functions. [26] uses sieve to solve the resulting optimization problem in the IV setting. [19] proposes a neural network framework for sensitivity analysis under unobserved confounding. ", "page_idx": 2}, {"type": "text", "text": "Organization of this paper In Section 2, we introduce the notations and some basic concepts used throughout the paper. Next, in Section 3, we demonstrate how to construct an NCM so that they can approximate a given SCM arbitrarily well. Two kinds of architecture are given along with an approximation error analysis. In Section 4, we highlight the importance of Lipschitz regularization by giving a counterexample that is not consistent. Then, leveraging the previous approximation results, we are able to prove the consistency of this approach under regularization. Finally, we compare our method with the traditional polynomial programming method empirically in Section 4.1. ", "page_idx": 2}, {"type": "text", "text": "2 Preliminary ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "First, we introduce the definition of an SCM. Throughout the paper, we use bold symbols to represent sets of random variables. ", "page_idx": 2}, {"type": "text", "text": "Definition 1. (Structural Causal Model) A Structural Causal Model (SCM) is a tuple $\\mathcal{M}\\,=$ $(V,U,F,P(U),\\mathcal{G}_{0})$ , where $\\pmb{V}~=~\\{V_{i}\\}_{i=1}^{n_{V}}$ is the set of observed variables; $U\\;=\\;\\{U_{j}\\}_{j=1}^{n_{U}}$ is the set of latent variables; $P(U)$ is the distribution of latent variables; $\\mathcal{G}_{0}$ is an acyclic directed graph whose nodes are $V$ . The values of each observed variable $V_{i}$ are generated by ", "page_idx": 2}, {"type": "equation", "text": "$$\nV_{i}=f_{i}\\left(P a(V_{i}),U_{V_{i}}\\right),\\quad w h e r e\\:V_{i}\\notin P a(V_{i})\\:a n d\\:U_{V_{i}}\\subset U,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $F=\\left(f_{1},\\cdot\\cdot\\cdot,f_{n_{V}}\\right)$ , $P a(V)$ is the set of parents of $V$ in graph $\\mathcal{G}_{0}$ and $U_{V_{i}}$ is the set of latent variables that affect $V_{i}$ . $V_{i}$ takes either continuous values $\\mathbb{R}^{d_{i}}$ or categorical in $[n_{i}]$ . We extend graph $\\mathcal{G}_{\\mathrm{0}}$ by adding bi-directed arrows between any $V_{i}$ $\\mathbf{\\chi}_{i}^{r},V_{j}\\in\\mathcal{G}_{0}$ if there exists a correlated latent variable pair $(U_{k},U_{l}),U_{k}\\in{\\pmb U}_{V_{i}},U_{l}\\in{\\pmb U}_{V_{j}}$ . We call the extended graph $\\mathcal{G}_{\\mathcal{M}}$ the causal graph of $\\mathcal{M}$ . ", "page_idx": 2}, {"type": "text", "text": "When we write $\\mathrm{Pa}(V_{i})$ , we refer to the parents of $V_{i}$ in the $\\mathcal{G}_{0}$ . To connect with the literature, the causal graph we define is a kind of Acyclic Directed Mixed Graph (ADMG), which is often used to represent SCMs with unobserved variables [49]. Note that we allow one latent variable to enter several nodes, which differs from the common definition. We use $n_{U}$ and $n_{V}$ to denote the number of latent variables and observable variables. Let $T\\subset V$ be a set of treatment variables. The goal is to estimate causal quantities under a given intervention $\\mathbf{\\nabla}T=t$ . Formally, the structural equations of the intervened model are ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{T_{i}=t_{i},\\quad\\forall T_{i}\\in\\mathbf{T},}\\\\ &{V_{i}(t)=f_{i}\\left(\\mathbf{Pa}(V_{i}),\\mathbf{U}_{V_{i}}\\right),\\quad\\forall V_{i}\\notin\\mathbf{T}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "We denote $V_{i}(\\pmb{t})$ to be the value of $V_{i}$ under the intervention $\\textbf{\\textit{T}}=\\textbf{\\em t}$ and $P^{\\mathcal{M}}(V(t))$ to be the distribution of $\\dot{V}(t)$ in $\\mathcal{M}$ . The notion of a $C^{2}$ component [51] is defined as follows. ", "page_idx": 3}, {"type": "text", "text": "Definition 2 ( $C^{2}$ -Component). For a causal graph $\\mathcal{G}$ , a subset $C\\subset V$ is $C^{2}$ -component if each pair $V_{i},V_{j}\\in C$ is connected by a bi-directed arrow in $\\mathcal{G}$ and $C$ is maximal. ", "page_idx": 3}, {"type": "text", "text": "We provide a concrete example in Appendix A to explain all these notions. We will make the following standard assumption about the independence of latent variables. Note that since we allow latent variables to enter in multiple structural equations, this is more a notational convention and not an actual assumption. Also note that under this convention a bi-directed arrow essentially represents the existence of a common latent parental variable. ", "page_idx": 3}, {"type": "text", "text": "Assumption 1. All the latent variables in $U$ are independent. ", "page_idx": 3}, {"type": "text", "text": "To deal with categorical variables, we assume that latent variables that influence categorical variables contain two parts: the shared confounding that influences the propensity functions and the independent noise that generates categorical distributions. ", "page_idx": 3}, {"type": "text", "text": "Assumption 2. The set of latent variables consists of two parts $\\pmb{U}~=~\\{U_{1},\\pmb{\\cdot}\\cdot\\cdot,U_{n_{U}}\\}~\\cup$ $\\{G_{V_{i}}:\\bar{V}_{i}$ is categorical}. Precisely, if $V_{i}\\in V$ is a categorical variable, the data generation process of $V_{i}$ satisfies $\\begin{array}{r}{V_{i}=\\arg\\operatorname*{max}_{k\\in[n_{i}]}\\left\\{g_{k}^{V_{i}}+\\log\\left(f_{i}\\left(P a(V_{i}),U_{V_{i}}\\right)\\right)_{k}\\right\\}\\sim C a t e g o r i c a l(f_{i}/\\|f_{i}\\|_{1}),}\\end{array}$ , where $G_{V_{i}}=(g_{1}^{V_{i}},\\cdot\\cdot\\cdot\\,,g_{n_{i}}^{V_{i}})$ are i.i.d. standard Gumbel variables, $U_{V_{i}}\\subset\\{U_{1},\\cdot\\cdot\\cdot,U_{n_{U}}\\}$ . ", "page_idx": 3}, {"type": "text", "text": "This convention is without loss of generality at this point, but will be useful when introducing Lispchitz restrictions on the structural equation functions. The Gumbel variables in the assumption can be replaced by any random variables that can generate categorical variables. It can be proven that all discrete SCMs satisfy this assumption. Note that we implicitly assume that all categorical variables $V_{i}$ are supported on $[n_{i}]$ for some $n_{i}$ . It is straightforward to generalize all results to any finite support. Next, we introduce Neural Causal Models (NCMs). ", "page_idx": 3}, {"type": "text", "text": "Definition 3. (Neural Causal Model) A Neural Causal Model (NCM) is a special kind of SCM where $U=\\{U_{1},\\cdot\\cdot\\cdot,U_{n_{U}}\\}\\cup\\{G_{V_{i}}:V_{i}$ categorica $l\\}$ , all $U_{i}$ are i.i.d. multivariate uniform variables, $G_{V_{i}}$ are i.i.d Gumblel variables and functions in (2) are Neural Networks (NNs). ", "page_idx": 3}, {"type": "text", "text": "The definition of NCMs we use is slightly different from that in [52] because we need to deal with mixed variables in our models. In (1), we usually take $\\mathcal{C}$ to be the set of all SCMs. However, it is difficult to search over all SCMs since (1) becomes an infinite-dimensional polynomial programming problem. As an alternative, we can search over all NCMs. One quantity of common interest in causal inference is the Average Treatment Effect (ATE). ", "page_idx": 3}, {"type": "text", "text": "Definition 4. (Average Treatment Effect). For SCM $\\mathcal{M}$ , the ATE at $\\mathbf{\\nabla}T=t$ with respect to ${\\mathbf{}}T=t_{0}$ is given by $A T E_{\\mathcal{M}}(t)=\\mathbb{E}_{{\\mathbf{u}}\\sim P(U)}[Y(\\ddot{t})-Y(t_{0})]$ . ", "page_idx": 3}, {"type": "text", "text": "Partial identification can be formulated as estimating the solution to the optimization problems (1) [3]. The max and min values $\\overline{F}$ and $\\underline{{F}}$ define the interval $[\\underline{{F}},\\overline{{F}}]$ which is the smallest interval we can derive from the observed data without additional assumptions. In particular, if ${\\overline{{F}}}={\\underline{{F}}}$ , then the causal quantity is point-identified. ", "page_idx": 3}, {"type": "text", "text": "Notations. We use $\\|\\cdot\\|,\\|\\cdot\\|_{\\infty}$ for the 1-norm and $\\infty$ -norm and $[n]$ for $\\{1,\\cdot\\cdot\\cdot,n\\}$ . Bold letters represent sets of random variables. We let $\\mathcal{F}_{L}(K_{1},K_{2})$ be the class of Lipschitz $L$ -continuous functions $f:K_{1}\\to K_{2}$ . We may omit the domain and use $\\mathcal{F}_{L}$ when the domain is clear from context. Let $\\mathcal{H}(K_{1},K_{2})$ be the set of homeomorphisms from $K_{1}$ to $K_{2}$ , i.e., injective and continuous maps in both directions. We define $\\begin{array}{r}{\\epsilon(\\mathcal{F}_{1},\\mathcal{F}_{2})=\\operatorname*{sup}_{f_{2}\\in\\mathcal{F}_{2}}\\operatorname*{inf}_{f_{1}\\in\\mathcal{F}_{1}}\\left\\|f_{2}-f_{1}\\right\\|}\\end{array}$ for function classes $\\mathcal{F}_{1},\\mathcal{F}_{2}$ . We use standard asymptotic notation $O(\\cdot),\\Omega(\\cdot)$ . Given a measure $\\mu$ on $\\mathbb{R}^{d_{1}}$ and a measurable function $f:\\mathbb{R}^{d_{1}}\\rightarrow\\mathbb{R}^{d_{2}}$ , the push-forward measure $f_{\\#\\mu}$ is defined as $f_{\\#\\mu(B)}=\\mu\\left(f^{-1}(B)\\right)$ for all measurable sets $B$ . We use $P(X)$ to represent the distribution of random variable $X$ . Let $\\Delta_{n}=$ $\\{(p_{1},\\cdot\\cdot\\cdot,p_{n}):\\textstyle\\sum_{i=1}^{n}p_{i}=1,p_{i}\\geqslant0\\}$ be the probability simplex. We use Categorical $(p)$ , $\\pmb{p}\\in\\Delta_{n}$ to represent cat egorical distribution with event probability $\\pmb{p}$ . We let $W(\\cdot,\\cdot)$ be the Wasserstein-1 distance and $S_{\\lambda}(\\mu,\\nu)$ be the Sinkhorn distance [12] with regularization parameter $\\lambda>0$ . ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "3 Approximation Error of Neural Causal Models ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we study the expressive power of NCMs, which serves as a key ingredient in proving the consistency result. In particular, given an SCM $\\mathcal{M}^{\\ast}$ , we want to construct an NCM $\\hat{\\mathcal{M}}$ such that the two causal models produce similar interventional results. Unlike in the discrete case [4, 51], latent distributions can be extremely complicated in general cases. The main challenge is how to design the structure of NCMs to ensure strong approximation power. ", "page_idx": 4}, {"type": "text", "text": "In the following, we first derive an upper bound on the Wasserstein distance between two causal models sharing the same causal graph. Using this result, we decompose the approximation error into two parts: the error caused by approximating structural functions via neural networks and the error of approximating the latent distributions. Then, we design different architectures for these two parts. ", "page_idx": 4}, {"type": "text", "text": "Decomposing the Approximation Error First, we present a canonical representation of an SCM, which essentially states that we only need to consider the case where each latent variable $U_{i}$ corresponds to a $C^{2}$ component of $\\mathcal{G}$ . ", "page_idx": 4}, {"type": "text", "text": "Definition 5 (Canonical representation). A SCM $\\mathcal{M}$ with causal graph $\\mathcal{G}$ has canonical form if ", "page_idx": 4}, {"type": "text", "text": "1. The set of latent variables consists of two sets, $U=\\left\\{U_{C}:C\\;i s\\;a\\,C^{2}{\\mathrm{-}}c o m p o n e n t\\;o f\\,{\\mathcal{G}}\\right\\}\\cup\\left\\{G_{V_{i}}=\\left(g_{1}^{V_{i}},\\cdots,g_{n_{i}}^{V_{i}}\\right):V_{i}\\;i s\\;c a t e g o r i c a l\\right\\},$ where $U_{C}$ and $g_{j}^{V_{i}}$ are independent and $g_{j}^{V_{i}}$ are standard Gumbel variables.   \n2. The structure equations have the form $V_{i}=\\left\\{\\begin{array}{l l}{f_{i}\\left(P a(V_{i}),U_{V_{i}}\\right),}&{V_{i}\\;i s\\;c o n t i n u o u s,}\\\\ {\\mathrm{arg\\;max}_{k\\in[n_{i}]}\\left\\{g_{k}^{V_{i}}+\\log\\left(f_{i}\\left(P a(V_{i}),U_{V_{i}}\\right)\\right)_{k}\\right\\},}&{V_{i}\\;i s\\;c a t e g o r i c a l,\\|f_{i}\\|_{1}=1,}\\end{array}\\right.$ (3) where $U_{V_{i}}=\\{U_{C}:V_{i}\\in C,C$ is a $C^{2}$ -component of $\\mathcal{G}\\}$ and $(x)_{k}$ is the $k$ -th coordinate of the vector $x$ . We further assume that $f_{i}$ are normalized for categorical variables. ", "page_idx": 4}, {"type": "text", "text": "Given a function class $\\mathcal{F}$ , the SCM class ${\\mathcal{M}}({\\mathcal{G}},{\\mathcal{F}},U)$ consists of all canonical SCM models with causal graph $\\mathcal{G}$ such that $f_{i}\\in\\mathcal{F},i\\in[n_{V}]$ . ", "page_idx": 4}, {"type": "text", "text": "Proposition 4 in the appendix shows that any SCM satisfying Assumption 1,2 can be represented in this way and we provide an example in Appendix B.1 to illustrate how to obtain the canonical representation for a given SCM. Therefore, we restrict our attention to the class $\\mathcal{M}(\\mathcal{G},\\mathcal{F},U)$ . For two SCM classes $\\mathcal{M}(\\mathcal{G},\\mathcal{F},U),\\mathcal{M}(\\mathcal{G},\\hat{\\mathcal{F}},\\hat{U})$ , we want to study how well we can represent the models in the first class by the second class. The Wasserstein distance between the intervention distributions is used to measure the quality of the approximation. To approximate the functions in the structural equations, we need to make the following regularity assumptions on the functions. ", "page_idx": 4}, {"type": "text", "text": "Assumption 3. If $V_{i}$ is continuous, $f_{i}$ in (3) are $L_{f}$ -Lipschitz continuous. If $V_{i}$ is categorical, the propensity functions $f_{i}(P a(V_{i}),U_{V_{i}})\\ \\triangleq\\ \\mathbb{P}(V_{i}\\ =\\ j|P a(V_{i}),U_{V_{i}}),j\\ \\in\\ [n_{i}]$ are $L_{f}$ -Lipschitz continuous. There exists a constant $K>0$ such that $\\begin{array}{r}{\\operatorname*{max}_{i\\in[n_{V}],j\\in[n_{U}]}\\{\\|V_{i}\\|_{\\infty},\\|U_{j}\\|_{\\infty}\\}\\leqslant K}\\end{array}$ . ", "page_idx": 4}, {"type": "text", "text": "The following theorem summarizes our approximation error decomposition. ", "page_idx": 4}, {"type": "text", "text": "Theorem 1. Given any SCM model $\\mathcal{M}\\in\\mathcal{M}(\\mathcal{G},\\mathcal{F}_{L},U)$ , let the treatment variable set be ${\\textbf{\\em T}}=$ $\\{T_{k}\\}_{k=1}^{n_{t}}$ and suppose that Assumption $^{\\,I}$ , 2 and $^3$ hold for $\\mathcal{M}$ with Lipschitz constant $L$ , constant $K$ ", "page_idx": 4}, {"type": "image", "img_path": "GEbnPxD9EF/tmp/d5e7bf76d2ac2775da4e05d60432040ca996bc1e6f6e972f2d45a1ee7ec61621.jpg", "img_caption": ["(a) Architecture of wide neural network for $^{4-}$ dimensional output. The first (yellow) part approximates the distribution on different connected components of the support using the results from [43]. The width and depth of each block in this part are $W_{1}$ and $L_{1}$ . The second (blue) part transforms the distributions on the unit cube to the distributions on the support. The width and depth of each block in the blue part are $W_{2}$ and $L_{2}$ . The third (green) part is the Gumbel-Softmax layer. It combines the distributions on different connected components of the support together and outputs the final distribution. ", "(b) This figure demonstrates the first two parts of our architecture. Each interval in the yellow box corresponds to one coordinate of input in the left figure. We first push forward uniform distributions to different cubes. Then, using Assumption 4, we adapt the shape of the support and push the measure from unit cubes to the original support of $P(U)$ . In this way, we can approximate complicated measures by pushing forward uniform variables. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "For any intervention $\\mathbf{\\nabla}T=t$ and $\\hat{\\mathcal{M}}\\in\\mathcal{M}(\\mathcal{G},\\hat{\\mathcal{F}},\\hat{U})$ , we have ", "page_idx": 5}, {"type": "equation", "text": "$$\nW\\left(P^{\\mathcal M}(V(t)),P^{\\hat{\\mathcal M}}(\\hat{V}(t))\\right)\\leqslant C_{\\mathcal G}(L,K)\\left(\\sum_{i=1}^{n\\nu}\\|f_{i}-\\hat{f_{i}}\\|_{\\infty}+W\\left(P^{\\mathcal M}(U),P^{\\hat{\\mathcal M}}(\\hat{U})\\right)\\right),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $C_{\\mathcal{G}}(L,K)$ is a constant that only depends on $L,K$ and the causal graph $\\mathcal{G}$ and $f_{i},\\hat{f}_{i}$ are structural functions of $\\mathcal{M}$ and $\\hat{\\mathcal{M}}$ respectively. ", "page_idx": 5}, {"type": "text", "text": "Theorem 1 separates the approximation error into two parts, which motivates us to construct the NCM in the following way. First, we approximate the functions $f_{i}$ in (3) by NNs $\\hat{f}_{i}$ . Then, we approximate the distribution of latent variables by pushing forward uniform and Gumbel variables using neural networks, i.e., $\\hat{U}_{C_{j}}=\\hat{g}_{j}(Z_{C_{j}})$ , where $\\{C_{j}\\}$ are $C^{2}$ components and $Z_{C_{j}}$ are multi-variate uniform and Gumbel random variables. The structural equations of the resulting approximated model M\u02c6 are ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\hat{V}_{i}=\\left\\{\\begin{array}{l l}{\\hat{f}_{i}\\left(\\mathbf{Pa}(\\hat{V}_{i}),(\\hat{g}_{j}(Z_{C_{j}}))_{U_{C_{j}}\\in U_{V_{i}}}\\right),}&{V_{i}\\mathrm{~is~continuous,}}\\\\ {\\arg\\operatorname*{max}_{k\\in[n_{i}]}\\left\\{g_{k}+\\log\\left(\\hat{f}_{i}\\left(\\mathbf{Pa}(\\hat{V}_{i}),(\\hat{g}_{j}(Z_{C_{j}}))_{U_{C_{j}}\\in U_{V_{i}}}\\right)\\right)_{k}\\right\\},}&{V_{i}\\mathrm{~is~categorical,}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "wheres $N_{C,j}$ are constants to be specified later. ", "page_idx": 5}, {"type": "text", "text": "For the first part, we need to approximate Lipschitz continuous functions. For simplicity, we assume that the domain of the functions are uniform cubes. Similar arguments hold for any bounded cubes. We denote $\\mathcal{N N}_{k_{1},k_{2}}(W,L)$ to be the set of ReLu NNs with input dimension $k_{1}$ , output dimension $k_{2}$ , width $W$ and depth $L$ . It has been shown that $\\epsilon(\\mathcal{N N}_{k_{1},1}(2d_{1}+10,L_{0}),\\mathcal{F}_{L}([0,1]^{d_{1}},\\mathbb{R}))\\,\\leqslant$ $O(L_{0}^{-2/k_{1}})$ [53], where $\\epsilon(\\cdot,\\cdot)$ denotes the approximation error defined in Section 2. For a vector valued function, we can use a wider NN to approximate each coordinate and get a similar rate. ", "page_idx": 5}, {"type": "text", "text": "For the second part, we approximate each $U_{i}$ individually by pushing forward i.i.d. multivariate uniform and Gumbel variables $\\hat{U}_{C_{i}}=\\hat{g}_{i}(Z_{C_{i}})$ since the latent variables are independent by Assumption 1. To do so, we examine under what assumptions on the measure $\\mathbb{P}$ over $\\mathbb{R}^{n}$ we can find a $\\operatorname{NN}\\hat{g}$ such that $W(\\hat{g}_{\\#}\\lambda,\\mathbb{P})$ is small, where $\\lambda(\\cdot)$ is some reference measure. ", "page_idx": 5}, {"type": "text", "text": "3.1 Approximating Mixed Distributions by Wide Neural Networks ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this subsection, we will extend the results in [43] to construct a wide neural network as the push-forward map. It turns out that to get a good approximation of the targeted distribution, the shape of the support is essential. ", "page_idx": 5}, {"type": "text", "text": "Assumption 4 (Mixed Distribution). The support of measure $\\mathbb{P}$ has finite connected components $C_{1},C_{2}\\cdot\\cdot\\cdot C_{N_{C}}$ , i.e., $s u p p(\\mathbb{P})\\;=\\;\\cup_{i=1}^{N_{C}}C_{i},$ , and each component $C_{i}$ satisfies $\\mathcal{H}([0,1]^{d_{i}^{\\bar{C}}},C_{i})\\cap$ $\\mathcal{F}_{L}([0,1]^{d_{i}^{C}},C_{i})\\,\\ne\\,\\emptyset$ for some $d_{i}^{C}\\,\\geq\\,0$ . Recall that $\\mathcal{H}(K_{1},K_{2})$ is the set of homeomorphisms from $K_{1}$ to $K_{2}$ defined at the end of Section 2. ", "page_idx": 6}, {"type": "text", "text": "Assumption 4 encompasses almost all natural distributions. For example, distributions supported on $[0,\\dot{1}]^{d}$ and closed balls, finitely supported distributions and mixtures of them all satisfy this assumption. Assumption 4 allows us to transform the support of the targeted distribution into unit cubes and the nice geometric properties of unit cubes facilitate our construction. ", "page_idx": 6}, {"type": "text", "text": "Now, we briefly explain the construction of the push-forward maps. An example is provided in Appendix B.1 to illustrate the construction. The NN architecture consists of three parts (see Figure 1a). The input dimension is the same as the number of connected components of the support $N_{C}$ . For each component $C_{i}$ , let $H_{i}\\,\\in\\,\\mathcal{H}([0,1]^{d_{i}^{C}},C_{i})\\cap\\mathcal{F}_{L}([0,1]^{d_{i}^{C}},C_{i}).$ , where $d_{i}^{C}$ is the dimension of component $C_{i}$ in Assumption 4. By $\\mathbb{P}=(H_{i})_{\\#}(H_{i}^{-1})_{\\#}\\mathbb{P}$ on $C_{i}$ , we can approximate $(H_{i}^{-1})_{\\#}\\mathbb{P}$ first, which is supported on a unit cube. [43] constructs a wide $\\operatorname{NN}\\hat{g}$ of width $W$ and constant depth such that $\\begin{array}{r}{W(\\hat{g}_{\\#}\\lambda,(H_{i})_{\\#}^{-1}\\mathbb{P})\\leqslant O(W^{-1/d_{i}^{C}})}\\end{array}$ where $\\lambda$ is the uniform measure on $[0,1]$ . Then, we approximate the Lipschitz map $H_{i}$ to $C_{i}$ to pull the distribution back to $C_{i}$ . These are the first two parts (yellow and blue blocks in Figure 1a) of the architecture. ", "page_idx": 6}, {"type": "text", "text": "Suppose that the output of $i$ -th coordinate in the first two parts is $\\vec{v_{i}}$ , the Gumbel-softmax layer in the third part (green box) combines different components of the support. In particular, we want to output $\\vec{v_{i}}$ with probability $p_{i}=\\mathbb{P}(C_{i})$ . Let $V=\\left[\\vec{v}_{1},\\cdot\\cdot\\cdot{},\\vec{v}_{i}\\right]$ , this can be achieved by outputting $V X$ , where $X=(X_{1},\\cdot\\cdot\\cdot\\,,X_{N_{C}})^{\\sf T}$ is a one-hot random vector with $\\mathbb{P}(X_{i}=1)=p_{i}$ . To use backpropagation in training, we use the Gumbel-Softmax trick [31] to (approximately) simulate such a random vector, kNe=Cx1 pe(x(lpo(g( lpoig +pkGi+)G/\u03c4k))/\u03c4), where \u03c4 > 0 is the temperature parameter (a hyperparameter) and $G_{i}\\sim{\\mathrm{Gumbel}}(0,1)$ are i.i.d. standard Gumbel variables. As $\\tau\\rightarrow0$ , the distribution of $\\hat{X}^{\\tau}$ converges almost surely to the categorical distribution [36, Proposition 1]. In particular, when $\\tau=0$ , we denote $\\hat{X}_{i}^{0}=\\mathbb{I}_{i=\\arg\\operatorname*{max}_{j}\\{\\log p_{i}+G_{i}\\}}$ . The output of the last layer is $V\\hat{X}^{\\tau}$ . Note that the Gumbel-softmax function is differentiable with respect to parameter $\\{\\log(p_{i})\\}_{i=1,\\cdots,N_{C}}$ . Therefore, we can train the network with common gradient-based algorithms. Putting things together, we obtain the following theorem. ", "page_idx": 6}, {"type": "text", "text": "Theorem 2. Given any probability measure $\\mathbb{P}$ on $\\mathbb{R}^{d}$ that satisfies Assumption 4, let $\\lambda$ be the Lebesgue measure on $[0,\\dot{1}]^{N_{C}}$ , where $N_{C}$ is defined in Assumption 4. There exists a neural network $\\hat{g}=\\hat{g}_{3}^{\\tau}\\circ\\hat{g}_{2}\\circ\\hat{g}_{1}$ with the above architectsure (Figure 1a) such that ", "page_idx": 6}, {"type": "equation", "text": "$$\nW(\\hat{g}_{\\#}\\lambda,\\mathbb{P})\\leqslant O\\left(W_{1}^{-1/\\operatorname*{max}_{i}\\{d_{i}^{C}\\}}+L_{2}^{-2/\\operatorname*{max}_{i}\\{d_{i}^{C}\\}}+(\\tau-\\tau\\log\\tau)\\right).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Here, $\\hat{g}_{i},i=1,2$ has the separable form $\\Big(\\hat{g}_{i}^{1}(x_{1}),\\cdot\\cdot\\cdot\\cdot,\\hat{g}_{i}^{N_{C}}(x_{N_{C}})\\Big)$ and $\\hat{g}_{1}^{j}\\in\\mathcal{N N}_{1,d_{j}^{C}}(W_{1},\\Theta(d_{j}^{C})),$ $\\hat{g}_{2}^{j}\\in\\mathcal{N N}_{d_{j}^{C},d}(\\Theta(d\\cdot d_{j}^{C}),L_{2})$ , $j\\in[N_{C}]$ . $\\{d_{j}^{C}\\}$ are the dimension of cubes in Assumption 4. $\\hat{g}_{3}^{\\tau}$ is the Gumbel-Softmax layer with temperature parameter $\\tau>0$ . ", "page_idx": 6}, {"type": "text", "text": "Note that $\\hat{g}_{3}^{\\tau}$ (the Gumbel-softmax layer) is actually a random function since the coefficient vector $\\hat{X}^{\\tau}$ is a random variable. In this sense, $\\hat{g}_{\\#}\\lambda$ can be viewed as pushing forward uniform and Gumbel variables using a neural net. ", "page_idx": 6}, {"type": "text", "text": "3.2 Approximating Mixed Distributions by Deep Neural Networks ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this subsection, we will show that under one additional assumption on the distribution, deep ReLu networks have a stronger approximation power in approximating distributions, which means we can use fewer computational units to achieve the same worst-case theoretical approximation error. ", "page_idx": 6}, {"type": "text", "text": "Assumption 5 (Lower Bound). Suppose that $\\mathbb{P}$ is supported on a compact set $K\\subset\\mathbb{R}^{D}$ , there exists $a$ constant $C_{f}>0$ , $f\\in\\mathcal{H}([0,1]^{d},\\bar{K})\\cap\\mathcal{F}_{L}([0,1]^{d},\\bar{K})$ , such that for any measurable set $B\\subset[0,1]^{d}$ , $\\mathbb{P}\\left(f(B)\\right)\\geqslant C_{f}\\lambda(B)$ . Besides, if $d>0$ , $f_{\\#}\\mathbb{P}$ vanishes on the boundary $\\partial[0,1]^{d}$ . ", "page_idx": 6}, {"type": "text", "text": "Assumption 5 implies that $d\\lambda/d(f_{\\#}^{-1}\\mathbb{P})$ exists and is lowered bounded by a constant $C_{f}$ . The next proposition extends the Skorohod representation theorem [48]. It shows that under Assumption 5, it is possible to simulate any distribution on the unit cubes with H\u00f6lder continuous curves and uniform distribution on $[0,1]$ . ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Proposition 1. Let $\\lambda$ be the Lebesgue measure on $[0,1]$ . Given any probability measure $\\mathbb{P}$ that satisfies Assumption 5, there exists a continuous curve $\\gamma:[0,1]\\to s u p p(\\mathbb{P})$ such that $\\gamma_{\\#}\\lambda=\\mathbb{P}$ . Furthermore, if $d\\geqslant1$ , $\\gamma$ is $1/d$ -H\u00f6lder continuous. ", "page_idx": 7}, {"type": "text", "text": "Results from [53] show that we can approximate any H\u00f6lder continuous $d_{\\cdot}$ -dimensional function with index $\\alpha$ by a deep ReLu network with depth $L$ and error $O(L^{-2\\alpha/d})$ . Leveraging this result, we can replace the first part of the architecture in the previous subsection with deep ReLu networks (See Figure 8 in the appendix). The remaining two parts are the same as the construction in Figure 1a. The following theorem gives a sharper bound on the approximation error compared with Theorem 2. ", "page_idx": 7}, {"type": "text", "text": "Theorem 3. Under the Assumption 4, if in addition, $\\mathbb{P}$ constrained to each component satisfies Assumption 5, there exists a neural network $\\hat{g}=\\hat{g}_{3}^{\\tau}\\circ\\hat{g}_{2}\\circ\\hat{g}_{1}$ with the above architecture such that ", "page_idx": 7}, {"type": "equation", "text": "$$\nW(\\hat{g}_{\\#}\\lambda,\\mathbb{P})\\leqslant O\\left(L_{1}^{-2/\\operatorname*{max}_{i}\\{d_{i}^{C}\\}}+L_{2}^{-2/\\operatorname*{max}_{i}\\{d_{i}^{C}\\}}+(\\tau-\\tau\\log\\tau)\\right),\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $d_{i}^{C}$ are the dimensions of the connected components in Assumption 4, $\\hat{g}_{i},i=1,2$ has the form $\\Big(\\hat{g}_{i}^{1}(x_{1}),\\cdot\\cdot\\cdot\\cdot,\\hat{g}_{i}^{N_{C}}(x_{N_{C}})\\Big)$ and $\\hat{g}_{1}^{j}\\in\\mathcal{N N}_{1,d_{j}^{C}}(\\Theta(d_{j}^{C}),L_{1}).~\\hat{g}_{2}^{j},\\hat{g}_{3}^{\\tau},\\tau$ are the same as in Theorem 2. ", "page_idx": 7}, {"type": "text", "text": "Let $N$ be the number of nonzero weights in a neural network, Theorem 3 shows that a deep NN can achieve $O(N^{-2/d})$ error while by Theorem 2 a wide network can only achieve $O(N^{-1/d})$ error. Now, we can put things together to construct NCM approximations. For simplicity, we omit the input and output dimensions of the neural network. As we mention in previous sections, our construction (5) consists of two parts, $\\hat{f}_{i}$ approximating the structural functions $f_{i}$ in (3), and $\\hat{g}_{j}(Z_{j})=\\hat{g}_{3,j}^{\\tau}\\circ\\hat{g}_{2,j}\\circ\\hat{g}_{1,j}(Z_{j})$ approximating the latent variables $U_{j}$ . Let $\\mathrm{NCM}_{\\mathcal{G}}(\\mathcal{F}_{0},\\mathcal{F}_{1},\\mathcal{F}_{2},\\tau)$ be a collection of NCMs with structural equation (5) and ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\hat{f}_{i}\\in\\mathcal{F}_{0},\\hat{g}_{1,j}=(\\hat{g}_{1,j}^{1},\\cdot\\cdot\\cdot\\cdot,\\hat{g}_{1,j}^{N_{C,j}}),\\hat{g}_{1,j}^{i}\\in\\mathcal{F}_{1},\\hat{g}_{2,j}=(\\hat{g}_{2,j}^{1},\\cdot\\cdot\\cdot,\\hat{g}_{2,j}^{N_{C,j}}),\\hat{g}_{2,j}^{i}\\in\\mathcal{F}_{2},\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $N_{C,j}$ is the number of connected components for $U_{j}$ and ${\\mathcal{F}}_{i}$ are function classes. ", "page_idx": 7}, {"type": "text", "text": "Corollary 1. Given a causal model $\\mathcal{M}^{*}\\in\\mathcal{M}(\\mathcal{G},\\mathcal{F}_{L},U)$ , suppose that Assumptions 1-3 hold and the distributions of $U_{C}$ for all $C^{2}$ -component satisfy the assumptions of Theorem 3. Let $d_{\\mathrm{max}}^{i n}$ and $d_{\\mathrm{max}}^{o u t}$ be the largest input and output dimension of $f_{i}$ in (3) and $\\bar{d}_{\\mathrm{max}}^{U}$ be the largest dimension of all latent variables. There exists a neural causal model ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\hat{\\mathcal{M}}\\in N C M_{\\mathcal{G}}(N\\mathcal{N}(\\Theta(d_{\\operatorname*{max}}^{i n}d_{\\operatorname*{max}}^{o u t}),L_{0}),N\\mathcal{N}\\left(\\Theta\\left(d_{\\operatorname*{max}}^{U}\\right),L_{1}\\right),\\mathcal{N}\\mathcal{N}\\left(\\Theta\\left((d_{\\operatorname*{max}}^{U})^{2}\\right),L_{2}\\right),\\tau)\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "with structure equations (5). For any intervention $\\textstyle T=t,{\\hat{\\mathcal{M}}}$ satisfies ", "page_idx": 7}, {"type": "equation", "text": "$$\nW\\left(P^{\\mathcal{M}^{\\ast}}(V(t)),P^{\\hat{M}}(V(t))\\right)\\leqslant O(L_{0}^{-2/d_{\\operatorname*{max}}^{n}}+L_{1}^{-2/d_{\\operatorname*{max}}^{U}}+L_{2}^{-2/d_{\\operatorname*{max}}^{U}}+(\\tau-\\tau\\log\\tau)).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Similar approximation results also hold for wide NN approximation, as presented in Section 3.1. The proof can be easily adapted to the wide NNs architecture. ", "page_idx": 7}, {"type": "text", "text": "4 Consistency of Neural Causal Partial Identification ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we prove the consistency of the max/min optimization approach to partial identification.   \nIn the finite sample setting, we consider the following estimator $F_{n}$ of the optimal values of (1). ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{F_{n}=\\underset{\\hat{\\mathcal{M}}\\in\\mathrm{NCM}_{\\mathcal{G}}(\\mathcal{F}_{0,n},\\mathcal{F}_{1,n},\\mathcal{F}_{2,n},\\tau_{n})}{\\arg\\operatorname*{min}}\\mathbb{E}_{t\\sim\\mu_{T}}\\mathbb{E}_{\\hat{\\mathcal{M}}}[F(V_{1}(t),\\cdot\\cdot\\cdot,V_{n_{V}}(t))],}\\\\ {\\hat{\\mathcal{M}}\\mathrm{{eNCM}}_{\\mathcal{G}}(\\mathcal{F}_{0,n},\\mathcal{F}_{1,n},\\mathcal{F}_{2,n},\\tau_{n})\\,~~~~~~~~~~~~~~~~}\\\\ {s.t.~~S_{\\lambda_{n}}(P_{m_{n}}^{\\hat{\\mathcal{M}}}(V),{P}_{n}^{\\mathcal{M}^{*}}(V))\\leqslant\\alpha_{n},~~~~~~~~~~~~}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $P_{n}^{\\mathcal{M}^{*}},P_{m_{n}}^{\\mathcal{M}}$ are the empirical distribution of $P^{\\mathcal{M}^{\\ast}},P^{\\mathcal{M}}$ with sample size $n,m_{n},\\mu_{T}$ is some given measure and ${\\mathcal{F}}_{i,n}$ will be specified later. For example, the counterfactual outcome $\\mathbb{E}[Y(1)]$ is a special case of the objective. Our results can be easily generalized to any linear combination of objective functions of this form. We use the Sinkhorn distance because it can be computed efficiently in practice [15]. We want to study if $F_{n}$ gives a useful lower bound as $n\\to\\infty$ . ", "page_idx": 7}, {"type": "text", "text": "To match the observational distribution, we need to increase the width or depth of the NNs we use. As the sample size increases, the number of parameters also increases to infinity, which creates difficulty in the analysis. To obtain consistency, we need to regularize the functions while preserving their approximation power. Surprisingly, if we do not use any regularization, the following proposition implies that consistency may not hold even if the SCM is identifiable. ", "page_idx": 8}, {"type": "text", "text": "Proposition 2 (Informal, see Proposition 5 for a formal version). There exists a constant $c>0$ and an identifiable SCM $\\mathcal{M}^{\\ast}$ satisfying Assumptions 1-5 such that for any $\\epsilon>0,$ , there exists an SCM $\\mathcal{M}_{\\epsilon}$ satisfying $W(P^{\\mathcal{M}^{*}}(V),P^{\\mathcal{M}_{\\epsilon}}(V))\\leq\\epsilon$ and $\\left|A T E_{\\mathcal{M}^{\\ast}}-A T E_{\\mathcal{M}_{\\epsilon}}\\right|>c.$ . ", "page_idx": 8}, {"type": "text", "text": "Here, $\\mathcal{M}^{\\ast}$ is the ground-truth model and $\\mathcal{M}_{\\epsilon}$ are the models we use to approximate $\\mathcal{M}^{\\ast}$ . Proposition 2 implies that we need some regularization on $\\mathcal{M}_{\\epsilon}$ . Otherwise, even if the observation distributions are close, their ATEs can be far away. In particular, we may want to regularize the Lipschitz constant of the NN. Much work has been done to impose Lipschitz regularization during the training process [13, 50, 22, 42, 10]. We denote $\\operatorname{Lip}(f)$ to be the Lipschitz constant of a function $f$ and define the truncated Lipschitz NN class, ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\mathcal{N N}_{d_{1},d_{2}}^{L_{f},K}(W,L)=\\left\\{\\operatorname*{max}\\{-K,\\operatorname*{min}\\{f,K\\}\\}:f\\in\\mathcal{N N}_{d_{1},d_{2}}\\left(W,L\\right),\\mathrm{Lip}(f)\\leqslant L_{f}\\right\\}.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "For simplicity, we omit the dimensions and use shorthand $\\mathcal{N N}^{L_{f},K}(W,L)$ . The next theorem gives the consistency result of the min estimator. To state the theorem, we define $F_{*}\\ =$ $\\mathbb{E}_{t\\sim\\mu_{T}}\\mathbb{E}_{\\mathcal{M}^{*}}[F(V_{1}(t),\\cdot\\cdot\\cdot\\,,V_{n_{V}}(t))]$ to be the true value and $\\underline{{F}}^{L}$ to be the optimal value of the following optimization problem over SCMs with Lipschitz constant $L$ . ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\underline{{F}}^{L}=\\underset{\\hat{\\mathcal{M}}\\in\\mathcal{M}\\big(\\mathcal{G},\\mathcal{F}_{L}^{K},U\\big),P(U)}{\\arg\\operatorname*{min}}\\mathbb{E}_{t\\sim\\mu_{T}}\\mathbb{E}_{\\hat{\\mathcal{M}}}[F(V_{1}(t),\\cdot\\cdot\\cdot\\cdot,V_{n_{V}}(t))],}\\\\ {s.t.\\mathcal{W}(P^{\\hat{\\mathcal{M}}}(V),P^{\\mathcal{M}^{*}}(V))=0,\\qquad\\qquad}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where the minimum is taken over $\\mathcal{M}\\left(\\mathcal{G},\\mathcal{F}_{L}^{K},U\\right)$ with $\\mathcal{F}_{L}^{K}\\,=\\,\\{f\\,:\\,\\|f\\|\\,\\leqslant\\,K,f\\,\\in\\,\\mathcal{F}_{L}\\}$ and all latent distributions $P(U)$ . Note that if $L$ is the Lipschitz bound $L_{f}$ that we assume on our structural functions, then $\\underline{{F}}^{L}$ is the sharp lower bound. ", "page_idx": 8}, {"type": "text", "text": "Theorem 4. Let $\\mathcal{M}^{\\ast}$ be any SCM satisfying the assumptions of Corollary $^{\\,l}$ . Suppose that the Lipschitz constant of functions in $\\mathcal{M}$ is $L_{f}$ , $F:\\mathbb{R}^{n_{V}}\\rightarrow\\mathbb{R}$ in (6) is Lipschitz continuous and $\\tau_{n}>0$ , let $K>0$ be the constant in Assumption 3, $\\begin{array}{r}{\\hat{L}_{f}=\\sqrt{d_{\\mathrm{max}}^{i n}d_{\\mathrm{max}}^{o u t}}L_{f},}\\end{array}$ , ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{F}_{0,n}=\\!\\!\\mathcal{N}\\!\\mathcal{N}^{\\hat{L}_{f},K}\\left(W_{0,n},\\Theta\\left(\\log d_{\\operatorname*{max}}^{i n}\\right)\\right),\\mathcal{F}_{1,n}=\\!\\mathcal{N}\\!\\mathcal{N}^{\\infty,\\infty}(\\Theta(d_{\\operatorname*{max}}^{U}),L_{i,n}),}\\\\ &{\\mathcal{F}_{2,n}=\\!\\!\\mathcal{N}\\!\\mathcal{N}^{\\infty,K}(\\Theta((d_{\\operatorname*{max}}^{U})^{2}),L_{i,n}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $d_{\\operatorname*{max}}^{U},d_{\\operatorname*{max}}^{i n}$ and $d_{\\mathrm{max}}^{o u t}$ are defined in Corollary $^{\\,l}$ , take the radius to be $\\alpha_{n}=\\epsilon_{n}+s_{n},\\epsilon_{n}=$ $\\begin{array}{r}{O(W_{0,n}^{-1/d_{\\mathrm{max}}^{i n}}+\\sum_{i=1}^{2}L_{i,n}^{-2/d_{\\mathrm{max}}^{U}}+\\tau_{n}\\log\\tau_{n})}\\end{array}$ ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ddots={\\cal O}(m_{n}^{-1/\\left(d_{\\operatorname*{max}}^{U}+2\\right)}\\log m_{n}+\\delta_{n}+\\log(n m_{n})\\lambda_{n}),\\quad\\delta_{n}={\\cal O}(n^{-1/\\operatorname*{max}\\{2,d_{\\operatorname*{max}}^{U}\\}}\\log^{2}(n)).}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad}\\\\ &{\\qquad\\qquad\\quad n=\\Omega(n),L_{i,n}=\\Theta(m_{n}^{d_{\\operatorname*{max}}^{U}\\mathord{\\left/{\\vphantom{0}}\\right.\\kern-\\nulldelimiterspace}2d_{\\operatorname*{max}}^{U}+4\\right)},i=1,2,\\operatorname*{lim}_{n\\to\\infty}\\operatorname*{min}\\{\\tau_{n}^{-1},W_{0,n},(\\log(n m_{n})\\lambda_{n})^{-1}\\}=}\\\\ &{\\qquad\\qquad\\quad t h e n\\ w i t h\\ p r o b a b i l i t y\\ I,\\ [\\operatorname*{lim}\\operatorname*{linf}_{n\\to\\infty}F_{n},\\operatorname*{lim}\\operatorname*{sup}_{n\\to\\infty}F_{n}]\\subset[{\\underline{{F}}}^{\\hat{L}_{f}},F_{*}].}\\end{array}\n$$$\\infty$ ", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Note that our theorem shows that the lower limit of the solution is large than the point $\\underline{{F}}^{\\hat{L}_{f}}$ , where $\\hat{L}_{f}$ is slightly larger than the original constraint $L_{f}$ we impose on the structural functions. Hence, this point can potentially be slightly smaller than the sharp bound $\\underline{{F}}^{L_{f}}$ . This worsening is due to the fact that we need to use NNs that satisfy a slightly worse Lipschitz property, to ensure that we have sufficient approximation power. Although Theorem 4 does not guarantee $\\left\\{\\boldsymbol{F_{n}}\\right\\}$ converges to a point, it states that $\\{\\boldsymbol{F}_{n}\\}$ may oscillate in the interval $[\\underline{{F}}^{\\hat{L}_{f}},F_{*}]$ , which will still give a useful lower bound to ground-truth value $F_{*}$ . In particular, if the graph is identifiable, we have $F^{*}=\\underline{{F}}^{\\hat{L}_{f}}$ and $\\{\\boldsymbol{F}_{n}\\}$ converges to $F^{*}$ . Also, note that in Theorem 4, we use wide NNs rather than deep NN for ${\\mathcal{F}}_{0,n}$ because results in [53] show that wide NNs can approximate Lipschitz functions while controlling their Lipschitz constants (a property that is not yet established for deep NNs). Similar results can be obtained if wide neural nets are used for all components, invoking Theorem 2. ", "page_idx": 8}, {"type": "text", "text": "As a special case, we leverage Theorem 4 for a non-asymptotic rate for the ATE without confounding. ", "page_idx": 8}, {"type": "text", "text": "Proposition 3 (H\u00f6lder continuity of ATE). Given two causal models $\\mathcal{M},\\hat{\\mathcal{M}}\\,\\in\\,\\mathcal{M}(\\mathcal{G},\\mathcal{F}_{L},U)$ satisfying Assumption 1 and Assumption $^3$ , let their observational distributions be $\\nu,\\mu$ . Suppose the norms of all variables are bounded by $K>0$ . $H(l)$ (Overlap) $\\nu$ is absolutely continuous with respect to one probability measure $P$ and the density $p_{\\nu}$ $(t|P a(T)=x)\\geqslant\\delta>0$ for $x$ almost surely and $t\\in[t_{1},t_{2}]$ and (2) (No Confounding) there is no confounding in the causal graph $\\mathcal{G}$ , we have ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\int_{t_{1}}^{t_{2}}(\\mathbb{E}_{M}[Y(t)]-\\mathbb{E}_{\\hat{M}}[\\hat{Y}(t)])^{2}P(d t)\\leqslant\\frac{2C_{W}}{\\delta}W(\\mu,\\nu)+2(L+1)^{n\\nu}W^{2}(\\mu,\\nu)(t_{2}-t_{1}),\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "where $C_{W}=4(L+1)^{n_{V}}K+2K\\operatorname*{max}{\\{(L+1)^{n_{V}},1\\}}.$ ", "page_idx": 9}, {"type": "text", "text": "Corollary 2. Let $F,\\mu_{T}$ in Theorem $^{4}$ to be ${\\cal F}(V)=Y,\\mu_{T}\\sim U n i f([t_{1},t_{2}]),\\epsilon>0.$ . Suppose that the assumptions in Proposition $^3$ and Theorem 4 hold, with probability at least $1-O(n^{-2})$ , we have $|F_{n}-\\^{\\prime}F_{*}|\\leqslant O(\\sqrt{\\alpha_{n}}),$ , where $F_{n},F_{*},\\alpha_{n}$ are defined in Theorem 4. ", "page_idx": 9}, {"type": "text", "text": "4.1 Experiments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this section, we examine the performance of our algorithm in two settings. We compare our algorithm with the Autobounds algorithm [14] in a binary IV example in [14] and in a continuous IV model 1. Since Autobounds can only deal with discrete models, we discretize the continuous model for comparison purposes. The implementation details are provided in the Appendix D. The setting of the first experiment is taken from [14, Section D.1]. This is a binary IV problem and we can calculate the optimal bound using LP [4]. We find that our bound is close to the optimal bound. The second experiment is a general IV example where the treatment is binary but the rest of the variables are continuous. The program that Autobounds solves after discretization contains $\\approx2^{14}$ variables. Even with such a fine discretization, the bound obtained by Autobounds is not tighter than our NCM approach. The details of the structural equations and analysis can be found in Appendix D. We also provide an extra experiment on the counterexample of Proposition 5 in the appendix to show the effect of Lipschitz regularization. ", "page_idx": 9}, {"type": "table", "img_path": "GEbnPxD9EF/tmp/e27746af5ad0521b4a15a6f6cf91b882000ee0e1108695d7c0cc9d455a0098bb.jpg", "table_caption": [], "table_footnote": ["Table 1: Experiment results of 2 IV settings. The sample sizes are taken to be 5000 in each experiment. STD is the standard derivation. The experiments are repeated 10 times for binary IV and 50 times for continuous IV. In all experiments, the bounds given by both algorithms all cover the true values. "], "page_idx": 9}, {"type": "text", "text": "Conclusion In this paper, we provide theoretical justification for using NCMs for partial identification. We show that NCMs can be used to represent SCMs with complex unknown latent distributions under mild assumptions and prove the asymptotic consistency of the max/min estimator for partial identification of causal effects in general settings with both discrete and continuous variables. Our results also provide guidelines on the practical implementation of this method and on what hyperparameters are important, as well as recommendations on values that these hyperparameters should take for the consistency of the method. These practical guidelines were validated with a small set of targeted experiments, which also showcase superior performance of the neural-causal approach as compared to a prior main contender approach from econometrics and statistics, that involves discretization and polynomial programming. ", "page_idx": 9}, {"type": "text", "text": "An obvious next step in the theoretical foundation of neural-causal models is providing finite sample guarantees for this method, which requires substantial further theoretical developments in the understanding of the geometry of the optimization program that defines the bounds on the causal effect of interest. We take a first step in that direction for the special case, when there are no unobserved confounders and view the general case as an exciting avenue for future work. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgement Vasilis Syrgkanis is supported by NSF Award IIS-2337916 and a 2022 Amazon Research Award. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Joshua Angrist and Guido Imbens. Identification and estimation of local average treatment effects, 1995. [2] Martin Anthony, Peter L Bartlett, Peter L Bartlett, et al. Neural network learning: Theoretical foundations, volume 9. cambridge university press Cambridge, 1999. [3] Vahid Balazadeh, Vasilis Syrgkanis, and Rahul G. Krishnan. Partial identification of treatment effects with implicit generative models, October 2022. [4] Alexander Balke and Judea Pearl. Counterfactual probabilities: Computational methods, bounds and applications. In Uncertainty Proceedings 1994, pages 46\u201354. Elsevier, 1994. [5] Alexander Balke and Judea Pearl. Bounds on treatment effects from studies with imperfect compliance. Journal of the American Statistical Association, 92(439):1171\u20131176, September 1997. [6] Peter L Bartlett, Nick Harvey, Christopher Liaw, and Abbas Mehrabian. Nearly-tight vcdimension and pseudodimension bounds for piecewise linear neural networks. Journal of Machine Learning Research, 20(63):1\u201317, 2019.   \n[7] Alexis Bellot. Towards bounding causal effects under markov equivalence. arXiv preprint arXiv:2311.07259, 2023. [8] Blai Bonet. Instrumentality tests revisited, January 2013. [9] L\u00e9on Bottou, Jonas Peters, Joaquin Qui\u00f1onero-Candela, Denis X Charles, D Max Chickering, Elon Portugaly, Dipankar Ray, Patrice Simard, and Ed Snelson. Counterfactual reasoning and learning systems: The example of computational advertising. Journal of Machine Learning Research, 14(11), 2013.   \n[10] Leon Bungert, Ren\u00e9 Raab, Tim Roith, Leo Schwinn, and Daniel Tenbrinck. CLIP: Cheap Lipschitz Training of Neural Networks. In Abderrahim Elmoataz, Jalal Fadili, Yvain Qu\u00e9au, Julien Rabin, and Lo\u00efc Simon, editors, Scale Space and Variational Methods in Computer Vision, volume 12679, pages 307\u2013319, Cham, 2021. Springer International Publishing.   \n[11] Victor Chernozhukov, Han Hong, and Elie Tamer. Estimation and confidence regions for parameter sets in econometric models 1. Econometrica, 75(5):1243\u20131284, 2007.   \n[12] Lenaic Chizat, Pierre Roussillon, Flavien L\u00e9ger, Fran\u00e7ois-Xavier Vialard, and Gabriel Peyr\u00e9. Faster wasserstein distance estimation with the sinkhorn divergence. Advances in Neural Information Processing Systems, 33:2257\u20132269, 2020.   \n[13] Moustapha Cisse, Piotr Bojanowski, Edouard Grave, Yann Dauphin, and Nicolas Usunier. Parseval networks: Improving robustness to adversarial examples. In International conference on machine learning, pages 854\u2013863. PMLR, 2017.   \n[14] Guilherme Duarte, Noam Finkelstein, Dean Knox, Jonathan Mummolo, and Ilya Shpitser. An automated approach to causal inference in discrete settings, September 2021.   \n[15] Jean Feydy. Geometric data analysis, beyond convolutions. Applied Mathematics, 2020.   \n[16] Jean Feydy, Thibault S\u00e9journ\u00e9, Fran\u00e7ois-Xavier Vialard, Shun-ichi Amari, Alain Trouve, and Gabriel Peyr\u00e9. Interpolating between optimal transport and mmd using sinkhorn divergences. In The 22nd International Conference on Artificial Intelligence and Statistics, pages 2681\u20132690, 2019.   \n[17] Nicolas Fournier and Arnaud Guillin. On the rate of convergence in wasserstein distance of the empirical measure. Probability theory and related fields, 162(3-4):707\u2013738, 2015.   \n[18] Dennis Frauen, Tobias Hatt, Valentyn Melnychuk, and Stefan Feuerriegel. Estimating average causal effects from patient trajectories. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 7586\u20137594, 2023.   \n[19] Dennis Frauen, Fergus Imrie, Alicia Curth, Valentyn Melnychuk, Stefan Feuerriegel, and Mihaela van der Schaar. A Neural Framework for Generalized Causal Sensitivity Analysis, April 2024.   \n[20] David A Freedman. Statistical models and causal inference: a dialogue with the social sciences. Cambridge University Press, 2010.   \n[21] Lee-Ad Gottlieb, Aryeh Kontorovich, and Robert Krauthgamer. Efficient regression in metric spaces via approximate lipschitz extension. IEEE Transactions on Information Theory, 63(8):4838\u20134849, 2017.   \n[22] Henry Gouk, Eibe Frank, Bernhard Pfahringer, and Michael J. Cree. Regularisation of neural networks by enforcing Lipschitz continuity. Machine Learning, 110(2):393\u2013416, February 2021.   \n[23] Henry Gouk, Eibe Frank, Bernhard Pfahringer, and Michael J Cree. Regularisation of neural networks by enforcing lipschitz continuity. Machine Learning, 110:393\u2013416, 2021.   \n[24] Florian Gunsilius. A path-sampling method to partially identify causal effects in instrumental variable models, June 2020.   \n[25] Chris H. Hamilton and Andrew Rau-Chaplin. Compact hilbert indices for multi-dimensional data. In First International Conference on Complex, Intelligent and Software Intensive Systems (CISIS\u201907), pages 139\u2013146. IEEE.   \n[26] Sukjin Han and Shenshen Yang. A computational approach to identification of treatment effects for policy evaluation. Journal of Econometrics, 240(1):105680, 2024.   \n[27] James J. Heckman and Edward J. Vytlacil. Instrumental variables, selection models, and tight bounds on the average treatment effect. In Wolfgang Franz, Michael Lechner, and Friedhelm Pfeiffer, editors, Econometric Evaluation of Labour Market Policies, volume 13, pages 1\u201315. Physica-Verlag HD, Heidelberg, 2001.   \n[28] David Hilbert. \u00dcber die stetige abbildung einer linie auf ein fl\u00e4chenst\u00fcck. Dritter Band: Analysis\u00b7 Grundlagen der Mathematik\u00b7 Physik Verschiedenes: Nebst Einer Lebensgeschichte, pages 1\u20132, 1935.   \n[29] Yaowei Hu, Yongkai Wu, Lu Zhang, and Xintao Wu. A generative adversarial framework for bounding confounded causal effects. Proceedings of the AAAI Conference on Artificial Intelligence, 35(13):12104\u201312112, May 2021.   \n[30] Yaowei Hu, Yongkai Wu, Lu Zhang, and Xintao Wu. A generative adversarial framework for bounding confounded causal effects. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 12104\u201312112, 2021.   \n[31] Eric Jang, S. Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. ArXiv, abs/1611.01144, 2016.   \n[32] Niki Kilbertus, Matt J. Kusner, and Ricardo Silva. A class of algorithms for general instrumental variable models, October 2020.   \n[33] Murat Kocaoglu, Christopher Snyder, Alexandros G Dimakis, and Sriram Vishwanath. Causalgan: Learning causal implicit generative models with adversarial training. arXiv preprint arXiv:1709.02023, 2017.   \n[34] Mohammad Lotfollahi, Anna Klimovskaia Susmelj, Carlo De Donno, Yuge Ji, Ignacio L Ibarra, F Alexander Wolf, Nafissa Yakubova, Fabian J Theis, and David Lopez-Paz. Compositional perturbation autoencoder for single-cell response modeling. BioRxiv, 2021.   \n[35] Giulia Luise, Alessandro Rudi, Massimiliano Pontil, and Carlo Ciliberto. Differential Properties of Sinkhorn Approximation for Learning with Wasserstein Distance, May 2018.   \n[36] Chris J. Maddison, Andriy Mnih, and Yee Whye Teh. The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables, March 2017.   \n[37] Charles F Manski. Nonparametric bounds on treatment effects. The American Economic Review, 80(2):319\u2013323, 1990.   \n[38] Chengzhi Mao, Augustine Cha, Amogh Gupta, Hao Wang, Junfeng Yang, and Carl Vondrick. Generative interventions for causal learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3947\u20133956, 2021.   \n[39] Valentyn Melnychuk, Dennis Frauen, and Stefan Feuerriegel. Partial Counterfactual Identification of Continuous Outcomes with a Curvature Sensitivity Model. Advances in Neural Information Processing Systems, 36:32020\u201332060, December 2023.   \n[40] Caleb H. Miles, Phyllis Kanki, Seema Meloni, and Eric J. Tchetgen Tchetgen. On Partial Identification of the Pure Direct Effect, September 2015.   \n[41] Kirtan Padh, Jakob Zeitler, David Watson, Matt Kusner, Ricardo Silva, and Niki Kilbertus. Stochastic causal programming for bounding treatment effects. In Conference on Causal Learning and Reasoning, pages 142\u2013176. PMLR, 2023.   \n[42] Patricia Pauli, Anne Koch, Julian Berberich, Paul Kohler, and Frank Allg\u00f6wer. Training robust neural networks using Lipschitz bounds. IEEE Control Systems Letters, 6:121\u2013126, 2021.   \n[43] Dmytro Perekrestenko, L\u00e9andre Eberhard, and Helmut B\u00f6lcskei. High-Dimensional Distribution Generation Through Deep Neural Networks, August 2022.   \n[44] Roland R. Ramsahai. Causal bounds and observable constraints for non-deterministic models. Journal of Machine Learning Research, 13(29):829\u2013848, 2012.   \n[45] Amy Richardson, Michael G. Hudgens, Peter B. Gilbert, and Jason P. Fine. Nonparametric Bounds and Sensitivity Analysis of Treatment Effects. Statistical Science, 29(4), November 2014.   \n[46] Michael C. Sachs, Gustav Jonzon, Arvid Sj\u00f6lander, and Erin E. Gabriel. A general method for deriving tight symbolic bounds on causal effects, December 2021.   \n[47] E. Shchepin. On h\u00f6lder maps of cubes. Mathematical Notes, 87:757\u2013767, 2010.   \n[48] Anatoly V Skorokhod. Limit theorems for stochastic processes. Theory of Probability & Its Applications, 1(3):261\u2013290, 1956.   \n[49] Jin Tian and Judea Pearl. On the testable implications of causal models with hidden variables, December 2002.   \n[50] Aladin Virmaux and Kevin Scaman. Lipschitz regularity of deep neural networks: Analysis and efficient estimation. Advances in Neural Information Processing Systems, 31, 2018.   \n[51] Kevin Xia, Kai-Zhan Lee, Yoshua Bengio, and Elias Bareinboim. The causal-neural connection: Expressiveness, learnability, and inference, October 2022.   \n[52] Kevin Xia, Yushu Pan, and Elias Bareinboim. Neural causal models for counterfactual identification and estimation. arXiv preprint arXiv:2210.00035, 2022.   \n[53] D. Yarotsky. Optimal approximation of continuous functions by very deep relu networks. ArXiv, abs/1802.03620, 2018.   \n[54] Junzhe Zhang and Elias Bareinboim. Bounding causal effects on continuous outcome. Proceedings of the AAAI Conference on Artificial Intelligence, 35(13):12207\u201312215, May 2021.   \n[55] Junzhe Zhang and Elias Bareinboim. Non-parametric methods for partial identification of causal effects. Columbia CausalAI Laboratory Technical Report, 2021.   \n[56] Junzhe Zhang, Jin Tian, and Elias Bareinboim. Partial counterfactual identification from observational and experimental data, October 2021. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Illustration of Notions in Section 2 ", "text_level": 1, "page_idx": 13}, {"type": "image", "img_path": "GEbnPxD9EF/tmp/b8d6dfcea22c72b1ea261b04d0eb970795a2e13de8e488408bbad2e3309db070.jpg", "img_caption": ["Figure 2: An SCM example. "], "img_footnote": [], "page_idx": 13}, {"type": "image", "img_path": "GEbnPxD9EF/tmp/075b6e23d30736f36a53c113c788f5fb137d25b006c813981044e2eb9aa509ef.jpg", "img_caption": ["Figure 3: The causal graph of this SCM. "], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "To further explain the notions in Section 2, we consider the following example. Let $\\mathcal{M}$ be an SCM with the following structure equations. ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V_{1}=f_{1}(V_{2},U_{1}),}\\\\ &{V_{2}=f_{2}(U_{1},U_{2}),}\\\\ &{V_{3}=f_{3}(V_{1},V_{2},U_{2},U_{4}),}\\\\ &{V_{4}=f_{4}(V_{3}.U_{3},U_{4}),}\\\\ &{V_{5}=f_{5}(V_{4},U_{3}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $U_{1}$ and $U_{2}$ are correlated and $U_{3},U_{4}$ and $(U_{1},U_{2})$ are independent. The causal model is shown in Figure 2 and its causal graph is shown in Figure 3. In this example, $U_{V_{1}}=\\{U_{1}\\},U_{V_{2}}=$ $\\{U_{1},U_{2}\\},U_{V_{3}}\\,=\\,\\{U_{2},U_{4}\\},U_{V_{4}}\\,=\\,\\{U_{3},U_{4}\\},U_{V_{5}}\\,=\\,\\{U_{3}\\}.$ . Since $U_{1},U_{2}$ are correlated, $C_{1}=$ $\\left\\{V_{1},V_{2},V_{3}\\right\\}$ is one $\\stackrel{\\cdot}{C}^{2}$ component because all nodes in $C_{1}$ are connected by bi-directed edges. Note that $\\{V_{1},V_{2},V_{3},V_{4}\\}$ is not a $C^{2}$ component because $V_{4}$ and $V_{2}$ is not connected by any bi-directive edge. The rest of $C^{2}$ components are $C_{2}=\\{V_{4},V_{5}\\},C_{3}=\\{V_{3},V_{4}\\}$ . ", "page_idx": 13}, {"type": "text", "text": "Now, we consider the intervention $V_{1}=t$ . Under this intervention, the structure equations can be obtained by setting $V_{1}=t$ while keeping all other equations unchanged, i.e., ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V_{1}(t)=t,}\\\\ &{V_{2}(t)=f_{2}(U_{1},U_{2}),}\\\\ &{V_{3}=f_{3}(V_{1},V_{2},U_{2},U_{4}),}\\\\ &{V_{4}=f_{4}(V_{3}.U_{3},U_{4}),}\\\\ &{V_{5}(t)=f_{5}(V_{4},U_{3}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "The figure of this model under intervention is shown in Figure 4. ", "page_idx": 13}, {"type": "image", "img_path": "GEbnPxD9EF/tmp/a621f028d20152d70d9a627b1e66abb750efb89723dcaf4496cbc49f8f6e5621.jpg", "img_caption": ["Figure 4: The SCM after intervention $V_{1}=t$ . "], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "B Proof of approximation Theorems ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "B.1 Illustration of how to construct canonical representations and neural architectures ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this section, we illustrate how to construct canonical representations and neural architectures given a causal graph via a simple example. We consider the example in the previous section. The causal graph is shown in Figure 3. ", "page_idx": 14}, {"type": "image", "img_path": "GEbnPxD9EF/tmp/ed4f101301942c90469882df5d844b8f5616c8974951772cc807a1a547bd5f29.jpg", "img_caption": ["Figure 5: The canonical representation of Figure 3. "], "img_footnote": [], "page_idx": 14}, {"type": "image", "img_path": "GEbnPxD9EF/tmp/905fdab4006e6333c27634ede1a62ee771246679da3d45e21a7f81e15f1c432c.jpg", "img_caption": ["Figure 6: The neural network architecture. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "Following the construction in Proposition 4, we use one latent variable for each $C^{2}$ component. As we explain in Appendix A, this causal model has three $C^{2}$ components, $\\{V_{1},V_{2},V_{3}\\},\\{V_{3},\\bar{V}_{4}\\},\\{V_{4},V_{5}\\}$ . In the canonical representation, exactly the latent variables enter their corresponding $C^{2}$ component. The structure equation of this SCM is as follows. ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V_{1}=f_{1}(V_{2},E_{1}),}\\\\ &{V_{2}=f_{2}(E_{1}),}\\\\ &{V_{3}=f_{3}(V_{1},V_{2},E_{1},E_{2}),}\\\\ &{V_{4}=f_{4}(V_{3}.E_{2}),}\\\\ &{V_{5}=f_{5}(V_{4},E_{3}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "If we set $E_{1}=(U_{1},U_{2}),E_{2}=U_{4},E_{3}=U_{3}$ , (10) is equivalent to (8). Therefore, we can see from this example that the canonical representation does not lose any information about the SCM. ", "page_idx": 14}, {"type": "text", "text": "Now, we show how to construct the NCM architecture from a canonical representation. As we mentioned in Section 3, we approximate the latent distribution by pushing forward uniform and Gumbel variables. The structure equation of the NCM is ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V_{1}=f_{1}^{\\theta_{1}}\\big(V_{2},g_{1}^{\\theta_{1}}(Z_{1})\\big),}\\\\ &{V_{2}=f_{2}^{\\theta_{2}}\\big(g_{1}^{\\theta_{1}}(Z_{1})\\big),}\\\\ &{V_{3}=f_{3}^{\\theta_{2}}\\big(V_{1},V_{2},g_{1}^{\\theta_{1}}(Z_{1}),g_{2}^{\\theta_{1}}(Z_{2})\\big),}\\\\ &{V_{4}=f_{4}^{\\theta_{4}}\\big(V_{3}.g_{2}^{\\theta_{1}}(Z_{2})\\big),}\\\\ &{V_{5}=f_{5}^{\\theta_{5}}\\big(V_{4},g_{2}^{\\theta_{3}}(Z_{3})\\big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $f_{i}^{\\theta_{i}},g_{j}^{\\theta_{j}}$ are neural networks, $Z_{i}$ are join distribution of independent uniform and Gumbel variables, and $g_{j}^{\\theta_{j}}$ has the special architecture described in Section 3.1 and Section 3.2. Figure 6 shows the architecture of the NCM. Each in-edge represents an input of a neural net. ", "page_idx": 14}, {"type": "text", "text": "B.2 Proof of Theorem 1 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The causal graph $\\mathcal{G}$ does not specify how latent variables influence the observed variables. There could be many ways of recovering the latent variables from a graph $\\mathcal{G}$ . Figure 7 shows an example where two different causal models have the same causal graph. The next proposition gives a canonical representation of a causal model. ", "page_idx": 14}, {"type": "image", "img_path": "GEbnPxD9EF/tmp/6bc117d13abdce9cb29c7beecc3419a2d0feedababf023ca014a74fcd5c6b6df.jpg", "img_caption": ["Figure 7: Example: two different SCMs with the same causal graph. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "Proposition 4. Suppose that Assumption 1 and Assumption 2 hold, given any SCM M with causal graph $\\mathcal{G}$ and latent variables $U$ , we can construct a canonical SCM $\\bar{\\mathcal{M}}$ of the form (3) by merging the latent variables in $U$ such that $\\mathcal{M}$ and $\\hat{\\mathcal{M}}$ produce the same intervention results. Besides, functions in $\\hat{\\mathcal{M}}$ have the same smoothness as $\\mathcal{M}$ . ", "page_idx": 15}, {"type": "text", "text": "Proof of Proposition $^{4}$ . Let $G\\,=\\,\\{G_{V_{i}}:V_{i}$ is categorical} and the latent variables in the original model $\\mathcal{M}$ be $U=\\{U_{1},\\cdot\\cdot\\cdot,U_{n_{U}}\\}\\cup G$ . By Assumption 1 and Assumption 2, $U$ are independent and the structure equations of $\\mathcal{M}$ have the form ", "page_idx": 15}, {"type": "equation", "text": "$$\nV_{i}=\\left\\{\\begin{array}{l l}{f_{i}\\left(\\mathrm{Pa}(V_{i}),U_{V_{i}}\\right),}&{V_{i}\\mathrm{~is~continuous,}}\\\\ {\\mathrm{arg\\,max}_{k\\in[n_{i}]}\\left\\{g_{k}^{V_{i}}+\\log\\left(f_{i}\\left(\\mathrm{Pa}(V_{i}),U_{V_{i}}\\right)\\right)_{k}\\right\\},}&{V_{i}\\mathrm{~is~categorical,}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $g_{k}^{V_{i}}$ are i.i.d. standard Gumbel variables and $U_{V_{i}}~\\subset~\\{U_{1},\\cdots,U_{n_{U}}\\}$ contains the latent variables that affect $V_{i}$ . We regroup and merge the variables $U_{i}$ to make the model have a canonical form while not changing the functions in the structure equations. Let $D_{1},\\cdot\\cdot\\cdot\\,,D_{n_{C}}\\subset V$ be the $C^{2}$ -component of $\\mathcal{M}$ . For each $U_{i}$ , we define the vertices that are affected by $U_{i}$ as ", "page_idx": 15}, {"type": "equation", "text": "$$\nI(U_{i})=\\{V_{j}:U_{i}\\in U_{V_{j}}\\}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We partition $\\{U_{1},\\cdot\\cdot\\cdot,U_{n_{U}}\\}$ into $n_{C}$ sets $\\hat{U}_{1},\\cdot\\cdot\\cdot,\\hat{U}_{n c}$ in the following way. For each $U_{k},U_{k}$ is in the set $\\hat{U}_{i}$ if $I(U_{k})\\subset D_{i}$ . If there are two components $D_{i},D_{j}$ satisfy the condition, we put $U_{k}$ into either of the sets $\\hat{U}_{i},\\hat{U}_{j}$ . Let $\\hat{U}_{i}$ have the same distribution as the joint distribution of the random variables in set $\\hat{U}_{i}$ . Let $\\hat{\\mathcal{M}}\\in\\mathcal{M}(\\mathcal{G},\\mathcal{F},\\hat{U})$ the SCM with structure equations ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\gamma_{i}=\\left\\{f_{i}\\left(\\mathbf{P}\\mathbf{a}(\\hat{V}_{i}),\\hat{U}_{k_{1}},\\cdots,\\hat{U}_{k_{n_{i}}}\\right),\\right.\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad V_{i}\\mathrm{~is~continuous},}\\\\ {\\left.\\quad\\quad\\quad\\mathrm{arg}\\operatorname*{max}_{k\\in[n_{i}]}\\left\\{g_{k}^{V_{i}}+\\log\\left(f_{i}\\left(\\mathbf{P}\\mathbf{a}(\\hat{V}_{i}),\\hat{U}_{k_{1}},\\cdots,\\hat{U}_{k_{n_{i}}}\\right)\\right)_{k}\\right\\},\\quad V_{i}\\mathrm{~is~categorical},}\\end{array}\\right.\\hat{U}_{k_{i}\\cap}\\cap U_{V_{i}}\\mathrm{~,}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Here, we slightly abuse the notation $f_{i}.~f_{i}$ ignores inputs from $\\hat{U}_{k_{1}},\\cdot\\cdot\\cdot,\\hat{U}_{k_{n_{i}}}$ that are not in $U_{V_{i}}$ . Note that $\\hat{U}_{1},\\cdot\\cdot\\cdot\\,,\\hat{U}_{n_{C}},G$ has the same distribution as $U$ because we only merge some latent variables. In addition, the functions in the new model $\\hat{\\mathcal{M}}$ has the same smoothness as the original model. ", "page_idx": 15}, {"type": "text", "text": "We first verify that the causal graph of $\\hat{\\mathcal{M}}$ is $\\mathcal{G}$ . If there is a bi-directed arrow between nodes $V_{i},V_{j}$ in $\\mathcal{G}$ , by the independence assumption, there must be one latent variable $U_{k}\\in\\pmb{U}_{V_{i}}\\cap\\pmb{U}_{V_{j}}$ . There exist one $D_{l}$ such that $I(U_{k})\\subset D_{l}$ and $\\hat{U}_{l}\\cap{\\pmb U}_{V_{i}}\\neq\\emptyset,\\hat{U}_{l}\\cap{\\pmb U}_{V_{j}}\\neq\\emptyset.$ . Therefore, $\\hat{U_{l}}$ will affect both $V_{i}$ and $V_{j}$ and there is a bi-directed arrow between node $V_{i},V_{j}$ in $\\mathcal{G}_{\\hat{\\mathcal{M}}}$ . Suppose that there is a bi-directed arrow between node $V_{i},V_{j}$ in $\\mathcal{G_{A}}$ , it means there exist a $\\hat{U_{k}}$ such that $\\hat{U}_{k}\\cap{\\pmb U}_{V_{i}}\\neq\\emptyset,\\hat{U}_{k}\\cap{\\pmb U}_{V_{j}}\\neq\\emptyset$ Let $U_{l_{1}}\\in\\hat{U}_{k}\\cap U_{V_{i}},U_{l_{2}}\\in\\hat{U}_{k}\\cap U_{V_{j}}$ , then ", "page_idx": 15}, {"type": "equation", "text": "$$\nV_{i}\\in I(U_{l_{1}})\\subset D_{k},V_{j}\\in I(U_{l_{2}})\\subset D_{k}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Since $D_{k}$ is a $C^{2}$ -component, there exist a bi-directed arrow between node $V_{i},V_{j}$ in $\\mathcal{G}$ . Therefore, G = G M\u02c6. ", "page_idx": 15}, {"type": "text", "text": "Finally, we verify that $\\mathcal{M}$ and $\\hat{\\mathcal{M}}$ produce the same intervention results. The intervention distribution can be viewed as the push-forward of the latent distribution, i.e., $V=F(U)$ , and the intervention operation only changes the function $F$ . In our construction, we only merge the latent variables and leave the functions in structure equations being the same (except that they may ignore some coordinates in input). For any intervention $T=t$ , suppose in $\\mathcal{M}$ we have $\\pmb{V}(t)\\dot{=}\\-\\dot{F_{t}}(\\pmb{\\bar{U}})$ . Then, in $\\hat{\\mathcal{M}}$ , we get $\\hat{V}(t)=F_{t}(\\hat{U})$ . Since $U$ and $\\hat{U}$ has the same distribution, the distribution of $\\dot{V}(t),\\hat{V}(t)$ are the same. \u53e3 ", "page_idx": 16}, {"type": "text", "text": "Proof of Theorem $^{\\,l}$ . The structure equations of $\\mathcal{M}$ have the form ", "page_idx": 16}, {"type": "equation", "text": "$$\nV_{i}=\\left\\{\\begin{array}{l l}{f_{i}\\left(\\mathbf{Pa}(V_{i}),U_{V_{i}}\\right),}&{V_{i}\\mathrm{~is~continuous},}\\\\ {\\mathrm{arg~max}_{k\\in[n_{i}]}\\left\\{g_{k}^{V_{i}}+\\log\\left(f_{i}\\left(\\mathbf{Pa}(V_{i}),U_{V_{i}}\\right)\\right)_{k}\\right\\},}&{V_{i}\\mathrm{~is~categorical},\\|f_{i}\\|_{1}=1,}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $g_{k}^{V_{i}}$ are i.i.d. standard Gumbel variables and $\\hat{\\mathcal{M}}\\in\\mathcal{M}(\\mathcal{G},\\hat{\\mathcal{F}},\\hat{U})$ has structure equations ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{V}_{i}=\\left\\{\\begin{array}{l l}{\\hat{f}_{i}\\left(\\mathbf{P}\\mathbf{a}(\\hat{V}_{i}),\\hat{U}_{V_{i}}\\right),}&{V_{i}\\mathrm{~is~continuous,}}\\\\ {\\mathrm{arg~max}_{k\\in[n_{i}]}\\left\\{\\hat{g}_{k}^{V_{i}}+\\log\\left(\\hat{f}_{i}\\left(\\mathbf{P}\\mathbf{a}(\\hat{V}_{i}),\\hat{U}_{V_{i}}\\right)\\right)_{k}\\right\\},}&{V_{i}\\mathrm{~is~categorical,}\\|\\hat{f}_{1}\\|_{1}=1,}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\hat{g}_{k}^{V_{i}}$ are i.i.d. standard Gumbel variables. Let the treatment variables set be $\\pmb{T}=\\{T_{1},\\cdots,T_{n_{T}}\\}$ We may give all vertices and $U_{0}=\\{U_{1},\\cdot\\cdot\\cdot,U_{n_{U}}\\}$ , an topology ordering (rearrange the subscript if necessary) ", "page_idx": 16}, {"type": "equation", "text": "$$\nU_{1},\\cdot\\cdot\\cdot,U_{n_{U}},T_{1},\\cdot\\cdot\\cdot\\,,T_{n_{T}},V_{1}(t),\\cdot\\cdot\\cdot\\,,V_{n_{V}-n_{T}}(t)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "such that for each directed edge $(V_{i}(t),V_{j}(t))$ , vertex $V_{i}(t)$ lies before vertex $V_{j}(t)$ in the ordering, ensuring that all edges start from vertices that appear early in the order and end at vertices that appear later in the order. We put $U_{0}$ and $\\textbf{\\emph{T}}$ at the beginning because they are the root nodes of the intervened model. We denote $\\mu_{U_{0},T,1:k}^{T}$ (resp. $\\bar{\\mu}_{U_{0},T,1:k}^{T})$ to be the distribution of $U_{0},{\\pmb T},V_{1}(t),\\cdot\\cdot\\cdot\\mathrm{~,~}V_{k}(t)$ (resp. $\\hat{U}_{0},{\\bf{T}},\\hat{V}_{1}(t),\\cdot\\cdot\\cdot\\hat{\\bf{\\Omega}},\\hat{V}_{k}(t))$ , $\\mu_{V_{k}|U_{0},T,1:k-1}^{T}$ the distribution of $V_{k}$ given $U_{0},{\\bf T},V_{1}(t),\\cdot\\cdot\\cdot,V_{k-1}(t)$ . + ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{W\\left(\\mu_{U_{0},T,1:k}^{T},\\hat{\\mu}_{\\hat{U}_{0},T,1:k}^{T}\\right)\\leqslant(L+1)W\\left(\\mu_{U_{0},T,1:k-1}^{T},\\hat{\\mu}_{\\hat{U}_{0},T,1:k-1}^{T}\\right)+2K^{2}S.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "By definition of Wasserstein-1 distance, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathit{\\Pi}_{^{0,T,1:k}},\\mathit{\\hat{\\mu}}_{\\hat{U}_{0},T,1:k}^{T}\\right)=\\displaystyle\\operatorname*{sup}_{g\\in L i p(1)}\\int g(u,t,v_{1},\\cdots,v_{k})d\\left(\\mu_{U_{0},T,1:k}^{T}-\\hat{\\mu}_{\\hat{U}_{0},T,1:k}^{T}\\right)}&{}\\\\ {=\\displaystyle\\operatorname*{sup}_{g\\in L i p(1)}\\int g(u,t,v_{1},\\cdots,v_{k})\\;d\\mu_{V_{k}|U_{0},T,1:k-1}^{T}\\;d\\mu_{U_{0},T,1:k-1}^{T}}&{}\\\\ {-\\displaystyle\\int g(u,t,v_{1},\\cdots,v_{k})\\;d\\hat{\\mu}_{\\hat{V}_{k}|\\hat{U}_{0},T,1:k-1}^{T}\\;d\\hat{\\mu}_{\\hat{U}_{0},T,1:k-1}^{T}}&{}\\\\ {=\\displaystyle\\operatorname*{sup}_{g\\in L i p(1)}\\int g(u,t,v_{1},\\cdots,v_{k})\\;d\\left(\\mu_{V_{k}|U_{0},T,1:k-1}^{T}-\\hat{\\mu}_{\\hat{V}_{k}|\\hat{U}_{0},T,1:k-1}^{T}\\right)\\;d\\hat{\\mu}_{U_{0},T,1:k-1}^{T}}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "For (1), if $V_{k}$ is a continuous variable, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\int g(\\boldsymbol{u},t,\\boldsymbol{v}_{1},\\cdots,\\boldsymbol{v}_{k})\\ d\\left(\\mu_{V_{k}|U_{0},T,1:k-1}^{T}-\\hat{\\mu}_{\\hat{V}_{k}|\\hat{U}_{0},T,1:k-1}^{T}\\right)}\\\\ &{=\\displaystyle g\\left(\\boldsymbol{u},t,\\boldsymbol{v}_{1},\\cdots,\\boldsymbol{v}_{k-1},f_{k}\\left(\\mathtt{p a}(\\boldsymbol{v}_{k}),\\boldsymbol{u}_{V_{k}}\\right)\\right)-g\\left(\\boldsymbol{u},t,\\boldsymbol{v}_{1},\\cdots,\\boldsymbol{v}_{k-1},\\hat{f}_{k}\\left(\\mathtt{p a}(\\boldsymbol{v}_{k}),\\boldsymbol{u}_{V_{k}}\\right)\\right)}\\\\ &{\\leqslant\\left\\|f_{k}\\left(\\mathtt{p a}(\\boldsymbol{v}_{k}),\\boldsymbol{u}_{V_{k}}\\right)-\\hat{f}_{k}\\left(\\mathtt{p a}(\\boldsymbol{v}_{k}),\\boldsymbol{u}_{V_{k}}\\right)\\right\\|_{\\infty}\\leqslant S,}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where we have used the Lipschitz property of $g$ in the first inequality. If $V_{k}$ is categorical, let $\\widehat{p}\\left(\\mathrm{pa}(v_{k}),\\boldsymbol{u}_{V_{k}}\\right)\\,=\\,\\left(\\widehat{p}_{1},\\cdot\\cdot\\cdot\\,,\\widehat{p}_{n_{i}}\\right)\\,=\\,\\widehat{f}\\left(\\mathrm{pa}(v_{k}),\\boldsymbol{u}_{V_{k}}\\right)$ and $p\\left(\\mathtt{p a}(v_{k}),\\mathtt{u}_{V_{k}}\\right)\\,=\\,\\left(p_{1},\\cdot\\cdot\\cdot\\,,p_{n_{i}}\\right)\\,=$ $f\\left(\\mathbf{pa}(v_{k}),\\mathbf{u}_{V_{k}}\\right)$ , we get ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\int g(u,t,v_{1},\\cdots,v_{k})\\ d\\left(\\mu_{V_{k}|U_{0},T,1;k-1}^{T}-\\hat{\\mu}_{\\hat{V}_{k}|\\hat{U}_{0},T,1;k-1}^{T}\\right)}\\\\ &{=\\displaystyle\\sum_{k=1}^{n_{i}-1}(g(u,t,v_{1},\\cdots,k)\\ -\\ g(u,t,v_{1},\\cdots,n_{i}))(p_{k}-\\hat{p}_{k})\\leqslant K\\displaystyle\\sum_{k=1}^{n_{i}-1}|p_{k}-\\hat{p}_{k}|\\leqslant K^{2}S,}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where we use $\\|V_{k}\\|_{\\infty}\\leqslant K,n_{i}\\leqslant K$ . ", "page_idx": 17}, {"type": "text", "text": "For (2), if $V_{i}$ is continuous, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\int g(\\boldsymbol{u},t,\\boldsymbol{v}_{1},\\cdots,\\boldsymbol{v}_{k})\\,d\\mu_{V_{k}|U_{0},T,1:k-1}^{T}\\,d\\left(\\mu_{U_{0},T,1:k-1}^{T}-\\hat{\\mu}_{\\dot{U}_{0},T,1:k-1}^{T}\\right)}\\\\ &{=\\displaystyle\\int g\\left(\\boldsymbol{u},t,\\boldsymbol{v}_{1},\\cdots,\\boldsymbol{v}_{k-1},f_{k}\\left(\\mathrm{pa}(\\boldsymbol{v}_{k}),\\boldsymbol{u}_{V_{k}}\\right)\\right)\\,\\,d\\left(\\mu_{U_{0},T,1:k-1}^{T}-\\hat{\\mu}_{\\dot{U}_{0},T,1:k-1}^{T}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Since $g,f_{k}$ are Lipschitz continuous functions, $g\\left(\\pmb{u},\\pmb{t},v_{1},\\cdot\\cdot\\cdot\\mathrm{\\boldmath~},v_{k-1},f_{k}\\left(\\mathrm{pa}(v_{k}),\\pmb{u}_{V_{k}}\\right)\\right)$ is $(L+1)$ - Lipschitz continuous with respect to $({\\boldsymbol{u}},t,v_{1},\\cdots\\,,v_{k-1})$ . We have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\int g\\left(\\boldsymbol{u},t,\\boldsymbol{v}_{1},\\cdots,\\boldsymbol{v}_{k-1},f_{k}\\left(\\mathtt{p a}(\\boldsymbol{v}_{k}),\\boldsymbol{u}_{V_{k}}\\right)\\right)d\\left(\\mu_{U_{0},T,1;k-1}^{T}-\\hat{\\mu}_{U_{0},T,1;k-1}^{T}\\right)}\\quad}&{}\\\\ &{\\leq(L+1)W\\left(\\mu_{U_{0},T,1;k-1}^{T},\\hat{\\mu}_{\\bar{U}_{0},T,1;k-1}^{T}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "If $V_{i}$ is categorical, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\int g(\\pmb{u},t,x_{1},\\cdot\\cdot\\cdot\\cdot,x_{k})\\,d\\mu_{X_{k}|U,\\pmb{T},1;k-1}^{T}\\,d\\left(\\mu_{U,T,1:k-1}^{T}-\\hat{\\mu}_{\\dot{U},\\pmb{T},1:k-1}^{T}\\right)}\\\\ &{\\displaystyle\\quad=\\sum_{k=1}^{n_{i}}p_{k}\\int g(\\pmb{u},t,x_{1},\\cdot\\cdot\\cdot\\cdot,k)\\,d\\left(\\mu_{U,T,1:k-1}^{T}-\\hat{\\mu}_{\\dot{U},\\pmb{T},1:k-1}^{T}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Since $g,f_{k}$ are $L$ -Lipschitz continuous functions, $g(\\mathbf{u},t,v_{1},\\cdots,v_{k-1},i)$ is $L$ -Lipschitz continuous with respect to $({\\boldsymbol{u}},t,v_{1},\\cdots,v_{k-1})$ . We have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{n_{i}}\\int p_{k}g(u,t,x_{1},\\cdots,k)\\,d\\left(\\mu_{U_{0},T,1:k-1}^{T}-\\hat{\\mu}_{U_{0},T,1:k-1}^{T}\\right)\\leqslant L W\\left(\\mu_{U_{0},T,1:k-1}^{T},\\hat{\\mu}_{\\bar{U}_{0},T,1:k-1}^{T}\\right).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Combine (12)-(16), we prove Equation (11). By induction, one can easily get ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{V\\left(\\mu_{U_{0},T,1:n_{V}-n_{T}}^{T},\\hat{\\mu}_{U_{0},T,1:n_{V}-n_{T}}^{T}\\right)\\leqslant(L+1)^{n_{V}-n_{T}}\\left(W\\left(\\mu_{U_{0},T}^{T},\\hat{\\mu}_{\\bar{U}_{0},T}^{T}\\right)+2K^{2}S/L\\right)-2K^{2}S/L\\right)}\\\\ {=\\frac{(L+1)^{n_{V}-n_{T}}-1}{L}2K^{2}S+(L+1)^{n_{V}-n_{T}}W\\left(\\mu_{U_{0},T}^{T},\\hat{\\mu}_{\\bar{U}_{0},T}^{T}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "$\\begin{array}{r}{C_{\\mathcal{G}}(L,K)=2K^{2}\\cdot\\operatorname*{max}\\{\\frac{(L+1)^{n_{V}-n_{T}}-1}{L},(L+1)^{n_{V}-n_{T}}\\}}\\end{array}$ and notice that $W\\left(\\mu_{U_{0},T}^{T},\\hat{\\mu}_{\\hat{U}_{0},T}^{T}\\right)=$ $W\\left(P^{\\mathcal{M}}(U),P^{\\hat{\\mathcal{M}}}(\\hat{U})\\right)$ because interventions $\\mathbf{\\nabla}T=t$ are the same, we get ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{W\\left(P^{\\mathcal M}(V(t)),P^{\\hat{\\mathcal M}}(V(t))\\right)\\leqslant W\\left(\\mu_{U_{0},T,1:n_{V}-n_{T}}^{T},\\hat{\\mu}_{\\hat{U}_{0},T,1:n_{V}-n_{T}}^{T}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leqslant C_{\\mathcal G}(L,K)\\left(S+W\\left(P^{\\mathcal M}(U),P^{\\hat{\\mathcal M}}(\\hat{U})\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "B.3 Proof of results in Section 3.1 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Proof of Theorem 2. Recall that the neural network consists of three parts $\\hat{g}=\\hat{g}_{3}^{\\tau}\\circ\\hat{g}_{2}\\circ\\hat{g}_{1}$ and $\\hat{g}_{2},\\hat{g}_{1}$ have a separable form, dealing with each coordinate of the input individually. As mentioned before, each coordinate approximates the distribution of one connected component of the support. We first construct $\\hat{g}_{1}$ and $\\hat{g}_{2}$ , then use Gumbel-Softmax layer to combine each component together. ", "page_idx": 18}, {"type": "text", "text": "To construct the first two parts, we only need to consider each coordinate individually. For the $i$ -th component $C_{i}$ , let $\\mu_{i}=\\mathbb{P}_{C_{i}}/\\mathbb{P}(C_{i})$ , where $\\mathbb{P}_{C_{i}}$ is measure $\\mathbb{P}$ restricted to component $C_{i}$ . Under Assumption 4, there exists a Lipschitz map $g_{2}^{i}\\in\\dot{\\mathcal{H}}([0,1]^{d_{i}^{C}},C_{i})\\cap\\mathcal{F}_{L}([0,1]^{d_{i}^{C}},C_{i})$ . By [43, Theorem VIII.1], there exists quantized ReLu networ2k $\\hat{g}_{1}^{i}$ of width $W_{1}$ and depth $\\Theta(d_{i}^{C})$ (let $\\begin{array}{r}{\\dot{s}=\\frac{1}{\\lceil n(n-1)\\rceil}}\\end{array}$ in [43, Theorem VIII.1]), such that ", "page_idx": 18}, {"type": "equation", "text": "$$\nW\\left(\\left((g_{2}^{i})^{-1}\\right)_{\\#}\\mu_{i},P(\\hat{g}_{1}^{i}(U_{i}))\\right)\\leqslant O\\left(W_{1}^{-1/d_{i}^{C}}\\right),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $U_{i}$ are i.i.d. uniform random variables on $[0,1]$ . By [53, Theorem 2], there exist a deep ReLu network $\\hat{g}_{2}^{i,j}$ of width $\\Theta\\left(d_{i}^{C}\\right)$ and depth $L_{2}$ such that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\left(g_{2}^{i}\\right)_{j}-\\hat{g}_{2}^{i,j}\\|_{\\infty}\\leqslant O\\left(L_{2}^{-2/d_{i}^{C}}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Let $\\hat{g}_{2}^{i}(x)=\\left(\\hat{g}_{2}^{i,1}(x),\\cdots\\,,\\hat{g}_{2}^{i,d}(x)\\right)$ , we get ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\|g_{2}^{i}-\\hat{g}_{2}^{i}\\|_{\\infty}\\leqslant O\\left(L_{2}^{-2/d_{i}^{C}}\\right).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Thus, the width of $\\hat{g}_{2}^{i}$ is $\\Theta\\left(d\\cdot d_{i}^{C}\\right)$ . Let ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\hat{g}_{i}(x)=\\left(\\hat{g}_{i}^{1}(x_{1}),\\cdot\\cdot\\cdot\\cdot,\\hat{g}_{i}^{N_{C}}(x_{N_{C}})\\right),i=1,2,x_{k}\\in\\mathbb{R}^{d_{i}},d_{1}=1,d_{2}=d,k=1,\\cdot\\cdot\\cdot,N_{C},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $N_{C}$ is the number of connected components. By Lemma 3, we get ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{W\\left(\\mu_{i},P(\\hat{g}_{2}^{i}\\circ\\hat{g}_{1}^{i}(U_{i}))\\right)=W\\left(\\left(g_{2}^{i}\\right)_{\\#}\\left(g_{2}^{i}\\right)_{\\#}^{-1}\\mu_{i},\\left(\\hat{g}_{2}^{i}\\right)_{\\#}P(\\hat{g}_{1}^{i}(U_{i}))\\right)}\\\\ &{\\qquad\\qquad\\qquad\\leqslant L W\\left(\\left(g_{2}^{i}\\right)_{\\#}^{-1}\\mu_{i},P(\\hat{g}_{1}^{i}(U_{i}))\\right)+\\|g_{2}^{i}-\\hat{g}_{2}^{i}\\|_{\\infty}}\\\\ &{\\qquad\\qquad\\qquad\\qquad=O\\left(W_{1}^{-1/d_{i}^{C}}+L_{2}^{-2/d_{i}^{C}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Next, let $p_{i}=\\mathbb{P}(C_{i})$ and the distribution of $\\hat{g}_{2}^{k}\\circ\\hat{g}_{1}^{k}(U_{k})$ be $\\hat{\\mu}_{k}$ , then we have $\\begin{array}{r}{\\mathbb{P}=\\sum_{k=1}^{N_{C}}p_{k}\\mu_{k}}\\end{array}$ k=1 pk\u00b5k. By Lemma 4, we have ", "page_idx": 18}, {"type": "equation", "text": "$$\nW\\left(\\mathbb{P},\\sum_{k=1}^{N_{C}}p_{k}\\hat{\\mu}_{k}\\right)\\leqslant\\sum_{k=1}^{N_{C}}p_{k}W\\left(\\mu_{i},\\hat{\\mu}_{k}\\right)=O\\left(W_{1}^{-1/\\operatorname*{max}\\{d_{i}^{C}\\}}+L_{2}^{-2/\\operatorname*{max}\\{d_{i}^{C}\\}}\\right).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Finally, we analyze the error caused by the Gumbel-Softmax layer. Note that as temperature parameter $\\tau\\rightarrow0$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\nw^{\\tau}=\\left(\\frac{\\exp((\\log p_{i}+G_{i})/\\tau)}{\\sum_{k=1}^{N_{C}}\\exp((\\log p_{k}+G_{k})/\\tau)}\\right)_{i=1,\\cdots,N_{C}}\\xrightarrow{a.s.}\\mathrm{One-hot}(\\arg\\operatorname*{max}_{i}\\{G_{i}+\\log p_{i}\\})\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where One-hot $(k)$ is $N_{C}$ -dimensional vector with the $k$ -th coordinate equals to 1 and the remaining coordinates are 0 and $G_{i}\\sim{\\mathrm{Gumbel}}(0,1)$ are i.i.d. Gumbel distribution. Let $\\nu^{\\tau}$ be the distribution of $w^{\\tau}$ . By Lemma 6, for $0<\\tau<1$ , we have ", "page_idx": 18}, {"type": "equation", "text": "$$\nW\\left(\\nu^{\\tau},\\nu^{0}\\right)\\leqslant O(\\tau-\\tau\\log\\tau).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "By Lemma 5, ", "page_idx": 18}, {"type": "equation", "text": "$$\nW\\left(\\left(\\nu^{\\tau},P(\\left(\\hat{g}_{2}^{i}\\circ\\hat{g}_{1}^{i}\\right)(U_{i}))\\right),\\left(\\nu^{0},P(\\left(\\hat{g}_{2}^{i}\\circ\\hat{g}_{1}^{i}\\right)(U_{i}))\\right)\\right)\\leqslant O\\left(\\tau-\\tau\\log\\tau\\right).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Let ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\hat{g}_{3}^{\\tau}(\\boldsymbol{x}_{1},\\cdots,\\boldsymbol{x}_{N_{C}})=\\sum_{k=1}^{N_{C}}w_{k}^{\\tau}\\cdot\\boldsymbol{x}_{k},\\quad\\boldsymbol{x}_{k}\\in\\mathbb{R}^{d},\\;w^{\\tau}=\\big(w_{1}^{\\tau},\\cdots,w_{N_{C}}^{\\tau}\\big)\\,.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We denote $g_{3}^{0}=\\ensuremath{\\mathrm{lim}}_{\\tau\\rightarrow0}\\,g_{3}^{\\tau}$ . By Assumption 4, the support is bounded. Thus, $g_{2}^{i}$ and $\\hat{g}_{2}^{i}$ are bounded, i.e., $\\|\\hat{g}_{2}^{i}\\;(x)\\|_{\\infty}\\,\\leqslant\\,K$ . Since functions $h(w,X)\\,=\\,w^{\\mathsf{T}}X$ is Lipschitz continuous in the region $\\|w\\|_{1}=1,\\|X\\|_{\\infty}\\leqslant K$ , we get ", "page_idx": 19}, {"type": "equation", "text": "$$\nW\\left(P(\\hat{g}_{3}^{0}\\circ\\hat{g}_{2}\\circ\\hat{g}_{1}(U_{k})),P(\\hat{g}_{3}^{\\tau}\\circ\\hat{g}_{2}\\circ\\hat{g}_{1}(U_{k}))\\right)\\leqslant O\\left(\\tau-\\tau\\log\\tau\\right)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "from (18). Putting things together, we get ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V\\left(\\mathbb{P},P(\\hat{g}_{3}^{\\tau}\\circ\\hat{g}_{2}\\circ\\hat{g}_{1}(U_{k}))\\right)\\leqslant W\\left(\\mathbb{P},\\displaystyle\\sum_{k=1}^{N_{C}}p_{k}\\hat{\\mu}_{k}\\right)+W\\left(\\displaystyle\\sum_{k=1}^{N_{C}}p_{k}\\hat{\\mu}_{k},P(\\hat{g}_{3}^{\\tau}\\circ\\hat{g}_{2}\\circ\\hat{g}_{1}(U_{k}))\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=W\\left(\\mathbb{P},\\displaystyle\\sum_{k=1}^{N_{C}}p_{k}\\hat{\\mu}_{k}\\right)+W\\left(P(\\hat{g}_{3}^{0}\\circ\\hat{g}_{2}\\circ\\hat{g}_{1}(U_{k})),P(\\hat{g}_{3}^{\\tau}\\circ\\hat{g}_{2}\\circ\\hat{g}_{1}(U_{k}))\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leqslant O\\left(\\tau-\\tau\\log\\tau+W_{1}^{-1/\\operatorname*{max}\\{d_{i}^{C}\\}}+L_{2}^{-2/\\operatorname*{max}\\{d_{i}^{C}\\}}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where we use (19). ", "page_idx": 19}, {"type": "text", "text": "B.4 Proof of results in Section 3.2 ", "text_level": 1, "page_idx": 19}, {"type": "image", "img_path": "GEbnPxD9EF/tmp/b3d89968d0978ecaf860713f8e352f766d7e8ae8907af6dd7b5026fdba796ab5.jpg", "img_caption": [], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Figure 8: Architecture of the deep neural network for 4\u2212dimensional output. The first (yellow) part approximates the distribution on different connected components using deep ReLu Networks. The remaining two parts are similar to the wide neural network in Figure 1a. ", "page_idx": 19}, {"type": "text", "text": "Before we prove Proposition 1, we need to introduce some important notions. Let $\\{{\\mathcal{C}}_{n}\\}$ be a sequence of partitions of the unit cube $[0,1]^{d}$ . We say $\\{{\\mathcal{C}}_{n}\\}$ has property $({*})$ if the following properties are satisfied. ", "page_idx": 19}, {"type": "text", "text": "1. $\\mathcal{C}_{0}=\\{C_{-1}\\},C_{-1}=[0,1]^{d}$   \n2. $\\mathcal{C}_{n+1}=\\{C_{i_{1},i_{2}}...\\ ,i_{n+1}\\}_{i_{j}=0,\\cdots,2^{d}-1}$ is a refinement of $\\mathcal{C}_{n}=\\{C_{i_{1},i_{2}\\cdots,i_{n}}\\}_{i_{j}=0,\\cdots,2^{d}-1}$ , i.e., $\\begin{array}{r}{C_{i_{1},i_{2}\\cdots,i_{n}}=\\bigcup_{i_{n+1}=0}^{2^{d}-1}C_{i_{1},i_{2}\\cdots,i_{n+1}}}\\end{array}$ . Besides, $\\mathcal{C}_{n}$ is obtained by cutting the unit cube $[0,1]^{d}$ by planes that are paralleled to the coordinate planes. ", "page_idx": 19}, {"type": "text", "text": "The following lemma is important in the construction of Hilbert curve. ", "page_idx": 19}, {"type": "text", "text": "Lemma 1 (Algorithm 3 in [25]). Given a sequence of partitions of the unit cube $\\{{\\mathcal{C}}_{n}\\}$ that has property $({}^{*})$ , there exist a ordering for each partition ${\\mathcal{C}}_{n}$ , i.e., $O_{n}\\ :\\ \\left\\{1,\\cdot\\cdot\\cdot,2^{n d}\\right\\}\\ \\rightarrow$ $\\left\\{{\\overline{{i_{1}i_{2}\\cdot\\cdot\\cdot i_{n}}}}:i_{j}=0,\\cdot\\cdot\\cdot,2^{d}-1\\right\\}$ , such that the following two conclusions hold. ", "page_idx": 19}, {"type": "text", "text": "1. If $(j-1)\\cdot2^{d}<i\\leqslant j\\cdot2^{d}$ , the first $n$ digits of $O_{n+1}(i)$ are the same as $O_{n}(j)$ . 2. Adjacent cubes in the ordering intersect, i.e., $C_{O_{n}(i)}\\cap C_{O_{n}(i+1)}\\ \\neq\\ \\emptyset$ . Furthermore, $C_{O_{n}(i)}\\cap C_{O_{n}(i+1)}$ is a $d-1$ dimensional cube. ", "page_idx": 19}, {"type": "text", "text": "Proof of Proposition $^{\\,l}$ . Without loss of generality, we can assume that $\\mathbb{P}$ is supported on $[0,1]^{d}$ , vanishes at the boundary and $\\mathbb{P}(B)\\geqslant C_{1}^{\\mathsf{\\bar{\\Gamma}}}\\lambda(B)$ for some constant $C_{1}>0$ and all measurable sets $B\\in[0,1]^{d}$ . Suppose we have proven the conclusion for this case. By Assumption 5, there exists $f\\in\\mathcal{H}([0,1]^{d},K)\\cap\\mathcal{F}_{L}([0,1]^{d},K)$ such that $f_{\\#}^{-1}\\mathbb{P}$ satisfies these conditions. There exists continuous function $\\gamma$ such that $f_{\\#}^{-1}\\mathbb{P}=\\gamma_{\\#}\\lambda$ , which implies $\\mathbb{P}=(f\\circ\\gamma)_{\\#}\\lambda$ . In particular, if $\\gamma$ is $1/d$ -Holder continuous, $f\\circ\\gamma$ is also $1/d$ -Holder continuous because $f$ is Lipschitz continuous. ", "page_idx": 20}, {"type": "text", "text": "When $d=0$ , the constant map satisfies the requirement. We focus on the case $d\\geqslant1$ in the following. The proof is to modify the construction of the Hilbert space-fliling curve. By changing the velocity of the curve, the push-forward measure of the Hilbert space-fliling curve can simulate a great number of distributions. We use $\\lambda$ to represent the Lebesgue measure [0, 1]. ", "page_idx": 20}, {"type": "text", "text": "Proof Sketch: The construction of $f$ is inspired by the famous Hilbert space-filling curve [28]. To illustrate the idea, let us first assume that $\\mathbb{P}$ is absolutely continuous with respect to the Lebesgue measure $\\lambda$ and consider $d=2$ . In the $k$ -th step, we divide the unit cube into $2^{2\\bar{k}}$ evenly closed cubes $\\mathcal{C}_{n}=\\{C_{1,n},\\cdot\\cdot\\cdot\\,,C_{2^{2k},n}\\}$ such that $\\ensuremath{{\\mathcal{C}}}_{n}$ is a refinement of $\\mathcal{C}_{n-1}$ . ", "page_idx": 20}, {"type": "text", "text": "The construction of a standard Hilbert curve is to find a sequence of curves $\\gamma_{n}$ that go through all the cubes in $\\mathcal{C}_{n}$ . The curve $\\gamma_{n}$ has one special property. If $\\gamma_{n-1}(t)\\,\\in\\,C_{k,n-1}$ , then $\\gamma_{m}(t)\\in$ $C_{k,n-1},\\forall m\\geqslant n$ . For example, in Figure 9, the points on the curve in the lower-left cubes will stay in the lower-left cubes. Note that $(\\gamma_{n})_{\\#}\\lambda(C_{k,n})=\\lambda\\left(\\gamma_{n}^{-1}(C_{k,n})\\right)$ is the time curve $\\gamma_{n}$ stays in cubes ${C}_{k,n}$ . The idea is to change the speed of the curve so that $\\mathbb{P}$ and $(\\gamma_{n})_{\\#}\\lambda$ agree on cubes in $\\ensuremath{{\\mathcal{C}}}_{n}$ , i.e., $\\mathbb{P}(C_{k,n})=\\lambda\\left(\\gamma_{n}^{-1}(C_{k,n})\\right)$ . Since we assume that $\\mathbb{P}$ is absolutely continuous, $\\mathbb{P}(\\partial C_{k,n})=0$ and we don\u2019t need to worry about how to divide the mass on the boundary. For example, let the green cubes in the Figure 9 to be $C_{0}$ and suppose that $\\gamma_{1}$ starts from $C_{0}$ . We will change the speed so that $\\gamma_{1}$ spends $\\mathbb{P}(C_{0})$ time in this region. In the next step, we divide $C_{0}$ into four colored cubes $C_{1},\\cdot\\cdot\\cdot\\,,C_{4}$ on the right. We change the speed again to let the time spent in each cube equal to $\\mathbb{P}(C_{i})$ . Note that this construction preserves the aforementioned property, i.e., for $t\\in[0,\\mathbb{P}(\\bar{C_{0}}\\bar{)}]$ , $\\gamma_{n}(t)\\in C_{0},\\forall n\\geqslant1$ . As $n\\to\\infty$ , it can be proven that $\\gamma_{n}$ converges uniformly to a curve $\\gamma$ . We can also prove that $\\mathbb{P}$ and $\\gamma_{\\#}\\lambda$ agree on $\\cup_{n=1}^{\\infty}\\mathcal{C}_{n}$ . Given that $\\cup_{n=1}^{\\infty}\\mathcal{C}_{n}$ generate the standard Borel algebra on $[0,1]^{2}$ , we can conclude that $\\mathbb{P}=\\gamma_{\\#}\\lambda$ . ", "page_idx": 20}, {"type": "text", "text": "In the general case, $\\mathbb{P}$ may not be absolutely continuous. As a result, the boundary may not be $\\mathbb{P}$ -measure zero. We will need to perturb the cutting planes to ensure their boundaries are measured zero. ", "page_idx": 20}, {"type": "image", "img_path": "GEbnPxD9EF/tmp/e61b1444aeb4c13ec4f1c042934c6b4b13f8251f56e4a88c71638cc6c5084397.jpg", "img_caption": ["Figure 9: The construction of Hilbert curve. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "Preparation. For a $n$ -dimensional cube $\\begin{array}{r}{C\\,=\\,\\prod_{i=1}^{n}[a_{i},b_{i}]}\\end{array}$ , the diameter of $C$ is $\\dim(C)\\ =$ $\\sqrt{\\sum_{i=1}^{n}(b_{i}-a_{i})^{2}}$ . We also define the function $R$ that measures the ratio between maximum edge and minimum edge. For a collection of cubes $\\begin{array}{r}{\\mathcal{C}=\\{C_{1},\\cdots,C_{m}\\},C_{k}=\\prod_{i=1}^{n}\\left[a_{i}^{k},b_{i}^{k}\\right]}\\end{array}$ , $R(C)$ is defined to be ", "page_idx": 20}, {"type": "equation", "text": "$$\nR(\\mathcal{C})=\\frac{\\operatorname*{max}_{i,k}|a_{i}^{k}-b_{i}^{k}|}{\\operatorname*{min}_{i,k}|a_{i}^{k}-b_{i}^{k}|}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "$R({\\mathcal{C}})$ measures the shape of the cubes in a collection. We need to control $R({\\mathcal{C}})$ in the construction to obtain Holder continuity in Step 2. ", "page_idx": 21}, {"type": "text", "text": "Step 1: Define partition. First, we construct a sequence of partitions that has the property $(^{*})$ recursively. Let ${\\mathcal C}_{0}~=~\\{[0,1]^{d}\\}$ . Suppose we have defined close cubes collection ${\\mathcal{C}}_{n}\\ =$ $\\{C_{i_{1},i_{2}\\cdots,i_{n}}\\}_{i_{j}=0,\\cdots,2^{d}-1},$ , such that ", "page_idx": 21}, {"type": "text", "text": "1. $\\ensuremath{{\\mathcal{C}}}_{n}$ is obtained by cutting the unit cube $[0,1]^{d}$ by planes that are paralleled to the coordinate planes. ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{C_{i_{1},i_{2}\\cdots,i_{n-1}}=\\bigcup_{i_{n}}C_{i_{1},i_{2}\\cdots,i_{n}}\\;\\mathrm{and}\\;\\mathrm{diam}(C_{i_{1},i_{2}\\cdots,i_{n-1}})\\leqslant2/3\\cdot\\mathrm{diam}(C_{i_{1},i_{2}\\cdots,i_{n}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Next, we construct $\\mathcal{C}_{n+1}$ from $\\ensuremath{{\\mathcal{C}}}_{n}$ that preserves these properties. We refine the division $\\mathcal{C}_{n}\\,=$ $\\{C_{i_{1},i_{2}\\cdots,i_{n}}\\}_{i_{j}=0,\\cdots,2^{d}-1}$ by union of hyperplane $P_{i,r}=\\{y:y_{i}=r\\}$ to get $\\{C_{i_{1},i_{2}\\cdots,i_{n+1}}\\}$ such that $\\begin{array}{r}{C_{i_{1},i_{2}\\cdots,i_{n}}=\\bigcup_{i_{n+1}}C_{i_{1},i_{2}\\cdots,i_{n+1}}}\\end{array}$ and $\\mathrm{diam}(C_{i_{1},i_{2}\\cdots,i_{n-1}})\\leqslant2/3\\cdot\\mathrm{diam}(C_{i_{1},i_{2}\\cdots,i_{n}})$ . By property 1, ${\\mathcal{C}}_{n}$ is obtained by dividing $[0,1]^{d}$ using ", "page_idx": 21}, {"type": "equation", "text": "$$\nP_{1,r_{0}^{1}},\\cdot\\cdot\\cdot\\,,P_{1,r_{2^{n}-1}^{1}},\\cdot\\cdot\\cdot\\,,P_{i,r_{j}^{i}},\\cdot\\cdot\\cdot\\,,P_{d,r_{2^{n}}^{d}},r_{0}^{i}=0,r_{2^{n}}^{i}=1,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "we can further cut the unit cube using hyperplane $\\{P_{i,\\left(r_{j}^{i}+r_{j+1}^{i}\\right)/2)}\\}i{=}1,\\cdots d,j{=}1,\\cdots2^{n}{-}1$ . However, this construction may not satisfy property 3, $\\mathbb{P}(\\partial C_{i_{1},i_{2}\\cdots,i_{n+1}})=\\dot{0}$ . We need to perturb each hyperplane to ensure a measure-zero boundary. Since ", "page_idx": 21}, {"type": "equation", "text": "$$\nZ_{i}=\\left\\{r\\in\\mathbb{R}:\\mathbb{P}\\left(\\left\\{x\\in[0,1]^{d},[x]_{i}=r\\right\\}\\right)\\neq0\\right\\}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "is at most countable, we can perturb the hyperplanes a little bit, i.e., $\\{P_{i,\\left(r_{j}^{i}+r_{j+1}^{i}\\right)/2+\\epsilon_{i,j}}\\}i{=}1,\\cdots d,j{=}1,\\cdots2^{n}{-}1$ to ensure property 3. In this way, we can choose $|\\epsilon_{i,j}|$ to be sufficiently small such that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathrm{diam}(C_{i_{1},i_{2}\\cdots,i_{n+1}})\\leqslant2/3\\cdot\\mathrm{diam}(C_{i_{1},i_{2}\\cdots,i_{n}}),\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\frac{\\operatorname*{max}_{i,j}\\left(\\left(r_{j+1}^{i}-r_{j}^{i}\\right)/2+|\\epsilon_{i,j}|\\right)}{\\operatorname*{min}_{i,j}\\left(\\left(r_{j+1}^{i}-r_{j}^{i}\\right)/2-|\\epsilon_{i,j}|\\right)}\\leqslant\\frac{\\operatorname*{max}_{i,j}\\left(r_{j+1}^{i}-r_{j}^{i}\\right)}{\\operatorname*{min}_{i,j}\\left(r_{j+1}^{i}-r_{j}^{i}\\right)}\\cdot\\left(1+2^{-n}\\right)\\quad.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Therefore, $R(\\mathcal{C}_{n+1})\\leqslant(1+2^{-n})\\,R(\\mathcal{C}_{n})$ and it is easy to see that properties 1,2,3 are satisfied. ", "page_idx": 21}, {"type": "text", "text": "Step 2: Construct Hilbert Curve. By Lemma 1, there exist an sequence of ordering $\\{O_{n}\\}$ of $\\{{\\mathcal{C}}_{n}\\}$ that satisfies ", "page_idx": 21}, {"type": "text", "text": "\u2022 The first $n$ digits of $O_{n+1}(i)$ with $O_{n}(j),\\;(j-1)\\cdot2^{d}<i\\leqslant j\\cdot2^{d}.$ \u2022 $C_{O_{n+1}(i)}\\cap C_{O_{n+1}(i+1)}$ are $d-1$ dimensional cubes. ", "page_idx": 21}, {"type": "text", "text": "Let ${\\cal Z}_{0}~=~\\{[0,1]\\}$ , we define ${\\mathcal{Z}}_{n}$ recursively. Suppose we have define interval collection ${\\mathcal Z}_{n}\\;=\\;\\{\\{I_{i_{1},i_{2}}...\\,,i_{n}\\}_{i_{j}=0,\\cdots,2^{d}-1}\\}$ such that ${\\overline{{I_{i_{1},i_{2}\\cdots,i_{n-1}}}}}\\,=\\,\\bigcup_{i_{n}}\\,I_{i_{1},i_{2}\\cdots,i_{n}}$ and $\\mathbb{P}(C_{i_{1},i_{2}\\cdots,i_{n}})\\,=$ $|I_{i_{1},i_{2}\\cdots,i_{n}}|$ . Since $\\mathbb{P}(\\partial C_{i_{1},i_{2}\\cdots,i_{n+1}})=0$ , we have $\\begin{array}{r}{\\mathbb{P}(C_{i_{1},i_{2}\\cdots,i_{n}})=\\sum_{i_{n+1}}\\mathbb{P}(C_{i_{1},i_{2}\\cdots,i_{n+1}})}\\end{array}$ . With the ordering, we divide each $I_{O_{n}(j)},0\\leqslant j\\leqslant2^{n d}-1$ into $2^{d}$ closed sub-interval $I_{O_{n+1}(i)},j2^{d}\\leqslant i<$ $(j+1)2^{d}$ such that $\\begin{array}{r}{I_{O_{n+1}(i)}=[\\sum_{k=1}^{i-1}p_{k},\\sum_{k=1}^{i}p_{k}]}\\end{array}$ , where $p_{i}=\\mathbb{P}(C_{O_{n+1}(i)})$ . This construction satisfies $|I_{{\\cal Q}_{n+1}(i)}|=\\mathbb P({\\cal C}_{{\\cal O}_{n+1}(i)})$ . Note that by condition 3 in step 1, $\\mathbb{P}$ vanishes at the boundary of the cubes. Therefore, ", "page_idx": 21}, {"type": "equation", "text": "$$\n|I_{O_{n}(i)}|=\\mathbb{P}(C_{O_{n}(i)})=\\sum_{j=(i-1)2^{d}+1}^{i2^{d}}\\mathbb{P}(C_{O_{n+1}(j)})=\\sum_{j=(i-1)2^{d}+1}^{i2^{d}}|I_{O_{n+1}(j)}|.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Now, we can construct a piecewise linear function $\\gamma_{n+1}(t)$ such that ", "page_idx": 21}, {"type": "equation", "text": "$$\n(\\gamma_{n+1})_{\\#}\\lambda(C_{O_{n+1}(i)})=\\mathbb{P}(C_{O_{n+1}(i)}),(\\gamma_{n+1})_{\\#}\\lambda(\\partial C_{O_{n+1}(i)})=0,\\forall i.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "The idea is to construct a piecewise linear curve going through all cubes in $\\mathcal{C}_{n+1}$ exactly once and modify its speed. Take $v_{0}\\,\\in\\,\\mathbf{Center}(C_{O_{n+1}(1)}),v_{i}\\,\\in\\,\\mathbf{Center}(C_{O_{n+1}(i)}\\cap C_{O_{n+1}(i+1)}),v_{2^{d(n+1)}}\\,\\in\\,\\mathbf{Center}(C_{O_{n+1}(i)}\\cap C_{O_{n+1}(i+1)})$ Center $\\left(C_{O_{n+1}\\left(2^{d\\left(n+1\\right)}\\right)}\\right)$ , where C $\\begin{array}{r}{\\mathrm{enter}(\\prod_{i=1}^{k}[a_{i},b_{i}])=((a_{i}+b_{i})/2)_{i=1,\\cdots,k}}\\end{array}$ is the center of a cube. $v_{i}$ is well-defined since $C_{O_{n+1}(i)}\\cap C_{O_{n+1}(i+1)}$ are cubes of dimension $d-1$ by Lemma 1. One possible choice of $\\gamma_{n+1}(t)$ is ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\gamma_{n+1}(t)=v_{0}+\\sum_{i=1}^{2^{(n+1)d}}\\frac{1}{p_{i}}(v_{i}-v_{i-1})\\left(\\left(t-\\sum_{k=1}^{i-1}p_{k}\\right)^{+}-\\left(t-\\sum_{k=1}^{i}p_{k}\\right)^{+}\\right).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "The first two curves $\\gamma_{1},\\gamma_{2}$ are shown in Figure 9. It is straightforward to verify that $\\begin{array}{r}{\\gamma_{n+1}\\bigl(\\sum_{k=1}^{i}p_{k}\\bigr)=v_{i}}\\end{array}$ and ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\gamma_{n+1}(I_{O_{n+1}(i)})=\\gamma_{n+1}([\\sum_{k=1}^{i-1}p_{k},\\sum_{k=1}^{i}p_{k}])\\subset C_{O_{n+1}(i)}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "In fact, since $v_{i-1},v_{i}$ are on different surfaces of the cube $C_{O_{n+1}(i)}$ , the line segment between $v_{i-1},v_{i}$ lies inside the interior of $C_{O_{n+1}(i)}$ and $\\gamma_{n+1}$ goes through all the cubes $C_{O_{n+1}(i)}$ exactly once. We have $\\begin{array}{r}{\\gamma_{n+1}^{-1}(\\mathrm{Int}(C_{O_{n+1}(i)}))\\ \\subset\\ [\\sum_{k=1}^{i-1}p_{k},\\sum_{k=1}^{i}p_{k}]}\\end{array}$ and $(\\gamma_{n+1})_{\\#}\\lambda(C_{O_{n+1}(i)})\\;=$ $\\mathbb{P}(C_{O_{n+1}(i)})$ , where $\\operatorname{Int}(\\cdot)$ is the interior of a set. Besides, we also have $(\\gamma_{n+1})_{\\#}\\lambda(\\partial C_{O_{n+1}(i)})\\leqslant$ $\\lambda(\\gamma_{n+1}^{-1}(\\{v_{j}\\}_{j=1\\cdots2^{d(n+1)}}))=0$ . ", "page_idx": 22}, {"type": "text", "text": "Finally, for any $k_{1}\\geqslant k_{2}\\geqslant n,t\\in[0,1]$ , by property \u221a2 in step 1, (20) and (22), $\\gamma_{k_{1}}(t),\\gamma_{k_{2}}(t)$ are in one cube $C_{i_{1},i_{2}\\cdots,i_{n}}$ . and $\\begin{array}{r}{\\mathrm{diam}(C_{i_{1},i_{2}\\cdots,i_{n}})\\leqslant\\left(\\frac{2}{3}\\right)^{n}{\\sqrt{d}}}\\end{array}$ . Thus, $\\begin{array}{r}{|\\gamma_{k_{1}}(t)-\\gamma_{k_{2}}(t)|\\leqslant\\left(\\frac{2}{3}\\right)^{n}\\sqrt{d}}\\end{array}$ which implies $\\{\\gamma_{n}(t)\\}$ converges uniformly to one continuous function $\\gamma(t)$ . ", "page_idx": 22}, {"type": "text", "text": "Step 3: Verify Conclusion $\\mathbb{P}=\\gamma_{\\#}\\lambda.$ . By $W((\\gamma_{k})_{\\#}\\lambda,\\gamma_{\\#}\\lambda)\\leqslant\\|\\gamma_{k}-\\gamma\\|_{\\infty}\\rightarrow0,(\\gamma_{k})_{\\#}$ converges weakly to $\\gamma_{\\#}\\lambda$ . By construction, $\\mathbb{P}(\\partial{C_{i_{1},i_{2}\\cdots,i_{n}}})=0$ and $(\\gamma_{n})_{\\#}\\lambda(C_{i_{1},i_{2}\\cdots,i_{n}})\\,=\\,\\operatorname{\\mathbb{P}}(C_{i_{1},i_{2}\\cdots,i_{n}})$ . Therefore, by condition 2 in step 1, for any $k\\geqslant n$ , ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{(\\gamma_{k})_{\\#}\\lambda(C_{i_{1},i_{2}\\cdots,i_{n}})=(\\gamma_{k})_{\\#}\\lambda(\\bigcup_{i=1,\\cdots,i_{k}}C_{i_{1},i_{2}\\cdots,i_{k}})}}\\\\ &{=\\sum_{i_{n+1},\\cdots,i_{k}}(\\gamma_{k})_{\\#}\\lambda(C_{i_{1},i_{2}\\cdots,i_{k}})}\\\\ &{=\\sum_{i_{n+1},\\cdots,i_{k}}\\lambda\\left(\\gamma_{k}^{-1}(C_{i_{1},i_{2}\\cdots,i_{k}})\\right)}\\\\ &{=\\sum_{i_{n+1},\\cdots,i_{k}}\\mathbb{P}(C_{i_{1},i_{2}\\cdots,i_{k}})=\\mathbb{P}(C_{i_{1},i_{2}\\cdots,i_{n}}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where we use $\\mathbb{P}(\\partial C_{i_{1},i_{2}\\cdots,i_{n}})=0$ and $(\\gamma_{k})_{\\#}\\lambda(\\partial C_{i_{1},i_{2}\\cdots,i_{n}})=0$ in the second and the last equation. We claim that $\\gamma_{\\#}\\lambda(\\partial C_{i_{1},i_{2}\\cdots,i_{n}})=0$ . For $k>n$ , we define ", "page_idx": 22}, {"type": "equation", "text": "$$\nB_{k}=\\bigcup_{\\partial C_{i_{1},i_{2}\\cdots,i_{k}}\\cap\\partial C_{i_{1},i_{2}\\cdots,i_{n}}\\neq\\emptyset}C_{i_{1},i_{2}\\cdots,i_{k}}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Let $k>k_{1}\\geqslant n$ and $B_{n}^{\\epsilon}=\\{x:d(x,\\partial C_{i_{1},i_{2}\\cdots,i_{n}})<\\epsilon\\}$ , we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n(\\gamma_{k})_{\\#}\\lambda\\left(\\operatorname{Int}(B_{k_{1}})\\right)\\leqslant(\\gamma_{k})_{\\#}\\lambda(B_{k_{1}})=\\mathbb{P}(B_{k_{1}})\\leqslant\\mathbb{P}\\left(B_{n}^{(2/3)^{k_{1}}\\sqrt{d}}\\right),\\quad\\forall k>k_{1},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where we use (23) and the f\u221aact that $(\\gamma_{k})_{\\#}\\lambda$ vanishes on $\\partial C_{i_{1},i_{2}\\cdots,i_{k}}$ in the first equality and $\\begin{array}{r}{\\mathrm{diam}(C_{i_{1},i_{2}\\cdots,i_{k_{1}}})\\ \\leqslant\\ \\left(\\frac{2}{3}\\right)^{k_{1}}\\sqrt{d}}\\end{array}$ in the last inequality. Let $k\\,\\rightarrow\\,\\infty$ , by Portmanteau Theorem, we get ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\varphi\\lambda(\\partial C_{i_{1},i_{2}\\cdots,i_{n}})\\leqslant\\gamma_{\\#}\\lambda\\left(\\operatorname{Int}(B_{k_{1}})\\right)\\leqslant\\operatorname*{lim}_{k\\to\\infty}(\\gamma_{k})_{\\#}\\lambda(B_{k_{1}})\\leqslant\\operatorname*{lim}_{k\\to\\infty}\\mathbb{P}\\left(B_{n}^{(2/3)^{k_{1}}\\sqrt{d}}\\right)=\\mathbb{P}\\left(B_{n}^{(2/3)}\\right)\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Since $k_{1}$ is arbitrary, let $k_{1}\\rightarrow\\infty$ , ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\gamma_{\\#}\\lambda(\\partial C_{i_{1},i_{2}\\cdots,i_{n}})\\leqslant\\operatorname*{lim}_{k_{1}\\rightarrow\\infty}\\mathbb{P}\\left(B_{n}^{(2/3)^{k_{1}}\\sqrt{d}}\\right)=\\mathbb{P}(\\partial C_{i_{1},i_{2}\\cdots,i_{n}})=0\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "and we conclude that $\\gamma_{\\#}\\lambda(\\partial C_{i_{1},i_{2}\\cdots,i_{n}})\\;=\\;0$ . By Portmanteau Theorem, let $k\\,\\rightarrow\\,\\infty$ in (23), we obtain $\\gamma_{\\#}\\lambda(C_{i_{1},i_{2}\\cdots,i_{n}})\\,=\\,\\mathbb{P}(C_{i_{1},i_{2}\\cdots,i_{n}})$ . Let $\\textstyle{\\mathcal{C}}^{*}\\,=\\,\\bigcup_{i=1}^{\\infty}{\\mathcal{C}}_{i}$ . Notice that for any open set $U\\in[0,1]^{d}$ , $\\begin{array}{r}{U=\\bigcup_{C\\subset U,C\\in{\\mathcal{C}}^{*}}C}\\end{array}$ , which means the $\\sigma$ -algebra generated by $\\mathcal{C}^{*}$ contains all Borel sets. Besides, $\\mathcal{C}^{*}$ is a $\\pi$ -system. Hence, $\\lambda\\left(\\gamma^{-1}(U)\\right)=\\mathbb{P}(U)$ for all Lebesgue measurable set $U$ . ", "page_idx": 23}, {"type": "text", "text": "Step 4: Holder Continuity of $\\gamma$ . We verify the condition of [47, Theorem 1]. ${\\mathcal{Z}}_{n}$ and $\\ensuremath{{\\mathcal{C}}}_{n}$ in the above construction correspond to developments $\\alpha_{k},\\beta_{k}$ in [47, Theorem 1]. Define $f_{k}(I_{O_{k}(i)})=$ $C_{O_{k}(i)},\\;i=1,\\cdots\\,,2^{k d}$ as a map from ${\\mathcal{Z}}_{n}$ to $\\ensuremath{{\\mathcal{C}}}_{n}$ . ", "page_idx": 23}, {"type": "text", "text": "1. By construction, if $I_{1}\\in\\mathcal{Z}_{k},I_{2}\\in\\mathcal{Z}_{k-1},I_{1}\\subset I_{2},\\,f_{k}(I_{1})\\subset f_{k-1}(I_{2}).$ ", "page_idx": 23}, {"type": "text", "text": "2. If $I_{1},I_{2}\\in{\\mathcal{T}}_{k}$ and $I_{1}\\cap I_{2}\\neq\\emptyset,\\,I_{1},\\,I_{2}$ are adjacent. By Lemma 1, $f_{k}(I_{1}),f_{k}(I_{2})$ share a $(d-1)$ -dimensional boundary. Therefore, $f_{k}\\bar{(}I_{1})\\cap f_{k}\\bar{(}I_{2})\\neq\\emptyset$ . ", "page_idx": 23}, {"type": "text", "text": "3. We have $|{\\mathcal{C}}_{k-1}|\\leqslant2^{d}|{\\mathcal{C}}_{k}|$ . ", "page_idx": 23}, {"type": "text", "text": "4. By property 3, ", "page_idx": 23}, {"type": "equation", "text": "$$\nR(\\mathcal C_{n})\\leqslant\\prod_{k=1}^{n-1}\\left(1+2^{-k}\\right)<\\prod_{k=1}^{\\infty}\\left(1+2^{-k}\\right)<\\infty.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Let $\\begin{array}{r}{A=\\prod_{k=1}^{\\infty}\\left(1+2^{-k}\\right)}\\end{array}$ . We have for any $C\\in{\\mathcal{C}}_{n}$ , by Assumption 5 ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\dim^{d}(C)\\leqslant R(\\mathcal C_{n})\\lambda(C)\\leqslant A C_{1}^{-1}{\\mathbb P}(C)\\leqslant A C_{1}^{-1}\\operatorname*{max}_{I\\in\\mathbb Z}\\left(\\dim(I)\\right),\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $C_{1}$ is the constant in Assumption 5. ", "page_idx": 23}, {"type": "text", "text": "5. Since $\\mathcal{T}_{k}$ consists of one-dimensional intervals, we have diam $\\begin{array}{r}{\\mathsf{\\Omega}_{1}(\\mathbb{Z}_{k})=\\operatorname{gap}(\\mathbb{Z}_{k})}\\end{array}$ . ", "page_idx": 23}, {"type": "text", "text": "Therefore, $\\gamma$ is $1/d$ -Holder continuous. ", "page_idx": 23}, {"type": "text", "text": "Proof of Theorem 3. The proof is similar to the proof of Theorem 2. The difference is that in the first part, we use a deep ReLU network to approximate the Holder continuous curve $\\gamma$ instead of wide ReLU network. ", "page_idx": 23}, {"type": "text", "text": "Let $\\mu_{i}=\\mathbb{P}_{C_{i}}/\\mathbb{P}(C_{i})$ . By Assumption 4 and Assumption 5, for each connected component $C_{i}$ of the support, there exists Lipschitz maps $g_{2}^{i}\\in\\mathcal{H}([0,1]^{d_{i}^{C}},C_{i})\\cap\\mathcal{F}_{L}([0,1]^{d_{i}^{C}},C_{i})$ such that ", "page_idx": 23}, {"type": "equation", "text": "$$\n(g_{2}^{i})_{\\#}^{-1}\\mu_{i}(B)\\geqslant C_{g}\\lambda(B)/\\mathbb{P}(C_{i}),\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "for any measurable set $B\\subset[0,1]^{d_{i}^{C}}$ . By Proposition 1, there exist a $1/d_{i}^{C}$ -Holder continuous curve $\\gamma_{i}:[0,1]\\rightarrow[0,1]^{d_{i}^{C}}$ such that $(\\gamma_{i})_{\\#}\\lambda=(g_{2}^{i})_{\\#}^{-1}\\mu_{i}$ . ", "page_idx": 23}, {"type": "text", "text": "By [53, Theorem 2] (take $\\omega(x)=C\\|x\\|^{1/d}$ where $C$ is the Holder continuity constant factor), there exists deep ReLu network $\\hat{g}_{1}^{i,j}$ of width 12 and depth $L_{1}$ such that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\hat{g}_{1}^{i,j}-(\\gamma_{i})_{j}\\|_{\\infty}\\leqslant O\\left(L_{1}^{-2/d_{i}^{C}}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $(\\gamma_{i})_{j}$ is the $j$ -coordinate of $\\gamma_{i}$ . Let $\\hat{g}_{1}^{i}(x)=\\left(\\hat{g}_{1}^{i,1}(x),\\cdot\\cdot\\cdot\\,,\\hat{g}_{1}^{i,d}(x)\\right)$ , we get ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\|\\hat{g}_{1}^{i}-\\gamma_{i}\\|_{\\infty}\\leqslant O\\left(L_{1}^{-2/d_{i}^{C}}\\right).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Thus, the width of $\\hat{g}_{1}^{i}$ is $\\Theta\\left(d_{i}^{C}\\right)$ . By Lemma 3 ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{W\\left(\\left(g_{2}^{i}\\right)_{\\#}^{-1}\\mu_{i},P(\\hat{g}_{1}^{i}(U_{i}))\\right)=W\\left((\\gamma_{i})_{\\#}U_{i},P(\\hat{g}_{1}^{i}(U_{i}))\\right)\\leqslant\\|\\hat{g}_{1}^{i}-\\gamma_{i}\\|_{\\infty}\\leqslant O\\left(L_{1}^{-2/d_{i}^{C}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "The rest are the same as proof of Theorem 2. ", "page_idx": 23}, {"type": "text", "text": "Proof of Corollary $^{\\,I}$ . Suppose that structure equations of $\\mathcal{M}$ are ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\boldsymbol{\\gamma}_{i}=\\left\\{\\begin{array}{l l}{f_{i}\\left(\\mathbf{Pa}(V_{i}),U_{V_{i}}\\right),}&{V_{i}\\mathrm{~is~continuous},}\\\\ {\\mathrm{arg~max}_{k\\in\\left[n_{i}\\right]}\\left\\{g_{k}^{V_{i}}+\\log\\left(f_{i}\\left(\\mathbf{Pa}(V_{i}),U_{V_{i}}\\right)\\right)_{k}\\right\\},}&{V_{i}\\mathrm{~is~categorical,}\\|f_{i}\\|_{1}=1}\\end{array}\\right.,\\quad f_{i}\\in\\mathcal{F}_{L}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Let $d_{i}^{\\mathrm{in}}$ be the input dimension of $f_{i}$ , $d_{i}^{\\mathrm{out}}$ be the output dimension, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{f}_{i}=\\left\\{\\begin{array}{l l}{\\arg\\operatorname*{min}_{\\hat{f}_{i}\\in\\mathcal{N}\\mathcal{M}_{i_{i}^{\\mathrm{in}},d_{i}^{\\mathrm{out}}}(d_{i}^{\\mathrm{out}}(2d_{i}^{\\mathrm{in}}+10),L_{0})}\\left\\|\\hat{f}_{i}-f_{i}\\right\\|_{\\infty},}&{V_{i}\\;\\mathrm{continuous},}\\\\ {\\operatorname*{max}\\{0,\\arg\\operatorname*{min}_{\\hat{f}_{i}\\in\\mathcal{N}\\mathcal{M}_{i_{i}^{\\mathrm{in}},d_{i}^{\\mathrm{out}}}(d_{i}^{\\mathrm{out}}(2d_{i}^{\\mathrm{in}}+10),L_{0})}\\left\\|\\hat{f}_{i}-f_{i}\\right\\|_{\\infty}\\},}&{V_{i}\\;\\mathrm{categorical}.}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "We truncate the neural network at 0 for categorical variables because the propensity functions are required to be non-negative. This truncate operation will not influence the approximation error since $f_{i}$ are non-negative. According to Assumption 3, $f_{i}$ are Lipschitz continuous. By [53, Theorem 2], we have, if $V_{i}$ is continuous, $\\|\\hat{f}_{i}-f_{i}\\|_{\\infty}\\leqslant O(L_{0}^{-1/d_{i}^{\\mathrm{in}}})\\leqslant O(L_{0}^{-1/d_{\\operatorname*{max}}^{\\mathrm{in}}})$ . If $V_{i}$ is categorical, let $S\\,=\\,\\|\\hat{f}\\left(\\operatorname{pa}(v_{k}),\\boldsymbol{u}_{V_{k}}\\right)-f\\left(\\operatorname{pa}(v_{k}),\\boldsymbol{u}_{V_{k}}\\right)\\|_{\\infty},\\,p\\,=\\,f\\left(\\operatorname{pa}(v_{k}),\\boldsymbol{u}_{V_{k}}\\right),\\hat{p}\\,=$ $\\hat{f}\\left(\\mathrm{pa}(v_{k}),\\pmb{u}_{V_{k}}\\right)/\\|\\hat{f}\\|_{1}$ . If $S>1/(2K)$ , $\\|\\pmb{p}-\\hat{\\pmb{p}}\\|_{\\infty}\\leqslant2K S$ . If $S\\leqslant1/(2K)$ , $\\begin{array}{l}{\\displaystyle\\|p-\\hat{p}\\|_{\\infty}\\leqslant\\frac{1}{\\|\\hat{f}\\|_{1}}\\|\\hat{f}\\left(\\mathtt{p a}(v_{k}),u_{V_{k}}\\right)-f\\left(\\mathtt{p a}(v_{k}),u_{V_{k}}\\right)\\|_{\\infty}+\\frac{1}{\\|\\hat{f}\\|_{1}}|\\|\\hat{f}\\|_{1}-1|\\|f\\|_{\\infty}}\\\\ {\\leqslant\\frac{\\left(n_{i}+1\\right)}{\\|\\hat{f}\\|_{1}}S\\leqslant\\frac{\\left(n_{i}+1\\right)}{1-n_{i}S}S\\leqslant2(K+1)S}\\end{array}$ where we use Assumption 3 that $n_{i}\\leqslant K$ and $S\\leqslant1/(2K)$ . Thus, ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|f_{i}-\\hat{f}_{i}/\\|\\hat{f}_{i}\\|_{1}\\|_{\\infty}\\leqslant O(\\|f_{i}-\\hat{f}_{i}\\|_{\\infty})\\leqslant O(L_{0}^{-1/d_{\\operatorname*{max}}^{\\mathrm{in}}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "By Theorem 3, there exists a neural network ${\\hat{g}}_{j}$ with architecture in Theorem 3 such that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{W(P(U_{j}),P(\\hat{g}_{j}(Z_{j})))\\leqslant O\\left(L_{1}^{-2/d_{j}^{U}}+L_{2}^{-2/d_{j}^{U}}+(\\tau-\\tau\\log\\tau)\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $Z_{j}\\sim U([0,1]^{N_{C,j}})$ i.i.d., $N_{C,j}$ is the number of connected components of support of $U_{j},d_{j}^{U}$ is the dimension of latent variables $U_{j}$ and $L_{1},L_{2},\\tau$ are hyperparameters of the neural network defined in Theorem 3. Let $\\hat{U}_{j}=\\hat{g}_{j}(Z_{j})$ , By Lemma 5, ", "page_idx": 24}, {"type": "equation", "text": "$$\nV(P(U_{1},\\cdots,U_{n_{U}}),P(\\hat{U}_{1},\\cdots,\\hat{U}_{n_{U}}))\\leqslant\\sum_{j=1}^{n_{U}}W(P(U_{j}),P(\\hat{g}_{j}(Z_{j})))\\leqslant O\\left(L_{1}^{-2/d_{j}^{U}}+L_{2}^{-2/d_{j}^{U}}+(\\gamma+\\gamma)\\beta_{j}\\right),\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Let $\\hat{\\mathcal{M}}$ be the casual model with the following structure equations ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\hat{V}_{i}=\\left\\{\\begin{array}{l l}{\\hat{f}_{i}\\left(\\mathbf{Pa}(\\hat{V}_{i}),(\\hat{g}_{j}(Z_{C_{j}}))_{U_{C_{j}}\\in U_{V_{i}}}\\right),}&{V_{i}\\mathrm{~is~continuous,}}\\\\ {\\arg\\operatorname*{max}_{k\\in[n_{i}]}\\Big\\{g_{k}+\\log\\left(\\hat{f}_{i}\\left(\\mathbf{Pa}(\\hat{V}_{i}),(\\hat{g}_{j}(Z_{C_{j}}))_{U_{C_{j}}\\in U_{V_{i}}}\\right)\\right)_{k}\\Big\\},}&{V_{i}\\mathrm{~is~categorical.}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "By Theorem 1, we get ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{W\\left(P^{M^{*}}(V(t)),P^{\\hat{M}}(V(t))\\right)\\leqslant O(\\sum_{V_{i}\\mathrm{~coninuous}}\\lVert f_{i}-\\hat{f}_{i}\\rVert_{\\infty}+\\sum_{V_{i}\\mathrm{~categoical}}\\lVert f_{i}-\\hat{f}_{i}/\\rVert\\hat{f}_{i}\\rVert_{1}\\lVert_{\\infty}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+\\ W(P(U_{1},\\cdots,U_{n_{U}}),P(\\hat{U}_{1},\\cdots,\\hat{U}_{n_{U}})))}\\\\ &{\\leqslant O(L_{0}^{-2/\\hat{d}_{\\operatorname*{max}}^{3}}+L_{1}^{-2/d_{\\operatorname*{max}}^{U}}+L_{2}^{-2/d_{\\operatorname*{max}}^{U}}+(\\tau-\\tau\\log\\tau)),}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "for any intervention $\\mathbf{\\nabla}T=\\mathbf{\\nabla}t$ . ", "page_idx": 24}, {"type": "text", "text": "C Proof of Consistency ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "C.1 Inconsistent Counterexample (Proposition 2) ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Proposition 5. There exists a constant $c>0$ and an SCM $\\mathcal{M}^{\\ast}$ satisfying Assumptions 1-5 with a backdoor causal graph such that there is no unobserved confounding in $\\mathcal{M}^{\\ast}$ . For any $\\epsilon\\,>\\,0$ , there exists an $S C M\\,\\mathcal{M}_{\\epsilon}$ with the same causal graph such that $W(P^{\\mathcal{M}^{*}}(V),P^{\\mathcal{M}_{\\epsilon}}(V))\\leq\\epsilon$ and $\\lvert A T E_{\\mathcal{M}^{*}}-A T E_{\\mathcal{M}_{\\epsilon}}\\rvert>c$ . ", "page_idx": 24}, {"type": "text", "text": "Proof. Let $V\\,=\\,\\{X,T,Y\\}$ be the covariate, treatment and outcome respectively. $T$ is a binary variable. Let structure equations of causal model $\\mathcal{M}_{\\delta}$ be ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{X=\\left\\{\\begin{array}{l l}{-1}&{,\\mathrm{w.p.}\\,1/2}\\\\ {0}&{,\\mathrm{w.p.}\\,1/4}\\\\ {\\delta}&{,\\mathrm{w.p.}\\,1/4,}\\end{array}\\right.}\\\\ {P(T=1|X=x)=\\left\\{\\begin{array}{l l}{1/2}&{,x=-1\\;\\mathrm{or}\\;0,}\\\\ {3/4}&{,x=\\delta,}\\end{array}\\right.}\\\\ {Y=\\left\\{\\begin{array}{l l}{T+U_{y}}&{,X<0}\\\\ {X/\\delta+U_{y}}&{,X\\geqslant0}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where all latent variables are independent and $U_{y}$ is mean-zero noise. The distribution of $\\mathcal{M}_{\\delta}$ is ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{P_{X,T,Y}(0,1,y)=p_{U_{y}}(y)/8,P_{X,T,Y}(0,0,y)=p_{U_{y}}(y)/8\\mathrm{~~\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\}}\\\\ &{}&{P_{X,T,Y}(\\delta,1,y)=3p_{U_{y}}(y-1)/16,P_{X,T,Y}(\\delta,0,y)=p_{U_{y}}(y-1)/16\\mathrm{.~\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\}}\\\\ &{}&{P_{X,T,Y}(-1,1,y)=p_{U_{y}}(y-1)/4,P_{X,T,Y}(-1,0,y)=p_{U_{y}}(y)/4.\\mathrm{~\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "As $\\delta\\rightarrow0$ , distribution of $\\mathcal{M}_{\\delta}\\stackrel{d}{\\to}\\mathcal{M}^{*}$ , where structure equations of $\\mathcal{M}^{\\ast}$ are ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{P(U_{1}=1)=3/5,P(U_{1}=0)=2/5,P(U_{2}=1)=1/3,P(U_{1}=0)=2/3,}&{}\\\\ {X=\\left\\{\\begin{array}{l l}{-1}&{,\\mathrm{w.p.1}/2,}\\\\ {0}&{,\\mathrm{w.p.1}/2,}\\end{array}\\right.}\\\\ {P(T=1|X=x)\\,=\\,\\left\\{\\begin{array}{l l}{1/2}&{,x=-1,}\\\\ {5/8}&{,x=0,}\\end{array}\\right.}\\\\ {Y\\,=\\,\\left\\{\\begin{array}{l l}{T+U_{y}}&{,X=-1,}\\\\ {U_{1}+U_{y}}&{,X=0,T=1,}\\\\ {U_{2}+U_{y}}&{,X=0,T=0,}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $U_{1},U_{2},U_{y}$ are independent. The distribution of $\\mathcal{M}^{\\ast}$ is ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{P_{X,T,Y}(0,1,y)\\ =p_{U_{y}}(y)/8+3p_{U_{y}}(y-1)/16,}\\\\ &{P_{X,T,Y}(0,0,y)\\ =p_{U_{y}}(y)/8+p_{U_{y}}(y-1)/16,}\\\\ &{P_{X,T,Y}(-1,1,y)\\ =p_{U_{y}}(y-1)/4,}\\\\ &{P_{X,T,Y}(-1,0,y)\\ =p_{U_{y}}(y)/4.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "It is easy to see that $\\mathcal{M}^{\\ast}$ satisfies Assumption 1-5. Some calculation gives $W(P^{\\mathcal{M}^{\\ast}}(V),P^{\\mathcal{M}_{\\delta}}(V))\\leqslant\\delta/2$ . It is easy to calculate the ATE of the two models. ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathrm{ATE}_{\\mathcal{M}^{\\ast}}=19/30,\\,\\mathrm{ATE}_{\\mathcal{M}_{\\delta}}=1/2.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "This example implies that even as the Wasserstein distance between $P^{\\mathcal{M}^{\\ast}}(V),P^{\\mathcal{M}_{\\delta}}(V)$ converges to zero, their ATEs do not change. As mentioned in the main body, this problem is caused by the violation of Lipschitz continuity assumption for $\\mathcal{M}_{\\delta}$ . Note that in $\\mathcal{M}_{\\delta}\\bar{,}\\,\\mathbb{E}[Y|X,T]\\,=\\,X/\\delta$ . As $\\delta\\rightarrow0$ , the Lipschitz constant explodes. ", "page_idx": 25}, {"type": "text", "text": "This problem may also arise in (6). If the distribution ball $B_{n}\\,=\\,\\{\\hat{\\mathcal{M}}\\,:\\,W(P^{\\hat{\\mathcal{M}}},P_{n}^{\\mathcal{M}^{\\ast}})\\,\\leqslant\\,\\alpha_{n}\\}$ includes a small ball around the true distribution $S_{\\epsilon}\\,=\\,\\{\\hat{\\mathcal{M}}\\,:\\,W(P^{\\hat{\\mathcal{M}}},P^{\\mathcal{M}^{\\ast}})\\,\\leqslant\\,\\epsilon\\}\\,\\subset\\,B_{n}$ and the NCM is expressive enough to approximate all the SCMs in the $S_{\\epsilon}$ , this example tells us the confidence interval may not shrink to one point as sample size increases to infinity even if the model is identifiable. ", "page_idx": 25}, {"type": "text", "text": "C.2 Proof of Theorem 4 ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "To prove Theorem 4, we will need the following proposition. ", "page_idx": 25}, {"type": "text", "text": "Proposition 6. Given a metric space $(M,d)$ and sets $\\Theta_{1}\\subset\\Theta_{2}\\cdots\\subset\\Theta_{n}\\subset\\cdots\\subset\\Theta_{\\infty}\\subset M$ , where $\\Theta_{\\infty}\\,=\\,\\overline{{\\cup_{n=1}^{\\infty}\\Theta_{n}}}$ , positive sequences $\\{\\epsilon_{n}\\}_{n\\in\\mathbb{N}}$ , $\\{\\delta_{n}\\}_{n\\in\\mathbb{N}}$ such that $\\begin{array}{r}{\\operatorname*{lim}_{n\\to\\infty}\\epsilon_{n}=\\operatorname*{lim}_{n\\to\\infty}\\tau_{n}=}\\end{array}$ $\\operatorname*{lim}_{n\\rightarrow\\infty}\\delta_{n}=0$ and continuous functions $f,g_{n}:\\Theta_{\\infty}\\to\\mathbb{R},$ , suppose that ", "page_idx": 26}, {"type": "text", "text": "1. $\\Theta_{\\infty}$ is compact. ", "page_idx": 26}, {"type": "text", "text": "2. $g_{n}$ satisfies ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\begin{array}{c}{\\|g_{n}(\\theta_{1})-g_{n}(\\theta_{2})\\|\\leqslant L_{g}d(\\theta_{1},\\theta_{2})+\\tau_{n},\\quad\\forall\\theta_{1},\\theta_{2}\\in\\Theta}\\\\ {a n d\\operatorname*{sup}_{\\theta\\in\\Theta_{\\infty}}\\|g_{n}(\\theta)-g(\\theta)\\|\\leqslant\\delta_{n},g_{n}\\geqslant0.}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "3. There exists a compact subset $\\tilde{\\Theta}_{\\infty}\\subset\\Theta_{\\infty}$ such that for any $\\theta_{0}\\in\\tilde{\\Theta}_{\\infty}$ , there exists $\\theta\\in\\Theta_{n}$ such that $d(\\theta,\\theta_{0})\\leqslant\\epsilon_{n}$ . ", "page_idx": 26}, {"type": "text", "text": "Consider the following optimization problems: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\operatorname*{min}_{\\theta\\in\\Theta_{n}}f(\\theta),}\\\\ {s.t.\\ g_{n}(\\theta)\\leqslant L_{g}\\epsilon_{n}+\\delta_{n},}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "equation", "text": "$$\n\\displaystyle\\operatorname*{min}_{\\theta\\in\\Theta_{\\infty}}f(\\theta),\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "and ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\displaystyle\\operatorname*{min}_{\\theta\\in\\tilde{\\Theta}_{\\infty}}f(\\theta),\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "We assume that the feasible region of (26) and (27) are nonempty. Let $f_{n}^{*},f^{*},\\tilde{f}^{*}$ be the minimal of (25), (26) and (27) respectively. Then, $[\\operatorname*{lim}\\operatorname*{inf}_{k\\to\\infty}f_{r}^{:}$ , $\\operatorname*{lim}\\operatorname*{sup}_{k\\to\\infty}f_{n}^{*}]\\subset[f^{*},\\tilde{f}^{*}]$ . ", "page_idx": 26}, {"type": "text", "text": "Proof of Proposition 6. By compactness of $\\Theta_{\\infty}$ and $\\tilde{\\Theta}_{\\infty}$ , the minimizers of (26) and (27) are achievable insider these two sets. Let $\\theta^{*}\\in\\Theta_{\\infty},\\tilde{\\theta}^{*}\\in\\tilde{\\Theta}_{\\infty}$ be the minimizer of (26) and (27). We first prove that $\\operatorname*{lim}\\operatorname*{sup}_{n\\to\\infty}f_{n}^{*}\\leqslant\\tilde{f}^{*}$ . Note that by (24), the limiting function $g$ is $L_{g}$ -Lipschitz. By condition 3, there exist $\\theta_{n}\\in\\Theta_{n}$ such that $d\\left(\\theta_{n},\\tilde{\\theta}^{*}\\right)\\leqslant\\epsilon_{n}$ . Note that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{g_{n}(\\theta_{n})\\leqslant|g(\\theta_{n})-g_{n}(\\theta_{n})|+|g(\\theta_{n})-g\\left(\\theta^{*}\\right)|+|g\\left(\\theta^{*}\\right)|}\\\\ &{\\leqslant\\delta_{n}+L_{g}d\\left(\\theta_{n},\\theta^{*}\\right)\\leqslant\\delta_{n}+L_{g}\\epsilon_{n}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Therefore, $\\theta_{n}$ is a feasible point of (25). We have $\\begin{array}{r l}{\\operatorname*{lim}\\operatorname*{sup}_{n\\to\\infty}f_{n}^{*}\\;\\leqslant\\;\\operatorname*{lim}\\operatorname*{sup}_{n\\to\\infty}f(\\theta_{n})\\;=}\\end{array}$ $f\\left(\\tilde{\\theta}^{*}\\right)=\\tilde{f}^{*}$ . ", "page_idx": 26}, {"type": "text", "text": "Next, we argue that $\\operatorname*{lim}\\operatorname*{inf}_{n\\to\\infty}f_{n}^{*}\\,\\geq\\,f^{*}$ . If this equation does not hold, there exists $\\epsilon>0$ and subsequence $\\left\\{f_{n_{k}}^{*}\\right\\}_{k\\in\\mathbb{N}}$ such that $f_{n_{k}}^{*}<f^{*}-\\epsilon,\\forall k\\in\\mathbb{N}$ . By compactness of $\\Theta_{\\infty}$ , for each $k$ , there exists a subsequence of $\\theta_{n_{k}}^{\\ast}\\in\\Theta_{\\infty}$ such that $\\theta_{n_{k}}^{*}$ satisfies constraint of (25) and $f_{n_{k}}^{*}=f\\left(\\theta_{n_{k}}^{*}\\right)$ . By compactness, \u03b8\u2217nk k\u2208 has a converging subseqence. Without loss of generality, we may assume that $\\left\\{\\theta_{n_{k}}^{*}\\right\\}_{k\\in\\mathbb{N}}$ converges to $\\hat{\\theta}^{*}\\in\\Theta_{\\infty}$ . Since $g_{k}$ converge uniformly to $g$ , ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{k\\rightarrow\\infty}g_{k}\\left(\\theta_{n_{k}}^{*}\\right)\\leqslant\\operatorname*{lim}_{k\\rightarrow\\infty}\\left|g_{k}\\left(\\theta_{n_{k}}^{*}\\right)-g\\left(\\theta_{n_{k}}^{*}\\right)\\right|+g\\left(\\theta_{n_{k}}^{*}\\right)=g\\left(\\hat{\\theta}^{*}\\right)\\leqslant\\operatorname*{lim}_{k\\rightarrow\\infty}\\alpha_{k}=0.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "${\\hat{\\theta}}^{*}$ is a feasible point of Equation (26). Since $f$ is continuous on $\\Theta$ , ", "page_idx": 26}, {"type": "equation", "text": "$$\nf\\left(\\hat{\\theta}^{*}\\right)=\\operatorname*{lim}_{k\\to\\infty}f\\left(\\theta_{n_{k}}^{*}\\right)\\leqslant f^{*}-\\epsilon.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "which leads to contradiction. ", "page_idx": 26}, {"type": "text", "text": "Proof of Theorem 4. We begin by defining a proper metric space. By Assumption 3, all random variables are bounded. Suppose that $\\mathrm{max}_{i,j}\\,\\bigl\\{\\|V_{i}\\|_{\\infty},\\|U_{j}\\|_{\\infty}\\bigr\\}\\leqslant K$ . A canonical causal model $\\mathcal{M}\\in\\mathcal{M}(\\mathcal{G},\\mathcal{F},U)$ is decided by ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\theta_{\\mathcal{M}}=(f_{1},\\cdot\\cdot\\cdot,f_{n_{V}},P(U_{1}),\\cdot\\cdot\\cdot\\,,P(U_{n_{U}})),}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where $f_{i}$ are functions in the structure equations (3) and $U_{j}$ are uniform parts of the latent variables. We denote ${\\mathcal{M}}^{\\theta}$ to be the SCM represented by $\\theta$ , the underlying SCM be $\\boldsymbol{\\mathcal{M}}^{\\theta^{*}}$ and $P^{\\theta}$ to be the distribution of ${\\mathcal{M}}^{\\theta}$ . We consider the space ", "page_idx": 27}, {"type": "equation", "text": "$$\nM={\\mathcal{F}}_{V_{1}}\\times\\cdot\\cdot\\cdot\\times{\\mathcal{F}}_{V_{n_{V}}}\\times{\\mathcal{P}}\\left(\\left[-K,K\\right]^{d_{1}^{U}}\\right)\\cdot\\cdot\\cdot\\times{\\mathcal{P}}\\left(\\left[-K,K\\right]^{d_{n_{U}}^{U}}\\right),\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathcal{F}_{V_{i}}=\\left\\{\\begin{array}{l l}{\\mathcal{F}_{L}^{K}\\left([-K,K]^{d_{i,\\mathrm{in}}^{V}},[-K,K]^{d_{i,\\mathrm{out}}^{V}}\\right),}&{V_{i}\\mathrm{~continuous},}\\\\ {\\{f:\\|f\\|_{1}=1,f\\in\\mathcal{F}_{L}^{K}\\left([-K,K]^{d_{i,\\mathrm{in}}^{V}},[-K,K]^{d_{i,\\mathrm{out}}^{V}}\\right)\\}}&{V_{i}\\mathrm{~categorical},}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "$d_{i,\\mathrm{in}}^{V},d_{i,\\mathrm{out}}^{V}$ are the input and output dimensions of $f_{i}$ , ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathcal{F}_{L}^{K}=\\{f:\\|f\\|_{\\infty}\\leqslant K,f\\,\\mathrm{Lipschitz}\\,\\mathrm{continuous}\\}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "and $\\mathcal{P}(K)$ is the probability space on $K$ . For $\\theta\\;=\\;\\left(f_{1},\\cdot\\cdot\\cdot\\,,\\,f_{n_{V}},P(U_{1}),\\cdot\\cdot\\cdot\\,,P(U_{n_{U}})\\right),\\theta^{\\prime}\\;=$ $\\left(f_{1}^{\\prime},\\cdot\\cdot\\cdot,f_{n_{V}}^{\\prime},P(U_{1}^{\\prime}),\\cdot\\cdot\\cdot\\cdot,P(U_{n_{U}}^{\\prime})\\right)$ , we define a metric on $M$ ", "page_idx": 27}, {"type": "equation", "text": "$$\nd(\\theta,\\theta^{\\prime})=\\sum_{k=1}^{n_{V}}\\|f_{k}-f_{k}^{\\prime}\\|_{\\infty}+\\sum_{k=1}^{n_{U}}W(P(U_{k}),P(U_{k}^{\\prime})).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Theorem 1 states that the Wasserstein distance between two causal models is Lipschitz with respect to metric $d$ . Now, we define $\\Theta_{n}$ . Let ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathcal{P}_{n}=\\left\\{P(U):U\\mathrm{~is~the~latent~distribution~of~}\\hat{\\mathcal{M}}\\in\\operatorname{NCM}_{\\mathcal{G}}\\left(\\mathcal{F}_{0,n},\\mathcal{F}_{1,n},\\mathcal{F}_{2,n},\\tau_{n}\\right)\\right\\}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "In other words, ${\\mathcal{P}}_{n}$ contains all the push-forward measures of the uniform distribution by neural networks. We denote $\\Theta_{n}=\\hat{\\mathcal{F}}_{V_{1},n}\\stackrel{\\cdot}{\\times}\\cdot\\cdot\\cdot\\times\\hat{\\mathcal{F}}_{V_{n_{V}},n}\\times\\times\\mathcal{P}_{n}$ , where ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\hat{\\mathcal{F}}_{V_{i},n}=\\left\\{\\begin{array}{l l}{\\mathcal{F}_{0,n},}&{V_{i}\\ \\mathrm{continuous},}\\\\ {\\{f/\\|f\\|_{1}:f\\in\\mathcal{F}_{0,n}\\}}&{V_{i}\\ \\mathrm{categorical},}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "and ${\\mathcal{F}}_{0,n}$ is defined in Theorem 4. Note that by construction, latent variables are independent and ${\\mathcal{P}}_{n}$ can be decomposed into direct produce $\\mathcal{P}_{n,1}\\times\\cdot\\cdot\\cdot\\times\\mathcal{P}_{n,n_{U}}.$ . Let ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\Theta_{\\infty}=\\overline{{\\cup_{n=1}^{\\infty}\\Theta_{n}}},g_{n}(\\theta)=S_{\\lambda_{n}}\\left(P_{n}^{\\theta^{*}}(V),P_{m_{n}}^{\\theta}(V)\\right),f(\\theta)=\\mathbb{E}_{t\\sim\\mu_{T}}\\mathbb{E}_{M^{\\theta}}[F(V_{1}(t),\\cdot\\cdot\\cdot\\,,V_{n\\nu}(t))]\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "and $\\tilde{\\Theta}_{\\infty}=\\{\\theta^{\\ast}\\}$ . Note that $f(\\theta)$ is a continuous function since by Theorem 1 and the fact that $F$ is Lispchitz continuous, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{|f(\\theta)-f(\\theta^{\\prime})|\\leqslant\\mathbb{E}_{t\\sim\\mu_{T}}|W(P^{\\theta}(\\pmb{V}(t)),P^{\\theta^{\\prime}}(\\pmb{V}(t))|}&{}\\\\ {\\leqslant\\mathbb{E}_{t\\sim\\mu_{T}}[O(d(\\theta,\\theta^{\\prime}))]=O(d(\\theta,\\theta^{\\prime})).}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Now, we verify the conditions in Proposition 6. ", "page_idx": 27}, {"type": "text", "text": ". By Arzel\u00e0\u2013Ascoli theorem, $\\mathcal{F}_{L}^{K}\\left([-K,K]^{d_{i,\\mathrm{in}}^{V}},[-K,K]^{d_{i,\\mathrm{out}}^{V}}\\right)$ are precompact set with respect to the infinity norm in space of continuous functions. And thus ${\\mathcal{F}}_{V_{i}}$ are compact sets. ", "page_idx": 27}, {"type": "text", "text": "Since measures in $\\mathcal{P}\\left([-K,K]^{d_{j}^{U}}\\right)$ are tight , $\\mathcal{P}\\left([-K,K]^{d_{j}^{U}}\\right)$ are compact with respect to weak topology by Prokhorov\u2019s theorem. And the Wasserstein distance metricizes the weak topology. Thus, $\\mathcal{P}\\left([-K,K]^{d_{j}^{U}}\\right)$ are compact and the space $(M,d)$ is compact space. Closed set $\\Theta_{\\infty}\\subset M$ is also compact. ", "page_idx": 27}, {"type": "text", "text": "2. By definition of $g_{n}$ , ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|g_{n}(\\theta)-g_{n}(\\theta^{\\prime})|=|S_{\\lambda_{n}}\\left(P_{n}^{\\theta^{*}}(V),P_{m_{n}}^{\\theta}(V)\\right)-S_{\\lambda_{n}}\\left(P_{n}^{\\theta^{*}}(V),P_{m_{n}}^{\\theta^{\\prime}}(V)\\right)|}\\\\ &{\\phantom{\\quad\\quad\\quad}\\leqslant|W\\left(P_{n}^{\\theta^{*}}(V),P_{m_{n}}^{\\theta}(V)\\right)-W\\left(P_{n}^{\\theta^{*}}(V),P_{m_{n}}^{\\theta^{\\prime}}(V)\\right)|+4(\\log(m n)+1)\\lambda_{n}}\\\\ &{\\phantom{\\quad\\quad\\quad}\\leqslant W\\left(P_{m_{n}}^{\\theta}(V),P_{m_{n}}^{\\theta^{\\prime}}(V)\\right)+4(\\log(m n)+1)\\lambda_{n}}\\\\ &{\\phantom{\\quad\\quad\\quad}\\leqslant O(d(\\theta,\\theta^{\\prime}))+4(1+\\log(n m_{n}))\\lambda_{n},}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where we use Lemma 7 in the second inequality and triangle inequality and Theorem 1 in the third inequality. By the condition in Theorem 4, the second term $(1+\\log(n m_{n}))\\lambda_{n}\\to0$ as $n\\to\\infty$ . Therefore, (24) is verified with $\\tau_{n}=4(1+\\log(n m_{n}))\\lambda_{n}$ . ", "page_idx": 28}, {"type": "text", "text": "We then verify the uniform convergence of $g_{n}$ . By triangle inequality, ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|g_{n}(\\theta)-g(\\theta)|=|S_{\\lambda_{n}}\\left(P_{n}^{\\theta^{*}}(V),P_{m_{n}}^{\\theta}(V)\\right)-W\\left(P^{\\theta^{*}}(V),P^{\\theta}(V)\\right)|}\\\\ &{\\qquad\\qquad\\leqslant|S_{\\lambda_{n}}\\left(P_{n}^{\\theta^{*}}(V),P_{m_{n}}^{\\theta}(V)\\right)-W\\left(P_{n}^{\\theta^{*}}(V),P_{m_{n}}^{\\theta}(V)\\right)|}\\\\ &{\\qquad\\qquad\\qquad+\\left|W\\left(P_{n}^{\\theta^{*}}(V),P_{m_{n}}^{\\theta}(V)\\right)-W\\left(P^{\\theta^{*}}(V),P^{\\theta}(V)\\right)\\right|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "By Lemma 7, $|S_{\\lambda_{n}}\\left(P_{n}^{\\theta^{*}}(V),P_{m_{n}}^{\\theta}(V)\\right)-W\\left(P_{n}^{\\theta^{*}}(V),P_{m_{n}}^{\\theta}(V)\\right)|\\leqslant2(\\log(m n)+1)\\lambda_{n}$ and we get ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|g_{n}(\\theta)-g(\\theta)|\\leqslant2(\\log(m n)+1)\\lambda_{n}+|W\\left(P_{n}^{\\theta^{*}}(V),P_{m_{n}}^{\\theta}(V)\\right)-W\\left(P_{n}^{\\theta^{*}}(V),P^{\\theta}(V)\\right)|}\\\\ &{\\qquad\\qquad\\qquad+\\left|W\\left(P_{n}^{\\theta^{*}}(V),P^{\\theta}(V)\\right)-W\\left(P^{\\theta^{*}}(V),P^{\\theta}(V)\\right)\\right|}\\\\ &{\\qquad\\qquad\\leqslant2(\\log(m n)+1)\\lambda_{n}+W\\left(P_{m_{n}}^{\\theta}(V),P^{\\theta}(V)\\right)+W(P_{n}^{\\theta^{*}}(V),P^{\\theta^{*}}(V))}\\\\ &{\\qquad\\qquad\\leqslant2(\\log(m n)+1)\\lambda_{n}+O\\left(W(P_{n}^{\\theta^{*}}(U),P^{\\theta^{*}}(U))+W\\left(P^{\\theta}(U),P_{m_{n}}^{\\theta}(U)\\right)\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where we use Theorem 1 in the last inequality. ", "page_idx": 28}, {"type": "text", "text": "We first bound $\\operatorname*{sup}_{\\theta}W\\left(P^{\\theta}({\\pmb U}),P_{m_{n}}^{\\theta}({\\pmb U})\\right)$ using standard VC dimension argument. Note that $\\{U_{1},\\cdot\\cdot\\cdot,U_{n_{U}}\\}$ are independent. By Lemma 5, ", "page_idx": 28}, {"type": "equation", "text": "$$\nW\\left(P^{\\theta}(\\boldsymbol{U}),P_{m_{n}}^{\\theta}(\\boldsymbol{U})\\right)\\leqslant\\sum_{i=1}^{n_{U}}W\\left(P^{\\theta}(U_{i}),P_{m_{n}}^{\\theta}(U_{i})\\right).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "By the construction in Theorem 3, $\\begin{array}{r c l}{P^{\\theta}(U_{i})}&{\\sim}&{\\sum_{j\\in[N_{C,i}]}p_{j}P(f_{\\theta_{i,j}}(Z_{i,j})),Z_{i,j}}&{\\sim}\\end{array}$ ${\\mathrm{Unif}}(0,1)$ , where $f_{\\theta_{i,j}}$ are neural networks with constant width and depth $L_{1,n}+L_{2,n},N_{C,}$ is the number of connected components of sup $\\mathfrak{p}(P(U_{i}))$ and $\\theta_{i,j}$ are the parameters of the neural networks. By Lemma 4, ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{\\theta}W\\left(P^{\\theta}(U_{i}),P_{m_{n}}^{\\theta}(U_{i})\\right)\\leqslant O(\\operatorname*{max}_{j\\in[N_{C,j}]}\\operatorname*{sup}_{\\theta_{i,j}}W(P(f_{\\theta_{i,j}}(Z_{i,j})),P_{m_{n}}(f_{\\theta_{i,j}}(Z_{i,j}))).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "By [6], the pseudo-dimension of $\\mathcal{N N}(\\Theta(1),L)$ is $O\\left(L^{2}\\log L\\right)$ . By boundness of all the neural networks and Lemma $8\\,^{2}$ , with probability at least ", "page_idx": 28}, {"type": "equation", "text": "$$\n1-\\mathrm{exp}\\left(O\\left((\\epsilon_{n}^{'})^{-d_{\\mathrm{max}}^{U}}\\log(\\epsilon_{n}^{'})^{-1}+(L_{n,1}+L_{n,2})^{2}\\log(L_{n,1}+L_{n,2})\\log\\epsilon_{n}^{-1}-m_{n}\\epsilon_{n}^{2}\\right)\\right),\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "the following event happens. ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{\\theta_{i,j}}W(P(f_{\\theta_{i,j}}(Z_{i,j})),P_{m_{n}}(f_{\\theta_{i,j}}(Z_{i,j}))\\leqslant\\epsilon_{n}+\\epsilon_{n}^{'}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Let ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\epsilon_{n}^{'}=m_{n}^{-1/\\left(d_{\\mathrm{max}}^{U}+2\\right)}\\log{m_{n}},L_{n,i}=m_{n}^{d_{\\mathrm{max}}^{U}/\\left(2d_{\\mathrm{max}}^{U}+4\\right)}\\log{m_{n}},\\epsilon_{n}=C m_{n}^{-1/\\left(d_{\\mathrm{max}}^{U}+2\\right)}\\log{m_{n}}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "with constant $C>0$ sufficiently large, we get ", "page_idx": 29}, {"type": "equation", "text": "$$\nP\\left(\\operatorname*{sup}_{\\theta_{i,j}}W(P(f_{\\theta_{i,j}}(Z_{i,j})),P_{m_{n}}(f_{\\theta_{i,j}}(Z_{i,j})))\\leqslant O\\left(m_{n}^{-1/\\left(d_{\\operatorname*{max}}^{U}+2\\right)}\\log m_{n}\\right)\\right)\\geqslant1-O\\left(m_{n}^{-2}\\right).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "As long as $m_{n}~=~\\Omega(n)$ , the Borel-Cantelli lemma implies that almost surely, there exists $N~>~0$ such that when $n~>~N$ , $\\begin{array}{r l}{\\operatorname*{sup}_{\\theta_{i,j}}W(P(f_{\\theta_{i,j}}(Z_{i,j})),P_{m_{n}}(f_{\\theta_{i,j}}(Z_{i,j})))\\leqslant}\\end{array}$ mn\u22121/(dmax+2)log mn for all i, j. Therefore, almost surely, for sufficiently large n, ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{\\theta}W\\left(P^{\\theta}(U_{i}),P_{m_{n}}^{\\theta}(U_{i})\\right)\\leqslant O\\left(m_{n}^{-1/\\left(d_{\\operatorname*{max}}^{U}+2\\right)}\\log m_{n}\\right),\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "[17, Theorem 2] shows that for sufficiently large $n$ , if $\\delta_{n}=C_{1}n^{-1/\\operatorname*{max}\\{d_{\\operatorname*{max}}^{U},2\\}}\\log^{2}(n)$ , where $C_{1}>0$ is a constant, ", "page_idx": 29}, {"type": "equation", "text": "$$\nP(W(P^{\\theta^{*}}(U_{i}),P_{n}^{\\theta^{*}}(U_{i}))>\\delta_{n})\\leqslant\\left\\{\\exp\\left(\\frac{\\exp(-C C_{1}\\log^{2}(n))}{\\log^{2}\\left(2+C_{1}^{-1}n^{1/2}\\log^{-2}(n)\\right)}\\right)\\quad d_{\\operatorname*{max}}^{U}=2,}\\\\ {\\exp(-C C_{1}\\log^{2/d_{\\operatorname*{max}}^{U}}(n))\\quad}&{d_{\\operatorname*{max}}^{U}>2,}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where $C>0$ is a constant. Let $C_{1}$ sufficiently large such that ", "page_idx": 29}, {"type": "equation", "text": "$$\nP(W(P^{\\theta^{*}}(U_{i}),P_{n}^{\\theta^{*}}(U_{i}))>\\delta_{n})\\leqslant O\\left(n^{-2}\\right).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Therefore, $\\begin{array}{r}{\\sum_{n=1}^{\\infty}P(W(P^{\\theta^{*}}(U_{i}),P_{n}^{\\theta^{*}}(U_{i}))>\\delta_{n})<\\infty}\\end{array}$ . By Borel-Cantelli lemma, with probability 1, there exist $N\\,>\\,0$ such that $W(P^{\\theta^{*}}(U_{i}),P_{n}^{\\theta^{*}}(U_{i}))\\,\\leqslant\\,\\delta_{n},\\forall n\\,>\\,N$ . By Equation (29), with probability 1, there exist $N>0$ such that ", "page_idx": 29}, {"type": "equation", "text": "$$\nW(P^{\\theta^{*}}(U),P_{n}^{\\theta^{*}}(U))\\leqslant O(\\delta_{n}),\\forall n>N.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Therefore, by Equations (28), (30) and (31), almost surely, the following inequality holds eventually, ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{\\theta\\in\\Theta_{\\infty}}|g_{n}(\\theta)-g(\\theta)|\\leqslant O\\left(\\log(n m_{n})\\lambda_{n}+\\delta_{n}+m_{n}^{-1/\\left(d_{\\operatorname*{max}}^{U}+2\\right)}\\log m_{n}\\right)=s_{n},\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where $s_{n}$ is defined in Theorem 4. ", "page_idx": 29}, {"type": "text", "text": "3. [53, Proposition 1] shows that given a $L$ -Lipschitz continuous function $f:[0,1]^{m}\\rightarrow\\mathbb{R}$ , there exist $\\hat{f}\\in\\mathcal{N N}_{m,1}(W,\\Theta(\\log(m))$ such that $\\|f-\\hat{f}\\|_{\\infty}\\leqslant O\\left(W^{-1/d}\\right)$ and $\\hat{f}$ is $\\sqrt{m}L$ - Lipschitz continuous. 3 For a multivariate function with output dimension $m^{\\prime}$ , we can approximate each coordinate individually and get a neural network approximation $\\hat{f}$ that is $\\sqrt{m m^{\\prime}}L$ -Lipschitz continuous and $\\|f-\\hat{f}\\|_{\\infty}\\leqslant O\\left(W^{-1/m}\\right)$ . ", "page_idx": 29}, {"type": "text", "text": "Next, we define the truncate operator $T_{K}f=\\operatorname*{min}\\{K,\\operatorname*{max}\\{-K,f\\}\\}$ , which is a contraction mapping, and prove that applying the truncate operator will not increase approximation error. If $|f|\\leqslant K$ , we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\|f-T_{K}\\hat{f}\\|_{\\infty}=\\|T_{K}f-T_{K}\\hat{f}\\|_{\\infty}\\leqslant\\|f-\\hat{f}\\|_{\\infty}\\leqslant O\\left(W^{-1/m}\\right).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "3The original proof does not specify the depth of the neural network. The authors show that the network architectures can be chosen as consisting of $O(W)$ parallel blocks each having the same architecture. Each block is used to realize the function ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\phi(x)=(\\operatorname*{min}(\\operatorname*{min}_{k\\neq s}(x_{k}-x_{s}+1),\\operatorname*{min}_{k}(1+x_{k}),\\operatorname*{min}_{k}(1-x_{k})))_{+},\\quad x\\in\\mathbb{R}^{m}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "This function can be realized by feed-forward network with width $O\\left(m^{2}\\right)$ and depth ${\\cal O}(\\log m)$ . ", "page_idx": 29}, {"type": "text", "text": "Therefore, we get that ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{\\substack{f\\in\\mathcal{F}_{L}^{K}\\left([-K,K]^{m},[-K,K]^{m^{\\prime}}\\right)\\,\\hat{f}\\in\\mathcal{N}\\mathcal{N}_{m,m^{\\prime}}^{\\sqrt{m m^{\\prime}L},K}\\left(W_{0,n},\\Theta(\\log m)\\right)}}\\|f-\\hat{f}\\|_{\\infty}\\leqslant O\\left(W_{0,n}^{-1/m}\\right).\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Theorem 3 implies that for each latent variable $U_{i}$ , there exist a neural network $g_{i}$ with architecture in Corollary 1 such that ", "page_idx": 30}, {"type": "equation", "text": "$$\nW(P(U_{i}),P(g_{i}(Z_{i})))\\leqslant O(\\sum_{i=1}^{n}L_{i,n}^{-2/d_{\\operatorname*{max}}}+\\tau_{n}(1-\\log\\tau_{n})).\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "By Assumption 3, we know $\\|U_{i}\\|_{\\infty}\\leqslant K$ , which implies ", "page_idx": 30}, {"type": "equation", "text": "$$\nW(P(U_{i}),P((T_{K}g_{i})(Z_{i})))\\leqslant W(P(U_{i}),P(g_{i}(Z_{i})))\\leqslant O(\\sum_{i=1}^{n}L_{i,n}^{-2/d_{\\operatorname*{max}}^{U}}+\\tau_{n}(1-\\log\\tau_{n})).\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Combining Equations (32) and (33) with the same proof as Corollary 1, it can be proven in the same way as Corollary 1 that there exists a $\\theta_{n}\\in\\Theta_{n}$ satisfying ", "page_idx": 30}, {"type": "equation", "text": "$$\nd(\\theta^{*},\\theta_{n})\\leqslant\\epsilon_{n}=O\\left(W_{0,n}^{-1/d_{\\operatorname*{max}}^{\\mathrm{in}}}+L_{1,n}^{-2/d_{\\operatorname*{max}}^{U}}+L_{2,n}^{-2/d_{\\operatorname*{max}}^{U}}+\\tau_{n}(1-\\log\\tau_{n})\\right).\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Let $\\tilde{\\Theta}_{\\infty}=\\{\\theta^{\\ast}\\}$ , we have verified the third assumption in Proposition 6. ", "page_idx": 30}, {"type": "text", "text": "Take the Wasserstein radius to be $\\alpha_{n}=O(s_{n}+\\epsilon_{n})$ , Proposition 6 implies the conclusion. ", "page_idx": 30}, {"type": "text", "text": "C.3 Proof of Proposition 3 ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Lemma 2. Let $(\\hat{T},\\hat{Y})\\sim\\mu,(T,Y)\\sim\\nu$ and suppose that $f(t)=\\mathbb{E}_{\\nu}[Y|T],\\hat{f}(t)=\\mathbb{E}_{\\mu}[\\hat{Y}|\\hat{T}]$ are $L$ -Lipschitz continuous and $|f(t)|\\leqslant K,|\\hat{f}(t)|\\leqslant K$ , then we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\int(f(t)-\\hat{f}(t))^{2}d\\nu(d t)\\leqslant C_{W}W(\\mu,\\nu),\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where $C_{W}=4L K+2K\\operatorname*{max}\\{L,1\\}$ ", "page_idx": 30}, {"type": "text", "text": "Proof of Lemma 2. By the duality formulation of Wasserstein-1 distance, we have ", "page_idx": 30}, {"type": "equation", "text": "$$\nW(\\mu,\\nu)=\\operatorname*{sup}_{g\\in\\mathrm{Lip}(1)}\\mathbb{E}_{\\mu}[g(\\hat{T},\\hat{Y})]-\\mathbb{E}_{\\nu}[g(T,Y)].\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Let $g_{0}(t,y)=(\\hat{f}(t)\\!-\\!f(t))y$ , we verify $g_{0}$ is a Lipschitz continuous function in $\\{(t,y):\\|(t,y)\\|_{\\infty}\\leqslant$ $K\\}$ . ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|g_{0}(t_{1},y_{1})-g_{0}(t_{2},y_{2})|\\leqslant|g_{0}(t_{1},y_{1})-g_{0}(t_{1},y_{2})|+|g_{0}(t_{1},y_{2})-g_{0}(t_{2},y_{2})|}\\\\ &{\\qquad\\qquad\\qquad\\qquad=|(\\hat{f}(t_{1})-f(t_{1}))(y_{1}-y_{2})|+|y_{2}(\\hat{f}(t_{1})-f(t_{1})-(\\hat{f}(t_{2})-f(t_{2})))|}\\\\ &{\\qquad\\qquad\\qquad\\leqslant2K|y_{1}-y_{2}|+K(|\\hat{f}(t_{1})-\\hat{f}(t_{2})|+|f(t_{1})-f(t_{2})|)}\\\\ &{\\qquad\\qquad\\qquad\\leqslant2K|y_{1}-y_{2}|+2K L|t_{1}-t_{2}|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Let $L_{g}=2K\\operatorname*{max}\\{L,1\\}$ , we have proven that $g_{0}$ is $L_{g}$ -Lipschitz continuous in $\\{(t,y):\\|(t,y)\\|_{\\infty}\\leqslant$ $K\\}$ . Thus, ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{W(\\mu,\\nu)\\geqslant\\displaystyle\\frac{1}{L_{g}}\\big(\\mathbb{E}_{\\mu}[g_{0}(\\hat{T},\\hat{Y})]-\\mathbb{E}_{\\nu}[g_{0}(T,Y)]\\big)}&{}\\\\ {=\\displaystyle\\frac{1}{L_{g}}\\big(\\mathbb{E}_{\\mu}[(\\hat{f}(\\hat{T})-f(\\hat{T}))\\mathbb{E}[\\hat{Y}|\\hat{T}=t]]-\\mathbb{E}_{\\nu}[(\\hat{f}(T)-f(T))\\mathbb{E}[Y|T=t]]\\big)}&{}\\\\ {=\\displaystyle\\frac{1}{L_{g}}\\big(\\mathbb{E}_{\\mu}[(\\hat{f}(\\hat{T})-f(\\hat{T}))\\hat{f}(\\hat{T})]-\\mathbb{E}_{\\nu}[(\\hat{f}(T)-f(T))f(T)]\\big).}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Now, let $h(t)\\,=\\,(\\hat{f}(t)-f(t))\\hat{f}(t)$ . Following the same argument, it can be proven that $h(t)$ is Lipschitz continuous with Lipschitz constant being $L_{h}=4L K$ . Hence, ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mu}[h(\\hat{T})]\\geqslant\\mathbb{E}_{\\nu}[h(T)]-L_{h}W(\\mu,\\nu).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Plug into (34), and we get ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(L_{g}+L_{h})W(\\mu,\\nu)\\geqslant\\mathbb{E}_{\\nu}[(\\hat{f}(T)-f(T))\\hat{f}(T)-(\\hat{f}(T)-f(T))f(T)]}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\mathbb{E}_{\\nu}\\left[(\\hat{f}(T)-f(T))^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Proof of Proposition 3. If $Y$ is not descendant of $T$ $\\',\\mathbb{E}[Y(t)]=\\mathbb{E}[Y]$ and we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\int_{t_{1}}^{t_{2}}(\\mathbb{E}_{M}[Y(t)]-\\mathbb{E}_{\\hat{M}}[\\hat{Y}(t)])^{2}d t=\\int_{t_{1}}^{t_{2}}(\\mathbb{E}_{M}[Y]-\\mathbb{E}_{\\hat{M}}[\\hat{Y}])^{2}d t}}\\\\ &{=(t_{2}-t_{1})(\\mathbb{E}_{M}[Y]-\\mathbb{E}_{\\hat{M}}[\\hat{Y}])^{2}\\leqslant(t_{2}-t_{1})W(P^{M}(V),P^{\\hat{M}}(\\hat{V}))^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Now, suppose that $Y$ is a descendant of $T$ . Let $X=\\operatorname{Pa}(T)$ . Note that $f_{y}(x,t)\\,=\\,\\mathbb{E}_{\\mathcal{M}}[Y|X\\,=$ $x,T=t]$ is $L_{y}$ -Lipschitz continuous with Lipschitz constant $L_{y}\\,\\leqslant\\,(L+\\'1)^{n_{V}}$ . This is because $Y$ can be written as $Y=F_{0}(X,T,U_{y})$ where $U_{y}$ are latent variables independent of $T,X$ and $F_{0}$ is composition of $f_{i}$ in structure equations. The composition of Lipschitz functions is still Lipschitz and $F_{0}$ is a composition of at most $n_{V}\\,L$ -Lipschitz functions. $F_{0}$ is $(L+1)^{n_{V}}$ -Lipschitz and so is $f_{y}(x,t)=\\mathbb{E}_{U_{y}}\\bar{[F_{0}(X,T,U_{y})]}$ . Recall that $\\nu,\\mu$ are the observation distributions of $\\mathcal{M}$ and $\\hat{\\mathcal{M}}$ respectively in Proposition 3. Similarly, $\\hat{f}_{y}(x,t)=\\mathbb{E}_{\\hat{\\mathcal{M}}}[\\hat{Y}|X=x,T=t]$ is $L_{y}$ -Lipschitz continuous. By Lemma 2 and the overlap assumption, we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{C_{W}W(\\mu,\\nu)\\geqslant\\mathbb{E}_{\\nu}\\left[(f_{y}(X,T)-\\hat{f}_{y}(X,T))^{2}\\right]}\\\\ &{\\qquad\\qquad=\\int(f_{y}(x,t)-\\hat{f}_{y}(x,t))^{2}\\nu(d t|x)\\nu(d x)}\\\\ &{\\qquad\\qquad\\geqslant\\delta\\int_{t_{1}}^{t_{2}}\\int(f_{y}(x,t)-\\hat{f}_{y}(x,t))^{2}\\nu(d x)P(d t).}\\\\ &{\\qquad\\qquad\\geqslant\\delta\\int_{t_{1}}^{t_{2}}\\left(\\int{f}_{y}(x,t)\\nu(d x)-\\int\\hat{f}_{y}(x,t)\\nu(d x)\\right)^{2}P(d t).}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Note that $\\hat{f}_{y}(x,t)$ is Lipschitz continuous, we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\left|\\int{\\hat{f}}_{y}(x,t)\\nu(d x)-\\int{\\hat{f}}_{y}(x,t)\\mu(d x)\\right|\\leqslant L_{y}W(\\mu,\\nu).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Therefore, ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\left(\\int f_{y}(x,t)\\nu(d x)-\\int\\hat{f}_{y}(x,t)\\nu(d x)\\right)^{2}\\geqslant\\frac{1}{2}\\left(\\int f_{y}(x,t)\\nu(d x)-\\int\\hat{f}_{y}(x,t)\\mu(d x)\\right)^{2}}}\\\\ &{}&{-\\left(\\int\\hat{f}_{y}(x,t)\\mu(d x)-\\int\\hat{f}_{y}(x,t)\\nu(d x)\\right)^{2}}\\\\ &{}&{\\geqslant\\frac{1}{2}\\left(\\int_{y}(x,t)\\nu(d x)-\\int\\hat{f}_{y}(x,t)\\mu(d x)\\right)^{2}-L_{y}^{2}W^{2}(\\mu,\\nu(d x))}\\\\ &{}&{=\\frac{1}{2}(\\mathbb{E}_{M}[Y(t)]-\\mathbb{E}_{\\hat{M}}[\\hat{Y}(t)])^{2}-L_{y}^{2}W^{2}(\\mu,\\nu).}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Combine all the equations, we get ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\int_{t_{1}}^{t_{2}}(\\mathbb{E}_{M}[Y(t)]-\\mathbb{E}_{\\hat{M}}[\\hat{Y}(t)])^{2}P(d t)\\leqslant\\frac{2C_{W}}{\\delta}W(\\mu,\\nu)+2L_{y}^{2}W^{2}(\\mu,\\nu)(t_{2}-t_{1}).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Proof of Corollary 2. In the proof of Theorem 4, we know with probability at least $1-O(n^{-2})$ , (6) has feasible solutions and ", "page_idx": 32}, {"type": "equation", "text": "$$\nW(P_{n}^{\\mathcal{M}^{*}}(V),P^{\\mathcal{M}^{*}}(V))\\leqslant O(\\alpha_{n}),\\quad W(P_{m_{n}}^{\\theta_{n}}(V),P^{\\theta_{n}}(V))\\leqslant O(\\alpha_{n}),\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where notations $\\theta$ and $P^{\\theta}$ are defined in the proof of Theorem 4 and $\\theta_{n}$ is one of the minimizers of (6). By Lemma 7, we know that ", "page_idx": 32}, {"type": "equation", "text": "$$\nW(P_{n}^{\\mathcal{M}^{*}}(V),P_{m_{n}}^{\\theta_{n}}(V))\\leqslant S_{\\lambda_{n}}(P_{n}^{\\mathcal{M}^{*}}(V),P_{m_{n}}^{\\theta_{n}}(V))+2(\\log(m_{n}n)+1)\\lambda_{n}\\leqslant O(\\alpha_{n}).\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "We get that ", "page_idx": 32}, {"type": "equation", "text": "$$\nV(P^{M^{*}}(V),P^{\\theta_{n}}(V))\\leqslant W(P_{n}^{{M^{*}}}(V),P^{{M^{*}}}(V))+W(P^{{M^{*}}}(V),P^{\\theta_{n}}(V))+W(P_{m_{n}}^{\\theta_{n}}(V),P^{\\theta_{n}}(V))\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "By Proposition 3, we get $|F_{n}-F_{*}|\\leqslant O(\\sqrt{\\alpha_{n}})$ . ", "page_idx": 32}, {"type": "text", "text": "D Experiments ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "The structure equations of the generative models are (5). We use three-layer feed-forward neural networks with width 128 for each $\\hat{f}_{i}$ and six-layer neural networks with width 128 for each ${\\hat{g}}_{j}$ . We use the Augmented Lagrangian Multiplier (ALM) method to solve the optimization problems as in [3]. We run 600 epochs and use a batch size of 2048 in each epoch. $m_{n}$ is set to be $m_{n}=n$ . The \"geomloss\" package [16] is used to calculate the Sinkhorn distance. To impose Lipschitz regularization, we use the technique from [23] to do layer-wise normalization to the weight matrices with respect to infinity norm. The upper bound of the Lipschitz constant in each layer is set to be 8. The $\\tau$ in the Gumbel-softmax layer is set to be 0. ", "page_idx": 32}, {"type": "text", "text": "For the choice of Wasserstein ball radius $\\alpha_{n}$ , we use the subsampling technique from [11, Section 3.4]. We can take the criterion function in [11] to be $Q(\\theta)\\,=\\,W(P^{\\mathcal M^{\\ast}}(V),P^{\\mathcal M^{\\theta}}(V))$ and its empirical estimation to be $Q_{n}(\\theta)\\,=\\,W(P_{n}^{\\mathcal{M}^{\\ast}}(V),P_{n}^{\\mathcal{M}^{\\theta}}(V))$ . In the first step, we minimize the Wasserstein distance $\\hat{\\theta}_{n}^{*}=\\arg\\operatorname*{min}_{\\theta}W(P_{n}^{\\mathcal{M}^{*}}(V),P_{n}^{\\mathcal{M}^{\\theta}}(V))$ . Then, [11] propose to refine the radius $Q_{n}(\\hat{\\theta}_{n}^{*})$ by taking the quantile of subsample $\\{\\operatorname*{sup}_{\\theta:Q_{n}(\\hat{\\theta})\\leqslant\\beta Q_{n}(\\hat{\\theta}_{n}^{*})}Q_{j,b}(\\theta)\\}$ , where $Q_{j,b}$ denotes the criterion function estimated at $j$ -th subsample of size $b$ . However, it is time-consuming to solve this optimization problem many times. In practice, we set the radius $\\alpha_{n}$ to be the $95\\%$ quantile of $\\{W(\\bar{P}_{j}^{\\mathcal{M}^{*}}\\}(V),P_{n}^{\\mathcal{M}^{\\hat{\\theta}_{n}^{*}}}(V))$ , where we use 50 subsamples $\\bar{P}_{j}^{\\mathcal{M}^{\\ast}},j=1,\\cdots,50$ from $P^{\\mathcal{M}_{n}^{*}(V)}$ with size $b=\\lfloor15{\\sqrt{n}}\\rfloor$ . ", "page_idx": 32}, {"type": "text", "text": "D.1 Discrete IV [14] ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "We consider the noncompliance binary IV example in [14, Section D.1]. The causal graph is shown in Figure 10. We could see from Table 2 that the bound given by NCM is slightly worse than Autobounds but is still close to the optimal bound. Besides, the Autobounds bound does not cover the optimal bound in this example. ", "page_idx": 32}, {"type": "image", "img_path": "GEbnPxD9EF/tmp/621fb3fd67c7110c95bebe0d4fcc0b5a8c40025a06a656e01b87557d8cbddf96.jpg", "img_caption": ["Figure 10: Instrumental Variable (IV) graph. $Z$ is the instrumental variable, $T$ is the treatment and $Y$ is the outcome. "], "img_footnote": [], "page_idx": 32}, {"type": "table", "img_path": "GEbnPxD9EF/tmp/4ac3279289fd840377216a0fdbd44ab6c9281e23535e6fcd4877fc638862e5d1.jpg", "table_caption": [], "table_footnote": ["Table 2: The sample size is taken to be 5000. We average over 10 runs with different random seeds. SD is short for the standard derivation. "], "page_idx": 33}, {"type": "text", "text": "D.2 Continuous IV ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Now, we turn to the continuous IV setting, where $T$ is still binary, but $Y$ and $Z$ can be continuous. Let $E_{i}^{\\lambda}\\sim\\lambda Z_{1}+(1-\\lambda)\\mathrm{Unif}(-1,1)$ , where $Z_{1}$ is the Gaussian variable conditioning on $[-1,1]$ and the structure equations of $\\mathcal{M}^{\\lambda}$ to be ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{E_{Y}=E_{1}^{\\lambda},U=E_{2}^{\\lambda},Z=E_{3}^{\\lambda},}}\\\\ {{P(T=1|Z)=(1.5+Z+0.5U)/3,}}\\\\ {{Y=3T-1.5T U+U+E_{Y},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where $E_{i}^{\\lambda}$ are independent. It is easy to see the ATE of this model is 3 regardless of $\\lambda$ . In the experiment, we randomly choose ten $\\lambda\\sim\\mathrm{Unif}(0,1)$ . For each $\\lambda$ , we run the algorithms 5 times to get the bound. We choose different latent distributions (indexed by $\\lambda$ ) in the experiments to create more difficulty. ", "page_idx": 33}, {"type": "text", "text": "Since Autobounds can only deal with discrete variables, we discretize $Z$ and $Y$ . Suppose that $Z\\in[l,u]$ , we map all points in intervals $[l+i(u-l)/k,l+(i+1)(u-l)/k],I=0,1\\cdot\\cdot\\cdot\\cdot,k-1$ to the middle point $\\bar{l}+(\\bar{i}+1/2)(u-l)/k$ . We choose $k=8$ in the following experiments. The choice will give rise to polynomial programming problems with $2^{14}$ decision variables, which is quite large. ", "page_idx": 33}, {"type": "text", "text": "Table 3 demonstrate the results. While both algorithms cover the true ATE well, we can see that NCM gives much tighter bounds on average. The main reason may be that the discretized problem does not approximate the original problem well enough. It is possible that a larger discretized parameter $k$ can give a better bound, but since the size of the polynomial problem grows exponentially with $k$ , the optimization problem may be intractable for large $k$ . On the contrary, NCM does not suffer from computational difficulties. ", "page_idx": 33}, {"type": "table", "img_path": "GEbnPxD9EF/tmp/c94129e5704490fbe35bfdda367c9d738b6f80d01206e52f875088373660b21a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 33}, {"type": "text", "text": "Table 3: We take a sample size of 5000. We randomly choose $\\overline{{10\\,\\lambda\\sim\\mathrm{Unif}(0,1)}}$ and get 10 models $\\mathcal{M}^{\\lambda_{i}}$ . For each model $\\mathcal{M}^{\\lambda_{i}}$ , we run the two algorithms for 5 times. The success rate is the number of times when the obtained bounds cover the true ATE divided by the total number of experiments. SD is short for the standard derivation. ", "page_idx": 33}, {"type": "text", "text": "D.3 Counterexample ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "We test our neural causal method on the counterexample in Appendix C.1. We choose the noise $U_{y}$ to be the normal variable. The structure equations are ", "page_idx": 33}, {"type": "equation", "text": "$$\nP(U_{1}=1)=3/5,P(U_{1}=0)=2/5,P(U_{2}=1)=1/3,P(U_{1}=0)=2/3,\n$$", "text_format": "latex", "page_idx": 33}, {"type": "equation", "text": "$$\nX=\\left\\{{-1\\quad,\\mathrm{w.p.~1/2},}\\right.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "equation", "text": "$$\nP(T=1|X=x)\\ ={\\binom{1/2}{5/8}}\\ \\ \\ ,x=-1,\n$$", "text_format": "latex", "page_idx": 33}, {"type": "equation", "text": "$$\nY\\,=\\,\\left\\{\\!\\!{\\begin{array}{l l}{{T+U_{y}}}&{{,X=-1,}}\\\\ {{U_{1}+U_{y}}}&{{,X=0,T=1,}}\\\\ {{U_{2}+U_{y}}}&{{,X=0,T=0,}}\\end{array}}\\!\\!\\right.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "We compare the confidence intervals of the unregularized neural casual method and the Lipschitz regularized one under different architectures. We choose the layerwise Lispchitz constants upper bound to be 5 and 1.2. As a benchmark, we also include the bound obtained by the Double Machine Learning estimator using the EconML package. The result is shown in Appendix D.3. For this identifiable case example, the double ML estimator produces better bounds for the ATE. The intervals given by regularized and unregularized NCM are similar to the regularized one slightly better in the left figure, where we use medium-sized NNs. However, in the right figure, the obtained intervals after regularization are tighter, although slightly biased, compared with the regularized setting. From these two experiments, we conclude that the architecture of NNs will also influence the results and adding regularization during the training process can prevent extreme confidence intervals or inconsistency. ", "page_idx": 34}, {"type": "image", "img_path": "GEbnPxD9EF/tmp/2c4b956c0d96d7567ebcc87d04a15bdbe80a21eaa0d261d9a7e072c9fb11e349.jpg", "img_caption": ["Figure 11: Comparison of Lipschitz regularized and unregularized neural causal algorithm. The two figures show the results of different architectures. The figure on the left side uses a medium-sized NN (width 128, depth 3) to approximate each structural function, while the right figure uses extremely small NNs (width 3, depth 1). In all experiments, we use the projected gradient to regularize the weight of the neural network. For each sample size, we repeat the experiment 5 times and take the average of the upper (lower) bound. "], "img_footnote": [], "page_idx": 34}, {"type": "text", "text": "E Technical Lemmas ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Lemma 3. Let $\\mu,\\hat{\\mu}$ be two measures on compact set $K\\,\\subset\\,\\mathbb{R}^{d_{1}}$ , for any measurable functions $\\boldsymbol{F},\\hat{\\boldsymbol{F}}:\\boldsymbol{K}\\rightarrow\\mathbb{R}^{d_{2}}$ , if $F$ is $L$ -Lipschitz continuous, then ", "page_idx": 34}, {"type": "equation", "text": "$$\nW(F_{\\#}\\mu,\\hat{F}_{\\#}\\hat{\\mu})\\leqslant L W(\\mu,\\hat{\\mu})+\\|F_{1}-F_{2}\\|_{\\infty}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Proof. For any 1-Lipschitz function $g:\\mathbb{R}^{d_{2}}\\rightarrow\\mathbb{R}$ , ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}_{X\\sim F_{\\#}\\mu}[g(X)]-\\mathbb{E}_{X\\sim\\hat{F}_{\\#}\\hat{\\mu}}[g(X)]=\\mathbb{E}_{X\\sim\\mu}[g\\circ F(X)]-\\mathbb{E}_{X\\sim\\hat{\\mu}}[g\\circ\\hat{F}(X)]}&{}\\\\ {=(\\mathbb{E}_{X\\sim\\mu}[g\\circ F(X)]-\\mathbb{E}_{X\\sim\\hat{\\mu}}[g\\circ F(X)])}&{}\\\\ {+\\left(\\mathbb{E}_{X\\sim\\hat{\\mu}}[g\\circ F(X)]-\\mathbb{E}_{X\\sim\\hat{\\mu}}[g\\circ\\hat{F}(X)]\\right)\\!.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Note that for any $\\boldsymbol{x},\\boldsymbol{y}\\in\\mathbb{R}^{d_{1}}$ , ", "page_idx": 34}, {"type": "equation", "text": "$$\n|g\\circ F(x)-g\\circ F(y)|\\leqslant\\|F(x)-F(y)\\|\\leqslant L\\|x-y\\|.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Thus, $g\\circ F$ is $L$ -Lipschitz continuous and ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathbb{E}_{X\\sim\\mu}[g\\circ F(X)]-\\mathbb{E}_{X\\sim\\hat{\\mu}}[g\\circ F(X)]\\leqslant L W(\\mu,\\hat{\\mu}).\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "For the second term, ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{X\\sim\\hat{\\mu}}[g\\circ F(X)]-\\mathbb{E}_{X\\sim\\hat{\\mu}}[g\\circ\\hat{F}(X)]\\leqslant\\mathbb{E}_{X\\sim\\hat{\\mu}}[|g\\circ F(X)-g\\circ\\hat{F}(X)|]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leqslant\\mathbb{E}_{X\\sim\\hat{\\mu}}[\\|F(X)-\\hat{F}(X)\\|]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leqslant\\|F(X)-\\hat{F}(X)\\|_{\\infty}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Combine (35) and (36), we have the conclusion. ", "page_idx": 34}, {"type": "text", "text": "Lemma 4. Let $\\mu_{1},\\cdot\\cdot\\cdot,\\mu_{n},\\hat{\\mu}_{1},\\cdot\\cdot\\cdot,\\hat{\\mu}_{n}$ be measure on $\\mathbb{R}^{d}$ , for any $p_{1},\\cdot\\cdot\\cdot\\mathrm{\\boldmath~,~}p_{n}\\,\\in\\,[0,1]$ such that $\\textstyle\\sum_{k=1}^{n}p_{k}=1$ , we have ", "page_idx": 35}, {"type": "equation", "text": "$$\nW\\left(\\sum_{k=1}^{n}p_{k}\\mu_{k},\\sum_{k=1}^{n}p_{k}\\hat{\\mu}_{k}\\right)\\leqslant\\sum_{k=1}^{n}p_{k}W(\\mu_{k},\\hat{\\mu}_{k}).\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Proof. For any 1-Lipschitz function $f:\\ensuremath{\\mathbb{R}^{d}}\\to\\ensuremath{\\mathbb{R}}$ , we have ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathbb{E}_{X\\sim\\sum_{k=1}^{n}p_{k}\\mu_{k}}[f(X)]-\\mathbb{E}_{X\\sim\\sum_{k=1}^{n}p_{k}\\hat{\\mu}_{k}}[f(X)]=\\sum_{k=1}^{n}p_{k}(\\mathbb{E}_{X_{k}\\sim\\mu_{k}}[f(X_{k})]-\\mathbb{E}_{X_{k}\\sim\\hat{\\mu}_{k}}[f(X_{k})])}}\\\\ &{}&{\\leqslant\\displaystyle\\sum_{k=1}^{n}p_{k}W(\\mu_{k},\\hat{\\mu}_{k}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "By dual formulation of Wasserstein-1 distance, ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{W\\left(\\sum_{k=1}^{n}p_{k}\\mu_{k},\\sum_{k=1}^{n}p_{k}\\mu_{k}\\right)=\\operatorname*{sup}_{f_{\\mathrm{\\Lambda}}\\mathrm{Lipschitz}}\\mathbb{E}_{X\\sim\\sum_{k=1}^{n}p_{k}\\mu_{k}}[f(X)]-\\mathbb{E}_{X\\sim\\sum_{k=1}^{n}p_{k}\\hat{\\mu}_{k}}[f(X)]}}\\\\ &{}&{\\leqslant\\sum_{k=1}^{n}p_{k}W(\\mu_{k},\\hat{\\mu}_{k}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Lemma 5. Given two measures $\\mu=\\mu_{1}\\otimes\\cdot\\cdot\\cdot\\otimes\\mu_{n},\\hat{\\mu}=\\hat{\\mu}_{1}\\otimes\\cdot\\cdot\\cdot\\otimes\\hat{\\mu}_{n}$ , we have ", "page_idx": 35}, {"type": "equation", "text": "$$\nW(\\mu,\\hat{\\mu})\\leqslant\\sum_{k=1}^{n}W(\\mu_{k},\\hat{\\mu}_{k}).\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Proof. For any $\\epsilon>0$ , let $\\pi_{k}$ be a coupling of $\\mu_{k},\\hat{\\mu}_{k}$ such that ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\mathbb{E}_{X_{k},Y_{k}\\sim\\pi_{k}}[\\|X_{k}-Y_{k}\\|_{1}]\\leqslant W(\\mu_{k},\\hat{\\mu}_{k})+\\epsilon.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Then, ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{W(\\mu,\\hat{\\mu})\\leqslant\\mathbb{E}_{\\pi_{1}\\otimes\\cdots\\otimes\\pi_{n}}[\\|X-Y\\|]}}\\\\ &{=\\displaystyle\\sum_{k=1}^{n}\\mathbb{E}_{X_{k},Y_{k}\\sim\\pi_{k}}[\\|X_{k}-Y_{k}\\|_{1}]}\\\\ &{\\leqslant W(\\mu_{k},\\hat{\\mu}_{k})+\\epsilon.}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Let $\\epsilon\\rightarrow0$ , we get the result. ", "page_idx": 35}, {"type": "text", "text": "Lemma 6 (Approximation error of Gumbel-Softmax layer). Given $1>\\tau>0$ and $p_{1},\\cdot\\cdot\\cdot\\;,p_{n}>0$ , let $\\mu^{\\tau}\\sim X^{\\tau}=(X_{1}^{\\tau},\\cdot\\cdot\\cdot\\,,X_{n}^{\\tau})$ be ", "page_idx": 35}, {"type": "equation", "text": "$$\nX_{k}=\\frac{\\exp((\\log p_{k}+G_{k})/\\tau)}{\\sum_{k=1}^{n}\\exp((\\log p_{k}+G_{k})/\\tau)},\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where $G_{k}\\sim\\mathrm{exp}(-x-\\mathrm{exp}(-x))$ are i.i.d. standard Gumbel variables. Let $\\mu^{0}$ be the distribution of ${\\cal X}^{0}=\\mathrm{lim}_{\\tau\\rightarrow0}\\,{\\cal X}^{\\tau}=\\left({\\cal X}_{1}^{0},\\cdot\\cdot\\cdot\\,,{\\cal X}_{n}^{0}\\right)$ . Then, ", "page_idx": 35}, {"type": "equation", "text": "$$\nW\\left(\\mu^{\\tau},\\mu^{0}\\right)\\leqslant2(n-1)\\tau-2n(n-1)e^{-1}\\tau\\log\\tau.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Proof. Without loss of generality, we may assume that $\\textstyle\\sum_{k=1}^{n}p_{k}=1$ . Otherwise, we can divide the denominator and numerator in (37) by $\\scriptstyle\\sum_{k=1}^{n}p_{k}$ . Let $\\begin{array}{r}{\\Delta_{n}^{\\cdots}\\stackrel{{}=}\\{(x_{1},\\cdots,x_{n}):\\sum_{k=1}^{n}x_{k}=1\\}}\\end{array}$ . We construct a transportation map $T:\\Delta_{n}\\rightarrow\\Delta_{n}$ as follows. $T(x_{1},\\cdot\\cdot\\cdot,x_{n})$ is a $n$ -dimensional vector with all zeros except that the $i=\\arg\\operatorname*{max}_{j}x_{j}$ -th entry being one. If there are multiple coordinates that is maximum, we choose the first coordinate and let the remaining coordinate be zero. It is easy to verify that $T_{\\#}\\mu^{\\tau}$ and $\\mu^{0}$ have the same distribution [36]. For any $\\delta>0$ , we have ", "page_idx": 35}, {"type": "text", "text": "", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l l}&{\\widetilde{x}_{k_{1}}+\\log{p_{k_{1}}}-(G_{k_{2}}+\\log{p_{k_{2}}})|<\\delta)=\\displaystyle\\int_{-\\infty}^{\\infty}\\int_{y+\\log{\\frac{p_{k_{2}}}{p_{k_{1}}}}-\\delta}^{y+\\log{\\frac{p_{k_{2}}}{p_{k_{1}}}}+\\delta}\\exp(-x-y-\\exp(-y)-\\exp(-x))d y}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\leqslant2\\delta e^{-1}\\displaystyle\\int_{-\\infty}^{\\infty}\\exp(-y-\\exp(-y))d y=2\\delta,}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where we have used $\\exp(-x-\\exp(-x))\\leqslant\\exp(-1)$ . Let event $E_{\\delta}=\\left\\{\\exists i,j\\in\\left[n\\right]:\\left|G_{i}+\\log p_{i}\\right.-\\right.$ $(G_{j}+\\log p_{j})|<\\delta\\}$ , we have ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\quad P(E_{\\delta})\\leqslant\\displaystyle\\sum_{i\\neq j}P(|G_{i}+\\log p_{i}-(G_{j}+\\log p_{j})|<\\delta)}&{}\\\\ {\\leqslant\\delta n(n-1)e^{-1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Now, we calculate the transportation cost ", "page_idx": 36}, {"type": "text", "text": "\u6b63 $\\begin{array}{r}{\\{\\}_{X^{\\tau}\\sim\\mu^{\\tau}}\\left[\\|T\\left(X^{\\tau}\\right)-X^{\\tau}\\|_{1}\\right]\\leqslant P(E_{\\delta})\\mathbb{E}_{X^{\\tau}\\sim\\mu^{\\tau}}\\left[\\|T\\left(X^{\\tau}\\right)-X^{\\tau}\\|_{1}|E_{\\delta}\\right]+P\\left(E_{\\delta}^{c}\\right)\\mathbb{E}_{X^{\\tau}\\sim\\mu^{\\tau}}\\left[\\|T\\left(X^{\\tau}\\right)-X^{\\tau}\\|_{1}|E_{\\delta}\\right]\\}\\,.}\\end{array}$ X \u22251|E\u03b4] . Note that $\\|T\\left(X^{\\tau}\\right)-X^{\\tau}\\|_{1}\\leqslant2$ . For the first term, we have ", "page_idx": 36}, {"type": "equation", "text": "$$\nP(E_{\\delta})\\mathbb{E}_{X^{\\tau}\\sim\\mu^{\\tau}}\\left[\\|T\\left(X^{\\tau}\\right)-X^{\\tau}\\|_{1}|E_{\\delta}\\right]\\leqslant2\\delta n(n-1)e^{-1}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "For the second term, under event $E_{\\delta}^{c}$ , let $k_{\\operatorname*{max}}=\\arg\\operatorname*{max}_{i}X_{i}$ , if $j=k_{\\mathrm{max}}=\\arg\\operatorname*{max}_{i}(\\log p_{k}+G_{k})$ ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\Big|\\frac{\\exp((\\log p_{j}+G_{j})/\\tau)}{\\sum_{k=1}^{n}\\exp((\\log p_{k}+G_{k})/\\tau)}-1\\Big|=\\Big|\\frac{\\sum_{k\\neq j}\\exp((\\log p_{k}+G_{k})/\\tau)}{\\sum_{k=1}^{n}\\exp((\\log p_{k}+G_{k})/\\tau)}\\Big|}&{}\\\\ &{\\leqslant\\Big|\\frac{\\sum_{k\\neq j}\\exp((\\log p_{k}+G_{k})/\\tau)}{\\exp((\\log p_{j}+G_{j})/\\tau)}\\Big|}\\\\ &{=\\displaystyle\\sum_{k\\neq j}\\exp((\\log p_{k}+G_{k}-(\\log p_{j}+G_{j}))/\\tau)}\\\\ &{\\leqslant(n-1)\\exp(-\\delta/\\tau).}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "If $j\\neq k_{\\mathrm{max}}$ , we have ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\Big|\\frac{\\exp((\\log p_{j}+G_{j})/\\tau)}{\\sum_{k=1}^{n}\\exp((\\log p_{k}+G_{k})/\\tau)}\\Big|\\leqslant\\exp((\\log p_{j}+G_{j}-(\\log p_{\\operatorname*{max}}+G_{\\operatorname*{max}}))/\\tau)\\leqslant\\exp(-\\delta/\\tau).\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Therefore, ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\mathbb{E}_{X^{\\tau}\\sim\\mu^{\\tau}}\\left[\\|T\\left(X^{\\tau}\\right)-X^{\\tau}\\|_{1}|E_{\\delta}^{c}\\right]\\leqslant2(n-1)\\exp(-\\delta/\\tau).\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "We get an upper bound of the transportation cost, ", "page_idx": 36}, {"type": "equation", "text": "$$\nW\\left(\\mu^{\\tau},\\mu^{0}\\right)\\leqslant\\mathbb{E}_{X^{\\tau}\\sim\\mu^{\\tau}}\\left[\\|T\\left(X^{\\tau}\\right)-X^{\\tau}\\|_{1}\\right]\\leqslant2\\delta n(n-1)e^{-1}+2(n-1)\\exp(-\\delta/\\tau).\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Let $\\delta=-\\tau\\log\\tau$ , we get ", "page_idx": 36}, {"type": "equation", "text": "$$\nW\\left(\\mu^{\\tau},\\mu^{0}\\right)\\leqslant2(n-1)\\tau-2n(n-1)e^{-1}\\tau\\log\\tau.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Lemma 7 (Approximation Error of Sinkhorn distance). For any $\\mu\\in\\Delta_{n},\\nu\\in\\Delta_{m}$ and $\\lambda>0$ , we have ", "page_idx": 36}, {"type": "equation", "text": "$$\n0\\leqslant W_{1,\\lambda}(\\mu,\\nu)-W(\\mu,\\nu)\\leqslant(\\log(m n)+1)\\lambda,\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where $W_{1,\\lambda}(\\cdot,\\cdot)$ is the entropy regularized Wasserstein- $^{\\,I}$ distance. Moreover, we have the following estimation of approximation error. ", "page_idx": 36}, {"type": "equation", "text": "$$\n|S_{\\lambda}(\\mu,\\nu)-W(\\mu,\\nu)|\\leqslant2(\\log(m n)+1)\\lambda.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Proof. Let $\\begin{array}{r}{h(T)=-\\sum_{i,j=1}^{n,m}T_{i j}\\log T_{i j}+1}\\end{array}$ , by [35, Proposition 1], we have ", "page_idx": 37}, {"type": "equation", "text": "$$\n0\\leqslant W_{1,\\lambda}(\\mu,\\nu)-W(\\mu,\\nu)\\leqslant c\\lambda.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where $c=\\operatorname*{max}\\left\\{h(T)\\right.$ : transportation plan $T$ is achieve optimal loss $W(\\mu,\\nu)\\}$ . Since $T_{i j}\\in[0,1]$ and $f(x)=-x\\log x$ is concave, we have ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle h(T)=-\\,n m\\cdot\\frac1{n m}\\sum_{i,j=1}^{n,m}T_{i j}\\log T_{i j}+1}.}\\\\ {{\\displaystyle\\leqslant-n m\\cdot\\left(\\frac1{n m}\\sum_{i,j=1}^{n,m}T_{i j}\\right)\\log\\left(\\frac1{n m}\\sum_{i,j=1}^{n,m}T_{i j}\\right)+1}}\\\\ {{\\displaystyle=1+\\log(n m).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "By definition of Sinkhorn distance, ", "page_idx": 37}, {"type": "equation", "text": "$$\nS_{\\lambda}(\\mu,\\nu)=W_{1,\\lambda}(\\mu,\\nu)-W_{1,\\lambda}(\\mu,\\mu)/2-W_{1,\\lambda}(\\nu,\\nu)/2.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "By (38), we get ", "page_idx": 37}, {"type": "equation", "text": "$$\n|S_{\\lambda}(\\mu,\\nu)-W(\\mu,\\nu)|\\leqslant2(\\log(m n)+1)\\lambda.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Lemma 8. Given a measure $\\mu$ on $\\mathbb{R}^{d}$ and a real function class $\\mathcal{F}$ with output dimension $d,$ suppose that the pseudo-dimension of $\\mathcal{F}$ is less than $d_{\\mathcal{F}}\\ <\\ \\infty$ and there exists $K\\ >\\ 0$ such that $|f(x)|\\;\\;\\leqslant\\;\\;K,\\forall f\\;\\;\\in\\;\\;\\mathcal{F},x\\;\\;\\in\\;\\;\\mathbb{R}^{d^{\\prime}}$ , then with probability at least $1\\,-$ $\\exp\\left(O\\left(\\delta^{-d}\\log\\left(\\delta^{-1}\\right)+d_{\\mathcal{F}}\\log\\epsilon^{-1}-n\\epsilon^{2}\\right)\\right)$ , ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{f\\in{\\mathcal{F}}}W(f_{\\#}\\mu,f_{\\#}\\mu_{n})\\leqslant\\delta+\\epsilon\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "for all $\\delta,\\epsilon>0$ , where $\\mu_{n}$ is the empirical distribution of $\\mu$ . ", "page_idx": 37}, {"type": "text", "text": "Proof. By the dual formulation of the Wasserstein distance, ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{f\\in\\mathcal{F}}W(f_{\\#}\\mu,f_{\\#}\\mu_{n})=\\operatorname*{sup}_{h\\in\\mathcal{F}_{1}(\\mathbb{R}^{d},\\mathbb{R}^{d}),h(0)=0}\\operatorname*{sup}_{f\\in\\mathcal{F}}\\mathbb{E}_{X\\sim\\mu_{n}}[h\\circ f(X)]-\\mathbb{E}_{X\\sim\\mu}[h\\circ f(X)].\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "We define ", "page_idx": 37}, {"type": "equation", "text": "$$\n{\\mathcal{N}}_{1}(\\epsilon,{\\mathcal{F}},n)=\\operatorname*{sup}_{x_{1},\\cdots,x_{n}}{\\mathcal{N}}(\\epsilon,\\{(f(x_{1}),\\cdots,f(x_{n}):f\\in{\\mathcal{F}})\\},\\|\\cdot\\|_{1}),\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where $\\mathcal{N}(\\epsilon,S,\\|\\cdot\\|_{1})$ is the covering number of set $S$ in $\\ell_{1}$ norm. Obviously, if $h$ is 1-Lipschitz, ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\mathcal{N}_{1}(\\epsilon,h\\circ\\mathcal{F},n)\\leqslant\\mathcal{N}_{1}(\\epsilon,\\mathcal{F},n)\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "By Theorem 18.4 in [2], for any fixed $h$ , ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\mathcal{N}_{1}(\\epsilon,h\\circ\\mathcal{F},n)\\leqslant\\mathcal{N}_{1}(\\epsilon,\\mathcal{F},n)\\leqslant e(d_{\\mathcal{F}}+1)\\left(\\frac{2e}{\\epsilon}\\right)^{d_{\\mathcal{F}}}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "By Theorem 17.1 in [2], ", "page_idx": 37}, {"type": "equation", "text": "$$\nP\\left(\\operatorname*{sup}_{f\\in{\\mathcal F}}\\mathbb{E}_{X\\sim\\mu_{n}}[h\\circ f(X)]-\\mathbb{E}_{X\\sim\\mu}[h\\circ f(X)]>\\epsilon\\right)\\leqslant\\exp\\left(O\\left(d_{\\mathcal{F}}\\log\\epsilon^{-1}-n\\epsilon^{2}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Let $\\mathcal{H}_{\\delta}$ be the $\\delta$ -net of the set $\\mathcal{H}=\\left\\{h:h\\in\\mathcal{F}_{1}\\left([-K,K]^{d},[-K,K]^{d}\\right),h(0)=0\\right\\}\\mathrm{in}$ $\\ell_{\\infty}$ norm. By Lemma 6 in [21], ", "page_idx": 37}, {"type": "equation", "text": "$$\n|\\mathcal{H}_{\\delta}|\\leqslant\\exp\\left(O\\left(\\delta^{-d}\\log\\delta^{-1}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Therefore, with probability no more than $\\exp\\left(O\\left(\\delta^{-d}\\log\\delta^{-1}+d_{\\mathcal{F}}\\log\\epsilon^{-1}-n\\epsilon^{2}\\right)\\right)$ ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{h\\in\\mathcal{H}_{\\delta},\\,f\\in\\mathcal{F}}\\mathbb{E}_{X\\sim\\mu_{n}}[h\\circ f(X)]-\\mathbb{E}_{X\\sim\\mu}[h\\circ f(X)]>\\epsilon.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Notice that ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{\\epsilon\\mathcal{H},f\\in\\mathcal{F}}\\mathbb{E}_{X\\sim\\mu_{n}}[h\\circ f(X)]-\\mathbb{E}_{X\\sim\\mu}[h\\circ f(X)]\\leqslant2\\delta+\\operatorname*{sup}_{h\\in\\mathcal{H}_{\\delta},f\\in\\mathcal{F}}\\mathbb{E}_{X\\sim\\mu_{n}}[h\\circ f(X)]-\\mathbb{E}_{X\\sim\\mu}[h\\circ f(X)].\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "which implies that with probability no more than $\\exp\\left(O\\left(\\delta^{-d}\\log\\delta^{-1}+d_{\\mathcal{F}}\\log\\epsilon^{-1}-n\\epsilon^{2}\\right)\\right)$ ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{h\\in\\mathcal{H},\\,f\\in\\mathcal{F}}\\mathbb{E}_{X\\sim\\mu_{n}}[h\\circ f(X)]-\\mathbb{E}_{X\\sim\\mu}[h\\circ f(X)]>2\\delta+\\epsilon\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: We accurately summarize our contributions in the abstract and introduction. Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 38}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Justification: We mention some limitations as future directions in the conclusion section. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 38}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: All assumptions and detailed proof are given. Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 39}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: We provide all implementation details in the appendix. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 39}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: We provide all implementation details in the appendix. Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 40}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: We provide all implementation details in the appendix. Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 40}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Justification: We report the confidence interval. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 40}, {"type": "text", "text": "", "page_idx": 41}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 41}, {"type": "text", "text": "Justification: We provide all implementation details in the appendix. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 41}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Justification: We have reviewed the Code. Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 41}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 41}, {"type": "text", "text": "Justification: This is a theoretical work. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to ", "page_idx": 41}, {"type": "text", "text": "generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. ", "page_idx": 42}, {"type": "text", "text": "\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 42}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 42}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 42}, {"type": "text", "text": "Justification: This is a theoretical work. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 42}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 42}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 42}, {"type": "text", "text": "Justification: This is a theoretical work. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 42}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 42}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 43}, {"type": "text", "text": "Justification:This is a theoretical work. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 43}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 43}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 43}, {"type": "text", "text": "Justification: This is a theoretical work. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 43}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 43}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 43}, {"type": "text", "text": "Justification: This is a theoretical work. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 43}]