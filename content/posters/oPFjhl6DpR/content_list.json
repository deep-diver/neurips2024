[{"type": "text", "text": "Enhancing Efficiency of Safe Reinforcement Learning via Sample Manipulation ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Shangding $\\mathbf{Gu}^{1,3}$ ,\u2217 Laixi $\\mathbf{S}\\mathbf{h}\\mathbf{i}^{2*}$ , Yuhao Ding1, Alois $\\mathbf{Knoll^{3}}$ , Costas Spanos1, Adam Wierman2, Ming Jin4 ", "page_idx": 0}, {"type": "text", "text": "1University of California, Berkeley, USA 2California Institute of Technology, USA 3Technical University of Munich, Germany 4Virginia Tech, USA ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Safe reinforcement learning (RL) is crucial for deploying RL agents in real-world applications, as it aims to maximize long-term rewards while satisfying safety constraints. However, safe RL often suffers from sample inefficiency, requiring extensive interactions with the environment to learn a safe policy. We propose Efficient Safe Policy Optimization (ESPO), a novel approach that enhances the efficiency of safe RL through sample manipulation. ESPO employs an optimization framework with three modes: maximizing rewards, minimizing costs, and balancing the trade-off between the two. By dynamically adjusting the sampling process based on the observed confilct between reward and safety gradients, ESPO theoretically guarantees convergence, optimization stability, and improved sample complexity bounds. Experiments on the Safety-MuJoCo and Omnisafe benchmarks demonstrate that ESPO significantly outperforms existing primal-based and primaldual-based baselines in terms of reward maximization and constraint satisfaction. Moreover, ESPO achieves substantial gains in sample efficiency, requiring $25{-}29\\%$ fewer samples than baselines, and reduces training time by $21{-}38\\%$ . ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Reinforcement learning (RL) [49] has demonstrated powerful capabilities in various domains [25, 46, 11]. However, ensuring safety in RL, particularly in real-world applications such as autonomous driving, robotics, and power grids, is crucial [6, 12, 31, 35, 38, 39, 41, 48, 55, 57, 17, 59]. Safe RL aims to maximize long-term cumulative rewards while adhering to additional safety cost constraints. ", "page_idx": 0}, {"type": "text", "text": "Most state-of-the-art (SOTA) safe RL methods, including primal-based baselines (e.g., CRPO [53], PCRPO [30]) and primal-dual-based methods (e.g., CUP [54], PPOLag [32]), optimize cost and reward with a predetermined sample size for each iteration. However, this approach may lead to sample inefficiency due to two main reasons: ", "page_idx": 0}, {"type": "text", "text": "\u2759 Wasted samples and computational resources in simple scenarios, where the (computational/physical) cost of obtaining these samples may outweigh their learning benefits. \u2759 Insufficient exploration in complex cases with high uncertainty or conflicting objectives, potentially hindering the learning of a safe and optimal policy. ", "page_idx": 0}, {"type": "text", "text": "A key insight from optimization literature suggests that the selection of sample size is worthwhile but a delicate issue, as it may vary depending on the optimization stage and landscape [13, 28, 51]. However, this insight remains largely unexplored in the context of safe RL, where the consideration of safety adds complexity. The presence of safety constraints can create regions with high conflict between reward and safety objectives, requiring careful balancing and potentially more samples to resolve. Therefore, an unresolved question in safe RL is: Can we enhance sample efficiency by dynamically adapting the sample size, while simultaneously improving reward performance and guaranteeing safety? ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "To address this question, we focus on primal-based approaches, which do not require fine-tuning of dual parameters or heavily rely on initialization, unlike primaldual-based optimization [30, 53]. The key to effectively enhancing sample efficiency is to establish reliable criteria for determining sample size requirements. Inspired by insights from multi-objective optimization/RL [40, 37, 30], we use gradient confilct between rewards and costs as an effective signal for adjusting sample size in each iteration. Intuitively, when gradient confilct occurs, balancing reward and safety optimization with a uniform sample size becomes challenging; conversely, when there is gradient alignment, optimizing with fewer samples is more straightforward. This motivates us to adopt a three-mode optimization framework: 1) optimizing cost exclusively upon a safety violation; 2) simultaneously optimizing both reward and cost during a soft constraint violation; 3) optimizing only the reward when no violations are present. This allows tailored sample size adjustment based on the optimization regime. We increase the sample size in situations of gradient conflict to incorporate more informative samples and reduce it in cases of gradient alignment to prevent unnecessary costs and training time. This sampling adjustment is effective in each policy learning mode (cost only, simultaneous reward and cost, and reward only), enabling the search for improved policies that prioritize safety, rewards, or a balance of both. ", "page_idx": 1}, {"type": "image", "img_path": "oPFjhl6DpR/tmp/ef90ef8c38d78142fd89cf1d091cfc33784e122a0544a7cdb798374854659c16.jpg", "img_caption": ["Figure 1: Oscillation analysis compared our method with existing safe RL methods in three modes of optimization. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "This study makes three key contributions emphasizing sample manipulation for safe RL: ", "page_idx": 1}, {"type": "text", "text": "$\\textcircled{1}$ We propose Efficient Safe Policy Optimization (ESPO), an algorithm that depart from prior arts byincorporating sample manipulation by leveraging gradient conflict signals as criteria to enhance sample efficiency and reduce unnecessary interactions with the environments, . $\\circledcirc$ We provide a comprehensive theoretical analysis of ESPO, including convergence rates, the advantages of reducing optimization oscillation, and provable sample efficiency. The theoretical results inspire ESPO\u2019s sample manipulation approach and could be of independent interest for broad RL applicability. $\\circled{3}$ We evaluate ESPO through comparative and ablation experiments on two benchmarks: SafetyMuJoCo[30] and Omnisafe[32]. The results demonstrate that ESPO improves reward performance and safety compared to SOTA primal-based and primal-dual-based baselines. Notably, ESPO significantly reduces the number of samples used during policy learning and minimizes training costs while ensuring safety and achieving superior reward performance. ", "page_idx": 1}, {"type": "text", "text": "2 Related Works ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Various methodologies have been developed to enhance safety in RL [12, 31], including constrained optimization-based methods, control-based methods [18, 19, 33, 29], and formal methods [43]. Among these, constrained optimization-based methods have gained notable popularity due to their ease of use and reduced dependency on external knowledge [31]. ", "page_idx": 1}, {"type": "text", "text": "Constrained optimization-based methods can be categorized into primal-dual (e.g., CPO [1], PCPO [56], CUP [54]) and primal approaches. Primal-dual methods face challenges in tuning dual multipliers, ensuring feasible initialization, and sensitivity to learning rates [53, 30]. Primal methods offer a distinct advantage by eliminating the need for dual multipliers. A prominent primal-based method is CRPO [53], which focuses on directly optimizing the primal problem. When safety violations occur, CRPO exclusively improves the violated constraints. However, it encounters significant challenges with conflicting gradients between optimizing rewards and constraints, which can impact ensuring both performance and ongoing safety compliance. PCRPO [30] addresses this issue by balancing the trade-offs between reward and safety performance through strategic gradient manipulation. However, it lacks comprehensive convergence and sample complexity analysis and faces computational challenges due to the need to compute reward and safety gradients in each gradient handling step. ", "page_idx": 1}, {"type": "text", "text": "Several efficient safe RL methods have been recently proposed [16, 21, 22, 23, 24, 36, 42, 47, 50], including offline [47] and off-policy settings [36, 42]. Our model-free, on-policy approach is distinguished by its dynamic calibration of sampling based on the interplay between reward maximization and safety assurance. Closely related works are [21] and [24]. [21] employs symbolic reasoning for safety but relies on external knowledge, potentially limiting applicability. [24] proposes a non-stationary safe RL approach with regret bounds using linear function approximation but may struggle with complex tasks and inherits issues common in primal-dual safe RL [22, 23]. Our primal-based method circumvents these drawbacks. ", "page_idx": 2}, {"type": "text", "text": "Adaptive sampling methods in optimization can be categorized into prescribed (e.g., geometric) sample size increase [13, 26] [13, 8], gradient approximation test [14, 7, 13, 8, 15, 10, 5], and derivative-free [45, 9] and simulation-based methods [44] (see [20] for a review). These methods focus on controlling the variance of gradient approximations or function evaluations (e.g., through inner product [8] or norm tests [14, 15]) to balance computational efficiency and sample complexity. Adaptive sampling methods have also been applied to constrained stochastic optimization problems with convex feasible sets [4, 52]. A recent work [60] extends adaptive sampling to a multi-objective setting, but their criteria are still based on variance. Our research introduces a novel perspective by focusing on confilct-aware updates based on safety and performance gradients in safe RL, making it the first adaptive sampling method for this important domain. ", "page_idx": 2}, {"type": "text", "text": "3 Problem Formulation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "A Constrained Markov Decision Process (CMDP) [3] is often used to model safe RL problems. A CMDP is denoted as $(S,{\\mathcal{A}},P,r,c,b,\\gamma)$ , where $\\boldsymbol{S}$ is the state space, $\\boldsymbol{\\mathcal{A}}$ is the action space, $P:$ $\\mathcal{S}\\times\\mathcal{A}\\times\\mathcal{S}\\rightarrow[0,1]$ is the transition probability function, $r:S\\times A\\to\\mathbb{R}$ is the reward function, and $\\gamma$ is the discount factor. To encode safety, $c=(c_{1},\\ldots,c_{n}):S\\times A\\to\\mathbb{R}^{n}$ is the cost function assigning costs to state-action pairs, with higher costs indicating higher risks, $b=(b_{1},\\dots,b_{n})\\in\\mathbb{R}^{n}$ contains safety thresholds for each constraint. This CMDP framework searches for a safe policy $\\pi$ in the stochastic Markov policy set $\\Pi$ , balancing rewards and safety constraints. ", "page_idx": 2}, {"type": "text", "text": "The expected cumulative reward values are defined as $\\begin{array}{r}{V_{r}^{\\pi}(s)=\\mathbb{E}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}r\\left(s_{t},a_{t}\\right)\\bigg\\rvert\\pi,s_{0}=s\\right]}\\end{array}$ $\\begin{array}{r}{Q_{r}^{\\pi}(s,a)=\\mathbb{E}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}r\\left(s_{t},a_{t}\\right)\\bigg\\rvert\\pi,s_{0}=s,a_{0}=a\\right]}\\end{array}$ for states and state-action pairs, respectively. Similarly, safety is quantified using the cost state values $V_{c}^{\\pi}(s)$ and cost state-action values $Q_{c}^{\\pi}(s,a)$ . The primary objective in safe RL is to maximize the accumulative reward while ensuring safety, under an initial state distribution $\\rho$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\pi\\in\\Pi}\\;V_{r}^{\\pi}(\\rho):=\\mathbb{E}_{s\\sim\\rho}\\left[V_{r}^{\\pi}(s)\\right],\\;\\;\\mathrm{s.t.}\\;V_{c}^{\\pi}(\\rho):=\\mathbb{E}_{s\\sim\\rho}[V_{c}^{\\pi}(s)]\\le b.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "However, conflicts often arise in safe RL between the reward gradient $\\mathbf{g}_{r}=\\nabla V_{r}^{\\pi}(\\rho)$ and negative cost gradient $\\mathbf{g}_{c}\\;=\\;-\\nabla V_{c}^{\\pi}(\\rho)$ . These conflicts can lead to unstable policy updates that cause experiences violating safety constraints, forcing reversion to prior policies and wasting samples. Such unstable dynamics further impede efficient exploration, risking premature convergence and squandering of computational resources. This study aims to efficiently search for a safe policy by manipulating samples to reduce waste and improve safe RL efficiency. ", "page_idx": 2}, {"type": "text", "text": "4 Algorithm Design and Analysis ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "4.1 Three-Mode Optimization ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "To improve learning efficiency and mitigate oscillations, we leverage PCRPO [30] and categorize performance optimization into three distinct strategies: focusing on reward, on both reward and cost simultaneously, or solely on cost. Two essential parameters are introduced to construct a soft constraint region \u2014 $h^{-}$ on the lower side and $h^{+}$ on the upper side. With $h^{-},h^{+}$ in hand, [30] divides the optimization process into three modes as below. Throughout the paper, we parameterize the policy $\\pi$ by $w$ . ", "page_idx": 2}, {"type": "text", "text": "$\\bullet\\,\\mathbf{l})$ ) Safety Violations. When the cost values $V_{c}^{\\pi}(\\rho)>(h^{+}+b)$ , we apply (2) to update the policy parameter $w_{t}$ with learning rate $\\eta$ . In such mode, since the constraints are violated, we prioritize safety and choose to minimize the cost objective to achieve compliance with safety standards. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\nw_{t+1}=w_{t}+\\eta\\mathbf{g}_{c}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "\u2022 2) Soft Constraint Violations. When $V_{c}^{\\pi}(\\rho)\\,\\in\\,[h^{-}+b,h^{+}+b]$ , we leverage (3) and (4) for simultaneous optimization of reward and safety performance. Specifically, when within the soft constraint region, the confilct between the reward and cost gradients is characterized by the angle $\\theta_{r,c}$ between the reward gradient ${\\bf g}_{r}$ and the cost gradient $\\mathbf{g}_{c}$ . When $\\theta_{r,c}>90^{\\circ}$ , it indicates the directions that optimize the reward and the safety performance are in conflict, and the update rule is (3). ", "page_idx": 3}, {"type": "equation", "text": "$$\nw_{t+1}=\\left\\{w_{t}+\\eta\\left[x_{t}^{r}\\left(\\mathbf{g}_{r}-\\frac{\\mathbf{g}_{r}\\cdot\\mathbf{g}_{c}}{\\|\\mathbf{g}_{c}\\|^{2}}\\mathbf{g}_{c}\\right)+x_{t}^{c}\\left(\\mathbf{g}_{c}-\\frac{\\mathbf{g}_{c}\\cdot\\mathbf{g}_{r}}{\\|\\mathbf{g}_{r}\\|^{2}}\\mathbf{g}_{r}\\right)\\right],\\right.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $x_{t}^{r},x_{t}^{c}\\geq0$ and $x_{t}^{r}+x_{t}^{c}=1$ for all $t\\in T$ . It employs gradient projection techniques [30, 58], projecting reward and cost gradients onto their normal planes and ensuring that the policy adjustment balances the conflicting objectives of maximizing rewards and minimizing costs. In contrast, when $\\theta_{r,c}\\leq90^{\\circ}$ , namely, the directions for maximizing rewards and minimizing costs are aligned or do not significantly oppose each other, we use the update rule (4). In this scenario, the gradient for the update is computed based on the weight of the reward and cost gradients. This method leverages the synergistic potential between reward maximization and cost minimization, aiming for a policy update that harmoniously improves both aspects. ", "page_idx": 3}, {"type": "text", "text": "\u2022 3) No Violations. When $V_{c}^{\\pi}(\\rho)<(h^{-}+b)$ , the update rule in (5) is applied to optimize the policy: ", "page_idx": 3}, {"type": "equation", "text": "$$\nw_{t+1}=w_{t}+\\eta\\mathbf{g}_{r}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "In other words, given that the policy adheres to all specified constraints, only the reward objective is considered. ", "page_idx": 3}, {"type": "text", "text": "4.2 Sample Size Manipulation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "As introduced above, PCRPO [30] allows for adaptive optimization updates based on different conditions. However, PCRPO and other existing safe RL methods usually apply an identical sample size during the learning process, resulting in potentially unnecessary computation cost for simpler tasks and inadequate exploration for more complex tasks. Furthermore, there is no existing theoretical analysis for PCRPO, leaving the performance guarantees of it somewhat uncharted. To address the above challenges, we propose a method called ESPO based on a crucial sample manipulation approach that will be introduced momentarily. A comprehensive theoretical analysis of ESPO is provided in Section 4.4. ", "page_idx": 3}, {"type": "text", "text": "Throughout the framework of three-mode optimization, our proposed method dynamically adjusts the number of samples utilized at each iteration based on the criteria of gradient conflict, to meet specific demands of reducing unnecessary samples in simpler scenarios and increasing exploration in more complex situations. Specifically, we consider the three-mode optimization classified by the gradient-conflict criteria respectively. 2)(a) Soft Constraint Violations with Gradient Conflict, where $\\theta_{r,c}\\:>\\:90^{\\circ}$ (cf. (6)): the cases with slight safe constraint violation and gradient conflict between reward and safety objectives. In this scenario, adjusting the sample size becomes crucial for sufficiently exploring the environments to identify a careful balanced udpate direction. We increase the sample size in (6) to enhance the likelihood of achieving a near-optimal balance between the reward and cost objectives. 2)(b)Soft Constraint Violations without Gradient Conflict, where $\\theta_{r,c}\\leq90^{\\circ}$ (cf. (7)): the cases with slight safe constraint violation and gradient alignment between reward and safet objectives. Considering it is easier to search for a update direction that beneftis the aligned reward and cost objectives, we reduce the sample size in (7) to achieve efficient learning. 1) and 3) Safety Violations and No Violations: only reward or cost objective is considered. It indicates that there is no gradient confilct since only one objective is targeted, where we also employ the update rule in (7). ", "page_idx": 3}, {"type": "text", "text": "For more details, we dynamically adjust the sample size $X_{t}$ ( $X$ denote a default fixed sample size), with $\\zeta_{t}^{+}$ and $\\zeta_{t}^{-}$ representing some sample size adjustment parameters. ", "page_idx": 3}, {"type": "equation", "text": "$$\nX_{t+1}={\\left\\{\\begin{array}{l l}{X+X\\zeta_{t}^{+},{\\mathrm{~if~}}\\,\\theta_{r,c}>90^{\\circ},}\\\\ {X+X\\zeta_{t}^{-},{\\mathrm{~if~}}\\,\\theta_{r,c}\\leq90^{\\circ}.}\\end{array}\\right.}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "This gradient-confilct-based sample manipulation is a crucial feature of our proposed method, which enables adaptively sample size tailored to the specific nature of the joint reward-safety objective landscape at each update iteration. ", "page_idx": 4}, {"type": "text", "text": "4.3 Efficient Safe Policy Optimization (ESPO) ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Building upon the above two modules \u2014 three-mode optimization and sample size manipulation, we have formulated a practical algorithm. The details of this algorithm are summarized in Algorithm 1 in Appendix B. This algorithm encompasses a strategic approach to sample size adjustment and policy updates under various conditions: 1) Safety Violations: When a safety violation occurs, we adjust the sample size $X_{t}$ using Equation (7). Simultaneously, the policy $\\pi_{w_{t}}$ is updated to ensure safety, as dictated by Equation (2). 2)(a) Soft Constraint Violations with Gradient Angle $\\leq90^{\\circ}$ : In modes of soft region violation where the angle $\\theta_{r,c}$ between gradients ${\\bf g}_{r}$ and $\\mathbf{g}_{c}$ is less than or equal to $90^{\\circ}$ , we adjust the sample size $X_{t}$ using Equation (7). The policy $\\pi_{w_{t}}$ is then updated in accordance with Equation (3). 2)(b) Soft Constraint Violations with Gradient $A n g l e>90^{\\circ}$ : Conversely, if the soft region violation occurs with a gradient angle $\\theta_{r,c}$ exceeding $90^{\\circ}$ , the sample size $X_{t}$ is adjusted via Equation (6). Policy updates are made using Equation (4). 3) No Violations: In the absence of any violations, the sample size $X_{t}$ is altered using Equation (7). The policy $\\pi_{w_{t}}$ is then updated to maximize the reward $V_{r}^{\\pi}(\\rho)$ , following Equation (5). This practical algorithm reflects an insightful analysis of the interplay between reward maximization and safety assurance in safe RL, tailoring the learning process to the specific demands of each scenario. ", "page_idx": 4}, {"type": "text", "text": "4.4 Theoretical analysis of ESPO ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we provide theoretical guarantees for the proposed ESPO, including the convergence rate guarantee and provable optimization stability and sample complexity advancements. ", "page_idx": 4}, {"type": "text", "text": "Tabular setting with softmax policy class. In this paper, we focus on a fundamental tabular setting with finite state and action space. We consider the class of policies with the softmax parameterization which is complete including all stochastic policies. Specifically, a policy $\\pi_{w}$ associated with $w\\in$ $\\mathbb{R}^{|S||A|}$ is defined as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\forall(s,a)\\in S\\times A:\\quad\\pi_{w}(a|s):=\\frac{\\exp(w(s,a))}{\\sum_{a^{\\prime}\\in A}\\exp(w(s,a^{\\prime}))}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Before proceeding, we introduce some useful notations. When executing ESPO (cf. Algorithm 1), let $B_{r}$ , $B_{\\mathrm{soft}}$ , and $B_{\\mathsf{c}}$ denote the set of iterations using Safety Violation Response (mode 1), Soft Constraint Violation Response (mode 2), and No Violation Response (mode 3) in Section 4.3, respectively. ", "page_idx": 4}, {"type": "text", "text": "I: Provable convergence of ESPO. First, we present the convergence rate of our proposed ESPO in terms of both the optimal reward and the constraint requirements in the following theorem; the proof is given in Appendix A.3. ", "page_idx": 4}, {"type": "text", "text": "Theorem 4.1. Consider tabular setting with policy class defined in (8), and any $\\delta\\,\\in\\,(0,1)$ . For Algorithm $^{\\,l}$ , \u221aapplying $\\begin{array}{r}{T_{\\mathfrak{p i}}=\\widetilde{O}\\big(\\frac{T\\log(\\frac{|S||A|}{\\delta})}{(1-\\gamma)^{3}|S||A|}\\big)^{2}}\\end{array}$ iterations for each policy evaluation step, set tolerance $\\begin{array}{r}{h^{+}=\\widetilde{O}\\big(\\frac{2\\sqrt{|S||A|}}{(1-\\gamma)^{1.5}\\sqrt{T}}\\big)}\\end{array}$ $\\eta=(1-\\gamma)^{1.5}/\\sqrt{|\\cal{S}||\\cal{A}|{T}}$ output $\\widehat{\\pi}$ of Algorithm $^{\\,I}$ satisfies that with probability at least $1-\\delta$ , ", "page_idx": 4}, {"type": "equation", "text": "$$\nV_{r}^{\\pi^{*}}(\\rho)-\\mathbb{E}[V_{r}^{\\widehat{\\pi}}(\\rho)]\\le\\widetilde O\\left(\\sqrt{\\frac{|S||\\mathcal{A}|}{(1-\\gamma)^{3}T}}\\right),\\:\\:\\mathbb{E}[V_{c}^{\\widehat{\\pi}}(\\rho)]-V_{c}^{\\pi^{*}}(\\rho)\\le\\widetilde O\\left(\\sqrt{\\frac{|S||\\mathcal{A}|}{(1-\\gamma)^{3}T}}\\right).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Here, the expectation is taken with respect to the randomness of the output $\\widehat{\\pi}$ , which is randomly selected from $\\{\\pi_{w_{t}}\\}_{1\\le i\\le T}$ with a certain probability distribution (specified in   Appendix (30)). ", "page_idx": 4}, {"type": "text", "text": "Theorem 4.1 demonstrates that taking the output policy $\\widehat{\\pi}$ as a random one selected from $\\{\\pi_{w_{t}}\\}_{1\\le i\\le T}$ following some distribution, the proposed ESPO algorithm achieves convergence to a globally optimal policy $\\pi^{\\star}$ within the feasible safe set, following the convergence rate of $\\begin{array}{r}{\\widetilde{O}\\left(\\sqrt{\\frac{S A}{(1-\\gamma)^{3}T}}\\right)}\\end{array}$ . The convergence rate for constraint violations towards 0 is also $\\begin{array}{r}{\\widetilde{O}\\left(\\sqrt{\\frac{S A}{(1-\\gamma)^{3}T}}\\right)}\\end{array}$ . While note that the implementation of Algorithm 1 in practice only need to output the final $\\widehat{\\pi}=\\pi_{w_{T}}$ for simplicity. The randomized procedure is only used for theoretical analysis. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "We observe that ESPO enjoys the same convergence rate as the well-known primal safe RL algorithm \u2014 CRPO [53]. In addition, Theorem (4.1) directly indicates the same convergence rate guarantee for PCRPO [30] \u2014 the three-mode optimization framework that our ESPO refer to, which closes the gap between practice and theoretical guarantees for PCRPO [30]. Technically, to handle the variation in ESPO\u2019s update rules across a three-mode optimization process compared to CRPO, deriving the results necessitates to overcome additional challenges by tailoring a new distribution probability for the algorithm that is used to randomly select policies from $\\{\\pi_{w_{t}}\\}_{1\\le i\\le T}$ . ", "page_idx": 5}, {"type": "text", "text": "Besides the efficient convergence, in the following, we present two advantages of ESPO in terms of both optimization benefits and sample efficiency; the proof are provided in Appendix A.4 and A.5 respectively. ", "page_idx": 5}, {"type": "text", "text": "II: Efficient optimization with reduced oscillation. Shown qualitatively in Figure 1, compared to other primal safe RL algorithms (such as CRPO), our proposed ESPO can significantly increase the ratios of iterations for maximizing the reward objective within the (relaxed) soft safe region by reducing oscillation across the safe region boundary. We provide a rigorous quantitative analysis for such advancement as below: ", "page_idx": 5}, {"type": "text", "text": "Proposition 4.2. Suppose CRPO [53] and ESPO (ours) are initialized at an identical point $w_{0}\\in$ $\\mathbb{R}^{|S||A|}$ . Denote the set of iterations that CRPO updates according to the reward objective as $B_{\\mathsf{r}}^{\\mathsf{C R P O}}$ . Then by adaptively choosing the parameters $(x_{t}^{r},x_{t}^{c})$ of Algorithm $^{\\,l}$ , if there exist iteration $t_{\\mathsf{i n}}<T$ such that $t\\in B_{\\mathsf{r}}\\cup B_{\\mathsf{s o f t}}$ , one has ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\forall t_{\\mathrm{in}}\\leq t\\leq T:\\quad t\\in\\mathcal{B}_{\\mathsf{r}}\\cup\\mathcal{B}_{\\mathsf{s o f t}},}\\\\ &{|\\mathcal{B}_{\\mathsf{r}}|+|\\mathcal{B}_{\\mathsf{s o f t}}|=T-t_{\\mathrm{in}}\\geq\\mathcal{B}_{r}^{\\mathsf{C R P O}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "In words, (9a) shows that as long as ESPO (cf. Algorithm 1) enters the safe region that the constraint is violated at most $h^{+}$ , it will stay and always (at least partially) optimizes the reward objective without oscillation across the safe region boundary. In addition, (9b) indicates that the proposed ESPO enables more iterations to maximize the reward objective inside the safe region with comparison to CRPO, accelerating the optimization towards the global optimal policy. These two theoretical guarantees are further corroborated by the phenomena in practice (shown in Table 3): ESPO spends more iterations $(99.4\\%$ steps) on optimizing the reward objective inside the safe region compared to CRPO $(35.6\\%$ steps), while only a few on solely cost objective. ", "page_idx": 5}, {"type": "text", "text": "III: Sample efficiency with sample size manipulation. Besides the efficient optimization of ESPO, the following proposition presents the provable sample efficiency of ESPO. ", "page_idx": 5}, {"type": "text", "text": "Proposition 4.3. Consider any $\\begin{array}{r}{0\\leq\\varepsilon_{1},\\varepsilon_{2}\\leq\\frac{1}{1-\\gamma}}\\end{array}$ . To meet the following goals of performance gaps ", "page_idx": 5}, {"type": "equation", "text": "$$\nV_{r}^{\\pi^{\\star}}(\\rho)-\\mathbb{E}[V_{r}^{\\widehat{\\pi}}(\\rho)]\\leq\\varepsilon_{1},\\,\\mathbb{E}[V_{c}^{\\widehat{\\pi}}(\\rho)]-V_{c}^{\\pi^{\\star}}(\\rho)\\leq\\varepsilon_{2},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "ESPO (Algorithm $^{\\,l}$ ) needs fewer number of samples than that without the sample manipulation in Section 4.2. ", "page_idx": 5}, {"type": "text", "text": "The result demonstrates that, considering the accuracy level/constraint violation requirements, the sample manipulation module contributes to a more sample-efficient algorithm ESPO (Algorithm 1). Additionally, the conflict between reward and cost gradients emerges as an effective metric for determining sample size requirements. ", "page_idx": 5}, {"type": "text", "text": "5 Experiments and Evaluation ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "To evaluate the effectiveness of our algorithm, we compare it with two key paradigms in safe RL frameworks. The first paradigm is based on the primal framework, including PCRPO [30] and CRPO [53] as the representative baselines. The second paradigm includes methods that leverage the primal-dual framework, with PCPO [56], CUP [54], and PPOLag [32] serving as representative methodologies. Our algorithm is developed within the primal framework, thereby highlighting the importance of comparing it against these paradigmatic safe RL algorithms to clearly demonstrate its performance. Experiments are conducted using both primal and primal-dual benchmarks. The Omnisafe3 [32] benchmark is leveraged for primal-dual based methods, where representative techniques such as PCPO [56], CUP [54], and PPOLag [32] generally exhibit stronger performance compared to existing primal methods like CRPO [53], a finding discussed in [27]. Additionally, we use the Safety-MuJo $C o^{4}$ [30] benchmark for primal-based methods. This benchmark, developed in 2024, is relatively new and primarily supports primal-based methods due to the specific implementation efforts involved. The detailed experimental settings are provided in Appendix D. Furthermore, to thoroughly evaluate the effectiveness of our method, we conduct a series of ablation experiments regarding different cost limits and sample manipulation techniques. In particular, we provide performance update analysis in terms of constraint violations. These experiments are specifically designed to dissect and understand the impact of various factors integral to our approach. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "5.1 Experiments of Comparison with Primal-Based Methods ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We deploy our algorithm on the Safety-MuJoCo benchmark and carry out experiments compared with representative primal algorithms, PCRPO [30] and CRPO [53]. Specifically, we conduct experiments on a set of challenging tasks, namely, SafetyReacher-v4, SafetyWalker-v4, SafetyHumanoidStandup$\\nu4$ . ", "page_idx": 6}, {"type": "image", "img_path": "oPFjhl6DpR/tmp/173e9f8fc90873db7c067945afdabad493211b56739a2dec504ca71d79ac4e17.jpg", "img_caption": [], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Figure 2: Compare our algorithm (ESPO) with PCRPO [30] and CRPO [53] on the Safety-MuJoCo benchmark. Our algorithm consistently and remarkably outperforms the SOTA baseline across multiple performance metrics, including reward maximization, safety assurance, and learning efficiency. ", "page_idx": 6}, {"type": "table", "img_path": "oPFjhl6DpR/tmp/d56cd9efc356f69f8173da71aca44a7bb7a5f2265d7b20d3adbc66b45e7fb9ee.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Table 1: Comparison of sampling steps with primal-based methods (The lower, the better). M denotes one million. ", "page_idx": 6}, {"type": "text", "text": "In the experiments conducted on the SafetyReacher-v4 task, as depicted in Figures 2(a)-(c), our method demonstrates superior performance compared to SOTA primal baselines, CRPO and PCRPO. For instance, our method achieves better reward performance than CRPO and PCRPO. Another notable aspect of ESPO\u2019s performance is its training efficiency, which is largely attributed to sample manipulation. Specifically, as depicted in Table 1, while CRPO and PCRPO utilize 8 million samples for the SafetyReacher-v4 task, our method requires only 5.7 million samples for the same task. Crucially, our method improves reward and efficiency performance without sacrificing safety. However, CRPO and PCRPO are struggling to ensure safety during policy learning. Ensuring safety is a pivotal aspect of RL in safety-critical environments. The experiment results indicate that our method\u2019s ability to balance safety with other performance metrics is a significant improvement. As illustrated in Figures 2(d)-(f), our comparison experiments on the challenging SafetyWalker- $\\nu4$ task, yielding findings consistent with those observed in SafetyReacher- $\\nu4$ tasks. Due to space limits, additional experiments on SafetyHumanoidStandup- $\\cdot\\nu4$ are postponed to Appendix D. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "5.2 Experiments of Comparison with Primal-Dual-Based Methods ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "The Omnisafe Benchmark is a popular platform for evaluating the performance of safe RL algorithms. To further examine the effectiveness of our method, we have implemented our algorithm within the Omnisafe framework and conducted an extensive series of experiments compared with SOTA primal-dual-based baselines, e.g., PPOLag [32], CUP [54] and PCPO [56], focusing mainly on challenging tasks such as SafetyHopperVelocity-v1 and SafetyAntVelocity- $\\nu I$ . ", "page_idx": 7}, {"type": "image", "img_path": "oPFjhl6DpR/tmp/74422c5da8154e1799d788a0fdad82e42fded2072cd3e9bc5391c0208985a644.jpg", "img_caption": [], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Figure 3: Compare our algorithm (ESPO) with PCPO [56], CUP [54] and PPOLag [32] on the Omnisafe benchmark. Our algorithm performs significantly better than the SOTA baselines regarding reward, safety, and efficiency performance. ", "page_idx": 7}, {"type": "table", "img_path": "oPFjhl6DpR/tmp/f49c3c8080755fdb9279baf072fe7884279286d7a10daab057ca15628313d8db.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Table 2: Comparison of sampling steps with primal-dual based methods (The lower, the better). M denotes one million samples. ", "page_idx": 7}, {"type": "text", "text": "The efficacy of our algorithm, ESPO, is demonstrated in Figures 3(a)-(c), where it is benchmarked against SOTA baselines on the SafetyHopperVelocity- $\\cdot\\nu l$ tasks. Firstly, ESPO is remarkably able to achieve better reward performance than the SOTA primal-dual-based baselines. Secondly, a critical aspect of our algorithm is its capability to ensure safety. It is particularly significant considering that some of the compared baselines, such as CUP [54] and PPOLag [32], struggle to maintain safety within the same task parameters. Thirdly, an outstanding feature of ESPO is its efficiency, as evidenced by approximately half the training time required compared to the SOTA baselines like CUP and PPOLag. This efficiency in training time demonstrates ESPO\u2019s practicality for use in various applications, especially where computational resources and time are constraints. Moreover, while PCPO [56] manages to ensure safety, its reward performance is inferior to ESPO\u2019s. PCPO also requires more training time than ESPO, underscoring our algorithm\u2019s reward, safety performance, and training efficiency advantages. Particularly, as illustrated in Table 2, across the entire training period, all the benchmark baselines, including PCPO, CUP, and PPOLag, utilized 10 million samples for tasks on SafetyHopperVelocity- $\\nu I$ . In contrast, our method required only 7.3 million samples for the SafetyHopperVelocity-v1 task. The trends observed in the performance of our algorithm on the ", "page_idx": 7}, {"type": "text", "text": "SafetyHopperVelocity- $\\nu l$ task are similarly reflected in the results presented in Figures 3(d)-(f), about the SafetyAntVelocity- $\\cdot\\nu l$ task. These findings further prove the effectiveness of ESPO in various tasks. Note that the reduction in samples may not equate to a corresponding reduction in training time, as this can vary depending on the characteristics of the benchmarks and the algorithms applied to different tasks. Factors such as the action space of the task and the settings of parallel processing supported by the benchmark can influence the overall training time. ", "page_idx": 8}, {"type": "text", "text": "These results on Omnisafe tasks further highlight the strengths of ESPO in improving reward performance with safety assurance while maintaining greater efficiency in training. The ability of ESPO validates its potential as an effective solution for further exploration and application in real-world environments. ", "page_idx": 8}, {"type": "text", "text": "5.3 Ablation Experiments ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We conducted ablation studies focusing on various cost limits, sample sizes, learning rates, gradient weights, and update styles to further assess our method\u2019s effectiveness. These studies are crucial for gaining deeper insights into our method, highlighting its strengths, and identifying potential areas for improvement. Through this evaluation, we aim to demonstrate the adaptability of our method, confirming its applicability and efficacy across a broad spectrum of safe RL scenarios. Due to space limits, details of the ablation studies are provided in Appendix C. ", "page_idx": 8}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In the study, we improved the efficiency of safe RL through a three-mode optimization scheme employing sample manipulation. We provide an in-depth theoretical analysis of convergence, stability, and sample complexity. These theoretical insights inform a practical algorithm for safety-critical control. Extensive experiments on two major benchmarks, Safety-MuJoCo and Omnisafe, indicate that our method not only surpasses the SOTA baselines in terms of efficiency but also achieves higher reward performance while maintaining safety. Moving forward, we plan to assess our method\u2019s capabilities in real world control applications to further expand its influential reach into safety-critical domains. Impact and limitation statements are provided in Appendix E. ", "page_idx": 8}, {"type": "text", "text": "Acknowledgement ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "The work of L. Shi is supported in part by the Resnick Institute and Computing, Data, and Society Postdoctoral Fellowship at California Institute of Technology. The work of A. Wierman is supported in part from CNS-2146814, CPS-2136197, CNS-2106403, and NGSDI-2105648. M. Jin acknowledges the support from NSF ECCS-2331775. The work of S. Gu is supported by funds from the Prof. Spanos\u2019 Andrew S. Grove Endowed Chair. ", "page_idx": 8}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "[1] Joshua Achiam, David Held, Aviv Tamar, and Pieter Abbeel. Constrained policy optimization. In International conference on machine learning, pages 22\u201331. PMLR, 2017.   \n[2] Alekh Agarwal, Sham M Kakade, Jason D Lee, and Gaurav Mahajan. Optimality and approximation with policy gradient methods in Markov decision processes. arXiv preprint arXiv:1908.00261, 2019.   \n[3] Eitan Altman. Constrained Markov Decision Processes, volume 7. CRC Press, 1999.   \n[4] Florian Beiser, Brendan Keith, Simon Urbainczyk, and Barbara Wohlmuth. Adaptive sampling strategies for risk-averse stochastic optimization with constraints. IMA Journal of Numerical Analysis, 43(6):3729\u20133765, 2023.   \n[5] Albert S Berahas, Liyuan Cao, and Katya Scheinberg. Global convergence rate analysis of a generic line search algorithm with noise. SIAM Journal on Optimization, 31(2):1489\u20131518, 2021.   \n[6] Felix Berkenkamp, Matteo Turchetta, Angela Schoellig, and Andreas Krause. Safe model-based reinforcement learning with stability guarantees. Advances in neural information processing systems, 30, 2017.   \n[7] Dimitri Bertsekas, Angelia Nedic, and Asuman Ozdaglar. Convex analysis and optimization, volume 1. Athena Scientific, 2003.   \n[8] Raghu Bollapragada, Richard Byrd, and Jorge Nocedal. Adaptive sampling strategies for stochastic optimization. SIAM Journal on Optimization, 28(4):3312\u20133343, 2018.   \n[9] Raghu Bollapragada, Cem Karamanli, and Stefan M Wild. Derivative-free optimization via adaptive sampling strategies. arXiv preprint arXiv:2404.11893, 2024.   \n[10] L\u00e9on Bottou, Frank E Curtis, and Jorge Nocedal. Optimization methods for large-scale machine learning. SIAM review, 60(2):223\u2013311, 2018.   \n[11] Noam Brown and Tuomas Sandholm. Superhuman ai for multiplayer poker. Science, 365(6456):885\u2013890, 2019.   \n[12] Lukas Brunke, Melissa Greeff, Adam W Hall, Zhaocong Yuan, Siqi Zhou, Jacopo Panerati, and Angela P Schoellig. Safe learning in robotics: From learning-based control to safe reinforcement learning. Annual Review of Control, Robotics, and Autonomous Systems, 5:411\u2013444, 2022.   \n[13] Richard H Byrd, Gillian M Chin, Jorge Nocedal, and Yuchen Wu. Sample size selection in optimization methods for machine learning. Mathematical programming, 134(1):127\u2013155, 2012.   \n[14] Richard G Carter. On the global convergence of trust region algorithms using inexact gradient information. SIAM Journal on Numerical Analysis, 28(1):251\u2013265, 1991.   \n[15] Coralia Cartis and Katya Scheinberg. Global convergence rate analysis of unconstrained optimization methods based on probabilistic models. Mathematical Programming, 169:337\u2013 375, 2018.   \n[16] Hongyi Chen and Changliu Liu. Safe and sample-efficient reinforcement learning for clustered dynamic environments. IEEE Control Systems Letters, 6:1928\u20131933, 2021.   \n[17] Xin Chen, Guannan Qu, Yujie Tang, Steven Low, and Na Li. Reinforcement learning for selective key applications in power systems: Recent advances and future challenges. IEEE Transactions on Smart Grid, 13(4):2935\u20132958, 2022.   \n[18] Yinlam Chow, Ofir Nachum, Edgar Duenez-Guzman, and Mohammad Ghavamzadeh. A lyapunov-based approach to safe reinforcement learning. Advances in neural information processing systems, 31, 2018.   \n[19] Yinlam Chow, Ofir Nachum, Aleksandra Faust, Edgar Duenez-Guzman, and Mohammad Ghavamzadeh. Lyapunov-based safe policy optimization for continuous control. arXiv preprint arXiv:1901.10031, 2019.   \n[20] Frank E Curtis and Katya Scheinberg. Adaptive stochastic optimization: A framework for analyzing stochastic optimization algorithms. IEEE Signal Processing Magazine, 37(5):32\u201342, 2020.   \n[21] Floris Den Hengst, Vincent Fran\u00e7ois-Lavet, Mark Hoogendoorn, and Frank van Harmelen. Planning for potential: efficient safe reinforcement learning. Machine Learning, 111(6):2255\u2013 2274, 2022.   \n[22] Dongsheng Ding, Xiaohan Wei, Zhuoran Yang, Zhaoran Wang, and Mihailo Jovanovic. Provably efficient safe exploration via primal-dual policy optimization. In Proceedings of The 24th International Conference on Artificial Intelligence and Statistics, volume 130 of Proceedings of Machine Learning Research, pages 3304\u20133312. PMLR, 13\u201315 Apr 2021.   \n[23] Dongsheng Ding, Kaiqing Zhang, Jiali Duan, Tamer Ba\u00b8sar, and Mihailo R Jovanovic\u00b4. Convergence and sample complexity of natural policy gradient primal-dual methods for constrained mdps. arXiv preprint arXiv:2206.02346, 2022.   \n[24] Yuhao Ding and Javad Lavaei. Provably efficient primal-dual reinforcement learning for cmdps with non-stationary objectives and constraints. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 7396\u20137404, 2023.   \n[25] Yan Duan, Xi Chen, Rein Houthooft, John Schulman, and Pieter Abbeel. Benchmarking deep reinforcement learning for continuous control. In International conference on machine learning, pages 1329\u20131338. PMLR, 2016.   \n[26] Michael P Friedlander and Mark Schmidt. Hybrid deterministic-stochastic methods for data fitting. SIAM Journal on Scientific Computing, 34(3):A1380\u2013A1405, 2012.   \n[27] Milan Ganai, Zheng Gong, Chenning Yu, Sylvia Herbert, and Sicun Gao. Iterative reachability estimation for safe reinforcement learning. Advances in Neural Information Processing Systems, 36, 2024.   \n[28] Zhan Gao, Alec Koppel, and Alejandro Ribeiro. Balancing rates and variance via adaptive batch-size for stochastic optimization problems. IEEE Transactions on Signal Processing, 70:3693\u20133708, 2022.   \n[29] Fangda Gu, He Yin, Laurent El Ghaoui, Murat Arcak, Peter Seiler, and Ming Jin. Recurrent neural network controllers synthesis with stability guarantees for partially observed systems. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 5385\u20135394, 2022.   \n[30] Shangding Gu, Bilgehan Sel, Yuhao Ding, Lu Wang, Qingwei Lin, Ming Jin, and Alois Knoll. Balance reward and safety optimization for safe reinforcement learning: A perspective of gradient manipulation. In AAAI, 2024.   \n[31] Shangding Gu, Long Yang, Yali Du, Guang Chen, Florian Walter, Jun Wang, Yaodong Yang, and Alois Knoll. A review of safe reinforcement learning: Methods, theory and applications. arXiv preprint arXiv:2205.10330, 2022.   \n[32] Jiaming Ji, Jiayi Zhou, Borong Zhang, Juntao Dai, Xuehai Pan, Ruiyang Sun, Weidong Huang, Yiran Geng, Mickel Liu, and Yaodong Yang. Omnisafe: An infrastructure for accelerating safe reinforcement learning research. arXiv preprint arXiv:2305.09304, 2023.   \n[33] Ming Jin and Javad Lavaei. Stability-certified reinforcement learning: A control-theoretic perspective. IEEE Access, 8:229086\u2013229100, 2020.   \n[34] Sham Kakade and John Langford. Approximately optimal approximate reinforcement learning. In Proc. International Conference on Machine Learning (ICML), volume 2, pages 267\u2013274, 2002.   \n[35] Dohyeong Kim, Kyungjae Lee, and Songhwai Oh. Trust region-based safe distributional reinforcement learning for multiple constraints. Advances in neural information processing systems, 36, 2024.   \n[36] Dohyeong Kim and Songhwai Oh. Efficient off-policy safe reinforcement learning using trust region conditional value at risk. IEEE Robotics and Automation Letters, 7(3):7644\u20137651, 2022.   \n[37] Bo Liu, Xingchao Liu, Xiaojie Jin, Peter Stone, and Qiang Liu. Confilct-averse gradient descent for multi-task learning. Advances in Neural Information Processing Systems, 34:18878\u201318890, 2021.   \n[38] Puze Liu, Haitham Bou-Ammar, Jan Peters, and Davide Tateo. Safe reinforcement learning on the constraint manifold: Theory and applications. arXiv preprint arXiv:2404.09080, 2024.   \n[39] Zuxin Liu, Zijian Guo, Zhepeng Cen, Huan Zhang, Jie Tan, Bo Li, and Ding Zhao. On the robustness of safe reinforcement learning under observational perturbations. In The Eleventh International Conference on Learning Representations, 2022.   \n[40] Debabrata Mahapatra and Vaibhav Rajan. Multi-task learning with user preferences: Gradient descent with controlled ascent in pareto optimization. In International Conference on Machine Learning, pages 6597\u20136607. PMLR, 2020.   \n[41] Rohan Mitta, Hosein Hasanbeig, Jun Wang, Daniel Kroening, Yiannis Kantaros, and Alessandro Abate. Safeguarded progress in reinforcement learning: Safe bayesian exploration for control policy synthesis. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 21412\u201321419, 2024.   \n[42] R\u00e9mi Munos, Tom Stepleton, Anna Harutyunyan, and Marc Bellemare. Safe and efficient off-policy reinforcement learning. Advances in neural information processing systems, 29, 2016.   \n[43] Anitha Murugesan, Mohammad Moghadamfalahi, and Arunabh Chattopadhyay. Formal methods assisted training of safe reinforcement learning agents. In NASA Formal Methods: 11th International Symposium, NFM 2019, Houston, TX, USA, May 7\u20139, 2019, Proceedings 11, pages 333\u2013340. Springer, 2019.   \n[44] Raghu Pasupathy, Peter Glynn, Soumyadip Ghosh, and Fatemeh S Hashemi. On sampling rates in simulation-based recursions. SIAM Journal on Optimization, 28(1):45\u201373, 2018.   \n[45] Sara Shashaani, Fatemeh S Hashemi, and Raghu Pasupathy. Astro-df: A class of adaptive sampling trust-region algorithms for derivative-free stochastic optimization. SIAM Journal on Optimization, 28(4):3145\u20133176, 2018.   \n[46] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. nature, 529(7587):484\u2013489, 2016.   \n[47] Dylan Z Slack, Yinlam Chow, Bo Dai, and Nevan Wichers. Safer: Data-efficient and safe reinforcement learning via skill acquisition. In Decision Awareness in Reinforcement Learning Workshop at ICML 2022, 2022.   \n[48] Yanan Sui, Alkis Gotovos, Joel Burdick, and Andreas Krause. Safe exploration for optimization with gaussian processes. In International conference on machine learning, pages 997\u20131005. PMLR, 2015.   \n[49] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.   \n[50] Daniel Tabas and Baosen Zhang. Computationally efficient safe reinforcement learning for power systems. In 2022 American Control Conference (ACC), pages 3303\u20133310. IEEE, 2022.   \n[51] Tim Tsz-Kit Lau, Han Liu, and Mladen Kolar. Adadagrad: Adaptive batch size schemes for adaptive gradient methods. arXiv e-prints, pages arXiv\u20132402, 2024.   \n[52] Yuchen Xie, Raghu Bollapragada, Richard Byrd, and Jorge Nocedal. Constrained and composite optimization via adaptive sampling methods. IMA Journal of Numerical Analysis, 44(2):680\u2013 709, 2024.   \n[53] Tengyu Xu, Yingbin Liang, and Guanghui Lan. Crpo: A new approach for safe reinforcement learning with convergence guarantee. In International Conference on Machine Learning, pages 11480\u201311491. PMLR, 2021.   \n[54] Long Yang, Jiaming Ji, Juntao Dai, Linrui Zhang, Binbin Zhou, Pengfei Li, Yaodong Yang, and Gang Pan. Constrained update projection approach to safe policy optimization. Advances in Neural Information Processing Systems, 35:9111\u20139124, 2022.   \n[55] Qisong Yang, Thiago D Sim\u00e3o, Simon H Tindemans, and Matthijs TJ Spaan. Wcsac: Worst-case soft actor critic for safety-constrained reinforcement learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 10639\u201310646, 2021.   \n[56] Tsung-Yen Yang, Justinian Rosca, Karthik Narasimhan, and Peter J. Ramadge. Projection-based constrained policy optimization. In International Conference on Learning Representations, 2020.   \n[57] Ming Yu, Zhuoran Yang, Mladen Kolar, and Zhaoran Wang. Convergent policy optimization for safe reinforcement learning. Advances in Neural Information Processing Systems, 32, 2019.   \n[58] Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, and Chelsea Finn. Gradient surgery for multi-task learning. Advances in Neural Information Processing Systems, 33:5824\u20135836, 2020.   \n[59] Weiye Zhao, Tairan He, Rui Chen, Tianhao Wei, and Changliu Liu. State-wise safe reinforcement learning: a survey. In Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence, pages 6814\u20136822, 2023.   \n[60] Yong Zhao, Wang Chen, and Xinmin Yang. Adaptive sampling stochastic multigradient algorithm for stochastic multiobjective optimization. Journal of Optimization Theory and Applications, 200(1):215\u2013241, 2024. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A Proof of the theoretical analysis ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Inspired by [53], the theoretical results in this section are established by tailoring to our algorithm ESPO to ensure the key recursion relation still hold for the proposed complex update rules \u2014 different update rules in three different modes. ", "page_idx": 13}, {"type": "text", "text": "A.1 Preliminaries ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "To proceed, we first introduce some notations and invoke several key facts and results that have been derived by prior arts. ", "page_idx": 13}, {"type": "text", "text": "Notation. We recall and introduce some useful notation throughout this section. ", "page_idx": 13}, {"type": "text", "text": "\u2022 $\\bar{Q}_{t}^{r},\\bar{Q}_{t}^{c}$ : this two function represent the policy evaluation results from Algoriathm 1, namely, the estimates of true Q-functions $Q_{r}^{w_{t}},Q_{c}^{w_{t}}$ .   \n\u2022 $\\eta$ : the learning rate of the NPG update rule in Algoriathm 1.   \n\u2022 Bsnooft, Bscoofntf : we denote the set of iterations when Algorithm 1 executes (4) (resp. (3)) as $B_{\\mathrm{soft}}^{\\mathsf{n o}}$ (resp. $B_{\\mathsf{s o f t}}^{\\mathsf{c o n f}}$ ).   \n\u2022 $(x_{t}^{r},x_{t}^{c})$ : when the iteration $t\\in\\mathcal{B}_{\\mathrm{soft}}^{\\mathsf{n o}}$ (no conflict between the gradients of reward and cost objectives), $\\boldsymbol{x}_{t}^{r}$ (resp. $x_{t}^{c}$ ) represents the weight of the gradient w.r.t. the reward objective (resp. the cost function). So it is easily verified that $0\\leq x_{t}^{r},x_{t}^{c}\\leq1$ and $x_{t}^{r}+x_{t}^{c}=1$ .   \n\u2022 $(y_{t}^{r},y_{t}^{c})$ : when the iteratiorn $t\\,\\in\\,{\\mathcal B}_{\\mathrm{soft}}^{\\mathsf{c o n f}}$ (the gradients of reward and cost objectives are confilct with each other), (resp. $y_{t}^{c}$ ) represents the weight of the gradient w.r.t. the reward objective (resp. the cost function). So it is easily verified that $y_{t}^{r},y_{t}^{c}\\geq0$ .   \n\u2022 $v_{\\mathrm{max}}$ : without loss of generality, we assume $r(s,a)\\in[0,v_{\\mathrm{max}}]$ and $c_{i}(s,a)\\in[0,v_{\\operatorname*{max}}]$ for all $1\\leq i\\leq n$ .   \n\u2022 $h^{+},h^{-}$ : for simplicity, we let $h_{t}^{+}=h^{+},h_{t}^{-}=h^{-}$ for all $1\\leq t\\leq T$ . ", "page_idx": 13}, {"type": "text", "text": "Lemma A.1 (Performance difference lemma [34] ). For any policies $\\pi$ , $\\pi^{\\prime}$ and initial distribution $\\rho$ , one has ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\forall i\\in\\{c,r\\}:\\quad V_{i}^{\\pi}(\\rho)-V_{i}^{\\pi^{\\prime}}(\\rho)=\\frac{1}{1-\\gamma}\\mathbb{E}_{s\\sim d_{\\rho}}\\left[\\mathbb{E}_{a\\sim\\pi(\\cdot|s)}[A_{i}^{\\pi^{\\prime}}(s,a)]\\right],\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $V_{i}^{\\pi}(\\rho)$ and $d_{\\rho}$ denote the accumulated reward (cost) function and state-action visitation distribution under policy $\\pi$ when the initial state distribution is $\\rho$ . Here, $A_{i}^{\\pi^{\\prime}}(s,a)=Q_{i}^{\\pi^{\\prime}}(s,a)\\:-\\:$ $V_{i}^{\\pi^{\\prime}}(s)$ is the advantage function of policy $\\pi$ over state-action pair $(s,a)$ . ", "page_idx": 13}, {"type": "text", "text": "Lemma A.2. Considering the approximated NPG update rule and Algorithm $^{\\,l}$ in the tabular setting, the NPG update in four possible diverse modes take the form: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\{w_{t+1}=w_{t}+\\frac{\\eta}{1-\\gamma}\\bar{Q}_{t}^{r}\\quad a n d\\quad\\pi_{w_{t+1}}(a|s)=\\pi_{w_{t}}(a|s)\\frac{\\exp\\left(\\frac{\\eta Q_{t}^{r}(s,a)}{(1-\\gamma)}\\right)}{Z_{t}^{r}(s)},\\quad\\begin{array}{r l r}&{i f t\\in B_{r}}\\\\ {w_{t+1}=w_{t}+\\frac{\\eta\\left(x_{t}^{r}\\bar{Q}_{t}^{r}+x_{t}^{c}\\bar{Q}_{t}^{c}\\right)}{1-\\gamma}\\;a n d\\;\\pi_{w_{t+1}}(a|s)=\\pi_{w_{t}}(a|s)\\frac{\\exp\\left(\\frac{\\eta(x_{t}^{r}\\bar{Q}_{t}^{r}(s,a)+x_{t}^{c}\\bar{Q}_{t}^{c}(s,a))}{Z_{t}^{r,c}(s)}\\right)}{Z_{t}^{r,c}(s)},}&{i f t\\in B_{\\mathrm{sot}}^{\\mathrm{nop}}}\\\\ {w_{t+1}=w_{t}+\\frac{\\eta\\left(y_{t}^{r}\\bar{Q}_{t}^{r}+y_{t}^{c}\\bar{Q}_{t}^{c}\\right)}{1-\\gamma}\\;a n d\\;\\pi_{w_{t+1}}(a|s)=\\pi_{w_{t}}(a|s)\\frac{\\exp\\left((\\frac{\\eta(y_{t}^{r}\\bar{Q}_{t}^{r}(s,a)+y_{t}^{c}\\bar{Q}_{t}^{c}(s,a))}{(1-\\gamma)}\\right)}{Z_{t}^{r,c}(s)},}&{i f t\\in B_{\\mathrm{sot}}^{\\mathrm{cop}}}\\\\ {w_{t+1}=w_{t}+\\frac{\\eta}{1-\\gamma}\\bar{Q}_{t}^{c}\\quad a n d\\quad\\pi_{w_{t+1}}(a|s)=\\pi_{w_{t}}(a|s)\\frac{\\exp\\left(\\frac{\\eta Q_{t}^{c}(s,a)}{(1-\\gamma)}\\right)}{Z_{t}^{r,c}(s)},}&{i f t\\in B_{\\mathrm{sot}}^{\\mathrm{cop}}}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{Z_{t}^{r}(s)=\\displaystyle\\sum_{a\\in A}\\pi_{w_{t}}(a|s)\\exp\\left(\\frac{\\eta\\bar{Q}_{t}^{r}(s,a)}{1-\\gamma}\\right),}\\\\ &{Z_{t}^{r,c,1}(s)=\\displaystyle\\sum_{a\\in A}\\pi_{w_{t}}(a|s)\\exp\\left(\\frac{\\eta\\left(x_{t}^{r}\\bar{Q}_{t}^{r}(s,a)+x_{t}^{c}\\bar{Q}_{t}^{c}(s,a)\\right)}{(1-\\gamma)}\\right)}\\\\ &{Z_{t}^{c}(s)=\\displaystyle\\sum_{a\\in A}\\pi_{w_{t}}(a|s)\\exp\\left(\\frac{\\eta\\bar{Q}_{t}^{c}(s,a)}{1-\\gamma}\\right),}\\\\ &{Z_{t}^{r,c,2}(s)=\\displaystyle\\sum_{a\\in A}\\pi_{w_{t}}(a|s)\\exp\\left(\\frac{\\eta\\left(y_{t}^{r}\\bar{Q}_{t}^{r}(s,a)+y_{t}^{c}\\bar{Q}_{t}^{c}(s,a)\\right)}{(1-\\gamma)}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Proof. The first line of (12) has been verified by [Lemma 5.6. [2]]. Following the same proof pipeline for the update rules of Algorithm 1 in different modes completes the proof. \u53e3 ", "page_idx": 14}, {"type": "text", "text": "A.2 Key lemmas ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The proof of Theorem 4.1 heavily count on several key lemmas in the following. ", "page_idx": 14}, {"type": "text", "text": "First, we introduce the performance improvement bound for the update rules of Algorithm 1 in different modes, which is a fundamental result for its convergence; the proof is postponed to Appendix A.6.1. ", "page_idx": 14}, {"type": "text", "text": "Lemma A.3 (Performance improvement bound for approximated NPG). Consider any initial state distribution $\\rho$ and the iterate $\\pi_{w_{t}}$ generated by Algorithm 1 at time step t. One has when iteration $t\\in\\mathcal{B}_{\\mathfrak{r}}$ : ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V_{r}^{\\pi_{w t+1}}(\\rho)-V_{r}^{\\pi_{w t}}(\\rho)}\\\\ &{\\geq\\displaystyle\\frac{1-\\gamma}{\\eta}\\mathbb{E}_{s\\sim\\rho}\\left(\\log Z_{t}^{r}(s)-\\frac{\\eta}{1-\\gamma}V_{r}^{\\pi_{w t}}(s)+\\frac{\\eta}{1-\\gamma}\\sum_{a\\in A}\\pi_{w t}(a|s)\\left|\\bar{Q}_{t}^{r}(s,a)-Q_{r}^{\\pi_{w t}}(s,a)\\right|\\right)}\\\\ &{\\quad-\\displaystyle\\frac{1}{1-\\gamma}\\mathbb{E}_{s\\sim d_{\\rho}}\\sum_{a\\in A}\\pi_{w_{t}}(a|s)\\left|\\bar{Q}_{t}^{r}(s,a)-Q_{r}^{\\pi_{w t}}(s,a)\\right|}\\\\ &{\\quad-\\displaystyle\\frac{1}{1-\\gamma}\\mathbb{E}_{s\\sim d_{\\rho}}\\sum_{a\\in A}\\pi_{w_{t+1}}(a|s)\\left|\\bar{Q}_{t}^{r}(s,a)-Q_{r}^{\\pi_{w_{t}}}(s,a)\\right|:=\\mathrm{diff}_{t}^{r}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Similarly, we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\forall t\\in\\mathcal{B}_{\\mathsf{c}}:\\quad V_{c}^{\\pi_{w_{t+1}}}(\\rho)-V_{c}^{\\pi_{w_{t}}}(\\rho)\\geq\\mathsf{d i f f}_{t}^{c},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "and then ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\int x_{t}^{r}\\left(V_{r}^{\\pi_{w_{t+1}}}(\\rho)-V_{r}^{\\pi_{w_{t}}}(\\rho)\\right)+x_{t}^{c}\\left(V_{c}^{\\pi_{w_{t+1}}}(\\rho)-V_{c}^{\\pi_{w_{t}}}(\\rho)\\right)\\geq x_{t}^{r}\\mathsf{d i f f}_{t}^{r}+x_{t}^{c}\\mathsf{d i f f}_{t}^{c}\\quad i f\\,t\\in B_{\\mathrm{soft}}^{\\mathrm{no}}}\\\\ {y_{t}^{r}\\left(V_{r}^{\\pi_{w_{t+1}}}(\\rho)-V_{r}^{\\pi_{w_{t}}}(\\rho)\\right)+y_{t}^{c}\\left(V_{c}^{\\pi_{w_{t+1}}}(\\rho)-V_{c}^{\\pi_{w_{t}}}(\\rho)\\right)\\geq y_{t}^{r}\\mathsf{d i f f}_{t}^{r}+y_{t}^{c}\\mathsf{d i f f}_{t}^{c}\\quad i f\\,t\\in B_{\\mathrm{soft}}^{\\mathrm{conf}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Armed with above lemma, now we can control the performance gap between the current poliy $\\pi_{w_{t}}$ and the optimal policy $\\pi^{\\star}$ in the following lemma; the proof is postponed to Appendix A.6.2. ", "page_idx": 14}, {"type": "text", "text": "Lemma A.4 (Suboptimality gap bound for update rules of Algorithm 1). Consider the approximated NPG updates in (12). When iteration $t\\in\\mathcal{B}_{\\mathfrak{r}}$ , denoting the visitation distribution under the optimal policy as $d^{\\star}$ , we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V_{r}^{\\pi^{*}}(\\rho)-V_{r}^{\\pi_{w_{t}}}(\\rho)}\\\\ &{\\leq\\cfrac{1}{\\eta}\\mathbb{E}_{s\\sim d^{*}}(D_{K L}(\\pi^{*}||\\pi_{w_{t}})-D_{K L}(\\pi^{*}||\\pi_{w_{t+1}}))+\\cfrac{2\\eta|S||A|v_{\\operatorname*{max}}^{2}}{(1-\\gamma)^{3}}+\\cfrac{3(1+\\eta v_{\\operatorname*{max}})}{(1-\\gamma)^{2}}\\|Q_{\\pi_{w_{t}}}^{r}-\\bar{Q}_{r}^{\\pi_{w_{t}}}\\|}\\\\ &{\\leq\\cfrac{1}{\\eta}\\sum_{s=\\bar{t}^{*}}\\left(D_{K L}(\\pi^{*}||\\pi_{w_{t}})-D_{K L}(\\pi^{*}||\\pi_{w_{t+1}})\\right)+\\cfrac{2\\eta|S||A|v_{\\operatorname*{max}}^{2}}{(1-\\gamma)^{3}}+\\cfrac{3(1+\\eta v_{\\operatorname*{max}})}{(1-\\gamma)^{2}}\\|Q_{\\pi_{w_{t}}}^{r}-\\bar{Q}_{r}^{\\pi_{w_{t}}}\\|}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": ":= gaptr ", "page_idx": 14}, {"type": "text", "text": "Similarly, we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\forall t\\in\\mathcal{B}_{\\mathtt{c}}:\\quad V_{c}^{\\pi^{*}}(\\rho)-V_{c}^{\\pi_{w_{t}}}(\\rho)\\leq\\mathtt{g a p}_{t}^{c}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "In addition, for other iterations: if $t\\in B_{\\mathrm{soft}}^{\\mathsf{n o}}$ , we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{x_{t}^{r}\\left(V_{r}^{\\pi^{*}}(\\rho)-V_{r}^{\\pi_{w_{t}}}(\\rho)\\right)+x_{t}^{c}\\left(V_{c}^{\\pi^{*}}(\\rho)-V_{c}^{\\pi_{w_{t}}}(\\rho)\\right)}\\\\ &{\\leq\\cfrac{1}{\\eta}\\mathbb{E}_{s\\sim d^{*}}(D_{K L}(\\pi^{*}||\\pi_{w_{t}})-D_{K L}(\\pi^{*}||\\pi_{w_{t+1}}))+\\cfrac{2\\eta v_{\\operatorname*{max}}^{2}|S||A|}{(1-\\gamma)^{3}}}\\\\ &{\\quad+\\cfrac{3(1+\\eta v_{\\operatorname*{max}})}{(1-\\gamma)^{2}}\\left[x_{t}^{r}\\left\\|Q_{r}^{\\pi_{w_{t}}}(s,a)-\\bar{Q}_{t}^{i}(s,a)\\right\\|_{2}+x_{t}^{c}\\left\\|Q_{c}^{\\pi_{w_{t}}}(s,a)-\\bar{Q}_{t}^{c}(s,a)\\right\\|_{2}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and if t \u2208Bscoofntf , we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{y_{t}^{r}\\left(V_{r}^{\\pi^{*}}(\\rho)-V_{r}^{\\pi_{w_{t}}}(\\rho)\\right)+y_{t}^{c}\\left(V_{c}^{\\pi^{*}}(\\rho)-V_{c}^{\\pi_{w_{t}}}(\\rho)\\right)}\\\\ &{\\leq\\cfrac{1}{\\eta}\\mathbb{E}_{s\\sim d^{*}}(D_{K L}(\\pi^{*}||\\pi_{w_{t}})-D_{K L}(\\pi^{*}||\\pi_{w_{t+1}}))+\\cfrac{2\\eta v_{\\operatorname*{max}}^{2}(y_{t}^{r}+y_{t}^{c})|S||A|}{(1-\\gamma)^{3}}}\\\\ &{\\quad+\\cfrac{3(1+\\eta v_{\\operatorname*{max}})}{(1-\\gamma)^{2}}\\left[x_{t}^{r}\\left\\|Q_{r}^{\\pi_{w_{t}}}(s,a)-\\bar{Q}_{t}^{i}(s,a)\\right\\|_{2}+x_{t}^{c}\\left\\|Q_{c}^{\\pi_{w_{t}}}(s,a)-\\bar{Q}_{t}^{c}(s,a)\\right\\|_{2}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Now we are ready to develop a key lemma that is associated with the expectation of the performance gap directly. The proof is provided in Appendix A.6.3 ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\eta\\displaystyle\\sum_{t\\in B_{r}}(V_{r}^{\\pi^{*}}(\\rho)-V_{r}^{\\pi_{w}}t(\\rho))+\\eta\\displaystyle\\sum_{t\\in B_{\\mathrm{sot}}^{\\infty}}x_{r}^{t}(V_{r}^{\\pi_{w}}(\\rho)-V_{r}^{\\pi^{*}}(\\rho))+\\eta\\displaystyle\\sum_{t\\in B_{\\mathrm{sot}}^{\\infty}}y_{r}^{t}(V_{r}^{\\pi_{w}}t(\\rho)-V_{r}^{\\pi^{*}}(\\rho))}\\\\ &{\\quad+\\,\\eta h^{+}|B_{\\mathrm{c}}|-\\eta h^{-}\\displaystyle\\sum_{t\\in B_{\\mathrm{sot}}^{\\infty}}x_{r}^{t}-\\eta h^{-}\\displaystyle\\sum_{t\\in B_{\\mathrm{sot}}^{\\infty}}y_{r}^{t}}\\\\ &{\\le\\mathbb{E}_{s\\sim d^{*}}D_{K L}(\\pi^{*}||\\pi_{w_{0}})+\\frac{2\\eta^{2}v_{\\mathrm{max}}^{2}|S||A|}{(1-\\gamma)^{3}}\\Big[(T-|B_{\\mathrm{sot}}^{\\mathrm{conf}}|)+\\displaystyle\\sum_{t\\in B_{\\mathrm{sot}}^{\\mathrm{conf}}}(y_{t}^{c}+y_{t}^{r})\\Big]+\\frac{3\\eta(1+\\eta v_{\\mathrm{max}})}{(1-\\gamma)^{2}}\\epsilon_{p}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "equation", "text": "$$\n\\leq\\mathbb{E}_{s\\sim d^{*}}D_{K L}(\\pi^{*}||\\pi_{w_{0}})+\\frac{4\\eta^{2}v_{\\operatorname*{max}}^{2}|S||A|T}{(1-\\gamma)^{3}}+\\frac{3\\eta(1+\\eta v_{\\operatorname*{max}})\\sqrt{|S||A|T}}{(1-\\gamma)^{1.5}},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\epsilon_{\\mathsf{p i}}:=\\displaystyle\\sum_{t\\in B_{r}}\\left\\|Q_{r}^{\\pi_{w}}-\\bar{Q}_{t}^{r}\\right\\|_{2}+\\displaystyle\\sum_{t\\in B_{c}}\\left\\|Q_{c}^{\\pi_{w}}-\\bar{Q}_{t}^{c}\\right\\|_{2}+}\\\\ &{\\qquad+\\displaystyle\\sum_{t\\in B_{\\mathrm{sot}}^{\\circ}}\\left(x_{t}^{r}\\|Q_{r}^{\\pi_{w}}-\\bar{Q}_{t}^{r}\\|_{2}+x_{t}^{c}\\|Q_{c}^{\\pi_{w}}-\\bar{Q}_{t}^{c}\\|_{2}\\right)}\\\\ &{\\qquad+\\displaystyle\\sum_{t\\in B_{\\mathrm{sot}}^{\\circ\\mathsf{r}}}\\left(y_{t}^{r}\\|Q_{r}^{\\pi_{w}}-\\bar{Q}_{t}^{r}\\|_{2}+y_{t}^{c}\\|Q_{c}^{\\pi_{w}}-\\bar{Q}_{t}^{c}\\|_{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Finally, we introduce the following lemma which indicates the number of iterations that optimize the reward objective is in the order of $T$ as long as $h^{+}$ and $h^{-}$ are chosen properly. The proof is provided in Appendix A.6.4 ", "page_idx": 15}, {"type": "text", "text": "Lemma A.6 (The frequency of optimizing reward objective). Consider any $0<\\delta<1$ and $h^{-}=0$ . Suppose ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac12\\eta h^{+}T\\geq\\mathbb{E}_{s\\sim d^{*}}D_{K L}(\\pi^{*}||\\pi_{w_{0}})+\\frac{4\\alpha^{2}v_{\\operatorname*{max}}^{2}|S||A|T}{(1-\\gamma)^{3}}+\\frac{3\\eta(1+\\eta v_{\\operatorname*{max}})\\sqrt{|S||A|T}}{(1-\\gamma)^{1.5}},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "then with probability at least $1-\\delta$ , the following fact holds ", "page_idx": 15}, {"type": "text", "text": "1. $B_{\\mathsf{r}}\\cup B_{\\mathsf{s o f t}}\\neq\\emptyset$ . ", "page_idx": 16}, {"type": "text", "text": "2. Either of the following claims holds: (a) $|B_{r}\\cup B_{\\mathsf{s o f t}}|\\ge T/2$ ; (b) The weighted performance gap is non-positive: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t\\in B_{r}}(V_{r}^{\\pi^{*}}(\\rho)-V_{r}^{\\pi_{w_{t}}}(\\rho))+\\sum_{t\\in B_{\\mathrm{soft}}^{n}}x_{r}^{t}(V_{r}^{\\pi^{*}}(\\rho)-V_{r}^{\\pi_{w_{t}}}(\\rho))}\\\\ &{\\quad+\\displaystyle\\sum_{t\\in B_{\\mathrm{soft}}^{\\mathrm{conf}}}y_{r}^{t}({V_{r}^{\\pi^{*}}(\\rho)}-V_{r}^{\\pi_{w_{t}}}(\\rho))\\leq0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "A.3 Proof of Theorem 4.1 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Now we are ready to provide the proof for Theorem 4.1. Recall the goal is to prove ", "page_idx": 16}, {"type": "equation", "text": "$$\nV_{r}^{\\pi^{\\star}}(\\rho)-\\mathbb{E}[V_{r}^{\\widehat{\\pi}}(\\rho)]\\leq\\widetilde{O}\\left(\\sqrt{\\frac{S A}{(1-\\gamma)^{3}T}}\\right),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the expectation is taken with respect to a weighted average over all $\\{\\pi_{w_{t}}\\}_{1\\le t\\le T}$ ", "page_idx": 16}, {"type": "text", "text": "We still consider the modes when the policy evaluation results are accurate such that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\epsilon_{\\sf p i}\\leq\\sqrt{(1-\\gamma)|S||A|T},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "which combined with Lemma A.5 yields ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\eta\\displaystyle\\sum_{t\\in B_{c}}(V_{r}^{\\pi^{*}}(\\rho)-V_{r}^{\\pi_{w t}}(\\rho))+\\eta\\displaystyle\\sum_{t\\in B_{\\mathrm{sot}}^{\\infty}}\\left[x_{r}^{t}(V_{r}^{\\pi^{*}}(\\rho)-V_{r}^{\\pi_{w t}}(\\rho))+x_{c}^{t}(V_{c}^{\\pi^{*}}(\\rho)-V_{c}^{\\pi_{w t}}(\\rho))\\right]}\\\\ &{\\quad+\\eta\\displaystyle\\sum_{t\\in B_{\\mathrm{sot}}^{\\infty}}\\left[y_{r}^{t}(V_{r}^{\\pi^{*}}(\\rho)-V_{r}^{\\pi_{w t}}(\\rho))+y_{c}^{t}(V_{c}^{\\pi^{*}}(\\rho)-V_{c}^{\\pi_{w t}}(\\rho))\\right]}\\\\ &{\\quad+\\eta h^{+}|B_{c}|-\\eta h^{-}\\displaystyle\\sum_{t\\in B_{\\mathrm{sot}}^{\\infty}}x_{r}^{t}-\\eta h^{-}\\displaystyle\\sum_{t\\in B_{\\mathrm{sot}}^{\\infty}}y_{r}^{t}}\\\\ &{\\quad\\le\\eta\\mathbb{E}_{s\\sim d^{*}}D_{\\mathrm{KL}}(\\pi^{*}||\\pi_{w_{0}})+\\frac{2\\eta^{2}v_{\\mathrm{max}}^{2}|S||A|T}{(1-\\gamma)^{3}}+\\frac{3\\eta(1+\\eta v_{\\mathrm{max}})\\sqrt{|S||A|T}}{(1-\\gamma)^{1.5}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The probability distribution associated with the expectation. Here, we let the weighs (probability distribution) to be proportion to ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\{\\begin{array}{l l}{1\\ \\ \\ }&{\\mathrm{if}\\ t\\in\\mathcal{B}_{\\mathrm{r}}}\\\\ {x_{t}^{r}\\ }&{\\mathrm{if}\\ t\\in\\mathcal{B}_{\\mathrm{soft}}^{\\mathrm{no}}}\\\\ {y_{t}^{r}\\ }&{\\mathrm{if}\\ t\\in\\mathcal{B}_{\\mathrm{soft}}^{\\mathrm{conf}}}\\\\ {0\\ \\ }&{\\mathrm{if}\\ t\\in\\mathcal{B}_{\\mathrm{c}},}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "which will be normalized by ", "page_idx": 16}, {"type": "equation", "text": "$$\nT_{\\mathrm{weighted}}^{r}=|\\mathcal{B}_{\\sf r}|+\\sum_{t\\in\\mathcal{B}_{\\mathrm{soft}}^{\\mathrm{no}}}x_{t}^{r}+\\sum_{t\\in\\mathcal{B}_{\\mathrm{soft}}^{\\mathrm{conf}}}y_{t}^{r}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "eann dw natsr othdeu csea amne  iams pthoert amnot dfae $y_{t}^{r}$ afnodr $y_{t}^{c}$ .e  rReewcaarlld  tahnatd  wcohset,n $t\\in B_{\\mathsf{s o f t}}^{\\mathsf{c o n f}}$ , nkt eies pcionngs ttrhue ctweedi gahsts $\\boldsymbol{x}_{t}^{r}$ $x_{t}^{c}$ $t\\in\\mathcal{B}_{\\mathrm{soft}}^{\\mathsf{n o}}$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\bf g}_{t}=x_{t}^{r}\\left({\\bf g}_{r}-\\frac{{\\bf g}_{r}\\cdot{\\bf g}_{c}}{\\|{\\bf g}_{c}\\|^{2}}{\\bf g}_{c}\\right)+x_{t}^{c}\\left({\\bf g}_{c}-\\frac{{\\bf g}_{c}\\cdot{\\bf g}_{r}}{\\|{\\bf g}_{r}\\|^{2}}{\\bf g}_{r}\\right)}\\ ~}\\\\ {{\\displaystyle~=x_{t}^{r}\\left(1+\\frac{\\cos\\theta_{r c}^{t}\\|{\\bf g}_{c}\\|}{\\|{\\bf g}_{r}\\|}\\right){\\bf g}_{r}+x_{t}^{c}\\left(1+\\frac{\\cos\\theta_{r c}^{t}\\|{\\bf g}_{r}\\|}{\\|{\\bf g}_{c}\\|}\\right){\\bf g}_{c}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "which indicates ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\forall t\\in B_{\\mathrm{soft}}^{\\mathrm{conf}}:\\quad y_{t}^{r}=x_{t}^{r}\\left(1+\\frac{\\cos\\theta_{r c}^{t}\\|\\mathbf{g}_{c}\\|}{\\|\\mathbf{g}_{r}\\|}\\right)\\geq x_{t}^{r}\\quad\\mathrm{and}\\quad y_{t}^{c}=x_{t}^{c}\\left(1+\\frac{\\cos\\theta_{r c}^{t}\\|\\mathbf{g}_{r}\\|}{\\|\\mathbf{g}_{c}\\|}\\right)\\geq x_{t}^{c},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "since $\\cos\\theta_{r c}^{t}\\ge0$ as $t\\in B_{\\mathsf{s o f t}}^{\\mathsf{c o n f}}$ . The above fact directly gives that letting $x_{t}^{r}\\geq1/2$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathrm{If}\\,\\lvert\\mathcal{B}_{\\mathsf{r}}\\cup\\mathcal{B}_{\\mathsf{s o f t}}\\rvert\\ge\\frac{T}{2}:T_{\\mathsf{w e i g h t e d}}^{r}\\ge\\frac{T}{4}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The reward objective. We first consider the performance gap w.r.t. the reward. Armed with above facts, we can see if (26) holds, with the weights in (30) then we directly have ", "page_idx": 17}, {"type": "equation", "text": "$$\nV_{r}^{\\pi^{\\star}}(\\rho)-\\mathbb{E}[V_{r}^{\\widehat{\\pi}}(\\rho)]\\leq0.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Otherwise, applying Lemma A.6 gives ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{T_{\\mathrm{weighted}}^{r}\\eta\\left(V_{r}^{\\pi^{*}}(\\rho)-\\mathbb{E}[V_{r}^{\\widehat{\\pi}}(\\rho)]\\right)}\\\\ &{\\!=\\eta\\displaystyle\\sum_{t\\in B_{r}}(V_{r}^{\\pi^{*}}(\\rho)-V_{r}^{\\pi_{w t}}(\\rho))+\\eta\\displaystyle\\sum_{t\\in B_{\\mathrm{sot}}^{\\infty}}x_{r}^{t}(V_{r}^{\\pi^{*}}(\\rho)-V_{r}^{\\pi_{w t}}(\\rho))}\\\\ &{\\quad\\:+\\eta\\displaystyle\\sum_{t\\in B_{\\mathrm{sot}}^{\\infty}}y_{r}^{t}(V_{r}^{\\pi^{*}}(\\rho)-V_{r}^{\\pi_{w t}}(\\rho))}\\\\ &{\\le\\mathbb{E}_{s\\sim d^{*}}D_{\\mathrm{KL}}(\\pi^{*}||\\pi_{w_{0}})+\\frac{2\\eta^{2}v_{\\mathrm{max}}^{2}|S||A|T}{(1-\\gamma)^{3}}+\\frac{3\\eta(1+\\eta v_{\\mathrm{max}})\\sqrt{|S||A|T}}{(1-\\gamma)^{1.5}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "which indicates ", "page_idx": 17}, {"type": "equation", "text": "$$\nV_{r}^{\\pi^{\\star}}(\\rho)-\\mathbb{E}[V_{r}^{\\widehat{\\pi}}(\\rho)]\\leq\\frac{2\\sqrt{|S||A|}}{(1-\\gamma)^{1.5}\\sqrt{T}}\\left(\\mathbb{E}_{s\\sim d^{*}}D_{\\mathrm{KL}}(\\pi^{*}||\\pi_{w_{0}})+4v_{\\operatorname*{max}}^{2}+6v_{\\operatorname*{max}}\\right).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Here, the last inequality hols by letting the learning rate $\\eta=(1-\\gamma)^{1.5}/\\sqrt{|\\cal{S}||\\cal{A}|{T}}$ . ", "page_idx": 17}, {"type": "text", "text": "Constraint violation. Now we move on to the cost objective. Taking the probability distribution of the expectation in (30) as well, we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[V_{\\alpha}^{n}(\\rho)]-b}\\\\ &{\\leq\\displaystyle\\sum_{r\\neq\\theta(n)}\\left(\\sum_{\\ell\\in\\mathcal{R}_{n}^{n}}\\nu\\xi^{m_{\\ell}}(\\rho)+\\sum_{\\ell\\in\\mathcal{R}_{n}^{n}}x_{\\ell}^{n}\\xi^{m_{\\ell}}(\\rho)+\\sum_{\\ell\\in\\mathcal{R}_{n}^{n}}y_{\\ell}^{n}\\xi^{m_{\\ell}}(\\rho)\\right)-b}\\\\ &{\\leq\\displaystyle\\sum_{r\\neq\\theta(n)}\\left(\\sum_{\\ell\\in\\mathcal{R}_{n}^{n}}\\left(\\sum_{\\ell^{\\prime}}\\nu\\xi^{m_{\\ell}}(\\rho)-b\\right)+\\sum_{\\ell\\in\\mathcal{R}_{n}^{n}}x_{\\ell}^{n}\\left(y_{\\ell^{\\prime}}^{n}(\\rho)-b\\right)+\\sum_{\\ell\\in\\mathcal{R}_{n}^{n}}y_{\\ell}^{n}\\left(y_{\\ell^{\\prime}}^{n}(\\rho)-b\\right)\\right)}\\\\ &{\\quad+\\displaystyle\\sum_{r\\neq\\theta(n)}\\left(\\sum_{\\ell\\in\\mathcal{R}_{n}^{n}}\\left(y_{\\ell^{\\prime}}^{n}(\\rho)-b\\right)^{r}\\xi^{m_{\\ell}}(\\rho)\\right)+\\displaystyle\\sum_{\\ell\\in\\mathcal{R}_{n}^{n}}x_{\\ell}^{n}\\left(y_{\\ell^{\\prime}}^{n}(\\rho)-b\\right)^{r}}\\\\ &{\\qquad\\qquad\\qquad+\\displaystyle\\sum_{r\\in\\mathcal{R}_{n}^{n}}y_{\\ell}^{n}\\left(y_{\\ell^{\\prime}}^{n}(\\rho)-b\\xi^{m_{\\ell}}(\\rho)\\right)+\\displaystyle\\sum_{\\ell\\in\\mathcal{R}_{n}^{n}}x_{\\ell}^{n}\\left(y_{\\ell^{\\prime}}^{n}(\\rho)-b\\xi^{m_{\\ell}}(\\rho)\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\displaystyle\\left\\{\\sum_{\\ell^{\\prime}}\\nu\\xi^{m_{\\ell}}(\\rho)-b\\xi^{m_{\\ell}}(\\rho)\\right\\}}\\\\ &{\\leq b^{\\mu}+\\displaystyle\\frac{1}{T_{\\alpha\\in\\mathcal{R}_{n}^{n}}}\\left(\\sum_{\\ell^{\\prime}}\\nu\\xi^{m_{\\ell}}(\\rho)-\\overline{{\\xi}}_{\\ell}^{\\prime}\\right)+\\displaystyle\\sum_{\\ell\\in\\mathcal{R}_{n}^{\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the last inequality holds by (34). Finally, also considering the mode when the policy evaluation error in (28), we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\left(\\sum_{t\\in B_{v}}\\left|Q_{c}^{\\pi_{w_{t}}}-\\overline{Q}_{t}^{c}\\right|+\\sum_{t\\in B_{\\mathrm{sot}}^{\\infty}}x_{r}^{t}\\left|Q_{c}^{\\pi_{w_{t}}}-\\overline{Q}_{t}^{c}\\right|+\\sum_{t\\in B_{\\mathrm{sot}}^{\\infty}}y_{r}^{t}\\left|Q_{c}^{\\pi_{w_{t}}}-\\overline{Q}_{t}^{c}\\right|\\right)\\leq\\epsilon_{\\mathfrak{p}\\mathfrak{t}}\\leq\\sqrt{(1-\\gamma)|S||A|T}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Then without loss of generality, taking the tolerance level $h^{-}=0$ and ", "page_idx": 18}, {"type": "equation", "text": "$$\nh^{+}=\\frac{2\\sqrt{|\\cal S||\\cal A|}}{(1-\\gamma)^{1.5}\\sqrt{T}}\\left(\\mathbb{E}_{s\\sim d^{*}}D_{\\mathrm{KL}}(\\pi^{*}||\\pi_{w_{0}})+4v_{\\operatorname*{max}}^{2}+6v_{\\operatorname*{max}}\\right)\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "complete the proof by showing ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}[V_{c}^{\\widehat{\\pi}}(\\rho)]-b\\leq h^{+}+\\frac{4}{T}\\left(\\sum_{t\\in B_{r}}\\left|Q_{c}^{\\pi_{w_{t}}}-\\overline{{Q}}_{t}^{c}\\right|+\\sum_{t\\in B_{\\mathrm{sot}}^{\\infty}}x_{r}^{t}\\left|Q_{c}^{\\pi_{w_{t}}}-\\overline{{Q}}_{t}^{c}\\right|+\\sum_{t\\in B_{\\mathrm{sot}}^{\\infty}}y_{r}^{t}\\left|Q_{c}^{\\pi_{w_{t}}}-\\overline{{Q}}_{t}^{c}\\right|\\right)\n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\n\\leq\\frac{2\\sqrt{|S||A|}}{(1-\\gamma)^{1.5}\\sqrt{T}}\\left(\\mathbb{E}_{s\\sim d^{*}}D_{\\mathrm{KL}}(\\pi^{*}||\\pi_{w_{0}})+4v_{\\operatorname*{max}}^{2}+6v_{\\operatorname*{max}}\\right)+\\frac{4\\sqrt{|S||A|}}{(1-\\gamma)^{1.5}\\sqrt{T}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "A.4 Proof of proposition 4.2 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We consider the ideal mode when the number of iterations of policy evaluation $T_{\\mathsf{p i}}\\to\\infty$ such that the ground truth cost function $V_{c}^{\\pi_{w_{t}}}=\\overline{{V}}_{t_{\\mathrm{in}}}^{c}$ . ", "page_idx": 18}, {"type": "text", "text": "First, we will focus on verifying the fact in (9a). Recall that there exists an iteration $t_{\\mathsf{i n}}<T$ such that $t_{\\mathsf{i n}}\\in B_{\\mathsf{r}}\\cup B_{\\mathsf{s o f t}}$ . So for the next step $t=t_{\\mathrm{in}}+1$ , we consider two different modes separately. ", "page_idx": 18}, {"type": "text", "text": "\u2022 When $t_{\\mathrm{in}}\\in B_{\\mathfrak{r}}$ . In this mode, we directly have ", "page_idx": 18}, {"type": "equation", "text": "$$\nV_{c}^{\\pi_{w_{t_{\\mathrm{in}}}}}(\\rho)=\\overline{{V}}_{t_{\\mathrm{in}}}^{c}\\leq b-h^{-}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Then we know that for the next step $t=t_{\\mathrm{in}}+1$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\nV_{c}^{\\pi_{w_{t}}}(\\rho)\\leq V_{c}^{\\pi_{w_{t_{\\mathrm{in}}}}}(\\rho)+\\eta\\|\\nabla_{w}V_{r}^{\\pi_{w_{t_{\\mathrm{in}}}}}(\\rho)\\|_{2}\\leq b-h^{-}+\\frac{2v_{\\operatorname*{max}}\\eta}{1-\\gamma}\\leq b+h^{+},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the penultimate inequality holds by the bound of the policy gradient established in [53, Lemma 5], and the last inequality holds by when the learning rate $\\eta$ is small enough such that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{2v_{\\operatorname*{max}}\\eta}{1-\\gamma}\\leq\\frac{2\\sqrt{|S||A|}}{(1-\\gamma)^{1.5}\\sqrt{T}}\\left(\\mathbb{E}_{s\\sim d^{*}}D_{\\mathrm{KL}}(\\pi^{*}||\\pi_{w_{0}})+4v_{\\operatorname*{max}}^{2}+6v_{\\operatorname*{max}}\\right)=h^{+}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The observation in (44) shows that the next time step $t=t_{\\mathsf{i n}}+1\\in B_{\\mathsf{r}}\\cup B_{\\mathsf{s o f t}}$ . ", "page_idx": 18}, {"type": "text", "text": "\u2022 When $t_{\\mathrm{in}}\\in\\mathcal{B}_{\\mathsf{s o f t}}$ . One has ", "page_idx": 18}, {"type": "equation", "text": "$$\nV_{c}^{\\pi_{w_{t_{\\mathrm{in}}}}}(\\rho)=\\overline{{V}}_{t_{\\mathrm{in}}}^{c}\\leq b+h^{+}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Then we can adaptively choose the weights for the reward and cost function $\\boldsymbol{x}_{t}^{c},\\boldsymbol{x}_{t}^{r}$ . Invoking Lemma A.3, we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\{x_{t}^{r}\\left(V_{r}^{\\pi_{w_{t}+1}}(\\rho)-V_{r}^{\\pi_{w_{t}}}(\\rho)\\right)+x_{t}^{c}\\left(V_{c}^{\\pi_{w_{t}}}(\\rho)-V_{c}^{\\pi_{w_{t+1}}}(\\rho)\\right)\\geq0\\quad\\mathrm{~if~}\\quad t\\in B_{\\mathrm{soft}}^{\\mathrm{no}}}\\\\ {y_{t}^{r}\\left(V_{r}^{\\pi_{w_{t+1}}}(\\rho)-V_{r}^{\\pi_{w_{t}}}(\\rho)\\right)+y_{t}^{c}\\left(V_{c}^{\\pi_{w_{t}}}(\\rho)-V_{c}^{\\pi_{w_{t+1}}}(\\rho)\\right)\\geq0\\quad\\mathrm{~if~}\\quad t\\in B_{\\mathrm{soft}}^{\\mathrm{conf}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Then observing that when $t=t_{\\mathsf{i n}}$ , in the mode with $x_{t}^{c}=1$ , we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{\\left(V_{c}^{\\pi_{w_{t_{\\mathrm{in}}}}}(\\rho)-V_{c}^{\\pi_{w_{t}}}(\\rho)\\right)\\geq0}&{\\mathrm{if}\\quad t\\in\\mathcal{B}_{\\mathrm{soft}}^{\\mathrm{no}}}\\\\ {\\left(V_{c}^{\\pi_{w_{t_{\\mathrm{in}}}}}(\\rho)-V_{c}^{\\pi_{w_{t}}}(\\rho)\\right)\\geq0}&{\\mathrm{if}\\quad t\\in\\mathcal{B}_{\\mathrm{soft}}^{\\mathrm{conf}},}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "which implies that ", "page_idx": 19}, {"type": "equation", "text": "$$\nV_{c}^{\\pi_{w_{t}}}(\\rho)\\leq V_{c}^{\\pi_{w_{t_{\\mathrm{in}}}}}(\\rho)\\leq b+h^{+}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "So we have the next time step $t=t_{\\mathsf{i n}}+1\\in B_{\\mathsf{r}}\\cup B_{\\mathsf{s o f t}}$ . ", "page_idx": 19}, {"type": "text", "text": "This implies that as long as $\\boldsymbol{x}_{t}^{c},\\boldsymbol{x}_{t}^{r}$ are chosen properly ensuring (48) holds, we can achieve $t=t_{\\mathsf{i n}}+1\\in B_{\\mathsf{r}}\\cup B_{\\mathsf{s o f t}}$ . ", "page_idx": 19}, {"type": "text", "text": "Summing up the two modes and applying them recursively, we complete the proof of (9a). ", "page_idx": 19}, {"type": "text", "text": "Finally, to verify (9b), we suppose ESPO and CRPO are initialized at the same point. Then observing that ESPO and CRPO execute the same update rule until the iteration $t_{\\mathsf{i n}}\\in B_{\\mathsf{r}}\\cup B_{\\mathsf{s o f t}}$ . Then applying (9a), we know that ", "page_idx": 19}, {"type": "equation", "text": "$$\n|B_{\\mathsf{r}}\\cup B_{\\mathsf{s o f t}}|=T-t_{\\mathsf{i n}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "While CRPO may has some iterations later such that falls into $B_{\\mathsf{c}}$ . So we have the number of iterations when CRPO update according to the reward objective $B_{\\mathsf{r}}^{\\mathsf{C R P O}}\\leq T-t_{\\mathsf{i n}}$ . We complete the proof. ", "page_idx": 19}, {"type": "text", "text": "A.5 Proof of proposition 4.3 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Recall the goal of the algorithm is to achieve ", "page_idx": 19}, {"type": "equation", "text": "$$\nV_{r}^{\\pi^{\\star}}(\\rho)-\\mathbb{E}[V_{r}^{\\widehat{\\pi}}(\\rho)]\\leq\\varepsilon_{1},\\;\\mathbb{E}[V_{c}^{\\widehat{\\pi}}(\\rho)]-V_{c}^{\\pi^{\\star}}(\\rho)\\leq\\varepsilon_{2}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "with as few samples as possible. ", "page_idx": 19}, {"type": "text", "text": "We start from considering $V_{r}^{\\pi^{\\star}}(\\rho)\\mathrm{~-~}\\mathbb{E}[V_{r}^{\\widehat{\\pi}}(\\rho)]\\,\\le\\,\\varepsilon_{1}$ . We observe that if (26) holds, taking the expectation w.r.t. the probability distribution in (30), we directly have ", "page_idx": 19}, {"type": "equation", "text": "$$\nV_{r}^{\\pi^{\\star}}(\\rho)-\\mathbb{E}[V_{r}^{\\widehat{\\pi}}(\\rho)]\\leq0\\leq\\varepsilon_{1}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Otherwise, applying Lemma A.6 and (22) gives ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V_{r}^{\\pi^{*}}(\\rho)-\\mathbb{E}[V_{r}^{\\pi}(\\rho)]}\\\\ &{=\\displaystyle\\sum_{t\\in B_{r}}(V_{r}^{\\pi^{*}}(\\rho)-V_{r}^{\\pi_{w t}}(\\rho))+\\sum_{t\\in B_{\\mathrm{sot}}^{m}}x_{r}^{t}(V_{r}^{\\pi^{*}}(\\rho)-V_{r}^{\\pi_{w t}}(\\rho))+\\sum_{t\\in B_{\\mathrm{sot}}^{\\infty}}y_{r}^{t}(V_{r}^{\\pi^{*}}(\\rho)-V_{r}^{\\pi_{w t}}(\\rho))}\\\\ &{\\le\\displaystyle\\frac{1}{\\eta}\\mathbb{E}_{s\\sim d^{*}}D_{\\mathrm{KL}}(\\pi^{*}||\\pi_{w_{0}})+\\frac{2\\eta v_{\\mathrm{max}}^{2}|S||A|T}{(1-\\gamma)^{3}}+\\frac{3(1+\\eta v_{\\mathrm{max}})}{(1-\\gamma)^{2}}\\epsilon_{\\mathrm{pi}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The first two terms are independent to the sample size. So we focus on control 3(1(1+\u2212\u03b7v\u03b3)m2ax)\u03f5 to meet the goal, namely, we need to achieve ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\epsilon_{\\mathsf{p i}}=\\displaystyle\\sum_{t\\in\\mathcal{B}_{r}}\\big\\|Q_{r}^{\\pi_{w_{t}}}-\\bar{Q}_{t}^{r}\\big\\|_{2}+\\displaystyle\\sum_{t\\in\\mathcal{B}_{\\mathrm{sot}}^{\\infty}}\\big(x_{t}^{r}\\|Q_{r}^{\\pi_{w_{t}}}-\\bar{Q}_{t}^{r}\\|_{2}+x_{t}^{c}\\|Q_{c}^{\\pi_{w_{t}}}-\\bar{Q}_{t}^{c}\\|_{2}\\big)}\\\\ &{\\qquad+\\displaystyle\\sum_{t\\in\\mathcal{B}_{\\mathrm{sot}}^{\\mathrm{conf}}}\\big(y_{t}^{r}\\|Q_{r}^{\\pi_{w_{t}}}-\\bar{Q}_{t}^{r}\\|_{2}+y_{t}^{c}\\|Q_{c}^{\\pi_{w_{t}}}-\\bar{Q}_{t}^{c}\\|_{2}\\big)\\leq\\varepsilon_{1}^{\\prime}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "for some $\\varepsilon_{1}^{\\prime}\\leq\\varepsilon_{1}$ ", "page_idx": 19}, {"type": "text", "text": "To continue, without loss of generality, we let $x_{t}^{r}\\,=\\,1,\\,x_{t}^{c}\\,=\\,0$ , and $|\\beta_{\\mathsf{r}}|\\,=\\,0$ (in this mode, the sampling approach is fixed), we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\epsilon_{\\mathrm{pi}}=\\displaystyle\\sum_{t\\in B_{\\mathrm{sot}}^{\\mathrm{po}}}\\left\\|Q_{r}^{\\pi_{w t}}-\\bar{Q}_{t}^{r}\\right\\|_{2}+\\displaystyle\\sum_{t\\in B_{\\mathrm{sot}}^{\\mathrm{conf}}}y_{t}^{r}\\|Q_{r}^{\\pi_{w t}}-\\bar{Q}_{t}^{r}\\|_{2}}\\\\ &{\\quad=\\displaystyle\\sum_{t\\in B_{\\mathrm{sot}}^{\\mathrm{po}}}\\left\\|Q_{r}^{\\pi_{w t}}-\\bar{Q}_{t}^{r}\\right\\|_{2}+\\displaystyle\\sum_{t\\in B_{\\mathrm{sot}}^{\\mathrm{conf}}}\\left(1+\\frac{\\cos\\theta_{r c}^{t}\\left\\|\\mathbf{g}_{c}\\right\\|}{\\left\\|\\mathbf{g}_{r}\\right\\|}\\right)\\|Q_{r}^{\\pi_{w t}}-\\bar{Q}_{t}^{r}\\|_{2}}\\\\ &{\\quad=\\displaystyle\\sum_{t\\in B_{\\mathrm{sot}}^{\\mathrm{po}}}\\delta_{t}+\\displaystyle\\sum_{t\\in B_{\\mathrm{sot}}^{\\mathrm{conf}}}\\left(1+\\frac{\\cos\\theta_{r c}^{t}\\left\\|\\mathbf{g}_{c}\\right\\|}{\\left\\|\\mathbf{g}_{r}\\right\\|}\\right)\\delta_{t}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where the penultimate inequality holds by the relation between $y_{t}^{r},x_{t}^{r}$ in (33), and the last inequality follows from denoting $\\left|\\left|Q_{r}^{\\bar{\\pi}_{w_{t}}}-\\bar{Q}_{t}^{r}\\right|\\right|_{2}=\\delta_{t}$ . ", "page_idx": 20}, {"type": "text", "text": "Now we are ready to show the advantages of using different batch size for different modes when $t\\,\\in\\,\\mathcal{B}_{\\mathrm{soft}}^{\\mathfrak{n o}}$ or $t\\,\\in\\,\\bar{B}_{\\mathrm{soft}}^{\\tt c o n f}$ . We make the following assumption about the relation between $\\delta_{t}$ and sample size (the number of iterations for the policy evaluation of Algorithm 1), which is qualitatively consistent with the policy evaluation bound in [53, Lemma 2]. ", "page_idx": 20}, {"type": "text", "text": "Assumption A.7. Suppose for any $t\\in\\mathcal{B}_{\\mathrm{soft}}$ , when the sample size varies around some basic size, the possible feasible $\\delta_{t}$ is in the range such that $\\delta_{t}=Y-\\bar{\\alpha^{}s_{t}^{\\mathsf{B}}}$ such that $Y$ is some small constant and $s_{t}^{\\mathsf{B}}$ is the sample size used for policy evaluation at $t$ -th iteration. ", "page_idx": 20}, {"type": "text", "text": "With the above assumption in hand, (55) can be written as ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\epsilon_{\\mathsf{p i}}=\\sum_{t\\in\\mathcal{B}_{\\mathsf{s o f t}}^{\\mathsf{n o}}}Y-\\alpha s_{t}^{\\mathsf{B}}+\\sum_{t\\in\\mathcal{B}_{\\mathsf{s o f t}}^{\\mathrm{conf}}}\\left(1+\\frac{\\cos\\theta_{r c}^{t}\\|\\mathbf{g}_{c}\\|}{\\|\\mathbf{g}_{r}\\|}\\right)(Y-\\alpha s_{t}^{\\mathsf{B}})=\\varepsilon_{1}^{\\prime}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "If there is no adaptive sampling, then we have $s_{t}^{\\mathsf{B}}=s_{t^{\\prime}}^{\\mathsf{B}}$ for any $t,t^{\\prime}\\in B_{\\mathsf{s o f t}}$ , which leads to the total number of samples as ", "page_idx": 20}, {"type": "equation", "text": "$$\nN_{\\mathrm{all}}=s_{\\mathrm{batch}}|B_{\\mathrm{soft}}|=\\frac{Y|B_{\\mathrm{soft}}|}{\\alpha}-\\frac{\\widetilde{\\varepsilon_{1}}|B_{\\mathrm{soft}}|}{\\alpha\\left(|B_{\\mathrm{soft}}^{\\mathrm{no}}|+\\sum_{t\\in B_{\\mathrm{soft}}^{\\mathrm{couf}}}\\left(1+\\frac{\\cos\\theta_{r c}^{t}\\|\\mathbf{g}_{c}\\|}{\\|\\mathbf{g}_{r}\\|}\\right)\\right)},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $s_{\\mathsf{b a t c h}}$ is the number of iterations in this mode. ", "page_idx": 20}, {"type": "text", "text": "Our proposed algorithm ESPO will increase the sample size when $t\\,\\in\\,{\\mathcal B}_{\\mathrm{soft}}^{\\mathsf{c o n f}}$ Bscoofntf and decrease the sample size when $t~\\in~\\mathcal{B}_{\\mathrm{soft}}^{\\mathrm{no}}$ . So as long as there exists at least one iteration $t^{\\star}\\,\\in\\,\\mathcal{B}_{\\mathsf{s o f t}}^{\\mathsf{c o n f}}$ with $\\begin{array}{r}{\\Big(1+\\frac{\\cos\\theta_{r c}^{t^{\\star}}\\|\\mathbf{g}_{c}\\|}{\\|\\mathbf{g}_{r}\\|}\\Big)>1}\\end{array}$ , we can increase the $s_{t^{\\star}}^{\\mathsf{B}}$ by $\\begin{array}{r}{s_{\\tt e x t r a}\\,<\\,s_{\\tt b a t c h}\\left(1+\\frac{\\cos\\theta_{r c}^{t^{\\star}}\\|{\\bf g}_{c}\\|}{\\|{\\bf g}_{r}\\|}\\right)}\\end{array}$ and decrease any $s_{t}^{\\mathsf{B}}$ by $\\begin{array}{r}{s_{\\tt e x t r a}\\cdot\\left(1+\\frac{\\cos\\theta_{r c}^{t^{\\star}}\\|{\\bf g}_{c}\\|}{\\|{\\bf g}_{r}\\|}\\right)}\\end{array}$ at time $t\\in\\mathcal{B}_{\\mathrm{soft}}^{\\mathsf{n o}}$ . Consequently, the total number of samples are smaller and (56) still holds. So we complete the proof. ", "page_idx": 20}, {"type": "text", "text": "A.6 Proof of auxiliary results ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "A.6.1 Proof of Lemma A.3 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "To begin with, note that the first two statements (15) and (16) has already been established in [53, Lemma 6]. So the remainder of the proof will focus on (17), which we recall here ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\int x_{t}^{r}\\left(V_{r}^{\\pi_{w t+1}}(\\rho)-V_{r}^{\\pi_{w t}}(\\rho)\\right)+x_{t}^{c}\\left(V_{c}^{\\pi_{w t+1}}(\\rho)-V_{c}^{\\pi_{w t}}(\\rho)\\right)\\geq x_{t}^{r}\\mathrm{d}\\mathsf{i f f}_{t}^{r}+x_{t}^{c}\\mathrm{d}\\mathsf{i f f}_{t}^{c}\\quad\\mathrm{if}\\quad t\\in B_{\\mathrm{sot}}^{\\mathrm{no}}}\\\\ &{\\Big\\{y_{t}^{r}\\left(V_{r}^{\\pi_{w t+1}}(\\rho)-V_{r}^{\\pi_{w t}}(\\rho)\\right)+y_{t}^{c}\\left(V_{c}^{\\pi_{w t+1}}(\\rho)-V_{c}^{\\pi_{w t}}(\\rho)\\right)\\geq y_{t}^{r}\\mathrm{d}\\mathsf{i f f}_{t}^{r}+y_{t}^{c}\\mathrm{d}\\mathsf{i f f}_{t}^{c}\\quad\\mathrm{~if~}\\quad t\\in B_{\\mathrm{sot}}^{\\mathrm{corf}}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Towards this, the left hand side of the first line can be written out as ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{x_{t}^{r}\\left(V_{r}^{\\pi_{e^{+1}}}(\\rho)-V_{r}^{\\pi_{e}}(\\rho)\\right)+x_{t}^{c}\\left(V_{c}^{\\pi_{e^{+1}}}(\\rho)-V_{c}^{\\pi_{e}}(\\rho)\\right)}\\\\ &{=x_{t}^{r}\\frac{1}{1-\\gamma}\\mathbb{E}_{s\\sim d_{r}}\\sum_{\\pi\\in A}\\pi_{\\pi+1}(a|s)A_{r}^{\\pi_{e}}(s,a)+x_{t}^{c}\\frac{1}{1-\\gamma}\\mathbb{E}_{s\\sim d_{r}}\\sum_{\\pi\\in A}\\pi_{\\pi+1}(a|s)A_{c}^{\\pi_{e}}(s,a)}\\\\ &{=\\frac{1}{1-\\gamma}\\mathbb{E}_{s\\sim d_{r}}\\sum_{\\pi\\in A}\\pi_{\\pi+1}(a|s)\\left(x_{t}^{r}Q_{r}^{\\pi_{e^{+1}}}(s,a)+x_{t}^{r}Q_{c}^{\\pi_{e^{+1}}}(s,a)\\right)}\\\\ &{\\quad-\\frac{1}{1-\\gamma}\\mathbb{E}_{s\\sim d_{r}}\\left(x_{t}^{r}V_{r}^{\\pi_{e^{+1}}}(s)+x_{t}^{c}V_{c}^{\\pi_{e}}(s)\\right)}\\\\ &{=\\frac{1}{1-\\gamma}\\mathbb{E}_{s\\sim d_{r}}\\sum_{\\pi\\in A}\\pi_{\\pi+1}(a|s)\\left(x_{t}^{r}\\overline{{Q}}_{t}^{r}(s,a)+x_{t}^{c}\\overline{{Q}}_{t}^{c}(s,a)\\right)}\\\\ &{\\quad-\\frac{1}{1-\\gamma}\\mathbb{E}_{s\\sim d_{r}}\\left(x_{t}^{r}V_{r}^{\\pi_{e^{+}}}(s)+x_{t}^{c}V_{c}^{\\pi_{e}}(s)\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad+\\displaystyle\\frac{1}{1-\\gamma}\\mathbb{E}_{s\\sim d_{\\theta}}\\sum_{\\alpha\\in A}\\pi_{w+1}(a|s)\\left[x_{t}^{r}\\left(Q_{r}^{\\pi_{w}}(s,a)-\\bar{Q}_{t}^{i}(s,a)\\right)+x_{t}^{c}\\left(Q_{c}^{\\pi_{w}}(s,a)-\\bar{Q}_{t}^{c}(s,a)\\right)\\right]}\\\\ &{\\stackrel{(i)}{=}\\displaystyle\\frac{1}{\\eta}\\mathbb{E}_{s\\sim d_{\\theta}}\\sum_{\\alpha\\in A}\\pi_{w+1}(a|s)\\log\\left(\\frac{\\pi_{w+1}(a|s)Z_{t}^{r,c_{1}}(s)}{\\pi_{w}(a|s)}\\right)-\\displaystyle\\frac{1}{1-\\gamma}\\mathbb{E}_{s\\sim d_{\\theta}}\\left(x_{t}^{r}V_{r}^{\\pi_{w}}(s)+x_{t}^{c}V_{c}^{\\pi_{w}}(s)\\right)}\\\\ &{\\quad+\\displaystyle\\frac{1}{1-\\gamma}\\mathbb{E}_{s\\sim d_{\\theta}}\\sum_{\\alpha\\in A}\\pi_{w+1}(a|s)\\left[x_{t}^{r}\\left(Q_{r}^{\\pi_{w}}(s,a)-\\bar{Q}_{t}^{i}(s,a)\\right)+x_{t}^{c}\\left(Q_{c}^{\\pi_{w}}(s,a)-\\bar{Q}_{t}^{c}(s,a)\\right)\\right]}\\\\ &{=\\displaystyle\\frac{1}{\\eta}\\mathbb{E}_{s\\sim d_{\\theta}}D_{\\mathbb{H}}(\\pi_{w+1}|\\pi_{w_{t}})+\\displaystyle\\frac{1}{\\eta}\\mathbb{E}_{s\\sim d_{\\theta}}\\log Z_{t}^{r,c_{1}}(s)-\\displaystyle\\frac{1}{1-\\gamma}\\mathbb{E}_{s\\sim d_{\\theta}}\\left(x_{t}^{r}V_{r}^{\\pi_{w}}(s)+x_{t}^{c}V_{c}^{\\pi_{w}}(s)\\right)}\\\\ &{\\quad+\\displaystyle\\frac{1}{1-\\gamma}\\mathbb{E}_{s\\sim d_{\\theta}}\\sum_{\\alpha\\in A}\\pi_{w+1}(a|s)\\left[x_{t}^{r}\\left(Q_{r}^{\\pi_{w}}(s,a)-\\bar{Q}_{t}^{i}(s,a)\\right)+x_{t}^{c}\\left(Q_{c}^{\\pi_{w}}(s,a)-\\bar{Q} \n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $(i)$ follows from the update rule in Lemma A.2. To continue, invoking the basic fact $D_{\\mathrm{KL}}(\\cdot\\,|\\,\\cdot)\\ge0$ , we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{x_{i}\\left(x_{j}^{\\top\\ast\\alpha_{i}}(p-x_{j}^{\\top\\ast\\alpha_{j}}(p)+x_{i}^{\\top}(x_{j}^{\\top\\ast\\alpha_{i}}(p)-x_{j}^{\\top\\ast\\alpha_{j}}(p))\\right)}\\\\ &{\\geq\\frac{1}{n!}\\frac{1}{n!}\\log\\left(\\log\\overline{{x_{i}^{\\alpha_{i}(1)}}}(t)-\\frac{1}{n!}\\frac{1}{n!}\\log\\overline{{x_{i}^{\\alpha_{i}(1)}}}(x_{i}^{\\top\\ast\\alpha_{i}}(p)+x_{i}^{\\top\\ast\\alpha_{i}}(p))\\right.}\\\\ &{\\left.\\ +\\frac{1}{n!}\\frac{1}{n!}\\frac{1}{n!}\\frac{1}{n!}\\alpha_{i}(u_{i})[x_{j}^{\\top}\\left\\lbrace\\phi^{\\top\\alpha_{i}}(u_{i})-\\bar{Q}(u_{i})]+x_{i}^{\\top}\\left\\lbrace\\phi^{\\top\\alpha_{i}}(u_{i})-\\bar{Q}(u_{i})\\right\\rbrace\\right.\\right.}\\\\ &{\\left.\\ \\ \\ -\\frac{1}{n!}\\frac{1}{n!}\\frac{1}{n!}\\sum_{i\\in\\mathcal{N}_{i}}\\sum_{k=1}^{n}w_{i}(\\mathrm{in})[\\phi_{i}^{\\top}]\\left(\\phi^{\\top\\alpha_{i}}(u_{i})-\\bar{Q}(u_{i})\\right)+x_{i}^{\\top}\\left\\lbrace\\phi^{\\top\\alpha_{i}}(u_{i})-\\bar{Q}(u_{i})\\right\\rbrace\\right.}\\\\ &{\\left.\\ \\ -\\frac{1}{n!}\\frac{1}{n!}\\frac{1}{n!}\\log\\overline{{x_{i}^{\\alpha_{i}(1)}}}(\\log\\overline{{u_{i}^{\\alpha_{i}(1)}}}(t))\\right\\rceil\\left\\lbrace\\phi^{\\top\\alpha_{i}(1)}-\\bar{Q}(u_{i})\\right\\rbrace+\\frac{1}{n!}\\left\\lbrace\\phi^{\\top\\alpha_{i}(1)}\\right\\rbrace\\right.}\\\\ &{\\left.\\ \\ \\ -\\frac{1}{n!}\\frac{1}{n!}\\log\\overline{{x_{i}^{\\alpha_{i}(1)}}}(\\log\\overline{{x_{i}^{\\alpha_{i}(1)}}}(t))\\right\\rbrace\\left\\lbrace\\phi^{\\top\\alpha_{i}(1)}-(\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where the penultimate inequality holds by the fact $\\|d_{\\rho}/\\rho\\|_{\\infty}\\geq1-\\gamma$ and the following claim which will be proved momentarily: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\log\\,Z_{t}^{r,c,1}(s)-\\displaystyle\\frac{\\eta}{1-\\gamma}\\mathbb{E}_{s\\sim d_{\\rho}}\\left(x_{t}^{r}V_{r}^{\\pi_{w_{t}}}(s)+x_{t}^{c}V_{c}^{\\pi_{w_{t}}}(s)\\right)}}\\\\ {{\\displaystyle\\quad+\\,\\displaystyle\\frac{\\eta}{1-\\gamma}\\sum_{a\\in A}\\pi_{w_{t}}(a|s)\\left[x_{t}^{r}\\left|Q_{r}^{\\pi_{w_{t}}}(s,a)-\\bar{Q}_{t}^{i}(s,a)\\right|+x_{t}^{c}\\left|Q_{c}^{\\pi_{w_{t}}}(s,a)-\\bar{Q}_{t}^{c}(s,a)\\right|\\right]\\geq0.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "So the rest of the proof is to verify (61). To do so, applying the definition of $Z_{t}^{r,c,1}$ in (13), we observe that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\log Z_{t}^{r,c,1}(s)-\\displaystyle\\frac{\\eta}{1-\\gamma}\\mathbb{E}_{s\\sim d_{\\rho}}\\left(x_{t}^{r}V_{r}^{\\pi_{t}}(s)+x_{t}^{c}V_{c}^{\\pi_{u_{t}}}(s)\\right)}\\\\ &{=\\log\\left(\\displaystyle\\sum_{a\\in\\mathcal{A}}\\pi_{w_{t}}(a|s)\\exp\\left(\\frac{\\eta\\left(x_{t}^{r}\\bar{Q}_{t}^{r}(s,a)+x_{t}^{c}\\bar{Q}_{t}^{c}(s,a)\\right)}{(1-\\gamma)}\\right)\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad-\\frac{\\eta}{1-\\gamma}\\mathbb{E}_{s\\sim d_{\\rho}}\\left(x_{t}^{r}V_{r}^{\\pi_{e u t}}(s)+x_{t}^{c}V_{c}^{\\pi_{e u t}}(s)\\right)}\\\\ &{\\geq\\displaystyle\\sum_{a\\in A}\\pi_{w_{t}}(a|s)\\frac{\\eta\\left(x_{t}^{r}\\bar{Q}_{t}^{r}(s,a)+x_{t}^{c}\\bar{Q}_{t}^{c}(s,a)\\right)}{(1-\\gamma)}-\\frac{\\eta}{1-\\gamma}\\mathbb{E}_{s\\sim d_{\\rho}}\\left(x_{t}^{r}V_{r}^{\\pi_{e u t}}(s)+x_{t}^{c}V_{c}^{\\pi_{e u t}}(s)\\right)}\\\\ &{=\\displaystyle\\sum_{a\\in A}\\pi_{w_{t}}(a|s)\\frac{\\eta}{1-\\gamma}\\left[x_{t}^{r}\\left(\\bar{Q}_{t}^{r}(s,a)-Q_{r}^{\\pi_{e u t}}(s,a)\\right)+x_{t}^{c}\\left(\\bar{Q}_{t}^{c}(s,a)-Q_{c}^{\\pi_{e u t}}(s,a)\\right)\\right]}\\\\ &{\\quad+\\displaystyle\\sum_{a\\in A}\\pi_{w_{t}}(a|s)\\frac{\\eta}{1-\\gamma}\\left(x_{t}^{r}Q_{r}^{\\pi_{e u t}}(s,a)+x_{t}^{c}Q_{c}^{\\pi_{e u t}}(s,a)\\right)-\\frac{\\eta}{1-\\gamma}\\mathbb{E}_{s\\sim d_{\\rho}}\\left(x_{t}^{r}V_{r}^{\\pi_{e u t}}(s)+x_{t}^{c}V_{c}^{\\pi_{e u t}}(s)\\right)}\\\\ &{=\\displaystyle\\sum_{a\\in A}\\pi_{w_{t}}(a|s)\\frac{\\eta}{1-\\gamma}\\left[x_{t}^{r}\\left(\\bar{Q}_{t}^{r}(s,a)-Q_{r}^{\\pi_{e u t}}(s,a)\\right)+x_{t}^{c}\\left(\\bar{Q}_{t}^{c}(s,a)-Q_{c}^{\\pi_{e u t}}(s,a)\\right)\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "which complete the proof of the first line of (17). The second line of (17) can be proved analogously. ", "page_idx": 22}, {"type": "text", "text": "A.6.2 Proof of Lemma A.4 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "First, the first two statements (65) and (68) have already been established in [53, Lemma 7]. So we focus on (17) throughout this subsection. ", "page_idx": 22}, {"type": "text", "text": "Consider the first line of (17), applying Lemma (A.1) and following the pipeline for (59) yields ", "page_idx": 22}, {"type": "text", "text": "$\\begin{array}{r l}{\\mathbb{E}_{\\lambda}^{\\{\\alpha,\\{c,v\\}\\{t|\\}}}\\approx\\frac{\\alpha}{\\eta}\\sum_{k=t}^{t}(\\lambda\\log|\\mathcal{E}_{\\lambda}|)-D_{k}(\\alpha|^{s}|^{p}|\\mathcal{E}_{\\lambda}|)}&{}\\\\ {+\\frac{1}{\\eta^{2}\\eta}\\sum_{k=t}^{t}\\bigg(\\|\\mathcal{E}_{\\lambda}\\|^{\\frac{p_{k}^{*}(s)+\\alpha}{p_{k}(s)+\\beta}}-\\frac{1}{\\eta}\\|\\mathcal{E}_{\\lambda}\\|_{L^{p_{k}^{*}(s)}}^{\\frac{p_{k}^{*}(s)+\\alpha}{p_{k}(s)+\\beta}}+\\chi\\|_{\\mathbb{F}_{\\lambda}}^{\\alpha^{*}\\{s;\\alpha}}(s)\\|\\bigg)}\\\\ {+\\frac{1}{\\eta^{2}\\eta}\\sum_{k=t}^{t}\\alpha_{k}(\\phi_{1}^{*}|^{p}|\\mathcal{E}_{\\lambda}|)^{\\frac{p_{k}^{*}(s)+\\alpha}{p_{k}(s)+\\beta}}-\\mathcal{E}_{\\lambda}(s)+\\frac{1}{\\eta^{2}\\eta^{2}\\eta^{2}}\\sum_{k=t}^{p_{k}(s)}-\\mathcal{E}_{\\lambda}(s)\\|_{\\frac{p_{k}^{*}(s)+\\alpha}{p_{k}(s)+\\beta}}}\\\\ {+\\frac{1}{\\eta^{2}\\eta}\\sum_{k=t}^{t}\\sum_{\\alpha=t}^{\\infty}\\sqrt{\\pi}^{\\alpha}(\\lambda\\phi_{1}^{*}|^{p}|\\mathcal{E}_{\\alpha}|^{\\alpha}-(\\lambda\\phi_{1}^{*})-\\mathcal{E}_{1}(s)+\\mathcal{E}_{1}^{\\alpha^{*}}(\\Delta^{*}\\|\\mathcal{E}_{\\alpha}|^{\\alpha}-\\mathcal{E}_{1}(s)))}\\\\ {\\frac{1}{\\eta^{2}\\eta}\\sum_{k=t}^{t}\\alpha_{k}(\\lambda\\log|\\mathcal{E}_{1}|)-D_{k}(\\alpha^{*}|^{p_{k}^{*}}|\\mathcal{E}_{1}|)}\\\\ {+\\frac{1}{\\eta^{2}\\eta}\\sum_{k=t}^{t}\\bigg(\\|\\mathcal{E}_{\\alpha}\\|_{L^{p_{k}^{*}(s)}}^{2}(t)- $ $\\begin{array}{r l}&{\\mathcal{E}\\left(\\left\\{V^{*}\\left(\\rho\\right)-V^{*}\\left(\\rho\\right)+\\gamma\\right\\}+\\sigma\\left(V^{*}\\left(\\rho\\right)-V^{*}\\left(\\rho\\right)\\right)\\right)}\\\\ &{=\\frac{1}{\\eta_{0}\\omega_{p}}\\gamma\\left((\\rho\\right)\\gamma\\right)_{p}-\\lambda\\sigma_{0}\\left(\\rho\\right)\\left(V^{*}\\left(\\rho\\right)\\gamma\\right)_{p+1}-\\frac{1}{\\eta_{0}\\omega_{p}^{2}}\\gamma\\sigma\\left(\\rho\\right)}\\\\ &{\\quad-\\frac{1}{\\eta_{0}\\omega_{p}}\\gamma\\sigma\\left(\\rho\\right)\\gamma^{*}\\sigma\\left(\\rho\\right)+\\gamma\\frac{\\gamma}{\\eta_{0}\\omega_{p}^{2}}\\gamma\\sigma\\left(\\rho\\right)}\\\\ &{\\quad+\\frac{1}{\\eta_{0}\\omega_{p}}\\gamma\\sigma_{0}\\left(\\rho\\right)\\gamma^{*}\\sigma\\left(\\rho\\right)+\\gamma\\frac{\\gamma}{\\eta_{0}\\omega_{p}}\\gamma\\sigma\\left(\\rho\\right)\\gamma+\\frac{\\gamma}{\\eta_{0}\\omega_{p}}\\gamma\\sigma\\left(\\rho^{2}\\gamma^{*}\\left(\\rho\\right),\\sigma\\right)}\\\\ &{\\quad+\\frac{1}{\\eta_{0}\\omega_{p}}\\gamma\\sigma\\left(\\rho\\right)\\gamma\\left(\\rho\\right)\\gamma^{*}\\left(\\rho\\right)\\gamma-\\frac{\\gamma}{\\eta_{0}\\omega_{p}}\\gamma\\sigma\\left(\\rho\\right)+\\gamma\\gamma^{*}\\sigma^{2}\\left(\\rho^{2}\\gamma^{*}\\left(\\rho\\right),\\sigma\\right)\\gamma\\right)}\\\\ &{\\quad\\$ ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\leq\\displaystyle\\frac{1}{\\eta}\\mathbb E_{s\\sim d^{\\star}}\\big(D_{\\mathrm{KL}}\\big(\\pi^{*}||\\pi_{w_{t}}\\big)-D_{\\mathrm{KL}}\\big(\\pi^{*}||\\pi_{w_{t+1}}\\big)\\big)+\\frac{2\\eta v_{\\operatorname*{max}}^{2}|S||\\mathcal{A}|}{(1-\\gamma)^{3}}}\\\\ &{\\quad+\\displaystyle\\frac{3(1+\\eta v_{\\operatorname*{max}})}{(1-\\gamma)^{2}}\\left[x_{t}^{r}\\left\\|Q_{r}^{\\pi_{w_{t}}}(s,a)-\\bar{Q}_{t}^{i}(s,a)\\right\\|_{2}+x_{t}^{c}\\left\\|Q_{c}^{\\pi_{w_{t}}}(s,a)-\\bar{Q}_{t}^{c}(s,a)\\right\\|_{2}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where (i) holds by applying Lemma A.3, the penultimate inequality holds by the Lipschitz property of $V_{r}^{\\pi_{w}}(\\rho)$ and $V_{c}^{\\pi_{w}}(\\rho)$ , and the last inequality can be verified following the last line in the proof of [53, Lemma 7]. ", "page_idx": 23}, {"type": "text", "text": "Similarly, we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{y_{t}^{r}\\left(V_{r}^{\\pi^{*}}(\\rho)-V_{r}^{\\pi_{w_{t}}}(\\rho)\\right)+y_{t}^{c}\\left(V_{c}^{\\pi^{*}}(\\rho)-V_{c}^{\\pi_{w_{t}}}(\\rho)\\right)}\\\\ &{\\leq\\cfrac{1}{\\eta}\\mathbb{E}_{s\\sim d^{*}}(D_{\\mathrm{KL}}(\\pi^{*}||\\pi_{w_{t}})-D_{\\mathrm{KL}}(\\pi^{*}||\\pi_{w_{t+1}}))+\\frac{2\\eta v_{\\operatorname*{max}}^{2}(y_{t}^{r}+y_{t}^{c})|S||A|}{(1-\\gamma)^{3}}}\\\\ &{\\quad+\\cfrac{3(1+\\eta v_{\\operatorname*{max}})}{(1-\\gamma)^{2}}\\left[x_{t}^{r}\\left\\|Q_{r}^{\\pi_{w_{t}}}(s,a)-\\bar{Q}_{t}^{i}(s,a)\\right\\|_{2}+x_{t}^{c}\\left\\|Q_{c}^{\\pi_{w_{t}}}(s,a)-\\bar{Q}_{t}^{c}(s,a)\\right\\|_{2}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "which complete the proof. ", "page_idx": 23}, {"type": "text", "text": "A.6.3 Proof of Lemma A.5 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Invoking Lemma (A.4) for the four modes when $t\\,\\in\\,\\mathcal{B}_{r},\\,t\\,\\in\\,\\mathcal{B}_{\\mathsf{s o f t}}^{\\mathsf{n o}},\\,t\\,\\in\\,\\mathcal{B}_{\\mathsf{s o f t}}^{\\mathsf{c o n f}}$ , and $t\\,\\in\\,B_{\\mathsf{c}}$ and summing up them together for $t=1,2,\\cdots,T$ yields ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\Big/\\sum_{t\\in B_{r}^{\\mathrm{sc}}}(V_{r}^{\\pi^{*}}(\\rho)-V_{r}^{\\pi_{w t}}(\\rho))+\\eta\\sum_{t\\in B_{\\mathrm{st}}^{\\mathrm{so}}}\\Big[x_{r}^{t}(V_{r}^{\\pi_{w t}}(\\rho)-V_{r}^{\\pi^{*}}(\\rho))+x_{c}^{t}(V_{c}^{\\pi^{*}}(\\rho)-V_{c}^{\\pi_{w t}}(\\rho))\\Big]}\\\\ &{\\displaystyle\\quad+\\,\\eta\\sum_{t\\in B_{\\mathrm{sot}}^{\\mathrm{sc}}}\\Big[y_{r}^{t}(V_{r}^{\\pi_{w t}}(\\rho)-V_{r}^{\\pi^{*}}(\\rho))+y_{c}^{t}(V_{c}^{\\pi^{*}}(\\rho)-V_{c}^{\\pi_{w t}}(\\rho))\\Big]+\\eta\\sum_{t\\in B_{c}}(V_{r}^{\\pi_{w t}}(\\rho)-V_{r}^{\\pi^{*}}(\\rho))}\\\\ &{\\displaystyle\\le\\mathbb{E}_{s\\sim d^{*}}D_{\\mathrm{KL}}(\\pi^{*}\\|\\pi_{w_{0}})+\\frac{2\\eta^{2}v_{\\mathrm{max}}^{2}\\|S\\|\\boldsymbol{A}\\|}{(1-\\gamma)^{3}}\\Big[(T-|B_{\\mathrm{sot}}^{\\mathrm{conf}}|)+\\sum_{t\\in B_{\\mathrm{st}}^{\\mathrm{cot}}}(y_{t}^{c}+y_{t}^{r})\\Big]+\\frac{3\\eta(1+\\eta v_{\\mathrm{max}})}{(1-\\gamma)^{2}}\\epsilon_{\\mathrm{pis}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $\\epsilon_{\\mathsf{p i}}$ is defined in (24). ", "page_idx": 23}, {"type": "text", "text": "Then we consider several different modes separately: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V_{c}^{\\pi_{w_{t}}}(\\rho)-V_{c}^{\\pi^{*}}(\\rho)}\\\\ &{=\\overline{{V}}_{t}^{c}(\\rho)-V_{c}^{\\pi^{*}}(\\rho)+V_{c}^{\\pi_{w_{t}}}(\\rho)-\\overline{{V}}_{t}^{c}\\geq h^{+}-|V_{c}^{\\pi_{w_{t}}}(\\rho)-\\overline{{V}}_{t}^{c}|\\geq h^{+}-\\|Q_{c}(\\pi_{w_{t}})-\\overline{{Q}}_{t}^{c}\\|_{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V_{c}^{\\pi_{w_{t}}}(\\rho)-V_{c}^{\\pi^{*}}(\\rho)}\\\\ &{=\\overline{{V}}_{t}^{(\\rho)}-V_{c}^{\\pi^{*}}(\\rho)+V_{c}^{\\pi_{w_{t}}}(\\rho)-\\overline{{V}}_{t}^{c}\\geq-h^{-}-|V_{c}^{\\pi_{w_{t}}}(\\rho)-\\overline{{V}}_{t}^{c}|\\geq-h^{-}-\\|Q_{c}(\\pi_{w_{t}})-\\overline{{Q}}_{t}^{c}\\|_{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Summing up the above two modes and plugging them back to (65) leads to ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\eta\\displaystyle\\sum_{t\\in B_{r}}(V_{r}^{\\pi^{*}}(\\rho)-V_{r}^{\\pi_{w t}}(\\rho))+\\eta\\displaystyle\\sum_{t\\in B_{\\mathrm{sot}}^{\\pi_{w t}}}x_{r}^{t}(V_{r}^{\\pi_{w t}}(\\rho)-V_{r}^{\\pi^{*}}(\\rho))+\\eta\\displaystyle\\sum_{t\\in B_{\\mathrm{sot}}^{\\mathrm{cot}}}y_{r}^{t}(V_{r}^{\\pi_{w t}}(\\rho)-V_{r}^{\\pi^{*}}(\\rho))}\\\\ &{\\quad+\\eta h^{+}|B_{c}|-\\eta h^{-}\\displaystyle\\sum_{t\\in B_{\\mathrm{sot}}^{\\pi_{w}}}x_{r}^{t}-\\eta h^{-}\\displaystyle\\sum_{t\\in B_{\\mathrm{sot}}^{\\mathrm{cot}}}y_{r}^{t}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "equation", "text": "$$\n\\leq\\mathbb{E}_{s\\sim d^{*}}D_{\\mathrm{KL}}(\\pi^{*}||\\pi_{w_{0}})+\\frac{2\\eta^{2}v_{\\mathrm{max}}^{2}|S||A|}{(1-\\gamma)^{3}}\\Big[(T-|B_{\\mathrm{sot}}^{\\mathrm{conf}}|)+\\sum_{t\\in B_{\\mathrm{sot}}^{\\mathrm{conf}}}(y_{t}^{c}+y_{t}^{r})\\Big]+\\frac{3\\eta(1+\\eta v_{\\mathrm{max}})}{(1-\\gamma)^{2}}\\epsilon_{\\mathrm{pt}}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "To continue, invoking [53, Lemma 2] leads to when the iterations of policy evaluation obey $T_{\\mathsf{p i}}=$ $\\begin{array}{r}{\\widetilde{O}\\big(\\frac{T\\log\\big(\\frac{|\\mathcal{S}||\\mathcal{A}|}{\\delta}\\big)}{(1-\\gamma)^{3}|\\mathcal{S}||\\mathcal{A}|}\\big)}\\end{array}$ $1-\\delta$ $1\\leq t\\leq T$ ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\|Q_{r}^{\\pi_{w_{t}}}-\\bar{Q}_{t}^{r}\\|_{2}\\leq\\frac{1}{2}\\sqrt{\\frac{(1-\\gamma)|S||A|}{T}}}&\\\\ &{\\mathrm{~and~}}&{\\|Q_{c}^{\\pi_{w_{t}}}-\\bar{Q}_{t}^{c}\\|_{2}\\leq\\frac{1}{2}\\sqrt{\\frac{(1-\\gamma)|S||A|}{T}}\\leq\\sqrt{\\frac{(1-\\gamma)|S||A|}{T}}.}&\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Combining this fact with the definition in (24) directly leads to ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bar{\\mathfrak{z}}_{\\mathfrak{p}}\\mathrm{i}=\\displaystyle\\sum_{t\\in B_{r}}\\left\\|Q_{r}^{\\pi_{w}}-\\bar{Q}_{t}^{r}\\right\\|_{2}+\\displaystyle\\sum_{t\\in B_{c}}\\left\\|Q_{c}^{\\pi_{w}}-\\bar{Q}_{t}^{c}\\right\\|_{2}+\\displaystyle\\sum_{t\\in B_{\\mathrm{sot}}^{r}}\\left(x_{t}^{r}\\|Q_{r}^{\\pi_{w}}-\\bar{Q}_{t}^{r}\\|_{2}+x_{t}^{c}\\|Q_{c}^{\\pi_{w}}-\\bar{Q}_{t}^{c}\\|_{2}\\right)}\\\\ &{\\qquad+\\displaystyle\\sum_{t\\in B_{\\mathrm{sot}}^{c o n t}}\\left(y_{t}^{r}\\|Q_{r}^{\\pi_{w}}-\\bar{Q}_{t}^{r}\\|_{2}+y_{t}^{c}\\|Q_{c}^{\\pi_{w}}-\\bar{Q}_{t}^{c}\\|_{2}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad(70)}\\\\ &{\\leq\\sqrt{(1-\\gamma)|S||A|T}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Plugging (70) back into (68) complete the proof: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\eta\\displaystyle\\sum_{t\\in B_{r}}(V_{r}^{\\pi^{*}}(\\rho)-V_{r}^{\\pi_{v}}(\\rho))+\\eta\\displaystyle\\sum_{t\\in B_{\\mathrm{sot}}^{m}}x_{r}^{t}(V_{r}^{\\pi_{v}}(\\rho)-V_{r}^{\\pi^{*}}(\\rho))+\\eta\\displaystyle\\sum_{t\\in B_{\\mathrm{sot}}^{m}}y_{r}^{t}(V_{r}^{\\pi_{v}}(\\rho)-V_{r}^{\\pi^{*}}(\\rho))}\\\\ &{\\quad+\\,\\eta h^{+}|{B_{\\mathrm{c}}}|-\\eta h^{-}\\displaystyle\\sum_{t\\in B_{\\mathrm{sot}}^{m}}x_{r}^{t}-\\eta h^{-}\\displaystyle\\sum_{t\\in B_{\\mathrm{sot}}^{m}}y_{r}^{t}}\\\\ &{\\le\\mathbb{E}_{s\\sim d^{*}}D_{\\mathrm{KL}}(\\pi^{*}||\\pi_{w_{0}})+\\frac{2\\eta^{2}v_{\\mathrm{max}}^{2}|S||A|}{(1-\\gamma)^{3}}\\Big[(T-|B_{\\mathrm{sot}}^{\\mathrm{conf}}|)+\\displaystyle\\sum_{t\\in B_{\\mathrm{sot}}^{c o n f}}(y_{t}^{c}+y_{t}^{r})\\Big]+\\frac{3\\eta(1+\\eta v_{\\mathrm{max}})}{(1-\\gamma)^{2}}\\epsilon_{\\mathrm{p}}}\\\\ &{\\le\\mathbb{E}_{s\\sim d^{*}}D_{\\mathrm{KL}}(\\pi^{*}||\\pi_{w_{0}})+\\frac{4\\eta^{2}v_{\\mathrm{max}}^{2}|S||A|T}{(1-\\gamma)^{3}}+\\frac{3\\eta(1+\\eta v_{\\mathrm{max}})\\sqrt{|S||A|T}}{(1-\\gamma)^{1.5}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "since $(y_{t}^{c}+y_{t}^{r})\\leq2$ . ", "page_idx": 24}, {"type": "text", "text": "A.6.4 Proof of Lemma A.6 ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "The first claim is easily verified since if $B_{\\mathsf{r}}\\cup B_{\\mathsf{s o f t}}=\\emptyset$ , then $|\\beta_{\\mathsf{c}}|=T$ . Applying Lemma A.5 gives ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\eta h^{+}|\\mathcal{B}_{\\mathrm{c}}|=\\eta h^{+}T\\leq\\mathbb{E}_{s\\sim d^{*}}D_{\\mathrm{KL}}(\\pi^{*}||\\pi_{w_{0}})+\\frac{4\\eta^{2}v_{\\operatorname*{max}}^{2}|\\mathcal{S}||A|T}{(1-\\gamma)^{3}}+\\frac{3\\eta(1+\\eta v_{\\operatorname*{max}})\\sqrt{|\\mathcal{S}||A|T}}{(1-\\gamma)^{1.5}},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "which contradict with the assumption (25). So we have $B_{\\mathsf{r}}\\cup B_{\\mathsf{s o f t}}\\neq\\emptyset$ . ", "page_idx": 24}, {"type": "text", "text": "Then the rest of the proof focus on the second claim. Towards this, if ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\sum_{t\\in B_{r}}(V_{r}^{\\pi^{*}}(\\rho)-V_{r}^{\\pi_{w t}}(\\rho))+\\sum_{t\\in B_{\\mathrm{sot}}^{\\mathrm{po}}}x_{r}^{t}(V_{r}^{\\pi^{*}}(\\rho)-V_{r}^{\\pi_{w t}}(\\rho))+\\sum_{t\\in B_{\\mathrm{sot}}^{\\mathrm{corf}}}y_{r}^{t}(V_{r}^{\\pi^{*}}(\\rho)-V_{r}^{\\pi_{w t}}(\\rho))\\leq0,\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "then the condition (b) holds. Otherwise, applying Lemma A.5 yields ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\eta h^{+}|\\mathcal{B}_{\\mathrm{c}}|-\\eta h^{-}\\displaystyle\\sum_{t\\in B_{\\mathrm{sot}}^{n=}}x_{r}^{t}-\\eta h^{-}\\displaystyle\\sum_{t\\in B_{\\mathrm{sot}}^{\\mathrm{cont}}}y_{r}^{t}}\\\\ &{\\leq\\mathbb{E}_{s\\sim d^{*}}D_{\\mathrm{KL}}(\\pi^{*}||\\pi_{w_{0}})+\\displaystyle\\frac{4\\eta^{2}v_{\\mathrm{max}}^{2}|S||A|T}{(1-\\gamma)^{3}}+\\displaystyle\\frac{3\\eta(1+\\eta v_{\\mathrm{max}})\\sqrt{|S||A|T}}{(1-\\gamma)^{1.5}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Then if $|B_{\\mathsf{r}}\\cup B_{\\mathsf{s o f t}}|<T/2$ , we have $\\begin{array}{r}{|B_{\\mathsf{c}}|\\geq\\frac{T}{2}}\\end{array}$ and thus ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\eta h^{+}T}{2}-\\eta h^{-}T\\leq\\eta h^{+}|\\mathcal{B}_{\\mathrm{c}}|-2(T-|\\mathcal{B}_{\\mathrm{c}}|)\\eta h^{-}\\leq\\eta h^{+}|\\mathcal{B}_{\\mathrm{c}}|-\\eta h^{-}\\displaystyle\\sum_{t\\in B_{\\mathrm{sot}}^{\\mathrm{po}}}1-\\eta h^{-}\\displaystyle\\sum_{t\\in B_{\\mathrm{sot}}^{\\mathrm{po}}}2(T-|\\mathcal{B}_{\\mathrm{sot}}|)\\eta h^{+}}\\\\ &{\\leq\\eta h^{+}|\\mathcal{B}_{\\mathrm{c}}|-\\eta h^{-}\\displaystyle\\sum_{t\\in B_{\\mathrm{sot}}^{\\mathrm{po}}}x_{r}^{t}-\\eta h^{-}\\displaystyle\\sum_{t\\in B_{\\mathrm{sot}}^{\\mathrm{cont}}}y_{r}^{t}}\\\\ &{\\leq\\mathbb{E}_{s\\sim d^{*}}D_{\\mathrm{KL}}(\\pi^{*}||\\pi_{w_{0}})+\\frac{4\\eta^{2}v_{\\mathrm{max}}^{2}|S||A|T}{(1-\\gamma)^{3}}+\\frac{3\\eta(1+\\eta v_{\\mathrm{max}})\\sqrt{|S||A|T}}{(1-\\gamma)^{1.5}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "which yields ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\frac{\\eta h^{+}T}{2}\\leq\\mathbb{E}_{s\\sim d^{*}}D_{\\mathrm{KL}}(\\pi^{*}||\\pi_{w_{0}})+\\frac{4\\eta^{2}v_{\\operatorname*{max}}^{2}|S||A|T}{(1-\\gamma)^{3}}+\\frac{3\\eta(1+\\eta v_{\\operatorname*{max}})\\sqrt{|S||A|T}}{(1-\\gamma)^{1.5}}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "that is contradict with the assumption (25). ", "page_idx": 25}, {"type": "text", "text": "B Practical Algorithm ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "1: Inputs: initial policy with parameters $\\pi_{w_{0}}$ , positive slack value $h_{t}^{+}\\in[0,+\\infty)$ , negative slack   \nvalue $h_{t}^{-}\\,\\in\\,(-\\infty,0]$ , the cost value as $V_{c_{t}}^{\\pi_{w_{0}}}(\\rho)$ at step $t$ , the cost limit as $b$ , positive sample   \npenalty $\\zeta^{+}\\in[0,+\\infty)$ , negative sample penalty $\\zeta^{-}\\in(-1,0]$ , gradient angles $\\theta_{r,c}$ , sample size   \n$X$ .   \n2: for $t=0,\\dots,T-1$ do   \n3: if $h^{+}$ iteratively decreases then   \n4: $h_{t}^{+}\\gets h_{t}^{+}-\\dot{h}_{t}^{+}/T$   \n5: end if   \n6: if $h_{t}^{-}$ iteratively increases then   \n7: $\\dot{h_{t}^{-}}\\gets h_{t}^{-}-h_{t}^{-}/T$   \n8: end if   \n9: if $\\zeta_{t}^{+}$ iteratively decreases then   \n10: $\\bar{\\zeta_{t}^{+}}\\gets\\zeta_{t}^{+}-\\zeta_{t}^{+}/T$   \n11: end if   \n12: if $\\zeta_{t}^{-}$ iteratively increases then   \n13: $\\zeta_{t}^{-}\\gets\\zeta_{t}^{-}-\\zeta_{t}^{-}/T$   \n14: end if   \n15: if $V_{c_{t}}^{\\pi_{w_{t}}}(\\rho)>(h_{t}^{+}+b)$ then   \n16: Adjust sample size $X_{t}$ with Equation (7).   \n17: Update policy $\\pi_{w_{t}}$ to ensure safety with Equation (2).   \n18: else if $\\left(h_{t}^{-}+b\\right)\\leq\\dot{V}_{c_{t}}^{\\pi_{w_{t}}}(\\rho)\\leq\\left(h_{t}^{+}+b\\right)$ then   \n19: if For gradients ${\\bf g}_{r}$ and $\\mathbf{g}_{c}$ , $\\theta_{r,c}\\leq90^{\\circ}$ then   \n20: Adjust sample size $X_{t}$ with Equation (7).   \n21: Update the policy $\\pi_{w_{t}}$ with Equation (3).   \n22: else   \n23: Adjust sample size $X_{t}$ with Equation (6).   \n24: Update the policy $\\pi_{w_{t}}$ with Equation (4).   \n25: end if   \n26: else if $V_{c_{t}}^{\\pi_{w_{t}}}(\\rho)<(h_{t}^{-}+b)$ then   \n27: Adjust sample size $X_{t}$ with Equation (7).   \n28: Update policy $\\pi_{w_{t}}$ to maximize reward $V_{r,t}^{\\pi_{w t}}(\\rho)$ with Equation (5).   \n29: end if   \n30: Policy evaluation under $\\pi_{w_{t}}$ involves estimating the values of rewards and constraints.   \n31: Sample pairs $(s_{j},a_{j})$ from the buffer $B_{t}$ according to the distribution $\\rho\\cdot\\pi_{w_{t}}$ and compute the   \nestimation $V_{r,t}^{\\pi_{w t}}(\\rho)$ and $V_{c_{t}}^{\\pi_{w t}}(\\rho)$ , where $s_{j}$ represents the state and $a_{j}$ represents the action, $j$   \nis is the index for the sampled pairs.   \n32: end for   \n33: Outputs: $\\pi_{w_{t}}$ . ", "page_idx": 26}, {"type": "image", "img_path": "oPFjhl6DpR/tmp/42e042d1ca8955ad4c3d9eb7edde1d333637c50415ffb4a2bbbd54f3b0920236.jpg", "img_caption": ["Figure 4: Ablation experiments: Experiments of different cost limits and sample sizes. "], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "C Ablation Experiments ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "To further evaluate the effectiveness of our method, we conduct a series of ablation experiments regarding different cost limits, different sample sizes, learning rates, gradient weights, and update style analysis. These ablation experiments are instrumental in providing a deeper insight into our method, shedding light on its strengths and potential areas for improvement. Through this rigorous evaluation, we aim to substantiate the adaptability of our method, ensuring its applicability and effectiveness in a wide range of safe RL scenarios. ", "page_idx": 27}, {"type": "text", "text": "Different Cost Limits: As depicted in Figures 4(a)-(c), we evaluate our method on the SafetyWalker2d- $\\cdot\\nu4$ tasks under different cost limits, maintaining identical sample manipulation settings. Our method exhibits similar reward performance at cost limits of 30 and 40. This similarity in performance is attributed to our method\u2019s capacity to dynamically adjust the sample size, a critical factor in optimizing for reward maximization while ensuring safety. Moreover, the training time for the task with a cost limit of 30 is 63 minutes, slightly longer than the 58 minutes required for the limit of 40. This observation can be explained by the increased challenge and larger confilct between reward and safety presented at the lower constraint limit of 30, necessitating a more significant number of samples for effective optimization. Notably, our method can ensure safety across these various constraint-limited tasks and outperforms CRPO in reward performance and training efficiency. ", "page_idx": 27}, {"type": "text", "text": "Different Sample Sizes: As illustrated in Figures 4(d)-(f), we conduct an assessment of our method on the SafetyWalker2d- $\\cdot\\nu4$ tasks, exploring different sample sizes while keeping the cost limit settings constant. In these experiments, we compare the outcomes of using sample sizes set at $1.2X$ and $0.5X$ against $1.0X$ and $0.5X$ . Notably, both settings successfully ensured safety. On the one hand, the reward performance achieved with a sample size of $1.2X$ and $0.5X$ surpasses that of $1.0X$ and $0.5X$ , indicating the effectiveness of larger sample size in enhancing performance; on the other hand, the training time for the sample size of $1.2X$ and $0.5X$ is recorded at 67 minutes, which is longer than the 58 minutes required for the sample size of $1.0X$ and $0.5X$ . Despite this increased training time, it remains less than the 71 minutes recorded for CRPO. These results underscore the potential benefits of utilizing more samples to improve performance in safe RL tasks. Importantly, in both sample manipulation settings, our method ensures safety and outperforms CRPO in terms of reward performance and training efficiency. ", "page_idx": 27}, {"type": "text", "text": "Different Gradient Weights: $\\boldsymbol{x}_{t}^{r}$ and $x_{t}^{c}$ represent the weight of the reward gradient (resp. the safety cost gradient) in the final gradient $w_{t+1}$ . So $x_{t}^{r}+x_{t}^{c}=1$ all the time and, for instance, $\\boldsymbol x_{t}^{r}=1$ (resp. $x_{t}^{c}=1\\,$ ) indicates we only use reward gradient (resp. safety cost gradient), and $x_{t}^{r}=x_{t}^{c}=0.5$ denotes reward and safety cost objectives are considered equally important in the overall gradient. In general, $\\boldsymbol{x}_{t}^{r}$ and $\\boldsymbol x_{t}^{c}$ are hyperparameters in the framework that we can either pre-set as a fixed value or adaptively adjust during the running process as needed. For instance, we can set $\\boldsymbol{x}_{t}^{r}$ to be larger if we care more about the reward performance; otherwise, we set $\\boldsymbol x_{t}^{c}$ to be larger to enhance safety. Throughout our experiments, we just set $x_{t}^{r}=x_{t}^{c}=0.5$ for simplicity, which also showed superior performance than prior arts (see Section 5). We provide an ablation study for the hyperparameters to evaluate its effect on the performance, shown in Figures 5(a)-(c). To test the sensitivity of the performance w.r.t. the hyperparameters $\\boldsymbol{x}_{t}^{r}$ and $\\boldsymbol x_{t}^{c}$ , we carry out experiments using other two combinations of $\\boldsymbol{x}_{t}^{r}$ and $\\boldsymbol x_{t}^{c}$ $\\textstyle x_{t}^{r}=0.4$ , $x_{t}^{c}=0.6$ or $x_{t}^{r}=0.6,x_{t}^{c}=0.4)$ . The results show that our proposed ESPO performs well using such different $\\boldsymbol{x}_{t}^{r},\\boldsymbol{x}_{t}^{c}$ settings and is even better than using $x_{t}^{r}=x_{t}^{c}=0.5$ sometimes. ", "page_idx": 27}, {"type": "image", "img_path": "oPFjhl6DpR/tmp/2d2148b12c265be619018aa6ceeeb3b0337eb280f45e93868a8afe8eaf939c7e.jpg", "img_caption": ["Figure 5: Ablation experiments: Experiments of different gradient weights $x^{r}$ and $x^{c}$ and different learning rates. "], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "Different Learning Rates: We conduct ablation studies on hyperparameters \u2014 learning rates $\\left(l_{r}\\right)$ , shown in Figure Figures 5(d)-(f), reveal that ESPO is robust to variations in learning rates. The results demonstrate that ESPO performs well under reasonably varying learning rates. ", "page_idx": 28}, {"type": "text", "text": "Update Style Analysis: The analysis of update style in our experiments, as illustrated in Table 3 for the SafetyHumanoidStandup- $\\cdot\\nu4$ task, offers insightful contrasts between our algorithm, ESPO, and CRPO method. In these experiments, we observe the following update patterns: 1) CRPO\u2019s Update Style: CRPO\u2019s approach to optimization involved 178 updates focused solely on reward optimization and 322 updates dedicated to cost optimization. This distribution suggests a significant emphasis on cost optimization, indicating that CRPO struggles to manage safety constraints. 2) ESPO\u2019s Update Style: ESPO, on the other hand, showed a more dynamic update pattern. It conducted 298 updates focused on reward optimization, indicating a more efficient approach toward maximizing rewards. However, unlike CRPO, ESPO engages 199 updates characterized by simultaneous optimization of both reward and cost. Additionally, 3 updates focused exclusively on optimizing cost. By optimizing both aspects simultaneously, ESPO demonstrates a novel method of navigating the complex landscape of safe RL, which may contribute to its overall efficiency and effectiveness as observed in the task performance. ", "page_idx": 28}, {"type": "table", "img_path": "oPFjhl6DpR/tmp/aa26ecf51f1acba37d0daedb892fca1ce2c06bc7b402d87061d3ddd9c9ce7b2f.jpg", "table_caption": [], "table_footnote": [], "page_idx": 28}, {"type": "text", "text": "Table 3: Update style analysis. The Reward update represents the number of times the algorithm updates its policy primarily focusing on maximizing rewards, the Cost update refers to the number of cost updates where the safety violation happens and the primary focus is on minimizing costs, the Reward & Cost update corresponds to the number of times the optimization of reward and cost updates are executed simultaneously. ", "page_idx": 28}, {"type": "text", "text": "D Detailed Experiments ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "D.1 Additional Experiments ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "The results of our experimental evaluations on the SafetyHumanoidStandup-v4 task, as depicted in Figures 6(a)-(c), show the superior performance of our algorithm, ESPO, in comparison with SOTA primal baselines, CRPO and PCRPO. Key observations from these results include: ESPO demonstrates a remarkable ability to outperform CRPO and achieve comparable performance with PCRPO in reward while ensuring safety. Another notable aspect of ESPO\u2019s performance is that our method required less time to reach convergence than these baselines. This efficiency is crucial in practical applications where time and computational resources are often limited. ESPO requires only approximately $76.5\\%$ and $74.01\\%$ of the training time that CRPO and PCRPO need, respectively, to achieve superior performance. Specifically, as depicted in Table 1, while CRPO and PCRPO utilize 8 million samples for the SafetyHumanoidStandup-v4 task, our method requires only 5.1 million samples for the same task. This reduction in samples is a significant advantage, highlighting ESPO\u2019s effectiveness in learning efficiency. ", "page_idx": 29}, {"type": "text", "text": "These results from the SafetyHumanoidStandup- $\\cdot\\nu4$ task further demonstrate the effectiveness of our method in safe RL environments, showcasing its potential as a reliable and efficient solution for optimizing rewards while adhering to safety constraints. ", "page_idx": 29}, {"type": "image", "img_path": "oPFjhl6DpR/tmp/b7b807234eb5a36a6a532a3633d227726e9b05bf53f688ee5a416268ffd1b1d7.jpg", "img_caption": ["Figure 6: Performance comparisons of safe RL methods on SafetyHumanoidStandup- $\\cdot\\nu4$ tasks. "], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "D.2 Experiment Settings ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "The Safety-MuJoCo benchmark is primarily used for primal-based methods, while the Omnisafe benchmark is mainly utilized for primal-dual based methods. Moreover, the Safety-MuJoCo benchmark is different from the Omnisafe benchmark in safety settings. Safety-MuJoCo encompasses broad safety constraints including both velocity limits and overall robot health. Accounting for multiple factors requires algorithms to consider both speed regulation and broader integrity. In contrast, the Omnisafe benchmark primarily focuses on robot velocity as the critical constraint. For instance, a cost of 1 is emitted whenever the robot\u2019s velocity exceeds a predefined limit. This singular focus on velocity provides a more targeted, yet still challenging, evaluation context. Through these experimental setups, we aim to comprehensively assess the effectiveness of our method in varying scenarios, ranging from the multifaceted safety constraints in Safety-MuJoCo to the velocity-centric constraints in Omnisafe. For more details, see [30] and [32]. To ensure a fair evaluation of our method\u2019s effectiveness, we conducted all experiments using at least three different random seeds. ", "page_idx": 29}, {"type": "text", "text": "The key parameters used in the tasks of Safety-MuJoCo benchmarks are provided in Table 4, Table 5 and Table 6. Note, to encourage more learning exploration, we initiate the optimization of safety after 40 epochs. Experiments in the tasks of Safety-MuJoCo benchmarks are conducted on a Ubuntu 20.04.3 LTS system, with an AMD Ryzen-7-2700X CPU and an NVIDIA GeForce RTX 2060 GPU. ", "page_idx": 29}, {"type": "text", "text": "The key parameters used on the tasks of Omnisafe benchmarks are provided in Table 5, Table 6, and Table 7. Experiments on the tasks of Omnisafe benchmarks are conducted on a Ubuntu 20.04.6 LTS system, with 2 AMD EPYC-7763 CPUs and 6 NVIDIA RTX A6000 GPUs. ", "page_idx": 29}, {"type": "table", "img_path": "oPFjhl6DpR/tmp/a5a0cba335c8e4b53ea8ffbd304d0834c545f14fe25603143dd06684a872db9b.jpg", "table_caption": [], "table_footnote": [], "page_idx": 30}, {"type": "text", "text": "Table 4: Key parameters used in Safety-MuJoCo benchmarks. In ESPO, the sample size of each epoch is determined by Algorithm 1, with Equations (7) and (6), in which the $X$ is 16000. ", "page_idx": 30}, {"type": "table", "img_path": "oPFjhl6DpR/tmp/0085eef35ea1298bc7ee19ff590f1d88b2078377d591412cf4fe6a7e35b51a3b.jpg", "table_caption": [], "table_footnote": [], "page_idx": 30}, {"type": "table", "img_path": "oPFjhl6DpR/tmp/52385ae73c072e28c9f0fe75ec38de6c2a05824c4571219cfabd38d8add524a8.jpg", "table_caption": ["Table 5: Sample parameters used in Omnisafe and Safety-MuJoCo experiments. The results of SafetyHopperVelocity- $\\nu l$ and SafetyAntVelocity- $\\nu l$ are shown in Figure 3, the results of SafetyHumanoidStandup- $\\cdot\\nu4$ and SafetyWalker2d- $\\cdot\\nu4$ are shown in Figure 2, the results of SafetyWalker2d-v4- $^a$ are shown in Figures 4 (a), (b) and (c), the results of SafetyWalker2d-v4-a and SafetyWalker2d-v4- $^{b}$ are shown in Figures 4 (d), (e) and (f); the results of SafetyReacher-v4 experiments are shown in Figure 2. "], "table_footnote": [], "page_idx": 30}, {"type": "text", "text": "Table 6: Cost limit and slack parameters used in Omnisafe and Safety-MuJoCo experiments. The results of SafetyHopperVelocity- $\\nu l$ and SafetyAntVelocity- $\\nu l$ are shown in Figure 3, the results of SafetyHumanoidStandup- $\\cdot\\nu4$ and SafetyWalker2d- $\\cdot\\nu4$ are shown in Figure 2, the results of SafetyWalker2d-v4-a and SafetyWalker2d-v4- $^{a}$ are shown in Figures 4 (a), (b) and (c), the results of SafetyWalker2d-v4- $^{b}$ are shown in Figures 4 (d), (e) and (f); the results of SafetyReacher- $\\nu4$ experiments are shown in Figure 2. ", "page_idx": 30}, {"type": "table", "img_path": "oPFjhl6DpR/tmp/cab442fef82606f36fea099768022ddf79f46be698139bbe6df0fea0fa14fce0.jpg", "table_caption": [], "table_footnote": ["Table 7: Key hyparameters used in Omnisafe experiments. In ESPO, the steps of each epoch is determined by Algorithm 1, with Equations (7) and (6), in which the $X$ is 20000. The parameters for the baselines are consistent with those of Omnisafe, and their performance is meticulously fine-tuned in Omnisafe [32]. "], "page_idx": 31}, {"type": "text", "text": "E Impact and Limitation Statements ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Impact Statements: This paper presents work aiming to advance the field of safe RL, and we believe the study can significantly benefti multi-objective optimization efficiency, such as optimizations with ten or even hundreds of objectives. Additionally, this paper shares the general societal impact of progress in this domain, including both potential positive applications and negative consequences such as misuse. As capabilities in safe RL advance toward real-world deployment, continued monitoring and assessment of broader impacts remain warranted, as well as ensuring ethical deployment. ", "page_idx": 32}, {"type": "text", "text": "Limitation Statements: The study currently focuses on simulation experiments; however, in the future, we aim to deploy our method in real-world applications. Additionally, although it is necessary to set sample size parameters for policy optimization, our method demonstrates superior performance across multiple tasks compared to SOTA baselines. Notably, our method\u2019s performance won\u2019t be heavily sensitive to the hyperparameters from our observations, as observed in our analysis (refer to Section 5.3). It is important to acknowledge that there is no free launch, and an algorithm could not address everything5. ", "page_idx": 32}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: See Abstract and Introduction. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 33}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Justification: See Appendix E. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 33}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: See Section 4.4 and Appendix A. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 34}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: See Experiment Section 5, and Appendix D.2. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 34}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Justification: See Experiment Section 5. Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 35}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: See Appendix D.2. Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 35}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Justification: See Experiment Section 5. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 35}, {"type": "text", "text": "", "page_idx": 36}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: See Appendix D.2. Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 36}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: The research conducted with the NeurIPS Code of Ethics. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 36}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Justification: See Appendix E. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to ", "page_idx": 36}, {"type": "text", "text": "generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. ", "page_idx": 37}, {"type": "text", "text": "\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 37}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: The research does not pose such risks. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 37}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: The paper already cited related packages that we used. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 37}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: See Experiment Section 5. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 38}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 38}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 38}]