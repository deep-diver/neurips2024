[{"figure_path": "oPFjhl6DpR/figures/figures_1_1.jpg", "caption": "Figure 1: Oscillation analysis compared our method with existing safe RL methods in three modes of optimization.", "description": "This figure illustrates the optimization trajectories of ESPO and existing safe RL methods across three optimization modes: reward-only, reward-cost balance, and cost-only.  The different colored regions represent the dominance of each objective. ESPO's trajectory (red dashed line) is smoother and more efficient, avoiding the oscillations seen in the existing methods (purple dashed line) which frequently cross boundaries between the optimization modes due to conflicts between safety and reward gradients. This highlights ESPO's ability to dynamically adjust sample size based on gradient conflicts, leading to improved sample efficiency and stable optimization.", "section": "1 Introduction"}, {"figure_path": "oPFjhl6DpR/figures/figures_6_1.jpg", "caption": "Figure 2: Compare our algorithm (ESPO) with PCRPO [30] and CRPO [53] on the Safety-MuJoCo benchmark. Our algorithm consistently and remarkably outperforms the SOTA baseline across multiple performance metrics, including reward maximization, safety assurance, and learning efficiency.", "description": "This figure compares the performance of the proposed ESPO algorithm with two state-of-the-art (SOTA) baselines, PCRPO and CRPO, across three different tasks within the Safety-MuJoCo benchmark.  The results are presented in multiple subplots showing average episode reward, average episode cost, and training time (in minutes) for each algorithm on each task.  ESPO consistently outperforms both PCRPO and CRPO in terms of reward maximization, maintaining safety (cost constraints), and showing significantly improved sample efficiency (fewer samples and faster training time).", "section": "5.1 Experiments of Comparison with Primal-Based Methods"}, {"figure_path": "oPFjhl6DpR/figures/figures_7_1.jpg", "caption": "Figure 2: Compare our algorithm (ESPO) with PCRPO [30] and CRPO [53] on the Safety-MuJoCo benchmark. Our algorithm consistently and remarkably outperforms the SOTA baseline across multiple performance metrics, including reward maximization, safety assurance, and learning efficiency.", "description": "This figure compares the performance of ESPO with two state-of-the-art primal-based safe RL algorithms (PCRPO and CRPO) across three different tasks from the Safety-MuJoCo benchmark.  The plots show the average episode reward, average episode cost, and training time (in minutes) over the course of training, demonstrating that ESPO consistently achieves higher rewards while satisfying safety constraints, using significantly fewer samples and less training time.", "section": "5.1 Experiments of Comparison with Primal-Based Methods"}, {"figure_path": "oPFjhl6DpR/figures/figures_27_1.jpg", "caption": "Figure 2: Compare our algorithm (ESPO) with PCRPO [30] and CRPO [53] on the Safety-MuJoCo benchmark. Our algorithm consistently and remarkably outperforms the SOTA baseline across multiple performance metrics, including reward maximization, safety assurance, and learning efficiency.", "description": "This figure compares the performance of the proposed algorithm, ESPO, against two state-of-the-art (SOTA) baselines, PCRPO and CRPO, on three different tasks from the Safety-MuJoCo benchmark.  The results demonstrate ESPO's superiority across multiple metrics. Subfigures (a) and (d) show the average episode reward, showcasing ESPO's superior reward maximization capabilities.  Subfigures (b) and (e) illustrate the average episode cost, emphasizing ESPO's robust safety assurance. Finally, subfigures (c) and (f) display the training time, highlighting ESPO's improved learning efficiency. In summary, this figure provides strong empirical evidence of ESPO's effectiveness in terms of reward performance, safety guarantees, and sample efficiency.", "section": "5.1 Experiments of Comparison with Primal-Based Methods"}, {"figure_path": "oPFjhl6DpR/figures/figures_28_1.jpg", "caption": "Figure 2: Compare our algorithm (ESPO) with PCRPO [30] and CRPO [53] on the Safety-MuJoCo benchmark. Our algorithm consistently and remarkably outperforms the SOTA baseline across multiple performance metrics, including reward maximization, safety assurance, and learning efficiency.", "description": "This figure presents a comparison of the proposed ESPO algorithm against two state-of-the-art (SOTA) primal-based safe reinforcement learning algorithms, PCRPO and CRPO, on three tasks from the Safety-MuJoCo benchmark.  The results demonstrate that ESPO consistently outperforms the baselines in terms of average episode reward, average episode cost (i.e., safety), and training time.  The improvement is significant across all three metrics, showcasing ESPO's efficiency gains and enhanced safety capabilities.", "section": "5.1 Experiments of Comparison with Primal-Based Methods"}, {"figure_path": "oPFjhl6DpR/figures/figures_29_1.jpg", "caption": "Figure 6: Performance comparisons of safe RL methods on SafetyHumanoidStandup-v4 tasks.", "description": "This figure compares the performance of ESPO with CRPO and PCRPO on the SafetyHumanoidStandup-v4 task, illustrating the average episode reward, average episode cost, and training time for each algorithm.  It demonstrates that ESPO achieves a comparable reward to PCRPO while significantly outperforming CRPO in both reward and training time efficiency.", "section": "5.1 Experiments of Comparison with Primal-Based Methods"}]