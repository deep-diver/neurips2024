[{"type": "text", "text": "Autoregressive Image Diffusion: Generation of Image Sequence and Application in MRI ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Guanxiong Luo University Medical Center G\u00f6ttingen guanxiong.luo@med.uni-goettingen.de ", "page_idx": 0}, {"type": "text", "text": "Shoujin Huang Shenzhen Technology University ", "page_idx": 0}, {"type": "text", "text": "Martin Uecker Graz University of Technology uecker@tugraz.at ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Magnetic resonance imaging (MRI) is a widely used non-invasive imaging modality. However, a persistent challenge lies in balancing image quality with imaging speed. This trade-off is primarily constrained by $\\mathbf{k}\\cdot$ -space measurements, which traverse specific trajectories in the spatial Fourier domain ${}^{\\mathrm{k}}$ -space). These measurements are often undersampled to shorten acquisition times, resulting in image artifacts and compromised quality. Generative models learn image distributions and can be used to reconstruct high-quality images from undersampled $\\mathbf{k}$ -space data. In this work, we present the autoregressive image diffusion (AID) model for image sequences and use it to sample the posterior for accelerated MRI reconstruction. The algorithm incorporates both undersampled $\\mathbf{k}\\cdot$ -space and pre-existing information. Models trained with fastMRI dataset are evaluated comprehensively. The results show that the AID model can robustly generate sequentially coherent image sequences. In MRI applications, the AID can outperform the standard diffusion model and reduce hallucinations, due to the learned inter-image dependencies. The project code is available at https://github.com/mrirecon/aid. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Magnetic resonance imaging (MRI) is a non-invasive imaging modality widely used in clinical practice to visualize soft tissue. Despite its utility, a persistent challenge in MRI is the trade-off between image quality and imaging speed. The trade-off is influenced by the $\\boldsymbol{\\mathrm{k}}$ -space (spatial Fourier domain) measurements, which traverse spatial frequency data points along given sampling trajectories. To reduce acquisition time, the $\\mathbf{k}\\cdot$ -space measurements are often undersampled, resulting in image artifacts and reduced image quality. ", "page_idx": 0}, {"type": "text", "text": "In recent years, deep learning-based methods have emerged to improve image reconstruction in MRI. These methods are formulated as an inverse problem building upon compressed sensing techniques [1, 2] and benefit from the learned prior information instead of hand-crafted priors [3\u20135]. Another successful approach involves learning an image prior parameterized by a generative neural network [6, 7], which is then used as the learned and decoupled regularization on the image. Generative priors offer flexibility in handling changes in the forward model and perform well in reconstructing high-quality images from undersampled data. ", "page_idx": 0}, {"type": "text", "text": "Diffusion models [8\u201310], a class of generative models, have gained attention in recent years and are making an impact in many fields, including MRI reconstruction [11, 12]. These models learn to reverse a diffusion process that transforms random noise into structured images, producing highquality, detailed images. Various approaches, including denoising diffusion probabilistic models (DDPMs) [10], denoising score matching [9], and continuous formulations based on stochastic differential equations (SDEs) [13], have been proposed for deriving diffusion models. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Recent studies demonstrate the effectiveness of diffusion models in accelerated MRI and their flexibility in handling various sampling patterns [11, 14\u201317]. For example, training score-based generative models using Langevin dynamics yields competitive reconstruction results for both indistribution and out-of-distribution data [11]. Additionally, score-based diffusion models trained solely on magnitude images can reconstruct complex-valued data [15]. Comprehensive approaches using data-driven Markov chains facilitate efficient MRI reconstruction across variable sampling schemes and enable the generation of uncertainty maps [16]. ", "page_idx": 1}, {"type": "text", "text": "Autoregressive models are statistical models that predict the current value of a variable based on its past values, capturing temporal dependencies and patterns within the data. They are widely used in various fields such as time series analysis, signal processing, and sequence modeling. In natural language processing, autoregressive models like generative pre-trained transform (GPT) [18, 19] predict each token in a sequence based on previously generated tokens, enabling the generation of coherent and contextually relevant text. Similarly, in image modeling, autoregressive models like PixelCNN [20] and ImageGPT [21] generate images by predicting each pixel value based on previously generated pixel values, often in a left-to-right, top-to-bottom order. Instead of directly modeling pixels, which can be computationally expensive for high-resolution images, the study [22] proposes to first compress the image into a smaller representation using vector quantized variational autoencoder (VQVAE). This VQVAE learns a codebook of visually meaningful image components. Then, a transformer is applied to model the autoregressive relationship between these components, effectively capturing the global structure of the image. By predicting each image component based on previous ones, the model generates high-resolution images in a sequential manner, maintaining consistency and coherence across the entire image. ", "page_idx": 1}, {"type": "text", "text": "The clinical practice of MRI often involves acquiring volumetric image sequences to monitor disease progression and treatment response; modeling and generating these image sequences is challenging. Autoregressive models can be employed to model the joint distribution of image sequences and extract the dependencies between images. The diffusion process is effective in modelling images by treating each image independently. Therefore, we aim to combine these two models and propose autoregressive image diffusion (AID) model to generate sequences of images. ", "page_idx": 1}, {"type": "text", "text": "The contributions of this work are the following aspects. We present how to derive the autoregressive image diffusion training loss starting from a common diffusion loss and how to optimize loss in parallel for efficient training. We present the algorithm to sample the posterior for accelerated MRI reconstruction when using AID to facilitate the incorporation of pre-existing information. We performed experiments to evaluate its ability in generating images when different the amount of initial information is given and to validate its effectiveness in MRI reconstruction. The results show that the AID model can stably generate highly coherent image sequences even without any pre-existing information. When used as a generative prior in MRI reconstruction, the AID outperforms the standard diffusion model and reduces the hallucinations in the reconstructed images, beneftiing from the learned prior knowledge about the relationship between images and pre-existing information. ", "page_idx": 1}, {"type": "text", "text": "2 Methods ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2.1 Autoregressive image diffusion ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Given a dataset $X$ consisting of multiple sequences of images, each sequence represented as $\\mathbf{x}=$ $\\{x_{1},x_{2},\\dots,x_{N}\\}$ , our goal is to model the joint distribution of these images. This joint distribution is autoregressively factorized into the product of conditional probabilities: ", "page_idx": 1}, {"type": "equation", "text": "$$\np(\\mathbf{x})=q(x_{1}|x_{0})\\prod_{t=2}^{N}q(x_{n}|x_{<n}),\n$$", "text_format": "latex", "page_idx": 1}, {"type": "image", "img_path": "jIh4W7r0rn/tmp/487a9b3aa3a372e093c0d9d46b48d3ef5c7026ddc560a75103e475cda42e339d.jpg", "img_caption": ["Figure 1: The interaction between the images in conditioning sequence occurs in the DiTBlock, which has a causal attention module to ensure $x_{n}$ is conditioned on previous images $x_{<n}$ . During training, the net predicts the noise for each noisy image that is sampled from the target sequence given the conditioning sequence in parallel. During generation, the net iteratively refines the noisy input to produce a clean image, which is then appended to the conditioning sequence. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "where $x_{<n}\\,=\\,\\{x_{1},x_{2},.\\dots,x_{n-1}\\}$ and the image $x_{0}$ is known. The model parameterized by $\\theta$ is trained by minimizing the negative log-likelihood of the data: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{L}_{A I D}=\\mathbb{E}_{X}\\left[-\\log p_{\\theta}(\\mathbf{x})\\right]=\\mathbb{E}_{X}\\left[-\\log p_{\\theta}(x_{1}|x_{0})-\\sum_{t=2}^{N}\\log p_{\\theta}(x_{n}|x_{<n})\\right].\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Sohl-Dickstein et al. (2015) and $\\mathrm{Ho}$ et al. (2020) introduced the denoising diffusion probabilistic model (DDPM). This model gradually introduces fixed Gaussian noise to an observed data point $x^{0}$ using known scales $\\beta_{t}$ , generating a series of progressively noisier values $x^{1},x^{2},\\ldots,x^{T}$ . The final noisy output $x^{T}$ follows a Gaussian distribution with zero and identity covariance matrix $I$ , containing no information about the original data point. The series of positive noise scales $\\beta_{1},\\ldots,\\beta_{T}$ must be increasing, ensuring that the first noisy output $x^{1}$ closely resembles the original data $x^{0}$ , while the final value $x^{T}$ represents pure noise. We apply this process to the conditional probability $q(x_{n}|x_{<n})$ in Equation (2) by adding the noise to the image independent of the position in the sequence, i.e., $x_{n}^{t}$ and $\\dot{x}_{<n}^{0}$ are conditionally independent given $\\bar{x_{n}^{t-1}}$ . Then the transition from $x_{n}^{t-1}$ to $\\bar{x_{n}^{t}}$ is defined as: ", "page_idx": 2}, {"type": "equation", "text": "$$\nq(x_{n}^{t}|x_{n}^{t-1},x_{<n}^{0})=q(x_{n}^{t}|x_{n}^{t-1})=\\mathcal{N}(x_{n}^{t};\\sqrt{1-\\beta_{t}}x_{n}^{t-1},\\beta_{t}\\mathbf{I})\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Here, $x_{n}^{t}$ represents the image $x_{n}$ at time $t$ , $x_{n}^{t-1}$ is the image at the previous time step, and $\\boldsymbol{x}_{<n}^{0}$ denotes all images preceding $x_{n}$ at the initial time step. The parameter $\\beta_{t}$ controls the drift and diffusion of this process. The objective is to learn to reverse this process. The reverse process is defined as: ", "page_idx": 2}, {"type": "equation", "text": "$$\np_{\\theta}(x_{n}^{t-1}|x_{n}^{t},x_{<n}^{0})=\\mathcal{N}(x_{n}^{t-1};\\mu_{\\theta}(x_{n}^{t},x_{<n}^{0},t),\\Sigma_{\\theta}(x_{n}^{t},x_{<n}^{0},t)),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\mu_{\\theta}$ and $\\Sigma_{\\theta}$ are parameterized by a neural network $\\theta$ , taking $x_{n}^{t}$ , $\\boldsymbol{x}_{<n}^{0}$ , and $t$ as inputs. Using the variational lower bound, the reverse process can be learned by minimizing the negative log-likelihood of the data: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbb{E}[-\\log p_{\\theta}(x_{n}|x_{<n}^{0})]\\leq\\mathbb{E}\\left[-\\log p(x_{n}^{T})-\\sum_{t\\geq1}\\log\\frac{p_{\\theta}(x_{n}^{t-1}|x_{n}^{t},x_{<n}^{0})}{q(x_{n}^{t}|x_{n}^{t-1},x_{<n}^{0})}\\right]:=L_{D_{n}},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Given the initial image $x_{n}^{0}$ and that $x_{n}^{t}$ and $\\boldsymbol{x}_{<n}^{0}$ are conditionally independent given $x_{n}^{0},x_{n}^{t}$ at an arbitrary time step $t$ is sampled from a Gaussian distribution: ", "page_idx": 3}, {"type": "equation", "text": "$$\nq(x_{n}^{t}|x_{n}^{0},x_{<n}^{0})=\\mathcal{N}(x_{n}^{t};\\sqrt{\\bar{\\alpha}_{t}}x_{n}^{0},(1-\\bar{\\alpha}_{t})\\mathbf{I}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "using $\\alpha_{t}\\,=\\,1\\,-\\,\\beta_{t}$ and $\\begin{array}{r}{\\bar{\\alpha}_{t}=\\prod_{s=1}^{t}\\alpha_{s}}\\end{array}$ . The posterior distribution $x_{n}^{t-1}$ given $x_{n}^{0}$ and $x_{n}^{t}$ is then calculated as: ", "page_idx": 3}, {"type": "equation", "text": "$$\nq(x_{n}^{t-1}|x_{n}^{t},x_{n}^{0},x_{<n}^{0})=\\mathcal{N}(x_{n}^{t-1};\\tilde{\\mu}_{t}(x_{n}^{t},x_{n}^{0}),\\tilde{\\beta}_{t}{\\bf I}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\tilde{\\mu}_{t}(x_{n}^{t},x_{n}^{0}):=\\frac{\\sqrt{\\alpha_{t-1}}\\beta_{t}}{1-\\bar{\\alpha}_{t}}x_{n}^{0}+\\frac{\\sqrt{\\alpha_{t}}(1-\\bar{\\alpha}_{t-1})}{1-\\bar{\\alpha}_{t}}x_{n}^{t}\\mathrm{~and~}\\tilde{\\beta}_{t}:=\\frac{1-\\tilde{\\alpha}_{t-1}}{1-\\tilde{\\alpha}_{t}}\\beta_{t}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The training objective Equation (5) is further written as minimizing the Kullback-Leibler (KL) divergence between the forward and reverse processes in Equation (4) and Equation (7), as proposed by Sohl-Dickstein et al. (2015). (See Appendix A for details.) ", "page_idx": 3}, {"type": "text", "text": "In practice, the approach proposed by Ho et al. (2020) involves reparame\u221aterizing $\\mu_{\\theta}$ \u221a and predicting the noise $\\epsilon$ for $\\bar{x}_{n}^{\\bar{t}}$ . The expression for $x_{n}^{t}$ is given by $x_{n}^{t}(x_{n}^{0},\\stackrel{\\cdot}{\\epsilon})\\;=\\;\\sqrt{\\bar{\\alpha}_{t}}x_{n}^{0}\\stackrel{\\sim}{+}\\sqrt{1-{\\bar{\\alpha}_{t}}}\\epsilon$ , with $\\Sigma_{\\theta}(x_{n}^{t},x_{<n}^{0},t)=\\\"\\beta_{t}$ fixed. We realized this with a neural network $\\epsilon_{\\theta}(x_{n}^{t},t,x_{<n}^{0})$ shown in Figure 1, which predicts the noise for $x_{n}^{t}$ at each time step given $\\boldsymbol{x}_{<n}^{0}$ . In the end, the objective function in Equation (2) for training autoregressive image diffusion is written as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{L}_{A I D}\\geq\\sum_{n=1}^{N}L_{D_{n}}=\\sum_{n=1}^{N}\\mathbb{E}_{t,\\epsilon|x_{n}^{0},x_{<n}^{0}}\\left[\\left|\\epsilon_{\\theta}\\big(\\sqrt{\\bar{\\alpha}_{t}}x_{n}^{0}+\\sqrt{1-\\bar{\\alpha}_{t}}\\epsilon,x_{<n}^{0},t\\big)-\\epsilon\\right|_{2}^{2}\\right],\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where the expectation is taken over the noise $\\epsilon\\sim\\mathcal{N}(0,I)$ and the time step $t\\sim\\mathcal{U}(1,...,T)$ . To generate an image sequence, we begin with the noise $x_{1}^{T}$ and update it iteratively using Equation (4) with the given x00 , following the sequence $(x_{1}^{T}\\to x^{T-\\hat{1}}\\to\\cdots\\to x_{1}^{0})$ ). This process yields a clean sample $\\boldsymbol{x}_{1}^{0}$ . Subsequently, we can sample $\\boldsymbol{x}_{2}^{0}$ in the same manner using the generated images $\\boldsymbol{x}_{<2}^{0}$ , and continue this process iteratively to generate the entire sequence of images. ", "page_idx": 3}, {"type": "text", "text": "2.2 Architecture ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "To optimize the objective function in Equation (8) efficiently, ordered images are loaded as sequences of a certain length $N+1$ during the training phase. We take the first $N$ images $\\mathbf{x}_{c o n}=$ $\\{x_{0},x_{1},...,x_{N-1}\\}$ as the conditioning sequence and the last $N$ images $\\mathbf{x}_{t a r g e t}=\\{x_{1},....,x_{N}\\}$ as the target sequence, as shown in Figure 1. We adopt an architecture built on an Unet [23] with capabilities of temporal-spatial conditioning (TSC), designed to process the conditioning sequence and predict the noise for the target sequence. The term \"temporal\" refers to conditioning in previous frames along the $N$ dimensions, while the \"spatial\" refers to the conditioning in the previous frame among the $H\\times W$ dimensions. Additionally, the TSC block is conditioned on the time steps $t$ of the diffusion process. ", "page_idx": 3}, {"type": "text", "text": "The only interaction between images in the conditioning sequence occurs during the attention operation. To maintain proper conditioning with autoregressive property, we implemented a standard upper triangular mask on the $n\\times n$ matrix of attention logits. This causal attention module is used in DiTBlock [18, 24]. The modified DiTBlock is followed by a ResNet block [25], which is a standard building block in the Unet architecture. The features output by the TSC block are then passed to the corresponding encoder block in the Unet, which process the target sequence. The change in tensor dimensions inside TSC Block is handled by the einops library1and illustrated in Figure 1. ", "page_idx": 3}, {"type": "text", "text": "During training, the net predicts the noise in parallel for each noisy image that is sampled from the target sequence, given the conditioning sequence. During generation of sequence, the net iteratively refines the noisy input to produce a clean image, which is then appended to the conditioning sequence. ", "page_idx": 3}, {"type": "text", "text": "2.3 Application in MRI inverse problem ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Image reconstruction is formulated as a Bayesian problem where the posterior of image $p(x|y)$ is expressed as ", "page_idx": 3}, {"type": "equation", "text": "$$\np(x|y)=\\frac{p(y|x)\\cdot p(x)}{p(y)}\\;.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Here, $y$ represents the measured $\\mathbf{k}$ -space data, $x$ denotes the image, and $p(x)$ is a generative prior. The minimum mean square error (MMSE) estimator for the posterior minimizes the mean square error, given by: ", "page_idx": 4}, {"type": "equation", "text": "$$\nx_{\\mathrm{MMSE}}=\\arg\\operatorname*{min}_{\\tilde{x}}\\int\\|\\tilde{x}-x\\|^{2}p(x|y)d x=\\mathbb{E}[x|y]\\;.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "2.4 Likelihood function for $\\mathbf{k}$ -space ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The image $\\boldsymbol{x}\\,\\in\\,\\mathbb{C}^{n\\times n}$ is represented as a complex matrix , where $n\\times n$ is the image size, and $y\\,\\in\\,\\mathbb{C}^{m\\bar{\\times}m_{C}}$ is a vector of complex-valued $\\mathbf{k}$ -space samples from $m_{C}$ receive coils. Assuming circularly-symmetric normal noise $\\eta$ with zero mean and covariance matrix $\\sigma_{\\eta}^{2}\\mathbf{I}$ , the likelihood $p(y|x)$ of observing $y$ given $x$ is formulated as a complex normal distribution: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{p(y|x)=\\mathcal{C N}(y;\\mathcal{A}x,\\sigma_{\\eta}^{2}\\mathbf{I})}\\\\ &{\\quad\\quad\\quad=(\\sigma_{\\eta}^{2}\\pi)^{-N_{p}}e^{-\\|\\sigma_{\\eta}^{-1}\\cdot(y-\\mathcal{A}x)\\|_{2}^{2}}\\;,}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathbf{I}$ is the identity matrix, $\\sigma_{\\eta}$ is the standard deviation of the noise, $\\boldsymbol{A x}$ represents the mean, and $N_{p}$ is the length of the $\\mathbf{k}$ -space data vector. The operator $A:\\mathbb{C}^{n\\times n}\\rightarrow\\mathbb{C}^{m\\times m_{C}}$ maps the image $x$ to $\\mathbf{k}\\cdot$ -space and is composed of the coil sensitivity maps $\\boldsymbol{S}$ , the two-dimensional Fourier transform $\\mathcal{F}$ , and the $\\mathbf{k}$ -space sampling mask $\\mathcal{P}$ , defined as $\\mathcal{A}=\\mathcal{P}\\mathcal{F}\\mathcal{S}$ . For more details and visual understanding on the forward operator, please refer to Appendix C. ", "page_idx": 4}, {"type": "text", "text": "2.5 Sampling the posterior ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Given a sequence of $\\boldsymbol{\\mathrm{k}}$ -space $\\mathbf{y}=\\{y_{1},\\dots,y_{N}\\}$ , each posterior in $\\{p_{\\theta}(x_{n}|y_{n},x_{<n}^{0})|1<n<N\\}$ is expressed as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{p_{\\theta}(x_{n}|y_{n},x_{<n}^{0})=\\displaystyle\\frac{p(y_{n}|x_{n},x_{<n}^{0})p_{\\theta}(x_{n}|x_{<n}^{0})}{p(y_{n}|x_{<n}^{0})}=\\displaystyle\\frac{p(y_{n}|x_{n})p_{\\theta}(x_{n}|x_{<n}^{0})}{p(y_{n})}}}\\\\ {{\\propto p(y_{n}|x_{n})p_{\\theta}(x_{n}|x_{<n}^{0})~,}}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "when the acquisition of $y_{n}$ is independent of the image $\\boldsymbol{x}_{<n}^{0}$ , $y_{n}$ and $\\boldsymbol{x}_{<n}^{0}$ are conditionally independent given $x_{n}$ . Following the Reference [8], we have ", "page_idx": 4}, {"type": "equation", "text": "$$\np_{\\theta}(x_{n}^{t-1}|x_{n}^{t},y_{n},x_{<n}^{0})\\propto p(y_{n}|x_{n}^{t})p_{\\theta}(x_{n}^{t-1}|x_{<n}^{t},x_{<n}^{0})\\;.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The details for Equation (13) is in Appendix B. To sample the above posterior, the learned reverse process in Equation (4) is used, and the algorithm is constructed with two gradient updates using the log of the prior and k-space likelihood: the DDIM (Denoising Diffusion Implicit Model) reverse step proposed by Song et al. (2020), and a data fidelity step derived from the likelihood function Equation (11), which are described as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{x}_{n}^{t-1}\\gets\\sqrt{\\alpha_{t-1}}\\left(\\frac{x_{n}^{t}-\\sqrt{1-\\alpha_{t}}\\epsilon_{\\theta}(x_{n}^{t},x_{<n}^{0},t)}{\\sqrt{\\alpha_{t}}}\\right)+\\sqrt{1-\\alpha_{t-1}}\\epsilon_{\\theta}(x_{n}^{t},x_{<n}^{0},t)}\\\\ &{x_{n}^{t-1}\\gets\\tilde{x}_{n}^{t-1}+\\lambda\\cdot\\nabla_{x_{n}^{t-1}}\\log p(y_{n}|\\tilde{x}_{n}^{t-1})\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\lambda$ is the step size, and $\\nabla_{x_{n}^{t-1}}\\log p(y_{n}|x_{n}^{t-1})$ is the gradient of the log-likelihood of Equation (11). Then, the reconstruction of a sequence images from the undersampled $\\mathbf{k}$ -space data is achieved by sequentially sampling the posterior in $\\{p(x_{n}|y_{n},x_{<n}^{0})|1<n<N\\}$ using autoregressive diffusion model as prior. The algorithm is summarized in Algorithm 1. ", "page_idx": 4}, {"type": "text", "text": "3 Experiments and Results ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "3.1 Model training ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Two autoregressive diffusion models were trained on separate datasets: one in image space and the other in latent space. The image space model was trained on brain images that are from the fastMRI training dataset, which includes T1-weighted (some with post-contrast), T2-weighted, and FLAIR images [27]. These complex images were reconstructed from fully sampled multi-channel k-space ", "page_idx": 4}, {"type": "text", "text": "Algorithm 1 Sample the posterior in $\\{p(x_{n}|y_{n},x_{<n}^{0})|1<n<N\\}$ using autoregressive diffusion model as prior. ", "page_idx": 5}, {"type": "text", "text": "1: Initial image sequence: $x_{<n}^{0}=x_{0}$ ; Time steps: $T$ ; Step size: $\\lambda$ ; Iterations for data fidelity step:   \n; Number of samples: $S$ ;   \n2: for $y_{n}$ in $\\mathbf{y}=\\{y_{1},y_{2},...,y_{N}\\}$ do   \n3: Initialize $x_{n}^{T}$ with Gaussian noise.   \n4: Construct the forward operator $\\boldsymbol{\\mathcal{A}}$ with sampling pattern $\\mathcal{P}$ and coil sensitivities $\\boldsymbol{S}$ .   \n5: for $t$ in $\\{T-1,\\ldots,0\\}$ do   \n6: Run the DDIM reverse step in Equation (14) to get $x_{n}^{t-1}$ given $x_{n}^{t}$ and $\\boldsymbol{x}_{<n}^{0}$ .   \n7: Run the data fidelity step in Equ\u221aation (15) to update xtn\u2212 for $K$ step.   \n8: Add Gaussian noise scaled by  1 \u2212\u03b1t\u22121 to xtn\u2212   \n9: end for   \n10: Update $x_{<n}^{0}\\leftarrow\\{x_{n}^{0},\\ldots,x_{0}^{0}\\}$ .   \n11: end for ", "page_idx": 5}, {"type": "text", "text": "volumes, with coil sensitivity maps computed using the BART toolbox [28]. The images were then normalized to a maximum magnitude of 1, and the real and imaginary parts were treated as separate channels when input into the neural network. The number of images in each volume ranged from 12 to 16. Images were loaded without reordering and resized to $320\\!\\times\\!320$ pixels if they were not already that size. ", "page_idx": 5}, {"type": "text", "text": "The latent space model is trained with the cardiac dataset that contains cine images reconstructed by the SSA-FARY method [29]. Firstly, a VQVAE was trained on the cine images that were preprocessed similarly to images in fastMRI dataset. The cine images have a size of $256\\!\\times\\!256$ pixels. Then, it generates latent space for the training AID. (See the details for configuration of VQVAE in Appendix J). All the training was performed on 4 NVIDIA A100 GPUs with 80GB memory. The models were trained using the Adam optimizer with a learning rate of $10^{-4}$ and a batch size of 1 for image space model and 4 for latent space model. Two models were trained for 440,000 iterations. It took around 2 hours to train brain model for $10\\mathbf{k}$ steps and 1.2 hours for cardiac model. The length of conditioning sequence $N$ for brain and cardiac models are 10 and 42. The network as illustrated in Figure 1 was implemented based on OpenAI\u2019s guided diffusion codebase2. We also trained a standard diffusion model, Guide, on the brain dataset for comparison. The Guide model was trained using the same hyperparameters as the AID model, except the batch size is 10. The Guide model uses the same Unet blocks as AID. ", "page_idx": 5}, {"type": "text", "text": "3.2 Generating sequence of images ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "To test different aspects of the autoregressive diffusion models, we generate the sequence of images using the following two approaches. ", "page_idx": 5}, {"type": "text", "text": "Retrospective sampling: This method generates a new sequence of images $\\{\\tilde{x}_{1},\\ldots,\\tilde{x}_{N}\\}$ based on the given sequence $\\bar{\\{x_{0},\\ldots,x_{N-1}\\}}$ . $\\tilde{x}_{n}$ is sampled from Equation (4) given $\\{x_{0},\\ldots,x_{n-1}\\}$ . ", "page_idx": 5}, {"type": "text", "text": "Prospective sampling: A fixed-length sliding window is initialized with the given sequence $x_{<N}=$ $\\{x_{0},...,x_{N-1}\\}$ . $x_{N}$ is generated from Equation (4) with the current window as conditioning. Subsequently, the window is updated by appending the newly generated $x_{N}$ and removing the earliest image $x_{0}$ . This autoregressive sampling process is repeated until the stop condition is met. We refer to this process as a warm start. In a cold start, the window is initialized with zeros, and each element $x_{n}$ in it is updated with newly generated images from the beginning to the end, after which the generation is warmed up. ", "page_idx": 5}, {"type": "text", "text": "In the retrospective sampling, the model generates a sequence of images that are sequentially coherent and visually similar to the conditioning sequence, as shown in Figure 2 (a). The prospective sampling generates a sequence of images that extends the initial images in the sliding window and constitutes multiple volumes, as shown in Figure 2 (b). As for a cold start, Figure 3 demonstrates the model\u2019s ability to generate a sequence of images using black background as initial status. This shows the model\u2019s generative capabilities from a minimal initial condition, thereby proving its robustness and flexibility. Due to the limit of space, the samples with similar quality from the model trained on the cardiac dataset are shown in Appendix D. We also implemented a boosted sampling technique which use previous slice with added noise as the initial image for the current slice. This requires less iterations to generate the sequence of images. Further details can be found in our codebase. ", "page_idx": 5}, {"type": "image", "img_path": "jIh4W7r0rn/tmp/079010c7355dd5c35398a78ce4bfc4495a6d3cecded3c15f08399a550abcd0f2.jpg", "img_caption": ["Figure 2: (a): A sequence of images from dataset is shown in the first row and is used as conditioning to generate retrospective samples that are shown in the second row. (b): With the given sequence in (a) as a warm start, prospective samples extending it are shown. "], "img_footnote": [], "page_idx": 6}, {"type": "image", "img_path": "jIh4W7r0rn/tmp/7397529d3453b54bf06bbce7e361f486bd1ff4542ebf26c876c8007fa26bfc90.jpg", "img_caption": ["Figure 3: Prospective samples with cold start. The initial images generated in the cold start are not sequentially coherent, but as the sampling process continues, the model progressively generates more sequentially coherent and realistic images. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "3.3 MRI reconstruction ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "The MMSE estimator in Equation (10) cannot be computed in a closed form, and numerical approximations are typically required. Once the samples from the posterior is obtained with Algorithm 1, a consistent estimate of $x_{n\\mathrm{MMSE}}$ can be computed by averaging those samples, i.e. the empirical mean of samples converges in probability to $x_{n\\mathrm{MMSE}}$ due to weak law of large numbers. The variance of those samples provides a solution to the error assessment in the reconstruction assuming the trained model is trusted. To highlight the regions with large uncertainty, we compute the pseudo-confidence intervals based on the assumption that each pixel\u2019s intensity is normally distributed. This involves determining the standard error from the variance, then multiplying it by the t-score corresponding to a $95\\%$ confidence level. ", "page_idx": 6}, {"type": "text", "text": "Unfolding of aliased single-coil image: To investigate how the trained model, AID, reduces the folding artifacts in the reconstruction, we designed the single coil unfolding experiment. The singlechannel $\\mathbf{k}$ -space is simulated out of multichannel $\\boldsymbol{\\mathrm{k}}$ -space data. The odd lines in $\\mathbf{k}$ -space are retained, $y$ . Ten samples were drawn from the posterior $p(x_{1}|y,x_{0})$ using Algorithm 1 with parameters: $T=1000,\\lambda=1,K=5$ . The experiment was repeated using a standard diffusion model, Guide. The results are shown in Figure 4. The AID model significantly reduces the errors in the region of folding artifacts compared to the Guide model. The mean over samples, $x_{\\mathrm{MMSE}}$ , is highlighted with a confidence interval computed from the variance of samples. The highlighted mean image shows the reconstruction by AID is more trustworthy in the folding region. In general, the highlighted region lies in the folding region, where large errors remains, as we expected. ", "page_idx": 7}, {"type": "image", "img_path": "jIh4W7r0rn/tmp/f88e782cdd0ee53c4488b186e35e28ac13b23861e5d06bcf728702163ea7bcda.jpg", "img_caption": ["Figure 4: (a): The folded single-coil image caused by two-times undersampling mask. (b): The comparison of unfolding ability by the autoregressive and the standard diffusion model, i.e., AID (top) and Guide (bottom). Reference image is reconstructed from k-space data without undersampling. The error is the difference between the mean, $x_{\\mathrm{MMSE}}$ , and the reference image. The \"Mean $^{+}$ std\" is the mean highlighted with confidence interval, which indicates the reconstruction by AID is more trustworthy in the region of folding artifacts. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Reconstruction from undersampled data: To further investigate the model\u2019s performance in reconstruction, we conducted experiments on 20 volumes from the fastMRI validation dataset where $\\mathbf{k}$ -space data was retrospectively undersampled using various sampling masks. We created four types of sampling masks: random with autocalibration signal (ACS), random without ACS, equispaced with ACS, and equispaced without ACS. The undersampling factor is 12. Setting parameters: $T=1000,\\lambda=1,K=4$ for Algorithm 1, the images were reconstructed from the undersampled k-space data using the AID and Guide as prior, respectively. Another method proposed in Ref. [11] is used as the baseline (CSGM), which uses a scored-based model (NCSNv2) from Ref. [30] trained on the fastMRI dataset. All the reconstruction tasks are performed by sampling the posterior. The likelihood $p(y|x)$ is determined by forward model and the image prior is determined by the trained models, such as NCSNv2, Guide, and AID. This means that when the sampling method remains consistent, the performance of the reconstruction task is determined by the quality of the image prior. Our algorithm treats $p(y|x)$ in the same manner, and the key difference is the image prior. ", "page_idx": 7}, {"type": "text", "text": "We used peak-signal-noise-ratio (PSNR in dB) and normalized root-mean-square error (NRMSE) to evaluate the reconstruction quality against the reference image that is reconstructed from full k-space. The comparison of metrics across experimental conditions is illustrated in Figure 5. The proposed AID model outperforms the Guide and NCSNv2 in terms of PSNR and NRMSE especially in the absence of ACS, demonstrating its superior performance in image reconstruction from undersampled $\\mathbf{k}\\cdot$ -space data. The results are consistent across different undersampling factors and sampling masks, indicating the model\u2019s robustness and flexibility in handling various types of undersampled $\\mathbf{k}$ -space data. ", "page_idx": 7}, {"type": "text", "text": "For the visual impression of the improvement by the AID model in reconstruction, we show the reconstructed images in Figure 6 and more of them in Appendix E. The images reconstructed using AID are more visually similar to the reference images than using Guide, even which also provides aliased-free images. Furthermore, it is worth noting that more visually notable hallucinations were introduced by the Guide model than the AID model, which means AID is more trustworthy. ", "page_idx": 7}, {"type": "image", "img_path": "jIh4W7r0rn/tmp/a86df5dbcee8ea751d1a0964546c4af9876fb05d43d3e418165404035de11201.jpg", "img_caption": ["Figure 5: E: equispaced, R: random. (a): PSNR and (b): NRMSE of the images reconstructed from the twelve-times undersampled $\\mathbf{k}$ -space data using the autoregressive diffusion model (AID), the standard diffusion model (Guide), and the baseline method CSGM. PSNR higher is better, and NRMSE lower is better. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "jIh4W7r0rn/tmp/edb1d89d8a9195b56bbc7d23d4aa94898d73369d1782ef3a059f7e4548afcdf9.jpg", "img_caption": [], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 6: E: equispaced, R: random. The last column shows the reference and the random sampling mask in $\\mathrm{k}$ -space. The red lines are autocalibration signal (ACS) and equispaced mask is not shown. Zero-filled images are computed by inverse Fourier transform of the zero-filled $\\mathbf{k}$ -space data. The hallucinations are pointed with red arrows. ", "page_idx": 8}, {"type": "text", "text": "3.4 Other models for image sequence generation ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "To further evaluate the model\u2019s ability to generate image sequences, we further trained two AID models on two datasets: one using brain images from autism studies called ABIDE [31, 32], and the other using images from Unmanned Aerial Vehicle (UAV) view dataset [33]. We reported the computation and model complexity in Appendix F for all AID models trained in this work. We also implemented a two-stage training to improve the efficiency for training models on ABIDE and presented correspondingly generated samples in Appendix G. We demonstrated the natural image sequence generation in Appendix H and showed the sample consistency along the temporal axis in Appendix I. ", "page_idx": 8}, {"type": "text", "text": "4 Discussion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this work, we propose an autoregressive image diffusion model for generating image sequences, with specific applications to accelerated MRI reconstruction. We conducted comprehensive evaluation of its performance as an image prior in reconstruction algorithms, comparing it to a standard diffusion model. Due to the learned prior information on inter-image dependencies, the proposed model outperforms the standard diffusion model across various scenarios. Our model is particularly wellsuited for medical applications where image sequences are often acquired (e.g., in volumetric format) from patients in clinical practice. For instance, when different contrast images are acquired during an examination session [34], our model is designed to capture the relationships between these images. This enables more accurate and coherent reconstructions from undersampled k-space data using the proposed Algorithm 1. Additionally, other medical imaging tasks like dynamic MRI, multi-contrast, super-resolution, and denoising could benefit from our model\u2019s ability by leveraging inter-image dependencies [35]. Furthermore, the proposed algorithm holds great promise for facilitating the incorporation pre-existing information from other imaging modalities into MRI image reconstruction. This opens up a wide range of potential medical applications, with the potential to improve patient care and reduce healthcare costs by enabling faster and more accurate image acquisition and diagnosis. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Privacy Issue: As this model has the capability to generate coherent image sequences, it is crucial to consider the privacy implications associated with its use, particularly in clinical settings. The generation of such images may inadvertently expose sensitive patient information, including identifiable features such as facial characteristics. Safeguarding patient privacy must be a top priority when deploying it. We recommend that the model be used in a controlled environment where access to the generated images is restricted to authorized personnel only. Additionally, it is essential to ensure that the model is trained on anonymized data and that the generated images are not stored or shared without proper consent. ", "page_idx": 9}, {"type": "text", "text": "Limitation and future work: We did not evaluate the model on a common image dataset such as ImageNet or Cifar-10, nor did we compute metrics such as FID and Inception Score, which could be a limitation of our work. We plan to address these limitations in future work by running the model on a large dataset and comparing it with other state-of-the-art models. Additionally, given the model\u2019s suitability for modeling image sequences, it is worth exploring its potential for optimizing MRI $\\boldsymbol{\\mathrm{k}}$ -space acquisition strategies, as the acquisition process constitutes a sequence of operations. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The proposed autoregressive image diffusion model offers an approach to generating image sequences, with significant potential as a trustworthy prior in accelerated MRI reconstruction. In various experiments, it outperforms the standard diffusion model in terms of both image quality and robustness by taking the advantage of the prior information on inter-image dependencies. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was supported by DZHK (German Centre for Cardiovascular Research) funding code: 81Z0300115. We acknowledge funding by the \"Nieders\u00e4chsisches Vorab\" funding line of the Volkswagen Foundation. This work was supported by the Federal Ministry of Education and Research (BMBF), Germany under the AI service center KISSKI (grant no. 01IS22093A). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] M. Lustig, D. Donoho, and J. M. Pauly. Sparse MRI: The application of compressed sensing for rapid MR imaging. Magn. Reson. Med., 58(6):1182\u20131195, 2007.   \n[2] K. T. Block, M. Uecker, and J. Frahm. Undersampled radial MRI with multiple coils. Iterative image reconstruction using a total variation constraint. Magn. Reson. Med., 57(6):1086\u20131098, 2007. ISSN 1522-2594. doi: 10.1002/mrm.21236.   \n[3] Yan Yang, Jian Sun, Huibin Li, and Zongben Xu. Deep ADMM-Net for Compressive Sensing MRI. In Advances in Neural Information Processing Systems, volume 29. Curran Associates, Inc., 2016.   \n[4] Kerstin Hammernik, Teresa Klatzer, Erich Kobler, Michael P. Recht, Daniel K. Sodickson, Thomas Pock, and Florian Knoll. Learning a variational network for reconstruction of accelerated MRI data. Magn. Reson. Med., 79(6):3055\u20133071, 2017. ISSN 1522-2594. doi: 10.1002/mrm.26977. [5] Morteza Mardani, Enhao Gong, Joseph Y Cheng, Shreyas S Vasanawala, Greg Zaharchuk, Lei Xing, and John M Pauly. Deep generative adversarial neural networks for compressive sensing mRI. IEEE transactions on medical imaging, 38(1):167\u2013179, 2018. [6] Kerem C Tezcan, Christian F Baumgartner, Roger Luechinger, Klaas P Pruessmann, and Ender Konukoglu. MR image reconstruction using deep density priors. IEEE transactions on medical imaging, 38(7):1633\u20131642, 2019. doi: 10.1109/TMI.2018.2887072. [7] Guanxiong Luo, Na Zhao, Wenhao Jiang, Edward S. Hui, and Peng Cao. MRI reconstruction using deep bayesian estimation. Magn. Reson. Med., 84(4):2246\u20132261, apr 2020. doi: 10.1002/ mrm.28274.   \n[8] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International conference on machine learning, pages 2256\u20132265. PMLR, 2015. [9] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d\u2019Alch\u00e9- Buc, Emily B. Fox, and Roman Garnett, editors, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 11895\u201311907, 2019.   \n[10] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:6840\u20136851, 2020.   \n[11] Ajil Jalal, Marius Arvinte, Giannis Daras, Eric Price, Alexandros G Dimakis, and Jon Tamir. Robust compressed sensing mri with deep generative priors. Advances in Neural Information Processing Systems, 34:14938\u201314954, 2021.   \n[12] Ling Yang, Zhilong Zhang, Yang Song, Shenda Hong, Runsheng Xu, Yue Zhao, Wentao Zhang, Bin Cui, and Ming-Hsuan Yang. Diffusion models: A comprehensive survey of methods and applications. ACM Computing Surveys, 56(4):1\u201339, 2023.   \n[13] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In International Conference on Learning Representations, 2021.   \n[14] Alper G\u00fcng\u00f6r, Salman UH Dar, \u00b8Saban \u00d6zt\u00fcrk, Yilmaz Korkmaz, Hasan A Bedel, Gokberk Elmas, Muzaffer Ozbey, and Tolga \u00c7ukur. Adaptive diffusion priors for accelerated mri reconstruction. Medical Image Analysis, 88:102872, 2023.   \n[15] Hyungjin Chung and Jong Chul Ye. Score-based diffusion models for accelerated mri. Medical image analysis, 80:102479, 2022.   \n[16] Guanxiong Luo, Moritz Blumenthal, Martin Heide, and Martin Uecker. Bayesian mri reconstruction with joint uncertainty estimation using diffusion models. Magnetic Resonance in Medicine, 90(1):295\u2013311, 2023.   \n[17] Martin Zach, Florian Knoll, and Thomas Pock. Stable deep mri reconstruction using generative priors. IEEE Transactions on Medical Imaging, 2023.   \n[18] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.   \n[19] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. 2018.   \n[20] Aaron Van den Oord, Nal Kalchbrenner, Lasse Espeholt, Oriol Vinyals, Alex Graves, et al. Conditional image generation with pixelcnn decoders. Advances in neural information processing systems, 29, 2016.   \n[21] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever. Generative pretraining from pixels. In International conference on machine learning, pages 1691\u20131703. PMLR, 2020.   \n[22] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 12873\u201312883, 2021.   \n[23] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In Medical image computing and computer-assisted intervention\u2013MICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18, pages 234\u2013241. Springer, 2015.   \n[24] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 4195\u20134205, 2023.   \n[25] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.   \n[26] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020.   \n[27] Florian Knoll, Jure Zbontar, Anuroop Sriram, Matthew J Muckley, Mary Bruno, Aaron Defazio, Marc Parente, Krzysztof J Geras, Joe Katsnelson, Hersh Chandarana, et al. fastmri: A publicly available raw k-space and dicom dataset of knee images for accelerated mr image reconstruction using machine learning. Radiology: Artificial Intelligence, 2(1):e190007, 2020.   \n[28] Moritz Blumenthal, Martin Heide, Christian Holme, Martin Juschitz, Bernhard Rapp, Philip Schaten, Nick Scholand, Jon Tamir, Christian T\u00f6nnes, and Martin Uecker. mrirecon/bart: version 0.9.00, December 2023. URL https://doi.org/10.5281/zenodo.10277939.   \n[29] Sebastian Rosenzweig, Nick Scholand, H Christian M Holme, and Martin Uecker. Cardiac and respiratory self-gating in radial mri using an adapted singular spectrum analysis (ssa-fary). IEEE transactions on medical imaging, 39(10):3029\u20133041, 2020.   \n[30] Yang Song and Stefano Ermon. Improved techniques for training score-based generative models. Advances in neural information processing systems, 33:12438\u201312448, 2020.   \n[31] A. Di Martino, C-G Yan, Q. Li, E. Denio, F. X. Castellanos, K. Alaerts, J. S. Anderson, M. Assaf, S. Y. Bookheimer, M. Dapretto, B. Deen, S. Delmonte, I. Dinstein, B. Ertl-Wagner, D. A. Fair, L. Gallagher, D. P. Kennedy, C. L. Keown, C. Keysers, J. E. Lainhart, C. Lord, B. Luna, V. Menon, N. J. Minshew, C. S. Monk, S. Mueller, R. A. Mueller, M. B. Nebel, J. T. Nigg, K. O\u2019Hearn, K. A. Pelphrey, S. J. Peltier, J. D. Rudie, S. Sunaert, Marc Thioux, J. M. Tyszka, L. Q. Uddin, J. S. Verhoeven, N. Wenderoth, J. L. Wiggins, S. H. Mostofsky, and M. P. Milham. The autism brain imaging data exchange: towards a large-scale evaluation of the intrinsic brain architecture in autism. Molecular Psychiatry, 19(6):659\u2013667, June 2014. ISSN 1359-4184. doi: 10.1038/mp.2013.78.   \n[32] Adriana Di Martino, David O\u2019connor, Bosi Chen, Kaat Alaerts, Jeffrey S Anderson, Michal Assaf, Joshua H Balsters, Leslie Baxter, Anita Beggiato, Sylvie Bernaerts, Laura M E Blanken, Susan y Bookheimer, B. Blair Braden, Lisa Byrge, F. Xavier Castellanos, Mirella Dapretto, Richard Delorme, Damien A Fair, Inna Fishman, Jacqueline Fitzgerald, Louise Gallagher, R. Joanne Jao Keehn, Daniel P Kennedy, Janet E Lainhart, Beatriz Luna, Stewart H Mostofsky, Ralph-Axel M\u00fcller, Mary Beth Nebel, Joel T Nigg, Kirsten O\u2019hearn, Marjorie Solomon, Roberto Toro, Chandan J Vaidya, Nicole Wenderoth, Tonya White, R. Cameron Craddock, Catherine Lord, Bennett L. Leventhal, and Michael Milham. Enhancing studies of the connectome in autism using the autism brain imaging data exchange II. Scientific Data, 4:170010, March 2017.   \n[33] \u02d9Ibrahim Deliba\u00b8so\u02d8glu. Pesmod: Small moving object detection benchmark dataset for moving cameras. In 2022 7th International Conference on Frontiers of Signal Processing (ICFSP), pages 23\u201329. IEEE, 2022.   \n[34] Brett Levac, Ajil Jalal, Kannan Ramchandran, and Jonathan I Tamir. Conditional score-based reconstructions for multi-contrast mri. arXiv preprint arXiv:2303.14795, 2023.   \n[35] Guangyuan Li, Chen Rao, Juncheng Mo, Zhanjie Zhang, Wei Xing, and Lei Zhao. Rethinking diffusion model for multi-contrast mri super-resolution. arXiv preprint arXiv:2404.04785, 2024. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Loss function derivation ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Below is a derivation of Equation (5), the reduced variance variational bound for diffusion models. This adapted from Sohl-Dickstein et al. (2015) and $\\mathrm{Ho}$ et al. (2020). We include it here only for completeness. In the forward process, $x_{n}^{t}$ and $\\boldsymbol{x}_{<n}^{0}$ are conditionally independent given $x_{n}^{t-1}$ . ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{L=\\mathbb{E}_{\\boldsymbol{q}}\\left[-\\log(\\pi_{n}^{T}|\\frac{\\alpha_{n}^{0}}{\\sigma_{n}^{*}})-\\sum_{\\boldsymbol{v}^{\\perp}}\\log\\frac{p_{0}(x_{n}^{\\pi-1}|x_{n}^{\\pi,\\infty},x_{\\infty}^{0})}{q(x_{n}^{\\pi}|x_{n}^{\\pi-1},x_{\\infty}^{0})}-\\log\\frac{p_{0}(x_{n}^{\\pi-1}|x_{n}^{\\pi,\\infty},x_{\\infty}^{0})}{q(x_{n}^{\\pi}|x_{n}^{\\pi,\\infty},x_{\\infty}^{0})}\\right]\\qquad\\qquad\\qquad(1)}\\\\ &{\\quad=\\mathbb{E}_{\\boldsymbol{q}}\\left[-\\log(\\pi_{n}^{T}|x_{n}^{\\pi}\\!)-\\sum_{\\boldsymbol{v}^{\\perp}}\\log\\frac{p_{0}(x_{n}^{\\pi-1}|x_{n}^{\\pi,\\infty},x_{\\infty}^{0})}{q(x_{n}^{\\pi-1}|x_{n}^{\\pi,0},x_{\\infty}^{0})}\\right.\\cdot\\frac{q(x_{n}^{\\pi-1}|x_{n}^{\\pi})}{q(x_{n}^{\\pi}|x_{n}^{\\pi})}-\\log\\frac{p_{0}(x_{n}^{\\pi}|x_{n}^{\\pi,\\infty},x_{\\infty}^{0})}{q(x_{n}^{\\pi}|x_{n}^{\\pi,\\infty},x_{\\infty}^{0})}\\right]}\\\\ &{\\quad=\\mathbb{E}_{\\boldsymbol{q}}\\left[-\\log\\frac{p(x_{n}^{\\pi}|x_{n}^{\\pi,\\infty})}{q(x_{n}^{\\pi}|x_{n}^{\\pi,\\infty},x_{\\infty}^{0})}-\\sum_{\\boldsymbol{v}^{\\perp}}\\log\\frac{p_{0}(x_{n}^{\\pi-1}|x_{n}^{\\pi,\\infty},x_{\\infty}^{0})}{q(x_{n}^{\\pi-1}|x_{n}^{\\pi,\\infty},x_{\\infty}^{0},x_{\\infty}^{0})}-\\log\\frac{p_{0}(x_{n}^{\\pi}|x_{n}^{\\pi,\\infty},x_{\\infty}^{0})}{q(x_{n}^{\\pi}|x_{n}^{\\pi,\\infty},\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "B Posterior derivation ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "When samples drawn from the posterior started from the standard Gaussian noise, with Equation (12) we have ", "page_idx": 12}, {"type": "equation", "text": "$$\np({x}_{n}^{t}|{y}_{n},{x}_{<n}^{0})\\propto p({y}_{n}|{x}_{n}^{t})p({x}_{n}^{t}|{x}_{<n}^{0})\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "for all the reverse time steps. Because ", "page_idx": 12}, {"type": "equation", "text": "$$\np(x_{n}^{t}|x_{<n}^{0})=\\int p(x_{n}^{t}|x_{n}^{t+1},x_{<n}^{0})p(x_{n}^{t+1})d x_{n}^{t+1}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "and ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\int p(x_{n}^{t}|x_{n}^{t+1},y_{n},x_{<n}^{0})p(x_{n}^{t+1})d x_{n}^{t+1}=p(x_{n}^{t}|y_{n},x_{<n}^{0})\\;,}}\\\\ &{}&{=\\frac{p(y_{n}|x_{n}^{t})p(x_{n}^{t}|x_{<n}^{0})}{p(y_{n})}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "then we have ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\int p(x_{n}^{t}|x_{n}^{t+1},y_{n},x_{<n}^{0})p(x_{n}^{t+1})d x_{n}^{t+1}=\\frac{p(y_{n}|x_{n}^{t})}{p(y_{n})}\\cdot\\int p(x_{n}^{t}|x_{n}^{t+1},x_{<n}^{0})p(x_{n}^{t+1})d x_{n}^{t+1}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Therefore, we have ", "page_idx": 12}, {"type": "equation", "text": "$$\np(x_{n}^{t}|x_{n}^{t+1},y_{n},x_{<n}^{0})=\\frac{p(y_{n}|x_{n}^{t})p(x_{n}^{t}|x_{n}^{t+1},x_{<n}^{0})}{p(y_{n})}\\;.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "$p(y_{n})$ is a constant for evidence. Then with gradient based method, the posterior $p(x_{n}^{t}|x_{n}^{t+1},y_{n},x_{<n}^{0})$ is sampled from the likelihood $p(y_{n}|x_{n}^{t})$ and the reverse process $p(x_{n}^{t}|x_{n}^{t+1},x_{<n}^{0})$ , ", "page_idx": 12}, {"type": "text", "text": "C Likelihood function for $\\mathbf{k}$ -space ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "The autocalibration signal (ACS) region are lines through the center of $\\mathbf{k}$ -space, however, are fully sampled. The sensitivity of a coil is a spatial profile that describes the receiving field that induces signals in the coil. The simultaneous data acquisition, with each coil\u2019s sensitivity corresponding to a different subregion, leads to a complete image without aliasing artifacts. ", "page_idx": 12}, {"type": "image", "img_path": "jIh4W7r0rn/tmp/eb46b2bad44d627195a4fbd206a8b59e01cc2a9fa44da120f4e43dbcf5740c0b.jpg", "img_caption": ["Figure 7: The relationship between k-space and image. The Nyquist theorem states that the sampling rate must be at least twice the highest frequency component in the signal. "], "img_footnote": [], "page_idx": 13}, {"type": "image", "img_path": "jIh4W7r0rn/tmp/d052e0eb56c7294038896759c5fcdb6649b93d003683ab3bef3c3b6567b2a9f3.jpg", "img_caption": ["Figure 8: The signal detected by a coil is weighted by its local coil proflie, which is called sensitivities and imposes weights on the signal intensity. Consequently, it causes dark and bright regions in coil images. The ground truth image is the combination of all coil images. "], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "D Cardiac samples ", "text_level": 1, "page_idx": 14}, {"type": "image", "img_path": "jIh4W7r0rn/tmp/28d397e497bfa9ddeb0d806d41a1d21bb62096a1821d39d1f823310cb07073a8.jpg", "img_caption": ["Figure 9: Prospective samples from the model trained on cardiac dataset. "], "img_footnote": [], "page_idx": 14}, {"type": "image", "img_path": "jIh4W7r0rn/tmp/34e776a872a8dd5ea3883fa7da47e46874c7a9d85e24fed8b6a12018e0783420.jpg", "img_caption": ["Figure 10: Retrospective samples from the model trained on cardiac dataset. "], "img_footnote": [], "page_idx": 14}, {"type": "image", "img_path": "jIh4W7r0rn/tmp/e4d721db65521317b8c15dc3ed6821d602fa1c7164b564dc62f8c9b1597b64e5.jpg", "img_caption": ["E Reconstruction from undersampled data "], "img_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "jIh4W7r0rn/tmp/21e03aa255d4306d55b39f95d3f0d406e98efa90251e13002eb632cacabb000f.jpg", "img_caption": [], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "F Computation and model complexity ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Table 1 presents the computation needed to train the AID models on four different datasets with different model complexities. ", "page_idx": 16}, {"type": "text", "text": "\u2022 Dataset: the name of the dataset (fastMRI, cardiac, ABIDE, UAV).   \n\u2022 Length: the length of the image sequence.   \n\u2022 Image size: the dimensions of the images in the dataset.   \n\u2022 Latent: the latent space representation used for the model, with options like VQVAE, AutoencoderKL, or None.   \n\u2022 Two-stage: a boolean indicating whether a two-stage training process was used. Two-stage training is explained in the following section.   \n\u2022 Parameters: the number of parameters in the model.   \n\u2022 Train steps/s: training speed in steps per second.   \n\u2022 Inference (it/s): inference speed in iterations per second. ", "page_idx": 16}, {"type": "table", "img_path": "jIh4W7r0rn/tmp/b528e66c6735e2398524a3c8e35736b2640330248d796af33f4c37349c23ff89.jpg", "table_caption": ["Table 1: Datasets and computational resources used to train the four different AID models. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "G Two-stage training ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We implemented a two-stage training process to improve training efficiency. In the first stage, we trained the U-net model. In the second stage, we trained the temporal-spatial conditioning block with the pre-trained U-net model frozen. By doing so, we are able to train an AID model on ABIDE dataset, where the image sequence has a dimension of $46{\\times}128{\\times}128$ after preprocessing. The generated image squeence is shown in Figure 11. ", "page_idx": 16}, {"type": "text", "text": "H Natural image sequence generation ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We trained an AID model on an UAV dataset in the latent space and generated images using the trained model. The generated images are displayed in Figure 12. The generated images demonstrate the effectiveness of the proposed method in learning sequentially coherent natural images generation. Each frame in Figure 2 shows an aerial view of a rural landscape with roads and/or a water pond. ", "page_idx": 16}, {"type": "image", "img_path": "jIh4W7r0rn/tmp/41320cde28e6b643e6335011ef306f063473370ef1026ca7a69e957de1b824ef.jpg", "img_caption": ["Figure 11: The generated ABIDE image sequence captures the changes in the brain structure. "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "jIh4W7r0rn/tmp/c719a9f8cd8e6cb03a3800e53d0a738ca9049af5728a56863313576b055fced5.jpg", "img_caption": ["Figure 12: The light changes in the water surface are captured in the generated UAV image sequence. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "I Sample consistency along the temporal axis ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Figure 13 shows the sample consistency along the temporal (or z) axis. Columns 1 and 2: Show sagittal and coronal views of a brain image sequence. These images appear to be medical scans with clearly stretched anatomical structures. Column 3: Displays the x-t plane of a cardiac image sequence. This displays the heart\u2019s activity over time and shows the diastolic and systolic phases from left to right. Columns 4 and 5: Show the x-t plane of a UAV image sequence, both generated and real. These images show the change in aerial views of a landscape over time. The generated x-t plane are generally consistent with the real x-t plane images but suffer from the striped artifacts. ", "page_idx": 17}, {"type": "image", "img_path": "jIh4W7r0rn/tmp/3b05f87a511ca9249c182cb37159d500dedee822f00edec2efb0590aa866b16b.jpg", "img_caption": ["Figure 13: Temporal consistency of the images generated by AID models trained on different datasets. The first two columns show the sagittal and coronal view of brain image sequence. The x-t plane of cardiac image and UAV sequence are shown in the last three columns. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "J VQVAE configuration for cardiac dataset ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "The VQVAE is trained on the cardiac dataset to generate the latent space for the training of the autoregressive diffusion model, using the official implementation3. The VQVAE is trained with the following configuration: ", "page_idx": 18}, {"type": "text", "text": "base_learning_rate: 4.5e-06   \nparams: embed_dim: 3 n_embed: 8192 ddconfig: double_z: false z_channels: 3 resolution: 256 in_channels: 3 out_ch: 3 ch: 128 ch_mult: [1, 2, 4] num_res_blocks: 2 attn_resolutions: [] dropout: 0.0 lossconfig: target: losses.vqperceptual.VQLPIPSWithDiscriminator params: disc_conditional: false disc_in_channels: 3 disc_start: 30001 disc_weight: 0.8 codebook_weight: 1.0 ", "page_idx": 18}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The papers not including the checklist will be desk rejected. The checklist should follow the references and follow the (optional) supplemental material. The checklist does NOT count towards the page limit. ", "page_idx": 19}, {"type": "text", "text": "Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist: ", "page_idx": 19}, {"type": "text", "text": "\u2022 You should answer [Yes] , [No] , or [NA] .   \n\u2022 [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.   \n\u2022 Please provide a short (1\u20132 sentence) justification right after your answer (even for NA). ", "page_idx": 19}, {"type": "text", "text": "The checklist answers are an integral part of your paper submission. They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper. ", "page_idx": 19}, {"type": "text", "text": "The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a proper justification is given (e.g., \"error bars are not reported because it would be too computationally expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found. ", "page_idx": 19}, {"type": "text", "text": "IMPORTANT, please: ", "page_idx": 19}, {"type": "text", "text": "\u2022 Delete this instruction block, but keep the section heading \u201cNeurIPS paper checklist\", \u2022 Keep the checklist subsection headings, questions/answers and guidelines below. \u2022 Do not modify the questions and only use the provided macros for your answers. ", "page_idx": 19}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Justification: See the last paragraph of the introduction in Page 2. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 19}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: See the last paragraph of discussion in Page 10. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 20}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Justification: See Appendix A and B Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 20}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: The paper provides all the information needed to reproduce the main experimental results, include data, training details, algorithm details, and evaluation metrics in the section of experiments. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 20}, {"type": "text", "text": "\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 21}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: The code is available at https://github.com/mrirecon/aid. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). ", "page_idx": 21}, {"type": "text", "text": "\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 22}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: The paper provides all the necessary details to understand the results in the section of experiments. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 22}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: The paper reports error bar in Figure 5. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 22}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: See Section 3.1 ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. ", "page_idx": 22}, {"type": "text", "text": "\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 23}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: See Privacy Issue in the section of discussion. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 23}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: See Privacy Issue in the section of discussion. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 23}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: The paper does not release data or models that have a high risk for misuse. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks. ", "page_idx": 23}, {"type": "text", "text": "\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 24}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: The creators or original owners of assets are properly credited and the license and terms of use are explicitly mentioned and properly respected. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 24}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: The paper does not submit new assets. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 24}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [No] ", "page_idx": 24}, {"type": "text", "text": "Justification: This work does not involve crowdsourcing nor research with human subjects. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 25}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: This paper paper use publicly released fastMRI dataset and have been approved by the local IRB. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 25}]