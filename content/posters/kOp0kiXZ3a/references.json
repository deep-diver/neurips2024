{"references": [{"fullname_first_author": "S. Zhou", "paper_title": "Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients", "publication_date": "2016-06-16", "reason": "This paper is foundational to the field of low-bitwidth convolutional neural network training, introducing techniques that are further developed in the target paper."}, {"fullname_first_author": "Q. Jin", "paper_title": "Adabits: Neural network quantization with adaptive bit-widths", "publication_date": "2020-06-16", "reason": "This paper directly addresses the challenge of adaptive bit-width in neural network quantization, a key problem that the target paper aims to improve."}, {"fullname_first_author": "K. Xu", "paper_title": "Multiquant: Training once for multi-bit quantization of neural networks", "publication_date": "2022-07-07", "reason": "This work tackles multi-bit quantization in neural networks, a problem closely related to the target paper's focus on multi-precision quantization."}, {"fullname_first_author": "H. Yu", "paper_title": "Any-precision deep neural networks", "publication_date": "2021-01-01", "reason": "This paper explores the concept of any-precision deep neural networks, directly relevant to the adaptive precision approach in the target paper."}, {"fullname_first_author": "A. Bulat", "paper_title": "Bit-mixer: Mixed-precision networks with runtime bit-width selection", "publication_date": "2021-10-10", "reason": "This paper introduces the concept of mixed-precision networks and runtime bit-width selection, a key technique further refined in the target paper."}]}