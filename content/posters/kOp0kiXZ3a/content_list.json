[{"type": "text", "text": "Nearly Lossless Adaptive Bit Switching ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 Model quantization is widely applied for compressing and accelerating deep neural   \n2 networks (DNNs). However, conventional Quantization-Aware Training (QAT)   \n3 focuses on training DNNs with uniform bit-width. The bit-width settings vary   \n4 across different hardware and transmission demands, which induces considerable   \n5 training and storage costs. Hence, the scheme of one-shot joint training multiple   \n6 precisions is proposed to address this issue. Previous works either store a larger   \n7 FP32 model to switch between different precision models for higher accuracy or   \n8 store a smaller INT8 model but compromise accuracy due to using shared quanti  \n9 zation parameters. In this paper, we introduce the Double Rounding quantization   \n10 method, which fully utilizes the quantized representation range to accomplish   \n11 nearly lossless bit-switching while reducing storage by using the highest integer   \n12 precision instead of full precision. Furthermore, we observe a competitive inter  \n13 ference among different precisions during one-shot joint training, primarily due   \n14 to inconsistent gradients of quantization scales during backward propagation. To   \n15 tackle this problem, we propose an Adaptive Learning Rate Scaling (ALRS) tech  \n16 nique that dynamically adapts learning rates for various precisions to optimize the   \n17 training process. Additionally, we extend our Double Rounding to one-shot mixed   \n18 precision training and develop a Hessian-Aware Stochastic Bit-switching (HASB)   \n19 strategy. Experimental results on the ImageNet-1K classification demonstrate that   \n20 our methods have enough advantages to state-of-the-art one-shot joint QAT in both   \n21 multi-precision and mixed-precision. Our codes are available at here. ", "page_idx": 0}, {"type": "text", "text": "22 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "23 Recently, with the popularity of mobile and edge devices, more and more researchers have attracted   \n24 attention to model compression due to the limitation of computing resources and storage. Model   \n25 quantization [1; 2] has gained significant prominence in the industry. Quantization maps floating-point   \n26 values to integer values, significantly reducing storage requirements and computational resources   \n27 without altering the network architecture.   \n28 Generally, for a given pre-trained model, the quantization bit-width configuration is predefined for a   \n29 specific application scenario. The quantized model then undergoes retraining, i.e., QAT, to mitigate   \n30 the accuracy decline. However, when the model is deployed across diverse scenarios with different   \n31 precisions, it often requires repetitive retraining processes for the same model. A lot of computing   \n32 resources and training costs are wasted. To address this challenge, involving the simultaneous   \n33 training of multi-precision [3; 4] or one-shot mixed-precision [3; 5] have been proposed. Among   \n34 these approaches, some involve sharing weight parameters between low-precision and high-precision   \n35 models, enabling dynamic bit-width switching during inference.   \n36 However, bit-switching from high precision (or bit-width) to low precision may introduce significant   \n37 accuracy degradation due to the Rounding operation in the quantization process. Additionally, there is   \n38 severe competition in the convergence process between higher and lower precisions in multi-precision   \n39 scheme. In mixed-precision scheme, previous methods often incur vast searching and retraining costs   \n40 due to decoupling the training and search stages. Due to the above challenges, bit-switching remains   \n41 a very challenging problem. Our motivation is designing a bit-switching quantization method that   \n42 doesn\u2019t require storing a full-precision model and achieves nearly lossless switching from high-bits to   \n43 low-bits. Specifically, for different precisions, we propose unified representation, normalized learning   \n44 steps, and tuned probability distribution so that an efficient and stable learning process is achieved   \n45 across multiple and mixed precisions, as depicted in Figure 1.   \n46 To solve the bit-switching problem, prior methods either store the floating-point parameters [6; 7; 4; 8]   \n47 to avoid accuracy degradation or abandon some integer values by replacing rounding with floor[3; 9]   \n48 but leading to accuracy decline or training collapse at lower bit-widths. We propose Double Rounding,   \n49 which applies the rounding operation twice instead of once, as shown in Figure1 (a). This approach   \n50 ensures nearly lossless bit-switching and allows storing the highest bit-width model instead of the   \n51 full-precision model. Specifically, the lower precision weight is included in the higher precision   \n52 weight, reducing storage constraints.   \n53 Moreover, we empirically find severe competition between higher and lower precisions, particularly   \n54 in 2-bit precision, as also noted in [10; 4]. There are two reasons for this phenomenon: The optimal   \n55 quantization interval itself is different for higher and lower precisions. Furthermore, shared weights   \n56 are used for different precisions during joint training, but the quantization interval gradients for   \n57 different precisions exhibit distinct magnitudes during training. Therefore, we introduce an Adaptive   \n58 Learning Rate Scaling (ALRS) method, designed to dynamically adjust the learning rates across   \n59 different precisions, which ensures consistent update steps of quantization scales corresponding to   \n60 different precisions, as shown in the Figure 1 (b).   \n61 Finally, we develop an efficient one-shot mixed-precision quantization approach based on Double   \n62 Rounding. Prior mixed-precision approaches first train a SuperNet with predefined bit-width lists,   \n63 then search for optimal candidate SubNets under restrictive conditions, and finally retrain or fine-tune   \n64 them, which incurs significant time and training costs. However, we use the Hessian Matrix Trace [11]   \n65 as a sensitivity metric for different layers to optimize the SuperNet and propose a Hessian-Aware   \n66 Stochastic Bit-switching (HASB) strategy, inspired by the Roulette algorithm [12]. This strategy   \n67 enables tuned probability distribution of switching bit-width across layers, assigning higher bits to   \n68 more sensitive layers and lower bits to less sensitive ones, as shown in Figure 1 (c). And, we add the   \n69 sensitivity to the search stage as a constraint factor. So, our approach can omit the last stage. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "image", "img_path": "kOp0kiXZ3a/tmp/3c3447a7c5ea415116d9b0f61dda2a8f927fd0e810aba93f15456a0ea43046a3.jpg", "img_caption": ["Figure 1: Overview of our proposed lossless adaptive bit-switching strategy. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "70 In conclusion, our main contributions can be described as: ", "page_idx": 2}, {"type": "text", "text": "71 \u2022 Double Rounding quantization method for multi-precision is proposed, which stores a single   \n72 integer weight to enable adaptive precision switching with nearly lossless accuracy.   \n73 \u2022 Adaptive Learning Rate Scaling (ALRS) method for the multi-precision scheme is intro  \n74 duced, which effectively narrows the training convergence gap between high-precision   \n75 and low-precision, enhancing the accuracy of low-precision models without compromising   \n76 high-precision model accuracy.   \n77 \u2022 Hessian-Aware Stochastic Bit-switching (HASB) strategy for one-shot mixed-precision   \n78 SuperNet is applied, where the access probability of bit-width for each layer is determined   \n79 based on the layer\u2019s sensitivity.   \n80 \u2022 Experimental results on the ImageNet1K dataset demonstrate that our proposed methods are   \n81 comparable to state-of-the-art methods across different mainstream CNN architectures. ", "page_idx": 2}, {"type": "text", "text": "82 2 Related Works ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "83 Multi-Precision. Multi-Precision entails a single shared model with multiple precisions by one-shot   \n84 joint Quantization-Aware Training (QAT). This approach can dynamically adapt uniform bit-switching   \n85 for the entire model according to computing resources and storage constraints. AdaBits [13] is the   \n86 first work to consider adaptive bit-switching but encounters convergence issues with 2-bit quantization   \n87 on ResNet50 [14]. Bit-Mixer [9] addresses this problem by using the LSQ [2] quantization method   \n88 but discards the lowest state quantized value, resulting in an accuracy decline. Multi-Precision   \n89 joint QAT can also be viewed as a multi-objective optimization problem. Any-precision [6] and   \n90 MultiQuant [4] combine knowledge distillation techniques to improve model accuracy. Among these   \n91 methods, MultiQuant\u2019s proposed \"Online Adaptive Label\" training strategy is essentially a form of   \n92 self-distillation [15]. Similar to our method, AdaBits and Bit-Mixer can save an 8-bit model, while   \n93 other methods rely on 32-bit models for bit switching. Our Double Rounding method can store the   \n94 highest bit-width model (e.g., 8-bit) and achieve almost lossless bit-switching, ensuring a stable   \n95 optimization process. Importantly, this leads to a reduction in training time by approximately $10\\%$ [7]   \n96 compared to separate quantization training.   \n97 One-shot Mixed-Precision. Previous works mainly utilize costly approaches, such as reinforcement   \n98 learning [16; 17] and Neural Architecture Search (NAS) [18; 19; 20], or rely on partial prior knowl  \n99 edge [21; 22] for bit-width allocation, which may not achieve global optimality. In contrast, our   \n100 proposed one-shot mixed-precision method employs Hessian-Aware optimization to refine a SuperNet   \n101 via gradient updates, and then obtain the optimal conditional SubNets with less search cost without   \n102 retraining or fine-tuning. Additionally, Bit-Mixer [9] and MultiQuant [4] implement layer-adaptive   \n103 mixed-precision models, but Bit-Mixer uses a naive search method to attain a sub-optimal solution,   \n104 while MultiQuant requires 300 epochs of fine-tuning to achieve ideal performance. Unlike NAS   \n105 approaches [20], which focus on altering network architecture (e.g., depth, kernel size, or channels),   \n106 our method optimizes a once-for-all SuperNet using only quantization techniques without altering   \n107 the model architecture. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "108 3 Methodology ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "109 3.1 Double Rounding ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "110 Conventional separate precision quantization using Quantization-Aware Training (QAT) [23] attain   \n111 a fixed bit-width quantized model under a pre-trained FP32 model. A pseudo-quantization node is   \n112 inserted into each layer of the model during training. This pseudo-quantization node comprises two   \n113 operations: the quantization operation $q u a n t(x)$ , which maps floating-point (FP32) values to lower  \n114 bit integer values, and the dequantization operation dequant $(x)$ , which restores the quantized integer   \n115 value to its original floating-point representation. It can simulate the quantization error incurred   \n116 when compressing float values into integer values. As quantization involves a non-differentiable   \n117 Rounding operation, Straight-Through Estimator (STE) [24] is commonly used to handle the non  \n118 differentiability.   \n119 However, for multi-precision quantization, bit-switching can result in significant accuracy loss,   \n120 especially when transitioning from higher bit-widths to lower ones, e.g., from 8-bit to 2-bit. To   \n121 mitigate this loss, prior works have mainly employed two strategies: one involves bit-switching from   \n122 a floating-point model (32-bit) to a lower-bit model each time using multiple learnable quantization   \n123 parameters, and the other substitutes the Rounding operation with the Floor operation, but this   \n124 results in accuracy decline (especially in 2-bit). In contrast, we propose a nearly lossless bit  \n125 switching quantization method called Double Rounding. This method overcomes these limitations by   \n126 employing a Rounding operation twice. It allows the model to be saved in the highest-bit (e.g., 8-bit)   \n127 representation instead of full-precision, facilitating seamless switching to other bit-width models. A   \n128 detailed comparison of Double Rounding with other quantization methods is shown in Figure 2.   \n129 Unlike AdaBits, which relies on the Dorefa [1] quantization method where the quantization scale is   \n130 determined based on the given bit-width, the quantization scale of our Double Rounding is learned   \n131 online and is not fixed. It only requires a pair of shared quantization parameters, i.e., scale and   \n132 zero-point. Quantization scales of different precisions adhere to a strict \"Power of Two\" relationship.   \n133 Suppose the highest-bit and the target low-bit are denoted as $h$ -bit and $l$ -bit respectively, and the   \n134 difference between them is $\\Delta=h-l$ . The specific formulation of Double Rounding is as follows: ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "image", "img_path": "kOp0kiXZ3a/tmp/45e161da0b9cc82a1d64674e22f157c486d619e4cce715b0a05c99e26054deb9.jpg", "img_caption": ["Figure 2: Comparison of four quantization schemes:(from left to right) used in $L S Q$ [2], AdaBits [3], Bit-Mixer [9] and Ours Double Rounding. In all cases $y=d e q u a n t(q u a n t(x))$ . "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\widetilde{W}_{h}=\\mathrm{clip}(\\left\\lfloor\\frac{W-\\mathbf{z}_{h}}{\\mathbf{s}_{h}}\\right\\rceil,-2^{h-1},2^{h-1}-1)}}\\\\ {{\\widetilde{W}_{l}=\\mathrm{clip}(\\left\\lfloor\\frac{\\widetilde{W}_{h}}{2^{\\Delta}}\\right\\rfloor,-2^{l-1},2^{l-1}-1)}}\\\\ {{\\widehat{W}_{l}=\\widetilde{W}_{l}\\times\\mathbf{s}_{h}\\times2^{\\Delta}+\\mathbf{z}_{h}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "135 where the symbol $\\lfloor.\\rceil$ denotes the Rounding function, and $\\mathrm{clip}(x,l o w,u p p e r)$ means $x$ is limited   \n136 to the range between low and upper. Here, $W$ represents the FP32 model\u2019s weights, $\\mathbf{s}_{h}\\,\\in\\,\\mathbb{R}$   \n137 and $\\mathbf{z}_{h}\\in\\mathbb{Z}$ denote the highest-bit (e.g., 8-bit) quantization scale and zero-point respectively. $\\widetilde{W}_{h}$   \n138 represent the quantized weights of the highest-bit, whileW l andW l represent the quantized weights   \n139 and dequantized weights of the low-bit respectively.   \n140 Hardware shift operations can efficiently execute the division and multiplication by $2^{\\Delta}$ . Note that in   \n141 our Double Rounding, the model can also be saved at full precision by using unshared quantization   \n142 parameters to run bit-switching and attain higher accuracy. Because we use symmetric quantization   \n143 scheme, the $\\mathbf{z}_{h}$ is 0. Please refer to Section A.4 for the gradient formulation of Double Rounding.   \n144 Unlike fixed weights, activations change online during inference. So, the corresponding scale and   \n145 zero-point values for different precisions can be learned individually to increase overall accuracy.   \n146 Suppose $X$ denotes the full precision activation, and $\\widetilde{X_{b}}$ andXb are the quantized activation and   \n147 dequantized activation respectively. The quantization p rocess c an be formulated as follows: ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\widetilde{X_{b}}=\\mathrm{clip}(\\left\\lfloor\\frac{X-{\\bf z}_{b}}{{\\bf s}_{b}}\\right\\rceil,0,2^{b}-1)}}\\\\ {{\\widehat{X_{b}}=\\widetilde{X_{b}}\\times{\\bf s}_{b}+{\\bf z}_{b}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "148 where $\\mathbf{s}_{b}\\in\\mathbb{R}$ and $\\mathbf{z_{b}}\\in\\mathbb{Z}$ represent the quantization scale and zero-point of different bit-widths   \n149 activation respectively. Note that $\\mathbf{z}_{b}$ is 0 for the ReLU activation function. ", "page_idx": 3}, {"type": "text", "text": "150 3.2 Adaptive Learning Rate Scaling for Multi-Precision ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "151 Although our proposed Double Rounding method represents a significant improvement over most   \n152 previous multi-precision works, the one-shot joint optimization of multiple precisions remains   \n153 constrained by severe competition between the highest and lowest precisions [10; 4]. Different   \n154 precisions simultaneously impact each other during joint training, resulting in substantial differences   \n155 in convergence rates between them, as shown in Figure 3 (c). We experimentally find that this   \n156 competitive relationship stems from the inconsistent magnitudes of the quantization scale\u2019s gradients   \n157 between high-bit and low-bit quantization during joint training, as shown in Figure 3 (a) and (b). For   \n158 other models statistical results please refer to Section A.6 in the appendix.   \n159 Motivated by these observations, we introduce a technique termed Adaptive Learning Rate Scaling   \n160 (ALRS), which dynamically adjusts learning rates for different precisions to optimize the training   \n161 process. This technique is inspired by the Layer-wise Adaptive Rate Scaling (LARS) [25] optimizer.   \n162 Specifically, suppose the current batch iteration\u2019s learning rate is $\\lambda$ , we set learning rates $\\lambda_{b}$ of   \n163 different precisions as follows: ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "image", "img_path": "kOp0kiXZ3a/tmp/d59dd0580bcfcc55560ad5338d9b0fc5862c5762393ae45c0ed51b5584abbbb8.jpg", "img_caption": ["Figure 3: The statistics of ResNet18 on ImageNet-1K dataset. (a) and (b): The quantization scale gradients\u2019 statistics for the weights, with outliers removed for clarity. (c) and (d): The multi-precision training processes of our Double Rounding without and with the ALRS strategy. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\lambda_{b}=\\eta_{b}\\left(\\lambda-\\displaystyle\\sum_{i=1}^{L}\\frac{\\operatorname*{min}\\left(\\operatorname*{max}_{-}\\mathrm{abs}\\left(\\mathrm{clip}_{-}\\mathrm{grad}\\left(\\nabla\\mathbf{s}_{b}^{i},1.0\\right)\\right),1.0\\right)}{L}\\right),}\\\\ &{\\eta_{b}=\\left\\{1\\times10^{-\\frac{\\Delta}{2}},\\quad\\quad\\quad\\mathrm{if}\\;\\Delta\\mathrm{~is~even}\\right.}\\\\ &{\\eta_{b}=\\left\\{5\\times10^{-(\\frac{\\Delta+1}{2})},\\quad\\mathrm{if}\\;\\Delta\\mathrm{~is~odd}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "164 where the $L$ is the number of layers, clip_grad $(.)$ represents gradient clipping that prevents gradient   \n165 explosion, max_abs $(.)$ denotes the maximum absolute value of all elements. The $\\ '\\mathbf{\\mathbf{V}}\\mathbf{s}_{b}^{i}$ denotes the   \n166 quantization scale\u2019s gradients of layer $i$ and $\\eta_{b}$ denotes scaling hyperparameter of different precisions,   \n167 e.g., 8-bit is 1, 6-bit is 0.1, and 4-bit is 0.01. Note that the ALRS strategy is only used for updating   \n168 quantization scales. It can adaptively update the learning rates of different precisions and ensure   \n169 that model can optimize quantization parameters at the same pace, ultimately achieving a minimal   \n170 convergence gap in higher bits and 2-bit, as shown in Figure 3 (d).   \n171 In multi-precision scheme, different precisions share the same model weights during joint training.   \n172 For conventional multi-precision, the shared weight computes $n$ forward processes at each training   \n173 iteration, where $n$ is the number of candidate bit-widths. The losses attained from different precisions   \n174 are then accumulated, and the gradients are computed. Finally, the shared parameters are updated.   \n175 For detailed implementation please refer to Algorithm A.1 in the appendix. However, we find that   \n176 if different precision losses separately compute gradients and directly update shared parameters at   \n177 each forward process, it attains better accuracy when combined with our ALRS training strategy.   \n178 Additionally, we use dual optimizers to update the weight parameters and quantization parameters   \n179 simultaneously. We also set the weight-decay of the quantization scales to 0 to achieve stable   \n180 convergence. For detailed implementation please refer to Algorithm A.2 in the appendix. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "181 3.3 One-Shot Mixed-Precision SuperNet ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "182 Unlike multi-precision, where all layers uniformly utilize the same bit-width, mixed-precision   \n183 SuperNet provides finer-grained adaptive by configuring the bit-width at different layers. Previous   \n184 methods typically decouple the training and search stages, which need a third stage for retraining   \n185 or fine-tuning the searched SubNets. These approaches generally incur substantial search costs in   \n186 selecting the optimal SubNets, often employing methods such as greedy algorithms [26; 9] or genetic   \n187 algorithms [27; 4]. Considering the fact that the sensitivity [28], i.e., importance, of each layer   \n188 is different, we propose a Hessian-Aware Stochastic Bit-switching (HASB) strategy for one-shot   \n189 mixed-precision training.   \n190 Specifically, the Hessian Matrix Trace (HMT) is utilized to measure the sensitivity of each layer. We   \n191 first need to compute the pre-trained model\u2019s HMT by around 1000 training images [11], as shown in   \n192 Figure 4 (c). Then, the HMT of different layers is utilized as the probability metric for bit-switching.   \n193 Higher bits are priority selected for sensitive layers, while all candidate bits are equally selected for   \n194 unsensitive layers. Our proposed Roulette algorithm is used for bit-switching processes of different   \n195 layers during training, as shown in the Algorithm 1. If a layer\u2019s HMT exceeds the average HMT of   \n196 all layers, it is recognized as sensitive, and the probability distribution of Figure 4 (b) is used for bit   \n197 selection. Conversely, if the HMT is below the average, the probability distribution of Figure 4 (a) is   \n198 used for selection. Finally, the Integer Linear Programming (ILP) [29] algorithm is employed to find   \n99 the optimal SubNets. Considering each layer\u2019s sensitivity during training and adding this sensitivity   \n200 to the ILP\u2019s constraint factors (e.g., model\u2019s FLOPs, latency, and parameters), which depend on   \n201 the actual deployment requirements. We can efficiently attain a set of optimal SubNets during the   \n202 search stage without retraining, thereby significant reduce the overall costs. All the searched SubNets   \n203 collectively constitute the Pareto Frontier optimal solution, as shown in Figure 4 (d). For detailed   \n204 mixed-precision training and searching process (i.e., ILP) please refer to the Algorithm A.3 and the   \nAlgorithm 2 respectively. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "image", "img_path": "kOp0kiXZ3a/tmp/e557750ffecd45ca9c4212cfcffb4603e570c6799ce80b9f20bde5b2415a525d.jpg", "img_caption": ["Figure 4: The HASB stochastic process and Mixed-precision of ResNet18 for {2,4,6,8}-bit. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "image", "img_path": "kOp0kiXZ3a/tmp/294158557d3305a65488069b2e61fbc6120ba4d9df579a5c144c134a945a1525.jpg", "img_caption": [], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "205 ", "page_idx": 5}, {"type": "text", "text": "206 4 Experimental Results ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "207 Setup. In this paper, we mainly focus on ImageNet-1K [30] classification task using both classical   \n208 networks (ResNet18/50 [14]) and lightweight networks (MobileNetV2 [31]), which same as previous   \n209 works. Experiments cover joint quantization training for multi-precision and mixed precision. We   \n210 explore two candidate bit configurations, i.e., {8,6,4,2}-bit and {4,3,2}-bit, each number represents   \n211 the quantization level of the weight and activation layers. Like previous methods, we exclude batch   \n212 normalization layers from quantization, and the first and last layers are kept at full precision. We   \n213 initialize the multi-precision models with a pre-trained FP32 model, and initialize the mixed-precision   \n214 models with a pre-trained multi-precision model. All models use the Adam optimizer [32] with a batch   \n215 size of 256 for 90 epochs and use a cosine scheduler without warm-up phase. The initial learning   \n216 rate is 5e-4 and weight decay is 5e-5. Data augmentation uses the standard set of transformations   \n217 including random cropping, resizing to $224\\!\\times\\!224$ pixels, and random filpping. Images are resized to   \n218 $256\\!\\times\\!256$ pixels and then center-cropped to $224\\!\\times\\!224$ resolution during evaluation. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "219 4.1 Multi-Precision ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "220 Results. For {8,6,4,2}-bit configuration, the Top-1 validation accuracy is shown in Table 1. The   \n221 network weights and the corresponding activations are quantized into w-bit and a-bit respectively.   \n222 Our double-rounding combined with ALRS training strategy surpasses the previous state-of-the-art   \n223 (SOTA) methods. For example, in ResNet18, it exceeds Any-Precision [6] by $2.7\\%$ (or $2.83\\%$ ) under   \n224 w8a8 setting without(or with) using KD technique [15], and outperforms MultiQuant [4] by $0.63\\%$ (or   \n225 $0.73\\%$ under w4a4 setting without(or with) using KD technique respectively. Additionally, when   \n226 the candidate bit-list includes 2-bit, the previous methods can\u2019t converge on MobileNetV2 during   \n227 training. So, they use {8,6,4}-bit precision for MobileNetV2 experiments. For consistency, we   \n228 also test $\\{8,6,4\\}$ -bit results, as shown in the \"Ours {8,6,4}-bit\" rows of Table 1. Our method achieves   \n229 $0.25\\%/0.11\\%/0.56\\%$ higher accuracy than AdaBits [3] under the w8a8/w6a6/w4a4 settings.   \n230 Notably, our method exhibits the ability to converge but shows a big decline in accuracy on Mo  \n231 bileNetV2. On the one hand, the compact model exhibits significant differences in the quantization   \n232 scale gradients of different channels due to involving DeepWise Convolution [33]. On the other hand,   \n233 when the bit-list includes 2-bit, it intensifies competition between different precisions during training.   \n234 To improve the accuracy of compact models, we suggest considering the per-layer or per-channel   \nlearning rate scaling techniques in future work.   \n236 For {4,3,2}-bit configuration, Table 2 demonstrate that our double-rounding consistently surpasses   \n237 previous SOTA methods. For instance, in ResNet18, it exceeds Bit-Mixer [9] by $0.63\\%/0.7\\%/1.2\\%$ (or   \n238 $0.37\\%/0.64\\%/1.02\\%)$ under $\\mathrm{w}4\\mathrm{a}4/\\mathrm{w}3\\mathrm{a}3/\\mathrm{w}2\\mathrm{a}2$ settings without(or with) using KD technique, and   \n239 outperforms ABN[10] by $0.87\\%/0.74\\%/1.12\\%$ under $\\mathrm{w}4\\mathrm{a}4/\\mathrm{w}3\\mathrm{a}3/\\mathrm{w}2\\mathrm{a}2$ settings with using KD   \n240 technique respectively. In ResNet50, Our method outperforms Bit-Mixer [9] by $\\bar{0.86}\\%/0.63\\%/0.1\\%$   \n241 under $\\mathrm{w}4\\mathrm{a}4/\\mathrm{w}3\\mathrm{a}3/\\mathrm{w}2\\mathrm{a}2$ settings.   \n242 Notably, the overall results of Table 2 are worse than the {8,6,4,2}-bit configuration for joint training.   \n243 We analyze that this discrepancy arises from information loss in the shared lower precision model   \n244 (i.e., 4-bit) used for bit-switching. In other words, compared with 4-bit, it is easier to directly optimize   \n245 8-bit quantization parameters to converge to the optimal value. So, we recommend including 8-bit for   \n246 multi-precision training. Furthermore, independently learning the quantization scales for different   \n247 precisions, including weights and activations, significantly improves accuracy compared to using   \n248 shared scales. However, it requires saving the model in 32-bit format, as shown in \"Ours\\*\" of Table 2. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "table", "img_path": "kOp0kiXZ3a/tmp/7d56bde9147bed7146a0723bcd543d5c8ea6c08e5353a64e6bdc417a3948aff8.jpg", "table_caption": ["Table 1: Top1 accuracy comparisons on multi-precision of {8,6,4,2}-bit on ImageNet-1K datasets. \u2019KD\u2019 denotes knowledge distillation. The \"\u2212\" represents the unqueried value. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "table", "img_path": "kOp0kiXZ3a/tmp/10f10b64c1c1a752d2da10bfba3c07aa24d01e1199dea214d6ed2845e00103cf.jpg", "table_caption": ["Table 2: Top1 accuracy comparisons on multi-precision of {4,3,2}-bit on ImageNet-1K datasets. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "249 4.2 Mixed-Precision ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "250 Results. We follow previous works to conduct mixed-precision experiments based on the {4,3,2}-bit   \n251 configuration. Our proposed one-shot mixed-precision joint quantization method with the HASB tech  \n252 nique comparable to the previous SOTA methods, as presented in Table 3. For example, in ResNet18,   \n253 our method exceeds Bit-Mixer [9] by $0.83\\%/0.72\\%/0.77\\%/7.07\\%$ under $\\mathrm{w4a4/w3a3/w2a2/3MP}$   \n254 settings and outperforms EQ-Net[5] by $0.2\\%$ under 3MP setting. The results demonstrate the effec  \n255 tiveness of one-shot mixed-precision joint training to consider sensitivity with Hessian Matrix Trace   \n256 when randomly allocating bit-widths for different layers. Additionally, Table 3 reveals that our results   \n257 do not achieve optimal performance across all settings. We hypothesize that extending the number of   \n258 training epochs or combining ILP with other efficient search methods, such as genetic algorithms,   \n259 may be necessary to achieve optimal results in mixed-precision optimization. ", "page_idx": 7}, {"type": "table", "img_path": "kOp0kiXZ3a/tmp/e2e0234bc6a4115762d240f61494afa6749fb7c03fe324a4634ed7e6d2bc7eb3.jpg", "table_caption": ["Table 3: Top1 accuracy comparisons on mixed-precision of {4,3,2}-bit on ImageNet-1K dataset. \"MP\" denotes average bit-width for mixed-precision. The \"\u2212\" represents the unqueried value. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "260 4.3 Ablation Studies ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "261 ALRS vs. Conventional in Multi-Precision. To verify the effectiveness of our proposed ALRS train  \n262 ing strategy, we conduct an ablation experiment without KD, as shown in Table 4, and observe overall   \n263 accuracy improvements, particularly for the 2bit. Like previous works, where MobileNetV2 can\u2019t   \n264 achieve stable convergence with {4,3,2}-bit, we also opt for $\\{8,6,4\\}$ -bit to keep consistent. However,   \n265 our method can achieve stable convergence with {8,6,4,2}-bit quantization. This demonstrates the   \n266 superiority of our proposed Double-Rounding and ALRS methods.   \n267 Multi-Precision vs. Separate-Precision in Time Cost. We statistic the results regarding the time cost   \n268 for multi-precision compared to separate-precision quantization, as shown in Table 5. Multi-precision   \n269 training costs stay approximate constant as the number of candidate bit-widths.   \n270 Pareto Frontier of Different Mixed-Precision Configurations. To verify the effectiveness of our   \n271 HASB strategy, we conduct ablation experiments on different bit-lists. Figure 5 shows the search   \n272 results of Mixed-precision SuperNet under {8,6,4,2}-bit, {4,3,2}-bit and {8,4}-bit configurations   \n273 respectively. Where each point represents a SubNet. These results are obtained directly from ILP   \n274 sampling without retraining or fine-tuning. As the figure shows, the highest red points are higher than   \n275 the blue points under the same bit width, indicating that this strategy is effective. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "table", "img_path": "kOp0kiXZ3a/tmp/9772f5a4b601ac7770fe1c405ce9cbecd5d0edfd73eb1eac9c097a8b905d9948.jpg", "table_caption": ["Table 4: Ablation studies of multi-precision, ResNet20 on CIFAR-10 dataset and other models on ImageNet-1K dataset. Note that MobileNetV2 uses {8,6,4}-bit instead of {4,3,2}-bit. "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "kOp0kiXZ3a/tmp/6a5c479a0119421e98b6a2b4cfe758730f7287e3ef0e293ea268fc56264de507.jpg", "table_caption": ["Table 5: Training costs for multi-precision and separate-precision are averaged over three runs. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "image", "img_path": "kOp0kiXZ3a/tmp/183f6c2d8b693577dc8743fc36a6a3b1058063c49f061b282f860d502b8c5d3c.jpg", "img_caption": ["Figure 5: Comparison of HASB and Baseline approaches for Mixed-Precision on ResNet18. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "276 5 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "277 This paper first introduces Double Rounding quantization method used to address the challenges   \n278 of multi-precision and mixed-precision joint training. It can store single integer-weight parameters   \n279 and attain nearly lossless bit-switching. Secondly, we propose an Adaptive Learning Rate Scaling   \n280 (ALRS) method for multi-precision joint training that narrows the training convergence gap between   \n281 high-precision and low-precision, enhancing model accuracy of multi-precision. Finally, our proposed   \n282 Hessian-Aware Stochastic Bit-switching (HASB) strategy for one-shot mixed-precision SuperNet   \n283 and efficient searching method combined with Integer Linear Programming, achieving approximate   \n284 Pareto Frontier optimal solution. Our proposed methods aim to achieve a flexible and effective model   \n285 compression technique for adapting different storage and computation requirements. ", "page_idx": 8}, {"type": "text", "text": "286 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "287 [1] S. Zhou, Y. Wu, Z. Ni, X. Zhou, H. Wen, and Y. Zou, \u201cDorefa-net: Training low bitwidth convolutional   \n288 neural networks with low bitwidth gradients,\u201d arXiv preprint arXiv:1606.06160, 2016.   \n289 [2] S. K. Esser, J. L. McKinstry, D. Bablani, R. Appuswamy, and D. S. Modha, \u201cLearned step size quantization,\u201d   \n290 arXiv preprint arXiv:1902.08153, 2019.   \n291 [3] Q. Jin, L. Yang, and Z. Liao, \u201cAdabits: Neural network quantization with adaptive bit-widths,\u201d in Proceed  \n292 ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 2146\u20132156.   \n293 [4] K. Xu, Q. Feng, X. Zhang, and D. Wang, \u201cMultiquant: Training once for multi-bit   \n294 quantization of neural networks,\u201d in IJCAI, L. D. Raedt, Ed. International Joint Conferences   \n295 on Artificial Intelligence Organization, 7 2022, pp. 3629\u20133635, main Track. [Online]. Available:   \n296 https://doi.org/10.24963/ijcai.2022/504   \n297 [5] K. Xu, L. Han, Y. Tian, S. Yang, and X. Zhang, \u201cEq-net: Elastic quantization neural networks,\u201d in   \n298 Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023, pp. 1505\u20131514.   \n299 [6] H. Yu, H. Li, H. Shi, T. S. Huang, and G. Hua, \u201cAny-precision deep neural networks,\u201d in Proceedings of   \n300 the AAAI Conference on Artificial Intelligence, vol. 35, no. 12, 2021, pp. 10 763\u201310 771.   \n301 [7] K. Du, Y. Zhang, and H. Guan, \u201cFrom quantized dnns to quantizable dnns,\u201d CoRR, vol. abs/2004.05284,   \n302 2020. [Online]. Available: https://arxiv.org/abs/2004.05284   \n303 [8] X. Sun, R. Panda, C.-F. R. Chen, N. Wang, B. Pan, A. Oliva, R. Feris, and K. Saenko, \u201cImproved techniques   \n304 for quantizing deep networks with adaptive bit-widths,\u201d in Proceedings of the IEEE/CVF Winter Conference   \n305 on Applications of Computer Vision, 2024, pp. 957\u2013967.   \n306 [9] A. Bulat and G. Tzimiropoulos, \u201cBit-mixer: Mixed-precision networks with runtime bit-width selection,\u201d   \n307 in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 5188\u20135197.   \n308 [10] C. Tang, H. Zhai, K. Ouyang, Z. Wang, Y. Zhu, and W. Zhu, \u201cArbitrary bit-width network:   \n309 A joint layer-wise quantization and adaptive inference approach,\u201d 2022. [Online]. Available:   \n310 https://arxiv.org/abs/2204.09992   \n311 [11] Z. Dong, Z. Yao, D. Arfeen, A. Gholami, M. W. Mahoney, and K. Keutzer, \u201cHawq-v2: Hessian aware   \n312 trace-weighted quantization of neural networks,\u201d Advances in neural information processing systems,   \n313 vol. 33, pp. 18 518\u201318 529, 2020.   \n314 [12] Y. Dong, R. Ni, J. Li, Y. Chen, H. Su, and J. Zhu, \u201cStochastic quantization for learning accurate low-bit   \n315 deep neural networks,\u201d International Journal of Computer Vision, vol. 127, pp. 1629\u20131642, 2019.   \n316 [13] Q. Jin, L. Yang, and Z. Liao, \u201cTowards efficient training for neural network quantization,\u201d arXiv preprint   \n317 arXiv:1912.10207, 2019.   \n318 [14] K. He, X. Zhang, S. Ren, and J. Sun, \u201cDeep residual learning for image recognition,\u201d CoRR, vol.   \n319 abs/1512.03385, 2015. [Online]. Available: http://arxiv.org/abs/1512.03385   \n320 [15] K. Kim, B. Ji, D. Yoon, and S. Hwang, \u201cSelf-knowledge distillation with progressive refinement of targets,\u201d   \n321 in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 6567\u20136576.   \n322 [16] K. Wang, Z. Liu, Y. Lin, J. Lin, and S. Han, \u201cHaq: Hardware-aware automated quantization with mixed   \n323 precision,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,   \n324 2019, pp. 8612\u20138620.   \n325 [17] A. Elthakeb, P. Pilligundla, F. Mireshghallah, A. Yazdanbakhsh, S. Gao, and H. Esmaeilzadeh, \u201cReleq: an   \n326 automatic reinforcement learning approach for deep quantization of neural networks,\u201d in NeurIPS ML for   \n327 Systems workshop, 2018, 2019.   \n328 [18] B. Wu, Y. Wang, P. Zhang, Y. Tian, P. Vajda, and K. Keutzer, \u201cMixed precision quantization of convnets   \n329 via differentiable neural architecture search,\u201d arXiv preprint arXiv:1812.00090, 2018.   \n330 [19] Z. Guo, X. Zhang, H. Mu, W. Heng, Z. Liu, Y. Wei, and J. Sun, \u201cSingle path one-shot neural architecture   \n331 search with uniform sampling,\u201d in Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow,   \n332 UK, August 23\u201328, 2020, Proceedings, Part XVI 16. Springer, 2020, pp. 544\u2013560.   \n333 [20] M. Shen, F. Liang, R. Gong, Y. Li, C. Li, C. Lin, F. Yu, J. Yan, and W. Ouyang, \u201cOnce quantization-aware   \n334 training: High performance extremely low-bit architecture search,\u201d in Proceedings of the IEEE/CVF   \n335 International Conference on Computer Vision (ICCV), October 2021, pp. 5340\u20135349.   \n336 [21] J. Liu, J. Cai, and B. Zhuang, \u201cSharpness-aware quantization for deep neural networks,\u201d arXiv preprint   \n337 arXiv:2111.12273, 2021.   \n338 [22] Z. Yao, Z. Dong, Z. Zheng, A. Gholami, J. Yu, E. Tan, L. Wang, Q. Huang, Y. Wang, M. Mahoney   \n339 et al., \u201cHawq-v3: Dyadic neural network quantization,\u201d in International Conference on Machine Learning.   \n340 PMLR, 2021, pp. 11 875\u201311 886.   \n341 [23] B. Jacob, S. Kligys, B. Chen, M. Zhu, M. Tang, A. G. Howard, H. Adam, and D. Kalenichenko,   \n342 \u201cQuantization and training of neural networks for efficient integer-arithmetic-only inference,\u201d CoRR, vol.   \n343 abs/1712.05877, 2017. [Online]. Available: http://arxiv.org/abs/1712.05877   \n344 [24] Y. Bengio, N. L\u00e9onard, and A. Courville, \u201cEstimating or propagating gradients through stochastic neurons   \n345 for conditional computation,\u201d arXiv preprint arXiv:1308.3432, 2013.   \n346 [25] Y. You, I. Gitman, and B. Ginsburg, \u201cLarge batch training of convolutional networks,\u201d arXiv preprint   \n347 arXiv:1708.03888, 2017.   \n348 [26] Z. Cai and N. Vasconcelos, \u201cRethinking differentiable search for mixed-precision neural networks,\u201d in   \n349 Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 2349\u2013   \n350 2358.   \n351 [27] Z. Guo, X. Zhang, H. Mu, W. Heng, Z. Liu, Y. Wei, and J. Sun, \u201cSingle path one-shot neural architecture   \n352 search with uniform sampling,\u201d in European conference on computer vision. Springer, 2020, pp. 544\u2013560.   \n353 [28] Z. Dong, Z. Yao, A. Gholami, M. W. Mahoney, and K. Keutzer, \u201cHawq: Hessian aware quantization of   \n354 neural networks with mixed-precision,\u201d in Proceedings of the IEEE/CVF International Conference on   \n355 Computer Vision, 2019, pp. 293\u2013302.   \n356 [29] Y. Ma, T. Jin, X. Zheng, Y. Wang, H. Li, Y. Wu, G. Jiang, W. Zhang, and R. Ji, \u201cOmpq: Orthogonal mixed   \n357 precision quantization,\u201d in Proceedings of the AAAI conference on artificial intelligence, vol. 37, no. 7,   \n358 2023, pp. 9029\u20139037.   \n359 [30] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, \u201cImagenet: A large-scale hierarchical image   \n360 database,\u201d in 2009 IEEE conference on computer vision and pattern recognition. Ieee, 2009, pp. 248\u2013255.   \n361 [31] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L.-C. Chen, \u201cMobilenetv2: Inverted residuals and   \n362 linear bottlenecks,\u201d in Proceedings of the IEEE conference on computer vision and pattern recognition,   \n363 2018, pp. 4510\u20134520.   \n364 [32] D. P. Kingma and J. Ba, \u201cAdam: A method for stochastic optimization,\u201d arXiv preprint arXiv:1412.6980,   \n365 2014.   \n366 [33] T. Sheng, C. Feng, S. Zhuo, X. Zhang, L. Shen, and M. Aleksic, \u201cA quantization-friendly separable   \n367 convolution for mobilenets,\u201d in 2018 1st Workshop on Energy Efficient Machine Learning and Cognitive   \n368 Computing for Embedded Applications (EMC2). IEEE, 2018, pp. 14\u201318.   \n369 [34] Q. Sun, X. Li, Y. Ren, Z. Huang, X. Liu, L. Jiao, and F. Liu, \u201cOne model for all quantization: A quantized   \n370 network supporting hot-swap bit-width adjustment,\u201d arXiv preprint arXiv:2105.01353, 2021.   \n371 [35] M. Alizadeh, A. Behboodi, M. van Baalen, C. Louizos, T. Blankevoort, and M. Welling, \u201cGradient l1   \n372 regularization for quantization robustness,\u201d arXiv preprint arXiv:2002.07520, 2020.   \n373 [36] B. Chmiel, R. Banner, G. Shomron, Y. Nahshan, A. Bronstein, U. Weiser et al., \u201cRobust quantization: One   \n374 model to rule them all,\u201d Advances in neural information processing systems, vol. 33, pp. 5308\u20135317, 2020.   \n375 [37] H. Wu, R. He, H. Tan, X. Qi, and K. Huang, \u201cVertical layering of quantized neural networks for heteroge  \n376 neous inference,\u201d IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 45, no. 12, pp.   \n377 15 964\u201315 978, 2023.   \n378 [38] Y. Bhalgat, J. Lee, M. Nagel, T. Blankevoort, and N. Kwak, \u201cLsq+: Improving low-bit quantization through   \n379 learnable offsets and better initialization,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision   \n380 and Pattern Recognition Workshops, 2020, pp. 696\u2013697.   \n381 [39] J. Yu, L. Yang, N. Xu, J. Yang, and T. Huang, \u201cSlimmable neural networks,\u201d arXiv preprint   \n382 arXiv:1812.08928, 2018. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "383 A Appendix / supplemental material ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "384 A.1 Overview ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "385 In this supplementary material, we present more explanations and experimental results. ", "page_idx": 11}, {"type": "text", "text": "386 \u2022 First, we provide a detailed explanation of the different quantization types under QAT.   \n387 \u2022 We then present a comparison of multi-precision and separate-precision on the ImageNet-1k dataset.   \n388 \u2022 Furthermore, we provide the gradient formulation of Double Rounding.   \n389 \u2022 And, the algorithm implementation of both multi-precision and mixed-precision training approaches.   \n390 \u2022 Finally, we provide more gradient statistics of learnable quantization scales in different networks. ", "page_idx": 11}, {"type": "text", "text": "391 A.2 Different Quantization Types ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "392 In this section, we provide a detailed explanation of the different quantization types during Quantization-Aware Training (QAT), as is shown in Figure 6. ", "page_idx": 11}, {"type": "image", "img_path": "kOp0kiXZ3a/tmp/1cbe9666bc3078d03e5d4eba256e6fadf7b65828206da5f63ad0a498fb9a2721.jpg", "img_caption": [], "img_footnote": [], "page_idx": 11}, {"type": "text", "text": "Figure 6: Comparison between different quantization types during quantization-aware training. 393 ", "page_idx": 11}, {"type": "text", "text": "394 A.3 Multi-Precision vs. Separate-Precision. ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "395 We provide the comparison of Multi-Precision and Separate-Precision on ImageNet-1K dataset. 396 Table 6 shows that our Multi-Precision joint training scheme has comparable accuracy of different 397 precisions compared to Separate-Precision with multiple re-train. This further proves the effectiveness of our proposed One-shot Double Rounding Multi-Precision method. ", "page_idx": 11}, {"type": "text", "text": "Table 6: Top1 accuracy comparisons on multi-precision of {8,6,4,2}-bit on ImageNet-1K datasets. ", "page_idx": 11}, {"type": "table", "img_path": "kOp0kiXZ3a/tmp/f839785d63637ae7c53622fd7204ebf379a3ed34d90f4194f0594e231a1872a6.jpg", "table_caption": [], "table_footnote": [], "page_idx": 11}, {"type": "text", "text": "398 ", "page_idx": 11}, {"type": "text", "text": "399 A.4 The Gradient Formulation of Double Rounding ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "400 A general formulation for uniform quantization process is as follows: ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\widetilde{W}=\\displaystyle\\mathrm{clip}(\\left\\lfloor\\frac{W}{\\mathsf{s}}\\right\\rceil+\\mathbf{z},-2^{b-1},2^{b-1}-1)}}\\\\ {{\\widehat{W}=(\\widetilde{W}-\\mathbf{z})\\times\\mathbf{s}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "401 where the symbol \u230a.\u2309denotes the Rounding function, $\\mathrm{clip}(x,l o w,u p p e r)$ expresses $x$ below low   \n402 are set to low and above upper are set to upper. $b$ denotes the quantization level (or bit-width),   \n403 $\\mathbf{s}\\in\\mathbb{R}$ and $\\mathbf{z}\\in\\mathbb{Z}$ represents the quantization scale (or interval) and zero-point associated with each $b$ ,   \n404 respectively. $W$ represents the FP32 model\u2019s weights,W  signifies the quantized integer weights, and   \n405 $\\widehat{W}$ represents the dequantized floating-point weights.   \n406 The quantization scale of our Double Rounding is learned online and not fixed. And it only needs a   \n407 pair of shared quantization parameters, i.e., scale and zero-point. Suppose the highest-bit and the   \n408 low-bit are denoted as $h$ -bit and $l$ -bit respectively, and the difference between them is $\\Delta=h-l$ .   \n409 The specific formulation is as follows: ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\widetilde{W}_{h}=\\mathrm{clip}(\\left\\lfloor\\frac{W-\\mathbf{z}_{h}}{\\mathbf{s}_{h}}\\right\\rfloor,-2^{h-1},2^{h-1}-1)}}\\\\ {{\\widetilde{W}_{l}=\\mathrm{clip}(\\left\\lfloor\\frac{\\widetilde{W}_{h}}{2^{\\Delta}}\\right\\rfloor,-2^{l-1},2^{l-1}-1)}}\\\\ {{\\widehat{W}_{l}=\\widetilde{W}_{l}\\times\\mathbf{s}_{h}\\times2^{\\Delta}+\\mathbf{z}_{h}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "410 where $\\mathbf{s}_{h}\\in\\mathbb{R}$ and $\\mathbf{z}_{h}\\in\\mathbb{Z}$ denote the highest-bit quantization scale and zero-point respectively. $\\widetilde{W_{h}}$   \n411 and $\\widetilde{W_{l}}$ represent the quantized weights of the highest-bit and low-bit respectively. Hardware shift   \n412 ope rations can efficiently execute the division and multiplication by $2^{\\Delta}$ . And the $\\mathbf{z}_{h}$ is 0 for the   \n413 weight quantization in this paper. The gradient formulation of Double Rounding for one-shot joint   \n414 training is represented as follows: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\frac{\\partial\\widehat{Y}}{\\partial\\mathbf{s}_{h}}\\simeq\\left\\lbrace\\frac{\\left|\\frac{Y-\\mathbf{z}_{h}}{\\mathbf{s}_{h}}\\right|-\\frac{Y-\\mathbf{z}_{h}}{\\mathbf{s}_{h}}\\quad i f\\,n<\\frac{Y-\\mathbf{z}_{h}}{\\mathbf{s}_{h}}<p,}{n}\\right.}\\\\ {\\displaystyle n\\quad o r\\quad p\\qquad\\qquad\\left.o t h e r w i s e.\\right.}\\\\ {\\displaystyle\\frac{\\partial\\widehat{Y}}{\\partial\\mathbf{z}_{h}}\\simeq\\left\\lbrace0\\quad i f\\,n<\\frac{Y-\\mathbf{z}_{h}}{\\mathbf{s}_{h}}<p,}\\\\ {\\displaystyle1\\quad o t h e r w i s e.}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "415 where $n$ and $p$ denote the lower and upper bounds of the integer range $[N_{m i n},N_{m a x}]$ for quantizing   \n416 the weights or activations respectively. $Y$ represents the FP32 weights or activations, andY represents   \n417 the dequantized weights or activations. Unlike weights, activation quantization scale and zero-point   \n418 of different precisions are learned independently. However, the gradient formulation is the same. ", "page_idx": 12}, {"type": "text", "text": "419 A.5 Algorithms ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "420 This section provides the algorithm implementations of multi-precision, one-shot mixed-precision   \n421 joint training, and the search stage of SubNets. ", "page_idx": 12}, {"type": "text", "text": "422 A.5.1 Multi-Precision Joint Training ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "423 The multi-precision model with different quantization precisions shares the same model weight(e.g.,   \n424 the highest-bit) during joint training. In conventional multi-precision, the shared weight (e.g., multi  \n425 precision model) computes $n$ forward processes at each training iteration, where $n$ is the number of   \n426 candidate bit-widths. Then, all attained losses of different precisions perform an accumulation, and   \n427 update the parameters accordingly. For specific implementation details please refer to Algorithm A.1.   \n428 However, we find that if separate precision loss and parameter updates are performed directly after   \n429 calculating a precision at each forward process, it will lead to difficulty convergence during training   \n430 or suboptimal accuracy. In other words, the varying gradient magnitudes of quantization scales of   \n431 different precisions make it hard to attain stable convergence during joint training. To address this   \n432 issue, we introduce an adaptive approach (e.g., Adaptive Learning Rate Scaling, ALRS) to alter the   \n433 learning rate for different precisions during training, aiming to achieve a consistent update pace.   \n434 This method allows us to directly update the shared parameters after calculating the loss after every   \n435 forward. We update both the weight parameters and quantization parameters simultaneously using   \n436 dual optimizers. We also set the weight-decay of the quantization scales to 0 to achieve more stable   \n437 convergence. For specific implementation details, please refer to Algorithm A.2. ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Algorithm A.1 Conventional Multi-precision training approach ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Require: Candidate bit-widths set $b\\in B$ ;   \n1: Initialize: Pretrained model $M$ with FP32 weights $W$ , the quantization scales s including of weights $\\mathbf{s}_{w}$   \nand activations $\\mathbf{s}_{x}$ , BatchNorm layers: $\\{B N\\}_{b=1}^{n}$ , optimizer:optim $\\mathbf{\\Psi}_{!}(W,\\mathbf{s},w d)$ , learning rate: $\\lambda,w d$ : weight   \ndecay, $C E$ : CrossEntropyLoss, $D_{t r a i n}$ : training dataset;   \n2: For one epoch:   \n3: Sample mini-batch data $(\\mathbf{x},\\mathbf{y})\\in\\{D_{t r a i n}\\}$   \n4: for $^b$ in $B$ do   \n5: forwar $l(M,\\mathbf{x},\\mathbf{y},b)$ :   \n6: for each quantization layer do   \n7: $\\begin{array}{l l}{\\widehat{W}^{b}=\\overset{\\cdot}{d e q u a n t}(q u a n t(W,\\mathbf{s}_{w}^{b}))}\\\\ {\\widehat{X}^{b}=d e q u a n t(q u a n t(X,\\mathbf{s}_{x}^{b}))}\\\\ {O^{b}=C o n v(\\widehat{W}^{b},\\widehat{X}^{b})}\\end{array}$   \n8:   \n9:   \n10: end for   \n11: ${\\bf o}^{b}=F C(W,O^{b})$   \n12: Update $B N^{b}$ layer   \n13: Compute loss: $\\check{\\boldsymbol{\\mathcal{L}}}^{b}=C E(\\mathbf{o}^{b},\\mathbf{y})$   \n14: Compute gradients: ${\\mathcal{L}}^{b}$ .backward()   \n15: end for   \n16: Update weights and scales: optim.step(\u03bb)   \n17: Clear gradient: optim.zero_grad();   \nNote that $_n$ and $L$ represent the number of candidate bit-widths and model layers respectively. ", "page_idx": 13}, {"type": "text", "text": "Algorithm A.2 Our Multi-precision training approach ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Require: Candidate bit-widths set $b\\in B$   \n1: Initialize: Pretrained model $M$ with FP32 weights $W$ , the quantization scales s including of weights $\\mathbf{s}_{w}$ and   \nactivations $\\mathbf{s}_{x}$ , BatchNorm layers: $\\{B N\\}_{b=1}^{n}$ , optimizers: o $p t i m_{1}(W,w d)$ , $o p t i m_{2}(\\mathbf{s},w d=0)$ , learning   \nrate: $\\lambda,w d$ : weight decay, $C E$ : CrossEntropyLoss, $D_{t r a i n}$ : training dataset;   \n2: For every epoch:   \n3: Sample mini-batch data $(\\mathbf x,\\mathbf y)\\in\\{D_{t r a i n}\\}$   \n4: for $b$ in $B$ do   \n5: forwar $l(M,\\mathbf{x},\\mathbf{y},b)$ :   \n6: for each quantization layer do   \n7: $\\begin{array}{r l}&{\\widehat{W}^{b}=\\overset{\\cdot}{d e q u a n t}(q u a n t(W,\\mathbf{s}_{w}^{b}))}\\\\ &{\\widehat{X}^{b}=d e q u a n t(q u a n t(X,\\mathbf{s}_{x}^{b}))}\\\\ &{O^{b}=C o n v(\\widehat{W}^{b},\\widehat{X}^{b})}\\end{array}$   \n8:   \n9:   \n10: end for   \n11: $\\mathbf{o}^{b}=F C(W,O^{b})$   \n12: Update $B N^{b}$ layer   \n13: Compute loss: $\\check{\\boldsymbol{\\mathcal{L}}}^{b}=C E(\\mathbf{o}^{b},\\mathbf{y})$   \n14: Compute gradients: ${\\mathcal{L}}^{b}$ .backward()   \n15: Compute learning rate: $\\lambda_{b}$ # please see formula (6) of the main paper   \n16: Update weights and quantization scales: optim1.step(\u03bb); optim2.step $\\left(\\lambda_{b}\\right)$   \n17: Clear gradient: optim1.zero_grad(); optim2.zero_grad()   \n18: end for   \nNote that $_n$ and $L$ represent the number of candidate bit-widths and model layers respectively. ", "page_idx": 13}, {"type": "text", "text": "438 A.5.2 One-shot Joint Training for Mixed Precision SuperNet ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "439 Unlike multi-precision joint quantization, the bit-switching of mixed-precision training is more   \n440 complicated. In multi-precision training, the bit-widths calculated in each iteration are fixed, e.g.,   \n441 {8,6,4,2}-bit. In mixed-precision training, the bit-widths of different layers are not fixed in each   \n442 iteration, e.g., {8,random-bit,2}-bit, where \"random-bit\" is any bits of e.g., {7,6,5,4,3,2}-bit, similar   \n443 to the sandwich strategy of [39]. Therefore, mixed precision training often requires more training   \n444 epochs to reach convergence compared to multi-precision training. Bit-mixer [9] conducts the same   \n445 probability of selecting bit-width for different layers. However, we take the sensitivity of each layer   \n446 into consideration which uses sensitivity (e.g. Hessian Matrix Trace [11]) as a metric to identify the   \n447 selection probability of different layers. For more sensitive layers, preference is given to higher-bit   \n448 widths, and vice versa. We refer to this training strategy as a Hessian-Aware Stochastic Bit-switching   \n449 (HASB) strategy for optimizing one-shot mixed-precision SuperNet. Specific implementation details   \n450 can be found in Algorithm A.3. In additionally, unlike multi-precision joint training, the BN layers   \n451 are replaced by TBN (Transitional Batch-Norm) [9], which compensates for the distribution shift   \n452 between adjacent layers that are quantized to different bit-widths. To achieve the best convergence   \n453 effect, we propose that the threshold of bit-switching $(i.e.,\\sigma)$ also increases as the epoch increases. ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "Algorithm A.3 Our one-shot Mixed-precision SuperNet training approach ", "text_level": 1, "page_idx": 14}, {"type": "image", "img_path": "kOp0kiXZ3a/tmp/606d508e64233e8074121cfea25a8aa8cf6652f3f239dd1b6137eed02c354e28.jpg", "img_caption": [], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "454 A.5.3 Efficient one-shot searching for Mixed Precision SuperNet ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "455 After training the mixed-precision SuperNet, the next step is to select the appropriate optimal SubNets   \n456 based on conditions, such as model parameters, latency, and FLOPs, for actual deployment and   \n457 inference. To achieve optimal allocations for candidate bit-width under given conditions, we employ   \n458 the Iterative Integer Linear Programming (ILP) approach. Since each ILP run can only provide   \n459 one solution, we obtain multiple solutions by altering the values of different average bit widths.   \n460 Specifically, given a trained SuperNet (e.g., RestNet18), it takes less than two minutes to solve   \n461 candidate SubNets. It can be implemented through the Python PULP package. Finally, these searched   \n462 SubNets only need inference to attain final accuracy, which needs a few hours. This forms a Pareto   \n463 optimal frontier. From this frontier, we can select the appropriate subnet for deployment. Specific   \n464 implementation details of the searching process by ILP can be found in Algorithm 2. ", "page_idx": 14}, {"type": "text", "text": "465 A.6 The Gradient Statistics of Learnable Scale of Quantization ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "466 In this section, we analyze the changes in gradients of the learnable scale for different models during   \n467 the training process. Figure 7 and Figure 8 display the gradient statistical results for ResNet20 on   \n468 CIFAR-10. Similarly, Figure 9 and Figure 10 show the gradient statistical results for ResNet18 on   \n469 ImageNet-1K, and Figure 11 and Figure 12 present the gradient statistical results for ResNet50 on   \n470 ImageNet-1K. These figures reveal a similarity in the range of gradient changes between higher-bit   \n471 quantization and 2-bit quantization. Notably, they illustrate that the value range of 2-bit quantization   \n472 is noticeably an order of magnitude higher than the value ranges of higher-bit quantization. ", "page_idx": 14}, {"type": "image", "img_path": "kOp0kiXZ3a/tmp/05e146fb04cae7f800fe2e586ff31963a9c026b610ac3691fdf7326ecd52c5a0.jpg", "img_caption": [], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "Figure 7: The scale gradient statistics of weight of ResNet20 on CIFAR-10 dataset. Note that the outliers are removed for exhibition. ", "page_idx": 15}, {"type": "image", "img_path": "kOp0kiXZ3a/tmp/866ef3e4860f8c372bc931bee91155bfef3b4307e660590fca95e4d10b2d34bc.jpg", "img_caption": [], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "Figure 8: The scale gradient statistics of activation of ResNet20 on CIFAR-10 dataset. Note that the first and last layers are not quantized. ", "page_idx": 15}, {"type": "image", "img_path": "kOp0kiXZ3a/tmp/64c3e288dc4f5c754800741a8e666765a0d67d67707f5c46b318119bd1a2cf6d.jpg", "img_caption": [], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "Figure 9: The scale gradient statistics of weight of ResNet18 on ImageNet dataset. Note that the outliers are removed for exhibition. ", "page_idx": 15}, {"type": "image", "img_path": "kOp0kiXZ3a/tmp/806be09f3f937945fc46e19e8bb28d0bdba4fa700856491c1362d8e991e2d7df.jpg", "img_caption": [], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "Figure 10: The scale gradient statistics of activation of ResNet18 on ImageNet dataset. Note that the outliers are removed for exhibition. ", "page_idx": 15}, {"type": "image", "img_path": "kOp0kiXZ3a/tmp/b445cdcd23941ca8bfb49388c8389b23467cfe8674006792c48c4cf2edb88481.jpg", "img_caption": ["Figure 11: The scale gradient statistics of weight of ResNet50 on ImageNet dataset. Note that the outliers are removed for exhibition, and the first and last layers are not quantized. "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "kOp0kiXZ3a/tmp/0a0c077b791ccf061cdf3999e03d14125d95b77227b7bf788c181e429deed467.jpg", "img_caption": ["Figure 12: The scale gradient statistics of activation of ResNet50 on ImageNet dataset. Note that the outliers are removed for exhibition. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "473 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "475 Question: Do the main claims made in the abstract and introduction accurately reflect the   \n476 paper\u2019s contributions and scope? ", "page_idx": 17}, {"type": "text", "text": "Justification: [TODO] Please refer to the Abstract Section and Section 1, where related material for the question can be found. ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 17}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [TODO][Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: [TODO] Although our proposed methods have achieved comparable results in multi-precision and mixed-precision, this paper has several limitations and improvements. (1) Due to time and computing resource constraints, our methods are only tested on common CNNs-based networks and aren\u2019t tested on ViTs-based networks. (2) For multi-precision, compact networks, e.g., MobileNet, still have a big drop in 2bit. We will try to use per-layer or per-channel adaptive learning rate adjustment in the future. (3) For mixed precision, relying only on one-shot ILP-based SubNets search may yield a suboptimal solution. We further need to combine it with other efficient search methods, e.g., genetic algorithms, to achieve global optimal. ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best ", "page_idx": 17}, {"type": "text", "text": "526 judgment and recognize that individual actions in favor of transparency play an impor  \n527 tant role in developing norms that preserve the integrity of the community. Reviewers   \n528 will be specifically instructed to not penalize honesty concerning limitations.   \n529 3. Theory Assumptions and Proofs   \n530 Question: For each theoretical result, does the paper provide the full set of assumptions and   \n531 a complete (and correct) proof?   \n532 Answer: [Yes]   \n533 Justification: [TODO] We display index numbers wherever formulas and theoretical support   \n534 are needed. For example, please refer to Section 3.   \n535 Guidelines:   \n536 \u2022 The answer NA means that the paper does not include theoretical results.   \n537 \u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross  \n538 referenced.   \n539 \u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n540 \u2022 The proofs can either appear in the main paper or the supplemental material, but if   \n541 they appear in the supplemental material, the authors are encouraged to provide a short   \n542 proof sketch to provide intuition.   \n543 \u2022 Inversely, any informal proof provided in the core of the paper should be complemented   \n544 by formal proofs provided in appendix or supplemental material.   \n545 \u2022 Theorems and Lemmas that the proof relies upon should be properly referenced.   \n546 4. Experimental Result Reproducibility   \n547 Question: Does the paper fully disclose all the information needed to reproduce the main ex  \n548 perimental results of the paper to the extent that it affects the main claims and/or conclusions   \n549 of the paper (regardless of whether the code and data are provided or not)?   \n550 Answer: [Yes] To ensure that our experimental results can be reproduced: (1) we describe   \n551 the experimental training settings and algorithm pseudocode in detail in Section 4 and   \n552 Section A.5, and (2) we also provide the code related to all experiments in this paper,   \n553 allowing the community to improve and conduct further research.   \n554 Justification: [TODO]   \n555 Guidelines:   \n556 \u2022 The answer NA means that the paper does not include experiments.   \n557 \u2022 If the paper includes experiments, a No answer to this question will not be perceived   \n558 well by the reviewers: Making the paper reproducible is important, regardless of   \n559 whether the code and data are provided or not.   \n560 \u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken   \n561 to make their results reproducible or verifiable.   \n562 \u2022 Depending on the contribution, reproducibility can be accomplished in various ways.   \n563 For example, if the contribution is a novel architecture, describing the architecture fully   \n564 might suffice, or if the contribution is a specific model and empirical evaluation, it may   \n565 be necessary to either make it possible for others to replicate the model with the same   \n566 dataset, or provide access to the model. In general. releasing code and data is often   \n567 one good way to accomplish this, but reproducibility can also be provided via detailed   \n568 instructions for how to replicate the results, access to a hosted model (e.g., in the case   \n569 of a large language model), releasing of a model checkpoint, or other means that are   \n570 appropriate to the research performed.   \n571 \u2022 While NeurIPS does not require releasing code, the conference does require all submis  \n572 sions to provide some reasonable avenue for reproducibility, which may depend on the   \n573 nature of the contribution. For example   \n574 (a) If the contribution is primarily a new algorithm, the paper should make it clear how   \n575 to reproduce that algorithm.   \n576 (b) If the contribution is primarily a new model architecture, the paper should describe   \n577 the architecture clearly and fully.   \n578 (c) If the contribution is a new model (e.g., a large language model), then there should   \n579 either be a way to access this model for reproducing the results or a way to reproduce   \n580 the model (e.g., with an open-source dataset or instructions for how to construct   \n581 the dataset).   \n582 (d) We recognize that reproducibility may be tricky in some cases, in which case   \n583 authors are welcome to describe the particular way they provide for reproducibility.   \n584 In the case of closed-source models, it may be that access to the model is limited in   \n585 some way (e.g., to registered users), but it should be possible for other researchers   \n586 to have some path to reproducing or verifying the results. ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "587 5. Open access to data and code ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "588 Question: Does the paper provide open access to the data and code, with sufficient instruc  \n589 tions to faithfully reproduce the main experimental results, as described in supplemental   \n590 material? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: [TODO] Our code are available at here and the data is open source dataset. Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 19}, {"type": "text", "text": "613 6. Experimental Setting/Details ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "14 Question: Does the paper specify all the training and test details (e.g., data splits, hyper  \n15 parameters, how they were chosen, type of optimizer, etc.) necessary to understand the   \n16 results? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: [TODO] Our codes are available here and include all related training and test details. ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 19}, {"type": "text", "text": "626 7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "627 Question: Does the paper report error bars suitably and correctly defined or other appropriate   \n628 information about the statistical significance of the experiments? ", "page_idx": 19}, {"type": "text", "text": "629 Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "630 Justification: [TODO] Please refer to the Section A.6 in the appendix.   \n631 Guidelines:   \n632 \u2022 The answer NA means that the paper does not include experiments.   \n633 \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confi  \n634 dence intervals, or statistical significance tests, at least for the experiments that support   \n635 the main claims of the paper.   \n636 \u2022 The factors of variability that the error bars are capturing should be clearly stated (for   \n637 example, train/test split, initialization, random drawing of some parameter, or overall   \n638 run with given experimental conditions).   \n639 \u2022 The method for calculating the error bars should be explained (closed form formula,   \n640 call to a library function, bootstrap, etc.)   \n641 \u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n642 \u2022 It should be clear whether the error bar is the standard deviation or the standard error   \n643 of the mean.   \n644 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should   \n645 preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis   \n646 of Normality of errors is not verified.   \n647 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or   \n648 figures symmetric error bars that would yield results that are out of range (e.g. negative   \n649 error rates).   \n650 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how   \n651 they were calculated and reference the corresponding figures or tables in the text.   \n652 8. Experiments Compute Resources   \n653 Question: For each experiment, does the paper provide sufficient information on the com  \n654 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n655 the experiments?   \n656 Answer: [Yes]   \n657 Justification: [TODO] Please refer to the Table 5.   \n658 Guidelines:   \n659 \u2022 The answer NA means that the paper does not include experiments.   \n660 \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,   \n661 or cloud provider, including relevant memory and storage.   \n662 \u2022 The paper should provide the amount of compute required for each of the individual   \n663 experimental runs as well as estimate the total compute.   \n664 \u2022 The paper should disclose whether the full research project required more compute   \n665 than the experiments reported in the paper (e.g., preliminary or failed experiments that   \n666 didn\u2019t make it into the paper).   \n667 9. Code Of Ethics   \n668 Question: Does the research conducted in the paper conform, in every respect, with the   \n669 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n670 Answer: [Yes]   \n671 Justification: [TODO] We have read the NeurIPS Code of Ethics and conform to it.   \n672 Guidelines:   \n673 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n674 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n675 deviation from the Code of Ethics.   \n676 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid  \n677 eration due to laws or regulations in their jurisdiction). ", "page_idx": 20}, {"type": "text", "text": "678 10. Broader Impacts ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "679 Question: Does the paper discuss both potential positive societal impacts and negative   \n680 societal impacts of the work performed?   \n681 Answer: [NA]   \n682 Justification: [TODO] Due to space limitations, this social impact aspect is not discussed   \n683 in the main paper. This paper doesn\u2019t involve negative societal impacts including potential   \n684 malicious or unintended uses. Our proposed methods aim to achieve an efficient and   \n685 effective model compression technique to flexible adaptive different storage and computation   \n686 requirements, which are beneficial to social advancement.   \n687 Guidelines:   \n688 \u2022 The answer NA means that there is no societal impact of the work performed.   \n689 \u2022 If the authors answer NA or No, they should explain why their work has no societal   \n690 impact or why the paper does not address societal impact.   \n691 \u2022 Examples of negative societal impacts include potential malicious or unintended uses   \n692 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n693 (e.g., deployment of technologies that could make decisions that unfairly impact specific   \n694 groups), privacy considerations, and security considerations.   \n695 \u2022 The conference expects that many papers will be foundational research and not tied   \n696 to particular applications, let alone deployments. However, if there is a direct path to   \n697 any negative applications, the authors should point it out. For example, it is legitimate   \n698 to point out that an improvement in the quality of generative models could be used to   \n699 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n700 that a generic algorithm for optimizing neural networks could enable people to train   \n701 models that generate Deepfakes faster.   \n702 \u2022 The authors should consider possible harms that could arise when the technology is   \n703 being used as intended and functioning correctly, harms that could arise when the   \n704 technology is being used as intended but gives incorrect results, and harms following   \n705 from (intentional or unintentional) misuse of the technology.   \n706 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n707 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n708 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n709 feedback over time, improving the efficiency and accessibility of ML).   \n710 11. Safeguards   \n711 Question: Does the paper describe safeguards that have been put in place for responsible   \n712 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n713 image generators, or scraped datasets)?   \n714 Answer: [NA]   \n715 Justification: [TODO] This paper doesn\u2019t have any high risk for misuse.   \n716 Guidelines:   \n717 \u2022 The answer NA means that the paper poses no such risks.   \n718 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n719 necessary safeguards to allow for controlled use of the model, for example by requiring   \n720 that users adhere to usage guidelines or restrictions to access the model or implementing   \n721 safety filters.   \n722 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n723 should describe how they avoided releasing unsafe images.   \n724 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n725 not require this, but we encourage authors to take this into account and make a best   \n726 faith effort.   \n727 12. Licenses for existing assets   \n728 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n729 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n730 properly respected?   \n731 Answer: [Yes]   \n732 Justification: [TODO] We conform to the CC-BY 4.0 license.   \n733 Guidelines:   \n734 \u2022 The answer NA means that the paper does not use existing assets.   \n735 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n736 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n737 URL.   \n738 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n739 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n740 service of that source should be provided.   \n741 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n742 package should be provided. For popular datasets, paperswithcode.com/datasets   \n743 has curated licenses for some datasets. Their licensing guide can help determine the   \n744 license of a dataset.   \n745 \u2022 For existing datasets that are re-packaged, both the original license and the license of   \n746 the derived asset (if it has changed) should be provided.   \n747 \u2022 If this information is not available online, the authors are encouraged to reach out to   \n748 the asset\u2019s creators.   \n749 13. New Assets   \n750 Question: Are new assets introduced in the paper well documented and is the documentation   \n751 provided alongside the assets?   \n752 Answer: [NA]   \n753 Justification: [TODO] This paper does not release new assets   \n754 Guidelines:   \n755 \u2022 The answer NA means that the paper does not release new assets.   \n756 \u2022 Researchers should communicate the details of the dataset/code/model as part of their   \n757 submissions via structured templates. This includes details about training, license,   \n758 limitations, etc.   \n759 \u2022 The paper should discuss whether and how consent was obtained from people whose   \n760 asset is used.   \n761 \u2022 At submission time, remember to anonymize your assets (if applicable). You can either   \n762 create an anonymized URL or include an anonymized zip file.   \n763 14. Crowdsourcing and Research with Human Subjects   \n764 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n765 include the full text of instructions given to participants and screenshots, if applicable, as   \n766 well as details about compensation (if any)?   \n767 Answer: [NA]   \n768 Justification: [TODO] This paper does not involve crowdsourcing nor research with human   \n769 subjects.   \n770 Guidelines:   \n771 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n772 human subjects.   \n773 \u2022 Including this information in the supplemental material is fine, but if the main contribu  \n774 tion of the paper involves human subjects, then as much detail as possible should be   \n775 included in the main paper.   \n776 \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation,   \n777 or other labor should be paid at least the minimum wage in the country of the data   \n778 collector.   \n779 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "Subjects ", "page_idx": 22}, {"type": "text", "text": "81 Question: Does the paper describe potential risks incurred by study participants, whether   \n82 such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)   \n83 approvals (or an equivalent approval/review based on the requirements of your country or   \n84 institution) were obtained?   \n[NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: [TODO] This paper does not involve crowdsourcing nor research with human ", "page_idx": 23}, {"type": "text", "text": "87 subjects.   \n88 Guidelines:   \n89 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n90 human subjects.   \n91 \u2022 Depending on the country in which research is conducted, IRB approval (or equivalent)   \n92 may be required for any human subjects research. If you obtained IRB approval, you   \n93 should clearly state this in the paper.   \n94 \u2022 We recognize that the procedures for this may vary significantly between institutions   \n95 and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the   \n96 guidelines for their institution.   \n97 \u2022 For initial submissions, do not include any information that would break anonymity (if   \n98 applicable), such as the institution conducting the review. ", "page_idx": 23}]