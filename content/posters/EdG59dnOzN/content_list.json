[{"type": "text", "text": "Remove that Square Root: A New Efficient Scale-Invariant Version of AdaGrad ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Sayantan Choudhury\\* Nazari Tupitsa MBZUAI & Johns Hopkins University MBZUAI & Innopolis University ", "page_idx": 0}, {"type": "text", "text": "Nicolas Loizou Samuel Horvath Martin Takac Eduard Gorbunov Johns Hopkins University MBZUAI MBZUAI MBZUAI ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Adaptive methods are extremely popular in machine learning as they make learning rate tuning less expensive. This paper introduces a novel optimization algorithm named KATE, which presents a scale-invariant adaptation of the well-known AdaGrad algorithm. We prove the scale-invariance of KATE for the case of Generalized Linear Models. Moreover, for general smooth non-convex problems, we establish a convergencerate of $\\mathcal{O}(\\log T/\\bar{\\sqrt{T}})$ for KATE, matching the best-known ones for AdaGrad and Adam. We also compare KATE to other state-of-the-art adaptive algorithms Adam and AdaGrad in numerical experiments with different problems, including complex machine learning tasks like image classification and text classification on real data. The results indicate that KATE consistently outperforms AdaGrad and matches/surpasses the performance of Adam in all considered scenarios. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In this work, we consider the following unconstrained optimization problem: ", "page_idx": 0}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{w\\in\\mathbb{R}^{d}}f(w),\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "where $f:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ is a $L$ -smooth and generally non-convex function. In particular, we are interested in the situations when the objective has either expectation $f(w)\\,=\\,\\mathbb{E}_{\\xi\\sim\\mathcal{D}}[f_{\\xi}(w)]$ or finite-sum $\\begin{array}{r}{f(w)=\\frac{1}{n}\\sum_{i=1}^{n}f_{i}(w)}\\end{array}$ fuizabln $w$ corresponds to the model parameters. Solving these problems with stochastic gradient-based optimizers has gained much interest owing to their wider applicability and low computational cost. Stochastic Gradient Descent (SGD) (Robbins and Monro, 1951) and similar algorithms require the knowledge of parameters like $L$ for convergence and are very sensitive to the choice of the stepsize in general. Therefore, SGD requires hyperparameter tuning, which can be computationally expensive. To address these issues, it is common practice to use adaptive variants of stochastic gradient-based methods that can converge without knowing the function's structure. ", "page_idx": 0}, {"type": "text", "text": "There exist many adaptive algorithms such as AdaGrad (Duchi et al., 2011), Adam (Kingma and Ba, 2014), AMSGrad (Reddi et al., 2019), D-Adaptation (Defazio and Mishchenko, 2023), Prodigy (Mishchenko and Defazio, 2023), Al-SARAH (Shi et al., 2023) and their variants. These adaptive techniques are capable of updating their step sizes on the fly. For instance, the AdaGrad method determines its step sizes using a cumulative sum of the coordinate-wise squared (stochastic) gradient ", "page_idx": 0}, {"type": "text", "text": "of all the previous iterates: ", "page_idx": 1}, {"type": "equation", "text": "$$\n{\\mathsf{A d a G r a d}}\\colon w_{t+1}=w_{t}-{\\frac{\\beta g_{t}}{\\sqrt{\\operatorname{diag}\\left(\\Delta I+\\sum_{\\tau=1}^{t}g_{\\tau}g_{\\tau}^{\\top}\\right)}}},\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $g_{t}$ represents an unbiased estimator of $\\nabla f(w_{t})$ , i.e., $\\mathbb{E}\\left[g_{t}\\mid w_{t}\\right]=\\nabla f(w_{t})$ \uff0c $\\mathrm{diag}(M)\\in\\mathbb{R}^{d}$ is a vector of diagonal elements of matrix $M\\in\\mathbb{R}^{d\\times d}$ \uff0c $\\Delta>0$ , and the division by vector is done component-wise. Ward et al. (2020) has shown that this method achieves a convergence rate of $\\mathcal{O}\\left(\\log T/\\sqrt{T}\\right)$ for smooth functions, similar to SGD, without prior knowledge of the functions\u2019 parameters. However, the performance of AdaGrad deteriorates when applied to data that may exhibit poor scaling or ill-conditioning. In this work, we propose a novel algorithm, KATE, to address the issues of poor data scaling. KATE is also a stochastic adaptive algorithm that can achieve a convergence rate of ${\\mathcal{O}}\\left(\\log T/{\\sqrt{T}}\\right)$ for smooth non-convex functions in terms of $\\begin{array}{r}{\\operatorname*{min}_{t\\in[T]}\\mathbb{E}\\left[\\|\\nabla f(w_{t})\\|\\right]^{2}}\\end{array}$ ", "page_idx": 1}, {"type": "text", "text": "1.1 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "A significant amount of research has been done on adaptive methods over the years, including AdaGrad (Duchi et al., 2011; McMahan and Streeter, 2010), AMSGrad (Reddi et al., 2019), RMSProp (Tieleman and Hinton, 2012), Al-SARAH (Shi et al., 2023), and Adam (Kingma and Ba, 2014). However, all these works assume that the optimization problem is contained in a bounded set. To address this issue, Li and Orabona (2019) proposes a variant of the AdaGrad algorithm, which does not use the gradient of the last iterate (this makes the step sizes of $t$ -th iteration conditionally independent of $g_{t}$ ) for computing the step sizes and proves convergence for the unbounded domain. ", "page_idx": 1}, {"type": "text", "text": "Each of these works considers a vector of step sizes for each coefficient. Duchi et al. (2011) and McMahan and Streeter (2010) simultaneously proposed the original AdaGrad algorithm. However, McMahan and Streeter (2010) was the first to consider the vanilla scalar form of AdaGrad, known as ", "page_idx": 1}, {"type": "equation", "text": "$w_{t+1}=w_{t}-\\frac{\\beta g_{t}}{\\sqrt{\\Delta+\\sum_{\\tau=0}^{t}\\left\\Vert g_{\\tau}\\right\\Vert^{2}}}.$ ", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "Later, Ward et al. (2020) analyzed AdaGradNorm for minimizing smooth non-convex functions. In a follow-up study, Xie et al. (2020) proves a linear convergence of AdaGradNorm for strongly convex functions. Recently, Liu et al. (2022) analyzed AdaGradNorm for solving smooth convex functions without the bounded domain assumption. Moreover, Liu et al. (2022) extends the convergence guarantees of AdaGradNorm to quasar-convex functions 2 using the function value gap. ", "page_idx": 1}, {"type": "text", "text": "Recently, Defazio and Mishchenko (2023) introduced the D-Adaptation method, which has gathered considerable attention due to its promising empirical performances. In order to choose the adaptive step size optimally, one requires knowledge of the initial distance from the solution, i.e., ${\\cal D}:=$ $\\|w_{0}-w_{*}\\|$ where $w_{*}\\,\\in\\,\\mathrm{argmin}_{w\\in\\mathbb{R}^{d}}f(w)$ . The D-Adaptation method works by maintaining an estimate of $D$ and the stepsize choice in this case is $d_{t}/\\sqrt{\\textstyle\\sum_{\\tau=0}^{t}\\|g_{\\tau}\\|^{2}}$ for the $t$ -th iteration (here $d_{t}$ is an estimate of $D$ ). Mishchenko and Defazio (2023) further modifies the algorithm in a follow-up work and introduces Prodigy (with stepsize choice $d_{t}^{2}/\\sqrt{\\textstyle\\sum_{\\tau=0}^{t}d_{\\tau}^{2}\\|g_{\\tau}\\|^{2}})$ to improve the convergence speed. ", "page_idx": 1}, {"type": "text", "text": "Another exciting line of work on adaptive methods is Polyak stepsizes. Polyak (1969) first proposed Polyak stepsizes for subgradient methods, and recently, the stochastic version (also known as SPS) was introduced by Oberman and Prazeres (2019); Loizou et al. (2021); Abdukhakimov et al. (2024, 2023); Li et al. (2023) and Gower et al. (2021). For a finite sum problem $\\begin{array}{r}{\\operatorname*{min}_{w\\in\\mathbb{R}^{d}}f(w)\\,:=\\,\\frac{1}{n}\\sum_{i=1}^{n}f_{i}(w)}\\end{array}$ , Loizou et al. (2021) uses f:(w) as their stepsize choices (here f\\* := minweRd f(w), while Oberman and Prazeres 2019)uses 2 2(f(wt)f\") for k-th iteration. However, these methods are impractical when $f^{*}$ or $f_{i}^{*}$ is unknown. Following its introduction, several variants of the SPS algorithm emerged (Li et al., 2023; D'Orazio et al., 2021). Lately, Orvieto et al. (2022) tackled the issues with unknown $f_{i}^{*}$ and developed a truly adaptive variant. In practice, the SPS method shows excellent empirical performance on overparameterized deep learning models (which satisfy the interpolation condition i.e. $f_{i}^{*}=0$ $\\forall i\\in[n]!$ (Loizou et al., 2021). ", "page_idx": 1}, {"type": "text", "text": "Table 1: Summary of convergence guarantees for closely-related adaptive algorithms to solve smooth non-convex stochastic optimization problems. Convergence rates are given in terms of $\\begin{array}{r}{\\operatorname*{min}_{t\\in[T]}\\mathbb{E}\\left[\\|\\nabla f(w_{t})\\|\\right]^{2}}\\end{array}$ . We highlight KATE's scale-invariance property for problems of type (4). ", "page_idx": 2}, {"type": "table", "img_path": "EdG59dnOzN/tmp/387c69d097acb8f72fb9e5e08fd56e08ca0047c444bacc192de868f57fd26dd4.jpg", "table_caption": [], "table_footnote": [], "page_idx": 2}, {"type": "text", "text": "1.2  Main Contribution ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Our main contributions are summarized below. ", "page_idx": 2}, {"type": "text", "text": "\u00b7 KATE: new scale-invariant version of AdaGrad. We propose a new method called KATE that can be seen as a version of AdaGrad, which does not use a square root in the denominator of the stepsize. To compensate for this change, we introduce a new sequence defining the numerator of the stepsize. We prove that KATE is scale-invariant for generalized linear models: if the starting point is zero, then the loss values (and training and test accuracies in the case of classification) at points generated by KATE are independent of the data scaling (Proposition 2.1), meaning that the speed of convergence of KATE is the same as for the best scaling of the data. ", "page_idx": 2}, {"type": "text", "text": "\u00b7 Convergence for smooth non-convex problems.  We prove that for smooth non-convex problems with noise having bounded variance KATE has ${\\mathcal{O}}{\\bigl(}\\log(T){\\big/}{\\sqrt{T}}{\\bigr)}$ convergence rate (Theorem 3.4), matching the best-known rates for AdaGrad and Adam (D\u00e9fossez et al., 2020). ", "page_idx": 2}, {"type": "text", "text": "\u00b7 Numerical experiments. We empirically illustrate the scale-invariance of KATE on the logistic regression task and test its performance on logistic regression (see Section 4.1), image classification, and text classification problems (see Section 4.2). In all the considered scenarios, KATE outperforms AdaGrad and works either better or comparable to Adam. ", "page_idx": 2}, {"type": "text", "text": "1.3Notation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We denote the set $\\{1,2,\\cdots\\,,n\\}$ as $[n]$ . For a vector $a\\in\\mathbb{R}^{d},a[k]$ is the $k$ -th coordinate of $a$ and $a^{2}$ represents the element-wise suqare of $a$ , i.e., $a^{2}[k]=(a[k])^{2}$ For two vectors $a$ and $b$ \uff0c $\\frac{a}{b}$ stands for $a$ $b$ $k$ $\\frac{a}{b}$ $\\frac{a[k]}{b[k]}$ Givna fintion $h:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ use $\\nabla h\\in\\mathbb{R}^{d}$ to denote its gradient and $\\nabla_{k}h$ to indicate the $k$ -th component of $\\nabla h$ Throughout the paper $\\|\\cdot\\|$ represents the $\\ell_{2}$ -norm and $f_{*}=\\operatorname*{inf}_{w\\in\\mathbb{R}^{d}}f(w)$ . Moreover, we use $\\|w\\|_{A}$ for a positivedefinite matrix $A$ to define $\\|w\\|_{A}:=\\sqrt{w^{\\top}A w}$ . Furthermore, $\\mathbb{E}\\left[\\cdot\\right]$ denotes the total expectation while $\\mathbb{E}_{t}\\left[\\cdot\\right]$ denotes the conditional expectation conditioned on alliterates up to step $t$ i.e. $w_{0},w_{1},\\dots,w_{t}$ ", "page_idx": 2}, {"type": "text", "text": "2  Motivation and Algorithm Design ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We focus on solving the minimization problem (1) using a variant of AdaGrad. We aim to design an algorithm that performs well, irrespective of how poorly the data is scaled. ", "page_idx": 2}, {"type": "text", "text": "Generalized linear models. Here, we consider the parameter estimation problem in generalized linear models (GLMs) (Nelder and Wedderburn, 1972; Agresti, 2015) using maximum likelihood estimation. GLMs are an extension of linear models and encompass several other valuable models, such as logistic (Hosmer Jr et al., 2013) and Poisson regression (Frome, 1983), as special cases. The parameter estimation to fit GLM on dataset $\\{x_{i},y_{i}\\}_{i=1}^{n}$ (where $x_{i}\\in\\mathbb{R}^{d}$ are feature vectors and $y_{i}$ are response variables) can be reformulated as ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{w\\in\\mathbb{R}^{d}}f(w):=\\frac{1}{n}\\sum_{i=1}^{n}\\varphi_{i}\\left(x_{i}^{\\top}w\\right)\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "for differentiable functions $\\varphi_{i}:\\mathbb{R}\\rightarrow\\mathbb{R}$ (Shalev-Shwartz and Ben-David, 2014; Nguyen et al., 2017b; Takac et al., 2013; He et al., 2018; Chezhegov et al., 2024). For example, the linear regression on data $\\{x_{i},y_{i}\\}_{i=1}^{n}$ is equivalent to solving (4) with $\\varphi_{i}(z)=(z-y_{i})^{2}$ . Next, the choice of $\\varphi_{i}$ for logistic regression is $\\varphi_{i}(\\bar{z})=\\log\\left(1+\\exp^{\\cdot}\\!\\left(-y_{i}z\\right)\\right)$ ", "page_idx": 3}, {"type": "text", "text": "Scale-invariance. Now consider the instances of fitting GLMs on two datasets $\\{x_{i},y_{i}\\}_{i=1}^{n}$ and $\\{V x_{i},y_{i}\\}_{i=1}^{n}$ , where $V\\,\\in\\,\\mathbb{R}^{d\\times d}$ is a diagonal matrix with positive entries. Note that the second dataset is a scaled version of the first one where the $k$ -th component of feature vectors $x_{i}$ are multiplied by a scalar $V_{k k}$ . Then, the minimization problems corresponding to datasets $\\{x_{i},y_{i}\\}_{i=1}^{n}$ and $\\{\\bar{V}x_{i},y_{i}\\}_{i=1}^{n}$ are (4) and ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{w\\in\\mathbb{R}^{d}}f^{V}(w)\\quad:=\\quad\\frac{1}{n}{\\sum}_{i=1}^{n}\\varphi_{i}\\left(x_{i}^{\\top}V w\\right),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "respectively, for functions $\\varphi_{i}$ . In this work, we want to design an algorithm with equivalent performance for the problems (4) and (5). If we can do that, the new algorithm's performance will not deteriorate for poorly scaled data. To develop such an algorithm, we replace the denominator of AdaGrad step size with its square (remove the square root from the denominator), i.e., $\\forall k\\in[d]$ ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{w_{t+1}[k]}&{=}&{w_{t}[k]-\\frac{\\beta m_{t}[k]}{\\sum_{\\tau=0}^{t}g_{\\tau}^{2}[k]}g_{t}[k]}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "for some $m_{t}\\in\\mathbb{R}^{d}$ The following proposition hows tha this methd 6) satisies a scaleinvarianc property with respect to functional value. ", "page_idx": 3}, {"type": "text", "text": "Proposition 2.1 (Scale invariance). Suppose we solve problems (4) and (5) using algorithm (6). Then, the iterates $\\hat{w}_{t}$ and $\\hat{w}_{t}^{V}$ corresponding to (4) and (5) follow: $\\forall k\\in[d]$ ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{{\\hat{w}_{t+1}[k]}}&{{=}}&{{\\hat{w}_{t}[k]-\\displaystyle\\frac{\\beta m_{t}[k]}{\\sum_{\\tau=0}^{t}g_{\\tau}^{2}[k]}g_{t}[k],}}\\\\ {{\\hat{w}_{t+1}^{V}[k]}}&{{=}}&{{\\hat{w}_{t}^{V}[k]-\\displaystyle\\frac{\\beta m_{t}[k]}{\\sum_{\\tau=0}^{t}\\left(g_{\\tau}^{V}[k]\\right)^{2}}g_{t}^{V}[k]}}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "with $g_{\\tau}=\\varphi_{i_{\\tau}}^{\\prime}(x_{i_{\\tau}}^{\\top}\\hat{w}_{\\tau})x_{i_{\\tau}}$ and $g_{\\tau}^{V}=\\varphi_{i_{\\tau}}^{\\prime}(x_{i_{\\tau}}^{\\top}V\\hat{w}_{\\tau})V x_{i_{\\tau}}$ for $i_{\\tau}$ chosen uniformly from $[n],\\tau=$ $0,1,\\dots,t,t\\geq0$ Moreover, updates (7) and (8) satisfy ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\hat{w}_{t}=V\\hat{w}_{t}^{V},\\quad V g_{t}=g_{t}^{V},\\quad f\\left(\\hat{w}_{t}\\right)=f^{V}\\left(\\hat{w}_{t}^{V}\\right)\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "for all $t\\geq0$ when $\\hat{w}_{0}=\\hat{w}_{0}^{V}=0\\in\\mathbb{R}^{d}$ . Furthermore we have ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\left\\|g_{t}^{V}\\right\\|_{V^{-2}}^{2}}&{{}=}&{\\left\\|g_{t}\\right\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The Proposition 2.1 highlights that the update rule of the form (6) satisfies a scale-invariance property for GLMs. In contrast, AdaGrad does not satisfy (9) and (10). In Appendix C, we illustrate numerically the scale-invariance of KATE and the lack of the scale-invariance of AdaGrad. ", "page_idx": 3}, {"type": "text", "text": "Design of KATE. In order to construct an algorithm following the update rule (6), one may choose $m_{t}[k]=1\\;\\forall k\\in[d]$ . However, the step size from (6) in this case may decrease very fast, and the resulting method does not necessarily converge. Therefore, we need a more aggressive choice of $m_{t}$ ,whichgrows with $t$ . It motivates the construction of our algorithm KATE (Algorithm 1),4 where we choose mt[] = \u221a/ m[k]b2[] + Z\\*=0  \u00b7 . Note that the term t=o is scale-invariant ", "page_idx": 3}, {"type": "text", "text": "Require: Initial point $w_{0}\\in\\mathbb{R}^{d}$ , step size $\\beta>0,\\eta\\in\\mathbb{R}_{+}^{d}$ and $b_{-1},m_{-1}=0$   \n1: for $t=0,1,...,T$ do   \n2: Compute $\\boldsymbol{g}_{t}\\in\\mathbb{R}^{d}$ such that $\\mathbb{E}\\left[g_{t}\\right]=\\nabla f(w_{t})$ \uff1a   \n3: $b_{t}^{2}=\\dot{b}_{t-1}^{2}+g_{t}^{2}$   \n4: $\\begin{array}{r l}&{m_{t}^{2}=m_{t-1}^{2}+\\eta g_{t}^{2}+\\frac{g_{t}^{2}}{b_{t}^{2}}}\\\\ &{w_{t+1}=w_{t}-\\frac{\\beta m_{t}}{b_{t}^{2}}g_{t}}\\end{array}$   \n5: ", "page_idx": 4}, {"type": "text", "text": "for GLMs (follows from Proposition 2.1). To make $m_{t}$ scale-invariant, we choose $\\eta\\in\\mathbb{R}^{d}$ in the following way: ", "page_idx": 4}, {"type": "text", "text": "$\\underline{{\\eta}}\\to0$ : When $\\eta$ is very small, $m_{t}$ is also approximately scale-invariant for GLMs. $\\eta=1/(\\nabla f(w_{0}))^{2}$ : In this case $\\eta b_{t}^{2}=b_{t}^{2}\\big/(\\nabla f(\\boldsymbol{w}_{0}))^{2}$ is scale-invariant for GLMs (follows from Proposition 2.1) as well as $m_{t}$ ", "page_idx": 4}, {"type": "text", "text": "KATE can be rewritten in the following coordinate form ", "page_idx": 4}, {"type": "equation", "text": "$$\nw_{t+1}[k]=w_{t}[k]-\\nu_{t}[k]g_{t}[k],\\qquad\\forall k\\in[d],\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $g_{t}$ is an unbiased estimator of $\\nabla f(w_{t})$ and the per-coefcient step size $\\nu_{t}[k]$ is defined as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\nu_{t}[k]:=\\frac{\\beta\\sqrt{\\eta[k]b_{t}^{2}[k]+\\sum_{\\tau=0}^{t}\\frac{g_{\\tau}^{2}[k]}{b_{\\tau}^{2}[k]}}}{b_{t}^{2}[k]}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Note that the numerator of the steps $\\nu_{t}[k]$ is increasing with iterations $t$ . However, one of the crucial properties of this step size choice is that the steps always decrease with $t$ , which we rely on in our convergence analysis. ", "page_idx": 4}, {"type": "text", "text": "Lemma 2.2 (Decreasing step size). For $\\nu_{t}[k]$ defined in (11) we have ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\nu_{t+1}[k]\\leq\\nu_{t}[k],\\qquad\\forall k\\in[d].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "3 Convergence Analysis ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we present and discuss the convergence guarantees of KATE. In the first subsection, we list the assumptions made about the problem. ", "page_idx": 4}, {"type": "text", "text": "3.1 Assumptions ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In all our theoretical results, we assume that $f$ is smooth as defined below. ", "page_idx": 4}, {"type": "text", "text": "Assumption 3.1 ( $L$ -smooth). Function $f$ is $L$ -smooth, i.e. for all $w,w^{\\prime}\\in\\mathbb{R}^{d}$ ", "page_idx": 4}, {"type": "equation", "text": "$$\nf(w^{\\prime})\\leq f(w)+\\left\\langle\\nabla f(w),w^{\\prime}-w\\right\\rangle+\\frac{L}{2}\\left\\Vert w-w^{\\prime}\\right\\Vert^{2}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "This assumption is standard in the literature of adaptive methods (Li and Orabona, 2019; Ward et al. 2020; Liu et al., 2022; Nguyen et al., 2018, 2021, 2017a; Beznosikov and Takac, 2021). More0ver, we assume that at any iteration $t$ of KATE, we can access $g_{t}$ \u2014 a noisy and unbiased estimate of $\\nabla f(w_{t})$ . We also make the following assumption on the noise of the gradient estimate $g_{t}$ ", "page_idx": 4}, {"type": "text", "text": "Assumption 3.2 (Bounded Variance). For fixed constant $\\sigma\\,>\\,0$ , the variance of the stochastic gradient $g_{t}$ (unbiased estimate of $\\nabla f(w_{t}))$ at any time $t$ satisfies ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbb{E}_{t}\\left[\\|g_{t}-\\nabla f(w_{t})\\|^{2}\\right]\\leq\\sigma^{2}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Bounded variance is a common assumption to study the convergence of stochastic gradient-based methods. Several assumptions on stochastic gradients are used in the literature to explore the adaptive methods. Ward et al. (2020) used the BV, while Liu et al. (2022) assumed the sub-Weibull noise, i.e. $\\mathbb{E}\\left[\\exp\\left(\\|g_{t}-\\nabla f(w_{t})\\|/\\sigma\\right)^{1/\\theta}\\right]\\le\\exp\\left(1\\right)$ for some $\\theta>0$ , to prove the convergence of AdaGradNorm. Li and Orabona (2019) assumes sub-Gaussian ( $\\mathit{\\Delta}\\theta={}^{1/2}\\mathit{\\Delta}$ in sub-Weibull condition) noise to study a variant of AdaGrad. However, sub-Gaussian noise is strictly stronger than BV. Recently, Faw et al. (2022) analyzed AdaGradNorm under a more relaxed condition known as affine variance i.e. $\\mathbb{E}_{t}\\left[\\left\\|g_{t}-\\dot{\\nabla}f(w_{t})\\right\\|^{2}\\right]\\le\\sigma_{0}^{2}+\\sigma_{1}^{2}\\left\\|\\nabla f(w_{t})\\right\\|^{2}\\right)$ ", "page_idx": 5}, {"type": "text", "text": "3.2  Main Results ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we cover the main convergence guarantees of KATE for both deterministic and stochastic setups. ", "page_idx": 5}, {"type": "text", "text": "Deterministic setting.  We first present our results for the deterministic setting. In this setting, we consider the gradient estimate to have no noise (i.e. $\\sigma^{2}=0$ )and $g_{t}=\\nabla f(w_{t})$ . The main result in this setting is summarized below. ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.3. Suppose $f$ satisfy Assumption 3.1 and $g_{t}\\,=\\,\\nabla f(w_{t})$ .Moreover, $\\beta\\,>\\,0$ and $\\eta[k]>0$ are chosen such that $\\begin{array}{r}{\\nu_{0}[\\dot{k}]\\leq\\frac{1}{L}}\\end{array}$ for all $k\\in[d]$ . Then the iterates of KATE satisfies ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{t\\le T}\\|\\nabla f(w_{t})\\|^{2}\\;\\;\\leq\\;\\;\\frac{\\Big(\\frac{2(f(w_{0})-f_{*})}{\\sqrt{\\eta_{0}}\\beta}+\\sum_{k=1}^{d}b_{0}[k]\\Big)^{2}}{T+1},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\eta_{0}:=\\operatorname*{min}_{k\\in[d]}\\eta[k]$ ", "page_idx": 5}, {"type": "text", "text": "Discussion on Theorem 3.3.  Theorem 3.3 establishes an ${\\mathcal{O}}\\left({1}/{T}\\right)$ convergence rate for KATE, which is optimal for finding a first-order stationary point of a non-convex problem (Carmon et al., 2020). However, this result is not parameter-free. To prove the convergence, we assume that $\\nu_{0}[k]\\leq$ $\\scriptstyle{\\frac{1}{L}}$ $\\forall k\\in[d]$ in Theorem 3.3, which is equivalent to $\\beta\\sqrt{1+\\eta_{0}\\left(\\nabla_{k}f(w_{0})\\right)^{2}}\\le(\\nabla_{k}f(w_{0}))^{2}\\big/L$ $\\forall k\\in$ $[d]$ . Note that the later condition holds for sufficiently small (dependent on $L$ )values of $\\beta,\\eta_{0}>0$ ", "page_idx": 5}, {"type": "text", "text": "However, it is possible to derive a parameter-free version of Theorem 3.3. Indeed, Lemma 2.2 implies that the step sizes are decreasing. Therefore, we can break down the analysis of KATE into two phases: Phase I when $\\nu_{0}[k]>1\\overline{{/}}\\overline{{L}}$ and Phase II when $\\nu_{0}[k]\\leq1/L$ , when the current analysis works, and then follow the proof techniques of Ward et al. (2020) and Xie et al. (2020). We leave this extension as a possible future direction of our work. ", "page_idx": 5}, {"type": "text", "text": "Stochastic setting. Next, we present the convergence guarantees for KATE in the stochastic case, when we can access an unbiased gradient estimate $g_{t}$ with non-zero noise. ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.4. Suppose $f$ satisfy Assumption 3.1 and $g_{t}$ is an unbiased estimator of $\\nabla f(w_{t})$ such that BV holds. Moreover, we assume $\\|\\nabla^{*}\\!f(w_{t})\\|^{2}\\leq\\gamma^{\\tilde{2}}$ for all $t$ . Then the iterates of KATE satisfy ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{t\\le T}\\mathbb{E}\\left[\\|\\nabla f(w_{t})\\|\\right]\\le\\left(\\frac{\\|g_{0}\\|}{T}+\\frac{2(\\gamma+\\sigma)}{\\sqrt{T}}\\right)^{1/2}\\sqrt{\\frac{2{\\mathcal C}_{f}}{\\beta\\sqrt{\\eta_{0}}}},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\eta_{0}:=\\operatorname*{min}_{k\\in[d]}\\eta[k]$ and ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\mathcal{C}_{f}}&{:=}&{f(w_{0})-f_{*}+2\\beta\\sigma\\sum_{k=1}^{d}\\sqrt{\\eta[k]}\\log\\left(\\frac{e(\\sigma^{2}+\\gamma^{2})T}{g_{0}^{2}[k]}\\right)}\\\\ &&{+\\sum_{k=1}^{d}\\left(\\frac{\\beta^{2}\\eta[k]L}{2}+\\frac{\\beta^{2}L}{2g_{0}^{2}[k]}\\right)\\log\\left(\\frac{e(\\sigma^{2}+\\gamma^{2})T}{g_{0}^{2}[k]}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Comparison with prior work. Theorem 3.4 shows an ${\\mathcal{O}}\\big(\\log^{1/2}T\\big/T^{1/4}\\big)$ convergence rate for KATE with respect to the metric $\\begin{array}{r}{\\operatorname*{min}_{t\\leq T}\\mathbb{E}\\left[\\|\\nabla f(w_{t})\\|\\right]}\\end{array}$ for the stochastic setting. Note that, in the stochastic ", "page_idx": 5}, {"type": "text", "text": "setting, KATE achieves a slower rate than Theorem 3.3 due to noise accumulation. Up to the logarithmic factor, this rate is optimal (Arjevani et al., 2023). Similar rates for the same metric follow from the results5 of (D\u00e9fossez et al., 2020) for AdaGrad and Adam. ", "page_idx": 6}, {"type": "text", "text": "Finally, Li and Orabona (2019) considers a variant of AdaGrad closely related to KATE: ", "page_idx": 6}, {"type": "equation", "text": "$$\nw_{t+1}=w_{t}-{\\frac{\\beta g_{t}}{\\left(\\operatorname{diag}\\left(\\Delta I+\\sum_{\\tau=1}^{t-1}\\!g_{\\tau}g_{\\tau}^{\\top}\\right)\\right)^{\\frac{1}{2}+\\varepsilon}}},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "for some $\\varepsilon\\in[0,{^1\\mathord{/}{\\vphantom{^{1}}^{2}}2})$ and $\\Delta>0$ . It differs from AdaGrad in two key aspects: the denominator of the stepsize does not contain the last stochastic gradient, and also, instead of the square root of the sum of squared gradients, this sum is taken in the power of $1/2+\\varepsilon$ . However, the results from Li and Orabona (2019) do not imply convergence for the case of $\\varepsilon$ , which is expected since, in this case, the stepsize converges to zero too quickly in general. To compensate for such a rapid decrease, in KATE, we introduce an increasing sequence $m_{t}$ in the numerator of the stepsize. ", "page_idx": 6}, {"type": "text", "text": "Proof technique. Compared to the AdaGrad, KATE uses more aggressive steps (the larger numerator of KATE due to the extra term $\\textstyle\\sum_{\\tau=0}^{t}g_{\\tau}^{2}[k]\\big/b_{\\tau}^{2}[k]\\big)$ Therefore, we expect KATE to have better empirical performance. However, introducing $\\textstyle\\sum_{\\tau=0}^{t}g_{\\tau}^{2}[k]\\big/b_{\\tau}^{2}[k]$ in the numerator raises additional technical difficulties in the proof technique. Fortunately, as we rigorously show, the KATE steps $\\nu_{t}[k]$ retain some of the critical properties of AdaGrad steps. For instance, they (i) are lower bounded by AdaGrad steps up to a constant,(i decrease with iteration $t$ (Lemma 2.2), and (ii) have closed-form upper bounds for $\\textstyle\\sum_{t=0}^{T}\\nu_{t}^{2}[k]g_{t}^{2}[k]$ . These are indeed the primary building blocks of our proof technique. ", "page_idx": 6}, {"type": "text", "text": "4  Numerical Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we implement KATE in several machine learning tasks to evaluate its performance. To ensure transparency and facilitate reproducibility, we provide an access to the source code for all of our experiments at https : / /github. com/nazya/KATE. ", "page_idx": 6}, {"type": "text", "text": "4.1 Logistic Regression ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we consider the logistic regression model ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{w\\in\\mathbb{R}^{d}}f(w)=\\frac{1}{n}\\sum_{i=1}^{n}\\log\\left(1+\\exp\\left(-y_{i}x_{i}^{\\top}w\\right)\\right),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "to elaborate on the scale-invariance and robustness of KATE for various initializations. For the experiments of this Section 4.1, we used Mac mini (M1, 2020), RAM 8 GB and storage 256 GB. Each of these plots took about 20 minutes to run. ", "page_idx": 6}, {"type": "text", "text": "4.1.1 Robustness of KATE ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "To conduct this experiment, we set the total number of samples to 1000 (i.e. $n=1000)$ 0. Here, we simulate the independent vectors $\\boldsymbol{x}_{i}\\,\\in\\,\\mathbb{R}^{20}$ such that each entry is from $\\mathcal{N}(0,1)$ . Moreover, we generate a diagonal matrix $V\\in\\mathbb{R}^{20\\times20}$ such that $\\log V_{k k}\\stackrel{\\mathrm{iid}}{\\sim}\\mathrm{Unif}(-10,10)$ \uff0c $\\forall k\\in$ [20]. Similarly, we generate $\\boldsymbol{w}^{\\ast}\\in\\mathbb{R}^{20}$ with each component from ${\\mathcal{N}}(0,1)$ and set the labels ", "page_idx": 6}, {"type": "equation", "text": "$$\ny_{i}=\\left\\{\\begin{array}{l l}{1,\\qquad x_{i}^{\\top}V w^{*}\\geq0,}\\\\ {-1,\\qquad x_{i}^{\\top}V w^{*}<0,}\\end{array}\\right.\\qquad\\forall i\\in[n].\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "We compare KATE's performance with four other algorithms: AdaGrad, AdaGradNorm, SGDdecay and SGD-constant, similar to the section 5.1 of Ward et al. (2020). For each algorithm, we initialize with $w_{0}~=~0~\\in~\\mathbb{R}^{20}$ and independently_ draw a sample of mini-batch size 10 to update the weight vector $w_{t}$ . We compare the algorithms $\\bullet$ AdaGrad with stepsize + \u00b7AdaGraNorm with step ice V+Z\\*=oll-12 \u00b7 SGD-decay with stepsize ", "page_idx": 6}, {"type": "image", "img_path": "EdG59dnOzN/tmp/28173b31a9f640c1b5f784ed747332f51da293d3d476b70a9675db61d8cacfdb.jpg", "img_caption": [], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Figure 1: Comparison of KATE with AdaGrad, AdaGradNorm, SGD-decay and SGD-constant for differentvaluesof $\\Delta$ (on $x$ -axis for logistic regression model. Figure 1a, 1b and 1c plots the functional value $f(w_{t})$ (on $y$ -axis) after $10^{\\bar{4}},5\\times10^{\\bar{4}}$ , and $10^{5}$ iterations respectively. ", "page_idx": 7}, {"type": "image", "img_path": "EdG59dnOzN/tmp/c02cd3ca970cba6ca7183cfd42220436f14eaa226d90982b29489bf215e13e1d.jpg", "img_caption": ["Figure 2: Comparison of KATE with AdaGrad, AdaGradNorm, SGD-decay and SGD-constant on datasets heart, australian, and splice from LIBSVM. Figures 2a, 2b and 2c plot the functional value $f(w_{t})$ , while 2d, 2e and 2f plot the accuracy on $y$ -axis for 5, 000 iterations. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "$^{\\beta}/\\Delta\\sqrt{t+1}$ , and\u00b7SGD-constant with step size $\\beta/\\Delta$ . Similarly, for KATE we use stepsize $\\frac{\\beta m_{t}}{b_{t}^{2}}$ where $\\begin{array}{r}{m_{t}^{2}=\\eta b_{t}^{2}+\\sum_{\\tau=0}^{t}g_{\\tau\\Big/b_{\\tau}^{2}}^{2}}\\end{array}$ and $\\begin{array}{r}{b_{t}^{2}=\\Delta+\\sum_{\\tau=0}^{t}g_{\\tau}^{2}}\\end{array}$ Here, we choose $\\beta=f(w_{0})-f(w^{*})$ and vary $\\Delta$ in $\\{10^{-8},10^{-6},10^{-4},10^{-2},1,10^{2},10^{4},10^{6},10^{8}\\}$ ", "page_idx": 7}, {"type": "text", "text": "In Figures 1a, 1b, and 1c, we plot the functional value $f(w_{t})$ (on the $y$ -axis) after $10^{4},5\\times10^{4}$ and $\\mathrm{\\bar{10}^{5}}$ iterations, respectively. In theory, the convergence of SGD requires the knowledge of smoothness constant $L$ . Therefore, when the $\\Delta$ is small (hence the stepsize is large), SGD-decay and SGD-constant diverge. However, the adaptive algorithms KATE, AdaGrad, and AdaGradNorm can auto-tune themselves and converge for a wide range of $\\Delta s$ (even when the $\\Delta$ is too small). As we observe in Figure 1, when the $\\Delta$ is small, KATE outperforms all other algorithms. For instance, when $\\Delta=10^{-8}$ , KATE achieves a functional value of $10^{\\bar{-}3}$ after only $10^{4}$ iterations (see Figure la), while other algorithms fail to achieve this even after $10^{5}$ iterations (see Figure 1c). Furthermore, KATE performs as well as AdaGrad and better than other algorithms when the $\\Delta$ is large. In particular, this experiment highlights that KATE is robust toinitialization $\\Delta$ ", "page_idx": 7}, {"type": "text", "text": "4.1.2Peformance of KATE on Real Data ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we examine KATE's performance on real data. We test KATE on three datasets: heart, australian, and splice from the LIBSVM library (Chang and Lin, 2011). The response variables $y_{i}$ of each of these datasets contain two classes, and we use them for binary classification tasks using a logistic regression model (16). We take $\\eta=1/(\\nabla f(w_{0}))^{2}$ for KATE and tune $\\beta$ in all the experiments. For tuning $\\beta$ , we do a grid search on the list $\\{10^{-10},10^{-8},10^{-6},10^{-4},10^{-2},1\\}$ . Similarly, we tune stepsizes for other algorithms. We take 5 trials for each of these algorithms and plot the mean of their trajectories. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "We plot the functional value $f(w_{t})$ (i.e. loss function) in Figures 2a, 2b and 2c, whereas Figures 2d, 2e and 2f plot the corresponding accuracy of the weight vector $w_{t}$ On the $y$ -axis for 5, 000 iterations. We observe that KATE performs superior to all other algorithms, even on real datasets. ", "page_idx": 8}, {"type": "image", "img_path": "EdG59dnOzN/tmp/88660bbc828c6184545803c0663bc3b9bce4dada5f761e19f0ddc2db6925aa5d.jpg", "img_caption": [], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "4.2 Training of Neural Networks ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, we compare the performance of KATE, AdaGrad and Adam on two tasks, i.e. training ResNet18 (He et al., 2016) on the CIFAR10 dataset (Krizhevsky and Hinton, 2009) and BERT (Devlin et al., 2018) fine-tuning on the emotions dataset (Saravia et al., 2018) from the Hugging Face Hub. We use internal cluster with the following hardware: AMD EPYC 7552 48-Core Processor, 512GiB RAM, NVIDIA A100 40GB GPU, $200g\\mathrm{b}$ user storagespace. ", "page_idx": 8}, {"type": "text", "text": "General comparison.We choose standard parameters for Adam ( $\\beta_{1}=0.9$ and $\\beta_{2}=0.999)$ that are default values in PyTorch and select the learning rate of $10^{-5}$ for all considered methods. We run KATE with different values of $\\eta\\in\\{0,10^{-1},10^{-2}\\}$ . For the image classification task, we normalize the images (similar to Horvath and Richtarik (2020)) and use a mini-batch size of 500. For the BERT fine-tuning, we use a mini-batch size 160 for all methods. ", "page_idx": 8}, {"type": "text", "text": "Figures 3-8 report the evolution of top-1 accuracy and cross-entropy loss (on the $y$ -axis)calculated on the test data. For the image classification task, we observe that KATE with different choices of $\\eta$ outperforms Adam and AdaGrad. Finally, we also observe that KATE performs comparably to Adam on the BERT fine-tuning task and is better than AdaGrad. These preliminary results highlight the potential of KATE to be applied for training neural networks for different tasks. For BERT each run takes about 35 minutes, and 25 minutes for ResNet. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Hyper-parameters tuning. Next, we compare baselines presented in Saravia et al. (2018) for emotions classification and Zhang et al. (2019) for image classification. These papers provide efficient setups for learning rates and learning rate schedulers that are reasonable to compare with. Saravia et al. (2018) performs a search of efficient learning rate and uses a linear learning rate scheduler with warmup for Adam optimizer. A different learning rate (1e-5), $\\Delta{=}1\\mathrm{e}{-}5$ and the same scheduler applied for KATE lead to the same performance, see Figure 9. We would like to point out that it is challenging to find a reference for hyper-parameters for a certain setup. Thus, to fairly compare with Saravia et al. (2018) we use distilrobertabase model. Zhang et al. (2019) did a grid search for an effcient learning rate and used a multi-step scheduler for Adam optimizer, decaying the learning rate by a factor of 5 at the 60th, 120th, and 160th epochs. Zhang et al. (2019) refers to DeVries and Taylor (2017) for the code implementing special techniques, namely data augmentation and cutout to achieve higher accuracy. A different learning rate (1e-3), the same scheduler and $\\scriptstyle\\Delta=1\\mathrm{e}-3$ appliedfor ", "page_idx": 9}, {"type": "image", "img_path": "EdG59dnOzN/tmp/013dbce3e9867173df3e6fcb9db354679394a3667c70337c99288b0e6bf3c665.jpg", "img_caption": ["Figure 9: Cifar10: $\\eta=0.001$ "], "img_footnote": [], "page_idx": 9}, {"type": "image", "img_path": "EdG59dnOzN/tmp/e6d8e04d7838c460ec8b78b2141afbaf5517d3fdf959698938368e4c0a28a283.jpg", "img_caption": [], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "KAIE demonstrates comparable performance, see Figure 10. For Figure 10: Emotion: $\\eta=0.001$ BERT each run takes about 20 minutes, while 100 minutes for ResNet. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Abdukhakimov, F., Xiang, C., Kamzolov, D., Gower, R., and Taka&, M. (2023). Sania: Polyak-type optimization framework leads to scale invariant stochastic algorithms. arXiv preprint arXiv:2312.17369. ", "page_idx": 9}, {"type": "text", "text": "Abdukhakimov, F., Xiang, C., Kamzolov, D., and Takac, M. (2024). Stochastic gradient descent with preconditioned polyak step-size. Computational Mathematics and Mathematical Physics, 64(4):621-634.   \nAgresti, A. (2015). Foundations of linear and generalized linear models. John Wiley & Sons.   \nArjevani, Y., Carmon, Y., Duchi, J. C., Foster, D. J., Srebro, N., and Woodworth, B. (2023). Lower bounds for non-convex stochastic optimization. Mathematical Programming, 199(1-2):165-214.   \nBeznosikov, A. and Taka&, M. (2021). Random-reshuffed sarah does not need a full gradient computations. In Optimizationfor MachineLearningWorkshop @NeurIPS 2021.   \nCarmon, Y., Duchi, J. C., Hinder, O., and Sidford, A. (2020). Lower bounds for finding stationary points i. MathematicalProgramming,184(1-2):71-120.   \nChang, C.-C. and Lin, C.-J. (2011). Libsvm: a library for support vector machines. ACM transactions on intelligent systems and technology (TIST), 2(3):1-27.   \nChezhegov, S., Skorik, S., Khachaturov, N., Shalagin, D., Avetisyan, A., Beznosikov, A., Takac, M., Kholodov, Y., and Gasnikov, A. (2024). Local methods with adaptivity via scaling. arXiv preprint arXiv:2406.00846.   \nDefazio, A. and Mishchenko, K. (2023). Learning-rate-free learning by D-adaptation. arXiv preprint arXiv:2301.07733.   \nD\u00e9fossez, A., Bottou, L., Bach, F., and Usunier, N. (2020). A simple convergence proof of adam and adagrad. arXiv preprint arXiv:2003.02395.   \nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2018). BERT: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.   \nDeVries, T. and Taylor, G. W. (2017). Improved regularization of convolutional neural networks with cutout. arXiv preprint arXiv:1708.04552.   \nD'Orazio, R., Loizou, N., Laradji, I., and Mitliagkas, I. (2021). Stochastic mirror descent: Convergence analysis and adaptive variants via the mirror stochastic polyak stepsize. arXiv preprint arXiv:2110.15412.   \nDuchi, J., Hazan, E., and Singer, Y. (2011). Adaptive subgradient methods for online learning and stochastic optimization. Journal of machine learning research,12(7).   \nFaw, M., Tziotis, I, Caramanis, C., Mokhtari, A., Shakkottai, S., and Ward, R. (2022). The power of adaptivity in sgd: Self-tuning step sizes with unbounded gradients and affine variance. In Conference on Learning Theory, pages 313-355. PMLR.   \nFrome, E. L. (1983). The analysis of rates using poisson regression models. Biometrics, pages 665-674.   \nGower, R. M., Defazio, A., and Rabbat, M. (2021). Stochastic polyak stepsize with a moving target. arXiv preprint arXiv:2106.11851.   \nHe, K.,Zhang, X., Ren, S., and Sun, J. (2016). Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770-778.   \nHe, X., Tappenden, R., and Takac, M. (2018). Dual free adaptive minibatch sdca for empirical risk minimization. Frontiers in Applied Mathematics and Statistics, 4:33.   \nHorvath, S. and Richtarik, P. (2020). A better alternative to error feedback for communication-efficient distributed learning. arXiv preprint arXiv:2006.11077.   \nHosmer Jr, D.W., Lemeshow, S., and Sturdivant, R. X. (2013). Applied logistic regression, volume 398. John Wiley & Sons.   \nKingma, D. P. and Ba, J (2014). Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980.   \nKrizhevsky, A. and Hinton, G. (2009). Learning multiple layers of features from tiny images.   \nLi, S., Swartworth, W. J., Takac, M., Needell, D., and Gower, R. M. (2023). Sp2: A second order stochastic polyak method. ICLR.   \nLi, X. and Orabona, F. (2019). On the convergence of stochastic gradient descent with adaptive stepsizes. In The 22nd international conference on artificial inteligence and statistics, pages 983-992. PMLR.   \nLiu, Z., Nguyen, T. D., Ene, A., and Nguyen, H. L. (2022). On the convergence of adagrad on $\\mathbb{R}^{d}$ : Beyond convexity, non-asymptotic rate and acceleration. arXiv preprint arXiv:2209.14827.   \nLoizou, N., Vaswani, S., Laradji, I. H., and Lacoste-Julien, S. (2021). Stochastic polyak step-size for sgd: An adaptive learning rate for fast convergence. In International Conference on Artificial Intelligence and Statistics, pages 1306-1314. PMLR.   \nMcMahan, H B.and Streeter, M. (2010). Adaptive bound optimization for online convex optimization. arXiv preprint arXiv: 1002.4908.   \nMishchenko, K. and Defazio, A. (2023). Prodigy: An expeditiously adaptive parameter-fre learner. arXiv preprint arXiv:2306.06101.   \nNelder, J. A. and Wedderburn, R. W.(1972). Generalized linear models. Journal of the Royal Statistical Society Series A: Statistics in Society, 135(3):370-384.   \nNguyen, L., Liu, J., Scheinberg, K., and Taka, M. (2017a). Sarah: A novel method for machine learning problems using stochastic recursive gradient. In In 34th International Conference on Machine Learning, ICML 2017.   \nNguyen, L. M., Liu, J., Scheinberg, K., and Takac, M. (2017b). Stochastic recursive gradient algorithm for nonconvex optimization. arXiv preprint arXiv:1705.07261.   \nNguyen, L. M., Nguyen, P. H., van Dijk, M., Richtarik, P., Scheinberg, K., and Takac, M. (2018). Sgd and hogwild! convergence without the bounded gradients assumption. In In 34th International Conference on Machine Learning, ICML 2018.   \nNguyen, L. M, Scheinberg, K, and Takac, M. (2021). Inexact sarah algorithm for stochastic optimization. Optimization Methods and Software, 36(1):237-258.   \nOberman, A. M. and Prazeres, M. (2019). Stochastic gradient descent with polyak's learning rate. arXiv preprint arXiv:1903.08688.   \nOrvieto, A., LacosteJulien, S., and Loizou, N. (2022). Dynamics of sgd with stochastic polyak stepsizes: Truly adaptive variants and convergence to exact solution. Advances in Neural Information Processing Systems, 35:26943-26954.   \nPolyak, B. T. (1969). Minimization of unsmooth functionals. USSR Computational Mathematics and Mathematical Physics, 9(3):14-29.   \nReddi, S. J., Kale, S., and Kumar, S. (2019). On the convergence of adam and beyond. arXiv preprint arXiv:1904.09237.   \nRobbins, H. and Monro, S. (1951). A stochastic approximation method. The annals of mathematical statistics, pages 400-407.   \nSaravia, E., Liu, H.-C. T., Huang, Y.-H., Wu, J., and Chen, Y-S. (2018). CARER: Contextualized affect representations for emotion recognition. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3687-3697, Brussels, Belgium. Association for Computational Linguistics.   \nShalev-Shwartz, S. and Ben-David, S. (2014). Understanding machine learning: From theory to algorithms. Cambridge university press.   \nShi, Z., Sadiev, A., Loizou, N., Richtarik, P., and Taka&, M. (2023). AI-SARAH: Adaptive and implicit stochastic recursive gradient methods. Transactions on Machine Learning Research.   \nTaka, M., Bijral, A., Richtarik, P., and Srebro, N. (2013). Mini-batch primal and dual methods for svms. In In 30th International Conference on Machine Learning, ICML 2013.   \nTieleman, T. and Hinton, G. (2012). Rmsprop: Divide the gradient by a running average of its recent magnitude. coursera: Neural networks for machine learning. COURSERA Neural Networks Mach. Learn, 17.   \nWard, R., Wu, X., and Bottou, L. (2020). Adagrad stepsizes: Sharp convergence over nonconvex landscapes. TheJournal of MachineLearningResearch,21(1):9047-9076.   \nXie, Y., Wu, X., and Ward, R. (2020). Linear convergence of adaptive stochastic gradient descent. In International conference on artificial intelligence and statistics, pages 1475-1485. PMLR.   \nZhang, M., Lucas, J., Ba, J., and Hinton, G. E. (2019). Lookahead optimizer: k steps forward, 1 step back. Advances in neural information processing systems, 32. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "Supplementary Material ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Contents ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Introduction 1.1 Related Work 2 1.2 Main Contribution 3 1.3 Notation 3 ", "page_idx": 12}, {"type": "text", "text": "2  Motivation and Algorithm Design 3 ", "page_idx": 12}, {"type": "text", "text": "3 Convergence Analysis 5   \n3.1 Assumptions 5   \n3.2 Main Results 6   \nNumerical Experiments   \n4.1 Logistic Regression 7   \n4.1.1 Robustness of KATE 7   \n4.1.2 Peformance of KATE on Real Data 8   \n4.2 Training of Neural Networks 9 ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "ATechnical Lemmas 14 ", "page_idx": 12}, {"type": "text", "text": "ProoI o1 MIain Kesuts 18   \nB.1 Proof of Proposition 2.1 18   \nB.2 Proof of Lemma 2.2 19   \nB.3Proof of Theorem 3.3 20   \nB.4Proof of Theorem 3.4 22 ", "page_idx": 12}, {"type": "text", "text": "C   Additional Experiments: Scale-Invariance Verification 25 ", "page_idx": 12}, {"type": "text", "text": "A Technical Lemmas ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Lemma A.1 (AM-GM). For $\\lambda>0$ we have ", "page_idx": 13}, {"type": "equation", "text": "$$\na b\\leq\\frac{\\lambda}{2}a^{2}+\\frac{1}{2\\lambda}b^{2}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Lemma A.2 (Cauchy-Schwarz Inequality). For $a_{1},\\dotsc\\dotsc,a_{n},b_{1},\\dotsc\\dotsc,b_{n}\\in\\mathbb{R}$ wehave ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\left(\\sum_{i=1}^{n}a_{i}^{2}\\right)\\left(\\sum_{i=1}^{n}b_{i}^{2}\\right)\\;\\;\\geq\\;\\;\\left(\\sum_{i=1}^{n}a_{i}b_{i}\\right)^{2}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Lemma A.3 (Holder's Inequality). Suppose $X,Y$ are two random variables and $p,q>1$ satisfy $\\textstyle{\\frac{1}{p}}+{\\frac{1}{q}}=1$ Then ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left(|X Y|\\right)\\leq\\left(\\mathbb{E}\\left(|X|^{p}\\right)\\right)^{\\frac{1}{p}}\\left(\\mathbb{E}\\left(|Y|^{q}\\right)\\right)^{\\frac{1}{q}}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Lemma A.4 (Jensen's Inequality). For a convex function $g:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ and a random variable $X$ such that $\\mathbb{E}(\\Psi(X))$ and $\\Psi\\left(\\mathbb{E}(X)\\right)$ are finite, we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\Psi\\,(\\mathbb{E}(X))\\leq\\mathbb{E}(\\Psi(X)).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Lemma A.5. For $a_{1},a_{2},\\cdot\\cdot\\cdot\\,,a_{n}\\geq0$ and $b_{1},b_{2},\\cdot\\cdot\\cdot\\,,b_{n}>0$ we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{n}{\\frac{a_{i}}{\\sqrt{b_{i}}}}\\geq{\\frac{\\sum_{i=1}^{n}a_{i}}{\\sqrt{\\sum_{i=1}^{n}b_{i}}}}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Proof. Expanding the LHS of (21) we get ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{l l l}{\\displaystyle\\left(\\sum_{i=1}^{n}\\frac{a_{i}}{\\sqrt{b_{i}}}\\right)^{2}}&{=}&{\\displaystyle\\sum_{i=1}^{n}\\frac{a_{i}^{2}}{b_{i}}+2\\sum_{i\\neq j}\\frac{a_{i}a_{j}}{\\sqrt{b_{i}b_{j}}}}\\\\ &{\\displaystyle\\ge}&{\\displaystyle\\sum_{i=1}^{n}\\frac{a_{i}^{2}}{b_{i}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "The last inequality folows roem $\\begin{array}{r}{\\frac{a_{i}}{\\sqrt{b_{i}}}\\geq0}\\end{array}$ for alli $i\\in[n]$ w uinahaqult ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\left(\\sum_{i=1}^{n}{\\frac{a_{i}^{2}}{b_{i}}}\\right)\\left(\\sum_{i=1}^{n}b_{i}\\right)\\ \\ \\geq\\ \\ \\left(\\sum_{i=1}^{n}a_{i}\\right)^{2}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Then combining (22) and (23), we get ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\left(\\sum_{i=1}^{n}{\\frac{a_{i}}{\\sqrt{b_{i}}}}\\right)^{2}\\left(\\sum_{i=1}^{n}b_{i}\\right)\\quad\\geq\\quad\\left(\\sum_{i=1}^{n}a_{i}\\right)^{2}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Finally dividing both sides by $\\textstyle\\sum_{i=1}^{n}b_{i}$ andtaking square rtweget thedesiredreult ", "page_idx": 13}, {"type": "image", "img_path": "EdG59dnOzN/tmp/ece055977e91133fffe88cbc47249bca0f7a32d6da7d98a4743e0bd30fa0b1d9.jpg", "img_caption": [], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "Proof. Note that, using $\\begin{array}{r}{\\nu_{t}[k]\\ge\\frac{\\beta\\sqrt{\\eta[k]}}{b_{t}\\left[k\\right]}}\\end{array}$ we have ", "page_idx": 14}, {"type": "equation", "text": "-vt[] $$\n\\begin{array}{r l}&{\\sqrt{\\beta}\\int_{\\Omega}\\left(1+\\nabla r_{1}(\\rho_{2})\\right)^{\\alpha}+\\sigma^{2}\\nu^{\\alpha\\beta}}\\\\ {\\leq}&{\\delta\\exp\\left(\\frac{1}{\\rho_{2}\\pi_{2}^{\\alpha}}\\right)+(1-\\frac{\\alpha}{\\gamma})\\int_{\\Omega}\\left(1+\\sigma_{2}^{2}\\nu\\right)^{\\alpha-\\beta}}\\\\ &{+\\delta\\exp\\left(\\frac{1}{\\rho_{2}\\pi_{2}^{\\alpha}}\\right)}\\\\ {=}&{\\delta\\exp\\left(\\frac{\\alpha\\int_{\\Omega}\\left(1+\\sigma_{2}\\nu\\right)^{\\alpha}-(1+\\sigma_{2}\\nu)^{2}-(1+\\sigma_{2}\\nu)^{2}-\\sigma^{2}}{\\left(1+\\sigma_{2}\\nu\\right)\\left(1+\\sigma_{2}\\nu\\right)^{2}+\\sigma^{2}\\nu^{\\alpha\\beta}}\\right)\\times}\\\\ &{-\\delta\\exp\\left(\\frac{\\alpha\\int_{\\Omega}\\left(1-\\sigma_{2}\\nu\\right)^{\\alpha}-(1+\\sigma_{2}\\nu)\\left(1-\\sigma_{2}\\nu\\right)^{\\alpha-\\beta}}{\\left(1+\\sigma_{2}\\nu\\right)\\left(1+\\sigma_{2}\\nu\\right)^{\\alpha}+\\sigma^{2}\\nu^{\\alpha\\beta}}\\right)\\times}\\\\ {=}&{\\delta\\exp\\left(\\frac{\\beta\\int_{\\Omega}\\left(1+\\sigma_{2}\\nu\\right)^{\\alpha-\\beta}+\\sigma^{2}\\left(1+\\sigma_{2}\\nu\\right)^{\\alpha-\\beta}}{\\left(1+\\sigma_{2}\\nu\\right)\\left(1+\\sigma_{2}\\nu\\right)^{\\alpha}+\\sigma^{2}\\nu^{\\alpha\\beta}}\\right)\\times}\\\\ &{\\geq\\delta\\exp\\left(\\frac{\\alpha\\int_{\\Omega}\\left(1+\\sigma_{2}\\nu\\right)^{\\alpha}-(1+\\sigma_{2}\\nu)\\left(1+\\sigma_{2}\\nu\\right)-\\sigma^{2}}{\\left(1+\\sigma_{2}\\nu\\right)\\left(1+\\sigma_{2}\\nu\\right)^{\\alpha}+\\sigma^{2}\\nu^{\\alpha\\beta}}\\right)\\times}\\\\ {\\leq}&{\\frac{\\delta\\exp\\left(\\frac{\\alpha\\int_{\\Omega}\\left(1+\\sigma_{2}\\nu\\right)^{\\alpha}+\\sigma^{2}\\left(1-\\sigma_{2}\\nu\\right)\\left(1+\\sigma_{2}\\nu\\right)-\\sigma^{2}}{\\left(1+\\sigma_{2}\\nu\\right)\\left(1+\\sigma_{2}\\nu\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Note that the second last inequality follows from the use of triangle inequality in the following way ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\left(g_{t}[k]+\\nabla_{k}f(w_{t})\\right)\\left(g_{t}[k]-\\nabla_{k}f(w_{t})\\right)-\\sigma^{2}}&{\\leq}&{\\left|\\left(g_{t}[k]+\\nabla_{k}f(w_{t})\\right)\\left(g_{t}[k]-\\nabla_{k}f(w_{t})\\right)-\\sigma^{2}\\right|}\\\\ &{}&{\\left|\\left(g_{t}[k]+\\nabla_{k}f(w_{t})\\right)\\left(g_{t}[k]-\\nabla_{k}f(w_{t})\\right)\\right|+\\sigma^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "while the last inequality follows from ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{b_{t}[k]+\\sqrt{b_{t-1}^{2}[k]+\\left(\\nabla_{k}f(w_{t})\\right)^{2}+\\sigma^{2}}}&{\\ge\\quad|g_{t}[k]|+|\\nabla_{k}f(w_{t})|\\ge|g_{t}[k]+\\nabla_{k}f(w_{t})|\\,,}\\\\ {b_{t}[k]+\\sqrt{b_{t-1}^{2}[k]+\\left(\\nabla_{k}f(w_{t})\\right)^{2}+\\sigma^{2}}}&{\\ge\\quad\\sigma.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Then from (25) we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{t}\\left[\\left(\\frac{\\beta\\sqrt{\\eta[k]}}{\\sqrt{b_{t-1}^{2}[k]+(\\nabla_{k}f(w_{t}))^{2}+\\sigma^{2}}}-\\nu_{t}[k]\\right)\\nabla_{k}f(w_{t})g_{t}[k]\\right]}\\\\ {\\leq}&{\\underbrace{\\beta\\sqrt{\\eta[k]}\\mathbb{E}_{t}\\left[\\frac{\\left\\vert g_{t}\\left[k\\right]-\\nabla_{k}f(w_{t})\\right\\vert\\left\\vert\\nabla_{k}f(w_{t})\\right\\vert\\left\\vert g_{t}\\left[k\\right]\\right\\vert}{b_{t}[k]\\sqrt{b_{t-1}^{2}[k]+(\\nabla_{k}f(w_{t}))^{2}+\\sigma^{2}}}\\right]}_{\\mathrm{tem1}}}\\\\ &{+\\underbrace{\\beta\\sqrt{\\eta[k]}\\mathbb{E}_{t}\\left[\\frac{\\sigma\\left\\vert\\nabla_{k}f(w_{t})\\right\\vert\\left\\vert g_{t}\\left[k\\right]\\right\\vert}{b_{t}[k]\\sqrt{b_{t-1}^{2}[k]+(\\nabla_{k}f(w_{t}))^{2}+\\sigma^{2}}}\\right].}_{\\mathrm{f}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "For term Iin (26), we use Lemma A.1 with ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lambda}&{=}&{\\cfrac{2\\sigma^{2}}{\\sqrt{b_{t-1}^{2}[k]+(\\nabla_{k}f(w_{t}))^{2}+\\sigma^{2}}},}\\\\ {a}&{=}&{\\cfrac{|g_{t}[k]|}{b_{t}[k]},}\\\\ {b}&{=}&{\\cfrac{|g_{t}[k]-\\nabla_{k}f(w_{t})|\\,|\\nabla_{k}f(w_{t})|}{\\sqrt{b_{t-1}^{2}[k]+(\\nabla_{k}f(w_{t}))^{2}+\\sigma^{2}}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "to get ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\beta\\sqrt{\\eta|\\mathcal{k}|}\\mathbb{E}_{t}\\left[\\frac{\\vert g_{t}[\\boldsymbol{k}]-\\nabla_{\\boldsymbol{k}}f(w_{t})\\vert\\,\\vert\\nabla_{\\boldsymbol{k}}f(w_{t})\\vert\\,\\vert g_{t}[\\boldsymbol{k}]\\vert}{b_{t}[\\boldsymbol{k}]\\sqrt{b_{t-1}^{2}[k]+(\\nabla_{\\boldsymbol{k}}f(w_{t}))^{2}+\\sigma^{2}}}\\right]}\\\\ {\\leq}&{\\frac{\\beta\\sqrt{\\eta|\\mathcal{k}|}\\sqrt{b_{t-1}^{2}[k]}\\,\\vert\\boldsymbol{k}\\vert+(\\nabla_{\\boldsymbol{k}}f(w_{t}))^{2}+\\sigma^{2}}{4\\sigma^{2}}\\frac{(\\nabla_{\\boldsymbol{k}}f(w_{t}))^{2}\\,\\mathbb{E}_{t}\\left[g_{t}[\\boldsymbol{k}]-\\nabla_{\\boldsymbol{k}}f(w_{t})\\right]^{2}}{b_{t-1}^{2}\\left[k\\right]+(\\nabla_{\\boldsymbol{k}}f(w_{t}))^{2}+\\sigma^{2}}}\\\\ &{+\\frac{\\beta\\sqrt{\\eta|\\mathcal{k}|}\\sigma^{2}}{\\sqrt{b_{t-1}^{2}[k]+(\\nabla_{\\boldsymbol{k}}f(w_{t}))^{2}+\\sigma^{2}}}\\mathbb{E}_{t}\\left[\\frac{g_{t}^{2}[\\boldsymbol{k}]}{b_{t}^{2}[\\boldsymbol{k}]}\\right]}\\\\ {\\leq}&{\\frac{\\beta\\sqrt{\\eta|\\mathcal{k}|}\\left(\\nabla_{\\boldsymbol{k}}f(w_{t})\\right)^{2}}{4\\sqrt{b_{t-1}^{2}[k]+(\\nabla_{\\boldsymbol{k}}f(w_{t}))^{2}+\\sigma^{2}}}+\\beta\\sqrt{\\eta|\\mathcal{k}|}\\sigma\\mathbb{E}_{t}\\left[\\frac{g_{t}^{2}[\\boldsymbol{k}]}{b_{t}^{2}[\\boldsymbol{k}]}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The last inequality follows from BV. Similarly, we again use Lemma A.1 with ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\lambda}&{=}&{\\displaystyle\\frac{2}{\\sqrt{b_{t-1}^{2}[k]+(\\nabla_{k}f(w_{t}))^{2}+\\sigma^{2}}},}\\\\ {a}&{=}&{\\displaystyle\\frac{\\sigma\\,|g_{t}[k]|}{b_{t}[k]},}\\\\ {b}&{=}&{\\displaystyle\\frac{|\\nabla_{k}f(w_{t})|}{\\sqrt{b_{t}^{2}[k]+(\\nabla_{k}f(w_{t}))^{2}+\\sigma^{2}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and $\\sqrt{b_{t}^{2}[k]+\\left(\\nabla_{k}f(w_{t})\\right)^{2}+\\sigma^{2}}\\ge\\sigma$ to get ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\beta\\sqrt{\\eta[k]}\\mathbb{E}_{t}\\left[\\frac{\\sigma\\left|\\nabla_{k}f(w_{t})\\right|\\left|g_{t}\\left[k\\right]\\right|}{b_{t}[k]\\sqrt{b_{t-1}^{2}[k]+(\\nabla_{k}f(w_{t}))^{2}+\\sigma^{2}}}\\right]}&{\\leq}&{\\beta\\sqrt{\\eta[k]}\\sigma\\mathbb{E}_{t}\\left[\\frac{g_{t}^{2}[k]}{b_{t}^{2}[k]}\\right]}\\\\ &{}&{+\\frac{\\beta\\sqrt{\\eta[k]}\\left(\\nabla_{k}f(w_{t})\\right)^{2}}{4\\sqrt{b_{t-1}^{2}[k]+(\\nabla f(w_{t}))^{2}+\\sigma^{2}}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Therefore using (27) and (28) in (27) we get ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\mathbb{\\xi}_{t}\\left[\\left(\\frac{\\beta\\sqrt{\\eta[k]}}{\\sqrt{b_{t-1}^{2}[k]+(\\nabla_{k}f(w_{t}))^{2}+\\sigma^{2}}}-\\nu_{t}[k]\\right)\\nabla_{k}f(w_{t})g_{t}[k]\\right]}&{\\le}&{2\\beta\\sqrt{\\eta[k]}\\sigma\\mathbb{E}_{t}\\left[\\frac{g_{t}^{2}[k]}{b_{t}^{2}[k]}\\right]}\\\\ &{}&{+\\frac{\\beta\\sqrt{\\eta[k]}\\left(\\nabla_{k}f(w_{t})\\right)^{2}}{2\\sqrt{b_{t-1}^{2}[k]+(\\nabla f(w_{t}))^{2}+\\sigma^{2}}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "This completes the proof of this Lemma. ", "page_idx": 16}, {"type": "text", "text": "Lemma A.7. ", "text_level": 1, "page_idx": 16}, {"type": "equation", "text": "$$\n\\sum_{t=0}^{T}\\frac{g_{t}^{2}[k]}{b_{t}^{2}[k]}\\leq\\log\\left(\\frac{b_{T}^{2}[k]}{b_{0}^{2}[k]}\\right)+1\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof. Using $\\begin{array}{r}{b_{t}^{2}[k]=\\sum_{\\tau=0}^{t}g_{\\tau}^{2}[k]}\\end{array}$ we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{{\\displaystyle\\sum_{t=0}^{T}\\hat{b}_{t}^{2}[k]}}&{{=}}&{{{\\displaystyle1+\\frac{T}{\\epsilon_{1}+b_{i}^{2}}\\,g_{i}^{2}[k]}}}\\\\ {{}}&{{=}}&{{{\\displaystyle1+\\frac{T}{\\epsilon_{1}+b_{i}^{2}}\\,\\hat{b}_{i}^{2}[k]-b_{i-1}^{2}[k]}}}\\\\ {{}}&{{=}}&{{{\\displaystyle1+\\frac{T}{\\epsilon_{1}+b_{i}^{2}}\\,\\hat{b}_{i}^{2}[k]}\\,\\hat{b}_{i-1}^{2}[k]}}\\\\ {{}}&{{\\leq}}&{{{\\displaystyle1+\\frac{T}{\\epsilon_{1}+b_{i-1}^{2}}\\,\\hat{b}_{i}^{2}[k]}\\,\\frac{d z}{2}}}\\\\ {{}}&{{=}}&{{{\\displaystyle1+\\int_{\\hat{b}_{\\hat{b}_{\\hat{b}}}^{2}[k]}\\frac{d z}{2}\\,\\hat{b}_{i}^{2}[k]}}}\\\\ {{}}&{{=}}&{{{\\displaystyle1+\\log\\left(\\frac{b_{i}^{2}[k]}{b_{i}^{2}[k]}\\right)}\\,.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The inequalty folows from thefat $\\begin{array}{r}{\\frac{1}{b_{t}^{2}[k]}\\,\\leq\\,\\frac{1}{z}}\\end{array}$ when $b_{t-1}^{2}[k]\\leq z\\leq b_{t}^{2}[k]$ Thiscompletes the proof of the Lemma. \u53e3 ", "page_idx": 16}, {"type": "text", "text": "B Proof of Main Results ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "B.1Proof of Proposition 2.1 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Proposition B.1 (Scale invariance). Suppose we solve problems (4) and (5) using algorithm (6). Then, the iterates $\\hat{w}_{t}$ and $\\hat{w}_{t}^{V}$ corresponding to (4) and (5) follow: $\\bar{\\forall}k\\in[d]$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\hat{w}_{t+1}[k]}&{=}&{\\hat{w}_{t}[k]-\\frac{\\beta m_{t}[k]}{\\sum_{\\tau=0}^{t}\\!g_{\\tau}^{2}\\left[k\\right]}g_{t}[k],}\\\\ {\\hat{w}_{t+1}^{V}[k]}&{=}&{\\hat{w}_{t}^{V}[k]-\\frac{\\beta m_{t}[k]}{\\sum_{\\tau=0}^{t}\\,\\left(g_{\\tau}^{V}\\left[k\\right]\\right)^{2}}g_{t}^{V}[k]}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "with $g_{\\tau}=\\varphi_{i_{\\tau}}^{\\prime}(x_{i_{\\tau}}^{\\top}\\hat{w}_{\\tau})x_{i_{\\tau}}$ and $g_{\\tau}^{V}=\\varphi_{i_{\\tau}}^{\\prime}(x_{i_{\\tau}}^{\\top}V\\hat{w}_{\\tau})V x_{i_{\\tau}}$ for $i_{\\tau}$ chosen uniformly from $[n]$ $,\\tau=0,1,\\ldots,t$ $t\\geq0$ . Moreover, updates (30) and (31) satisfy ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\hat{w}_{t}=V\\hat{w}_{t}^{V},\\quad V g_{t}=g_{t}^{V},\\quad f\\left(\\hat{w}_{t}\\right)=f^{V}\\left(\\hat{w}_{t}^{V}\\right)\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "for all $t\\geq0$ when $\\hat{w}_{0}=\\hat{w}_{0}^{V}=0\\in\\mathbb{R}^{d}$ . Furthermore we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\left\\|g_{t}^{V}\\right\\|_{V^{-2}}^{2}}&{{}=}&{\\left\\|g_{t}\\right\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof. First, we will show $\\hat{w}_{t}=V\\hat{w}_{t}^{V}$ and $V g_{t}=g_{t}^{V}$ using induction. Note that for $\\tau=1$ and $k\\in[d]$ we get ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{{\\hat{w}_{1}[k]}}&{{=}}&{{\\frac{-\\beta m_{0}[k]\\varphi_{i_{0}}^{\\prime}(0)x_{i_{0}}[k]}{\\left(\\varphi_{i_{0}}^{\\prime}(0)x_{i_{0}}[k]\\right)^{2}}=\\frac{-\\beta m_{0}[k]}{\\varphi_{i_{0}}^{\\prime}(0)x_{i_{0}}[k]},}}\\\\ {{\\hat{w}_{1}^{V}[k]}}&{{=}}&{{\\frac{-\\beta m_{0}[k]\\varphi_{i_{0}}^{\\prime}(0)V_{k k}x_{i_{0}}[k]}{\\left(\\varphi_{i_{0}}^{\\prime}(0)V_{k k}x_{i_{0}}[k]\\right)^{2}}=\\frac{-\\beta m_{0}[k]}{\\varphi_{i_{0}}^{\\prime}(0)V_{k k}x_{i_{0}}[k]}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "as $\\hat{w}_{0}\\,=\\,\\hat{w}_{0}^{V}\\,=\\,0$ . Therefore, we have $\\forall k\\in[d],\\hat{w}_{1}[k]=V_{k k}\\hat{w}_{1}^{V}[k]$ . This can be equivalently written as $\\hat{w}_{1}=V\\hat{w}_{1}^{V}$ ,as $V$ is a diagonal matrix. Then it is easy to check ", "page_idx": 17}, {"type": "equation", "text": "$$\nV g_{1}=\\varphi_{i_{1}}^{\\prime}\\left(x_{i_{1}}^{\\top}\\hat{w}_{1}\\right)V x_{i_{1}}=\\varphi_{i_{1}}^{\\prime}\\left(x_{i_{1}}^{\\top}V\\hat{w}_{1}^{V}\\right)V x_{i_{1}}=g_{1}^{V},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the second equality follows from $\\hat{w}_{1}=V\\hat{w}_{1}^{V}$ . Now, we assume the proposition holds for $\\tau=1,\\cdots\\,,t$ Then, we need to prove this proposition for $\\tau=t+1$ . Note that, from (7) we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{w}_{t+1}[k]=\\hat{w}_{t}[k]-\\frac{\\beta m_{t}[k]}{\\sum_{\\tau=0}^{t}g_{\\tau}^{2}[k]}g_{t}[k]=V_{k k}\\hat{w}_{t}^{V}[k]-\\frac{\\beta m_{t}[k]V_{k k}^{2}}{\\sum_{\\tau=0}^{t}\\left(g_{\\tau}^{V}[k]\\right)^{2}}\\frac{g_{t}^{V}[k]}{V_{k k}}=V_{k k}\\hat{w}_{t+1}^{V}[k].}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Here, the second last equality follows from $\\hat{w}_{\\tau}\\,=\\,V\\hat{w}_{\\tau}^{V}$ and $V g_{\\tau}\\,=\\,g_{\\tau}^{V}\\quad\\forall\\tau\\,\\in\\,[t]$ , while the last equality holds due to (31). Therefore, we have $\\hat{w}_{t+1}\\,=\\,V\\hat{w}_{t+1}^{V}$ . Then similar to (33) we get $V g_{t+1}\\,=\\,g_{t+1}^{V}$ using $\\hat{w}_{t+1}=V\\hat{w}_{t+1}^{V}$ . Again, using $\\hat{w}_{t}=V\\hat{w}_{t}^{V}$ , we can rewrite $f(\\hat{w}_{t})$ as follow ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{f(\\hat{w}_{t})=\\frac{1}{n}\\sum_{i=1}^{n}\\varphi_{i}\\left(x_{i}^{\\top}\\hat{w}_{t}\\right)\\,=\\frac{1}{n}\\sum_{i=1}^{n}\\varphi_{i}\\left(x_{i}^{\\top}V\\hat{w}_{t}^{V}\\right)\\,=f^{V}\\left(\\hat{w}_{t}^{V}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The last equality follows from (5). This proves $f(\\hat{w}_{t})=f^{V}\\left(\\hat{w}_{t}^{V}\\right)$ . Finally using $V g_{t}=g_{t}^{V}$ weget ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\left\\|g_{t}^{V}\\right\\|_{V^{-2}}^{2}=\\left(g_{t}^{V}\\right)^{\\top}V^{-2}g_{t}^{V}=g_{t}^{\\top}V V^{-2}V g_{t}=\\left\\|g_{t}\\right\\|^{2}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "This completes the proof of Proposition 2.1. ", "page_idx": 17}, {"type": "text", "text": "B.2Proof of Lemma 2.2 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Lemma B.2 (Decreasing step size). For $\\nu_{t}[k]$ defined in (11) we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\nu_{t+1}[k]\\leq\\nu_{t}[k]\\qquad\\forall k\\in[d].\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof. We want to show that $\\nu_{t+1}[k]\\le\\nu_{t}[k]$ . Taking square and rearranging the terms (13) is equivalent to proving ", "page_idx": 18}, {"type": "equation", "text": "$$\nb_{t}^{4}[k]m_{t+1}^{2}[k]\\leq b_{t+1}^{4}[k]m_{t}^{2}[k].\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Using the expansion of $m_{t+1}^{2}[k],b_{t+1}^{2}[k]$ , LHS of (34) can be expanded as follow ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{b_{t}^{4}[k]m_{t+1}^{2}[k]}&{=}&{b_{t}^{4}[k]\\left(m_{t}^{2}[k]+\\eta[k]g_{t+1}^{2}[k]+\\frac{g_{t+1}^{2}[k]}{b_{t}^{2}[k]+g_{t+1}^{2}[k]}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Similarly, the RHS of (34) can be expanded to ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{{b_{t+1}^{4}[k]m_{t}^{2}[k]}}&{{=}}&{{m_{t}^{2}[k]\\left(b_{t}^{2}[k]+g_{t+1}^{2}[k]\\right)^{2}}}\\\\ {{}}&{{=}}&{{m_{t}^{2}[k]b_{t}^{4}[k]+m_{t}^{2}[k]g_{t+1}^{4}[k]+2m_{t}^{2}[k]g_{t+1}^{2}[k]b_{t}^{2}[k].}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Therefore using (35) and (36), inequality (34) is equivalent to ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{b_{t}^{4}[k]\\left(m_{t}^{2}[k]+\\eta[k]g_{t+1}^{2}[k]+\\frac{g_{t+1}^{2}[k]}{b_{t}^{2}[k]+g_{t+1}^{2}[k]}\\right)}&{\\leq}&{m_{t}^{2}[k]b_{t}^{4}[k]+m_{t}^{2}[k]g_{t+1}^{4}[k]}\\\\ &&{+2m_{t}^{2}[k]g_{t+1}^{2}[k]b_{t}^{2}[k].}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Now subtracting $m_{t}^{2}[k]b_{t}^{4}[k]$ from both sides of (37) and then multiplying both sides by $b_{t}^{2}[k]+g_{t+1}^{2}[k]$ (37)is equivalentto ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\eta[k]g_{t+1}^{2}[k]b_{t}^{6}[k]+\\eta[k]g_{t+1}^{4}[k]b_{t}^{4}[k]+g_{t+1}^{2}[k]b_{t}^{4}[k]}&{\\le}&{m_{t}^{2}[k]g_{t+1}^{4}[k]b_{t}^{2}[k]+2m_{t}^{2}[k]g_{t+1}^{2}[k]b_{t}^{4}[k]}\\\\ &{}&{+m_{t}^{2}[k]g_{t+1}^{6}[k]+2m_{t}^{2}[k]g_{t+1}^{4}[k]b_{t}^{2}[k].~~~\\mathrm{()}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Therefore, proving (13) is equivalent to proving (38). Note that, from the expansion $m_{t}^{2}[k]\\,=\\,\\eta[k]b_{t}^{2}[k]\\,+$ $\\scriptstyle\\sum_{\\tau=0}^{t}{\\frac{g_{t}^{2}[k]}{b_{t}^{2}[k]}}$ wc have $\\begin{array}{r}{m_{t}^{2}[k]\\ge\\frac{g_{0}^{2}[k]}{b_{0}^{2}[k]}=1}\\end{array}$ $m_{t}^{2}[k]\\geq\\eta[k]b_{t}^{2}[k]$ Thnusig $m_{t}^{2}[k]\\geq1$ Wwegetr ", "page_idx": 18}, {"type": "equation", "text": "$$\ng_{t+1}^{4}[k]b_{t}^{2}[k]\\leq m_{t}^{2}[k]g_{t+1}^{4}[k]b_{t}^{2}[k].\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Again, using $m_{t}^{2}[k]\\geq\\eta[k]b_{t}^{2}[k]$ we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\eta[k]g_{t+1}^{2}[k]b_{t}^{6}[k]+\\eta[k]g_{t+1}^{4}[k]b_{t}^{4}[k]}&{\\leq}&{m_{t}^{2}[k]g_{t+1}^{2}[k]b_{t}^{4}[k]+m_{t}^{2}[k]g_{t+1}^{4}[k]b_{t}^{2}[k].}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Then adding (39) and (40) we get ", "page_idx": 18}, {"type": "text", "text": "$\\begin{array}{r l r}{\\eta[k]g_{t+1}^{2}[k]b_{t}^{6}[k]+\\eta[k]g_{t+1}^{4}[k]b_{t}^{4}[k]+g_{t+1}^{2}[k]b_{t}^{4}[k]}&{\\le}&{m_{t}^{2}[k]g_{t+1}^{4}[k]b_{t}^{2}[k]+2m_{t}^{2}[k]g_{t+1}^{2}[k]b_{t}^{4}[k]\\beta_{t+1}^{2}.}\\end{array}$ (41) Therefore, (38) is true due to (41) and $m_{t}^{2}[k]g_{t+1}^{6}[k]+2m_{t}^{2}[k]g_{t+1}^{4}[k]b_{t}^{2}[k]\\geq0.$ This completes the proof of the Lemma. \u53e3 ", "page_idx": 18}, {"type": "text", "text": "B.3Proof of Theorem 3.3 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Theorem B.3. Suppose $f$ .s $L$ smooth, $g_{t}=\\nabla f(w_{t})$ and $\\eta,\\beta$ are chosen such that $\\begin{array}{r}{\\nu_{0}[k]\\leq\\frac{1}{L}}\\end{array}$ for all $k\\in[d]$ Then for (11) we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{t\\le T}\\|\\nabla f(w_{t})\\|^{2}\\le\\frac{1}{T+1}\\left(\\sum_{k=1}^{d}b_{0}[k]+\\frac{2(f(w_{0})-f_{*})}{\\sqrt{\\eta}\\beta}\\right)^{2}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof. Suppose $g_{t}=\\nabla f(w_{t})$ . Then using the smoothness of $f$ we get ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l c l}{f(w_{T+1})}&{\\leq}&{f(w_{T})+\\langle g_{T},w_{T+1}-w_{T}\\rangle+\\displaystyle\\frac{L}{2}\\left\\|w_{T+1}-w_{T}\\right\\|^{2}}\\\\ &{=}&{f(w_{T})+\\displaystyle\\sum_{k=1}^{d}g_{T}[k]\\left(w_{T+1}[k]-w_{T}[k]\\right)+\\displaystyle\\frac{L}{2}\\sum_{k=1}^{d}\\left(w_{T+1}[k]-w_{T}[k]\\right)^{2}}\\\\ &{=}&{f(w_{T})-\\displaystyle\\sum_{k=1}^{d}\\nu_{T}[k]g_{T}^{2}[k]+\\displaystyle\\frac{L}{2}\\sum_{k=1}^{d}\\nu_{T}^{2}[k]g_{T}^{2}[k]}\\\\ &{=}&{f(w_{T})-\\displaystyle\\sum_{k=1}^{d}\\nu_{T}[k]\\left(1-\\nu_{T}[k]\\displaystyle\\frac{L}{2}\\right)g_{T}^{2}[k].}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Then using this bound recursively we get ", "page_idx": 19}, {"type": "equation", "text": "$$\nf(w_{T+1})\\quad\\leq\\quad f\\bigl(w_{0}\\bigr)-\\sum_{t=0}^{T}\\sum_{k=1}^{d}\\nu_{t}[k]\\left(1-\\nu_{t}[k]\\frac{L}{2}\\right)g_{t}^{2}[k].\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Note that, we initialized KATE such that $\\begin{array}{r}{\\nu_{0}[k]\\le\\frac{1}{L}\\forall k\\in[d]}\\end{array}$ . Therefore using Lemma 2.2 we have $\\begin{array}{r}{\\nu_{t}[k]\\leq\\frac{1}{L}}\\end{array}$ which is equivalent to $\\begin{array}{r}{1-\\nu_{t}[k]\\frac{L}{2}\\geq\\frac{1}{2}}\\end{array}$ for all $k\\in[d]$ . Hence from (42) we have ", "page_idx": 19}, {"type": "equation", "text": "$$\nf(w_{T+1})\\quad\\leq\\quad f(w_{0})-\\sum_{t=0}^{T}\\sum_{k=1}^{d}\\frac{\\nu_{t}[k]}{2}g_{t}^{2}[k].\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Then rearranging the terms and using $f(w_{T+1})\\geq f_{*}$ we get ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\sum_{t=0}^{T}\\sum_{k=1}^{d}\\frac{\\nu_{t}[k]}{2}g_{t}^{2}[k]\\leq f(w_{0})-f_{*}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Then from (42) and $m_{t}[k]\\geq\\sqrt{\\eta_{0}}b_{t}[k]$ we get ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\sum_{t=0}^{T}\\sum_{k=1}^{d}\\frac{g_{t}^{2}[k]}{b_{t}[k]}\\le\\frac{2(f(w_{0})-f_{*})}{\\sqrt{\\eta_{0}}\\beta}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Now from the definition of $b_{t}^{2}[k]$ , we have $b_{t}^{2}[k]=b_{t-1}^{2}[k]+g_{t}^{2}[k]$ . This can be rearranged to get ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{b_{T}[k]}&{=}&{b_{T-1}[k]+\\displaystyle\\frac{g_{T}^{2}[k]}{b_{T}[k]+b_{T-1}[k]}}\\\\ &{\\leq}&{b_{T-1}[k]+\\displaystyle\\frac{g_{T}^{2}[k]}{b_{T}[k]}}\\\\ &{\\leq}&{b_{0}[k]+\\displaystyle\\sum_{t=0}^{T}\\displaystyle\\frac{g_{t}^{2}[k]}{b_{t}[k]}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Here the last inequality (45) follows from recursive use of (44). Then, taking squares on both sides and summing over $k\\in[d]$ we get ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\displaystyle\\sum_{k=1}^{d}{b}_{T}^{2}[k]}&{\\le}&{\\displaystyle\\sum_{k=1}^{d}\\left(b_{0}[k]+\\displaystyle\\sum_{t=0}^{T}\\frac{g_{t}^{2}[k]}{b_{t}[k]}\\right)^{2}}\\\\ &{\\le}&{\\displaystyle\\left(\\sum_{k=1}^{d}b_{0}[k]+\\sum_{t=0}^{T}\\sum_{k=1}^{d}\\frac{g_{t}^{2}[k]}{b_{t}[k]}\\right)^{2}}\\\\ &{\\le}&{\\displaystyle\\left(\\sum_{k=1}^{d}b_{0}[k]+\\frac{2\\left(f(w_{0})-f_{*}\\right)}{\\sqrt{\\eta_{0}}\\beta}\\right)^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "$\\begin{array}{r}{b_{0}[k]+\\sum_{t=0}^{T}\\frac{g_{t}^{2}[k]}{b_{t}[k]}\\geq0}\\end{array}$ $k\\in[d]$ Now note that $\\begin{array}{r}{\\sum_{t=0}^{T}\\|g_{t}\\|^{2}\\,=\\,\\sum_{t=0}^{T}\\sum_{k=1}^{d}g_{t}^{2}[k]\\,=\\,\\dot{\\sum}_{k=1}^{d}\\,b_{t}^{2}[k]}\\end{array}$ . Therefore dividing both sides of (46) by $T+1$ we get ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{t\\le T}\\|\\nabla f(w_{t})\\|^{2}\\le\\frac{1}{T+1}\\left(\\sum_{k=1}^{d}b_{0}[k]+\\frac{2(f(w_{0})-f_{*})}{\\sqrt{\\eta_{0}}\\beta}\\right)^{2}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "This completes the proof of the theorem. ", "page_idx": 20}, {"type": "text", "text": "B.4Proof of Theorem 3.4 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Theorem B.4. Suppose $f$ is a $L$ -smooth function and $g_{t}$ is an unbiased estimator of $\\nabla f(w_{t})$ such that BV holds. Moreover, we assume $\\|\\nabla f(w_{t})\\|^{2}\\leq\\gamma^{2}$ for all $t$ . Then KATE satisfies ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\underset{t\\le T}{\\operatorname*{min}}\\mathbb{E}\\|\\nabla f(w_{t})\\|}&{\\le}&{\\left(\\frac{\\|g_{0}\\|}{T}+\\frac{2(\\gamma+\\sigma)}{\\sqrt{T}}\\right)^{1/2}\\sqrt{\\frac{2\\mathcal{C}_{f}}{\\beta\\sqrt{\\eta_{0}}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathcal{C}_{f}=f(w_{0})-f_{*}+\\sum_{k=1}^{d}\\left(2\\beta\\sqrt{\\eta[k]}\\sigma+\\frac{\\beta^{2}\\eta[k]L}{2}+\\frac{\\beta^{2}L}{2g_{0}^{2}[k]}\\right)\\left(\\log\\left(\\frac{(\\sigma^{2}+\\gamma^{2})T}{g_{0}^{2}[k]}\\right)+1\\right).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof. Using smoothness, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{f(w_{t+1})}&{\\leq}&{f(w_{t})+\\langle\\nabla f(w_{t}),w_{t+1}-w_{t}\\rangle+\\displaystyle\\frac{L}{2}\\left\\|w_{t+1}-w_{t}\\right\\|^{2}}\\\\ &{=}&{f(w_{t})+\\displaystyle\\sum_{k=1}^{d}\\nabla_{k}f(w_{t})\\left(w_{t+1}[k]-w_{t}[k]\\right)+\\displaystyle\\frac{L}{2}\\sum_{k=1}^{d}\\left(w_{t+1}[k]-w_{t}[k]\\right)^{2}}\\\\ &{=}&{f(w_{t})-\\displaystyle\\sum_{k=1}^{d}\\nu_{t}[k]\\nabla_{k}f(w_{t})g_{t}[k]+\\displaystyle\\frac{L}{2}\\sum_{k=1}^{d}\\nu_{t}^{2}[k]g_{t}^{2}[k].}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Then, taking the expectation conditioned on $w_{t}$ ,we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\mathbb{E}_{t}\\left[f(w_{t+1})\\right]}&{\\leq}&{f(w_{t})-\\displaystyle\\sum_{k=1}^{d}\\mathbb{E}_{t}\\left[\\nu_{t}|k|\\nabla_{k}f(w_{t})g(k|)\\right]+\\displaystyle\\frac{L}{2}\\sum_{k=1}^{d}\\mathbb{E}_{t}\\left[\\nu_{t}^{2}|k|g_{t}^{2}|k\\right]}\\\\ &{=}&{f(w_{t})-\\displaystyle\\sum_{k=1}^{d}\\mathbb{E}_{t}\\left[\\nu_{t}|k|\\nabla_{k}f(w_{t})g(k|)\\right]+\\displaystyle\\frac{L}{2}\\sum_{k=1}^{d}\\mathbb{E}_{t}\\left[\\nu_{t}^{2}|k|g_{t}^{2}|k\\right]}\\\\ &&{-\\displaystyle\\sum_{k=1}^{d}\\frac{\\beta\\sqrt{\\eta}\\big[\\mathsf{K}\\big]}{\\sqrt{\\nu_{t}^{2}-[\\mathsf{K}]\\big[\\mathsf{K}]}+\\big(\\nabla_{k}f(w_{t})\\big)^{2}+\\sigma^{2}}\\mathbb{E}_{t}\\left[\\nabla_{k}f(w_{t})-g_{t}|k|\\right]}\\\\ &{=}&{f(w_{t})+\\displaystyle\\sum_{k=1}^{d}\\mathbb{E}_{t}\\left[\\left(\\frac{\\beta\\sqrt{\\eta}\\big[k]}{\\sqrt{\\eta_{t-1}^{2}-[\\mathsf{K}]+[\\nabla_{t}f(w_{t})]^{2}+\\sigma^{2}}}-\\nu_{t}|k\\right)\\right]\\nabla_{k}f(w_{t})g_{t}|k\\right]}\\\\ &&{+\\displaystyle\\frac{L}{2}\\sum_{k=1}^{d}\\mathbb{E}_{t}\\left[\\nu_{t}^{2}|k|g_{t}^{2}|k\\right]-\\displaystyle\\sum_{k=1}^{d}\\frac{\\beta\\sqrt{\\eta}\\big[k]\\big[\\nabla_{k}f(w_{t})\\big]^{2}}{\\sqrt{\\eta_{t-1}^{2}-[\\mathsf{K}]}+[\\nabla_{k}f(w_{t})]^{2}+\\sigma^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "The second last equality follows from Et [Vkf(wt)(Vkf(wt)-gt[k])] $\\nabla_{k}f(w_{t})\\left(\\nabla_{k}f(w_{t})-\\mathbb{E}_{t}\\left[g_{t}[{\\bar{k}}]\\right]\\right)=0$ Now we use (24) to get ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\mathbb{E}_{t}\\left[f(w_{t+1})\\right]}&{\\leq}&{f(w_{t})+\\displaystyle\\sum_{k=1}^{d}2\\beta\\sqrt{\\eta[k]}\\sigma\\mathbb{E}_{t}\\left[\\frac{g_{t}^{2}[k]}{b_{t}^{2}[k]}\\right]+\\frac{L}{2}\\displaystyle\\sum_{k=1}^{d}\\mathbb{E}_{t}\\left[\\nu_{t}^{2}[k]g_{t}^{2}[k]\\right]}\\\\ &&{-\\displaystyle\\sum_{k=1}^{d}\\frac{\\beta\\sqrt{\\eta[k]}\\left(\\nabla_{k}f(w_{t})\\right)^{2}}{2\\sqrt{b_{t-1}^{2}[k]+\\left(\\nabla_{k}f(w_{t})\\right)^{2}+\\sigma^{2}}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Then rearranging the terms we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\displaystyle\\sum_{k=1}^{d}\\frac{\\beta\\sqrt{\\eta[k]}\\left(\\nabla_{k}f(w_{t})\\right)^{2}}{2\\sqrt{b_{t-1}^{2}[k]+\\left(\\nabla_{k}f(w_{t})\\right)^{2}+\\sigma^{2}}}}&{\\le}&{f(w_{t})-\\mathbb{E}_{t}\\left[f(w_{t+1})\\right]+\\displaystyle\\sum_{k=1}^{d}2\\beta\\sqrt{\\eta[k]}\\sigma\\mathbb{E}_{t}\\left[\\frac{g_{t}^{2}[k]}{b_{t}^{2}[k]}\\right]}\\\\ &&{+\\displaystyle\\frac{L}{2}\\sum_{k=1}^{d}\\mathbb{E}_{t}\\left[\\nu_{t}^{2}[k]g_{t}^{2}[k]\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Now we take the total expectations to derive ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\displaystyle\\sum_{k=1}^{d}\\mathbb{E}\\left[\\frac{\\beta\\sqrt{\\eta[k]}\\left(\\nabla_{k}f(w_{t})\\right)^{2}}{2\\sqrt{b_{t-1}^{2}[k]+(\\nabla_{k}f(w_{t}))^{2}+\\sigma^{2}}}\\right]}&{\\le}&{\\displaystyle\\mathbb{E}\\left[f(w_{t})\\right]-\\mathbb{E}\\left[f(w_{t+1})\\right]+\\sum_{k=1}^{d}2\\beta\\sqrt{\\eta[k]}\\sigma\\mathbb{E}\\left[\\frac{g_{t}^{2}[k]}{b_{t}^{2}[k]}\\right]}\\\\ &&{\\displaystyle+\\frac{L}{2}\\sum_{k=1}^{d}\\mathbb{E}\\left[\\nu_{t}^{2}[k]g_{t}^{2}[k]\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "The above inequality holds for any $t$ . Therefore summing up from $t=0$ to $t=T$ and using $f(w_{T+1})\\geq f_{*}$ we get ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\displaystyle\\sum_{t=0}^{T}\\sum_{k=1}^{d}\\mathbb{E}\\left[\\frac{\\beta\\sqrt{\\eta[k]}\\left(\\nabla_{k}f(w_{t})\\right)^{2}}{2\\sqrt{b_{t-1}^{2}[k]+\\left(\\nabla_{k}f(w_{t})\\right)^{2}+\\sigma^{2}}}\\right]}&{\\le}&{f(w_{0})-f_{*}+\\displaystyle\\sum_{t=0}^{T}\\sum_{k=1}^{d}2\\beta\\sqrt{\\eta[k]}\\sigma\\mathbb{E}\\left[\\frac{g_{t}^{2}[k]}{b_{t}^{2}[k]}\\right]}\\\\ &&{\\displaystyle+\\frac{L}{2}\\sum_{t=0}^{T}\\sum_{k=1}^{d}\\mathbb{E}\\left[\\nu_{t}^{2}[k]g_{t}^{2}[k]\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Note that, using the expansion of $\\nu_{t}^{2}[k]$ we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\nu_{t}^{2}[k]}&{=}&{\\frac{\\beta^{2}\\eta[k]b_{t}^{2}[k]+\\beta^{2}\\sum_{j=0}^{t}\\frac{g_{j}^{2}[k]}{b_{j}^{2}[k]}}{b_{t}^{4}[k]}}\\\\ &{=}&{\\frac{\\beta^{2}\\eta[k]}{b_{t}^{2}[k]}+\\frac{\\beta^{2}}{b_{t}^{4}[k]}\\sum_{j=0}^{t}\\frac{g_{j}^{2}[k]}{b_{j}^{2}[k]}}\\\\ &{\\leq}&{\\frac{\\beta^{2}\\eta[k]}{b_{t}^{2}[k]}+\\frac{\\beta^{2}}{b_{t}^{4}[k]b_{0}^{2}[k]}\\sum_{j=0}^{t}g_{j}^{2}[k]}\\\\ &{=}&{\\frac{\\beta^{2}\\eta[k]}{b_{t}^{2}[k]}+\\frac{\\beta^{2}}{b_{t}^{2}[k]g_{0}^{2}[k]}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Here (48) follows from $b_{j}^{2}[k]\\geq b_{0}^{2}[k]$ and (49) from $\\begin{array}{r}{b_{t}^{2}[k]=\\sum_{j=0}^{t}g_{j}^{2}[k]}\\end{array}$ . Then using (49) in (47) we derive ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{=0}^{T}\\sum_{k=1}^{d}\\mathbb{E}\\left[\\frac{\\beta\\sqrt{\\eta[k]}\\,\\left(\\nabla_{k}f(w_{t})\\right)^{2}}{2\\sqrt{b_{t-1}^{2}[k]+(\\nabla_{k}f(w_{t}))^{2}+\\sigma^{2}}}\\right]\\leq f(w_{0})-f_{*}+\\sum_{t=0}^{T}\\sum_{k=1}^{d}\\left(2\\beta\\sqrt{\\eta[k]}\\sigma+\\frac{\\beta^{2}\\eta[k]\\,L}{2}+\\frac{\\beta^{2}L}{2g_{0}^{2}[k]}\\right)\\cdot}}\\\\ &{}&{\\leq f(w_{0})-f_{*}\\,\\,}\\\\ &{}&{\\quad+\\sum_{k=1}^{d}\\left(2\\beta\\sqrt{\\eta[k]}\\sigma+\\frac{\\beta^{2}\\eta[k]L}{2}+\\frac{\\beta^{2}L}{2g_{0}^{2}[k]}\\right)\\mathbb{E}\\left[\\log\\left(\\frac{b_{T}^{2}[k]}{b_{0}^{2}[k]}\\right)+\\frac{\\beta^{2}L}{2}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Here the last inequality follows from (29). Now using Jensen's Inequality (20) with $\\Psi(z)=\\log(z)$ wehave ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{=0}^{T}\\sum_{k=1}^{d}\\mathbb{E}\\left[\\frac{\\beta\\sqrt{\\eta[k]}\\,(\\nabla_{k}f(w_{t}))^{2}}{2\\sqrt{b_{t-1}^{2}[k]+(\\nabla_{k}f(w_{t}))^{2}+\\sigma^{2}}}\\right]\\leq f(w_{0})-f_{*}}}\\\\ &{}&{\\quad+\\sum_{k=1}^{d}\\left(2\\beta\\sqrt{\\eta[k]}\\sigma+\\frac{\\beta^{2}\\eta[k]L}{2}+\\frac{\\beta^{2}L}{2g_{0}^{2}[k]}\\right)\\left(\\log\\left(\\frac{\\mathbb{E}\\left[b_{T}^{2}[k]\\right]}{b_{0}^{2}[k]}\\right)\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Now note that $\\begin{array}{r}{\\mathbb{E}\\left[b_{T}^{2}[k]\\right]=\\sum_{t=0}^{T}\\mathbb{E}\\left[g_{t}^{2}[k]\\right]=\\sum_{t=0}^{T}\\mathbb{E}\\left[g_{t}[k]-\\nabla_{k}f(w_{t})\\right]^{2}+(\\nabla_{k}f(w_{t}))^{2}\\le(\\sigma^{2}+\\gamma^{2})T}\\end{array}$ Therefore, we have the bound ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{t=0}^{T}\\displaystyle\\sum_{k=1}^{d}\\mathbb{E}\\left[\\frac{\\beta\\sqrt{\\eta[k]}\\left(\\nabla_{k}f(w_{t})\\right)^{2}}{2\\sqrt{b_{t-1}^{2}[k]+\\left(\\nabla_{k}f(w_{t})\\right)^{2}+\\sigma^{2}}}\\right]\\leq}&{\\ f(w_{0})-f_{*}+2\\beta\\sigma\\displaystyle\\sum_{k=1}^{d}\\sqrt{\\eta[k]}\\log\\left(\\frac{e(\\sigma^{2}+\\gamma^{2})T}{b_{0}^{2}[k]}\\right)}\\\\ &{+\\displaystyle\\sum_{k=1}^{d}\\left(\\frac{\\beta^{2}\\eta[k]L}{2}+\\frac{\\beta^{2}L}{2g_{0}^{2}[k]}\\right)\\log\\left(\\frac{e(\\sigma^{2}+\\gamma^{2})T}{b_{0}^{2}[k]}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Here the RHS is exactly $\\mathcal{C}_{f}$ . Using (21) we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\displaystyle\\sum_{k=1}^{d}\\frac{\\left(\\nabla_{k}f(w_{t})\\right)^{2}}{\\sqrt{b_{t-1}^{2}[k]+\\left(\\nabla_{k}f(w_{t})\\right)^{2}+\\sigma^{2}}}}&{\\ge}&{\\displaystyle\\frac{\\sum_{k=1}^{d}\\left(\\nabla_{k}f(w_{t})\\right)^{2}}{\\sqrt{\\sum_{k=1}^{d}b_{t-1}^{2}[k]+\\left(\\nabla_{k}f(w_{t})\\right)^{2}+\\sigma^{2}}}}\\\\ &{=}&{\\displaystyle\\frac{\\left\\|\\nabla f(w_{t})\\right\\|^{2}}{\\sqrt{\\left\\|b_{t-1}\\right\\|^{2}+\\left\\|\\nabla f(w_{t})\\right\\|^{2}+d\\sigma^{2}}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Therefore using (51) in (50) we arrive at ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\sum_{t=0}^{T}\\mathbb{E}\\left[\\frac{\\left\\Vert\\nabla f(w_{t})\\right\\Vert^{2}}{\\sqrt{\\left\\Vert b_{t-1}\\right\\Vert^{2}+\\left\\Vert\\nabla f(w_{t})\\right\\Vert^{2}+d\\sigma^{2}}}\\right]}&{\\leq}&{\\frac{2\\mathcal{C}_{f}}{\\beta\\sqrt{\\eta_{0}}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Now we use Holder's Inequality (19) ${\\frac{\\mathbb{E}(X Y)}{\\left(\\mathbb{E}|Y|^{3}\\right)^{\\frac{1}{3}}}}\\leq\\left(\\mathbb{E}|X|^{\\frac{3}{2}}\\right)^{\\frac{2}{3}}$ with ", "page_idx": 23}, {"type": "equation", "text": "$$\nX=\\left({\\frac{\\|\\nabla f(w_{t})\\|^{2}}{\\sqrt{\\|b_{t-1}\\|^{2}+\\|\\nabla f(w_{t})\\|^{2}+d\\sigma^{2}}}}\\right)^{\\frac{2}{3}}\\quad{\\mathrm{and}}\\quad Y=\\left({\\sqrt{\\|b_{t-1}\\|^{2}+\\|\\nabla f(w_{t})\\|^{2}+d\\sigma^{2}}}\\right)^{\\frac{2}{3}}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "to get a lower bound on LHS of (52): ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}\\left[\\frac{\\lVert\\nabla f(w_{t})\\rVert^{2}}{\\sqrt{\\lVert b_{t-1}\\rVert^{2}+\\lVert\\nabla f(w_{t})\\rVert^{2}+d\\sigma^{2}}}\\right]}&{\\ge\\;\\;\\frac{\\mathbb{E}\\left[\\lVert\\nabla f(w_{t})\\rVert^{\\frac{4}{3}}\\right]^{\\frac{3}{2}}}{\\sqrt{\\mathbb{E}\\left(\\lVert b_{t-1}\\rVert^{2}+\\lVert\\nabla f(w_{t})\\rVert^{2}+d\\sigma^{2}\\right)}}}\\\\ &{\\ge\\;\\;\\frac{\\mathbb{E}\\left[\\lVert\\nabla f(w_{t})\\rVert^{\\frac{4}{3}}\\right]^{\\frac{3}{2}}}{\\sqrt{\\lVert b_{0}\\rVert^{2}+2t(\\gamma^{2}+d\\sigma^{2})}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Therefore from (52) and (53) we get ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\frac{T}{\\sqrt{\\|b_{0}\\|^{2}+2T(\\gamma^{2}+d\\sigma^{2})}}\\operatorname*{min}_{t\\leq T}\\mathbb{E}\\left[\\|\\nabla f(w_{t})\\|^{\\frac{4}{3}}\\right]^{\\frac{3}{2}}\\ \\ \\leq\\ \\ \\frac{2\\mathcal{C}_{f}}{\\beta\\sqrt{\\eta_{0}}}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Then multiplying both sides by Iloll+/2T(r+Vaa) wehave ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\underset{t\\leq T}{\\operatorname*{min}}\\mathbb{E}\\left[\\|\\nabla f(w_{t})\\|^{\\frac{4}{3}}\\right]^{\\frac{3}{2}}}&{\\leq}&{\\left(\\frac{\\|b_{0}\\|}{T}+\\frac{2(\\gamma+\\sigma)}{\\sqrt{T}}\\right)\\frac{2\\mathcal{C}_{f}}{\\beta\\sqrt{\\eta_{0}}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Here we use $\\mathbb{E}\\left[\\lVert\\nabla f(w_{t})\\rVert^{\\frac{4}{3}}\\leq\\mathbb{E}\\left[\\lVert\\nabla f(w_{t})\\rVert^{\\frac{4}{3}}\\right]\\right]$ (follows from Jensen's Inequality 20) with $\\Psi(z)=z^{4/3},$ in the above equation to get ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{t\\le T}\\mathbb{E}\\left[\\|\\nabla f(w_{t})\\|\\right]^{2}\\;\\;\\le\\;\\;\\left(\\frac{\\|b_{0}\\|}{T}+\\frac{2(\\gamma+\\sigma)}{\\sqrt{T}}\\right)\\frac{2\\mathcal{C}_{f}}{\\beta\\sqrt{\\eta_{0}}}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "This completes the proof of the Theorem. ", "page_idx": 23}, {"type": "text", "text": "C Additional Experiments: Scale-Invariance Verification ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "In this experiment, we implement KATE on problems (4) (for unscaled data) and (5) (for scaled data) with ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\varphi_{i}(z)=\\log\\left(1+\\exp\\left(-y_{i}z\\right)\\right).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "We generate the data similar to Section 4.1.1. We run KATE for $10,000$ iterations with mini-batch size 10, $\\eta=1\\big/(\\nabla f(w_{0}))^{2}$ and plot functional value $f(w_{t})$ and accuracy in Figures 1la and 11b. We use the proportion of correctly classified samples to compute accuracy, i.e. $\\begin{array}{r}{\\frac{1}{n}\\sum_{i=1}^{n}\\mathbf{1}_{\\left\\{y_{i}x_{i}^{\\top}w_{t}\\geq0\\right\\}}}\\end{array}$ ", "page_idx": 24}, {"type": "text", "text": "In plots 1la and 11b, the functional value and accuracy of KATE coincide, which aligns with our theoretical findings (Proposition 2.1). Figure 11c plots $\\|\\nabla f(w_{t})\\|^{2}$ and $\\|\\nabla f(w_{t})\\|_{V^{-2}}^{2}$ for unscaled and scaled data respectively. Here, (10) explains the identical values taken by the corresponding gradient norms of KATE iterates for the scaled and unscaled data. Similarly, in Figure 12, we compare the performance of AdaGrad on scaled and un-scaled data. This figure illustrates the lack of the scale-invariance for AdaGrad. ", "page_idx": 24}, {"type": "image", "img_path": "EdG59dnOzN/tmp/366f4afe2cb0b7953fcd9bbfee925d6691bf641110510cd3e6601e33af2c8667.jpg", "img_caption": [], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "Figure 11: Comparison of KATE on scaled and un-scaled data. Figures 1la, and 11b plot the functional value $f(w_{t})$ and accuracy on scaled and unscaled data, respectively. Figure 1lc plots $\\|\\nabla f(w_{t})\\|^{2}$ and $\\|\\nabla f(w_{t})\\|_{V-2}^{2}$ for unscaled and scaled data respectively. ", "page_idx": 24}, {"type": "image", "img_path": "EdG59dnOzN/tmp/d327da592c721d72ad0467192d6fe0beab66ed11883afb62b39816beb9568cce.jpg", "img_caption": [], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "Figure 12: Comparison of AdaGrad on scaled and un-scaled data. Figures 12a, and 12b plot the functionalvalue $f(w_{t})$ and accuracy on scaled and unscaled data, respectively. Figure 12c plots $\\|\\nabla f(w_{t})\\|^{2}$ and $\\|\\nabla f(w_{t})\\|_{V-2}^{2}$ for unscaled and scaled data respectively. ", "page_idx": 24}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "1. Claims ", "page_idx": 25}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately refect the paper's contributions and scope? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: the new method and its scale invariance property are introduced in Section 2, main convergence results are provided in Section 3, and the numerical results are provided in Section 4. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and refect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by thepaper. ", "page_idx": 25}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: see Section 3. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u00b7 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational efciency of the proposed algorithms and how theyscalewith dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 25}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: see Section 3 and the Appendix. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] Justification: see Section 4. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u00b7 Ine answer NA means that tne paper does not inciude experiments.   \n\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (t) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g.. with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g.. to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 26}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: see Section 4. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please see the NeurIPS code and data submission guidelines (https: //nips.cc/public/ guides/CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https : //nips . cc/public/ guides/CodeSubmissionPolicy) for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: see Section 4. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.   \n\u00b7 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 27}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: the results are consistent for different runs. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example. train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 27}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We have added all the details in Section 4. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. ", "page_idx": 27}, {"type": "text", "text": "\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 28}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPs Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: the paper follows NeurIPS Code of Ethics. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 28}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: the paper is mostly theoretical and does not have a direct societal impact. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "\u00b7 The answer NA means that there is no societal impact of the work performed.   \n\u00b7 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u00b7 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 28}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: we do not release data or models. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u00b7 The answer NA means that the paper poses no such risks.   \n\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 28}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes]   \nJustification: see Section 4.   \nGuidelines: \u00b7 The answer NA means that the paper does not use existing assets. \u00b7 The authors should cite the original paper that produced the code package or dataset. \u00b7 The authors should state which version of the asset is used and, if possible, include a URL. \u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset. \u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. \u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode . com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. \u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. \u00b7 If this information is not available online, the authors are encouraged to reach out to the asset's creators. ", "page_idx": 29}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 29}, {"type": "text", "text": "Justification: not applicable.   \nGuidelines: \u00b7 The answer NA means that the paper does not release new assets. \u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. \u00b7 The paper should discuss whether and how consent was obtained from people whose asset is used. \u00b7 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 29}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] Justification: not applicable. ", "page_idx": 29}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 29}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects Question: Does the paper describe potential risks incured by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA]   \nJustification: not applicable. Guidelines:   \n\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 30}]