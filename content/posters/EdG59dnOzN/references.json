{"references": [{"fullname_first_author": "John Duchi", "paper_title": "Adaptive subgradient methods for online learning and stochastic optimization", "publication_date": "2011-07-01", "reason": "This paper introduced the AdaGrad algorithm, a foundational adaptive method that KATE builds upon and improves."}, {"fullname_first_author": "Diederik P. Kingma", "paper_title": "Adam: A method for stochastic optimization", "publication_date": "2014-12-01", "reason": "Adam is a widely used adaptive optimization algorithm, providing a benchmark for comparison against KATE's performance."}, {"fullname_first_author": "Sashank J. Reddi", "paper_title": "On the Convergence of Adam and Beyond", "publication_date": "2019-04-01", "reason": "This paper analyzed the convergence properties of Adam and related algorithms, which is relevant to understanding KATE's convergence guarantees."}, {"fullname_first_author": "Richard Ward", "paper_title": "Adagrad stepsizes: Sharp convergence over nonconvex landscapes", "publication_date": "2020-01-01", "reason": "This paper provided sharp convergence analysis for AdaGrad, directly informing the theoretical analysis for KATE's convergence rate."}, {"fullname_first_author": "Xiaoyu Li", "paper_title": "On the Convergence of Stochastic Gradient Descent with Adaptive Stepsizes", "publication_date": "2019-01-01", "reason": "This work addressed AdaGrad's limitations in unbounded domains and proposed a variant, which KATE further improves with its scale-invariant approach."}]}