[{"figure_path": "EdG59dnOzN/figures/figures_7_1.jpg", "caption": "Figure 1: Comparison of KATE with AdaGrad, AdaGradNorm, SGD-decay and SGD-constant for different values of \u2206 (on x-axis) for logistic regression model. Figure 1a, 1b and 1c plots the functional value f (wt) (on y-axis) after 104, 5 \u00d7 104, and 105 iterations respectively.", "description": "This figure compares the performance of KATE against four other algorithms (AdaGrad, AdaGradNorm, SGD-decay, and SGD-constant) on a logistic regression task.  It demonstrates KATE's robustness across various initializations (represented by \u0394). The plots illustrate the functional value, accuracy, and gradient norm after different numbers of iterations, highlighting KATE's efficiency and stability compared to the other methods.", "section": "4.1 Logistic Regression"}, {"figure_path": "EdG59dnOzN/figures/figures_7_2.jpg", "caption": "Figure 2: Comparison of KATE with AdaGrad, AdaGradNorm, SGD-decay and SGD-constant on datasets heart, australian, and splice from LIBSVM. Figures 2a, 2b and 2c plot the functional value f(wt), while 2d, 2e and 2f plot the accuracy on y-axis for 5, 000 iterations.", "description": "The figure compares the performance of KATE against four other algorithms (AdaGrad, AdaGradNorm, SGD-decay, and SGD-constant) on three different datasets from LIBSVM (heart, australian, and splice).  The plots show both the functional value (loss) and the accuracy over 5000 iterations.  This illustrates KATE's performance in comparison to other methods on real-world datasets.", "section": "4 Numerical Experiments"}, {"figure_path": "EdG59dnOzN/figures/figures_8_1.jpg", "caption": "Figure 1: Comparison of KATE with AdaGrad, AdaGradNorm, SGD-decay and SGD-constant for different values of \u2206 (on x-axis) for logistic regression model. Figure 1a, 1b and 1c plots the functional value f (wt) (on y-axis) after 104, 5 \u00d7 104, and 105 iterations respectively.", "description": "This figure compares the performance of KATE against AdaGrad, AdaGradNorm, SGD with decay, and SGD with constant step size on a logistic regression task.  The x-axis represents different values of \u0394, a hyperparameter related to scaling.  The y-axis in subfigures (a), (b), and (c) shows the functional value f(w<sub>t</sub>) at various iteration counts (10<sup>4</sup>, 5 * 10<sup>4</sup>, and 10<sup>5</sup>, respectively). The experiment highlights KATE's robustness and scale invariance across different initializations.", "section": "4 Numerical Experiments"}, {"figure_path": "EdG59dnOzN/figures/figures_9_1.jpg", "caption": "Figure 3: CIFAR10: \u03b7 = 0", "description": "This figure compares the performance of KATE, AdaGrad and Adam on the task of training ResNet18 on the CIFAR10 dataset.  The x-axis represents the number of iterations, and the y-axis shows both the test accuracy and cross-entropy loss. Different learning rates (indicated by different colors and line styles) are used for each algorithm. The figure demonstrates that KATE achieves better performance than AdaGrad and Adam across a range of learning rates.", "section": "4.2 Training of Neural Networks"}, {"figure_path": "EdG59dnOzN/figures/figures_9_2.jpg", "caption": "Figure 10: Emotion: \u03b7 = 0.001", "description": "This figure compares the performance of KATE and Adam optimizers on the emotion classification task using the RoBERTa model.  The x-axis represents the number of epochs (training iterations), and the y-axis shows the test accuracy.  The plot demonstrates that KATE achieves comparable performance to Adam on this specific task.", "section": "4.2 Training of Neural Networks"}, {"figure_path": "EdG59dnOzN/figures/figures_14_1.jpg", "caption": "Figure 1: Comparison of KATE with AdaGrad, AdaGradNorm, SGD-decay and SGD-constant for different values of \u2206 (on x-axis) for logistic regression model. Figure 1a, 1b and 1c plots the functional value f (wt) (on y-axis) after 104, 5 \u00d7 104, and 105 iterations respectively.", "description": "This figure compares the performance of KATE against AdaGrad, AdaGradNorm, SGD with decay, and SGD with constant step size on a logistic regression task.  The x-axis represents different values of \u2206 (a hyperparameter), while the y-axis shows the functional value (loss) after 10<sup>4</sup>, 5 \u00d7 10<sup>4</sup>, and 10<sup>5</sup> iterations (across subfigures (a), (b), and (c), respectively).  The results illustrate KATE's robustness and superior performance across a range of \u2206 values, highlighting its adaptability and efficiency.", "section": "4.1 Logistic Regression"}, {"figure_path": "EdG59dnOzN/figures/figures_24_1.jpg", "caption": "Figure 11: Comparison of KATE on scaled and un-scaled data. Figures 11a, and 11b plot the functional value f(wt) and accuracy on scaled and unscaled data, respectively. Figure 11c plots ||\u2207f(wt)||\u00b2 and ||\u2207f(wt)||v-2 for unscaled and scaled data respectively.", "description": "This figure demonstrates the scale invariance of KATE.  Subfigures (a) and (b) show that the functional value and accuracy are virtually identical for both scaled and unscaled datasets, supporting the theoretical findings of scale invariance. Subfigure (c) further corroborates this by showing that the gradient norms are also the same for scaled and unscaled data. This indicates the algorithm's performance is unaffected by data scaling.", "section": "C Additional Experiments: Scale-Invariance Verification"}, {"figure_path": "EdG59dnOzN/figures/figures_24_2.jpg", "caption": "Figure 12: Comparison of AdaGrad on scaled and un-scaled data. Figures 12a, and 12b plot the functional value f(wt) and accuracy on scaled and unscaled data, respectively. Figure 12c plots ||\u2207f(wt)||\u00b2 and ||\u2207f(wt)||v-2 for unscaled and scaled data respectively.", "description": "This figure compares the performance of the AdaGrad algorithm on scaled and unscaled data.  Subfigure (a) shows the functional value f(wt) over iterations for both datasets. Subfigure (b) displays the accuracy achieved on both datasets, and subfigure (c) shows the gradient norms (||\u2207f(wt)||\u00b2 and ||\u2207f(wt)||v-2) for the unscaled and scaled datasets, respectively, to demonstrate the lack of scale invariance of AdaGrad.", "section": "4.1 Logistic Regression"}]