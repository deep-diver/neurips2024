[{"figure_path": "JPtobPtxKT/tables/tables_7_1.jpg", "caption": "Table 1: Comparisons on six MLLM benchmarks, including MMBench, MME, ScienceQA, HallusionBench, MMMU, and CCBench. vis. tok. denotes the number of visual tokens involved in the LLM. Bolded numbers indicate the best results, and underlined numbers are the second-best results.", "description": "This table compares the performance of VLORA against other state-of-the-art Multimodal Large Language Models (MLLMs) across six different benchmark datasets.  The benchmarks assess various aspects of vision-language capabilities.  The table shows each model's size (in billions of parameters), the number of visual tokens used as input (vis. tok.), and its performance score on each benchmark.  The best and second-best results are highlighted.  Noteworthy is VLORA's performance with zero visual tokens, highlighting its efficiency.", "section": "4.3 Comparison with State-of-the-arts"}, {"figure_path": "JPtobPtxKT/tables/tables_7_2.jpg", "caption": "Table 2: Comparison to LLaVA-v1.5 with various settings on six MLLM benchmarks, including MMBench, MME, ScienceQA, HallusionBench, MMMU, and CCBench. PT data represents the pre-training data. vis. tok. denotes the number of visual tokens involved in LLM.", "description": "This table compares the performance of VLORA against LLaVA-v1.5 across six multimodal large language model (MLLM) benchmarks under different pre-training data and visual token configurations.  It highlights VLORA's efficiency by showing comparable performance with zero visual tokens, contrasting with LLaVA's use of hundreds of visual tokens.", "section": "4.3 Comparison with State-of-the-arts"}, {"figure_path": "JPtobPtxKT/tables/tables_8_1.jpg", "caption": "Table 3: The impact of weights type that equipped perceptual weights. q, k, v, and o denote the query, key, value, and output weights in the self-attention module, respectively. m denotes the weights of the feed-forward network.", "description": "This table shows the impact of using perceptual weights on different types of weights within the LLM's decoder block.  It compares the performance on six benchmarks (MMBench, MME, ScienceQA, HallusionBench, MMMU, and CCBench) when perceptual weights are added to the query (q), key (k), value (v), output (o) weights of the self-attention module, and the weights (m) of the feed-forward network, in various combinations. The results indicate which weight types are most crucial for incorporating visual information into the LLM effectively.", "section": "5.2 Analysis of each component"}, {"figure_path": "JPtobPtxKT/tables/tables_8_2.jpg", "caption": "Table 4: The impact of perceptual weights' rank. The rank of the generated perceptual weights indicates the extent of visual information compression.", "description": "This table presents the results of an ablation study on the VLORA model, investigating the impact of different ranks of the perceptual weights generator on the model's performance across various benchmarks.  The rank parameter controls the level of compression applied to the visual features before they are integrated into the LLM's weights. The table shows that increasing the rank initially improves performance, suggesting that a higher-rank representation retains more visual information beneficial for the tasks. However, increasing the rank beyond a certain point (r = 64 in this case) leads to a decrease in performance, potentially due to overfitting or increased noise from the less compressed representation.", "section": "4.3 Comparison with State-of-the-arts"}, {"figure_path": "JPtobPtxKT/tables/tables_9_1.jpg", "caption": "Table 5: The impact of different numbers of blocks of perceptual weights generator.", "description": "This table presents the results of an ablation study on the VLORA model, investigating the impact of varying the number of blocks in the perceptual weights generator.  The table shows the performance of the model on six different benchmarks (MMBench, MME, ScienceQA, HallusionBench, MMMU, and CCBench) when the perceptual weight generator has 4, 8, or 12 blocks. The results indicate the optimal number of blocks for achieving the best overall performance across the benchmarks.", "section": "5.2 Analysis of each component"}, {"figure_path": "JPtobPtxKT/tables/tables_14_1.jpg", "caption": "Table 6: Comparisons between VLORA and LLaVA-v1.5 on fine-grained benchmarks, including TextVQA, DocVQA, InfoVQA, and OCRBench.", "description": "This table compares the performance of VLORA and LLaVA-v1.5 on four fine-grained vision-language benchmarks: TextVQA, DocVQA, InfoVQA, and OCRBench.  It shows the average scores for each model on these benchmarks.  The number of visual tokens used by each model is also listed. Notably, VLORA uses 0 visual tokens, highlighting its efficiency advantage.", "section": "4.3 Comparison with State-of-the-arts"}, {"figure_path": "JPtobPtxKT/tables/tables_14_2.jpg", "caption": "Table 7: Comparisons of Training Speed and GPU Memory Requirements between VLORA and LLaVA-v1.5", "description": "This table compares the training speed and GPU memory usage of VLORA and LLaVA-v1.5 during both pre-training and fine-tuning phases.  It shows that VLORA achieves significantly faster training speeds and lower GPU memory requirements during pre-training, while maintaining comparable memory usage during fine-tuning.", "section": "4.3 Comparison with State-of-the-arts"}]