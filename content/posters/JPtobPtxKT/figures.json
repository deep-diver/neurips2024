[{"figure_path": "JPtobPtxKT/figures/figures_2_1.jpg", "caption": "Figure 1: Overview of the input space alignment and the parameter space alignment paradigms. The input space alignment paradigm is aligning visual features with the input space of LLM and concatenating visual tokens with text tokens as input for LLM. Our proposed VLORA follows the parameter space alignment paradigm that aligns visual features with the parameters of LLM and merges perceptual weights generated by the perceptual weights generator with LLM's weights.", "description": "This figure compares the traditional input space alignment method with the proposed VLORA method. In the input space alignment method, visual features are aligned with the input space of the LLM and concatenated with text tokens to form a unified input sequence. In the VLORA method, visual features are converted into perceptual weights, which are merged with the LLM's weights. This reduces the input sequence length and improves computational efficiency.", "section": "1 Introduction"}, {"figure_path": "JPtobPtxKT/figures/figures_2_2.jpg", "caption": "Figure 1: Overview of the input space alignment and the parameter space alignment paradigms. The input space alignment paradigm is aligning visual features with the input space of LLM and concatenating visual tokens with text tokens as input for LLM. Our proposed VLORA follows the parameter space alignment paradigm that aligns visual features with the parameters of LLM and merges perceptual weights generated by the perceptual weights generator with LLM's weights.", "description": "This figure compares two different approaches for incorporating visual information into large language models (LLMs): input space alignment and parameter space alignment.  Input space alignment involves aligning visual features with the LLM's input space, creating visual tokens that are concatenated with text tokens before being fed to the LLM. This method, while effective, can be computationally expensive due to the increased input sequence length.  In contrast, the parameter space alignment method, used by the proposed VLORA model, merges visual information, represented as perceptual weights generated from visual features, directly into the LLM's weights. This improves efficiency by avoiding the addition of visual tokens to the input sequence. The figure visually represents these two paradigms, highlighting the difference in how visual information is processed and integrated with the LLM.", "section": "1 Introduction"}, {"figure_path": "JPtobPtxKT/figures/figures_3_1.jpg", "caption": "Figure 2: Details of the LLM Decoder Block. (a) illustrates the details of the LLM decoder block, including the multi-head self-attention module and the feed-forward network. (b) provides a detailed view of the multi-head self-attention module, which incorporates four types of weights: WQ, WK, Wv, and Wo. (c) depicts the feed-forward network, which consists of the weights W\u2081 and W\u2082.", "description": "This figure shows a detailed breakdown of the Large Language Model (LLM) decoder block architecture. It's comprised of three sub-figures:\n(a) A high-level overview of the decoder block, illustrating its modular components such as the self-attention and feed-forward networks.\n(b) A zoomed-in view of the multi-head self-attention module within the decoder block, highlighting its four weight matrices: WQ, WK, WV, and Wo. These matrices are crucial for the attention mechanism to perform calculations.\n(c) A detailed illustration of the feed-forward network, indicating its two weight matrices: W\u2081 and W\u2082. These are crucial for sequential processing in the network.\nIn essence, this figure details the internal structure and weight parameters of the LLM's decoder block.", "section": "3 Method"}, {"figure_path": "JPtobPtxKT/figures/figures_4_1.jpg", "caption": "Figure 3: Perceptual Weights Generator. Figure (a) illustrates the pipeline of our perceptual weights generator. We set k learnable perceptual queries, which interact with image features in N decoder blocks, and obtain k visual parameters. Then, a shared linear layer and k independent linear layers are used to convert these visual parameters to perceptual weights AW. Figure (b) demonstrates that our approach is formally consistent with LoRA.", "description": "This figure illustrates the architecture of the perceptual weights generator, a key component of the proposed VLORA model.  The generator takes visual features as input and produces perceptual weights (\u0394W) that are added to the LLM's weights. The figure shows a decoder-only architecture with cross-attention layers processing perceptual queries and visual features to generate these low-rank perceptual weights, exhibiting a structure similar to LoRA. The right part shows that the generated perceptual weights are equivalent to the LoRA weight.", "section": "3.3 Perceptual Weights Generator"}, {"figure_path": "JPtobPtxKT/figures/figures_5_1.jpg", "caption": "Figure 4: Comparison of FLOPs. This figure shows the FLOPs of LLaVA and VLORA with different numbers of input visual tokens. The left subplot illustrates the change in GFLOPs, the right subplot plots the ratio of GFLOPs for VLORA to LLaVA, and C denotes the number of text tokens.", "description": "This figure compares the computational cost (GFLOPs) of LLaVA and VLORA for different numbers of input visual tokens and various text token counts (C). The left plot shows the absolute GFLOPs, while the right plot displays the ratio of VLORA's GFLOPs to LLaVA's GFLOPs, illustrating VLORA's significant computational efficiency gains, particularly noticeable when the number of visual tokens increases.", "section": "3.4 Analysis of the Computational Cost"}, {"figure_path": "JPtobPtxKT/figures/figures_13_1.jpg", "caption": "Figure 1: Overview of the input space alignment and the parameter space alignment paradigms. The input space alignment paradigm is aligning visual features with the input space of LLM and concatenating visual tokens with text tokens as input for LLM. Our proposed VLORA follows the parameter space alignment paradigm that aligns visual features with the parameters of LLM and merges perceptual weights generated by the perceptual weights generator with LLM's weights.", "description": "The figure compares three approaches for multimodal large language models (MLLMs): (a) visual feature extraction, (b) input space alignment, and (c) the proposed VLORA method using parameter space alignment.  Input space alignment concatenates visual tokens with text tokens, increasing computational cost.  VLORA merges perceptual weights (derived from visual features) directly with LLM weights, improving efficiency.", "section": "1 Introduction"}]