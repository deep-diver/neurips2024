[{"type": "text", "text": "Can Language Models Perform Robust Reasoning in Chain-of-thought Prompting with Noisy Rationales? ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Zhanke Zhou1 Rong Tao1 Jianing Zhu1 Yiwen Luo2 Zengmao Wang2 Bo Han1\u2020 ", "page_idx": 0}, {"type": "text", "text": "1TMLR Group, Hong Kong Baptist University 2Wuhan University {cszkzhou, csrongtao, csjnzhu, bhanml}@comp.hkbu.edu.hk {luoyiwen, wangzengmao}@whu.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "This paper investigates an under-explored challenge in large language models (LLMs): chain-of-thought prompting with noisy rationales, which include irrelevant or inaccurate reasoning thoughts within examples used for in-context learning. We construct NoRa dataset that is tailored to evaluate the robustness of reasoning in the presence of noisy rationales. Our findings on NoRa dataset reveal a prevalent vulnerability to such noise among current LLMs, with existing robust methods like self-correction and self-consistency showing limited efficacy. Notably, compared to prompting with clean rationales, GPT-3.5 drops by $1.4\\%$ -1 $9.8\\%$ in accuracy with irrelevant thoughts and more drastically by $2.2\\%{-}40.4\\%$ with inaccurate thoughts. ", "page_idx": 0}, {"type": "text", "text": "Addressing this challenge necessitates external supervision that should be accessible in practice. Here, we propose the method of contrastive denoising with noisy chain-of-thought (CD-CoT). It enhances LLMs\u2019 denoising-reasoning capabilities by contrasting noisy rationales with only one clean rationale, which can be the minimal requirement for denoising-purpose prompting. This method follows a principle of exploration and exploitation: (1) rephrasing and selecting rationales in the input space to achieve explicit denoising and (2) exploring diverse reasoning paths and voting on answers in the output space. Empirically, CD-CoT demonstrates an average improvement of $17.8\\%$ in accuracy over the base model and shows significantly stronger denoising capabilities than baseline methods. The source code is publicly available at: https://github.com/tmlr-group/NoisyRationales. ", "page_idx": 0}, {"type": "image", "img_path": "FbuODM02ra/tmp/f2619d1c5f9e54a89aa043cf4f6f3b863d4fba8eacfe52869f3835d9e20abf75.jpg", "img_caption": [], "img_footnote": [], "page_idx": 0}, {"type": "text", "text": "Figure 1: Exemplars of noisy questions [68] and noisy rationales (our new research problem). Each input includes three prompting examples and one test question. Notably, the test question asks about base-9 calculation, while the misguiding base-10 information is given in noisy questions or rationales. ", "page_idx": 0}, {"type": "image", "img_path": "FbuODM02ra/tmp/1b479cae00a80d442e594523d0c21f08dbe8e1b19e8edef2d26f22972abc96bb.jpg", "img_caption": ["Figure 2: Results of GPT-3.5 with 0-shot, 3-shot clean rationales, and 3-shot noisy rationales: Both inaccurate and irrelevant rationales degenerate performance significantly, while the proposed CDCoT improves robustness against noisy rationales. ", "Figure 3: Chain modeling of the noisy rationale problem: Recovering chain (3) from chain (1) with the guidance of chain (2). From question $x_{i}$ to answer $y_{i}$ , the rationale of chain (3) includes clean thoughts T i(j) and noisy thoughts T\u02c6 i(j). "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In-context learning (ICL) is a common approach in large language models (LLMs), enabling models to extrapolate from a few examples and adapt without fine-tuning [4, 84, 16]. However, ICL\u2019s efficacy is closely tied to the quality and clarity of the prompting examples, particularly in the prevailing chain-of-thought (CoT) strategy that provides rationales, i.e., intermediate reasoning steps to solve a question [85]. Recent research has shown that LLMs struggle with noisy questions: they are easily distracted by irrelevant context and exhibit instability with slight input modifications [68, 78, 107]. ", "page_idx": 1}, {"type": "text", "text": "Notably, this work shifts focus from the well-studied noisy questions (Noisy-Q) problem to the underexplored noisy rationales (Noisy-R) problem, wherein factually inaccurate or irrelevant reasoning steps are paired with valid question-answer examples, as illustrated in Fig. 1. Here, the emphasis on Noisy-R is due to its practical challenges, with examples drawn from diverse sources such as crowdsourced platforms, dialogue systems, and machine-generated data 2 [25, 45, 73, 2, 77, 48]. However, the robustness of LLMs against Noisy-R remains unknown. A new benchmarking dataset is needed to conduct a systematic evaluation of current LLMs and verify the corresponding countermeasures. ", "page_idx": 1}, {"type": "text", "text": "In this work, we first construct the NoRa (Noisy Rationales) dataset, a comprehensive testbed to evaluate the robustness of LLM reasoning against noisy rationales across various reasoning domains (in Sec. 3). The NoRa contains a total of 26391 questions, covering three types of reasoning tasks: mathematical, symbolic, and commonsense. We uniformly formalize the generation of noisy rationales by inserting irrelevant or inaccurate thoughts, controlling reasoning difficulty through noise ratios, and guaranteeing the overall prompting correctness without modifying the question or answer. ", "page_idx": 1}, {"type": "text", "text": "With the NoRa dataset, we evaluate several LLMs and reveal that all of them are intrinsically vulnerable to noisy rationales (in Sec. 4). For example, compared to prompting with clean rationales, GPT-3.5 exhibits an average $3.0\\%\\ \u2013\\ 33.3\\%$ decrease in accuracy with noisy rationales, as in Fig. 2. Besides, only limited improvements are achieved with existing robust methods based on the model\u2019s intrinsic denoising ability, e.g., self-consistency [83] and self-denoise [102]. We show that Noisy-R is much more challenging than Noisy-Q, requiring context-specific knowledge to guide the denoising. ", "page_idx": 1}, {"type": "text", "text": "To solve this, we propose to rectify the rationales with only one clean CoT demonstration that can be the most attainable supervision in practice (in Sec. 5). We assume that LLMs can rectify rationales by contrasting a noisy rationale with a clean one, as in Fig. 3. Guided by this principle, we design the framework of Contrastive Denoising with noisy CoT (CD-CoT) with four steps: rationale rephrasing, rationale selecting, rationale exploring, and answer voting. Technically, the first two steps aim to achieve explicit denoising, while the last two steps are for diverse reasoning paths. Empirically, CD-CoT achieves an average improvement of $1\\bar{7}.8\\%$ in accuracy w.r.t. the base model (refer to Tab. 8). Notably, it presents much stronger denoising power than baselines in rectifying the rationales. ", "page_idx": 1}, {"type": "text", "text": "Contributions. To our best knowledge, we are the first to investigate the problem of noisy rationales. ", "page_idx": 1}, {"type": "text", "text": "\u2022 We formalize the under-explored noisy rationale problem in the prevailing chain-of-thought prompting and construct the NoRa dataset to benchmark the robustness of LLMs against noisy rationales (Sec. 3). \u2022 We systematically evaluate LLMs with NoRa dataset and extract several insightful observations, e.g., the unsatisfactory robustness and limited denoising power of LLMs under noisy rationales (Sec. 4). \u2022 We propose to rectify the noisy rationales with only one clean CoT demonstration, design a simple yet effective method, CD-CoT, and verify its effectiveness through comprehensive experiments (Sec. 5). ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Limitations of in-context learning (ICL). Though effective, ICL suffers from the susceptibility to manual generation and selection of demonstrations (examples), where the ultimate performance is closely tied to the demonstrations\u2019 quality and clarity. Recent investigations on noisy questions have shown that (i) LLMs can be distracted by irrelevant or adversarial context, as they are designed to pay close attention to the context provided in the prompt [32, 58, 68, 78] and (ii) LLM reasoning is unstable, namely, small modifications to the prompt could potentially cause large variations in the model\u2019s output [102, 107]. Besides, another line of research regarding noisy answers [42, 18] justifies the feasibility of misleading an LLM to agree factual errors such as $\"1{+}1{=}3\"$ in base-10 calculation. ", "page_idx": 2}, {"type": "text", "text": "Countermeasures. Two intrinsic traits of LLMs are desirable for addressing the above limitations: ", "page_idx": 2}, {"type": "text", "text": "\u2022 Self-correction, wherein LLMs attempt to correct their initial responses based solely on their inherent capabilities without external feedback, e.g., by refining prompts through iterative corrections of responses or question trajectories [91, 89]. Although LLMs can learn to ignore irrelevant information by examples or instructions [68], they are proved to be still struggling to correct their responses without external feedback, and at times, their performance might even degrade after self-correction [29, 81]. ", "page_idx": 2}, {"type": "text", "text": "\u2022 Self-consistency aims to obtain a consistent answer against input perturbations. This is achieved by generating multiple responses via randomized smoothing on input questions [98] or diverse paths for answering one question [83] followed by the answer aggregation. This strategy brings improvements with extra costs for repeated reasoning. Moreover, it cannot explicitly rectify questions or rationales. ", "page_idx": 2}, {"type": "text", "text": "Noisy rationales, as the research focus of this work, mainly originates from (1) the inherent imperfections, inconsistencies, and inaccuracies of humans\u2019 cognitive processes [53, 10] and (2) the diversity, unpredictability, and hallucination of the LLMs\u2019 generative mechanisms [103, 30, 101]. A detailed literature review and discussion of noise rationales are in Appendix B and C, respectively. ", "page_idx": 2}, {"type": "text", "text": "3 The NoRa Dataset ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we introduce the NoRa (Noisy Rationales) dataset for benchmarking the robustness against noisy rationales. NoRa consists of 26391 questions and 5 subsets, covering mathematical, symbolic, and commonsense reasoning tasks, where ICL and CoT demonstrations play a crucial role. ", "page_idx": 2}, {"type": "text", "text": "3.1 Definition of Noisy Rationales ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We start by formalizing the ICL and CoT demonstrations. Given a test question $x_{\\mathrm{{test}}}$ and an LLM $f_{\\theta}$ , one expects to get the correct answer $y_{\\mathrm{{test}}}$ as $f_{\\theta}(\\boldsymbol{x}_{\\mathrm{test}})\\mapsto y_{\\mathrm{test}}$ . This zero-shot manner cannot guarantee effectiveness, especially when encountering unfamiliar contexts or scenarios. To boost effectiveness, the ICL techniques prompt the LLM with a few examples ${\\cal{S}}_{n}\\!=\\!\\{(x_{i},y_{i})\\}_{i=1}^{n}$ collected in the current context, each composed of a question $x_{i}$ and answer $y_{i}$ , and then construct the new input $x_{\\mathrm{ICL}}$ as ", "page_idx": 2}, {"type": "equation", "text": "$$\nx_{\\mathrm{ICL}}=\\left[S_{n},x_{\\mathrm{test}}\\right]=\\left[x_{1},y_{1},\\ldots,x_{n},y_{n},\\ x_{\\mathrm{test}}\\right].\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The guidance by $S_{n}$ makes $f_{\\theta}(x_{\\mathrm{ICL}})\\!\\mapsto\\!y_{\\mathrm{test}}$ much easier than $f_{\\theta}(\\boldsymbol{x}_{\\mathrm{test}})\\mapsto y_{\\mathrm{test}}$ . Then, the CoT further refines $x_{\\mathrm{ICL}}$ by constructing the step-by-step rationale $\\mathcal{T}_{i}$ , consisting of several thoughts $T_{i}^{(j)}$ , namely, ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{x_{\\mathrm{CoT}}=\\left[x_{1},\\mathcal{T}_{1},y_{1},\\ldots,x_{n},\\mathcal{T}_{n},y_{n},\\ x_{\\mathrm{tesl}}\\right],\\ \\ \\mathrm{where}\\ \\ \\mathcal{T}_{i}=\\left[T_{i}^{(1)},T_{i}^{(2)},T_{i}^{(3)},\\ldots,T_{i}^{(k)}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "However, as aforementioned, the thoughts in CoT (Eqn. 2) can be noisy in practice. This noise can be attributed to (1) irrelevant thoughts, which are irrelevant but correct, or (2) inaccurate thoughts, which are relevant but factually wrong. Here, we uniformly formalize these two kinds of noise as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\hat{\\mathcal{T}}_{i}=\\left[T_{i}^{(1)},\\hat{T}_{i}^{(1)},T_{i}^{(2)},\\hat{T}_{i}^{(2)},\\dots,T_{i}^{(k)},\\hat{T}_{i}^{(k)}\\right],\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where ${\\hat{T}}_{i}^{\\left(j\\right)}$ represents a noisy thought (irrelevant or inaccurate) that is coherent with the previous clean thought $T_{i}^{(j)}$ (relevant and correct). The following introduces the definition of noisy thoughts. ", "page_idx": 2}, {"type": "text", "text": "Irrelevant thoughts refer to incorporating irrelevant information unhelpful for solving the question, e.g., discussing the genetic overlap of siblings when the task is to deduce family roles in relationship reasoning. Redundant information may be introduced by the LLM\u2019s diverse response generation or by humans when clarifying concepts in problem-solving examples [5, 104]. Alternatively, we explore various semantic levels of \"irrelevance\" for constructing diverse irrelevant thoughts in Appendix F.4. ", "page_idx": 2}, {"type": "table", "img_path": "FbuODM02ra/tmp/f70896d8b071c562fe40925452379b6d071cd3e3e788fbb944c02aaaca9bed16.jpg", "table_caption": [], "table_footnote": ["Table 1: Noisy rationales (consisting noisy thoughts) sampled from the NoRa dataset. Full examples of NoRa are in Appendix C.6, and real-world examples of noisy rationales are in Appendix C.3. "], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Inaccurate thoughts refer to factual errors in rationales that are common in mathematical calculation or transcription, e.g., \" $^{\\prime}5{+}5{=}10\"$ is wrong in base-9 calculation. This noise comes from algorithmic limitations, errors in training data, misinterpretations of instructions, and logical fallacies [40, 64]. Remark 3.1. Both types of noise only impact the finer details of the reasoning chain without affecting the correctness of question $x_{i}$ and answer $y_{i}$ . This distinction ensures that the reasoning based on the noisy demonstration is not fundamentally flawed, namely, only the reasoning rationale $\\hat{\\mathcal{T}}_{i}$ is noisy. ", "page_idx": 3}, {"type": "text", "text": "3.2 Tasks and Statistics ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The NoRa dataset covers the three types of reasoning tasks listed below. In noise generation, irrelevant thoughts, sourced from extraneous scientific or social facts, and inaccurate thoughts, arising from misguided reasoning, are both based on contextual thoughts of Eqn. 2 (see examples in Tab. 1). ", "page_idx": 3}, {"type": "text", "text": "\u2022 NoRa-Math. This task is derived from the Base Calculation dataset [88] for evaluating non-standard base arithmetic skills and features two sub-tasks of base-9 and base-11 addition. Here, the mastery of mathematical concepts and the calculation rules of specific bases are the keys to solving these tasks. ", "page_idx": 3}, {"type": "text", "text": "\u2022 NoRa-Symbolic. We utilize the SCAN dataset [41] here, which aims to transform natural language into symbolic, machine-understandable instructions. This transformation is learned from the prompting examples, comprising two sub-tasks: (1) equal-length subtask, where the transformed instructions in both $S_{n}$ and $x_{\\mathrm{{test}}}$ have the equal length; and (2) longer-length subtask, where the transformed instructions in $x_{\\mathrm{{test}}}$ is longer than those in $S_{n}$ , presenting an easy-to-hard generalization challenge. ", "page_idx": 3}, {"type": "text", "text": "\u2022 NoRa-Commonsense. This task is constructed based on the CLUTRR dataset [71], which is geared towards family relation path reasoning, e.g., \"who is aunt\u2019s sister\u2019s mother?\" It requires the mastery and application of commonsense knowledge of relationships and cognitive skills for reasoning. ", "page_idx": 3}, {"type": "text", "text": "Noise Ratio. Given the noise ratio $\\epsilon\\in(0,1)$ , the expected number of added noisy thoughts for a $k$ -length CoT demonstration is $\\left\\lfloor\\epsilon\\cdot k+1/2\\right\\rfloor$ . Specifically, for an irrelevant thought $\\hat{T}_{i}^{\\left(j\\right)}$ in $j$ -th position of $i$ -th example, a Bernoulli distribution $\\mathtt{B e r n}(\\epsilon)\\in\\{0,1\\}$ is adopted to indicate its binary existence. We also further investigate the impact of the number of noisy thoughts, provided in Appendix F.5. ", "page_idx": 3}, {"type": "text", "text": "Statistics. Tab. 2 provides a categorization of task difficulties as Easy, Medium, and Hard, with corresponding noise ratios of 0.3, 0.5, and 0.8. Regarding the number of thoughts in a rationale, Math entails 8 thoughts, Symbolic varies from 2 to 12, and Commonsense requires 5 thoughts. Additionally, a detailed introduction to NoRa is in Appendix C.4. ", "page_idx": 3}, {"type": "table", "img_path": "FbuODM02ra/tmp/e56bb10e7b19581051ee27b5883e57a340a7ae47b83d936b1cfaa4c5df97426f.jpg", "table_caption": ["Table 2: Statistics of NoRa dataset. "], "table_footnote": [], "page_idx": 3}, {"type": "table", "img_path": "FbuODM02ra/tmp/05c3b0d5f9e443799d70dd1fd489f8bc2a055660f5df17be6a2b741a944478db.jpg", "table_caption": [], "table_footnote": ["Table 3: Reasoning accuracy on NoRa dataset with 3-shot prompting examples with clean, irrelevant, or inaccurate rationales. The boldface numbers mean the best results, while the underlines numbers indicate the second-best results. Note the referenced results of Base model are highlighted in gray. "], "page_idx": 4}, {"type": "text", "text": "4 Evaluating Language Models on NoRa dataset ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we comprehensively evaluate representative LLMs and robust methods on the newly constructed NoRa dataset. We first introduce the basic evaluation setups and then present several observations on the unsatisfactory robustness of current LLMs and methods under noisy rationales. ", "page_idx": 4}, {"type": "text", "text": "Baseline methods. We select five representative methods as baselines to ensure a comprehensive assessment that encompasses the two traits of self-correction and self-consistency. ISC [29] and SP [89] exemplify self-correction, focusing on response rectification and prompt rephrasing, respectively. SM [62], SD [102], and SC [83] fall under self-consistency: SM [62] injects perturbations into prompts for robustness, SD [102] masks prompts and asks LLMs to reconstruct them, while SC directly samples outputs without denoising. These methods are further introduced in Appendix E.1. ", "page_idx": 4}, {"type": "text", "text": "LLM basis. We employ GPT-3.5-turbo-0613 [17] as our base LLM (denoted as Base ) for the analyses presented in this study. In addition, we conduct evaluations on three supplementary models, including Gemini-Pro (Jan. 2024) [76], Llama2-70B [79], and Mixtral- $\\mathrm{8x7B}$ [33]. For all baselines, we consistently set the temperature parameter $\\tau$ to the value of 1. In order to obtain consistent results, we evaluate 300 questions for each task and repeat the model reasoning five times for each question. ", "page_idx": 4}, {"type": "text", "text": "Evaluation metric. Given a set of test question $\\mathcal{Q}\\,=\\,\\{(x_{\\mathrm{{test}}},y_{\\mathrm{{test}}})\\}$ and a set of CoT-prompting examples $\\mathcal{P}=[x_{1},\\mathcal{T}_{1},y_{1},\\ldots,x_{n},T_{n},y_{n}]$ , we define the accuracy of the denoising method $\\mathcal{M}$ with a specific LLM $f_{\\theta}$ , namely, $\\begin{array}{r}{\\operatorname{Acc}(\\mathcal{M},\\mathcal{Q},\\mathcal{P})=\\sum_{(x_{\\mathrm{test}},y_{\\mathrm{test}})\\in\\mathcal{Q}}\\mathbf{1}[\\mathcal{M}(\\mathcal{P},x_{\\mathrm{test}})\\!=\\!y_{\\mathrm{test}}]\\big/|\\mathcal{Q}|}\\end{array}$ |. We report the results in percentage $(\\%)$ with one decimal point. Therein, $\\operatorname{Acc}(\\mathcal{M},\\mathcal{Q},\\mathcal{P}_{\\mathrm{clean}})$ , $\\operatorname{Acc}(\\mathcal{M},\\mathcal{Q},\\mathcal{P}_{\\mathrm{irrelevant}})$ , and $\\operatorname{Acc}(\\mathcal{M},\\mathcal{Q},\\mathcal{P}_{\\mathrm{inaccurate}})$ indicate accuracy with clean, irrelevant, and inaccurate rationales, respectively. When there is no prompting example, i.e., $\\mathcal{P}=\\emptyset$ , then $\\operatorname{Acc}(\\mathcal{M},\\mathcal{Q},\\emptyset)$ represents the zero-shot result. ", "page_idx": 4}, {"type": "text", "text": "Unreliability revealing with noisy rationales. We conduct the reasoning tasks on LLM with Noisy$\\mathbf{R}$ and summarize the results in Tab. 3. Overall, the base LLM with all the existing reasoning methods is severely affected by irrelevant or inaccurate noise, with overall showing a $0.2\\%{-}25.3\\%$ decrease with irrelevant noise and a more drastic $0.1\\%{-}54.0\\%$ decrease with inaccurate noise compared with clean rationales. While robust methods like SP and SD exhibit resilience to noise on partial tasks, their performance remains inconsistent and often declines. To further reveal the unreliability, we start by analyzing the two categories of robust methods mentioned above in the following observations. ", "page_idx": 4}, {"type": "table", "img_path": "FbuODM02ra/tmp/4028d89fa52a92a70e9c0c745929afe61f1b4885797012d9d8787da2f14b1009.jpg", "table_caption": [], "table_footnote": [], "page_idx": 5}, {"type": "table", "img_path": "FbuODM02ra/tmp/229d4598b6f02cc85bda0e9bc3bd934c1f9ad72e00961f208e5d98bc43bb0a3a.jpg", "table_caption": ["Table 4: Comparing perfor- Table 5: Comparing perfor- Table 6: Comparing LLMs with mances of the base model mances of the base model with 0-shot, 3-shot clean, and 3-shot with different temperatures. a varying number of examples medium irrelevant (irr.) / inaccuSym.(E)/(L) are symbolic tasks. (\"\u2014\" denotes over token limit). rate (ina.) rationales. ", "Table 7: Performance (in accuracy $\\%$ ) on NoRa dataset under different few-shot shuffle configurations. "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "Observation 4.1. Self-correction methods perform poorly on most tasks with noisy rationales. Therein, ISC [29] and SP [89] rely on the inherent capabilities of LLMs to enhance the quality of generated responses. However, in the absence of external feedback, the model\u2019s self-correction ability in reasoning tasks is limited, often resulting in the miscorrection of the given content (see Tab. 12). SP can only slightly improve the accuracy of commonsense tasks, while ISC performs unsatisfactorily across all tasks. As can be seen from Tab. 3, these methods perform even worse than the base model. ", "page_idx": 5}, {"type": "text", "text": "Observation 4.2. Self-consistency methods can improve robustness without true denoising. Two self-consistency approaches, SM [62] and SD [102], are originally proposed to address Noisy-Q issues. When applied to our Noisy-R scenarios, they tend to easily disrupt the intrinsic logical coherence within the thought chain. Although these methods utilizing smooth strategies (e.g., random smoothing or masking) perform well on the commonsense dataset, they can hardly handle the more difficult reasoning tasks and even degenerate close to $0\\%$ , e.g., in the Symbolic Longer task. Another method, SC [83], performs better than the base model in all tasks, improving both clean and noisy reasoning performance. However, SC does not conduct explicit denoising on rationales during its reasoning procedure. In addition, SC also requires a high computation cost (refer to Appendix F.2). ", "page_idx": 5}, {"type": "text", "text": "Besides these methods, next, we analyze LLMs\u2019 intrinsic properties under noisy rationales as follows. ", "page_idx": 5}, {"type": "text", "text": "Observation 4.3. Adjusting model temperature can help reasoning under noisy rationales. In Tab. 4, we evaluate the base LLM using different temperatures on 3-shot demonstrations. Overall, reducing temperature can enhance the model\u2019s accuracy under both noisy and clean rationale reasoning, compared to the default temperature of 1. However, the relationship between temperature and accuracy is not linear for noisy reasoning; instead, there are multiple peaks in accuracy within the temperature range of 0 to 1. Additionally, it is found that excessively low temperatures (e.g., 0) tend to result in verbose and repeated responses, which cause the model to exceed token limits up to $30\\%$ in symbolic tasks where the length of expected answers is quite variable among different questions. ", "page_idx": 5}, {"type": "text", "text": "Observation 4.4. Prompting with more noisy examples boosts reasoning accuracy on most tasks. In Tab. 5, we evaluate the model using different numbers of exemplars while keeping the temperature at 1. In general, the LLM\u2019s accuracy will still improve as the number of noisy examples increases in the clean and noisy settings. However, it should be noted that in tasks with high-level noise from NoRa-Math, increasing prompting examples can degenerate accuracy. For example, in the base-9 inaccurate-hard task, prompting with noisy rationales is even worse than the 0-shot accuracy of $7.2\\%$ . Further, we provide a deeper analysis of increasing the number of noisy examples in Appendix F.6. ", "page_idx": 5}, {"type": "image", "img_path": "FbuODM02ra/tmp/1e580445db2e7147cf061ea437d078dee0990bcf2b9bff48ba8966857cec99a7.jpg", "img_caption": ["Figure 4: CD-CoT\u2019s first two steps for data denois- Figure 5: CD-CoT constructs $M$ inputs ( $K$ -shot) ing. First, it rephrases the $i$ -th noisy example by by allocating the $K\\cdot M$ rephrased rationales. contrasting it with the clean example. Then, with These inputs are concatenated with the clean exthe obtained $N$ rephrased examples, it selects the ample and test question and then fed to an LLM $M$ qualified candidates by checking the validity for reasoning separately. The obtained $D$ answers of the rephrased answers $\\hat{y}_{i1},\\dotsc,\\hat{y}_{i N}$ w.r.t. $y_{i}$ . are equally voted to obtain the final answer $y$ . "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Observation 4.5. Different LLMs are generally vulnerable to noisy rationales. In Tab. 6, we evaluate different LLMs across three settings: 0-shot CoT, 3-shot clean rationales, and 3-shot mediumlevel noisy rationales. Notably, Gemini-Pro outperforms GPT-3.5 in overall performance. However, it demonstrates a similar degree of sensitivity to noise, with a $2.4\\%{-}15.7\\%$ performance decline with irrelevant rationales and a $7.8\\%{-}66.8\\%$ decline with inaccurate rationales compared to clean rationales. While Mixtral 8x7B shows a slight underperformance compared to GPT-3.5, it also manifests a vulnerability to noise, incurring a $1.4\\%{-}\\bar{1}1.2\\%$ loss with irrelevant rationales and a greater $4.2\\%{-}23.8\\%$ loss with inaccurate rationales. By contrast, Llama2-70B performs suboptimally, with a $0.4\\%{-2.0\\%}$ drop for irrelevant thoughts and a larger $1.0\\%{-2.2\\%}$ drop for inaccurate thoughts. ", "page_idx": 6}, {"type": "text", "text": "Further investigation. Inspired by Min et al. [54], we further explore the mapping among questions, rationales, and answers through shuffling experiments. Specifically, given the 3-shot prompting examples $\\{(x_{1},\\mathcal{T}_{1},y_{1}),(x_{2},\\mathcal{T}_{2},y_{2}),(x_{3},\\bar{\\mathcal{T}}_{3},\\bar{y_{3}})\\}$ , we test three configurations, i.e., shuffle questions $\\{(x_{1},7_{3},y_{3}),(x_{2},7_{1},y_{1}),(x_{3},7_{2},y_{2})\\}$ , shuffle answers $\\{(x_{1},\\overrightharpoon_{1},y_{3}),(x_{2},\\mathcal{T}_{2},y_{1}),(x_{3},\\overrightharpoonup_{3},y_{2})\\}$ , and shuffle rationales $\\{(x_{1},\\mathcal{T}_{3},y_{1}),(x_{2},\\mathcal{T}_{1},y_{2}),(x_{3},\\mathcal{T}_{2},y_{3})\\}$ . These break the original mappings. The results under these configurations are shown in Tab. 7, which induces the following observation. ", "page_idx": 6}, {"type": "text", "text": "Observation 4.6. Shuffling the mappings of prompting examples degenerates the reasoning but still performs better than without prompting. This means that while LLMs may not heavily rely on the exact mapping (of question, rationale, and answer), they still benefti from demonstrating information even with shuffling. Notably, this finding is consistent with the conclusions of [54] that LLMs learn more abstract task information from the demonstrations rather than simply memorizing question-answer pairs. More importantly, LLMs are less vulnerable to shuffled mappings than noisy rationales. Unlike shuffling, the irrelevant or inaccurate information in noisy rationales introduces misleading elements that significantly interfere with the model\u2019s ability to learn correct task patterns, thereby resulting in more severe performance degradation. This extends [54]\u2019s finding and shows that the quality of reasoning steps can be more crucial than the exact mapping of prompting examples. ", "page_idx": 6}, {"type": "text", "text": "5 Method ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "This section aims to enable LLMs to discern and remove noisy thoughts. The observations in Sec. 4 and previous works show that current LLMs cannot achieve this with their intrinsic denoising ability, even enhanced with self-denoising methods. Therefore, we would claim that the external supervision is necessary for enhancement, which should be sufficient for denoising and accessible in practice. Existing methods with external supervision [29, 81, 9] require (1) oracle feedback on the test question, (2) human feedback of errors on specific tokens or positions, or (3) expert knowledge to construct detailed descriptions of specific tasks. By contrast, we believe that a clean CoT demonstration is more attainable and practical, which can be the minimal requirement for denoising-purpose prompting. ", "page_idx": 6}, {"type": "text", "text": "Therein, we assume that LLMs can identify noisy thoughts by contrasting a pair of noisy and clean rationales and discerning their differences, similar to contrastive learning [26, 6, 35]. Here, the denoising power could come from the abilities of the instruction following and step-by-step reasoning [84, 66]. Hence, we propose the framework of CD-CoT, Contrastive Denoising with noisy CoT. The design principle is to explore and then exploit, i.e., (1) rephrasing and selecting rationales in input space to achieve explicit denoising, and then (2) exploring diverse rationales and voting answers in output space for deriving the final answer, as in Figs. 4 & 5. The details are as follows. ", "page_idx": 6}, {"type": "text", "text": "Algorithm 1 CD-CoT: Contrastive Denoising with Noisy Chain-of-Thought. ", "page_idx": 7}, {"type": "text", "text": "Require: an LLM $f_{\\theta}$ , the prompt of contrastive denoising $\\mathcal{P}_{\\mathrm{denoise}}$ , one test question $x_{\\mathrm{{test}}}$ , one clean   \nexample $(x_{\\mathrm{{C}}},\\tau_{\\mathrm{{C}}},y_{\\mathrm{{C}}})$ , K prompting examples ${\\cal{S}}_{n}=\\{(x_{i},T_{i},y_{i})\\}_{i=1}^{K}$ , hyper-parameters $N,M$ ,   \nand reasoning budget $\\{B_{i}\\}_{i=1}^{M}$ (satisfies that $\\Sigma_{i=1}^{M}B_{i}=D$ , where $D$ is the total budget).   \n1: for $i=1\\dots K$ do   \n2: initialize the set of rephrased results of $i$ -th example $\\mathcal{R}_{i}\\gets\\emptyset$ .   \n3: for $j=1\\ldots N$ do   \n4: # Step-1: Rationale Rephrasing via Supervised Contrasting   \n5: obtain a rephrased example as $(x_{i},\\hat{{\\cal T}_{i}},\\hat{y}_{i})\\gets f_{\\boldsymbol{\\theta}}\\Big(\\mathcal{P}_{\\mathrm{denoise}}(x_{\\mathrm{C}},\\mathcal{T}_{\\mathrm{C}},y_{\\mathrm{C}},x_{i},\\mathcal{T}_{i},y_{i})\\Big)$ .   \n6: if match answer $\\hat{y}_{i}=y_{i}$ , then store the rephrased example as $\\mathcal{R}_{i}\\leftarrow\\mathcal{R}_{i}\\cup\\{(x_{i},\\hat{T}_{i},\\hat{y}_{i})\\}$ .   \n7: end for   \n8: # Step-2: Rationale Selection   \n9: randomly select $M$ rephrased examples from $\\mathcal{R}_{i}$ and obtain $\\tilde{\\mathcal{R}}_{i}=\\{(x_{i s},\\hat{\\mathcal{T}}_{i s},\\hat{y}_{i s})\\}_{s=1}^{M}$ .   \n10: end for   \n11: # Step-3: Rationale Exploration   \n12: initialize the set of answers $y\\gets\\emptyset$ .   \n13: for $i=1\\dots M$ do   \n14: construct an input $\\mathcal{P}_{i}\\leftarrow\\{(x_{j i},\\hat{\\mathcal{T}}_{j i},\\hat{y}_{j i})\\}_{j=1}^{K}$ , where $(x_{j i},\\hat{\\mathcal{T}}_{j i},\\hat{y}_{j i})$ is the $i$ -th element of $\\hat{\\mathcal{R}}_{j}$ .   \n15: concatenate $\\mathcal{P}_{i}$ with the clean example and test question as $\\mathcal{P}_{i}\\leftarrow\\mathcal{P}_{i}\\cup\\{(x_{\\mathrm{C}},T_{\\mathrm{C}},y_{\\mathrm{C}}),x_{\\mathrm{test}}\\}$ .   \n16: for $j=1\\ldots B_{M}$ do   \n17: get one answer by LLM reasoning as $y_{j}\\leftarrow f_{\\theta}(\\mathcal{P}_{i})$ .   \n18: store the answer as $y\\leftarrow y\\cup\\{y_{j}\\}$ .   \n19: end for   \n20: end for   \n21: # Step-4: Answer Voting   \n22: initialize the dictionary of answer count $\\mathcal{C}$ that $\\forall y_{j}\\in\\mathcal{Y},\\mathcal{C}[y_{j}]=0$ .   \n23: for $j=1\\ldots D$ do   \n24: update $\\mathcal{C}[y_{j}]\\gets(\\mathcal{C}[y_{j}]+1)$ .   \n25: end for   \n26: get the final answer $y$ with maximum counts as $y\\gets\\arg\\operatorname*{max}_{y}\\mathcal{C}[y]$ .   \n27: return the answer $y$ . ", "page_idx": 7}, {"type": "text", "text": "5.1 Implementation ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Step-1: Rephrasing via Supervised Contrasting (1 to $N$ ). First, we establish a general prompt of contrastive rephrasing to construct a pair of contrastive examples, as shown in the template below. This steers the model towards learning from the clean example and then rephrasing and rectifying the noisy examples. To be specific, given one clean example and $K$ noisy examples, we generate $N$ rephrased rationales for each noisy example independently and obtain $K\\!\\cdot\\!N$ rephrased rationales. ", "page_idx": 7}, {"type": "text", "text": "Prompt of Contrastive Rationale Rephrasing:   \nHere are two examples for the same type of task: the first example has correct explanation and correct answer, and the second example has distracted explanation and correct answer. Please follow the first example and give me the correct explanation and answer for the second example, which should be logically consistent with the first one.   \nFirst Example: Q: [Question], E: [Explanation], A: [Answer].   \nSecond Example: Q: [Question], E: [Explanation], A: [Answer]. ", "page_idx": 7}, {"type": "text", "text": "Step-2: Rationale Selection ( $N$ to $M$ , $N\\!\\geq M)$ . Next, we employ answer matching to select those rephrased examples with unchanged answers, leaving behind a refined candidate pool. Subsequently, we randomly select $M$ rephrased rationales from the pool and concatenate them to form the contexts. ", "page_idx": 7}, {"type": "text", "text": "Step-3: Rationale Exploration $M$ to $D$ , $M\\leq D$ ). For the $M$ different contexts, we explore rationales by repeated reasoning with the budget of $D$ reasoning repetitions. Notably, a higher temperature parameter, e.g., 1, is set to introduce more randomness in generating diverse rationales. ", "page_idx": 7}, {"type": "text", "text": "Step-4: Answer Voting ( $D$ to 1). Ultimately, all the $D$ answers are equally voted into a final answer. ", "page_idx": 7}, {"type": "text", "text": "Instantiation. By tuning the hyper-parameters $N,M$ , and $D$ , we balance exploration and exploitation in the input and output space. The overall procedure of our proposed CD-CoT is presented in Algorithm 1. Besides, we further explain the details of each step of this algorithm in Appendix E.2. ", "page_idx": 7}, {"type": "table", "img_path": "FbuODM02ra/tmp/c5e7d40fa55cacafe49c8884fb7123ec89c7067d9b7d514aac64b2787982c506.jpg", "table_caption": [], "table_footnote": ["Table 8: Performance of denoising methods that require additional information for supervision. "], "page_idx": 8}, {"type": "text", "text": "Theoretical analysis. To understand the underlying mechanism of CD-CoT, we also conduct the theoretical analysis based on the distinguishability [90] of in-context learning. The full analysis is in Appendix D, where we find that the noisy demonstration in ICL can decrease the distinguishability of in-context matching with the clean-prompt distribution, while our method can mitigate this issue. Besides, we build a self-supervised variant of CD-CoT and empirically evaluate it in Appendix F.7. ", "page_idx": 8}, {"type": "text", "text": "5.2 Empirical Study ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this part, we empirically verify the effectiveness of CD-CoT and start by introducing the baselines. ", "page_idx": 8}, {"type": "text", "text": "Baseline methods. We employ three methods that require additional information: (1) Self-Correction with Oracle Feedback (SCO) [29] utilizes the ground truth answers of test questions to determine when to terminate the self-correction loop; (2) Backtracking (BT) [81] guides self-correction by providing the model with the position of the first noisy thought; (3) Contrastive Chain-of-Thought (CC) [9] conducts direct reasoning with all the noisy or clean examples without implicit or explicit denoising. ", "page_idx": 8}, {"type": "text", "text": "Main results. As in Tab. 8, CD-CoT demonstrates a significant performance improvement across all datasets, with an average improvement of $17.8\\%$ compared with the base model under noisy settings. Notably, on Math-Base-9, Math-Base-11, and Symbolic-Equal, CD-CoT surpasses all baseline methods by a significant margin. On Symbolic-Longer and Commonsense, CD-CoT only slightly lags behind SCO. However, SCO requires the ground truth answer to the test question, which should be unknown in practice, as pointed out in [29]. In comparison, CD-CoT only necessitates an additional clean demonstration, making it much more practical to apply across realistic scenarios. Notably, CD-CoT outperforms SCO in 20 out of 30 settings and surpasses BT, CC in all 30 settings. ", "page_idx": 8}, {"type": "text", "text": "Besides, CD-CoT displays remarkable resistance to the magnitude of noise. Therein, CD-CoT demonstrates enhanced resilience against inaccurate noise on mathematical tasks, which are quite challenging. For instance, on Math Base-9 with inaccurate rationales, the average accuracies of SCO and BT decline significantly by $28.8\\%$ and $26.3\\%$ compared to the accuracies with clean rationales. In contrast, CD-CoT exhibits a more modest decline of $7.0\\%$ . An ablation study of components in Appendix F.3 demonstrates the denoising power and performance gain of CD-CoT, attributed to its contrastive denoising with rationale rephrasing as well as repeated reasoning with voting components. ", "page_idx": 8}, {"type": "text", "text": "Ablation study of varying hyper-parameters. By manipulating the values of $N$ , M, $D$ , and $C$ , we generate diverse algorithm instances. Here, $D$ denotes the reasoning times allocated to the $M$ inputs, while $C$ signifies whether the clean example is used in step 3. As demonstrated in Tab. 9, the clean example utilized by CD-CoT during the reasoning process plays a pivotal role. The omission of this clean example results in an average decrease of $3.3\\%$ and $4.5\\%$ in accuracy under irrelevant noise and inaccurate noise, respectively. Besides, the accuracy exhibits subtle variations when employing different algorithm instances, with the highest average accuracy observed at $51.3\\%$ and the lowest average accuracy at $49.3\\%$ . Further, Tab. 10 presents the average number of tokens used in reasoning. We set $M\\!=\\!2$ to strike a balance. Please refer to Appendix E.3 for detailed hyper-parameter selection. ", "page_idx": 8}, {"type": "table", "img_path": "FbuODM02ra/tmp/d8bf9374322a841fd9369e8e8538ab6c41a745e024cf76c887361606db45189b.jpg", "table_caption": [], "table_footnote": ["Table 11: Comparing methods with different LLMs. tionales by different reasoning methods. "], "page_idx": 9}, {"type": "text", "text": "Ablation study with different LLMs. We examine the generalization of CD-CoT across different LLMs. As shown in Tab. 11, with comparably more powerful LLMs, e.g., GPT-3.5-turbo and GeminiPro, CD-CoT demonstrates notable improvements in average accuracy. It respectively achieves increases of $23.4\\%$ and $21.6\\%$ in accuracy compared to base models and surpasses all the baselines. ", "page_idx": 9}, {"type": "text", "text": "Case Study. We illustrate the denoising effects of various robust methods using Math Base-9 as an example. The results in Tab. 12 indicate that the introduction of random characters by SM disrupts the logic of the rationale. SD fails to eliminate all the noise while recovering the input content, and SP alters the original rationale\u2019s reasoning process even when noise removal is successful. In contrast, CD-CoT significantly removes noisy thoughts and ensures format alignment with the original rationale. More denoising examples and an entire case study are in Appendix F.9 and G, respectively. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we investigate the under-explored problem of noisy rationales in LLMs. We introduce the NoRa dataset, which tests LLMs against irrelevant or inaccurate thoughts in question-answer scenarios. Our findings show LLMs\u2019 vulnerability to noisy rationales is inadequately mitigated by existing robust methods. We thereby design the CD-CoT method to enhance the robustness via contrastive denoising. The extension advocates for advancing LLMs by strategies, e.g., external knowledge bases with a retrieval-augmented framework, robust inductive reasoning to extract rules from noisy examples, and multi-modal data integration to enhance the robustness of LLM reasoning. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "ZKZ, RT, JNZ, and BH were supported by Guangdong Basic and Applied Basic Research Foundation Nos. 2022A1515011652 and 2024A1515012399, NSFC General Program No. 62376235, HKBU Faculty Niche Research Areas No. RC-FNRA-IG/22-23/SCI/04, and HKBU CSD Departmental Incentive Scheme. YWL and ZMW were supported by National Natural Science Foundation of China under Grants 62271357, Natural Science Foundation of Hubei Province under Grants 2023BAB072, and Fundamental Research Funds for the Central Universities under Grants 2042023kf0134. The authors especially thank Jiangchao Yao, Xuan Li, and Xiao Feng for the constructive discussions and insightful suggestions that helped improve this work. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.   \n[2] Ayush Agrawal, Lester Mackey, and Adam Tauman Kalai. Do language models know when they\u2019re hallucinating references? arXiv preprint arXiv:2305.18248, 2023.   \n[3] Afra Feyza Aky\u00fcrek, Ekin Aky\u00fcrek, Aman Madaan, Ashwin Kalyan, Peter Clark, Derry Wijaya, and Niket Tandon. Rl4f: Generating natural language feedback with reinforcement learning for repairing model outputs. arXiv preprint arXiv:2305.08844, 2023.   \n[4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. In NeurIPS, 2020.   \n[5] Paul Chandler and John Sweller. Cognitive load theory and the format of instruction. Cognition and instruction, 1991. [6] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In ICML, 2020.   \n[7] Yongqiang Chen, Yonggang Zhang, Yatao Bian, Han Yang, Kaili Ma, Binghui Xie, Tongliang Liu, Bo Han, and James Cheng. Learning causally invariant representations for out-ofdistribution generalization on graphs. In NeurIPS, 2022. [8] Yongqiang Chen, Wei Huang, Kaiwen Zhou, Yatao Bian, Bo Han, and James Cheng. Understanding and improving feature learning for out-of-distribution generalization. In NeurIPS, 2023. [9] Yew Ken Chia, Guizhen Chen, Luu Anh Tuan, Soujanya Poria, and Lidong Bing. Contrastive chain-of-thought prompting. arXiv preprint arXiv:2311.09277, 2023.   \n[10] Eunsol Choi, He He, Mohit Iyyer, Mark Yatskar, Wen-tau Yih, Yejin Choi, Percy Liang, and Luke Zettlemoyer. Quac: Question answering in context. arXiv preprint arXiv:1808.07036, 2018.   \n[11] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 2023.   \n[12] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.   \n[13] Jeremy Cohen, Elan Rosenfeld, and Zico Kolter. Certified adversarial robustness via randomized smoothing. In ICML, 2019.   \n[14] Michela Del Vicario, Antonio Scala, Guido Caldarelli, H Eugene Stanley, and Walter Quattrociocchi. Modeling confirmation bias and polarization. Scientific reports, 2017. ", "page_idx": 10}, {"type": "text", "text": "[15] Ronald A DeVore and George G Lorentz. Constructive approximation. 1993. ", "page_idx": 11}, {"type": "text", "text": "[16] Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. A survey for in-context learning. arXiv preprint arXiv:2301.00234, 2022.   \n[17] Luciano Floridi and Massimo Chiriatti. Gpt-3: Its nature, scope, limits, and consequences. Minds and Machines, 2020.   \n[18] C Daniel Freeman, Laura Culp, Aaron Parisi, Maxwell L Bileschi, Gamaleldin F Elsayed, Alex Rizkowsky, Isabelle Simpson, Alex Alemi, Azade Nova, Ben Adlam, et al. Frontier language models are not robust to adversarial arithmetic, or\" what do i need to say so you agree $2+2{=}\\,5?$ arXiv preprint arXiv:2311.07587, 2023.   \n[19] Chengguang Gan and Tatsunori Mori. Sensitivity and robustness of large language models to prompt in japanese. arXiv preprint arXiv:2305.08714, 2023.   \n[20] Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daum\u00e9 Iii, and Kate Crawford. Datasheets for datasets. Communications of the ACM, 2021.   \n[21] Zelalem Gero, Chandan Singh, Hao Cheng, Tristan Naumann, Michel Galley, Jianfeng Gao, and Hoifung Poon. Self-verification improves few-shot clinical information extraction. arXiv preprint arXiv:2306.00024, 2023.   \n[22] Chen Gong, Qizhou Wang, Tongliang Liu, Bo Han, Jane You, Jian Yang, and Dacheng Tao. Instance-dependent positive and unlabeled learning with labeling bias estimation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021.   \n[23] Thomas L Grifftihs. Understanding human intelligence through human limitations. Trends in Cognitive Sciences, 2020.   \n[24] Bo Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao Xu, Weihua Hu, Ivor Tsang, and Masashi Sugiyama. Co-teaching: Robust training of deep neural networks with extremely noisy labels. In NeurIPS, 2018.   \n[25] Alex Havrilla and Maia Iyer. Understanding the effect of noise in llm training data with algorithmic chains of thought. arXiv preprint arXiv:2402.04004, 2024.   \n[26] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In CVPR, 2020.   \n[27] Qianyu He, Jie Zeng, Wenhao Huang, Lina Chen, Jin Xiao, Qianxi He, Xunzhe Zhou, Jiaqing Liang, and Yanghua Xiao. Can large language models understand real-world complex instructions? In AAAI, 2024.   \n[28] Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei Han. Large language models can self-improve. arXiv preprint arXiv:2210.11610, 2022.   \n[29] Jie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu Steven Zheng, Adams Wei Yu, Xinying Song, and Denny Zhou. Large language models cannot self-correct reasoning yet. In ICLR, 2024.   \n[30] Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, et al. A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. arXiv preprint arXiv:2311.05232, 2023.   \n[31] Irving L Janis. Groupthink. IEEE Engineering Management Review, 2008.   \n[32] Robin Jia and Percy Liang. Adversarial examples for evaluating reading comprehension systems. In EMNLP, 2017.   \n[33] Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024.   \n[34] Shuyang Jiang, Yuhao Wang, and Yu Wang. Selfevolve: A code evolution framework via large language models. arXiv preprint arXiv:2306.02907, 2023.   \n[35] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. In NeurIPS, 2020.   \n[36] Geunwoo Kim, Pierre Baldi, and Stephen McAleer. Language models can solve computer tasks. arXiv preprint arXiv:2303.17491, 2023.   \n[37] BJK Kleijn and AW van der Vaart. The bernstein-von-mises theorem under misspecification. Electronic Journal of Statistics, 2012.   \n[38] Sin Yee Koh. The inversion of majority/minority at the de/reterritorialised urban higher education enclave: Xiamen university malaysia. Urban Studies, 2022.   \n[39] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. In NeurIPS, 2022.   \n[40] Seonmin Koo, Chanjun Park, Seolhwa Lee, Jaehyung Seo, Sugyeong Eo, Hyeonseok Moon, and Heuiseok Lim. Uncovering the risks and drawbacks associated with the use of synthetic data for grammatical error correction. IEEE Access, 2023.   \n[41] Brenden Lake and Marco Baroni. Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks. In ICML, 2018.   \n[42] Fangyu Lei, Xiang Li, Yifan Wei, Shizhu He, Yiming Huang, Jun Zhao, and Kang Liu. S \u02c63 hqa: A three-stage approach for multi-hop text-table hybrid question answering. arXiv preprint arXiv:2305.11725, 2023.   \n[43] Miaoran Li, Baolin Peng, and Zhu Zhang. Self-checker: Plug-and-play modules for factchecking with large language models. arXiv preprint arXiv:2305.14623, 2023.   \n[44] Xuan Li, Zhanke Zhou, Jianing Zhu, Jiangchao Yao, Tongliang Liu, and Bo Han. Deepinception: Hypnotize large language model to be jailbreaker. arXiv preprint arXiv:2311.03191, 2023.   \n[45] Yuan Li, Yixuan Zhang, and Lichao Sun. Metaagents: Simulating interactions of human behaviors for llm-based task-oriented coordination via collaborative generative agents. arXiv preprint arXiv:2310.06500, 2023.   \n[46] Ke Liang, Yue Liu, Sihang Zhou, Wenxuan Tu, Yi Wen, Xihong Yang, Xiangjun Dong, and Xinwang Liu. Knowledge graph contrastive learning based on relation-symmetrical structure. IEEE Transactions on Knowledge and Data Engineering, 2023.   \n[47] Ke Liang, Lingyuan Meng, Meng Liu, Yue Liu, Wenxuan Tu, Siwei Wang, Sihang Zhou, Xinwang Liu, Fuchun Sun, and Kunlun He. A survey of knowledge graph reasoning on graph types: Static, dynamic, and multi-modal. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024.   \n[48] Alisa Liu, Zhaofeng Wu, Julian Michael, Alane Suhr, Peter West, Alexander Koller, Swabha Swayamdipta, Noah A Smith, and Yejin Choi. We\u2019re afraid language models aren\u2019t modeling ambiguity. arXiv preprint arXiv:2304.14399, 2023.   \n[49] Feng Liu, Bo Han, Tongliang Liu, Chen Gong, Gang Niu, Mingyuan Zhou, Masashi Sugiyama, et al. Probabilistic margins for instance reweighting in adversarial training. In NeurIPS, 2021.   \n[50] Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. What makes good in-context examples for gpt-3? arXiv preprint arXiv:2101.06804, 2021.   \n[51] Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. arXiv preprint arXiv:2104.08786, 2021.   \n[52] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with self-feedback. arXiv preprint arXiv:2303.17651, 2023.   \n[53] Richard E Mayer. Thinking and problem solving: An introduction to human cognition and learning. Scott, Foresman, 1977.   \n[54] Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. Rethinking the role of demonstrations: What makes in-context learning work? In EMNLP, 2022.   \n[55] Carey K Morewedge and Daniel Kahneman. Associative processes in intuitive judgment. Trends in cognitive sciences, 2010.   \n[56] Paul Norris. Emotional reasoning. University of Massachusetts Amherst, 2000.   \n[57] Liangming Pan, Michael Saxon, Wenda Xu, Deepak Nathani, Xinyi Wang, and William Yang Wang. Automatically correcting large language models: Surveying the landscape of diverse self-correction strategies. arXiv preprint arXiv:2308.03188, 2023.   \n[58] Lalchand Pandia and Allyson Ettinger. Sorting through the noise: Testing robustness of information processing in pre-trained language models. In EMNLP, 2021.   \n[59] Debjit Paul, Mete Ismayilzada, Maxime Peyrard, Beatriz Borges, Antoine Bosselut, Robert West, and Boi Faltings. Refiner: Reasoning feedback on intermediate representations. arXiv preprint arXiv:2304.01904, 2023.   \n[60] Ethan Perez, Douwe Kiela, and Kyunghyun Cho. True few-shot learning with language models. In NeurIPS, 2021.   \n[61] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 2019.   \n[62] Alexander Robey, Eric Wong, Hamed Hassani, and George J Pappas. Smoothllm: Defending large language models against jailbreaking attacks. arXiv preprint arXiv:2310.03684, 2023.   \n[63] Tomas Rokicki, Herbert Kociemba, Morley Davidson, and John Dethridge. The diameter of the rubik\u2019s cube group is twenty. SIAM REVIEW, 2014.   \n[64] Nithya Sambasivan, Shivani Kapania, Hannah Highflil, Diana Akrong, Praveen Paritosh, and Lora M Aroyo. \u201ceveryone wants to do the model work, not the data work\u201d: Data cascades in high-stakes ai. In CHI, 2021.   \n[65] Abulhair Saparov and He He. Language models are greedy reasoners: A systematic formal analysis of chain-of-thought. arXiv preprint arXiv:2210.01240, 2022.   \n[66] Rylan Schaeffer, Brando Miranda, and Sanmi Koyejo. Are emergent abilities of large language models a mirage? In NeurIPS, 2023.   \n[67] J\u00e9r\u00e9my Scheurer, Jon Ander Campos, Tomasz Korbak, Jun Shern Chan, Angelica Chen, Kyunghyun Cho, and Ethan Perez. Training language models with language feedback at scale. arXiv preprint arXiv:2303.16755, 2023.   \n[68] Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed H Chi, Nathanael Sch\u00e4rli, and Denny Zhou. Large language models can be easily distracted by irrelevant context. In ICML, 2023.   \n[69] Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik R Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning. In NeurIPS, 2023.   \n[70] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. Nature, 2016.   \n[71] Koustuv Sinha, Shagun Sodhani, Jin Dong, Joelle Pineau, and William L Hamilton. Clutrr: A diagnostic benchmark for inductive reasoning from text. arXiv preprint arXiv:1908.06177, 2019.   \n[72] Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adri\u00e0 Garriga-Alonso, et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615, 2022.   \n[73] Joe Stacey, Jianpeng Cheng, John Torr, Tristan Guigue, Joris Driesen, Alexandru Coca, Mark Gaynor, and Anders Johannsen. Lucid: Llm-generated utterances for complex and interesting dialogues. arXiv preprint arXiv:2403.00462, 2024.   \n[74] Zhenheng Tang, Yuxin Wang, Xin He, Longteng Zhang, Xinglin Pan, Qiang Wang, Rongfei Zeng, Kaiyong Zhao, Shaohuai Shi, Bingsheng He, et al. Fusionai: Decentralized training and deploying llms with massive consumer-level gpus. arXiv preprint arXiv:2309.01172, 2023.   \n[75] Zhenheng Tang, Xueze Kang, Yiming Yin, Xinglin Pan, Yuxin Wang, Xin He, Qiang Wang, Rongfei Zeng, Kaiyong Zhao, Shaohuai Shi, Amelie Chi Zhou, Bo Li, Bingsheng He, and Xiaowen Chu. Fusionllm: A decentralized llm training system on geo-distributed gpus with adaptive compression. arXiv preprint arXiv:2410.12707, 2024.   \n[76] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023.   \n[77] David Thorstad. Cognitive bias in large language models: Cautious optimism meets antipanglossian meliorism. arXiv preprint arXiv:2311.10932, 2023.   \n[78] Qingyuan Tian, Hanlun Zhu, Lei Wang, Yang Li, and Yunshi Lan. R3 prompting: Review, rephrase and resolve for chain-of-thought reasoning in large language models under noisy context prompting: Review, rephrase and resolve for chain-of-thought reasoning in large language models under noisy context. In EMNLP, 2023.   \n[79] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.   \n[80] Amos Tversky and Daniel Kahneman. Judgment under uncertainty: Heuristics and biases: Biases in judgments reveal some heuristics of thinking under uncertainty. Science, 1974.   \n[81] Gladys Tyen, Hassan Mansoor, Peter Chen, Tony Mak, and Victor Ca\u02d8rbune. Llms cannot find reasoning errors, but can correct them! arXiv preprint arXiv:2311.08516, 2023.   \n[82] Karthik Valmeekam, Matthew Marquez, Sarath Sreedharan, and Subbarao Kambhampati. On the planning abilities of large language models-a critical investigation. In NeurIPS, 2023.   \n[83] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In ICLR, 2023.   \n[84] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682, 2022.   \n[85] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. In NeurIPS, 2022.   \n[86] Jerry Wei, Jason Wei, Yi Tay, Dustin Tran, Albert Webson, Yifeng Lu, Xinyun Chen, Hanxiao Liu, Da Huang, Denny Zhou, et al. Larger language models do in-context learning differently. arXiv preprint arXiv:2303.03846, 2023.   \n[87] G Elliott Wimmer and Daphna Shohamy. Preference by association: how memory mechanisms in the hippocampus bias decisions. Science, 2012.   \n[88] Zhaofeng Wu, Linlu Qiu, Alexis Ross, Ekin Aky\u00fcrek, Boyuan Chen, Bailin Wang, Najoung Kim, Jacob Andreas, and Yoon Kim. Reasoning or reciting? exploring the capabilities and limitations of language models through counterfactual tasks. arXiv preprint arXiv:2307.02477, 2023. [89] Zhiheng Xi, Senjie Jin, Yuhao Zhou, Rui Zheng, Songyang Gao, Tao Gui, Qi Zhang, and Xuanjing Huang. Self-polish: Enhance reasoning in large language models via problem refinement. arXiv preprint arXiv:2305.14497, 2023.   \n[90] Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of in-context learning as implicit bayesian inference. arXiv preprint arXiv:2111.02080, 2021. [91] Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V Le, Denny Zhou, and Xinyun Chen. Large language models as optimizers. arXiv preprint arXiv:2309.03409, 2023.   \n[92] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. In NeurIPS, 2023. [93] Seonghyeon Ye, Yongrae Jo, Doyoung Kim, Sungdong Kim, Hyeonbin Hwang, and Minjoon Seo. Selfee: Iterative self-revising llm empowered by self-feedback generation. Blog post, May, 2023.   \n[94] Xi Ye and Greg Durrett. The unreliability of explanations in few-shot prompting for textual reasoning. In NeurIPS, 2022.   \n[95] Simon Chi Lok Yu, Jie He, Pasquale Minervini, and Jeff Z Pan. Evaluating the adversarial robustness of retrieval-based in-context learning for large language models. arXiv preprint arXiv:2405.15984, 2024.   \n[96] Wenhao Yu, Zhihan Zhang, Zhenwen Liang, Meng Jiang, and Ashish Sabharwal. Improving language models via plug-and-play retrieval feedback. arXiv preprint arXiv:2305.14002, 2023.   \n[97] Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. Star: Bootstrapping reasoning with reasoning. In NeurIPS, 2022.   \n[98] Jiehang Zeng, Jianhan Xu, Xiaoqing Zheng, and Xuanjing Huang. Certified robustness to text adversarial attacks by randomized [mask]. Computational Linguistics, 2023.   \n[99] Yiming Zhang, Shi Feng, and Chenhao Tan. Active example selection for in-context learning. arXiv preprint arXiv:2211.04486, 2022.   \n[100] Yongqi Zhang, Zhanke Zhou, Quanming Yao, Xiaowen Chu, and Bo Han. Adaprop: Learning adaptive propagation for graph neural network based knowledge graph reasoning. In SIGKDD, 2023.   \n[101] Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, Longyue Wang, Anh Tuan Luu, Wei Bi, Freda Shi, and Shuming Shi. Siren\u2019s song in the ai ocean: A survey on hallucination in large language models. arXiv preprint arXiv:2309.01219, 2023.   \n[102] Zhen Zhang, Guanhua Zhang, Bairu Hou, Wenqi Fan, Qing Li, Sijia Liu, Yang Zhang, and Shiyu Chang. Certified robustness for large language models with self-denoising. arXiv preprint arXiv:2307.07171, 2023.   \n[103] Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. Automatic chain of thought prompting in large language models. In ICLR, 2023.   \n[104] Haiyan Zhao, Hanjie Chen, Fan Yang, Ninghao Liu, Huiqi Deng, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, and Mengnan Du. Explainability for large language models: A survey. ACM Transactions on Intelligent Systems and Technology, 2023.   \n[105] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. A survey of large language models. arXiv preprint arXiv:2303.18223, 2023.   \n[106] Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. Calibrate before use: Improving few-shot performance of language models. In ICML, 2021.   \n[107] Hongyi Zheng and Abulhair Saparov. Noisy exemplars make large language models more robust: A domain-agnostic behavioral analysis. In EMNLP, 2023.   \n[108] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. In NeurIPS, 2023.   \n[109] Denny Zhou, Nathanael Sch\u00e4rli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, et al. Least-to-most prompting enables complex reasoning in large language models. arXiv preprint arXiv:2205.10625, 2022.   \n[110] Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy Ba. Large language models are human-level prompt engineers. arXiv preprint arXiv:2211.01910, 2022.   \n[111] Zhanke Zhou, Jiangchao Yao, Jiaxu Liu, Xiawei Guo, Quanming Yao, Li He, Liang Wang, Bo Zheng, and Bo Han. Combating bilateral edge noise for robust link prediction. In NeurIPS, 2023.   \n[112] Zhanke Zhou, Chenyu Zhou, Xuan Li, Jiangchao Yao, Quanming Yao, and Bo Han. On strengthening and defending graph reconstruction attack with markov chain approximation. In ICML, 2023.   \n[113] Zhanke Zhou, Yongqi Zhang, Jiangchao Yao, Quanming Yao, and Bo Han. Less is more: One-shot subgraph reasoning on large-scale knowledge graphs. In ICLR, 2024.   \n[114] Yilun Zhu, Joel Ruben Antony Moniz, Shruti Bhargava, Jiarui Lu, Dhivya Piraviperumal, Site Li, Yuan Zhang, Hong Yu, and Bo-Hsiang Tseng. Can large language models understand context? arXiv preprint arXiv:2402.00858, 2024. ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "A Further Discussion 19 ", "page_idx": 17}, {"type": "text", "text": "B Related Work 19 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "B.1 In-context Learning 20   \nB.2 Self-correction 21   \nB.3 Self-consistency . . 22   \nB.4 External Supervision 22   \nB.5 Relation with Literature . 23 ", "page_idx": 17}, {"type": "text", "text": "C Benchmark 24 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "C.1 Motivation . 24   \nC.2 A Further Discussion on Noisy Rationales in CoT Demonstrations 24   \nC.3 Real-world Examples . . 27   \nC.4 Noise Generation 31   \nC.5 NoRa Datasheet . 32   \nC.6 Full Examples of the NoRa Dataset . 34 ", "page_idx": 17}, {"type": "text", "text": "D Theoretical Analysis 37 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "E Implementation Details 40 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "E.1 Baseline Methods 40   \nE.2 Contrastive Denoising with Noisy Chain-of-Thought 41   \nE.3 Hyper-parameter Optimization 41 ", "page_idx": 17}, {"type": "text", "text": "F Full Experiments 43 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "F.1 Detailed Setups of the Experiments . 43   \nF.2 Supplementary Results of the Main Experiments 43   \nF.3 The Superior Performance and Denoising Effectiveness . 45   \nF.4 The Difficulty of Irrelevant Noise Semantics 49   \nF.5 The Number of Noisy Thoughts 51   \nF.6 The Number of Noisy Examples 52   \nF.7 Variants of CD-CoT . . 53   \nF.8 New Datasets and Large-scale Real Problems 56   \nF.9 Rephrased Examples of Different Denoising Methods . 59 ", "page_idx": 17}, {"type": "text", "text": "G Further Case Study 62 ", "page_idx": 17}, {"type": "text", "text": "H NeurIPS Checklist 64 ", "page_idx": 17}, {"type": "text", "text": "A Further Discussion ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Broader impact. This paper presents work that aims to advance the field of trustworthy machine learning and large language models. We do not find any negative societal consequences of our work. This paper does not raise any ethical concerns. This study does not involve human subjects, practices, data set releases, potentially harmful insights, methodologies, applications, potential conflicts of interest and sponsorship, discrimination/bias/fairness concerns, privacy and security issues, legal compliance, or research integrity issues. ", "page_idx": 18}, {"type": "text", "text": "Limitations. The proposed CD-CoT method is currently dependent on human-annotated, clean rationales. Future enhancements could include developing a self-supervised variant that does not rely on such examples. Exploring strategies like using contrasting noisy examples or incorporating an external knowledge base, possibly through a retrieval-augmented denoising framework, may offer significant advances in automation and robustness of reasoning. ", "page_idx": 18}, {"type": "text", "text": "Extensions. CoT and its variants have predominantly focused on deductive reasoning, leaving inductive reasoning largely unexplored. Investigating the ability of LLMs to extract rules from noisy examples is a compelling area. Additionally, theoretical analysis of noisy ICL can offer deeper insights into the Noisy-R problem. Expanding the NoRa dataset to include multi-modal scenarios, particularly visual data, is also crucial for a more comprehensive understanding of the robustness of foundation models. Further extensions include knowledge-enhanced denoising [100, 46, 47, 113], generalization to out-of-distribution noisy scenarios [7, 8], and training to fundamentally improve the robustness of language models [75, 74]. ", "page_idx": 18}, {"type": "text", "text": "B Related Work ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In this section, we provide a detailed literature review as an extension of the preliminaries (Sec. 2), including in-context learning (Appendix B.1), self-correction methods (Appendix B.2), self-consistency methods (Appendix B.3), and external supervision (Appendix B.4). We further discuss the relation between our work and literature in Appendix B.5. We also provide Fig. 6 to better understand different reasoning settings. ", "page_idx": 18}, {"type": "image", "img_path": "FbuODM02ra/tmp/bf455c6ce0fea772bf1133d341b1a0bd424a14ece6613a873eff8ea31001708c.jpg", "img_caption": ["Figure 6: Illustrations of different reasoning settings. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "B.1 In-context Learning ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "With the scaling of model size and corpus size [61, 4, 11, 105], large language models (LLMs) have demonstrated remarkable performance across a variety of tasks through in-context learning (ICL), that is, learning from a few input-output examples in the context. As a training-free framework, ICL can save on expensive training costs and be easily adapted to diverse new tasks [88, 1]. ", "page_idx": 19}, {"type": "text", "text": "However, standard ICL faces challenges when addressing intricate reasoning problems. Wei et al. [85] proposes chain-of-thought prompting (CoT), a methodology that explicitly guides LLMs in generating sequential reasoning steps to enhance their performance on intricate reasoning tasks. In particular, CoT replaces the input-output exemplars in ICL with triplets in the form of <input, rationale, output>, thereby enabling the model to learn explicit reasoning processes. ", "page_idx": 19}, {"type": "text", "text": "In the literature, various versions of chain-of-thought prompting have been proposed. To alleviate the human effort required for constructing triplet exemplars, Kojima et al. [39] proposes zero-shot CoT, a method that explicitly encourages models to generate a chain of reasoning first and then derive the final answer by employing natural language prompts like \u201cLet\u2019s think step by step\u201d. Wang et al. [83] shows that self-consistency sampling can improve reasoning accuracy by sampling diverse reasoning paths and then taking the majority vote. Zelikman et al. [97] proposes STaR, which leverages a small amount of human-annotated rationale data, along with a large dataset of question-label pairs without rationales. This approach iteratively generates additional rationales and enhances the model\u2019s ability to generate reasoning steps by fine-tuning. ", "page_idx": 19}, {"type": "text", "text": "Least-to-most prompting (LtM) [109] enhances the reasoning capability of models by decomposing a given problem into sub-problems. In particular, LtM initially breaks down the task into a series of consecutive sub-problems and subsequently answers them one by one. During the process of responding, the answer to the preceding sub-problem is incorporated into the prompt for the succeeding one. Tree of Thoughts (ToT) [92] extends LtM by exploring multiple reasoning possibilities at each step. Specifically, ToT first decomposes a given problem into several reasoning steps and generates multiple answers for each step, ultimately constructing a tree structure. Subsequently, ToT employs BFS or DFS to traverse the tree, yielding the final rationale and answer. ", "page_idx": 19}, {"type": "text", "text": "Vulnerabilities of ICL. Despite being promising, some works point out the brittleness and oversensitivity of ICL. Liu et al. [50], Perez et al. [60], Zhang et al. [99] demonstrate that ICL performance depends heavily on the choice of exemplars. Meanwhile, Zhao et al. [106], Lu et al. [51] observe that the arrangement order of in-context examples is also crucial to the ICL performance, potentially shifting results from near state-of-the-art to a random guessing. Moreover, Ye and Durrett [94], Gan and Mori [19], Zheng and Saparov [107], Zhang et al. [102] reveal LLMs\u2019 deficiencies when handling subtle perturbations within the prompts, even when such perturbations do not alter any semantic meaning. Yu et al. [95] explores the robustness of retrieval-augmented in-context learning (ICL) against demonstration attacks and test sample attacks. It focuses on perturbing the example questions (i.e., noisy questions) or labels, while our work focuses on the rationales of the examples (i.e., noisy rationales). In addition, previous work on safety [44, 112] and data noise [24, 49, 22, 111] might also inspire the robust problems in ICL. Overall, the enhancement of reasoning performance brought about by ICL is inherently unstable and susceptible to example selection, example ordering, and prompt perturbations. These observations underscore the importance the robustness of other aspects. ", "page_idx": 19}, {"type": "text", "text": "The aforementioned efforts primarily revolve around the idealized ICL, which utilizes high-quality prompts free from any noise or interference. Conversely, a parallel line of research has emerged, exploring the impact of noisy prompts on the performance of LLMs. Min et al. [54] examines the impact of in-context examples on ICL. This work observes that incorporating out-of-distribution input texts significantly diminishes the performance of standard question answering. Wei et al. [86] devises two different set-ups of ICL: ICL with flipped labels and ICL with semantically unrelated labels. Their investigation reveals that LLMs possess the capability to override semantic priors when confronted with in-context exemplars that contradict these priors. This phenomenon also suggests that larger models may be more susceptible to the influence of the noise present in examples. Shi et al. [68] examines the impact of irrelevant context on LLMs, and the results suggest that the inclusion of irrelevant information can significantly impair the performance of the models. These studies further illuminate the fragility and instability inherent in the reasoning capabilities of LLMs. ", "page_idx": 19}, {"type": "text", "text": "However, the previous works mainly consider the noisy questions/answers in standard ICL. In contrast, we move to the under-explored noisy rationales problem in the context of CoT, as illustrated in Fig. 6. ", "page_idx": 19}, {"type": "text", "text": "Numerous strategies have been proposed to address the vulnerabilities of LLM reasoning during in-context learning. These approaches can be categorized into self-correction and self-consistency, which are introduced as follows. ", "page_idx": 20}, {"type": "text", "text": "B.2 Self-correction ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Self-correction emerges as a promising direction to enhance LLM reasoning, where LLMs attempt to correct their initial responses based on feedback. One popular line of research involves utilizing manual labor or external systems to evaluate and refine models. However, this can be costly due to the manual labor involved. Another line of research leverages the LLM\u2019s inherent capabilities to correct its initial responses without the crutch of external feedback. This methodology is a promising way to make LLM-based solutions practical and deployable [57]. ", "page_idx": 20}, {"type": "text", "text": "Self-correction with internal feedback. In this line of research, the LLM is required to correct response trajectories based solely on its inherent capabilities. Huang et al. [28] first demonstrates the self-improvement potential of LLMs by utilizing a pre-trained LLM to generate rationale-augmented answers for unlabeled questions using CoT and majority voting and then fine-tuning the LLM using those self-generated labels, eventually improving the general reasoning ability. ", "page_idx": 20}, {"type": "text", "text": "When addressing problems, people typically engage in trial and error, coupled with reflective thinking, to discern the correct solutions. Inspired by this, Madaan et al. [52] proposes Self-refine, a simple and direct approach to improving LLM\u2019s output. In this approach, an LLM is used to create an initial output. Then, the model provides feedback on its own output in multiple dimensions. Based on this feedback, the model refines its initial output and repeats this process until it reaches a specified limit or the LLM determines that no further adjustments are necessary. ", "page_idx": 20}, {"type": "text", "text": "Encouraged by the augmented efficacy achieved through self-feedback mechanisms, Ye et al. [93] releases SelFee, a new instruction-following language model that generates self-feedback on its response and self-revises based on the feedback. The development of SelFee involves the fine-tuning of LLaMA by utilizing training instances generated by ChatGPT. ", "page_idx": 20}, {"type": "text", "text": "In addition, Gero et al. [21] introduces Self-verification, suggesting that by asking LLMs to provide provenance for their own outputs and conducting checks, it is possible to alleviate LLMs\u2019 issues regarding accuracy and interpretability in crucial domains such as healthcare. On the other hand, Xi et al. [89] focuses on the simplicity and comprehensibility of the given questions, proposing Selfpolish (SP). This method instructs the LLM to iteratively refine the test question by removing irrelevant information and rearranging the logical structure, thereby improving the reasoning performance. ", "page_idx": 20}, {"type": "text", "text": "While the self-correction methodologies based on internal feedback appear promising, [29] categorizes such self-correction methods as intrinsic self-correction (ISC) and demonstrates that the model\u2019s performance drops on all benchmarks after using ISC. This work points out that LLMs struggle to self-correct their responses without external feedback, and the corrected responses often exhibit inferior quality compared to their initial counterparts. Saparov and He [65] reveals that while models are able to produce valid reasoning steps with high probability when dealing with proof problems, they struggle with proof planning. In other words, when models occasionally generate incorrect proof steps, they are not able to return to the correct path. ", "page_idx": 20}, {"type": "text", "text": "Building upon this observation, Tyen et al. [81] further decomposes the self-correction process into two core components: mistake finding and output correction. This work demonstrates that current state-of-the-art LLMs cannot find mistakes reliably, even in the most simple and unambiguous cases, and suggests this is a main contributing factor to LLMs\u2019 inability to self-correct reasoning errors. ", "page_idx": 20}, {"type": "text", "text": "Self-correction with external feedback. External feedback offers a valuable external perspective, proving particularly advantageous in pinpointing errors that the large language model may not inherently recognize [57]. The sources of external feedback can be categorized as 1) human feedback [67, 69, 36, 81], 2) external tools [34, 96, 43], and 3) other models [59, 3]. ", "page_idx": 20}, {"type": "text", "text": "Scheurer et al. [67] proposes Imitation Learning from Language Feedback (ILF), an approach leveraging informative human feedback that involves conditioning the model on input, initial output, and feedback; selecting the most feedback-incorporated refinement; and fine-tuning the model to maximize the chosen refinement\u2019s likelihood given the input. ", "page_idx": 20}, {"type": "text", "text": "Similarly, Shinn et al. [69] introduces Reflexion, which fortifies language agents by relying on linguistic feedback generated by themselves rather than weight updates, resulting in noteworthy enhancements compared to a baseline agent across a spectrum of tasks. Kim et al. [36] demonstrates the effectiveness of self-critique on computer tasks by proposing Recursively Criticizes and Improves (RCI). This method utilizes a pre-trained LLM agent to execute computer tasks, guiding it using a simple prompting scheme where the agent recursively criticizes and improves its output. ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "Although Reflexion [69] and RCI [36] do not explicitly utilize human feedback, Huang et al. [29] points out that these approaches employ the labels of test questions to determine when to halt the self-correction loop. Consequently, they are implicitly guided by human feedback. Huang et al. [29] categorizes these methods as self-correction with oracle feedback (SCO). ", "page_idx": 21}, {"type": "text", "text": "Due to the difficulty of LLMs relying solely on their inherent capabilities to correct responses for reasoning tasks [29], Tyen et al. [81] decomposes the self-correction process and observes challenges in accurately identifying error locations for LLMs. Consequently, they propose Backtracking (BT), which guides the correction process of models by providing the initial error location in the problem. ", "page_idx": 21}, {"type": "text", "text": "Furthermore, the incorporation of external tools can enhance the model\u2019s self-correction performance. For instance, to improve the code generation capabilities of the model, Jiang et al. [34] utilizes error messages generated by an external interpreter as feedback, requiring LLM to debug the initially generated code acting as a professional programmer. Introducing external sources of knowledge, such as Wikipedia, can also significantly enhance the accuracy of the model in reasoning tasks [96, 43]. ", "page_idx": 21}, {"type": "text", "text": "In addition to leveraging external tools, certain efforts revolve around the utilization of external models to generate feedback. Paul et al. [59] introduces a critic model that generates structured feedback for the responses produced by the LLM during the self-correction process. The feedback is then incorporated into the input, allowing the LLM to refine its own output iteratively. Likewise, Aky\u00fcrek et al. [3] presents a collaborative framework involving multiple agents, where the critique generator is trained with the aim of maximizing the performance of the reasoning LLM. ", "page_idx": 21}, {"type": "text", "text": "B.3 Self-consistency ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In our study, self-consistency represents a category of robustness methods, typically involving the generation of multiple diverse answers to a given question and subsequently converging them. This approach serves to mitigate the impact of noise inherent in the question. Among existing methods, two primary approaches are employed to generate distinct answers: (1) generating multiple samples via randomized smoothing on input, followed by separate and independent reasoning processess [62, 102, 98], and (2) leveraging the inherent randomness in the decoding process of the LLM to sample multiple distinct reasoning paths for the same input [83]. ", "page_idx": 21}, {"type": "text", "text": "Sampling diverse outputs. Relying on the idea that an intricate reasoning problem usually allows for various distinct thinking ways, each leading to the unique correct solution, Wang et al. [83] first explores a varied array of reasoning paths and subsequently identifies the most consistent answer by majority vote. This approach boosts the model\u2019s performance across various reasoning tasks. ", "page_idx": 21}, {"type": "text", "text": "Randomized smoothing over inputs. Randomized smoothing serves as a straightforward yet effective mechanism commonly employed to defend against adversarial attacks by introducing multiple perturbations to the input to smooth potential noise [13, 98, 62, 102]. ", "page_idx": 21}, {"type": "text", "text": "To address the vulnerability of LLMs to jailbreaking attacks, Robey et al. [62] proposes Smooth LLM (SM), which is based on randomized smoothing. Utilizing the discovery that prompts generated adversarially exhibit fragility to character-level modifications, this method initiates by randomly introducing perturbations to multiple copies of a given input prompt. Subsequently, it consolidates the associated predictions to identify adversarial inputs. ", "page_idx": 21}, {"type": "text", "text": "However, as pointed out in [102], randomized smoothing relies on introducing additional noise to input, and its certification success hinges on the model\u2019s performance with corrupted data. To address the issue, Zhang et al. [102] proposes Self-denoising (SD), which denoises the corrupted inputs with LLMs using the multitasking nature of LLMs. ", "page_idx": 21}, {"type": "text", "text": "B.4 External Supervision ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In addition to self-correction and self-consistency methods, there is a line of methods that require external supervision or information. In our study, we compare diverse denoising methods that require varying degrees of external supervision, including (1) ground truth for test questions, (2) noise positions, and (3) additional clean demonstrations. ", "page_idx": 21}, {"type": "text", "text": "To provide a thorough comparison, We adopt Self-correction with Oracle feedback (SCO) [29] as a strong baseline, which uses the labels of test questions to determine when to terminate the self-correction loop, as discussed in B.2. We also employ Backtracking (BT) [81], which necessitates the location of the initial noise in the prompt, to demonstrate the effectiveness of our method. In addition, we utilize Contrastive Chain-of-Thought (CC) [9] in experiments which needs similar external supervision as our proposed method: a clean demonstration. This approach enhances reasoning by presenting the LLM with all the clean and noisy examples concurrently. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "B.5 Relation with Literature ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "There are some implicit connections with existing papers. Here, we would like to further clarify our work\u2019s major differences and unique contributions, compared with [92], [54] and [9]. ", "page_idx": 22}, {"type": "text", "text": "As discussed in B.1, Yao et al. [92] proposes Tree-of-Thought (ToT) for deliberate reasoning tasks. ToT decomposes a given problem into several reasoning steps and generates multiple answers for each step, ultimately constructing a tree structure. Subsequently, ToT employs BFS or DFS to traverse the tree, yielding the final rationale and answer. Namely, ToT focuses on the rationale (thoughts) generation in the output space of an LLM, wherein the generated thoughts can be irrelevant or inaccurate. By contrast, Our work focuses on the input space with noisy prompting examples. From this view of input space and output space, ToT is orthogonal to our work. ", "page_idx": 22}, {"type": "text", "text": "Moreover, ToT focuses on problem-solving by reasoning, e.g., Game of 24 and Mini Crosswords, while our work investigates the robustness problem of noisy rationales. ToT does not consider imperfect scenarios with noisy inputs. This means that when ToT is prompted by examples with noisy rationales, the reasoning might also be degenerated. Hence, a further investigation on reasoning robustness with these up-to-date prompting techniques, possibly with a focus on the output space, can be conducted based on our work. ", "page_idx": 22}, {"type": "text", "text": "As for [54], this work explores the question-answer mapping of prompting examples. They find that breaking the mapping has nearly no influence on LLM\u2019s in-context learning. ", "page_idx": 22}, {"type": "text", "text": "However, it\u2019s important to note that [54] primarily focuses on the relationship between input and label, investigating the question-answer mapping of prompting examples. Their work does not delve into the role of rationales in the examples, which is the central focus of our research. In contrast, our experiments keep the input-label mapping intact and instead explore the impact of introducing noise into the rationales. This allows us to isolate the effect of rationale quality on the model\u2019s performance without altering the fundamental relationship between questions and answers. ", "page_idx": 22}, {"type": "text", "text": "Hence, when the form of prompting example shifts from (question, answer) to (question, rationale, answer), more aspects can be investigated. We conduct additional experiments to further explore the mapping among question, rationale, and answer in Sec. 4. ", "page_idx": 22}, {"type": "text", "text": "Chia et al. [9] proposes Contrastive Chain-of-Thought (CC) [9] to enhance LLM reasoning. In our work, we focus on improving reasoning in noisy settings and define the noisy rationale problem as \u201cfactually inaccurate or irrelevant reasoning steps paired with valid question-answer prompts.\u201d Fig. 1 shows an example. Here, only one rationale is given in each demonstration, which can be potentially noisy but unknown to the model. However, in CC\u2019s setting, each ICL demonstration explicitly includes a clean rationale and a wrong rationale. Notably, this rationale is wrong instead of noisy as it induces the wrong answer. Therefore, CC\u2019s setting and ours are totally different. In addition, empirical results in Tab. 8 show that our method CD-CoT significantly outperforms CC when given the same information. ", "page_idx": 22}, {"type": "text", "text": "The problem of noisy rationale is challenging and non-trivial. We believe that the investigation of noisy rationale is still in the budding stage. Several potential solutions can be progressively devised to address this problem. For example, accessing external databases is a promising solution. However, this retrieve augmented generation (RAG) is usually used to address the lack of knowledge of LLMs but not for data denoising. Therein, verifying the factual relevance and accuracy of a rationale is quite challenging: (1) there is a semantic gap between a rationale and the corresponding query instructions of a database; (2) how can the models realize the need for data verification and then find a suitable database for that are also cutting-edge research directions? ", "page_idx": 22}, {"type": "text", "text": "Hence, we believe that RAG for denoising is not a trivial solution. More research attention and effort should be paid to achieve this solution. In this work, we verify that an LLM can achieve good denoising results when prompted with only one clean rationale and several noisy rationales. This solution, sharing a similar spirit to RAG that requires additional information, significantly mitigates the intrinsic vulnerabilities of LLMs. ", "page_idx": 22}, {"type": "text", "text": "C Benchmark ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In this section, we provide a comprehensive overview of the NoRa benchmark, including the motivation for constructing NoRa (Appendix C.1), a further discussion on the origins and impact of noisy rationales in CoT demonstrations (Appendix C.2), real-world examples of noisy rationales from both model and human perspectives (Appendix C.3), the detailed process of generating noise for the NoRa dataset (Appendix C.4), a comprehensive datasheet for the NoRa dataset (Appendix C.5), and full examples of the NoRa dataset across various reasoning tasks (Appendix C.6). ", "page_idx": 23}, {"type": "text", "text": "C.1 Motivation ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "The robustness of LLMs against noisy inputs deserves more attention, particularly in the context of Chain-of-Thought (CoT) reasoning. While CoT methods have shown remarkable effectiveness, they face two fundamental challenges: the susceptibility to selected demonstrations and the complexity of generating high-quality demonstrations. ", "page_idx": 23}, {"type": "text", "text": "Recent studies have revealed that LLMs can be easily distracted by irrelevant or adversarial context, and their reasoning process is unstable under small prompt modifications. Furthermore, misleading information can cause LLMs to agree with factual errors. These vulnerabilities become particularly concerning when we consider that demonstrations in practice are not always clean or accurate. ", "page_idx": 23}, {"type": "text", "text": "The problem of noisy rationales in demonstrations arises from various practical scenarios: ", "page_idx": 23}, {"type": "text", "text": "1. Human-generated noise: Just as humans make mistakes in data annotation (which has motivated extensive research in label-noise learning), they can also provide imperfect demonstrations. Different users with varying expertise and background knowledge may contribute demonstrations of varying quality.   \n2. System-generated noise: Noise can emerge from automated systems, including dialogue systems and machine-generated data, especially in crowdsourced platforms.   \n3. Demonstration selection challenges: While incorporating human supervision in dealing with noisy rationales is possible, it is often expensive and impractical at scale. The sensitivity of LLMs to in-context learning examples makes this particularly challenging. ", "page_idx": 23}, {"type": "text", "text": "Our empirical studies have revealed widespread vulnerability among prevailing LLMs to noisy rationales, with existing reasoning methods showing limited efficacy. Compared to clean rationales, we observe performance decreases of $0.4\u201319.8\\%$ with irrelevant noise and more severe degradation of $2.0\u201366.8\\%$ with inaccurate noise. ", "page_idx": 23}, {"type": "text", "text": "The NoRa benchmark addresses this crucial gap in current research. While most existing work assumes clean demonstrations for in-context learning, we argue that noisy rationales represent a practical and challenging problem that requires immediate attention. This benchmark serves two primary purposes: ", "page_idx": 23}, {"type": "text", "text": "1. It provides a systematic way to evaluate LLM robustness against different types of noise in rationales.   \n2. It helps identify the limitations of current approaches and motivates the development of more robust reasoning strategies. ", "page_idx": 23}, {"type": "text", "text": "We believe the NoRa dataset and our findings will contribute significantly to the development of more trustworthy foundation models capable of handling real-world, imperfect demonstrations. ", "page_idx": 23}, {"type": "text", "text": "C.2 A Further Discussion on Noisy Rationales in CoT Demonstrations ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In this part, we further clarify the background and settings of the Noisy-R problem investigated in our work, i.e., noisy rationales in CoT demonstrations. Having established the presence of irrelevant and inaccurate thoughts as primary noise types, we now turn to their origins and impact within human-model interactions. ", "page_idx": 23}, {"type": "text", "text": "Irrelevant and inaccurate thoughts can be generated by both the model and humans. On the one hand, irrelevant or inaccurate thoughts are likely to appear within the rationales of LLMs\u2019 answers. These rationales, drawn from historical dialogues between users and the model, can create a noisy context for new conversations. On the other hand, human demonstrators actively provide in-context demonstrations to enhance the reasoning capabilities of models. However, these demonstrations, whether crafted by humans or sourced from datasets, can also contain noisy rationales. ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "Irrelevant thoughts - model perspective: LLMs tend to explain the concept of terms during reasoning. For instance, if you ask GPT-4 to debug an error related to the \u2019concurrent\u2019 package, it tends to start by explaining what the \u2019concurrent\u2019 package is rather than immediately addressing the debug request. Such explanations, while informative, may introduce irrelevant noise into the conversation. Here are some key reasons for their emergence. ", "page_idx": 24}, {"type": "text", "text": ". Cognitive bias of models. When addressing complex queries, LLMs tend to include explanations for terms or concepts mentioned in the prompt that are unnecessary for solving the specific problem presented, as a kind of irrelevant thought. This behavior exhibits a form of cognitive bias where the model, unable to assess the inquirer\u2019s level of understanding, leads to explicating background information. This is comparable to a lecturer explaining the basics of a subject to a class without first assessing their students\u2019 existing knowledge, potentially leading to irrelevant elaborations. ", "page_idx": 24}, {"type": "text", "text": "2. Lack of precise contextual understanding. Despite LLMs\u2019 proficiency in processing language and recognizing patterns, they cannot always fully grasp the precise context or specific requirements of a problem. This shortfall can result in the production of thoughts that, although seemingly related, do not directly contribute to resolving the question at hand. Their responses might align more with the broader theme of the discussion rather than the specific, critical details needed for a precise solution. ", "page_idx": 24}, {"type": "text", "text": "3. User query ambiguity of the dialogue mechanism. Ambiguities in user queries can stem from the use of vague or multifaceted language, prompting the model to generate a wide array of responses. This situation is similar to a search engine returning a variety of results for a query that lacks specificity. The model, attempting to cover all potential meanings of the query, may produce responses that contain thoughts unrelated to the user\u2019s actual intent. For instance, if someone asks, \u201cWhat is force calculation?\u201d, the model might provide information on both Newtonian mechanics and quantum mechanics. These responses, encompassing a broad range of topics, could influence the answers to subsequent physics questions, leading to a continuation of the ambiguity and further complicating the conversation. ", "page_idx": 24}, {"type": "text", "text": "4. Progressive disclosure bias of the dialogue mechanism. When engaging with LLMs, individuals often initiate the dialogue by describing simpler concepts and progressively work their way toward articulating the complex question at hand because of human limitations in language or comprehension abilities, which prevent a person from presenting the entire complexity of an issue in one go. This step-by-step approach, while natural for humans who struggle to directly convey intricate problems, can introduce extraneous content that contributes to noise within the model\u2019s contextual reasoning. As the conversation builds, the LLM will factor in these initial, possibly tangential, explanations into its understanding of the context, potentially leading to a dilution of the focus necessary for solving the specific issue. This phenomenon reflects a human cognitive strategy in communication that may not be optimally aligned with the operational mechanisms of LLMs for efficient problem-solving. ", "page_idx": 24}, {"type": "text", "text": "Irrelevant thoughts - human perspective: When a human is actively demonstrating CoT reasoning, the introduction of irrelevant thoughts could be due to a variety of reasons. ", "page_idx": 24}, {"type": "text", "text": ". Cognitive overload of humans. Humans may introduce irrelevant information when they are trying to process too much information at once, which can lead to a loss of focus and the inclusion of tangential thoughts. For instance, a programmer is struggling with a bug in a complex piece of software and asks an LLM for help. To provide context, the programmer starts explaining the issue with a CoT rationale approach, intending to walk the LLM through their thought process. However, due to the complexity of the code and the stress of finding the bug, the programmer gets sidetracked. They include unnecessary details about the different error messages encountered in the past, unrelated functions in the code, and general thoughts on software development. ", "page_idx": 24}, {"type": "text", "text": "2. Associative thinking of humans. Humans naturally think in an associative manner, where one thought may lead to another that is only loosely related to the task at hand. This can result in straying from the main point during a CoT explanation. For example, while a programmer outlines the steps to diagnose a software issue for an LLM, they might recall a similar problem encountered in a different project. This memory could lead them to mention troubleshooting strategies, tools, or anecdotes from that experience, which, although related to the broader theme of problem-solving, do not directly contribute to the current issue. ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "3. Irrelevant content in datasets. In the future, it is likely that companies or professional organizations will increasingly utilize databases to assemble CoT prompts. However, these databases, whether privately maintained or publicly accessible, can contain irrelevant reasoning processes. This is especially true for databases sourced from crowdsourcing platforms or open forums, where the information is contributed by a diverse set of individuals with varying levels of expertise and focus. When these datasets are used to provide incontext information for CoT reasoning, the noise can originate from the inclusion of off-topic discussions, personal opinions, or overly verbose explanations that do not directly address the problem at hand. Such noise can be inadvertently introduced into the CoT process when humans provide explanations that contain unnecessary or tangential information. ", "page_idx": 25}, {"type": "text", "text": "Similarly, we analyze the two sources of inaccurate thoughts as follows. ", "page_idx": 25}, {"type": "text", "text": "Inaccurate thought - model perspective: For models, LLMs may produce erroneous thoughts during the reasoning process, especially when dealing with complex problems. For example, when tackling a base-9 math problem in a zero-shot setting, GPT-3.5 may generate some inaccurate reasoning steps. The former dialogue will become inaccurate and noisy in the context of subsequent dialogues. Here are some key reasons for their emergence. ", "page_idx": 25}, {"type": "text", "text": "1. Outdated or incomplete training data of the model. Language models are built upon datasets that may not be current or fully comprehensive. When faced with problems that require up-to-date knowledge or complete understanding, which are absent in their training data, models may rely on outdated or incomplete information, resulting in inaccurate outputs. For example, in the field of medicine, if new research suggests a change in treatment protocol after the model\u2019s last update, it wouldn\u2019t be able to advise on the new information. ", "page_idx": 25}, {"type": "text", "text": "2. Adaptation to novel reasoning contexts of the model. New challenges may require models to reason within contexts that slightly or significantly differ from their training data. For instance, a model extensively trained on base-10 arithmetic might struggle with a base-9 math problem because it requires a shift in the underlying numerical framework. This kind of scenario demands on-the-fly adaptation to a novel reasoning context, which can lead to generating thoughts that do not accurately apply the learned principles from the base-10 system to the newly introduced base-9 system. ", "page_idx": 25}, {"type": "text", "text": "3. Misinterpretation of complex subjects of dialogue mechanism. Users often fail to clearly articulate their complete requirements at the outset of an inquiry, leading to LLMs generating misunderstandings and inaccurate thoughts that do not align with user expectations. The process of correcting these thoughts is inherently a reasoning process laden with noisy contexts. As users provide feedback to refine the model\u2019s output, the iterative nature of this interaction can introduce additional inaccuracies as the model attempts to reconcile the new information with the previously misunderstood context. ", "page_idx": 25}, {"type": "text", "text": "Inaccurate thought - human perspective: Inaccurate thoughts in CoT can stem from the information provided by humans, whether it is self-made on the spot or sourced from a database for in-context learning by LLMs. These CoT demos can include inaccurate noise due to various factors. ", "page_idx": 25}, {"type": "text", "text": "1. Personal knowledge limitations of human. Individuals may possess incomplete or outdated knowledge on a given subject, leading to the provision of incorrect information when creating a CoT. For instance, a person without expertise in mathematics might attempt to construct a CoT for a complex math problem and inadvertently introduce incorrect steps or conclusions. Their understanding may be based on heuristics or educational background that hasn\u2019t been updated to reflect more recent methodologies or discoveries in the field. ", "page_idx": 25}, {"type": "text", "text": "2. Cognitive biases of human. Human reasoning can be influenced by a range of cognitive biases, such as confirmation bias, where an individual tends to search for, interpret, and remember information in a way that confirms their preconceptions, neglecting contrary information. Or the oversimplification of complex issues might lead to inaccurate reasoning steps within a CoT. These biases can skew the logic flow and result in conclusions that do not hold up under scrutiny or are based on flawed premises. ", "page_idx": 25}, {"type": "text", "text": "3. Data quality issues of database. The databases that humans rely on for creating CoTs might contain errors or biases introduced during data collection and processing. If this flawed data is used for in-context learning by LLMs, it can impart incorrect patterns of thought or factual inaccuracies. For example, a dataset with biased sampling methods might lead to generalizations that do not accurately represent the broader population or situation.   \n4. Contextual misplacement of databases. Information from databases may be stripped of its original context, leading to misinterpretation when reused. When humans include such decontextualized information in a CoT, they might not properly align it with the new context, introducing misunderstandings or inaccuracies. This is particularly problematic in nuanced fields where context heavily influences the meaning and applicability of information, such as legal precedents or cultural studies. ", "page_idx": 26}, {"type": "text", "text": "Given the convenience and adaptability of CoT reasoning, broader adoption in LLM applications is expected in the future. This structured approach enables LLMs to break down complex problems and explain their reasoning in a way that resembles how humans think, proving essential for sophisticated problem-solving. Nonetheless, we are bound to face the noisy reasoning challenges, stemming from both model-generated and human-contributed contexts, as mentioned above. ", "page_idx": 26}, {"type": "text", "text": "To address these challenges, we must focus on continuously improving training methods, keeping models updated with the latest information, enhancing their ability to parse context and ambiguity, and refining algorithms to diminish biases and logical inaccuracies. ", "page_idx": 26}, {"type": "text", "text": "Differences between inaccurate and irrelevant thoughts. Given the context and question, a thought is either relevant or irrelevant. Within the relevant thoughts, those accurate ones are desirable, and those inaccurate, termed as \"inaccurate thoughts\" in this work, are proven to be harmful to LLMs. Hence, the extreme case of \"inaccurate and irrelevant thought\" is not covered in this work. For clarity and simplicity, we investigate these two kinds of noise separately. Specifically, in definitions, ", "page_idx": 26}, {"type": "text", "text": "\u2022 Irrelevant thoughts refer to incorporating extraneous details that are unhelpful for solving the question. Redundant information may be introduced by the LLM\u2019s diverse response generation or by humans when clarifying concepts in problem-solving examples; \u2022 Inaccurate thoughts refer to factual errors in rationales that are common in mathematical calculation or transcription. The emergence of noise can be due to algorithmic limitations, errors in training data, misinterpretations of context or instructions, and logical fallacies. ", "page_idx": 26}, {"type": "text", "text": "Note that the \"accuracy\" and \"relevance\" of thoughts are related to the context of the given question. Basically, the question provides the context, and an LLM generates thought conditional on the context. Taking the examples of the Base-9 dataset in Tab. 1, ", "page_idx": 26}, {"type": "text", "text": "\u2022 Irrelevant thought is \"There are five oceans on Earth: the Atlantic, Pacific, Indian, Arctic, and Southern.\", which is accurate but not relevant and not useful;   \n\u2022 Inaccurate thought here is $\"5+9=14\"$ , which is relevant to the question and previous thought but is inaccurate for the base-9 calculation. ", "page_idx": 26}, {"type": "text", "text": "In the NoRa dataset, only the relevant thought with factual error will be classified as an \"inaccurate thought\". There is no irrelevant and inaccurate thought in NoRa, which should be rare in practice. ", "page_idx": 26}, {"type": "text", "text": "We focus on the two major types of noise in this work. And empirically, inaccurate thoughts bring severe degradations. Compared with clean rationales, a $1.4\\%{-}1\\bar{9}.8\\%$ decrease with irrelevant noise and a more drastic $2.2\\%{-}40.4\\%$ decrease with inaccurate noise. ", "page_idx": 26}, {"type": "text", "text": "C.3 Real-world Examples ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "The emphasis on noisy rationale is due to its practical challenges, with examples drawn from diverse sources such as crowdsourced platforms, dialogue systems, and AI-generated data. Here, the Noisy-R mainly originates from (1) the inherent imperfections, inconsistencies, and inaccuracy of humans\u2019 cognitive processes and (2) the diversity, unpredictability, and hallucination of the LLMs\u2019 generative mechanisms, as discussed in more detail in Appendix C.2. ", "page_idx": 26}, {"type": "text", "text": "Briefly, irrelevant and inaccurate thoughts can be generated by both the model and humans. From model perspective, the generated rationales by the model can be noisy. From human perspective, the annotated rationales by humans can be noisy. ", "page_idx": 26}, {"type": "text", "text": "In the following four parts, we elaborate on the evidence (including sources and examples) of irrelevant and inaccurate thoughts from both the model perspective and the human perspective. ", "page_idx": 27}, {"type": "text", "text": "C.3.1 Irrelevant Thoughts from Model Perspective ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Cognitive bias of models: ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Source. The presence of cognitive biases in LLMs is demonstrated, and the implications of using biased reasoning under the guise of expertise are discussed, calling for stronger education, risk management, and continued research as the widespread adoption of this technology increases [38]. ", "page_idx": 27}, {"type": "text", "text": "Thorstad [77] examines the inherent cognitive biases within large language models, suggesting that while there is reason for cautious optimism regarding the potential to mitigate these biases, a realistic, anti-Panglossian approach is necessary to drive meaningful improvements. It advocates for a balanced perspective that neither idealizes the perfectibility of AI nor dismisses the progress made in addressing its limitations. ", "page_idx": 27}, {"type": "text", "text": "\u2022 Examples. Tab. 13 demonstrates examples of cognitive bias of models. ", "page_idx": 27}, {"type": "table", "img_path": "FbuODM02ra/tmp/6f3baa145ac2cf9b158f4e012194e737695a4e1e767ec9f9ab8fc12d7a21264e.jpg", "table_caption": ["Table 13: Cognitive bias of models of the model examples. "], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "Lack of precise contextual understanding: ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "\u2022 Source. Zhu et al. [114] introduces a context understanding benchmark by adapting existing datasets to suit the evaluation of generative models\u2019 ability to understand the context and finds that pre-trained dense models struggle with understanding more nuanced contextual features when compared to state-of-the-art fine-tuned models. He et al. [27] designs eight features for complex instructions and construct a comprehensive evaluation dataset from real-world scenarios, and compares the performance of representative Chinese-oriented and English-oriented models in following complex instructions through extensive experiments. ", "page_idx": 27}, {"type": "text", "text": "\u2022 Examples. Tab. 14 demonstrates the lack of precise contextual understanding in LLMs. ", "page_idx": 27}, {"type": "text", "text": "C.3.2 Irrelevant Thoughts from Human Perspective ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Associative thinking of humans: ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "\u2022 Source. Morewedge and Kahneman [55] investigates the underlying cognitive mechanisms of intuition, focusing on how unconscious associative processes can lead to rapid and efficient problem-solving and decision-making. It aims to elucidate the often-overlooked ", "page_idx": 27}, {"type": "text", "text": "Question: He was trying to procreate with many individuals, this led to a what? Answer Choices: (A) moaning (B) die $C$ kiss (D) std (E) sanity ", "page_idx": 28}, {"type": "text", "text": "Rationale: Let\u2019s think step by step. He was trying to procreate with many individuals. This would lead to many offspring, which would be a good thing, right? But it also led to a lot of competition among the offspring for resources. This would lead to a lot of fighting and eventually to the death of many individuals. So while he may have been trying to create more life, he also caused a lot of death. ", "page_idx": 28}, {"type": "text", "text": "Answer: Therefore, among A through E, the answer is D. ", "page_idx": 28}, {"type": "text", "text": "Explanation: The reasoning is mainly about the fact that many people will have a lot of offspring, and the offspring will bring competition for resources and wars, which will cause a lot of deaths, but the final result of the reasoning is that it is indeed a sexually transmitted disease, and there is no information related to the sexually transmitted disease in the reasoning. So it is irrelevant thought. ", "page_idx": 28}, {"type": "text", "text": "Example 2 ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Can Hulk\u2019s alter ego explain atomic events? ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Rationale: Let\u2019s think step by step. First, we need to understand what an atom is. Atoms are the basic units of matter and the defining structure of elements. The term \"atom\" comes from the Greek word for indivisible, because it was once thought that atoms were the smallest things in the universe and could not be divided. The structure of an atom is a central nucleus composed of protons and neutrons with electrons orbiting around this nucleus. Now that we know what an atom is, we can move on to understanding what an atomic event is. An atomic event is a process that involves. Answer: Yes, Hulk\u2019s alter ego can explain atomic events. ", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "Explanation: The model\u2019s rationale focuses on defining atoms and atomic events but fails to connect this information to the context of the question, which is about the ability of Hulk\u2019s alter ego, Bruce Banner, to explain these concepts. The model does not demonstrate an understanding of the alter ego\u2019s scientific expertise and instead provides a general explanation of atomic structure. This lack of precise contextual understanding leads to an unsupported conclusion. ", "page_idx": 28}, {"type": "text", "text": "Table 14: Examples demonstrating the lack of precise contextual understanding in LLMs. ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "influence of automatic, associative thinking on our intuitive judgments, challenging the traditional emphasis on deliberate and logical reasoning in cognitive psychology. ", "page_idx": 28}, {"type": "text", "text": "Wimmer and Shohamy [87] presents evidence that the hippocampus is involved in decisionmaking by enabling the transfer of value between memories, affecting choices among novel alternatives. Using functional brain imaging, the study shows that monetary rewards activate a network of memories, influencing individuals to later choose associated, but unrewarded, items, with this bias being predicted by hippocampal activity and its connections with memory and reward brain regions. ", "page_idx": 28}, {"type": "text", "text": "\u2022 Examples. Please refer to Tab. 15 for examples. ", "page_idx": 28}, {"type": "text", "text": "Example 1 ", "text_level": 1, "page_idx": 28}, {"type": "table", "img_path": "FbuODM02ra/tmp/79a216f4f5bddac590885674c8996c0e3add484a1858e2244f67f3cfb028d127.jpg", "table_caption": [], "table_footnote": [], "page_idx": 28}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "Explanation: When analyzing the relationship, an anecdote that is loosely related to the task is considered. Although the anecdote is about the relationship between John and Mary, it does not help in reasoning. ", "page_idx": 28}, {"type": "text", "text": "Example 2 ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Calculate the length of the hypotenuse in a right-angled triangle with one leg of length 3 and the other leg of length 4. ", "page_idx": 28}, {"type": "text", "text": "Rationale: ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "1. Using Pythagoras\u2019 Theorem: In a right-angled triangle, the sum of the squares of the lengths of the legs is equal to the square of the length of the hypotenuse, i.e., $\\bar{a^{2}}+b^{2}=c^{2}$ . Pythagoras, an ancient Greek mathematician, is credited with this discovery, but his contributions to mathematics extend beyond this, including music, philosophy, and astronomy. 2. Substituting Known Values to Solve: In our problem, the lengths of the legs are 3 and 4, thus $3^{2}+4^{2}=c^{2}$ , which is $9+16=c^{2}$ . This leads to $c^{2}=25$ .   \n3. Solving the Equation to Find the Length of the Hypotenuse: Solving $c^{2}=25$ yields $c=5$ . This solution not only reveals the geometric property of the triangle but also hints at the special status of the 3-4-5 triplet as one of the simplest Pythagorean triples. Pythagorean triples are sets of three positive integers that satisfy Pythagoras\u2019 theorem, with 3-4-5 being among the most well-known sets, often used in construction and engineering to create right angles.   \nAnswer: The length of the hypotenuse is 5. ", "page_idx": 28}, {"type": "text", "text": "Explanation: Introduction of Pythagoras and the special status of the 3-4-5 triplet do not help in calculation of the length of the hypotenuse, which is unnecessary detail about the problem. ", "page_idx": 28}, {"type": "text", "text": "Irrelevant content in datasets: ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "\u2022 Source. Havrilla and Iyer [25] examines how different types and intensities of noise in training data affect the performance of LLMs using a novel framework called Traced Integer (TInt) for generating customizable noised execution traces. The study reveals that while fine-tuned models display high robustness to static noise, they are more susceptible to dynamic noise, and it offers insights into the importance of filtering out training samples with dynamic noise to improve model performance. ", "page_idx": 29}, {"type": "text", "text": "\u2022 Examples. Please refer to Tab. 16 for examples. ", "page_idx": 29}, {"type": "text", "text": "Example 1 ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "1. Using the Area Formula: The area of a circle can be calculated using the formula $A=\\pi r^{2}$ , where $A$ is the area and $r$ is the radius of the circle. $\\pi$ (pi) is a mathematical constant approximately equal to 3.14159, representing the ratio of a circle\u2019s circumference to its diameter. $\\pi$ is not only widely used in mathematics but also serves as a fundamental element in physics, engineering, and other scientific fields.   \n2. Substituting the Given Value: Substituting the radius $r=7$ , we get $A=\\pi\\times7^{2}=49\\pi$ . This result not only provides the area of the circle but also reminds us of the irrational nature of $\\pi-\\mathrm{an}$ infinite, non-repeating decimal, representing a form of endless beauty in mathematics.   \n3. Practical Application of the Area: In practice, calculating the area of a circle has many applications, such as designing ", "page_idx": 29}, {"type": "text", "text": "fountains in parks, making round tables, or calculating the surface area of planets. This simple formula underlies limitless exploration and potential applications. ", "page_idx": 29}, {"type": "text", "text": "Explanation: Application about $\\pi$ does not help in calculation of area of a circle, which is unnecessary detail about the problem. ", "page_idx": 29}, {"type": "table", "img_path": "FbuODM02ra/tmp/db91f964ca9725e8471b3fa87ab3dae955bf6f3071f29560053b32ad6ea60b45.jpg", "table_caption": [], "table_footnote": [], "page_idx": 29}, {"type": "text", "text": "Question: Calculate the length of the hypotenuse in a right-angled triangle with one leg of length 3 and the other leg of length 4. ", "page_idx": 29}, {"type": "text", "text": "Rationale: ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "1. Using Pythagoras\u2019 Theorem: In a right-angled triangle, the sum of the squares of the lengths of the legs is equal to the square of the length of the hypotenuse, i.e., $\\bar{a^{2}}+b^{2}=c^{2}$ . Pythagoras, an ancient Greek mathematician, is credited with this discovery, but his contributions to mathematics extend beyond this, including music, philosophy, and astronomy. 2. Substituting Known Values to Solve: In our problem, the lengths of the legs are 3 and 4, thus $3^{2}+4^{2}=c^{2}$ , which is $9+16=c^{2}$ . This leads to $c^{2}=25$ . ", "page_idx": 29}, {"type": "text", "text": "3. Solving the Equation to Find the Length of the Hypotenuse: Solving $c^{2}=25$ yields $c=5$ . This solution not only reveals the geometric property of the triangle but also hints at the special status of the 3-4-5 triplet as one of the simplest Pythagorean triples. Pythagorean triples are sets of three positive integers that satisfy Pythagoras\u2019 theorem, with 3-4-5 being among the most well-known sets, often used in construction and engineering to create right angles. Answer: The length of the hypotenuse is 5. ", "page_idx": 29}, {"type": "text", "text": "Explanation: Introduction of Pythagoras and the special status of the 3-4-5 triplet do not help in calculation of the length of the hypotenuse, which is unnecessary detail about the problem. ", "page_idx": 29}, {"type": "text", "text": "Table 16: Examples of irrelevant content in datasets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "C.3.3 Inaccurate Thought from Human Perspective ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Personal knowledge limitations of human: ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "\u2022 Source. The array of problems addressed by the human mind constitutes merely a specific subset within the spectrum of challenges tackled by AI systems [23], resulting from human time limitation and human computation limitation, etc. ", "page_idx": 29}, {"type": "text", "text": "Due to the limited lifespan, whatever a human learns must be acquired in less than a million hours of real-time experience. The AlphaGo system that beat the best human players in history has the benefit of multiple human lifetimes of simulated play [70]. ", "page_idx": 29}, {"type": "text", "text": "There exists a disparity between human intelligence and machine intelligence in addressing the Rubik\u2019s Cube conundrum. Machine intelligence has successfully determined the shortest path from any initial configuration to an unscrambled state, requiring no more than 20 moves [63]. Conversely, the most adept human solvers necessitate approximately 50 moves to achieve the same outcome. ", "page_idx": 29}, {"type": "text", "text": "\u2022 Examples. Please refer to Tab. 17 for examples. ", "page_idx": 29}, {"type": "text", "text": "Cognitive biases of human: ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "\u2022 Source. Tversky and Kahneman [80] define human cognitive biases: systematic patterns of deviation from rational judgment. Here we list three types of cognitive biases of human. ", "page_idx": 29}, {"type": "table", "img_path": "FbuODM02ra/tmp/dd38715a31ff3f56ad36cb110e680e60131cc10407bcc650368608f3731de5d9.jpg", "table_caption": [], "table_footnote": [], "page_idx": 30}, {"type": "text", "text": "Table 17: Examples of human knowledge limitation. ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Confirmation bias is a common cognitive bias, referring to the tendency of people to seek, remember, or emphasize information that supports their existing views or beliefs, while ignoring or disregarding information that contradicts their viewpoints [14]. ", "page_idx": 30}, {"type": "text", "text": "When members within a group suppress their individual capacity for independent thinking in order to maintain team harmony or consensus, groupthink occurs [31]. Emotional reasoning happens when people judge the authenticity or value of things based on their own emotional state [56]. ", "page_idx": 30}, {"type": "text", "text": "\u2022 Examples. Please refer to Tab. 18 for examples. ", "page_idx": 30}, {"type": "table", "img_path": "FbuODM02ra/tmp/366c3e878a5e1327c45180f3ff24f6aa0c4028cea23132137b536867996a2a48.jpg", "table_caption": ["Table 18: Examples of cognitive bias. "], "table_footnote": [], "page_idx": 30}, {"type": "text", "text": "C.4 Noise Generation ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "As detailed in Sec. 3, we have introduced both irrelevant and inaccurate noises into our dataset as insertions. These insertional noises are carefully integrated into the rationales, ensuring they neither modify the existing reasoning pathways nor affect the final answers. Furthermore, they are purposefully crafted based on the intermediate steps of the reasoning sequence, which guarantees that each piece of noise is contextually related to the stage of reasoning it accompanies. To maintain consistency, we treat each sentence in the reasoning sequence as a single thought; accordingly, each noise we introduce is also fashioned as only one sentence. For irrelevant noise, we draw from unrelated scientific or social facts. Alternatively, inaccurate noise involves the extension of an intermediate thought, which is redundant and incorrect. ", "page_idx": 30}, {"type": "text", "text": "NoRa-Math. In NoRa-Math problems, an intermediate reasoning result typically manifests as a numerical value. For instance, from the equation \"We have $4+2=6$ in base-10. \", we take the derived number 6 and craft a sentence of noise to follow this particular reasoning step. Take the numeral $\"6\"$ ; ", "page_idx": 30}, {"type": "text", "text": "we might introduce an unrelated fact such as \"According to the Standard Model of physics, there are six types of quarks, the fundamental constituents of matter.\" For inaccurate thoughts, we generate a decimal addition related to 6, such as $\"6+5=11$ . \", which is inaccurate in base-9 representation. ", "page_idx": 31}, {"type": "text", "text": "NoRa-Symbolic. NoRa-Symbolic problems are handled similarly; here, intermediate results are individual elements of a navigation instruction. We insert a sentence of noise that aligns with these specific components. For example, if the element is the directive \"right,\" we fabricate a directionrelated noise. One irrelevant thought can be \"Turning right in countries that drive on the right side of the road typically does not intersect with oncoming traffic.\". For inaccurate thoughts, to ensure that the noise does not contradict the previous reasoning, we select another related instruction on the same term domain. For example, when explaining the instruction \"right,\" we might generate noise related to the instruction \"left\", such as \"left means I_TURN_RIGHT.\", which is incorrect and does not contradict the previous thought, and this thought itself is redundant. ", "page_idx": 31}, {"type": "text", "text": "NoRa-Commonsense. When it comes to NoRa-Commonsense problems, we introduce noise that mirrors the relationships discerned during the reasoning process. For example, after establishing that \"mother\u2019s sister is aunt,\" we craft a noise sentence associated with the concept of \"aunt.\" An irrelevant noise insertion could be: \"Aunts often play pivotal roles in the social development of primates, akin to their influence in human societies.\" To generate inaccurate thoughts, we might produce a statement like \"an aunt\u2019s mother is also an aunt,\" which is logically inaccurate. ", "page_idx": 31}, {"type": "text", "text": "In addition to inserting a fixed number of noisy thoughts per chain of thought shot, as detailed in the main text, we further introduce variability in noise addition by randomly inserting noise following each thought. This randomness follows a Bernoulli distribution $\\mathtt{B e r n}(\\bar{\\epsilon})\\in\\{0,1\\}$ , where noise is added after a thought only if the Bernoulli trial results in 1. Consequently, while the fixed number approach guarantees a set amount of noise within a CoT shot, the random addition allows for the possibility of varying amounts of noise in each shot, dictated by the probability parameter $\\epsilon$ . ", "page_idx": 31}, {"type": "text", "text": "C.5 NoRa Datasheet ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Following the framework of Gebru et al. [20], we provide a comprehensive datasheet for the NoRa dataset in Tab. 19. This datasheet documents the motivation, composition, collection process, preprocessing details, intended uses, distribution plan, and maintenance strategy of our dataset. ", "page_idx": 31}, {"type": "table", "img_path": "FbuODM02ra/tmp/a42052e83df50483291b1be0b56838a29510e1cbb395b74f4e9888dee0dbece7.jpg", "table_caption": [], "table_footnote": [], "page_idx": 31}, {"type": "table", "img_path": "FbuODM02ra/tmp/0263dbd0099e96c068b471bfa189de4757d8e8d7802a6f2cb8367776f7422126.jpg", "table_caption": [], "table_footnote": [], "page_idx": 32}, {"type": "table", "img_path": "FbuODM02ra/tmp/67f529059bcec7826c0bf9e8ae54abe496af3bb44740dcc71c0014941046b9be.jpg", "table_caption": ["Do any export controls or other regulatory re- Not to our knowledge. strictions apply to the dataset or to individual instances? "], "table_footnote": ["Table 19: Datasheet for the NoRa dataset, following the framework introduced by Gebru et al. [20]. "], "page_idx": 33}, {"type": "text", "text": "C.6 Full Examples of the NoRa Dataset ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "In this part, we show the full examples of the NoRa dataset. ", "page_idx": 33}, {"type": "text", "text": "NoRa-Math: \u2022 Base-9 Irrelevant (Tab. 20) \u2022 Base-9 Inaccurate (Tab. 21) \u2022 Base-11 Irrelevant (Tab. 22) \u2022 Base-11 Inaccurate (Tab. 23) ", "page_idx": 33}, {"type": "text", "text": "NoRa-Symbolic: \u2022 Equal Irrelevant (Tab. 24) \u2022 Equal Inaccurate (Tab. 25) \u2022 Longer Irrelevant (Tab. 26) \u2022 Longer Inaccurate (Tab. 27) ", "page_idx": 33}, {"type": "text", "text": "NoRa-Commonsense: \u2022 Irrelevant (Tab. 28) \u2022 Inaccurate (Tab. 29) ", "page_idx": 33}, {"type": "image", "img_path": "FbuODM02ra/tmp/e885a252a5a90cac10edd6f19855b25820e80b83da81281f7e051c543f51fa81.jpg", "img_caption": [], "img_footnote": [], "page_idx": 34}, {"type": "text", "text": "Table 22: Example of NoRa-Math Base-11 with irrelevant rationales ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Example: ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "User: In base-11, what is $97{+}74?$ Please reason it step by step. End the response with the result in \"Answer:\\boxed{result $\\}^{\\ast}$ . Assistant: In base-11, the digits are \"0123456789A\". We have $7+4=11$ in base-10. Since we\u2019re in base-11, that exceeds the maximum value of A for a single digit. $10+5=15$ . 11 mod $11=0$ , so the digit is 0 and the carry is 1. We have $9+7+1$ $=17$ in base 10. 17 mod $11={\\bar{6}}$ , so the digit is 6 and the carry is 1. A leading digit is 1. So the answer is 160. $\\underline{{0+11=11.}}$ Answer:\\box{160}. ", "page_idx": 34}, {"type": "text", "text": "Table 23: Example of NoRa-Math Base-11 with inaccurate rationales ", "page_idx": 34}, {"type": "text", "text": "Example: ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "User: With IN:walk right twice and look right twice, what is OUT?   \nPlease reason it step by step, and provide the final action sequence as the answer. End the response with \"So, final answer is OUT: <action sequence>\"   \nAssistant: Since command is walk right twice and look right twice, we should consider Step1: \"walk right twice\" firstly, \"walk right\" means the agent needs to turn right and walk. The \u2019right\u2019 corresponds to the command I_TURN_RIGHT. Turning right in countries that drive on the right side of the road typically does not intersect with oncoming traffic. Subsequently, \u2019walk\u2019 translates to I_WALK. Therefore, the action sequence is I_TURN_RIGHT I_WALK. Since we need do twice in command \"walk right twice\", this entire sequence is repeated 2 times, so the action sequence to \"walk right twice\" is :I_TURN_RIGHT I_WALK I_TURN_RIGHT I_WALK.   \nNow, we consider Step2:\"look right twice\", \"look right\" means the agent needs to turn right and look. Motorists in many areas are allowed to turn right at a red traffic signal after coming to a complete stop and ensuring the road is clear. The \u2019right\u2019 corresponds to the command I_TURN_RIGHT. Certain traffic circles or roundabouts require all entering traffic to turn right regardless of the direction they intend to take. Subsequently, \u2019look\u2019 translates to I_LOOK. Therefore, the action sequence is I_TURN_RIGHT I_LOOK. Since we need do twice in command \"look right twice\", this entire sequence is repeated 2 times, so the action sequence to \"look right twice\" is :I_TURN_RIGHT I_LOOK I_TURN_RIGHT I_LOOK.   \nAbove all \u2013 So, final answer is OUT:I_TURN_RIGHT I_WALK I_TURN_RIGHT I_WALK I_TURN_RIGHT I_LOOK I_TURN_RIGHT I_LOOK. ", "page_idx": 34}, {"type": "text", "text": "Table 24: Example of NoRa-Symbolic Equal with irrelevant rationales ", "page_idx": 34}, {"type": "text", "text": "Example: ", "text_level": 1, "page_idx": 34}, {"type": "image", "img_path": "FbuODM02ra/tmp/3f92f96ddd403b9c639de93dcb4cf8e9aa15625927c12190b28dd4b40798364b.jpg", "img_caption": [], "img_footnote": [], "page_idx": 34}, {"type": "text", "text": "Table 25: Example of NoRa-Symbolic Equal with inaccurate rationales ", "page_idx": 34}, {"type": "text", "text": "Example: ", "text_level": 1, "page_idx": 35}, {"type": "image", "img_path": "FbuODM02ra/tmp/f257985ddfbbfb7271311ce6f4f47b46b1e89b321f9e0b9a2bbcd11de9e71570.jpg", "img_caption": [], "img_footnote": [], "page_idx": 35}, {"type": "text", "text": "Example: ", "text_level": 1, "page_idx": 35}, {"type": "image", "img_path": "FbuODM02ra/tmp/b4934058cf5c691923278e108709bbc2a8d2195fd63928aafdcd627250ebdb94.jpg", "img_caption": [], "img_footnote": [], "page_idx": 35}, {"type": "text", "text": "Example: ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "User: In a family tree, if Chris is Frances\u2019s husband\u2019s son\u2019s son. Question: Chris is Frances\u2019s what? Please reason it step by step, and provide a single word answer describing the relationship. End the response in the format \"Answer: relation \" Assistant: Chris is Frances\u2019s husband\u2019s son\u2019s son, so the relations path is husband, son, son. For son\u2019s son, we have son\u2019s son is grandson. The parent-son relationship plays a crucial role in a son\u2019s emotional and social development. So the relations path are reduced to husband, grandson. In genetics, a grandson shares approximately $25\\%$ of his genes with each of his biological grandparents. For husband\u2019s grandson, we have husband\u2019s grandson is grandson. So the relations path are reduced to grandson. Therefore, Answer: grandson. ", "page_idx": 35}, {"type": "text", "text": "Table 28: Example of NoRa-Commonsense with irrelevant rationales ", "page_idx": 35}, {"type": "text", "text": "Example: ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "User: In a family tree, if Chris is Frances\u2019s husband\u2019s son\u2019s son. Question: Chris is Frances\u2019s what? Please reason it step by step, and provide a single word answer describing the relationship. End the response in the format \"Answer: relation \" Assistant: Chris is Frances\u2019s husband\u2019s son\u2019s son, so the relations path is husband, son, son. For son\u2019s son, we have son\u2019s son is grandson. So the relations path are reduced to husband, grandson. We have grandson\u2019s daughter is brother. For husband\u2019s grandson, we have husband\u2019s grandson is grandson. So the relations path are reduced to grandson. We have grandson\u2019s mother is grandfather Therefore, Answer: grandson. ", "page_idx": 35}, {"type": "text", "text": "Table 29: Example of NoRa-Commonsense with inaccurate rationales ", "page_idx": 35}, {"type": "text", "text": "D Theoretical Analysis ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "In this part, we provide the theoretical results of ICL with the noisy demonstrations based on the distinguishability [90], which acts as a crucial condition in understanding ICL as the implicit Bayesian inference. We reveal that the noisy demonstrations can enlarge the 0-1 risk of the in-context predictor through the mismatch between the noisy prompt distribution and the clean pre-trained distributions. ", "page_idx": 36}, {"type": "text", "text": "First, we briefly review some definitions and preliminary setups, as well as the distinguishability condition established in [90], which is used to formalize when in-context learning occurs. ", "page_idx": 36}, {"type": "text", "text": "Clean and noisy demonstrations. As stated previously, the ICL techniques prompt the LLM with a few supporting examples ${\\cal{S}}_{n}\\!=\\!\\{(x_{i},y_{i})\\}_{i=1}^{n}$ collected in the current context, each composed of a question $x_{i}$ and answer $y_{i}$ , and then constructing the new input of $x_{\\mathrm{{ICL}}}$ as ", "page_idx": 36}, {"type": "equation", "text": "$$\nx_{\\mathrm{ICL}}=\\left[S_{n},x_{\\mathrm{test}}\\right]=\\left[x_{1},y_{1},\\ldots,x_{n},y_{n},\\ x_{\\mathrm{test}}\\right].\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "With $S_{n}$ , the $f_{\\theta}(x_{\\mathrm{ICL}})\\mapsto y_{\\mathrm{test}}$ is easier than $f_{\\theta}(\\boldsymbol{x}_{\\mathrm{test}})\\mapsto y_{\\mathrm{test}}$ . Then, the CoT further refines $x_{\\mathrm{ICL}}$ by constructing the step-by-step rationale $\\tau_{i}$ , consisting several thoughts $T_{i}^{(j)}$ , i.e., ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r}{x_{\\mathrm{CoT}}=[x_{1},7_{1},y_{1},\\dots,x_{n},7_{n},y_{n},\\ x_{\\mathrm{tesl}}]\\,,\\quad\\mathrm{where}\\ \\ T_{i}=\\left[T_{i}^{(1)},T_{i}^{(2)},T_{i}^{(3)},\\dots,T_{i}^{(k)}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "However, as mentioned, the thoughts in CoT (Eqn. 2) can be noisy in practice. This noise can be attributed to (1) irrelevant thoughts, which are irrelevant but correct, or (2) inaccurate thoughts, which are relevant but factually wrong. Here, we uniformly formalize these two kinds of noise as ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{\\mathcal{T}}_{i}=\\left[T_{i}^{(1)},\\hat{T}_{i}^{(1)},T_{i}^{(2)},\\hat{T}_{i}^{(2)},\\dots,T_{i}^{(k)},\\hat{T}_{i}^{(k)}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where $\\hat{T}_{i}^{(j)}$ represents a noisy thought (irrelevant or inaccurate) that is coherent with previous clean thought $T_{i}^{(j)}$ (relevant and correct) in Eqn. 4, and it make the $x_{\\mathrm{{CoT}}}$ be noisy. ", "page_idx": 36}, {"type": "text", "text": "In-context learning setups. Keeping the same setups in [90], we analyze the latent concept $\\theta$ from a family of concepts $\\Theta$ that defines a distribution over the observed tokens $o$ from a vocabulary $\\scriptscriptstyle\\mathcal{O}$ . Following [90], we assume that the pertaining document is a length $T$ sequence can be generated with the prior $p(\\theta)$ as $\\begin{array}{r}{p(o_{1},\\cdots,o_{T})=\\int_{\\theta\\in\\Theta}p(\\bar{o}_{1},\\cdots,o_{T}|\\theta)p(\\theta)\\bar{d}\\theta}\\end{array}$ , where $p(o_{1},\\cdot\\cdot\\cdot\\,,o_{T}|\\theta)$ is defined by the hidden markov model. We have the prompt distribution $p_{p r o m p t}$ that generates the prompts for in-context learning, in which all the demonstrations share the prompt concept $\\theta^{*}$ . ", "page_idx": 36}, {"type": "text", "text": "We build the derivation by analyzing the in-context predictor $f_{n}(x_{\\mathrm{test}})\\,=\\,\\arg\\operatorname*{max}_{y}\\,p(y|S_{n},x_{\\mathrm{test}})$ , which indicates the most likely prediction over the pertaining distribution conditioned on the prompt from the prompt distribution, assuming that greedy decoding is adopted in practice. Specifically, we study the expected 0-1 error with $n$ examples $L_{0-1}\\dot{(f_{n})}=\\mathbb{E}\\[\\mathbb{1}[f_{n}\\bar{(x_{\\mathrm{CoT}}\\neq\\bar{y}_{\\mathrm{test}})}]]$ . ", "page_idx": 36}, {"type": "text", "text": "Given the output distribution $p_{\\theta}^{i}(o):=p(\\mathcal{O}[i]=o|\\mathcal{O}[1:i-1],\\theta)$ of the $i$ -th token with previous tokens and the analogous distribution $p_{p r o m p t}^{i}:=p_{p r o m p t}(\\mathcal{O}[i]=o|\\mathcal{O}[1:i-1])$ under the prompt distribution, the distinguishability condition is dependent on a KL divergence between the previous two distributions and the error terms $\\epsilon_{\\theta}$ resulting from the distribution mismatch between the prompt and the pertaining distributions for each example. Letting $p_{\\theta}^{i}(o)$ and $p_{p r o m p t}^{i}$ correspond to the concepts $\\theta$ and $\\theta^{*}$ , the KL divergence are defined as follows, ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\mathrm{KL}_{i}(\\theta^{*}||\\theta):=\\mathbb{E}_{\\mathcal{O}[1:i-1]\\sim p_{p r o m p t}}[\\mathrm{KL}(p_{p r o m p t}^{i}||p_{\\theta}^{i})].\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Then, the distinguishability is provided as, ", "page_idx": 36}, {"type": "text", "text": "Condition D.1 (distinguishability [90]). $\\theta^{*}$ is distinguishable if for all $\\theta\\in\\Theta,\\theta\\neq\\theta^{*}$ , ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{k}\\mathrm{KL}_{i}\\big(\\theta^{*}||\\theta\\big)>\\epsilon_{\\theta},\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where $\\epsilon_{\\theta}$ is a small error term corresponding to the latent concept $\\theta$ . It has been empirically verified [86, 85, 89] that the larger example lengths $k$ can increase the left-hand side and improve distinguishability. Intuitively, the larger example lengths increase the proportion of the prompt sampled from the pre-trained distribution, providing more evidence for Bayesian inference. According to the noisy demonstration introduced in Eqn. (3), we can naturally have the following condition on the distinguishability under the noisy scenario. ", "page_idx": 36}, {"type": "text", "text": "Condition D.2 (noisy distinguishability). With the irrelevant or inaccurate noise injection, the distribution mismatch between the prompt and the pre-trained distributions is also enlarged, resulting in a varied requirement with error term $\\xi_{\\theta}(\\delta)$ (where $\\delta$ indicates the noise ratio of the noisy rational in the whole demonstration) for $\\theta^{*}$ being distinguishable if for all $\\theta\\in\\Theta$ , $\\theta\\neq\\theta^{*}$ , ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{k}\\mathrm{KL}_{i}(\\theta^{*}||\\theta)>\\epsilon_{\\theta}+\\xi_{\\theta}(\\delta),\\quad\\mathrm{where}\\quad\\xi_{\\theta}(\\delta)\\propto\\delta.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Given the above Condition D.2, we can analyze the expected 0-1 error when it fails according to the noisy scenarios considered in our work. It indicates that ICL cannot induce the correct output with the noise demonstration. First, we provide the following lemma based on the results of DeVore and Lorentz [15] and Xie et al. [90], which relate the bound on KL divergence with 0-1 loss given the Condition D.2. ", "page_idx": 37}, {"type": "text", "text": "Lemma D.3 (noisy-relaxed bound in [90]). let $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ denotes the set of $\\theta$ which does not satisfy Condition $D.2$ . We assume that $K L(p_{p r o m p t}(y_{t e s t}|x_{t e s t}))||p(y_{t e s t}|x_{t e s t},\\bar{\\theta})$ is bounded for all $\\theta$ and $\\theta^{*}$ minimizes the multi-class logistic risk as, ", "page_idx": 37}, {"type": "equation", "text": "$$\nL_{C E}(\\theta)=-\\mathbb{E}_{x_{t e s t}\\sim p_{p r o m p t}}[p_{p r o m p t}(y_{t e s t}|x_{t e s t})\\cdot\\log p(y_{t e s t}|x_{t e s t},\\theta)].\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "We can have $i f$ ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\mathbb{E}_{x_{t e s t}\\sim p_{p r o m p t}}[K L(p_{p r o m p t}(y_{t e s t}|x_{t e s t},\\delta)||p(y_{t e s t}|x_{t e s t},\\theta))]\\leq\\epsilon_{\\theta}+\\xi_{\\theta}(\\delta),\\quad\\forall\\quad\\theta\\in B,\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "then ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{n\\to\\infty}L_{0-1}(f_{n})\\leq\\operatorname*{inf}_{f}L_{0-1}(f)+g^{-1}\\biggl(\\operatorname*{sup}_{\\theta\\in B}(\\epsilon_{\\theta}+\\xi_{\\theta}(\\delta))\\biggr),\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where $\\begin{array}{r}{g(\\tau)=\\frac{1}{2}\\big((1-\\tau)\\log(1-\\tau)+(1+\\tau)\\log(1+\\tau)\\big)}\\end{array}$ is the calibration function for the multiclass logistic loss for $\\tau\\in[0,1]$ . ", "page_idx": 37}, {"type": "text", "text": "Based on the previous work [37], we assume that there is continuity between the concept parameter $\\theta$ and its corresponding output distribution $p_{\\theta}$ , where the KL divergence is assumed to have the 2nd-order Taylor expansion. Then, we have the following theorem and proof. ", "page_idx": 37}, {"type": "text", "text": "Theorem D.4. Given the previous assumption on multi-class logistic risk, we can have that $k\\geq2$ and $n\\to\\infty$ , the risk of an in-context learning predictor $f_{n}$ is bounded as ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{n\\to\\infty}L_{0-1}(f_{n})\\leq\\operatorname*{inf}_{f}L_{0-1}(f)+g^{-1}\\biggl(O\\bigl(\\frac{\\gamma_{\\theta}\\cdot\\operatorname*{sup}_{\\theta\\in\\Theta}\\bigl(\\epsilon_{\\theta}+\\xi_{\\theta}(\\delta)\\bigr)}{k-1}\\bigr)\\biggr),\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where $\\begin{array}{r}{g(\\tau)=\\frac{1}{2}\\big((1-\\tau)\\log(1-\\tau)+(1+\\tau)\\log(1+\\tau)\\big)}\\end{array}$ is the calibration function for multi-class logistic loss for $\\tau\\in[0,1)$ . The overall result indicates that the upper bound of the risk is loosed by $\\xi_{\\theta}(\\delta)$ of the noisy demonstration. ", "page_idx": 37}, {"type": "text", "text": "Proof of Theorem D.4. Assume KL divergence have a 2nd-order Taylor expansion around $\\theta^{*}$ as, ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\forall i>1,\\mathrm{KL}_{i}(\\theta^{*}||\\theta)=\\frac{1}{2}(\\theta-\\theta^{*})^{\\mathrm{T}}I_{i,\\theta^{*}}(\\theta-\\theta^{*})+O(||\\theta-\\theta^{*}||^{3}),\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where $I_{i,\\theta^{*}}$ is the Fisher information matrix of the $i$ -th token distribution $w.r.t.\\ \\ \\theta^{*}$ . Let $\\gamma_{\\theta^{*}}~=$ $\\frac{\\operatorname*{max}_{i}\\lambda_{\\operatorname*{max}}\\left(I_{i,\\theta^{*}}\\right)}{\\operatorname*{min}_{i}\\lambda_{\\operatorname*{min}}\\left(I_{i,\\theta^{*}}\\right)}$ and $\\lambda_{\\operatorname*{max}},\\lambda_{\\operatorname*{min}}$ is the largest and smallest eigenvalues. We can have the following relationship for $\\theta\\in{\\mathcal{B}}$ that, ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{i=2}^{k}\\mathrm{KL}_{i}(\\theta^{*}||\\theta)\\geq\\frac{1}{2}(\\theta-\\theta^{*})^{\\mathrm{T}}I_{i,\\theta^{*}}(\\theta-\\theta^{*})+O(||\\theta-\\theta^{*}||^{3})}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\geq\\displaystyle\\frac{1}{2}(k-1)\\lambda_{\\operatorname*{min}}(I_{i,\\theta^{*}})||\\theta-\\theta^{*}||^{2},}\\\\ &{\\quad\\quad\\quad\\quad\\quad|\\vert\\theta-\\theta^{*}\\vert|^{2}\\leq\\displaystyle\\frac{\\epsilon_{\\theta}+\\xi_{\\theta}(\\delta)}{\\frac{1}{2}(k-1)(\\operatorname*{min}_{i}\\lambda_{\\operatorname*{min}}(I_{i,\\theta^{*}}))}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Then we can bound the last KL term as, ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{KL}_{k}(\\theta^{*}||\\theta)=\\displaystyle\\frac{1}{2}(\\theta-\\theta^{*})^{\\mathrm{T}}I_{i,\\theta^{*}}(\\theta-\\theta^{*})+O(||\\theta-\\theta^{*}||^{3})}\\\\ &{\\phantom{\\mathrm{KL}_{k}}\\leq\\displaystyle\\frac{1}{2}(\\operatorname*{max}_{i}\\lambda_{\\operatorname*{max}}(I_{i,\\theta^{*}})||\\theta-\\theta^{*}||^{2}+O||\\theta-\\theta||^{2})}\\\\ &{\\phantom{\\mathrm{KL}_{k}}\\leq\\displaystyle\\frac{\\left((\\epsilon_{\\theta}+\\xi_{\\theta}(\\delta))\\operatorname*{max}_{i}\\lambda_{\\operatorname*{max}}(I_{i,\\theta^{*}})+O(1)\\right)}{(k-1)\\operatorname*{min}_{i}\\lambda_{\\operatorname*{min}}(I_{i,\\theta^{*}})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Denoting $\\mathrm{KL}_{k}(\\theta^{*}||\\theta)$ with $\\mathbb{E}_{x_{\\mathrm{test}}\\sim p_{p r o m p t}}[\\mathrm{KL}(p_{p r o m p t}(y_{\\mathrm{test}}|x_{\\mathrm{test}}(\\delta))||p(y_{\\mathrm{test}}|x_{\\mathrm{test}},\\theta))]$ , we have ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\mathbb{\\xi}_{x_{\\mathrm{test}}\\sim p_{p r o m p t}}[\\mathbf{KL}(p_{p r o m p t}(y_{\\mathrm{test}}|x_{\\mathrm{test}}(\\delta))||p(y_{\\mathrm{test}}|x_{\\mathrm{test}},\\theta))]\\leq\\frac{((\\epsilon_{\\theta}+\\xi_{\\theta}(\\delta))\\operatorname*{max}_{i}\\lambda_{\\mathrm{max}}(I_{i,\\theta^{*}})+O(1))}{(k-1)\\operatorname*{min}_{i}\\lambda_{\\mathrm{min}}(I_{i,\\theta^{*}})}.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Then, with the previous Lemma D.3, we can have the theorem result: for $k\\geq2$ and $n\\to\\infty$ , the risk of an ICL predictor $f_{n}$ is bounded as, ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{n\\to\\infty}L_{0-1}(f_{n})\\leq\\operatorname*{inf}_{f}L_{0-1}(f)+g^{-1}\\biggl(O\\bigl(\\frac{\\gamma_{\\theta}\\cdot\\operatorname*{sup}_{\\theta\\in\\Theta}\\bigl(\\epsilon_{\\theta}+\\xi_{\\theta}(\\delta)\\bigr)}{k-1}\\bigr)\\biggr),\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where $g(\\tau)=\\textstyle{\\frac{1}{2}}{\\bigl(}(1-\\tau)\\log(1-\\tau)+(1+\\tau)\\log(1+\\tau){\\bigr)}$ is the calibration function for multi-class logistic loss for $\\tau\\in[0,1)$ , and $g^{-1}$ is roughly linear in $\\epsilon$ as analyzed in [90]. ", "page_idx": 38}, {"type": "text", "text": "With the above results based on [90], we can find that when the demonstration length is fixed, the multi-class logistic loss can have a loosened upper bound controlled by our noise injection. Intuitively, when the noise rate is large, the performance of ICL can be significantly affected, which can be empirically verified by the Easy-to-Hard performance trend evaluated in Tab. 3. On the other hand, considering the fixed noisy injection in the demonstration, our method can reduce the effect of the mismatching error by rephrasing the demonstration with the pre-trained distribution of the LLM itself and then decreasing the error term in RHS of Eqn. (20), which increase the distinguishability of ICL to induce the correct outputs successfully. ", "page_idx": 38}, {"type": "text", "text": "E Implementation Details ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "In this section, we provide a comprehensive overview of the implementation details for our experiments, including the specifics of the baseline methods (Appendix E.1), the full algorithm of our proposed CD-CoT method (Appendix E.2), and the hyper-parameter optimization process (Appendix E.3). For reproduction, the source code and full dialogs are provided at: https: //github.com/tmlr-group/NoisyRationales. ", "page_idx": 39}, {"type": "text", "text": "To conduct the full range of experiments on GPT-3.5-turbo-0613 mentioned in this paper, we utilized a total of 2.03B tokens. This consisted of 1.21B input tokens and 0.82B output tokens. ", "page_idx": 39}, {"type": "text", "text": "E.1 Baseline Methods ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "In this part, we introduce the implementation details of the baseline methods. ", "page_idx": 39}, {"type": "text", "text": "Self-correction Methods: ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "\u2022 Intrinsic Self-correction (ISC) [29] asks LLM to endeavor to rectify its initial responses based solely on its inherent capabilities. We employ the prompts from [29], instructing LLMs to review and revise their answers to NoRa tasks with \"Review your previous answer and find problems with your answer. Based on the problems you found, improve your answer. Please reiterate your answer.\"   \n\u2022 Self-polish (SP) [89] teaches LLM to eliminate noisy information, rearrange the logic structure, and organize local conditions into new ones in parallel. We implement this method by (1) prompting LLMs to individually refine each noisy CoT examplar without additional information, repeating the process three times, and (2) combining these rephrased demos to form the context for the task. ", "page_idx": 39}, {"type": "text", "text": "Self-consistency Methods: ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "\u2022 SmoothLLM (SM) [62] enhances robustness by injecting perturbations into the prompts and utilizing self-consistency to mitigate these effects. We apply the described disturbance methods to noisy rationale demonstrations and feed them into LLMs for reasoning tasks. This process is repeated five times, with the most common answer across iterations selected as the voted answer. ", "page_idx": 39}, {"type": "text", "text": "\u2022 Self-denoise (SD) [102] improves LLM robustness by preprocessing prompts with random masks; the LLMs then work to reconstruct the masked content, reducing noise and aiding incoherent reasoning. Our implementation involves (1) applying the masking method to the noisy rationales on each shot, (2) prompting the LLMs to infer and flil the mask sections of each demonstration, and (3) using the reconstructed CoT demonstrations for task reasoning. This process is also repeated five times, and the most common answer is selected. ", "page_idx": 39}, {"type": "text", "text": "\u2022 Self-consistency (SC) [83] boosts reasoning performance by sampling multiple outputs and conducting majority voting without engaging in any input processing. To apply this method, we run the same task 5 times and vote for the maximum number of the same answers. ", "page_idx": 39}, {"type": "text", "text": "External Supervision Methods: ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "\u2022 Self-correction with Oracle Feedback (SCO) [29] enhances the ISC framework by providing a definitive ground truth. To implement this method, we use ISC to guide LLMs in refining their reasoning outcomes. The correction process is iterative, allowing for up to two adjustments, but ceases as soon as the correct answer is obtained.   \n\u2022 Backtracking (BT) [81] is a method of providing the position where the first mistake occurs to prompt LLMs to correct mistakes. To implement this method, (1) we prompt LLMs to correct the noisy rationales in each CoT example by providing the first noise sentence in it, and (2) we combine these rephrased examples to form the in-context for the task reasoning.   \n\u2022 ContrastiveCoT (CC) [9] uses contrastive CoT demonstration to improve LLMs\u2019 generalization abilities. We implement this method by providing an extra example with clean rationales as a positive demonstration, and the original prompts with noisy rationales are treated as negative examples. We then prompt the task queries with these positive and negative examples using the contrastive structure prompt mentioned in the paper to guide the LLMs in the reasoning process. ", "page_idx": 39}, {"type": "text", "text": "E.2 Contrastive Denoising with Noisy Chain-of-Thought ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Step-1: Rephrasing via Supervised Contrasting $(1\\mathbf{t}\\mathbf{o}N)$ . First, we establish a general prompt to construct a pair of contrastive examples, as shown in Sec. 5.1, steering the model towards learning from the clean example and then rephrasing as well as rectifying the noisy examples. It is worth noting that for each dataset, the clean demonstration utilized is fixed, aiming to minimize the usage of additional supervision. In the $K$ -shot in-context learning, we formulate a rephrasing prompt for each noisy example in the context, following the template in Sec. 5.1. The rephrasing time of each noisy example is controlled by the parameter $N$ . In our implementation, we set $N=5$ to strike a balance between the diversity of rephrased results and computational costs. ", "page_idx": 40}, {"type": "text", "text": "Step-2: Rationale Selection $(N{\\bf t o}M,N\\!\\ge\\!M)$ . Given that rephrased results may still contain noise, we employ answer matching for further denoising. Specifically, for each noisy example and its corresponding rephrased rationales, as each rationale can deduce an answer to the question, we select the rationales that the corresponding answers match the given (true) answer of this demonstration. This is called the \u201canswer matching\u201d and does not require an LLM for inference. The selected rationales form the refined candidate pool for that noisy example. ", "page_idx": 40}, {"type": "text", "text": "For example, $(x,\\tau,y)$ indicates the question, rationale, and answer of a given noisy demonstration. The rationale rephrasing (step 1) obtains three rephrased demonstrations: $(x_{1},\\tau_{1},\\dot{y}_{1}),(x_{2},\\tau_{2},y_{2})$ , and $(x_{3},\\tau_{3},y_{3})$ . Then, if $y_{1}=y_{2}=y$ and $y_{3}\\neq y$ , we will select the first two rephrased rationales, $\\mathcal{T}_{1}$ and $\\mathcal{T}_{2}$ . Namely, only rephrased results with consistent answers are retained, forming the refined candidate pool for that noisy demonstration of in-context learning. Notably, in cases where no consistent answer is found, the algorithm randomly selects one of the rephrased results as the candidate pool for the noisy example. Subsequently, we assemble $M$ contexts using the $K$ candidate pools. In particular, we randomly select one candidate from each pool and concatenate the $K$ selected candidates, forming a context. This process is repeated until $M$ contexts are obtained. ", "page_idx": 40}, {"type": "text", "text": "Step-3: Rationale Exploration $(M{\\bf t o}D,M\\!\\le\\!D)$ . For each of the $M$ distinct contexts, we perform multiple reasoning to explore various rationales, resulting in a total number of reasoning times equal to $D$ . To maintain a fair comparison with other baseline methods and to introduce more randomness in generating diverse reasoning paths, we employ the default temperature parameter, i.e., 1. Notably, given the fixed values for $M$ and $D$ , distinct allocation strategies of reasoning times can be generated, leading to performance and cost differences, as illustrated in Tab. 9 and Tab. 10. ", "page_idx": 40}, {"type": "text", "text": "Step-4: Answer Voting $(D\\mathbf{tol})$ . Given $D$ answers from step 3, we equally vote them into a final answer. For example, if the answer set is $1,1,1,2,3$ where $D=5$ , the answer voting will select \u201c1\u201d as the final answer for its highest frequency. ", "page_idx": 40}, {"type": "text", "text": "E.3 Hyper-parameter Optimization ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "We would like to further clarify the selection of hyper-parameters. To ensure a fair comparison with other baselines (for example, in self-consistency where we have the LLM reason 5 times and then vote), we fixed both $D$ and $N$ to be 5. Specifically, $D$ represents reasoning 5 times over the rephrased results, and $N$ indicates performing contrastive rephrasing on the input sample 5 times. This allows us to study the effects of reasoning with different rephrased inputs. ", "page_idx": 40}, {"type": "text", "text": "$M$ represents the number of selected rephrased results from the generated ones, therefore $M$ must be less than or equal to $N$ , and the value of $M$ depends on the breakdown combination of $D$ . ", "page_idx": 40}, {"type": "text", "text": "When $D\\,=\\,5$ , the possible combinations are $[(1,1,1,1,1),(1,1,1,2),(1,2,2),(3,2)$ , (5)], with corresponding $M\\mathrm{s}$ of $[5,4,3,2,1]$ . For example, $(1,1,1,1,1)$ means inputting 5 different rephrased results and reasoning once for each, totaling 5 times of reasoning. It\u2019s worth mentioning that the greater the value of $M$ , the larger the computational cost. ", "page_idx": 40}, {"type": "text", "text": "Finally, to balance the effectiveness and the cost of the algorithm, we test the results on a subset of the dataset with $M=1,2,3,5$ . We find that the accuracy exhibits subtle variations when employing different algorithm instances, with the highest average accuracy observed at $51.3\\%$ and the lowest average accuracy at $49.3\\%$ . However, the difference in computational cost is relatively significant. Therefore, we ultimately select the algorithm instance with $M=2$ as the default setting and test it across the entire dataset. ", "page_idx": 40}, {"type": "text", "text": "We present the impact of parameters $M,D$ , and $C$ on token usage in Tab. 10. Here, we conduct additional experiments to figure out the effect of varying $N$ on the computational cost. Specifically, we maintain a constant number of reasoning repetitions $D=5$ while adjusting $N$ and other parameters. Here are the configurations for testing: ", "page_idx": 41}, {"type": "text", "text": "1. $_{\\mathrm{N}=1}$ , $\\scriptstyle\\mathbf{M}=1$ , $C{=}[5]$ , $\\scriptstyle\\mathrm{D}=5$   \n2. $_{\\mathrm{N}=2}$ , ${\\bf M}\\!\\!=\\!\\!2$ , $\\mathrm{C}\\!=$ [3,2], $\\scriptstyle\\mathrm{D}=5$   \n3. $_{\\mathrm{N}=3}$ , ${\\bf M}\\!\\!=\\!\\!2$ , $\\mathrm{C}\\!=$ [3,2], $\\scriptstyle\\mathrm{D}=5$   \n4. $_{\\mathrm{N}=4}$ , ${\\bf M}\\!\\!=\\!\\!2$ , $\\mathrm{C}\\!=$ [3,2], $\\scriptstyle\\mathrm{D}=5$   \n5. $_{\\mathrm{N}=5}$ , ${\\bf M}\\!\\!=\\!\\!2$ , $C{=}[3,2]$ , $\\scriptstyle\\mathrm{D}=5$ (default) ", "page_idx": 41}, {"type": "text", "text": "These experiments are conducted on the NoRa-Math ase-9 task with irrelevant hard noise. The Tab. 30 shows the total number of tokens consumed by CD-CoT for complete reasoning on 300 test samples. This includes tokens used for both rephrasing and reasoning steps. ", "page_idx": 41}, {"type": "table", "img_path": "FbuODM02ra/tmp/18bd9bfa36ef803c05fc54a55aa6e7e10d52750faf71538064dc3cb1e20e5cf0.jpg", "table_caption": ["Table 30: Ablation study of parameter $N$ . "], "table_footnote": [], "page_idx": 41}, {"type": "text", "text": "As we can observe, the number of tokens generated increases as $N$ increases. This growth in token count directly correlates with computational needs. However, it\u2019s important to note that the increase is not linear, suggesting that the computational cost does not scale linear proportionally with $N$ . ", "page_idx": 41}, {"type": "text", "text": "F Full Experiments ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "In this section, we provide additional experimental details and analyses to supplement the main results presented in the paper. We conduct experiments on various LLMs and reasoning tasks to comprehensively evaluate the effectiveness of our proposed Contrastive Denoising Chain-of-Thought (CD-CoT) method in handling noisy rationales. Specifically, ", "page_idx": 42}, {"type": "text", "text": "\u2022 In Appendix F.1, we describe the detailed experimental setups, including the base language models used, evaluation settings, and CoT settings.   \n\u2022 In Appendix F.2, we present additional experimental results on different language models, and computational costs, and introduce the Normalized Difference in Accuracy (NDA) metric for evaluating the efficacy of methods under noisy scenarios.   \n\u2022 In Appendix F.3, we conduct an ablation study to further justify the denoising power of the proposed CD-CoT method and highlight the importance of its two key components: contrastive denoising with rationale rephrasing and repeated reasoning with voting.   \n\u2022 In Appendix F.4, we explore the impact of different semantic difficulty levels of irrelevant noise on model performance and the effectiveness of CD-CoT under these scenarios.   \n\u2022 In Appendices F.5 and F.6, we investigate how the number of noisy thoughts per example and the number of noisy examples affect the model\u2019s reasoning performance.   \n\u2022 In Appendix F.7, we discuss the variants of CD-CoT, including a self-supervised variant that does not rely on manually crafted clean examples.   \n\u2022 In Appendix F.8, we further explore CD-CoT\u2019s robustness by introducing new datasets. Moreover, we investigate the noisy rationale problem in large-scale real-world scenarios by evaluating the impact of noisy context on model performance in multi-turn conversational QA tasks using the MT-Bench [108] dataset.   \n\u2022 In Appendix F.9, we provide additional qualitative results, showcasing the denoised examples generated by different robust methods across various tasks under medium-level noise. ", "page_idx": 42}, {"type": "text", "text": "These additional experiments and analyses aim to provide a comprehensive understanding of the noisy rationale problem, the effectiveness of the proposed CD-CoT method, and the factors influencing model performance under noisy scenarios. ", "page_idx": 42}, {"type": "text", "text": "F.1 Detailed Setups of the Experiments ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "We employ GPT-3.5-turbo-0613 [17] as our base LLM (denoted as Base) for the analyses presented in this study. In addition, we conduct evaluations on three supplementary models, including Gemini-Pro (Jan. 2024) [76], Llama2-70B [79], and Mixtral- $\\mathbf{\\nabla}{8\\mathbf{x}7\\mathbf{B}}$ [33]. While evaluating baseline methods on various, we consistently keep the temperature parameter $\\tau$ and the top-p setting at their default value of 1, along with all other hyperparameters of models set to defaults. We conducted experiments on the first 300 questions for each task and repeated reasoning 5 times for each question. ", "page_idx": 42}, {"type": "text", "text": "We assume all CoT experiments with clean rationales or noisy rationales are conducted in a 3-shot setting unless specified otherwise. Furthermore, all CoT examples are constructed by randomly drawing from all available questions, except for the NoRa-Symbolic Longer task, which has predefined demonstrating and testing scopes. ", "page_idx": 42}, {"type": "text", "text": "F.2 Supplementary Results of the Main Experiments ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Different LLMs. Fig. 7 displays the result of the GPT-3.5-Turbo model\u2019s evaluation on the NoRa Dataset. It corresponds to base model results in Tab. 3. We have also conducted comprehensive experiments on the Gemini model to evaluate various types of noise. Fig. 8 shows the full performance evaluation of Gemini on the NoRa dataset. ", "page_idx": 42}, {"type": "text", "text": "Computation Cost. Tab. 31 shows that the cost of CD-CoT is within an acceptable range compared with other methods. ", "page_idx": 42}, {"type": "text", "text": "The Normalized Difference in Accuracy (NDA) Metric. We propose a new evaluation score, Normalized Difference in Accuracy (NDA), to quantify the efficacy of $\\mathcal{M}$ under the noisy scenario, ", "page_idx": 42}, {"type": "image", "img_path": "FbuODM02ra/tmp/fdcc9e934c1f30186091cf0d159a1b534164374720f8f8500a4b6551a16c6f81.jpg", "img_caption": ["Figure 7: GPT-3.5-Turbo Full Performance Evaluation on the NoRa Dataset. "], "img_footnote": [], "page_idx": 43}, {"type": "image", "img_path": "FbuODM02ra/tmp/03708ea9f1766518991793259b88fdd73e6efec13c37e5af32470072431bd705.jpg", "img_caption": ["Figure 8: Gemini Full Performance Evaluation on the NoRa Dataset. "], "img_footnote": [], "page_idx": 43}, {"type": "equation", "text": "$$\n\\mathrm{NDA}(\\mathcal{M},\\mathcal{Q},\\mathcal{P})\\!=\\!\\frac{\\mathrm{Acc}(\\mathcal{M},\\mathcal{Q},\\mathcal{P}_{\\mathrm{noisy}})\\!-\\!\\mathrm{Acc}(\\mathcal{M},\\mathcal{Q},\\emptyset)}{\\mathrm{Acc}(\\mathcal{M},\\mathcal{Q},\\mathcal{P}_{\\mathrm{clean}})\\!-\\!\\mathrm{Acc}(\\mathcal{M};,\\mathcal{Q},\\emptyset)},\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "where $\\operatorname{Acc}(\\mathcal{M},\\mathcal{Q},\\mathcal{P}_{\\mathrm{clean}})$ , $\\operatorname{Acc}(\\mathcal{M},\\mathcal{Q},\\mathcal{P}_{\\mathrm{noisy}})$ and $\\operatorname{Acc}(\\mathcal{M};,\\mathcal{Q},\\emptyset)$ represent the accuracy of method $\\mathcal{M}$ with clean rationales, noisy rationales (irrelevant or inaccurate), and without CoT demos. Fig. 9 is the illustration of the NDA metric. ", "page_idx": 43}, {"type": "text", "text": "Please note that: (1) The main evaluation metric used throughout this paper is the accuracy score as introduced in Section 4; (2) The NDA metric serves as an auxiliary tool for analyzing empirical results, specifically designed to quantify how effectively a given LLM and denoising method perform under noisy scenarios. ", "page_idx": 43}, {"type": "text", "text": "Tab. 32 presents a comparison of the accuracy and NDA across all methods. A negative value in NDA indicates that the accuracy of noisy rationales falls below that of 0-shot. We observe that CD-CoT consistently excels in NDA, nearing $100\\%$ in most tasks and even surpassing it in certain instances. ", "page_idx": 43}, {"type": "image", "img_path": "FbuODM02ra/tmp/da54ee566ef68f400a11f60ebd1f6a4758bf786b1f40f3ae81db919031e491ba.jpg", "img_caption": ["Figure 9: Illustration of the NDA metric "], "img_footnote": [], "page_idx": 43}, {"type": "text", "text": "Standard Deviation Comparison. Tab. 33 provides a comprehensive comparison of the standard deviations $(\\sigma)$ across various methods on the NoRa-Math Base-9 task, using the GPT-3.5-Turbo model. The table presents the $\\sigma$ values for each method under different conditions, including clean rationale, irrelevant rationale (easy, medium, and hard), and inaccurate rationale (easy, medium, and hard). These results supplement the main experimental findings by offering insights into the variability and consistency of the methods\u2019 performance. ", "page_idx": 43}, {"type": "table", "img_path": "FbuODM02ra/tmp/23a98dfcc204da93c30acaf72a8e44e87a60c8fbbd8ad9772513ca5a48b923fa.jpg", "table_caption": ["Table 31: Computation cost (#tokens) of all methods. "], "table_footnote": [], "page_idx": 44}, {"type": "text", "text": "F.3 The Superior Performance and Denoising Effectiveness ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "The proposed CD-CoT method denoises noisy rationales, which leads to better reasoning performance. To be specific: ", "page_idx": 44}, {"type": "text", "text": "\u2022 The first two steps of CD-CoT are for explicit data denoising. First, it rephrases the noisy example by contrasting it with the clean example. Then, with the obtained rephrased examples, it selects qualified candidates by checking the validity of the rephrased answers. \u2022 The rephrased (denoise) rationales by different robust methods are shown in Tab. 12. As can be seen, CD-CoT significantly removes noise and also ensures format alignment with the original rationale. More examples of denoising can be found in Appendix. F.9. Hence, these empirical results adequately justify the denoising power of CD-CoT. ", "page_idx": 44}, {"type": "table", "img_path": "FbuODM02ra/tmp/2409aec95ab1448ffa0e7ca4abc0bd25bc1a070141544cf62302df4e36196ace.jpg", "table_caption": [], "table_footnote": [], "page_idx": 45}, {"type": "table", "img_path": "FbuODM02ra/tmp/2ebec2cf166ef510f4fc12be8237c92aedb371a9724f2ad14a92571314924ef3.jpg", "table_caption": ["Table 32: Comparing accuracy and NDA metrics across all methods. The boldface numbers mean the best results; underlines indicate the second-best. \"\u2014\" denotes methods with poor results, where zero-shot performance approximates clean 3-shot outcomes, rendering NDA calculation meaningless. ", "Table 33: Comparing the mean values and standard deviation values of accuracy across all methods. "], "table_footnote": [], "page_idx": 45}, {"type": "text", "text": "The two important aspects of the proposed method are (1) contrastive denoising with rationale rephrasing $\\scriptstyle\\left({+\\,C D}\\right)$ Rephrase) and (2) repeated reasoning with voting $\\downarrow$ Vote). Here, we conduct an extra ablation study on the NoRa-Math Base-9 dataset to justify the denoising power of CD-CoT. ", "page_idx": 45}, {"type": "text", "text": "In this ablation study, we compare the performance of CD-CoT with different combinations of clean and noisy rationales on the NoRa-Math Base-9 dataset. The \"Base model\" column represents the accuracy of the base model without any additional components. The $\"+\\mathrm{CD}$ Rephrase\" column shows the accuracy when the contrastive denoising with rationale rephrasing (CD Rephrase) is applied. The $\"+$ Vote\" column represents the accuracy when the final answer is determined by a majority vote of five runs. Finally, the \"CD-CoT\" column shows the accuracy of the complete CD-CoT method, which includes both CD rephrasing and voting. ", "page_idx": 45}, {"type": "text", "text": "Since CD-CoT requires 3-shot noisy rationales and 1-shot clean rationale for contrastive rephrasing, we compare its performance with the following setups. The results are summarized in the Tab. 34. ", "page_idx": 45}, {"type": "text", "text": "\u2022 3-shot clean rationales (without CD-CoT)   \n\u2022 3-shot noisy rationales (irrelevant or inaccurate) of varying difficulty levels $+\\,1$ -shot clean rationale (with CD-CoT)   \n\u2022 4-shot clean rationales (without CD-CoT) ", "page_idx": 45}, {"type": "table", "img_path": "FbuODM02ra/tmp/ed59329ebb844f75584fe00ceff13d5d95b948cb023adf9900e06d378c37360a.jpg", "table_caption": ["Table 34: Ablation study of CD rephrase and vote components on the NoRa-Math Base-9 dataset. "], "table_footnote": [], "page_idx": 46}, {"type": "text", "text": "From the experimental results, we can observe that: ", "page_idx": 46}, {"type": "text", "text": "Observation F.1 (CD-CoT significantly improves reasoning performance across various noisy scenarios). CD-CoT brings a significant improvement compared to the base model when prompted with 3 noisy rationales $+\\,1$ clean rationale across various noise ratios of both irrelevant and inaccurate noise. The base accuracy of CD-CoT can be attributed to the improvement brought by the contrastive denoising with rationale rephrasing method. This accuracy is further enhanced in the voting method, which demonstrates the effectiveness of the self-consistency approach in improving reasoning performance. ", "page_idx": 46}, {"type": "text", "text": "Observation F.2 (Noisy rationales with CD-CoT does not outperform full clean rationales). The accuracy before voting and the accuracy after voting in CD-CoT do not exceed the accuracy of the base model prompted with 4 clean rationales. This indicates that while CD-CoT is effective at denoising rationales and enhancing reasoning performance in noisy situations, it does not inherently improve reasoning beyond the capabilities of the model when clean rationales are provided. ", "page_idx": 46}, {"type": "text", "text": "Observation F.3 (Contrastive denoising and voting in CD-CoT are important). The results highlight the importance and effectiveness of the two key aspects of CD-CoT: (1) contrastive denoising with rationale rephrasing, which helps to fliter out the noise and align the format of the rationales, and (2) repeated reasoning with voting, which enhances the robustness and accuracy of the final predictions. Notably, CD-CoT proves to be a valuable approach for improving the reasoning performance in the presence of noisy rationales, though it does not surpass the performance with full clean rationales. ", "page_idx": 46}, {"type": "text", "text": "Observation F.4 (CD-CoT is more effective at managing irrelevant noise compared to inaccurate noise). As shown in Tab. 33, CD-CoT is more effective at handling irrelevant noise than it is at managing inaccurate noise at the same level. This is because irrelevant noise is easier to differentiate from the target information, making it simpler to remove during the rephrasing step of the contrasting denoising process. Additionally, Appendix. F.9 presents the denoised results for high-noise scenarios. ", "page_idx": 46}, {"type": "text", "text": "Variants of prompt for CD-CoT. We also conduct additional experiments to test the sensitivity of CD-CoT to prompt designs. Specifically, we generate several variants of prompts for the proposed CD-CoT method, including a shorter prompt, the default prompt, and a longer one: ", "page_idx": 46}, {"type": "table", "img_path": "FbuODM02ra/tmp/b9b521db010b65f19dc68dcded8432639325701d2562908ab1db458b86560573.jpg", "table_caption": [], "table_footnote": [], "page_idx": 47}, {"type": "text", "text": "We conduct experiments on NoRa-Math Base-9 to compare these three prompts. The results in Tab. 35 show that the performance of CD-CoT is only marginally influenced by these prompts. Note that the prompt of CD-CoT remains simple. Cooperating with advanced methods for iterating prompts and rationales, such as APE [110] and Star [97], can further improve this method. ", "page_idx": 47}, {"type": "table", "img_path": "FbuODM02ra/tmp/7aaafd3f2a0e182cb3e6b3d7117820f13ebc8cc18ddd10c40e70d126775eaf4b.jpg", "table_caption": ["Table 35: Ablation study of different prompts in CD-CoT. "], "table_footnote": [], "page_idx": 47}, {"type": "text", "text": "Rephrasing performance of SD. Besides, we observe that the SD method performs better on some tasks when using irrelevant and inaccurate rationales compared to clean ones. A thorough examination of the empirical results from the log flies reveals some inconsistencies in the performance of the SD method, especially in the NoRa-Symbolic Equal and NoRa-Commonsense tasks. ", "page_idx": 47}, {"type": "text", "text": "Specifically, for NoRa-Symbolic Equal task: ", "page_idx": 47}, {"type": "text", "text": "\u2022 In this task, comparing SD and SC reveals that the domain knowledge of LLM is relatively weak. According to the experiment results, when SD rephrases symbolic tasks, it often directly returns the final answer of the original response in many cases. \u2022 The quality of the rephrased results basically does not change with the increase in noise. \u2022 As a result, the accuracies of SD under clean and noisy settings are very close, with a maximum difference of less than $1\\%$ , which we think is within the acceptable range of error. ", "page_idx": 47}, {"type": "text", "text": "For NoRa-Commonsense task: ", "page_idx": 47}, {"type": "text", "text": "\u2022 We observe different behaviors of SD in clean and noisy settings.   \n\u2022 In noisy settings, SD does not follow the prompt to make minor corrections. Instead, SD generates short rationales with only the answer, discarding the original rationale (as shown in Tables. 36-37). This simplification can benefit commonsense tasks where the model already has a strong inherent understanding.   \n\u2022 On the other hand, in the clean setting, SD unexpectedly generates entirely new rationales instead of making minor corrections. These new rationales may deviate from the original correct reasoning, potentially introducing errors or inconsistencies that weren\u2019t present in the initial, accurate solution. ", "page_idx": 47}, {"type": "text", "text": "In Tab. 36 and Tab. 37, we present two representative examples of SD in the commonsense task. Our findings show that: (i) when provided with a clear rationale, SD generates entirely new rationales rather than simply making minor corrections; (ii) when given a noisy rationale, SD produces brief rationales that include only the answer, completely discarding the original rationale. ", "page_idx": 47}, {"type": "text", "text": "Overall, SD\u2019s denoising effect relies on LLMs\u2019 intrinsic capability. SD performs five maskings and reconstructions per noisy example, concatenating results into five prompts for LLM inference. In contrast, SC performs five direct inferences on noisy prompts without explicit denoising. By comparing SD and SC\u2019s performance, we can gain insights into the model\u2019s denoising effects. ", "page_idx": 47}, {"type": "text", "text": "In Tab. 3, SC outperforms SD in NoRa-Math and NoRa-Symbolic tasks, while SD only marginally excels in the NoRa-Commonsense task. This pattern persists in the clean settings. This suggests that ", "page_idx": 47}, {"type": "text", "text": "LLMs struggle to reconstruct masked prompts in complex, domain-specific tasks but perform better in simpler common tasks, highlighting the varying levels of internal knowledge within LLMs across different domains. ", "page_idx": 48}, {"type": "text", "text": "Besides, the counterintuitive results on the NoRa-Commonsense task may be attributed to taskspecific characteristics. Observation of reconstructed masked prompts in this task indicates that LLMs tend to bypass reconstruction instructions in noisy settings, directly providing final answers. This accidental removal of all rationales, including noisy ones, effectively acts as a noise filter. Combined with the LLM\u2019s natural strength in the commonsens task, this unplanned filtering likely explains the small improvement in accuracy under noisy conditions. ", "page_idx": 48}, {"type": "text", "text": "The original clean rationale ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Carrie is Hugh\u2019s sister\u2019s husband\u2019s daughter, so the relations path is sister, husband, daughter. For husband\u2019s daughter, we have husband\u2019s daughter is daughter. So the relations path are reduced to sister, daughter. For sister\u2019s daughter, we have sister\u2019s daughter is niece. So the relations path are reduced to niece. Therefore, Answer: niece. ", "page_idx": 48}, {"type": "text", "text": "The rephrased rationale by SD ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "To determine Carrie\u2019s relationship to Hugh, we can break down the given information step by step: 1. Carrie is Hugh\u2019s sister\u2019s husband\u2019s daughter. 2. Hugh\u2019s sister is Carrie\u2019s sibling. 3. Hugh\u2019s sister\u2019s husband is Carrie\u2019s brother-in-law. 4. Carrie is the daughter of Hugh\u2019s sister\u2019s husband. Therefore, the answer is: Carrie is Hugh\u2019s niece. ", "page_idx": 48}, {"type": "text", "text": "The original noisy rationale ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Carrie is Hugh\u2019s sister\u2019s husband\u2019s daughter, so the relations path is sister, husband, daughter. For husband\u2019s daughter, we have husband\u2019s daughter is daughter. We have daughter\u2019s grandmother is niece. So the relations path are reduced to sister, daughter. For sister\u2019s daughter, we have sister\u2019s daughter is niece. We have daughter\u2019s niece is niece. So the relations path are reduced to niece. Therefore, Answer: niece. ", "page_idx": 48}, {"type": "text", "text": "The rephrased rationale by SD niece. ", "page_idx": 48}, {"type": "text", "text": "Table 36: Rephrasing performance of SD under different noisy settings. ", "page_idx": 48}, {"type": "text", "text": "The original clean rationale ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Francisco is Lynn\u2019s son\u2019s father\u2019s brother, so the relations path is son, father, brother. For father\u2019s brother, we have father\u2019s brother is uncle. So the relations path are reduced to son, uncle. For son\u2019s uncle, we have son\u2019s uncle is brother. So the relations path are reduced to brother. Therefore, Answer: brother. ", "page_idx": 48}, {"type": "text", "text": "The rephrased rationale by SD ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "To determine Francisco\u2019s relationship with Lynn, we start with the information provided: Francisco is Lynn\u2019s son\u2019s father\u2019s brother. Step 1: Lynn\u2019s son\u2019s father is Lynn\u2019s husband. Step 2: Francisco is Lynn\u2019s husband\u2019s brother. Based on this information, we can conclude that Francisco is Lynn\u2019s brother-in-law. Answer: brother-in-law ", "page_idx": 48}, {"type": "text", "text": "The original noisy rationale ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Francisco is Lynn\u2019s son\u2019s father\u2019s brother, so the relations path is son, father, brother. For father\u2019s brother, we have father\u2019s brother is uncle. So the relations path are reduced to son, uncle. For son\u2019s uncle, we have son\u2019s uncle is brother. We have uncle\u2019s niece is nephew. So the relations path are reduced to brother. We have brother\u2019s nephew is mother. Therefore, Answer: brother. ", "page_idx": 48}, {"type": "text", "text": "The rephrased rationale by SD Answer: brother. ", "page_idx": 48}, {"type": "text", "text": "Table 37: Rephrasing performance of SD under different noisy settings. ", "page_idx": 48}, {"type": "text", "text": "F.4 The Difficulty of Irrelevant Noise Semantics ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "We conduct extra experiments with different semantic difficulties. Specifically, we set up three levels of semantic difficulty: ", "page_idx": 48}, {"type": "text", "text": "\u2022 Level-1: Topic-irrelevant. Noisy thought has little relevance to the task. Here, noisy thought involves concepts that are clearly irrelevant to the knowledge required for the task. For example, (1) in physics, the number one is the charge of a single unit of positive or negative electricity; (2) in chemistry, two is the atomic number of helium, the second most abundant element in the universe. These two are irrelevant thoughts. ", "page_idx": 48}, {"type": "text", "text": "\u2022 Level-2: Topic-relevant and task-irrelevant. The noisy thought has some apparent relevance to the task but does not help solve the problem. Noisy thought may involve some terminology or concepts that are relevant to the task but do not provide useful information. ", "page_idx": 48}, {"type": "text", "text": "For example, (1) the modulo operation (mod) finds the remainder after the division of one number by another. (2) In base-9, the leading digit 1 indicates that the number is greater than or equal to $1*9^{2}=81$ but less than $2^{-}*9^{\\overline{{2}}}=162$ . (3) In base-9, the number 9 is represented as $'10\"$ since it is the base value itself. (4) The number 11 in base-9 is equivalent to the decimal value ninety-nine, which is one less than the square of the base value. ", "page_idx": 49}, {"type": "text", "text": "\u2022 Level-3: Topic-relevant, task-relevant, but not helpful. Here, the noisy thought has some relevance to the task, but is not necessary information to solve the problem. The noisy thought may involve concepts, principles, or methods that are relevant to the task, but not directly applicable to the problem at hand. ", "page_idx": 49}, {"type": "text", "text": "For example, (1) in a base-9 system, the place values are units, nines, eighty-ones, and so on, analogous to units, tens, hundreds, etc., in base-10; (2) the concept of carrying is essential when performing addition in positional numeral systems like base-9 and base-10. The modulo operation is a crucial step in the process of adding numbers in base-9, as it determines the digit and carry values; (3) the modulo operation is a crucial step in the process of adding numbers in base-9, as it determines the digit and carry values. ", "page_idx": 49}, {"type": "text", "text": "Based on the above definition and prompt, we generate the data with the following four steps. ", "page_idx": 49}, {"type": "text", "text": "\u2022 Step-1: Initial generation and human evaluation. We used this prompt to generate an initial set of irrelevant statements for each level. These were then manually reviewed and flitered to ensure they accurately represented the intended level of irrelevance. We selected high-quality examples for each level.   \n\u2022 Step-2: Scaled generation. Using these high-quality examples as ICL demonstrations, we prompted GPT-4 to generate a larger set of irrelevant statements for each level.   \n\u2022 Step-3: Validation through similarity analysis. To confirm that our generated statements indeed represented different levels of irrelevance, we conducted a cosine similarity analysis. This analysis quantitatively demonstrated the semantic differences between levels, as shown in our previous response.   \n\u2022 Step-4: Dataset construction. we integrated these generated irrelevant statements into our dataset. We inserted them into relevant demonstrations at appropriate positions, following a probability distribution that corresponds to the intended difficulty level of the task. ", "page_idx": 49}, {"type": "text", "text": "The experiment results with the above irrelevant thoughts are elaborated as follows: Here, the setup is kept the same as the main experiments to be consistent. Therein, we test the GPT-3.5 model with 3-shot irrelevant rationales where the noise ratio is 0.8. ", "page_idx": 49}, {"type": "text", "text": "For a reasonable quantification of relevance, we choose the metric of cosine similarity with the embeddings API provided by OpenAI with model: text-embedding-ada-002. We calculate the cosine similarity between all noise-inserted sample sentences and the original clean inference sentences and then take their average value. ", "page_idx": 49}, {"type": "table", "img_path": "FbuODM02ra/tmp/8e51ce1166a70471744eccc45575c76b8811d273fc98d68e6e32272e0806ce14.jpg", "table_caption": ["Table 38: Performance of GPT-3.5-turbo under different noise semantics. "], "table_footnote": [], "page_idx": 49}, {"type": "text", "text": "For comparison, the accuracy when prompting with clean rationales is $46.4\\%$ for NoRa-Math Base-9 and $32.7\\%$ for NoRa-Symbolic Equal. Specifically, we have the following observations: ", "page_idx": 49}, {"type": "text", "text": "Observation F.5 (CD-CoT method is still effective under different semantic levels of noise). Notably, we further verify that the proposed CD-CoT method is still effective under this imperfect scenario with different semantic levels of noise. Specifically, CD-CoT can achieve $58.3\\%$ , $60.0\\%$ , and $59.7\\%$ accuracy on the three semantic levels of noise for the NoRa-Math Base-9 tasks and $44.0\\%$ , $46.0\\%$ , and $45.0\\%$ accuracy on the NoRa-Symbolic Equal tasks. Compared to the base model, CD-CoT shows a significant improvement of $29.4\\%$ and $19.1\\%$ in accuracy on semantic level-2 reasoning for the NoRa-Math Base-9 and NoRa-Symbolic Equal tasks, respectively. Similarly, CD-CoT demonstrates a $26.7\\%$ and $17.3\\%$ increase in accuracy on semantic level-3 reasoning for the NoRa-Math Base-9 and NoRa-Symbolic Equal tasks. ", "page_idx": 50}, {"type": "text", "text": "Observation F.6 (CD-CoT maintains a robust and stable performance across different semantic levels of noise.). As the semantic level of noise increases, the base model exhibits a certain level of robustness to the irrelevant noise but still suffers a considerable performance drop ( $15.8\\%$ and $5.8\\%$ decrease for semantic level-2, and $13.4\\%$ and $5.0\\%$ decrease for semantic level-3 on the NoRa-Math Base-9 and NoRa-Symbolic Equal tasks, respectively). In contrast, CD-CoT maintains a robust and stable performance across different semantic levels of noise. ", "page_idx": 50}, {"type": "text", "text": "F.5 The Number of Noisy Thoughts ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "As introduced in the main text, we use noise ratios of 0.3, 0.5, and 0.8 to control the number of noisy thoughts. The statistics of the NoRa dataset are shown in Tab. 2, showing the exact number of clean/noisy thoughts in each task. NoRa-Math entails 8 thoughts, NoRa-Symbolic varies from 2 to 12, and NoRa-Commonsense requires 5 thoughts. ", "page_idx": 50}, {"type": "text", "text": "To figure out the exact number of noisy thoughts for the model to be robust, we conduct further experiments to cover a wider range of noisy thoughts. Specifically, ", "page_idx": 50}, {"type": "text", "text": "\u2022 We compare the model\u2019s performance on NoRa-Math Base-9 and NoRa-Symbolic Equal tasks using clean and noisy rationales containing varying numbers of irrelevant or inaccurate thoughts.   \n\u2022 We gradually increase the number of noisy thoughts from 1 to 6 to investigate the impact of the noise level on the model\u2019s accuracy.   \n\u2022 The total number of thoughts per shot, including both clean and noisy thoughts, is shown in parentheses for the NoRa-Math Base-9 task.   \n\u2022 For the NoRa-Symbolic Equal task, the number of thoughts in the clean demonstrations varies across different examples.   \n\u2022 To ensure a consistent comparison, we select a subset of examples with a similar number of thoughts (ranging from 8 to 12) for both clean and noisy rationale experiments.   \n\u2022 We report the average number of thoughts (10.2) for the clean rationales in this subset.   \n\u2022 Consequently, the total number of thoughts per shot for noisy rationales in the NoRaSymbolic Equal task is also calculated based on this average. ", "page_idx": 50}, {"type": "text", "text": "According to the experimental results in the Tab. 39, we can observe that: ", "page_idx": 50}, {"type": "text", "text": "Observation F.7 (The impact of noisy thoughts varies across different noise types). For the NoRaMath Base-9 task, the model\u2019s accuracy remains relatively stable when only one irrelevant noisy thought is introduced. However, the accuracy drops significantly as the number of irrelevant noisy thoughts increases. On the other hand, for the NoRa-Symbolic Equal task, even a single irrelevant or inaccurate noisy thought can lead to a noticeable decrease in accuracy. ", "page_idx": 50}, {"type": "text", "text": "Observation F.8 (Inaccurate noisy thoughts have a more severe impact on the model\u2019s performance compared to irrelevant noisy thoughts). In the NoRa-Math Base-9 task, the accuracy drops drastically from $46.4\\%$ to $34.8\\%$ with just one inaccurate, noisy thought, and it further deteriorates to $6.2\\%$ when the number of inaccurate, noisy thoughts reaches 6. In contrast, with 6 irrelevant noisy thoughts, the accuracy decreases to $25.3\\%$ , which is still higher than that of inaccurate thoughts. ", "page_idx": 50}, {"type": "text", "text": "Observation F.9 (Model\u2019s robustness to noisy thoughts varies depending on the nature of the task). The NoRa-Symbolic Equal task exhibits a similar trend, where the accuracy decreases as the number of noisy thoughts increases, regardless of the noise type. However, the decline in accuracy is more gradual compared to the NoRa-Math Base-9 task, suggesting that the model\u2019s robustness to noisy thoughts varies depending on the nature of the task. ", "page_idx": 50}, {"type": "table", "img_path": "FbuODM02ra/tmp/f60dadbd33b8a238bf797a15de24d75c70791ee5fa9c9106ac2a790e93b2dbda.jpg", "table_caption": ["Table 39: Impact of the number of noisy thoughts on GPT-3.5 performance. "], "table_footnote": [], "page_idx": 51}, {"type": "text", "text": "The experimental results demonstrate that the presence of noisy thoughts, even in small numbers, can significantly impact the model\u2019s reasoning performance. The extent of the impact depends on factors such as the task type, noise type, and number of noisy thoughts. ", "page_idx": 51}, {"type": "text", "text": "F.6 The Number of Noisy Examples ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "In this part, we provide a deeper analysis and extract more insights into in-context learning with varying numbers of noisy examples. ", "page_idx": 51}, {"type": "text", "text": "First, we obtain the accuracy of the base model with different numbers of noisy prompting examples, including 0-shot, 1-shot, 3-shot, and 5-shot. According to the results shown in Tab. 40, we propose the following hypothesis: The trend of model performance as the number of noisy examples increases depends on the change in performance between 1-shot and 0-shot learning. ", "page_idx": 51}, {"type": "text", "text": "Namely, if the performance in 1-shot learning is better than in 0-shot learning, this indicates that noisy examples still help the model in recognizing task patterns. Therefore, model performance can improve with an increasing number of noisy examples, albeit at a slower rate compared to clean examples. On the other hand, if the performance in one-shot learning is less than in zero-shot learning, it suggests that noisy examples interfere with the model\u2019s existing knowledge of the task, causing model performance to decline as the number of noisy examples increases. ", "page_idx": 51}, {"type": "table", "img_path": "FbuODM02ra/tmp/6ece919672995646824c8e54e0e6bf37b6df5e7daa7ff104b04d68047e9d3816.jpg", "table_caption": [], "table_footnote": [], "page_idx": 51}, {"type": "text", "text": "Table 40: Performances of base model under varying number of examples (\"\u2014\" denotes over token limit, bolded number denotes the highest accuracy). ", "page_idx": 51}, {"type": "text", "text": "Next, we conduct a further quantitative analysis of the impact of noisy shots on model performance. We evaluate (0-shot, 1-shot, 3-shot) results partition, there are 8 possible results in total: (CCC), (CCW), (CWC), (CWW), (WCC), (WCW), (WWC), (WWW). Specifically, \u2019C\u2019 stands for correct and \u2019W\u2019 stands for wrong. For example, CCC means the cases in which test questions are correctly answered in 0-shot (C), 1-shot (C), and 3-shot (C). WWC means the cases in which test questions are wrongly answered in 0-shot (W), 1-shot (W), and correctly answered in 3-shot (C). ", "page_idx": 52}, {"type": "text", "text": "To analyze the effects of noisy shots, we categorize these eight results into three types: ", "page_idx": 52}, {"type": "text", "text": "\u2022 The model answers correctly at zero-shot but starts to answer incorrectly as noisy shots increase. The corresponding results are (CCW), (CWC), and (CWW). \u2022 The model answers incorrectly at zero-shot but starts to answer correctly as noisy shots increase. The corresponding results are (WCC), (WCW), and (WWC). \u2022 The model\u2019s performance is consistent at zero-shot and with noisy shots. The corresponding results are (CCC) and (WWW). ", "page_idx": 52}, {"type": "table", "img_path": "FbuODM02ra/tmp/bd4628be117f71054420b39ab76b66bc10d80652a149ea619d80839a50d50b31.jpg", "table_caption": ["Table 41: Results partition of (0-shot, 1-shot, 3-shot). "], "table_footnote": [], "page_idx": 52}, {"type": "text", "text": "By observing Tab. 41, we arrive at the following analysis. ", "page_idx": 52}, {"type": "text", "text": "Observation F.10 (Compared to zero-shot, models can benefit from noisy shots in many cases). Across all datasets, (WWW) is the most prevalent outcome, while the second most common results are almost always (WWC) or (WCW). This implies that there is a relatively large proportion of samples for which the model answers incorrectly at zero-shot, yet correctly as noisy shots increase. Observation F.11 (The instances where noisy shots have a negative effect are very rare.). For the first category of results, namely (CCW), (CWC), and (CWW), the average proportion across all datasets is 0.039. This indicates that the proportion of samples where the model answers correctly at zero-shot but incorrectly as noisy shots increase is very small. In other words, the instances where noisy shots have a negative effect are rare. ", "page_idx": 52}, {"type": "text", "text": "Observation F.12 (There are significantly more instances where the model benefits from increasing noisy shots). For the second category of results, which includes (WCC), (WCW), and (WWC), the average proportion across all datasets is 0.186, which is greater than that of the first category. This means that there is a larger proportion of samples where the model answers incorrectly at zero-shot but correctly as noisy shots increase. Namely, there are more instances where noisy shots have a positive effect. ", "page_idx": 52}, {"type": "text", "text": "Observation F.13 (The type of instances where the model produces consistent answers is dominant across all datasets). For the third category of results, namely (CCC) and (WWW), the average proportion across all datasets is the highest, at 0.776. This category of results does not allow for an assessment of the effect of noisy shots. ", "page_idx": 52}, {"type": "text", "text": "We also provide a case study of in-context learning using a varying number of noisy examples. We provide cases corresponding to some possible outcomes (i.e., [(CCC), (CCW), (CWC)]) for (0-shot, 1-shot, 3-shot) on the NoRa-Math Base-9 dataset with inaccurate easy-level noise in Tables. 42-44. ", "page_idx": 52}, {"type": "text", "text": "F.7 Variants of CD-CoT ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "Acquiring a human-annotated and clean rationale can be challenging in some practical scenarios. However, we would further clarify the relatively low cost and the necessity of this clean rationale to guarantee the effectiveness of robust reasoning. ", "page_idx": 52}, {"type": "text", "text": "Current LLMs cannot discern and remove noisy thoughts with their intrinsic denoising ability, even when enhanced with the self-denoising method. This is supported by the empirical observations ", "page_idx": 52}, {"type": "text", "text": "Question ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "In base-9, what is $25+18?$ Please reason it step by step. End the response with the result in \"Answer: result \" ", "page_idx": 53}, {"type": "text", "text": "0-shot (Correct) ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "To perform addition in base-9, we need to first understand the place values. In base-9, the place values range from right to left as follows: ones, nines, eighty-ones, and so on. Let\u2019s start with the ones place. 5 plus 8 is equal to 13. However, in base-9, we can only have digits from 0 to 8. Therefore, we need to regroup. Since 13 is greater than 9, we can subtract 9 from it and carry over 1 to the next place value (nines place). The remainder is 4. Now let\u2019s move on to the nines place. We have 1 (carry over) plus 2 plus 1 (from the nines place) which is equal to 4. Therefore, when we add 25 and 18 in base-9, we get 44. Answer: 44 ", "page_idx": 53}, {"type": "text", "text": "1-shot (Correct) ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "In base-9, the digits are \"012345678\". We have $5+8=13$ in base-10. Since we\u2019re in base-9, that exceeds the maximum value of 8 for a single digit. 13 mod $9=4$ , so the digit is 4 and the carry is 1. Next, we have $2+1+1$ (carry) $=4$ in base-10. 4 in base-9 is written as 4. Since there\u2019s no carry this time, we have no additional digits. So, the sum is 44 in base-9. Answer: 44 . ", "page_idx": 53}, {"type": "text", "text": "3-shot (Correct) ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "In base-9, the digits are \"012345678\". First, we add the digits in the rightmost place value, which is $5+8$ . We have $5+8=13$ in base-10. Since we\u2019re in base-9, the maximum value for a single digit is 8. 13 mod $9=4$ , so the digit is 4 and the carry is 1. Next, we add the digits in the next place value, which is 1 (from the carry) $+\\,2+1$ . We have $1+2+1=4$ in base 10. Again, 4 mod $9=4$ , so the digit is 4 and there is no carry. Therefore, the final result is 44. Answer: 44 . ", "page_idx": 53}, {"type": "text", "text": "Table 42: Case for (CCC): 0-shot correct, 1-shot correct, 3-shot correct. ", "page_idx": 53}, {"type": "text", "text": "Question ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "In base-9, what is $57\\!+\\!26?$ Please reason it step by step. End the response with the result in \"Answer: $\\boxed{r e s u l t}\\textcircled{\\cdot}$ ", "page_idx": 53}, {"type": "text", "text": "0-shot (Correct) ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "To add in base-9, we start by adding the digits in the rightmost column. In the units column, $7+6=13$ . Since we are in base-9, we need to regroup. We can rewrite 13 as 4 in the units column and carry-over 1 to the next column. In the next column, we add the carried-over 1 to the digits in that column. We have $1+5+2=8$ . So, the result in the next column is 8. Finally, we put the digits 8 and 4 together to get the final answer. Answer: 84 . ", "page_idx": 53}, {"type": "text", "text": "1-shot (Correct) ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "To add 57 and 26 in base-9, we\u2019ll start with the units digit. In base-9, the digits are $\"012345678\"$ . $7+6=13$ . Since 13 is greater than 8 (the maximum digit value in base-9), we need to regroup. We can write 13 as $13=1*9+4$ . Therefore, the units digit is 4 and there is a carry of 1 to the next column. Moving on to the next column, we have $5+2+1$ (the carried digit) $=8$ . 8 is a valid digit in base-9, so we do not need to regroup. The tens digit is 8. Therefore, the sum of 57 and 26 in base-9 is 84. Answer: 84 . ", "page_idx": 53}, {"type": "text", "text": "3-shot (Wrong) ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "In base-9, the digits are $\"012345678\"$ . We have $7+6=13$ . Since we\u2019re in base-9, we need to determine the digit and carry. 13 divided by 9 is 1 with a remainder of 4. So the digit is 4, and we carry 1. Next, we have $5+2+1=8$ . So the digit is 8 and there is no carry. A leading digit is not needed since there is no carry. Thus, the answer is 48. Answer: 48 . ", "page_idx": 53}, {"type": "text", "text": "Table 43: Case for (CCW): 0-shot correct, 1-shot correct, 3-shot wrong. ", "page_idx": 53}, {"type": "text", "text": "in Sec. 4. Hence, external supervision is necessary, which should be sufficient for denoising and accessible in practice. Existing methods with external supervision require (1) oracle feedback on the test question, (2) human feedback of errors on specific tokens or positions, or (3) expert knowledge to construct detailed descriptions of specific tasks. ", "page_idx": 53}, {"type": "text", "text": "By contrast, we believe that a clean CoT demonstration is more attainable and practical, which can be the minimal requirement for denoising-purpose prompting. With the proposed CD-CoT method, we show that LLMs can identify noisy thoughts by contrasting a noisy rationale with a clean one. ", "page_idx": 53}, {"type": "text", "text": "Besides, we build a self-supervised variant of CD-CoT that does not rely on manually crafted clean examples, i.e., this variant can get a clean rationale automatically. Specifically, we prompt LLM with a fixed question using the zero-shot CoT approach [39], allowing it to answer N times. We then select the responses with the correct answer based on the ground-truth label. ", "page_idx": 53}, {"type": "text", "text": "Then, we conduct experiments on two datasets, NoRa-Math Base-9 and NoRa-Symbolic Equal, with the default model (GPT-3.5-0613). The hyper-parameters are temperature $_{:=0.1}$ and top- $\\scriptstyle\\cdot\\mathbf{p}=1$ . We set $_\\mathrm{N=50}$ for NoRa-Math Base-9 and $N{=}100$ for NoRa-Symbolic Equal. Other settings are kept the same as default with the original CD-CoT. ", "page_idx": 53}, {"type": "text", "text": "Question ", "text_level": 1, "page_idx": 54}, {"type": "text", "text": "0-shot (Correct) ", "text_level": 1, "page_idx": 54}, {"type": "text", "text": "To add in base-9, we start by adding the digits in the rightmost place value and moving leftwards. In the units place, $7\\!+\\!5\\!=\\!12$ . However, in base-9, there are only digits from 0 to 8. Therefore, we need to do a \"carry\" to the next place value. The digit 12 is written as 13 in base-9, since the digit 12 in base-9 is represented as 10, followed by 11. Now, we move to the next place value, the 9s place. We have a carry of 1 from the units place, so the addition becomes $1{+}6{+}2=9$ . In base-9 notation, 9 is written as 10. Therefore, the sum of $67\\!+\\!25$ in base-9 is 103. Answer: 103 . ", "page_idx": 54}, {"type": "text", "text": "1-shot (Wrong) ", "text_level": 1, "page_idx": 54}, {"type": "text", "text": "In base-9, the digits are \"012345678\". We have $7+5=12$ in base-10. Since we\u2019re in base-9, that doesn\u2019t exceed the maximum value of 8 for a single digit. 12 mod $9=3$ , so the digit is 3 and the carry is 1. We have $1+6+2=9$ in base-10. 9 mod $9=0$ , so the digit is 0 and the carry is 1. We have $1+1+0=2$ in base-10. So the final digit is 2. Therefore, the answer is 203. Answer: 203 . ", "page_idx": 54}, {"type": "text", "text": "3-shot (Correct) ", "text_level": 1, "page_idx": 54}, {"type": "text", "text": "In base-9, the digits are $\"012345678\"$ . We have $7+5=12$ in base-10. Since we\u2019re in base-9, that exceeds the maximum value of 8 for a single digit. $12{\\bmod{9}}=3$ , so the digit is 3 and the carry is 1. We have $6+2+1=9$ in base 10. 9 mod 9 $=0$ , so the digit is 0 and the carry is 1. A leading digit is 1. So the answer is 103. Answer: 103 . ", "page_idx": 54}, {"type": "text", "text": "Table 44: Case for (CWC): 0-shot correct, 1-shot wrong, 3-shot correct. ", "page_idx": 54}, {"type": "table", "img_path": "FbuODM02ra/tmp/ed2104578519f9d9a9881676b96f35808c4ab76f05b4bc94b719f3095c0cd2d5.jpg", "table_caption": ["Table 45: Performances of different variants of CD-CoT. "], "table_footnote": [], "page_idx": 54}, {"type": "text", "text": "On the NoRa-Math Base-9 dataset, the LLM answers incorrectly in the initial 20 attempts. When we increase N to 50, the LLM gets it right twice. We use one of the self-generated correct rationales to execute CD-CoT, testing the effect of LLM\u2019s self-generated rationale. On the NoRa-Symbolic Equal dataset, we need to increase N to 100 to get a correct self-generated response. ", "page_idx": 54}, {"type": "text", "text": "Regarding the empirical results in Tab. 45, we derive the following detailed analysis. ", "page_idx": 54}, {"type": "text", "text": "Observation F.14 (The self-supervised variant of CD-CoT achieves enhancements across various tasks). On NoRa-Math Base-9, the average accuracy is increased by 26.8 percentage points. On NoRa-Symbolic Equal, it gains an average increase of 3.9 points in accuracy. ", "page_idx": 54}, {"type": "text", "text": "Observation F.15 (LLM struggles to generate a clean and correct rationale on its own). Considering the fact that the model requires 50 attempts on the simple NoRa-Math Base-9 task to obtain the correct result, it indicates that the LLM struggles to generate a clean and correct rationale on its own. Additionally, experiments on the NoRa-Symbolic Equal indicate that as the task\u2019s difficulty increases, it becomes more challenging for the model to resolve it by relying solely on its inherent capabilities. ", "page_idx": 54}, {"type": "text", "text": "Observation F.16 (Self-generated clean examples are not as effective as manually designed clean examples). Although the self-supervised variant of CD-CoT acquires improvement across tasks, its gains on the difficult NoRa-Symbolic Equal tasks are quite modest, with an average increase of only 3.9 points. Conversely, the original CD-CoT registers an average accuracy enhancement of 39.5 points on NoRa-Math Base-9 tasks and 15.1 points on NoRa-Symbolic Equal tasks, respectively. We can conclude that self-generated clean examples can provide some guidance to the LLM on simpler tasks, but they are not as effective as manually designed clean examples. On more difficult tasks, the guidance provided by self-generated clean examples is very limited, and these tasks require additional supervision to steer the model effectively. ", "page_idx": 54}, {"type": "text", "text": "In addition to the self-supervised CD-CoT, exploring strategies like using contrasting noisy examples or incorporating an external knowledge base, possibly through a retrieval-augmented denoising framework, may offer significant advances in automation and robustness of reasoning. We believe that these are promising directions that deserve further investigation. ", "page_idx": 54}, {"type": "text", "text": "F.8 New Datasets and Large-scale Real Problems ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "Recall that the constructed NoRa benchmark covers five prevailing datasets from three different domains. The current LLMs present significant vulnerability to noisy rationales in all five datasets of NoRa, while the proposed method CD-CoT has shown advanced and consistent robustness against noisy rationales. Specifically, ", "page_idx": 55}, {"type": "text", "text": "\u2022 Robustness with different datasets: CD-CoT consistently outperforms other methods in all five datasets in NoRa.   \n\u2022 Robustness with different noise levels: The results shown in Tab. 8 demonstrate the remarkable robustness of CD-CoT to varying noise levels. Across the NoRa-Math, NoRa-Symbolic, and NoRa-Commonsense tasks, the performance decline of CD-CoT remains modest as the noise level increases.   \n\u2022 Robust with different LLMs: The results in Tab. 11 further indicate that CD-CoT substantially improves over all three other baselines on the more powerful LLMs. Even on the relatively smaller Mistral-8x7B, CD-CoT significantly outperforms the other baselines on most tasks. ", "page_idx": 55}, {"type": "text", "text": "Note that Sec. 3 introduces a general framework for generating noisy rationales with existing datasets. This means, more datasets can be integrated into NoRa if necessary in future research. Here, we conduct additional experiments to generalize CD-CoT to three new datasets that are not covered in NoRa, including GSM-8K [12], Blocksworld [82], and BIG-Bench Hard Dyck Languages [72]. Specifically, ", "page_idx": 55}, {"type": "text", "text": "\u2022 GSM-8K: A math dataset of linguistically diverse grade school math word problems.   \n\u2022 Blocksworld: A planning dataset simulating block stacking tasks.   \n\u2022 BIG-Bench Hard Dyck Languages: A symbolic dataset designed for predicting the sequence of closing parentheses in a Dyck-4 word. ", "page_idx": 55}, {"type": "text", "text": "Then, we generate noisy rationales and compare the following setups: ", "page_idx": 55}, {"type": "text", "text": "\u2022 Zero-shot: Base model with no demonstration.   \n\u2022 CoT (clean rationales): Base model with 3 clean demonstrations.   \n\u2022 CoT (noisy rationales): Base model with 3 noisy demonstrations.   \n\u2022 CD-CoT (noisy rationales): Base model with 3 noisy demonstrations and our CD-CoT method. ", "page_idx": 55}, {"type": "text", "text": "The reasoning accuracy in Tab. 46 shows that CD-CoT consistently outperforms the zero-shot setting and standard CoT prompting under noisy rationales. This is consistent with the findings in Sec. 5.2, showing CD-CoT\u2019s strong capability of generalization to new datasets. ", "page_idx": 55}, {"type": "table", "img_path": "FbuODM02ra/tmp/798e1a6426e65a8606c5f3c3602a89ff888b4fa7eb1626fe3dc386665297220a.jpg", "table_caption": ["Table 46: Performances of different methods on new datasets. "], "table_footnote": [], "page_idx": 55}, {"type": "text", "text": "Along with the introduction of new datasets, we also examine the issue of noisy rationale in largescale real-world scenarios, particularly in conversational question-answering (QA) tasks. While our primary focus is on reasoning tasks, we discuss the differences and relationships between these areas. ", "page_idx": 55}, {"type": "text", "text": "Conversational QA tasks are usually conducted in a zero-shot setting, where users directly input the task instructions and hardly provide examples to the model. NoRa, on the other hand, considers the in-context learning scenario, where the model is given a few examples before answering questions. In-context learning can significantly enhance the model\u2019s performance on complex or new tasks. The purpose of NoRa is to assess the model\u2019s robustness to noisy examples in in-context learning. ", "page_idx": 55}, {"type": "text", "text": "However, for multi-turn conversational QA tasks, the content of previous answers can be considered as context for the model when answering the next question. Therefore, by evaluating how noisy context affects a model\u2019s subsequent question responses, we can explore the issue of noisy rationales within the realm of conversational QA tasks. ", "page_idx": 56}, {"type": "text", "text": "We conduct experiments on MT-Bench [108] to investigate the noisy rationale problem in QA scenarios. MT-Bench is a multi-turn question collection and covers eight principal types of tasks: Writing, Roleplay, Extraction, Reasoning, Math, Coding, Knowledge I, and Knowledge II. ", "page_idx": 56}, {"type": "text", "text": "For clean context, we directly use the original question to prompt the model, and the first round of dialogue generated serves as the clean context. For noisy context, we prompt the model to insert some irrelevant information as noise when answering the first turn\u2019s question, and the first round of dialogue serves as the noisy context. We then continue to use the second turn\u2019s question along with the generated context to prompt the model to obtain the target response. ", "page_idx": 56}, {"type": "text", "text": "We conduct conversational QA tasks on GPT-3.5-turbo-0613 and score the generated responses with GPT-4. The GPT-4 is to score the responses from both the first and second turns separately. ", "page_idx": 56}, {"type": "table", "img_path": "FbuODM02ra/tmp/361c9f14ac4a77d8c2ef0351d42033ff6afe70c741b112582b1f6697ce6b3c24.jpg", "table_caption": [], "table_footnote": [], "page_idx": 56}, {"type": "table", "img_path": "FbuODM02ra/tmp/c28777fe2da0bd1ce3beace9e233c9a0fab516d0e0ef062989c8da1682dadd48.jpg", "table_caption": ["Table 47: Average scores with different contexts. ", "Table 48: Average scores across types of tasks with different contexts. "], "table_footnote": [], "page_idx": 56}, {"type": "text", "text": "As the results shown in Tab. 47, the noisy context decreases both the scores (quality) of the first-round and second-round QA. Specifically, the average scores across different types of tasks are shown in Tab. 48. Notably, a $0.48\\%$ decrease in average score appears in the second response, which is the final target of the conversation. ", "page_idx": 56}, {"type": "text", "text": "In addition, we provide a case study for illustration. As shown in Tab. 49, the noisy context in round 1 leads to a bad, noisy response in round 2. ", "page_idx": 56}, {"type": "text", "text": "In the clean context, the model provides an accurate analysis and answer to the initial question. When the correct round serves as the context for the second question, the model consistently delivers a precise analysis and response. ", "page_idx": 56}, {"type": "text", "text": "In the noisy context, the model\u2019s response and analysis of the initial question contain errors. Specifically, as shown in Tab. 49, it incorrectly counts multiple integers for both case 1 and case 2, culminating in a wrong answer for the first question. When the flawed round serves as the context for the second question, the model generates the same intermediate results as in the first round and ultimately provides an incorrect answer to the second question. ", "page_idx": 56}, {"type": "text", "text": "In summary, during multi-turn conversational QA tasks, the model\u2019s response to the current question is influenced by the preceding dialogue content (i.e., the context), particularly for tasks that require reasoning and mathematical computations. ", "page_idx": 56}, {"type": "text", "text": "Clean Context: ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "Round 1 ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "User: How many integers are in the solution of the inequality $|x+5|<10?$ ", "page_idx": 57}, {"type": "text", "text": "Assistant: To solve the inequality $|x+5|<10$ , we can break it down into two separate inequalities: 1. $x+5<10,$ 2. $-(x+5)<10$ . Solving the first inequality: $x+5<10$ . Subtracting 5 from both sides: $\\bar{x}<5$ . Solving the second inequality: $-(x+5)<\\bar{10}$ . Multiplying both sides by $^{-1}$ (which reverses the inequality): $x+5>-10$ . Subtracting 5 from both sides: $x>-15$ . Combining the two solutions, we have $-15<x<5$ . The integers in this range are -14, -13, -12, -11, $-10,-9,-8,-7,-6,-5,-4,-3,-2,-1,0,1,2,3,4.$ Therefore, there are 19 integers in the solution of the inequality $|x+5|<10$ . ", "page_idx": 57}, {"type": "text", "text": "Round 2 ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "User: What about $|x+10|<5?$ ", "page_idx": 57}, {"type": "text", "text": "Assistant: To solve the inequality $|x+10|<5$ , we can break it down into two separate inequalities: 1. $x+10<5,2$ $-(x+10)<5$ . Solving the first inequality: $x+10<5$ . Subtracting 10 from both sides: $x<-5$ . Solving the second inequality: $-(x+10)<5$ . Multiplying both sides by $^{-1}$ (which reverses the inequality): $x+10>-5$ . Subtracting 10 from both sides: $x>-15$ . Combining the two solutions, we have $-15<x<-5$ . The integers in this range are -14, -13, -12, -11, -10, -9, -8, -7, -6. Therefore, there are 9 integers in the solution of the inequality $|x+10|<5$ . ", "page_idx": 57}, {"type": "text", "text": "Noisy Context: ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "User: How many integers are in the solution of the inequality $|x+5|<10?$   \nAssistant: The solution to the inequality $|x+5|<{\\\\\\dot{10}}$ consists of all the integers that satisfy the inequality. To find the solution, we can break it down into two cases: Case 1: $x+5<10$ . In this case, we have $x+5<10$ , which means $x<5$ . The integers that satisfy this inequality are $-4,\\,-3$ , -2, -1, $0,\\,1,\\,2,\\,3,$ , and 4. Case 2: $-(x+5)<10$ . In this case, we have $-(x+5)<10$ , which means $x+5>-10$ . By rearranging the inequality, we get $x>-15$ . The integers that satisfy this inequality are -14, -13, -12, -11, -10, -9, -8, -7, -6, -5, -4, -3, -2, -1, 0, 1, 2, 3, 4, $5$ , and so on. Combining the solutions from both cases, we find that the integers that satisfy the inequality $\\overline{{|x+5|<10}}$ are -14, -13, -12, -11, -10, -9, -8, -7, -6, -5, -4, -3, -2, -1, 0, 1, 2, 3, 4, and 5. So, there are 21 integers in the solution. ", "page_idx": 57}, {"type": "text", "text": "Round 2 ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "User: What about $|x+10|<5?$ ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "Assistant: The solution to the inequality $|x+10|<5$ consists of all the integers that satisfy the inequality. To find the solution, we can break it down into two cases: Case 1: $x+10<5$ . In this case, we have $x+1{\\dot{0}}<5$ , which means $x<-5$ . The integers that satisfy this inequality are -6, -7, -8, -9, -10, and so on. Case 2: $-(x+10)<5$ . In this case, we have $-(x+{\\bar{10}})<5$ , which means $\\bar{x}+10^{\\cdot}>-5$ . By rearranging the inequality, we get $x>-15$ . The integers that satisfy this inequality are -14, -13, -12, -11, -10, -9, -8, -7, -6, $^{-5}$ , -4, $^{-3}$ , $^{-2}$ , -1, 0, 1, 2, 3, 4, 5, and so on. Combining the solutions from both cases, we find that the integers that satisfy the inequality $|x+10|<5$ are -14, -13, -12, -11, -10, -9, -8, -7, -6, -5, -4, -3, -2, -1, 0, 1, 2, 3, and 4. So, there are 19 integers in the solution. ", "page_idx": 57}, {"type": "text", "text": "F.9 Rephrased Examples of Different Denoising Methods ", "text_level": 1, "page_idx": 58}, {"type": "text", "text": "Data Denoising. Here, we show the rephrased (denoising) examples using different robust methods. ", "page_idx": 58}, {"type": "text", "text": "Medium-level, irrelevant examples: \u2022 NoRa-Math Base-9 (Tab. 50) \u2022 NoRa-Commonsense (Tab. 52) ", "page_idx": 58}, {"type": "text", "text": "Medium-level, inaccurate examples: \u2022 NoRa-Symbolic Longer (Tab. 51) \u2022 NoRa-Commonsense (Tab. 53) ", "page_idx": 58}, {"type": "text", "text": "Original Noisy Rationale ", "text_level": 1, "page_idx": 58}, {"type": "text", "text": "In base-9, the digits are $\"012345678\"$ . Nine is the atomic number of fluorine in chemistry, a highly reactive and abundant element. We have $6+7=13$ in base-10. Thirteen is the atomic number of aluminium in chemistry, a post-transition metal that\u2019s widely used for its lightness and resistance to corrosion. Since we\u2019re in base-9, that exceeds the maximum value of 8 for a single digit.13 mod $9=4$ , so the digit is 4 and the carry is 1. We have $8+5+1=14$ in base 10. 14 mod $9=5$ , so the digit is 5 and the carry is 1. A leading digit is 1. The \u2019one-gene-one-enzyme hypothesis\u2019 in genetics proposes that each gene is responsible for producing a single enzyme that influences a specific metabolic pathway. So the answer is 154. Four is the atomic number of beryllium in chemistry, which is used as an alloying agent in producing beryllium copper. Answer: 154} ", "page_idx": 58}, {"type": "table", "img_path": "FbuODM02ra/tmp/6e4e04db2165339136a6b8a0cf5600d2e13300c18b33f3c86c68628480caa549.jpg", "table_caption": [], "table_footnote": ["Table 50: Rephrased rationales by different robust methods on NoRa-Math Base-9 irrelevant medium task. "], "page_idx": 58}, {"type": "text", "text": "ince command is turn opposite right and run around right twice, we should consider Step1: \"turn opposite right\" firstly, \"turn opposite right\" means the agent needs to turn right twice. \u2019right\u2019 corresponds to the command I_TURN_RIGHT. The term \u2019left\u2019 translates to I_TURN_RIGHT. The term \u2019opposite\u2019 implies a 180-degree turn, which requires the agent to perform the turn right twice. The term \u2019around\u2019 implies a 360-degree loop, which requires I_TURN_LEFT, I_TURN_RIGHT, I_TURN_RIGHT. Therefore, the action sequence is I_TURN_RIGHT I_TURN_RIGHT. The term \u2019around\u2019 implies a 360-degree loop, which requires I_TURN_LEFT, I_TURN_LEFT, I_TURN_LEFT. Now, we consider Step2:\"run around right twice\", \"run around right\" means the agent needs to turn right and run, and repeat this action sequence four times to complete a 360-degree loop. \u2019right\u2019 corresponds to the command I_TURN_RIGHT. \u2019run\u2019 translates to I_RUN. The term \u2019look\u2019 corresponds to the command I_WALK. The term \u2019around\u2019 implies to complete a 360-degree loop by repeating the sequence of I_TURN_RIGHT, I_RUN four times. The term \u2019opposite\u2019 implies a 180-degree turn, which requires I_TURN_RIGHT. Therefore, the action sequence is I_TURN_RIGHT I_RUN I_TURN_RIGHT I_RUN I_TURN_RIGHT I_RUN I_TURN_RIGHT I_RUN. Since we need do twice in command \"run around right twice\", this entire sequence is repeated 2 times, so the action sequence to \"run around right twice\" is :I_TURN_RIGHT I_RUN I_TURN_RIGHT I_RUN I_TURN_RIGHT I_RUN I_TURN_RIGHT I_RUN I_TURN_RIGHT I_RUN I_TURN_RIGHT I_RUN I_TURN_RIGHT I_RUN I_TURN_RIGHT I_RUN. Above all \u2013 So, final answer is OUT:I_TURN_RIGHT I_TURN_RIGHT I_TURN_RIGHT I_RUN I_TURN_RIGHT I_RUN I_TURN_RIGHT I_RUN I_TURN_RIGHT I_RUN I_TURN_RIGHT I_RUN I_TURN_RIGHT I_RUN I_TURN_RIGHT I_RUN I_TURN_RIGHT I_RUN. ", "page_idx": 59}, {"type": "table", "img_path": "FbuODM02ra/tmp/da893091b7d2a06f363335f18a3b0b73126418b0d63d5c303dd423322552b6dd.jpg", "table_caption": [], "table_footnote": ["Table 51: Rephrased rationales by different robust methods on NoRa-Symbolic Longer inaccurate medium task. "], "page_idx": 59}, {"type": "table", "img_path": "FbuODM02ra/tmp/470023e8f43695ad5d4eaa5e5aa8de9030cf13c1479b97f7c83828b51233187d.jpg", "table_caption": ["Table 52: Rephrased rationales by different robust methods on NoRa-Commonsense irrelevant medium task. "], "table_footnote": [], "page_idx": 60}, {"type": "table", "img_path": "FbuODM02ra/tmp/3b582d83e2a7e9d82284531f61e21d61b8389b72adce65dfc2f6ec984fb688cb.jpg", "table_caption": ["Table 53: Rephrased rationales by different robust methods on NoRa-Commonsense inaccurate medium task. "], "table_footnote": [], "page_idx": 60}, {"type": "text", "text": "G Further Case Study ", "text_level": 1, "page_idx": 61}, {"type": "text", "text": "In this section, we provide a complete example for NoRa-Math Base-9 of irrelevant hard-level noise in Figs. 10-14. This instance is composed of five figures. The initial three figures present contrastive rephrasing prompts and the resulting rephrased outcomes for three distinct noisy examples. Following that, the last two figures exhibit the constructed two diverse contexts along with their respective reasoning results. ", "page_idx": 61}, {"type": "image", "img_path": "FbuODM02ra/tmp/6bb1bb09c0004feccce63d8c091f31449ce4b0a0767e4921522f2f13f269ba2b.jpg", "img_caption": ["Figure 10: NoRa-Math Base-9 Irrelevant Hard: Rephrasing Prompt-1. "], "img_footnote": [], "page_idx": 61}, {"type": "image", "img_path": "FbuODM02ra/tmp/09c79cf4c313c4ea5bb129fb0fd7700da3104697401dfa22b33a580158454797.jpg", "img_caption": [], "img_footnote": [], "page_idx": 61}, {"type": "image", "img_path": "FbuODM02ra/tmp/5df15f63b472ffcfc78c9342075ed23970fc0e5af5ada17af18fe3c5477cfb59.jpg", "img_caption": ["Figure 11: NoRa-Math Base-9 Irrelevant Hard: Rephrasing Prompt-2. "], "img_footnote": [], "page_idx": 61}, {"type": "image", "img_path": "FbuODM02ra/tmp/ae09cf7d6f69ec3db66ce7dffbb0d30ef98d5ea68cd57a17dd1e1f5fc77ebd09.jpg", "img_caption": [], "img_footnote": [], "page_idx": 62}, {"type": "image", "img_path": "FbuODM02ra/tmp/8b494d9cccd78fe9e72f4fa3978353433ff8aba1e7de7920d36dcf2fa963744f.jpg", "img_caption": ["Figure 12: NoRa-Math Base-9 Irrelevant Hard: Rephrasing Prompt-3. "], "img_footnote": [], "page_idx": 62}, {"type": "image", "img_path": "FbuODM02ra/tmp/76f8df7b1d24c582d0ea059e41083bd48a4a56315514276e0709a273269e4e3b.jpg", "img_caption": [], "img_footnote": [], "page_idx": 62}, {"type": "image", "img_path": "FbuODM02ra/tmp/21e742745f02c39d09237c951846ac80bf522b447344739d0bada957e84c48b4.jpg", "img_caption": ["Figure 13: NoRa-Math Base-9 Irrelevant Hard: In-context Learning-1. "], "img_footnote": [], "page_idx": 62}, {"type": "image", "img_path": "FbuODM02ra/tmp/6ab0ec84e58839d4bd082740bc044ff12d808b5a5211934ccc0a5917718d061b.jpg", "img_caption": ["Figure 14: NoRa-Math Base-9 Irrelevant Hard: In-context Learning-2. "], "img_footnote": [], "page_idx": 62}, {"type": "text", "text": "H NeurIPS Checklist ", "text_level": 1, "page_idx": 63}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 63}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 63}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 63}, {"type": "text", "text": "Justification: We have summarized the position and key contributions of the paper in the abstract and introduction parts. ", "page_idx": 63}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 63}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 63}, {"type": "text", "text": "Justification: The limitations are discussed in Appendix A. ", "page_idx": 63}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 63}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 63}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 63}, {"type": "text", "text": "Justification: The assumptions and proof are provided in Appendix D. ", "page_idx": 63}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 63}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 63}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 63}, {"type": "text", "text": "Justification: The technical details of implementation are introduced in Appendix E. ", "page_idx": 63}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 63}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 63}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 63}, {"type": "text", "text": "Justification: The source files are publicly available at: https://github.com/ tmlr-group/NoisyRationales. ", "page_idx": 63}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 63}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 63}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 63}, {"type": "text", "text": "Justification: The experiment setups are introduced in Appendix F, and technical details of implementation are introduced in Appendix E. ", "page_idx": 63}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 63}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 63}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 63}, {"type": "text", "text": "Justification: We conduct evaluations on 300 questions for each task and repeat the reasoning 5 times for each question to obtain consistent and reliable results. The empirical results with mean and std values are provided in Tab. 33. ", "page_idx": 63}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 63}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 63}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 63}, {"type": "text", "text": "Justification: We employ GPT-3.5-turbo-0613 [17] as our base LLM for the analyses presented in this study. In addition, we conduct evaluations on three supplementary models, including Gemini-Pro [76], Llama2-70B [79], and Mixtral-8x7B [33]. ", "page_idx": 64}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 64}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 64}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 64}, {"type": "text", "text": "Justification: We have carefully checked the NeurIPS Code of Ethics and confirmed that our paper obeys it. ", "page_idx": 64}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 64}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 64}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 64}, {"type": "text", "text": "Justification: The broader impacts are introduced in Appendix A. We believe that this paper does not raise any negative societal impacts or ethical concerns. ", "page_idx": 64}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 64}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 64}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 64}, {"type": "text", "text": "Justification: This paper does not release new models. The constructed NoRa dataset is based on existing datasets. ", "page_idx": 64}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 64}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 64}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 64}, {"type": "text", "text": "Justification: In the paper, we have introduced the resources of the models and datasets used. ", "page_idx": 64}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 64}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 64}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 64}, {"type": "text", "text": "Justification: We construct the NoRa dataset. Its details are in Appendix C. ", "page_idx": 64}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 64}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 64}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 64}, {"type": "text", "text": "Justification: The paper is not about crowdsourcing experiments or research with human subjects. ", "page_idx": 64}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 64}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 64}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 64}, {"type": "text", "text": "Justification: The paper is not about research with human subjects. ", "page_idx": 64}]