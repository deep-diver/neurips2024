[{"figure_path": "BJv1t4XNJW/tables/tables_9_1.jpg", "caption": "Table 1: Performance on CATER Snitch Localization Task.", "description": "This table presents the results of the CATER Snitch Localization task, which evaluates the ability of models to predict the location of a hidden object in a 3D environment. The table shows the top-1 and top-5 accuracy for four different models: Single State SSM, SlotTransformer, SlotSSM, and OC-SlotSSM. The results are broken down into two categories: \"No Pre-train\", where models are trained directly on the task, and \"Pre-train\", where models are first pre-trained on a reconstruction objective and then fine-tuned on the task.", "section": "7.4 3D Visual Reasoning"}, {"figure_path": "BJv1t4XNJW/tables/tables_19_1.jpg", "caption": "Table 2: Hyperparameters of our model used in our experiments.", "description": "This table lists the hyperparameters used in the experiments for both the Blinking Color Balls dataset (using SlotSSMs) and the MOVi-A dataset (using OC-SlotSSMs).  It details settings for general training parameters (batch size, training steps, sequence length, optimizer, weight decay, learning rate), Slot Encoder parameters (input tokenizer, encoder type, applied layers, hidden size, dropout, heads), SlotSSM parameters (hidden size, number of slots, SSM model, state size, state expansion factor), and Slot Mixer parameters (dropout and heads).", "section": "C Additional Implementation Details"}, {"figure_path": "BJv1t4XNJW/tables/tables_20_1.jpg", "caption": "Table 3: The CNN encoder architecture used for object-centric learning.", "description": "This table presents the architecture of the Convolutional Neural Network (CNN) encoder used in the object-centric learning experiments.  It details the specifications for each convolutional layer, including kernel size, stride, padding, number of channels, and activation function.  This encoder processes image inputs to extract features before they are fed into the rest of the object-centric model.", "section": "7.3 Unsupervised Object-Centric Learning"}, {"figure_path": "BJv1t4XNJW/tables/tables_20_2.jpg", "caption": "Table 4: Spatial broadcast decoder architecture for image reconstruction in object-centric learning, it outputs RGB and alpha-mixing logits.", "description": "This table details the architecture of the spatial broadcast decoder used in the object-centric learning experiments.  It shows the layers, kernel size, stride, padding, number of channels, and activation function used in each layer of the decoder. The decoder takes slot representations as input and generates object images and alpha masks, which are combined to create the final reconstructed image.", "section": "5.1 Object-Centric SlotSSMs (OC-SlotSSMs)"}, {"figure_path": "BJv1t4XNJW/tables/tables_22_1.jpg", "caption": "Table 5: Depth Estimation MSE (\u2193) on Different Datasets.", "description": "This table shows the mean squared error (MSE) for depth estimation on three different datasets: UT Egocentric, Waymo, and TikTok.  The lower the MSE, the better the performance. Two models are compared: SAVi++ and OC-SlotSSM (Ours).  The results indicate that OC-SlotSSM achieves lower MSE than SAVi++ on all datasets, demonstrating its effectiveness in depth estimation.", "section": "7.3 Unsupervised Object-Centric Learning"}]