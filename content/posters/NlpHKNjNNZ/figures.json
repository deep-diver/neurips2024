[{"figure_path": "NlpHKNjNNZ/figures/figures_0_1.jpg", "caption": "Figure 1: Overview. We present PGT-Aug, a novel cost-effective pipeline that generates and augments pseudo-LiDAR samples (from miniatures and web videos) to effectively reduce the performance gap between majority-class vs. minority-class objects.", "description": "The figure illustrates the difference between existing methods and the proposed method (PGT-Aug) for augmenting pseudo-LiDAR point clouds. Existing methods copy and paste ground truth LiDAR points into scenes, resulting in limited placeability. PGT-Aug generates pseudo-LiDAR samples from low-cost miniatures or real-world videos, enabling more flexible placeability.", "section": "1 Introduction"}, {"figure_path": "NlpHKNjNNZ/figures/figures_2_1.jpg", "caption": "Figure 2: Overview of Pseudo GT (PGT)-Aug Framework. Given multiview images, we first reconstruct their volumetric representations (Section 3.1). We post-process RGB point clouds using spatial rearrangement and LiDAR intensity simulator (Section 3.2), producing pseudo-LiDAR point clouds. Such points are stored in a psuedo LiDAR bank, and we paste the sampled objects into the target scene with the proposed augmentation scheme (Section 3.3).", "description": "This figure illustrates the Pseudo Ground Truth Augmentation (PGT-Aug) framework.  It shows the pipeline's three main stages: (1) Volumetric 3D Instance Collection uses multiview images from miniatures and web videos to reconstruct 3D volumetric representations of objects. (2) Object-level Domain Alignment transforms the RGB point clouds into pseudo-LiDAR point clouds by simulating spatial and intensity characteristics. (3) Pseudo LiDAR Point Clouds Augmentation integrates the pseudo-LiDAR points into real scenes using a hybrid context-aware placement method.  The framework generates and stores pseudo-LiDAR points in a bank for later augmentation of real LiDAR data.", "section": "3 Method"}, {"figure_path": "NlpHKNjNNZ/figures/figures_4_1.jpg", "caption": "Figure 3: Effect of Reprojection on Different Datasets.", "description": "This figure shows the effect of reprojection on different LiDAR datasets (nuScenes, KITTI, and Lyft).  The leftmost panel displays the original RGB point cloud.  The remaining panels demonstrate how the points are reprojected into a range view representation, considering the unique field of view and azimuth resolution of each LiDAR sensor. This process simulates the realistic data variations observed in real-world LiDAR data, thereby reducing the domain gap between simulated and real-world LiDAR data.", "section": "3.2 Object-level Domain Alignment"}, {"figure_path": "NlpHKNjNNZ/figures/figures_4_2.jpg", "caption": "Figure 4: Region Matching Loss.", "description": "This figure illustrates the region matching loss used in the LiDAR intensity simulation module.  It shows how generated points (GDrgb\u2192Dintensity(Prgb)) and real-world points (Pintensity) are grouped into ball patches.  The Hungarian matching algorithm finds the optimal assignment of these patches, and the loss is calculated based on the intensity differences between corresponding pairs of patches. This helps align the intensity values between the generated pseudo-LiDAR points and the real LiDAR points, reducing the domain gap between them.", "section": "3.2 Object-level Domain Alignment"}, {"figure_path": "NlpHKNjNNZ/figures/figures_5_1.jpg", "caption": "Figure 5: Comparison of Ground-only and Ground+Map Scene Composition. Blue and Pink-colored points denote the feasible location of insertion derived from (a) ground-only and (b) ground+map synthesized insertions, respectively.", "description": "This figure compares two different approaches for inserting synthetic objects into a LiDAR point cloud scene: ground-only composition and ground+map composition.  The ground-only method uses only ground estimations to determine feasible insertion areas, while the ground+map approach combines ground estimations with map information for more realistic placement. The figure shows that the ground+map method provides a broader range of feasible insertion areas and more realistic scene compositions than the ground-only approach.", "section": "3.3 Pseudo LiDAR Point Clouds Augmentation"}, {"figure_path": "NlpHKNjNNZ/figures/figures_5_2.jpg", "caption": "Figure 6: Examples of generated pseudo-LiDAR point samples with different orientations and ranges given reconstructed 3D volumetric representations.", "description": "This figure shows examples of generated pseudo-LiDAR point samples.  The samples demonstrate the variation in orientation (object heading) and range (distance from the sensor) that can be achieved using the proposed PGT-Aug method. This variety is crucial for effectively augmenting the training data and improving the performance of 3D object detectors on minority classes.", "section": "4 Experiments"}, {"figure_path": "NlpHKNjNNZ/figures/figures_16_1.jpg", "caption": "Figure 1: Overview. We present PGT-Aug, a novel cost-effective pipeline that generates and augments pseudo-LiDAR samples (from miniatures and web videos) to effectively reduce the performance gap between majority-class vs. minority-class objects.", "description": "This figure provides a high-level comparison between existing methods and the proposed PGT-Aug method for data augmentation in LiDAR-based 3D object detection.  Existing methods typically involve copying and pasting ground truth LiDAR points from other scenes, which has limitations in terms of sample diversity and suitable placement. In contrast, PGT-Aug uses pseudo-LiDAR point clouds generated from low-cost miniatures or real-world videos, offering greater flexibility and diversity. The figure visually illustrates the difference in input scene placeability between existing methods and PGT-Aug, highlighting the latter's superior flexibility.", "section": "1 Introduction"}, {"figure_path": "NlpHKNjNNZ/figures/figures_17_1.jpg", "caption": "Figure 8: Dataset Collection. We demonstrate our collection of miniature images and crawled web videos.", "description": "This figure shows a collection of images used in the paper's experiments.  The top half shows various miniature vehicles (cars, trucks, motorcycles, bicycles, construction equipment), while the bottom half displays images of real-world vehicles captured from web sources. These images were used to generate 3D volumetric representations of minority classes for data augmentation to address class imbalance in 3D object detection. The diversity of vehicles in the images helps to enrich the training dataset and improve the model's ability to detect these less frequent objects.", "section": "A.1 Dataset Details"}, {"figure_path": "NlpHKNjNNZ/figures/figures_18_1.jpg", "caption": "Figure 2: Overview of Pseudo GT (PGT)-Aug Framework. Given multiview images, we first reconstruct their volumetric representations (Section 3.1). We post-process RGB point clouds using spatial rearrangement and LiDAR intensity simulator (Section 3.2), producing pseudo-LiDAR point clouds. Such points are stored in a psuedo LiDAR bank, and we paste the sampled objects into the target scene with the proposed augmentation scheme (Section 3.3).", "description": "This figure illustrates the Pseudo Ground Truth Augmentation (PGT-Aug) framework.  It shows the three main steps: 1) Volumetric 3D instance reconstruction from multi-view images using a 2D-to-3D view synthesis model; 2) Object-level domain alignment through spatial rearrangement and LiDAR intensity simulation; and 3) Hybrid context-aware placement of the generated pseudo-LiDAR point clouds into the target scene using ground and map information. The generated point clouds are stored in a PGT bank for later use.", "section": "3 Method"}, {"figure_path": "NlpHKNjNNZ/figures/figures_19_1.jpg", "caption": "Figure 2: Overview of Pseudo GT (PGT)-Aug Framework. Given multiview images, we first reconstruct their volumetric representations (Section 3.1). We post-process RGB point clouds using spatial rearrangement and LiDAR intensity simulator (Section 3.2), producing pseudo-LiDAR point clouds. Such points are stored in a psuedo LiDAR bank, and we paste the sampled objects into the target scene with the proposed augmentation scheme (Section 3.3).", "description": "This figure illustrates the Pseudo Ground Truth Augmentation (PGT-Aug) framework.  It shows the process of collecting multi-view images, reconstructing volumetric representations using a 2D-to-3D view synthesis model, aligning object-level domains with LiDAR intensity simulation, and placing the resulting pseudo-LiDAR point clouds into target scenes using a hybrid context-aware placement method.  The framework involves three key steps: Volumetric Instance Collection, Object-level Domain Alignment, and Pseudo LiDAR Point Clouds Augmentation.  The generated point clouds are stored in a pseudo LiDAR bank for later use in augmentation.", "section": "3 Method"}, {"figure_path": "NlpHKNjNNZ/figures/figures_22_1.jpg", "caption": "Figure 1: Overview. We present PGT-Aug, a novel cost-effective pipeline that generates and augments pseudo-LiDAR samples (from miniatures and web videos) to effectively reduce the performance gap between majority-class vs. minority-class objects.", "description": "This figure provides a high-level comparison of existing data augmentation methods and the proposed PGT-Aug method. Existing methods typically involve copying and pasting LiDAR points from other scenes.  This approach is limited by the lack of sample diversity and suitable placement for minority classes. In contrast, PGT-Aug generates pseudo-LiDAR samples from miniatures and web videos and incorporates them into the scene flexibly. This process leads to a greater diversity of samples and enhanced placement flexibility, resulting in improved performance, especially for minority classes.", "section": "1 Introduction"}, {"figure_path": "NlpHKNjNNZ/figures/figures_24_1.jpg", "caption": "Figure 8: Dataset Collection. We demonstrate our collection of miniature images and crawled web videos.", "description": "This figure shows a collection of images used in the paper's dataset.  The images depict various miniature models of vehicles (cars, trucks, motorcycles, bicycles, construction equipment) and real-world vehicles from public web sources. These images were used in the volumetric 3D instance collection process of the proposed PGT-Aug framework for augmenting pseudo-LiDAR point clouds to resolve class imbalance issues in object detection.", "section": "A.1 Dataset Details"}, {"figure_path": "NlpHKNjNNZ/figures/figures_25_1.jpg", "caption": "Figure 13: Examples of RGB Point Clouds and Generated Pseudo LiDAR from Various 2D to 3D Renderers.", "description": "This figure shows the RGB point clouds and the generated pseudo LiDAR point clouds from various 2D-to-3D renderers (Plenoxels, Gaussian Splatting, and DUSt3R) for different datasets (nuScenes, KITTI, and Lyft).  It visually demonstrates the consistency of the proposed framework in generating high-quality LiDAR objects, regardless of the underlying 3D rendering technique used.", "section": "C.1 Generalization by Different Renderers"}]