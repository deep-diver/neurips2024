{"references": [{"fullname_first_author": "Tim Dettmers", "paper_title": "QLORA: Efficient finetuning of quantized LLMs", "publication_date": "2023-12-01", "reason": "This paper introduces a method for efficiently fine-tuning quantized large language models, which is directly relevant to the current paper's focus on quantized pretraining."}, {"fullname_first_author": "Jiawei Zhao", "paper_title": "GaLore: Memory-efficient LLM training by gradient low-rank projection", "publication_date": "2024-07-21", "reason": "This paper proposes GaLore, a method for reducing the memory footprint of optimizer states during training, which is a key component of the current paper's LoQT method."}, {"fullname_first_author": "Edward J Hu", "paper_title": "LoRA: Low-rank adaptation of large language models", "publication_date": "2021-01-01", "reason": "This foundational paper introduces the concept of low-rank adapters for efficient fine-tuning of large language models, which is a central idea in the development of the LoQT method."}, {"fullname_first_author": "Vladislav Lialin", "paper_title": "ReLoRA: High-rank training through low-rank updates", "publication_date": "2023-07-01", "reason": "This paper builds upon LoRA by introducing a method that combines low-rank updates with full-rank training, which is relevant to the LoQT method's approach to periodically merging low-rank factors into the full-rank weight matrix."}, {"fullname_first_author": "Hugo Touvron", "paper_title": "Llama 2: Open foundation and fine-tuned chat models", "publication_date": "2023-07-01", "reason": "This paper introduces Llama 2, a large language model used in the experiments of the current paper to demonstrate the effectiveness of the LoQT method."}]}