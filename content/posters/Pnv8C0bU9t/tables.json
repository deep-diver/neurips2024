[{"figure_path": "Pnv8C0bU9t/tables/tables_4_1.jpg", "caption": "Table 1: Comparison of low-rank pre-training methods for LLaMA2-style language models on the C4 dataset. The table shows validation perplexity, memory estimates, and quantization states for LoQT. The rank ratio r/dmodel is relative to the largest weight matrix dimension. Perplexity values are averaged over three seeds showing mean and standard error. (*) Denotes results from GaLore [8]. Only one seed was used for the 1B experiment due to compute constraints.", "description": "This table compares the performance of various low-rank pre-training methods (LoQT, LoQT without quantization, GaLore, LORA, ReLORA) on LLaMA2-style language models, trained on the C4 dataset.  It shows validation perplexity (a measure of model accuracy), memory usage, and the model's quantization state (whether or not quantization was used). The table highlights LoQT's efficiency in memory usage while achieving comparable performance to full-rank models.", "section": "3 Experiments"}, {"figure_path": "Pnv8C0bU9t/tables/tables_4_2.jpg", "caption": "Table 1: Comparison of low-rank pre-training methods for LLaMA2-style language models on the C4 dataset. The table shows validation perplexity, memory estimates, and quantization states for LoQT. The rank ratio r/dmodel is relative to the largest weight matrix dimension. Perplexity values are averaged over three seeds showing mean and standard error. (*) Denotes results from GaLore [8]. Only one seed was used for the 1B experiment due to compute constraints.", "description": "This table compares the performance of various low-rank pretraining methods (LoQT, LoQT without quantization, GaLore, LoRA, and ReLoRA) on LLaMA2-style language models using the C4 dataset.  It shows validation perplexity, memory usage estimations, and quantization configurations.  The results highlight LoQT's memory efficiency while maintaining competitive performance compared to other methods.", "section": "3 Experiments"}, {"figure_path": "Pnv8C0bU9t/tables/tables_5_1.jpg", "caption": "Table 1: Comparison of low-rank pre-training methods for LLaMA2-style language models on the C4 dataset. The table shows validation perplexity, memory estimates, and quantization states for LoQT. The rank ratio r/dmodel is relative to the largest weight matrix dimension. Perplexity values are averaged over three seeds showing mean and standard error. (*) Denotes results from GaLore [8]. Only one seed was used for the 1B experiment due to compute constraints.", "description": "This table compares the performance of various low-rank pre-training methods for large language models on the C4 dataset.  It shows the validation perplexity, estimated memory usage, and whether quantization was used for each method. The table includes results for models of different sizes (60M, 130M, 350M, and 1B parameters) and provides a rank ratio relative to the model's largest weight matrix.  Perplexity is averaged over three trials (except for the 1B model, which used only one due to computational constraints).  Results marked with an asterisk (*) represent data taken from the GaLore paper.", "section": "3 Experiments"}, {"figure_path": "Pnv8C0bU9t/tables/tables_5_2.jpg", "caption": "Table 1: Comparison of low-rank pre-training methods for LLaMA2-style language models on the C4 dataset. The table shows validation perplexity, memory estimates, and quantization states for LoQT. The rank ratio r/dmodel is relative to the largest weight matrix dimension. Perplexity values are averaged over three seeds showing mean and standard error. (*) Denotes results from GaLore [8]. Only one seed was used for the 1B experiment due to compute constraints.", "description": "This table compares the performance of several low-rank pre-training methods on the C4 dataset using LLaMA2-style language models.  It presents key metrics including validation perplexity (a measure of how well the model predicts the next word in a sequence), memory usage estimates, and the quantization level used (e.g., whether the model weights were quantized to lower precision for reduced memory footprint).  The results are averaged across multiple training runs to provide a more reliable comparison.", "section": "3 Experiments"}, {"figure_path": "Pnv8C0bU9t/tables/tables_6_1.jpg", "caption": "Table 3: GSM8K LLaMA-2 7B and 13B test accuracy with std. error. Best mean is in bold.", "description": "This table presents the results of fine-tuning quantized Llama-2 models (7B and 13B parameters) on the GSM8K dataset for arithmetic reasoning.  It compares the test set accuracy of LoQT against several other methods (LoRA, QLoRA, LoftQ, ApiQ), showing the performance in terms of accuracy and standard error.  The best-performing method for each model size is highlighted in bold.", "section": "3.2 Memory-Efficient Finetuning"}, {"figure_path": "Pnv8C0bU9t/tables/tables_6_2.jpg", "caption": "Table 1: Comparison of low-rank pre-training methods for LLaMA2-style language models on the C4 dataset. The table shows validation perplexity, memory estimates, and quantization states for LoQT. The rank ratio r/dmodel is relative to the largest weight matrix dimension. Perplexity values are averaged over three seeds showing mean and standard error. (*) Denotes results from GaLore [8]. Only one seed was used for the 1B experiment due to compute constraints.", "description": "This table compares the performance of various low-rank pre-training methods (LoQT, LoQT-nq, GaLore, LoRA, ReLORA) on LLaMA2-style language models using the C4 dataset.  It presents validation perplexity, memory usage estimates, and quantization configurations for each method across four different model sizes (60M, 130M, 350M, and 1B parameters). The results highlight the memory efficiency and performance trade-offs of different approaches, especially LoQT's ability to achieve comparable performance to full-rank training while significantly reducing memory.", "section": "3 Experiments"}, {"figure_path": "Pnv8C0bU9t/tables/tables_7_1.jpg", "caption": "Table 1: Comparison of low-rank pre-training methods for LLaMA2-style language models on the C4 dataset. The table shows validation perplexity, memory estimates, and quantization states for LoQT. The rank ratio r/dmodel is relative to the largest weight matrix dimension. Perplexity values are averaged over three seeds showing mean and standard error. (*) Denotes results from GaLore [8]. Only one seed was used for the 1B experiment due to compute constraints.", "description": "This table compares the performance of various low-rank pre-training methods on LLaMA2-style language models using the C4 dataset.  It shows validation perplexity (a measure of model accuracy), estimated memory usage, and whether quantization was used.  The rank ratio indicates the relative size of the low-rank approximation.  Perplexity is averaged across multiple training runs to account for random variation, and the GaLore results marked with an asterisk are taken from a previous study. Due to computational limitations, only a single training run was performed for the 1B parameter model.", "section": "3 Experiments"}, {"figure_path": "Pnv8C0bU9t/tables/tables_15_1.jpg", "caption": "Table 1: Comparison of low-rank pre-training methods for LLaMA2-style language models on the C4 dataset. The table shows validation perplexity, memory estimates, and quantization states for LoQT. The rank ratio r/dmodel is relative to the largest weight matrix dimension. Perplexity values are averaged over three seeds showing mean and standard error. (*) Denotes results from GaLore [8]. Only one seed was used for the 1B experiment due to compute constraints.", "description": "This table compares the performance of different low-rank pre-training methods for large language models on the C4 dataset.  It shows validation perplexity (a measure of model accuracy), estimated memory usage, and whether quantization was used.  Results are shown for models of various sizes (60M, 130M, 350M, and 1B parameters). The table highlights LoQT's performance and memory efficiency relative to other methods, particularly in the context of quantization.", "section": "3 Experiments"}, {"figure_path": "Pnv8C0bU9t/tables/tables_15_2.jpg", "caption": "Table 2: Results for LoQT, LOQT-nq, and GaLore using DeBERTaV3-base models on the GLUE development set. We report mean and standard error over three seeds. The best mean results on each dataset are shown in bold.", "description": "This table presents the results of fine-tuning experiments on the GLUE benchmark using three different methods: LoQT, LOQT-nq (a non-quantized version of LoQT), and GaLore.  The table shows the accuracy scores achieved by each method on various GLUE tasks.  The best performing method for each task is highlighted in bold.  The results are averaged over three independent runs and standard errors are included to show the variability of the results.", "section": "3.2 Memory-Efficient Finetuning"}, {"figure_path": "Pnv8C0bU9t/tables/tables_17_1.jpg", "caption": "Table 1: Comparison of low-rank pre-training methods for LLaMA2-style language models on the C4 dataset. The table shows validation perplexity, memory estimates, and quantization states for LoQT. The rank ratio r/dmodel is relative to the largest weight matrix dimension. Perplexity values are averaged over three seeds showing mean and standard error. (*) Denotes results from GaLore [8]. Only one seed was used for the 1B experiment due to compute constraints.", "description": "This table compares the performance of LoQT with other low-rank pre-training methods for LLaMA2-style language models trained on the C4 dataset.  It shows validation perplexity (a measure of model accuracy), memory usage estimates, and whether quantization was used.  The results highlight LoQT's memory efficiency compared to other methods, especially when using quantization.", "section": "3 Experiments"}, {"figure_path": "Pnv8C0bU9t/tables/tables_18_1.jpg", "caption": "Table 1: Comparison of low-rank pre-training methods for LLaMA2-style language models on the C4 dataset. The table shows validation perplexity, memory estimates, and quantization states for LoQT. The rank ratio r/dmodel is relative to the largest weight matrix dimension. Perplexity values are averaged over three seeds showing mean and standard error. (*) Denotes results from GaLore [8]. Only one seed was used for the 1B experiment due to compute constraints.", "description": "This table compares the performance of several low-rank pre-training methods for large language models on the C4 dataset.  It shows key metrics for different model sizes (60M, 130M, 350M, and 1B parameters), including validation perplexity (a measure of model performance), memory usage, and whether quantization was used.  The results highlight the memory efficiency of the LoQT method while demonstrating its competitive performance compared to other approaches like LoRA and GaLore.  The table also provides details on the rank of the low-rank matrices used relative to the model size and the number of training tokens used for each experiment.", "section": "3 Experiments"}]