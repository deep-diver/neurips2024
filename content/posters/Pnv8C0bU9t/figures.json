[{"figure_path": "Pnv8C0bU9t/figures/figures_0_1.jpg", "caption": "Figure 1: Memory usage of Llama 13B, rank 1024. LW: per-layer gradient updates. A8bit: Adam 8bit.", "description": "This figure compares the memory usage of different methods for training a 13B parameter Llama model with a rank of 1024 on an RTX 4090 GPU.  The methods compared are: Adam (standard optimizer), GaLore (low-rank optimizer), LoQT (low-rank adapters for quantized training), and variations using 8-bit Adam and per-layer gradient updates.  The bars represent the memory usage for each component: optimizer states, model weights, forward activations, gradients, and unknown memory.  The figure highlights that LoQT significantly reduces memory usage compared to the other methods, especially when combined with 8-bit Adam and per-layer updates.  The red dashed line indicates the 24GB VRAM limit of a typical consumer-grade GPU.  The figure demonstrates the feasibility of training large language models with LoQT on consumer hardware.", "section": "1 Introduction"}, {"figure_path": "Pnv8C0bU9t/figures/figures_1_1.jpg", "caption": "Figure 2: Overview of LoQT. (1) Low-rank factors P and B are periodically initialized from the gradient of the dequantized model weights VW, (2) then only B is trained while Pq and Wq are kept quantized and frozen, over an exponentially increasing interval until T, (3) the low-rank factors are merged back into the quantized model. The process is repeated until training halts.", "description": "This figure illustrates the LoQT training process.  It consists of three main steps that are periodically repeated: 1. Initialization: Low-rank factors P and B are initialized using the singular value decomposition (SVD) of the gradient of the dequantized model weights (VW).  P is quantized (Pq), and B is calculated to minimize the difference between the quantized and original weight matrices (Wq and W, respectively). 2. Training: Only matrix B is trained while keeping Pq and Wq fixed and quantized. This training happens over an exponentially increasing interval until T_i. 3. Merging: The low-rank factors Pq and B are merged into the quantized weight matrix Wq to create an updated weight matrix for the next cycle.  This process of training and merging repeats until the model training is complete. This iterative process reduces memory usage by optimizing only a low-rank component while maintaining quantized model weights.", "section": "Low-Rank Adapters for Quantized Training (LoQT)"}, {"figure_path": "Pnv8C0bU9t/figures/figures_7_1.jpg", "caption": "Figure 4: Ablation results for update intervals, error-compensation, quantization using model size 130m, and rank 256. Wq: quantized W; Pq: quantized P; No Q: no quantization. The dynamic update interval 100 + 1.22 grows exponentially for each step i \u2208 N.", "description": "This figure presents ablation studies on the impact of different factors on model performance. The left subplot shows the effect of various quantization strategies on perplexity.  The right subplot analyzes the influence of different update interval schedules (fixed vs. exponentially increasing) on perplexity.  The results illustrate the importance of error compensation and exponentially increasing update intervals for achieving comparable performance to models without quantization.", "section": "4 Ablations"}, {"figure_path": "Pnv8C0bU9t/figures/figures_16_1.jpg", "caption": "Figure 5: Rank ablation for LoQT and LoQT-nq showing perplexity as a function of steps.", "description": "This figure displays the validation perplexity over training steps for different ranks using LoQT-nq (non-quantized) and LoQT (quantized).  An exponentially increasing update frequency is used, starting at 100 steps and increasing with a factor of 1.2 per interval.  The results show similar trajectories for quantized and non-quantized models across various ranks (64, 128, 256, 512), indicating that LoQT maintains comparable performance to LoQT-nq even with quantization. However, a divergence is seen at rank 64, suggesting a minimum rank threshold for effective quantization.", "section": "C Rank Ablation"}, {"figure_path": "Pnv8C0bU9t/figures/figures_17_1.jpg", "caption": "Figure 1: Memory usage of Llama 13B, rank 1024. LW: per-layer gradient updates. A8bit: Adam 8bit.", "description": "This figure compares the memory usage of different methods for training a 13B parameter Llama model with a rank of 1024.  The methods compared include using Adam (a standard optimizer), GaLore (a low-rank optimizer), and LoQT (the proposed method in the paper) with and without 8-bit Adam and per-layer gradient updates.  The figure visually demonstrates the significant memory savings achieved by LoQT, especially when combined with 8-bit Adam and per-layer updates, showcasing its effectiveness in training large models on hardware with limited memory.", "section": "1 Introduction"}, {"figure_path": "Pnv8C0bU9t/figures/figures_18_1.jpg", "caption": "Figure 4: Ablation results for update intervals, error-compensation, quantization using model size 130m, and rank 256. Wq: quantized W; Pq: quantized P; No Q: no quantization. The dynamic update interval 100 + 1.22 grows exponentially for each step i \u2208 N.", "description": "This figure shows the ablation study of LoQT on a 130 million parameter model.  It compares different configurations including: only quantizing weights (Wq), quantizing both weights and projection matrix (Wq, Pq), adding error compensation (EC), using exponentially increasing update intervals (EI), and a control group with no quantization (No Q). The x-axis represents the training step, and the y-axis represents the validation perplexity. The results demonstrate that exponentially increasing update intervals and error compensation significantly improve the model's performance, especially for quantized models.", "section": "4 Ablations"}, {"figure_path": "Pnv8C0bU9t/figures/figures_19_1.jpg", "caption": "Figure 6: Memory usage for LoQT vs baselines for different model sizes. LW means per-layer gradient updates as per [13], and A8bit means with Adam 8-bit. We evaluate using a token batch size of 256.", "description": "This figure compares the memory usage of LoQT with various baselines (Adam, GaLore, and Adam 8-bit) for different model sizes and sequence lengths.  The 'LW' designation indicates the use of per-layer gradient updates, a technique that reduces memory requirements during training. The 8-bit versions of Adam and GaLore represent the use of 8-bit precision for these optimizers, further conserving memory. Horizontal lines indicate standard VRAM capacities of 16GB, 24GB, and 40GB to illustrate which configurations fit within the capabilities of different GPUs.", "section": "3.3 Memory and Throughput"}, {"figure_path": "Pnv8C0bU9t/figures/figures_19_2.jpg", "caption": "Figure 6: Memory usage for LoQT vs baselines for different model sizes. LW means per-layer gradient updates as per [13], and A8bit means with Adam 8-bit. We evaluate using a token batch size of 256.", "description": "This figure compares the memory usage of LoQT against several baseline methods (Adam, Adam 8-bit, Adam with per-layer weight updates, GaLore, and GaLore with per-layer updates and 8-bit Adam) for different model sizes (1B, 3B, 7B, and 13B parameters).  It demonstrates that LoQT consistently uses less memory than the baselines, especially when combined with per-layer updates and 8-bit Adam. The reduction in memory usage is significant, highlighting LoQT's efficiency in training large language models.", "section": "3.3 Memory and Throughput"}]