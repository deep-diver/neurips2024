[{"figure_path": "p4a1nSvwD7/figures/figures_4_1.jpg", "caption": "Figure 1: Schematic of Enhanced Anchor Dictionary Representation (EAD).", "description": "This figure illustrates the Enhanced Anchor Dictionary (EAD) representation strategy used in the STONE model.  The EAD method is designed to improve the efficiency and robustness of subspace representation learning, particularly when dealing with limited or noisy data. Instead of directly using the observed data as a dictionary, EAD selects a subset of data points as anchors. These anchors, along with additional latent data points, construct a smaller, more efficient dictionary that is less prone to the issues associated with using the full dataset directly.  The figure visually shows how the anchor dictionary (Av), the subspace representation (Zv), the projection matrix (Pv), the observed data (Xv), and the reconstruction error (Ev) interact to form the subspace representation of view v in the STONE framework.", "section": "3 The Proposed Method"}, {"figure_path": "p4a1nSvwD7/figures/figures_4_2.jpg", "caption": "Figure 2: Tensor Rank Approximation: HTR vs. TNN and TLSN.", "description": "This figure compares the performance of the proposed Hyperbolic Tangent Rank (HTR) approximation to existing tensor rank approximations, namely Tensor Nuclear Norm (TNN) and Truncated Logarithmic Schatten-p Norm (TLSPN).  It shows that HTR more precisely approximates the tensor rank across a range of values, particularly for those close to zero and larger values.  This more accurate approximation allows HTR to better distinguish between significant variations and noise within the singular values of the tensor data.", "section": "3 The Proposed Method"}, {"figure_path": "p4a1nSvwD7/figures/figures_6_1.jpg", "caption": "Figure 3: Impact of Parameter \u03b4 on the STONE Model.", "description": "This figure demonstrates the effect of varying the parameter \u03b4 (delta) in the STONE model on the clustering performance across different datasets. The x-axis represents the values of \u03b4, and the y-axis represents the clustering performance metrics (ACC and NMI). The different curves in the figure represent the results obtained for different datasets (NGs, HW, and MSRCV1). It shows how the choice of \u03b4 impacts the clustering results, revealing that an optimal \u03b4 value exists for each dataset, maximizing the effectiveness of the STONE model's capability to capture non-linear correlations.", "section": "4.2 Comparison of Clustering Performance and Efficiency"}, {"figure_path": "p4a1nSvwD7/figures/figures_7_1.jpg", "caption": "Figure 4: The Influence of Anchor Quantity on STONE Model Performance.", "description": "This figure displays the impact of varying the number of anchors on the STONE model's performance across three different datasets (NGs, HW, and MSRCV1).  The x-axis represents the number of anchors (multiples of a base number 'c'), and the y-axis shows the clustering accuracy (ACC) and Normalized Mutual Information (NMI).  The results indicate that choosing a smaller number of discriminative anchors is better than using a larger number of non-discriminative anchors, and the optimal performance is achieved with 'c' or '2c' anchors. This demonstrates the effectiveness of the anchor selection strategy in STONE.", "section": "4 Experiment"}, {"figure_path": "p4a1nSvwD7/figures/figures_8_1.jpg", "caption": "Figure 5: Sensitivity Analysis of the STONE Model to the Balance Parameters \u03b1, \u03b2 and \u03b3.", "description": "This figure presents a sensitivity analysis of the STONE model's performance concerning its three balancing parameters: \u03b1, \u03b2, and \u03b3.  The plots show how the accuracy (ACC) of the model changes as these parameters are varied across different values, providing insights into their relative importance and optimal ranges for achieving the best clustering performance. Each subplot represents the result for a specific dataset (HW in this case), showing the interplay between \u03b1 and \u03b2 at various values of \u03b3, allowing for a comprehensive understanding of the parameters' impact on the model's efficacy.", "section": "4.3 Parameters Analysis"}, {"figure_path": "p4a1nSvwD7/figures/figures_9_1.jpg", "caption": "Figure 6: Convergence Curves of STONE on Three Datasets.", "description": "This figure displays the convergence behavior of the STONE model's optimization algorithm across three different datasets: NGs, HW, and MSRCV1.  The plots show the reconstruction error (RE) and matching error (ME) over a series of iterations.  The rapid decrease and subsequent stabilization of both RE and ME indicate the algorithm's efficient convergence to a solution, demonstrating its robust behavior across varied datasets.", "section": "4.4 Convergence Behavior"}, {"figure_path": "p4a1nSvwD7/figures/figures_18_1.jpg", "caption": "Figure 7: Contrasting Consensus Affinity Matrices: STONE vs. SOTA on NGs Dataset.", "description": "The figure compares the consensus affinity matrices generated by different multi-view clustering methods, including MVCtopl, MSC2D, GMC, MVSCTM, TBGL, and the proposed STONE model.  The visualization highlights how the STONE model produces a more well-defined block-diagonal structure compared to other methods, indicating a more effective clustering of the data points. This showcases the superiority of STONE in learning high-order correlations and local geometric structures within the data.", "section": "4.3 Additional Experimental Results"}, {"figure_path": "p4a1nSvwD7/figures/figures_19_1.jpg", "caption": "Figure 8: Comparative t-SNE Visualization Analysis: View-Specific Graphs vs. Consensus Graph.", "description": "This figure visualizes the results of t-SNE dimensionality reduction applied to the MSRCV1 dataset.  It shows separate t-SNE plots for each of the five individual views (a-e), as well as a t-SNE plot (f) of the consensus representation learned by the STONE model which integrates information across all views. The visualization aims to demonstrate that the consensus representation obtained by the STONE model provides a more distinct separation of clusters compared to the individual views, highlighting the benefits of multi-view integration in improving clustering performance.", "section": "4. Experiment"}, {"figure_path": "p4a1nSvwD7/figures/figures_20_1.jpg", "caption": "Figure 9: Impact of Parameter \u03b4 on the STONE Model on Eight Datasets.", "description": "This figure displays the impact of the built-in parameter \u03b4 on the performance of the STONE model across eight datasets.  The parameter \u03b4 controls the degree of shrinkage applied to different singular values, affecting how well the model distinguishes between significant variations and noise within the tensor data. The graphs show that changes in the value of \u03b4 lead to fluctuations in clustering performance (ACC and NMI), demonstrating its sensitivity and the importance of careful tuning for optimal results. The performance varies for each dataset, highlighting the data-dependent nature of this parameter and the need for careful selection.", "section": "4.3 Parameters Analysis"}, {"figure_path": "p4a1nSvwD7/figures/figures_20_2.jpg", "caption": "Figure 9: Impact of Parameter \u03b4 on the STONE Model on Eight Datasets.", "description": "This figure displays the impact of the built-in parameter \u03b4 on the STONE model's performance across eight different datasets. The parameter \u03b4 dynamically controls the degree of shrinkage applied to different singular values of tensor data.  The x-axis represents the various values of \u03b4 tested, while the y-axis shows the corresponding ACC and NMI values. The plots illustrate how different values of \u03b4 influence clustering performance (ACC and NMI) for each dataset. This analysis helps in understanding how the non-convex penalty term (HTR) works and its sensitivity to different \u03b4 values.", "section": "4.3 Parameters Analysis"}, {"figure_path": "p4a1nSvwD7/figures/figures_21_1.jpg", "caption": "Figure 11: Sensitivity Analysis of the STONE Model to Parameters \u03b1, \u03b2 and \u03b3 on Eight Datasets.", "description": "This figure displays a sensitivity analysis of the STONE model's performance to its three balancing parameters (\u03b1, \u03b2, and \u03b3) across eight different datasets.  Each subfigure shows a 3D surface plot illustrating the relationship between the three parameters and the resulting accuracy (ACC) achieved on a specific dataset.  The plots help to visualize the optimal parameter ranges for achieving high clustering accuracy and highlight the interplay between the parameters in influencing performance.", "section": "4.3 Parameters Analysis"}, {"figure_path": "p4a1nSvwD7/figures/figures_21_2.jpg", "caption": "Figure 11: Sensitivity Analysis of the STONE Model to Parameters \u03b1, \u03b2 and \u03b3 on Eight Datasets.", "description": "This figure displays a sensitivity analysis of the STONE model's performance across eight datasets with respect to variations in three balancing parameters (\u03b1, \u03b2, and \u03b3). Each subfigure represents a different dataset, showing how changes in the parameters affect the accuracy (ACC) and normalized mutual information (NMI) scores.  The 3D plots visualize the parameter space, providing insights into the model's robustness and optimal parameter ranges for different datasets.", "section": "4.3 Parameters Analysis"}, {"figure_path": "p4a1nSvwD7/figures/figures_21_3.jpg", "caption": "Figure 6: Convergence Curves of STONE on Three Datasets.", "description": "This figure displays the convergence behavior of the STONE model's optimization process on three datasets: NGs, HW, and MSRCV1.  The plots track two key metrics: reconstruction error (RE) and matching error (ME) over a series of iterations.  The rapid decrease and eventual stabilization of both RE and ME demonstrate the model's efficient and robust convergence to a solution.", "section": "4.4 Convergence Behavior"}]