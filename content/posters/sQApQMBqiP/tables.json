[{"figure_path": "sQApQMBqiP/tables/tables_17_1.jpg", "caption": "Table 1: Spearman correlations (ps) of each model\u2019s degree of representational alignment with the environment and its performance according to each metric. Mean reward should be maximized, and all other metrics minimized, for value alignment.", "description": "This table presents the Spearman correlation coefficients between the degree of representational alignment and several performance metrics for three different machine learning models: Support Vector Regression, Kernel Regression, and Gaussian Process Regression.  The metrics evaluated include mean reward, unique actions taken, non-optimal actions taken, immoral actions taken, and iterations to convergence.  A higher correlation indicates a stronger relationship between representational alignment and the performance metric.  The p-values are all highly statistically significant, suggesting a strong relationship between alignment and performance in this context.", "section": "A.8 Additional Results for Multiple Human Values"}, {"figure_path": "sQApQMBqiP/tables/tables_20_1.jpg", "caption": "Table 2: Spearman correlations (ps) between representational alignment and language model kernel performance on each human value, measured for mean reward and bad actions taken in both the personalization and generalization phases. Correlations are taken with 3 different measures of representational alignment: using the full similarity matrix (full), using only alignment against personalization actions (pers.), and using alignment between all personalization/generalization action pairs only (cross). A value-aligned agent should have higher mean reward and a lower number of bad actions taken. All results are statistically significant (p < 0.0001) with a few exceptions that are addressed in Section 5.", "description": "This table presents the Spearman correlations between representational alignment and the performance of language models on various human values.  It shows correlations for both personalization (safe exploration) and generalization phases, using three different measures of representational alignment.  The goal is to assess the impact of representational alignment on an agent's ability to learn and generalize human values.", "section": "Results"}, {"figure_path": "sQApQMBqiP/tables/tables_21_1.jpg", "caption": "Table 3: Results from the personalization phase of the control experiment, where we define a new trivial reward function and similarity kernel based on the length of each action description. Mean reward should be maximized and bad actions taken minimized for a value-aligned agent.", "description": "This table presents the results from the personalization phase of a control experiment. In this experiment, a new reward function and similarity kernel were defined based on the length of each action description instead of human-evaluated values. The table shows the mean reward and the number of bad actions taken for different values (social status, morality, challenging, compassion, enjoyability, fairness, honesty, integrity, loyalty, and popularity) using both human-based and length-based similarity kernels and reward functions. This experiment helps to isolate the effect of representational alignment by comparing human-based values with arbitrary (length-based) ones.", "section": "A.9 Control Experiment"}, {"figure_path": "sQApQMBqiP/tables/tables_22_1.jpg", "caption": "Table 2: Spearman correlations (ps) between representational alignment and language model kernel performance on each human value, measured for mean reward and bad actions taken in both the personalization and generalization phases. Correlations are taken with 3 different measures of representational alignment: using the full similarity matrix (full), using only alignment against personalization actions (pers.), and using alignment between all personalization/generalization action pairs only (cross). A value-aligned agent should have higher mean reward and a lower number of bad actions taken. All results are statistically significant (p < 0.0001) with a few exceptions that are addressed in Section 5.", "description": "This table presents the Spearman correlation coefficients between representational alignment and model performance across ten different human values.  It shows the correlation for both the personalization (learning) and generalization (applying learned knowledge to new situations) phases.  Three different measures of representational alignment are used: full similarity matrix, personalization-only alignment, and cross-alignment between personalization and generalization actions.  The table indicates whether higher representational alignment correlates with better performance (higher mean reward, fewer bad actions).", "section": "Results"}, {"figure_path": "sQApQMBqiP/tables/tables_23_1.jpg", "caption": "Table 2: Spearman correlations (ps) between representational alignment and language model kernel performance on each human value, measured for mean reward and bad actions taken in both the personalization and generalization phases. Correlations are taken with 3 different measures of representational alignment: using the full similarity matrix (full), using only alignment against personalization actions (pers.), and using alignment between all personalization/generalization action pairs only (cross). A value-aligned agent should have higher mean reward and a lower number of bad actions taken. All results are statistically significant (p < 0.0001) with a few exceptions that are addressed in Section 5.", "description": "This table presents the Spearman correlations between representational alignment and the performance of language models on ten different human values.  Three different methods are used to measure representational alignment: using the full similarity matrix, considering only personalization actions, and considering only the similarity between personalization and generalization actions.  The results show correlations between the level of representational alignment and both the mean reward and the number of \"bad\" actions for each human value across both the personalization and generalization phases.  Most results are statistically significant (p<0.0001).", "section": "Results"}]