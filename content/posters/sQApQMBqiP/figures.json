[{"figure_path": "sQApQMBqiP/figures/figures_1_1.jpg", "caption": "Figure 1: A visualization of our experimental setup. Representation spaces are modeled via pairwise similarity judgments given by language models and humans over the same set of stimuli. A machine learning agent takes such a representation space and tries to learn a human value function over those representations. We simulate personalization (the process of learning the value function), evaluating the agent on safe exploration, and evaluate the agent's ability to generalize to unseen examples.", "description": "This figure illustrates the experimental setup used in the paper. Pairwise similarity judgments from both language models and humans are used to model representation spaces. A machine learning agent learns a human value function using these representations.  The experiment simulates personalization and assesses the agent's performance in safe exploration and generalization to unseen examples.", "section": "1 Introduction"}, {"figure_path": "sQApQMBqiP/figures/figures_1_2.jpg", "caption": "Figure 1: A visualization of our experimental setup. Representation spaces are modeled via pairwise similarity judgments given by language models and humans over the same set of stimuli. A machine learning agent takes such a representation space and tries to learn a human value function over those representations. We simulate personalization (the process of learning the value function), evaluating the agent on safe exploration, and evaluate the agent's ability to generalize to unseen examples.", "description": "This figure illustrates the experimental setup used in the paper. Pairwise similarity judgments collected from both language models and humans are used to model representation spaces. A machine learning agent learns a human value function using this representation space.  The setup simulates the personalization process (learning the value function) and evaluates safe exploration and generalization to unseen examples. The left side depicts the personalization phase where the agent interacts with a human to learn their value function, while the right side shows the generalization phase where the agent uses its learned value function to evaluate actions in situations not previously encountered during the personalization process.", "section": "1 Introduction"}, {"figure_path": "sQApQMBqiP/figures/figures_6_1.jpg", "caption": "Figure 2: Agent performance in simulated experiments, plotted against representational alignment.", "description": "This figure shows the results of simulated experiments, where the performance of different machine learning agents (Support Vector Regression, Kernel Regression, Gaussian Process Model) is evaluated across various levels of representational alignment with human values. The x-axis represents the degree of representational alignment, ranging from 0 (no alignment) to 1 (perfect alignment).  The y-axis shows the performance metrics: Mean reward, Number of Non-optimal Actions Taken, Number of Immoral Actions Taken, Iterations to Convergence, Number of Unique Actions Taken. The red dashed line represents the performance of a Thompson Sampling baseline agent. The shaded regions around the lines represent the standard error of the mean.  The figure visually demonstrates how representational alignment affects the speed and safety of learning, and the ability to generalize to unseen actions.", "section": "3.2 Synthetic Experiments"}, {"figure_path": "sQApQMBqiP/figures/figures_8_1.jpg", "caption": "Figure 2: Agent performance in simulated experiments, plotted against representational alignment.", "description": "This figure displays the results of simulated experiments, showing the relationship between an agent's representational alignment with humans and its performance on a moral decision-making task.  It illustrates how different metrics like mean reward, the number of immoral actions taken, the number of unique actions taken, and the number of iterations to convergence vary as the agent's representational alignment changes. The plots demonstrate that higher representational alignment generally correlates with better performance (higher mean reward, fewer bad actions) and faster learning.", "section": "3.2 Synthetic Experiments"}, {"figure_path": "sQApQMBqiP/figures/figures_8_2.jpg", "caption": "Figure 2: Agent performance in simulated experiments, plotted against representational alignment.", "description": "This figure displays the performance of different reinforcement learning agents across various metrics (mean reward, immoral actions taken, unique actions taken, and iterations to convergence) plotted against their representational alignment with a human.  It visually demonstrates the relationship between how well an agent's internal representation aligns with a human's and its ability to learn human values safely and efficiently. Higher representational alignment generally leads to better performance across all metrics.", "section": "3.2 Synthetic Experiments"}, {"figure_path": "sQApQMBqiP/figures/figures_18_1.jpg", "caption": "Figure 2: Agent performance in simulated experiments, plotted against representational alignment.", "description": "This figure presents the results of simulated experiments designed to test the impact of representational alignment on the performance of reinforcement learning agents in learning human values. The x-axis represents the degree of representational alignment between the agent and humans, while the y-axis shows different performance metrics such as mean reward, the number of non-optimal actions taken, immoral actions taken, and the number of unique actions taken.  The results across multiple models support the theory presented in the paper, demonstrating that higher representational alignment correlates with better performance, including safer exploration and faster learning.", "section": "Synthetic Experiments"}, {"figure_path": "sQApQMBqiP/figures/figures_19_1.jpg", "caption": "Figure 2: Agent performance in simulated experiments, plotted against representational alignment.", "description": "This figure shows the results of simulated experiments evaluating the performance of different reinforcement learning agents in relation to their representational alignment with human values.  The x-axis represents the degree of representational alignment, ranging from 0 (no alignment) to 1 (perfect alignment). The y-axis displays four different performance metrics: mean reward, number of non-optimal actions, immoral actions, and iterations to convergence. The results show a clear correlation between higher representational alignment and better performance across all metrics, indicating that learning human-like representations is beneficial for learning human values safely and efficiently.", "section": "3.2 Synthetic Experiments"}, {"figure_path": "sQApQMBqiP/figures/figures_29_1.jpg", "caption": "Figure 2: Agent performance in simulated experiments, plotted against representational alignment.", "description": "This figure displays the results of simulated experiments designed to test the impact of representational alignment on the performance of AI agents learning human values.  Four different metrics are shown across a range of representational alignments, each calculated by correlating the agent's similarity judgments to those of humans:\n\n* **Mean Reward:** The average reward received by the agent per timestep.\n* **Unique Actions Taken:** The number of unique actions taken by the agent during the learning process.\n* **Non-Optimal Actions Taken:** The number of times the agent chose an action that was not the most optimal (i.e., not the action with the highest morality score).\n* **Immoral Actions Taken:** The number of times the agent chose an action with a morality score below a predefined threshold (50).\n\nThe results show that agents with higher representational alignment (i.e., closer similarity to human judgments) generally achieve higher rewards, take fewer non-optimal actions, and exhibit safer exploration by taking fewer immoral actions.", "section": "Synthetic Experiments"}, {"figure_path": "sQApQMBqiP/figures/figures_30_1.jpg", "caption": "Figure 2: Agent performance in simulated experiments, plotted against representational alignment.", "description": "This figure shows the results of simulated experiments to test the effect of representational alignment on agent performance.  Four metrics are plotted against representational alignment: mean reward (a measure of how well the agent learned the values), number of non-optimal actions taken, number of immoral actions taken, and the number of unique actions taken.  The results demonstrate that higher representational alignment leads to better performance across all four metrics, supporting the paper's central claim.", "section": "3.2 Synthetic Experiments"}, {"figure_path": "sQApQMBqiP/figures/figures_30_2.jpg", "caption": "Figure 2: Agent performance in simulated experiments, plotted against representational alignment.", "description": "This figure displays the results of simulated experiments designed to test the impact of representational alignment on the performance of AI agents learning human values.  The x-axis represents the degree of representational alignment (Spearman correlation), while the y-axis shows various performance metrics: mean reward, number of non-optimal actions, number of immoral actions, iterations to convergence and unique actions taken.  Each metric's trend is shown for three different reinforcement learning algorithms: Gaussian process regression, kernel regression, and support vector regression.  A dashed red line represents a baseline Thompson sampling method.  The plots illustrate that as representational alignment increases, the mean reward increases, while the number of suboptimal actions, immoral actions, and iterations to converge decrease, indicating improved and safer learning performance.", "section": "3.2 Synthetic Experiments"}]