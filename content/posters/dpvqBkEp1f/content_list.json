[{"type": "text", "text": "Noether\u2019s Razor: Learning Conserved Quantities ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Tycho F. A. van der Ouderaa Mark van der Wilk Pim de Haan Imperial College London University of Oxford CuspAI London, UK Oxford, UK Amsterdam, NL ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Symmetries have proven useful in machine learning models, improving generalisation and overall performance. At the same time, recent advancements in learning dynamical systems rely on modelling the underlying Hamiltonian to guarantee the conservation of energy. These approaches can be connected via a seminal result in mathematical physics: Noether\u2019s theorem, which states that symmetries in a dynamical system correspond to conserved quantities. This work uses Noether\u2019s theorem to parameterise symmetries as learnable conserved quantities. We then allow conserved quantities and associated symmetries to be learned directly from train data through approximate Bayesian model selection, jointly with the regular training procedure. As training objective, we derive a variational lower bound to the marginal likelihood. The objective automatically embodies an Occam\u2019s Razor effect that avoids collapse of conservation laws to the trivial constant, without the need to manually add and tune additional regularisers. We demonstrate a proof-ofprinciple on $n$ -harmonic oscillators and $n$ -body systems. We find that our method correctly identifies the correct conserved quantities and $\\mathrm{U}(n)$ and ${\\mathrm{SE}}(n)$ symmetry groups, improving overall performance and predictive accuracy on test data. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Symmetries provide strong inductive biases, effectively reducing the volume of the hypothesis space. A celebrated example of this is the convolutional layer embedding translation equivariance in neural networks, which can be generalised to other symmetry groups [Cohen and Welling, 2016]. ", "page_idx": 0}, {"type": "text", "text": "Meanwhile, physics-informed machine learning models [Greydanus et al., 2019, Cranmer et al., 2020], typically relying on neural differential equations [Chen et al., 2018], embed constraints known from classical mechanics into model architectures to improve accuracy on physical dynamical systems. ", "page_idx": 0}, {"type": "text", "text": "Rather than strictly constraining a model to certain symmetries, recent works have explored whether invariance and equivariance symmetries in machine learning models can also be automatically learned from data. This often relies on separate validation data [Maile et al., 2022], explicit regularisers [Finzi et al., 2021] or additional outer loops [Cubuk et al., 2018]. Alternatively, we can take a Bayesian approach where we embed symmetries into the prior and empirically learn them through Bayesian model selection [van der Wilk et al., 2018, Immer et al., 2022, van der Ouderaa et al., 2022]. ", "page_idx": 0}, {"type": "text", "text": "We propose to use Noether\u2019s theorem [Noether, 1918] to parameterise symmetries in Hamiltonian machine learning models in terms of their conserved quantities. To do so, we propose to symmetrise a learnable Hamiltonian using a set of learnable quadratic conserved quantitites. By choosing the conserved quantities to be quadratic, we can find closed-form transformations that can be used to obtain an unbiased estimate of the symmetrised Hamiltonian. ", "page_idx": 0}, {"type": "text", "text": "Secondly, we phase symmetries implied by conserved quantities in the prior over Hamiltonians an leverage the Occam\u2019s razor effect of Bayesian model selection [Rasmussen and Ghahramani, 2000, van der Wilk et al., 2018] to learn conserved quantities and their implied symmetries directly from train data. We derive a practical lower bound using variational inference [Hoffman et al., 2013] resulting in a single end-to-end training procedure capable of learning the Hamiltonian of a system jointly with its conserved quantities. As far as we know, this is the first case in which Bayesian model selection with variational inference is successfully scaled to deep neural networks, an achievement in its own right, whereas most works so far have relied on Laplace approximations [Immer et al., 2022]. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Experimentally, we evaluate our Noether\u2019s razor method on various dynamical systems, including $n$ -simple harmonic oscillators and $n$ -body systems. Our results suggest that our method is indeed capable of learning the conserved quantities that give rise to correct symmetry groups for the problem at hand. Quantitatively, we find that our method that learns symmetries from data matches the performance of models with the correct symmetries built-in as oracle. We outperform vanilla training, resulting in improved test generalisation and predictions that remain accurate over longer time periods. ", "page_idx": 1}, {"type": "text", "text": "2 Background ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2.1 Hamiltonian mechanics ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Hamiltonian mechanics is a framework that describes dynamical systems in phase space, denoted $\\mathcal{M}=\\mathbb{R}^{M}$ , with $M$ even. Phase space elements $(q,p)\\in\\mathcal{M}$ follow Hamiltonian equations of motion: ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\dot{q}_{i}=\\frac{\\partial H}{\\partial p_{i}},\\quad\\dot{p}_{i}=-\\frac{\\partial H}{\\partial q_{i}}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where the Hamiltonian $H:\\mathcal{M}\\rightarrow\\mathbb{R}$ is an observable1, which are smooth functions on the phase space, that corresponds to the energy of the system. It is often simpler to write $\\pmb{x}=(\\pmb{q},\\pmb{p})$ , so that we have: $\\dot{\\pmb{x}}=J\\nabla H$ and $J=\\left[\\begin{array}{c c}{0}&{I}\\\\ {-I}&{0}\\end{array}\\right]$ , where $I$ is the identity matrix, $J$ is called the symplectic form and $\\nabla H=\\nabla_{\\pmb{x}}H(\\pmb{x})$ is the gradient of phase space coordinates. ", "page_idx": 1}, {"type": "text", "text": "Example: $n$ -body problem in $3d$ . If we consider a $d{=}3$ dimensional Euclidean space containing $n$ bodies, our position and velocity spaces are each $\\mathbb{R}^{3n}$ making up phase space $\\mathbf{\\dot{\\mathcal{M}}}=\\mathbb{R}^{2\\cdot3n}$ . Our Hamiltonian $H:\\mathbb{R}^{3n}\\times\\mathbb{R}^{3n}\\rightarrow\\dot{\\mathbb{R}}$ , which in this case is a separable function $\\bar{H}(q,p)=K(q)+P(p)$ of kinetic energy $\\begin{array}{r}{K(q)=\\sum_{i}m_{i}||p||^{2}/2}\\end{array}$ and the potential energy $\\begin{array}{r}{P(p)=\\sum_{i\\neq j}G m_{i}m_{j}/||q_{i}-q_{j}||}\\end{array}$ where $m_{i}$ is the mass of a body $i$ and $G$ is the gravitational constant. ", "page_idx": 1}, {"type": "text", "text": "2.2 Learning Hamiltonian mechanics from data ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We can model the Hamiltonian from data [Greydanus et al., 2019, Ross and Heinonen, 2023, Tanaka et al., 2022, Zhong et al., 2019]. Concretely, we are interested in a posterior over functions that the Hamiltonian can take $p(H_{\\pmb{\\theta}}\\mid\\mathcal{D})$ , conditioned on trajectory data $\\bar{D}\\doteq\\{(\\pmb{x}_{t}^{n},\\pmb{x}_{t^{\\prime}}^{n})\\}_{n=1}^{N}$ sampled from phase space at different time points $(t,t^{\\prime})$ , or time difference $\\scriptstyle\\Delta t=t^{\\prime}-t$ . Given a new data point $\\pmb{x}_{t}^{*}$ , we would like to make predictions $p(\\mathbf{\\boldsymbol{x}}_{t^{\\prime}}^{*}|\\mathbf{\\boldsymbol{x}}_{t}^{*},H_{\\theta},\\mathcal{D})$ over phase space trajectories into the future $t^{\\prime}$ . ", "page_idx": 1}, {"type": "text", "text": "Hamiltonian neural networks Hamiltonian neural networks [Greydanus et al., 2019, Toth et al., 2019, Rezende et al., 2019] model the Hamiltonian $H$ using a learnable Hamiltonian $H_{\\theta}:\\mathcal{M}\\rightarrow\\mathbb{R}$ parameterised by $\\pmb{\\theta}\\in\\mathbb{R}^{P}$ . With a straightforward Gaussian likelihood $p(\\mathbf{\\boldsymbol{x}}_{t^{\\prime}}|\\mathbf{\\boldsymbol{x}}_{t},\\pmb{\\theta})=\\mathcal{N}(\\mathbf{\\boldsymbol{x}}_{t^{\\prime}}|\\mathbf{\\boldsymbol{x}}_{t}+$ $\\bar{J}\\nabla H_{\\pmb\\theta}(\\pmb x_{t})\\Delta t,\\dot{\\sigma}_{\\mathrm{data}}^{2}\\pmb I)$ with a small observation noise $\\sigma_{\\mathrm{data}}^{2}$ , a maximum likelihood fti can be found by minimising the negative log-likelihood $\\begin{array}{r}{\\pmb{\\theta}_{*}\\!=\\!\\arg\\operatorname*{min}_{\\pmb{\\theta}}\\sum_{i}\\sum_{t}-\\log p(\\pmb{x}_{t+\\Delta t}^{i}|\\pmb{x}_{t}^{i},\\pmb{\\theta})}\\end{array}$ on minibatches of data using stochastic gradient descent. The mean of this likelihood represents a single Euler integration step (Sec. 2.1 of David and M\u00e9hats [2023]), which bounds the possible accuracy of the fti to the true Hamiltonian $H$ . In practice, we may replace this by more accurate differentiable numerical integrators [Kidger, 2022]. ", "page_idx": 1}, {"type": "text", "text": "2.3 Noether\u2019s theorem ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "The theorem of [Noether, 1918], here presented in the Hamiltonian formalism [Baez, 2020, Arnold, 1989], links the concepts of an observable being conserved, to the Hamiltonian being invariant to the symmetries generated by an observable. ", "page_idx": 1}, {"type": "text", "text": "Conserved quantity Let $\\scriptscriptstyle\\mathcal{O}$ be the set of observables, which are smooth real-valued functions $\\mathcal{M}\\rightarrow\\mathbb{R}$ on the phase space. Given a trajectory ${\\mathbf{}}x(t)$ generated by the Hamiltonian $H$ , we can compute the variation of an observable $O\\in{\\mathcal{O}}$ in time via the chain rule and Hamilton\u2019s equations of motion (Equation (1)) ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}O}{\\mathrm{d}t}=\\sum_{i}\\frac{\\partial O}{\\partial q_{i}}\\dot{q}_{i}+\\frac{\\partial O}{\\partial p_{i}}\\dot{p}_{i}=\\sum_{i}\\frac{\\partial O}{\\partial q_{i}}\\frac{\\partial H}{\\partial p_{i}}-\\frac{\\partial O}{\\partial p_{i}}\\frac{\\partial H}{\\partial q_{i}}=\\{O,H\\},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where the last equality defines the Poisson bracket $\\{\\cdot,\\cdot\\}:\\mathcal{O}\\times\\mathcal{O}\\rightarrow\\mathcal{O}$ . The Poisson bracket relates to the symplectic form via $\\{O,H\\}({\\pmb x})=\\nabla O({\\pmb x})\\cdot J{\\dot{\\nabla}}H({\\pmb x})$ . An observable that does not change along any trajectory is called a conserved quantity. As we can see from Equation (2), an observable $O$ is conserved if and only if $\\{O,H\\}=0$ . ", "page_idx": 2}, {"type": "text", "text": "From two conserved quantities $O,O^{\\prime}\\,\\in\\,{\\mathcal{O}}$ , we can create a new conserved quantity by linear combination $\\alpha{\\cal O}+\\beta{\\cal O}^{\\prime}\\in{\\cal O}$ with coefficients for $\\alpha,\\beta\\in\\mathbb{R}$ , which is conserved because the Poisson bracket is linear in both arguments. Also, we can take the product $O O^{\\prime}\\in O$ , with $(O O^{\\prime})({\\pmb x})=$ $O(x)O^{\\prime}(x)$ , which is conserved because the Poisson bracket satisfies Leibniz\u2019s law of differentiation $\\{\\dot{O O^{\\prime}},\\dot{H}\\}\\,=\\,\\{O,H\\}O^{\\prime}+O\\{O^{\\prime},H\\}$ . Finally, the Poisson bracket of the conserved quantities $\\{O,O^{\\prime}\\}\\in{\\mathcal{O}}$ is also conserved, because of the Jacobi identity. ", "page_idx": 2}, {"type": "text", "text": "Symmetries generated by observables Referring back as to the Hamiltonian equations of motion in Equation (1), note that these equations work not just for the Hamiltonian $H\\in{\\mathcal{O}}$ of the system, but for any observable $O\\in{\\mathcal{O}}$ . So given any starting point $x_{0}$ , we can generate a trajectory $\\ensuremath{\\boldsymbol{{x}}}(\\tau)$ satisfying ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{x}(0)=\\pmb{x}_{0}\\qquad\\qquad\\qquad\\qquad\\dot{\\pmb{x}}(\\tau)=J\\nabla O(\\pmb{x}(\\tau)).}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "We have used a different symbol to not conflate the ODE time $\\tau$ with regular time $t$ of the trajectory generated by the Hamiltonian. Denote the flow associated to this ODE generated by observable $O$ by $\\Phi_{O}^{\\tau}:\\mathcal{M}\\to\\mathcal{M}$ , mapping $\\scriptstyle x_{0}$ to $\\Phi_{O}^{\\tau}(x_{0})=x(\\tau)$ . Note that any ODE flow satisfies $\\Phi_{O}^{0}=\\mathrm{id}_{\\mathcal{M}}$ and $\\Phi_{O}^{\\tau+\\kappa}=\\Phi_{O}^{\\tau}\\circ\\Phi_{O}^{\\kappa}$ . Hence, the observable $O$ generates a one-dimensional group $\\mathcal{G}_{O}$ , parametrized by $\\tau$ , that is a subgroup of the group $\\mathrm{Diff}({\\mathcal{M}})$ of diffeomorphisms $\\mathcal{M}\\rightarrow\\mathcal{M}$ . ", "page_idx": 2}, {"type": "text", "text": "Theorem 1 (Noether). The observable $O\\in{\\mathcal{O}}$ is a conserved quantity on the trajectories generated by Hamiltonian $H\\in{\\mathcal{O}}$ if and only if $H$ is invariant to $g_{O}$ , meaning that for all $\\tau\\in\\mathbb{R},$ , $H\\circ\\Phi_{O}^{\\tau}=H$ . ", "page_idx": 2}, {"type": "text", "text": "Proof. By reasoning analogous to that in Equation (2), the value of the Hamiltonian changes under the flow generated by observable $O$ as $\\begin{array}{r}{\\frac{\\mathrm{d}H}{\\mathrm{d}\\tau}=\\{H,O\\}}\\end{array}$ . Noting that the Poisson bracket is anti-symmetric, we have that: $O$ is a conserved quantity $\\iff\\{{\\bar{O}},H\\}=0\\iff\\{H,O\\}=0\\iff H$ is invariant to the flow generated by $O$ . \u53e3 ", "page_idx": 2}, {"type": "text", "text": "2.4 Automatic symmetry discovery ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Symmetries play an important role in machine learning models, most notably group invariance and equivariance constraints [Cohen and Welling, 2016]. Instead of having to define symmetries explicitly in advance, recent attempts have been made to learn symmetries automatically from data. Even if learnable symmetries can be differentiably parameterised, learning them can remain difficult as symmetries act as constraints on the functions a model can represent and are, therefore, not encouraged by objectives that solely optimise train data fit. As a result, even if a symmetry would lead to better test generalisation, the training collapses into selecting no symmetry. Common ways to overcome this are designing explicit regularisers that encourage symmetry [Benton et al., 2020, van der Ouderaa et al., 2022], which often require tuning, or use of validation data [Alet et al., 2021, Maile et al., 2022, Zhou et al., 2020]. Learning symmetries for integrable systems was proposed in [Bondesan and Lamacraft, 2019], whereas our framework works more generally also for nonintegrable systems, such as the 3-body problem. Recent works have demonstrated effectivity of Bayesian model selection to learn symmetries directly from training data. This works by optimising the marginal likelihood, which embodies an Occam\u2019s razor effect that trades off data fit and model complexity. For Gaussian processes, the quantity can often be computed in closed-form [van der Wilk et al., 2018], and can be scaled to neural networks through variational inference [van der Ouderaa and van der Wilk, 2021] and linearised Laplace approximations [Immer et al., 2022]. ", "page_idx": 2}, {"type": "text", "text": "3 Symmetrising Hamiltonians with Conserved Quantities ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Our method introduced in the next section will learn the Hamiltonian of a system together with a set of conserved quantities. First, in this section we discuss how the learned conserved quantities will be parametrised, and how we can make the Hamiltonian invariant to the symmetry generated by conserved quantities. ", "page_idx": 3}, {"type": "text", "text": "3.1 Parameterising conserved quantities ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this work, we limit ourselves to modelling up to a fixed maximum number of $K$ conserved quantities C\u03b71, C\u03b72, . . $C_{\\eta}^{1},C_{\\eta}^{2},\\dots,C_{\\eta}^{K}:\\mathcal{M}\\to\\mathbb{R}$ are observables parameterised by symmetrisation parameters $\\eta$ , to distinguish them from the model parameters $\\pmb{\\theta}$ parameterising the Hamiltonian scalar field. ", "page_idx": 3}, {"type": "text", "text": "In this paper, we consider quadratic conserved quantities of the form $C_{\\pmb{\\eta}}(\\pmb{x})=\\pmb{x}^{T}\\pmb{A}\\pmb{x}/2+\\pmb{b}^{T}\\pmb{x}+c.$ . As we use the conserved quantities only through their gradients, the constant is arbitrary and can be ignored. The learnable symmetrisation parameters are thus ${\\boldsymbol{\\eta}}=\\{{\\boldsymbol{A}},{\\boldsymbol{b}}\\}$ , for a symmetric matrix $\\pmb{A}$ . A quadratic conserved quantity $C$ generates a symmetry transformation whose scalar field $\\dot{\\pmb{x}}=J\\bar{\\nabla}C(\\pmb{x})=J\\pmb{A}\\pmb{x}+J\\pmb{b}$ is affine, or linear on the homogeneous coordinates $(x,1)$ . Its flow can be analytically solved ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left[\\Phi_{C}^{\\tau}(\\pmb{x})\\right]=\\exp\\left(\\tau\\left[\\begin{array}{c c}{J\\pmb{A}}&{J\\pmb{b}}\\\\ {\\mathbf{0}^{T}}&{0}\\end{array}\\right]\\right)\\left[\\pmb{x}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "using the matrix exponential $\\exp(\\cdot)$ for which efficient numerical algorithms exist [Moler and Van Loan, 2003]. This equation can be verified to have the correct scalar field and boundary condition, and thus forms the unique solution to the ODE in Equation (3). ", "page_idx": 3}, {"type": "text", "text": "3.2 Symmetrising observables ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Given an observable $C\\in{\\mathcal{O}}$ , we want to transform an observable $f$ into $\\hat{f}$ that is invariant to the transformations generated by $C$ . This means that $\\hat{f}\\circ\\Phi_{C}^{\\tau}=\\hat{f}$ for all symmetry time $\\tau\\in\\mathbb R$ . Via Noether\u2019s theorem, we know that this is equivalent to $C$ being conserved in the trajectories generated by $f$ , and also equivalent to $\\{C,\\hat{f}\\}=0$ . However, this equation does not prescribe how to obtain such $\\hat{f}$ . Instead, we\u2019ll create $\\hat{f}$ by symmetrizing over the symmetry group generated by $C$ . This is done by averaging over the orbit of the transformation ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\hat{f}(\\pmb{x})=\\int_{\\mathbb{R}}f(\\Phi_{C}^{\\tau}(\\pmb{x}))\\mu(\\tau).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "with a measure $\\mu$ over symmetry time $\\tau$ . This measure $\\mu$ induces a measure on the 1-dimensional subgroup $g_{C}$ of the group of diffeomorphisms $\\mathcal{M}\\,\\rightarrow\\,\\mathcal{M}$ . If this measure on $\\mathcal{G}_{C}$ is uniform (specifically, a right-invariant measure [Halmos, 1950]), then $\\hat{f}$ is indeed invariant. ", "page_idx": 3}, {"type": "text", "text": "Instead of a single symmetry generator, we can also have a set $\\mathcal{C}=\\{C_{1},...,C_{K}\\}$ of observables and we want to make $f$ invariant to all of these. Assume that this set spans a vector space of observables that is closed under the Poisson bracket (i.e. they form a Lie subalgebra). In that case, the groups of tirs apnasrfaormmetaetirioznesd  obf yt hae  voebcsteorrv oafb lseys mcommetbriyn teidm gees .r oTuhp $\\mathcal{G}_{\\mathcal{C}}$ r[rHesalpl,o n2d0i1n5g,  fTlohwm .i s5 $\\tau\\in\\mathbb{R}^{\\breve{K}}$ $\\bar{\\Phi}_{\\mathcal{C}}^{\\mathcal{\\bar{\\tau}}}=\\bar{\\Phi}_{\\sum_{i}\\tau_{i}C_{i}}^{1}\\bar{\\Psi_{i}}$ To make an observable $f$ invariant to the symmetries of all conserverved quantities $\\mathcal{C}$ , equivalently to the group $\\mathcal{G}_{\\mathcal{C}}$ , we symmetrize ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\hat{f}(\\pmb{x})=\\int_{\\mathbb{R}^{K}}f(\\Phi_{\\mathcal{C}}^{\\tau}(\\pmb{x}))\\mu(\\pmb{\\tau}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "with some measure $\\mu$ over $\\mathbb{R}^{K}$ . As before, if this induces a uniform measure over $\\mathcal{G}_{\\mathcal{C}}$ , then this symmetrization indeed makes $\\hat{f}$ invariant to $\\mathcal{G}_{\\mathcal{C}}$ . ", "page_idx": 3}, {"type": "text", "text": "However, a probability measure $\\mu(\\tau)$ that gives a uniform distribution over $\\mathcal{G}_{\\mathcal{C}}$ might not exist, for example when the group contains a non-compact group of translations. Even when such a measure does exist, it may be hard to construct, and the symmetrisation integral in Equation (5) may be intractable to compute. So instead, in practice, we approximate this by choosing $\\mu(\\tau)$ to be a unit normal distribution $\\mathcal{N}(0,\\pmb{I}_{K})$ or uniform distribution. This results in a relaxed notion of symmetry in $\\hat{f}$ which can be interpreted as a form of robustness to actions of the symmetry group implied by the conserved quantity, by smoothing the function in this direction around data, in contrast to strict invariance by definition closed under group actions along the full orbit. Finally, we approximate the integral by an unbiased Monte Carlo estimate with $S$ samples. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "4 Automatic Symmetry Discovery using Noether\u2019s Razor ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Now that we have a way of parameterising symmetry differentiably as conservation laws through Noether\u2019s theorem, we need an objective function that is capable of selecting the right symmetry. Unfortunately, regular training objectives that only rely on data fti can not necessarily distinguish the correct inductive bias, as noted in prior work [van der Wilk et al., 2018, Immer et al., 2022, van der Ouderaa and van der Wilk, 2021]. This is because, even if train data originates from a symmetric distribution, there can be both non-symmetric and symmetric solutions that fit the train data equally well, given a sufficiently flexible model. Consequently, the regular maximum likelihood objective that only measures train data fit will not necessarily favour a symmetric model, even if we expect this to generalise best on test data. Instead of having to resort to cross-validation to select the right symmetry inductive bias, we propose to use an approximate marginal likelihood on the train data. This has the additional benefit of being differentiable, allowing symmetrisation to be learned with back-propagation along with regular parameters in a single training procedure. In our case, we use Noether\u2019s theorem to parameterise symmetries in our prior through conserved quantities, which we can optimise with back-propagation using a differentiable lower bound on the marginal likelihood. This quantity, also known as the \u2018evidence\u2019, differs distinctly from maximum likelihood in that it balances both train fti as well as model complexity. The Occam\u2019s razor effect encourages symmetry and leverages the symmetrisation process to \u2018cut away\u2019 prior density over Hamiltonian functions that are not symmetric, if this does not result in a worse data fit. The resulting posterior predictions automatically becomes symmetric if observed data obeys a symmetry (high evidence for symmetry), but can become non-symmetric if this does not match the data (low evidence for symmetry). Hence, the name of our proposed method for automatic inductive bias selection is Noether\u2019s razor. ", "page_idx": 4}, {"type": "text", "text": "4.1 Probabilistic model with symmetries embedded in the prior. ", "text_level": 1, "page_idx": 4}, {"type": "image", "img_path": "dpvqBkEp1f/tmp/34b172856fe2569e880fa621eb3dc69c2504896600aabe0b5c7a6abaec385a34.jpg", "img_caption": [], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "To be more explicit about our probabilistic model, we can introduce four variables, namely a non-symmetrised observable $F_{\\theta}$ , a set of conserved quantities $C_{\\eta}$ , which induce a symmetrised Hamiltonian $H$ generating the observed trajectory data $X$ . We treat trajectory data an an observed variable, consider the conserved quantities as part of an empirical prior as we optimise over them, and integrate out the Hamiltonian as latent. The construction can be interpreted a placing a sophisticated prior over the functions that the symmetrised Hamiltonian $H$ can represent, which is the variable of primary interest. The underlying non-symmetrised $F_{\\theta}$ does not have a direct physical meaning as $H$ does, but defines a prior over neural networks to flexibly define a density over a rich class of possible functions. The conserved quantities $C_{\\eta}$ control the amount of symmetry in the effective prior over symmetrised Hamiltonians $H$ . Empirically optimising $C_{\\eta}$ through Bayesian model selection allows us to \u2018cut ", "page_idx": 4}, {"type": "text", "text": "Figure 1: Graphical probabilistic model. Trajectory data $X$ depends on a symmetrised Hamiltonian $H$ induced by non-symmetrised observable $F$ and conservation laws $C$ . ", "page_idx": 4}, {"type": "text", "text": "away\u2019 density in the prior over $H$ that correspond to functions that are not symmetric - as the symmetrisation averages functions in $F_{\\theta}$ that lie in the same orbit and thereby increases the relative density of symmetric functions in $H$ . We hypothesise that we will not over-fti conserved quantities as $\\eta$ is relatively low-dimensional, only representing quadratic functions, while we integrate out the high-dimensional neural network model parameters $\\pmb{\\theta}$ that parameterises the observable $F_{\\theta}$ . In future work, it would be interesting to explore a richer function classes for conserved quantities, such as neural networks, although we do expect this to be more difficult and to require additional priors or regularisation techniques to avoid over-fitting. ", "page_idx": 4}, {"type": "text", "text": "4.2 Bayesian model selection for symmetry discovery ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "To learn the right symmetry from data, we propose to use Bayesian model selection through optimisation of the marginal likelihood. In the previous sections, we have phrased symmetries parameterised by $\\eta$ as part of the prior over Hamiltonians. The symmetry parameters $\\eta$ parameterise the space of possible \u2018models\u2019 that we consider, whereas the model parameters $\\pmb{\\theta}$ parameterise the weights of a single model. To perform Bayesian model selection on the symmetries, we are interested in computing the marginal likelihood: ", "page_idx": 5}, {"type": "equation", "text": "$$\np(\\pmb{x}|\\pmb{\\eta})=\\int_{\\pmb{\\theta}}p(\\pmb{x}|\\pmb{\\theta},\\pmb{\\eta})p(\\pmb{\\theta})\\mathrm{d}\\pmb{\\theta}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "which requires integrating (marginalising) the likelihood over model parameters $\\pmb{\\theta}$ weighted by the prior, and is sometimes referred to as the \u2018evidence\u2019 for a particular model. Unlike maximum likelihood, the marginal likelihood has an Occam\u2019s razor effect [Smith and Spiegelhalter, 1980, Rasmussen and Ghahramani, 2000] that balancing both data fit and model complexity, allowing optimisation of symmetry parameters $\\eta$ . Although the marginal likelihood is typically intractable, certain approximate Bayesian inference techniques can provide differentiable estimates. In the next sections, we will use variational inference to derive a tractable and differentiable lower bound to the marginal likelihood that can be used to find a posterior over $\\pmb{\\theta}$ and optimise symmetries $\\eta$ . ", "page_idx": 5}, {"type": "text", "text": "Why the marginal likelihood can learn symmetry To understand why the marginal likelihood objective is capable of learning the right symmetry (to learn $\\eta$ ), Sec. 3.2 [van der Wilk et al., 2018] proposed to decompose it through the product rule: ", "page_idx": 5}, {"type": "equation", "text": "$$\np(\\mathbf{x}\\mid\\eta)=p(\\mathbf{x}_{1}\\mid\\eta)p(\\mathbf{x}_{2}|\\mathbf{x}_{1},\\eta)p(x_{3}\\mid x_{1:2},\\eta)\\prod_{c=4}^{C}p(x_{c}\\mid x_{1:c-1},\\eta)\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "which shows that the marginal likelihood measures how much parts of the dataset predict other parts of the data - a measure of generalisation that does not require cross-validation. Given a perfect data fit, the marginal likelihood will be higher when the right symmetry is selected, as parts of the dataset will result in better and more certain predictions on other part of the data. This is unlike the maximum likelihood, which is always maximised with perfect data fit, with or without the right symmetry. For some posterior approximations, such as linearised Laplace approximations, it can be analytically shown that symmetry maximises the approximate marginal likelihood (App. G.2 of Immer et al. [2022]). Our method is very similar, but uses more expressive variational inference which can optimise the posterior globally, rather than relying on a local Taylor expansion. ", "page_idx": 5}, {"type": "text", "text": "4.3 Lower bounding the marginal likelihood ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The marginal likelihood of an Hamiltonian neural network is typically not tractable in closed-form. However, we can derive a lower bound to the marginal likelihood using variational inference (VI): ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{og}\\,p(x\\mid\\eta)\\geq\\mathbb{E}_{\\theta}\\left[\\log p(x\\mid\\theta,\\eta)\\right]-\\mathrm{KL}(q_{m,S}(\\theta)\\mid\\mid p(\\theta))}\\\\ &{\\qquad\\qquad\\qquad\\geq\\mathbb{E}_{\\theta}\\left[\\mathbb{E}_{\\tau}\\left[\\sum_{i=1}^{N}\\log N(x_{t^{\\prime}}^{i}\\mid\\hat{H}_{\\theta,\\eta}^{\\tau}(x_{t}^{i}),\\sigma_{\\mathrm{dat}}^{2}I)\\right]\\right]-\\mathrm{KL}(q_{m,S}(\\theta)\\mid\\mid p(\\theta\\mid0,\\sigma_{\\mathrm{pior}}^{2}I))}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\begin{array}{r}{\\widehat{H}_{\\pmb{\\theta},\\eta}^{\\tau}(\\pmb{x}_{t}^{i})=\\frac{1}{S}\\sum_{s=1}^{S}H_{\\pmb{\\theta},\\eta}(\\Phi_{\\eta}^{\\tau^{(s)}}(\\pmb{x}_{t}^{i}))}\\end{array}$ and $\\widehat{H}$ is an unbiased $S$ -sample Monte Carlo estimator of the  symmetrised Hamiltonian. We write $\\mathbb{E}_{\\theta}:=\\mathbb{E}_{\\theta}{\\sim}q_{m,S}$ and $\\begin{array}{r}{\\mathbb{E}_{\\tau}=\\mathbb{E}_{\\tau\\sim\\prod_{s=1}^{S}\\mu(\\tau)}}\\end{array}$ for which we can obtain an unbiased estimate by taking Monte Carlo samples. The first inequality is the standard VI lower bound. The second inequality follows from applying Jensen\u2019s inequality (again) which uses the fact that the log likelihood is a convex function. Similar lower bounds to invariant models that average over a symmetry group have recently appeared in prior work [van der Ouderaa and van der Wilk, 2021, Schw\u00f6bel et al., 2022, Nabarro et al., 2022]. Full derivation in Appendix A.1. ", "page_idx": 5}, {"type": "text", "text": "4.4 Improved variational inference for scalable Bayesian model selection ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Variational inference is a common tool to perform Bayesian inference on models with intractable marginal likelihoods, including neural networks. In deep learning literature, however, its use is typically limited to better predictive uncertainty estimation and rarely for Bayesian model selection. Meanwhile, linearised Laplace approximations have recently been successfully applied to Bayesian model selection [Immer et al., 2021] and symmetry learning in specific [Immer et al., 2022, van der Ouderaa et al., 2024], with a few reported cases of model selection using VI only in single neural network layers [van der Ouderaa and van der Wilk, 2021, Schw\u00f6bel et al., 2021]. Optimising Bayesian neural networks with variational inference is much less established than training regular neural networks, for which many useful heuristics are available. This work, however, provides evidence that it is also possible to perform approximate Bayesian model selection using VI in deep neural networks, which we deem an interesting observation in its own right. To make sure the lower bound on the marginal likelihood is sufficiently tight, we employ a series of techniques, including a richer non-mean field family of matrix normal posteriors [Louizos and Welling, 2016], and closedform updates of the prior precision and output variances derived with expectation maximisation. Details on how we train a Bayesian neural network using variational inference can be found in Appendix D. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "5 Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we will discuss how the learned symmetries are analysed and then list our experiments and results. ", "page_idx": 6}, {"type": "text", "text": "5.1 Analyzing learned symmetries ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In our experiments, we will find a set of $K$ conserved quantities $C_{k}:\\mathcal{M}\\rightarrow\\mathbb{R}$ . As we consider quadratic conserved quantities in particular, we can equivalently analyze the resulting generators of the associated symmetries $\\hat{G}_{k}(\\pmb{x})=J\\nabla C_{k}$ which are affine and thus representable with a matrix $\\hat{G}_{k}\\in\\mathbb{R}^{(M+1)\\times(M+1)}$ on homogeneous coordinates $(x,1)$ . In Appendix B, we list for each system the $L$ ground truth conserved quantities generators $G_{l}^{\\star}$ . The learned and ground truth generators can be stacked in to the matrices $\\hat{G}\\in\\mathbb{R}^{K\\times(M+1)^{2}}$ , $\\stackrel{\\cdot}{G}^{*}\\,\\in\\,\\mathbb{R}^{L\\times(M+1)^{2}}$ respectively. As we can identify the symmetries only up to linear combinations, we have learned the correct symmetries if the learned generators span a linear subspace of $\\mathbb{R}^{(M+1)^{2}}$ that coincides with the space spanned by the ground truth generators. To verify this, we test two properties. First, we show that the matrix $\\hat{G}$ has $L$ non-zero singular values. Secondly, for the first $L$ right singular vectors $v_{i}\\in\\mathbb{R}^{(M+1)^{2}}$ , we decompose $v_{i}=v_{i}^{\\bar{\\|}}+v_{i}^{\\perp}$ in a vector in ground truth subspace, and one orthogonal to it. The learned $v_{i}$ is a correct conserved quantity if $v_{i}^{\\perp}=0$ , or equivalently, because the singular vectors are normalized, if $\\|v_{i}^{\\parallel}\\|=1$ . We call this measure the \u201cparallelness\u201d. ", "page_idx": 6}, {"type": "text", "text": "5.2 Simple Harmonic Oscillator ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We start with a demonstration on the simple harmonic oscillator. This text book example has a 2-dimensional phase space, making learned Hamiltonians amenable to visualisation. Further, it has a clear rotational symmetry SO(2), relating to the conserved phase. On a finite set of generated train data, we model the Hamiltonian using a vanilla HNN, our symmetry learning ", "page_idx": 6}, {"type": "image", "img_path": "dpvqBkEp1f/tmp/70b446307e3c305d7128ed549c5885b5afccda2ded8813c70146603fbe7f5d62.jpg", "img_caption": ["Figure 2: Learned Hamiltonians on phase space of simple harmonic oscillator by HNN models. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "method, and a model with true symmetry built-in as reference oracle (experimental details in Appendix B.1). In Figure 2, we find that our symmetry learning method results in a rotationally invariant Hamiltonian that matches the fixed rotational SO(2) symmetry. Further away from the origin, the learned Hamiltonian differs from the ground truth Hamiltonian, as there is no data in that region. In Table 1, we find that the learned symmetry has a better ELBO on the train set and matches the improved predictive performance of the model with the correct symmetry built-in. The symmetry learning method outperforms the vanilla model in terms of predictive performance on the test set. ", "page_idx": 6}, {"type": "text", "text": "Table 1: Learning Hamiltonian dynamics of the simple harmonic oscillator. We compare a vanilla HNN, our symmetry learning method, and a model with the correct SO(2) symmetry built-in as reference oracle. Our method achieves reference oracle performance, indicating correct symmetry learning, and outperforms the vanilla model by improving predictive performance on the test set. ", "page_idx": 7}, {"type": "table", "img_path": "dpvqBkEp1f/tmp/e2670b5b4fcca5eb83f3e8417df04628da0cffdd267d53295903382ae93f5ae6.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "5.3 $n$ \u2212Harmonic Oscillators ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Now, we consider $n$ \u2212harmonic oscillators. This system has as symmetry group the unitary Lie group $\\mathrm{U}(n)$ of dimensionality of $\\bar{n}^{2}$ (see Appendix B.2). We sample random trajectories from phase space and train a HNN neural network without and with symmetry learning using variational inference. Again, we find improved ELBO and test performance for learned symmetries Table 2. Following the protocol from Section 5.1, we analyze the learned symmetries. In Figure 3 (right), we see that for varying $n$ , we indeed find that the matrix of learned symmetries has $n^{2}$ nonzero singular values. Furthermore, the first $n^{2}$ singular vectors lie in the ground truth subspace of generators with measured parallelness $\\|v_{i}^{\\|}\\|>0.99$ , as seen in Figure 3 (left). This shows that the $U(n)$ symmetry is corectly learned. ", "page_idx": 7}, {"type": "image", "img_path": "dpvqBkEp1f/tmp/ebe817979fe63587e8a40f8a4ea43d894681d18630492a360561dd40bbdbd01b.jpg", "img_caption": ["Figure 3: Singular value and parallelness of the singular vectors of the learned generators, for $n$ oscilators. $\\mathrm{U}(n)$ is correctly learned. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Table 2: Learning Hamiltonian dynamics of 3\u2212fold harmonic oscillators. We compare HNN with symmetry learning to a vanilla HNN without symmetry learning and to the correct U(3) symmetry built-in as fixed reference oracle. We find that our method can discover the correct symmetry, achieves reference oracle performance, and outperforms vanilla training in both ELBO and test performance. ", "page_idx": 7}, {"type": "table", "img_path": "dpvqBkEp1f/tmp/4cb891e901f9d5a69630109b08ebaf0c960579113f3d4332a7f7915677da0562.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "5.4 $n$ -Body System ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "To investigate performance of our method on more interesting systems, we consider learning the Hamiltonian of an $n$ -body system with gravitational interaction. We use 3 bodies in 2 dimensions so that trajectories and generators remain easy to visualise. As the Hamiltonian depends only on the norm of the momenta and on positions via the relative distances of the bodies, the three dimensional group SE(2) of rototranslations is an invariance of the ground truth Hamiltonian. However, as explained in Appendix B.3, the Hamiltonian has four more quadratic conserved quantities. They generate a 7-dimensional ", "page_idx": 7}, {"type": "image", "img_path": "dpvqBkEp1f/tmp/cf292fc55264ee28095faab42185eaa0e5e3e7d42780acd993ea6ee14694a9e6.jpg", "img_caption": ["Figure 4: Singular value and parallelness of the singular vectors of the learned generators for three body system in two dimensions. The 7-dimensional Lie group $\\mathcal{G}$ of quadratic conserved quantities is correctly learned. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Lie group $\\mathcal{G}$ of symmetries. This group has the same orbits on the phase space as SE(2). Therefore, a function being invariant to SE(2) is equivalent to it being invariant to $\\mathcal{G}$ . We\u2019ll find that Noether\u2019s razor discovers not just SE(2), but all seven generators of $\\mathcal{G}$ . ", "page_idx": 7}, {"type": "text", "text": "Figure 5: Learned generators associated by conserved quantities and their singular value decomposition. We find a subspace spanned by the 7 linear generators that correspond to the correct symmetries (see Appendix B.3): (1) rotation of the center of mass $R^{\\mathrm{COM}}$ , (2) rotation around the origin $R^{\\mathrm{ABS}}$ , $(4\\!+\\!5)$ translation, $(5\\mathrm{+}6\\mathrm{+}7)$ momentum-dependent translations $P,Q$ , $(8+9+10)$ ) inactive $\\lambda<0.05)$ . The first 7 singular vectors lie in the ground truth subspace of generators with measured parallelness $||v_{i}^{||}||>0.95$ . ", "page_idx": 8}, {"type": "text", "text": "In Table 3, we compare performance of a vanilla variational HNN with our symmetry learning approach and a model that has the appropriate SE(2) symmetry of rototranslations built-in as an reference oracle. We find that our method is able to automatically discover the conserved quantities and associated generators that span the symmetry group. The model achieves the same performance as the model with the symmetry built-in as reference oracle, but without having required the prior knowledge. Compared to the vanilla baseline, our approach improves test accuracy on both indistribution as out-of-distribution test sets. ", "page_idx": 8}, {"type": "text", "text": "Table 3: Learning Hamiltonian dynamics of 2d 3-body system with variational Hamiltonian neural networks (HNN). We compare our symmetry learning method to a vanilla model without symmetry learning and a model with the correct SE(2) symmetry built-in as a reference oracle. Our method capable of discoverying symmetry achieves the oracle performance, outperforming the vanilla method. ", "page_idx": 8}, {"type": "table", "img_path": "dpvqBkEp1f/tmp/8b6a80ba82cf4874d7216caa1d0ac63318f14ca32f0a27f4f5271f79d82b42aa.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "After training, we can analyse the learned conserved quantities and implied symmetries by inspecting their associated generators. In Figure 5, we plot these generators as well as their singular value decomposition. We see that our method correctly learns 7 singular values with $\\lambda_{i}>0.05$ and the associated singular vectors lie in the ground truth subspace with $\\|v_{i}^{\\|}\\|>0.95$ . This indicates that our method is in fact capable of inferring the right symmetries from train data, beyond merely improving generalisation by improving predictive performance on the test set. ", "page_idx": 8}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this work, we propose to use Noether\u2019s theorem to parameterise symmetries in machine learning models of dynamical systems in terms of conserved quantities. Secondly, we propose to leverage the Occam\u2019s razor effect of Bayesian model selection by phrasing symmetries implied by conserved quantities in the prior and learning them by optimising an approximate marginal likelihood directly on train data, which does not require validation data or explicit regularisation of the conserved quantities. Our approach, dubbed Noether\u2019s razor, encourages symmetries by balancing both data fti and model complexity. We derive a variational lower bound on the marginal likelihood providing a concrete objective capable of jointly learning the neural network as well as the conserved quantities that symmetrise the Hamiltonian. As far as we know, this is also the first time differentiable Bayesian model selection using variational inference has been demonstrated on deep neural networks. We demonstrate our approach on $n$ -harmonic oscillators and $n$ -body systems. We find that our method learns the correct conserved quantities by analysing the singular values and correctness of the subspace spanned by the generators implied by learned conserved quantitites. Further, we find that our method performs on-par with models with the true symmetries built-in explicitly and we outperform vanilla model, improving generalisation and predictive accuracies on test data. ", "page_idx": 8}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Ferran Alet, Dylan Doblar, Allan Zhou, Josh Tenenbaum, Kenji Kawaguchi, and Chelsea Finn. Noether networks: meta-learning useful conserved quantities. Advances in Neural Information Processing Systems, 34, 2021.   \nJean-Pierre Amiet and Stefan Weigert. Commensurate harmonic oscillators: Classical symmetries. J. Math. Phys., 43(8):4110\u20134126, August 2002. URL http://dx.doi.org/10.1063/1.1488672.   \nV I Arnold. Mathematical methods of classical mechanics, 1989.   \nJohn C Baez. Getting to the bottom of noether\u2019s theorem, 2020. URL http://arxiv.org/abs/ 2006.14741.   \nGregory Benton, Marc Finzi, Pavel Izmailov, and Andrew Gordon Wilson. Learning invariances in neural networks. arXiv preprint arXiv:2010.11882, 2020.   \nRoberto Bondesan and Austen Lamacraft. Learning symmetries of classical integrable systems. arXiv preprint arXiv:1906.04645, 2019.   \nRicky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary differential equations. Advances in neural information processing systems, 31, 2018.   \nTaco Cohen and Max Welling. Group equivariant convolutional networks. In International conference on machine learning, pages 2990\u20132999. PMLR, 2016.   \nMiles Cranmer, Sam Greydanus, Stephan Hoyer, Peter Battaglia, David Spergel, and Shirley Ho. Lagrangian neural networks. arXiv preprint arXiv:2003.04630, 2020.   \nEkin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V Le. Autoaugment: Learning augmentation policies from data. arXiv preprint arXiv:1805.09501, 2018.   \nMarco David and Florian M\u00e9hats. Symplectic learning for hamiltonian neural networks. Journal of Computational Physics, 494:112495, 2023.   \nMarc Finzi, Gregory Benton, and Andrew G Wilson. Residual pathway priors for soft equivariance constraints. Advances in Neural Information Processing Systems, 34, 2021.   \nSamuel Greydanus, Misko Dzamba, and Jason Yosinski. Hamiltonian neural networks. Advances in neural information processing systems, 32, 2019.   \nRoger Grosse and James Martens. A kronecker-factored approximate fisher matrix for convolution layers. In International Conference on Machine Learning, pages 573\u2013582. PMLR, 2016.   \nBrian C Hall. Lie Groups, Lie Algebras, and Representations. Springer International Publishing, 2015. URL https://link.springer.com/book/10.1007/978-3-319-13467-3.   \nPaul R Halmos. Measure Theory. Springer New York, 1950. URL https://link.springer.com/ book/10.1007/978-1-4684-9440-2.   \nIrina Higgins, Loic Matthey, Arka Pal, Christopher P Burgess, Xavier Glorot, Matthew M Botvinick, Shakir Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual concepts with a constrained variational framework. ICLR (Poster), 3, 2017.   \nMatthew D Hoffman, David M Blei, Chong Wang, and John Paisley. Stochastic variational inference. Journal of Machine Learning Research, 2013.   \nAlexander Immer, Matthias Bauer, Vincent Fortuin, Gunnar R\u00e4tsch, and Khan Mohammad Emtiyaz. Scalable marginal likelihood estimation for model selection in deep learning. In International Conference on Machine Learning, pages 4563\u20134573. PMLR, 2021.   \nAlexander Immer, Tycho F. A. van der Ouderaa, Vincent Fortuin, Gunnar R\u00e4tsch, and Mark van der Wilk. Invariance learning in deep neural networks with differentiable laplace approximations, 2022. ", "page_idx": 9}, {"type": "text", "text": "Patrick Kidger. On neural differential equations. arXiv preprint arXiv:2202.02435, 2022. ", "page_idx": 10}, {"type": "text", "text": "Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.   \nChristos Louizos and Max Welling. Structured and efficient variational deep learning with matrix gaussian posteriors. In International conference on machine learning, pages 1708\u20131716. PMLR, 2016.   \nKaitlin Maile, Dennis George Wilson, and Patrick Forr\u00e9. Equivariance-aware architectural optimization of neural networks. In The Eleventh International Conference on Learning Representations, 2022.   \nCleve Moler and Charles Van Loan. Nineteen dubious ways to compute the exponential of a matrix, twenty-five years later. SIAM review, 45(1):3\u201349, 2003.   \nSeth Nabarro, Stoil Ganev, Adri\u00e0 Garriga-Alonso, Vincent Fortuin, Mark van der Wilk, and Laurence Aitchison. Data augmentation in bayesian neural networks and the cold posterior effect. In Uncertainty in Artificial Intelligence, pages 1434\u20131444. PMLR, 2022.   \nE. Noether. Invariante variationsprobleme. K\u00f6niglich Gesellschaft der Wissenschaften G\u00f6ttingen Nachrichten Mathematik-physik Klasse, 1918.   \nCarl Rasmussen and Zoubin Ghahramani. Occam\u2019s razor. Advances in neural information processing systems, 13, 2000.   \nDanilo Jimenez Rezende, S\u00e9bastien Racani\u00e8re, Irina Higgins, and Peter Toth. Equivariant hamiltonian flows. arXiv preprint arXiv:1909.13739, 2019.   \nMagnus Ross and Markus Heinonen. Learning energy conserving dynamics efficiently with hamiltonian gaussian processes. arXiv preprint arXiv:2303.01925, 2023.   \nPola Schw\u00f6bel, Martin J\u00f8rgensen, Sebastian W Ober, and Mark van der Wilk. Last layer marginal likelihood for invariance learning. arXiv preprint arXiv:2106.07512, 2021.   \nPola Schw\u00f6bel, Martin J\u00f8rgensen, Sebastian W Ober, and Mark Van Der Wilk. Last layer marginal likelihood for invariance learning. In International Conference on Artificial Intelligence and Statistics, pages 3542\u20133555. PMLR, 2022.   \nAdrian FM Smith and David J Spiegelhalter. Bayes factors and choice criteria for linear models. Journal of the Royal Statistical Society: Series B (Methodological), 42(2):213\u2013220, 1980.   \nYusuke Tanaka, Tomoharu Iwata, et al. Symplectic spectrum gaussian processes: Learning hamiltonians from noisy and sparse data. Advances in Neural Information Processing Systems, 35: 20795\u201320808, 2022.   \nPeter Toth, Danilo Jimenez Rezende, Andrew Jaegle, S\u00e9bastien Racani\u00e8re, Aleksandar Botev, and Irina Higgins. Hamiltonian generative networks. arXiv preprint arXiv:1909.13789, 2019.   \nTycho van der Ouderaa, Alexander Immer, and Mark van der Wilk. Learning layer-wise equivariances automatically using gradients. Advances in Neural Information Processing Systems, 36, 2024.   \nTycho FA van der Ouderaa and Mark van der Wilk. Learning invariant weights in neural networks. In Workshop in Uncertainty & Robustness in Deep Learning, ICML, 2021.   \nTycho FA van der Ouderaa, David W Romero, and Mark van der Wilk. Relaxing equivariance constraints with non-stationary continuous filters. arXiv preprint arXiv:2204.07178, 2022.   \nMark van der Wilk, Matthias Bauer, ST John, and James Hensman. Learning invariances using the marginal likelihood. arXiv preprint arXiv:1808.05563, 2018.   \nYaofeng Desmond Zhong, Biswadip Dey, and Amit Chakraborty. Symplectic ode-net: Learning hamiltonian dynamics with control. arXiv preprint arXiv:1909.12077, 2019.   \nAllan Zhou, Tom Knowles, and Chelsea Finn. Meta-learning symmetries by reparameterization. arXiv preprint arXiv:2007.02933, 2020. ", "page_idx": 10}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 11}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 11}, {"type": "text", "text": "Justification: The paper proposes a new method that can automatically discover correct symmetries and improved predictive performance on hold-out test data. This is justified by experiments that measure the correctness of symmetries and evaluate test performance. ", "page_idx": 11}, {"type": "text", "text": "Guidelines: ", "page_idx": 11}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 11}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 11}, {"type": "text", "text": "Justification: The paper states modelling assumptions in terms of used likelihood and prior.   \nFurther, the paper is restricted to fairly small scale experiments. ", "page_idx": 11}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 11}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 11}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 12}, {"type": "text", "text": "Justification: All theoretical results provides required assumptions and a complete proof. Guidelines: ", "page_idx": 12}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 12}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 12}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 12}, {"type": "text", "text": "Justification: Yes. For each dataset, we provide a detailed description how data points were generated in Appendix C. For each experiment, we provide training details and important hyperparameters in Appendix C. ", "page_idx": 12}, {"type": "text", "text": "Guidelines: ", "page_idx": 12}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 12}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 13}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 13}, {"type": "text", "text": "Justification: Code will be published upon acceptance. Guidelines: ", "page_idx": 13}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 13}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 13}, {"type": "text", "text": "Answer: Yes.   \nJustification: See Appendix C. Guidelines: ", "page_idx": 13}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 13}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 13}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 13}, {"type": "text", "text": "Justification: We report negative log likelihoods, ELBO, test predictive performance over datasets after optimising the ELBO for thousands of epochs. Train and test scores are computed over the full dataset, but not repeated for multiple seeds. ", "page_idx": 13}, {"type": "text", "text": "Guidelines: ", "page_idx": 13}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 14}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 14}, {"type": "text", "text": "Justification: All experiments were run on a single NVIDIA RTX 4090 GPU with 24GiB of GPU memory. ", "page_idx": 14}, {"type": "text", "text": "Guidelines: ", "page_idx": 14}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 14}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 14}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 14}, {"type": "text", "text": "Justification: The paper respects the NeurIPS Code of Ethics. ", "page_idx": 14}, {"type": "text", "text": "Guidelines: ", "page_idx": 14}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 14}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 14}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 14}, {"type": "text", "text": "Justification: This paper is a foundational paper not tied to a particular application. Guidelines: ", "page_idx": 14}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 14}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 15}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 15}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 15}, {"type": "text", "text": "Justification: This paper does not pose such risk and experiments only consider very simple text book problems. ", "page_idx": 15}, {"type": "text", "text": "Guidelines: ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 15}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 15}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 15}, {"type": "text", "text": "Justification: The paper does not use existing assets. ", "page_idx": 15}, {"type": "text", "text": "Guidelines: ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: The datasets used in this model are new, but details are clearly described in App. C. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 16}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 16}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 16}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 16}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 16}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 16}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "A Mathematical derivations ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "A.1 ELBO of Hamiltonian Neural Network ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We can find a lower bound on the marginal likelihood $\\log p(\\pmb{x}\\mid\\pmb{\\eta})$ through variational inference, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\log p(x\\mid\\eta)\\ge\\mathbb{E}_{\\theta}\\left[\\log p(x\\mid\\theta,\\eta)\\right]-\\mathbb{K}\\mathrm{L}(q_{m,s}(\\theta)\\mid|\\,p(\\theta))}&{\\quad{\\scriptstyle(\\mathcal{S})}}\\\\ &{\\quad=\\mathbb{E}_{\\theta}\\left[\\sum_{i=1}^{N}\\log\\mathcal{N}(x_{\\ell^{\\prime}}^{i}\\mid H_{\\theta,\\eta}(x_{i}^{i}),\\sigma_{\\hat{\\alpha}\\hat{\\alpha}}^{2}I)\\right]-\\mathbb{K}\\mathrm{L}(q_{m,s}(\\theta)\\mid|\\,p(\\theta\\mid\\mathbf{0},\\sigma_{\\hat{\\alpha}\\hat{\\beta}\\hat{\\alpha}}^{2}I))}\\\\ &{\\quad\\null=\\mathbb{E}_{\\theta}\\left[\\sum_{i=1}^{N}\\log\\mathcal{N}(x_{\\ell^{\\prime}}^{i}\\mid\\mathbb{E}_{\\pi}\\left[\\widehat{H}_{\\theta,\\eta}^{\\mathcal{I}}(x_{i}^{i})\\right],\\sigma_{\\hat{\\alpha}\\hat{\\alpha}}^{2}I)\\right]-\\mathbb{K}\\mathrm{L}(q_{m,s}(\\theta)\\mid|\\,p(\\theta\\mid\\mathbf{0},\\sigma_{\\hat{\\alpha}\\hat{\\beta}\\hat{\\alpha}}^{2}I)\\right]}\\\\ &{\\quad\\null\\ge\\mathbb{E}_{\\theta}\\left[\\mathbb{E}_{\\tau}\\left[\\sum_{i=1}^{N}\\log\\mathcal{N}(x_{\\ell^{\\prime}}^{i}\\mid\\widehat{H}_{\\theta,\\eta}^{\\mathcal{I}}(x_{i}^{i}),\\sigma_{\\hat{\\alpha}\\hat{\\alpha}}^{2}I)\\right]\\right]-\\mathbb{K}\\mathrm{L}(q_{m,s}(\\theta)\\mid|\\,p(\\theta\\mid\\mathbf{0},\\sigma_{p\\hat{\\alpha}\\hat{\\beta}}^{2}I)\\right]}\\\\ &{\\quad\\null\\le\\frac{1}{M}\\sum_{i=1}^{M}\\left[\\sum_{i=1}^{N}\\log\\mathcal{N}(x_{\\ell^{\\prime}}^{i}\\mid\\frac{1}{S}\\sum_{s=1}^{B}H_{\\theta,\\eta}(\\Phi_{C_{\\eta}}^{\\tau(s)}(x_{i}^{i}),\\sigma_{\\hat{\\alpha}\\hat{\\alpha}}^{2}I)\\right]-\\mathbb{K}\\mathrm{L}(q_{m,s}(\\theta)\\mid|\\,p(\\theta)\\mid)}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where we use $M$ samples to obtain an unbiased estimate of $\\mathbb{E}_{\\theta}:=\\mathbb{E}_{\\theta}{\\sim}q_{m,S}$ and a single sample $\\mathbb{E}_{\\tau}=\\mathbb{E}_{\\tau\\sim\\prod_{s=1}^{S}p(\\tau)}$ and use an $S$ -sampled Monte Carlo estimate of the symmetrised Hamiltonian: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\widehat{H}_{\\theta,\\eta}(x_{t}^{i})=\\frac{1}{S}\\sum_{s=1}^{S}H_{\\theta,\\eta}(\\Phi_{C_{\\eta}}^{\\tau^{(s)}}(x_{t}^{i}))\\mathrm{~with~samples~}\\tau^{(1)},\\tau^{(2)},\\dots,\\tau^{(S)}\\sim\\mu(\\tau)\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "with the fact that this yields an unbiased estimator of the true symmetrised Hamiltonian $\\mathbb{E}_{\\tau}\\left[\\widehat{H}_{\\theta,\\eta}(\\cdot)\\right]\\;=\\;\\widehat{H}_{\\theta,\\eta}(\\cdot)$ . where we obtained an unbiased estimate of expectations through $S$ sampled symmetry transformations and $M$ sampled parameters. The first inequality is the standard VI lower bound. The second inequality follows from applying Jensen\u2019s inequality (again), using the fact that the log likelihood is convex. Similar bounds to symmetrisation by averaging over orbits have appeared in prior work [van der Ouderaa and van der Wilk, 2021, Schw\u00f6bel et al., 2022, Nabarro et al., 2022]. ", "page_idx": 18}, {"type": "text", "text": "B Ground truth conserved quantities ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In this section, we\u2019ll discuss the conserved quantities present in the ground-truth Hamiltonians of the systems we discuss. ", "page_idx": 18}, {"type": "text", "text": "As stated in Section 2.3, we can combine conserved quantities into new ones by linear combinations, products, and Poisson brackets. Thus we\u2019ll speak of the generating set of conserved quantities, which combine into all conserved quantities. ", "page_idx": 18}, {"type": "text", "text": "B.1 Simple harmonic oscillator ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "For the simple harmonic oscillator, the phase space is $\\mathbb{R}\\times\\mathbb{R}$ . The ground truth Hamiltonian is ", "page_idx": 18}, {"type": "equation", "text": "$$\nH(p,q)={\\frac{p^{2}}{2m}}+{\\frac{k q^{2}}{2}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We choose $m=k=1$ , so that $H(p,q)=(p^{2}+q^{2})/2$ . Time evolution is a rotation of phase space.   \nThe Hamiltonian itself generates all conserved quantities. ", "page_idx": 18}, {"type": "text", "text": "B.2 $n$ \u2212Simple harmonic oscillators ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "The phase space is $\\mathbb{R}^{2n}$ . We choose all $k=m=1$ , so that the Hamiltonian is ", "page_idx": 19}, {"type": "equation", "text": "$$\nH(p,q)=\\|p\\|^{2}/2+\\|q\\|^{2}/2.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Time evolution rotates each pair $(q_{i},p_{i})$ . The conserved quantities are generated by the following set, for $i,j=1,...,n,i\\neq j$ . ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{H_{i}=(q_{i}^{2}+p_{i}^{2})/2}\\\\ {R_{i j}=q_{i}p_{j}-q_{j}p_{i}}\\\\ {F_{i j}=q_{i}q_{j}+p_{i}p_{j}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The conserved quantity $H_{i}$ rotates the pair $(q_{i},p_{i})$ . $R_{i j}$ rotates both pairs $(q_{i},q_{j})$ and $(p_{i},p_{j})$ and $F_{i j}$ rotates both pairs $(q_{i},p_{j})$ and $(q_{j},p_{i})$ . ", "page_idx": 19}, {"type": "text", "text": "Alternatively, we can interpret the phase space as $\\mathbb{C}^{n}$ [Arnold, 1989, Sec. 41E], with the positions being the real part and the momenta the imaginary part. In that case, the symplectic form $J$ becomes simply the complex number $-i$ . Then $\\bar{H(\\pmb{x})}\\stackrel{*}{=}\\dot{\\pmb{x}}^{\\dagger}\\pmb{x}/2$ . Time evolution is multiplication by the complex number $e^{-i t}$ . Conserved quantities are $H_{i}={x_{i}^{*}x_{i}}/{2}$ , which is real, and $C_{i j}=x_{i}^{*}x_{j}$ , whose real part corresponds to $F_{i j}$ and imaginary part to $R_{i j}$ . The conserved quantities $H_{i}$ and $C_{i j}$ are quadratic and thus their symmetries are generated by linear matrices, which are all skew-Hermitian. In fact, all skew-Hermitian matrices are spanned by these generators. This shows that the combined symmetry group is in fact $U(n)$ [Amiet and Weigert, 2002]. ", "page_idx": 19}, {"type": "text", "text": "B.3 $n$ -body ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In $D$ spatial dimensions, with $n$ bodies, the phase space is $\\mathbb{R}^{2n D}$ and the Hamiltonian is ", "page_idx": 19}, {"type": "equation", "text": "$$\nH(p,q)=\\sum_{i}{\\frac{\\|p_{i}\\|^{2}}{2m_{i}}}+\\sum_{i\\neq j}{\\frac{G m_{i}m_{j}}{\\sqrt{\\|q_{i}-q_{j}\\|^{2}+\\epsilon^{2}}}}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "with a small $\\epsilon$ to make it smooth. ", "page_idx": 19}, {"type": "text", "text": "The main conserved quantities are, for $d,d^{\\prime}=1,...D,d\\neq d^{\\prime}$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\quad T_{d}=\\displaystyle\\sum_{i}p_{i d}}\\\\ &{R_{d d^{\\prime}}^{\\mathrm{ABS}}=\\sum_{i}\\bigl(q_{i}\\wedge p_{i}\\bigr)_{d d^{\\prime}}}\\\\ &{R_{d d^{\\prime}}^{\\mathrm{COM}}=\\bigl(q_{\\mathrm{COM}}\\wedge p_{\\mathrm{COM}}\\bigr)_{d d^{\\prime}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "with $\\begin{array}{r}{q_{\\mathrm{COM}}=\\sum_{i}m_{i}q_{i}/\\sum_{i}m_{i}}\\end{array}$ and $\\begin{array}{r}{p_{\\mathrm{COM}}=\\sum_{i}m_{i}p_{i}/\\sum_{i}m_{i}}\\end{array}$ . ", "page_idx": 19}, {"type": "text", "text": "These generate further conserved quantities of interest: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{c}{P_{d}=T_{d}^{2}}\\\\ {Q_{d d^{\\prime}}=T_{d}T_{d^{\\prime}}}\\\\ {R_{d d^{\\prime}}^{\\mathrm{REL}}=\\displaystyle\\sum_{i}((q_{i}-q_{\\mathrm{COM}})\\wedge(p_{i}-p_{\\mathrm{COM}})_{d d^{\\prime}}=R_{d d^{\\prime}}^{\\mathrm{ABS}}-2R_{d d^{\\prime}}^{\\mathrm{COM}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "As $T_{d}$ is linear, we can still learn $P_{d}$ and $Q_{d d^{\\prime}}$ as quadratic conserved quantities. As the $R^{\\mathrm{REL}}$ is a linear combination of other conserved quantities, we disregard it in our analysis of the learned symmetries. ", "page_idx": 19}, {"type": "text", "text": "The corresponding symmetries are: $T_{d}$ translates all bodies in the $d$ direction. $R_{d d^{\\prime}}^{\\mathrm{COM}}$ rotates the center of mass of all the bodies in the plane $d d^{\\prime}$ , while preserving the positions relative to the center of mass. $R^{\\mathrm{ABS}}$ rotates all bodies relative to the origin. $\\operatorname{\\check{R}}^{\\mathrm{REL}}$ rotates all bodies relative to the center of mass. $P_{d}$ translates in the $d$ direction proportional to its COM momentum. $Q_{d d^{\\prime}}$ translates in direction $d$ proportional to $p_{\\mathrm{COM},d^{\\prime}}$ and vice versa. ", "page_idx": 19}, {"type": "text", "text": "These symmetries together generate a group we\u2019ll call $\\mathcal{G}$ . This group has as a subgroup $\\mathrm{SE}(D)$ , which is generated by $T$ and $R^{\\mathrm{AB5}}$ . The group $\\mathcal{G}$ has the same orbits as $\\mathrm{SE}(D)$ , as each element in $\\mathcal{G}$ can be seen as a rototranslation conditional on some property of phase space. Because the orbits are the same, for any observable $f:\\mathcal{M}\\rightarrow\\mathbb{R}$ , we have that $f$ invariant to $\\mathcal{G}$ is true if and only if $f$ is invariant to SE(2). ", "page_idx": 20}, {"type": "text", "text": "This system can have further conserved quantities, such as the Laplace\u2013Runge\u2013Lenz vector for $n=2$ . However, these are not expressible as a quadratic polynomial. As far as we know, the conserved quantities listed above are all that are expressible as a quadratic polynomial. ", "page_idx": 20}, {"type": "text", "text": "C Experimental details ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "All experiments were run on a single NVIDIA RTX 4090 GPU with 24GiB of GPU memory. ", "page_idx": 20}, {"type": "text", "text": "C.1 Simple harmonic oscillator experiment ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Data For the training data, we sampled 7 initial conditions from unit Gaussian and simulated 4 datapoints with $\\Delta t=0.2$ apart. For test data, we sampled 100 initial conditions from unit Gaussian and simulated 20 timesteps with $\\Delta t=0.2$ from each initial condition. ", "page_idx": 20}, {"type": "text", "text": "Training We use an MLP with 2 hidden layers, each consisting of 200 hidden neurons and a linear exponential unit activation function with $\\alpha=2$ . For symmetrisation, we use $S=200$ samples from a uniform measure for $\\mu(\\tau)$ . We use 20 Euler steps for time integration. We use fixed output noise and closed-form prior variance (Appendix D). We optimise the ELBO in full batch with Adam [Kingma and Ba, 2014] $(\\beta_{1}=0.9,\\beta_{2}=0.999)$ trained for 2000 epochs with a learning rate of 0.001, cosine annealed to 0. ", "page_idx": 20}, {"type": "text", "text": "C.2 $n$ \u2212Simple harmonic oscillators experiment ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Data For training data, we randomly sampled 200 initial conditions independently from a unit normal. For each initial condition, we simulated a trajectory consisting of 50 data points at 0.3 time units apart. ", "page_idx": 20}, {"type": "text", "text": "Training We use an MLP with 3 hidden layers with 200 hidden units and exponential linear activation functions with $\\alpha=1$ . We optimise the ELBO in mini batches of $B=20$ trajectories, using $S=100$ symmetrisation samples, 20 Euler steps for time integration, and $M=2$ weight samples using Adam [Kingma and Ba, 2014] $(\\beta_{1}=0.9,\\beta_{2}=0.999)$ ) for 2000 epochs with a learning rate starting from 0.001, cosine annealed to 0. ", "page_idx": 20}, {"type": "text", "text": "C.3 $n$ -body experiment ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Data For training data, we randomly sampled 200 initial conditions by independently sampling positions from a unit normal, shifted by a normal with a standard deviation of 3. From each initial condition, we simulated trajectories consisting of 50 data points 0.3 time units apart. ", "page_idx": 20}, {"type": "text", "text": "Training We use an MLP with 4 hidden layers with 250 hidden unit units and exponential linear unit activation functions with $\\alpha=1$ . We optimise the ELBO batches of $B=20$ trajectories, $S=100$ symmetrisation samples, 20 Euler steps for time integration, and $M=2$ weight samples using Adam [Kingma and Ba, 2014] $(\\beta_{1}=0.9,\\beta_{2}=0.999)$ for 2000 epochs with a learning rate starting from 0.001, cosine annealed to 0. ", "page_idx": 20}, {"type": "text", "text": "D On training a neural network with variational inference ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "D.1 Matrix normal variational posterior ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Naively, the covariance of a Gaussian posterior over weights grows quadratically with the number of parameters $|\\theta|^{2}$ . It is therefore common to disregard all correlations between weights, resulting in a diagonal or mean-field posterior. Although the ELBO remains a lower bound for any choice of approximate family, more crude approximations can increase the slack in the bound, possibly making it harder to use estimates for Bayesian model selection. We, therefore, propose to use matrix normal posteriors [Louizos and Welling, 2016] factorised per layer, ", "page_idx": 21}, {"type": "equation", "text": "$$\nq(\\theta)=\\prod_{i=1}^{N}q(\\theta_{l}),\\,\\mathrm{with}\\qquad\\qquad\\qquad q(\\theta_{l})={\\mathcal{N}}(\\theta_{l}\\mid,m_{l},S_{l}\\otimes A_{l})\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\pmb{\\theta}\\;=\\;(\\pmb{\\theta}_{1},\\pmb{\\theta}_{2},\\dots,\\pmb{\\theta}_{L})$ denote the weights of each layer $l$ . If we denote the parameters in terms of weight and bias matrices of each layer with $\\mathrm{in}_{l}$ input and $\\mathrm{out}_{l}$ output dimensions, $\\mathrm{vec}(\\pmb{\\theta}_{l})=[\\pmb{W}_{l}\\quad\\pmb{b}_{l}]\\in\\mathbb{R}^{\\mathrm{out}_{l}\\times(\\mathrm{in}_{l}+1)}$ , we can equivalently write this posterior as a factorised matrix normal distribution: ", "page_idx": 21}, {"type": "equation", "text": "$$\nq(\\theta)=\\prod_{i=1}^{N}q(\\theta_{l}),\\,\\mathrm{with}\\qquad\\qquad\\qquad q(\\theta_{l})=\\mathcal{M}\\mathcal{N}([W_{l}\\quad b_{l}]\\mid,M_{l},S_{l}\\otimes A_{l})\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "The variational parameters $\\{W_{l},S_{l},A_{l}\\}$ provide the mean $M_{l}$ as well as correlations between layer inputs and bias $\\pmb{A}_{l}\\in\\mathbb{R}^{(\\mathrm{in}_{l}+1),(\\mathrm{in}_{l}+1)}$ and layer outputs $S_{l}\\,\\in\\,\\mathbb{R}^{\\mathrm{out}_{l},\\mathrm{out}_{l}}$ . For $L$ hidden layers of width $H$ , the number of variational parameters scales quadratically $\\mathcal{O}(L H^{2})$ compared to the quartic number of variational parameters $O(L H^{4})$ we would need to represent the full covariance. This strikes a practical balance between taking important correlations into account while avoiding having to make a mean-field assumption. Further, we note that the matrix gaussian posterior of [Louizos and Welling, 2016] is the same approximate distribution as used in Kronecker-factored Laplace approximations [Grosse and Martens, 2016]. In Laplace approximations the covariance is the inverse Hessian, whereas in variational inference the covariance is optimised using the ELBO. Layer-factored matrix normal distributions have been succesfully applied to perform approximate Bayesian model selection based on the Laplace approximation in [Immer et al., 2021, 2022, van der Ouderaa et al., 2024]. This work provides evidence that variational inference can also be used to obtain approximate posteriors of this form and obtain a lower bound on the marginal likelihood that is sufficiently tight to perform Bayesian model selection in deep neural networks. ", "page_idx": 21}, {"type": "text", "text": "D.2 Closed-form output variance ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "It can be shown that the output variance that maximises the marginal likelihood $\\hat{\\sigma}_{\\mathrm{data}}^{2}$ is the empirical variance of the output. We, therefore, either fixing the output variance \u03c3\u02c6d2ata - typically to a very small number in noise-free settings, or setting the output variance to an empirical output variance. An exponentially weighted average of the empirical variance over mini-batches can be used. ", "page_idx": 21}, {"type": "text", "text": "On downscaling the KL term by a $\\beta$ \u2212scalar Many deep learning papers that use variational inference down-scale the KL term by a $\\beta$ \u2212parameter [Higgins et al., 2017]. We note that, for standard Gaussian likelihoods, scaling the output variance is equivalent to inversely scaling the KL term. We do advice against downscaling of the KL term, as it makes it less clear that the resulting objective is still a lower bound to the marginal likelihood, and hides the fact that the lower bound corresponds to a changed model with altered output variance. In MAP estimation under a Gaussian likelihood, the output variance is arbitrary as it only scales the objective not effecting the optimum, and the objective is often simplified as the mean squared error. In variational inference, the output variance does play an important role of balancing the relative importance between the log likelihood (data fit) and KL term (pull to prior). In this setting, using half mean squared error effectively corresponds to an output variance of \u03c3d2ata $\\dot{\\sigma}_{\\mathrm{data}}^{2}=1$ . In practice, this value is often too high because common machine learning datasets have little label noise. As a result, the log likelihood term is too weak and the KL term is too strong. We hypothesise that this has led to practitioners to down-weighting the KL term to obtain sensible posterior predictions, without necessarily realising that they were effectively altering the output variance of the model. Using automatic output variance, the optimal $\\hat{\\beta}$ can be set to (a running estimate of) the inverse of the empirical variance, also known as the empirical prior precision. ", "page_idx": 21}, {"type": "text", "text": "D.3 Closed-form prior variance minimising inverse KL ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Consider the setting of a $D$ -dimensional Gaussian $q$ , parameterised by mean $\\mathbf{\\nabla}m$ and covariance $\\boldsymbol{S}$ , and a zero mean Gaussian $p_{v}$ with scalar variance $v$ in each of the equally many dimensions: ", "page_idx": 22}, {"type": "equation", "text": "$$\nq=\\mathcal{N}(\\pmb{m},\\pmb{S}),\\qquad\\qquad\\qquad\\qquad p_{v}=\\mathcal{N}(\\pmb{0},\\pmb{v}\\pmb{I})\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where 0 denotes a zero vector and $\\boldsymbol{\\mathit{I}}$ an identity matrix. As the log likelihood does not depend on $v$ , we can find $v$ that optimises the marginal likelihood by finding the minimiser of the inverse KL: ", "page_idx": 22}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}&{\\underset{v}{\\operatorname{arg\\,min}}\\operatorname{KL}\\left[q\\mid\\mid p_{v}\\right]=\\underset{v}{\\operatorname{arg\\,min}}\\,{\\frac{1}{2}}\\left[\\log{\\frac{\\left|v I\\right|}{|S|}}-D+\\operatorname{Tr}((v I)^{-1}S)+(\\mathbf{0}-m)^{T}(v I)^{-1}(\\mathbf{0}-m)\\right]}\\\\ &{\\qquad\\qquad\\qquad=\\underset{v}{\\operatorname{arg\\,min}}\\,{\\frac{1}{2}}\\left[\\log{\\frac{\\left|v I\\right|}{|S|}}-D+\\operatorname{Tr}(S)/v+m^{T}m/v\\right]}\\\\ &{\\qquad\\qquad\\qquad=\\underset{v}{\\operatorname{arg\\,min}}\\left[D\\log(v)+\\operatorname{Tr}(S)/v+m^{T}m/v\\right]}\\end{array}}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Setting the derivative to zero: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l}{0=\\displaystyle\\frac{\\partial}{\\partial v}\\left[D\\log(v)+\\frac{1}{v}\\mathrm{Tr}(S)+m^{T}m/v\\right]}\\\\ {\\displaystyle0=-\\frac{-D v+\\mathrm{Tr}(S)+m^{T}m}{v^{2}}}\\\\ {v_{*}=\\frac{\\mathrm{Tr}(S)+m^{T}m}{D}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We found KL-minimising variance $v_{*}$ in closed-form as a function of $\\mathbf{\\nabla}m$ and $\\boldsymbol{S}$ . Verified numerically. ", "page_idx": 22}, {"type": "text", "text": "D.4 Plugging minimising variance into KL ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Plugging $v_{*}$ back into the Gaussian $p_{v_{*}}$ and computing the KL: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{KL}\\left[q\\mid\\mid p_{v_{\\ast}}\\right]=\\frac{1}{2}\\left[\\log\\frac{\\left|v_{\\ast}I\\right|}{\\left|S\\right|}-D+\\mathrm{Tr}((v_{\\ast}I)^{-1}S)+(\\mathbf{0}-m)^{T}(v_{\\ast}I)^{-1}(\\mathbf{0}-m)\\right]}\\\\ &{\\qquad\\qquad=\\frac{1}{2}\\left[\\log\\frac{\\left|v_{\\ast}I\\right|}{\\left|S\\right|}-D+\\frac{D\\mathrm{Tr}(S)}{\\mathrm{Tr}(S)+m^{T}m}+\\frac{D m^{T}m}{\\mathrm{Tr}(S)+m^{T}m}\\right]=\\frac{1}{2}\\left[\\log\\frac{\\left|v_{\\ast}I\\right|}{\\left|S\\right|}\\right]}\\\\ &{\\qquad\\qquad=\\frac{1}{2}\\left[D\\log\\left(\\frac{\\mathrm{Tr}(S)+m^{T}m}{D}\\right)-\\log\\left|S\\right|\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "sphoostwesr itohr,a t wthhie crhe scualnti nbge $\\mathrm{KL}$ ihse ro snilym pmliefaiesud riansg the relative volume ${\\frac{1}{2}}\\log{\\frac{|q|}{|p|}}$ between the prior and the ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathrm{KL}\\left[q\\,\\left|\\right|p_{v_{*}}\\right]=\\frac{1}{2}\\left[D\\log(\\mathrm{Tr}(S)+\\boldsymbol{m}^{T}\\boldsymbol{m})-D\\log(D)-\\log\\left|S\\right|\\right]\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "In practice, we might reparameterise $S=L L^{T}$ in terms of its triangular Cholesky factor $\\textbf{\\emph{L}}$ and use ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\log|{\\cal S}|=\\log|L L^{T}|=\\log|L|^{2}=2\\log|L|=2\\log\\prod_{i}L_{i i}=2\\sum_{i}\\log L_{i i}}}\\\\ {{\\displaystyle\\mathrm{Tr}({\\cal S})=\\mathrm{Tr}(L L^{T})=\\sum_{i,j}L_{i j}^{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "This gives the final expression of the KL ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathrm{KL}\\left[q\\left|\\right|p_{v_{*}}\\right]=\\frac{1}{2}\\left[D\\log\\left(\\sum_{i,j}{\\pmb L}_{i j}^{2}+\\sum_{i}{\\pmb m}_{i}^{2}\\right)-D\\log(D)-2\\sum_{i}\\log{\\pmb L}_{i i}\\right]\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "which implicitly uses the derived optimal variance $\\begin{array}{r}{v_{*}=\\frac{\\sum_{i,j}{\\bf L}_{i j}^{2}+\\sum_{i}{m_{i}}}{D}}\\end{array}$ ", "page_idx": 22}, {"type": "text", "text": "E Code and Questions ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "The code is available at https://github.com/tychovdo/noethers-razor.   \nFor any questions, please contact the corresponding author, Tycho van der Ouderaa, by email. ", "page_idx": 22}]