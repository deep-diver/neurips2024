[{"figure_path": "cqRgoDFaGN/figures/figures_1_1.jpg", "caption": "Figure 1: (Left) Problem Setting. We find the same sampling strategy gets different performances with different data. (Right) Performance of FasterDiT. We improve Diffusion Transformers (DiT) training speed by a large margin without any architecture modification.", "description": "The left panel shows that using the same sampling strategy (different lognormal distributions) leads to significantly different performance results depending on the dataset used.  This highlights the lack of robustness in existing strategies for diffusion model training. The right panel demonstrates the improvement achieved by FasterDiT, showcasing a substantial reduction in FID (Frechet Inception Distance) for various sampling strategies while using fewer training steps, effectively accelerating the training process significantly.", "section": "1 Introduction"}, {"figure_path": "cqRgoDFaGN/figures/figures_4_1.jpg", "caption": "Figure 2: Robustness of Different Noise Schedules. By scaling input to different standard deviations, we compare the data robustness of four schedules [22, 35, 29, 32], including diffusion and flow matching. Note that we set the prediction target as noise for a fair comparison. We find that different data signal intensities lead to different generative performances and different schedules have different robustness.", "description": "This figure shows the robustness of four different noise schedules (DDPM-linear, Flow-linear, DDPM-cosine, Flow-cosine) under varying data signal intensities. The x-axis represents the standard deviation (std) of the input data, and the y-axis represents the FID-10k score. Each subplot shows the performance of a particular noise schedule for different standard deviations. The figure demonstrates that the performance of each noise schedule varies significantly depending on the input data's signal intensity, indicating different levels of robustness. The figure highlights that a single schedule does not consistently perform well across different datasets, indicating a tradeoff between the performance and robustness.", "section": "3.2 Insights from PDF"}, {"figure_path": "cqRgoDFaGN/figures/figures_4_2.jpg", "caption": "Figure 3: SNR PDF of different noise schedules [22, 35, 29, 32]. The figure illustrates the signal-to-noise ratio (SNR) probability density functions (PDFs) for various schedules and standard deviations (see Section 2).", "description": "This figure shows the probability density functions (PDFs) of the signal-to-noise ratio (SNR) for four different noise schedules ([22, 35, 29, 32]) at three different standard deviations (std).  The x-axis represents the log-SNR (in dB), and the y-axis represents the density.  The different colored lines represent different schedules, and the different lines within each color represent different standard deviations. The figure illustrates how the distribution of SNR changes with different noise schedules and data signal strength. This is used to illustrate how different scheduling strategies affect the training process and robustness across data variations. The gray shaded area is mentioned in the text but not clearly depicted in the figure, making this detail ambiguous.", "section": "3 What can we learn from SNR PDF?"}, {"figure_path": "cqRgoDFaGN/figures/figures_5_1.jpg", "caption": "Figure 4: Influence of Weghting Dring Training. We use lognorm(0, 1) as Stable Diffusion3 [16]. The essence of this approach is to enhance the local focus of the PDF during the training process. This increases the upper bound of the training, but it also reduces the robustness of the training process to variations in the data.", "description": "This figure shows the impact of using a lognorm distribution for timestep sampling on the performance and robustness of different training schedules.  The left panels show the probability density function (PDF) of SNR for linear and lognorm-modified linear training schedules.  The right panels show how the FID changes with different standard deviations for these two schedules.  It highlights that while using lognorm can improve the upper bound of the performance, it can also decrease the robustness of training to changes in data intensity.", "section": "3 What can we learn from SNR PDF?"}, {"figure_path": "cqRgoDFaGN/figures/figures_5_2.jpg", "caption": "Figure 2: Robustness of Different Noise Schedules. By scaling input to different standard deviations, we compare the data robustness of four schedules [22, 35, 29, 32], including diffusion and flow matching. Note that we set the prediction target as noise for a fair comparison. We find that different data signal intensities lead to different generative performances and different schedules have different robustness.", "description": "This figure compares the robustness of four different noise schedules (DDPM linear, DDPM cosine, Flow linear, Flow cosine) across various signal intensities. The x-axis represents the standard deviation of the input data, scaled to simulate varying signal strengths. The y-axis shows the FID (Fr\u00e9chet Inception Distance), a metric evaluating the quality of generated images.  The results demonstrate that a single noise schedule's performance fluctuates greatly with changes in input data intensity, and that different noise schedules exhibit varying degrees of robustness across different input intensities.", "section": "3.2 Insights from PDF"}, {"figure_path": "cqRgoDFaGN/figures/figures_6_1.jpg", "caption": "Figure 7: Training Details. Our training pipeline involves only minimal modifications to the code.", "description": "This figure shows the effects of modulating the standard deviation of training data on the FID score.  The shaded region highlights the optimal standard deviation that shifts the probability density function of the signal-to-noise ratio (SNR) to improve the training efficiency.  The graph demonstrates that small modifications to the training process can lead to large improvements in the training performance of the diffusion transformers.", "section": "4 Improving DiT Training"}, {"figure_path": "cqRgoDFaGN/figures/figures_6_2.jpg", "caption": "Figure 2: Robustness of Different Noise Schedules. By scaling input to different standard deviations, we compare the data robustness of four schedules [22, 35, 29, 32], including diffusion and flow matching. Note that we set the prediction target as noise for a fair comparison. We find that different data signal intensities lead to different generative performances and different schedules have different robustness.", "description": "This figure compares the robustness of four different noise schedules (DDPM-linear, DDPM-cosine, Flow-linear, Flow-cosine) across varying data signal intensities.  The robustness is evaluated by observing changes in FID (Fr\u00e9chet Inception Distance) score as the standard deviation of the input data is scaled. The results show that different noise schedules exhibit varying levels of robustness, with some performing consistently well across different signal strengths, while others show significant performance fluctuations.", "section": "3.2 Insights from PDF"}, {"figure_path": "cqRgoDFaGN/figures/figures_8_1.jpg", "caption": "Figure 8: Visualization Results. We present visualization results for FasterDiT-XL/2 after training for 1,000k iterations, with CFG set to 4.0.", "description": "This figure shows several images generated by the FasterDiT-XL/2 model after training for 1,000,000 iterations with a CFG (classifier-free guidance) scale of 4.0.  The images demonstrate the model's ability to generate high-quality and diverse images across different categories, showcasing its performance after training.", "section": "4.4 Performance on Higher Resolution Images"}, {"figure_path": "cqRgoDFaGN/figures/figures_14_1.jpg", "caption": "Figure 9: Generation Results-1. We visualize generation results of FasterDiT, which is trained on ImageNet at 256 resolution for 1000k iterations.", "description": "This figure shows several image generation results from the FasterDiT model.  The model was trained on the ImageNet dataset at 256x256 resolution for 1,000,000 iterations. The images are categorized by their ImageNet index number and are representative samples showcasing the model's performance in generating various animal images.", "section": "B More Visualization Results"}, {"figure_path": "cqRgoDFaGN/figures/figures_15_1.jpg", "caption": "Figure 10: Generation Results-2. We visualize generation results of FasterDiT, which is trained on ImageNet at 256 resolution for 1000k iterations.", "description": "This figure shows several example images generated by the FasterDiT model after training on the ImageNet dataset at a resolution of 256 for 1,000,000 iterations.  The images are grouped by class and show the model's ability to generate high-quality, diverse samples.  It demonstrates the visual results obtained with FasterDiT after 1M iterations of training on ImageNet.", "section": "B More Visualization Results"}]