[{"figure_path": "mljDUaQpln/tables/tables_5_1.jpg", "caption": "Table 1: Synthetic logic corpora compared in this study, with their features categorized according to our proposed design principles (DP). Note that the last row of the ablation corpora lists variations of FLD\u00d72, each of which differs from the original regarding one of the design principles.", "description": "This table compares several synthetic logic corpora used in the study, including the proposed FLDx2 and existing ones like RuleTaker and PARARULE-Plus.  Each corpus is evaluated based on four design principles (DP): 1. Inclusion of unknown facts (DP1), 2. Inclusion of negative facts (DP2), 3. Diversity of reasoning rules and number of steps (DP3), and 4. Diversity of linguistic expressions (DP4).  The table shows the characteristics of each corpus according to these principles, such as vocabulary size, presence of distractors, types of deduction rules, number of logical steps, and the number of expressions per formula. The ablation corpora are variations of FLDx2, where each principle is removed to test its impact on the overall performance. ", "section": "3 Creating a Synthetic Corpus based on Design Principles"}, {"figure_path": "mljDUaQpln/tables/tables_7_1.jpg", "caption": "Table 2: 5-shot performance of LLMs before and after ALT. ALT-x denotes the LLM trained with ALT on the synthetic logic corpus x from Table 1. The color shows the rank in each column (darker is better). Each benchmark set, such as \"Logic\" and \"Math\", comprises various benchmarks in that domain (see Table E.7). \"Avg.\" represents the micro-average of all the benchmarks.", "description": "This table presents the performance of different LLMs (Large Language Models) on various reasoning tasks, before and after undergoing Additional Logic Training (ALT).  The LLMs are tested on a variety of benchmark sets, including logic, math, code, and others.  The table shows the average performance across all benchmarks, as well as the performance within each individual benchmark set. The color-coding helps to visually compare the relative performance of each LLM on each task.", "section": "5 Can Additional Logic Training Enhance LLMs' Capabilities?"}, {"figure_path": "mljDUaQpln/tables/tables_7_2.jpg", "caption": "Table 2: 5-shot performance of LLMs before and after ALT. ALT-x denotes the LLM trained with ALT on the synthetic logic corpus x from Table 1. The color shows the rank in each column (darker is better). Each benchmark set, such as \"Logic\" and \"Math\", comprises various benchmarks in that domain (see Table E.7). \"Avg.\" represents the micro-average of all the benchmarks.", "description": "This table presents the 5-shot performance of different LLMs (Large Language Models) before and after applying Additional Logic Training (ALT) using various synthetic logic corpora.  The performance is measured across several benchmark sets, including Logic, Math, Code, NLI (Natural Language Inference), and others.  The 'Avg.' column shows the micro-average performance across all benchmarks.  The color-coding helps to easily compare the relative performance of each model on each task.  ALT-x indicates that the LLM was trained with ALT using the synthetic logic corpus x (as defined in Table 1).", "section": "5 Can Additional Logic Training Enhance LLMs' Capabilities?"}, {"figure_path": "mljDUaQpln/tables/tables_7_3.jpg", "caption": "Table 3: LLaMA-3.1-8B trained on the ablation corpora.", "description": "This table presents the results of training the LLaMA-3.1-8B language model on several variations of the FLDx2 corpus. Each variation removes one of the four design principles (DP1-DP4) to understand their individual contributions to the model's performance.  The table shows performance metrics across various benchmarks, allowing for a comparison of the effects of each design principle on the model's ability to generalize across different reasoning tasks.", "section": "5 Can Additional Logic Training Enhance LLMs' Capabilities?"}, {"figure_path": "mljDUaQpln/tables/tables_8_1.jpg", "caption": "Table 4: Benchmark-wise 5-shot performance of LLaMA-3.1-70B before and after ALT on FLD\u00d72. Refer to Table F.9 for LLaMA-3.1-8B results. Table E.7 details each benchmark.", "description": "This table presents the results of the experiment comparing the performance of the LLaMA-3.1-70B language model before and after undergoing Additional Logic Training (ALT) on the FLDx2 dataset.  It shows the performance gains across various benchmark sets which include Logic, Math, Code, Natural Language Inference (NLI), and Other tasks.  The results are presented as 5-shot performance with error bars indicating variability. A reference to a supplementary table with results for the LLaMA-3.1-8B model is included and there is also a reference for the detailed breakdown of the individual benchmarks in Table E.7.", "section": "6.1 Logical Reasoning Tasks"}, {"figure_path": "mljDUaQpln/tables/tables_8_2.jpg", "caption": "Table 4: Benchmark-wise 5-shot performance of LLaMA-3.1-70B before and after ALT on FLD\u00d72. Refer to Table F.9 for LLaMA-3.1-8B results. Table E.7 details each benchmark.", "description": "This table presents the results of the experiments conducted to evaluate the impact of Additional Logic Training (ALT) on the performance of the LLaMA-3.1-70B language model.  It shows the 5-shot performance (meaning the model was provided 5 examples before being asked to perform the task) on various benchmarks across different categories: Logic, Math, Code, Natural Language Inference (NLI), and Others.  For each benchmark, the table shows the performance of the LLaMA-3.1-70B model before ALT and after ALT using the FLDx2 corpus.  The difference in performance highlights the impact of ALT on improving the model's reasoning abilities across diverse tasks.", "section": "6 What Capabilities Can Additional Logic Training Enhance and Why?"}, {"figure_path": "mljDUaQpln/tables/tables_9_1.jpg", "caption": "Table 2: 5-shot performance of LLMs before and after ALT. ALT-x denotes the LLM trained with ALT on the synthetic logic corpus x from Table 1. The color shows the rank in each column (darker is better). Each benchmark set, such as \"Logic\" and \"Math\", comprises various benchmarks in that domain (see Table E.7). \"Avg.\" represents the micro-average of all the benchmarks.", "description": "This table presents the results of experiments comparing the performance of Large Language Models (LLMs) before and after undergoing Additional Logic Training (ALT).  The LLMs were trained using different synthetic logic corpora, identified by 'ALT-x', where 'x' represents the specific corpus used (as detailed in Table 1).  Performance is measured across several benchmark sets, including logic, math, code, and others, with each set encompassing multiple individual benchmarks.  The table displays the average scores across all benchmarks, providing a comparative analysis of the LLMs' performance before and after ALT. The ranking of each LLM's performance in each category is visually represented by color-coding (darker color indicates better performance).", "section": "5 Can Additional Logic Training Enhance LLMs' Capabilities?"}, {"figure_path": "mljDUaQpln/tables/tables_24_1.jpg", "caption": "Table 2: 5-shot performance of LLMs before and after ALT. ALT-x denotes the LLM trained with ALT on the synthetic logic corpus x from Table 1. The color shows the rank in each column (darker is better). Each benchmark set, such as \"Logic\" and \"Math\", comprises various benchmarks in that domain (see Table E.7). \"Avg.\" represents the micro-average of all the benchmarks.", "description": "This table presents the 5-shot performance of Large Language Models (LLMs) before and after undergoing Additional Logic Training (ALT) using different synthetic logic corpora.  It compares the performance across various benchmarks categorized into Logic, Math, Code, and Others.  The color-coding helps visualize the ranking of each LLM's performance within each benchmark category.  The \"Avg.\" column shows the average performance across all benchmarks.", "section": "5 Can Additional Logic Training Enhance LLMs' Capabilities?"}, {"figure_path": "mljDUaQpln/tables/tables_25_1.jpg", "caption": "Table 2: 5-shot performance of LLMs before and after ALT. ALT-x denotes the LLM trained with ALT on the synthetic logic corpus x from Table 1. The color shows the rank in each column (darker is better). Each benchmark set, such as \"Logic\" and \"Math\", comprises various benchmarks in that domain (see Table E.7). \"Avg.\" represents the micro-average of all the benchmarks.", "description": "This table presents the 5-shot performance results of several Large Language Models (LLMs) before and after undergoing Additional Logic Training (ALT).  The LLMs were trained using different synthetic logic corpora (ALT-x, where x represents the corpus used).  The table shows performance across various benchmark sets, including Logic, Math, Code, Natural Language Inference (NLI), and others.  The average performance across all benchmarks is also provided.  Darker shades in the table indicate better performance.", "section": "Can Additional Logic Training Enhance LLMs' Capabilities?"}, {"figure_path": "mljDUaQpln/tables/tables_25_2.jpg", "caption": "Table 2: 5-shot performance of LLMs before and after ALT. ALT-x denotes the LLM trained with ALT on the synthetic logic corpus x from Table 1. The color shows the rank in each column (darker is better). Each benchmark set, such as \"Logic\" and \"Math\", comprises various benchmarks in that domain (see Table E.7). \"Avg.\" represents the micro-average of all the benchmarks.", "description": "This table presents the performance of different LLMs (Large Language Models) on various benchmark tasks, both before and after undergoing Additional Logic Training (ALT) with different synthetic logic corpora.  The results show the average scores across multiple runs, with color-coding to visually represent the ranking of each LLM on each benchmark.  The table highlights the impact of ALT and the choice of training corpus on the model's overall performance.", "section": "5 Can Additional Logic Training Enhance LLMs' Capabilities?"}, {"figure_path": "mljDUaQpln/tables/tables_25_3.jpg", "caption": "Table 4: Benchmark-wise 5-shot performance of LLaMA-3.1-70B before and after ALT on FLD\u00d72. Refer to Table F.9 for LLaMA-3.1-8B results. Table E.7 details each benchmark.", "description": "This table presents the 5-shot performance results of the LLaMA-3.1-70B language model on various benchmark tasks, before and after undergoing Additional Logic Training (ALT) using the Formal Logic Deduction Diverse (FLD\u00d72) corpus.  The results are broken down into categories (Logic, Math, Code, NLI, Others) showcasing improvements across a range of reasoning tasks after applying ALT.  The table also includes references to other tables providing further details on specific benchmarks and model performance on different model sizes.", "section": "6 What Capabilities Can Additional Logic Training Enhance and Why?"}, {"figure_path": "mljDUaQpln/tables/tables_25_4.jpg", "caption": "Table 2: 5-shot performance of LLMs before and after ALT. ALT-x denotes the LLM trained with ALT on the synthetic logic corpus x from Table 1. The color shows the rank in each column (darker is better). Each benchmark set, such as \"Logic\" and \"Math\", comprises various benchmarks in that domain (see Table E.7). \"Avg.\" represents the micro-average of all the benchmarks.", "description": "This table presents the performance of different LLMs (Large Language Models) before and after applying Additional Logic Training (ALT).  It compares the performance across various benchmark sets (Logic, Math, Code, NLI, Others)  using a 5-shot in-context learning approach. The results highlight the impact of ALT on improving the reasoning capabilities of LLMs.", "section": "5 Can Additional Logic Training Enhance LLMs' Capabilities?"}, {"figure_path": "mljDUaQpln/tables/tables_25_5.jpg", "caption": "Table 4: Benchmark-wise 5-shot performance of LLaMA-3.1-70B before and after ALT on FLD\u00d72. Refer to Table F.9 for LLaMA-3.1-8B results. Table E.7 details each benchmark.", "description": "This table presents the results of the 5-shot performance evaluation of the LLaMA-3.1-70B language model on various benchmarks before and after undergoing Additional Logic Training (ALT) using the Formal Logic Deduction Diverse (FLD\u00d72) corpus.  The table is divided into five sections representing different benchmark categories: Logic, Math, Code, Natural Language Inference (NLI), and Others. For each category, it shows the average performance and standard deviation (e.g., 83.8\u00b11.2) of the model before ALT and the improvement after ALT (e.g., from 83.8\u00b11.2 to 83.5\u00b10.5 for Logic). The table highlights the significant performance gains across various reasoning tasks following the ALT training, illustrating the effectiveness of the method.", "section": "6 What Capabilities Can Additional Logic Training Enhance and Why?"}]