[{"figure_path": "LmjLRHVCMG/figures/figures_3_1.jpg", "caption": "Figure 1: A visual comparison of Fisher, iEF and EF as pre-conditioners for a 2-parameter 2-datum linear least-squares regression problem inspired by [20] (see Appendix B for details). All three plots are loss landscapes with the x-axis and y-axis representing \u03b8\u2080 and \u03b8\u2081 respectively. The first plot shows the gradient vector field of the loss function and 5 sampled training trajectories for SGD updates. Similarly, the second plot is for NGD/iEF updates and the third plot is for EF updates (with a zoomed view). The global minimum (0, 0) is marked with a star where visible. The two dashed lines on all plots represent the optimal parameter sets for each training sample. It can be seen that the EF method has a highly distorted update vector field while the iEF and NGD methods adapt to the curvature of the problem successfully.", "description": "This figure compares the performance of three different optimization methods: SGD, NGD/iEF, and EF, on a simple 2-parameter, 2-data point linear least squares regression problem.  The plots visualize the loss landscape, gradient vector fields, and training trajectories for each method.  It highlights how the EF method suffers from an inversely scaled projection issue, leading to a distorted update vector field and inefficient training. In contrast, SGD and NGD/iEF efficiently converge to the global minimum.", "section": "4 Inversely-Scaled Projection Issue of Empirical Fisher"}, {"figure_path": "LmjLRHVCMG/figures/figures_7_1.jpg", "caption": "Figure 2: Four (log-scaled) ratios computed for checkpoints at various stages of training (sampled at the interval of one epoch) for 3 of the all 15 tasks. The x-axes represent the training stages of the model. 0% means the initialised model and 100% means model at the end of the last epoch. Each data point is averaged across 100 evaluations, and the error bars represent the standard deviation (1-sigma). The first plot shows \u03b3EF/\u03b3SGD, which denotes the relative approximation quality improvement of EF updates w.r.t. SGD updates (the lower the better). The second plot shows \u03b3iEF/\u03b3SGD, and the third plot shows \u03b3SF/\u03b3IEF. The last plot depicts the imbalance of gradient norms, which is the average ratio between the maximum and minimum gradient norm for each evaluated batch (a larger value indicates more imbalanced per-sample gradient norms, which should lead to a more significant inversely-scaled projection issue). Overall, the approximation quality follows \u03b3iEF > \u03b3SF > \u03b3EF.", "description": "This figure compares the approximation quality of three different Fisher information matrix approximation methods (EF, iEF, and SF) against standard SGD.  It shows the ratio of the approximation quality indicator (gamma) for each method relative to SGD across different training stages for three example tasks.  It also shows the gradient norm imbalance across training epochs.  The results indicate that iEF consistently provides a better approximation than EF and SF.", "section": "7 Experiments"}, {"figure_path": "LmjLRHVCMG/figures/figures_8_1.jpg", "caption": "Figure 3: Approximation quality (relative to SGD) of EF, SF and iEF methods w.r.t. damping factor \u03bb at different training stages of task CoLA+T5+LoRA. x-axes show the value of the damping factor, y-axes depict the relative approximation quality improvement of the target update method w.r.t. SGD (the lower the better). Each data point is averaged across 100 evaluations, and the error-bars represent the standard deviation (1-sigma). The first plot is for checkpoint saved at the end of the first training epoch, the second plot for the mid-way epoch and the third plot for the final epoch. It can be observed that iEF achieves the best approximation quality robustly for any near-zero \u03bb. In contrast, \u03bb has a non-linear impact on both SF and EF. When optimally tuned, an EF update can achieve better approximation quality than SGD, and an SF update can achieve comparable quality to iEF. However, the optimal damping factor for EF and SF changes greatly with training stages (and tasks).", "description": "This figure compares the approximation quality of EF, SF, and iEF methods to the exact natural gradient update with respect to the damping factor (\u03bb) at different training stages.  The results show that iEF consistently provides better approximations than EF and SF, especially with near-zero damping factors.  EF and SF are highly sensitive to the choice of damping factor, requiring careful tuning across tasks and training stages.", "section": "7 Experiments"}, {"figure_path": "LmjLRHVCMG/figures/figures_16_1.jpg", "caption": "Figure 1: A visual comparison of Fisher, iEF and EF as pre-conditioners for a 2-parameter 2-datum linear least-squares regression problem inspired by [20] (see Appendix B for details). All three plots are loss landscapes with the x-axis and y-axis representing \u03b8\u2080 and \u03b8\u2081 respectively. The first plot shows the gradient vector field of the loss function and 5 sampled training trajectories for SGD updates. Similarly, the second plot is for NGD/iEF updates and the third plot is for EF updates (with a zoomed view). The global minimum (0, 0) is marked with a star where visible. The two dashed lines on all plots represent the optimal parameter sets for each training sample. It can be seen that the EF method has a highly distorted update vector field while the iEF and NGD methods adapt to the curvature of the problem successfully.", "description": "This figure compares the update vector fields and training trajectories of four optimization methods (SGD, NGD, iEF, and EF) on a simple 2D linear least squares regression problem.  It visually demonstrates the inversely-scaled projection issue of the EF method, showing how its update vector field is distorted and biased towards well-trained samples, leading to inefficient training. In contrast, NGD and iEF demonstrate more efficient and accurate convergence to the global minimum.", "section": "4 Inversely-Scaled Projection Issue of Empirical Fisher"}, {"figure_path": "LmjLRHVCMG/figures/figures_21_1.jpg", "caption": "Figure 5: Approximation quality (relative to SGD) of \u201cun-damped\u201d iEF, ieKFAC, KFAC and eKFAC for 3 selected PEFT tasks (QNLI+LoRA, RTE+L0RA, MRPC+LoRA) across training stages. The style of the visualisation follows that for the first 3 plots of Fig. 2. This evaluation shows that, ieKFAC update has a similar approximation quality to the exact iEF method, and a much better approximation quality than both KFAC and eKFAC in most training stages. This demonstrates the effectiveness of using ieKFAC to approximate iEF and its potential of further improving the approximation quality of existing KFAC-based methods.", "description": "This figure compares the approximation quality of four methods: iEF, ieKFAC, KFAC, and eKFAC to the exact natural gradient descent update.  The results are shown for three different tasks across various training stages.  The y-axis shows the ratio of each method's approximation quality to that of SGD, lower values indicating better approximations. The plot shows that ieKFAC consistently performs better than KFAC and eKFAC and approximates iEF well.", "section": "D.2 Improving Existing Fisher-based Methods with IEF"}, {"figure_path": "LmjLRHVCMG/figures/figures_29_1.jpg", "caption": "Figure 2: Four (log-scaled) ratios computed for checkpoints at various stages of training (sampled at the interval of one epoch) for 3 of the all 15 tasks. The x-axes represent the training stages of the model. 0% means the initialised model and 100% means model at the end of the last epoch. Each data point is averaged across 100 evaluations, and the error bars represent the standard deviation (1-sigma). The first plot shows \u03b3EF/\u03b3SGD, which denotes the relative approximation quality improvement of EF updates w.r.t. SGD updates (the lower the better). The second plot shows \u03b3iEF/\u03b3SGD, and the third plot shows \u03b3SF/\u03b3iEF. The last plot depicts the imbalance of gradient norms, which is the average ratio between the maximum and minimum gradient norm for each evaluated batch (a larger value indicates more imbalanced per-sample gradient norms, which should lead to a more significant inversely-scaled projection issue). Overall, the approximation quality follows iEF > SF > EF.", "description": "This figure compares the approximation quality of EF, iEF, and SF methods to exact NG updates across different training stages.  Four plots are shown, visualizing: 1) EF's relative improvement over SGD; 2) iEF's relative improvement over SGD; 3) SF's relative improvement over iEF; and 4) the imbalance of gradient norms across samples.  The results indicate that iEF consistently provides better approximation quality than EF and SF.", "section": "7 Experiments"}, {"figure_path": "LmjLRHVCMG/figures/figures_29_2.jpg", "caption": "Figure 2: Four (log-scaled) ratios computed for checkpoints at various stages of training (sampled at the interval of one epoch) for 3 of the all 15 tasks. The x-axes represent the training stages of the model. 0% means the initialised model and 100% means model at the end of the last epoch. Each data point is averaged across 100 evaluations, and the error bars represent the standard deviation (1-sigma). The first plot shows YEF/YSGD, which denotes the relative approximation quality improvement of EF updates w.r.t. SGD updates (the lower the better). The second plot shows YiEF/YSGD, and the third plot shows YSF/YEF. The last plot depicts the imbalance of gradient norms, which is the average ratio between the maximum and minimum gradient norm for each evaluated batch (a larger value indicates more imbalanced per-sample gradient norms, which should lead to a more significant inversely-scaled projection issue). Overall, the approximation quality follows iEF > SF > EF.", "description": "This figure presents a comparison of the approximation quality of three different methods (EF, iEF, and SF) to the exact natural gradient update against SGD.  The plots show the ratio of the approximation quality of each method relative to SGD across different training stages, for three example tasks.  The final plot displays the gradient norm imbalance across these tasks, which correlates with the approximation quality of the methods. The results indicate that iEF generally provides the best approximation.", "section": "7 Experiments"}, {"figure_path": "LmjLRHVCMG/figures/figures_30_1.jpg", "caption": "Figure 3: Approximation quality (relative to SGD) of EF, SF and iEF methods w.r.t. damping factor \u03bb at different training stages of task CoLA+T5+LoRA. x-axes show the value of the damping factor, y-axes depict the relative approximation quality improvement of the target update method w.r.t. SGD (the lower the better). Each data point is averaged across 100 evaluations, and the error-bars represent the standard deviation (1-sigma). The first plot is for checkpoint saved at the end of the first training epoch, the second plot for the mid-way epoch and the third plot for the final epoch. It can be observed that iEF achieves the best approximation quality robustly for any near-zero \u03bb. In contrast, \u03bb has a non-linear impact on both SF and EF. When optimally tuned, an EF update can achieve better approximation quality than SGD, and an SF update can achieve comparable quality to iEF. However, the optimal damping factor for EF and SF changes greatly with training stages (and tasks).", "description": "This figure shows how the approximation quality of EF, SF, and iEF methods changes with different damping factors (\u03bb) at various training stages for the CoLA+T5+LoRA task.  The results demonstrate that iEF consistently outperforms EF and SF, especially with near-zero damping factors.  This highlights iEF's robustness and superior approximation quality compared to other methods.", "section": "Experiments"}, {"figure_path": "LmjLRHVCMG/figures/figures_30_2.jpg", "caption": "Figure 1: A visual comparison of Fisher, iEF and EF as pre-conditioners for a 2-parameter 2-datum linear least-squares regression problem inspired by [20] (see Appendix B for details). All three plots are loss landscapes with the x-axis and y-axis representing \u03b8\u2080 and \u03b8\u2081 respectively. The first plot shows the gradient vector field of the loss function and 5 sampled training trajectories for SGD updates. Similarly, the second plot is for NGD/iEF updates and the third plot is for EF updates (with a zoomed view). The global minimum (0, 0) is marked with a star where visible. The two dashed lines on all plots represent the optimal parameter sets for each training sample. It can be seen that the EF method has a highly distorted update vector field while the iEF and NGD methods adapt to the curvature of the problem successfully.", "description": "This figure compares the update vector fields of three different methods (Fisher, iEF, and EF) in a simple 2-parameter, 2-datum linear least-squares regression problem.  The plots show the loss landscapes and training trajectories for SGD, NGD/iEF, and EF updates. The EF method's update vector field is highly distorted compared to the NGD/iEF method, which adapts effectively to the problem's curvature.", "section": "4 Inversely-Scaled Projection Issue of Empirical Fisher"}, {"figure_path": "LmjLRHVCMG/figures/figures_31_1.jpg", "caption": "Figure 3: Approximation quality (relative to SGD) of EF, SF and iEF methods w.r.t. damping factor \u03bb at different training stages of task CoLA+T5+LoRA. x-axes show the value of the damping factor, y-axes depict the relative approximation quality improvement of the target update method w.r.t. SGD (the lower the better). Each data point is averaged across 100 evaluations, and the error-bars represent the standard deviation (1-sigma). The first plot is for checkpoint saved at the end of the first training epoch, the second plot for the mid-way epoch and the third plot for the final epoch. It can be observed that iEF achieves the best approximation quality robustly for any near-zero \u03bb. In contrast, \u03bb has a non-linear impact on both SF and EF. When optimally tuned, an EF update can achieve better approximation quality than SGD, and an SF update can achieve comparable quality to iEF. However, the optimal damping factor for EF and SF changes greatly with training stages (and tasks).", "description": "This figure compares the approximation quality of EF, SF, and iEF methods to the exact natural gradient update at different training stages for the CoLA+T5+LoRA task.  It shows how the approximation quality changes with the damping factor (\u03bb), demonstrating that iEF consistently performs well with near-zero damping, unlike EF and SF, which require careful tuning of \u03bb across different training stages.", "section": "Experiments"}, {"figure_path": "LmjLRHVCMG/figures/figures_31_2.jpg", "caption": "Figure 1: A visual comparison of Fisher, iEF and EF as pre-conditioners for a 2-parameter 2-datum linear least-squares regression problem inspired by [20] (see Appendix B for details). All three plots are loss landscapes with the x-axis and y-axis representing \u03b8\u2080 and \u03b8\u2081 respectively. The first plot shows the gradient vector field of the loss function and 5 sampled training trajectories for SGD updates. Similarly, the second plot is for NGD/iEF updates and the third plot is for EF updates (with a zoomed view). The global minimum (0, 0) is marked with a star where visible. The two dashed lines on all plots represent the optimal parameter sets for each training sample. It can be seen that the EF method has a highly distorted update vector field while the iEF and NGD methods adapt to the curvature of the problem successfully.", "description": "This figure compares the update vector fields and training trajectories of SGD, NGD/iEF, and EF methods on a simple 2D linear least squares regression problem.  It shows how EF updates are biased towards well-trained samples due to inverse scaling, resulting in a distorted update vector field and inefficient training. In contrast, NGD/iEF updates adapt to the curvature of the problem, leading to more efficient training.", "section": "Visual Illustration"}, {"figure_path": "LmjLRHVCMG/figures/figures_33_1.jpg", "caption": "Figure 1: A visual comparison of Fisher, iEF and EF as pre-conditioners for a 2-parameter 2-datum linear least-squares regression problem inspired by [20] (see Appendix B for details). All three plots are loss landscapes with the x-axis and y-axis representing \u03b8\u2080 and \u03b8\u2081 respectively. The first plot shows the gradient vector field of the loss function and 5 sampled training trajectories for SGD updates. Similarly, the second plot is for NGD/iEF updates and the third plot is for EF updates (with a zoomed view). The global minimum (0, 0) is marked with a star where visible. The two dashed lines on all plots represent the optimal parameter sets for each training sample. It can be seen that the EF method has a highly distorted update vector field while the iEF and NGD methods adapt to the curvature of the problem successfully.", "description": "This figure compares the update vector fields and training trajectories of three different optimization methods (SGD, NGD/iEF, and EF) on a simple 2D linear least squares regression problem.  The plots visualize the loss landscape, illustrating how each method's updates behave differently in response to the problem's curvature and the location of individual data points. The EF method shows a highly distorted update vector field and inefficient training trajectories, highlighting its limitations, while the NGD/iEF methods adapt well to the curvature, demonstrating their superiority.", "section": "4 Inversely-Scaled Projection Issue of Empirical Fisher"}, {"figure_path": "LmjLRHVCMG/figures/figures_34_1.jpg", "caption": "Figure 1: A visual comparison of Fisher, iEF and EF as pre-conditioners for a 2-parameter 2-datum linear least-squares regression problem inspired by [20] (see Appendix B for details). All three plots are loss landscapes with the x-axis and y-axis representing \u03b80 and \u03b81 respectively. The first plot shows the gradient vector field of the loss function and 5 sampled training trajectories for SGD updates. Similarly, the second plot is for NGD/iEF updates and the third plot is for EF updates (with a zoomed view). The global minimum (0, 0) is marked with a star where visible. The two dashed lines on all plots represent the optimal parameter sets for each training sample. It can be seen that the EF method has a highly distorted update vector field while the iEF and NGD methods adapt to the curvature of the problem successfully.", "description": "This figure compares three different preconditioners (Fisher, iEF, and EF) for a simple linear least-squares regression problem. The plots show the loss landscape and training trajectories under SGD, NGD/iEF, and EF updates.  It visualizes how the EF method suffers from an inversely-scaled projection issue, resulting in a distorted update vector field and inefficient training, while NGD/iEF updates efficiently adapt to the curvature of the problem.", "section": "4 Inversely-Scaled Projection Issue of Empirical Fisher"}, {"figure_path": "LmjLRHVCMG/figures/figures_36_1.jpg", "caption": "Figure 1: A visual comparison of Fisher, iEF and EF as pre-conditioners for a 2-parameter 2-datum linear least-squares regression problem inspired by [20] (see Appendix B for details). All three plots are loss landscapes with the x-axis and y-axis representing \u03b8\u2080 and \u03b8\u2081 respectively. The first plot shows the gradient vector field of the loss function and 5 sampled training trajectories for SGD updates. Similarly, the second plot is for NGD/iEF updates and the third plot is for EF updates (with a zoomed view). The global minimum (0, 0) is marked with a star where visible. The two dashed lines on all plots represent the optimal parameter sets for each training sample. It can be seen that the EF method has a highly distorted update vector field while the iEF and NGD methods adapt to the curvature of the problem successfully.", "description": "This figure compares the update vector fields of three different methods: SGD, NGD/iEF, and EF, on a simple 2-parameter, 2-data point linear least-squares regression problem.  The figure visually demonstrates the inversely-scaled projection issue in the EF method, showing how the EF updates are biased towards well-trained samples, leading to distorted trajectories and inefficient convergence. In contrast, SGD and NGD/iEF show efficient convergence to the global minimum.", "section": "Visual Illustration"}, {"figure_path": "LmjLRHVCMG/figures/figures_36_2.jpg", "caption": "Figure 3: Approximation quality (relative to SGD) of EF, SF and iEF methods w.r.t. damping factor \u03bb at different training stages of task CoLA+T5+LoRA. x-axes show the value of the damping factor, y-axes depict the relative approximation quality improvement of the target update method w.r.t. SGD (the lower the better). Each data point is averaged across 100 evaluations, and the error bars represent the standard deviation (1-sigma). The first plot is for checkpoint saved at the end of the first training epoch, the second plot for the mid-way epoch and the third plot for the final epoch. It can be observed that iEF achieves the best approximation quality robustly for any near-zero \u03bb. In contrast, \u03bb has a non-linear impact on both SF and EF. When optimally tuned, an EF update can achieve better approximation quality than SGD, and an SF update can achieve comparable quality to iEF. However, the optimal damping factor for EF and SF changes greatly with training stages (and tasks).", "description": "This figure compares the approximation quality of EF, SF, and iEF methods to the exact natural gradient update at different training stages for the CoLA+T5+LoRA task.  The x-axis represents the damping factor (\u03bb), and the y-axis represents the relative approximation quality improvement over SGD (lower is better). The figure shows that iEF consistently outperforms EF and SF, especially with near-zero damping, highlighting its robustness and superior approximation quality.", "section": "Experiments"}]