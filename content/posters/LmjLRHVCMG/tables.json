[{"figure_path": "LmjLRHVCMG/tables/tables_8_1.jpg", "caption": "Table 1: Average test performance of different optimisers for GLUE and CIFAR100. For GLUE tasks, the average metric results for the 7 tasks are used as the final test score. For tasks with two metrics, these metrics are averaged first [50]. For all tasks, the test result is computed for the best validation accuracy checkpoint. Refer to Table 7 for a more complete test performance report and detailed explanations on metrics.", "description": "This table presents the average test performance results for different optimizers (AdamW, Adafactor, SGD, EF, SF, and iEF) across various tasks from the GLUE benchmark and CIFAR100.  It summarizes the performance of each optimizer on these tasks by using average test metrics, considering both single and multiple metric scenarios and the best validation checkpoint for a comprehensive evaluation.  Detailed results can be found in Table 7 of the paper.", "section": "7 Experiments"}, {"figure_path": "LmjLRHVCMG/tables/tables_25_1.jpg", "caption": "Table 1: Average test performance of different optimisers for GLUE and CIFAR100. For GLUE tasks, the average metric results for the 7 tasks are used as the final test score. For tasks with two metrics, these metrics are averaged first [50]. For all tasks, the test result is computed for the best validation accuracy checkpoint. Refer to Table 7 for a more complete test performance report and detailed explanations on metrics.", "description": "This table presents the average test performance results for different optimizers across multiple tasks (GLUE and CIFAR100).  It shows the average test scores achieved by AdamW, Adafactor, SGD, EF, SF, and iEF optimizers. For GLUE tasks, the average across the seven individual tasks is reported. For tasks with two metrics, those metrics are averaged. The results highlight the best performing optimizer for each task, indicating the superior performance of the iEF optimizer compared to the others.", "section": "7 Experiments"}, {"figure_path": "LmjLRHVCMG/tables/tables_27_1.jpg", "caption": "Table 1: Average test performance of different optimisers for GLUE and CIFAR100. For GLUE tasks, the average metric results for the 7 tasks are used as the final test score. For tasks with two metrics, these metrics are averaged first [50]. For all tasks, the test result is computed for the best validation accuracy checkpoint. Refer to Table 7 for a more complete test performance report and detailed explanations on metrics.", "description": "This table presents the average test performance of different optimizers (AdamW, Adafactor, SGD, EF, SF, and iEF) on GLUE and CIFAR100 datasets.  For GLUE, the average across seven tasks is reported, with averages calculated for tasks having two metrics. The best validation accuracy checkpoint determines the reported test results.  For comprehensive details, refer to Table 7.", "section": "Experiments"}, {"figure_path": "LmjLRHVCMG/tables/tables_28_1.jpg", "caption": "Table 3: Optimisation details all involved tasks. Train epochs represent the number of epochs for which the model is trained. Evaluation frequency describes the number of update steps between each validation evaluation on the development set.", "description": "This table details the hyperparameters used for training different models on various tasks.  It specifies the number of epochs used for training and the frequency at which the model's performance was evaluated on a validation set during the training process.", "section": "H.2 Optimisation Experimental Setup"}, {"figure_path": "LmjLRHVCMG/tables/tables_29_1.jpg", "caption": "Table 7: Test performance for all task, structure and optimiser combinations. For all tasks, only one test result is reported for the best validation checkpoint across three random seed runs. Task-specific metrics (all multiplied by 100) on the test set are reported in this table. For SST-2, QNLI, RTE and CIFAR100, accuracy is reported. For CoLA, Matthew's Corr is reported. For MRPC and QQP, F1-score and Accuracy (in order) are reported. For MNLI, matched accuracy and unmatched Accuracy (in order) are reported.", "description": "This table shows the test performance of different optimizers (AdamW, Adafactor, SGD, EF, SF, and iEF) on various tasks and model architectures (Prompt Tuning and LoRA).  It displays the best test accuracy achieved for each combination and uses task-specific metrics (accuracy, F1-score, Matthew's correlation coefficient). The table highlights the performance of each optimizer compared to others across different tasks. ", "section": "Experiments"}, {"figure_path": "LmjLRHVCMG/tables/tables_30_1.jpg", "caption": "Table 1: Average test performance of different optimisers for GLUE and CIFAR100. For GLUE tasks, the average metric results for the 7 tasks are used as the final test score. For tasks with two metrics, these metrics are averaged first [50]. For all tasks, the test result is computed for the best validation accuracy checkpoint. Refer to Table 7 for a more complete test performance report and detailed explanations on metrics.", "description": "This table presents the average test performance results for different optimization methods applied to GLUE and CIFAR100 datasets.  It shows the average test scores for seven GLUE tasks and one CIFAR100 task, considering both Prompt Tuning and LoRA parameter-efficient fine-tuning methods.  The table highlights the best-performing optimization method (iEF) across various tasks.", "section": "7 Experiments"}, {"figure_path": "LmjLRHVCMG/tables/tables_31_1.jpg", "caption": "Table 1: Average test performance of different optimisers for GLUE and CIFAR100. For GLUE tasks, the average metric results for the 7 tasks are used as the final test score. For tasks with two metrics, these metrics are averaged first [50]. For all tasks, the test result is computed for the best validation accuracy checkpoint. Refer to Table 7 for a more complete test performance report and detailed explanations on metrics.", "description": "This table presents the average test performance results for different optimizers across various tasks (GLUE and CIFAR100).  It shows the average test score for seven GLUE tasks and the CIFAR100 task.  For GLUE, the average of the seven tasks' metrics is reported, and for tasks with two metrics, those are averaged. The test results are based on the best validation accuracy checkpoint for each model.  For more detailed results and metric explanations, refer to Table 7.", "section": "7 Experiments"}, {"figure_path": "LmjLRHVCMG/tables/tables_32_1.jpg", "caption": "Table 1: Average test performance of different optimisers for GLUE and CIFAR100. For GLUE tasks, the average metric results for the 7 tasks are used as the final test score. For tasks with two metrics, these metrics are averaged first [50]. For all tasks, the test result is computed for the best validation accuracy checkpoint. Refer to Table 7 for a more complete test performance report and detailed explanations on metrics.", "description": "This table shows the average test performance of different optimization methods (AdamW, Adafactor, SGD, EF, SF, iEF) on the GLUE and CIFAR100 datasets.  The average metric scores across the seven GLUE tasks are used as the final score; for tasks with multiple metrics, the averages are reported.  The results represent the best validation accuracy checkpoint for each task and optimization method. For more detailed information, including individual task scores, consult Table 7.", "section": "7 Experiments"}, {"figure_path": "LmjLRHVCMG/tables/tables_35_1.jpg", "caption": "Table 8: Validation and Test accuracy of different optimiser runs for MLP+CIFAR10 setup. For each optimiser run, only one test accuracy is evaluated for the checkpoint with the best validation accuracy.", "description": "This table presents the validation and test accuracy achieved by different optimizers (iEF, Adam, SGD, SF, EF) when training a Multilayer Perceptron (MLP) model from scratch on the CIFAR-10 dataset.  The results highlight the superior performance of the iEF optimizer in achieving higher accuracy compared to other methods.", "section": "H.7 Additional Train-from-Scratch Experiment"}]