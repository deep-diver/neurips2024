{"references": [{"fullname_first_author": "David Abel", "paper_title": "Near optimal behavior via approximate state abstraction", "publication_date": "2016-00-00", "reason": "This paper introduces the concept of approximate state abstraction, a crucial technique for efficient state representation in reinforcement learning, which is directly relevant to the core theme of the target paper."}, {"fullname_first_author": "Carles Gelada", "paper_title": "Deep-MDP: Learning continuous latent space models for representation learning", "publication_date": "2019-06-09", "reason": "This paper introduces DeepMDP, a key method for single-modality dynamics modeling in visual reinforcement learning, which the target paper extends to the multi-modal scenario."}, {"fullname_first_author": "Tuomas Haarnoja", "paper_title": "Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor", "publication_date": "2018-07-10", "reason": "This paper introduces the Soft Actor-Critic algorithm, a crucial foundation for the policy learning component of the target paper's multi-modal dynamics modeling method."}, {"fullname_first_author": "Danijar Hafner", "paper_title": "Learning latent dynamics for planning from pixels", "publication_date": "2019-06-09", "reason": "This paper introduces a method for learning latent dynamics for planning from pixels, providing insights for the target paper's approach to dynamics modeling in a multi-modal setting."}, {"fullname_first_author": "Danijar Hafner", "paper_title": "Mastering Atari with discrete world models", "publication_date": "2020-10-02", "reason": "This paper demonstrates the successful application of world models for mastering Atari games, offering inspiration for the target paper's approach to incorporating world models into multi-modal visual reinforcement learning."}]}