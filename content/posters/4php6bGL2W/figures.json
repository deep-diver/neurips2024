[{"figure_path": "4php6bGL2W/figures/figures_1_1.jpg", "caption": "Figure 1: Motivation of our method and comparison with DeepMDP [9] designed for single modality. Ot, Zt, at, and rt represent the observation, latent feature, action, and reward at time t, respectively. Mi and Mi are the decoupled consistent (Con.) and inconsistent (Incon.) features for i-th modality.", "description": "This figure illustrates the core idea behind the proposed Dissected Dynamics Modeling (DDM) method and compares it to the single-modality DeepMDP approach.  Panel (a) shows a multi-modal scenario (RGB images and event signals) where different modalities share some consistent information (e.g., presence of a car) but also have unique observations (e.g., a red light visible only in the RGB image, a car in the dark only visible in event signals).  These inconsistencies are crucial for effective decision-making. Panel (b) contrasts DDM with DeepMDP; DDM explicitly separates consistent and inconsistent information within each modality, using separate pathways to model environmental dynamics, whereas DeepMDP does not make this distinction.", "section": "1 Introduction"}, {"figure_path": "4php6bGL2W/figures/figures_1_2.jpg", "caption": "Figure 2: The proposed Dissected Dynamics Modeling (DDM) method. The input visual modalities are first passed through separate observation encoders to get partitioned modality-consistent and inconsistent features. These features then undergo modality-aware dynamics modeling to model accurate RL dynamics and promote feature quality. Finally, the optimized modality features are merged as state representations, which are then used for robust policy learning.", "description": "This figure illustrates the architecture of the proposed Dissected Dynamics Modeling (DDM) method.  It shows how multiple input modalities (e.g., RGB images and event camera data) are first processed by separate observation encoders. These encoders extract both modality-consistent and modality-inconsistent features.  The consistent features, representing shared information across modalities, are used in a mutual predictive process to refine feature quality and ensure consistency across time steps.  The inconsistent features, which capture modality-specific information, are processed separately to model the unique dynamics of each modality.  Finally, the refined consistent and inconsistent features are combined to form the state representation used for robust policy learning.", "section": "3 Methodology"}, {"figure_path": "4php6bGL2W/figures/figures_3_1.jpg", "caption": "Figure 2: The proposed Dissected Dynamics Modeling (DDM) method. The input visual modalities are first passed through separate observation encoders to get partitioned modality-consistent and inconsistent features. These features then undergo modality-aware dynamics modeling to model accurate RL dynamics and promote feature quality. Finally, the optimized modality features are merged as state representations, which are then used for robust policy learning.", "description": "This figure illustrates the architecture of the Dissected Dynamics Modeling (DDM) method.  It shows how multiple input modalities (e.g., RGB frames and event signals) are first processed by separate observation encoders to extract both modality-consistent and modality-inconsistent features. These features are then fed into distinct pathways within the modality-aware dynamics modeling component, where consistent features are used to predict future states and inconsistent features contribute to modality-specific dynamics.  Finally, the processed features are combined to form state representations, which are used for policy learning in a reinforcement learning context. The figure highlights the method's unique approach of separating consistent and inconsistent information across modalities to improve the accuracy and robustness of the dynamics model.", "section": "3 Methodology"}, {"figure_path": "4php6bGL2W/figures/figures_7_1.jpg", "caption": "Figure 2: The proposed Dissected Dynamics Modeling (DDM) method. The input visual modalities are first passed through separate observation encoders to get partitioned modality-consistent and inconsistent features. These features then undergo modality-aware dynamics modeling to model accurate RL dynamics and promote feature quality. Finally, the optimized modality features are merged as state representations, which are then used for robust policy learning.", "description": "This figure illustrates the DDM method's architecture.  Visual input modalities (e.g., RGB frames and event signals) are initially processed through separate observation encoders. These encoders extract both modality-consistent (common across modalities) and modality-inconsistent (unique to each modality) features.  These features are then fed into distinct dynamics modeling pathways. Consistent features undergo mutual predictive constraints to ensure information coherence across modalities and over time. Inconsistent features undergo orthogonalization to highlight their unique information.  Finally, a reward predictive function filters task-irrelevant information from both consistent and inconsistent features.  The resulting combined features, representing the state, are used for policy learning.", "section": "3 Methodology"}, {"figure_path": "4php6bGL2W/figures/figures_8_1.jpg", "caption": "Figure 4: Comparison between DeepMDP and DDM on policy robustness and sample efficiency.", "description": "This figure compares the performance of DeepMDP and DDM in terms of policy robustness and sample efficiency under various conditions on the CARLA benchmark.  The left panel (a) shows the driving distance achieved by each method under normal conditions, as well as when random noise and random masks are added to the input data. This demonstrates the robustness of DDM against these distractions. The right panel (b) illustrates the sample efficiency, showing that DDM converges faster and achieves better driving distance with fewer training steps compared to DeepMDP, particularly in more challenging conditions like 'Midnight', 'Dazzling', and 'Rainy'.", "section": "4 Experiments"}, {"figure_path": "4php6bGL2W/figures/figures_8_2.jpg", "caption": "Figure 5: t-SNE visualization of consistent and inconsistent features.", "description": "This figure visualizes the results of t-SNE dimensionality reduction applied separately to the consistent and inconsistent features extracted by the DDM method for RGB and event modalities under four different weather conditions in the CARLA driving simulator.  Each point represents a feature vector. The clustering of points within each modality suggests that DDM effectively separates features representing shared information across modalities (consistent) from those representing modality-specific information (inconsistent). The separation of consistent and inconsistent features highlights DDM's ability to disentangle shared and unique information from different sensory inputs, thereby enhancing the accuracy of the environmental dynamics model.", "section": "Visualization of Decomposed Modality Features"}, {"figure_path": "4php6bGL2W/figures/figures_8_3.jpg", "caption": "Figure 7: Comparing different inconsistency constraints.", "description": "This figure compares the performance of two different methods for handling inconsistencies between modalities in multi-modal visual reinforcement learning.  The x-axis represents different weather conditions (Normal, Midnight, Dazzling, Rainy) in the CARLA driving simulator, and the y-axis represents the driving distance achieved. Two approaches are compared:  one enforcing mutual exclusivity between consistent and inconsistent features within the same modality (Con. \u22a5 Incon. (Eq.6)), and the other enforcing orthogonality between inconsistent features across different modalities (Incon. \u22a5 Incon. (Eq.7)). The results show that the latter approach generally achieves better performance across varying conditions.  This demonstrates that the proposed orthogonality constraint (Eq.7) is more effective at extracting unique information from each modality while avoiding over-regularization and decreased feature expressiveness.", "section": "4.4 Discussion and Analysis"}, {"figure_path": "4php6bGL2W/figures/figures_9_1.jpg", "caption": "Figure 6: Visualization of feature heatmaps in DDM under normal and midnight conditions.", "description": "This figure visualizes feature heatmaps generated by the Dissected Dynamics Modeling (DDM) method under normal and midnight driving conditions in the CARLA simulator.  The heatmaps illustrate the learned feature representations for RGB frames and event signals, showing how DDM separates consistent and inconsistent information across modalities.  The top row represents a scene during the day, while the bottom row displays the same scene at night.  The figure highlights how DDM effectively extracts common features across modalities (consistency) while also identifying modality-specific features (inconsistency).  This ability to disentangle shared and unique information across different sensor inputs is a key contribution of the DDM method.", "section": "4.4 Discussion and Analysis"}, {"figure_path": "4php6bGL2W/figures/figures_15_1.jpg", "caption": "Figure 2: The proposed Dissected Dynamics Modeling (DDM) method. The input visual modalities are first passed through separate observation encoders to get partitioned modality-consistent and inconsistent features. These features then undergo modality-aware dynamics modeling to model accurate RL dynamics and promote feature quality. Finally, the optimized modality features are merged as state representations, which are then used for robust policy learning.", "description": "This figure illustrates the architecture of the Dissected Dynamics Modeling (DDM) method.  Visual modalities (e.g., RGB frames and event signals) are first processed by separate observation encoders. These encoders partition the features into modality-consistent and modality-inconsistent components. The consistent features, representing shared information, undergo a modality-aware dynamics modeling process to capture common scene dynamics.  In contrast, the inconsistent features, which contain modality-specific information, are processed through separate dynamics modeling pathways.  Finally, these optimized consistent and inconsistent features are merged to create a robust state representation, used for policy learning in reinforcement learning.", "section": "3 Methodology"}, {"figure_path": "4php6bGL2W/figures/figures_15_2.jpg", "caption": "Figure A2: Sample efficiency on DMControl. For each task, we provide the task returns of three methods at 100K and 500K steps, respectively.", "description": "This figure demonstrates the sample efficiency of three different methods (DeepMDP, EFNet, and DDM) on six different tasks within the DMControl environment.  Each bar represents the average episode reward achieved by each method after training for either 100,000 or 500,000 steps. The results show that DDM generally requires fewer training steps to achieve high rewards compared to the other two methods.", "section": "A Additional Experimental Results"}, {"figure_path": "4php6bGL2W/figures/figures_16_1.jpg", "caption": "Figure A3: Demonstration of RGB frames and event signals on the CARLA benchmark.", "description": "This figure shows a comparison of RGB frames and event signals under four different weather conditions in the CARLA driving simulator.  The top row displays RGB images, showcasing the visual scene under normal daylight, nighttime (Midnight), strong sunlight (Dazzling), and heavy rain. The bottom row shows corresponding event camera data, which highlights changes in brightness and motion in a distinctive visual representation.", "section": "B Detailed Experimental Setup"}]