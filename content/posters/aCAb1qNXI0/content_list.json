[{"type": "text", "text": "Hierarchical Federated Learning with Multi-Timescale Gradient Correction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Wenzhi Fang Purdue University fang375@purdue.edu ", "page_idx": 0}, {"type": "text", "text": "Dong-Jun Han Yonsei University djh@yonsei.ac.kr ", "page_idx": 0}, {"type": "text", "text": "Evan Chen Shiqiang Wang Christopher G. Brinton Purdue University IBM Research Purdue University chen4388@purdue.edu wangshiq@us.ibm.com cgb@purdue.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "While traditional federated learning (FL) typically focuses on a star topology where clients are directly connected to a central server, real-world distributed systems often exhibit hierarchical architectures. Hierarchical FL (HFL) has emerged as a promising solution to bridge this gap, leveraging aggregation points at multiple levels of the system. However, existing algorithms for HFL encounter challenges in dealing with multi-timescale model drift, i.e., model drift occurring across hierarchical levels of data heterogeneity. In this paper, we propose a multi-timescale gradient correction (MTGC) methodology to resolve this issue. Our key idea is to introduce distinct control variables to (i) correct the client gradient towards the group gradient, i.e., to reduce client model drift caused by local updates based on individual datasets, and (i) correct the group gradient towards the global gradient, i.e., to reduce group model drift caused by FL over clients within the group. We analytically characterize the convergence behavior of MTGC under general non-convex settings, overcoming challenges associated with couplings between correction terms. We show that our convergence bound is immune to the extent of data heterogeneity, confirming the stability of the proposed algorithm against multi-level non-i.i.d. data. Through extensive experiments on various datasets and models, we validate the effectiveness of MTGC in diverse HFL settings. The code for this project is available at https://github.com/wenzhifang/MTGC. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In the past several years, federated learning (FL) has emerged as a prevalent approach for distributed training [17, 22, 11, 52, 24]. Conventional FL has typically considered a star topology training architecture, where clients directly communicate with a central server for model synchronization [35, 26]. Scaling this architecture to large numbers of clients becomes problematic, however, given the heterogeneity in FL resource availability and dataset statistics manifesting over large geographies [14, 17, 10]. In practice, such communication networks are often comprised of a hierarchical architecture from clients to the main server, as observed in edge/fog computing [25, 38] and softwaredefined networks (SDN) [20], where devices are supported by intermediate edge servers that are in turn connected to the cloud. ", "page_idx": 0}, {"type": "text", "text": "To bridge this gap, researchers have proposed hierarchical federated learning (HFL) which integrates group aggregations into FL frameworks [30, 5, 47, 15]. In HFL (see Fig. 1), clients are segmented into multiple groups, and the training within each group is coordinated by a group aggregator node (e.g.. an edge server coordinating a cell). Meanwhile, the central server orchestrates the training globally by periodically aggregating models across all client groups, facilitated by the group aggregators. ", "page_idx": 0}, {"type": "text", "text": "Fundamental challenges. One of the key objectives in FL is to reduce communication overhead while maintaining model performance. Research in conventional FL has established how the global aggregation period, i.e., the number of local iterations during two consecutive communications between clients and the server, impacts FL performance according to the degree of non-i.i.d. (nonindependent or non-identically distributed) across client datasets: when local datasets are more heterogeneous, longer aggregation periods cause client models to drift further apart. In HFL, the situation becomes more complex, and is not yet well studied. There are multiple levels of aggregations within/across client groups, and the frequency of these aggregations diminishes further up the hierarchy (since the communication costs become progressively more expensive). As a result, model drift occurs across multiple levels of non-i.i.d., at different timescales. In the canonical two-level case from Fig. 1, we have (i) intra-group non-i.i.d., similar to conventional FL, and (ii) inter-group non-i.i.d., arising from data heterogeneity across different groups. This introduces (i) client model drift caused by local updates on individual datasets, usually at a shorter timescale, as well as (i) group model drift caused by FL over clients within the group, usually at a longer timescale. ", "page_idx": 1}, {"type": "text", "text": "In conventional star-topology FL, algorithms like ProxSkip [36], SCAFFOLD [18], and FedDyn [1] have shown promise for correcting client model drift through local regularization and gradient tracking/correction. However, these approaches are not easily extendable to the HFL scenario due to its multi-timescale communication architecture. Specifically, when integrating these methods into HFL, control variables introduced to handle data heterogeneity, such as gradient tracking or dynamic regularization, need to be carefully injected at each level of the hierarchy, taking into account their coupled effects in taming non-i.i.d. Convergence analysis elucidating the impact of different updating frequencies for such control variables remains an unsolved challenge. Existing works on HFL have also not aimed to directly correct for multi-timescale model drift. This can be seen by the fact that the convergence bounds in existing HFL methods [30, 5, 47, 13] become worse as the extent of non-i.i.d. in the system increases (e.g., gradient divergence between hierarchy levels in [47]). Some works have proposed adaptive control of the aggregation period in HFL [13, 31], but they require frequent model aggregations to prevent excessive drift. We thus pose the following question: ", "page_idx": 1}, {"type": "text", "text": "How can we tame multi-timescale model drift in non-i.i.d. hierarchical federated learning to provably enhance model convergence performance while not introducing frequent model aggregations? ", "page_idx": 1}, {"type": "text", "text": "1.1 Contributions ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In this paper, we propose multi-timescale gradient correction (MTGC), a methodology which can effectively address multi-level model drift over the topology of HFL with a theoretical guarantee. As depicted in Fig. 1, our key idea is to introduce coupled gradient correction terms - client-group correction and group-global correction - to (i) correct the client gradient towards the group gradient, i.e., to reduce client model drift caused by local updates based on their individual datasets, and (ii) correct the group gradient towards the global gradient, i.e., to reduce group model drift caused by ", "page_idx": 1}, {"type": "image", "img_path": "aCAb1qNXI0/tmp/f93de2b7203a5225012a3caeea1932238746897e590a6ceb6b6cd2a8fcb8d3de.jpg", "img_caption": ["Figure 1: Illustration of multi-timescale gradient correction (MTGC) for multi-level non-i.i.d. in HFL. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "FL across clients within the group, respectively. MTGC thus assists each client model to evolve towards improvements in global performance during HFL. We propose a strategy for updating these gradient correction terms after every group aggregation and global aggregation, respectively, and analyze the convergence behavior of MTGC. Due to the coupling of correction terms and their updates being performed at different timescales, additional challenges arise for theoretical analysis compared to prior work. We thoroughly investigate this problem and make the following contributions: ", "page_idx": 1}, {"type": "text", "text": "\u00b7 We develop the multi-timescale gradient correction (MTGC) algorithm for taming leveled model drift in HFL. MTGC incorporates coupled control variables for correcting client gradients and group gradients, effectively tackling model biases arising from various levels of non-i.i.d. data at different timescales. The estimation and update procedures for these control variables rely solely on the model updates, ensuring that no significant additional communication overhead is introduced. ", "page_idx": 1}, {"type": "text", "text": "\u00b7 We characterize the convergence rate for MTGC under the non-convex setup. This rate is immune to the extent of intra and inter-group data heterogeneity, confirming the stability of our approach against multi-level non-i.i.d. statistics. Our theoretical result also demonstrates that MTGC achieves ", "page_idx": 1}, {"type": "text", "text": "linear speedup in the number of local iterations, group aggregations, and clients. Also, we show that the convergence rate of MTGC recovers that of SCAFFOLD, i.e., the non-hierarchical case, when the number of groups and group aggregation period reduces to one. \u00b7 We conduct extensive experiments using various datasets and models across different parameter settings, which demonstrate the superiority of MTGC in diverse non-i.i.d. HFL environments. ", "page_idx": 2}, {"type": "text", "text": "1.2  Related Works ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Algorithms for conventional FL. The seminal work [35] developed the FedAvg algorithm, incorporating multiple local updates into distributed SGD [58] to relieve communication bottlenecks within conventional star-topology FL. However, FedAvg convergence analysis makes assumptions such as bounded gradients [28, 56, 41] or bounded gradient dissimilarity [45, 12], showing it is not resistant to non-i.i.d. data. To tackle this issue, numerous techniques have been proposed in the literature, including incorporating static/dynamic regularizers [27, 1, 57, 16], adaptive control variables [29, 8, 51, 50], and/or gradient tracking methods [18, 32]. Despite these efforts, existing FL algorithms are not easily extendable to HFL due to the timescale mismatch of multi-level model aggregations induced by the hierarchical system topology. Optimizing these algorithms and ensuring their theoretical convergence in the presence of hierarchical model drift remains unsolved. Our paper addresses these issues through a principled multi-timescale gradient correction method. ", "page_idx": 2}, {"type": "text", "text": "Hierarchical FL. The authors of [5, 30, 13, 49, 55, 47] explored a new FL branch, HFL, tailored for hierarchical systems consisting of a central server, group aggregators, and clients. To tackle the issue of limited communication resources, the authors of [5] developed a FedAvg-like algorithm called hierarchical FedAvg tailored to HFL, and analyzed its convergence behavior. However, their algorithm is built upon an assumption of i.i.d. data. Another work [47] investigated the convergence behavior of hierarchical FedAvg under the non-i.i.d. setup. However, the convergence bound becomes worse as the extent of data heterogeneity increases, making the algorithm vulnerable to non-i.i.d. data characteristics. In [34], ProxSkip-HUB is introduced, but requires clients to compute full batch gradients and upload them to group aggregators after every iteration, which is impractical especially when training large-scale models. Overall, there is still a lack of an algorithm that fully addresses the unique challenge of HFL, i.e., the multi-timescale model drift problem, with theoretical guarantees. We fill this gap by introducing multi-timescale gradient correction and providing theoretical insights. ", "page_idx": 2}, {"type": "text", "text": "Gradient tracking/correction. Both gradient tracking and gradient correction aim to fix the local updating directions of clients to mitigate the impact of model drift caused by data heterogeneity. The gradient tracking concept was originally proposed and analyzed in [9] and then extended to consider various factors like time-vary graphs and asynchronous updates [37, 40, 42, 54]. Subsequently, SCAFFOLD [18] applied gradient tracking in FL to mitigate the impact of data heterogeneity across clients, ensuring convergence and stability in non-i.i.d. settings. More recently, in [32, 2], the authors demonstrate the effectiveness of gradient tracking in fully decentralized FL, where clients conduct model aggregations through local client-to-client communications. In [6, 43], gradient tracking is further studied in a semi-decentralized FL setup. Compared to all prior research, our work is the earliest attempt to design an algorithm specifically tailored to multi-timescale model drift in HFL and its training process with periodic local/global aggregations. This presents new challenges in our algorithm design and convergence analysis due to the coupling of our correction terms through their updates at different timescales. In Section 5, we empirically validate the effectiveness of our approach over the prior gradient correction method. ", "page_idx": 2}, {"type": "text", "text": "2  Background and Motivation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 Problem Setup: Hierarchical FL ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We consider the hierarchical system depicted in Fig. 1. The central server is connected to $N$ group aggregators, each linked to the clients within its region, defined as a group. Each group $j\\in\\{1,2,\\dots,N\\}$ consists of a set of $n_{j}$ non-overlapping clients, denoted $\\mathcal{C}_{j}$ , resulting in a total of $\\textstyle\\sum_{j=1}^{N}n_{j}$ clients within the system. Each client $i$ has its own local data distribution $\\mathcal{D}_{i}$ . The goal of HFL is to construct an optimal global model $x^{*}$ considering the data distributions of all clients in the system. The role of each group aggregator $j$ involves coordinating the training for the $n_{j}$ clients within its region, while the central server orchestrates the training across all $N$ groups through interaction with the group aggregators. We can formally state the HFL learning objective as follows: ", "page_idx": 2}, {"type": "image", "img_path": "aCAb1qNXI0/tmp/dd078c638d17449afb855813a23ffa084fb0eacc30a9d705794c700f95195cb3.jpg", "img_caption": ["Figure 2: Visualization of the local update process using multi-timescale gradient correction (MTGC) with 4 clients and 2 groups. (a) Without any gradient correction (e.g., hierarchical FedAvg), each client model moves towards its respective optimal point, denoted by $\\pmb{x}_{i}^{*}$ . (b) When only client-group correction term $_{z_{i}}$ is applied, the model of client $i\\in\\mathcal{C}_{j}$ moves towards the group optimum $\\bar{\\pmb{x}}_{j}^{*}$ . (c) In MTGC, the gradient of client $i\\in\\mathcal{C}_{j}$ is adjusted by both the client-group correction term $\\mathscr{z}_{i}$ and the group-global correction variable $\\pmb{y}_{j}$ , assisting each client model to converge towards the global optimum $x^{*}$ during local iterations. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Here, $f:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ denotes the global loss function, $f_{j}:\\ensuremath{\\mathbb{R}^{d}}\\to\\ensuremath{\\mathbb{R}}$ is the loss specific to group $j$ , and $F_{i}:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ represents the local loss for client $i$ . In addition, $\\xi_{i}$ is the data point sampled from distribution $\\mathcal{D}_{i}$ . Note that our analysis can be easily extended to a weighted average form of (1) by incorporating positive coefficients for each $f_{j}({\\pmb x})$ or $F_{i}(x)$ . For simplicity, these coefficients are assumed to be included in $F_{i}(x)$ as in previous works [18, 48]. ", "page_idx": 3}, {"type": "text", "text": "2.2  Limitation of Existing Works ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In HFL algorithms, group aggregations are conducted after every. $H$ local client updates, while global aggregations are performed after every $E$ group aggregations, introducing different timescales. Moreover, different forms of data heterogeneity exist in HFL: (i) intra-group non-i.i.d., due to data heterogeneity across different clients $i\\in\\mathcal{C}_{j}$ , and (i inter-group non-i.i.d., arising from data heterogeneity across different groups $\\mathcal{C}_{1},...,\\mathcal{C}_{N}$ . These lead to client model drift and group model drift, respectively. The model drifts induced by multi-level data heterogeneity at different timescales hinder hierarchical FedAvg from converging. In Fig. 2(a), we see that during local training, each local model gradually converges towards the optimal point of its respective client's objective function. Hence, to guarantee theoretical convergence, existing HFL works either assume an i.i.d. setup [5] or rely on a bounded gradient dissimilarity assumption similar to the following [47, 15]: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{1}{N}\\sum_{j=1}^{N}\\|\\nabla f_{j}(x)-\\nabla f(x)\\|^{2}\\leq\\delta_{1}^{2},\\forall x\\;\\operatorname{and}\\;\\frac{1}{n_{j}}\\sum_{i\\in{\\mathcal{C}_{j}}}\\|\\nabla F_{i}(x)-\\nabla f_{j}(x)\\|^{2}\\leq\\delta_{2}^{2},\\forall x,\\forall j.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The first inequality is employed to limit the group drift, i.e., the deviation of group gradients from the global gradient, while the second one bounds the client drift, i.e., the divergence of client gradients from their group gradient. As a result, the convergence bounds of algorithms in these works become worse as data heterogeneity increases (i.e., as $\\delta_{1}$ Or $\\delta_{2}$ increase) [19]. Our approach, developed next, does not require these assumptions and remains stable regardless of the extent of data heterogeneity. ", "page_idx": 3}, {"type": "text", "text": "3 Algorithm ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "3.1  Intuition: Gradient Correction in Hierarchical FL ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "When relying on multiple local SGD iterations as in hierarchical FedAvg, the model update process is not stable even when the model has reached the optimal $x^{*}$ satisfying $\\nabla f(\\mathbf{x}^{*})=\\mathbf{0}$ . Specifically, with $\\gamma$ as the learning rate, we have $\\pmb{x}^{*}\\neq\\pmb{x}^{*}-\\gamma\\nabla F_{i}(\\pmb{x}^{*})$ , as the global optimum $x^{*}$ may not necessarily be optimal for each client's local loss due to data heterogeneity, i.e., $\\nabla F_{i}(\\mathbf{x}^{*})\\neq\\mathbf{0}$ [39]. Correcting the client gradient $\\nabla F_{i}({\\pmb x}^{*})$ to the global gradient $\\nabla f(x^{*})$ is thus necessary to stabilize the process. ", "page_idx": 3}, {"type": "text", "text": "Motivation and idea. In HFL, however, due to multi-level aggregations occurring at different timescales, it is infeasible to directly correct the client gradient to the global gradient. In particular, clients are not able to communicate with the central server directly, and there are multiple group aggregation steps before the group aggregators communicate with the main server. Our idea is thus to inject two gradient correction terms: client-group correction and group-global correction. Specifically, the desired iteration to obtain the updated model $x_{\\mathrm{new}}$ at the optimal point $x^{*}$ canbewritten as ", "page_idx": 3}, {"type": "image", "img_path": "aCAb1qNXI0/tmp/54080f1d82018839bf344ba7052f8ca67043b14596887ab4d277da37f6fcb646.jpg", "img_caption": [], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{x_{\\mathrm{new}}=x^{*}-\\gamma\\Big\\{\\nabla F_{i}(\\boldsymbol{x}^{*})+\\underbrace{(\\nabla f_{j}(\\boldsymbol{x}^{*})-\\nabla F_{i}(\\boldsymbol{x}^{*}))}_{\\mathrm{client.group~correction}}+\\underbrace{(\\nabla f(\\boldsymbol{x}^{*})-\\nabla f_{j}(\\boldsymbol{x}^{*}))}_{\\mathrm{group.global~correction}}\\Big\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\nabla f_{j}(\\pmb{x}^{*})-\\nabla F_{i}(\\pmb{x}^{*})$ and $\\nabla f(\\pmb{x}^{*})-\\nabla f_{j}(\\pmb{x}^{*})$ represent client-group and group-global correction terms, respectively. Since $\\nabla f(\\mathbf{x}^{*})=\\mathbf{0}$ , the two correction terms will enable the model to remain at the optimal point. Given this intuition, the ideal local iteration at client $i\\in\\mathcal{C}_{j}$ can be written as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\begin{array}{r}{x_{i,h+1}^{t,e}=x_{i,h}^{t,e}-\\gamma\\Big\\{\\nabla F_{i}(x_{i,h}^{t,e})+\\Big(\\nabla f_{j}(\\bar{x}_{j,h}^{t,e})-\\nabla F_{i}(x_{i,h}^{t,e})\\Big)+\\Big(\\nabla f(\\bar{x}_{h}^{t,e})-\\nabla f_{j}(\\bar{x}_{j,h}^{t,e})\\Big)\\Big\\},}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $t,e$ , and $h$ represent global communication rounds, group communication rounds, and client local iterations, respectively, ,h. $\\begin{array}{r}{\\bar{\\pmb{x}}_{j,h}^{t,e}\\ =\\ \\frac{1}{n_{j}}\\sum_{i\\in{\\cal C}_{j}}{\\pmb{x}}_{i,h}^{t,e}}\\end{array}$ is the averaged model within group $j$ , and $\\begin{array}{r}{\\bar{\\mathbf{x}}_{h}^{t,e}=\\frac{1}{N}\\sum_{j=1}^{N}\\frac{1}{n_{j}}\\sum_{i\\in\\mathcal{C}_{j}}{\\mathbf{x}}_{i,h}^{t,e}}\\end{array}$ bring each client model closer to the global optima during local updates, as illustrated in Fig. 2(c). ", "page_idx": 4}, {"type": "text", "text": "Challenge encountered in HFL. However, it is important to note that the update process in (4) still cannot be directly used in HFL. This is because client-group communication and group-global communication do not occur at every iteration of HFL training; instead, they happen at different timescales, and clients are not able to obtain the current group information $\\bar{\\nabla}f_{j}\\bar{(}\\bar{\\pmb{x}}_{i,h}^{t,e})$ and global information $\\nabla f({\\bar{\\pmb x}}_{i,h}^{t,e})$ at every local iteration. We next propose a strategy that mimics the gradient correction described above while ensuring theoretical convergence. ", "page_idx": 4}, {"type": "text", "text": "3.2 Multi-Timescale Gradient Correction (MTGC) ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Tackling multi-timescale model drifts. To approximate (4) during HFL training, we introduce two control variables $_{\\textit{z}}$ and $\\textit{\\textbf{y}}$ that track/approximate $\\nabla f_{j}-\\nabla F_{i}$ and $\\nabla f-\\nabla f_{j}$ , respectively. The variables $_{z}$ and $\\textit{\\textbf{y}}$ are then employed to correct the local gradients to prevent model drifts. The challenge here is to keep updating $_{z}$ and $\\textit{\\textbf{y}}$ appropriately in the multi-timescale communication scenario, given that communications between the clients and group aggregator, and between the group aggregators and global aggregator, are not always feasible. We propose a strategy to update $_{\\textit{z}}$ after every $H$ local iterations, i.e., whenever each client is able to communicate with the group aggregator, allowing the group information to be updated and shared among the clients within the same group. Similarly, we propose a strategy to update $\\textit{\\textbf{y}}$ after every $E$ group aggregations, i.e., whenever the group aggregators are able to communicate with the global aggregator, enabling the global information to be refreshed and shared across all clients in the system. We name our strategy multi-timescale gradient correction (MTGC) due to the updates of $_{\\textit{z}}$ and $\\textit{\\textbf{y}}$ occurring in different timescales, to tackle the issue of multi-level model drift coupled across the hierarchy in HFL. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "In Fig. 2(c), we illustrate MTGC during client-side model updates. In particular, at each local iteration $h$ of groupround $e$ of global round $t$ , each client $i\\in\\mathcal{C}_{j}$ updates its local model as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\pmb{x}_{i,h+1}^{t,e}=\\pmb{x}_{i,h}^{t,e}-\\gamma\\left(\\nabla F_{i}(\\pmb{x}_{i,h}^{t,e},\\xi_{i,h}^{t,e})+\\boldsymbol{z}_{i}^{t,e}+\\pmb{y}_{j}^{t}\\right).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "(i) Client-group correction term. In (5), $z_{i}^{t,e}$ is responsible for correcting the gradient of client $i\\in\\mathcal{C}_{j}$ towards the gradient of group $j$ at the $e$ -th group aggregation of global round $t$ .After every group aggregation $e$ at global round $t$ , this term is updated at each client $i$ as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{z_{i}^{t,e+1}=\\frac{1}{H}\\sum_{h=0}^{H-1}\\Bigg(\\Big(\\frac{1}{n_{j}}\\sum_{i\\in\\mathcal{C}_{j}}\\nabla F_{i}(\\pmb{x}_{i,h}^{t,e},\\xi_{i,h}^{t,e})\\Big)-\\nabla F_{i}(\\pmb{x}_{i,h}^{t,e},\\xi_{i,h}^{t,e})\\Bigg).}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "(i) Group-global correction term. $\\boldsymbol{y}_{j}^{t}$ in (5) aims to correct the gradient of group $j$ towards the global gradient. At the end of global round $t$ , this term is updated at group aggregator $j$ as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{y}_{j}^{t\\mathrm{+1}}=\\frac{1}{H E}\\Bigg(\\Big(\\frac{1}{N}\\sum_{j=1}^{N}\\frac{1}{n_{j}}\\sum_{i\\in\\mathcal{C}_{j}}\\nabla F_{i}(\\pmb{x}_{i,h}^{t,e},\\xi_{i,h}^{t,e})\\Big)-\\frac{1}{n_{j}}\\sum_{i\\in\\mathcal{C}_{j}}\\nabla F_{i}(\\pmb{x}_{i,h}^{t,e},\\xi_{i,h}^{t,e})\\Bigg).}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Key remarks. The updating policies for $z_{i}^{t,e}$ and $\\boldsymbol{y}_{j}^{t}$ follow similar paterns to the ideal corrections outlined in (4). Here, we observe that \u2265iec,2 $\\begin{array}{r}{\\sum_{i\\in{\\mathcal{C}}_{j}}z_{i}^{t,e}=\\mathbf{0},\\forall j}\\end{array}$ and $\\textstyle\\sum_{j=1}^{N}\\mathbf{\\boldsymbol{y}}_{j}^{t}\\,=\\,\\mathbf{\\boldsymbol{0}}$ indicatingthat the correction terms do not have an impact on the per-iteration model averages. Instead, the introduction of $z_{i}^{t,e}$ and $\\boldsymbol{y}_{j}^{t}$ eliminates model drifts of clients and groups, respectively, during local iterations. Intuitively, as the iteration approaches the global optimal point, we expect $z_{i}^{t,e}\\to\\nabla f_{j}(\\pmb{x}^{*})-\\nabla F_{i}(\\pmb{x}^{*})$ and $\\pmb{y}_{j}^{t}\\rightarrow\\nabla f(\\pmb{x}^{*})-\\nabla f_{j}(\\pmb{x}^{*})$ so that the update in (5) stabilizes at the global optimal point. We also see that $z_{i}^{t,e}$ and $\\pmb{y}_{j}^{t}$ are coupled (5), ie,the update of one o the terms affets $\\textbf{\\em x}$ which in turn ffects the other one, raising challenges for theoretical analysis. In Section 4, we will guarantee convergence of MTGC in general non-convex settings without relying on bounded data heterogeneity assumptions. ", "page_idx": 5}, {"type": "text", "text": "MTGC algorithm. The overall procedure of our training strategy is summarized in Algorithm 1, where we rewrite the updates of $z_{i}^{t,e}$ and $\\boldsymbol{y}_{j}^{t}$ in a different but equivalent manner to facilitate practical implementation of MTGC. Compared to hierarchical FedAvg, which does not consider any correction terms, we see that no additional communication is required for MTGC within each group round $e$ Additional communication is introduced only after $E$ group aggregations for initializing $z_{i}^{t,0}$ (Line 4)2 and broadcasting y (obtained in Line 14) to the clients in $\\mathcal{C}_{j}$ . We will see in Section 5 that these marginal additional costs lead to significant performance enhancements for HFL settings. ", "page_idx": 5}, {"type": "text", "text": "Generalization to arbitrary number of levels. The proposed MTGC algorithm can be extended to an HFL system architecture with an arbitrary number of levels. Further discussions and experimental results for a three-level case are provided in Appendix E. ", "page_idx": 5}, {"type": "text", "text": "3.3 Connection with SCAFFOLD ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "When the number of groups reduces to $N=1$ with $E\\,=\\,1$ , we have $\\pmb{{y}}_{j}^{t}\\,=\\,0$ (no group-global correction), and thus MTGC reduces to SCAFFOLD [18]. In SCAFFOLD, at each round $t$ , clients perform local updates according to $\\begin{array}{r}{\\pmb{x}_{i,h+1}^{t}=\\pmb{x}_{i,h}^{t}-\\gamma\\left(\\nabla F_{i}(\\pmb{x}_{i,h}^{t},\\xi_{i,h}^{t})-\\pmb{c}_{i}^{t}+\\pmb{c}^{t}\\right),h=0,1,\\ldots,H\\!-\\!1,}\\end{array}$ Where $\\begin{array}{r}{\\pmb{c}_{i}^{t+1}\\,=\\,\\pmb{\\dot{c}}_{i}^{t}\\,-\\,\\pmb{c}^{t}\\,+\\,\\frac{1}{H\\gamma}\\,\\left(\\pmb{\\bar{x}}^{\\,\\bar{t}}-\\pmb{x}_{i,H}^{t}\\right)\\,}\\end{array}$ and the server agregates local models and controlling variables as $\\begin{array}{r}{\\bar{\\pmb{x}}^{t+1}=\\frac{1}{N}\\sum_{i=1}^{N}{\\pmb{x}}_{i,H}^{t}}\\end{array}$ and $\\begin{array}{r}{\\pmb{c}^{t+1}=\\frac{1}{N}\\sum_{i=1}^{N}\\pmb{c}_{i}^{t+1}}\\end{array}$ We can show that $c_{i}^{t}-c^{t}$ in SCAFFOLD plays the same rle as $z_{i}^{t,e}$ in MTGC. However, th aditioal term $\\boldsymbol{y}_{j}^{t}$ introduced in MTGC for the multi-level setting makes the convergence guarantee more challenging, as $\\boldsymbol{y}_{j}^{t}$ is coupled with $z_{i}^{t,e}$ and both are updated at different time scales. These aspects will be thoroughly examined next. ", "page_idx": 5}, {"type": "text", "text": "4 Convergence Analysis ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we establish a convergence guarantee for the proposed MTGC algorithm. Our theoretical analysis relies on the following standard assumptions commonly used in the literature on stochastic optimization and $\\mathrm{FL}$ under non-convex settings [18, 44, 4]. ", "page_idx": 5}, {"type": "text", "text": "Assumption 1. Each local loss function $F_{i}$ is differentiable and $L$ -smooth, i.e., there exists a positive constant $L$ such that for any $\\textbf{\\em x}$ and $\\textit{\\textbf{y}}$ $\\lVert\\nabla F_{i}(\\pmb{x})-\\nabla F_{i}(\\pmb{y})\\rVert\\leq L\\lVert\\pmb{y}-\\pmb{x}\\rVert,\\forall i$ ", "page_idx": 6}, {"type": "text", "text": "Assumption 2. The stochastic gradient $\\nabla F_{i}({\\pmb x},\\xi_{i})$ is an unbiased estimate of the true gradient, i.e., $\\mathbb{E}_{\\xi_{i}\\sim\\mathcal{D}_{i}}[\\nabla F_{i}(\\pmb{x},\\xi_{i})]=\\nabla F_{i}\\bar{(\\pmb{x})},\\forall\\pmb{x}$ and the variance of the stochastic gradient $\\nabla F_{i}({\\pmb x},\\xi_{i})$ is uniformly bounded as $\\begin{array}{r}{\\mathbb{E}_{\\xi_{i}\\sim\\mathcal{D}_{i}}\\|\\nabla F_{i}(\\pmb{x},\\xi_{i})-\\nabla F_{i}(\\pmb{x})\\|^{2}\\leq\\sigma^{2},\\forall\\pmb{x}.}\\end{array}$ ", "page_idx": 6}, {"type": "text", "text": "Note that (i) global aggregation, (i) the update of upper-level correction variable $\\textit{\\textbf{y}}$ and local aggregation, and (i) the update of lower-level correction variable $_{z}$ are performed at different timescales in MTGC. If we directly consider $\\{\\nabla f({\\bar{\\mathbf{x}}}^{t})\\}$ as in SCAFFOLD, it is difficult to capture the effects of group aggregation and correction variable $_{z}$ . Moreover, it is hard to establish a tight conecton betwcen $\\nabla f({\\bar{\\mathbf{x}}}^{t})$ and $\\mathbf{\\boldsymbol{x}}_{i,h}^{t,e},\\forall i,h,\\tau$ sincethereis larg lag betwcen $\\pmb{x}_{i,h}^{t,e}$ and ${\\bar{\\pmb{x}}}^{t}$ To tackle this, we introduce a new metric, which is the gradient $\\nabla f(\\hat{\\mathbf{x}}^{t,e})$ at virtual sequence $\\{\\hat{\\mathbf{\\boldsymbol{x}}}^{t,e}\\}$ ,to characterize the convergence of MTGC. ", "page_idx": 6}, {"type": "text", "text": "We next state our main theoretical results. All the proofs are provided in Appendix F: ", "page_idx": 6}, {"type": "text", "text": "Theorem 4.1. Suppose Assumptions 1 and 2 hold and the learning rate satisfies  \u2264 40EHL .Then theiterates $\\{\\hat{\\mathbf{\\boldsymbol{x}}}^{t,e}\\}$ obtained by the MTGC algorithm satisfy ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\frac{1}{T E}\\sum_{t=0}^{T-1}\\sum_{e=0}^{E-1}\\mathbb{E}\\left\\|\\nabla f(\\hat{\\mathbf{x}}^{t,e})\\right\\|^{2}=\\mathcal{O}\\left(\\frac{f(\\bar{x}^{0})-f^{*}}{\\gamma T E H}+\\frac{\\gamma}{\\tilde{N}}L\\sigma^{2}+\\gamma^{2}E^{2}H^{2}L^{2}\\sigma^{2}\\right),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\begin{array}{r}{\\tilde{N}=\\left(\\frac{1}{N^{2}}\\sum_{j=1}^{N}\\!\\frac{1}{n_{j}}\\right)^{-1}}\\end{array}$ and $f^{*}$ is thelowe boundof $f({\\boldsymbol{x}})$ i.e, $f({\\mathbf{\\alpha}})\\geq f^{*}$ ", "page_idx": 6}, {"type": "text", "text": "There are two key steps in our proof. The first is the characterization of the evolution of ${\\left\\|{z_{i}^{t,e}+\\nabla F_{i}\\left(\\bar{\\pmb{x}}_{j}^{t,e}\\right)-\\nabla f_{j}\\left(\\bar{\\pmb{x}}_{j}^{t,e}\\right)}\\right\\|^{2}}$ and $\\left\\|\\pmb{{y}}_{j}^{t}+\\nabla{f}_{j}\\left(\\hat{\\pmb{x}}^{t,e}\\right)-\\nabla{f}\\left(\\hat{\\pmb{x}}^{t,e}\\right)\\right\\|^{2}$ By bounding these valuesthat capture the error between each control variable and the ideal correction, we are able to establish a connection between the local updating direction and the global gradient without relying on the bounded gradient dissimilarity assumption, laying the foundation for the whole proof. The second is that we extracted a recursive relationship for the accumulation of group-level and client-level model drifts, and designed a novel Lyapunov function to mitigate the interplay impact between these drifts. Further details are provided in the appendix. ", "page_idx": 6}, {"type": "text", "text": "Applying an appropriate learning rate $\\gamma$ to Algorithm 1 yields the following corollary: ", "page_idx": 6}, {"type": "text", "text": "Corollary 4.1.Under the assumptions of Theorem 4.1, let ${\\mathcal{F}}_{0}=f(\\bar{{\\pmb x}}^{0})-f^{*}$ .Thenthereexists $a$ learningrate $\\begin{array}{r}{\\gamma\\leq\\frac{1}{40E H L}}\\end{array}$ Ssuchthatheiterates $\\{\\hat{\\mathbf{\\boldsymbol{x}}}^{t,e}\\}$ satisfi ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\frac{1}{T E}\\sum_{t=0}^{T-1E-1}\\underset{e=0}{\\overset{\\mathrm{E}}{\\sum}}\\left\\|\\nabla f(\\hat{\\pmb{x}}^{t,e})\\right\\|^{2}\\leq\\mathcal{O}\\left(\\sqrt{\\frac{\\mathcal{F}_{0}L\\sigma^{2}}{\\tilde{N}T E H}}\\!+\\!\\left(\\frac{\\mathcal{F}_{0}L\\sigma}{T}\\right)^{\\frac{2}{3}}\\!+\\!\\frac{L\\mathcal{F}_{0}}{T}\\right).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Discussions. Corollary 4.1 provides the convergence upper bound of the MTGC algorithm. It shows that the error approaches zero as $T\\rightarrow\\infty$ If $\\sigma\\ne0$ , the upper bound is dominated by the first term in the right-hand side of (9), which characterizes the speed of convergence of MTGC to a stationary point in the stochastic case. This reveals MTGC achieves linear speedup in the number of group aggregations $E$ and local updates $H$ . In other words, we can attain the same level of performance with less global communication rounds, i.e., a smaller value of $T$ , by increasing the number of local iterations, i.e., $H$ , and group aggregations, i.e., $E$ When considering the special case $n_{j^{\\prime}}=n$ \uff0c $\\forall j^{\\prime}\\in$ $\\{1,2,\\ldots,N\\}$ with uniform client numbers, the rate becomes $\\mathcal{O}(\\sqrt{\\mathcal{F}_{0}L\\sigma^{2}}/\\sqrt{N n T E H})$ . This implies that MTGC attains linear speedup in the number of clients as well. ", "page_idx": 6}, {"type": "text", "text": "Moreover, we also see that our convergence rate recovers the results of SCAFFOLD when the number of groups reduces to $N=1$ and the number of group aggregations reduces to $E=1$ (see Appendix $\\mathrm{G}$ for more discussions). We also highlight that, different from prior works on HFL where the convergence bound becomes worse as the extent of data heterogeneity increases, our bound is stable against multi-level non-i.i.d. data due to the multi-timescale gradient correction approach. ", "page_idx": 6}, {"type": "text", "text": "5 Experimental Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "5.1Setup", "page_idx": 6}, {"type": "text", "text": "Dataset, model, hyperparameters, and compute setting. In our experiments, we consider four widely used datasets: EMNIST-Letters (EMNIST-L) [7], Fashion-MNIST [53], CIFAR-10 [23], and ", "page_idx": 6}, {"type": "image", "img_path": "aCAb1qNXI0/tmp/a741fa68283bbd99bd40b12ba8f6a1a83492c141bdd0a4d9596b75cbcdbfcd6d.jpg", "img_caption": ["Figure 3: Comparison with FL baselines. In this experiment, popular FL algorithms are extended to the HFL setup for comparison with MTGC. We consider four datasets in the group non-i.i.d. & client non-i.i.d. setting. Experiments are conducted over 3 random trials. We see that MTGC obtains the best testing accuracy in each case, validating our multi-level approach for correcting multi-timescale model drifts. "], "img_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "aCAb1qNXI0/tmp/b785f6bc95cb45a134d35c335b1626fd32dc057971663c81b0d9a467aa9a92ba.jpg", "img_caption": ["Figure 4: Comparison with gradient correction baselines. Three different data distribution scenarios are considered. We see that the local correction method is effective for handing client non-i.i.d. within each group (top row), while the group correction method is effective for handling non-i.i.d. across groups (middle row). MTGC obtains the most stable performance (all rows) by combining multiple correction levels. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "CIFAR-100 [23]. The former two are processed through a multi-layer perceptron (MLP) model, featuring two hidden layers, each comprising 200 neurons, and ending with a softmax layer. For the CIFAR-10 classification task, we employ a convolutional neural network (CNN) following the architecture outlined in seminal work [35]. For CIFAR-100, we adopt a ResNet-18 model with batch normalization layers substituted by group normalization layers. Across all algorithms considered, we maintain a consistent learning rate $\\eta=0.1$ and batch size 50. We conduct the experiments based on a cluster of 3 NVIDIA A100 GPUs with 40 GB memory. Our code is based on the framework of [1]. ", "page_idx": 7}, {"type": "text", "text": "FL data distribution. We set the total number of clients as 100, evenly distributed over $N=10$ groups. We also study the effect of $N$ in Appendix B. We consider three different data distribution settings: (i) group i.i.d. & client non-i.i.d., (ii) group non-i.i.d. & client i.i.d., and (i) group non-i.i.d. & client non-i.i.d. scenarios. First, in the group i.i.d. & client non-i.i.d. case, the training dataset is initially divided uniformly and randomly into $N$ segments corresponding to $N$ groups. Subsequently, each segment is further divided into $100/N$ partitions for the clients using a Dirichlet distribution [1]. Second, in the group non-i.i.d. & client i.i.d. case, the dataset is first segmented into $N$ partitions for the groups using a Dirichlet distribution, followed by a uniform random distribution of each segment to $100/N$ clients. Finally, when both groups and clients are non-i.i.d., the dataset is split into $N$ segments for the groups using a Dirichlet distribution, and then, each group\u2019s segment is distributed among $100/N$ clients through a Dirichlet distribution. The Dirichlet parameter is set to 0.1. ", "page_idx": 7}, {"type": "table", "img_path": "aCAb1qNXI0/tmp/b757c8fa7aee529875b8a4d748c1905d7fb8108116751cbe1843dff3bb6dceb5.jpg", "table_caption": ["Table 5.1: The number of global rounds required by different algorithms to attain the testing accuracy of $80\\%$ for CIFAR-10 under different settings. Taking HFedAvg as the benchmark, we show the speedup achieved by MTGC and other baselines as we vary aggregation periods $E$ and $H$ . MTGC consistently outperforms baselines, and the speedup gets more significant as $E$ and $H$ increase. Standard deviation is based on 3 random trials. ", "5.2  Results and Discussion "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Comparison with conventional FL algorithms. For comparison, we first apply the well-known FL methods, FedProx [27], SCAFFOLD [18], and FedDyn [1], to HFL, by running their training algorithms within each group of the hierarchical system. We also consider HFedAvg [47] as a baseline. Fig. 3 compares MTGC with these baselines in the group non-i.i.d. & client non-i.i.d. case. We observe that MTGC outperforms all the considered conventional algorithms, achieving the highest testing accuracy, especially for the complicated CIFAR-100 dataset. FedDyn achieves the lowest performance, demonstrating significant variance and instability. The significant performance gap between MTGC and FedDyn, in particular, can be attributed to the hierarchical setup disrupting the special structure of FedDyn. This result reveals that some algorithms designed for the conventional star-topology FL may be non-trivial to be extended to hierarchical setups. The overall results confirm the effectiveness of our approach that effectively tackles the multi-timescale drift problem in HFL. ", "page_idx": 8}, {"type": "text", "text": "Comparison with gradient correction baselines. In Fig. 4, we compare MTGC with the gradient correction baselines. Specifically, we apply local correction $(z_{i}^{t,e})$ to HFedAvg, and group correction $(\\pmb{y}_{j}^{t})$ to HFedAvg. These baselines can be viewed as schemes applying SCAFFOLD [18] within each group and across groups, respectively. We also report the results of the original HFedAvg to see the effects of gradient correction clearly. We make the following key observations. First, the testing accuracy achieved by HFedAvg decreases as the extent of data heterogeneity increases, e.g., from the first or second row to the third row in Fig. 4. This shows that data heterogeneity hinders the convergence of HFedAvg. Second, with the assistance of local or group correction, the algorithm attains a higher accuracy. In the case of group i.i.d. & client non-i.i.d., HFedAvg augmented with client local correction performs better than the variant with group correction. Conversely, in the scenario where groups are non-i.i.d. and clients are i.i.d., the opposite holds. This can be explained by the dominance of data heterogeneity in each case. In the former scenario, because the heterogeneity is primarily at the client-level, local client correction becomes more beneficial. On the other hand, in the latter scenario, where the heterogeneity shifts to the group level, group correction becomes more advantageous. Finally, we see that MTGC consistently outperforms baselines under all settings, where the performance gains brought by the multi-timescale gradient correction become more significant when it comes to the group non-i.i.d.& client non-i.i.d. case. ", "page_idx": 8}, {"type": "text", "text": "Speedup in $H$ and $E$ . In Table 5.1, we investigate the effects $H$ and $E$ , which determine the periods of group aggregation and global aggregation in HFL. We report the number of global rounds required to attain the desired testing accuracy of $80\\%$ for CIFAR-10 under different settings. We have the following observations: As $E$ or $H$ increases, the required number of global rounds of MTGC for achieving the desired accuracy decreases. This demonstrates the speedup of the proposed algorithm in the number of local iterations and group aggregations, which fits well with our theory discussed in Section 4. In addition, the speedup achieved by MTGC compared to HFedAvg gets more significant as $E$ or $H$ increases. For instance, in the group i.i.d. & client non-i.i.d. case, MTGC attains $3.3\\times$ speedup when $E=10\\:,H=20$ ,which increasesto $4.7\\times$ when $E=10$ \uff0c $H=40$ . This reveals that MTGC utilizes local iterations better compared with the baselines. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Impact of data heterogeneity. Consistent with the results in Fig. 4, we see from Table 5.1 that the required number of global rounds of HFedAvg increases as data heterogeneity increases, while MTGC is more stable against non-i.i.d. data. The gain of MTGC over HFedAvg becomes evident as data heterogeneity increases, confirming the effectiveness of our multi-timescale gradient correction approach for addressing the unique challenges of HFL. ", "page_idx": 9}, {"type": "text", "text": "Further experiments. Additional experimental results including the impacts of hierarchical system parameters and the performance in 3-level HFL are provided in Appendices B and E. ", "page_idx": 9}, {"type": "text", "text": "6  Conclusion and Limitation ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We have proposed MTGC, a multi-timescale gradient correction approach for HFL. Embedded with control variables updated in different timescales, MTGC effectively corrects gradient biases and alleviates both client model drift and group model drift in hierarchical setups. We established the convergence bound of MTGC in the non-convex setup and showed its stability against multi-level data heterogeneity. Finally, we confirmed the advantage of our MTGC through extensive experiments in different non-i.i.d. HFL settings. A limitation of our work is that despite providing experiments for HFL systems with more than two levels (in Appendix E), our convergence analysis focused on the two-level case, which provides an interesting future direction of investigation. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was supported by the National Science Foundation (NSF) under grants CNS-2146171 and CPS-2313109, and by the Air Force Office of Scientific Research (AFOSR) under grant FA9550-24- 1-0083. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Acar, D.A.E., Zhao, Y., Matas, R., Mattina, M., Whatmough, P., Saligrama, V.: Federated learning based on dynamic regularization. In: International Conference on Learning Representations (2020)   \n[2]  Alghunaim, S.A.: Local exact-diffusion for decentralized optimization and learning. IEEE Transactions on Automatic Control (2024)   \n[3] Bao, W., Wang, H., Wu, J., He, J.: Optimizing the collaboration structure in cross-silo federated learning. In: International Conference on Machine Learning. Pp. 1718-1736. PMLR (2023)   \n[4] Bottou, L., Curtis, F.E., Nocedal, J.: Optimization methods for large-scale machine learning (2018)   \n[5]  Castiglia, T., Das, A., Patterson, S.: Multi-level local SGD: Distributed SGD for heterogeneous hierarchical networks. In: International Conference on Learning Representations (2020)   \n[6] Chen, E., Wang, S., Brinton, C.G.: Taming subnet-drift in D2D-enabled fog learning: A hierarchical gradient tracking approach. arXiv preprint arXiv:2312.04728 (2023)   \n[7]  Cohen, G., Afshar, S., Tapson, J., van Schaik, A.: EMNIST: an extension of mnist to handwritten letters (2017)   \n[8] Condat, L., Agarsky, I., Malinovsky, G., Richtarik, P.: TAMUNA: Doubly accelerated federated learning with local training, compression, and partial participation. arXiv preprint arXiv:2302.09832 (2023)   \n[9]  Di Lorenzo, P., Scutari, G.: Next: In-network nonconvex optimization. IEEE Transactions on Signal and Information Processing over Networks 2(2), 120-136 (2016) [10] Fang, W., Han, D.J., Brinton, C.G.: Submodel partitioning in hierarchical federated learning: Algorithm design and convergence analysis. In: ICC 2024-IEEE International Conference on Communications. pp. 268-273. IEEE (2024) [11] Fang, W., Yu, Z., Jiang, Y, Shi, Y., Jones, C.N., Zhou, Y.: Communication-effcient stochastic zeroth-order optimization for federated learning. IEEE Transactions on Signal Processing 70,   \n5058-5073 (2022) [12] Haddadpour, F., Mahdavi, M.: On the convergence of local descent methods in federated learning. arXiv preprint arXiv: 1910.14425 (2019) [13] Hosseinalipour, S., Azam, S.S., Brinton, C.G., Michelusi, N., Aggarwal, V., Love, D.J., Dai, H.: Multi-stage hybrid federated learning over large-scale D2D-enabled fog networks. IEEE/ACM transactions on networking 30(4), 1569-1584 (2022) [14] Hosseinalipour, S., Brinton, C.G., Aggarwal, V., Dai, H., Chiang, M.: From federated to fog learning: Distributed machine learning over heterogeneous wireless networks. IEEE Communications Magazine 58(12), 41-47 (2020) [15] Jiang, X., Zhu, H.: On the convergence of hierarchical federated learning with partial worker participation. In: The 40th Conference on Uncertainty in Artificial Intelligence (2024) [16] Jiang, X., Rodomanov, A., Stich, S.U.: Federated optimization with doubly regularized drift correction. In: Forty-first International Conference on Machine Learning (2024) [17] Kairouz, P., McMahan, H.B., Avent, B.,Bellet, A., Bennis, M., Bhagoji, A.N., Bonawitz, K., Charles, Z., Cormode, G., Cummings, R., et al.: Advances and open problems in federated learning. Foundations and trends $^\\mathrm{\\textregistered}$ in machine learning 14(1-2), 1-210 (2021) [18] Karimireddy, S.P., Kale, S., Mohri, M., Reddi, S., Stich, S., Suresh, A.T.: SCAFFOLD: Stochastic controlled averaging for federated learning. In: International conference on machine learning. Pp. 5132-5143. PMLR (2020) [19] Khaled, A., Mishchenko, K., Richtarik, P: Tighter theory for local SGD on identical and heterogeneous data In: International Conference on Artificial Intelligence and Statistics. p.   \n4519-4529. PMLR (2020) [20] Kim, H., Feamster, N.: Improving network management with software defined networking. IEEE Communications Magazine 51(2), 114-119 (2013) [21] Koloskova, A., Loizou, N., Boreiri, S., Jaggi, M, Stich, S.: A unifed theory of decentralized SGD with changing topology and local updates. In: International Conference on Machine Learning Pp. 5381-5393. PMLR (2020) [22] Konecny, J., McMahan, H.B., Yu, F.X., Richtarik, P, Suresh, A.T., Bacon, D.: Federated learning: Strategies for improving communication efficiency. arXiv preprint arXiv:1610.05492   \n8 (2016) [23] Krizhevsky, A., Hinton, G, et al.: Learning multiple layers of features from tiny images. Toronto, ON, Canada (2009) [24] Lan, G., Liu, X.Y., Zhang, Y., Wang, X.: Communication-effcient federated learning for resource-constrained edge devices. IEEE Transactions on Machine Learning in Communications and Networking (2023) [25] Li, H., Ota, K., Dong, M.: Learning iot in edge: Deep learning for the internet of things with edge computing. IEEE network 32(1), 96-101 (2018) [26] Li, T., Sahu, A.K., Talwalkar, A., Smith, V.: Federated learning: Challenges, methods, and future directions. IEEE Signal Processing Magazine 37(3), 50-60 (2020) [27] Li, T., Sahu, A.K., Zaheer, M., Sanjabi, M., Talwalkar, A., Smith, V.: Federated optimization in heterogeneous networks. In: Proceedings of Machine learning and systems. vol. 2, pp. 429-450 (2020)   \n[28] Li, X., Huang, K., Yang, W., Wang, S., Zhang, Z.: On the convergence of FedAvg on Non-IID data. In: International Conference on Learning Representations (2019)   \n[29] Liang, X., Shen, S., Liu, J., Pan, Z., Chen, E., Cheng, Y: Variance reduced local SGD with lower communication complexity. arXiv preprint arXiv:1912.12844 (2019)   \n[30] Liu, L., Zhang, J., Song, S., Letaief, K.B.: Client-edge-cloud hierarchical federated learning. In: IEEE International Conference on Communications. pp. 1-6 (2020)   \n[31] Liu, L., Zhang, J., Song, S., Letaief, K.B.: Hierarchical federated learning with quantization: Convergence analysis and system design. IEEE Transactions on Wireless Communications 22(1), 2-18 (2022)   \n[32] Liu, Y., Lin, T, Koloskova, A., Stich, S.U.: Decentralized gradient racking with local sts. Optimization Methods and Software pp. 1-28 (2024)   \n[33] Ma, J., Long, G., Zhou, T., Jiang, J., Zhang, C.: On the convergence of clustered federated learning. arXiv preprint arXiv:2202.06187 (2022)   \n[34] Malinovsky, G, Yi, K., Richtarik, P: Variance reduced ProxSkip: Algorithm, theory and application to federated learning. In: Advances in Neural Information Processing Systems. vol. 35, Pp. 15176-15189 (2022)   \n[35] McMahan, B., Moore, E., Ramage, D., Hampson, S., y Arcas, B.A.: Communication-efficient learning of deep networks from decentralized data. In: Artificial intelligence and statistics. pp. 1273-1282. PMLR (2017)   \n[36] Mishchenko, K., Malinovsky, G., Stich, S., Richtarik, P: ProxSkip: Yes! local gradient steps provably lead to communication acceleration! finally! In: International Conference on Machine Learning. Pp. 15750-15769. PMLR (2022)   \n[37] Nedic, A., Olshevsky, A., Shi, W.: Achieving geometric convergence for distributed optimization over time-varying graphs. SIAM Journal on Optimization 27(4), 2597-2633 (2017)   \n[38] Nguyen, V.D., Chatzinotas, S., Ottersten, B., Duong, T.Q.: Fedfog: Network-aware optimization of federated learning over wireless fog-cloud systems. IEEE Transactions on Wireless Communications 21(10), 8581-8599 (2022)   \n[39] Pathak, R., Wainwright, MJ: Fedsplit: An algorithmic framework for fast federated optimization. In: Advances in Neural Information Processing Systems. vol. 33, pp. 7057-7066 (2020)   \n[40]  Scutari, G., Sun, Y.: Distributed nonconvex constrained optimization over time-varying digraphs. Mathematical Programming 176, 497-544 (2019)   \n[41]  Stich, S.U.: Local SGD converges fast and communicates little. In: International Conference on Learning Representations (2019)   \n[42] Tian, Y., Sun, Y., Scutari, G.: Achieving linear convergence in distributed asynchronous multiagent optimization. IEEE Transactions on Automatic Control 65(12), 5264-5279 (2020)   \n[43]  Wang, H., Chi, Y.: Communication-efficient federated optimization over semi-decentralized networks. In: ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). pPp. 13241-13245. IEEE (2024)   \n[44] Wang, J., Charles, Z., Xu, Z., Joshi, G., McMahan, H.B., y Arcas, B.A., Al-Shedivat, M., Andrew, G., Avestimehr, S., Daly, K., Data, D., Diggavi, S., Eichner, H., Gadhikar, A., Garrett, Z., Girgis, A.M., Hanzely, F, Hard, A., He, C., Horvath, S., Huo, Z., Ingerman, A., Jaggi, M, Javidi, T., Kairouz, P., Kale, S., Karimireddy, S.P., Konecny, J., Koyejo, S., Li, T., Liu, L., Mohri, M., Qi, H., Reddi, S.J., Richtarik, P, Singhal, K., Smith, V, Soltanolkotabi, M, Song, W., Suresh, A.T., Stich, S.U., Talwalkar, A., Wang, H, Woodworth, B., Wu, S., Yu, FX., Yuan, H., Zaheer, M.,Zhang, M., Zhang, T., Zheng, C., Zhu, C., Zhu, W.: A field guide to federated optimization (2021)   \n[45]  Wang, J., Liu, Q., Liang, H., Joshi, G., Poor, H.V.: Tackling the objective inconsistency problem in heterogeneous federated optimization. In: Advances in Neural Information Processing Systems. vol. 33, pp. 7611-7623 (2020)   \n[46] Wang, J., Liu, Q., Liang, H., Joshi, G., Poor, H.V: A novel framework for the analysis and design of heterogeneous federated learning IEEE Transactions on Signal Processing 69, 5234-5249 (2021)   \n[47] Wang, J., Wang, S., Chen, R.R., Ji, M: Demystifying why local aggregation helps: Convergence analysis of hierarchical SGD. In: Proceedings of the AAAI Conference on Artificial Intelligence. vol. 36, pp. 8548-8556 (2022)   \n[48]  Wang, S., Ji, M.: A lightweight method for tackling unknown participation statistics in federated averaging. In: International Conference on Learning Representations (2024)   \n[49] Wang, Z., Xu, H., Liu, J., Huang, H., Qiao, C., Zhao, Y: Resource-efficient federated learning with hierarchical aggregation in edge computing. In: IEEE Conference on Computer Communications. pp. 1-10 (2021)   \n[50] Wu, F., Guo, S., Qu, Z., He, S., Liu, Z., Gao, J.: Anchor sampling for federated learning with partial client participation. In: International Conference on Machine Learning. Pp. 37379-37416. PMLR (2023)   \n[51] Wu, F., Guo, S., Wang, H., Zhang, H., Qu, Z., Zhang, J., Liu, Z.: From deterioration to acceleration: A calibration approach to rehabilitating step asynchronism in federated optimization. IEEE Transactions on Parallel and Distributed Systems 34(5), 1548-1559 (2023)   \n[52] Wu, F, Li, Z., Li, Y., Ding, B., Gao, J.: Fedbiot: Llm local fine-tuning in federated learning without full model. In: Proceedings of the 3Oth ACM SIGKDD Conference on Knowledge Discovery and Data Mining. Pp. 3345-3355 (2024)   \n[53] Xiao, H., Rasul, K., Vollgraf, R.: Fashion-MNIST: a novel image dataset for benchmarking machine learning algorithms. arXiv preprint arXiv:1708.07747 (2017)   \n[54] Xu, J., Tian, Y., Sun, Y, Scutari, G.: Distributed algorithms for composite optimization: Unified framework and convergence analysis. IEEE Transactions on Signal Processing 69, 3555-3570 (2021)   \n[55]  Yang, H.: H-FL: A hierarchical communication-efficient and privacy-protected architecture for federated learning. arXiv preprint arXiv:2106.00275 (2021)   \n[56] Yu, H., Yang, S., Zhu, S.: Parallel restarted SGD with faster convergence and less communication: Demystifying why model averaging works for deep learning. In: Proceedings of the AAAI conference on artificial intelligence. vol. 33, pp. 5693-5700 (2019)   \n[57] Zhang, X., Hong, M., Dhople, S., Yin, W., Liu, Y.: FedPD: A federated learning framework with adaptivity to Non-IID data. IEEE Transactions on Signal Processing 69, 6055-6070 (2021)   \n[58] Zinkevich, M., Weimer, M., Li, L., Smola, A.: Parallelized stochastic gradient descent. In: Advances in Neural Information Processing Systems. vol. 23 (2010) ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A Connection Between HFL and Cluster FL 15   \nB  Additional Experiments on CIFAR-10 15   \nC  Experiments on Distribution Shift Datasets 16   \nD  Additional Experiments on CINIC-10 and Shakespear Datasets 17   \nE  Extension to the HFL System with Arbitrary Number of Levels 17 ", "page_idx": 13}, {"type": "text", "text": "F Proof of the Main Results 19 ", "page_idx": 13}, {"type": "text", "text": "F.1 Preliminaries 19   \nF.2 Proofs of Theorem 4.1 and Corollary 4.1 19   \nF.2.1 Proof of Theorem 4.1 . 21   \nF.2.2 Proof of Corollary 4.1 22   \nF.3 Proofs of Lemmas F.2.1-F.2.7 . 22   \nF.3.1 Proof of Lemma F.2.1 22   \nF.3.2 Proofs of Lemmas F.2.2 and F.2.6 24   \nF.3.3 Proof of Lemma F.2.3 25   \nF.3.4 Proof of Lemma F.2.4 28   \nF.3.5 Proof of Lemma F.2.5 29   \nF.3.6 Proof of Lemma F.2.7 32 ", "page_idx": 13}, {"type": "text", "text": "G Recovering SCAFFOLD's Results 34 ", "page_idx": 13}, {"type": "text", "text": "A Connection Between HFL and Cluster FL ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Our work focuses on HFL, employing a multi-layered structure consisting of local nodes, local aggregators, and a central server. Both clustered FL and HFL aim to improve FL learning efficiency by leveraging structured client groupings. The difference between them lies in the grouping criteria. HFL focuses on collaborative training over a given network topology, where clients are generally grouped based on their geographical location or network connection status, and aims to build a single global model under this setting. CFL groups clients to optimize model training, with different global models constructed depending on the group. [33] demonstrates how dynamic clustering based on data distributions can enhance model performance. [3] explores alleviating negative transfer from collaboration by clustering clients into non-overlapping coalitions based on their distribution distances and data quantities. ", "page_idx": 14}, {"type": "text", "text": "B Additional Experiments on CIFAR-10 ", "text_level": 1, "page_idx": 14}, {"type": "image", "img_path": "aCAb1qNXI0/tmp/fc375d637e2954b2a07d253d7b84546470a97c28f17d8961671870d6f067138e.jpg", "img_caption": ["Figure 5: Comparison of testing accuracy versus global communication round across different system parameters under both group non-i.i.d. and client non-i.i.d. setup. $E$ and $H$ are set to 30 and 20, respectively. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "The impact of system parameters. Fig. 5 shows how the performance of MTGC changes with different numbers of groups and clients in each group. From this figure, we observe that as the number of clients in each group, i.e., $n_{j}$ increases, client correction becomes more important. On the other hand, as the number of groups increases, the algorithm with group correction performs better than that with client correction. ", "page_idx": 14}, {"type": "image", "img_path": "aCAb1qNXI0/tmp/1a6f7b2a61cebef3f1e577b189f7663fd68456eea389fd9592f4443d9410409d.jpg", "img_caption": ["Figure 6: Performance of MTGC under a different number of local iterations, i.e., $H$ , and a different number of group aggregations, i.e., $E$ . The number of groups and clients in each group is set to $N=10$ and $n_{j}=10$ ,respectively. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "The impact of local iteration and group aggregation: Fig. 6 depicts the performance of MTGC under a different number of local iterations, i.e., $H$ , and a different number of group aggregations, i.e., $E$ . It's clear that MTGC achieves speedup in the number of local iterations and group aggregations. ", "page_idx": 14}, {"type": "text", "text": "Communication cost comparison. Compared to HFedAvg, MTGC requires initializing the correction variables at the start of each global round, which adds additionaly communication overhead. Specifically, for every $E$ steps of group aggregation, MTGC incurs an additional communication cost equivalent to one transmission of the model parameters. In other words, the per-aggregation communicationcomplexity ofMTGCis $\\textstyle{\\frac{E+1}{E_{.}}}$ times that of HFedAvg.To show this impact, we have added experiments comparing the communication cost and testing accuracy at the client side. This experiment was conducted on CIFAR-10 dataset with $E=30$ and $H=20$ under both client and group non-i.i.d. setup. The model and other parameters are the same as in the original manuscript. The results are shown in Fig. 7a. The results demonstrate that MTGC achieves higher testing accuracy for a given communication cost, highlighting the efficiency and effectiveness of our approach. ", "page_idx": 14}, {"type": "image", "img_path": "aCAb1qNXI0/tmp/4ffa19218935364d2e19e2445a87f20d9f994a3fa6964df0f239befe67908190.jpg", "img_caption": ["Figure 7: Comparison of communication cost (a); and running time for attaining $75\\%$ testing accuracy and finishing 100 global rounds on CIFAR-10 (b) "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "Running time comparison. We compared the computation time of our MTGC algorithm with the baselines. Using NVIDIA A100 GPUs with $40\\ \\mathrm{GB}$ memory, we conducted experiments on the CIFAR-10 dataset with $E=30$ and $H=20$ under both client and group non-i.i.d. setup. The model and other parameters are the same as in the original manuscript. We report the required time for attaining a preset accuracy of $75\\%$ and for running 100 global rounds in Fig. 7b of the attached pdf. The speedup in the convergence makes up for the introduced computation cost per iteration due to the extra operation induced by the correction variables. Actually, the computation cost incurred by the correction variable is relatively small compared to computing gradients in a neural network using backpropagation. ", "page_idx": 15}, {"type": "text", "text": "C  Experiments on Distribution Shift Datasets ", "text_level": 1, "page_idx": 15}, {"type": "image", "img_path": "aCAb1qNXI0/tmp/9701b5e040f25a24a257cada1aecdf0c2117fbf6103285789ca6dcd926def9c5.jpg", "img_caption": ["Figure 8: Performance comparisons on Fashion-MNIST under label shift and Fashion-MNIST under feature shift "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "To further show the robustness, we studied the performance of MTGC under another two different non-i.i.d. scenarios: label shift and feature shift, as referenced in [3, 33]. These experiments were performed using the Fashion-MNIST dataset. ", "page_idx": 15}, {"type": "text", "text": "For label shift [3, 33], we randomly assign 3 classes out of 10 classes to each group with a relatively balanced number of instances per class, and then assign 2 classes to each client. As discussed in [3], label shift adds more heterogeneity to this system. According to the results shown in Fig. 8a, it is clear that the proposed algorithm is more robust against data heterogeneity. Specifically, there is less oscillation in MTGC compared with HFedAvg and the attained accuracy of MTGC in the given communication round is higher than all baselines. ", "page_idx": 15}, {"type": "image", "img_path": "aCAb1qNXI0/tmp/6f86a566c3612d40182a706b839acf23c6e8954ca5df51127cc60e3ae60dc5e8.jpg", "img_caption": ["(a) Performance comparison on the Shakespeare (b) Performance comparison under CINIC-10 datasetbasedonLSTMmodel dataset ", "Figure 9: Performance evaluation on Shakespeare and CINIC-10 "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "For feature shift [3], we first partition data following the group non-i.i.d. & client non-i.i.d. case as in our original manuscript, and then let clients at different groups rotate images for different angles. Concretely, for the clients at the $i$ -th group, the angle is $-50+10\\times i$ . Note that this rotation is only applied to the training set. The feature shift increases the diversity between the training set and the testing set, which thus adds difficulty to this classification task. In Fig. 8b, we see that MTGC attains the best performance among these baselines. ", "page_idx": 16}, {"type": "text", "text": "D  Additional Experiments on CINIC-10 and Shakespear Datasets ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We conducted additional experiments on the larger Shakespeare and CINIC-10 datasets. For the Shakespeare dataset, we randomly pick 100 characters (people) in Shakespeare's plays. We let each client have 1,500 samples, where each sample is a sequence of 80 characters (words). Considering that there are 100 clients in the system, there are 150,000 train samples in total. This means that the number of samples is 3 times that of CIFAR-10 (or CIFAR-100), which has 50,000 train samples. The performance comparison is presented in Fig. 9a, where we use the LSTM model, the same as [1], and set the learning rate 0.5, $H=75$ and $E=30$ . It is seen that MTGC consistently outperforms the baseline methods in larger datasets. ", "page_idx": 16}, {"type": "text", "text": "The CINIC-10 dataset contains 90,000 training images, 90,000 validation images, and 90,000 test images, significantly larger than CIFAR-10 and CIFAR-100 with 60,000 images. It includes images from both CIFAR-10 and ImageNet, enhancing diversity. We believe that the larger size and diversity of CINIC-10 further confirm the validity of our experiments. The model and hyperparameters used for the CINIC-10 dataset are the same as those of the CIFAR-10 task shown in the original manuscript. As illustrated in Fig. 9b, MTGC maintains its superior performance on the CINIC-10 dataset, consistent with its performance on other tasks. ", "page_idx": 16}, {"type": "text", "text": "E Extension to the HFL System with Arbitrary Number of Levels ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We extend MTGC for the HFL system with $M$ levels in this subsection. For presentation ease, we adopt different notations than those used in the main text. Specifically, we denote the number of total iterations at clients as $r$ . The aggregation periods for level $m$ are denoted as $P_{m}$ . This means that the $m$ -th level aggregator aggregates the model from the clients within its coverage after every $P_{m}$ local iterations. The global server is treated as the frst level aggregator. Note that $P_{m}>P_{m+1}$ and $P_{m+1}\\mid P_{m},\\forall m=\\Bar{1},\\ldots,M-1$ . We denote the model maintained at the nodes connected to the $m$ -th level aggregator $(k_{1},k_{2},\\ldots,k_{m-1})$ $\\pmb{x}_{k_{1},\\hdots,k_{m}}^{r}$ , where $k_{m}\\in\\{1,\\ldots,N_{m}\\}$ The gradient correction term between nodes (k1, k, .., km--1) and (k1, k,.., km) is denoted as Vka,k2,,km \\* The overall procedures are summarized in Algorithm 2. ", "page_idx": 16}, {"type": "text", "text": "Algorithm 2: MTGC for the HFL System with Arbitrary Number of Levels ", "page_idx": 17}, {"type": "image", "img_path": "aCAb1qNXI0/tmp/1569d43399df66479ae270d1013393175702f5cbe586a69f0fea49ad30e3aa4f.jpg", "img_caption": [], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "aCAb1qNXI0/tmp/81efce93e5934faa812a6335a4ef05075e5d67fc09bc935d88821c34c526fc86.jpg", "img_caption": ["Figure 10: HFL system with 3-level topology "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "aCAb1qNXI0/tmp/71b34eaf354ebc3e84952c37f5c00f3d7738c0a70c464025a7902e1fe87fb876.jpg", "img_caption": ["Figure 1l: Performance of MTGC for three-level HFL with data non-i.i.d. across each level. Parameters aresetto $N_{1}=4$ $N_{2}=5$ \uff0c $N_{3}=5$ $P_{1}=500$ $P_{2}=100$ \uff0c $P_{3}=10$ "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "We numerically validate the performance of MTGC by conducting experiments in the three-level case as shown in Fig. 10. The results are shown in Fig. 11. The total number of clients is set tobe100while $N_{1}\\,=\\,4$ $N_{2}\\,=\\,5$ $N_{3}\\,=\\,5$ .Additionally, the aggregation periods are set to be $P_{1}=500,\\;P_{2}=100,\\;P_{3}=10,$ The data is non-i.i.d. distributed across each level. ", "page_idx": 17}, {"type": "text", "text": "FProof of the Main Results ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "F.1 Preliminaries ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Before proceeding to the proof of the main theorem. We introduce some basic inequalities in this subsection that will be frequently used in our proof. ", "page_idx": 18}, {"type": "text", "text": "Lemma E1.1. For any set of $K$ vectors $\\begin{array}{r l r}{\\{p_{k}\\}_{k=1}^{K},}&{\\left\\|\\frac{1}{K}\\sum_{k=1}^{K}p_{k}\\right\\|^{2}}&{\\leq}&{\\frac{1}{K}\\sum_{k=1}^{K}\\|p_{k}\\|^{2},}\\end{array}$ $\\begin{array}{r}{\\left\\|\\sum_{k=1}^{K}p_{k}\\right\\|^{2}\\leq K\\sum_{k=1}^{K}\\|p_{k}\\|^{2},}\\end{array}$ and ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{1}{K}\\sum_{k=1}^{K}\\left\\|p_{k}-\\frac{1}{K}\\sum_{k=1}^{K}p_{k}\\right\\|^{2}=\\frac{1}{K}\\sum_{k=1}^{K}\\left\\|p_{k}\\right\\|^{2}-\\left\\|\\frac{1}{K}\\sum_{k=1}^{K}p_{k}\\right\\|^{2}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Lemma F.1.2. For any two vectors $\\pmb{p},\\pmb{q}\\in\\mathbb{R}^{d}$ $\\begin{array}{r}{\\|p+q\\|^{2}\\leq\\left(1+\\alpha\\right)\\|p\\|^{2}+\\left(1+\\frac{1}{\\alpha}\\right)\\|q\\|^{2}.}\\end{array}$ ", "page_idx": 18}, {"type": "text", "text": "Lemma F.1.3. Suppose a sequence of random vectors $\\left\\{p_{k}\\right\\}_{k=1}^{K}{\\mathit{s a t i s f y}}\\,\\mathbb{E}[p_{k}]=\\mathbf{0},\\forall k.$ Then, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left\\|\\frac{1}{K}\\sum_{k=1}^{K}\\pmb{p}_{k}\\right\\|^{2}=\\frac{1}{K^{2}}\\sum_{k=1}^{K}\\mathbb{E}\\left\\|\\pmb{p}_{k}\\right\\|^{2}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Lemma E1.4. [46, Lemma 2] Suppose $a$ sequence of random vecors $\\{p_{k}\\}_{k=1}^{K}$ satisy $\\mathbb{E}[\\pmb{p}_{k}\\,|\\,\\pmb{p}_{k-1},\\pmb{p}_{k-2},\\dots,\\pmb{p}_{1}]\\,\\!=\\!\\mathbf{0},\\forall k$ .Then, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\Vert\\sum_{k=1}^{K}p_{k}\\right\\Vert^{2}\\right]=\\sum_{k=1}^{K}\\mathbb{E}\\left[\\left\\Vert p_{k}\\right\\Vert^{2}\\right].\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Lemma F.1.5. [21, Lemma 17] For any $a_{0}\\geq0,b\\geq0,c\\geq0,d>0,$ there exist a constant $\\begin{array}{r}{\\eta\\leq\\frac{1}{d}}\\end{array}$ suchthat ", "page_idx": 18}, {"type": "equation", "text": "$$\n{\\frac{a_{0}}{T\\eta}}+b\\eta+c\\eta^{2}\\leq2\\left({\\frac{a_{0}b}{T}}\\right)^{\\frac{1}{2}}+2c^{\\frac{1}{3}}\\left({\\frac{a_{0}}{T}}\\right)^{\\frac{2}{3}}+{\\frac{d a_{0}}{T}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "F.2  Proofs of Theorem 4.1 and Corollary 4.1 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "For the convenience of presentation, we introduce the following notations ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\displaystyle\\sum_{c=0}^{E-1}\\displaystyle\\frac{1}{N_{f}\\sum_{i=1}^{N}Z_{j}^{t,c}}=\\displaystyle\\sum_{c=0}^{E-1}\\displaystyle\\frac{1}{N_{f}\\sum_{i=1}^{N}\\overline{{n}}_{j}}\\displaystyle\\sum_{i\\in\\mathcal{C}_{j}}\\mathbb{E}\\left\\|z_{i}^{t,c}+\\nabla F_{i}\\left(\\overline{{x}}_{j}^{t,c}\\right)-\\nabla f_{j}\\left(\\overline{{x}}_{j}^{t,c}\\right)\\right\\|^{2}}}\\\\ {{\\displaystyle\\sum_{c=0}^{E-1}\\displaystyle\\frac{1}{N_{f}\\sum_{i=1}^{N}Y_{j}^{t,c}}=\\displaystyle\\sum_{c=0}^{E-1}\\displaystyle\\frac{1}{N_{f}\\sum_{i=1}^{N}\\mathbb{E}}\\left\\|y_{j}^{t}+\\nabla f_{j}\\left(\\widehat{x}^{t,c}\\right)-\\nabla f\\left(\\widehat{x}^{t,c}\\right)\\right\\|^{2}}}\\\\ {{\\displaystyle D_{t}=\\sum_{c=0}^{E-1}\\displaystyle\\frac{1}{N_{f}\\sum_{i=1}^{N}\\mathbb{E}}\\left\\|\\left\\|\\widehat{x}^{t,c}-\\widehat{x}_{j}^{t,c}\\right\\|^{2}}}\\\\ {{\\displaystyle Q_{t}=\\sum_{c=0}^{E-1}\\displaystyle\\frac{1}{N H}\\displaystyle\\sum_{j=1}^{N}\\displaystyle\\frac{1}{n_{i}!}\\sum_{i\\in\\mathcal{C}_{j}}\\displaystyle\\sum_{h=0}^{H-1}\\left\\|\\left\\|x_{j}^{t,c}-x_{i,h}^{t,c}\\right\\|^{2}}}\\\\ {{\\displaystyle\\sum_{c=0}^{E-1}\\displaystyle\\frac{1}{N_{f}\\sum_{i=1}^{N}\\Theta_{j}^{t,c}}=\\displaystyle\\sum_{c=0}^{E-1}\\displaystyle\\frac{1}{N_{i}!}\\displaystyle\\sum_{i=1}^{N}\\left\\|\\left\\|x_{j}^{t,c+1}-\\widehat{x}_{j}^{t,c}\\right\\|^{2},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $Z_{j}^{t,e}$ and $Y_{j}^{t,e}$ characterize the biases between client-group corection term $z_{i}^{t,e}$ t;e and $\\nabla F_{i}\\left({\\bar{\\pmb{x}}}_{j}^{t,e}\\right){-}\\nabla f_{j}\\left({\\bar{\\pmb{x}}}_{j}^{t,e}\\right)$ and between group-global correction term $\\boldsymbol{y}_{j}^{t}$ and $\\nabla f_{j}\\left({\\hat{\\pmb{x}}}^{t,e}\\right){-}\\nabla f\\left({\\hat{\\pmb{x}}}^{t,e}\\right)$ respectively, $D_{t}$ and $Q_{t}$ denote the group model drift and client model drift, respectively, and $\\Theta_{j}^{t,e}$ represents model progress for group $j$ ", "page_idx": 18}, {"type": "text", "text": "To prove the convergence of MTGC, we start with characterizing the evolution of the global loss, i.e., $f({\\boldsymbol{x}})$ through the following lemma. ", "page_idx": 18}, {"type": "text", "text": "LemmaF2.1. Suppose that Assumptions 1 and 2 hold and $\\begin{array}{r}{\\gamma\\leq\\frac{1}{2H L}}\\end{array}$ , then the iterates generated by Algorithm 1 satisfy ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{E}f(\\bar{\\pmb{x}}^{t+1})\\leq\\mathbb{E}f(\\bar{\\pmb{x}}^{t})-\\frac{\\gamma H}{2}\\sum_{e=0}^{E-1}\\mathbb{E}\\big\\lVert\\nabla f(\\hat{\\pmb{x}}^{t,e})\\big\\rVert^{2}\\!+\\!\\gamma L^{2}H\\left(Q_{t}\\!+\\!D_{t}\\right)\\!+\\!\\gamma^{2}L E H\\frac{1}{N^{2}}\\!\\sum_{j=1}^{N}\\!\\frac{1}{n_{j}}\\sigma^{2}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Lemma (F.2.1) implies that we need further to study the evolution of $Q_{t}$ and $D_{t}$ . In particular, we establish upper bounds for $Q_{t}$ and $D_{t}$ in Lemmas F.2.2 and F.2.3, respectively. ", "page_idx": 19}, {"type": "text", "text": "Lemma F2.2. Suppos that Assumptions $^{\\,I}$ and 2 hold and $\\begin{array}{r}{\\gamma\\le\\frac{1}{8H L}}\\end{array}$ then the client model drift $Q_{t}$ \uff0c defined in (11), can be bounded as ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{Q_{t}\\leq24\\gamma^{2}H^{2}L^{2}D_{t}+12\\gamma^{2}H^{2}\\sum_{e=0}^{E-1}\\frac{1}{N}\\sum_{j=1}^{N}Z_{t}^{t,e}+12\\gamma^{2}H^{2}\\sum_{e=0}^{E-1}\\frac{1}{N}\\sum_{j=1}^{N}Y_{j}^{t,e}}}\\\\ {{\\displaystyle{+\\,24\\gamma^{2}H^{2}\\sum_{e=0}^{E-1}\\mathbb{E}\\left\\|\\nabla f\\left(\\hat{\\pmb{x}}^{t,e}\\right)\\right\\|^{2}+3E H\\gamma^{2}\\sigma^{2}}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Lemma F2. Suppose that Assumptions $^{\\,l}$ and 2 hold and $\\gamma\\leq{\\frac{1}{10E H L}}$ . Then the goup model dift $D_{t}$ , defined in (11), can be bounded as ", "page_idx": 19}, {"type": "equation", "text": "$$\nD_{t}\\leq24\\gamma^{2}E^{2}H^{2}L^{2}Q_{t}+12\\gamma^{2}E^{2}H^{2}\\sum_{e=0}^{E-1}\\frac{1}{N}\\sum_{j=1}^{N}Y_{j}^{t,e}+3\\gamma^{2}E^{3}H\\frac{N-1}{N^{2}}\\sum_{j=1}^{N}\\frac{1}{n_{j}}\\sigma^{2}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The results shown in Lemmas F.2.2 and F.2.3 suggest that $Z_{j}^{t,e}$ and $Y_{j}^{t,e}$ are crucial for understanding the dynamis ofMGC.Hence, weerive up bous f $Z_{j}^{t,e}$ and $Y_{j}^{t,e}$ , which are presented in Lemmas F.2.4 and F.2.5, respectively. ", "page_idx": 19}, {"type": "text", "text": "Lemma F.2.4. Suppose that Assumptions 1 and 2 hold. Then the bias between client-group correction term $z_{i}^{t,e}$ and $\\nabla{\\bar{F_{i}}}^{\\bar{(}}\\bar{\\pmb{x}}_{j}^{t,e})\\!-\\!\\nabla f_{j}\\left(\\bar{\\pmb{x}}_{j}^{t,e}\\right)$ ,i.e., $Z_{j}^{t,e}$ canbeboundedas ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\sum_{e=0}^{E-1}\\frac{1}{N}\\sum_{j=1}^{N}Z_{j}^{t,e}\\leq4L^{2}Q_{t}+4L^{2}\\sum_{e=0}^{E-1}\\frac{1}{N}\\sum_{j=1}^{N}\\Theta_{j}^{t,e}+2\\frac{E}{H}\\sigma^{2}+\\sigma^{2}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Lemma F.2.5. Suppose that Assumptions $^{\\,l}$ and 2 hold. Then the bias between group-global correction term $\\boldsymbol{y}_{j}^{t}$ and $\\nabla f_{j}\\left({\\hat{\\pmb{x}}}^{t,e}\\right){-}\\nabla f\\left({\\hat{\\pmb{x}}}^{t,e}\\right)$ i.e, $Y_{j}^{t,e}$ defined in (11), $t\\geq1$ , can be bounded as ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{e=0}^{E-1}\\displaystyle\\frac{1}{N}\\displaystyle\\sum_{j=1}^{N}\\!Y_{j}^{t,e}\\!\\le\\left(8L^{2}\\!+\\!48\\gamma^{2}L^{4}E^{2}H^{2}\\right)(Q_{t-1}+D_{t-1})+48\\gamma^{2}L^{4}E^{2}H^{2}\\left(Q_{t}\\!+\\!D_{t}\\right)}&{}\\\\ {\\displaystyle+\\,48\\gamma^{2}L^{2}E^{2}H^{2}\\sum_{\\tau=0}^{E-1}\\left(\\mathbb{E}\\Vert\\nabla f(\\hat{\\pmb x}^{t-1,\\tau})\\Vert^{2}\\!+\\!\\mathbb{E}\\left\\Vert\\nabla f(\\hat{\\pmb x}^{t,\\tau})\\right\\Vert^{2}\\right)}&{}\\\\ {\\displaystyle+\\,32\\gamma^{2}L^{2}E^{2}H\\displaystyle\\frac{1}{N^{2}}\\sum_{j=1}^{N}\\displaystyle\\frac{1}{n_{j}}\\sigma^{2}+\\frac{2}{H}\\displaystyle\\frac{1}{N}\\sum_{j=1}^{N}\\displaystyle\\frac{1}{n_{j}}\\sigma^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Additionally, when $t=0,Y_{j}^{0,e}$ can be bounded as ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\sum_{e=0}^{E-1}\\frac{1}{N}\\sum_{j=1}^{N}Y_{j}^{0,e}\\leq\\!E\\frac{1}{N}\\sum_{j=1}^{N}\\frac{1}{n_{j}}\\sigma^{2}+L^{2}\\sum_{e=0}^{E-1}\\mathbb{E}\\left\\|\\hat{\\pmb{x}}^{0,e}-\\bar{\\pmb{x}}^{0}\\right\\|^{2}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "In addition, the upper bound of $\\Theta_{j}^{t,e}$ is presented in the following lemma. ", "page_idx": 19}, {"type": "text", "text": "Lemma F.2.6. Suppose that Assumptions $^{\\,l}$ and 2 hold. Then the group model progress at the $(t,e)$ $^{t h}$ round, i.e, $\\Theta_{j}^{t,e}$ , defined in (11), can be bounded as ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{e=0}^{E-1}\\frac{1}{N}\\sum_{j=1}^{N}\\Theta_{j}^{t,e}\\le8\\gamma^{2}H^{2}L^{2}Q_{t}+8\\gamma^{2}H^{2}L^{2}D_{t}+8\\gamma^{2}H^{2}\\sum_{e=0}^{E-1}\\frac{1}{N}\\sum_{j=1}^{N}Y_{j}^{t,e}}}\\\\ &{}&{\\quad+\\;8\\gamma^{2}H^{2}\\sum_{e=0}^{E-1}\\mathbb{E}\\big\\lVert\\nabla f\\left(\\hat{\\pmb x}^{t,e}\\right)\\big\\rVert^{2}+2\\gamma^{2}E H\\frac{1}{N}\\sum_{j=1}^{N}\\frac{1}{n_{j}}\\sigma^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Recalling Lemma (F.2.1), we can see that what we actually need is the evolution of $Q_{t}+D_{t}$ .With Lemmas F.2.2-F.2.6, we can characterize this evolution which is formalized in Lemma F.2.7. ", "page_idx": 20}, {"type": "text", "text": "LemmaE2.7. Suppose that Assumptions $^{\\,l}$ and 2 hold and $\\begin{array}{r}{\\gamma\\,\\leq\\,\\frac{1}{33E H L}}\\end{array}$ themodel deviation $\\Gamma_{t}=Q_{t}+D_{t}$ satisfies ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Gamma_{t}\\leq\\displaystyle\\frac{1}{2}\\Gamma_{t-1}+\\left(1152\\gamma^{4}H^{4}L^{2}+72\\gamma^{2}E^{2}H^{2}\\right)\\displaystyle\\sum_{\\tau=0}^{E-1}\\!\\left(\\mathbb{E}\\big\\|\\nabla f(\\hat{\\pmb{x}}^{t-1,\\tau})\\big\\|^{2}\\!+\\!\\mathbb{E}\\left\\|\\nabla f(\\hat{\\pmb{x}}^{t,\\tau})\\right\\|^{2}\\right)+294\\gamma^{2}E^{3}H\\sigma^{2},}\\\\ &{\\Gamma_{0}\\leq\\left(648\\gamma^{4}H^{4}L^{2}\\!+\\!42\\gamma^{2}E^{2}H^{2}\\right)\\displaystyle\\sum_{e=0}^{E-1}\\!\\mathbb{E}\\big\\|\\nabla f\\left(\\hat{\\pmb{x}}^{0,e}\\right)\\big\\|^{2}\\!+\\!146\\gamma^{2}E^{3}H^{2}\\sigma^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "The proofs of Lemmas F.2.1-F.2.7 are provided in Appendix F.3. With these lemmas, we are ready to prove Theorem 4.1. ", "page_idx": 20}, {"type": "text", "text": "F.2.1 Proof of Theorem 4.1 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Starting with Lemma F.2.1, i.e., ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{E}f(\\bar{\\pmb{x}}^{t+1})-\\gamma L^{2}H\\Gamma_{t}\\leq\\mathbb{E}f(\\bar{\\pmb{x}}^{t})-\\frac{\\gamma H}{2}\\sum_{e=0}^{E-1}\\mathbb{E}\\big\\|\\nabla f(\\hat{\\pmb{x}}^{t,e})\\big\\|^{2}+\\gamma^{2}L E H\\frac{1}{N^{2}}\\sum_{j=1}^{N}\\frac{1}{n_{j}}\\sigma^{2},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\Gamma_{t}=Q_{t}+D_{t}$ .Adding $2\\gamma L^{2}H\\Gamma_{t}$ on both sides of the above inequality and utilizing Lemma F.2.7, we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{\\xi}f(\\bar{x}^{t+1})+\\gamma L^{2}H\\Gamma_{t}\\leq\\mathbb{E}f(\\bar{x}^{t})+\\gamma L^{2}H\\Gamma_{t-1}-\\displaystyle\\frac{\\gamma H}{2}\\displaystyle\\sum_{c=0}^{E-1}\\mathbb{E}\\big\\|\\nabla f(\\hat{x}^{t,e})\\big\\|^{2}+\\gamma^{2}L E H\\displaystyle\\frac{1}{N^{2}}\\displaystyle\\sum_{j=1}^{N}\\displaystyle\\frac{1}{n_{j}}\\sigma^{2}}\\\\ &{+2\\gamma L^{2}H\\times294\\gamma^{2}E^{3}H\\sigma^{2}+\\big(2304\\gamma^{5}H^{5}L^{4}+144\\gamma^{3}E^{2}H^{3}L^{2}\\big)\\displaystyle\\sum_{\\tau=0}^{E-1}\\Bigl(\\mathbb{E}\\big\\|\\nabla f(\\hat{x}^{t-1,\\tau})\\big\\|^{2}+\\mathbb{E}\\left\\|\\nabla f(\\hat{x}^{t,\\tau})\\right\\|^{2}\\Bigr).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "For notation ease, we denote $\\Phi_{t+1}\\,=\\,\\mathbb{E}f(\\bar{\\mathbf{x}}^{t+1})\\,-\\,f^{*}+\\gamma L^{2}H\\Gamma_{t}$ \uff0c $\\Phi_{t}\\,\\geq\\,0,\\,\\forall t\\,\\geq\\,0$ . As long as $\\begin{array}{r}{\\gamma\\le\\frac{E}{8H L}}\\end{array}$ in Theorem 4.1, $2304\\gamma^{5}H^{5}L^{4}\\leq36\\gamma^{3}E^{2}H^{3}L^{2}$ , we thus have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\Phi_{t+1}\\leq\\Phi_{t}-\\displaystyle\\frac{\\gamma H}{2}\\displaystyle\\sum_{e=0}^{E-1}\\mathbb{E}\\big\\|\\nabla f(\\hat{\\pmb x}^{t,e})\\big\\|^{2}\\!+\\!180\\gamma^{3}E^{2}H^{3}L^{2}\\displaystyle\\sum_{\\tau=0}^{E-1}\\!\\Big(\\mathbb{E}\\big\\|\\nabla f(\\hat{\\pmb x}^{t-1,\\tau})\\big\\|^{2}\\!+\\!\\mathbb{E}\\left\\|\\nabla f(\\hat{\\pmb x}^{t,\\tau})\\right\\|^{2}\\!\\Big)}\\\\ {+2\\gamma L^{2}H\\times294\\gamma^{2}E^{3}H\\sigma^{2}\\!+\\!\\gamma^{2}L E H\\displaystyle\\frac{1}{N^{2}}\\displaystyle\\sum_{j=1}^{N}\\!\\frac{1}{n_{j}}\\sigma^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "When $\\begin{array}{r}{\\gamma\\leq\\frac{1}{40E H L}}\\end{array}$ wehave $\\begin{array}{r}{\\frac{\\gamma H}{2}-360\\gamma^{3}E^{2}H^{3}L^{2}\\geq\\frac{\\gamma H}{4}}\\end{array}$ Telescoping the above inequalityfom to $T-1$ ,wehave ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\Phi_{T}\\leq\\!\\Phi_{1}\\!-\\!\\frac{\\gamma H}{4}\\!\\sum_{t=1}^{T-1E-1}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\Phi_{T}\\leq\\!\\!\\!\\!\\Phi_{T}\\leq\\!\\!\\!\\!\\Phi_{T}\\leq\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!1\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\! \n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "According to Lemma F.2.1, when $t=0$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{E}f(\\bar{\\pmb{x}}^{1})\\leq\\!\\mathbb{E}f(\\bar{\\pmb{x}}^{0})\\!-\\!\\frac{\\gamma H}{2}\\!\\sum_{e=0}^{E-1}\\!\\mathbb{E}\\big\\|\\nabla f(\\hat{\\pmb{x}}^{0,e})\\big\\|^{2}\\!+\\!\\gamma L^{2}H\\Gamma_{0}\\!+\\!\\gamma^{2}L E H\\frac{1}{N^{2}}\\!\\sum_{j=1}^{N}\\!\\frac{1}{n_{j}}\\sigma^{2},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "it follows that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{E}f(\\bar{x}^{1})+\\gamma L^{2}H\\Gamma_{0}\\leq\\mathbb{E}f(\\bar{x}^{0})-\\frac{\\gamma H}{2}\\sum_{e=0}^{E-1}\\!\\mathbb{E}\\big\\lVert\\nabla f(\\hat{x}^{0,e})\\big\\rVert^{2}\\!+\\!2\\gamma L^{2}H\\Gamma_{0}\\!+\\!\\gamma^{2}L E H\\frac{1}{N^{2}}\\!\\sum_{j=1}^{N}\\!\\frac{1}{n_{j}}\\sigma^{2}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Combining (23) with (21) and plugging the upper bound of $\\Gamma_{0}$ , established in Lemma F.2.7, into the inequality, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Phi_{T}\\leq\\mathbb{E}f(\\bar{x}^{0})\\!-\\!f^{*}\\!+\\!2\\gamma L^{2}H\\times146\\gamma^{2}E^{3}H^{2}\\sigma^{2}\\!+\\!2\\gamma T L^{2}H\\times292\\gamma^{2}E^{3}H\\sigma^{2}\\!+\\!\\gamma^{2}T L E H\\frac{1}{N^{2}}\\!\\!\\sum_{j=1}^{N}\\!\\frac{1}{n_{j}}\\sigma^{2}}\\\\ &{\\qquad-\\left(\\frac{\\gamma H}{2}-180\\gamma^{3}E^{2}H^{3}L^{2}-2\\gamma L^{2}H\\times\\left(648\\gamma^{4}H^{4}L^{2}\\!+\\!42\\gamma^{2}E^{2}H^{2}\\right)\\right)\\!\\!\\displaystyle\\sum_{e=0}^{E-1}\\!\\mathbb{E}\\big\\|\\nabla f(\\hat{x}^{0,e})\\big\\|^{2}}\\\\ &{\\qquad-\\frac{\\gamma H}{4}\\displaystyle\\sum_{t=1}^{T-1E-1}\\!\\mathbb{E}\\big\\|\\nabla f(\\hat{x}^{t,e})\\big\\|^{2}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "The setup of $\\gamma$ presented in Theorem 4.1 is enough to guarantee $\\begin{array}{r}{\\frac{\\gamma H}{2}-180\\gamma^{3}E^{2}H^{3}L^{2}\\!-\\!2\\gamma L^{2}H\\times}\\end{array}$ $\\begin{array}{r}{\\left(648\\gamma^{4}H^{4}L^{2}\\!+\\!42\\gamma^{2}E^{2}H^{2}\\right)\\,\\geq\\,\\frac{\\gamma H}{4}}\\end{array}$ . Therefore, combining the last terms and taking some basic algebra operation, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\frac{1}{r E}\\sum_{t=0}^{T-1E-1}\\mathbb{E}\\Vert\\nabla f(\\hat{x}^{t,e})\\Vert^{2}\\leq4\\frac{\\mathbb{E}f(\\bar{x}^{0})-f^{*}}{\\gamma T E H}+4\\gamma L\\frac{1}{N^{2}}\\sum_{j=1}^{N}\\frac{1}{n_{j}}\\sigma^{2}+\\frac{1168}{T}\\gamma^{2}L^{2}E^{2}H^{2}\\sigma^{2}+2352\\gamma^{2}L^{2}E^{2}H\\sigma^{2}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "The above bound can be further simplified to ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\frac{1}{T E}\\sum_{t=0}^{T-1E-1}\\!\\mathbb{E}\\big\\|\\nabla f(\\hat{\\pmb{x}}^{t,e})\\big\\|^{2}\\leq\\!4\\frac{\\mathbb{E}f(\\bar{\\pmb{x}}^{0})\\!-\\!f^{*}}{\\gamma T E H}\\!+\\!4\\frac{\\gamma L\\sigma^{2}}{\\tilde{N}}\\!+\\!3520\\gamma^{2}E^{2}H^{2}L^{2}\\sigma^{2}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "This completes the proof of Theorem 4.1. ", "page_idx": 21}, {"type": "text", "text": "F.2.2 Proof of Corollary 4.1 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Rewriting the bound in 24 as ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\frac{1}{T E}\\sum_{t=0}^{T-1E-1}\\!\\sum_{e=0}^{\\infty}\\!\\mathbb{E}\\big\\|\\nabla f(\\hat{x}^{t,e})\\big\\|^{2}\\leq\\!\\frac{4\\left(\\mathbb{E}f(\\bar{x}^{0})-f^{*}\\right)}{T(\\gamma E H)}\\!+\\!\\frac{4L\\sigma^{2}}{\\tilde{N}E H}(\\gamma E H)\\!+\\!3520L^{2}\\sigma^{2}(\\gamma E H)^{2},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and recalling Lemma F.1.5, one can claim that there exists a learning rate $\\begin{array}{r}{(\\gamma E H)\\leq\\frac{1}{d}}\\end{array}$ such that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{T E}\\displaystyle\\sum_{t=0}^{T-1E-1}\\mathbb{E}\\big\\|\\nabla f(\\hat{x}^{t,e})\\big\\|^{2}\\leq8\\sqrt{\\frac{(\\mathbb{E}f(\\bar{x}^{0})-f^{*})L\\sigma^{2}}{\\tilde{N}T E H}}+96\\left(\\frac{(\\mathbb{E}f(\\bar{x}^{0})-f^{*})L\\sigma}{T}\\right)^{\\frac{2}{3}}+\\frac{d(\\mathbb{E}f(\\bar{x}^{0})-f^{*})}{T}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\sim\\mathcal{O}\\left(\\sqrt{\\frac{\\mathcal{F}_{0}L\\sigma^{2}}{\\tilde{N}T E H}}+\\left(\\frac{\\mathcal{F}_{0}L\\sigma}{T}\\right)^{\\frac{2}{3}}\\!+\\!\\frac{d\\mathcal{F}_{0}}{T}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Given that we need  \u2264 40EHL for Theorem 4.1, we can set $d=40L$ . We thus can find a step size in the range of (EH)\u2264 4,i.e.,  \u2264 40EHL such that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\frac{1}{T E}\\sum_{t=0}^{T-1E-1}\\!\\mathbb{E}\\big\\lVert\\nabla f(\\hat{\\pmb{x}}^{t,e})\\big\\rVert^{2}\\leq\\mathcal{O}\\left(\\sqrt{\\frac{\\mathcal{F}_{0}L\\sigma^{2}}{\\tilde{N}T E H}}\\!+\\!\\left(\\frac{\\mathcal{F}_{0}L\\sigma}{T}\\right)^{\\frac{2}{3}}\\!+\\!\\frac{L\\mathcal{F}_{0}}{T}\\right).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "This completes the proof of Corollary 4.1. ", "page_idx": 21}, {"type": "text", "text": "F.3Proofs of Lemmas F.2.1-F.2.7 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "F.3.1Proof of Lemma F.2.1 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Under the framework of MTGC, the virtual global model obeys the following iteration: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\hat{\\pmb{x}}^{t,e+1}=\\hat{\\pmb{x}}^{t,e}-\\gamma\\sum_{j=1}^{N}\\frac{1}{n_{j}}\\sum_{i\\in\\mathcal{C}_{j}}\\sum_{h=0}^{H-1}\\left(\\nabla F_{i}\\left(\\pmb{x}_{i,h}^{t,e},\\xi_{i,h}^{t,e}\\right)+z_{i}^{t,e}+\\pmb{y}_{j}^{t}\\right).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "As $\\textstyle\\sum_{i\\in{\\mathcal{C}}_{j}}z_{i}^{t,e}=\\mathbf{0}$ and $\\textstyle\\sum_{j=1}^{N}\\pmb{y}_{j}^{t}=\\mathbf{0}$ ththeviullballrati ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\hat{\\pmb{x}}^{t,e+1}=\\hat{\\pmb{x}}^{t,e}-\\gamma\\sum_{j=1}^{N}\\frac{1}{n_{j}}\\sum_{i\\in{\\mathcal{C}}_{j}}\\sum_{h=0}^{H-1}\\nabla F_{i}\\left(\\pmb{x}_{i,h}^{t,e},\\xi_{i,h}^{t,e}\\right).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "With Assumption 1, we have ", "page_idx": 22}, {"type": "equation", "text": "$$\nf({\\pmb y})\\leq f({\\pmb x})+<\\nabla f({\\pmb x}),{\\pmb y}-{\\pmb x}>+\\frac{L}{2}\\|{\\pmb y}-{\\pmb x}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Plugging (28) into (29), we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{E}f(\\hat{\\pmb{x}}^{t,e+1})\\leq\\mathbb{E}f(\\hat{\\pmb{x}}^{t,e})-\\gamma\\mathbb{E}\\left\\langle\\nabla f(\\hat{\\pmb{x}}^{t,e}),\\frac{1}{N}\\sum_{j=1}^{N}\\frac{1}{n_{j}}\\sum_{i\\in\\mathcal{C}_{j}}\\sum_{h=0}^{H-1}\\nabla F_{i}\\left(\\pmb{x}_{i,h}^{t,e},\\xi_{i,h}^{t,e}\\right)\\right\\rangle\n$$", "text_format": "latex", "page_idx": 22}, {"type": "equation", "text": "$$\n+\\left.\\gamma^{2}L\\underbrace{\\frac{1}{2}\\mathbb{E}}\\left\\|\\frac{1}{N}\\sum_{j=1}^{N}\\frac{1}{n_{j}}\\sum_{i\\in\\mathcal{C}_{j}}\\sum_{h=0}^{H-1}\\nabla F_{i}\\left(\\pmb{x}_{i,h}^{t,e},\\xi_{i,h}^{t,e}\\right)\\right\\|^{2}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Utlizing $\\begin{array}{r}{\\mathbb{E}\\Big[\\frac{1}{N}\\sum_{j=1}^{N}\\frac{1}{n_{j}}\\sum_{i\\in\\mathcal{C}_{j}}\\sum_{h=0}^{H-1}\\nabla F_{i}\\left(x_{i,h}^{t,e},\\xi_{i,h}^{t,e}\\right)-\\frac{1}{N}\\sum_{j=1}^{N}\\frac{1}{n_{j}}\\sum_{i\\in\\mathcal{C}_{j}}\\sum_{h=0}^{H-1}\\nabla F_{i}(x_{i,h}^{t,e})\\Big]\\,=\\,\\mathbf{0},}\\end{array}$ we rewrite $T_{1}$ as follows ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad-\\eta\\nabla f(\\mu^{*})^{*}\\frac{1}{N}\\nabla\\bar{f}_{j}\\left[e^{\\theta^{*}}\\right]^{*}\\frac{1}{N}\\frac{\\displaystyle{\\sum_{t=1}^{N}\\frac{\\mathrm{S}_{j}}{\\mathrm{S}_{j}(t,\\omega_{t}^{*})}}}{\\displaystyle{\\sum_{t=1}^{N}\\frac{\\mathrm{S}_{j}}{\\mathrm{S}_{j}(t,\\omega_{t}^{*})}}\\frac{1}{N}\\frac{\\displaystyle{\\sum_{t=1}^{N}\\frac{\\mathrm{S}_{j}}{\\mathrm{S}_{j}(t,\\omega_{t}^{*})}}}{\\displaystyle{\\sum_{t=1}^{N}\\frac{\\mathrm{S}_{j}}{\\mathrm{S}_{j}(t,\\omega_{t}^{*})}}\\frac{1}{N}\\sum_{t=1}^{N}\\frac{\\displaystyle{\\sum_{t=1}^{N-1}\\mathrm{S}_{j}(t,\\omega_{t}^{*})}}{\\displaystyle{\\sum_{t=1}^{N-1}\\frac{\\mathrm{S}_{j}(t,\\omega_{t}^{*})}{\\mathrm{S}_{j}(t,\\omega_{t}^{*})}}}\\right.}\\\\ &{\\left.\\quad-\\frac{2\\eta}{2}\\frac{1}{N}\\left|\\frac{\\mathrm{S}_{j}}{\\mathrm{S}_{j}(t,\\omega_{t}^{*})}\\frac{1}{N}\\frac{\\displaystyle{\\sum_{t=1}^{N}\\mathrm{S}_{j}(t,\\omega_{t}^{*})}}{\\displaystyle{\\sum_{t=1}^{N}\\frac{\\mathrm{S}_{j}(t,\\omega_{t}^{*})}{\\mathrm{S}_{j}(t,\\omega_{t}^{*})}}}\\right|^{2}+\\eta\\nabla f_{j}\\left[\\left.\\frac{1}{N}\\right]_{i}\\frac{\\mathrm{S}_{j}}{\\mathrm{S}_{i}(t,\\omega_{t}^{*})}\\frac{1}{N}\\right|^{2}-1/N}\\frac{\\displaystyle{\\sum_{t=1}^{N}\\frac{\\mathrm{S}_{j}}{\\mathrm{S}_{j}(t,\\omega_{t}^{*})}}}{\\displaystyle{\\sum_{t=1}^{N}\\frac{\\mathrm{S}_{j}(t,\\omega_{t}^{*})}{\\mathrm{S}_{j}(t,\\omega_{t}^{*})}}}\\right|^{2}}\\\\ &{\\left.\\quad\\left<\\eta\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the last inequality comes from Assumption 1. ", "page_idx": 22}, {"type": "text", "text": "On the other hand, we can bound $T_{2}$ as follows ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle T_{2}\\leq\\mathbb{E}\\Bigg\\|\\frac{1}{N}\\sum_{j=1}^{N}\\frac{1}{n_{j}}\\sum_{i\\in\\mathcal{C}_{j}}\\sum_{h=0}^{H-1}\\big(\\nabla F_{i}\\left({\\bf z}_{i,h}^{t,e},\\xi_{i,h}^{t,e}\\right)-\\nabla F_{i}({\\bf x}_{i,h}^{t,e})\\big)\\Bigg\\|^{2}+\\mathbb{E}\\left\\|\\frac{1}{N}\\sum_{j=1}^{N}\\frac{1}{n_{j}}\\sum_{i\\in\\mathcal{C}_{j}}\\sum_{h=0}^{H-1}\\nabla F_{i}({\\bf x}_{i,h}^{t,e})\\right\\|^{2}}}\\\\ {{\\displaystyle\\qquad\\leq\\frac{1}{N^{2}}\\sum_{j=1}^{N}\\frac{1}{n_{j}^{2}}\\sum_{i\\in\\mathcal{C}_{j}}\\mathbb{E}\\left\\|\\frac{H-1}{N=0}\\,g_{i,h}^{t,e}-\\sum_{h=0}^{H-1}\\nabla F_{i}({\\bf x}_{i,h}^{t,e})\\right\\|^{2}+\\mathbb{E}\\left\\|\\frac{1}{N}\\sum_{j=1}^{N}\\frac{1}{n_{j}}\\sum_{i\\in\\mathcal{C}_{j}}\\sum_{h=0}^{H-1}\\nabla F_{i}({\\bf x}_{i,h}^{t,e})\\right\\|^{2}}}\\\\ {{\\displaystyle\\qquad\\leq\\frac{H\\sigma^{2}}{N^{2}}\\sum_{j=1}^{N}\\frac{1}{n_{j}}+\\mathbb{E}\\left\\|\\frac{1}{N}\\sum_{j=1}^{N}\\frac{1}{n_{j}}\\sum_{i=0}^{N-1}\\nabla F_{i}({\\bf x}_{i,h}^{t,e})\\right\\|^{2},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where second inequality comes from Lemma F.1.3 and the last inequality follows Assumption 2 and Lemma F.1.4. ", "page_idx": 23}, {"type": "text", "text": "Plugging the derived upper bounds of $T_{1}$ and $T_{2}$ into 30 and utilized $\\begin{array}{r}{\\gamma\\leq\\frac{1}{2H L}}\\end{array}$ , we obtain ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{E}f\\big(\\hat{x}^{t,e+1}\\big)\\leq\\mathbb{E}f\\big(\\hat{x}^{t,e}\\big)-\\frac{\\gamma H}{2}\\mathbb{E}\\left\\|\\nabla f\\big(\\hat{x}^{t,e}\\big)\\right\\|^{2}+\\gamma^{2}L\\frac{H\\sigma^{2}}{N^{2}}\\sum_{j=1}^{N}\\frac{1}{n_{j}}}\\\\ {\\displaystyle+\\,\\gamma H L^{2}\\frac{1}{N H}\\!\\sum_{j=1}^{N}\\!\\sum_{h=0}^{H-1}\\frac{1}{n_{j}}\\sum_{i\\in\\mathcal{C}_{j}}\\!\\mathbb{E}\\Big\\|\\bar{x}_{j}^{t,e}\\!-\\!x_{i,h}^{t,e}\\Big\\|^{2}+\\gamma H L^{2}\\frac{1}{N}\\sum_{j=1}^{N}\\!\\mathbb{E}\\big\\|\\hat{x}^{t,e}\\!-\\bar{x}_{j}^{t,e}\\big\\|^{2}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Telescoping the above inequality from $e=0$ to $H\\!-\\!1$ gives rise to Lemma F.2.1. ", "page_idx": 23}, {"type": "text", "text": "F.3.2Proofs of Lemmas F.2.2 and F.2.6 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Part I Lemma E2.2) Let $\\begin{array}{r}{q_{j,h}^{t,e}=\\frac{1}{n_{j}}\\sum_{i\\in{\\mathcal{C}}_{j}}\\mathbb{E}\\left\\|\\bar{\\pmb{x}}_{j}^{t,e}\\!-\\!\\pmb{x}_{i,h}^{t,e}\\right\\|^{2},q_{j,0}^{t,e}=0.}\\end{array}$ For $0\\!\\leq\\!h\\!\\leq\\!H\\!-\\!2$ we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad_{\\mathcal{H}_{i}^{\\mathrm{f}},\\delta_{i}^{\\mathrm{f}},\\delta_{i}^{\\mathrm{f}}}}\\\\ &{=\\sum_{\\ell\\in\\mathcal{G}_{j}}\\left\\{\\mathbb{E}\\left|x_{\\ell,i}^{t,\\kappa}-\\gamma\\left(\\nabla F_{i}\\left(x_{\\ell,i}^{t,\\kappa},\\xi_{i}^{\\varepsilon,\\kappa}\\right)+y_{j}^{t}+z_{\\ell}^{t,\\kappa}\\right)-x_{j}^{t,\\varepsilon}\\right|^{2}\\right\\}}\\\\ &{\\le\\bigg(1+\\displaystyle\\frac{1}{H-1}\\bigg)\\mathbb{E}\\left\\|x_{\\ell,i}^{t,\\kappa}-x_{j}^{t,\\varepsilon}\\right\\|^{2}+H\\sum_{i\\in\\mathcal{G}_{j}}\\mathbb{E}\\left\\|\\gamma\\left(\\nabla F_{i}\\left(x_{\\ell,i}^{t,\\kappa}\\right)+y_{j}^{t}+z_{\\ell}^{t,\\kappa}\\right)\\right\\|^{2}+n_{j}\\gamma^{2}\\sigma^{2}}\\\\ &{=\\bigg(1+\\displaystyle\\frac{1}{H-1}\\bigg)\\mathbb{E}\\left\\|x_{\\ell,i}^{t,\\kappa}-x_{j}^{t,\\varepsilon}\\right\\|^{2}+\\gamma^{2}H\\sum_{i\\in\\mathcal{G}_{j}}\\mathbb{E}\\left\\|\\nabla F_{i}\\left(x_{\\ell,i}^{t,\\kappa}\\right)\\mp\\nabla F_{i}\\left(x_{j}^{t,\\varepsilon}\\right)\\mp\\nabla f_{j}\\left(x_{j}^{t,\\varepsilon}\\right)}\\\\ &{\\quad\\mp\\nabla f_{j}\\left(\\frac{\\alpha^{\\dagger}e^{-\\alpha}}{H-1}\\right)\\mp\\nabla f\\left(x_{\\ell}^{t,\\kappa}\\right)+y_{j}^{t}+z_{\\ell}^{t,\\kappa}\\|^{2}+n_{j}\\gamma^{2}\\sigma^{2}}\\\\ &{\\le\\bigg(1+\\displaystyle\\frac{1}{H-1}+4\\gamma^{2}H L^{2}\\bigg)\\sum_{i\\in\\mathcal{G}_{j}}\\mathbb{E}\\left\\|x_{\\ell,i}^{t,\\kappa}-x_{j}^{t,\\varepsilon}\\right\\|^{2}+4\\gamma^{2}H\\sum_{i\\in\\mathcal{G}_{j}}\\mathbb{E}\\left\\|z_{i}^{t,\\kappa}+\\nabla F_{i}\\left(x_{j}^{t,\\kappa}\\right)-\\nabla f_{j}\\left(x_{j}^{t,\\varepsilon}\\right)\\right\\|^{2}}\\\\ &{\\quad+4\\gamma^{2}H n_{j}\\mathbb{E\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where the first inequality comes from Lemma F.1.2 and Assumption 2 and the second inequality follows Lemma F.1.1. Let $\\begin{array}{r}{\\rho_{1}\\,=\\,\\Bigl(1+\\frac{1}{H-1}+4\\gamma^{2}H L^{2}\\Bigr)}\\end{array}$ .As $\\begin{array}{r}{\\gamma\\le\\frac{1}{8H L}}\\end{array}$ , we have $\\rho_{1}^{h}\\,\\le\\,\\rho_{1}^{H-1}\\,\\le$ $\\begin{array}{r}{\\left(1+\\frac{1}{H-1}+\\frac{1}{16\\left(H-1\\right)}\\right)^{H-1}\\leq e_{0}^{\\frac{17}{16}}<3}\\end{array}$ and $\\begin{array}{r}{\\sum_{h=0}^{H-1}\\rho_{1}^{h}\\leq3H}\\end{array}$ where $e_{0}$ denotes Eule's mumbe Wwe ", "page_idx": 23}, {"type": "text", "text": "thus have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad n_{j}q_{j,h+1}^{t,e}}\\\\ &{\\leq\\left(\\displaystyle\\sum_{\\tau=0}^{h}\\rho_{1}^{\\tau}\\right)\\left(4\\gamma^{2}H\\displaystyle\\sum_{i\\in C_{j}}\\mathbb{E}\\left\\|z_{i}^{t,e}+\\nabla F_{i}\\left(\\bar{x}_{j}^{t,e}\\right)-\\nabla f_{j}\\left(\\bar{x}_{j}^{t,e}\\right)\\right\\|^{2}+8\\gamma^{2}H L^{2}n_{j}\\mathbb{E}\\left\\|\\bar{x}_{j}^{t,e}-\\hat{x}^{t,e}\\right\\|\\right.}\\\\ &{\\quad\\left.+4\\gamma^{2}H n_{j}\\mathbb{E}\\left\\|y_{j}^{t}+\\nabla f_{j}\\left(\\bar{x}_{j}^{t,e}\\right)-\\nabla f\\left(\\bar{x}^{t}\\right)\\right\\|^{2}+8\\gamma^{2}H n_{j}\\mathbb{E}\\left\\|\\nabla f\\left(\\hat{x}^{t,e}\\right)\\right\\|^{2}+n_{j}\\gamma^{2}\\sigma^{2}\\right)}\\\\ &{\\le12\\gamma^{2}H^{2}\\displaystyle\\sum_{i\\in C_{j}}\\mathbb{E}\\left\\|z_{i}^{t,e}+\\nabla F_{i}\\left(\\bar{x}_{j}^{t,e}\\right)-\\nabla f_{j}\\left(\\bar{x}_{j}^{t,e}\\right)\\right\\|^{2}+12\\gamma^{2}H^{2}n_{j}\\mathbb{E}\\left\\|y_{j}^{t}+\\nabla f_{j}\\left(\\hat{x}^{t,e}\\right)-\\nabla f\\left(\\hat{x}^{t,e}\\right)\\right\\|^{2}}\\\\ &{\\quad+24\\gamma^{2}H^{2}L^{2}n_{j}\\mathbb{E}\\left\\|\\bar{x}_{j}^{t,e}-\\hat{x}^{t,e}\\right\\|+24\\gamma^{2}H^{2}n_{j}\\mathbb{E}\\left\\|\\nabla f\\left(\\hat{x}^{t,e}\\right)\\right\\|^{2}+3H n_{j}\\gamma^{2}\\sigma^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where the last inequality follows Assumption 1. ", "page_idx": 24}, {"type": "text", "text": "Pluging he drivedur ound of $n_{j}q_{j,h+1}^{t,e}$ into $\\begin{array}{r}{Q_{t}=\\sum_{e=0}^{E-1}\\frac{1}{N}\\sum_{j=1}^{N}\\left(\\frac{1}{H}\\sum_{h=0}^{H-1}q_{j,h}^{t,e}\\right)}\\end{array}$ gives rise to Lemma F.2.2. ", "page_idx": 24}, {"type": "text", "text": "Part II Lemma E2.6) Fist, $\\begin{array}{r}{\\bar{x}_{j}^{t,e+1}=\\bar{x}_{j}^{t,e}+\\gamma\\sum_{h=0}^{H-1}\\frac{1}{n_{j}}\\sum_{i\\in\\mathcal{C}_{j}}\\Big(\\nabla F_{i}\\left(x_{i,h}^{t,e},\\xi_{i,h}^{t,e}\\right)\\!+\\!z_{i}^{t,e}\\!+\\!y_{j}^{t}\\Big).}\\end{array}$ As $\\textstyle\\sum_{i\\in\\mathcal{C}_{j}}z_{i}^{t,e}=\\mathbf{0}$ , we can rewrite $\\Theta_{j}^{t,e}$ as ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\Theta_{j}^{t,e}=\\mathbb{E}\\left\\|\\bar{x}_{j}^{t,e+1}-\\bar{x}_{j}^{t,e}\\right\\|^{2}=\\mathbb{E}\\left\\|\\gamma\\sum_{h=0}^{H-1}\\frac{1}{n_{j}}\\sum_{i\\in\\mathcal{C}_{j}}\\left(\\nabla F_{i}\\left(x_{i,h}^{t,e},\\xi_{i,h}^{t,e}\\right)+y_{j}^{t}\\right)\\right\\|^{2}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Next, we establish an upper bound for $\\Theta_{j}^{t,e}$ as follows ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Psi_{j}^{t,e}\\le2^{2}r^{2}\\mathbb{E}\\left\\|\\displaystyle\\sum_{h=0}^{H-1}\\frac{1}{\\eta_{j}}\\sum_{i\\in\\mathcal{E}_{j}}\\big(\\nabla F_{i}(\\mathbf{x}_{i,h}^{t,e})+y_{j}^{t}\\big)\\right\\|^{2}+2^{2}r^{2}\\mathbb{E}\\left\\|\\displaystyle\\sum_{h=0}^{H-1}\\frac{1}{\\eta_{j}}\\sum_{i\\in\\mathcal{E}_{j}}\\big(\\nabla F_{i}\\left(\\mathbf{x}_{i,h}^{t,e},\\xi_{i,h}^{t,e}\\right)-\\nabla F_{i}\\left(\\mathbf{x}_{i,h}^{t,e}\\right)\\big)\\right\\|^{2}}\\\\ &{\\qquad\\le2^{2}r^{2}H\\displaystyle\\sum_{h=0}^{H-1}\\mathbb{E}\\left\\|\\displaystyle\\frac{1}{\\eta_{j}}\\sum_{i\\in\\mathcal{E}_{j}}\\nabla F_{i}\\left(\\mathbf{x}_{i,h}^{t,e}\\right)+y_{j}^{t}\\right\\|^{2}+2\\frac{1}{n_{j}^{2}}\\displaystyle\\sum_{i\\in\\mathcal{E}_{j}}\\gamma^{2}\\mathbb{E}\\left\\|\\displaystyle\\sum_{h=0}^{H-1}\\big(\\nabla F_{i}\\left(\\mathbf{x}_{i,h}^{t,e},\\xi_{i,h}^{t,e}\\right)-\\nabla F_{i}\\left(\\mathbf{x}_{i,h}^{t,e}\\right)\\right)\\right\\|^{2}}\\\\ &{\\qquad\\le2^{2}r^{2}H\\displaystyle\\sum_{h=0}^{H-1}\\mathbb{E}\\left\\|\\displaystyle\\frac{1}{\\eta_{j}}\\sum_{i\\in\\mathcal{E}_{j}}\\nabla F_{i}\\left(\\mathbf{x}_{i,h}^{t,e}\\right)\\mp\\nabla f_{j}\\left(\\mathbf{x}_{j}^{t,e}\\right)\\mp\\nabla f_{j}\\left(\\boldsymbol{\\hat{x}}^{t,e}\\right)\\mp\\nabla f\\left(\\boldsymbol{\\hat{x}}^{t,e}\\right)+y_{j}^{t}\\right\\|^{2}+2^{2}r^{2}\\displaystyle\\frac{H\\sigma^{2}}{n_{j}}}\\\\ &{\\qquad\\le8r^{2}H^{2}L^{-1}\\displaystyle\\frac{1}{H}\\displaystyle\\sum_{h=0}^{H-1}\\frac{1}{\\eta_{j}}\\sum_{i\\in\\mathcal{E}_{j}}\\mathbb{E}\\left\\|\\mathbf{x}_{i,h}^{t,e}- \n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where the second inequality holds due to Lemmas F.1.1 and F.1.3, the second third inequality comes from Lemmas F.1.4 and Assumption 2, and the last inequality follows Assumption 1. Plugging this upper bound into $\\begin{array}{r}{\\sum_{e=0}^{E-1}\\frac{1}{N}\\sum_{j=1}^{N^{-}}\\Theta_{j}^{t,e}}\\end{array}$ gives rise to Lemma F.2.6. ", "page_idx": 24}, {"type": "text", "text": "F.3.3Proof of Lemma F.2.3 ", "text_level": 1, "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathrm{0~boundD}_{i}=\\sum_{\\ell=0}^{E-1}\\frac{1}{N}\\sum_{j=1}^{N}\\mathbb{E}_{t}^{e}\\Big\\|\\overline{{\\boldsymbol{x}}}_{j}^{t,e}-\\dot{\\boldsymbol{x}}^{t,e}\\Big\\|^{2},\\mathrm{we~first~revrite~}\\mathbb{E}\\Big\\|\\overline{{\\boldsymbol{x}}}_{j}^{t,e+1}-\\dot{\\boldsymbol{x}}^{t,e+1}\\Bigg\\|^{2}\\mathrm{~as~follow}}\\\\ {\\displaystyle\\ ~~\\mathbb{E}\\Big\\|\\overline{{\\boldsymbol{x}}}_{j}^{t,e+1}-\\dot{\\boldsymbol{x}}^{t,e+1}\\Big\\|^{2}}\\\\ {\\displaystyle=\\mathbb{E}\\Bigg\\|\\overline{{\\boldsymbol{x}}}_{j}^{t,e}-\\frac{1}{n_{j}}\\sum_{i\\in\\mathcal{C}_{j}}\\sum_{h=0}^{n-1}\\gamma\\left(\\nabla F_{i}\\left(\\boldsymbol{x}_{i,h}^{t,e},\\boldsymbol{\\xi}_{i,h}^{t,e}\\right)+\\boldsymbol{y}_{j}^{t}+\\boldsymbol{z}_{i}^{t,e}\\right)}\\\\ {\\displaystyle\\ ~~-\\dot{\\boldsymbol{x}}^{t,e}+\\frac{1}{N}\\sum_{j=1}^{N}\\frac{1}{n_{j}}\\sum_{i\\in\\mathcal{C}_{j}}\\sum_{h=0}^{n-1}\\gamma\\left(\\nabla F_{i}\\left(\\boldsymbol{x}_{i,h}^{t,e},\\boldsymbol{\\xi}_{i,h}^{t,e}\\right)+\\boldsymbol{y}_{j}^{t}+\\boldsymbol{z}_{i}^{t,e}\\right)\\Bigg\\|^{2}}\\\\ {\\displaystyle=\\mathbb{E}\\Bigg\\|\\overline{{\\boldsymbol{x}}}_{j}^{t,e}-\\dot{\\boldsymbol{x}}^{t,e}-\\frac{1}{n_{j}}\\sum_{i\\in\\mathcal{C}_{j}}\\sum_{h=0}^{n-1}\\gamma\\left(\\boldsymbol{y}_{j}^{t}+\\nabla F_{i}\\left(\\boldsymbol{x}_{i,h}^{t,e},\\boldsymbol{\\xi}_{i,h}^{t,e}\\right)\\right)+\\frac{1}{N}\\sum_{j=1}^{N}\\frac{1}{n_{j}}\\sum_{i\\in\\mathcal{C}_{j}}\\sum_{h=0}^{n-1}\\gamma\\nabla F_{i}\\left(\\boldsymbol{x}_{i,h}^{t,e},\\boldsymbol{\\xi}_{i,h}^{t,e}\\right)\\Bigg\\|^\n$$", "text_format": "latex", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\|\\boldsymbol{x}_{j}^{t,\\star(1)}-\\boldsymbol{x}^{t,\\star(1)}\\|^{2}}\\\\ &{\\le\\bigg(1+\\displaystyle\\frac{1}{E^{2}}\\displaystyle\\prod_{i=1}^{n}\\frac{\\eta}{\\xi_{i}}-\\mu^{-\\varepsilon,\\epsilon}\\|^{2}+\\gamma^{2}E\\left[\\left|\\frac{\\eta-1}{k-u}\\right|^{\\frac{n-\\varepsilon}{\\xi_{i}}}\\right]+\\displaystyle\\sum_{k=0}^{n-1}\\frac{1}{n}\\sum_{i_{1}=\\varnothing}^{\\infty}\\big(\\mathrm{TF}\\big(\\alpha_{k,i}^{t,\\star}\\xi_{k,i}^{\\star}\\big)\\big)}\\\\ &{\\quad\\overset{\\eta\\le-1}{\\underset{k=0}{\\operatorname*{Tom}}}\\displaystyle\\sum_{\\eta,\\eta_{k}\\in\\mathcal{V}_{k}}\\big(\\alpha_{k,i}^{t,\\star}\\big)-\\sum_{k=0}^{n-1}\\frac{1}{N}\\sum_{i_{1}=\\iota_{1}}^{N}\\frac{1}{\\sum_{i_{2}=\\iota_{2}}^{N}}\\mathbb{T}F\\big(\\alpha_{k,i}^{t,\\star}\\xi_{k,i}^{\\star}\\big)\\geq\\displaystyle\\frac{n-1}{\\sum_{i=0}^{n-1}\\frac{N}{N}\\sum_{j=1}^{N}\\frac{1}{\\mu}\\sum_{i_{1},\\xi_{i}^{\\star}}\\nabla F_{i}\\big(\\alpha_{k,i}^{t,\\star}\\big)}\\bigg\\|^{2}}\\\\ &{\\le\\bigg(1+\\displaystyle\\frac{1}{E^{2}}\\displaystyle\\big)\\mathbb{E}\\|\\boldsymbol{\\mu}_{j}^{t,\\star}-\\boldsymbol{\\mu}^{t,\\star(1)}\\|^{2}}\\\\ &{\\quad+2\\gamma^{2}E\\displaystyle\\left\\|\\frac{1}{\\sum_{i=1}^{n}\\eta}\\sum_{i_{1}=0}^{\\infty}\\frac{1}{n}\\sum_{i_{2}=\\iota_{3}}^{\\infty}\\nabla F_{i}\\big(\\alpha_{k,i}^{t,\\star}\\big)-\\displaystyle\\sum_{k=0}^{n-1}\\frac{1}{N}\\sum_{j=1}^{N}\\frac{1}{n}\\sum_{i_{1}=\\iota_{3}}^{N}\\nabla F_{i}\\big(\\alpha_{k,i}^{t,\\star}\\big)\\right\\|^{2}}\\\\ &{\\quad+2\\gamma^{2}E\\displaystyle\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where the first inequality comes from Lemma F.1.2. We thus have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{1}{N}\\displaystyle\\sum_{j=1}^{N}\\mathbb{E}\\bigg\\|\\mathbf{x}_{j}^{t,\\epsilon+1}-\\hat{\\mathbf{z}}^{t,\\epsilon+1}\\bigg\\|^{2}}\\\\ &{\\le\\left(1+\\displaystyle\\frac{1}{E-1}\\right)\\displaystyle\\frac{1}{N}\\displaystyle\\sum_{j=1}^{N}\\mathbb{E}\\bigg\\|\\mathbf{x}_{j}^{t,\\epsilon}-\\hat{\\mathbf{z}}^{t,\\epsilon}\\bigg\\|^{2}}\\\\ &{\\quad+\\displaystyle\\;2\\gamma^{2}E H^{2}\\displaystyle\\frac{1}{N}\\displaystyle\\sum_{j=1}^{N}\\frac{1}{H}\\displaystyle\\sum_{h=0}^{H-1}\\mathbb{E}\\bigg\\|\\mathbf{y}_{j}^{t}+\\frac{1}{n_{j}}\\displaystyle\\sum_{i\\in\\mathcal{E}_{j}}\\nabla F_{i}\\left(\\mathbf{z}_{i,h}^{t,\\epsilon}\\right)-\\frac{1}{N}\\displaystyle\\sum_{j=1}^{N}\\frac{1}{n_{j}}\\sum_{i\\in\\mathcal{E}_{j}}\\nabla F_{i}\\left(\\mathbf{z}_{i,h}^{t,\\epsilon}\\right)\\bigg\\|^{2}}\\\\ &{\\quad+\\displaystyle\\;2\\gamma^{2}E\\displaystyle\\frac{N-1}{N^{2}}\\sum_{j=1}^{N}\\mathbb{E}\\bigg\\|\\displaystyle\\sum_{h=0}^{H-1}\\frac{1}{n_{j}}\\sum_{i\\in\\mathcal{E}_{j}}\\nabla F_{i}\\left(\\mathbf{z}_{i,h}^{t,\\epsilon},\\boldsymbol{\\xi}_{i,h}^{t,\\epsilon}\\right)-\\displaystyle\\sum_{h=0}^{H-1}\\frac{1}{n_{j}}\\sum_{i\\in\\mathcal{E}_{j}}\\nabla F_{i}\\left(\\mathbf{z}_{i,h}^{t,\\epsilon}\\right)\\bigg\\|^{2}}\\\\ &{\\le\\left(1+\\displaystyle\\frac{1}{E-1}\\right)\\displaystyle\\frac{1}{N}\\displaystyle\\sum_{j=1}^{N}\\mathbb{E}\\bigg\\|\\mathbf{z}_{j}^{t,\\epsilon}-\\hat{\\mathbf{z}}^{t,\\epsilon}\\bigg\\|^{2}+2\\gamma^{2}E H\\displaystyle\\frac{N-1}{N^{2}}\\displaystyle\\sum_{j=1}^{N}\\frac{1}{n_{j}}\\sigma^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "equation", "text": "$$\n+\\left.2\\gamma^{2}E H^{2}\\frac{1}{N}\\sum_{j=1}^{N}\\frac{1}{H}\\sum_{h=0}^{H-1}\\mathbb{E}\\right\\Vert y_{j}^{t}+\\frac{1}{n_{j}}\\sum_{i\\in\\mathcal{C}_{j}}\\nabla F_{i}\\left(x_{i,h}^{t,e}\\right)-\\frac{1}{N}\\sum_{j=1}^{N}\\frac{1}{n_{j}}\\sum_{i\\in\\mathcal{C}_{j}}\\nabla F_{i}\\left(x_{i,h}^{t,e}\\right)\\right\\Vert^{2},\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where the first equality comes from Lemmas F.1.1 and the second inequality follows Lemmas F.1.3 and F.1.4. Additionally, we bound $T_{3}$ as ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad=\\cfrac{1}{H}\\cfrac{N-1}{\\displaystyle\\sum_{j=1}^{N}}\\cfrac{N}{N}\\cfrac{\\mathbb{E}_{j}}{\\displaystyle\\int_{0}^{\\infty}}\\mathrm{E}_{j}\\left(y^{\\varepsilon};\\nabla f_{j}(\\hat{x}^{\\varepsilon,\\varepsilon})\\cdot\\nabla f\\left(\\hat{x}^{\\varepsilon,\\varepsilon}\\right)^{*}\\right)+\\cfrac{1}{H}\\cfrac{N}{\\displaystyle\\sum_{j=1}^{N-1}}\\cfrac{N}{N}\\cfrac{N}{j!}\\cfrac{N}{N!}\\cfrac{\\nabla_{j}\\nabla F_{j}(\\hat{x}_{\\varepsilon,\\varepsilon}^{\\varepsilon,\\varepsilon})}{\\displaystyle\\int_{0}^{\\infty}}}\\\\ &{\\quad\\leq2\\cfrac{1}{N}\\cfrac{N}{\\displaystyle\\int_{\\mathbb{R}_{\\varepsilon}}}\\cfrac{N}{\\displaystyle\\int_{\\mathbb{R}_{\\varepsilon}}}\\left\\|y\\right\\|_{\\mathcal{H}}^{*}+\\nabla f_{j}\\left(\\hat{x}^{\\varepsilon,\\varepsilon}\\right)-\\nabla f\\left(\\hat{x}^{\\varepsilon,\\varepsilon}\\right)^{*}\\right)\\bigg\\|^{2}+\\cfrac{1}{H}\\cfrac{N-1}{\\displaystyle\\int_{\\mathbb{R}_{\\varepsilon}}}\\cfrac{N}{N}\\cfrac{N}{\\displaystyle\\int_{\\mathbb{R}_{\\varepsilon}}}\\cfrac{1}{\\displaystyle\\int_{\\mathbb{R}_{\\varepsilon}}^{\\infty}}\\mathrm{E}_{j}\\left(x_{\\varepsilon,\\varepsilon}^{\\varepsilon,\\varepsilon}\\right)-\\nabla f_{j}\\left(\\hat{x}^{\\varepsilon,\\varepsilon}\\right)}\\\\ &{\\quad\\quad-\\left(\\cfrac{1}{N}\\cfrac{N}{\\displaystyle\\int_{\\mathbb{R}_{\\varepsilon}}}\\cfrac{1}{n_{j}_{\\varepsilon}}\\cfrac{\\nabla F_{j}\\left(x_{\\varepsilon,\\varepsilon}^{\\varepsilon,\\varepsilon}\\right)}{\\displaystyle\\int_{0}^{\\infty}}\\mathrm{E}_{j}\\left(x_{\\varepsilon,\\varepsilon}^{\\varepsilon,\\varepsilon}\\right)-\\nabla f\\left(\\hat{x}^{\\varepsilon,\\varepsilon}\\right)\\right)\\bigg\\|^{2}\\cfrac{1}{H}}\\\\ &{\\quad\\leq2\\cfrac{1}{N}\\cfrac{N}{\\displaystyle\\int_{\\mathbb{R}_{\\varepsilon}}}\\left\\|y_{\\varepsilon}^{\\varepsilon}+\\nabla f_{j}\\left(\\hat{x}^{\\varepsilon,\\varepsilon}\\right)-\\nabla f\\left(\\hat{x}^{\\varepsilon,\\varepsilon}\\right)\\right\\|^{2}+2\\cfrac{1}{H}\\cfrac{N-1}{\\displaystyle\\int_{\\mathbb{R}_{ \n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where the second inequality follows Lemma F.1.1 and the last inequality follows Assumption 1. Plugging the derived upper bound of $T_{3}$ into (31) gives rise to ", "page_idx": 26}, {"type": "text", "text": "$\\begin{array}{r l}&{\\displaystyle\\frac{1}{N}\\sum_{j=1}^{N}\\mathbb{E}\\|\\bar{x}_{j}^{t,e+1}-\\bar{x}^{t,e+1}\\|^{2}\\leq\\left(1\\!+\\!\\displaystyle\\frac{1}{E-1}\\!+\\!8\\gamma^{2}E H^{2}L^{2}\\right)\\displaystyle\\frac{1}{N}\\sum_{j=1}^{N}\\mathbb{E}\\big\\|\\bar{x}_{j}^{t,e}-\\hat{x}^{t,e}\\big\\|^{2}\\!+\\!2\\gamma^{2}E H\\displaystyle\\frac{N-1}{N^{2}}\\!\\sum_{j=1}^{N}\\!\\frac{1}{n_{j}}\\sigma^{2}}\\\\ &{+4\\gamma^{2}E H^{2}\\displaystyle\\frac{1}{N}\\sum_{j=1}^{N}\\mathbb{E}\\big\\|y_{j}^{t}+\\nabla f_{j}\\left(\\hat{x}^{t,e}\\right)-\\nabla f\\left(\\hat{x}^{t,e}\\right)\\big\\|^{2}\\!+\\!8\\gamma^{2}E H^{2}L^{2}\\displaystyle\\frac{1}{N}\\sum_{j=1}^{N}\\!\\displaystyle\\frac{1}{n_{j}}\\sum_{i\\in\\mathcal{C}_{j}}\\displaystyle\\frac{1}{H}\\sum_{h=0}^{H-1}\\mathbb{E}\\big\\|x_{i,h}^{t,e}-\\bar{x}_{j}^{t,e}\\big\\|^{2}\\,.}\\end{array}$ Let $\\begin{array}{r l r}{\\rho_{2}}&{{}=}&{1\\;+\\;\\frac{1}{E-1}\\;+\\;8\\gamma^{2}E H^{2}L^{2}}\\end{array}$ As $\\begin{array}{r l r}{\\gamma}&{{}\\le}&{\\frac{1}{10E H L}}\\end{array}$ , we have $\\begin{array}{r l r}{\\rho_{2}}&{{}\\le}&{\\rho_{2}^{E\\!-\\!1}\\quad:}\\end{array}$ $\\begin{array}{r}{\\left(1+\\frac{1}{E-1}+\\frac{1}{12(E-1)}\\right)^{E-1}\\leq e_{0}^{\\frac{13}{12}}<3}\\end{array}$ where $e_{0}$ denotes lulers number Threfoe we ave ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\displaystyle\\frac{1}{N}\\displaystyle\\sum_{j=1}^{N}\\mathbb{E}\\Big\\lVert\\bar{\\mathbf{x}}_{j}^{t,e+1}-\\bar{\\mathbf{x}}^{t,e+1}\\Big\\rVert^{2}}\\\\ &{\\leq\\biggl(\\displaystyle\\sum_{\\nu=0}^{c}\\rho_{j}^{\\nu}\\biggr)^{2}\\gamma^{2}E H\\displaystyle\\frac{N-1}{N^{2}}\\displaystyle\\sum_{j=1}^{N}\\overline{{\\eta}}_{j}^{1}\\sigma^{2}+4\\operatorname*{max}\\{\\rho_{j}^{\\nu}\\}\\gamma^{2}E H^{2}\\displaystyle\\sum_{\\nu=0}^{c}\\frac{1}{N}\\displaystyle\\sum_{\\nu=1}^{N}\\mathbb{E}\\Big\\lVert y_{j}^{t}+\\nabla f_{j}\\left(\\bar{\\mathbf{x}}^{t,\\nu}\\right)-\\nabla f\\left(\\bar{\\mathbf{x}}^{t,\\nu}\\right)\\Big\\rVert^{2}}\\\\ &{\\quad+\\,8\\operatorname*{max}\\{\\rho_{j}^{\\nu}\\}\\gamma^{2}E H^{2}L_{\\gamma=0}^{2}\\displaystyle\\sum_{N=0}^{c}\\displaystyle\\frac{1}{N}\\displaystyle\\sum_{j=1}^{N}\\displaystyle\\frac{1}{\\nu\\int_{\\mathbb{Z}}\\prod_{\\nu=0}^{c-1}}\\mathbb{E}\\Big\\lVert\\mathbf{x}_{0}^{t,\\nu}-\\bar{\\mathbf{x}}_{j}^{t,\\nu}\\Big\\rVert^{2}}\\\\ &{\\leq\\theta\\gamma^{2}(e+1)E H\\displaystyle\\frac{N-1}{N^{2}}\\displaystyle\\sum_{j=1}^{N}\\overline{{\\eta}}_{j}^{2}+12\\gamma^{2}E H^{2}\\displaystyle\\sum_{\\nu=0}^{c}\\frac{1}{N}\\displaystyle\\sum_{j=1}^{N}\\mathbb{E}\\Big\\lVert y_{j}^{t}+\\nabla f_{j}\\left(\\bar{\\mathbf{x}}^{t,\\nu}\\right)-\\nabla f\\left(\\bar{\\mathbf{x}}^{t,\\nu}\\right)\\Big\\rVert^{2}}\\\\ &{\\quad+\\,24\\gamma^{2}E H^{2}L_{\\gamma=0}^{2}\\displaystyle\\sum_{N=0}^{c}\\displaystyle\\sum_{N=1}^{N}\\sum_{j=1}^{N}\\sum_{n_{j}}\\displaystyle\\frac{1}{K}\\displaystyle\\sum_{i=1}^{N-1}\\mathbb{E}\\Big\\lVert\\mathbf{x}_{0}^{t,\\nu}-\\bar{\\mathbf{x}}_{j}^{t,\\nu}\\Big\\rVert^{2 \n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Hence, for $\\begin{array}{r}{D_{t}=\\sum_{e=0}^{E-1}\\frac{1}{N}\\sum_{j=1}^{N}\\mathbb{E}_{t}^{e}\\big\\|\\bar{\\pmb{x}}_{j}^{t,e}-\\hat{\\pmb{x}}^{t,e}\\big\\|^{2}}\\end{array}$ , we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{D_{t}\\leq3\\gamma^{2}E^{3}H\\displaystyle\\frac{N-1}{N^{2}}\\displaystyle\\sum_{j=1}^{N}\\displaystyle\\frac{1}{n_{j}}\\sigma^{2}+12\\gamma^{2}E^{2}H^{2}\\displaystyle\\sum_{e=0}^{E-1}\\displaystyle\\frac{1}{N}\\sum_{j=1}^{N}\\mathbb{E}\\big\\|y_{j}^{t}+\\nabla f_{j}\\left(\\hat{x}^{t,e}\\right)-\\nabla f\\left(\\hat{x}^{t,e}\\right)\\big\\|^{2}}\\\\ &{\\qquad+\\left.24\\gamma^{2}E^{2}H^{2}L^{2}\\displaystyle\\sum_{e=0}^{E-1}\\displaystyle\\frac{1}{N}\\sum_{j=1}^{N}\\displaystyle\\frac{1}{n_{j}}\\sum_{i\\in\\mathcal{C}_{j}}\\displaystyle\\frac{1}{H}\\sum_{h=0}^{H-1}\\mathbb{E}\\big\\|x_{i,h}^{t,e}-\\bar{x}_{j}^{t,e}\\big\\|^{2}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "This completes the proof of Lemma F.2.3. ", "page_idx": 26}, {"type": "text", "text": "F.3.4Proof of Lemma F.2.4 ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "For $e=0$ \uff0c $\\begin{array}{r}{z_{i}^{t,0}=-\\nabla F_{i}\\left(\\pmb{x}_{i,0}^{t,0},\\xi_{i,0}^{t,0}\\right)+\\frac{1}{n_{j}}\\sum_{i\\in\\mathcal{C}_{j}}\\nabla F_{i}\\left(\\pmb{x}_{i,0}^{t,0},\\xi_{i,0}^{t,0}\\right)}\\end{array}$ where $\\pmb{x}_{i,0}^{t,0}=\\bar{\\pmb{x}}_{j}^{t,0}$ , we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle Z_{j}^{t,0}=\\frac{1}{n_{j}}\\sum_{i\\in\\mathcal{C}_{j}}\\mathbb{E}\\left\\|-\\nabla F_{i}\\left(\\bar{x}_{j}^{t,0},\\xi_{i,0}^{t,0}\\right)+\\frac{1}{n_{j}}\\sum_{i\\in\\mathcal{C}_{j}}\\nabla F_{i}\\left(\\bar{x}_{j}^{t,0},\\xi_{i,0}^{t,0}\\right)+\\nabla F_{i}\\left(\\bar{x}_{j}^{t,0}\\right)-\\nabla f_{j}\\left(\\bar{x}_{j}^{t,0}\\right)\\right\\|^{2}}}\\\\ {{\\displaystyle\\qquad\\leq\\frac{1}{n_{j}}\\sum_{i\\in\\mathcal{C}_{j}}\\mathbb{E}\\left\\|\\nabla F_{i}\\left(\\bar{x}_{j}^{t,0}\\right)-\\nabla F_{i}\\left(\\bar{x}_{j}^{t,0},\\xi_{i,0}^{t,0}\\right)\\right\\|^{2}\\leq\\sigma^{2}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where the first inequality follows Lemma F.1.1 and the second inequality holds due to Assumption 2. In addition, when $e\\geq0,z_{i}^{t,e}$ obeys the following iteration ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle z_{i}^{t,e+1}=z_{i}^{t,e}+\\frac{1}{H\\gamma}\\left(x_{i,H}^{t,e}-x_{i,0}^{t,e}-\\frac{1}{n_{j}}\\sum_{i\\in\\mathcal{C}_{j}}\\left(x_{i,H}^{t,e}-x_{i,0}^{t,e}\\right)\\right)}}\\\\ {{\\displaystyle=z_{i}^{t,e}-\\frac{1}{H}\\sum_{h=0}^{H-1}\\left(\\left(\\nabla F_{i}\\left(x_{i,h}^{t,e},\\xi_{i,h}^{t,e}\\right)+y_{j}^{t}+z_{i}^{t,e}\\right)-\\frac{1}{n_{j}}\\sum_{i\\in\\mathcal{C}_{j}}\\left(\\nabla F_{i}\\left(x_{i,h}^{t,e},\\xi_{i,h}^{t,e}\\right)+y_{j}^{t}+z_{i}^{t,e}\\right)\\right).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "As $\\textstyle\\sum_{i\\in{\\mathcal{C}}_{j}}z_{i}^{t,e}=\\mathbf{0}$ we thus have ", "page_idx": 27}, {"type": "equation", "text": "$$\nz_{i}^{t,e+1}=\\frac{1}{H}\\!\\sum_{h=0}^{H-1}\\!\\left(\\frac{1}{n_{j}}\\sum_{i\\in\\mathcal{C}_{j}}\\nabla F_{i}\\left(\\pmb{x}_{i,h}^{t,e},\\xi_{i,h}^{t,e}\\right)-\\nabla F_{i}\\left(\\pmb{x}_{i,h}^{t,e},\\xi_{i,h}^{t,e}\\right)\\right)\\,.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "To establish an upper bound for $\\begin{array}{r}{\\sum_{e=0}^{E-1}\\frac{1}{N}\\sum_{j=1}^{N}Z_{j}^{t,e+1}}\\end{array}$ , we start with bounding $Z_{j}^{t,e}$ as follows ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{Z}_{n}^{\\varepsilon,-1}=\\frac{1}{n}\\displaystyle\\sum_{i,j\\in\\{j,\\ell\\}}\\left\\lbrace\\frac{\\mathrm{t}_{n}^{\\varepsilon}}{\\|u_{k}^{2}\\|_{\\infty}^{2}}\\left(\\frac{1}{n}\\sum_{u=0}^{n}\\mathrm{Cr}\\left(\\varepsilon_{u,i}^{\\varepsilon}\\varepsilon_{u,i}^{\\varepsilon}\\right)-\\nabla F_{u}\\left(\\varepsilon_{u,i}^{\\varepsilon}\\varepsilon_{u,i}^{\\varepsilon}\\right)\\right)+\\nabla F_{u}\\left(\\varepsilon_{u,i}^{\\varepsilon}\\varepsilon_{u,i}^{\\varepsilon}\\right)-\\nabla F_{u}\\left(\\varepsilon_{u,i}^{\\varepsilon}\\varepsilon_{u,i}^{\\varepsilon}\\right)\\right.}\\\\ &{\\left.+\\frac{1}{n}\\displaystyle\\sum_{u=0}^{n}\\mathrm{Cr}\\left(\\varepsilon_{u,i}^{\\varepsilon}\\left(n^{-1}\\right)^{-1}\\frac{1}{n^{\\frac{n}{2}}}\\nabla F_{u}\\left(\\varepsilon_{u,i}^{\\varepsilon}\\varepsilon_{u,i}^{\\varepsilon}\\right)-\\nabla F_{u}\\left(\\varepsilon_{u,i}^{\\varepsilon}\\varepsilon_{u,i}^{\\varepsilon}\\right)+\\frac{1}{n}\\displaystyle\\sum_{u=0}^{n}\\left(\\frac{1}{n}\\frac{\\nabla F_{u}^{\\varepsilon}}{\\|u_{k}^{2}\\|_{\\infty}^{2}}F_{u}\\left(\\varepsilon_{u,i}^{\\varepsilon}\\varepsilon_{u,i}^{\\varepsilon}\\right)\\right)\\right\\rbrace}\\\\ &{\\left.+\\frac{1}{n}\\displaystyle\\sum_{u=0}^{n}\\mathrm{Cr}\\left[\\nabla F_{u}\\left(\\varepsilon_{u,i}^{\\varepsilon}\\varepsilon_{u,i}^{\\varepsilon}\\right)-\\frac{1}{n}\\frac{\\mathrm{Cr}}{n}\\nabla F_{u}\\left(\\varepsilon_{u,i}^{\\varepsilon}\\varepsilon_{u,i}^{\\varepsilon}\\right)\\right]^{2}\\right.}\\\\ &{\\left.-\\frac{1}{n}\\displaystyle\\sum_{u=0}^{n}\\mathrm{Cr}\\left[\\frac{1}{n}\\frac{\\mathrm{Cr}}{n}\\right]\\left(\\nabla F_{u}\\left(\\varepsilon_{u,i}^{\\varepsilon}\\varepsilon_{u,i}^{\\varepsilon}\\right)-\\nabla F_{u}\\left(\\varepsilon_{u,i}^{\\varepsilon}\\varepsilon_{u,i}^{\\varepsilon}\\right)+\\nabla F_{u}\\left(\\varepsilon_{u,i}^{\\varepsilon}\\varepsilon_{u,i}^{\\varepsilon}\\right)\\right)\\right]^{2}}\\\\ &{\\left.+\\frac{1 \n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where the first inequality comes from Lemma F.1.1, the third inequality follows Lemma F.1.4 and Assumption 2, and the fourth inequality follows Assumption 1. Combining this bound with (34), we thus obtain Lemma F.2.4, i.e., ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\sum_{e=0}^{E-1}\\frac{1}{N}\\sum_{j=1}^{N}Z_{j}^{t,e}\\leq4L^{2}Q_{t}+4L^{2}\\sum_{e=0}^{E-1}\\frac{1}{N}\\sum_{j=1}^{N}\\Theta_{j}^{t,e}+2\\frac{E}{H}\\sigma^{2}+\\sigma^{2}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "F.3.5Proof of Lemma F.2.5 ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Part I $\\left(t\\geq1\\right)$ :As $\\textstyle\\sum_{i\\in{\\mathcal{C}}_{j}}z_{i}^{t,e}=\\mathbf{0}$ the updating rle of ${\\boldsymbol{y}}_{j}^{t+1}$ can be simplified as ", "page_idx": 28}, {"type": "equation", "text": "$$\ny_{j}^{t+1}=\\frac{1}{E H}\\sum_{\\tau=0}^{E-1}\\sum_{h=0}^{H-1}\\frac{1}{N}\\sum_{j=1}^{N}\\frac{1}{n_{j}}\\sum_{i\\in{\\mathcal{C}}_{j}}\\nabla F_{i}\\left(x_{i,h}^{t,\\tau},\\xi_{i,h}^{t,\\tau}\\right)-\\frac{1}{E H}\\sum_{\\tau=0}^{E-1}\\sum_{h=0}^{H-1}\\frac{1}{n_{j}}\\sum_{i\\in{\\mathcal{C}}_{j}}\\nabla F_{i}\\left(x_{i,h}^{t,\\tau},\\xi_{i,h}^{t,\\tau}\\right).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "We next bound Yt1,e as follows ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{Y_{j}^{\\nu+1,e}=\\frac{1}{N}\\displaystyle\\sum_{j=1}^{N}\\|y\\|_{\\mathcal{Y}_{j}}^{\\nu+1}+\\nabla f_{j}\\left(\\bar{\\alpha}^{\\nu+1,e}\\right)-\\nabla f\\left(\\bar{\\alpha}^{\\nu+1,e}\\right)\\|^{2}}\\\\ &{\\qquad=\\displaystyle\\frac{1}{N}\\displaystyle\\sum_{j=1}^{N}\\left\\|\\frac{1}{\\mathcal{Z}}\\displaystyle\\frac{\\bar{\\gamma}^{\\nu-1,e-1}}{\\Theta I}\\displaystyle\\sum_{s=0}^{N}\\frac{1}{N}\\displaystyle\\frac{1}{b_{j}!}\\sum_{i=1}^{N}\\nabla F_{i}\\left(x_{i,\\nu}^{\\nu,\\varepsilon},\\bar{x}_{i,\\nu}^{\\nu,\\varepsilon}\\right)-\\frac{1}{E H}\\displaystyle\\sum_{r=0}^{E-1-1}\\displaystyle\\frac{1}{N}\\displaystyle\\sum_{i_{r}\\to s=0}^{1}\\nabla F_{i}\\left(x_{i,\\nu}^{\\varepsilon,\\prime},\\bar{x}_{i,\\nu}^{\\nu,\\varepsilon}\\right)\\right.}\\\\ &{\\qquad\\left.\\qquad+\\nabla f_{j}\\left(\\bar{\\alpha}^{\\nu+1,e}\\right)-\\nabla f\\left(\\bar{\\alpha}^{\\nu+1,e}\\right)\\right\\|^{2}}\\\\ &{\\qquad\\leq\\displaystyle\\frac{1}{N}\\displaystyle\\sum_{j=1}^{N}\\left\\|\\nabla f_{j}\\left(\\bar{\\alpha}^{\\nu+1,e}\\right)-\\frac{1}{E H}\\displaystyle\\sum_{r=0}^{E-1-1}\\displaystyle\\sum_{n=1}^{1}\\frac{1}{b_{j}!\\in\\mathcal{Y}_{j}}\\left(\\nabla F_{i}\\left(x_{i,\\nu}^{\\prime,\\prime},\\bar{x}_{i,\\nu}^{\\prime,\\varepsilon}\\right)\\mp\\nabla F_{i}\\left(x_{i,\\nu}^{\\prime,\\prime}\\right)\\right)\\right\\|^{2}}\\\\ &{\\qquad\\leq\\displaystyle\\frac{2}{N}\\displaystyle\\sum_{j=1}^{N}\\left\\|\\nabla f_{j}\\left(\\bar{\\alpha}^{\\nu+1,e}\\right)-\\frac{1}{E H}\\displaystyle\\sum_{r=0}^{N-1}\\displaystyle\\sum_{n=1}^{i-1}\\frac{1}{b_{j}!}\\sum_{i\\in\\mathcal{Y}_{j}}\\nabla F_{i}\\left(x_{i,\\nu}^{\\prime,\\prime}\\right)\\right\\|^{2}}\\\\ &{\\qquad\\qquad+\\left.\\displaystyle\\frac{2}{N E^{2}H^{2}}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\leq\\frac{2}{N E H}\\sum_{\\tau=0}^{E-1}\\sum_{h=0}^{H-1}\\sum_{j=1}^{N}\\mathbb{E}\\left\\|\\frac{1}{n_{j}}\\sum_{i\\in\\mathcal{C}_{j}}\\nabla F_{i}\\left(\\mathbf{x}_{i,h}^{t,\\tau}\\right)-\\nabla f_{j}\\left(\\hat{\\mathbf{x}}^{t+1,e}\\right)\\right\\|^{2}+\\frac{2}{E H}\\frac{1}{N}\\sum_{j=1}^{N}\\frac{1}{n_{j}}\\sigma^{2}}\\\\ {\\displaystyle\\leq\\frac{2}{N E H}\\sum_{\\tau=0}^{E-1}\\sum_{h=0}^{H-1}\\sum_{j=1}^{N}\\frac{1}{n_{j}}\\sum_{i\\in\\mathcal{C}_{j}}\\mathbb{E}\\left\\|\\nabla F_{i}\\left(\\mathbf{x}_{i,h}^{t,\\tau}\\right)-\\nabla F_{i}\\left(\\hat{\\mathbf{x}}^{t+1,e}\\right)\\right\\|^{2}+\\frac{2}{E H}\\frac{1}{N}\\sum_{j=1}^{N}\\frac{1}{n_{j}}\\sigma^{2}}\\\\ {\\displaystyle\\leq\\frac{2L^{2}}{N E H}\\sum_{\\tau=0}^{E-1}\\sum_{h=0}^{H-1}\\sum_{j=1}^{N}\\frac{1}{n_{j}}\\sum_{i\\in\\mathcal{C}_{j}}\\mathbb{E}\\left\\|\\mathbf{x}_{i,h}^{t,\\tau}-\\hat{\\mathbf{x}}^{t+1,e}\\right\\|^{2}+\\frac{2}{E H}\\frac{1}{N}\\sum_{j=1}^{N}\\frac{1}{n_{j}}\\sigma^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where the first inequality holds due to Lemma F.1.1, the second inequality follows Lemmas F.1.1 and F.1.3, the third inequality follows Lemmas F.1.1 and F.1.4 and Assumption 2, the fourth inequality follows Lemma F.1.1, and the final one comes from Assumption 1. ", "page_idx": 28}, {"type": "text", "text": "Given the above inequality, it follows that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\sum_{e=0}^{E-1}\\frac{1}{N}\\sum_{j=1}^{N}Y_{j}^{t+1,e}\\le2L^{2}\\sum_{e=0}^{E-1}\\frac{1}{N E H}\\sum_{\\tau=0}^{E-1}\\sum_{h=0}^{H-1}\\sum_{j=1}^{N}\\frac{1}{n_{j}}\\sum_{i\\in C_{j}}\\mathbb{E}\\left\\lVert x_{i,h}^{t,\\tau}-\\hat{x}^{t+1,e}\\right\\rVert^{2}+\\frac{2}{H}\\frac{1}{N}\\sum_{j=1}^{N}\\frac{1}{n_{j}}\\sigma^{2}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Additionally, ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left\\|x_{i,h}^{t,\\tau}-\\hat{x}^{t+1,e}\\right\\|^{2}\\leq\\mathbb{E}\\left\\|x_{i,h}^{t,\\tau}\\mp\\bar{x}_{j}^{t,\\tau}\\mp\\hat{x}^{t,\\tau}\\mp\\bar{x}^{t+1}-\\hat{x}^{t+1,e}\\right\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad\\leq4\\mathbb{E}\\left\\|x_{i,h}^{t,\\tau}-\\bar{x}_{j}^{t,\\tau}\\right\\|^{2}+4\\mathbb{E}\\left\\|\\bar{x}_{j}^{t,\\tau}-\\hat{x}^{t,\\tau}\\right\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\,4\\mathbb{E}\\left\\|\\hat{\\pmb{x}}^{t,\\tau}-\\bar{\\pmb{x}}^{t+1}\\right\\|^{2}+4\\mathbb{E}\\left\\|\\bar{\\pmb{x}}^{t+1}-\\hat{\\pmb{x}}^{t+1,e}\\right\\|^{2},}\\\\ &{\\mathrm{~t~we~need~further~to~bound~}\\mathbb{E}\\left\\|\\hat{\\pmb{x}}^{t,\\tau}-\\bar{\\pmb{x}}^{t+1}\\right\\|^{2}\\mathrm{~and~}\\mathbb{E}\\left\\|\\bar{\\pmb{x}}^{t+1}-\\hat{\\pmb{x}}^{t+1,e}\\right\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "which implies tha ", "page_idx": 28}, {"type": "text", "text": "Under the framework of MTGC, the virtual global model obeys the following iteration: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\hat{\\pmb{x}}^{t+1,e}=\\bar{\\pmb{x}}^{t+1}-\\gamma\\sum_{j=1}^{N}\\frac{1}{n_{j}}\\sum_{i\\in\\mathcal{C}_{j}}\\sum_{\\tau=0}^{e-1}\\sum_{h=0}^{H-1}\\left(\\nabla F_{i}\\left(\\pmb{x}_{i,h}^{t+1,\\tau},\\xi_{i,h}^{t+1,\\tau}\\right)+z_{i}^{t+1,\\tau}+y_{j}^{t+1}\\right).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "$\\textstyle\\sum_{i\\in{\\mathcal{C}}_{j}}z_{i}^{t+1,\\tau}=\\mathbf{0}$ $\\begin{array}{r}{\\sum_{j=1}^{N}y_{j}^{t+1}=\\mathbf{0}}\\end{array}$ ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\hat{\\pmb{x}}^{t+1,e}=\\bar{\\pmb{x}}^{t+1}-\\gamma\\sum_{j=1}^{N}\\frac{1}{n_{j}}\\sum_{i\\in{\\mathcal{C}}_{j}}\\sum_{\\tau=0}^{e-1}\\sum_{h=0}^{H-1}\\nabla F_{i}\\left(\\pmb{x}_{i,h}^{t+1,\\tau},\\xi_{i,h}^{t+1,\\tau}\\right).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "For $\\mathbb{E}\\left\\|\\bar{\\pmb{x}}^{t+1}-\\hat{\\pmb{x}}^{t+1,e}\\right\\|^{2}$ , we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad-\\nu=\\frac{1}{2}\\left[\\begin{array}{l}{1}\\\\ {0}\\\\ {\\sqrt{\\nu}}\\\\ {0}\\\\ {\\left|\\frac{1}{2}\\frac{1}{\\sqrt{\\nu}}\\frac{1}{\\nu}\\frac{1}{\\nu}\\frac{1}{\\nu}\\frac{1}{\\nu}\\sum_{i=1}^{2}\\mathrm{e}^{-\\mathrm{i}k j}\\left(E(E(E)^{2},\\langle\\nu^{i}|\\,\\hat{\\nu}^{i}\\rangle)+\\frac{1}{2}\\frac{\\nu}{\\sqrt{\\nu}}\\frac{1}{\\nu}\\frac{1}{\\nu}\\frac{1}{\\nu}\\frac{1}{\\nu}\\frac{1}{\\nu}\\frac{1}{\\nu}\\frac{1}{\\nu}\\frac{1}{\\nu}\\frac{1}{\\nu}\\psi(E)^{2}\\right)\\right|^{2}\\right.}\\\\ &{\\quad\\left.+2\\nu^{2}\\left[\\left(\\frac{1}{2}\\frac{1}{\\sqrt{\\nu}}\\frac{1}{\\nu}\\frac{1}{\\nu}\\frac{1}{\\nu}\\frac{1}{\\nu}\\frac{1}{\\nu}\\frac{1}{\\nu}\\frac{1}{\\nu}\\frac{1}{\\nu}\\psi(E)^{2}+\\frac{2}{3}\\frac{1}{\\nu}\\frac{1}{\\nu}\\frac{1}{\\nu}\\frac{1}{\\nu}\\frac{1}{\\nu}\\frac{1}{\\nu}\\frac{1}{\\nu}\\frac{1}{\\nu}\\frac{1}{\\nu}\\frac{1}{\\nu}\\frac{1}{\\nu}\\frac{1}{\\nu}\\frac{1}{\\nu}\\psi(E)^{2}\\right)\\right]^{2}\\right.}\\\\ &{\\quad\\left.+2\\nu^{2}\\left[\\left(\\frac{1}{2}\\frac{1}{\\sqrt{\\nu}}\\frac{1}{\\nu}\\frac{1}{\\nu}\\frac{1}{\\nu}\\frac{1}{\\nu}\\frac{1}{\\nu}\\frac{1}{\\nu}\\frac{1}{\\nu}\\frac{1}{\\nu}\\frac{1}{\\nu}\\frac{1}{\\nu}\\frac{1}{\\nu}\\frac{1}{\\nu}\\frac{1}{\\nu}\\frac{1}{\\nu}\\frac{1}{\\nu}\\psi(E)^{2}\\right)\\right]^{2}\\right.}\\\\ &{\\quad\\left.-\\nu^{2}\\frac{1}{2}\\nu\\frac{1}{2}\\frac{1}{\\sqrt{\\nu}}\\frac{1}{\\nu}\\frac{1}{\\nu}\\frac{1}{\\nu}\\frac{1}{\\nu}\\frac{1}{\\nu}\\\n$$njieCj ", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where the second equality holds due to Lemma F.1.3, the second inequality follows Lemma F.1.4 and Assumption 2, the third inequality comes from Lemma F.1.1, the last inequality follows Lemma F.1.1 and Assumption 1. ", "page_idx": 29}, {"type": "text", "text": "Similarly, we can bound $\\mathbb{E}\\left\\|\\hat{\\mathbf{x}}^{t,\\tau}-\\bar{\\mathbf{x}}^{t+1}\\right\\|^{2}$ as ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}\\left\\|\\hat{\\mathbf{x}}^{t,\\tau}-\\bar{\\mathbf{x}}^{t+1}\\right\\|^{2}}\\\\ &{\\leq6\\gamma^{2}L^{2}E H\\displaystyle\\sum_{\\tau^{\\prime}=0}^{E-1{H-1}}\\displaystyle\\sum_{h=0}^{1}\\frac{1}{N}\\displaystyle\\sum_{j=1}^{N}\\!\\displaystyle\\sum_{n_{j}}\\mathbb{E}\\bigg\\|x_{i,h}^{t,\\tau^{\\prime}}\\!-\\!\\bar{x}_{j}^{t,\\tau^{\\prime}}\\bigg\\|^{2}\\!+\\!6\\gamma^{2}L^{2}E H^{2}\\displaystyle\\sum_{\\tau^{\\prime}=0}^{E-1}\\!\\frac{1}{N}\\!\\displaystyle\\sum_{j=1}^{N}\\mathbb{E}\\bigg\\|\\bar{x}_{j}^{t,\\tau^{\\prime}}\\!-\\!\\hat{x}^{t,\\tau^{\\prime}}\\bigg\\|^{2}}\\\\ &{\\quad+\\,6\\gamma^{2}E H^{2}\\displaystyle\\sum_{\\tau^{\\prime}=0}^{E-1}\\mathbb{E}\\bigg\\|\\nabla f(\\hat{x}^{t,\\tau^{\\prime}})\\bigg\\|^{2}+2\\gamma^{2}\\displaystyle\\frac{E H}{N^{2}}\\displaystyle\\sum_{j=1}^{N}\\frac{1}{n_{j}}\\sigma^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Given (41) and (42), it follows that ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}\\left\\|\\hat{\\mathbf{x}}^{t,\\tau}-\\bar{\\mathbf{x}}^{t+1}\\right\\|^{2}+\\mathbb{E}\\left\\|\\bar{\\mathbf{x}}^{t+1}-\\hat{\\mathbf{x}}^{t+1,e}\\right\\|^{2}}\\\\ &{\\leq\\!6\\gamma^{2}L^{2}E H^{2}Q_{t+1}+6\\gamma^{2}L^{2}E H^{2}D_{t+1}+6\\gamma^{2}E H^{2}\\displaystyle\\sum_{\\tau=0}^{E-1}\\mathbb{E}\\left\\|\\nabla f(\\hat{\\mathbf{x}}^{t+1,\\tau})\\right\\|^{2}+2\\gamma^{2}\\frac{E H}{N^{2}}\\displaystyle\\sum_{j=1}^{N}\\frac{1}{n_{j}}\\sigma^{2}}\\\\ &{\\quad+\\,6\\gamma^{2}L^{2}E H^{2}Q_{t}+6\\gamma^{2}L^{2}E H^{2}D_{t}+6\\gamma^{2}E H^{2}\\displaystyle\\sum_{\\tau=0}^{E-1}\\mathbb{E}\\left\\|\\nabla f(\\hat{\\mathbf{x}}^{t,\\tau})\\right\\|^{2}+2\\gamma^{2}\\frac{E H}{N^{2}}\\displaystyle\\sum_{j=1}^{N}\\frac{1}{n_{j}}\\sigma^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Combining (39) and (40), we can obtain ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\displaystyle\\sum_{\\epsilon=0}^{E-1}\\displaystyle\\frac{1}{N}\\sum_{j=1}^{N}Y_{j}^{t+1,e}}\\\\ &{\\leq2L^{2}\\displaystyle\\sum_{\\epsilon=0}^{E-1}\\displaystyle\\frac{1}{N E H}\\displaystyle\\sum_{\\tau=0}^{E-1}\\sum_{h=0}^{H-1}\\sum_{j=1}^{N}\\displaystyle\\frac{1}{n_{j}}\\sum_{i\\in\\mathcal{C}_{j}}\\mathbb{E}\\left\\|x_{i,h}^{t,\\tau}-\\hat{x}^{t+1,e}\\right\\|^{2}+\\displaystyle\\frac{2}{H}\\displaystyle\\frac{1}{N}\\sum_{j=1}^{N}\\displaystyle\\frac{1}{n_{j}}\\sigma^{2}}\\\\ &{\\leq8L^{2}\\displaystyle\\frac{1}{N H}\\displaystyle\\sum_{\\tau=0}^{E-1\\,t-1}\\sum_{h=0}^{N}\\sum_{j=1}^{N}\\displaystyle\\frac{1}{n_{j}}\\sum_{i\\in\\mathcal{C}_{j}}\\mathbb{E}\\left\\|x_{i,h}^{t,\\tau}-\\bar{x}_{j}^{t,\\tau}\\right\\|^{2}+8L^{2}\\displaystyle\\frac{1}{N}\\sum_{\\tau=0}^{E-1}\\sum_{j=1}^{N}\\mathbb{E}\\left\\|\\bar{x}_{j}^{t,\\tau}-\\hat{x}^{t,\\tau}\\right\\|^{2}}\\\\ &{\\qquad+8L^{2}\\displaystyle\\sum_{\\tau=0}^{E-1}\\mathbb{E}\\left\\|\\hat{x}^{t,\\tau}-\\bar{x}^{t+1}\\right\\|^{2}+8L^{2}\\displaystyle\\sum_{\\epsilon=0}^{E-1}\\mathbb{E}\\left\\|\\bar{x}^{t+1}-\\hat{x}^{t+1,e}\\right\\|^{2}+\\displaystyle\\frac{2}{H}\\displaystyle\\frac{1}{N}\\sum_{j=1}^{N}\\displaystyle\\frac{1}{n_{j}}\\sigma^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Plugging (43) into (44), we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\displaystyle\\sum_{e=0}^{E-1}\\displaystyle\\frac{1}{N}\\displaystyle\\sum_{j=1}^{N}\\!Y_{j}^{t+1,e}\\!\\le8L^{2}Q_{t}+8L^{2}D_{t}+32\\gamma^{2}L^{2}E^{2}H\\displaystyle\\frac{1}{N^{2}}\\displaystyle\\sum_{j=1}^{N}\\displaystyle\\frac{1}{n_{j}}\\sigma^{2}+\\displaystyle\\frac{2}{H}\\displaystyle\\frac{1}{N}\\displaystyle\\sum_{j=1}^{N}\\displaystyle\\frac{1}{n_{j}}\\sigma^{2}}}\\\\ {{\\displaystyle+48\\gamma^{2}L^{4}E^{2}H^{2}Q_{t}+48\\gamma^{2}L^{4}E^{2}H^{2}D_{t}+48\\gamma^{2}L^{2}E^{2}H^{2}\\displaystyle\\sum_{\\tau=0}^{E-1}\\mathbb{E}\\left\\lVert\\nabla f(\\hat{\\pmb{x}}^{t,\\tau})\\right\\rVert^{2}}}\\\\ {{\\displaystyle+48\\gamma^{2}L^{4}E^{2}H^{2}Q_{t+1}\\!+\\!48\\gamma^{2}L^{4}E^{2}H^{2}D_{t+1}\\!+\\!48\\gamma^{2}L^{2}E^{2}H^{2}\\displaystyle\\sum_{\\tau=0}^{E-1}\\!\\mathbb{E}\\left\\lVert\\nabla f(\\hat{\\pmb{x}}^{t+1,\\tau})\\right\\rVert^{2}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Part II $(t=0_{\\mathrm{.}}$ 0: On the other hand, when $t=0$ ,wehave ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\displaystyle\\sum_{c=0}^{E-1}\\displaystyle\\frac{1}{N}\\sum_{j=1}^{N}Y_{j}^{0,c}}\\\\ &{=\\displaystyle\\sum_{c=0}^{E-1}\\displaystyle\\frac{1}{N}\\sum_{j=1}^{N}\\mathbb{E}\\left\\|-\\frac{1}{n_{j}}\\sum_{i\\in C_{j}}\\nabla F_{i}(\\bar{x}^{0},\\xi_{i,0}^{0})+\\frac{1}{N}\\displaystyle\\sum_{j=1}^{N}\\displaystyle\\frac{1}{n_{j}}\\sum_{i\\in C_{j}}\\nabla F_{i}(\\bar{x}^{0},\\xi_{i,0}^{0})+\\nabla f_{j}\\left(\\bar{x}^{0,c}\\right)-\\nabla f\\left(\\bar{x}^{0,c}\\right)\\right\\|^{2}}\\\\ &{\\le\\displaystyle\\sum_{c=0}^{E-1}\\displaystyle\\frac{1}{N}\\sum_{j=1}^{N}\\mathbb{E}\\left\\|-\\displaystyle\\frac{1}{n_{j}}\\sum_{i\\in C_{j}}\\nabla F_{i}(\\bar{x}^{0},\\xi_{i,0}^{0})+\\nabla f_{j}\\left(\\bar{x}^{0}\\right)+\\displaystyle\\frac{1}{N}\\displaystyle\\sum_{j=1}^{N}\\displaystyle\\frac{1}{n_{j}}\\sum_{i\\in C_{j}}\\nabla F_{i}(\\bar{x}^{0},\\xi_{i,0}^{0})-\\nabla f\\left(\\bar{x}^{0}\\right)\\right\\|^{2}}\\\\ &{\\quad+\\displaystyle\\sum_{c=0}^{E-1}\\displaystyle\\frac{1}{N}\\displaystyle\\sum_{j=1}^{N}\\mathbb{E}\\left\\|\\nabla f_{j}\\left(\\bar{x}^{0,c}\\right)-\\nabla f_{j}\\left(\\bar{x}^{0}\\right)-\\nabla f\\left(\\bar{x}^{0,c}\\right)+\\nabla f\\left(\\bar{x}^{0}\\right)\\right\\|^{2}}\\\\ &{\\le\\displaystyle\\mathbb{E}\\displaystyle\\frac{1}{N}\\displaystyle\\sum_{j=1}^{N}\\displaystyle\\frac{1}{n_{j}}\\sigma^{2}+L^{2}\\displaystyle\\sum_{c=0}^{E-1}\\mathbb{E}\\left\\|\\bar{x}^{0,c}-\\bar{x}^{0}\\right\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Similar to (41), we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\sum_{e=0}^{E-1}\\mathbb{E}\\left\\|\\hat{\\mathbf{x}}^{0,e}-\\bar{\\mathbf{x}}^{0}\\right\\|^{2}}\\\\ {\\displaystyle\\leq6\\gamma^{2}L^{2}E^{2}H\\displaystyle\\sum_{\\tau=0}^{E-1H-1}\\sum_{h=0}^{N}\\displaystyle\\sum_{N}^{1}\\!\\sum_{j=1}^{1}\\!\\sum_{n_{j}}\\!\\sum_{i\\in\\mathcal{I}_{j}}\\!\\mathbb{E}\\big\\|x_{i,h}^{0,\\tau}\\!-\\!\\bar{x}_{j}^{0,\\tau}\\big\\|^{2}\\!+\\!6\\gamma^{2}L^{2}E^{2}H^{2}\\displaystyle\\sum_{\\tau=0}^{E-1}\\!\\frac{1}{N}\\!\\sum_{j=1}^{N}\\!\\mathbb{E}\\big\\|\\bar{x}_{j}^{0,\\tau}\\!-\\!\\bar{x}^{0,\\tau}\\big\\|^{2}}\\\\ {\\displaystyle\\quad+\\,6\\gamma^{2}E^{2}H^{2}\\displaystyle\\sum_{\\tau=0}^{E-1}\\mathbb{E}\\left\\|\\nabla f(\\hat{x}^{0,\\tau})\\right\\|^{2}+2\\gamma^{2}\\frac{E^{2}H}{N^{2}}\\displaystyle\\sum_{j=1}^{N}\\frac{1}{n_{j}}\\sigma^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Combining the above two inequalities gives rise to ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{e=0}^{E-1}\\frac{1}{N}\\sum_{j=1}^{N}Y_{j}^{0,e}\\leq\\ensuremath{E}\\frac{1}{N}\\sum_{j=1}^{N}\\frac{1}{n_{j}}\\sigma^{2}+6\\gamma^{2}L^{4}\\ensuremath{E^{2}}H^{2}\\left(Q_{0}+D_{0}\\right)+6\\gamma^{2}L^{2}\\ensuremath{E^{2}}H^{2}\\sum_{e=0}^{E-1}\\ensuremath{\\mathbb{E}\\left\\lVert\\nabla f(\\hat{x}^{0,e})\\right\\rVert^{2}}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad+\\;2\\gamma^{2}L^{2}\\frac{E^{2}H}{N^{2}}\\sum_{j=1}^{N}\\frac{1}{n_{j}}\\sigma^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "To this end, we complete the proof of Lemma F.2.5. ", "page_idx": 31}, {"type": "text", "text": "F.3.6Proof of Lemma F.2.7 ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Part i $\\left(t\\geq1\\right)$ $\\begin{array}{r}{\\sum_{e=0}^{E-1}\\frac{1}{N}\\sum_{j=1}^{N}\\Theta_{j}^{t,e}}\\end{array}$ F.2.6 (i.e., (18)), and then plugging it into (13), we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{Q_{t}\\leq24\\gamma^{2}H^{2}L^{2}D_{t}+12\\gamma^{2}H^{2}\\Bigg\\{4L^{2}Q_{t}+4L^{2}\\Bigg(8\\gamma^{2}H^{2}L^{2}Q_{t}+8\\gamma^{2}H^{2}L^{2}D_{t}+8\\gamma^{2}H^{2}\\displaystyle\\sum_{\\epsilon=0}^{E-1}\\frac{1}{N_{f}\\sum_{\\epsilon=1}^{N}}\\gamma_{f,\\epsilon}^{\\epsilon,\\epsilon}}\\\\ &{\\qquad+8\\gamma^{2}H^{2}\\displaystyle\\sum_{\\epsilon=0}^{E-1}\\mathbb{E}\\Big\\{\\nabla f\\left(\\frac{a^{\\epsilon,\\epsilon}}{N}\\right)\\Big\\Vert^{2}+2\\gamma^{2}E H\\frac{1}{N_{b}}\\sum_{\\epsilon=1}^{N}\\frac{1}{n_{b}}\\sigma^{2}\\Bigg\\}+2\\frac{E}{H}\\sigma^{2}+\\sigma^{2}\\Bigg\\}}\\\\ &{\\qquad+12\\gamma^{2}H^{2}\\displaystyle\\sum_{\\epsilon=0}^{E-1}\\frac{1}{N_{f}\\sum_{\\epsilon=1}^{N}}\\gamma_{f,\\epsilon}^{\\epsilon,\\epsilon}+2\\gamma^{2}H^{2}\\displaystyle\\sum_{\\epsilon=0}^{E-1}\\mathbb{E}\\Big\\{\\nabla f\\left(\\frac{a^{\\epsilon,\\epsilon}}{c^{\\epsilon}}\\right)\\Big\\Vert^{2}+3E H\\gamma^{2}\\sigma^{2}}\\\\ &{=\\left(48\\gamma^{2}H^{2}L^{2}+384\\gamma^{4}H^{2}L^{4}\\right)Q_{t}+\\left(24\\gamma^{2}H^{2}L^{2}+384\\gamma^{4}H^{4}L^{4}\\right)D_{t}}\\\\ &{\\qquad+\\left(384\\gamma^{4}H^{4}L^{2}+24\\gamma^{2}H^{2}\\right)\\displaystyle\\sum_{\\epsilon=0}^{E-1}\\mathbb{E}\\Big\\{\\Big\\lvert\\nabla f\\left(\\frac{a^{\\epsilon,\\epsilon}}{c^{\\epsilon}}\\right)\\Big\\rvert^{2}+\\left(384\\gamma^{4}H^{4}L^{2}+12\\gamma^{2}H^{2}\\right)\\displaystyle\\sum_{\\epsilon=0}^{E-1}\\frac{1}{N_{f}\\sum_{\\epsilon=1}^{N}}\\gamma_{f,\\epsilon}^{\\epsilon,\\epsilon}}\\\\ &{\\qquad+96\\gamma^{4}E H^{3}L^{2}\\displaystyle\\sum_{\\epsilon= \n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Summing (45) with the inequality established in Lemma F.2.3 and utilizing the upper bound of E-1 \u2265=1 yt,e established in Lemma F.2.5, we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{Q_{t}+D_{t}\\leq\\left(48^{\\gamma}2^{H^{2}L^{2}}384\\gamma^{4}H^{2}L^{4}+24\\gamma^{2}E^{2}H^{2}L^{2}\\right)Q_{t}+\\left(24\\gamma^{2}H^{2}L^{2}+384\\gamma^{4}H^{4}L^{4}\\right)D_{t}}}\\\\ &{+\\left(384\\gamma^{4}H^{4}L^{2}+24\\gamma^{2}H^{2}\\right)\\sum_{\\ell=0}^{E-1}\\mathbb{E}\\Vert\\nabla f\\left(\\hat{\\mu}^{\\ell\\,\\ell\\,\\ell}\\right)\\Vert^{2}}\\\\ &{+\\left(384\\gamma^{4}H^{4}L^{2}+12\\gamma^{2}H^{2}+12\\gamma^{2}E^{2}H^{2}\\right)\\Bigg\\{8(8L^{2}+48\\gamma^{2}L^{4}E^{2}H^{2})\\left(Q_{\\ell-1}+D_{\\ell-1}\\right)}\\\\ &{+48\\gamma^{2}L^{4}E^{2}H^{2}\\left(Q_{\\ell}+D_{\\ell}\\right)+48\\gamma^{2}L^{2}E^{2}H^{2}\\Bigg\\}\\left(8L^{2}+16\\gamma^{2}\\left(E^{4}\\hat{\\mu}^{\\ell-1}\\right)\\right)^{2}+\\mathbb{E}\\left\\Vert\\nabla f(\\hat{\\mu}^{\\ell\\,\\ell\\,\\ell})\\right\\Vert^{2}}\\\\ &{+38\\gamma^{2}L^{2}E^{2}H\\frac{1}{N^{2}}\\displaystyle\\sum_{j=1}^{N}\\frac{1}{n_{j}}\\sigma^{2}+\\frac{2}{H}\\displaystyle\\frac{1}{N}\\displaystyle\\sum_{j=1}^{N}\\frac{1}{n_{j}}\\sigma^{2}\\Bigg\\}}\\\\ &{+96\\gamma^{4}E^{2}H^{3}L_{\\gamma}^{2}\\displaystyle\\frac{1}{N}\\displaystyle\\sum_{j=1}^{N}\\frac{1}{n_{j}}\\sigma^{2}+27\\gamma^{2}E H\\sigma^{2}+12\\gamma^{2}H^{2}\\sigma^{2}+3\\gamma^{2}E^{3}H\\displaystyle\\frac{N-1}{N^{2}}\\sum_{j=1}^{N}\\frac{1}{n_{j}}\\sigma^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad Q_{t}+D_{t}}\\\\ &{\\leq\\left(48\\gamma^{2}H^{2}L^{2}+384\\gamma^{4}H^{4}L^{4}+24\\gamma^{2}E^{2}H^{2}L^{2}+\\left(384\\gamma^{4}H^{4}L^{2}+12\\gamma^{2}H^{2}+12\\gamma^{2}\\mathbb{Z}^{2}H^{2}\\right)\\times48\\gamma^{2}L^{4}E^{2}H^{2}\\right)}\\\\ &{\\quad\\times\\left(Q_{t}+D_{t}\\right)+\\left(384\\gamma^{4}H^{4}L^{2}+12\\gamma^{2}H^{2}+12\\gamma^{2}E^{2}H^{2}\\right)\\left(8L^{2}+48\\gamma^{2}L^{4}E^{2}H^{2}\\right)\\left(Q_{t-1}+D_{t-1}\\right)}\\\\ &{\\quad+\\left(384\\gamma^{4}H^{4}L^{2}+24\\gamma^{2}H^{2}+\\left(384\\gamma^{4}H^{4}L^{2}+12\\gamma^{2}H^{2}+12\\gamma^{2}E^{2}H^{2}\\right)\\times48\\gamma^{2}L^{2}E^{2}H^{2}\\right)}\\\\ &{\\quad\\times\\displaystyle\\sum_{\\tau=0}^{E-1}\\biggl(\\mathbb{E}\\left\\|\\nabla f(\\hat{x}^{t-1,\\tau},\\tau)\\right\\|^{2}+\\mathbb{E}\\left\\|\\nabla f(\\hat{x}^{t,\\tau})\\right\\|^{2}\\biggr)}\\\\ &{\\quad+\\left(384\\gamma^{4}H^{4}L^{2}+12\\gamma^{2}H^{2}+12\\gamma^{2}E^{2}H^{2}\\right)\\left\\{32\\gamma^{2}L^{2}E^{2}H\\displaystyle\\frac{1}{N^{2}}\\displaystyle\\sum_{j=1}^{N}\\displaystyle\\frac{1}{n_{j}}\\sigma^{2}+\\frac{2}{H}\\displaystyle\\frac{1}{N}\\displaystyle\\sum_{j=1}^{N}\\displaystyle\\frac{1}{n_{j}}\\sigma^{2}\\right\\}}\\\\ &{\\quad+96\\gamma^{4}E H^{3}L^{2}\\displaystyle\\frac{1}{N}\\displaystyle\\sum_{j=1}^{N}\\displaystyle\\frac{1}{n_{j}}\\sigma^{2}+27\\gamma^{2}E H\\sigma^{2}+12\\gamma^{2}H^{2}\\sigma^{2}+3\\gamma^{2}E^{3}H\\displaystyle\\frac\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "When $\\begin{array}{r}{\\gamma\\leq\\frac{1}{7E H L}}\\end{array}$ we have $48\\gamma^{2}L^{2}E^{2}H^{2}\\leq1$ and ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\left({1-\\left({60\\gamma^{2}H^{2}L^{2}+768\\gamma^{4}H^{4}L^{4}+36\\gamma^{2}E^{2}H^{2}L^{2}}\\right)}\\right)\\left({Q_{t}+D_{t}}\\right)}}\\\\ {{\\displaystyle\\le\\left({3840\\gamma^{4}H^{4}L^{4}+120\\gamma^{2}H^{2}L^{2}+120\\gamma^{2}E^{2}H^{2}L^{2}}\\right)\\left({Q_{t-1}+D_{t-1}}\\right)}}\\\\ {{\\displaystyle\\quad+\\left({768\\gamma^{4}H^{4}L^{2}+36\\gamma^{2}H^{2}+12\\gamma^{2}E^{2}H^{2}}\\right)\\sum_{\\tau=0}^{E-1}\\left({\\mathbb{E}}\\left\\|\\nabla f(\\hat{x}^{t-1,\\tau})\\right\\|^{2}+\\mathbb{E}\\left\\|\\nabla f(\\hat{x}^{t,\\tau})\\right\\|^{2}\\right)}}\\\\ {{\\displaystyle\\quad+\\left({384\\gamma^{4}H^{4}L^{2}+12\\gamma^{2}H^{2}+12\\gamma^{2}E^{2}H^{2}}\\right)\\frac{3}{H}\\frac{1}{N}\\sum_{j=1}^{N}\\frac{1}{n_{j}}\\sigma^{2}}}\\\\ {{\\displaystyle\\quad+96\\gamma^{4}E H^{3}L^{2}\\frac{1}{N}\\sum_{j=1}^{N}\\frac{1}{n_{j}}\\sigma^{2}\\!+\\!27\\gamma^{2}E H\\sigma^{2}\\!+\\!12\\gamma^{2}H^{2}\\sigma^{2}\\!+\\!3\\gamma^{2}E^{3}H\\frac{N-1}{N^{2}}\\!\\sum_{j=1}^{N}\\!\\frac{1}{n_{j}}\\sigma^{2}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "As $\\begin{array}{r}{\\gamma\\leq\\frac{1}{33E H L}}\\end{array}$ we have $\\begin{array}{r}{1-\\left(60\\gamma^{2}H^{2}L^{2}+768\\gamma^{4}H^{4}L^{4}+36\\gamma^{2}E^{2}H^{2}L^{2}\\right)\\geq\\frac{2}{3}}\\end{array}$ and $3840\\gamma^{4}H^{4}L^{4}+$ $\\scriptstyle120\\gamma^{2}H^{2}\\vec{L^{2}+120\\gamma^{2}E^{2}H^{2}L^{2}}\\,\\le\\,\\frac{1}{3}$ Hence, (46) can be simplified as ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{Q_{t}\\!+\\!D_{t}\\leq\\!\\displaystyle\\frac{1}{2}\\left(Q_{t-1}+D_{t-1}\\right)\\!+\\!\\left(1152\\gamma^{4}H^{4}L^{2}\\!+\\!72\\gamma^{2}E^{2}H^{2}\\right)}\\\\ &{\\qquad\\qquad\\times\\displaystyle\\sum_{\\tau=0}^{E-1}\\left(\\mathbb{E}\\big\\lVert\\nabla f(\\hat{\\pmb{x}}^{t-1,\\tau})\\big\\rVert^{2}\\!+\\!\\mathbb{E}\\left\\lVert\\nabla f(\\hat{\\pmb{x}}^{t,\\tau})\\right\\rVert^{2}\\right)+\\phi(\\gamma,\\sigma^{2}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\phi(\\gamma,\\sigma^{2})=\\left(1728\\gamma^{4}H^{4}L^{2}+216\\gamma^{2}E^{2}H^{2}\\right)\\displaystyle\\frac{1}{H}\\displaystyle\\frac{1}{N}\\sum_{j=1}^{N}\\displaystyle\\frac{1}{n_{j}}\\sigma^{2}+144\\gamma^{4}E H^{3}L^{2}\\displaystyle\\frac{1}{N}\\sum_{j=1}^{N}\\displaystyle\\frac{1}{n_{j}}\\sigma^{2}}}\\\\ {{+42\\gamma^{2}E H\\sigma^{2}+18\\gamma^{2}H^{2}\\sigma^{2}+5\\gamma^{2}E^{3}H\\displaystyle\\frac{1}{N}\\displaystyle\\sum_{j=1}^{N}\\displaystyle\\frac{1}{n_{j}}\\sigma^{2}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Further, we establish an upper bound for $\\phi(\\gamma,\\sigma^{2})$ as follows, ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\phi(\\gamma,\\sigma^{2})=1728\\gamma^{4}H^{3}L^{2}\\frac{1}{\\bar{n}}\\sigma^{2}+144\\gamma^{4}\\frac{1}{\\bar{n}}E H^{3}L^{2}\\sigma^{2}+\\gamma^{2}\\sigma^{2}\\left\\{216\\frac{E^{2}H}{\\bar{n}}+42E H+18H^{2}+5\\frac{E^{3}H}{\\bar{n}}\\right\\}}\\\\ &{\\qquad\\qquad\\leq1728\\gamma^{4}E H^{3}L^{2}\\frac{1}{\\bar{n}}\\sigma^{2}+144\\gamma^{4}\\frac{1}{\\bar{n}}E H^{3}L^{2}\\sigma^{2}+281\\gamma^{2}E^{3}H\\sigma^{2}}\\\\ &{\\qquad\\qquad\\leq294\\gamma^{2}E^{3}H\\sigma^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where the lst inequltyholds due to $\\begin{array}{r}{\\frac1n=\\frac1N\\sum_{j=1}^{N}\\frac1{n_{j}}\\le1}\\end{array}$ and $\\begin{array}{r}{\\gamma\\leq\\frac{\\sqrt{\\bar{n}}E}{12H L}\\leq\\frac{1}{33E H L}}\\end{array}$ ", "page_idx": 32}, {"type": "text", "text": "Part II $(t=0)$ : Summing (45) with the inequality established in Lemma F.2.3 and utilizing the upper bound of E-11Yy, stabished in Lemma F2.5, we have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\Gamma_{0}\\leq\\left(48\\gamma^{2}H^{2}L^{2}+384\\gamma^{4}H^{4}L^{4}+24\\gamma^{2}E^{2}H^{2}L^{2}\\right)\\Gamma_{0}+\\left(384\\gamma^{4}H^{4}L^{2}+24\\gamma^{2}H^{2}\\right)\\underset{e=0}{\\overset{E^{-1}}{\\sum}}\\mathbb{E}\\|\\nabla f\\left(\\hat{x}^{0,e}\\right)\\|^{2}}\\\\ {\\quad+\\left(384\\gamma^{4}H^{4}L^{2}+12\\gamma^{2}H^{2}+12\\gamma^{2}E^{2}H^{2}\\right)\\left\\{E\\frac{1}{N}\\underset{j=1}{\\overset{N}{\\sum}}\\frac{1}{n_{j}}\\sigma^{2}+6\\gamma^{2}L^{4}E^{2}H^{2}\\Gamma_{1}\\right.}\\\\ {\\quad+6\\gamma^{2}L^{2}E^{2}H^{2}\\underset{e=0}{\\overset{E-1}{\\sum}}\\mathbb{E}\\|\\nabla f(\\hat{x}^{0,e})\\|^{2}+2\\gamma^{2}L^{2}\\frac{E^{2}H}{N^{2}}\\underset{j=1}{\\overset{N}{\\sum}}\\frac{1}{n_{j}}\\sigma^{2}\\right\\}}\\\\ {\\quad+96\\gamma^{4}E H^{3}L^{2}\\frac{1}{N}\\underset{j=1}{\\overset{N}{\\sum}}\\frac{1}{n_{j}}\\sigma^{2}\\!+\\!27\\gamma^{2}E H\\sigma^{2}\\!+\\!12\\gamma^{2}H^{2}\\sigma^{2}\\!+\\!3\\gamma^{2}E^{3}H\\frac{N}{N^{2}}\\underset{j=1}{\\overset{N}{\\sum}}\\frac{1}{n_{j}}\\sigma^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Reorganizing the above inequality gives rise to ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\mathrm{\\boldmath~\\hat{\\s}~}}_{0}\\le\\left(50\\gamma^{2}H^{2}L^{2}+432\\gamma^{4}H^{4}L^{4}+26\\gamma^{2}E^{2}H^{2}L^{2}\\right)\\Gamma_{0}+\\left(26\\gamma^{2}H^{2}+432\\gamma^{4}H^{4}L^{2}+2\\gamma^{2}E^{2}H^{2}\\right)\\sum_{e=0}^{E-1}\\mathbb{E}\\Big\\|\\nabla f\\left(\\hat{x}^{0}\\right)\\Big\\|_{2}^{2}}}\\\\ {{\\displaystyle\\qquad+\\left(384\\gamma^{4}H^{4}L^{2}+12\\gamma^{2}H^{2}+12\\gamma^{2}E^{2}H^{2}\\right)\\left\\{E\\frac{1}{N}\\sum_{j=1}^{N}\\frac{1}{n_{j}}\\sigma^{2}+2\\gamma^{2}L^{2}\\frac{E^{2}H}{N^{2}}\\sum_{j=1}^{N}\\frac{1}{n_{j}}\\sigma^{2}\\right\\}}}\\\\ {{\\displaystyle\\qquad+96\\gamma^{4}E H^{3}L^{2}\\frac{1}{N}\\sum_{j=1}^{N}\\frac{1}{n_{j}}\\sigma^{2}{+27\\gamma^{2}}E H\\sigma^{2}{+12\\gamma^{2}}H^{2}\\sigma^{2}{+3\\gamma^{2}}E^{3}H\\frac{1}{N}\\sum_{j=1}^{N}\\frac{1}{n_{j}}\\sigma^{2}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Following the same derivation as in Part I, we obtain ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\Gamma_{0}\\leq\\left(648\\gamma^{4}H^{4}L^{2}\\!+\\!42\\gamma^{2}E^{2}H^{2}\\right)\\sum_{e=0}^{E\\!-\\!1}\\!\\mathbb{E}\\Vert\\nabla f\\left(\\hat{\\pmb{x}}^{0,e}\\right)\\Vert^{2}+\\psi(\\gamma,\\sigma^{2}).\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\psi(\\gamma,\\sigma^{2})=\\left(1044\\gamma^{4}H^{4}L^{2}+72\\gamma^{2}E^{2}H^{2}\\right)E\\displaystyle\\frac{1}{N}\\sum_{j=1}^{N}\\frac{1}{n_{j}}\\sigma^{2}+144\\gamma^{4}E H^{3}L^{2}\\frac{1}{N}\\sum_{j=1}^{N}\\frac{1}{n_{j}}\\sigma^{2}}}\\\\ {{+42\\gamma^{2}E H\\sigma^{2}+18\\gamma^{2}H^{2}\\sigma^{2}+5\\gamma^{2}E^{3}H\\displaystyle\\frac{1}{N}\\sum_{j=1}^{N}\\frac{1}{n_{j}}\\sigma^{2}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Utiizing $\\begin{array}{r}{\\frac1{\\bar{n}}=\\frac{1}{N}\\sum_{j=1}^{N}\\frac{1}{n_{j}}\\leq1}\\end{array}$ and $\\begin{array}{r}{\\gamma\\leq\\frac{\\sqrt{\\bar{n}}E}{12H L}\\leq\\frac{1}{33E H L}}\\end{array}$ 33EHL, we can further bound (, o\u00b2) as ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\psi(\\gamma,\\sigma^{2})=\\left(1044\\gamma^{4}H^{4}L^{2}+72\\gamma^{2}E^{2}H^{2}\\right)E\\frac{1}{N}\\sum_{j=1}^{N}\\frac{1}{n_{j}}\\sigma^{2}+144\\gamma^{4}E H^{3}L^{2}\\frac{1}{N}\\sum_{j=1}^{N}\\frac{1}{n_{j}}\\sigma^{2}}\\\\ {+42\\gamma^{2}E H\\sigma^{2}+18\\gamma^{2}H^{2}\\sigma^{2}+5\\gamma^{2}E^{3}H\\frac{1}{N}\\sum_{j=1}^{N}\\frac{1}{n_{j}}\\sigma^{2}}\\\\ {=1044\\gamma^{4}E H^{4}L^{2}\\frac{1}{\\bar{n}}\\sigma^{2}+144\\gamma^{4}L^{2}E H^{3}\\frac{1}{\\bar{n}}\\sigma^{2}+\\gamma^{2}\\sigma^{2}\\left\\{72\\frac{E^{3}H^{2}}{\\bar{n}}+42E H+18H^{2}+5\\frac{E^{3}H}{\\bar{n}}\\right\\}}\\\\ {\\leq146\\gamma^{2}E^{3}H^{2}\\sigma^{2}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "To this end, we complete the proof of Lemma F.2.7. ", "page_idx": 33}, {"type": "text", "text": "G  Recovering SCAFFOLD's Results ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "By comparing the communication complexity $T$ required to achieve a $\\epsilon$ -stationary solution, we can see that our result recovers that of SCAFFOLD when $N=1$ and $E=1$ . Specifically, for our MTGC algorithm, to achieve an $\\epsilon$ error bound, according to Corollary 4.1, we can find a $T$ to satisfy ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\sqrt{\\frac{{\\mathcal{F}}_{0}L\\sigma^{2}}{\\tilde{N}T E H}}\\le\\frac{\\epsilon}{3},\\ \\ \\left(\\frac{{\\mathcal{F}}_{0}L\\sigma}{T}\\right)^{\\frac{2}{3}}\\le\\frac{\\epsilon}{3},\\ \\ \\frac{L{\\mathcal{F}}_{0}}{T}\\le\\frac{\\epsilon}{3}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Equivalently, $\\begin{array}{r}{T\\ge\\frac{L\\sigma^{2}\\mathcal{F}_{0}}{\\tilde{N}E H\\epsilon^{2}}}\\end{array}$ \uff0c $\\begin{array}{r}{T\\ge\\frac{L\\sigma\\mathcal{F}_{0}}{(\\epsilon)^{\\frac{3}{2}}}}\\end{array}$ , and $\\begin{array}{r}{T\\ge\\frac{L\\mathcal{F}_{0}}{\\epsilon}}\\end{array}$ . In other words,the MTGC algorithm will have an expected error smaller than $\\epsilon$ if $T$ satisfies ", "page_idx": 34}, {"type": "equation", "text": "$$\nT=\\mathcal{O}\\left(\\frac{L\\sigma^{2}\\mathcal{F}_{0}}{\\tilde{N}E H\\epsilon^{2}}+\\frac{L\\sigma\\mathcal{F}_{0}}{(\\epsilon)^{\\frac{3}{2}}}+\\frac{L\\mathcal{F}_{0}}{\\epsilon}\\right).\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "According to Theorem $\\mathrm{II}$ of [18], to achieve the $\\epsilon$ error bound, the number of global communication rounds SCAFFOLD needs to take can be expressed as ", "page_idx": 34}, {"type": "equation", "text": "$$\nT=\\mathcal{O}\\left(\\frac{L\\sigma^{2}\\mathcal{F}_{0}}{n_{j}H\\epsilon^{2}}+\\frac{L\\mathcal{F}_{0}}{\\epsilon}\\right),\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where we have converted the notation from [18] to our notation ", "page_idx": 34}, {"type": "text", "text": "We see that the dominating term of the MTGC, $\\mathcal{O}\\left(\\frac{L\\sigma^{2}\\mathcal{F}_{0}}{\\tilde{N}E H\\epsilon^{2}}\\right)$ , recovers that of SCAFFOLD when $N=1$ (i.e., ${\\tilde{N}}=n_{j}\\,$ )and $E=1$ , which corresponds to the case of a single group with a single (global) aggregator. ", "page_idx": 34}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately refect the paper's contributions and scope? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: The abstract and introduction clearly state the claims made, including the contributions made in the paper. Additionally, the claims made in the abstract and introduction are supported by the theoretical analysis (see Sec. 4) and experiments (see Sec. 5). ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and refect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 35}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Justification: The limitation are included in Sec. 6. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "\u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u00b7 The authors should refect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 35}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: All the theorems, formulas, and proofs in the paper are numbered and crossreferenced. All assumptions are clearly stated in Sec. 4, and referenced in the statement of our theorem, i.e., Theorem 4.1. The proofs are provided in the supplemental material. Theorems and Lemmas that our proof relies upon are properly referenced. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 36}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: The paper discloses all necessary information to reproduce the main experimental results in Sec. 5. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 36}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: All the datasets used in this work are open-sourced while the information about the models and parameters are clearly reported in Sec. 5. For the convenience of reproducibility, the code is attached to our submission. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please see the NeurIPS code and data submission guidelines (https : //nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https : //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 37}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: The details about the setting of our experiments are reported in Sec. 5. Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u00b7 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 37}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: We report the standard deviations which are computed based on 3 random trials (see Sec. 5). ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 37}, {"type": "text", "text": "\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 38}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: We provide the information of the computer resources used for this work including the GPU type and memory information. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 38}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: Our work adheres to the NeurIPS Code of Ethics. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 38}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: There is no societal impact of the work performed. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: \u00b7 The answer NA means that there is no societal impact of the work performed. ", "page_idx": 38}, {"type": "text", "text": "\u00b7 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u00b7 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 39}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 39}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u00b7 The answer NA means that the paper poses no such risks.   \n\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 39}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properlyrespected? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: We credit the framework that our work is based on in Sec. 5. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not use existing assets.   \n\u00b7 The authors should cite the original paper that produced the code package or dataset.   \n\u00b7 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode . com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset's creators. ", "page_idx": 39}, {"type": "text", "text": "", "page_idx": 40}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 40}, {"type": "text", "text": "Justification: Our paper does not release new assets. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not release new assets.   \n\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose assetisused.   \n\u00b7 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 40}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 40}, {"type": "text", "text": "Justification: Our paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 40}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)were obtained? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 40}, {"type": "text", "text": "Justification: Our paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 40}, {"type": "text", "text": "", "page_idx": 41}]