[{"figure_path": "aCAb1qNXI0/figures/figures_1_1.jpg", "caption": "Figure 1: Illustration of multi-timescale gradient correction (MTGC) for multi-level non-i.i.d. in HFL.", "description": "This figure illustrates the hierarchical federated learning (HFL) architecture with multi-timescale gradient correction (MTGC).  The figure shows a central server (global aggregator) connected to multiple group aggregators, each responsible for aggregating models from a group of clients. Each client has its own local dataset, and non-i.i.d. data is present both within client groups (intra-group) and between groups (inter-group).  MTGC introduces two types of gradient corrections: client-group correction to reduce model drift among clients within a group and group-global correction to further reduce model drift across groups, addressing the multi-timescale model drift challenge.", "section": "1 Introduction"}, {"figure_path": "aCAb1qNXI0/figures/figures_3_1.jpg", "caption": "Figure 2: Visualization of the local update process using multi-timescale gradient correction (MTGC) with 4 clients and 2 groups. (a) Without any gradient correction (e.g., hierarchical FedAvg), each client model moves towards its respective optimal point, denoted by \u00e6. (b) When only client-group correction term z\u2081 is applied, the model of client i \u2208 Cj moves towards the group optimum. (c) In MTGC, the gradient of client i \u2208 Cj is adjusted by both the client-group correction term zi and the group-global correction variable yj, assisting each client model to converge towards the global optimum x* during local iterations.", "description": "This figure illustrates the local update process in hierarchical federated learning (HFL) scenarios with and without gradient correction. (a) shows the situation without gradient correction, where each client model converges towards its own local optimum. (b) demonstrates client-group correction, where the model updates are adjusted towards the group optimum. (c) showcases the multi-timescale gradient correction (MTGC) method proposed in the paper, which effectively corrects the client gradients towards both the group and global optima, leading to improved convergence.", "section": "2.2 Limitation of Existing Works"}, {"figure_path": "aCAb1qNXI0/figures/figures_4_1.jpg", "caption": "Figure 1: Illustration of multi-timescale gradient correction (MTGC) for multi-level non-i.i.d. in HFL.", "description": "This figure illustrates the hierarchical federated learning (HFL) architecture with multi-timescale gradient correction (MTGC).  The system comprises clients grouped into multiple groups, each coordinated by a group aggregator node.  The group aggregators, in turn, communicate with a central server. MTGC introduces coupled gradient correction terms to address model drift at different levels of the hierarchy (client-group and group-global corrections).  These corrections aim to improve model convergence in the presence of multi-level non-i.i.d. data by guiding the clients' model updates to align better with both group and global objectives. ", "section": "2 Background and Motivation"}, {"figure_path": "aCAb1qNXI0/figures/figures_7_1.jpg", "caption": "Figure 3: Comparison with FL baselines. In this experiment, popular FL algorithms are extended to the HFL setup for comparison with MTGC. We consider four datasets in the group non-i.i.d. & client non-i.i.d. setting. Experiments are conducted over 3 random trials. We see that MTGC obtains the best testing accuracy in each case, validating our multi-level approach for correcting multi-timescale model drifts.", "description": "The figure compares the performance of MTGC with several other popular federated learning (FL) algorithms (SCAFFOLD, FedProx, HFedAvg, and FedDyn) on four different datasets (EMNIST-Letters, Fashion-MNIST, CIFAR-10, and CIFAR-100). The experiments are conducted under a non-i.i.d. setting at both the group and client level.  The results show that MTGC consistently outperforms the other methods in terms of testing accuracy across all datasets, demonstrating its effectiveness in handling multi-timescale model drift in hierarchical federated learning (HFL) settings.", "section": "5.2 Results and Discussion"}, {"figure_path": "aCAb1qNXI0/figures/figures_7_2.jpg", "caption": "Figure 4: Comparison with gradient correction baselines. Three different data distribution scenarios are considered. We see that the local correction method is effective for handling client non-i.i.d. within each group (top row), while the group correction method is effective for handling non-i.i.d. across groups (middle row). MTGC obtains the most stable performance (all rows) by combining multiple correction levels.", "description": "This figure compares the performance of MTGC against baselines that use only client-level correction or only group-level correction.  Three different data heterogeneity scenarios are evaluated: group i.i.d. and client non-i.i.d.; group non-i.i.d. and client i.i.d.; and group non-i.i.d. and client non-i.i.d.  The results demonstrate that MTGC, by employing both client-level and group-level corrections, outperforms the baselines in all scenarios, achieving the most consistent and stable results.", "section": "5.2 Results and Discussion"}, {"figure_path": "aCAb1qNXI0/figures/figures_14_1.jpg", "caption": "Figure 4: Comparison with gradient correction baselines. Three different data distribution scenarios are considered. We see that the local correction method is effective for handling client non-i.i.d. within each group (top row), while the group correction method is effective for handling non-i.i.d. across groups (middle row). MTGC obtains the most stable performance (all rows) by combining multiple correction levels.", "description": "This figure compares the performance of MTGC against three baseline methods under different data heterogeneity scenarios: group i.i.d. & client non-i.i.d., group non-i.i.d. & client i.i.d., and group non-i.i.d. & client non-i.i.d.  Each row represents a scenario. The columns represent different datasets. The baseline methods are: HFedAvg (without correction), Local Correction (only correcting client-level drift), and Group Correction (only correcting group-level drift). MTGC combines both local and group corrections and is shown to be the most effective and stable across the board.", "section": "5 Experimental Results"}, {"figure_path": "aCAb1qNXI0/figures/figures_14_2.jpg", "caption": "Figure 4: Comparison with gradient correction baselines. Three different data distribution scenarios are considered. We see that the local correction method is effective for handling client non-i.i.d. within each group (top row), while the group correction method is effective for handling non-i.i.d. across groups (middle row). MTGC obtains the most stable performance (all rows) by combining multiple correction levels.", "description": "This figure compares the performance of MTGC against three baseline methods under three different data distribution scenarios: group i.i.d. & client non-i.i.d., group non-i.i.d. & client i.i.d., and group non-i.i.d. & client non-i.i.d.  The baseline methods are HFedAvg (without correction), HFedAvg with local correction, and HFedAvg with group correction.  The figure demonstrates that the local correction is effective for client-level non-i.i.d., the group correction for group-level non-i.i.d., and that MTGC combines these to achieve the most stable performance in all cases.", "section": "5.2 Results and Discussion"}, {"figure_path": "aCAb1qNXI0/figures/figures_15_1.jpg", "caption": "Figure 1: Illustration of multi-timescale gradient correction (MTGC) for multi-level non-i.i.d. in HFL.", "description": "This figure illustrates the multi-timescale gradient correction (MTGC) method for handling multi-level non-i.i.d. data in hierarchical federated learning (HFL). It shows a hierarchical architecture with a central server, group aggregators, and clients. The figure highlights the multi-timescale model drift occurring across different hierarchical levels and how MTGC corrects client model drift towards the group gradient and group gradient towards the global gradient.  Client-group correction and group-global correction terms are introduced to mitigate these drifts.", "section": "1 Introduction"}, {"figure_path": "aCAb1qNXI0/figures/figures_15_2.jpg", "caption": "Figure 4: Comparison with gradient correction baselines. Three different data distribution scenarios are considered. We see that the local correction method is effective for handling client non-i.i.d. within each group (top row), while the group correction method is effective for handling non-i.i.d. across groups (middle row). MTGC obtains the most stable performance (all rows) by combining multiple correction levels.", "description": "This figure compares the performance of MTGC against three baselines in three different data distribution scenarios: (i) group i.i.d. & client non-i.i.d., (ii) group non-i.i.d. & client i.i.d., and (iii) group non-i.i.d. & client non-i.i.d..  Each row represents one scenario.  The baselines are: HFedAvg (no correction), Local Correction (only client-group correction), and Group Correction (only group-global correction).  The figure shows that MTGC is most stable and provides the best accuracy by combining both client-group and group-global corrections.", "section": "5 Experimental Results"}, {"figure_path": "aCAb1qNXI0/figures/figures_16_1.jpg", "caption": "Figure 3: Comparison with FL baselines. In this experiment, popular FL algorithms are extended to the HFL setup for comparison with MTGC. We consider four datasets in the group non-i.i.d. & client non-i.i.d. setting. Experiments are conducted over 3 random trials. We see that MTGC obtains the best testing accuracy in each case, validating our multi-level approach for correcting multi-timescale model drifts.", "description": "This figure compares the performance of MTGC against four other popular Federated Learning (FL) algorithms (SCAFFOLD, FedProx, FedDyn, and HFedAvg) on four different datasets (EMNIST-Letters, Fashion-MNIST, CIFAR-10, and CIFAR-100).  The experiments simulate a hierarchical FL setting where data is non-identically distributed across both groups and individual clients, representing real-world challenges.  The results show that MTGC consistently achieves the highest testing accuracy, showcasing its effectiveness in handling multi-timescale model drift inherent in hierarchical federated learning.", "section": "5.2 Results and Discussion"}, {"figure_path": "aCAb1qNXI0/figures/figures_17_1.jpg", "caption": "Figure 1: Illustration of multi-timescale gradient correction (MTGC) for multi-level non-i.i.d. in HFL.", "description": "This figure illustrates the multi-timescale gradient correction (MTGC) method for handling multi-level non-i.i.d. data in hierarchical federated learning (HFL).  It shows a hierarchical architecture with a central server, group aggregators, and individual clients.  The key idea is to introduce coupled gradient correction terms at multiple levels to address client model drift (caused by local updates) and group model drift (caused by federated averaging within groups).  These corrections help each client's model converge towards a better global model, even with data heterogeneity across multiple levels.", "section": "Background and Motivation"}, {"figure_path": "aCAb1qNXI0/figures/figures_17_2.jpg", "caption": "Figure 1: Illustration of multi-timescale gradient correction (MTGC) for multi-level non-i.i.d. in HFL.", "description": "This figure illustrates the multi-timescale gradient correction (MTGC) methodology for handling multi-level non-i.i.d. data in hierarchical federated learning (HFL). It shows a hierarchical structure with a central server, group aggregators, and clients.  The figure highlights the concept of multi-timescale model drift occurring across multiple levels.  It also visually represents the key idea of MTGC, which introduces coupled gradient correction terms (client-group correction and group-global correction) to address this drift at different timescales.", "section": "1 Introduction"}, {"figure_path": "aCAb1qNXI0/figures/figures_17_3.jpg", "caption": "Figure 3: Comparison with FL baselines. In this experiment, popular FL algorithms are extended to the HFL setup for comparison with MTGC. We consider four datasets in the group non-i.i.d. & client non-i.i.d. setting. Experiments are conducted over 3 random trials. We see that MTGC obtains the best testing accuracy in each case, validating our multi-level approach for correcting multi-timescale model drifts.", "description": "This figure compares the performance of MTGC against four other federated learning (FL) algorithms (SCAFFOLD, FedProx, HFedAvg, and FedDyn) on four different datasets (EMNIST-Letters, Fashion-MNIST, CIFAR-10, and CIFAR-100).  Each algorithm was adapted for hierarchical federated learning (HFL). The results, averaged over three trials, show that MTGC achieves the highest testing accuracy on all datasets, highlighting its effectiveness in correcting multi-timescale model drifts in HFL.", "section": "5.2 Results and Discussion"}]