{"references": [{"fullname_first_author": "McMahan", "paper_title": "Communication-efficient learning of deep networks from decentralized data", "publication_date": "2017-00-00", "reason": "This paper introduced federated learning (FL), a foundational concept for the field, and its communication-efficient approach which is central to the current work."}, {"fullname_first_author": "Kairouz", "paper_title": "Advances and open problems in federated learning", "publication_date": "2021-00-00", "reason": "This survey paper provides a comprehensive overview of the FL field, highlighting key challenges and future directions, which are relevant to the hierarchical FL problem addressed in this paper."}, {"fullname_first_author": "Karimireddy", "paper_title": "SCAFFOLD: Stochastic controlled averaging for federated learning", "publication_date": "2020-00-00", "reason": "This paper proposed SCAFFOLD, a significant FL algorithm that addresses model drift effectively, inspiring the gradient correction methods used in the current work."}, {"fullname_first_author": "Li", "paper_title": "Federated optimization in heterogeneous networks", "publication_date": "2020-00-00", "reason": "This paper analyzes FL convergence under heterogeneous data distributions, a crucial consideration when extending FL to hierarchical settings as explored in this paper."}, {"fullname_first_author": "Wang", "paper_title": "Demystifying why local aggregation helps: Convergence analysis of hierarchical SGD", "publication_date": "2022-00-00", "reason": "This paper focuses on hierarchical SGD, providing relevant analysis for model drift in hierarchical structures, directly related to the multi-timescale problem studied in the current work."}]}