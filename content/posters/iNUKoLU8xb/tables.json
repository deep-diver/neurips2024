[{"figure_path": "iNUKoLU8xb/tables/tables_5_1.jpg", "caption": "Table 1: Comparison of different contrastive alignment objectives. Here we have C and C as constraint sets (denoted as B) defined in Equation (4) with their corresponding indicator function. \u201cIter\u201d refers to iterative methods.", "description": "This table summarizes the connections between the proposed Generalized Contrastive Alignment (GCA) framework and existing contrastive learning methods (INCE, RINCE, BYOL).  It shows how different choices for the divergence measures (dM, dr), constraint sets (B), and iterative algorithms (Iter) in the GCA formulation correspond to the specific objectives and algorithms of these existing methods.  The table highlights the flexibility of the GCA framework to encompass various contrastive learning approaches by adjusting these parameters.", "section": "4 Building Connections to Different CL Objectives"}, {"figure_path": "iNUKoLU8xb/tables/tables_8_1.jpg", "caption": "Table 2: Test accuracy (%) on a downstream classification task after pretraining. Results are provided for CIFAR-10 (ResNet18), CIFAR-100 (ResNet18), SVHN (ResNet50), and ImageNet100 (ResNet50) under standard and extreme (Ex) augmentation conditions (averaged over 5 seeds). The top model is bold and the second-place model is underlined. For INCE and RINCE, we also provide the improvement \u0394 by adding GCA to each method.", "description": "This table presents the test accuracy achieved on four different datasets (CIFAR-10, CIFAR-100, SVHN, and ImageNet100) after pretraining with various contrastive learning methods.  The results are shown for both standard and extreme augmentation conditions, highlighting the impact of different methods and augmentation strategies on downstream classification performance.  Improvements gained by incorporating the Generalized Contrastive Alignment (GCA) framework into existing methods (INCE and RINCE) are also indicated.", "section": "6.1 Comparison with CL Baselines"}]