[{"type": "text", "text": "Addressing Asynchronicity in Clinical Multimodal Fusion via Individualized Chest X-ray Generation ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Wenfang Yao1,\u2217 Chen Liu 1,3,\u2217 Kejing Yin 2,\u2020 William K. Cheung2, Jing Qin1 ", "page_idx": 0}, {"type": "text", "text": "1School of Nursing, The Hong Kong Polytechnic University 2Department of Computer Science, Hong Kong Baptist University 3School of Software Engineering, South China University of Technology ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Integrating multi-modal clinical data, such as electronic health records (EHR) and chest X-ray images (CXR), is particularly beneficial for clinical prediction tasks. However, in a temporal setting, multi-modal data are often inherently asynchronous. EHR can be continuously collected but CXR is generally taken with a much longer interval due to its high cost and radiation dose. When clinical prediction is needed, the last available CXR image might have been outdated, leading to suboptimal predictions. To address this challenge, we propose DDLCXR, a method that dynamically generates an up-to-date latent representation of the individualized CXR images. Our approach leverages latent diffusion models for patient-specific generation strategically conditioned on a previous CXR image and EHR time series, providing information regarding anatomical structures and disease progressions, respectively. In this way, the interaction across modalities could be better captured by the latent CXR generation process, ultimately improving the prediction performance. Experiments using MIMIC datasets show that the proposed model could effectively address asynchronicity in multimodal fusion and consistently outperform existing methods. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Clinical data in modern healthcare is documented through various complementary modalities [1, 2]. Electronic health records (EHRs), for instance, systematically record the progression of diseases over time, including medical histories, laboratory test results, and treatment outcomes [3\u20136]. In parallel, medical imaging, such as chest X-rays (CXRs), is valuable for providing visual insights into the patient\u2019s internal anatomy, organ functions, and potential abnormalities [7]. Recent studies have shown that strategic integration of multimodal clinical data could lead to improved performance for clinical predictions compared to relying solely on uni-modal data [8\u201313]. ", "page_idx": 0}, {"type": "text", "text": "Despite the promising results obtained, the inherent asynchronicity of multimodal clinical data still hinders effective integration. Take the intensive care unit (ICU) setting as an example, patients are subject to continuous monitoring systems that capture vital signs, including heart rate, blood pressure, and oxygen saturation, with this information being routinely recorded in the EHR [14, 15]. On the other hand, CXRs are captured only on an as-needed basis and often as less as possible, due to limitations of radiation dose and resources [16]. However, patients admitted to ICU are in life-threatening conditions, which means their medical status is prone to rapid changes and highly time-sensitive [17]. In the MIMIC-CXR dataset [18], it is observed that among patients with positive disease findings in their CXR, over $70\\%$ of subsequent CXR images \u2014 taken within a median interval of less than 24 hours \u2014 exhibit changes in CXR findings. This implies that when a clinical prediction is needed, CXRs captured even only a few hours ago could have become outdated, especially for ICU patients who commonly have respiratory, cardiac, infectious, and traumatic conditions [19]. Fig. 1 shows such an example of a real patient in the MIMIC dataset. ", "page_idx": 0}, {"type": "image", "img_path": "uCvdw0IOuU/tmp/90cf9f61e38bdeec26d765ee77c3410ae2d11d28d87a542d1ad3a256d9341dfd.jpg", "img_caption": ["Figure 1: A real ICU patient with rapid CXR changes. (a) Initial radiology findings: Low lung volumes but lungs are clear of consolidation or pulmonary vascular congestion. No acute cardiopulmonary process. (b) Radiology findings after 34 hours: Severe relatively symmetric bilateral pulmonary consolidation. (c) CXR generated by DDL-CXR given the initial CXR image shown in (a) and the EHR data within the 34 hours. Clear signs of bilateral pulmonary consolidation can be seen from the generated image. The visualization shows that DDL-CXR could generate updated CXR images that respect the anatomical structure of the patient and reflect the disease progression. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Motivation Existing works adopt the \u201ccarry-forward\" approach, i.e., using the last CXR image available for downstream prediction tasks [20, 10]. This strategy ignores the potential rapid changes between the prediction time and the time of the last CXR image taken and thus inevitably leads to suboptimal prediction performance. On the contrary, we hypothesize that generating an updated CXR image at the prediction time could mitigate the asynchronicity problem and enhance the prediction accuracy. Nevertheless, generating patient-specific CXR images presents unique challenges. While multimodal generation has been explored extensively in various fields, these methods are not readily adaptable for generating individualized CXR images. In domains such as text-to-audio [21] or text-to-image generation [22], the attributes that need to be controlled (e.g., painting style) can be explicitly defined in input modalities (e.g., the text prompt). However, in the clinical context, explicit descriptions of a patient\u2019s anatomical structures, organ functions, and disease progression, which are highly specific to individual patients and critical for downstream prediction, are not directly available. ", "page_idx": 1}, {"type": "text", "text": "Contribution To tackle the aforementioned challenge, we propose Diffusion-based Dynamic Latent Chest X-ray Image Generation (DDL-CXR)3, which utilizes a tailored latent diffusion model (LDM) [22] to generate individualized CXR images for clinical prediction. Specifically, DDL-CXR learns to generate representations in a latent space encoded by a variational auto-encoder (VAE). To incorporate detailed information about the patient\u2019s anatomical structure and organ specifics, we use a previous CXR image from the same patient as the reference image. To generate latent representations that align with the disease progression, we use a Transformer model [23] to encode the irregular EHR data spanning from the reference CXR to the prediction time. To further capture the implicit interactions between EHR and CXR, we use the encoded EHR representation to predict the labels of abnormality finding of the target image. To force the LDM to capture the disease course in the EHR data, we explore a contrastive learning approach for training the LDM. The generated up-to-date latent CXR is later fused with historical data for downstream clinical prediction. ", "page_idx": 1}, {"type": "text", "text": "We summarize our contributions as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 To our knowledge, DDL-CXR is the first work to generate an updated individual CXR image to improve clinical multimodal fusion, thereby alleviating the asynchronicity between EHR and CXR.   \n\u2022 We propose a contrastive learning approach for the LDM training to enable the disease course in EHR to be captured and utilized by the LDM.   \n\u2022 Experiments show that DDL-CXR outperforms existing methods in both multi-modal clinical prediction and individual CXR generation. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Clinical multi-modal fusion Integrating multi-modal clinical data has shown beneficial for various clinical prediction tasks [24], including COVID-19 prediction [25], pulmonary embolism diagnosis [8, 26], AD diagnosis [9] and X-ray image abnormality detection [27]. ", "page_idx": 2}, {"type": "text", "text": "Different strategies have been proposed to facilitate the fusion of multi-modal clinical data [2, 28]. Hayat et al. [10] adopts feature-level fusion with an LSTM layer, while Zhang et al. [29] utilizes a modality-correlated encoder to capture long-range dependencies across modalities. Zhang et al. [30] and Lee et al. [12] incorporate modality type embedding into the self-attention to capture the interaction. Despite the effort, existing methods for multi-modal fusion are driven only by downstream predictions. How to capture the more fundamental interaction between different data modalities remains an open challenge. ", "page_idx": 2}, {"type": "text", "text": "In the temporal setting, asynchronicity presents another major challenge. Unlike the settings of medical images and radiology reports [31], which are naturally aligned in time, EHR and CXR are often highly asynchronous, bringing extra difficulties to information integration. \u201cCarry-forward\u201d is a common strategy adopted, where the last available data from different modalities are used [10, 13]. Lee et al. [12] and Zhang et al. [17] also adopt this approach while modeling the time information of the last available data. ", "page_idx": 2}, {"type": "text", "text": "Conditional latent diffusion models The diffusion model is one of the state-of-the-art generative models [32, 33] that has found important applications in areas such as image generation [34], sound generation [35], joint audio and video generation [36], and tabular data generation [37]. To reduce the computational cost, LDM [22] proposes to train diffusion models on a latent space encoded via pre-trained VAE, thus improving training and sampling efficiency as well as preserving generation quality. It also incorporates an attention mechanism into its underlying neural backbone to allow more flexible conditioning. ", "page_idx": 2}, {"type": "text", "text": "Based on LDM, multi-modal generation models have been developed using priors obtained from large-scale contrastive pre-training, e.g., contrastive-image pairs for text-to-image generation [38] and contrastive language-audio pairs for text-to-audio generation [21]. However, it is infeasible to apply this method to clinical settings since many clinical data modalities, e.g., CXR and EHR, capture different aspects of patients and cannot be semantically aligned like the image and caption pairs as in CLIP. ", "page_idx": 2}, {"type": "text", "text": "In clinical settings, LDM-based models are developed for brain MRI image generation, conditioned on age, sex, brain structure volumes [39], and a subset of MRI slices [40]. For CXR image generation, Packh\u00e4user et al. [41] adopts a thoracic abnormality classifier-aided LDM to generate anonymous CXR images for privacy-protected data generation. Weber et al. [42] utilizes pathology labels, radiological reports, and radiologists\u2019 annotations for synthesizing customized CXR images. Gu et al. [43] explores counterfactual generation for CXR using information from imaging reports. Generating individual CXR images that reflect disease courses in EHR and applying them to medical predictions remains an open challenge. ", "page_idx": 2}, {"type": "text", "text": "3 DDL-CXR: The Proposed Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this work, we focus on improving multimodal clinical predictions by generating latent CXR images that are in line with patient conditions at prediction time. The generation process also works as a fusion module that captures the cross-modal interaction between EHR and CXR. The overview of DDL-CXR is depicted in Fig. 2. It consists of two stages: the LDM stage and the prediction stage. In the LDM stage, we use the consecutive image pairs to train an LDM that generates representations within a latent space encoded by a variational autoencoder (VAE). To generate patient-specific CXRs, an earlier CXR image of the same patient is used as a reference to capture the anatomical structure, and the EHR time series between the consecutive image pairs is used to capture the disease progression. In the prediction stage, conditioned on this composite information, DDL-CXR generates updated and informative CXR representations at prediction time, which are subsequently fused with available EHR data as well as the previous CXR image for downstream prediction tasks. ", "page_idx": 2}, {"type": "image", "img_path": "uCvdw0IOuU/tmp/715ad4bba80817f652560d9ff042e45ce0af784066f6c7da77124da036f1dd78.jpg", "img_caption": ["Figure 2: The overview of the proposed framework DDL-CXR. It consists of two stages. The LDM stage learns to generate an individualized up-to-date latent CXR at time $t_{1}$ , $\\hat{\\mathbf{Z}}_{t_{1}}$ , to address asynchronicity by conditioning on a previous CXR image taken at time $t_{0}$ , $\\mathbf{X}_{t_{0}}^{\\mathrm{CXR}}$ , w1hich provides the anatomical structure of the patient, as well as EHR data between $t_{0}$ and $t_{1}$ , ${\\bf X}_{(t_{0},t_{1})}^{\\mathrm{EHR}}$ , that provides information on disease progression. A contrastive loss and auxiliary loss are enforced for better EHR information integration. The generation module encapsulates cross-modal interactions to assist in clinical prediction. The prediction stage fuses the generated latent CXR, the most recent CXR image, and the complete EHR time series for clinical predictions. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "3.1 Notations and Preliminaries ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "EHR and CXR data Patient-wisely, we denote the EHR time series within the time interval between ti and tj as X(EtH,Rt i j) = [xti, xti+1, . . . , xtj], where xt \u2208RK is the variables recorded at time $t$ and $K$ is the number of features. We denote the grayscale CXR images taken at the time $t_{i}$ as $\\mathbf{X}_{t_{i}}^{\\mathrm{{CXR}}}\\in\\mathbb{R}^{W\\times H}$ , where $W$ and $H$ denote its width and height, respectively. For each CXR image, we extract the abnormality finding label, $\\mathbf{y}_{t_{i}}^{\\mathrm{CXR}}$ , from radiology reports using CheXpert [44]. ", "page_idx": 3}, {"type": "text", "text": "Predictive latent space for CXR Using diffusion models in a semantic latent space, rather than a high-dimensional data space, has shown a substantial decrease in computational expenses with minimal impact on synthesis quality [22]. To obtain an informative and expressive latent space, we first train a VAE [45] consisting of an encoder $\\mathcal{E}$ and a decoder $\\mathcal{D}$ . The data used for training VAE are all available CXR images in the training set with corresponding abnormality finding labels, $\\left(\\mathbf{X}_{t}^{\\mathrm{CXR}},\\mathbf{y}_{t}^{\\mathrm{CXR}}\\right)$ . The primary objective of the VAE is to reconstruct the original CXR image ${\\bf X}_{t}^{\\mathrm{CXR}}$ with $\\mathcal{D}(\\mathcal{E}(\\mathbf{X}_{t}^{\\mathrm{CXR}}))$ . We follow the VAE training process in [22], incorporating a pixel-wise reconstruction loss accompanied by a perceptual loss [46], an adversarial objective, and a lightlypenalized Kullback-Leibler loss towards a standard normal aiming at constraining the latent spaces from excessively high variance. Besides, to improve the encoder\u2019s ability to predict, we also include a prediction loss regarding the abnormality label $\\mathbf{y}_{t}^{\\mathrm{CXR}}$ . We denote the encoded latent CXR by $\\begin{array}{r}{\\mathbf{Z}_{t}=\\mathcal{E}(\\mathbf{X}_{t}^{\\mathrm{{CXR}}})\\in\\mathbb{R}^{C\\times\\frac{W}{r}\\times\\frac{H}{r}}}\\end{array}$ , where $C$ represents the channel of the compressed representation and $r$ represents the compression ratio. We first pre-train the VAE model and then freeze it throughout the training and inference of DDL-CXR. More details on VAE training are presented in Appendix A.1. ", "page_idx": 3}, {"type": "text", "text": "3.2 LDM Stage: Dynamic Latent CXR Generation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "As discussed previously, to generate an up-to-date, patient-specific latent CXR, it is important to incorporate the unique anatomical details of the individual patient. Furthermore, the generated image must accurately reflect the evolving pathology as documented in the irregular EHR time series. To this end, we extract all sequential image pairs and the EHR time series between them. We denote each sample as a quadruplet: $\\left(\\mathbf{X}_{t_{0}}^{\\mathrm{CXR}},\\bar{\\mathbf{X}}_{(t_{0},t_{1})}^{\\mathrm{EHR}},\\mathbf{X}_{t_{1}}^{\\mathrm{CXR}},\\mathbf{y}_{t_{1}}^{\\mathrm{CXR}}\\right)$ . CXR images are encoded using the pre-trained VAE as we aim to generate latent CXR images: ${\\bf Z}_{t_{0}}\\,=\\,\\boldsymbol{\\mathcal{E}}({\\bf X}_{t_{0}}^{\\mathrm{CXR}}),{\\bf Z}_{t_{1}}\\,=$ $\\mathcal{E}(\\mathbf{X}_{t_{1}}^{\\mathrm{CXR}})$ . We follow prior works on diffusion models to learn our LDM [22, 32]. It comes down to learning a network that predicts the noise added to the noisy latent $\\mathbf{Z}_{t_{1}}^{(n)}$ at denoising step $n$ as $\\begin{array}{r}{\\epsilon_{\\theta}\\left(\\mathbf{Z}_{t_{1}}^{\\left(n\\right)},\\mathbf{Z}_{t_{0}},f_{\\mathrm{cond}}^{\\mathrm{EHR}}(\\mathbf{X}_{(t_{0},t_{1})}^{\\mathrm{EHR}}),n\\right)}\\end{array}$ . Following prior works [22, 32], we parameterize $\\epsilon_{\\theta}$ by a standard UNet [47]. Here $f_{\\mathrm{cond}}^{\\mathrm{EHR}}(\\cdot)$ is the encoder for the irregular EHR time series to be detailed later. The detailed diffusion and denoising processes are presented in Appendix A.1. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Neural backbone and conditioning mechanisms Due to the remarkable capability of UNet [47] in capturing the spatial structure of images, we follow prior works and use a UNet as our neural backbone $\\epsilon_{\\theta}$ . It predicts the noise added in the diffusion process, conditioned on the reference image and the EHR time series. To explicitly capture and utilize the anatomical structure of individual patients, we first concatenate the reference latent CXR $\\mathbf{Z}_{t_{0}}$ and the step- $^{\\cdot n}$ noisy latent $\\mathbf{Z}_{t_{1}}^{(n)}$ . To further integrate the disease course embedded in the EHR time series, we use the cross-attention mechanism to capture the interaction between the two modalities. Formally, the input to the UNet layers is given by: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\mathrm{Attention}(\\mathbf{Q},\\mathbf{K},\\mathbf{V})=\\mathrm{softmax}\\left(\\frac{\\mathbf{Q}\\mathbf{K}^{\\top}}{\\sqrt{d}}\\right)\\cdot\\mathbf{V},}\\\\ {\\mathrm{with}\\ \\mathbf{Q}=\\mathbf{W}_{Q}\\cdot\\varphi\\left(\\mathbf{Z}_{t_{1}}^{(n)}||\\mathbf{Z}_{t_{0}}\\right),\\mathbf{K}=\\mathbf{W}_{K}\\cdot f_{\\mathrm{cond}}^{\\mathrm{EHR}}(\\mathbf{X}_{(t_{0},t_{1})}^{\\mathrm{EHR}}),\\mathbf{V}=\\mathbf{W}_{V}\\cdot f_{\\mathrm{cond}}^{\\mathrm{EHR}}(\\mathbf{X}_{(t_{0},t_{1})}^{\\mathrm{EHR}}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\varphi(\\cdot)$ denotes the flattened intermediate representation of the UNet and $||$ denotes concatenation. ", "page_idx": 4}, {"type": "text", "text": "Capturing disease course via EHR time series To effectively capture useful information on disease progression for future CXR generation, we adopt a multi-task Transformer-based time series encoder [48] with the masked self-attention mechanism to handle the variable length of EHR time series [49]. The encoded representation of EHR, $\\mathbf{E}_{(t_{0},t_{1})}$ , is given by ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{E}_{(t_{0},t_{1})}=f_{\\mathrm{cond}}^{\\mathrm{EHR}}(\\mathbf{X}_{(t_{0},t_{1})}^{\\mathrm{EHR}})=\\mathrm{Transformer}\\left(\\left[\\mathbf{h}_{\\mathrm{CLS}},\\phi(\\mathbf{x}_{t_{0}}),\\dots,\\phi(\\mathbf{x}_{t_{1}})\\right]\\right),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\phi(\\mathbf{x}_{t})$ projects the original EHR time series into an embedding space and applies the positional encoding at time step $t,\\,\\mathbf{h}_{\\mathrm{CLS}}$ is the class token. ", "page_idx": 4}, {"type": "text", "text": "To further extract information that is relevant to CXR generation and facilitate modality fusion at the LDM stage, we incorporate an auxiliary prediction task: using the class token from the encoded EHR to predict the abnormality findings $\\mathbf{y}_{t_{1}}^{\\mathrm{CX\\tilde{R}}}$ , associated with the CXR image $\\mathbf{X}_{t_{1.}}^{\\mathrm{CXR}}$ , i.e., $\\widehat{\\mathbf{y}}_{t_{1}}^{\\mathrm{CXR}}=g(\\mathbf{h}_{\\mathrm{CLS}})$ on  Laux :=M1 L1 fwuhnecrtie dgievneont ebsy t $\\begin{array}{r}{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\mathcal{L}_{\\mathrm{aux}}^{'}:=\\frac{1}{M}\\frac{1}{L}\\sum_{m=1}^{M}\\sum_{l=1}^{L^{'}}y_{m l}^{\\mathrm{CXR}}\\log(\\widehat{y}_{m l}^{\\mathrm{CXR}})+(1-y_{m l}^{\\mathrm{CXR}})\\mathrm{\\log(}\\bar{1}-\\widehat{y}_{m l}^{\\mathrm{CXR}})}\\end{array}$ ,z ew thheer el $M$ is the number of training samples for LDM and $L$ is the number of classes of abnormality labels of CXR. The auxiliary task enables the EHR encoder to extract CXR-related information, which further encourages the interaction between EHR and CXR to be captured in the subsequent generation. ", "page_idx": 4}, {"type": "text", "text": "Enhancing semantic multimodal fusion via contrastive LDM learning The generation conditioning on EHR data is challenging because the EHR and CXR data are highly heterogeneous and the interactions are implicit. To force the LDM to utilize EHR information during generation, we propose a contrastive way of learning the conditional LDM. Specifically, for each EHR time series, we obtain a perturbed version of its representation $\\widetilde{\\mathbf{E}}_{(t_{0},t_{1})}=(1-\\beta)\\mathbf{E}_{(t_{0},t_{1})}+\\beta\\delta$ , where $\\delta\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{I})$ is randomly drawn from a standard normal distribution, $\\beta$ is a hyperparameter controlling the strength of the noise. When the perturbed EHR is given as input, we expect the generated image to be far away from the target image. This leads to the following training objective function: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}_{\\mathrm{LDM}}:=\\!\\mathbb{E}_{{\\mathbf{Z}_{t_{1}}},{\\mathbf{Z}_{t_{0}}},{\\mathbf{X}_{\\tau_{(0,t_{1})}}^{\\mathrm{HR}}},\\epsilon\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{I}),n}\\left[\\left\\|\\epsilon-\\epsilon_{\\theta}\\left(\\mathbf{Z}_{t_{1}}^{(n)},\\mathbf{Z}_{t_{0}},f_{\\mathrm{cond}}^{\\mathrm{EHR}}(\\mathbf{X}_{(t_{0},t_{1})}^{\\mathrm{EHR}}),n\\right)\\right\\|_{2}^{2}\\right.}\\\\ &{\\qquad\\qquad\\left.+\\lambda_{1}\\operatorname*{max}\\left(\\left\\|\\epsilon-\\epsilon_{\\theta}\\left(\\mathbf{Z}_{t_{1}}^{(n)},\\mathbf{Z}_{t_{0}},\\mathbf{E}_{(t_{0},t_{1})},n\\right)\\right\\|_{2}^{2}-\\left\\|\\epsilon-\\epsilon_{\\theta}\\left(\\mathbf{Z}_{t_{1}}^{(n)},\\mathbf{Z}_{t_{0}},\\widetilde{\\mathbf{E}}_{(t_{0},t_{1})},n\\right)\\right\\|_{2}^{2}+\\alpha,0\\right)\\right]\\!,}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\alpha$ is a hyperparameter controlling the tolerance of the noisy-conditional generation. $\\lambda_{1}$ is a coefficient controlling the strength of the contrastive term. To ensure stability during training, we set the initial value of $\\lambda_{1}$ to zero and linearly increase it to one during training. ", "page_idx": 4}, {"type": "text", "text": "3.3 Prediction Stage ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In the prediction stage, we do not have access to an up-to-date CXR image. Therefore, we generate an updated latent CXR $\\widehat{\\mathbf{Z}}_{t_{1}}$ at the prediction time $t_{1}$ using the last available CXR image $\\mathbf{X}_{t_{0}}^{\\mathrm{CXR}}$ aasv atihlaeb rlee fEerHenRc, ew iem aadgoep ta anndo tthhee r EtiHmRe  tsiermiee ss eenriceosd ienr, $f_{\\mathrm{pred}}^{\\mathrm{EHR}}$ e, ewn ${\\bf X}_{(t_{0},t_{1})}^{\\mathrm{EHR}}$ .t heT os ammaek set rpurcetduircet iaos $f_{\\mathrm{cond}}^{\\mathrm{EHR}}$ nags ionb sEerqv. a(ti2o).n  Ntiomtee  tsheat ta fso 4r 8p rheoduircst,i oenn,s tuhrien Eg HthRe  daavtaai luasbleed ,i ${\\bf X}_{\\le48\\mathrm{h}}^{\\mathrm{EHR}}$ tcioovn eirss  faullll yE uHtiRli tziemd.e  Isne roitehs erw iwtho rtdhse, ${\\bf X}_{(t_{0},t_{1})}^{\\mathrm{EHR}}\\subseteq{\\bf X}_{\\leq48\\mathrm{h}}^{\\mathrm{EHR}}$ . In clinical practice, clinicians make predictions not only based on the latest CXR, but also on past CXR images as reference for disease basis. To this end, we employ all available data: $\\mathbf{X}_{t_{0}}^{\\mathrm{CXR}},\\mathbf{X}_{\\leq48\\mathrm{h}}^{\\mathrm{EHR}}$ and the generated latent CXR $\\widehat{\\mathbf{Z}}_{t_{1}}$ to make the final clinical prediction: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\boldsymbol{\\widehat{\\mathbf{y}}}=\\mathcal{G}_{\\psi}\\left(f_{\\mathrm{pred}}^{\\mathrm{CXR}}(\\mathbf{X}_{t_{0}}^{\\mathrm{CXR}}),\\ f_{\\mathrm{pred}}^{\\mathrm{EHR}}(\\mathbf{X}_{\\leq48\\mathrm{h}}^{\\mathrm{EHR}}),\\ f_{\\mathrm{pred}}^{\\mathrm{LAT}}(\\boldsymbol{\\widehat{\\mathbf{Z}}}_{t_{1}})\\right).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Here $f_{\\mathrm{pred}}^{i}$ $_{\\mathrm{d}},\\,i\\,\\in\\,\\{\\mathrm{CXR},\\,\\mathrm{EHR},\\mathrm{LAT}\\}$ are encoders for CXR, EHR, and the generated latent CXR, accordingly. We parameterize $f_{\\mathrm{pred}}^{\\mathrm{LAT}}$ and $f_{\\mathrm{pred}}^{\\mathrm{EHR}}$ using Transformer models, and $f_{\\mathrm{pred}}^{\\mathrm{CXR}}$ using a ResNet model. The predicting model $\\mathcal{G}_{\\psi}$ with $\\psi$ denoting the model parameter, is parameterized by a self-attention layer. We learn it by minimizing the cross-entropy (CE) loss: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{task}}:=\\sum_{m=1}^{M^{\\prime}}\\sum_{l=1}^{L^{\\prime}}y_{m l}\\log(\\widehat{y}_{m l})+(1-y_{m l})\\log(1-\\widehat{y}_{m l}),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $L^{\\prime}$ is the number of classes in the prediction task and $M^{\\prime}$ is the number of training samples in the prediction stage. ", "page_idx": 5}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "4.1 Experiment Settings ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Datasets We empirically evaluate the clinical predictive performance of DDL-CXR using MIMIC-IV [50] and MIMIC-CXR $[18]^{4}$ . MIMIC-IV comprises de-identified critical care data from adult patients admitted to either ICUs or the emergency department (EDs) of Beth Israel Deaconess Medical Center (BIDMC) between 2008 and 2019, and MIMIC-CXR contains chest $\\boldsymbol{\\mathrm{X}}$ -rays and reports collected from BIDMC, with a subset of patients matched with those in MIMIC-IV. For EHR data, we follow a preprocessing pipeline similar to that described in [10]. 17 clinical time series variables as well as age and gender are extracted. The details can be found in Appendix A.2. ", "page_idx": 5}, {"type": "text", "text": "Dataset construction and partition The inclusion criteria for this study involve ICU stays from the matched subset of MIMIC-IV and MIMIC-CXR that contain at least one CXR image (with Anterior-Posterior (AP) projection) during the ICU stay or within 24 hours before ICU admission. We exclude ICU stays with lengths shorter than 48 hours. The dataset is randomly split by the patient identifier with a ratio of 24:4:7 for training, validation, and testing, which avoids patient overlapping between subsets. ", "page_idx": 5}, {"type": "text", "text": "From the training patients, we further extract data for training the VAE, the LDM, and the prediction model. We extract all images from the training patients for training VAE and extract all CXR image pairs of the same patient taken at any interval greater than 12 hours for training the LDM, i.e., ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{D}_{\\mathrm{LDM}}=\\left\\{\\left(\\mathbf{X}_{t_{0}}^{\\mathrm{CXR}},\\mathbf{X}_{t_{1}}^{\\mathrm{CXR}},\\mathbf{X}_{(t_{0},t_{1})}^{\\mathrm{EHR}},\\mathbf{y}_{t_{1}}^{\\mathrm{CXR}}\\right)_{(t_{1}-t_{0})>12\\mathrm{h}}\\right\\},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where a single ICU stay may contain multiple data pairs for LDM training. This greatly enlarges the training subset for the LDM stage. ", "page_idx": 5}, {"type": "text", "text": "For the prediction stage, we extract the last available CXR image and the EHR time series in the first 48 hours and the label for the prediction task of each ICU stay, i.e., the triplet $\\left(\\mathbf{X}_{\\mathrm{last}}^{\\mathrm{CXR}},\\mathbf{X}_{\\le48\\mathrm{h}}^{\\mathrm{EHR}},\\mathbf{y}_{\\mathrm{task}}\\right)$ Note that the EHR time series used in the prediction stage differs from that in the LDM stage in their time interval since they serve for different purposes. ", "page_idx": 5}, {"type": "table", "img_path": "uCvdw0IOuU/tmp/946a299812872dbf6f4d2e9df3282d3bc06f018887a798a08f97042625e0a769.jpg", "table_caption": ["Table 1: Overall performance for the phenotype classification and mortality prediction task as measured by AUPRC and AUROC scores. DDL-CXR outperforms all baselines in these metrics. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "We use the same approach to extract the validation subsets for hyperparameter tuning of VAE, LDM, and the prediction model. Note that the testing patients are held out for evaluating prediction performance only, and are not involved in the training and model selection of VAE and LDM. ", "page_idx": 6}, {"type": "text", "text": "Prediction tasks and evaluation metrics We evaluate DDL-CXR with two clinical prediction tasks: in-hospital mortality prediction and phenotype classification using clinical data collected within the first 48 hours of ICU admissions. The phenotype classification is a multi-label classification task, where the labels are defined by the 25 disease phenotypes, extracted following [51]. The details of the label prevalence and data cohort statistics can be found in Appendix A.2. ", "page_idx": 6}, {"type": "text", "text": "We evaluate the performance using two metrics, the Area Under the Precision-Recall Curve (AUPRC) and the Area Under the Receiver Operating Characteristics (AUROC). For the phenotyping task, we report macro-averaged scores. We conduct each prediction experiment five times with distinct random seeds and reported the mean and standard deviation of the results. ", "page_idx": 6}, {"type": "text", "text": "Baseline Models We compare the following methods. (1) Uni-EHR, a single-modal classifier for EHR time series based on Transformer [23], (2) MMTM [52], a multi-modal fusion method based on CNNs through squeeze and excitation operations, (3) DAFT [9] a general-purpose module for fusing tabular clinical information and image data by dynamically rescaling and shifting the feature maps of a convolutional layer, (4) MedFuse [10], an LSTM-based multimodal fusion method developed for clinical prediction using EHR and CXR, (5) DrFuse [13], a disentangled learning approach that handles modality missing and modal inconsistency in clinical multi-modal fusion, and (6) GAN-based generation [53], a model originally proposed to generate individual brain images conditioning on age and Alzheimer\u2019s Disease (AD) status via training a conditional GAN. ", "page_idx": 6}, {"type": "text", "text": "4.2 Prediction Performance ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "DDL-CXR obtains the best overall performance. We summarize the overall performance of the phenotype classification and in-hospital mortality prediction in Table 1, where DDL-CXR outperforms all baselines. This shows that generating an updated CXR during test time is beneficial for downstream tasks. On the contrary, DrFuse, MedFuse, DAFT, and MMTM use the last available CXR for prediction, which might have been outdated. ", "page_idx": 6}, {"type": "text", "text": "The performance gain of DDL-CXR in terms of AUPRC is particularly noteworthy as the AUPRC metric is especially relevant in the context as it underscores the effectiveness of our approach in identifying the positive class in imbalanced medical datasets. DDL-CXR achieves relative improvements of $2.4\\%$ and $3.56\\%$ over the best baselines in terms of AUPRC for phenotype classification and mortality prediction, respectively. ", "page_idx": 6}, {"type": "text", "text": "Mortality prediction with varying time interval We define the time interval (by hour) between the prediction time and the time of the last available CXR as $\\delta$ and compute the evaluation metrics in patient groups with different ranges of $\\delta$ . The results are presented in Table 2. Since the label prevalence varies significantly between groups, making the comparison of AUPRC between groups less meaningful, we report AUROC in the paper and AUPRC in the appendix. DDL-CXR consistently outperforms the baseline models for most groups of $\\delta$ for the mortality prediction task. As the $\\delta$ increases, the last CXR becomes more \u201coutdated\u201d, and we observe a noticeable increase in the performance gap between the best baseline and DDL-CXR in the group $(\\delta\\geq36)$ ), from 0.806 to 0.830. This validates our hypothesis that the generation of a timely CXR, accounting for disease progression, can significantly enhance the performance of clinical predictions. ", "page_idx": 6}, {"type": "table", "img_path": "uCvdw0IOuU/tmp/2996507de966ce0211fd313e513d4cdda048c8ead9c50d61d286e16bf739f494.jpg", "table_caption": ["Table 2: The mean of AUROC score with standard deviation for mortality prediction for overall and different time gaps. $\\delta$ represents the time interval (by hour) between the prediction time and the time of the last available CXR. Numbers in bold indicate the best performance in each column. DDL-CXR outperforms all baselines in most settings. The AUPRC scores can be found in Appendix B.1. "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "uCvdw0IOuU/tmp/ecb9e24ca7e66d9533ec74397da10948692195d999ddd8b7b15b2a672b6f098e.jpg", "table_caption": ["Table 3: The AUPRC score of predicting each phenotype label. DDL-CXR obtains the highest average rank. Full names of phenotype labels and AUROC scores can be found in the Appendix. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Phenotype classification The class-wise AUPRC scores for the phenotyping task are detailed in Table 3, where DDL-CXR demonstrates notable performance improvements, achieving the highest average rank across all phenotype labels. Due to space limit, we report the standard deviations and the AUROC scores in Appendix B.2. The improvement over baseline multimodal fusion methods validates the effectiveness of facilitating fusion between EHR and CXR in the presence of asynchronicity. ", "page_idx": 7}, {"type": "image", "img_path": "uCvdw0IOuU/tmp/c9101c4adcff24d160afc766c2565e8e709bb721af77a69aa88742219c81fc23.jpg", "img_caption": ["Figure 3: Examples of images generated by DDL-CXR. From top to bottom, the three rows are reference images $\\mathbf{X}_{t_{0}}^{\\mathrm{CXR}}$ , ground-truth images ${\\bf X}_{t_{1}}^{\\mathrm{CXR}}$ , and generated images $\\widehat{\\mathbf{X}}_{t_{1}}^{\\mathrm{CXR}}$ , respectively. The gofe ndeirsaetaisoen sp rsohgorwe stshiaotn  DeDxLtr-aCctXeRd  cfarpotmur eEs HthRe  ias nbalteonmdiecda lw ienlflo tromwaatirodns  fgreonme $\\mathbf{X}_{t_{0}}^{\\mathrm{CXR}}$ .he information ${\\bf X}_{t_{1}}^{\\mathrm{CXR}}$ "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "4.3 Quality of Generated Chest X-ray Images ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Quantitative Evaluation We evaluate the quality of generated CXRs using the test set of the LDM stage, where the ground-truth target CXR is available. The Fr\u00e9chet Inception Distance (FID) score [54] evaluates the similarity between the distributions of generated and ground-truth target CXRs by computing Fr\u00e9chet distance on the representation obtained from a pre-trained Inception-v3 network. Besides, we directly measure the Fr\u00e9chet distance (FD) ", "page_idx": 8}, {"type": "text", "text": "and Wasserstein distance (WD) in the latent space of the VAE between the generated and the target CXRs. Results are shown in Table 4. Results of \u201cLast-CXR\u201d are obtained between reference images $\\mathbf{X}_{0}$ and target images $\\mathbf{X}_{1}$ , both are directly from the dataset without generation. Thus, this provides a reference to the lower bound of the metrics used. DDL-CXR surpasses GAN-based methods across all metrics and obtains the lowest FD and WD. \u201cw/o ${\\mathbf{Z}_{t_{0}}}^{,,}$ and \u201cw/o $\\mathbf{E}_{(t_{0},t_{1})}{}^{,,}$ are obtained by removing the condition of the last available CXR and EHR data, respectively. Notably, excluding EHR data from the generation conditions resulted in lower FID scores, which is natural since the generation becomes less restrictive. ", "page_idx": 8}, {"type": "table", "img_path": "uCvdw0IOuU/tmp/667103fa908fbca3c821b04a1e7ae3bb3fe21378b8693142016351931240b1e5.jpg", "table_caption": ["Table 4: Generation quality. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Qualitative Evaluation To further visually examine the generated CXR images, we decode the latent CXR and visualize seven examples in Fig. 3. The first row shows the last CXR images used as reference, the second row displays the ground-truth CXR images, and the last row showcases the generated CXR images. The comparison between the first and third rows indicates that the generated CXR could well capture anatomical structure, while the comparison between the second and third rows demonstrates that the generated CXRs are in line with the latest imaging manifestations, implying that the disease progression embedded in EHR could be captured and utilized in the generation process. We further retrieve the radiology reports and the discharge summary of the corresponding patients from the database for case studies. Fig. 1 shows one example of the case study (Sample #4), where the patient rapidly turned from normal CXR to severe pulmonary consolidation. The discharge summary shows that the patient experienced transfusion-related acute lung injury and sepsis. Evidently, the generated CXR could more accurately reflect the progressed condition of the patient. Due to space limits, we present more case studies in Appendix B.5. ", "page_idx": 8}, {"type": "text", "text": "4.4 Ablation Study ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "To better understand the factors contributing to the improved performance, we conducted an ablation study by removing the conditioning components in the LDM stage. The results are summarized in Table 5. The variant \u201cLast-CXR\u201d has the same architecture as the classifier of DDL-CXR but removes the generated latent CXR $\\widehat{\\mathbf{Z}}_{t_{1}}$ . The improvement over Last-CXR shows that learning an LDM for generating updated laten t CXR is a more effective approach to multimodal fusion, and hence benefits downstream prediction, especially for the mortality prediction task. The variants \u201cw/o ${\\mathbf{Z}_{t_{0}}}^{,,}$ and \u201cw/o $\\mathbf{E}_{(t_{0},t_{1})},$ \u201d remove the last available CXR and EHR data, respectively, from the condition during LDM training. \u201cw/o Contrastive\u201d removes the contrastive terms from the LDM objective function. \u201cw/o ${\\mathcal{L}}_{\\mathrm{aux}}$ \u201d removes the auxiliary loss which drives the EHR encoder to capture the CXR-related abnormality findings. The results show that adding each component brings slight improvement while incorporating the reference CXR, the EHR, the contrastive learning, and the auxiliary task achieves the best performance. We also remove the EHR data completely in the prediction stage and evaluate the performance using the last available CXR and the generated CXR, respectively. Results are shown as \u201cLast-CXR (w/o EHR)\u201d and \u201cDDL-CXR (w/o EHR)\u201d in Table 5. The results suggest that the generation of an updated CXR significantly beneftis downstream clinical predictions. Additional experiment results on robustness against reduced training data size can be found in Appendix B.4. ", "page_idx": 8}, {"type": "table", "img_path": "uCvdw0IOuU/tmp/37ed2eeafd7f8a9abb02d229e028aaf6a2e813f788bc4ddd07c30c8b49a312ac.jpg", "table_caption": ["Table 5: Results of the ablation study. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "5 Broader Impacts and Limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "DDL-CXR holds promise for societal beneftis, such as more precise and timely medical interventions, and offers an alternative for patients with limited access to $\\boldsymbol{\\mathrm{X}}$ -ray imaging. Nonetheless, the potential for generating fake profiles necessitates stringent safeguards, including expert validation of synthesized images, to prevent misuse and protect patient confidentiality, especially when applied to private datasets. Despite its promise, DDL-CXR has some limitations like the need for meticulous hyperparameter tuning and a performance gap across different time intervals, as indicated in Table 2. Addressing such potential biases is a priority for future research. Furthermore, while various metrics have been employed to assess generation quality, expert evaluation by radiologists would provide a more insightful measure of the model\u2019s efficacy. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we introduce DDL-CXR, which utilizes a powerful LDM to dynamically generate upto-date latent chest X-rays to tackle the asynchronicity of multi-modal clinical data for predictions. Our approach involves leveraging various conditions for patient-specific generation: the most recent available chest X-ray to incorporate detailed patient-specific anatomical structure, as well as the EHR data with variable durations for disease progression information. To improve multi-modal fusion in the generation, we develop a contrastive-learning-based LDM to capture and utilize disease courses in EHR. Through quantitative and qualitative validations, we demonstrate the superior performance of DDL-CXR in both image generation and enhancing multi-modal fusion via conditional generation for clinical prediction. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This work is partially supported by the General Research Fund of Hong Kong Research Grants Council (project no. 15218521), a grant under Theme-based Research Scheme of Hong Kong Research Grants Council (project no. T45-401/22-N), the General Research Fund RGC/HKBU12202621 from the Research Grant Council, the Research Matching Grant Scheme RMGS2021_8_06 from the Hong Kong Government, the National Natural Science Foundation of China (62302413), and the Health and Medical Research Fund (23220312). ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Rowa Aljondi and Salem Alghamdi. Diagnostic value of imaging modalities for COVID-19: scoping review. Journal of Medical Internet Research, 22(8):e19673, 2020.   \n[2] Farida Mohsen, Hazrat Ali, Nady El Hajj, and Zubair Shah. Artificial intelligence-based methods for fusion of electronic health records and imaging data. Scientific Reports, 12(1):17981, 2022.   \n[3] Kejing Yin, Dong Qian, and William K Cheung. PATNet: Propensity-adjusted temporal network for joint imputation and prediction using binary EHRs with observation bias. IEEE Transactions on Knowledge & Data Engineering, 36(06):2600\u20132613, 2024.   \n[4] Kejing Yin, William K Cheung, Benjamin CM Fung, and Jonathan Poon. Learning inter-modal correspondence and phenotypes from multi-modal electronic health records. IEEE Transactions on Knowledge & Data Engineering, 34(09):4328\u20134341, 2022.   \n[5] Kejing Yin, Ardavan Afshar, Joyce C Ho, William K Cheung, Chao Zhang, and Jimeng Sun. LogPar: Logistic PARAFAC2 factorization for temporal binary data with missing values. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 1625\u20131635, 2020.   \n[6] Lihong Song, Chin Wang Cheong, Kejing Yin, William K Cheung, Benjamin CM Fung, and Jonathan Poon. Medical concept embedding with multiple ontological representations. In Proceedings of the 28th International Joint Conference on Artificial Intelligence, pages 4613\u20134619, 2019.   \n[7] Shih-Cheng Huang, Anuj Pareek, Saeed Seyyedi, Imon Banerjee, and Matthew P Lungren. Fusion of medical imaging and electronic health records using deep learning: a systematic review and implementation guidelines. NPJ Digital Medicine, 3(1):136, 2020.   \n[8] Shih-Cheng Huang, Anuj Pareek, Roham Zamanian, Imon Banerjee, and Matthew P Lungren. Multimodal fusion with deep neural networks for leveraging CT imaging and electronic health record: a case-study in pulmonary embolism detection. Scientific Reports, 10(1):22147, 2020.   \n[9] Sebastian P\u00f6lsterl, Tom Nuno Wolf, and Christian Wachinger. Combining 3D image and tabular data via the dynamic affine feature map transform. In Medical Image Computing and Computer Assisted Intervention\u2013MICCAI 2021: 24th International Conference, Strasbourg, France, September 27\u2013October 1, 2021, Proceedings, Part V 24, pages 688\u2013698. Springer, 2021.   \n[10] Nasir Hayat, Krzysztof J. Geras, and Farah E. Shamout. MedFuse: Multi-modal fusion with clinical time-series data and chest X-ray images. In Proceedings of the 7th Machine Learning for Healthcare Conference, volume 182 of Proceedings of Machine Learning Research, pages 479\u2013503. PMLR, 05\u201306 Aug 2022.   \n[11] S\u00f6ren Richard Stahlschmidt, Benjamin Ulfenborg, and Jane Synnergren. Multimodal deep learning for biomedical data fusion: a review. Briefings in Bioinformatics, 23(2):bbab569, 2022.   \n[12] Kwanhyung Lee, Soojeong Lee, Sangchul Hahn, Heejung Hyun, Edward Choi, Byungeun Ahn, and Joohyung Lee. Learning missing modal electronic health records with unified multi-modal data embedding and modality-aware attention. In Proceedings of the 8th Machine Learning for Healthcare Conference, 2023.   \n[13] Wenfang Yao, Kejing Yin, William K Cheung, Jia Liu, and Jing Qin. DrFuse: Learning disentangled representation for clinical multi-modal fusion with missing modality and modal inconsistency. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 16416\u201316424, 2024.   \n[14] Julia Adler-Milstein, Catherine M DesRoches, Peter Kralovec, Gregory Foster, Chantal Worzala, Dustin Charles, Talisha Searcy, and Ashish K Jha. Electronic health record adoption in us hospitals: progress continues, but challenges persist. Health Affairs, 34(12):2174\u20132180, 2015.   \n[15] Arom Choi, Kyungsoo Chung, Sung Phil Chung, Kwanhyung Lee, Heejung Hyun, and Ji Hoon Kim. Advantage of vital sign monitoring using a wireless wearable device for predicting septic shock in febrile patients in the emergency department: A machine learning-based analysis. Sensors, 22(18):7054, 2022.   \n[16] Claudia I Henschke, David F Yankelevitz, Austin Wand, Sheila D Davis, and Maria Shiau. Accuracy and efficacy of chest radiography in the intensive care unit. Radiologic Clinics of North America, 34(1):21\u201331, 1996.   \n[17] Xinlu Zhang, Shiyang Li, Zhiyu Chen, Xifeng Yan, and Linda Ruth Petzold. Improving medical predictions by irregular multimodal electronic health records modeling. In International Conference on Machine Learning, pages 41300\u201341313. PMLR, 2023.   \n[18] Alistair EW Johnson, Tom J Pollard, Seth J Berkowitz, Nathaniel R Greenbaum, Matthew P Lungren, Chih-ying Deng, Roger G Mark, and Steven Horng. MIMIC-CXR, a de-identified publicly available database of chest radiographs with free-text reports. Scientific Data, 6(1):317, 2019.   \n[19] Anusoumya Ganapathy, Neill KJ Adhikari, Jamie Spiegelman, and Damon C Scales. Routine chest X-rays in intensive care units: a systematic review and meta-analysis. Critical Care, 16(2):1\u201312, 2012.   \n[20] Declan Grant, Bart\u0142omiej W Papie\u02d9z, Guy Parsons, Lionel Tarassenko, and Adam Mahdi. Deep learning classification of cardiomegaly using combined imaging and non-imaging ICU data. In Medical Image Understanding and Analysis: 25th Annual Conference, MIUA 2021, Oxford, United Kingdom, July 12\u201314, 2021, Proceedings 25, pages 547\u2013558. Springer, 2021.   \n[21] Haohe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu, Danilo Mandic, Wenwu Wang, and Mark D Plumbley. AudioLDM: Text-to-audio generation with latent diffusion models. In Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 21450\u201321474. PMLR, 23\u201329 Jul 2023.   \n[22] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10684\u201310695, 2022.   \n[23] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in Neural Information Processing Systems, 30, 2017.   \n[24] Chaoqi Yang, M Brandon Westover, and Jimeng Sun. ManyDG: Many-domain generalization for healthcare applications. In The Eleventh International Conference on Learning Representations, 2023.   \n[25] Zhicheng Jiao, Ji Whae Choi, Kasey Halsey, Thi My Linh Tran, Ben Hsieh, Dongcui Wang, Feyisope Eweje, Robin Wang, Ken Chang, Jing Wu, et al. Prognostication of patients with COVID-19 using artificial intelligence based on chest X-rays and clinical data: a retrospective study. The Lancet Digital Health, 3(5): e286\u2013e294, 2021.   \n[26] Zhuo Zhi, Moe Elbadawi, Adam Daneshmend, Mine Orlu, Abdul Basit, Andreas Demosthenous, and Miguel Rodrigues. Multimodal diagnosis for pulmonary embolism from EHR data and CT images. In 2022 44th Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC), pages 2053\u20132057. IEEE, 2022.   \n[27] Chihcheng Hsieh, Isabel Blanco Nobre, Sandra Costa Sousa, Chun Ouyang, Margot Brereton, Jacinto C Nascimento, Joaquim Jorge, and Catarina Moreira. MDF-Net for abnormality detection by fusing X-rays with clinical data. Scientific Reports, 13(1):15873, 2023.   \n[28] Can Cui, Haichun Yang, Yaohong Wang, Shilin Zhao, Zuhayr Asad, Lori A Coburn, Keith T Wilson, Bennett Landman, and Yuankai Huo. Deep multi-modal fusion of image and non-image data in disease diagnosis and prognosis: a review. Progress in Biomedical Engineering, 2023.   \n[29] Yao Zhang, Nanjun He, Jiawei Yang, Yuexiang Li, Dong Wei, Yawen Huang, Yang Zhang, Zhiqiang He, and Yefeng Zheng. mmFormer: Multimodal medical Tansformer for incomplete multimodal learning of brain tumor segmentation. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 107\u2013117. Springer, 2022.   \n[30] Chaohe Zhang, Xu Chu, Liantao Ma, Yinghao Zhu, Yasha Wang, Jiangtao Wang, and Junfeng Zhao. M3Care: Learning with missing modalities in multimodal healthcare data. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 2418\u20132428, 2022.   \n[31] Shruthi Bannur, Stephanie Hyland, Qianchu Liu, Fernando P\u00e9rez-Garc\u00eda, Maximilian Ilse, Daniel C. Castro, Benedikt Boecking, Harshita Sharma, Kenza Bouzid, Anja Thieme, Anton Schwaighofer, Maria Wetscherek, Matthew P. Lungren, Aditya Nori, Javier Alvarez-Valle, and Ozan Oktay. Learning to exploit temporal structure for biomedical vision-language processing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 15016\u201315027, June 2023.   \n[32] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems, 33:6840\u20136851, 2020.   \n[33] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In International Conference on Learning Representations, 2021.   \n[34] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-toimage diffusion models with deep language understanding. Advances in Neural Information Processing Systems, 35:36479\u201336494, 2022.   \n[35] Dongchao Yang, Jianwei Yu, Helin Wang, Wen Wang, Chao Weng, Yuexian Zou, and Dong Yu. Diffsound: Discrete diffusion model for text-to-sound generation. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2023.   \n[36] Ludan Ruan, Yiyang Ma, Huan Yang, Huiguo He, Bei Liu, Jianlong Fu, Nicholas Jing Yuan, Qin Jin, and Baining Guo. MM-Diffusion: Learning multi-modal diffusion models for joint audio and video generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10219\u201310228, 2023.   \n[37] Akim Kotelnikov, Dmitry Baranchuk, Ivan Rubachev, and Artem Babenko. TabDDPM: Modelling tabular data with diffusion models. In International Conference on Machine Learning, pages 17564\u201317579. PMLR, 2023.   \n[38] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with CLIP latents, 2022. URL https://arxiv. org/abs/2204.06125, 7, 2022.   \n[39] Walter HL Pinaya, Petru-Daniel Tudosiu, Jessica Dafflon, Pedro F Da Costa, Virginia Fernandez, Parashkev Nachev, Sebastien Ourselin, and M Jorge Cardoso. Brain imaging generation with latent diffusion models. In MICCAI Workshop on Deep Generative Models, pages 117\u2013126. Springer, 2022.   \n[40] Wei Peng, Ehsan Adeli, Tomas Bosschieter, Sang Hyun Park, Qingyu Zhao, and Kilian M Pohl. Generating realistic brain MRIs via a conditional diffusion probabilistic model. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 14\u201324. Springer, 2023.   \n[41] Kai Packh\u00e4user, Lukas Folle, Florian Thamm, and Andreas Maier. Generation of anonymous chest radiographs using latent diffusion models for training thoracic abnormality classification systems. In 2023 IEEE 20th International Symposium on Biomedical Imaging (ISBI), pages 1\u20135. IEEE, 2023.   \n[42] Tobias Weber, Michael Ingrisch, Bernd Bischl, and David R\u00fcgamer. Cascaded latent diffusion models for high-resolution chest X-ray synthesis. In Pacific-Asia Conference on Knowledge Discovery and Data Mining, pages 180\u2013191. Springer, 2023.   \n[43] Yu Gu, Jianwei Yang, Naoto Usuyama, Chunyuan Li, Sheng Zhang, Matthew P Lungren, Jianfeng Gao, and Hoifung Poon. Biomedjourney: Counterfactual biomedical image generation by instruction-learning from multimodal patient journeys. arXiv preprint arXiv:2310.10765, 2023.   \n[44] Jeremy Irvin, Pranav Rajpurkar, Michael Ko, Yifan Yu, Silviana Ciurea-Ilcus, Chris Chute, Henrik Marklund, Behzad Haghgoo, Robyn Ball, Katie Shpanskaya, et al. CheXpert: A large chest radiograph dataset with uncertainty labels and expert comparison. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 590\u2013597, 2019.   \n[45] Diederik P Kingma and Max Welling. Auto-encoding variational Bayes. In International Conference on Learning Representations, 2014.   \n[46] Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.   \n[47] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-Net: Convolutional networks for biomedical image segmentation. In Medical Image Computing and Computer-Assisted Intervention\u2013MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18, pages 234\u2013241. Springer, 2015.   \n[48] George Zerveas, Srideepika Jayaraman, Dhaval Patel, Anuradha Bhamidipaty, and Carsten Eickhoff. A Tansformer-based framework for multivariate time series representation learning. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining, pages 2114\u20132124, 2021.   \n[49] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in Neural Information Processing Systems, 30, 2017.   \n[50] Alistair EW Johnson, Lucas Bulgarelli, Lu Shen, Alvin Gayles, Ayad Shammout, Steven Horng, Tom J Pollard, Sicheng Hao, Benjamin Moody, Brian Gow, et al. MIMIC-IV, a freely accessible electronic health record dataset. Scientific Data, 10(1):1, 2023.   \n[51] Hrayr Harutyunyan, Hrant Khachatrian, David C Kale, Greg Ver Steeg, and Aram Galstyan. Multitask learning and benchmarking with clinical time series data. Scientific Data, 6(1):96, 2019.   \n[52] Hamid Reza Vaezi Joze, Amirreza Shaban, Michael L Iuzzolino, and Kazuhito Koishida. MMTM: Multimodal transfer module for CNN fusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13289\u201313299, 2020.   \n[53] Tian Xia, Agisilaos Chartsias, Chengjia Wang, Sotirios A Tsaftaris, Alzheimer\u2019s Disease Neuroimaging Initiative, et al. Learning to synthesise the ageing brain without longitudinal data. Medical Image Analysis, 73:102169, 2021.   \n[54] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. GANs trained by a two time-scale update rule converge to a local Nash equilibrium. Advances in Neural Information Processing Systems, 30, 2017.   \n[55] Mohammad Amin Morid, Olivia R Liu Sheng, and Joseph Dunbar. Time series prediction using deep learning methods in healthcare. ACM Transactions on Management Information Systems, 14(1):1\u201329, 2023. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Experiment Details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.1 Details of Architectures and Training Procedures of DDL-CXR ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The training and validation processes are executed on a server equipped with a RTX 4090-24GB GPU card and a 16 vCPU Intel Xeon Processor. The method is implemented using PyTorch 1.9.1 and PyTorch-Lightning 1.4.2. DDIM [33] sampling with 200 steps is employed to accelerate the sampling process, and AdamW optimizer is used for all model training. Our implementation is partially based on the repository of the latent diffusion model $[22]^{5}$ . ", "page_idx": 14}, {"type": "text", "text": "Variational autoencoder (VAE) The VAE training process, as outlined in [22], includes a pixelwise reconstruction loss, a perceptual loss [46], an adversarial objective, and a lightly-penalized Kullback-Leibler loss toward a standard normal to constrain latent spaces, given by: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}_{\\mathrm{VaE}}=\\underset{\\boldsymbol{\\mathcal{E}},\\boldsymbol{\\mathcal{D}},\\boldsymbol{\\Phi}}{\\operatorname*{min}}\\underset{\\boldsymbol{\\psi}}{\\operatorname*{max}}\\left(L_{\\mathrm{rec}}(\\mathbf{X}^{\\mathrm{CXR}},\\mathcal{D}(\\boldsymbol{\\mathcal{E}}(\\mathbf{X}^{\\mathrm{CXR}})))-L_{\\mathrm{adv}}(\\mathcal{D}(\\boldsymbol{\\mathcal{E}}(\\mathbf{X}^{\\mathrm{CXR}})))+\\log D_{\\omega}(\\mathbf{X}^{\\mathrm{CXR}}))\\right)}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad+\\ L_{\\mathrm{reg}}(\\mathbf{X}^{\\mathrm{CXR}});\\boldsymbol{\\mathcal{E}},\\mathcal{D})+L_{\\mathrm{CE}}(\\boldsymbol{\\Phi}(\\boldsymbol{\\mathcal{E}}(\\mathbf{X}^{\\mathrm{CXR}})),\\mathbf{y})\\Big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\Phi$ is a classifier that predicts the CheXpert labels associated with the image and we parameterize it with an MLP. All CXRs in the training subset of the LDM stage are gathered for VAE training. A compression rate $r=8$ is adopted, and the training continues for a maximum of 50 epochs. The model with the minimum validation error, as measured using CXRs from the validation subset, is selected. The resulting latent representation has a dimension of $4\\times28\\times28=3136$ . To restrict the normal prior in the latent space and prioritize reconstruction quality, a KL-divergence weighting of $1\\times10^{-6}$ is set. We use the base learning rate of $4.5\\times10^{-6}$ , which is scaled by the number of GPU cards and batch size. ", "page_idx": 14}, {"type": "text", "text": "Latent diffusion model (LDM) stage in DDL-CXR In the LDM stage of our DDL-CXR model, we employ the UNet architecture [47] as the neural backbone, denoted by $\\epsilon_{\\theta}$ . Meanwhile, we utilize a multivariate time series Transformer [48] for the EHR conditioning encoder $f_{\\mathrm{cond}}^{\\mathrm{EHR}}$ . The Transformer $f_{\\mathrm{cond}}^{\\mathrm{EHR}}$ is designed with one layer, a model dimension set to 128, and a maximum EHR data length of 70. The UNet model $\\epsilon_{\\theta}$ features an input channel of 8 and an output channel of 4. The encoding section comprises three blocks, with model channels set at 224, 448, and 672, consisting of a ResBlock module followed by a spatial transformer. The bottleneck consists of two ResBlock modules with a spatial transformer in between. The decoder mirrors the encoder architecture. As discussed in Section 3.2, we introduce the encoded EHR information through multi-head cross-attention to the spatial transformer module of $\\epsilon_{\\theta}$ . The context dimension is set to 128, and the number of attention heads is 8. The model is trained for 200 epochs with a batch size of 32, and the model with the smallest composite loss on the validation set is selected for subsequent latent Chest X-ray (CXR) generation. We set the hyperparameters $\\alpha$ to 0.2, and $\\beta$ to 0.5, empirically. ", "page_idx": 14}, {"type": "text", "text": "Latent CXR generation via LDM In the LDM stage, we aim to generate latent CXR images at time $t_{1}$ , conditioned on $\\mathbf{X}_{t_{0}}^{\\mathrm{CXR}}$ and ${\\bf X}_{(t_{0},t_{1})}^{\\mathrm{EHR}}$ . We first encode the CXR images using the pre-trained VAE, given by: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{Z}_{t_{0}}=\\mathcal{E}(\\mathbf{X}_{t_{0}}^{\\mathrm{CXR}}),\\quad\\mathbf{Z}_{t_{1}}=\\mathcal{E}(\\mathbf{X}_{t_{1}}^{\\mathrm{CXR}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "tEisosne $q(\\overline{{\\mathbf{Z}_{t_{1}}}}|\\mathbf{Z}_{t_{0}},\\mathbf{X}_{(t_{0},t_{1})}^{\\mathrm{EHR}})$ C.XRT hgee nLerDatMio na prperqouxiirmesa teuss  tthoi se sdtiismtraitbeu titohne  uvinad ear lyminogd eld adtias trdiibsturtiiboun$p_{\\theta}(\\mathbf{Z}_{t_{1}}^{(0)}|\\mathbf{Z}_{t_{0}},\\mathbf{X}_{(t_{0},t_{1})}^{\\mathrm{EHR}})$ , where $\\mathbf{Z}_{t_{1}}^{(0)}$ represents the prior of a CXR in the latent space encoded by the VAE. ", "page_idx": 14}, {"type": "text", "text": "We follow prior work on diffusion models to learn our LDM, which involves two processes [32, 22]. In the diffusion process, we gradually add Gaussian noise to $\\mathbf{Z}_{t_{1}}^{(0)}$ in $N$ steps, producing a sequence of noisy representations $\\mathbf{Z}_{t_{1}}^{(1)},\\mathbf{Z}_{t_{1}}^{(2)},...,\\mathbf{Z}_{t_{1}}^{(N)}$ , with the transition probability given by: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{q(\\mathbf{Z}_{t_{1}}^{(n)}|\\mathbf{Z}_{t_{1}}^{(n-1)})=\\mathcal{N}(\\mathbf{Z}_{t_{1}}^{(n)};\\sqrt{1-\\beta_{n}}\\mathbf{Z}_{t_{1}}^{n-1},\\beta_{n}\\mathbf{I}),}\\\\ &{}&{q(\\mathbf{Z}_{t_{1}}^{(n)}|\\mathbf{Z}_{t_{1}}^{(0)})=\\mathcal{N}(\\mathbf{Z}_{t_{1}}^{(n)};\\sqrt{\\bar{\\alpha}_{n}}\\mathbf{Z}_{t_{1}}^{(0)},(1-\\bar{\\alpha}_{n})\\epsilon),}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\pmb{\\epsilon}\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{I})$ represents the added noise. The noise level is represented by $\\begin{array}{r}{\\bar{\\alpha}_{n}:=\\prod_{s=1}^{n}(1\\!-\\!\\beta_{s})}\\end{array}$ , where $\\{\\beta_{n}\\in(0,1)\\}_{n=1}^{N}$ is a pre-defined variance schedule. At step $N$ , $\\mathbf{Z}_{t_{1}}^{(N)}\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{I})$ follows an isotropic Gaussian distribution. In the denoising process, we start with a sample from the isotropic Gaussian distribution $\\mathbf{Z}_{t_{1}}^{N}\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{I})$ and gradually recreate the latent CXR $\\mathbf{Z}_{t_{1}}^{(0)}$ , conditioned on the reference latent CXR, $\\mathbf{Z}_{t_{0}}$ , and the EHR data in between, X(EtH0,Rt1). The generation process is given by: ", "page_idx": 15}, {"type": "equation", "text": "$$\np_{\\theta}\\left(\\mathbf{Z}_{t_{1}}^{(0:N)}|\\mathbf{Z}_{t_{0}},\\mathbf{X}_{(t_{0},t_{1})}^{\\mathrm{EHR}}\\right)=p\\left(\\mathbf{Z}_{t_{1}}^{(N)}\\right)\\prod_{n=1}^{N}p_{\\theta}\\left(\\mathbf{Z}_{t_{1}}^{(n-1)}|\\mathbf{Z}_{t_{1}}^{(n)},\\mathbf{Z}_{t_{0}},\\mathbf{X}_{(t_{0},t_{1})}^{\\mathrm{EHR}}\\right),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where ", "page_idx": 15}, {"type": "equation", "text": "$$\np_{\\theta}\\left(\\mathbf{Z}_{t_{1}}^{(n-1)}|\\mathbf{Z}_{t_{1}}^{(n)},\\mathbf{Z}_{t_{0}},\\mathbf{X}_{(t_{0},t_{1})}^{\\mathrm{EHR}}\\right)=\\mathcal{N}\\left(\\mathbf{Z}_{t_{1}}^{(n-1)};\\mu_{\\theta}\\left(\\mathbf{Z}_{t_{1}}^{(n)},n,\\mathbf{Z}_{t_{0}},\\mathbf{X}_{(t_{0},t_{1})}^{\\mathrm{EHR}}\\right),\\sigma_{n}^{2}\\mathbf{I}\\right),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and the mean $\\pmb{\\mu}_{\\theta}$ and variance ${\\pmb\\sigma}_{n}^{2}$ are parameterized by ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mu_{\\theta}\\left(\\mathbf{Z}_{t_{1}}^{(n)},n,\\mathbf{Z}_{t_{0}},\\mathbf{X}_{(t_{0},t_{1})}^{\\mathrm{EHR}}\\right)=\\frac{1}{\\sqrt{\\alpha_{n}}}\\left(\\mathbf{Z}_{t_{1}}^{(n)}-\\frac{\\beta_{n}}{\\sqrt{1-\\bar{\\alpha}_{n}}}\\epsilon_{\\theta}\\left(\\mathbf{Z}_{t_{1}}^{(n)},\\mathbf{Z}_{t_{0}},f_{\\mathrm{cond}}^{\\mathrm{EHR}}\\big(\\mathbf{X}_{(t_{0},t_{1})}^{\\mathrm{EHR}}\\big),n\\right)\\right),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\pmb{\\sigma}_{n}^{2}=\\frac{1-\\bar{\\alpha}_{n-1}}{1-\\bar{\\alpha}_{n}}\\beta_{n},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $f_{\\mathrm{cond}}^{\\mathrm{EHR}}(\\cdot)$ is the encoder for the irregular EHR time series. $\\epsilon_{\\theta}\\left(\\mathbf{Z}_{t_{1}}^{(n)},\\mathbf{Z}_{t_{0}},f_{\\mathrm{cond}}^{\\mathrm{EHR}}(\\mathbf{X}_{(t_{0},t_{1})}^{\\mathrm{EHR}}),n\\right)$   \ndenotes the generation noise predicted by the neural backbone. ", "page_idx": 15}, {"type": "text", "text": "Prediction stage in DDL-CXR In the prediction stage, the EHR data is encoded using a one-layer Transformer with a model dimension of 128. We set the dimension of the feedforward layers to 512. The context dimension is also set to 128, and the number of attention heads is 8. We use another Transformer with the same architecture to encode the generated latent CXR $\\mathbf{Z}_{1}$ . We use a ResNet-34 model to encode the last available CXR image $\\mathbf{X}_{0}$ . The encoded EHR, the encoded latent CXR $\\mathbf{Z}_{1}$ , as well as the encoded $\\mathbf{X}_{0}$ are fed into a self-attention layer for final prediction. ", "page_idx": 15}, {"type": "text", "text": "A.2 Details of Data Preprocessing ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "EHR data preprocess We follow a similar EHR data extraction and processing pipeline as [10] but change the sampling frequency from 2h to 1h and introduce two static variables, age, and gender. We extract 17 clinical time series variables (12 continuous and 5 categorical) with discretization and standardization processes exactly the same as in [10]. In addition to the 17 clinical time series variables mentioned in the paper [10], e.g. five categorical (capillary refill rate, Glasgow Coma Scale eye-opening, Glasgow Coma Scale motor response, Glasgow Coma Scale verbal response, and Glasgow Coma Scale total) and 12 continuous (diastolic blood pressure, fraction of inspired oxygen, glucose, heart rate, height, mean blood pressure, oxygen saturation, respiratory rate, systolic blood pressure, temperature, weight, and pH), we introduce two crucial static variables (age and gender) to represent the demographic information of a patient, as patient demographic information is vital for achieving accurate predictions [55]. To construct our dataset, we sampled time series data at hourly intervals, followed by discretization and standardization processes. We adopt masks to handle missing values in time series to capture the missing pattern, acknowledging that the absence of medical data might be intentional and non-random, driven by caregivers [55]. ", "page_idx": 15}, {"type": "text", "text": "Data Cohort and Potential Selection Bias We summarize the number of samples in the LDM stage and the prediction stage in Table 6. The label prevalences of the two prediction tasks are summarized in Tables 7 and 8. We include the disease prevalence in Table 8 and data cohort statistics in Table 9. Here we summarize a few key points: ", "page_idx": 15}, {"type": "text", "text": "\u2022 The statistics of the clinical variables are close to each other, suggesting that patients in our data cohort generally have a similar distribution as that in the entire database.   \n\u2022 The overall disease phenotype prevalence is similar.   \n\u2022 For a few thorax-related diseases, our data cohort has a slightly higher prevalence, suggesting that potential selection bias exists. ", "page_idx": 15}, {"type": "table", "img_path": "uCvdw0IOuU/tmp/dc3d0c04ea899adca731c1c5f495ea3f538cbdaa99a20e079d4761eefab86f50.jpg", "table_caption": ["Table 6: Data statistics in training, validation, and testing sets for each stage. "], "table_footnote": [], "page_idx": 16}, {"type": "table", "img_path": "uCvdw0IOuU/tmp/b485b2b0876418cafa99185011876c6fb2465d9d278323f599d3e9b5ea857c91.jpg", "table_caption": ["Table 7: Label prevalence of in-hospital mortality prediction task. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "Table 8: Number of samples and prevalence of disease phenotypes in training, validation, and testing sets during the prediction stage. The prevalence of disease phenotypes among all ICU stays from MIMIC-IV database having $\\mathtt{L o S}\\geq48\\mathtt{h}$ is given in the last column. ", "page_idx": 16}, {"type": "table", "img_path": "uCvdw0IOuU/tmp/e8ff7e82a611a011c55290c908d4cd991a931d7e21e42eb4d47f928314f8617a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "table", "img_path": "uCvdw0IOuU/tmp/f82b6ead51dec2c669de778ad465e2d76242570ec89488c8e84c2df452550257.jpg", "table_caption": ["Table 9: Statistics of data cohort. We report mean\u00b1std for continuous variables and the mode for categorical variables. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "B Additional Results ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "B.1 AUPRC of Mortality Prediction ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We summarize the AUPRC score for the mortality prediction task in Table 10. ", "page_idx": 17}, {"type": "text", "text": "Table 10: The AUPRC score for mortality prediction for overall and different time gaps. $\\delta$ represents the time gap between the generation (or prediction) time and the occurrence time of the last available CXR. Numbers in bold indicate the best performance in each column. DDL-CXR outperforms all baselines in overall and $\\delta<24\\mathrm{h}$ settings. ", "page_idx": 17}, {"type": "table", "img_path": "uCvdw0IOuU/tmp/9eb33669b5b39322c440cce3805b0a4fc5d4cb7e2450d4fb7cebf0ec394cdd00.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "B.2 AUROC of Phenotype Prediction by Disease Label ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We summarize the AUROC score for the phenotype prediction task by disease label in Table 11. ", "page_idx": 17}, {"type": "text", "text": "Table 11: The AUROC score by disease labels. Results show that DDL-CXR effectively tackles the asynchronicity issue in clinical multi-modal fusion, achieving the highest average rank across all disease labels. ", "page_idx": 17}, {"type": "table", "img_path": "uCvdw0IOuU/tmp/4cb22e473c9b52c36bfabde698ae288154ea28a78a554ba1030fb033bed6fc69.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "B.3 AUPRC of Phenotype Prediction by Disease Label ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We summarize the AUPRC score for the phenotype prediction task by disease label in Table 12. ", "page_idx": 18}, {"type": "table", "img_path": "uCvdw0IOuU/tmp/93a3ea43f6be01421f2244951086f9e388354cea882112941393de32ca7f6369.jpg", "table_caption": ["Table 12: The AUPRC score by disease labels with detailed standard deviation. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "B.4 Ablation Study for a Reduced Percentage of Data ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "To investigate the sensitivity to the amount of training data of the proposed model and the main baselines, we conduct experiments with varying training sizes. The results are summarized in Table 13 and Table 14. Results show that DDL-CXR consistently outperforms baseline methods by a large margin, demonstrating its robustness against training data size. ", "page_idx": 18}, {"type": "table", "img_path": "uCvdw0IOuU/tmp/6804b6cb515be8b764ac42147450cc7e8ea04dd84f2a4e5008087a3c6abac570.jpg", "table_caption": ["Table 13: Performance of the phenotype classification task with different training sizes. Results are reported in mean\u00b1std. "], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "uCvdw0IOuU/tmp/3af47a42f6ab1a8d58c75efff9b865102cb01629a6c29e87922d7e8f9ba11d3a.jpg", "table_caption": ["Table 14: Performance of the mortality prediction task with different training sizes. Results are reported in mean\u00b1std. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "B.5 Additional Case Studies ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We demonstrate additional case studies in Fig. 4 \u2013 Fig. 9. We retrieve findings from the radiology reports. The case studies show that DDL-CXR could generate CXR images that align with the disease progression of the individual patient. ", "page_idx": 19}, {"type": "image", "img_path": "uCvdw0IOuU/tmp/e87c855828727da9829803a9fe0702206d4114fffec948a9fdfd64222e6d2aee.jpg", "img_caption": ["(a)Initial radiology findings: the chest demonstrates low lung volumes, which results in bronchovascular crowding. Bibasilar opacities may reflect atelectasis There is a probable small left pleural effusion. "], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "uCvdw0IOuU/tmp/1793498114bea00cef70881e67476b7d99ff2165b73a90ec880445b8137d636e.jpg", "img_caption": ["(b) Radiology findings after 8 hours: There is interval progression of interstitial pulmonary edema and potential interval increase in bibasal consolidations. "], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "uCvdw0IOuU/tmp/67dcba35e18ad7d5d91e358035e96f6cf5fe50e3b423ebb8384434c8c1c58909.jpg", "img_caption": ["(c) CXR image generated by DDL-CXR given the initial CXR image shown in (a) and the EHR data within the 8 hours. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Figure 4: Case Study of Sample #1 (b) Radiology findings after 18 hours: There are fluctuating patchy opacities at the right lung base suggestive of atelectasis. Low volumes with crowding of the pulmonary vasculature. ", "page_idx": 19}, {"type": "image", "img_path": "uCvdw0IOuU/tmp/517dfd7f04d0ed2b4c409731dd0bc2196c560e35279ad68ecb5454cbe1e89c81.jpg", "img_caption": ["(a) Heart remains enlarged with left ventricular prominence. Interval appearance of linear opacity in the right mid lung which may reflect fluid loculated within the minor fissure or possibly subsegmental atelectasis. "], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "uCvdw0IOuU/tmp/6d6f45a27d2a34db11fff5041fb5cb104d55856f6fb031278710fe6f80759a03.jpg", "img_caption": [], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "image", "img_path": "uCvdw0IOuU/tmp/e32188314ea369e84eee59ce62f246921d73665eff8563640c35ac596bd716af.jpg", "img_caption": ["(c) CXR image generated by DDL-CXR given the initial CXR image shown in (a) and the EHR data within the 18 hours. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Figure 5: Case Study of Sample #2 ", "text_level": 1, "page_idx": 19}, {"type": "image", "img_path": "uCvdw0IOuU/tmp/ad05c0857f95384f8e9f9e4380670461d1ec4b6ec670b044d54d9c1fd86c88c6.jpg", "img_caption": ["(a) The dense atelectatic streaks in the left mid zone has decreased. The bilateral chest tubes remain in place and there is no evidence of pneumothorax. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "(b) Radiology findings after 13 hours: No left pneumothorax is appreciated. The hemidiaphragms are now sharp be seen with only mild atelectatic changes at the bases. ", "page_idx": 19}, {"type": "image", "img_path": "uCvdw0IOuU/tmp/2fa6617ac932702c0ccf36708d2e79a96fc3758d7f1e2e05ae476886b387bf06.jpg", "img_caption": ["Figure 6: Case Study of Sample #3 "], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "uCvdw0IOuU/tmp/c37f35c72af54ee2124b275371f0f58f469975d8494dac1ebde2dcaec32c46bf.jpg", "img_caption": ["(c) CXR image generated by DDL-CXR given the initial CXR image shown in (a) and the EHR data within the 13 hours. "], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "uCvdw0IOuU/tmp/95553abdb9cbe3a2039f5e12a5c82dde9acc011779c0e9c0dfef710e9c7de69b.jpg", "img_caption": ["(a) Initial radiology findings:The lung volumes are low. Mild cardiomegaly without pulmonary edema. No pleural effusions. "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "uCvdw0IOuU/tmp/92c5fada2d580d5254bfd4b11d2baf8bfd12f98a3bea663e8121a4e16996fa16.jpg", "img_caption": ["(b) Radiology findings after 13 hours: There is mild bibasilar atelectasis. There is no pneumothorax or large pleural effusion. "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "uCvdw0IOuU/tmp/63c4067b7c3141dd8c7f7a303402f5709a46220923d18e6627b6fb4ec34c24c6.jpg", "img_caption": ["(c) CXR image generated by DDL-CXR given the initial CXR image shown in (a) and the EHR data within the 13 hours. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "Figure 7: Case Study of Sample #5 (b) Radiology findings after 31 hours: Bilateral nodular parenchymal opacities are unchanged in this patient with known lymphoma. There are likely layering effusions, left greater than right. ", "page_idx": 20}, {"type": "image", "img_path": "uCvdw0IOuU/tmp/813172b5d55243eca047a70c04ca4559f0be659247d7d05d36dad9491c050bec.jpg", "img_caption": ["(a) Initial radiology findings: The lungs bilaterally demonstrate severe, extensive rounded nodular densities. "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "uCvdw0IOuU/tmp/fe3a7a6505e73dc19b4225f2be199f9637c679327447604e7d172bf59f06cc29.jpg", "img_caption": [], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "image", "img_path": "uCvdw0IOuU/tmp/d182724a61567ed05772df0720c77790cee4d6b8eba6138d1b7984b5d7bbe95b.jpg", "img_caption": ["(c) CXR image generated by DDL-CXR given the initial CXR image shown in (a) and the EHR data within the 31 hours. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "Figure 8: Case Study of Sample #6 ", "page_idx": 20}, {"type": "image", "img_path": "uCvdw0IOuU/tmp/6829deb83cadaa8c9f4654d0ff0fabe104dcb30bd1f7f21664744f7b6c9cd035.jpg", "img_caption": ["(a)Initial radiology findings: Heart size is normal. Mediastinal and hilar contours are within normal limits. Pulmonary vasculature is normal. "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "uCvdw0IOuU/tmp/96a73d464a234e95137aa7ea32035d3afb1d03342feb89fe7b4f089c31589076.jpg", "img_caption": ["(b) Radiology findings after 29 hours: Small amount of right pleural effusion is present, more conspicuous than on the prior study. ", "Figure 9: Case Study of Sample #7 "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "uCvdw0IOuU/tmp/242cdebe5a38b49383a160c5dc17596f39ec1b04d0d3ad79b77df63f3196f164.jpg", "img_caption": ["(c) CXR image generated by DDL-CXR given the initial CXR image shown in (a) and the EHR data within the 29 hours. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: The abstract and introduction accurately reflected the paper\u2019s contributions and scope. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 21}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: The paper discusses the limitations in Section 5. We will address these limitations in future work. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 21}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: The paper does not include theoretical results. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 22}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: The related codes for reproducing the main experimental results are submitted in supplementary materials and the results are discussed in Section 4. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 22}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We submit the codes as supplemental material to the paper submission. The codes will be made publicly available upon paper acceptance. The datasets used in the paper are open-source and publicly accessible. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 23}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: The details of data splits and the hyperparameter searching space are illustrated in Section 4.1 and Appendix A. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 23}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: The results are reported by taking the average of five runs of model training along with the standard deviations. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 23}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 24}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: The details of computer resources for the experiments are given in Appendix A. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 24}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We conform in every respect with the NeurIPS Code of Ethics. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 24}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: The paper discusses both potential societal impacts and negative societal impacts of the work performed in Section 5. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. ", "page_idx": 24}, {"type": "text", "text": "\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 25}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: The paper poses no such risks. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 25}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We properly credited all code, data, and models used in the paper. The licenses are explicitly mentioned. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 26}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects.   \nThe paper utilizes de-identified public datasets. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 26}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: The study involves only public de-identified datasets and no IRB approvals needed. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 26}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 27}]