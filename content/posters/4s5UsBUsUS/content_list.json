[{"type": "text", "text": "VFIMamba: Video Frame Interpolation with State Space Models ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Guozhen Zhang1,2\u2217 Chunxu Liu1 Yutao Cui2 Xiaotong Zhao2 Kai Ma2 Limin Wang1,3\u2020 1State Key Laboratory for Novel Software Technology, Nanjing University 2Platform and Content Group (PCG), Tencent 3Shanghai AI Lab ", "page_idx": 0}, {"type": "text", "text": "https://github.com/MCG-NJU/VFIMamba ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Inter-frame modeling is pivotal in generating intermediate frames for video frame interpolation (VFI). Current approaches predominantly rely on convolution or attention-based models, which often either lack sufficient receptive fields or entail significant computational overheads. Recently, Selective State Space Models (S6) have emerged, tailored specifically for long sequence modeling, offering both linear complexity and data-dependent modeling capabilities. In this paper, we propose VFIMamba, a novel frame interpolation method for efficient and dynamic inter-frame modeling by harnessing the S6 model. Our approach introduces the Mixed-SSM Block (MSB), which initially rearranges tokens from adjacent frames in an interleaved fashion and subsequently applies multi-directional S6 modeling. This design facilitates the efficient transmission of information across frames while upholding linear complexity. Furthermore, we introduce a novel curriculum learning strategy that progressively cultivates proficiency in modeling inter-frame dynamics across varying motion magnitudes, fully unleashing the potential of the S6 model. Experimental findings showcase that our method attains state-of-the-art performance across diverse benchmarks, particularly excelling in high-resolution scenarios. In particular, on the X-TEST dataset, VFIMamba demonstrates a noteworthy improvement of 0.80 dB for 4K frames and 0.96 dB for 2K frames. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Video Frame Interpolation (VFI), a fundamental task in video data processing, is gaining substantial attention for its ability to generate intermediate frames between consecutive frames (Liu et al., 2017). Its utility spans many practical applications, including creating slow-motion videos through temporal upsampling (Jiang et al., 2018), enhancing video refresh rates (Reda et al., 2022), and generating novel views (Flynn et al., 2016; Szeliski, 1999). VFI typically encompass two primary stages (Zhang et al., 2023): firstly, conducting the inter-frame modeling of input consecutive frames; and secondly, leveraging the acquired information to estimate inter-frame motion and generate intermediate frame appearance. In practice, VFI often deals with high-resolution inputs (e.g., 4K) (Sim et al., 2021), which results in significant object displacement and imposes high demands on the large receptive field of the modules that model information between frames. Additionally, since VFI is commonly applied to long-duration videos such as movies, model speed is also of paramount importance. Thus, striking a delicate balance between a sufficient receptive field and fast processing speed in modeling inter-frame information is the key in crafting effective VFI models. ", "page_idx": 0}, {"type": "image", "img_path": "4s5UsBUsUS/tmp/86fc2cd096c868da516f7d50987eee1d2c9eb3ea8f92b95f951c488376c635ac.jpg", "img_caption": ["Figure 1: Equipped with the S6 model, our VFIMamba achieves the state-of-the-art performance on benchmarks across different input resolutions. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Current methods for modeling inter-frame information predominantly rely on convolutional neural networks (CNNs) (Liu et al., 2017; Kong et al., 2022; Huang et al., 2022) and attention-based models (Lu et al., 2022a; Zhang et al., 2023; Park et al., 2023; Liu et al., 2024a). However, as illustrated in the first three rows of Table 1, these methods either (1) lack flexibility and cannot adaptively model based on the input, (2) do not have sufficient receptive fields to capture inter-frame correlations at high resolutions, or (3) suffer from prohibitive computational complexity. ", "page_idx": 1}, {"type": "text", "text": "On the other hand, Natural Language Processing (NLP) has recently witnessed the emergence of structured state space models (SSMs) (Gu et al., 2021). Theoretically, SSMs combine the beneftis of Recurrent Neural Networks (RNNs) and CNNs, leveraging the global receptive field characteristic of RNNs and the computational efficiency inherent in CNNs. One particularly notable SSM is the Selective State Space Model (S6), also known as Mamba (Gu & Dao, 2023), which has garnered significant attention within the vision community. Mamba\u2019s novel feature of making SSM parameters time-variant (i.e., data-dependent) enables it to effectively select relevant context within sequences, a crucial factor for enhancing model performance. However, to the best of our knowledge, S6 has not yet been applied to low-level video tasks. ", "page_idx": 1}, {"type": "text", "text": "To address the challenges faced by current VFI models and to explore the potential of the S6 model (Gu & Dao, 2023) in low-level video tasks, we propose VFIMamba, a novel frame interpolation method that adapts the S6 model for efficient and dynamic inter-frame modeling. As shown in Table 1, VFIMamba provides the advantages of a global receptive field with linear complexity while maintaining data-dependent adaptability. ", "page_idx": 1}, {"type": "text", "text": "Specifically, we introduce the Mixed-SSM Block (MSB) to replace existing modules for inter-frame information transfer. The original S6 model can only process a single sequence, so it is necessary to merge tokens from two frames into one sequence for effective inter-frame modeling. After thorough analysis and exploration, we figured out that interleaving tokens from both frames into a \u201csuper image\u201d is more suitable for VFI. We then conduct multi-directional SSMs on this image to model inter-frame information. This interleaved approach facilitates interactions between adjacent tokens from different frames during sequence modeling and ensures that the intermediate tokens of any pair of tokens in the sequence are from their spatiotemporal neighborhood. By stacking MSB modules, our model effectively handles complex inter-frame information exchange. Finally, we use the extracted inter-frame features to estimate motion and generate the appearance of intermediate frames. ", "page_idx": 1}, {"type": "text", "text": "While the S6 model boasts the advantages listed in Table 1, it is crucial to employ appropriate training strategies to fully exploit its potential. Inspired by Bengio et al. (2009), we propose a novel curriculum learning strategy that progressively teaches the model to handle inter-frame modeling across varying motion amplitudes. Specifically, while maintaining training on Vimeo90K (Xue et al., ", "page_idx": 1}, {"type": "table", "img_path": "4s5UsBUsUS/tmp/5826b55c01f193750914f9c2d52c1de0ec58ff8f31f2ed082869408a91f77053.jpg", "table_caption": ["Table 1: Comparison of the model design for inter-frame modeling of VFIMamba and existing methods. VFIMamba enjoys both the advantages of a large receptive field and linear complexity. "], "table_footnote": [], "page_idx": 2}, {"type": "text", "text": "2019), we incrementally introduce large motion data from X-TRAIN (Sim et al., 2021), increasing the motion amplitude over time. This learning strategy enables VFIMamba to perform well across a wide range of motion amplitudes, thereby fully unleashing the potential of the S6 model. ", "page_idx": 2}, {"type": "text", "text": "To validate the effectiveness of VFIMamba across various types of video data, we conduct extensive testing on different benchmarks. As shown in Figure 1, VFIMamba achieves the state-of-the-art (SOTA) performance across diverse datasets. This is particularly evident in high-resolution and large-motion datasets such as X-TEST (Sim et al., 2021) and SNU-FILM (Choi et al., 2020). ", "page_idx": 2}, {"type": "text", "text": "Contribution. In summary, the contributions of this paper are as follows: (1) We are the first to adapt the S6 model to the video frame interpolation task. To better adapt the model for this task, we introduce the Mixed-SSM Block (MSB), providing a solid foundation for future architectural exploration in frame interpolation. (2) We propose a novel curriculum learning strategy that incrementally introduces data with varying motion amplitudes, thereby fully harnessing the potential of the S6 model. (3) Our model achieves the state-of-the-art performance across a wide range of datasets, potentially sparking interest in the exploration of the S6 model within the video low-level community. ", "page_idx": 2}, {"type": "text", "text": "2 Related work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 Video frame interpolation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The performance of VFI methods has seen significant advancements with the emergence of deep learning models. (1) CNNs-based approaches (Bao et al., 2019; Liu et al., 2017; Huang et al., 2022; Niklaus & Liu, 2018; Choi et al., 2020; Zhu et al., 2024b; Jia et al., 2022; Niklaus et al., 2017; Kalluri et al., 2023): Initially, DVF (Liu et al., 2017) utilized a U-Net-like (Ronneberger et al., 2015) network to model two input frames and predicted the voxel flow for warping the two frames into the intermediate frame. Following this, CtxSyn (Niklaus & Liu, 2018) introduced ContextNet and RefineNet, where ContextNet extracts context information from each frame, and RefineNet further refines the coarse intermediate frame produced by warping. RIFE (Huang et al., 2022) proposed a novel, efficient framework that employs self-distillation to significantly reduce computational load and parameters while maintaining high performance. Due to its simplicity, many convolutional modeling works (Kong et al., 2022; Jia et al., 2022) have improved upon RIFE. (2) Attention-based approaches (Lu et al., 2022a; Zhang et al., 2023; Park et al., 2023; Liu et al., 2024a): VFIFormer (Lu et al., 2022a) was the first to use attention to model inter-frame information, replacing the encoder part of U-Net with Transformer blocks. After that, EMA-VFI (Zhang et al., 2023) uses Swin-based (Liu et al., 2021) local attention to simultaneously capture local appearance and motion information. AMT (Li et al., 2023) used a multi-scale cost-volume construction similar to RAFT (Teed & Deng, 2020) to further enhance motion modeling capabilities. BiFormer (Park et al., 2023) introduced quasi-global bilateral attention to further increase the receptive field for large motions. SGM-VFI (Liu et al., 2024a) introduced sparse global matching to model motion between frames. However, current models struggle to balance sufficient receptive fields with computational overhead. In contrast, our method introduces the first interpolation model based on State Space Models (SSMs) (Gu & Dao, 2023) and further pushes the performance boundaries of VFI tasks. ", "page_idx": 2}, {"type": "text", "text": "2.2 State space models ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In the field of NLP, SSMs (Gu et al., 2021; Smith et al., 2022; Mehta et al., 2022; Fu et al., 2022) have recently emerged as one of the most promising contenders to challenge the dominance of Transformers. The Structured State Space Sequence Model (S4) (Gu et al., 2021) was initially introduced for linear complexity modeling of long sequences. Subsequent works have improved its computational efficiency and model capacity. S5 (Smith et al., 2022) proposed a parallel scan and MIMO SSM, and GSS (Mehta et al., 2022) enhanced the model\u2019s capability by introducing gated mechanisms. Mamba (S6) (Gu & Dao, 2023) has recently stood out due to its data-dependent parameter generation and efficient hardware implementation, outperforming Transformers in longsequence NLP tasks. In the visual domain, Vim (Zhu et al., 2024a) was the first to permute 2D images into sequences for global modeling using bidirectional SSMs. Vmamba (Liu et al., 2024b) extended to four directions and introduced a hierarchical structural design. VideoMamba (Li et al., 2024) was the first to apply S6 in the video domain by permuting all frames into a spatiotemporal sequence. MambaIR (Guo et al., 2024) was the first to use the S6 model for image restoration tasks, achieving superior performance over Transformers. In this work, we explore the potential of the S6 model in VFI tasks, validating its effectiveness through detailed analysis and experimentation. ", "page_idx": 2}, {"type": "image", "img_path": "4s5UsBUsUS/tmp/491630c08a28575edaed3f43629a906f8f8889eff7be743db7a6985ec855bc69.jpg", "img_caption": ["Figure 2: Overall pipeline of VFIMamba. Firstly, a lightweight feature extractor is employed to encode the input frames into shallow features. Subsequently, we utilize the Mixed-SSM Block (MSB) to conduct inter-frame modeling using S6, iterating $N$ times at each scale. Finally, these inter-frame features are leveraged to generate the intermediate frame. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "3 Method ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "3.1 Preliminaries ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "SSMs are mainly inspired by the continuous linear time-invariant (LTI) systems, which apply an implicit latent state $\\bar{h(t)}\\in\\bar{\\mathbb{R}^{N}}$ to map a 1-dimensional sequence or function $x(t)\\in\\mathbb{R}\\to y(t)\\in\\mathbb{R}$ Specifically, SSMs can be formulated as an ordinary differential equation (ODE): ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{h^{\\prime}(t)=A h(t)+B x(t),}}\\\\ {{y(t)=C h(t),}}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where contains evolution matrix $A\\in\\mathbb{R}^{N\\times N}$ , projection parameters $B\\in\\mathbb{R}^{N\\times1}$ and $C\\in\\mathbb{R}^{1\\times N}$ . However, it is hard to solve the above differential equation in deep learning settings and needs to be approximated through discretization. Recent SSMs (Gu et al., 2021) propose to introduce a timescale parameter $\\Delta$ to transform the $A,B$ to their discrete counterparts $\\bar{A},\\bar{B}$ , i.e., ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\boldsymbol{h}_{t}=\\bar{\\boldsymbol{A}}\\boldsymbol{h}_{t-1}+\\bar{\\boldsymbol{B}}\\boldsymbol{x}_{t},}\\\\ &{\\boldsymbol{y}_{t}=C\\boldsymbol{h}_{t},}\\\\ &{\\bar{\\boldsymbol{A}}=\\exp\\left(\\Delta\\boldsymbol{A}\\right),}\\\\ &{\\bar{\\boldsymbol{B}}=\\left(\\Delta\\boldsymbol{A}\\right)^{-1}\\left(\\exp\\left(\\Delta\\boldsymbol{A}-\\boldsymbol{I}\\right)\\right)\\cdot\\Delta\\boldsymbol{B}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The above SSMs are performed for each channel separately and their parameters are data-independent, meaning that $\\bar{A}$ , $\\bar{B}$ and $C$ are the same for any input in the same channel, limiting their flexibility in sequence modeling. Mamba (Gu & Dao, 2023) proposes the selective SSMs (S6), which dynamically ", "page_idx": 3}, {"type": "text", "text": "generate the parameters for each input data $x_{i}\\in\\mathbb{R}^{L}$ using the entire $x_{i}$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\nB_{i}=S_{B}x_{i},\\qquad C_{i}=S_{C}x_{i},\\qquad\\Delta_{i}={\\sf S o f t p l u s}\\left(S_{\\Delta}x_{i}\\right),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $s_{B}\\in\\mathbb{R}^{N\\times L},S_{C}\\in\\mathbb{R}^{N\\times L},S_{\\Delta}\\in\\mathbb{R}^{L\\times L}$ are linear projection layers. The $B_{i}$ and $C_{i}$ are shared for all channels of $x_{i}$ , $\\Delta_{i}$ contains $\\Delta$ of $L$ channels, and $A$ are the same as previous SSMs. By the discretization in equations 5 and 6, $\\bar{A}$ and $\\bar{B}$ become different based on input data. ", "page_idx": 4}, {"type": "text", "text": "3.2 Overall pipeline ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Given two consecutive frames $I_{0},I_{1}\\in\\mathbb{R}^{H\\times W\\times3}$ along with a timestep $t$ , the objective of the frame interpolation task is to generate the intermediate frame $I_{t}\\in\\mathbb{R}^{H\\times W\\times3}$ . As illustrated in Figure 2, the overall pipeline of VFIMamba consists of three main components: frame feature extraction, interframe modeling, and frame generation. Firstly, we employ a set of lightweight convolutional layers to independently extract shallow features from each frame, progressively reducing the resolution to facilitate more efficient inter-frame modeling. This process can be formulated as: ", "page_idx": 4}, {"type": "equation", "text": "$$\nF_{l}^{i}=\\mathcal{L}(I_{i}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathcal{L}$ represents the set of convolutional layers, and $F_{l}^{i}$ denotes the extracted low-level feature for $I_{i}$ . Next, we perform multi-resolution inter-frame modeling using the proposed Mixed-SSM Block (MSB). Each scale comprises $N$ MSBs, and downsampling between scales is achieved through overlapping patch embedding (Wang et al., 2022). We define the resulting inter-frame features as $F_{s s m}^{i}$ . Finally, we utilize these high-quality inter-frame features for frame generation, which involves motion estimation between two frames and appearance refinement: ", "page_idx": 4}, {"type": "equation", "text": "$$\nI_{t}=\\mathcal{G}(F_{s s m}^{0},F_{s s m}^{1}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathcal{G}$ denotes the frame generation network. Since this work primarily focuses on exploring the use of SSMs for inter-frame modeling, we largely follow the design from Zhang et al. (2023) and Huang et al. (2022) for the frame generation components, with detailed specifications provided in the appendix. ", "page_idx": 4}, {"type": "text", "text": "3.3 State space models for inter-frame modeling ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Effective inter-frame modeling is crucial for frame interpolation tasks (Zhang et al., 2023). Methods such as RIFE (Huang et al., 2022) and EMA-VFI (Zhang et al., 2023) employ simple convolution layers or local attention for inter-frame modeling, achieving high inference speeds but limiting receptive field. Conversely, SGM-VFI (Liu et al., 2024a) uses global inter-frame attention for motion estimation, which improves performance but sacrifices efficiency. In this work, we propose to use state space models (SSMs), specifically S6 (Gu & Dao, 2023), to achieve both efficiency and effectiveness in inter-frame modeling. ", "page_idx": 4}, {"type": "text", "text": "3.3.1 Mixed-SSM block ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "To facilitate more efficient inter-frame information exchange globally, we utilize SSMs for inter-frame modeling. As illustrated in Figure 2, we introduce the Mixed-SSM Block (MSB) for integrate the S6 model into VFI frameworks. The overall design of the MSB is analogous to Transformer (Vaswani et al., 2017) blocks, but with two pivotal distinctions: (1) We substitute the attention mechanism with an enhanced S6 Block (Gu & Dao, 2023), which could conduct global inter-frame modeling with linear complexity. (2) Drawing inspiration from Guo et al. (2024) and Behrouz et al. (2024), which identified the lack of locality and inter-channel interaction in SSMs, we replace the multilayer perceptron (MLP) with a Channel-Attention Block (CAB) (Hu et al., 2018). ", "page_idx": 4}, {"type": "text", "text": "The original S6 model is limited to processing one-dimensional sequences, necessitating a strategy for scanning the feature maps of two input frames for inter-frame modeling. As depicted in Figure 3, there are two primary methods to rearrange the two frames: sequential rearrangement, where the frames are concatenated into a single super image, and interleaved rearrangement, where the tokens of the two frames are interleaved to form a super image. Regardless of the rearrangement method, following Liu et al. (2024b), the super image can be scanned in four directions: horizontally, vertically, and in their respective reverse directions. The S6 Block is then employed to model each direction independently, and the resulting sequences are rearranged and merged back. ", "page_idx": 4}, {"type": "image", "img_path": "4s5UsBUsUS/tmp/a171d336bf5d8857218f5e33390247420987afa1064aa27b03b9c9e8cfc8abdc.jpg", "img_caption": ["Figure 3: Visualizations of different rearrangement methods and scan directions. The choice of rearrangement strategy impacts the information flow during inter-frame modeling with S6. For example, consider the 6-th token from $I_{0}$ and the 11-th token from $I_{1}$ . In sequential rearrangement, the intermediate tokens introduce too many irrelevant tokens, whereas interleaved rearrangement more effectively preserves the spatiotemporal locality. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "3.3.2 Analysis on rearrangement strategies ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Here, we discuss which rearrangement method is better for inter-frame modeling in the context of frame interpolation. First, let us introduce a conclusion from (Ali et al., 2024): the S6 layers can be approximated as hidden attention layers, with the attention weights given by: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\alpha_{i,j}\\approx Q_{i}K_{j}H_{i,j},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where ", "page_idx": 5}, {"type": "equation", "text": "$$\nQ_{i}=S_{C}x_{i},\\quad K_{j}=\\mathtt{R e L U}\\left(S_{\\Delta}x_{j}\\right)S_{B}x_{j},\\quad H_{i,j}=\\exp(\\sum_{k\\in(i,j)\\atop S_{\\Delta}x_{k}>0}(S_{\\Delta}x_{k}))A,\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "In this formulation, $\\alpha_{i,j}$ represents the hidden attention weight of the $j\\!\\cdot$ -th token $x_{j}$ to the $i$ -th token $x_{i}$ in the sequence. Unlike attention, which calculates weights based solely on the information from tokens $x_{i}\\left(Q_{i}\\right)$ and $x_{j}\\left(K_{j}\\right)$ , the S6 model includes $H_{i,j}$ , which encompasses the contextual information between the $i$ -th and $j$ -th tokens in the sequence. Based on this conclusion, we observe that in the interleaved rearrangement, the intermediate tokens of any pair of tokens in the sequence are from their spatiotemporal neighborhood. This means that $H_{i,j}$ incorporates more local modeling, which is beneficial for low-level tasks like frame interpolation. Additionally, the number of intermediate tokens between spatiotemporally adjacent tokens is generally smaller in the interleaved rearrangement. In contrast, in the sequential rearrangement, even spatiotemporally adjacent tokens are separated by many unrelated tokens in the sequence. This can introduce noise and interfere with the modeling of the relationship between these tokens. A specific example can be seen in Figure 3, where the tokens between the 6-th token of the first frame and the 11-th token of the second frame differs significantly between the two rearrangement methods. In summary, we believe that for video frame interpolation, the interleaved rearrangement method is more suitable for better local spatially-aware processing. Our experiments, detailed in Section 4.2, further validate this conclusion. ", "page_idx": 5}, {"type": "text", "text": "3.4 Curriculum learning for VFIMamba ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Despite the advantageous characteristics of the S6 model, such as data dependence and global receptive field, it is crucial to fully exploit its potential through appropriate training strategies. Currently, two main training strategies are employed for frame interpolation: (1) Vimeo90K Only: Most methods training models exclusively on the Vimeo90K (Xue et al., 2019). Although Vimeo90K offers a rich variety of video content, as analyzed by Liu et al. (2024a) and Kiefhaber et al. (2024), its motions have limited magnitude. This restriction hampers the model\u2019s performance on inputs with large motions or high resolution. (2) Sequential Learning: To mitigate the limitations of training solely on Vimeo90K, some methods (Liu et al., $2024\\mathrm{a}$ ; Park et al., 2023) further train the model on X-TRAIN (Sim et al., 2021), a dataset characterized by large motions and high-resolution content, after initial training on Vimeo90K. While this approach successfully enhances the model\u2019s performance on high-resolution data, it often leads to the forgetting of the small-motion modeling capabilities acquired from Vimeo90K. ", "page_idx": 5}, {"type": "table", "img_path": "4s5UsBUsUS/tmp/f804653bc0983c378603a038a1cc2b9f7f43356d91f581e49a2548296adb172b.jpg", "table_caption": ["Table 2: Quantitative comparison with SOTA methods on the low-resolution datasets, in terms of PSNR/SSIM (Wang et al., 2004). The best results and the second best results are boldfaced and underlined respectively. FLOPs was calculated for 720p input. $\\star$ indicates the results copied from Zhang et al. (2023) and Li et al. (2023). In \u201cTraining Dataset\u201d, \u201cV\u201d stands for the triplet of Vimeo-90K and \u201cX\u201d stands for X-TRAIN. In \u201cRuntime\u201d, we evaluate the inference speed of each method on $1024\\times{1024}$ resolution inputs by a 2080Ti GPU. In \u201cAverage\u201d, we calculate the average performance of each method in terms of PSNR and SSIM. "], "table_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "4s5UsBUsUS/tmp/2b2bd66624b5202139fe9b0a13a7bf386ffe6c1b9d6ad9147702bdc6654d21ce.jpg", "table_caption": ["Table 3: Quantitative comparison with SOTA methods on high-resolution datasets. \u201cOOM\u201d indicates \u201cOut of Memory\u201d on V100. All results are obtained through the same evaluation procedure. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "To address these issues and fully exploit the potential of the S6 model, inspired by Bengio et al. (2009), we propose a curriculum learning strategy for learning inter-frame modeling capabilities across varying motion magnitudes while maintaining the ability to model small motions. Specifically, while continuing training on Vimeo90K, we progressively incorporated data from X-TRAIN. The original size of X-TRAIN is $512\\!\\times\\!512$ , to co-train with Vimeo90K, we first resize the frames to $S\\times S$ and then random crop to the same as Vimeo90K. Every $T$ epochs, the resized size $S$ is increased by $10\\%$ (starting from 256), and the temporal interval between selected frames is doubled (starting from 2), which means the motion magnitude increases as training progresses. This strategy enables the model to gradually learn inter-frame modeling capabilities across varying motion magnitudes, starting with smaller motions and progressing to larger ones. ", "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We provide two models: a lightweight model, VFIMamba-S, and a high-performance model, VFIMamba. Both models have $N=3$ ; the only difference is that VFIMamba has twice the number of channels as VFIMamba-S. As described in Section 3.4, we employ a curriculum learning strategy in which $T=50$ and train for 300 epochs in total. More model configurations and training details are provided in the appendix. ", "page_idx": 6}, {"type": "image", "img_path": "4s5UsBUsUS/tmp/68e16908e9be96ef20438123f3d7cec018354a7ff88ab7caaf1167fc8e51fa0c.jpg", "img_caption": ["Figure 4: Visualizations from SNU-FILM (Reda et al., 2022) and X-TEST (Sim et al., 2021). "], "img_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "4s5UsBUsUS/tmp/a16bf87a24caac708f49cccfe8f32da645e4b68ce09bb3a94c1651362272af50.jpg", "img_caption": ["Figure 5: Comparisons of FLOPs and GPU memory usage with increasing resolution input. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "4.1 Comparison with the State-of-the-Art Methods ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Quantitative comparison. To validate the versatility of our proposed VFIMamba, we evaluated its performance (PSNR/SSIM) (Wang et al., 2004) across a variety of well-known benchmarks with different resolutions. The low-resolution datasets include Vimeo90K $(448\\times256)$ (Xue et al., 2019), UCF101 $(256\\!\\times\\!256)$ (Soomro et al., 2012), and SNU-FILM $1280\\!\\times\\!720\\!)$ ) (Reda et al., 2022). Notably, SNU-FILM is categorized into four levels of difficulty based on frame intervals: Easy, Medium, Hard, and Extreme. The high-resolution datasets include X-TEST (Sim et al., 2021), X-TEST-L (a more challenging subset selected by Liu et al. (2024a)), and Xiph (Montgomery, 1994). Originally, these datasets are in 4K resolution, and following Zhang et al. (2023), we also resize them to 2K for testing. ", "page_idx": 7}, {"type": "text", "text": "For $8\\mathbf{x}$ interpolation, we followed the testing procedure of FILM (Reda et al., 2022) and used an iterative approach for frame interpolation. Specifically, we first generated an intermediate frame based on the input two frames, and then, using a divide-and-conquer strategy, we further divided the first frame and the generated intermediate frame, as well as the generated intermediate frame and the last frame, to iteratively generate the remaining frames. ", "page_idx": 7}, {"type": "text", "text": "As shown in Tables 2 and 3, VFIMamba achieves state-of-the-art performance on almost all datasets with FLOPs comparable to efficient models (Kong et al., 2022; Zhang et al., 2023). Specifically, in large motion scenarios like X-TEST and X-TEST-L, VFIMamba demonstrates a noteworthy improvement compared with previous metod. This excellent performance underscores the potential of the S6 model in frame interpolation tasks, and we hope it will draw more attention to the application of SSMs in low-level video tasks. ", "page_idx": 7}, {"type": "text", "text": "Qualitative comparison. To further validate the practical effectiveness of VFIMamba, we also present a visual comparison with other frame interpolation methods. As illustrated in Figure 4, the arrows highlight areas where our method excels. VFIMamba demonstrates superior motion estimation and detail preservation in high-motion scenarios compared to other methods. This further substantiates that the incorporation of the S6 model enhances the performance of inter-frame interpolation tasks. ", "page_idx": 7}, {"type": "table", "img_path": "4s5UsBUsUS/tmp/46becaba5f99d76d8c51832b35575f8153b2f1646f58bcd84c40d3bd837a760c.jpg", "table_caption": ["Table 4: Ablation on different models for inter-frame modeling. We use the V100 GPU for evaluating and \u201cOOM\u201d indicates \u201cOut of Memory\u201d. "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "4s5UsBUsUS/tmp/e5052781f09985d2554b94671e016d3802edc24965cab3e83f23a1a83006300a.jpg", "table_caption": ["Table 5: Ablation on different rearrangement approachs. \u201cSequential\" means sequential rearrangement and \u201cInterleaved\" represents interleaved rearrangement . "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Efficiency comparison. To validate the efficiency of VFIMamba, we compared the FLOPs and GPU memory usage required by various high-performance methods (Li et al. (2023) and Lu et al. (2022b)) as the resolution increases. As shown in Figure 5, VFIMamba requires significantly fewer FLOPs and GPU memory as the input resolution grows, demonstrating the effectiveness of the S6 model in the VFI task. ", "page_idx": 8}, {"type": "text", "text": "4.2 Ablation Study ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, we conduct ablation studies using the VFIMamba-S model for efficiency. ", "page_idx": 8}, {"type": "text", "text": "Effect of the S6 for VFI. As a core contribution of this work, the S6 model balances computational efficiency and high performance for inter-frame modeling. To validate its effectiveness, as shown in Table 4, we experimented by removing the SSM model from the MSB (w/o SSM), replacing the MSB with convolutions from RIFE (Huang et al., 2022) (Convolution), or local inter-frame attention from EMA-VFI (Zhang et al., 2023) (Local Attention), or global inter-frame attention (Liu et al., 2024a) (Full Attention). We observed that only removing the S6 model resulted in a parameter reduction of only 0.7M but led to a significant performance drop across various datasets, underscoring the importance of S6. In comparisons with Convolution and Local Attention, we found that although the S6 model is relatively slower due to its multiple scanning directions, it achieves substantial performance improvements. Compared to Full Attention, S6 not only surpasses its performance but also offers faster inference speed and lower memory consumption. In summary, the S6 model indeed achieves a balance between computational efficiency and performance compared to existing models. ", "page_idx": 8}, {"type": "text", "text": "Frame rearrangement for inter-frame modeling. The rearrangement of input frames is crucial for inter-frame modeling using the S6 model. As analyzed in Section 3.3.2, we posit that interleaved rearrangement is more suitable for VFI tasks, and we provide experimental validation here. As shown in Table 5, we experimented with two different rearrangement methods in both horizontal and vertical scans. The results demonstrate that using interleaved rearrangement consistently achieves the best performance across all datasets, with significant improvements over other methods. These findings further validate our analysis that interleaved rearrangement offers superior spatiotemporal local modeling capabilities for VFI. ", "page_idx": 8}, {"type": "image", "img_path": "4s5UsBUsUS/tmp/1fd686b2b9f2f7a20d964af3427c61a02f33d4225381034da63dfe4c575cfd24.jpg", "img_caption": ["Figure 6: Performance of different learning methods, recorded every 30 epochs. Curriculum learning has the best performance in both the low-resolution and high-resolution benchmarks eventually. "], "img_footnote": [], "page_idx": 9}, {"type": "table", "img_path": "4s5UsBUsUS/tmp/c48eda0da056953fafa0df16aca32497e7def7f47b35028b4a095665d8cd7d48.jpg", "table_caption": ["Table 6: Performance of different methods without or with curriculum learning. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "Explore different learning strategy. As described in Section 3.4, we proposed a curriculum learning strategy to fully harness the global modeling capabilities of the S6 model. In Figure 6, we present the performance of different learning strategies over training epochs on both Vimeo90K and X-TEST. In addition to the Vimeo90K Only and Sequential Learning strategies mentioned in Section 3.4, we also compared a baseline approach where the two datasets were directly mixed for training (Mixed Learning). The results indicate that as epochs increase, the Vimeo90K Only strategy improves performance exclusively on Vimeo90K with negligible change on X-TEST. Sequential Learning, while eventually enhancing X-TEST performance, significantly degrades performance on Vimeo90K. Mixed Learning shows a gradual increase in performance on both datasets but fails to achieve competitive results. Our proposed curriculum learning strategy, however, achieves the best performance on both datasets simultaneously by the end of training. ", "page_idx": 9}, {"type": "text", "text": "Generalization of curriculum learning To validate the generalization capability of curriculum learning, we also trained the RIFE (Huang et al., 2022) and EMA-VFI (Zhang et al., 2023) from scratch using curriculum learning. As shown in Table 6, after training, all models maintained their performance on the low-resolution dataset Vimeo90K while significantly improving performance on the X-TEST and SNU-FILM, fully verified the generalization of curriculum learning. Among these, our VFIMamba achieved the most significant improvement and the highest performance ceiling, further demonstrating the potential of the S6 model. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we have introduced VFIMamba, the first approach to adapt the SSM model to the video frame interpolation task. To achieve global inter-frame modeling with linear complexity, we devise the Mixed-SSM Block (MSB) for efficient inter-frame modeling using S6. We also explore various rearrangement methods to convert two frames into a sequence, discovering that interleaved rearrangement is more suitable for VFI tasks. Additionally, we propose a curriculum learning strategy to further leverage the potential of the S6 model. Experimental results demonstrate that VFIMamba achieves the state-of-the-art performance across various datasets, in particular highlighting the potential of the SSM model for VFI tasks with high resolution. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgement ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This work is supported by the National Key R&D Program of China (No. 2022ZD0160900), the National Natural Science Foundation of China (No. 62076119), the Fundamental Research Funds for the Central Universities (No. 020214380119), the Nanjing University- China Mobile Communications Group Co., Ltd. Joint Institute, and the Collaborative Innovation Center of Novel Software Technology and Industrialization. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Ali, A., Zimerman, I., and Wolf, L. The hidden attention of mamba models. arXiv preprint arXiv:2403.01590, 2024.   \nBao, W., Lai, W.-S., Ma, C., Zhang, X., Gao, Z., and Yang, M.-H. Depth-aware video frame interpolation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 3703\u20133712, 2019.   \nBehrouz, A., Santacatterina, M., and Zabih, R. Mambamixer: Efficient selective state space models with dual token and channel selection. arXiv preprint arXiv:2403.19888, 2024.   \nBengio, Y., Louradour, J., Collobert, R., and Weston, J. Curriculum learning. In International Conference on Machine Learning, pp. 41\u201348, 2009.   \nChoi, M., Kim, H., Han, B., Xu, N., and Lee, K. M. Channel attention is all you need for video frame interpolation. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pp. 10663\u201310671, 2020.   \nFlynn, J., Neulander, I., Philbin, J., and Snavely, N. Deepstereo: Learning to predict new views from the world\u2019s imagery. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5515\u20135524, 2016.   \nFu, D. Y., Dao, T., Saab, K. K., Thomas, A. W., Rudra, A., and Re, C. Hungry hungry hippos: Towards language modeling with state space models. In International Conference on Learning Representations, 2022.   \nGu, A. and Dao, T. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023.   \nGu, A., Goel, K., and Re, C. Efficiently modeling long sequences with structured state spaces. In International Conference on Learning Representations, 2021.   \nGuo, H., Li, J., Dai, T., Ouyang, Z., Ren, X., and Xia, S.-T. Mambair: A simple baseline for image restoration with state-space model. arXiv preprint arXiv:2402.15648, 2024.   \nHe, K., Zhang, X., Ren, S., and Sun, J. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1026\u20131034, 2015.   \nHu, J., Shen, L., and Sun, G. Squeeze-and-excitation networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 7132\u20137141, 2018.   \nHu, P., Niklaus, S., Sclaroff, S., and Saenko, K. Many-to-many splatting for efficient video frame interpolation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 3553\u20133562, 2022.   \nHuang, Z., Zhang, T., Heng, W., Shi, B., and Zhou, S. Real-time intermediate flow estimation for video frame interpolation. In European Conference on Computer Vision, pp. 624\u2013642. Springer, 2022.   \nJia, Z., Lu, Y., and Li, H. Neighbor correspondence matching for flow-based video frame synthesis. In ACM MM, pp. 5389\u20135397, 2022.   \nJiang, H., Sun, D., Jampani, V., Yang, M.-H., Learned-Miller, E., and Kautz, J. Super slomo: High quality estimation of multiple intermediate frames for video interpolation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9000\u20139008, 2018.   \nKalluri, T., Pathak, D., Chandraker, M., and Tran, D. Flavr: Flow-agnostic video representations for fast frame interpolation. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pp. 2071\u20132082, 2023.   \nKiefhaber, S., Niklaus, S., Liu, F., and Schaub-Meyer, S. Benchmarking video frame interpolation. arXiv preprint arXiv:2403.17128, 2024.   \nKong, L., Jiang, B., Luo, D., Chu, W., Huang, X., Tai, Y., Wang, C., and Yang, J. Ifrnet: Intermediate feature refine network for efficient frame interpolation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1969\u20131978, 2022.   \nLee, H., Kim, T., Chung, T.-y., Pak, D., Ban, Y., and Lee, S. Adacof: Adaptive collaboration of flows for video frame interpolation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5316\u20135325, 2020.   \nLi, K., Li, X., Wang, Y., He, Y., Wang, Y., Wang, L., and Qiao, Y. Videomamba: State space model for efficient video understanding. arXiv preprint arXiv:2403.06977, 2024.   \nLi, Z., Zhu, Z.-L., Han, L.-H., Hou, Q., Guo, C.-L., and Cheng, M.-M. Amt: All-pairs multi-field transforms for efficient frame interpolation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9801\u20139810, 2023.   \nLiu, C., Zhang, G., Zhao, R., and Wang, L. Sparse global matching for video frame interpolation with large motion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024a.   \nLiu, P., Lyu, M., King, I., and Xu, J. Selflow: Self-supervised learning of optical flow. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4571\u20134580, 2019.   \nLiu, Y., Tian, Y., Zhao, Y., Yu, H., Xie, L., Wang, Y., Ye, Q., and Liu, Y. Vmamba: Visual state space model. arXiv preprint arXiv:2401.10166, 2024b.   \nLiu, Z., Yeh, R. A., Tang, X., Liu, Y., and Agarwala, A. Video frame synthesis using deep voxel flow. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 4463\u20134471, 2017.   \nLiu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., and Guo, B. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 10012\u201310022, 2021.   \nLu, L., Wu, R., Lin, H., Lu, J., and Jia, J. Video frame interpolation with transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 3532\u20133542, 2022a.   \nLu, L., Wu, R., Lin, H., Lu, J., and Jia, J. Video frame interpolation with transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 3532\u20133542, 2022b.   \nLuo, W., Li, Y., Urtasun, R., and Zemel, R. Understanding the effective receptive field in deep convolutional neural networks. Advances in Neural Information Processing Systems, 29, 2016.   \nMehta, H., Gupta, A., Cutkosky, A., and Neyshabur, B. Long range language modeling via gated state spaces. arXiv preprint arXiv:2206.13947, 2022.   \nMontgomery, C. Xiph.org video test media (derf\u2019s collection). In Online,https://media.xiph.org/video/derf/, 1994.   \nNiklaus, S. and Liu, F. Context-aware synthesis for video frame interpolation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1701\u20131710, 2018.   \nNiklaus, S. and Liu, F. Softmax splatting for video frame interpolation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5437\u20135446, 2020.   \nNiklaus, S., Mai, L., and Liu, F. Video frame interpolation via adaptive separable convolution. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 261\u2013270, 2017.   \nNottebaum, M., Roth, S., and Schaub-Meyer, S. Efficient feature extraction for high-resolution video frame interpolation. In British Machine Vision Conference BMVC, 2022.   \nPark, J., Kim, J., and Kim, C.-S. Biformer: Learning bilateral motion estimation via bilateral transformer for 4k video frame interpolation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1568\u20131577, 2023.   \nReda, F., Kontkanen, J., Tabellion, E., Sun, D., Pantofaru, C., and Curless, B. Film: Frame interpolation for large motion. In European Conference on Computer Vision, pp. 250\u2013266. Springer, 2022.   \nRonneberger, O., Fischer, P., and Brox, T. U-net: Convolutional networks for biomedical image segmentation. In Medical image computing and computer-assisted intervention\u2013MICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18, pp. 234\u2013241. Springer, 2015.   \nSim, H., Oh, J., and Kim, M. Xvfi: extreme video frame interpolation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 14489\u201314498, 2021.   \nSmith, J. T., Warrington, A., and Linderman, S. Simplified state space layers for sequence modeling. In International Conference on Learning Representations, 2022.   \nSoomro, K., Zamir, A. R., and Shah, M. Ucf101: A dataset of 101 human actions classes from videos in the wild. arXiv preprint arXiv:1212.0402, 2012.   \nSzeliski, R. Prediction error as a quality metric for motion and stereo. In Proceedings of the IEEE/CVF International Conference on Computer Vision, volume 2, pp. 781\u2013788. IEEE, 1999.   \nTeed, Z. and Deng, J. Raft: Recurrent all-pairs field transforms for optical flow. In European Conference on Computer Vision, pp. 402\u2013419. Springer, 2020.   \nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. Attention is all you need. Advances in Neural Information Processing Systems, 30, 2017.   \nWang, W., Xie, E., Li, X., Fan, D.-P., Song, K., Liang, D., Lu, T., Luo, P., and Shao, L. Pvt v2: Improved baselines with pyramid vision transformer. Computational Visual Media, 8(3):415\u2013424, 2022.   \nWang, Z., Bovik, A. C., Sheikh, H. R., and Simoncelli, E. P. Image quality assessment: from error visibility to structural similarity. IEEE TIP, 13(4):600\u2013612, 2004.   \nXue, T., Chen, B., Wu, J., Wei, D., and Freeman, W. T. Video enhancement with task-oriented flow. International Journal of Computer Vision, 127:1106\u20131125, 2019.   \nZhang, G., Zhu, Y., Wang, H., Chen, Y., Wu, G., and Wang, L. Extracting motion and appearance via inter-frame attention for efficient video frame interpolation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5682\u20135692, 2023.   \nZhu, L., Liao, B., Zhang, Q., Wang, X., Liu, W., and Wang, X. Vision mamba: Efficient visual representation learning with bidirectional state space model. arXiv preprint arXiv:2401.09417, 2024a.   \nZhu, Y., Zhang, G., Tan, J., Wu, G., and Wang, L. Dual detrs for multi-label temporal action detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 18559\u201318569, 2024b. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "image", "img_path": "4s5UsBUsUS/tmp/010477368f2369807c8bffa92b356ca597c3260c0f74d726c4119e872f3be6ce.jpg", "img_caption": ["Figure 7: Visualizations of Effective Receptive Field (ERF) (Luo et al., 2016) of different models before and after training. We utilize the red area in $I_{0}$ to inspect its corresponding ERF in $I_{1}$ . S6 model has a significantly larger receptive field and becomes more accurate after training. "], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "A.1 Broader impact ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this work, we introduce VFIMamba, the first video frame interpolation model based on SSMs. Video frame interpolation has wide-ranging applications in real-world video data processing, such as increasing the frame rate of AI-generated videos and generating slow-motion videos. Enhancing performance in various scenarios is crucial. However, as a research-oriented work, we trained our model on a very limited set of datasets (Vimeo90K (Xue et al., 2019) and X-TRAIN (Sim et al., 2021)), which might result in some degree of overfitting. Consequently, there could be significant artifacts when applied in real-world usage. This issue can be mitigated by training on a more diverse and extensive set of datasets. ", "page_idx": 13}, {"type": "text", "text": "A.2 Limitations and future work ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "As the first work to explore the application of SSM models in frame interpolation tasks, we have achieved high performance, but there are still some limitations. First, although our method is much faster than attention-based methods, it still falls short of real-time requirements. Future work on designing a more efficient SSMs would be highly valuable. Second, in this work, we primarily focused on the role of SSM in inter-frame modeling and did not explore its use in the frame generation module. In the future, directly using SSM for generating intermediate frames could also be a promising direction for exploration. ", "page_idx": 13}, {"type": "text", "text": "A.3 Visualizations on effective receptive field ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "To further evaluate the effective receptive field (ERF) of the S6 model in comparison with other efficient models (CNN, Local Attention) for inter-frame modeling, we used the method described by Luo et al. (2016). Given a specific region in $I_{0}$ , we visualized the corresponding receptive fields in $I_{1}$ for different methods. ", "page_idx": 13}, {"type": "text", "text": "As shown in Figure 7, when the motion between $I_{0}$ and $I_{1}$ is significant, neither convolution nor local attention can focus on the corresponding region in $I_{1}$ before or after training. In contrast, the S6 model exhibits a larger global receptive field even before training, with notable concentration in both horizontal and vertical directions. We attribute this to the sequence rearrangement, where tokens closer together tend to have higher weights, a phenomenon also observed in VMamba (Liu et al., 2024b). ", "page_idx": 13}, {"type": "text", "text": "After training, the S6 model\u2019s focus becomes more concentrated on the horizontal region of $I_{1}$ , aligning better with the specified region in $I_{0}$ . This indicates that the S6 model can better capture dynamics even with significant motion between frames. ", "page_idx": 13}, {"type": "image", "img_path": "4s5UsBUsUS/tmp/e5319c06d80b837494356df1e753f769e7b19931eae23cad45b2bc0cea81050e.jpg", "img_caption": ["Figure 8: More Visualizations from SNU-FILM (Reda et al., 2022) and X-TEST (Sim et al., 2021). "], "img_footnote": [], "page_idx": 14}, {"type": "image", "img_path": "4s5UsBUsUS/tmp/0814d827b97bfbcfb910264d6cc588c9b95753aeed2a8445abae8d9535599a05.jpg", "img_caption": ["Figure 9: Details of frame feature extraction. The same color represents the same block structure. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "A.4 More qualitative comparison ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "As shown in Figure 8, we provide more visualization comparisons. VFIMamba demonstrates better visual quality compared to other methods. ", "page_idx": 14}, {"type": "text", "text": "A.5 Model details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.5.1 Frame feature extraction ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "As shown in Figure 9, our frame feature extraction consists of multiple convolutional layers and PReLU (He et al., 2015). The first convolution maps the image from 3 channels to $C$ , with $C=16$ for VFIMamba-S and $C=32$ for VFIMamba. Each time patch embedding is applied, the image resolution is halved, and the number of channels is doubled. Finally, we obtain the shallow features $F_{l}^{i}$ for each frame. ", "page_idx": 14}, {"type": "text", "text": "A.5.2 Frame generation ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "As depicted in Figure 10, our frame generation includes an iterative intermediate flow estimation, local flow refinement, and appearance refinement using RefinNet. First, the intermediate flow estimation module uses the features $F_{s s m}^{i}$ obtained from inter-frame modeling with MSB for rough flow estimation. Specifically, we follow the design of EMA-VFI (Zhang et al., 2023), first utilizing features $F_{s s m}^{i}$ from the $\\begin{array}{r}{\\frac{H}{16}\\times\\frac{\\dot{W}}{16}}\\end{array}$ scale and the original image $I_{i}$ for predicting the flow $f$ and occlusion mask $M$ by several convolutional layers. Then, we iteratively estimate the flow residual $\\Delta f$ and mask residual $\\Delta M$ using the $F_{s s m}^{i}$ from the $\\textstyle{\\frac{H}{8}}\\,\\times\\,{\\frac{W}{8}}$ scale. After that, inspired by Jia et al. (2022), which recognizes that the flow obtained through global inter-frame modeling may be coarse for high-resolution or large-motion scenes, we also introduce the IFBlock (Huang et al., 2022) to further enhance flow accuracy in local details. We then use the predicted motion to backward warp (Huang et al., 2022) the input frames to get the coarse intermediate frame ${\\bar{I}}_{t}$ . Finally, we adopt a U-Netlike (Ronneberger et al., 2015) structure to predict the appearance residual using shallow features $F_{l}^{i}$ and inter-frame features $F_{s s m}^{i}$ , resulting in the final frame $I_{t}$ . ", "page_idx": 14}, {"type": "image", "img_path": "4s5UsBUsUS/tmp/2e745813a3d20e2b5b5ec67a0921af68647342dc29130eef08124e6dc93a9a1b.jpg", "img_caption": ["Figure 10: Details of frame generation. IFBlock is adopted from Huang et al. (2022), which is used optionally to enhance the local detail generation performance. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "A.6 Training details ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Training loss We used the same training loss as Zhang et al. (2023), which is a weighted combination of Laplacian loss (Niklaus & Liu, 2020) and warp loss (Liu et al., 2019), with weights of 1 and 0.5, respectively. ", "page_idx": 15}, {"type": "text", "text": "Training setting We used curriculum learning to train our model. For the data from Vimeo90K (Xue et al., 2019), we randomly cropped the frames from $256\\times448$ to $256\\times256$ . For the data from X-TRAIN (Sim et al., 2021), since each sample contains 64 consecutive frames, we first randomly select two frames, starting with an interval of 1, which doubles every 50 epochs. Then, we randomly resized the frames from $512\\times512$ to $S\\times S$ , where $S$ is initially 256 and increased by a factor of 1.1 every 50 epochs, and finally cropped them to $256\\times256$ for alignment. The larger the resize size, the greater the motion magnitude of the generated data. The batch size for Vimeo90K is 32, and for X-TRAIN it is 8. We then applied time reversal and random rotation augmentations. We used AdamW as our optimizer with $\\beta_{1}=0.9,\\beta_{2}=0.999$ , and a weight decay of $1\\times\\bar{1}0^{-4}$ . With warmup for 2,000 steps, the learning rate was gradually increased to $2\\times10^{-4}$ , and then we used cosine annealing for 300 epochs to reduce the learning rate from $2\\times10^{-4}$ to $2\\times10^{-5}$ . Following Jia et al. (2022); Park et al. (2023); Liu et al. (2024a), we also trained the IFBlock separately on Vimeo90K for 100 epochs with same training setting to further improve the accuracy of local optical flow at high resolutions. The same procedure is followed for all ablation experiments. ", "page_idx": 15}, {"type": "text", "text": "Training time VFIMamba and VFIMamba-S were both trained on 4 NVIDIA 32GB V100 GPUs.   \nTraining VFIMamba-S takes about 38 hours, while training VFIMamba takes about 108 hours. ", "page_idx": 15}, {"type": "text", "text": "A.7 Evaluation protocols ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In our paper, we primarily evaluated our methods on six benchmarks in terms of PSNR/SSIM(Wang et al., 2004): Vimeo90K (Xue et al., 2019), UCF101 Soomro et al. (2012), SNU-FILM (Choi et al., 2020), Xiph (Montgomery, 1994), X-TEST (Sim et al., 2021), and X-TEST-L (Liu et al., 2024a). ", "page_idx": 15}, {"type": "text", "text": "We followed the test procedures of Huang et al. (2022) for Vimeo90K and UCF101, Kong et al. (2022) for SNU-FILM, Niklaus & Liu (2020) for Xiph, Reda et al. (2022) for X-TEST with iterative $8\\times$ frame interpolation, and Liu et al. (2024a) for X-TEST-L with largest interval interpolation. ", "page_idx": 15}, {"type": "table", "img_path": "4s5UsBUsUS/tmp/b204529736b8e3b6359794bfdc9d5b01f3531795cda6ef5bab5413b88a2dd13f.jpg", "table_caption": ["Table 7: Licenses and URLs for every benchmark, code, and pretrained models used in this paper. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "A.8 License of datasets and pre-trained models ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "All the dataset we used in the paper are commonly used datasets for academic purpose. All the licenses of the used benchmark, codes, and pretrained models are listed in Table 7. ", "page_idx": 16}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: This work mainly focuses on exploring the potential of S6 model in VFI, and the abstract and introduction document our methods and contributions in detail. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 17}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Justification: We provide a discussion of limitations in the appendix A.2. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 17}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 17}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 17}, {"type": "text", "text": "Justification: The paper does not include theoretical results. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 18}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: We provide model details in Section 3.1-3.4 and appendix A.5. Training details and evaluation protocols are also provided in the appendix A.6 and A.7. The training code and models will be open-sourced after publication. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 18}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 19}, {"type": "text", "text": "Answer: [No] ", "page_idx": 19}, {"type": "text", "text": "Justification: We have no plans to open-source the source code at this time, but the training code and models will be open-sourced after publication. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 19}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: Training details and evaluation protocols are provided in the appendix A.6 and A.7. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 19}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 19}, {"type": "text", "text": "Answer: [No] ", "page_idx": 19}, {"type": "text", "text": "Justification: We followed the same experiment protocols as in previously published methods for video frame interpolation; no error bars are included. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 19}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 20}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We provide setting of compute resources in the appendix A.6. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 20}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We\u2019ve read it carefully. Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 20}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We provide the broader impacts of our work in the appendix A.1. Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 20}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 21}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: Video frame interpolation does not produce new semantic content. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 21}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We cited the original papers we used and respected their license. License details are listed in Appendix A.8. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: We have no plans to release the code and models at this time, but they will be open-sourced and well documented after publication. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 22}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] Justification: Not involved. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 22}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 22}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: Not involved. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}]