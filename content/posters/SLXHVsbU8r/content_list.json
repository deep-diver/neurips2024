[{"type": "text", "text": "End-to-End Autonomous Driving without Costly Modularization and 3D Manual Annotation ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 We propose UAD, a method for vision-based end-to-end autonomous driving   \n2 (E2EAD), achieving the best open-loop evaluation performance in nuScenes, mean  \n3 while showing robust closed-loop driving quality in CARLA. Our motivation stems   \n4 from the observation that current E2EAD models still mimic the modular archi  \n5 tecture in typical driving stacks, with carefully designed supervised perception   \n6 and prediction subtasks to provide environment information for oriented planning.   \n7 Although achieving groundbreaking progress, such design has certain drawbacks:   \n8 1) preceding subtasks require massive high-quality 3D annotations as supervision,   \n9 posing a significant impediment to scaling the training data; 2) each submodule   \n10 entails substantial computation overhead in both training and inference. To this end,   \n11 we propose UAD, an E2EAD framework with an unsupervised1 proxy to address   \n12 all these issues. Firstly, we design a novel Angular Perception Pretext to eliminate   \n13 the annotation requirement. The pretext models the driving scene by predicting the   \n14 angular-wise spatial objectness and temporal dynamics, without manual annota  \n15 tion. Secondly, a self-supervised training strategy, which learns the consistency of   \n16 the predicted trajectories under different augment views, is proposed to enhance   \n17 the planning robustness in steering scenarios. Our UAD achieves $38.7\\%$ relative   \n18 improvements over UniAD on the average collision rate in nuScenes and surpasses   \n19 VAD for 6.40 points on the driving score in CARLA\u2019s Town05 Long benchmark.   \n20 Moreover, the proposed method only consumes $44.3\\%$ training resources of UniAD   \n21 and runs $3.4\\times$ faster in inference. Our innovative design not only for the first time   \n22 demonstrates unarguable performance advantages over supervised counterparts,   \n23 but also enjoys unprecedented efficiency in data, training, and inference. ", "page_idx": 0}, {"type": "text", "text": "24 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "25 Recent decades have witnessed breakthrough achievements in autonomous driving. The end-to  \n26 end paradigm, which seeks to integrate perception, prediction, and planning tasks into a unified   \n27 framework, stands as a representative branch [33, 1, 39, 3, 35, 21, 22]. The latest advances in end-to  \n28 end autonomous driving significantly piqued researchers\u2019 interest [21, 22]. However, handcrafted and   \n29 resource-intensive supervised sub-tasks for perception and prediction, which have previously proved   \n30 their utility in environment modeling [35, 3, 20], continue to be indispensable, as shown in Fig. 1a.   \n31 Then what insights have we gained from the recent advances? It has come to our attention that one of   \n32 the most enlightening innovations lies in the Transformer-based pipeline, in which the queries act   \n33 as a connective thread, seamlessly bridging various tasks. Besides, the capability for environment   \n34 modeling has also seen a significant boost, primarily due to complicated interactions of supervised   \n35 sub-tasks. However, every coin has two sides. In comparison to the vanilla design [33] (see Fig. 1a),   \n36 modularized methods incur unavoidable computation and annotation overhead. As illustrated in   \n37 Fig. 1b, the training of the recent method UniAD [21] takes 48 GPU days while running at only 2.1   \n38 frames per second (FPS). Moreover, modules in existing perception and prediction design require large   \n39 quantities of high-quality annotated data. The financial overhead for human annotation significantly   \n40 impedes the scalability of such modularized methods with supervised subtasks to leverage massive   \n41 data. As proved by large foundation models [24, 31], scaling up the data volume is the key to bringing   \n42 the model capabilities to the next level. Thus we ask ourselves the question: Is it viable to devise an   \n43 efficient and robust E2EAD framework while alleviating the reliance on 3D annotation?   \n44 In this work, we show the answer is affirmative by proposing an innovative Unsupervised pretext task   \n45 for end-to-end Autonomous Driving (UAD), which seeks to efficiently model the environment. The   \n46 pretext task consists of an angular-wise perception module to learn spatial information by predicting   \n47 the objectness of each sector region in BEV space, and an angular-wise dreaming decoder to absorb   \n48 temporal knowledge by predicting inaccessible future states. The introduced angular queries link   \n49 the two modules as a whole pretext task to perceive the driving scene. Notably, our method shines   \n50 by completely eliminating the annotation requirement for perception and prediction. Such data   \n51 efficiency is not attainable for current methods with complex supervised modularization [21, 22]. The   \n52 supervision for learning spatial objectness is obtained by projecting the 2D region of interests (ROIs)   \n53 from an off-the-shelf open-set detector [28] to BEV space. While utilizing the publicly available open  \n54 set 2D detector pre-trained with manual annotation from other domains (e.g. COCO [27]), we avoid   \n55 the need for any additional 3D labels within our paradigm and target domains (e.g. nuScenes [2] and   \n56 CARLA [11]), thereby creating a pragmatically unsupervised setting [30]. Furthermore, we introduce   \n57 a self-supervised direction-aware learning strategy to train the planning model. Specifically, the   \n58 visual observations are augmented with different rotation angles, and the consistency loss is applied   \n59 to the predictions for robust planning. Without bells and whistles, the proposed UAD outperforms   \n60 UniAD for $0.13\\mathrm{m}$ in nuScenes Avg. L2 error, and surpasses VAD [22] for 9.92 points in CARLA   \n61 route completion score. Such unprecedented performance gain is achieved with a $3.4\\times$ inference   \n62 speed, a mere $44.3\\%$ training budget of UniAD, and zero annotations, as illustrated in Fig. 1b.   \n63 In summary, our contributions are as follows: 1) We propose an unsupervised pretext task to discard   \n64 the requirement of 3D manual annotation in end-to-end autonomous driving, potentially making   \n65 it more feasible to scale the training data to billions level without any labeling overload; 2) We   \n66 introduce a novel self-supervised direction-aware learning strategy to maximize the consistency of the   \n67 predicted trajectories under different augment views, which enhances planning robustness in steering   \n68 scenarios; 3) Our method shows superiority in both open- and closed-loop evaluation compared with   \n69 other vision-based E2EAD methods, with much lower computation and annotation cost. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "image", "img_path": "SLXHVsbU8r/tmp/f677f185376868b6517f3c43b68271139c9677b1c55bd58abbf546e75381fa27.jpg", "img_caption": ["Figure 1: (a) End-to-end autonomous driving paradigms. 1) The vanilla architecture that directly predicts control command. 2) The modularized design that combines various preceding tasks. 3) Our proposed framework with unsupervised pretext task. (b) Comparison of training cost, inference speed and average L2 error between our method and [21, 22] on 8 NVIDIA Tesla A100 GPUs. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "71 2.1 End-to-End Autonomous Driving ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "70 2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "72 End-to-end autonomous driving can be dated back to 1988, when the ALVINN [33] proposed by   \n73 Carnegie Mellon University could successfully navigate a vehicle over 400 meters. After that, to   \n74 improve the robustness of E2EAD, a series of modern approaches such as NEAT [6], P3 [35],   \n75 MP3 [3], ST-P3 [20] introduce the design of more dedicated modularization, which integrate auxiliary   \n76 information such as HD maps, and additional tasks like bird\u2019s-eye view (BEV) segmentation. Most re  \n77 cently, embracing advanced architectures like Transfromer [37] and visual occupancy prediction [29],   \n78 UniAD [21] and VAD [22] demonstrate impressive performance in open-loop evaluation. In this work,   \n79 instead of integrating complex supervised modular sub-tasks, we innovatively propose another path   \n80 proving that an efficient unsupervised pretext task without any human annotation like 3D bounding   \n81 boxes and point cloud categories, can achieve even superior performance than recent state-of-the-arts. ", "page_idx": 1}, {"type": "image", "img_path": "SLXHVsbU8r/tmp/9d27194976cea87b6c61c7d67dd8aaf41d8d10247471d73e119e1a5d81251de3.jpg", "img_caption": ["Figure 2: The architecture of our UAD. The inference pipeline is marked by black arrows with blue background, which plans ego trajectory based on the input multi-view images. The training pipeline consists of Angular Perception Pretext (orange arrows with khaki background) and Direction-Aware Planning (orange arrows with purple background). \u201cF\u201d in BEV feature indicates the driving direction. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "82 2.2 World Model ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "83 In pursuit of understanding the dynamic changes in environments, researchers in the fields of gaming   \n84 and robotics have proposed various world models [13, 14, 15, 16]. Recently, the autonomous driving   \n85 community introduces world models for safer maneuvering [32, 18, 12, 38]. MILE [18] considers   \n86 the environment as a high-level embedding and tends to predict its future state with historical   \n87 observations. Drive-WM [38] proposes a framework to integrate world models with existing E2E   \n88 methods to improve planning robustness. In this work, we propose an auto-regressive mechanism,   \n89 tailored to our unsupervised pretext, to capture angular-wise temporal dynamics within each sector. ", "page_idx": 2}, {"type": "text", "text": "90 3 Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "91 3.1 Overview ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "92 As illustrated in Fig. 2, our UAD framework consists of two essential components: 1) the Angular   \n93 Perception Pretext, aims to liberate E2EAD from costly modularized tasks in an unsupervised fashion;   \n94 2) the Direction-Aware Planning, learns self-supervised consistency of the augmented trajectories.   \n95 Specifically, UAD first models the driving environment with the pretext. The spatial knowledge   \n96 is acquired by estimating the objectness of each sector region within the BEV space. The angular   \n97 queries, each responsible for a sector, are introduced to extract features and predict the objectness.   \n98 The supervision label is generated by projecting the 2D regions of interests (ROIs) to the BEV space,   \n99 which are predicted with an available open-set detector GroundingDINO [28]. This way not only   \n100 eliminates the 3D annotation requirement, but also greatly reduces the training budget. Moreover, as   \n101 driving is inherently a dynamic and continuous process, we thus propose an angular-wise dreaming   \n102 decoder to encode the temporal knowledge. The dreaming decoder can be viewed as an augmented   \n103 world model [13] capable of auto-regressively predicting the future states.   \n104 Subsequently, direction-aware planning is introduced to train the planning module. The raw BEV   \n105 feature is augmented with different rotation angles, yielding rotated BEV representations and ego   \n106 trajectories. We apply self-supervised consistency loss to the predicted trajectories of each augmented   \n107 view, which is expected to improve the robustness for directional change and input noises. The   \n108 learning strategy can also be regarded as a novel data augmentation technique customized for end-to  \n109 end autonomous driving, which enhances the diversity of trajectory distribution. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "image", "img_path": "SLXHVsbU8r/tmp/193893aed5a4c6784b8827119302850f0c047017b898ccf5aa5552055e3971ff.jpg", "img_caption": ["Figure 3: (a) Label generation for angular perception pretext. (b) Illustration of dreaming decoder. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "110 3.2 Angular Perception Pretext ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "111 Spatial Representation Learning. Our model attempts to acquire spatial knowledge of the driving   \n112 scene by predicting the objectness of each sector region within the BEV space. Specifically, taking   \n113 multi-view images $\\left\\{\\mathbf{I}_{\\mathrm{i}}\\in\\mathbb{R}^{\\mathbf{\\tilde{\\}{\\it H_{\\mathrm{i}}}}\\times W_{\\mathrm{i}}\\times3}\\right\\}$ as input, the BEV encoder [25] first extracts visual information   \n114 into the BEV feature $\\mathbf{F}_{\\mathrm{b}}\\in\\mathbb{R}^{H_{\\mathrm{b}}\\times W_{\\mathrm{b}}\\tilde{\\times}C}$ . Then, $\\mathbf{F}_{\\mathrm{b}}$ is partitioned into $K$ sectors with a uniform angle $\\theta$   \n115 centered around ego car. Each sector contains several feature points in BEV space. Denoting feature   \n116 of a sector as $\\mathbf{f}\\in\\bar{\\mathbb{R}}^{N\\times C}$ , where $N$ is the maximum number of feature points in all sectors, we derive   \n117 angular BEV feature $\\mathbf{F}_{\\mathrm{a}}\\!\\in\\!\\mathbb{R}^{K\\times N\\times C}$ . Zero-padding is applied on sectors with fewer than $N$ points.   \n118 Then why do we partition the rectangular BEV feature to angular-wise formatting? The underlying   \n119 reason is that, in the absence of depth information, the region in BEV space corresponding to an ROI   \n120 in 2D image is a sector. As illustrated in Fig. 3a, by projecting 3D sampling points to images and   \n121 verifying their presence in 2D ROIs, a BEV object mask $\\mathbf{M}\\!\\in\\!\\breve{\\mathbb{R}}^{H_{\\mathrm{b}}\\times W_{\\mathrm{b}}\\times\\mathrm{I}^{\\mathbf{i}}}$ is generated, representing   \n122 the objectness in BEV space. Specifically, the sampling points falling within 2D ROIs are set to 1,   \n123 while the others are 0. It is noticed that the positive sectors are irregularly and sparsely distributed in   \n124 BEV space. To make the objectness label more compact, similar to the BEV feature partition, we   \n125 uniformly divide $\\mathbf{M}$ into $K$ equal parts. The segments overlapped with positive sectors are assigned   \n126 with 1, constituting the angular objectness label $\\mathbf{Y}_{\\mathrm{obj}}\\in\\mathbb{R}^{\\hat{K}\\times1}$ . Thanks to the rapid development   \n127 of open-set detection, it\u2019s now convenient to obtain 2D ROIs for the input multi-view images by   \n128 feeding the pre-defined prompts (e.g., vehicle, pedestrian, and barrier) to a 2D open-set detector like   \n129 GroundingDINO [28]. Such design is the key in reducing annotation cost and scaling up the dataset.   \n130 To predict the objectness score of each sector, we define angular queries $\\mathbf{Q}_{\\mathrm{a}}\\!\\in\\!\\mathbb{R}^{K\\times C}$ to summarize   \n131 $\\mathbf{F}_{\\mathrm{a}}$ . Each angular query $\\mathbf{q}_{\\mathrm{a}}\\!\\in\\!\\mathbb{R}^{1\\times C}$ in ${\\bf Q}_{\\mathrm{a}}$ will interact with corresponding f by cross attention [37], ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{q}_{\\mathrm{a}}=\\mathrm{CrossAttention}(\\mathbf{q}_{\\mathrm{a}},\\,\\mathbf{f}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "132 Finally, we map ${\\bf Q}_{\\mathrm{a}}$ to the objectness scores $\\mathbf{P}_{\\mathrm{a}}\\!\\in\\!\\mathbb{R}^{K\\times1}$ with a linear layer, which is supervised by   \n133 $\\mathbf{Y}_{\\mathrm{obj}}$ with binary cross-entropy loss (denoted as $\\mathcal{L}_{\\mathrm{spat}}$ ).   \n134 Temporal Representation Learning. We propose to capture the temporal information of driving   \n135 scenarios with the angular-wise dreaming decoder. As shown in Fig. 3b, the decoder auto-regressively   \n136 learns transition dynamics of each sector in a similar way of world model [14]. Assuming the planning   \n137 module predicts the trajectories of future $T$ steps, the dreaming decoder accordingly comprises $T$   \n138 layers, where each updates the input angular queries ${\\bf Q}_{\\mathrm{a}}$ and angular BEV feature $\\mathbf{F}_{\\mathrm{a}}$ based on the   \n139 learned temporal dynamics. At step $t$ , the queries $\\mathbf{Q}_{\\mathrm{a}}^{t-1}$ first grasp environmental dynamics from the   \n140 observation feature $\\mathbf{F}_{\\mathrm{a}}^{\\mathrm{t}}$ with a gated recurrent unit (GRU) [7], which generates $\\mathbf{Q}_{\\mathrm{a}}^{t}$ (hidden state), ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{Q}_{\\mathrm{a}}^{t}=\\operatorname{GRU}(\\mathbf{Q}_{\\mathrm{a}}^{t-1},\\mathbf{F}_{\\mathrm{a}}^{t}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "141 In previous world models, the hidden state $\\mathbf{Q}$ is solely used for perceiving observed scenes. The   \n142 GRU iteration thus ends at $t$ with the final observation $\\mathbf{F}_{\\mathrm{a}}^{t}$ . In our framework, $\\mathbf{Q}$ is also used for   \n143 predicting ego trajectories in the future. Yet, the future observation, e.g., $\\mathbf{F}_{\\mathrm{a}}^{t+1}$ , is unavailable, as   \n144 the world model [14] is designed for forecasting the future with only current observation. To obtain   \n145 $\\mathbf{Q}_{\\mathrm{a}}^{t+1}$ , we first propose to update $\\mathbf{F}_{\\mathrm{a}}^{t}$ to provide pseudo observations $\\hat{\\mathbf{F}}_{\\mathrm{a}}^{t+1}$ , ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{\\mathbf{F}}_{\\mathrm{a}}^{t+1}=\\operatorname{CrossAttention}(\\mathbf{F}_{\\mathrm{a}}^{t},\\,\\mathbf{Q}_{\\mathrm{a}}^{t}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "146 Then $\\mathbf{Q}_{\\mathrm{a}}^{t+1}$ can be generated with Eq. 2 and inputs of $\\hat{\\mathbf{F}}_{\\mathrm{a}}^{t+1}$ and $\\mathbf{Q}_{\\mathrm{a}}^{t}$ . ", "page_idx": 3}, {"type": "text", "text": "147 Following the loss design in world models [14, 15, 16], we respectively map $\\mathbf{Q}_{\\mathrm{a}}^{t-1}$ and $\\mathbf{Q}_{\\mathrm{a}}^{t}$ to distri  \n148 butions of $\\{\\mu_{\\mathrm{a}}^{t-1},\\sigma_{\\mathrm{a}}^{t-1}\\}\\in\\mathbb{R}^{K\\times C}\\}$ and $\\{\\boldsymbol{\\mu}_{\\mathrm{a}}^{t},\\boldsymbol{\\sigma}_{\\mathrm{a}}^{t}\\in\\mathbb{R}^{K\\times C}\\}$ , and then minimize their KL divergence.   \n149 For the prior distribution from $\\mathbf{Q}_{\\mathrm{a}}^{t-1}$ , it\u2019s regarded as a prediction of the future dynamics without   \n150 observation. In contrast, the posterior distribution from $\\bar{\\mathbf{Q}}_{\\mathrm{a}}^{t}$ represents the future dynamics with the   \n151 observation $\\mathbf{F}_{\\mathrm{a}}^{t}$ . The KL divergence between the two distributions measures the gap between the   \n152 imagined future (prior) and the true future (posterior). We expect to enhance the capability of future   \n153 prediction for long-term driving safety, which is realized by optimizing the dreaming loss $\\mathcal{L}_{\\mathrm{drm}}$ , ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{drm}}=\\mathrm{KL}(\\{\\mu_{\\mathrm{a}}^{t},\\sigma_{\\mathrm{a}}^{t}\\}||\\{\\mu_{\\mathrm{a}}^{t-1},\\sigma_{\\mathrm{a}}^{t-1}\\}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "154 3.3 Direction-Aware Planning ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "155 Planning Head. The outputs of angular perception pretext contain a group of angular queries   \n156 $\\{\\mathbf{Q}_{\\mathrm{a}}^{t}\\left(t=1,...,T\\right)\\}$ . For planning, we correspondingly initialize $T$ ego queries $\\{\\mathbf{Q}_{\\mathrm{ego}}^{t}\\!\\in\\!\\mathbb{R}^{1\\times\\dot{C}}\\left(t:\\right.$ ${\\mathbf{}}t=$   \n157 $1,...,T)\\}$ to extract planning-relevant information and predict the ego trajectory of each future time   \n158 step. The interaction between ego queries and angular queries is performed with cross attention, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{Q}_{\\mathrm{ego}}^{t}=\\operatorname{CrossAttention}(\\mathbf{Q}_{\\mathrm{ego}}^{t},\\,\\mathbf{Q}_{\\mathrm{a}}^{t}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "159 The output ego queries $\\{\\mathbf{Q}_{\\mathrm{ego}}^{t}\\}$ are then used to predict the ego trajectories of future $T$ steps.   \n160 Following previous works [21, 22], a high-level driving signal $c$ (turn left, turn right or go straight) is   \n161 provided as prior knowledge. The planning head takes the concatenated ego feature $\\mathbf{\\breve{F}}_{\\mathrm{ego}}\\in\\breve{\\mathbb{R}}^{T\\times C}$   \n162 from $\\{\\mathbf{Q}_{\\mathrm{ego}}^{t}\\}$ and the driving command $c$ as inputs, and outputs the planning trajectory $\\bar{\\mathbf{P}_{\\mathrm{traj}}}\\!\\in\\!\\mathbb{R}^{T\\times2}$ , ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{P}_{\\mathrm{traj}}=\\mathrm{PlanHead}(\\mathbf{F}_{\\mathrm{ego}},c),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "163 where the PlanHead is the same as UniAD [21]. We apply ${\\mathcal{L}}_{1}$ loss to minimize the distance between   \n164 the predicted ego trajectory $\\mathbf{P}_{\\mathrm{traj}}$ and the ground truth $\\mathbf{G}_{\\mathrm{traj}}$ , denoted as $\\mathcal{L}_{\\mathrm{imi}}$ . Notably, $\\mathbf{G}_{\\mathrm{traj}}$ is easy   \n165 to obtain, and manual annotation is not required in practical scenarios.   \n166 Directional Augmentation. Observed that the training data is predominated by the go straight   \n167 scenarios, we propose a directional augmentation strategy to balance the distribution. As shown   \n168 in Fig. 4, the BEV feature $\\mathbf{F}_{\\mathrm{b}}$ is rotated with different angles $r\\!\\in\\!R\\!=\\!\\{90^{\\circ},180^{\\circ},270^{\\circ}\\}$ , yielding   \n169 the rotated representations $\\{\\mathbf{F}_{\\mathrm{b}}^{r}\\}$ . The   \n170 augmented features will also be used for   \n171 the pretext and planning task, and super  \n172 vised by the aforementioned loss func  \n173 tions (e.g., $\\mathcal{L}_{\\mathrm{spat}})$ ). Notably, the BEV ob  \n174 ject mask M and the ground truth ego   \n175 trajectory $\\mathbf{G}_{\\mathrm{traj}}$ are also rotated to pro  \n176 vide corresponding supervision labels.   \n177 Furthermore, we propose an auxiliary task to enhance the steering capability. In specific, we predict   \n178 the planning direction that the ego car intends to maneuver (i.e., left, straight or right) based on the   \n179 ego query $\\bar{\\mathbf{Q}}_{\\mathrm{ego}}^{t}$ , which is mapped to the probabilities of three directions $\\breve{\\mathbf{P}_{\\mathrm{dir}}^{t}}\\!\\in\\!\\mathbb{R}^{1\\breve{\\times}3}$ . The direction   \n180 label $\\mathbf{Y}_{\\mathrm{dir}}^{t}$ is generated by comparing the $\\mathbf{X}$ -axis value of ground truth $\\mathbf{G}_{\\mathrm{traj}}^{t}(x)$ with the threshold   \n181 $\\delta$ . Specifically, $\\mathbf{Y}_{\\mathrm{dir}}^{t}$ is assigned to straight if $-\\delta<\\mathbf{G}_{\\mathrm{traj}}^{t}(x)<\\delta$ , otherwise $\\mathbf{Y}_{\\mathrm{dir}}^{t}=l e f t/r i g h t$   \n182 for $\\mathbf{G}_{\\mathrm{traj}}^{t}(x)\\leqslant-\\delta/\\mathbf{G}_{\\mathrm{traj}}^{t}(x)\\geqslant\\delta$ , respectively. We use the cross-entropy loss to minimize the gap   \n183 between the direction prediction $\\mathbf{P}_{\\mathrm{dir}}^{t}$ and the direction label $\\mathbf{Y}_{\\mathrm{dir}}^{t}$ , denoted as $\\mathcal{L}_{\\mathrm{dir}}$ .   \n184 Directional Consistency. Tailored to the introduced directional augmentation, we propose a direc  \n185 tional consistency loss to improve the augmented plan training in a self-supervised manner. It should   \n118867 tbhee  noortiigcienda tl hoant et $\\mathbf{P}_{\\mathrm{traj}}^{t,r=0}$ , ei.net.e, dB trEaVje fcetoatruy rpers edwiictthi odnifsf $\\mathbf{P}_{\\mathrm{traj}}^{t,r}$ riontcaotiropno raatneg ltehse.  sTahmeer esfcoernee,  iitn\u2019fs orremasatoinoanb ales   \n188 to consider the consistency among the predictions and regulate the noises caused by the rotation. The   \n189 planning head is expected to be more robust to directional change and input distractors. Specifically,   \n190 Ptt,rraj are first rotated back to the original scene direction, then L1 loss is applied with Ptt,rraj= 0, ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "image", "img_path": "SLXHVsbU8r/tmp/9a4e18f4f38478f202850c4bc368f848d2ac851e93b694cd33a8b9d1c681c6bf.jpg", "img_caption": ["Figure 4: Illustration of direction-aware learning strategy. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{\\mathrm{cons}}=\\frac{1}{T\\cdot|R|}\\sum_{t=1}^{T}\\sum_{r}^{R}||\\mathrm{Rot}(\\mathbf{P}_{\\mathrm{traj}}^{t,r})-\\mathbf{P}_{\\mathrm{traj}}^{t,r=0}||_{1},}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "191 where Rot is the inverse rotation. ", "page_idx": 4}, {"type": "text", "text": "192 To summarize, the overall objective for our UAD contains spatial objectness loss, dreaming loss from   \n193 the pretext, and imitation learning loss, direction loss, consistency loss from the planning task, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}=\\omega_{1}\\mathcal{L}_{\\mathrm{spat}}+\\omega_{2}\\mathcal{L}_{\\mathrm{drm}}+\\omega_{3}\\mathcal{L}_{\\mathrm{imi}}+\\omega_{4}\\mathcal{L}_{\\mathrm{dir}}+\\omega_{5}\\mathcal{L}_{\\mathrm{cons}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "194 where $\\omega_{1},\\omega_{2},\\omega_{3},\\omega_{4},\\omega_{5}$ are the weight coefficients. ", "page_idx": 4}, {"type": "table", "img_path": "SLXHVsbU8r/tmp/c1cf4c5ac85db1a26ff932036708267cf0f1a6db1af84ae31e1217ccf36cf1d2.jpg", "table_caption": ["Table 1: Open-loop planning performance in nuScenes [2]. \u2020 indicates LiDAR-based method and \u2021 denotes TemAvg evaluation protocol used in VAD and ST-P3 (see Eq. 9 for details). \u22c4means using ego status in the planning module and calculating collision rates following BEV-Planner [26]. "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "195 4 Experiment ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "196 4.1 Experimental Setup ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "197 We conduct experiments in nuScenes [2] for open-loop evaluation, that contains 40,157 samples,   \n198 of which 6,019 ones are used for evaluation. Following previous works [20, 21, 22], we adopt the   \n199 metrics of L2 error (in meters) and collision rate (in percentage). Notably, the intersection rate with   \n200 road boundary (in percentage), proposed in BEV-Planner [26], is also included for evaluation. For   \n201 the closed-loop setting, we follow previous works [34, 20] to perform evaluation in the Town05 [34]   \n202 benchmark of the CARLA simulator [11]. Route completion (in percentage) and driving score (in   \n203 percentage) are used as the evaluation metrics. We adopt the query-based view transformer [25] to   \n204 learn BEV features from multi-view images. The confidence threshold of the open-set 2D detector   \n205 is set to 0.35 to filter unreliable predictions. The angle $\\theta$ to partition the BEV space is set to $4^{\\circ}$   \n206 $(K\\mathrm{=}360^{\\circ}/4^{\\circ})$ ), and the default threshold $\\delta$ is $1.2m$ (see Sec. 3.3). The weight coefficients in Eq. 8 are   \n207 set to $2.0,0.1,1.0,2.0,1.0$ . Our model is trained for 24 epochs on 8 NVIDIA Tesla A100 GPUs with   \n208 a batch size of 1 per GPU. Other settings follow UniAD [21] unless otherwise specified.   \n209 We observed that ST-P3 [20] and VAD [22] adopt different open-loop evaluation protocols (L2 error   \n210 and collision rate) from UniAD in their official codes. We denote the setting in ST-P3 and VAD as   \n211 TemAvg and the one in UniAD as NoAvg, respectively. In specific, the TemAvg protocol calculates   \n212 metrics by averaging the performances from 0.5s to the corresponding timestamp. Taking the L2   \n213 error at 2s as an example, the calculation in TemAvg is ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathrm{L2@2s}=\\mathrm{Avg}(l2_{0.5s},l2_{1.0s},l2_{1.5s},l2_{2.0s}),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "214 where Avg is the average operation and $0.5s$ is the time interval between two consecutive annotated   \n215 frames in nuScenes [2]. For NoAvg protocol, $\\mathrm{L2@2s}=l2_{2.0s}$ . ", "page_idx": 5}, {"type": "text", "text": "216 4.2 Comparison with State-of-the-arts ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "217 Open-loop Evaluation. Tab. 1 presents the performance comparison in terms of L2 error, collision   \n218 rate, intersection rate with road boundary, and FPS. Since ST-P3 and VAD adopt different evaluation   \n219 protocols from UniAD to compute L2 error and collision rate (see Sec. 4.1), we respectively calculate   \n220 the results under different settings, i.e., NoAvg and TemAvg. As shown in Tab. 1, the proposed UAD   \n221 achieves superior planning performance over UniAD and VAD on all metrics, while running faster.   \n222 Notably, our UAD obtains $39.4\\%$ and $55.2\\%$ relative improvements on Collision $@3\\mathrm{s}$ compared   \n223 with UniAD and VAD under the NoAvg evaluation protocol (e.g., $39.4\\%{=}(0.71\\%{-}0.43\\%)/0.71\\%$ ,   \n224 demonstrating the longtime robustness of our method. Moreover, UAD runs at 7.2FPS, which is $3.4\\times$   \n225 and $1.4\\times$ faster than UniAD and VAD-Base, respectively, verifying the efficiency of our framework.   \n226 Surprisingly, our tiny version, UAD-Tiny, which aligns the settings of backbone, image size, and BEV   \n227 resolution in VAD-Tiny, runs at the fastest speed of 18.9FPS while clearly outperforming VAD-Tiny   \n228 and even achieving comparable performance with VAD-Base. This again proves the superiority of   \n229 our design. More detailed runtime comparisons and analyses are presented in the appendix. We adopt   \n230 the NoAvg evaluation protocol in the following ablation experiments unless otherwise specified.   \n231 Recent works discuss the effect of using ego status in the planning module [22, 26]. Following this   \n232 trend, we also fairly compare the ego status equipped version of our model with these works. It shows   \n233 that the superiority of our UAD is still preserved, which also achieves the best performance against   \n234 the compared methods. Moreover, BEV-Planner [26] introduces a new metric named \u201cinteraction\u201d   \n235 for better evaluating the performance of E2EAD methods. As shown in Tab. 1, our model obtains   \n236 the average interaction rate of $1.13\\%$ , obviously outperforming other methods. This again proves   \n237 the effectiveness of our UAD. On the other hand, this demonstrates the importance of designing a   \n238 suitable pretext for perceiving the environment. Only using ego status is not enough for safe driving.   \n239 Closed-loop Evaluation. The simulation results in CARLA [11] are shown in Tab. 2. Our UAD   \n240 achieves better performance compared with recent E2E planners ST-P3 [20] and VAD [22] in all   \n241 scenarios, proving the effectiveness. Notably, on challenging Town05 Long benchmark, UAD greatly   \n242 outperforms recent E2E method VAD by 6.40 points on the driving score and 9.92 points on route   \n243 completion, respectively. This proves the reliability of our UAD for long-term autonomous driving. ", "page_idx": 5}, {"type": "table", "img_path": "SLXHVsbU8r/tmp/0b2b86c84c5cbeb171b21f7f8df84454feef9319c3435511f19b5425415b7d1f.jpg", "table_caption": ["Table 2: Closed-loop evaluation in the Table 3: Ablation on the loss functions. We evaluate the CARLA simulator [11]. \u2020 denotes the influence of each designed module by applying correLiDAR-based method. sponding loss. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "244 4.3 Component-wise Ablation ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "245 Loss Functions. We first analyze the influence of different loss functions that correspond to the   \n246 proposed pretext task and self-supervised trajectory learning strategy. The experiments are conducted   \n247 on the validation split of the nuScenes [2], as shown in Tab. 3. The model with single imitation   \n248 loss $\\mathcal{L}_{\\mathrm{imi}}$ is considered as the baseline $\\mathbf{\\tau}(\\mathfrak{D})$ . With the enhanced perception capability by the spatial   \n249 objectness loss $\\mathcal{L}_{\\mathrm{spat}}$ , the average L2 error and collision rate are clearly improved to $1.00\\mathrm{m}$ and $0.71\\%$   \n250 from $3.18\\mathrm{m}$ and $\\bar{2}.43\\%$ , respectively $\\left(\\mathcal{Q}\\nu.s.\\ \\mathcal{D}\\right)$ . The dreaming loss $\\mathcal{L}_{\\mathrm{drm}}$ , direction loss $\\mathcal{L}_{\\mathrm{dir}}$ and   \n251 consistency loss $\\mathcal{L}_{\\mathrm{cons}}$ also respectively bring considerable gains on the average L2 error for $1.98\\mathrm{m}$ ,   \n252 $1.58\\mathrm{m}$ , $1.77\\mathrm{m}$ over the baseline model $\\mathbf{\\Omega}({\\pmb{3}},\\!{\\mathcal{Q}},\\!{\\mathcal{(5)}}$ v.s. $\\textcircled{1}$ ). The loss functions are finally combined to   \n253 construct our UAD $\\mathbf{\\eta}(\\widehat{\\pmb{\\Theta}})$ , which obtains the average L2 error of $0.90\\mathrm{m}$ and average collision rate of   \n254 $0.19\\%$ . The results demonstrate the effectiveness of each proposed component.   \n255 Temporal Learning with Dreaming Decoder. The temporal learning with the proposed dreaming   \n256 decoder is realized by Circular Update and Dreaming Loss. The circular update is in charge of both   \n257 extracting information from observed scenes (Eq. 2) and generating pseudo observations to predict   \n258 the ego trajectories of future frames (Eq. 3). We study the influence of each module in Tab. 4. Circular   \n259 Update and Dreaming Loss respectively bring performance gains of $0.70\\mathrm{m}/0.78\\mathrm{m}$ on the average L2   \n260 error $(\\mathcal{Q},\\mathcal{\\ @}\\nu.s.\\mathcal{0})$ , proving the effectiveness of our designs. Applying both two modules $\\left(\\circledast\\right)$ achieves   \n261 the best performance, showing their complementarity for temporal representation learning.   \n262 Direction Aware Learning Strategy. Directional Augmentation and Directional Consistency are the   \n263 two core components of the proposed direction-aware learning strategy. We prove their effectiveness   \n264 in Tab. 5. It shows that the Directional Augmentation improves the average L2 error for considerable   \n265 $0.05\\mathrm{m}\\left(\\?\\!\\left(\\mathbf{\\partial}\\right)\\!\\!\\nu.s.\\textcircled{\\mathbb{D}}\\right)$ . One interesting observation is that applying the augmentation brings more gains   \n266 for long-term planning than short-term ones, i.e., the L2 error of $1s/3\\mathrm{s}$ decreases for $0.01\\mathrm{m/0.08m}$   \n267 compared with $\\textcircled{1}$ , which proves the effectiveness of our augmentation on enhancing longer temporal   \n268 information. The Directional Consistency further reduces the average collision rate for impressive   \n269 $0.13\\%$ $(\\textcircled{3}\\nu.s.\\textcircled{2})$ , which enhances the robustness for driving directional change.   \n270 Angular Design. We further explore the influence of the proposed angular design by removing   \n271 the angular partition and angular queries. Specifically, the BEV feature is directly fed into the   \n272 dreaming decoder to predict pixel-wise objectness, which is supervised by the BEV object mask   \n273 (see Fig. 2) with binary cross-entropy loss. Besides, the ego query directly interacts with the BEV   \n274 feature by cross-attention to extract environmental information. The results are presented in Tab. 6.   \n276 ror degrades for $0.47\\mathrm{m}$ , and the average collision rate   \n277 consistently degrades for $1.18\\%$ . This demonstrates the   \n278 effectiveness of our angular design in perceiving com  \n279 plex environments and planning robust driving routes. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "table", "img_path": "SLXHVsbU8r/tmp/021f77c0fae1d4c713b09dc73d477c2f49213867de0f908f8638847276d5e7f2.jpg", "table_caption": ["Table 7: Performances under different driving scenes. \u2217denotes not using direction-aware learning. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "table", "img_path": "SLXHVsbU8r/tmp/9000d3b3fa360d93b47290457c9c1eff81d420d0a89d5db8efde85c832393b9d.jpg", "table_caption": ["Table 6: Ablation on the angular design. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "280 4.4 Further Analysis ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "281 Planning Performance in Different Driving Scenes. The direction-aware learning strategy is   \n282 designed to enhance the planning performance in scenarios of vehicle steering. We demonstrate the   \n283 superiority of our proposed model by evaluating the metrics of different driving scenes in Tab. 7.   \n284 According to the given driving command (i.e., go straight, turn left and turn right), we divide the   \n285 6,019 validation samples in nuScenes [2] into three parts, which contain 5,309, 301 and 409 ones,   \n286 respectively. Not surprisingly, all methods perform better under go straight scenes than the steering   \n287 scenes, proving the necessity of augmenting the imbalanced training data for robust planning. When   \n288 applying the proposed direction-aware learning strategy, our UAD achieves considerable gains on   \n289 the average collision rate of turn left and turn right scenes (UAD v.s. UAD\u2217). Notably, our model   \n290 outperforms UniAD and VAD by a large margin in steering scenes, proving its effectiveness.   \n291 Visualization of Angular Perception and Planning. The angular perception pretext is designed   \n292 to perceive the objects in each sector region. We show its capability by visualizing the predicted   \n293 objectness in nuScenes [2] in Fig. 5a. For a better view, we transform the discrete objectness   \n294 scores and ground truth to a pseudo-BEV mask. It shows that our model can successfully capture   \n295 surrounding objects. Fig. 5a also shows the open-loop planning results of recent SOTA UniAD [21],   \n296 VAD [22] and our UAD, proving the effectiveness of our method to plan a more reasonable ego   \n297 trajectory. Fig. 5b compares the closed-loop driving routes between Transfuser [34], ST-P3 [20] and   \n298 our UAD in CARLA [11]. Our method successfully notices the person and drives in a much safer   \n299 manner, proving the reliability of our UAD in handling safe-critical issues under complex scenarios.   \n300 Due to limited space, we present more analyses in the appendix, including 1) the influence of partition   \n301 angle $\\theta,2$ ) the influence of direction threshold $\\delta,3\\$ ) different backbones and pre-trained weights, 4)   \n302 replacing 2D ROIs from GroundingDINO with 2D GT boxes, 5) different settings of GroundingDINO   \n303 to generate 2D ROIs, 6) the influence of pre-training to previous method UniAD and our UAD, 7)   \n304 runtime analysis of each module in our UAD and modularized UniAD, 8) more visualizations, etc. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "305 4.5 Discussion ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "306 Ego Status and Open-loop Planning Evaluation. As revealed by [26, 40], it\u2019s not a challenge to   \n307 acquire decent performance of L2 error and collision rate (the original metrics in nuScenes [2]) in   \n308 the open-loop evaluation of nuScenes by using ego status in the planning module (see Tab. 1). The   \n309 question is: is open-loop evaluation meaningless? Our answer is NO. Firstly, the inherent reason for   \n310 the observation is that the simple cases of go straight dominate the nuScenes testing dataset. In these   \n311 cases, even a linear extrapolation of motion being sufficient for planning is not surprising. However,   \n312 as shown in Tab. 7, in more challenging cases like turn right and turn left, the open-loop metrics can   \n313 still clearly indicate the difficulty of steering scenarios and the differences in methods, which is also   \n314 proved in [26]. Therefore, open-loop evaluation is not meaningless, while the crux is the distribution   \n315 of the testing data and the metrics. Secondly, the advantage of open-loop evaluation is its efficiency,   \n316 which benefits the fast development of algorithms. This view is also revealed by a recent simulator   \n317 design study [9], which tries to transform the closed-loop evaluation into an open-loop fashion.   \n318 In our work, we thoroughly compare our model with other methods, which shows consistent improve  \n319 ments against previous works under various driving scenarios (straight or steering), different usage of   \n320 ego status $\\mathbf{\\dot{\\Omega}}$ or w/o.), diverse evaluation metrics (L2 error, collision rate or intersection rate from   \n321 [26]), and different evaluation types (open- or closed-loop). It thus again proves the importance of   \n322 designing suitable pretext tasks for end-to-end autonomous driving.   \n23 How to Guarantee Safety in Current Auto-Drive System? Safety is the first requirement of   \n24 autonomous driving systems in practical products, especially for L4-level auto-vehicles. To guarantee   \n25 safety, offline collision check with predicted 3D boxes is an inevitable post-process under current   \n26 technological conditions. Then, a question naturally arises: how to safely apply our model to   \n27 current auto-driving systems? Before answering this question, we reaffirm our claim that we believe   \n28 discarding 3D labels is an efficient, attractive, and potential direction for E2EAD, but it doesn\u2019t mean   \n29 we refuse to use any 3D labels if the relatively cheap ones are available in practical product engineering.   \n30 For instance, solely annotating bounding boxes without object identity for tracking is much cheaper   \n31 than labeling other elements like HD-map, and point-cloud segmentation labels for occupancy.   \n32 Therefore, we provide a degraded version of our method by arranging an additional 3D detection head.   \n333 Then our model can seamlessly integrate into auto  \n334 drive products, and offline collision check is achiev  \n335 able. As shown in Tab. 8, integrating the 3D detection   \n336 head doesn\u2019t bring additional improvements, which   \n337 again proves the design of our method has sufficiently   \n338 encoded 3D information to the planning module.   \n339 In a nutshell, 1) our work can easily integrate other 3D tasks if they are inevitable under current   \n340 technical conditions; 2) the experiments again prove from the side that our spatial-temporal module   \n341 has already encoded important 3D clues for planning; 3) we hope our frontier work can eliminate   \n342 some inessential 3D sub-tasks for both research and engineer usage of E2EAD models. An era of   \n343 cheap, laboratory-affordable but robust, practical E2EAD design will eventually come! ", "page_idx": 7}, {"type": "image", "img_path": "SLXHVsbU8r/tmp/e9b784f9061b45f6aa4de77fc24d1f37f93df072b67ad7cf622a70bd82c30a04.jpg", "img_caption": ["Figure 5: (a) Qualitative results in nuScenes. (b) Qualitative results in CARLA. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "table", "img_path": "SLXHVsbU8r/tmp/0917c8f7d72e7d12226503bede6f86f0dddb7eb964bf157f47c5f86f34ed1995.jpg", "table_caption": ["Table 8: Ablation on the 3D detection head. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "344 5 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "345 Our work seeks to liberate E2EAD from costly modularization and 3D manual annotation. With this   \n346 goal, we propose the unsupervised pretext task to perceive the environment by predicting angular  \n347 wise objectness and future dynamics. To improve the robustness in steering scenarios, we introduce   \n348 the direction-aware training strategy for planning. Experiments demonstrate the effectiveness and   \n349 efficiency of our method. As discussed, although the ego trajectories are easily obtained, it is almost   \n350 impossible to collect billion-level precisely annotated data with perception labels. This impedes the   \n351 further development of end-to-end autonomous driving. We believe our work provides a potential   \n352 solution to this barrier and may push performance to the next level when massive data are available. ", "page_idx": 8}, {"type": "text", "text": "353 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "354 [1] Mariusz Bojarski, Philip Yeres, Anna Choromanska, Krzysztof Choromanski, Bernhard Firner, Lawrence   \n355 Jackel, and Urs Muller. Explaining how a deep neural network trained with end-to-end learning steers a   \n356 car. arXiv preprint arXiv:1704.07911, 2017.   \n357 [2] Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan,   \n358 Yu Pan, Giancarlo Baldan, and Oscar Beijbom. nuscenes: A multimodal dataset for autonomous driving.   \n359 In CVPR, 2020.   \n360 [3] Sergio Casas, Abbas Sadat, and Raquel Urtasun. Mp3: A unified model to map, perceive, predict and plan.   \n361 In CVPR, 2021.   \n362 [4] Jun Cen, Peng Yun, Junhao Cai, Michael Yu Wang, and Ming Liu. Open-set 3d object detection. In 2021   \n363 International Conference on 3D Vision (3DV). IEEE, 2021.   \n364 [5] Dian Chen, Brady Zhou, Vladlen Koltun, and Philipp Kr\u00e4henb\u00fchl. Learning by cheating. In Conference   \n365 on Robot Learning, 2020.   \n366 [6] Kashyap Chitta, Aditya Prakash, and Andreas Geiger. Neat: Neural attention fields for end-to-end   \n367 autonomous driving. In ICCV, 2021.   \n368 [7] Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation of gated   \n369 recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555, 2014.   \n370 [8] Felipe Codevilla, Eder Santana, Antonio M L\u00f3pez, and Adrien Gaidon. Exploring the limitations of   \n371 behavior cloning for autonomous driving. In ICCV, 2019.   \n372 [9] NAVSIM Contributors. Navsim: Data-driven non-reactive autonomous vehicle simulation. https:   \n373 //github.com/autonomousvision/navsim, 2024.   \n374 [10] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical   \n375 image database. In CVPR. Ieee, 2009.   \n376 [11] Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, and Vladlen Koltun. Carla: An open   \n377 urban driving simulator. In Conference on robot learning, 2017.   \n378 [12] Zeyu Gao, Yao Mu, Ruoyan Shen, Chen Chen, Yangang Ren, Jianyu Chen, Shengbo Eben Li, Ping Luo,   \n379 and Yanfeng Lu. Enhance sample efficiency and robustness of end-to-end urban autonomous driving via   \n380 semantic masked world model. arXiv preprint arXiv:2210.04017, 2022.   \n381 [13] David Ha and J\u00fcrgen Schmidhuber. Recurrent world models facilitate policy evolution. NeurIPS, 2018.   \n382 [14] Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control: Learning   \n383 behaviors by latent imagination. ICLR, 2020.   \n384 [15] Danijar Hafner, Timothy Lillicrap, Mohammad Norouzi, and Jimmy Ba. Mastering atari with discrete   \n385 world models. ICLR, 2021.   \n386 [16] Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap. Mastering diverse domains through   \n387 world models. arXiv preprint arXiv:2301.04104, 2023.   \n388 [17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.   \n389 In CVPR, 2016.   \n390 [18] Anthony Hu, Gianluca Corrado, Nicolas Griffiths, Zachary Murez, Corina Gurau, Hudson Yeo, Alex   \n391 Kendall, Roberto Cipolla, and Jamie Shotton. Model-based imitation learning for urban driving. NeurIPS,   \n392 2022.   \n393 [19] Peiyun Hu, Aaron Huang, John Dolan, David Held, and Deva Ramanan. Safe local motion planning with   \n394 self-supervised freespace forecasting. In CVPR, 2021.   \n395 [20] Shengchao Hu, Li Chen, Penghao Wu, Hongyang Li, Junchi Yan, and Dacheng Tao. St-p3: End-to-end   \n396 vision-based autonomous driving via spatial-temporal feature learning. In ECCV. Springer, 2022.   \n397 [21] Yihan Hu, Jiazhi Yang, Li Chen, Keyu Li, Chonghao Sima, Xizhou Zhu, Siqi Chai, Senyao Du, Tianwei   \n398 Lin, Wenhai Wang, et al. Planning-oriented autonomous driving. In CVPR, 2023.   \n399 [22] Bo Jiang, Shaoyu Chen, Qing Xu, Bencheng Liao, Jiajie Chen, Helong Zhou, Qian Zhang, Wenyu Liu,   \n400 Chang Huang, and Xinggang Wang. Vad: Vectorized scene representation for efficient autonomous driving.   \n401 arXiv preprint arXiv:2303.12077, 2023.   \n402 [23] Tarasha Khurana, Peiyun Hu, Achal Dave, Jason Ziglar, David Held, and Deva Ramanan. Differentiable   \n403 raycasting for self-supervised occupancy forecasting. In ECCV, 2022.   \n404 [24] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete   \n405 Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. arXiv preprint   \n406 arXiv:2304.02643, 2023.   \n407 [25] Zhiqi Li, Wenhai Wang, Hongyang Li, Enze Xie, Chonghao Sima, Tong Lu, Yu Qiao, and Jifeng Dai.   \n408 Bevformer: Learning bird\u2019s-eye-view representation from multi-camera images via spatiotemporal trans  \n409 formers. In ECCV. Springer, 2022.   \n410 [26] Zhiqi Li, Zhiding Yu, Shiyi Lan, Jiahan Li, Jan Kautz, Tong Lu, and Jose M Alvarez. Is ego status all you   \n411 need for open-loop end-to-end autonomous driving? In CVPR, 2024.   \n412 [27] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r,   \n413 and C Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV. Springer, 2014.   \n414 [28] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang,   \n415 Hang Su, Jun Zhu, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object   \n416 detection. arXiv preprint arXiv:2303.05499, 2023.   \n417 [29] Lars Mescheder, Michael Oechsle, Michael Niemeyer, Sebastian Nowozin, and Andreas Geiger. Occupancy   \n418 networks: Learning 3d reconstruction in function space. In CVPR, 2019.   \n419 [30] Mahyar Najibi, Jingwei Ji, Yin Zhou, Charles R Qi, Xinchen Yan, Scott Ettinger, and Dragomir Anguelov.   \n420 Unsupervised 3d perception with 2d vision-language distillation for autonomous driving. In ICCV, 2023.   \n421 [31] OpenAI. Chatgpt [large language model]. https://chat.openai.com, 2023.   \n422 [32] Minting Pan, Xiangming Zhu, Yunbo Wang, and Xiaokang Yang. Iso-dream: Isolating and leveraging   \n423 noncontrollable visual dynamics in world models. NeurIPS, 2022.   \n424 [33] Dean A Pomerleau. Alvinn: An autonomous land vehicle in a neural network. NeurIPS, 1988.   \n425 [34] Aditya Prakash, Kashyap Chitta, and Andreas Geiger. Multi-modal fusion transformer for end-to-end   \n426 autonomous driving. In CVPR, 2021.   \n427 [35] Abbas Sadat, Sergio Casas, Mengye Ren, Xinyu Wu, Pranaab Dhawan, and Raquel Urtasun. Perceive,   \n428 predict, and plan: Safe motion planning through interpretable semantic representations. In ECCV. Springer,   \n429 2020.   \n430 [36] Wenwen Tong, Chonghao Sima, Tai Wang, Li Chen, Silei Wu, Hanming Deng, Yi Gu, Lewei Lu, Ping   \n431 Luo, Dahua Lin, et al. Scene as occupancy. In ICCV, 2023.   \n432 [37] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz   \n433 Kaiser, and Illia Polosukhin. Attention is all you need. NeurIPS, 2017.   \n434 [38] Yuqi Wang, Jiawei He, Lue Fan, Hongxin Li, Yuntao Chen, and Zhaoxiang Zhang. Driving into the future:   \n435 Multiview visual forecasting and planning with world model for autonomous driving. CVPR, 2024.   \n436 [39] Wenyuan Zeng, Wenjie Luo, Simon Suo, Abbas Sadat, Bin Yang, Sergio Casas, and Raquel Urtasun.   \n437 End-to-end interpretable neural motion planner. In CVPR, 2019.   \n438 [40] Jiang-Tian Zhai, Ze Feng, Jinhao Du, Yongqiang Mao, Jiang-Jiang Liu, Zichang Tan, Yifu Zhang, Xiaoqing   \n439 Ye, and Jingdong Wang. Rethinking the open-loop evaluation of end-to-end autonomous driving in nuscenes.   \n440 arXiv preprint arXiv:2305.10430, 2023. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "441 A Appendix ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "442 The appendix presents additional designing and explaining details of our Unpervised pretext task for   \n443 end-to-end Autonomous Driving (UAD) in the manuscript.   \n444 \u2022 Different Partition Angles   \n445 We explore the influence of different partition angles in angular pretext to learn better   \n446 spatio-temporal knowledge.   \n447 \u2022 Different Direction Thresholds   \n448 We explore the influence of different thresholds in direction prediction to enhance planning   \n449 robustness in complex driving scenarios.   \n450 \u2022 Different Backbones and Pre-trained Weights   \n451 We compare the performance of different backbones and pre-trained weights on our method.   \n452 \u2022 Objectness Label Generation with GT Boxes   \n453 We compare the generated objectness label between using the pseudo ROIs from Ground  \n454 ingDINO [28] and ground-truth boxes on different backbones.   \n455 \u2022 Settings for ROI Generation   \n456 We ablate different settings for the open-set 2D detector GroundingDINO, which provides   \n457 ROIs for the label generation of angular perception pretext.   \n458 \u2022 Different Image Sizes and BEV Resolution   \n459 We compare the performance with different input sizes of multi-view images and BEV   \n460 resolutions.   \n461 \u2022 Runtime Analysis   \n462 We evaluate the runtime of each module of UAD and compare with modularized UniAD [21],   \n463 which demonstrates the efficiency of our method.   \n464 \u2022 Classification of Angular Perception   \n465 We evaluate the objectness prediction in the angular perception pretext, which demonstrates   \n466 the enhanced perception capability in complex driving scenarios.   \n467 \u2022 Influence of Pre-training   \n468 We evaluate the influence of pre-training by detailing the training losses and planning   \n469 performances with different pre-trained weights.   \n470 \u2022 More Visualizations   \n471 We provide more visualizations for the predicted angular-wise objectness and planning re  \n472 sults in the open-loop evaluation of nuScenes [2] and closed-loop simulation of CARLA [11]. ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "473 A.1 Different Partition Angles ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "474 The proposed angular perception pretext divides the BEV space into multiple sectors. We explore the   \n475 influence of partition angle $\\theta$ in Tab 9. Experimental results show that the L2 error and inference   \n476 speed gradually increase with the partition angle. The model with partition angle of $1^{\\circ}(\\mathfrak{V})$ achieves   \n477 the best average L2 error of $0.85\\mathrm{m}$ . And the partition angle of $4^{\\circ}$ contributes to the best average   \n478 collision rate of $0.19\\%$ $\\mathbf{\\sigma}(\\mathfrak{D})$ . This reveals that a smaller partition angle helps learn more fine-grained   \n479 environmental representations, eventually benefiting planning. In contrast, the model with a large   \n480 partition angle sparsely perceives the scene. Despite reducing the computation cost, it will also   \n481 degrade the safety of the end-to-end autonomous driving system. ", "page_idx": 11}, {"type": "table", "img_path": "SLXHVsbU8r/tmp/0d4e78d64bc8d917a0862334c1ccefc16c64ebdd10013978d9a3379334a1eb11.jpg", "table_caption": ["Table 9: Ablation on different partition angle in the proposed angular pretext. "], "table_footnote": [], "page_idx": 11}, {"type": "table", "img_path": "SLXHVsbU8r/tmp/1614b72757a9ae27d930661bad4b1f1780f09aed6a46617d079c60f4fdff1f5b.jpg", "table_caption": ["Table 10: Ablation on different thresholds of direction prediction in the directional augmentation. "], "table_footnote": [], "page_idx": 11}, {"type": "table", "img_path": "SLXHVsbU8r/tmp/94d089f14214276beb39841eef28a9971a62f4fd037d932001110d0c0334c9fe.jpg", "table_caption": ["Table 11: Ablation on different backbones and pre-trained weights. "], "table_footnote": [], "page_idx": 12}, {"type": "table", "img_path": "SLXHVsbU8r/tmp/312e77aef39782ccc8cb02ba2806f0f3c6cd681e994227688a89efdf90b1fbb2.jpg", "table_caption": ["Table 12: Ablation on 2D object boxes in pretext label generation. "], "table_footnote": [], "page_idx": 12}, {"type": "text", "text": "482 A.2 Different Direction Thresholds ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "483 The direction prediction that the ego car intends to maneuver (i.e., left, straight and right) is proposed   \n484 to enhance the steering capability for autonomous driving. The label is generated with the threshold $\\delta$   \n485 (see Eq. 7 in the manuscript), which determines the ground-truth direction of each waypoint in the   \n486 expert trajectory. Here we explore the influence by ablating different thresholds, as shown in Tab. 10.   \n487 Experimental results show that the L2 error gradually increases with the direction threshold. The   \n488 model with $\\delta$ of $0.5\\mathfrak{m}\\left(\\mathfrak{V}\\right)$ achieves the lowest L2 error of $0.86\\mathrm{m}$ . It reveals that a smaller threshold   \n489 will force the planner to fit the expert navigation, leading to a closer distance between the predicted   \n490 trajectory and the ground truth. In contrast, the collision rate benefits more from larger thresholds.   \n491 The model with $\\delta$ of $2.0\\mathfrak{m}$ obtains the best collision rate at 2s of $0.08\\%$ $\\left(\\mathfrak{H}\\right)$ , showing the effectiveness   \n492 for robust planning. Notably, the threshold of $1.2\\mathrm{m}$ contributes to a great balance with the average L2   \n493 error of $0.90\\mathrm{m}$ and average collision rate of $0.19\\%$ . ", "page_idx": 12}, {"type": "text", "text": "494 A.3 Different Backbones and Pre-trained Weights ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "495 As a common sense, pre-training the backbone network with fundamental tasks like image classi  \n496 fication on ImageNet [10] will benefit the sub-tasks. The previous method UniAD [21] uses the   \n497 pre-trained weights of BEVFormer [25]. What surprised us is that when replacing the pre-trained   \n498 weights with the one learned on ImageNet, the performance of UniAD dramatically degraded (see   \n499 \u201cInfluence of Pre-training\u201d for more details). This inspires us to explore the influence of backbone   \n500 settings on our framework. As shown in Tab. 11, interestingly, even without any pre-training, our   \n501 model still outperforms UniAD with pre-trained ResNet101 and VAD with pre-trained ResNet50.   \n502 This verifies the effectiveness of our unsupervised pretext task on modeling the driving scenes. We   \n503 also use publicly available pre-trained weights on detection datasets like COCO [27] and nuImages [2]   \n504 to train our model, which shows better performance. These experimental results and observations   \n505 demonstrate that a potentially promising topic is how to pre-train a model for end-to-end autonomous   \n506 driving. We leave this to future research. ", "page_idx": 12}, {"type": "text", "text": "507 A.4 Objectness Label Generation with GT Boxes ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "508 As mentioned in the manuscript, the essence of generating the angular objectness label lies in the   \n509 2D ROIs, which come from the open-set 2D detector GroundingDINO [28]. Here we explore the   \n510 influence of using the ground-truth 2D boxes as ROIs, which provide more high-quality samples   \n511 for the representation learning in the angular perception pretext. Tab. 12 shows that training with   \n512 GT boxes achieves consistent performance gains on both ResNet50 [17] and ResNet101 [17] ${\\bf\\widehat{\\theta}}(\\widehat{\\bf{2}}),\\!\\widehat{\\bf{4}})$   \n513 v.s. $({\\bar{\\mathbb{1}}},{\\bar{\\mathbb{3}}})$ ). This reveals that accurate annotation does help to learn better spatio-temporal knowledge   \n514 and improve ego planning. Considering the cost in real-world deployment, training with accessible ", "page_idx": 12}, {"type": "text", "text": "Table 13: Ablation on the settings of ROI generation. The Conf. Thresh denotes the confidence threshold in GroundingDINO [28] to fliter unreliable predictions. vehicle,pedestrian,barrier represent the used prompt words to obtain ROIs of corresponding classes. Rule Filter indicates filtering the ROIs that are more than half of the length or width of the image. ", "page_idx": 13}, {"type": "table", "img_path": "SLXHVsbU8r/tmp/865abc46233d3bfda4523cd71352f1e014b4692aa7e004c07b863f1a398ee317.jpg", "table_caption": [], "table_footnote": [], "page_idx": 13}, {"type": "table", "img_path": "SLXHVsbU8r/tmp/a6acab916649f1fd2c7e9a968f0c60510c36af9c4a77fde29e766726d1803370.jpg", "table_caption": ["Table 14: Comparison with different backbones, image sizes and BEV resolutions. "], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "515 pseudo labels is a more efficient way compared with the manual annotation, which also shows   \n516 comparable performance in autonomous driving ( $\\ensuremath{\\stackrel{\\r{\\r}}{(1)}}$ v.s. $\\circledcirc$ and $\\textcircled{3}\\nu.s.\\textcircled{4},$ ). ", "page_idx": 13}, {"type": "text", "text": "517 A.5 Settings for ROI Generation. ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "518 The quality of learned spatio-temporal knowledge highly relies on the generated ROIs by the open-set   \n519 2D detector GroundingDINO [28], which are then projected as the BEV objectness label for training   \n520 the angular perception pretext. We explore the influence of generated ROIs with different settings,   \n521 as shown in Tab. 13. We take the setting with the confidence score of 0.35, prompt word of vehicle   \n522 and without the Rule Filter, as the baseline $\\mathbf{\\tau}(\\mathfrak{D})$ . By appending more prompt words (e.g., pedestrian,   \n523 barrier), the planning performance gradually improves $(\\textcircled{3},\\textcircled{2}\\,\\nu.s.\\textcircled{1})$ , showing the enhanced perception   \n524 capability with more diversified objects. Filtering the ROIs with overlarge size (i.e., Rule Filter)   \n525 brings considerable gains for the average L2 error of $0.07\\mathrm{m}$ and average collision rate of $0.10\\%$   \n526 $(\\textcircled{4}\\nu.s.\\textcircled{3})$ . One interesting observation is that decreasing the confidence threshold would slightly   \n527 improve the L2 error while causing higher collision rate $\\left(\\textcircled{5}\\nu.s.\\textcircled{4}\\right)$ . In contrast, increasing the threshold   \n528 obtains lower average collision rate of $0.17\\%$ and higher average L2 error of $0.98\\mathrm{m}$ . This reveals the   \n529 importance of providing diversified ROIs for angular perception learning as well as ensuring high   \n530 quality. The model with the confidence score of 0.35, all prompt words and Rule Filter achieves   \n531 balanced performance with the average L2 error of $0.90\\mathrm{m}$ and average collision rate of $0.19\\%$ . ", "page_idx": 13}, {"type": "text", "text": "532 A.6 Different Image Sizes and BEV Resolution ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "533 For safe autonomous driving, increasing the input size of the multi-view images and the resolution   \n534 of the built BEV representation is an effective way, which provide more detailed environmental   \n535 information. While benefiting perception and planning, it inevitably brings heavy computation cost.   \n536 We then ablate the image size and BEV resolution of our UAD to find a balanced version between   \n537 performance and efficiency, as shown in Tab. 14. The results show that our UAD with ResNet  \n538 101 [17], image size of $1600\\!\\times\\!900$ , BEV resolution of $200\\!\\times\\!200$ , achieves the best performance   \n539 compared with previous methods UniAD [21] and VAD-Base [22] while running faster with 7.2FPS   \n540 $\\big(\\circled{6}\\big)$ . By replacing the backbone with ResNet-50, our UAD is more efficient with little performance   \n541 degradation $\\left(\\mathfrak{H}\\right)\\,\\nu.s.\\,\\left(\\widehat{\\bullet}\\right)$ . We further align the settings of VAD-Tiny, which has an inference speed   \n542 of outstanding 17.6FPS $\\mathbf{\\sigma}(\\pmb{\\mathcal{D}})$ , to explore the influence of much smaller input sizes. Tab. 14 shows   \n543 that our UAD still achieves excellent performance even compared with VAD-Base of high-resolution   \n544 inputs $\\left(\\textcircled{4}\\nu.s.\\ \\textcircled{3}\\right)$ . Notably, our UAD of this version has the fastest inference speed of 18.9FPS. This   \n545 again proves the effectiveness of our method in performing fine-grained perception, as well as the   \n546 robustness to fit the inputs of different sizes. ", "page_idx": 13}, {"type": "image", "img_path": "SLXHVsbU8r/tmp/6d20a361dcc03ca4cce6ff89c40e373e66dc4466f0e5df5e2fa725789fda4613.jpg", "img_caption": ["Figure 7: Optimization of UniAD (a) and our UAD (b) with different pre-trained backbone weights. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "547 A.7 Runtime Analysis ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "548 Tab. 15 compares the runtime of each module between the modularized method UniAD [21] and   \n549 our UAD. As we adopt the Backbone and BEV Encoder from BEVFormer [25] that are the same in   \n550 UniAD, the latency of feature extraction is similar with little difference due to different pre-processing.   \n551 The modular sub-tasks in UniAD consume most of the runtime, i.e., significant $71.8\\%$ for Det&Track   \n552 $(31.2\\%)$ , Map $(19.8\\%)$ , Motion $(10.9\\%)$ and Occupancy $(9.9\\%)$ , respectively. In contrast, our UAD   \n553 performs simple Angular Partition and Dreaming Decoder, which take only $14.0\\%$ $(19.3\\mathrm{ms})$ to model   \n554 the complex environment. This demonstrates our insight that it\u2019s a necessity to liberate end-to-end   \n555 autonomous driving from costly modularization. The downstream Planning Head takes negligible   \n556 $1.5\\mathrm{ms}$ to plan the ego trajectory, compared with $9.7\\mathrm{ms}$ in UniAD. Finally, our UAD finishes the   \n557 inference with a total runtime of $138.3\\mathrm{ms}$ , $3.4\\times$ faster than the $465.1\\mathrm{ms}$ of UniAD, showing the   \n558 efficiency of our design. ", "page_idx": 14}, {"type": "text", "text": "559 A.8 Classification of Angular Perception ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "560 The proposed angular perception pretext learns spatio-temporal knowledge of the driving scene   \n561 by predicting the objectness of each sector region, which is supervised by the generated binary   \n562 angular-wise label. We show the perception ability by evaluating the classification metrics based on   \n563 the validation split of the nuScenes [2] dataset. Fig. 6 draws the Precision-Recall (PR) curve and   \n564 Receiver-Operating-Characteristic (ROC) curve in different driving scenes (i.e., turn left, go straight   \n565 and turn right). In the PR curve, our UAD achieves balanced precision and recall scores in different   \n566 driving scenes, showing the effectiveness of our pretext task to perceive the surrounding objects.   \n567 Notably, the performance of go straight scenes is slightly better than the steering ones under all   \n568 thresholds. This proves our insight to design tailored direction-aware learning strategy for improving   \n569 the safety-critical turn left and turn right scenes. The ROC curve shows the robustness of our angular   \n570 perception pretext to classify the objects from complex environmental observations. ", "page_idx": 14}, {"type": "image", "img_path": "SLXHVsbU8r/tmp/f3ebffa3c88756e5733518de5a999de43ee85e357adaa470e832e56e3190f90e.jpg", "img_caption": ["Figure 8: Visualization of the angular perception. "], "img_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "SLXHVsbU8r/tmp/a8056fc4d11c20132c07bf83af81ec25d31edaf6b03e9da88852437e5e055fd0.jpg", "img_caption": ["Figure 9: Visualization of the planning results. The first two rows show the success of our method in safe planning in complex scenarios, while the third row exhibits a failure case of our planner when no temporal information could be acquired when $t\\!=\\!0$ . "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "571 A.9 Influence of Pre-training ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "572 Pre-training the backbone network with fundamental tasks is a commonly used metric to benefit   \n573 representation learning. As mentioned in \u201cDifferent Backbones and Pre-trained Weights\u201d of Sec. 4.4   \n574 in the manuscript, the performance of the previous SOTA method UniAD [21] dramatically degrades   \n575 without the pre-trained weights from BEVFormer [25]. Here we further detail the influence by   \n576 comparing the training losses and planning performances with different pre-trained weights in Fig. 7.   \n577 Fig. 7a shows that the training losses increase by about 20 on average when replaced with the   \n578 pre-trained weights from ImageNet [10]. Correspondingly, the average L2 error is significantly higher   \n579 than the one with the pre-trained weights from BEVFormer. This reveals that UniAD heavily relies   \n580 on the perceptive pre-training in BEVFormer to optimize modularized sub-tasks. In contrast, our   \n581 UAD performs comparably even without any pre-training (see Fig. 7b), proving the effectiveness of   \n582 our designs for robust optimization. ", "page_idx": 15}, {"type": "image", "img_path": "SLXHVsbU8r/tmp/574b5ec86cb77ea8fecb3188461eb58c276630347353102af11a4072201d21c9.jpg", "img_caption": ["Figure 10: Visualization of angular perception and planning in Carla. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "583 A.10 More Visualizations ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "584 Open-loop Planning We provide more visualizations about the predicted angular-wise objectness   \n585 and planning results on nuScenes [2]. Fig. 8 compares the discrete objectness scores and ground   \n586 truth, proving the effectiveness of our angular perception pretext to perceive the objects in each sector   \n587 region. The planning results of previous SOTA methods (i.e., UniAD [21] and VAD [22]) and our   \n588 UAD are shown in Fig. 9. With the designed pretext and tailored training strategy, our method could   \n589 plan a more reasonable ego trajectory under different driving scenarios, proving the effectiveness   \n590 of our work. The third row shows the failure case of our planner. In this case, the ego car is given   \n591 the \u201cTurn Right\u201d command when $t=0$ (i.e., the first frame of the driving scenario), leading to   \n592 ineffectiveness of our planner in learning helpful temporal information. A possible solution to deal   \n593 with this is to apply an auxiliary trajectory prior for the first several frames, and we leave this to   \n594 future work.   \n595 Closed-loop Simulation Fig. 10 visualizes the predicted objectness and planning results in the   \n596 Town05 Long benchmark of CARLA [11]. Following the setting of ST-P3 [20] in closed-loop evalua  \n597 tion, we collect visual observations from the cameras of \u201cCAM_FRONT\u201d, \u201cCAM_FRONT_LEFT\u201d,   \n598 \u201cCAM_FRONT_RIGHT\u201d and \u201cCAM_BACK\u201d. It shows that the sector regions in which the surround  \n599 ing objects exist are successfully captured by our UAD, proving the effectiveness and robustness of   \n600 our design. Notably, the missed objects by GroundingDINO [28], e.g., the black car in the camera of   \n601 \u201cCAM_FRONT_LEFT\u201d at $t=145$ , are surprisingly perceived and marked in the corresponding sector.   \n602 This demonstrates our method has the capability of learning perceptive knowledge in a data-driven   \n603 manner, even with coarse supervision by the generated 2D pseudo boxes from GroundingDINO. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "604 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "08 Answer: [Yes]   \n09 Justification: The main claims made in the abstract and introduction accurately reflect the   \n10 paper\u2019s contributions and scope, please see Sec. 4.   \n11 Guidelines:   \n12 \u2022 The answer NA means that the abstract and introduction do not include the claims   \n13 made in the paper.   \n14 \u2022 The abstract and/or introduction should clearly state the claims made, including the   \n15 contributions made in the paper and important assumptions and limitations. A No or   \n16 NA answer to this question will not be perceived well by the reviewers.   \n17 \u2022 The claims made should match theoretical and experimental results, and reflect how   \n18 much the results can be expected to generalize to other settings.   \n19 \u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals   \n20 are not attained by the paper. ", "page_idx": 17}, {"type": "text", "text": "621 2. Limitations ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: The limitations of the work are discussed in this paper, please see Sec. 4.5. Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 17}, {"type": "text", "text": "652 3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "653 Question: For each theoretical result, does the paper provide the full set of assumptions and   \n654 a complete (and correct) proof? ", "page_idx": 17}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 17}, {"type": "text", "text": "56 Justification: This paper does not include theoretical results. Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 18}, {"type": "text", "text": "668 4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: All the information needed to reproduce the main experimental results of the paper is disclosed, please see Sec. 4.1. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 18}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 18}, {"type": "text", "text": "711 Answer: [Yes]   \n712 Justification: This paper provides open access to the data and code to reproduce the main   \n713 experimental results, please see Sec. 4.1.   \n714 Guidelines:   \n715 \u2022 The answer NA means that paper does not include experiments requiring code.   \n716 \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/   \n717 public/guides/CodeSubmissionPolicy) for more details.   \n718 \u2022 While we encourage the release of code and data, we understand that this might not be   \n719 possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not   \n720 including code, unless this is central to the contribution (e.g., for a new open-source   \n721 benchmark).   \n722 \u2022 The instructions should contain the exact command and environment needed to run to   \n723 reproduce the results. See the NeurIPS code and data submission guidelines (https:   \n724 //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n725 \u2022 The authors should provide instructions on data access and preparation, including how   \n726 to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n727 \u2022 The authors should provide scripts to reproduce all experimental results for the new   \n728 proposed method and baselines. If only a subset of experiments are reproducible, they   \n729 should state which ones are omitted from the script and why.   \n730 \u2022 At submission time, to preserve anonymity, the authors should release anonymized   \n731 versions (if applicable).   \n732 \u2022 Providing as much information as possible in supplemental material (appended to the   \n733 paper) is recommended, but including URLs to data and code is permitted.   \n734 6. Experimental Setting/Details   \n735 Question: Does the paper specify all the training and test details (e.g., data splits, hyper  \n736 parameters, how they were chosen, type of optimizer, etc.) necessary to understand the   \n737 results?   \n738 Answer: [Yes]   \n739 Justification: All the training and test details are specified in this paper, please see Sec. 4.1.   \n740 Guidelines:   \n741 \u2022 The answer NA means that the paper does not include experiments.   \n742 \u2022 The experimental setting should be presented in the core of the paper to a level of detail   \n743 that is necessary to appreciate the results and make sense of them.   \n744 \u2022 The full details can be provided either with the code, in appendix, or as supplemental   \n745 material.   \n746 7. Experiment Statistical Significance   \n747 Question: Does the paper report error bars suitably and correctly defined or other appropriate   \n748 information about the statistical significance of the experiments?   \n749 Answer: [Yes]   \n750 Justification: This paper reports information about the statistical significance of experiments,   \n751 please see Sec. 4.   \n752 Guidelines:   \n753 \u2022 The answer NA means that the paper does not include experiments.   \n754 \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confi  \n755 dence intervals, or statistical significance tests, at least for the experiments that support   \n756 the main claims of the paper.   \n757 \u2022 The factors of variability that the error bars are capturing should be clearly stated (for   \n758 example, train/test split, initialization, random drawing of some parameter, or overall   \n759 run with given experimental conditions).   \n760 \u2022 The method for calculating the error bars should be explained (closed form formula,   \n761 call to a library function, bootstrap, etc.)   \n762 \u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n763 \u2022 It should be clear whether the error bar is the standard deviation or the standard error   \n764 of the mean.   \n765 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should   \n766 preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis   \n767 of Normality of errors is not verified.   \n768 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or   \n769 figures symmetric error bars that would yield results that are out of range (e.g. negative   \n770 error rates).   \n771 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how   \n772 they were calculated and reference the corresponding figures or tables in the text.   \n773 8. Experiments Compute Resources   \n774 Question: For each experiment, does the paper provide sufficient information on the com  \n775 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n776 the experiments?   \n777 Answer: [Yes]   \n778 Justification: This paper provides sufficient information on the computer resources, please   \n779 see Sec. 4.1.   \n780 Guidelines:   \n781 \u2022 The answer NA means that the paper does not include experiments.   \n782 \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,   \n783 or cloud provider, including relevant memory and storage.   \n784 \u2022 The paper should provide the amount of compute required for each of the individual   \n785 experimental runs as well as estimate the total compute.   \n786 \u2022 The paper should disclose whether the full research project required more compute   \n787 than the experiments reported in the paper (e.g., preliminary or failed experiments that   \n788 didn\u2019t make it into the paper).   \n789 9. Code Of Ethics   \n790 Question: Does the research conducted in the paper conform, in every respect, with the   \n791 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n792 Answer: [Yes]   \n793 Justification: The conducted research conforms with the NeurIPS Code of Ethics.   \n794 Guidelines:   \n795 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n796 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n797 deviation from the Code of Ethics.   \n798 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid  \n799 eration due to laws or regulations in their jurisdiction).   \n800 10. Broader Impacts   \n801 Question: Does the paper discuss both potential positive societal impacts and negative   \n802 societal impacts of the work performed?   \n803 Answer: [Yes]   \n804 Justification: This paper discusses both potential positive societal impacts and negative   \n805 societal impacts of the work, please see Sec. 4.5.   \n806 Guidelines:   \n807 \u2022 The answer NA means that there is no societal impact of the work performed.   \n808 \u2022 If the authors answer NA or No, they should explain why their work has no societal   \n809 impact or why the paper does not address societal impact.   \n810 \u2022 Examples of negative societal impacts include potential malicious or unintended uses   \n811 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n812 (e.g., deployment of technologies that could make decisions that unfairly impact specific   \n813 groups), privacy considerations, and security considerations.   \n814 \u2022 The conference expects that many papers will be foundational research and not tied   \n815 to particular applications, let alone deployments. However, if there is a direct path to   \n816 any negative applications, the authors should point it out. For example, it is legitimate   \n817 to point out that an improvement in the quality of generative models could be used to   \n818 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n819 that a generic algorithm for optimizing neural networks could enable people to train   \n820 models that generate Deepfakes faster.   \n821 \u2022 The authors should consider possible harms that could arise when the technology is   \n822 being used as intended and functioning correctly, harms that could arise when the   \n823 technology is being used as intended but gives incorrect results, and harms following   \n824 from (intentional or unintentional) misuse of the technology.   \n825 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n826 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n827 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n828 feedback over time, improving the efficiency and accessibility of ML).   \n829 11. Safeguards   \n830 Question: Does the paper describe safeguards that have been put in place for responsible   \n831 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n832 image generators, or scraped datasets)?   \n833 Answer: [NA]   \n834 Justification: This paper poses no such risks.   \n835 Guidelines:   \n836 \u2022 The answer NA means that the paper poses no such risks.   \n837 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n838 necessary safeguards to allow for controlled use of the model, for example by requiring   \n839 that users adhere to usage guidelines or restrictions to access the model or implementing   \n840 safety filters.   \n841 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n842 should describe how they avoided releasing unsafe images.   \n843 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n844 not require this, but we encourage authors to take this into account and make a best   \n845 faith effort.   \n846 12. Licenses for existing assets   \n847 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n848 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n849 properly respected?   \n850 Answer: [Yes]   \n851 Justification: The assets used in this paper are credited and the license is respected.   \n852 Guidelines:   \n853 \u2022 The answer NA means that the paper does not use existing assets.   \n854 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n855 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n856 URL.   \n857 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n858 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n859 service of that source should be provided.   \n860 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n861 package should be provided. For popular datasets, paperswithcode.com/datasets   \n862 has curated licenses for some datasets. Their licensing guide can help determine the   \n863 license of a dataset.   \n864 \u2022 For existing datasets that are re-packaged, both the original license and the license of   \n865 the derived asset (if it has changed) should be provided.   \n866 \u2022 If this information is not available online, the authors are encouraged to reach out to   \n867 the asset\u2019s creators.   \n868 13. New Assets   \n869 Question: Are new assets introduced in the paper well documented and is the documentation   \n870 provided alongside the assets?   \n871 Answer: [NA]   \n872 Justification: This paper does not release new assets.   \n873 Guidelines:   \n874 \u2022 The answer NA means that the paper does not release new assets.   \n875 \u2022 Researchers should communicate the details of the dataset/code/model as part of their   \n876 submissions via structured templates. This includes details about training, license,   \n877 limitations, etc.   \n878 \u2022 The paper should discuss whether and how consent was obtained from people whose   \n879 asset is used.   \n880 \u2022 At submission time, remember to anonymize your assets (if applicable). You can either   \n881 create an anonymized URL or include an anonymized zip file.   \n882 14. Crowdsourcing and Research with Human Subjects   \n883 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n884 include the full text of instructions given to participants and screenshots, if applicable, as   \n885 well as details about compensation (if any)?   \n886 Answer: [NA]   \n887 Justification: This paper does not involve crowdsourcing nor research with human subjects.   \n888 Guidelines:   \n889 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n890 human subjects.   \n891 \u2022 Including this information in the supplemental material is fine, but if the main contribu  \n892 tion of the paper involves human subjects, then as much detail as possible should be   \n893 included in the main paper.   \n894 \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation,   \n895 or other labor should be paid at least the minimum wage in the country of the data   \n896 collector.   \n897 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human   \n898 Subjects   \n899 Question: Does the paper describe potential risks incurred by study participants, whether   \n900 such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)   \n901 approvals (or an equivalent approval/review based on the requirements of your country or   \n902 institution) were obtained?   \n903 Answer: [NA]   \n904 Justification: This paper does not involve crowdsourcing nor research with human subjects.   \n905 Guidelines:   \n906 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n907 human subjects.   \n908 \u2022 Depending on the country in which research is conducted, IRB approval (or equivalent)   \n909 may be required for any human subjects research. If you obtained IRB approval, you   \n910 should clearly state this in the paper.   \n911 \u2022 We recognize that the procedures for this may vary significantly between institutions   \n912 and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the   \n913 guidelines for their institution.   \n914 \u2022 For initial submissions, do not include any information that would break anonymity (if   \n915 applicable), such as the institution conducting the review. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}]