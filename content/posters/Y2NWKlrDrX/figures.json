[{"figure_path": "Y2NWKlrDrX/figures/figures_3_1.jpg", "caption": "Figure 1: Classic and conformal IO pipelines.", "description": "This figure compares the classic inverse optimization (IO) pipeline with the proposed conformal IO pipeline. The classic IO pipeline involves solving an inverse problem to estimate unknown parameters from decision data, followed by using those parameters to solve a forward optimization problem and prescribe new decisions. In contrast, the conformal IO pipeline first splits the data into training and validation sets. It then solves an inverse problem to estimate parameters, calibrates an uncertainty set using the validation data, and finally solves a robust optimization problem to generate decisions that are less sensitive to parameter uncertainty.", "section": "3.2 An Inverse Optimization Pipeline"}, {"figure_path": "Y2NWKlrDrX/figures/figures_4_1.jpg", "caption": "Figure 1: Classic and conformal IO pipelines.", "description": "The figure visualizes the difference between classic and conformal inverse optimization pipelines.  Classic IO involves obtaining a point estimation of unknown parameters and using it directly to prescribe decisions.  Conformal IO differs by first learning an uncertainty set for the parameters and then solving a robust optimization model to recommend decisions, aiming for higher quality and alignment with human intuition.", "section": "3.2 An Inverse Optimization Pipeline"}, {"figure_path": "Y2NWKlrDrX/figures/figures_7_1.jpg", "caption": "Figure 3: Empirical coverage achieved by the learned uncertainty set (error bar = range).", "description": "This figure shows the empirical coverage achieved by the learned uncertainty set under different target coverage levels and sample sizes of the validation set.  For both the shortest path and knapsack problems, the empirical coverage tends toward the target coverage as the validation sample size increases, demonstrating the asymptotic exactness property of the conformal inverse optimization approach.  When the validation set is small (Nval = 10), the uncertainty set tends to over-cover (conservatively valid), while with larger validation sets (Nval \u2208 {100, 200}) the coverage approaches the target level.", "section": "Uncertainty validity"}, {"figure_path": "Y2NWKlrDrX/figures/figures_8_1.jpg", "caption": "Figure 1: Classic and conformal IO pipelines.", "description": "This figure compares the classic inverse optimization pipeline with the proposed conformal inverse optimization pipeline. The classic pipeline involves a point estimation of unknown parameters, followed by a direct decision prescription. In contrast, the conformal pipeline introduces uncertainty set learning and robust optimization for decision recommendation, leading to more robust and human-aligned decisions. This illustrates the core difference in their approaches to handling uncertainties and ensuring decision quality.", "section": "3.2 An Inverse Optimization Pipeline"}, {"figure_path": "Y2NWKlrDrX/figures/figures_9_1.jpg", "caption": "Figure 5: Percentage reduction in AOG and POG when using the conformal IO vs classic IO.", "description": "This figure shows the percentage reduction in Actual Optimality Gap (AOG) and Perceived Optimality Gap (POG) achieved by using conformal inverse optimization (IO) compared to classic IO.  The results are shown across different percentages of the data used for validation (20%, 40%, 60%, 80%) and varying numbers of observed routes (160, 320, 480, 640, 800). The heatmap visualization makes it easy to see the impact of both the validation set size and the number of observed routes on the performance improvement.", "section": "5 Numerical Studies"}]