[{"figure_path": "tNhwg9U767/figures/figures_1_1.jpg", "caption": "Figure 1: Graph recall is a simple task but also a crucial pivot for other graph reasoning tasks.", "description": "This figure illustrates the graph recall task, which involves presenting a graph to a subject (human or LLM) through a textual description, and then asking them to recall and describe the graph structure. The figure highlights that accurate graph recall is crucial for success in downstream graph reasoning tasks, such as link prediction, graph summarization, and node/graph classification.  If the LLM cannot recall the graph accurately, its performance in these downstream tasks will likely be hindered.", "section": "Introduction"}, {"figure_path": "tNhwg9U767/figures/figures_3_1.jpg", "caption": "Figure 1: Graph recall is a simple task but also a crucial pivot for other graph reasoning tasks.", "description": "This figure illustrates the graph recall task, a fundamental step for more advanced graph reasoning tasks.  It shows how a user provides a textual description of a graph, the LLM processes this information, and then is prompted to recall the structure of the graph. The recalled graph is then compared to the ground truth. The figure emphasizes that the ability to accurately recall a graph is crucial for downstream graph reasoning tasks such as link prediction, graph summary, and node/graph classification.", "section": "1 Introduction"}, {"figure_path": "tNhwg9U767/figures/figures_6_1.jpg", "caption": "Figure 2: Experimental protocols for analyzing microstructures and accuracy of LLM's graph recall. See Sec.3.1 for detailed explanations.", "description": "This figure details the six steps involved in the experimental protocol for evaluating LLMs' graph recall capabilities.  Step 1 introduces the task to the LLM, Step 2 presents the graph vignette (a short paragraph describing the graph), Step 3 involves a memory clearance task (a word span test), Step 4 prompts the LLM to recall the graph, Step 5 retrieves edge probabilities from the LLM's response using token probabilities or Monte Carlo sampling, and finally, Step 6 analyzes the microstructures and performance using the Exponential Random Graph Model (ERGM).", "section": "3 Microstructures and Accuracy of Graph Recall by LLMs"}, {"figure_path": "tNhwg9U767/figures/figures_6_2.jpg", "caption": "Figure 4: Correlation between GPT-3.5's performance at graph recall (y) and link prediction (x).", "description": "The figure shows the correlation between the accuracy of GPT-3.5 in graph recall and link prediction tasks across five different real-world graph datasets (Facebook, CA Road, Reactome, DBLP, and Erdos-Renyi). Each point represents a single graph, with the x-axis showing the accuracy of graph recall and the y-axis showing the accuracy of link prediction.  The correlation coefficient (r) is displayed for each dataset, indicating the strength and direction of the linear relationship between the two tasks. The plots visually demonstrate the positive correlation observed in four of the datasets, with a higher recall accuracy generally leading to higher link prediction accuracy.  The Erdos-Renyi dataset, being a synthetic random graph, shows very little correlation as expected.", "section": "5 Correlation between LLM's Graph Recall and Link Prediction"}, {"figure_path": "tNhwg9U767/figures/figures_14_1.jpg", "caption": "Figure 2: Experimental protocols for analyzing microstructures and accuracy of LLM's graph recall. See Sec.3.1 for detailed explanations.", "description": "This figure illustrates the experimental setup used to evaluate LLMs' graph recall abilities.  It outlines a six-step process: 1. Task introduction, where the LLM is made aware of the graph recall task. 2. Presentation of a graph vignette (a short descriptive story encoding the graph structure). 3. Memory clearance (a word span test to clear short-term memory). 4. Prompting the LLM to recall the graph structure. 5. Retrieving edge probabilities from the LLM's response (using token probabilities or Monte Carlo sampling). 6. Microstructure analysis and performance measurement using ERGM (Exponential Random Graph Model) to analyze the recalled graph and compare it to the ground truth graph. The figure also shows examples of microstructures analyzed (edge, triangle, star, alt-triangle, alt-2-path).", "section": "3 Microstructures and Accuracy of Graph Recall by LLMs"}, {"figure_path": "tNhwg9U767/figures/figures_14_2.jpg", "caption": "Figure 2: Experimental protocols for analyzing microstructures and accuracy of LLM's graph recall. See Sec. 3.1 for detailed explanations.", "description": "This figure illustrates the six steps involved in the experiment to analyze the microstructures and accuracy of LLMs' graph recall.  Step 1 is task introduction; Step 2 is presenting a graph vignette; Step 3 involves memory clearance (a word span test); Step 4 is prompting the LLM to recall the graph; Step 5 focuses on retrieving edge probabilities using token probabilities or Monte Carlo sampling; Step 6 involves microstructure analysis using ERGM to identify statistical significance of various microstructural patterns and measure performance (accuracy and F1 score).", "section": "3 Microstructures and Accuracy of Graph Recall by LLMs"}, {"figure_path": "tNhwg9U767/figures/figures_15_1.jpg", "caption": "Figure 2: Experimental protocols for analyzing microstructures and accuracy of LLM's graph recall. See Sec.3.1 for detailed explanations.", "description": "This figure illustrates the six steps involved in the LLM graph recall experiment.  Step 1 introduces the task to the LLM. Step 2 presents a paragraph describing a graph. Step 3 involves a memory clearance task using a word span test to simulate delayed queries in real-world scenarios. Step 4 prompts the LLM to recall the graph structure. Step 5 retrieves edge probabilities from the LLM's responses. Finally, Step 6 analyzes microstructures and performance using the Exponential Random Graph Model (ERGM). The figure also includes a visual representation of the five microstructural patterns (edge, triangle, star, alternating triangle, alternating 2-path) used in the analysis.", "section": "3 Microstructures and Accuracy of Graph Recall by LLMs"}, {"figure_path": "tNhwg9U767/figures/figures_15_2.jpg", "caption": "Figure 1: Graph recall is a simple task but also a crucial pivot for other graph reasoning tasks.", "description": "The figure illustrates the graph recall task, which serves as a foundational step for more complex graph reasoning tasks.  The process begins with a user providing a paragraph describing a graph, followed by irrelevant tasks to clear the LLM's short-term memory. Finally, the LLM is prompted to recall the graph structure. The recalled graph is then compared to the original graph, evaluating accuracy and identifying biased microstructural patterns (local subgraph patterns).  These patterns and accuracy metrics are then used to assess the LLM's graph reasoning ability and its connection to other downstream tasks like link prediction, graph summarization, and node/graph classification.", "section": "1 Introduction"}, {"figure_path": "tNhwg9U767/figures/figures_19_1.jpg", "caption": "Figure 2: Experimental protocols for analyzing microstructures and accuracy of LLM's graph recall. See Sec.3.1 for detailed explanations.", "description": "This figure illustrates the experimental procedures used to analyze the microstructures and accuracy of LLMs in graph recall tasks.  It outlines six key steps: 1) Task introduction, where the LLM is informed about the task. 2) Presenting graph vignette, where the LLM is given a textual description of a graph. 3) Memory clearance, where a word span test is used to clear short-term memory. 4) Prompting, where the LLM is asked to recall the graph structure. 5) Retrieving edge probabilities, detailing how edge probabilities are extracted from the LLM's responses. 6) Microstructure analysis & performance measurement, which shows how ERGM is used to analyze the recalled graphs. The diagram shows the flow of information between each step and highlights the methods used for data collection and analysis.", "section": "3 Microstructures and Accuracy of Graph Recall by LLMs"}]