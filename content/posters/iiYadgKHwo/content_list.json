[{"type": "text", "text": "Variational Distillation of Diffusion Policies into Mixture of Experts ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Hongyi Zhou i\u2217\u2020 Denis Blessing\u2021 Ge Li\u2021 Onur Celik\u2021\u00a7 Xiaogang Jia\u2020\u2021 Gerhard Neumann\u2021\u00a7 Rudolf Lioutikov\u2020 ", "page_idx": 0}, {"type": "text", "text": "\u2020 Intuitive Robots Lab, Karlsruhe Institute of Technology \u2021 Autonomous Learning Robots, Karlsruhe Institute of Technology \u00a7 FZI Research Center for Information Technology ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "This work introduces Variational Diffusion Distillation (VDD), a novel method that distills denoising diffusion policies into Mixtures of Experts (MoE) through variational inference. Diffusion Models are the current state-of-the-art in generative modeling due to their exceptional ability to accurately learn and represent complex, multi-modal distributions. This ability allows Diffusion Models to replicate the inherent diversity in human behavior, making them the preferred models in behavior learning such as Learning from Human Demonstrations (LfD). However, diffusion models come with some drawbacks, including the intractability of likelihoods and long inference times due to their iterative sampling process. The inference times, in particular, pose a significant challenge to real-time applications such as robot control. In contrast, MoEs effectively address the aforementioned issues while retaining the ability to represent complex distributions but are notoriously difficult to train. VDD is the first method that distills pre-trained diffusion models into MoE models, and hence, combines the expressiveness of Diffusion Models with the beneftis of Mixture Models. Specifically, VDD leverages a decompositional upper bound of the variational objective that allows the training of each expert separately, resulting in a robust optimization scheme for MoEs. VDD demonstrates across nine complex behavior learning tasks, that it is able to: i) accurately distill complex distributions learned by the diffusion model, ii) outperform existing state-of-the-art distillation methods, and iii) surpass conventional methods for training MoE. The code and videos are available at https://intuitive-robots.github.io/vdd-website. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Diffusion models [1\u20134] have gained increasing attention with their great success in various domains such as realistic image generation [5\u20138]. More recently, diffusion models have shown promise in Learning from Human Demonstrations (LfDs) [9\u201313]. A particularly challenging aspect of LfD is the high variance and multi-modal data distribution resulting from the inherent diversity in human behavior [14]. Due to the ability to generalize and represent complex, multi-modal distributions, diffusion models are a particularly suitable class of policy representations for LfD. However, diffusion models suffer from several drawbacks such as long inference time and intractable likelihood calculation. Many diffusion steps are required for high-quality samples leading to a long inference time, limiting the use in real-time applications such as robot control, where decisions are needed at a high frequency. Moreover, important statistical properties such as exact likelihoods are not easily obtained for diffusion models, which poses a significant challenge for conducting post hoc optimization such as fine-tuning through well-established reinforcement learning (RL) approaches like policy gradients or maximum entropy RL objectives. ", "page_idx": 0}, {"type": "image", "img_path": "iiYadgKHwo/tmp/e23af6e63dfc4aa2f7d141c87b69bfe447a0959c3fe4e39698f2e81ae5f48abe.jpg", "img_caption": ["Figure 1: VDD distills a diffusion policy into an MoE. LfD is challenging due to the multimodality of human behaviour. For example, tele-operated demonstrations of an avoiding task often contain multiple solutions [13]. Lower: A diffusion policy can predict high quality actions but relies on an iterative sampling process from noise to data, shown as the red arrows. Upper: VDD uses the score function to distill a diffusion policy into an MoE, unifying the advantages of both approaches. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "A well-studied approach that effectively addresses these issues are Mixture of Experts (MoE). During inference, the MoE first selects an expert that is subsequently queried for a forward pass. This hierarchical structure provides a fast and simple sampling procedure, tractable likelihood computation, and the ability to represent multimodal distributions. These properties make them a well-suited policy representation for complex, multimodal behavior. However, training Mixture of Experts (MoEs) is often difficult and unstable [15]. The commonly used maximum likelihood objective can lead to undesired behavior due to mode-averaging, where the model fails to accurately represent certain modes. Yet, this limitation has been alleviated by recent methods that use alternative objectives, such as reverse KL-divergence, which do not exhibit mode-averaging behavior [14, 16]. ", "page_idx": 1}, {"type": "text", "text": "To obtain the beneftis of both models, i.e., learning highly accurate generative models with diffusion and obtaining simple, tractable models using a mixture of experts, this work introduces Variational Diffusion Distillation (VDD), a novel method that distills diffusion models to MoEs. Starting from the variational inference objective [17, 18], we derive a lower bound that decomposes the objective into separate per-expert objectives, resulting in a robust optimization scheme. Each per-expert objective elegantly leverages the gradient of the pre-trained score function such that the MoE beneftis from the diffusion model\u2019s properties. The resulting MoE policy performs on par with the diffusion model and covers the same modes, while being interpretable, faster during inference, and has a tractable likelihood. This final policy is readily available to the user for post hoc analysis or fast fine-tuning for more specific situations. A high-level architecture of the VDD model and its relation to the diffusion policy is shown in Fig. 1. VDD is thoroughly evaluated on nine complex behavior-learning tasks that demonstrate the aforementioned properties. As an additional insight, this paper observed that one-step continuous diffusion models already perform well, a finding not discussed in prior work. ", "page_idx": 1}, {"type": "text", "text": "In summary, this work presents VDD, a novel method to distill diffusion models to MoEs, by proposing a variational objective leading to individual and robust expert updates, effectively leveraging a pre-trained diffusion model. The thorough experimental evaluation on nine sophisticated behavior learning tasks show that VDD $i$ ) accurately distills complex distributions, ii) outperforms existing SOTA distillation methods and iii) surpasses conventional MoE training methods. ", "page_idx": 1}, {"type": "text", "text": "2 Related Works ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Diffusion Models for Behavior Learning. Diffusion models have been used in acquiring complex behaviors for solving sophisticated tasks in various learning frameworks. Most of these works train diffusion policies using offilne reinforcement learning [19\u201324], or imitation learning [9, 12, 11, 10, 25]. In contrast, VDD distills diffusion models into an MoE policy to overcome diffusion-based policy drawbacks such as long inference times or intractable likelihoods instead of optimizing policie directly from the data. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Mixture of Experts (MoE) for Behavior Learning. MoE models are well-studied, provide tractable likelihoods, and can represent multi-modality which makes them a popular choice in many domains such as in imitation learning [14, 26\u201331, 16, 13], reinforcement learning [32\u201338] and motion generation [39] to obtain complex behaviors. Although VDD also uses an MoE model, the behaviors are distilled from a pre-trained model using a variational objective and are not trained from scratch. The empirical evaluation demonstrates that VDD\u2019s stable training procedure results in improved performance compared to common MoE learning techniques. ", "page_idx": 2}, {"type": "text", "text": "Knowledge Distillation from Diffusion Models Knowledge distillation from diffusion models has been researched in various research areas. For instance, in text-to-3D modeling, training a NeRFbased text-to-3D model without any 3D data by mapping the 3D scene to a 2D image and leveraging a text-to-2D diffusion is proposed [7]. The work proposes minimizing the Score Distillation Sampling (SDS) loss that is inspired by probability density distillation [40] and incentivizes the 3D-model to be updated into higher density regions as indicated by the score function of the diffusion model. To overcome drawbacks such as over-smoothing and low-diversity problems when using the SDS loss, variational score distillation (VSD) treats the 3D scene as a random variable and optimizes a distribution over these scenes such that the projected 2D image aligns with the 2D diffusion model [8]. In a similar context, the work in [41] proposes distilling a trained diffusion model into another diffusion model while progressively reducing the number of steps. However, even though the number of diffusion steps is drastically reduced, a complete distillation, i.e. one-step inference as for VDD is not provided. Additionally, the resulting model suffers from the same drawbacks of diffusion models such as intractable likelihoods. In contrast, in consistency distillation (CD), diffusion models are distilled to consistency models (CM) [42\u201345] such that data generation is possible in one step from noise to data. However, one-step data generation typically results in lower sample quality, requiring a trade-off between iterative and single-step generation based on the desired outcome. As CMs, VDD performs one-step data generation but distills the pre-trained diffusion model to an MoE which has a tractable likelihood and is efficient in inference time. The experimental evaluations show the advantages of VDD over CMs. Diff-Instruct [46] proposes a two-step framework for distilling diffusion models into implicit generative models, whereas VDD considers an explicit generative model where the model\u2019s density can be directly evaluated. In addition, Diff-Instruct requires training an auxiliary diffusion model, while VDD only optimizes a single model. Score Regularized Policy Optimization (SRPO) [47] also leverages a diffusion behavior policy to regularize the offilne RL-based objective. However, in contrast to SRPO, VDD learns an MoE policy instead of a uni-modal Gaussian policy and explicitly distills a diffusion model instead of using it as guidance during optimization. Furthermore, VDD trains MoEs policies in imitation learning instead of reward-labeled data as in offline RL. A concurrent work, EM-Distillation (EMD)[48], introduces an EM-style distillation objective derived from the mode-covering forward KL divergence. In contrast, VDD proposes an EM-style objective based on the mode-seeking reverse KL but encourages mode-covering behavior by having multiple experts. ", "page_idx": 2}, {"type": "text", "text": "3 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Here, we introduce the notation and foundation for Denoising Diffusion and Mixture of Experts policies. Throughout this work, we assume access to samples from a behavior policy $\\pi^{*}$ and the corresponding state distribution $\\mu$ , that is $\\mathbf{a}\\sim\\pi^{*}(\\cdot|\\mathbf{s})$ and $\\mathbf{s}\\sim\\mu(\\cdot)$ , respectively. ", "page_idx": 2}, {"type": "text", "text": "Denoising Diffusion Policies. Denoising diffusion policies employ a diffusion process to smoothly convert data into noise. For a given state ${\\bf{s}}^{\\prime}$ , a diffusion process is modeled as stochastic differential equation (SDE) [3] ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathrm{d}\\mathbf{a}_{t}=\\mathbf{f}(\\mathbf{a}_{t},t)\\mathrm{d}t+g(t)\\mathrm{d}\\mathbf{w}_{t},\\quad\\mathbf{a}_{0}\\sim\\pi^{*}(\\cdot|\\mathbf{s}^{\\prime}),\\quad\\mathbf{s}^{\\prime}\\sim\\mu(\\cdot)\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "with drift f, diffusion coefficient $g(t)$ and Wiener process $\\mathbf{w}_{t}\\in\\mathbb{R}^{d}$ . The solution of the SDE is a diffusion process $\\left(\\mathbf{a}_{t}\\right)_{t\\in[0,T]}$ with marginal distributions $\\pi_{t}^{*}$ such that $\\pi_{T}\\approx\\mathcal{N}(\\mathbf{0},\\mathbf{I})$ and $\\pi_{0}=\\pi^{*}$ [49, 50] showed that the time-reversal of Eq. 1 is again an SDE given by ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbf{da}_{t}=\\left[\\mathbf{f}(\\mathbf{a}_{t},t)-g^{2}(t)\\nabla_{\\mathbf{a}_{t}}\\log\\pi_{t}^{*}(\\mathbf{a}_{t}|\\mathbf{s}^{\\prime})\\right]\\mathbf{d}t+g(t)\\mathbf{d}\\bar{\\mathbf{w}}_{t},\\quad\\mathbf{a}_{T}\\sim\\mathcal{N}(\\cdot|\\mathbf{0},\\mathbf{I}).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Simulating the SDE generates samples from $\\pi^{*}(\\cdot|\\mathbf{s}^{\\prime})$ starting from pure noise. For most distributions $\\pi^{*}$ , however, we do not have access to the scores $(\\nabla_{\\mathbf{a}_{t}}\\log\\pi_{t}^{*}(\\mathbf{a}_{t}|\\mathbf{s}^{\\prime}))_{t\\in[0,T]}$ . The goal of diffusion", "page_idx": 2}, {"type": "image", "img_path": "iiYadgKHwo/tmp/36ab4d8c6da8e6e9ae0a74f2a0c5f6d6e879c86a815f4c428c2696482767a814.jpg", "img_caption": [], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Figure 2: Illustration of training VDD using the score function for a fixed state in a 2D toy task. (a) The probability density of the distribution is depicted by the color map. The score function is shown by the gradient field, visualized as white arrows. From (b) to (f), we initialize and train VDD until convergence. We initialize 8 components, each represented by an orange circle. These components are driven by the score function to match the data distribution and avoid overlapping modes by utilizing the learning objective in Eq. (11). Eventually, they align with all data modes. ", "page_idx": 3}, {"type": "text", "text": "based modeling is therefore to approximate the intractable scores using a parameterized score function, i.e., $\\bar{\\pmb{f}}_{\\theta}(\\mathbf{a},\\mathbf{s},t)\\,\\approx\\,\\nabla\\log\\bar{\\pi}_{t}^{*}(\\mathbf{a}|\\mathbf{s})$ . To that end, several techniques have been proposed [51, 52], allowing for sample generation by approximately simulating Eq. 2. The most frequently employed SDEs in behavior learning are variance preserving (VP) [2, 10] and variance exploding (VE) [9]. For further details on diffusion-based generative modeling, we refer the reader to [3, 4]. While we only consider VE and VP in this work, VDD can be applied to any score-based method. ", "page_idx": 3}, {"type": "text", "text": "Gaussian Mixtures of Expert Policies. Mixtures of expert policies are conditional discrete latent variable models. Denoting the latent variable as $z$ , the marginal likelihood can be decomposed as ", "page_idx": 3}, {"type": "equation", "text": "$$\nq^{\\phi}(\\mathbf{a}|\\mathbf{s})=\\sum_{z}q^{\\xi}(z|\\mathbf{s})q^{\\nu_{z}}(\\mathbf{a}|\\mathbf{s},z),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $q^{\\xi}(z|\\mathbf{s})$ and $q^{\\nu_{z}}(\\mathbf{a}|\\mathbf{s},z)$ are referred to as gating and experts respectively. $\\xi$ and $\\nu_{z}$ denote the gating and expert parameters and thus $\\phi=\\xi\\cup\\{\\bar{\\nu}_{z}\\}_{z}$ . The gating is responsible for soft-partitioning the state space into sub-regions where the corresponding experts approximate the target density. To sample actions, that is, ${\\bf a}^{\\prime}\\,\\sim\\,q^{\\phi}(\\cdot|{\\bf s}^{\\prime})$ for some state ${\\bf{s}}^{\\prime}$ , we first sample a component index from the gating, i.e., $z^{\\prime}\\sim q^{\\xi}(\\cdot|\\mathbf{s}^{\\prime})$ . The component index selects the respective expert to obtain $\\mathbf{a}^{\\prime}\\sim q^{\\nu_{z}}(\\bar{\\cdot}|\\mathbf{s}^{\\prime},\\bar{z^{\\prime}})$ . For Gaussian MoE the experts are chosen as $q^{\\nu_{z}}(\\mathbf{a}|\\mathbf{s},z)\\overset{\\cdot}{=}N\\,(\\mathbf{a}|\\mu^{\\bar{\\nu_{z}}}(\\mathbf{s}),\\Sigma^{\\nu_{z}}(\\mathbf{s}))$ , where $\\mu^{\\nu_{z}},\\Sigma^{\\nu_{z}}$ could be neural networks parameterized by $\\nu_{z}$ . From the properties of Gaussian distributions, it directly follows that this model class admits tractable likelihoods and fast sampling routines. Furthermore, given enough components, Gaussian MoEs are universal approximators of densities [53], which makes them a good representation for distillation of diffusion policies. ", "page_idx": 3}, {"type": "text", "text": "4 Variational Distillation of Denoising Diffusion Policies ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we outline the mathematical formulations of the VDD model. Detailed descriptions of the model architecture and algorithms can be found in Appendix A. We aim to distill a given diffusion policy $\\pi(\\mathbf{a}|\\mathbf{s})$ by using a different policy representation $q^{\\phi}$ with parameters $\\phi$ . This is useful, e.g., if $q^{\\phi}(\\mathbf{a}|\\mathbf{s})$ has favorable properties such as likelihood tractability or admits fast inference schemes. Assuming that we can evaluate $\\pi$ point-wise, a common approach is to leverage variational inference (VI) to frame this task as an optimization problem by minimizing the reverse Kullback-Leibler (KL) [54] divergence between $q^{\\phi}$ and $\\pi$ , that is, ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\phi}\\;D_{\\mathrm{KL}}(q^{\\phi}(\\mathbf{a}|\\mathbf{s}^{\\prime})\\|\\pi(\\mathbf{a}|\\mathbf{s}^{\\prime}))=\\operatorname*{min}_{\\phi}\\;\\mathbb{E}_{q^{\\phi}(\\mathbf{a}|\\mathbf{s}^{\\prime})}\\left[\\log q^{\\phi}(\\mathbf{a}|\\mathbf{s}^{\\prime})-\\log\\pi(\\mathbf{a}|\\mathbf{s}^{\\prime})\\right],\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "for a specific state ${\\bf{s}}^{\\prime}$ . To obtain a scalable optimization scheme, we combine amortized and stochastic VI [55]. The former allows for learning a conditional model $q^{\\phi}(\\mathbf{a}|\\mathbf{s})$ instead of learning a separate $q^{\\phi}$ for each state, while the latter allows for leveraging mini-batch computations, that is, ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\phi}J(\\phi)=\\operatorname*{min}_{\\phi}\\ \\mathbb{E}_{\\mu({\\mathbf{s}})}D_{\\mathrm{KL}}(q^{\\phi}({\\mathbf{a}}|{\\mathbf{s}})||\\pi({\\mathbf{a}}|{\\mathbf{s}}))\\approx\\operatorname*{min}_{\\phi}\\frac{M}{N}\\sum_{{\\mathbf{s}}_{i}\\sim\\mu}D_{\\mathrm{KL}}(q^{\\phi}({\\mathbf{a}}|{\\mathbf{s}}_{i})||\\pi({\\mathbf{a}}|{\\mathbf{s}}_{i})),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "with batch size $M\\leq N$ . Thus, $J(\\phi)$ can be minimized using gradient-based optimization techniques with a gradient estimator such as reinforce [56, 57] or the reparameterization trick [58]. Note that, while the states are sampled from the given data set of the behavior policy, the actions needed to evaluate the $\\mathrm{KL}$ are generated using our estimated model $q^{\\phi}(\\mathbf{a}|\\mathbf{s}_{i})$ . Yet, there are two difficulties to directly apply this scheme in distilling a diffusion model into an MoE: i) we are not able to evaluate the likelihood $\\pi(\\mathbf{a}|\\mathbf{s})$ of a diffusion model, ii) training of MoE models is notoriously difficult [15]. We will address these two issues in Section 4.1 and Section 4.2, respectively. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "4.1 Scalable Variational Inference for Denoising Diffusion Policy Distillation ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Although we cannot directly evaluate the likelihood of the diffusion policy $\\pi(\\mathbf{a}|\\mathbf{s})$ , we have access to its score functions $\\nabla_{\\mathbf{a}_{t}}\\log\\pi_{t}(\\mathbf{a}_{t}|\\mathbf{s})=f_{\\theta}(\\mathbf{a}_{t},\\mathbf{s},t)$ , where $t\\in[0,T]$ is the diffusion time step. In practice, we would like to evaluate the score in the limit of $t\\rightarrow0$ as $\\begin{array}{r}{\\nabla_{\\mathbf{a}}\\log\\pi^{*}(\\mathbf{a}|\\mathbf{s})\\approx\\operatorname*{lim}_{t\\rightarrow0}f_{\\theta}(\\mathbf{a}_{t},\\mathbf{s},t)}\\end{array}$ . Yet, this might lead to an unstable optimization [59] as this score is often not estimated well throughout the action space, and, hence other diffusion time-step selection processes are needed [47]. For now, we will omit the diffusion time-step for the sake of simplicity and refer to Section 4.3 for a detailed discussion about time-step selection. Moreover, we will, for now, assume that the parametrization of $q^{\\phi}$ is amendable to the reparameterization trick [58]. In this case, we can express $\\mathbf{a}\\tilde{\\mathbf{\\Gamma}}\\sim q^{\\phi}(\\cdot|\\mathbf{s})$ using a transformation $h^{\\phi}(\\epsilon,\\mathbf{s})$ with an auxiliary variable $\\epsilon\\sim p(\\cdot)$ such that $\\mathbf{a}=h^{\\phi}(\\epsilon,\\mathbf{s})$ . We then express the gradient of $J$ w.r.t. $\\phi$ as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\nabla_{\\phi}J(\\phi)\\approx\\frac{M}{N}\\sum_{\\mathbf{s}_{i}\\sim\\mu}\\mathbb{E}_{p(\\epsilon)}\\left[\\nabla_{\\phi}\\log q^{\\phi}(h^{\\phi}(\\epsilon,\\mathbf{s}_{i})|\\mathbf{s}_{i})-\\nabla_{\\phi}\\log\\pi(h^{\\phi}(\\epsilon,\\mathbf{s}_{i})|\\mathbf{s}_{i})\\right].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Using the chain rule for derivatives, it is straightforward to see that ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\nabla_{\\phi}\\log\\pi(\\mathbf{a}|\\mathbf{s}_{i})=\\left(\\nabla_{\\mathbf{a}}\\log\\pi(\\mathbf{a}|\\mathbf{s}_{i})\\right)\\nabla_{\\phi}h^{\\phi}(\\epsilon,\\mathbf{s}_{i})=f_{\\theta}(\\mathbf{a},\\mathbf{s}_{i},t)\\nabla_{\\phi}h^{\\phi}(\\epsilon,\\mathbf{s}_{i}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "As $\\nabla_{\\mathbf{a}}\\log\\pi(\\mathbf{a}|\\mathbf{s}_{i})$ can be replaced by the given score of the pre-trained diffusion policy, we can directly use of VI for optimizing $J$ without evaluating the likelihoods of $\\pi$ . ", "page_idx": 4}, {"type": "text", "text": "4.2 Variational Inference via Mixture of Experts ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "To distill multimodal distributions learned by diffusion models, we require a more complex family of distributions than conditional diagonal Gaussian distributions, which are commonly used in amortized VI. We will therefore use Gaussian mixture of experts. To that end, we construct an upper bound of $J$ which is decomposable into single objectives per expert, allowing for reparameterizing each expert individually and therefore avoiding the need for techniques that perform reparameterization for the entire MoE [60, 61]. The upper bound $U(\\phi,\\tilde{q})$ can be obtained by making use of the chain rule for KL divergences [62, 63, 15], i.e., ", "page_idx": 4}, {"type": "equation", "text": "$$\nJ(\\phi)=U(\\phi,\\tilde{q})-\\mathbb{E}_{\\mu(\\mathbf{s})}\\mathbb{E}_{q^{\\phi}(\\mathbf{a}\\mid\\mathbf{s})}D_{\\mathrm{KL}}\\left(q^{\\phi}(z|\\mathbf{a},\\mathbf{s})\\|\\tilde{q}(z|\\mathbf{a},\\mathbf{s})\\right),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\tilde{q}$ is an arbitrary auxiliary distribution and upper bound ${\\cal U}(\\phi,\\tilde{q})=$ ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mu(\\mathbf{s})}[\\mathbb{E}_{q^{\\xi}(z|\\mathbf{s})}[\\underbrace{\\mathbb{E}_{q^{\\nu_{z}}(\\mathbf{a}|\\mathbf{s},z)}\\left[\\log q^{\\nu_{z}}(\\mathbf{a}|\\mathbf{s},z)-\\log\\pi(\\mathbf{a}|\\mathbf{s})-\\log\\tilde{q}(z|\\mathbf{a},\\mathbf{s})\\right]}_{U_{z}^{\\mathrm{s}}(\\nu_{z},\\tilde{q})}+\\log q^{\\xi}(z|\\mathbf{s})]],\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "with $U_{z}^{\\mathbf{s}}(\\nu_{z},\\tilde{q})$ being the objective function for a single expert $z$ and state s. For further details see Appendix B. Since the expected KL term on the right side of Eq. 8 is always positive, it directly follows that $U$ is an upper bound on $J$ for any $\\tilde{q}$ . This gives rise to an optimization scheme similar to the expectation-maximization algorithm [64], where we alternate between minimization (M-Step) and tightening of the upper bound $U$ (E-Step), that is, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\phi}\\;U(\\phi,\\widetilde{q})\\qquad\\mathrm{and}\\qquad\\operatorname*{min}_{\\widetilde{q}}\\;\\mathbb{E}_{\\mu({\\mathbf{s}})}\\mathbb{E}_{q^{\\phi}({\\mathbf{a}}|{\\mathbf{s}})}D_{\\mathrm{KL}}\\left(q^{\\phi}(z|{\\mathbf{a}},{\\mathbf{s}})\\|\\widetilde{q}(z|{\\mathbf{a}},{\\mathbf{s}})\\right),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "respectively. Please note that $\\tilde{q}$ is fixed during the M-Step and $\\phi$ during the E-Step. In what follows, we identify the M-Step as a hierarchical VI problem and elaborate on the E-Step. ", "page_idx": 4}, {"type": "text", "text": "M-Step for Updating the Experts. The decomposition in Eq. 8 allows for optimizing each expert separately. The optimization objective for a specific expert $z$ and state s, that is, $U_{z}^{\\mathbf{s}}(\\nu_{z},\\tilde{q})$ , is ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\nu_{z}}\\;U_{z}^{\\mathrm{s}}(\\nu_{z},\\tilde{q})=\\operatorname*{min}_{\\nu_{z}}\\;\\mathbb{E}_{q^{\\nu_{z}}({\\bf a}|{\\bf s},z)}\\left[q^{\\nu_{z}}({\\bf a}|{\\bf s},z)-\\log\\pi({\\bf a}|{\\bf s})-\\log\\tilde{q}(z|{\\bf a},{\\bf s})\\right].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Note that this objective corresponds to the standard reverse $\\mathrm{KL}$ objective from variational inference (c.f. Eq. 4) , with an additional term $\\log\\tilde{q}(z|\\mathbf{a},\\mathbf{s})$ , which acts as a repulsion force, keeping the individual components from concentrating on the same mode. Assuming that $\\tilde{q}$ is differentiable and following the logic in Section 4.1 it is apparent that the single component objective in $\\mathrm{Eq}\\ 11$ can be optimized only by having access to scores $\\operatorname{\\nabla}_{\\mathbf{a}}\\log\\pi(\\mathbf{a}|\\mathbf{s})$ . Moreover, we can again leverage amortized stochastic VI, that is $\\operatorname*{min}_{\\nu_{z}}$ $\\begin{array}{r}{{\\mathbb E}_{\\mu(\\mathbf{s})}\\left[U_{z}^{\\mathbf{s}}(\\nu_{z},\\tilde{q})\\right]\\approx\\operatorname*{min}_{\\nu_{z}}~\\frac{M}{N}\\sum_{\\mathbf{s}_{i}\\sim\\mu}U_{z}^{\\mathbf{s}_{i}}(\\nu_{z},\\tilde{q})}\\end{array}$ . ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "M-Step for Updating the Gating. The M-Step for the gating parameters, i.e., minimizing $U(\\phi,\\tilde{q})$ with respect to $\\xi\\subset\\phi$ is given by ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\boldsymbol{\\xi}}\\;U(\\phi,\\tilde{q})=\\operatorname*{max}_{\\boldsymbol{\\xi}}\\;\\mathbb{E}_{\\mu(\\mathbf{s})}\\mathbb{E}_{q^{\\xi}(\\boldsymbol{z}|\\mathbf{s})}\\left[q^{\\xi}(\\boldsymbol{z}|\\mathbf{s})-U_{z}^{\\mathbf{s}}(\\nu_{z},\\tilde{q})\\right].\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Using gradient estimators such as reinforce [57], requires evaluating $U_{z}^{\\mathbf{s}}(\\nu_{z},\\tilde{q})$ which is not possible as we do not have access to $\\log\\pi(\\mathbf{a}|\\mathbf{s})$ . This motivates the need for a different optimization scheme. We note that $q^{\\xi}$ is a categorical distribution and can approximate any distribution over $z$ . It does therefore not suffer from problems associated with the forward KL divergence such as mode averaging due to limited complexity of $q^{\\xi}$ . We thus propose using the following objective for optimizing $\\xi$ , i.e., ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\xi}\\;\\mathbb{E}_{\\mu({\\mathbf{s}})}D_{\\mathrm{KL}}(\\pi({\\mathbf{a}}|{\\mathbf{s}})||q^{\\phi}({\\mathbf{a}}|{\\mathbf{s}}))=\\operatorname*{max}_{\\xi}\\;\\mathbb{E}_{\\mu({\\mathbf{s}})}\\mathbb{E}_{\\pi({\\mathbf{a}}|{\\mathbf{s}})}\\mathbb{E}_{\\widetilde{q}(z|{\\mathbf{a}},{\\mathbf{s}})}\\left[\\log q^{\\xi}(z|{\\mathbf{s}})\\right],\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "resulting in a cross-entropy loss that does not require evaluating the intractable $\\log\\pi(\\mathbf{a}|\\mathbf{s})$ . Moreover, we note that using the forward KL does not change the minimizer as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\xi^{*}=\\underset{\\xi}{\\arg\\operatorname*{min}}\\ \\mathbb{E}_{\\mu({\\mathbf{s}})}D_{\\mathrm{KL}}(\\pi({\\mathbf{a}}|{\\mathbf{s}})||q^{\\phi}({\\mathbf{a}}|{\\mathbf{s}}))=\\underset{\\xi}{\\arg\\operatorname*{min}}\\ \\mathbb{E}_{\\mu({\\mathbf{s}})}D_{\\mathrm{KL}}(q^{\\phi}({\\mathbf{a}}|{\\mathbf{s}})||\\pi({\\mathbf{a}}|{\\mathbf{s}})),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "and, therefore does not affect the convergence guarantees of our method. Please note that the forward KL requires samples from the teacher model $\\pi$ . In practice, if the dataset used for training the diffusion model is available, it can be used as a proxy and prevent costly data regeneration. ", "page_idx": 5}, {"type": "text", "text": "E-Step: Tighening the Lower Bound. Using the properties of the KL divergence, it can easily be seen that the global minimizer of the E-Step, i.e., the optimization objective defined in Eq. 10 can be found by leveraging Bayes\u2019 rule, i.e., ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\widetilde{q}(z|\\mathbf{a},\\mathbf{s})=q^{\\phi^{\\mathrm{old}}}(z|\\mathbf{a},\\mathbf{s})=\\frac{q^{\\nu_{z}^{\\mathrm{old}}}(\\mathbf{a}|\\mathbf{s},z)q^{\\xi^{\\mathrm{old}}}(z|\\mathbf{s})}{\\sum_{z}q^{\\nu_{z}^{\\mathrm{old}}}(\\mathbf{a}|\\mathbf{s},z)q^{\\xi^{\\mathrm{old}}}(z|\\mathbf{s})}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The superscript \u2018old\u2019 refers to the previous iteration. Numerically, $\\phi^{\\mathrm{old}}$ can easily be obtained by using a stop-gradient operation which is crucial as $\\tilde{q}$ is fixed during the subsequent $\\mathbf{M}$ -step which requires blocking the gradients of $\\tilde{q}$ with respect to $\\phi$ . As the KL is set to zero after this update, the upper bound is tight after every $\\boldsymbol{\\mathrm E}$ -step ensuring $\\mathbb{E}_{\\mu(\\mathbf{s})}D_{\\mathrm{KL}}(q^{\\phi}(\\mathbf{a}|\\mathbf{s})\\|\\pi^{\\theta}(\\mathbf{a}|\\mathbf{s}))=U(\\phi,\\widetilde{q})$ . Hence, VDD has similar convergence guarantees to EM, i.e., every update step improves the original objective. ", "page_idx": 5}, {"type": "text", "text": "4.3 Choosing the Diffusion-Timestep ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In denoising diffusion models, the score function is usually characterized as a time-dependent function $\\nabla_{\\mathbf{a}_{t}}\\log\\pi_{t}(\\mathbf{a}_{t}|\\mathbf{s})=f_{\\theta}(\\mathbf{a}_{t},\\mathbf{s},t)$ , where $t$ is the diffusion timestep. Yet, the formulation in Section 4.1 only leverages the pretrained diffusion model at time $t\\rightarrow0$ . However, [47]showed that using an ensemble of scores from multiple diffusion time steps significantly improves performances. We, therefore, replace Eq. 11 with a surrogate objective that utilizes scores at different time steps, that is, ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\nu_{z}}\\;U_{z}^{\\mathbf{s}}(\\nu_{z},\\tilde{q})=\\operatorname*{min}_{\\nu_{z}}\\;\\mathbb{E}_{q^{\\nu_{z}}(\\mathbf{a}|\\mathbf{s},z)}\\mathbb{E}_{p(t)}\\left[q^{\\nu_{z}}(\\mathbf{a}|\\mathbf{s},z)-\\log\\pi(\\mathbf{a}|\\mathbf{s},t)-\\log\\tilde{q}(z|\\mathbf{a},\\mathbf{s})\\right],\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "with $p(t)$ being a distribution on $[0,T]$ . Furthermore, we provide an ablation study for different time step selection schemes and empirically confirm the findings from [47]. ", "page_idx": 5}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We conducted imitation learning experiments by distilling two types of diffusion models: variance preserving (VP) [2, 12] and variance exploding (VE) [65, 4]. We selected DDPM as the representative for VP and BESO as the representative for VE. We adopt the choices of samplers and the number of denoising steps in [9] and [13]. Additional evaluation of teacher models with different numbers of denoising steps can be found in Appendix F. In the experiments, VP-1 and VE-1 denote the ", "page_idx": 5}, {"type": "table", "img_path": "iiYadgKHwo/tmp/f86fa5cdd9a92bfc34ed929cf4e02789ef0d91823ea3bffbb813f5ed7703476b.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "iiYadgKHwo/tmp/fdf0f11c51389a606d02449f7d6bd9bb33dccea12b670fc502c9aadf38df98dd.jpg", "table_caption": ["(a) Task Success Rate (or Environment Return for Kitchen) ", "(b) Task Entropy "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Table 1: Comparison of distillation performance, (a) VDD achieves on-par performance with Consistency Distillation (CD) (b) VDD is able to possess versatile skills (indicated by high task entropy) while keeping high success rate. The best results for distillation are bolded, and the highest values except origin models are underlined. In most tasks VDD achieves both high success rate and entropy. Note: to better compare the distillation performance, we report the performance of origin diffusion model, therefore only seed 0 results of diffusion models are presented here. ", "page_idx": 6}, {"type": "text", "text": "results when performing only one denoising step of the respective diffusion models during inference. VDD-VP and VDD-VE denote the results of distilled VDD Additionally, we consider the SoTA Consistency Distillation (CD) [42] and Consistency Trajectory Model (CTM) [44] as baselines for comparing VDD\u2019s performance in distillation. For CD and CTM, we distill from the VE following the original works. For CTM we adapt the implementation and design choices from Consistency Policy [45], which are specialized for behavior learning. We compare VDD against MoE learning baselines, namely the widely-used Expectation-Maximization (EM) [66] approach as a representative of the maximum likelihood-based objective and the recently introduced SoTA method Information Maximizing Curriculum (IMC) as representative of the reverse KL-based objective. To make them stronger baselines, we extend them with the architecture described in Figure 5 and name the extended methods as EM-GPT and IMC-GPT, respectively. For a fair comparison, we used the same diffusion models as the origin model for all distillation methods that we have trained on seed 0. For a statistically significant comparison, all methods have been run on 4 random seeds, and the mean and the standard deviation are reported throughout the evaluation. Detailed descriptions regarding the baselines implementation and hyperparameters selection can be found in Appendix D and E. ", "page_idx": 6}, {"type": "text", "text": "The evaluations are structured as follows. Firstly, we demonstrate that VDD is able to achieve competitive performance with the SoTA diffusion distillation method and the original diffusion models on two established datasets. Next, we proceed to a recently proposed challenging benchmark with human demonstrations, where VDD outperforms existing diffusion distillation and SoTA MoE learning approaches. We then highlight the faster inference time of VDD. Following this, a series of ablation studies reflect the importance of VDD\u2019s essential algorithmic properties. Finally, we provide a visualization to offer deeper insights into our method. ", "page_idx": 6}, {"type": "text", "text": "5.1 Competitive Distillation Performance in Imitation Learning Datasets ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We first demonstrate the effectiveness of VDD using two widely recognized imitation learning datasets: Relay Kitchen [67] and XArm Block Push [68]. A detailed description of these environments is provided in Appendix C. To ensure a fair comparison, we follow the same evaluation process as outlined in [9]. The environment rewards for Relay Kitchen and the success rate for XArm Block Push are presented in Table 1a with mean and standard deviation resulting from 100 environment rollouts. The results indicate that VDD achieves a performance comparable to CD in both tasks, with slightly better outcomes in the block push dataset. An additional interesting finding is that BESO, with only one denoising step (VE-1), already proves to be a strong baseline in these tasks, as the original models outperformed the distillation results in both cases. We attribute this interesting observation to the possibility that the Relay Kitchen and the XArm Block Push tasks are comparably easy to solve and do not provide diverse, multi-modal data distributions. We therefore additionally evaluate the methods on a more recently published dataset (D3IL) [13] which is explicitly generated for complex robot imitation learning tasks and provides task entropy measurements. ", "page_idx": 6}, {"type": "table", "img_path": "iiYadgKHwo/tmp/c2773230bcad3624c4d2bee84a97bf8ac7dba9013fbc16558852d3e600d743fa.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "iiYadgKHwo/tmp/eaa16da21f021705a813efa753abf37df90380cd62c7639188d083c3f14e20a1.jpg", "table_caption": ["Table 2: Comparison between VDD and SoTA MoE approaches, with left: success rate and right: entropy. VDD consistently outperforms EM and IMC in terms of task success. For behavior versatility, VDD outperforms in 4 out of 7 D3IL tasks. ", "Table 3: Inference time in state-based pushing (left) and image-based stacking (right). The gray shaded area indicates the default setting for diffusion models. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "5.2 Replicating Diffusion Performance while Possessing Versatile Behavior ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "The D3IL benchmark provides human demonstrations for several challenging robot manipulation tasks, focusing on evaluating methods in terms of both success rate and versatility, i.e. the different behaviors that solve the same task. This benchmark includes a versatility measure for each task, referred to as task entropy. Task entropy is a scalar value ranging from 0 to 1, where 0 indicates the model has only learned one way to solve the task, and 1 indicates the model has covered all the skills demonstrated by humans. Detailed descriptions of the environments and the calculation of task entropy are provided in the Appendix C. The task success rate of the distilled polices is presented in Table 1a. The results show VDD outperforms consistency distillation and 1-step variants of origin models in 6 out of 7 tasks except the Aligning. However, in the Aligning task VDD achieves higher task entropy, indicating more diverse learned behaviors. The task entropy is presented in Table 1b. The results demonstrate that VDD achieves higher task entropy compared to both consistency models (CD, CTM) and the 1-step diffusion models (VP-1, VE-1) in 4 out of 7 tasks, which shows that our method replicates high-quality versatile behaviors from diffusion policies. ", "page_idx": 7}, {"type": "text", "text": "5.3 Comparison with MoE learning from scratch ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "The previous evaluation demonstrated that VDD can effectively distill diffusion models into MoEs, preserving the performance and behavioral versatility. In this section, we discuss the necessity of using VDD instead of directly learning MoEs from scratch by comparing VDD against EM-GPT and IMC-GPT. Both methods train MoE models from scratch but differ in their objectives. While EM is based on the well-known maximum likelihood objective, IMC is based on a reverse KL objective. The results in Table 2 show that VDD consistently outperforms both, EM-GPT and IMC-GPT across a majority of all tasks in terms of both success rate and task entropy. We attribute the performance boost leveraging the generalization ability of diffusion models and the stable updates provided by our decomposed lower bound Eq.(11). ", "page_idx": 7}, {"type": "text", "text": "5.4 Fast Inference with distilled MoE ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Inference with MoE models do not require an iterative denoising process and are therefore faster in sampling. We evaluate the inference time of VDD against DDPM and BESO on the state-based pushing task and the image-based stacking task and report the average results from 200 predictions in Table 3. In addition to the absolute inference time in milliseconds, we report the number of function evaluations (NFE) in Table 3 for better comparability. The results show that VDD is significantly faster than the original diffusion models in both cases, even when the diffusion model takes only one denoising step. For a fair comparison, all methods used an identical number of transformer layers. The predictions were conducted using the same system (RTX 3070 GPU, Intel i7-12700 CPU). ", "page_idx": 7}, {"type": "image", "img_path": "iiYadgKHwo/tmp/b4f69ec2c1c76b7be68ebcff8e81a2080cf2d8f9b0cecf311e8aea79235b8f83.jpg", "img_caption": ["Figure 3: Ablation studies for key design choices used in VDD. (a) Using only one expert leads to a higher success rate but is unable to solve the task in diverse manners. Sufficiently more experts can trade off task success and action diversities. (b)Learning the gating distribution improves the success rates in three $\\mathrm{D}3\\mathrm{L}$ tasks. (c) A Uniform gating leads to higher task entropy in three out of two tasks. (d) Sampling the score from multiple noise levels leads to a better distillation performance "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "5.5 Ablation Studies ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We assess the importance of VDD\u2019s key properties on different environments by reporting the task performance and task entropy averaged over four different seeds. ", "page_idx": 8}, {"type": "text", "text": "Number of experts matters for task entropy. We start by varying the number of experts of the MoE model while freezing all other hyperparameters on the avoiding task. Figure 3a shows the average task success rate and task entropy of MoE models trained with VDD. The success rate is almost constantly high for all numbers of experts, except for the single expert (i.e. a Gaussian policy) case which shows a slightly higher success rate. However, the single expert can only cover a single mode of the multi-modal behavior space and hence achieves a task entropy of 0. With an increasing number of experts, the task entropy increases and eventually converges after a small drop. ", "page_idx": 8}, {"type": "text", "text": "Training a gating distribution boosts performance. Figure 3b shows the success rates when training a parameterized gating network $\\bar{q}^{\\xi}(z|s)$ (red) and when fixing the probability of choosing expert $z$ to $q(z)=1/N$ , where $N$ is the number of experts (blue). While training a gating distribution increases the success rate over three different tasks, the task entropy (see Figure 3c) slightly decreases in two out of three tasks. This observation makes sense as the MoE with a trained gating distribution leads to an input-dependent specialization of each expert, while the experts with a fixed gating are forced to solve the task in every possible input. ", "page_idx": 8}, {"type": "text", "text": "Interval time step sampling increases task entropy. Here, we explore different time step distributions $p(t)$ , as introduced in Eq.(16). We consider several methods in Fig. 3d: using the minimum time step, i.e., $\\begin{array}{r}{p(t)=\\operatorname*{lim}_{t\\to0}\\bar{\\delta}(t)}\\end{array}$ , where $\\delta$ denotes a Dirac delta distribution, the maximum time step $p(t)=\\delta(T)$ , a uniform distribution on $[0,T]$ and on sub-intervals $[t_{0},t_{1}]\\subset[0,T]$ with interval bounds $t_{0},t_{1}$ being hyperparameters. While success rates were comparable across the variants, interval sampling yielded the highest task entropy with very high success rates. Thus, interval time-step sampling is adopted as our default setting. The results were obtained from the avoiding task. ", "page_idx": 8}, {"type": "text", "text": "5.6 Visualization of the per-Expert Behavior ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We provide additional visualizations on the Avoiding task from the $\\mathrm{D}3\\mathrm{L}$ task suite aiming to provide further intuition on how VDD leverages the individual experts. Figure 4 illustrates the expert selection according to the likelihood of the gating distribution at a given state, offering several key insights. First, VDD effectively distills experts with distinct behaviors, e.g., $z_{1}$ typically moves downward, $z_{2}$ tends to move upward, while $z_{3}$ and $z_{4}$ tend to generate horizontal movements. Second, the gating mechanism effectively deactivates redundant experts $(z_{6},z_{7},z_{8})$ in most states, demonstrating that a larger number of components can be used without harming performance, as the gating mechanism deactivates redundant experts. Lastly, using a single component ( $Z=1$ ) can achieve a perfect success rate at the cost of losing behavior diversity. On the contrary, using many experts potentially results in a slightly lower success rate but increased behavior diversity. These qualitative results are consistent with the quantitative results from the ablation study presented in Figure 3a. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "image", "img_path": "iiYadgKHwo/tmp/500111591b7801a3c7e66b22387f8d064600a36dae6d162024576598b0e6c15e.jpg", "img_caption": ["Figure 4: Trajectory visualization for VDD with different number of components $Z\\in\\{1,2,4,8\\}$ on the Avoiding task (left). Different colors indicate components with highest likelihood according to the learned gating network $q^{\\xi}(z|\\mathbf{s})$ at a state s. For each step we select the action by first sampling an expert from the categorical gating distribution and then take the mean of the expert prediction. We decompose the case $Z=8$ and visualize the individual experts $z_{i}$ (bottom row). Diverse behavior emerges as multiple actions are likely given the same state. For example, moving to the bottom right $(z_{1})$ and top right $(z_{2})$ . An extreme case of losing diversity is seen with $Z=1$ , where the policy is unable to capture the diverse behavior of the diffusion teacher, leading to deterministic trajectories. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work introduced Variational Diffusion Distillation (VDD), a novel method that distills a diffusion model to an MoE. VDD enables the MoE to benefit from the diffusion model\u2019s properties like generalization and complex, multi-modal data representation, while circumventing its shortcomings like long inference time and intractable likelihood calculation. Based on the variational objective, VDD derives a lower bound that enables optimizing each expert individually. The lower-bound leads to a stable optimization and elegantly leverages the gradient of the pre-trained score function such that the overall MoE model effectively benefits from the diffusion model\u2019s properties. The evaluations on nine sophisticated behavior learning tasks show that VDD achieves on-par or better distillation performance compared to SOTA methods while retaining the capability of learning versatile skills. The ablation on the number of experts reveals that a single expert is already performing well, but can not solve the tasks in a versatile manner. Additionally, the results show that training the gating distribution greatly boosts the performance of VDD, but reduces the task entropy. ", "page_idx": 9}, {"type": "text", "text": "Limitations. VDD is not straightforwardly applicable to generating very high-dimensional data like images due to the MoE\u2019s contextual mean and covariance prediction. Scaling VDD to images requires further extensions like prediction in a latent space. Additionally, the number of experts needs to be pre-defined by the user. However, a redundantly high number of experts could increase VDD\u2019s training time and potentially decrease the usage in post hoc fine-tuning using reinforcement learning. Similar to other distillation methods, the performance of VDD is bounded by the origin model. ", "page_idx": 9}, {"type": "text", "text": "Future Work. A promising avenue for further research is to utilize the features of the diffusion \u2018teacher\u2019 model to reduce training time and enhance performance. This can be achieved by leveraging the diffusion model as a backbone and fine-tuning an MoE head to predict the means and covariance matrices of the experts. The time-dependence of the diffusion model can be directly employed to train the MoE on multiple noise levels, effectively eliminating the need for the time-step selection scheme introduced in Section 4.3. ", "page_idx": 9}, {"type": "text", "text": "Broader Impact. Improving and enhancing imitation learning algorithms could make real-world applications like robotics more accessible, with both positive and negative impacts. We acknowledge that it falls on sovereign governments\u2019 responsibility to identify these potential negative impacts. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgement ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We thank Moritz Reuss for the valuable discussions and technical support. H.Z. and R.L. acknowledges funding by the German Research Foundation (DFG) \u2013 448648559. D.B. is supported by funding from the pilot program Core Informatics of the Helmholtz Association (HGF). G.L. is supported in part by the Helmholtz Association of German Research Centers. G.N. was supported in part by Carl Zeiss Foundation through the Project JuBot (Jung Bleiben mit Robotern). The authors also acknowledge support by the state of Baden-W\u00fcrttemberg through HoreKa supercomputer funded by the Ministry of Science, Research and the Arts Baden-W\u00fcrttemberg and by the German Federal Ministry of Education and Research. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International conference on machine learning, pages 2256\u20132265. PMLR, 2015.   \n[2] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:6840\u20136851, 2020.   \n[3] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In International Conference on Learning Representations, 2020.   \n[4] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. Advances in Neural Information Processing Systems, 35: 26565\u201326577, 2022.   \n[5] Alexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob Mcgrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. In International Conference on Machine Learning, pages 16784\u201316804. PMLR, 2022.   \n[6] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents.   \n[7] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. In The Eleventh International Conference on Learning Representations, 2022.   \n[8] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity and diverse text-to-3d generation with variational score distillation. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \n[9] Moritz Reuss, Maximilian Li, Xiaogang Jia, and Rudolf Lioutikov. Goal conditioned imitation learning using score-based diffusion policies. In Robotics: Science and Systems, 2023.   \n[10] Cheng Chi, Siyuan Feng, Yilun Du, Zhenjia Xu, Eric Cousineau, Benjamin Burchfiel, and Shuran Song. Diffusion policy: Visuomotor policy learning via action diffusion. In Proceedings of Robotics: Science and Systems (RSS), 2023.   \n[11] Paul Maria Scheikl, Nicolas Schreiber, Christoph Haas, Niklas Freymuth, Gerhard Neumann, Rudolf Lioutikov, and Franziska Mathis-Ullrich. Movement primitive diffusion: Learning gentle robotic manipulation of deformable objects. IEEE Robotics and Automation Letters, 2024.   \n[12] Tim Pearce, Tabish Rashid, Anssi Kanervisto, Dave Bignell, Mingfei Sun, Raluca Georgescu, Sergio Valcarcel Macua, Shan Zheng Tan, Ida Momennejad, Katja Hofmann, et al. Imitating human behaviour with diffusion models. In The Eleventh International Conference on Learning Representations (ICLR 2023), 2023.   \n[13] Xiaogang Jia, Denis Blessing, Xinkai Jiang, Moritz Reuss, Atalay Donat, Rudolf Lioutikov, and Gerhard Neumann. Towards diverse behaviors: A benchmark for imitation learning with human demonstrations. arXiv preprint arXiv:2402.14606, 2024.   \n[14] Denis Blessing, Onur Celik, Xiaogang Jia, Moritz Reuss, Maximilian Li, Rudolf Lioutikov, and Gerhard Neumann. Information maximizing curriculum: A curriculum-based approach for learning versatile skills. Advances in Neural Information Processing Systems, 36, 2024.   \n[15] Oleg Arenz, Philipp Dahlinger, Zihan Ye, Michael Volpp, and Gerhard Neumann. A unified perspective on natural gradient variational inference with gaussian mixture models. arXiv preprint arXiv:2209.11533, 2022.   \n[16] Philipp Becker, Oleg Arenz, and Gerhard Neumann. Expected information maximization: Using the i-projection for mixture density estimation. In International Conference on Learning Representations, 2019.   \n[17] David M Blei, Alp Kucukelbir, and Jon D McAuliffe. Variational inference: A review for statisticians. Journal of the American statistical Association, 112(518):859\u2013877, 2017.   \n[18] Denis Blessing, Xiaogang Jia, Johannes Esslinger, Francisco Vargas, and Gerhard Neumann. Beyond elbos: A large-scale evaluation of variational methods for sampling. arXiv preprint arXiv:2406.07423, 2024.   \n[19] Michael Janner, Yilun Du, Joshua Tenenbaum, and Sergey Levine. Planning with diffusion for flexible behavior synthesis. In International Conference on Machine Learning, pages 9902\u20139915. PMLR, 2022.   \n[20] Zhendong Wang, Jonathan J Hunt, and Mingyuan Zhou. Diffusion policies as an expressive policy class for offline reinforcement learning. International Conference on Learning Representations, 2023.   \n[21] Huayu Chen, Cheng Lu, Chengyang Ying, Hang Su, and Jun Zhu. Offilne reinforcement learning via high-fidelity generative behavior modeling. In The Eleventh International Conference on Learning Representations, 2023.   \n[22] Anurag Ajay, Yilun Du, Abhi Gupta, Joshua B Tenenbaum, Tommi S Jaakkola, and Pulkit Agrawal. Is conditional generative modeling all you need for decision making? In The Eleventh International Conference on Learning Representations, 2023.   \n[23] Haoran He, Chenjia Bai, Kang Xu, Zhuoran Yang, Weinan Zhang, Dong Wang, Bin Zhao, and Xuelong Li. Diffusion model is an effective planner and data synthesizer for multi-task reinforcement learning. Advances in neural information processing systems, 36, 2023.   \n[24] Bingyi Kang, Xiao Ma, Chao Du, Tianyu Pang, and Shuicheng Yan. Efficient diffusion policies for offline reinforcement learning. Advances in Neural Information Processing Systems, 36, 2023.   \n[25] Ajay Sridhar, Dhruv Shah, Catherine Glossop, and Sergey Levine. Nomad: Goal masked diffusion policies for navigation and exploration. arXiv preprint arXiv:2310.07896, 2023.   \n[26] Maximilian Xiling Li, Onur Celik, Philipp Becker, Denis Blessing, Rudolf Lioutikov, and Gerhard Neumann. Curriculum-based imitation of versatile skills. In 2023 IEEE International Conference on Robotics and Automation (ICRA), pages 2951\u20132957. IEEE, 2023.   \n[27] Niklas Freymuth, Nicolas Schreiber, Aleksandar Taranovic, Philipp Becker, and Gerhard Neumann. Inferring versatile behavior from demonstrations by matching geometric descriptors. In 6th Annual Conference on Robot Learning, 2022.   \n[28] Katharina M\u00fclling, Jens Kober, Oliver Kroemer, and Jan Peters. Learning to select and generalize striking movements in robot table tennis. The International Journal of Robotics Research, 32(3):263\u2013279, 2013.   \n[29] Marco Ewerton, Gerhard Neumann, Rudolf Lioutikov, Heni Ben Amor, Jan Peters, and Guilherme Maeda. Learning multiple collaborative tasks with a mixture of interaction primitives. In 2015 IEEE International Conference on Robotics and Automation (ICRA), pages 1535\u20131542. IEEE, 2015.   \n[30] You Zhou, Jianfeng Gao, and Tamim Asfour. Movement primitive learning and generalization: Using mixture density networks. IEEE Robotics & Automation Magazine, 27(2):22\u201332, 2020.   \n[31] Vignesh Prasad, Alap Kshirsagar, Dorothea Koert Ruth Stock-Homburg, Jan Peters, and Georgia Chalvatzaki. Moveint: Mixture of variational experts for learning human-robot interactions from demonstrations. IEEE Robotics and Automation Letters, 2024.   \n[32] Riad Akrour, Davide Tateo, and Jan Peters. Continuous action reinforcement learning from a mixture of interpretable experts. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44:6795\u20136806, 2020. URL https://api.semanticscholar.org/CorpusID: 219558472.   \n[33] Xue Bin Peng, Michael Chang, Grace Zhang, Pieter Abbeel, and Sergey Levine. Mcp: Learning composable hierarchical control with multiplicative compositional policies. Advances in Neural Information Processing Systems, 32, 2019.   \n[34] Jie Ren, Yewen Li, Zihan Ding, Wei Pan, and Hao Dong. Probabilistic mixture-of-experts for efficient deep reinforcement learning. arXiv preprint arXiv:2104.09122, 2021.   \n[35] Onur Celik, Dongzhuoran Zhou, Ge Li, Philipp Becker, and Gerhard Neumann. Specializing versatile skill libraries using local mixture of experts. In Conference on Robot Learning, pages 1423\u20131433. PMLR, 2022.   \n[36] Onur Celik, Aleksandar Taranovic, and Gerhard Neumann. Acquiring diverse skills using curriculum reinforcement learning with mixture of experts. arXiv preprint arXiv:2403.06966, 2024.   \n[37] Ahmed Hendawy, Jan Peters, and Carlo D\u2019Eramo. Multi-task reinforcement learning with mixture of orthogonal experts. In The Twelfth International Conference on Learning Representations, 2023.   \n[38] Samuele Tosatto, Georgia Chalvatzaki, and Jan Peters. Contextual latent-movements off-policy optimization for robotic manipulation skills. In 2021 IEEE International Conference on Robotics and Automation (ICRA), pages 10815\u201310821. IEEE, 2021.   \n[39] Kay Hansel, Julen Urain, Jan Peters, and Georgia Chalvatzaki. Hierarchical policy blending as inference for reactive robot control. In 2023 IEEE International Conference on Robotics and Automation (ICRA), pages 10181\u201310188. IEEE, 2023.   \n[40] Aaron Oord, Yazhe Li, Igor Babuschkin, Karen Simonyan, Oriol Vinyals, Koray Kavukcuoglu, George Driessche, Edward Lockhart, Luis Cobo, Florian Stimberg, et al. Parallel wavenet: Fast high-fidelity speech synthesis. In International conference on machine learning, pages 3918\u20133926. PMLR, 2018.   \n[41] Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. In International Conference on Learning Representations, 2021.   \n[42] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. In Proceedings of the 40th International Conference on Machine Learning, pages 32211\u201332252, 2023.   \n[43] Yang Song and Prafulla Dhariwal. Improved techniques for training consistency models. In The Twelfth International Conference on Learning Representations, 2024. URL https: //openreview.net/forum?id $\\cdot$ WNzy9bRDvG.   \n[44] Dongjun Kim, Chieh-Hsin Lai, Wei-Hsiang Liao, Naoki Murata, Yuhta Takida, Toshimitsu Uesaka, Yutong He, Yuki Mitsufuji, and Stefano Ermon. Consistency trajectory models: Learning probability flow ode trajectory of diffusion. In The Twelfth International Conference on Learning Representations, 2023.   \n[45] Aaditya Prasad, Kevin Lin, Jimmy Wu, Linqi Zhou, and Jeannette Bohg. Consistency policy: Accelerated visuomotor policies via consistency distillation. In Robotics: Science and Systems, 2024.   \n[46] Weijian Luo, Tianyang Hu, Shifeng Zhang, Jiacheng Sun, Zhenguo Li, and Zhihua Zhang. Diffinstruct: A universal approach for transferring knowledge from pre-trained diffusion models. Advances in Neural Information Processing Systems, 36, 2024.   \n[47] Huayu Chen, Cheng Lu, Zhengyi Wang, Hang Su, and Jun Zhu. Score regularized policy optimization through diffusion behavior. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=xCRr9DrolJ.   \n[48] Sirui Xie, Zhisheng Xiao, Diederik P Kingma, Tingbo Hou, Ying Nian Wu, Kevin Patrick Murphy, Tim Salimans, Ben Poole, and Ruiqi Gao. Em distillation for one-step diffusion models. arXiv preprint arXiv:2405.16852, 2024.   \n[49] Brian DO Anderson. Reverse-time diffusion equation models. Stochastic Processes and their Applications, 12(3):313\u2013326, 1982.   \n[50] Ulrich G Haussmann and Etienne Pardoux. Time reversal of diffusions. The Annals of Probability, pages 1188\u20131205, 1986.   \n[51] Yang Song, Sahaj Garg, Jiaxin Shi, and Stefano Ermon. Sliced score matching: A scalable approach to density and score estimation. In Uncertainty in Artificial Intelligence, pages 574\u2013584. PMLR, 2020.   \n[52] Yang Song and Stefano Ermon. Improved techniques for training score-based generative models. Advances in neural information processing systems, 33:12438\u201312448, 2020.   \n[53] Ian J. Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, Cambridge, MA, USA, 2016. http://www.deeplearningbook.org.   \n[54] Jonas K\u00f6hler, Andreas Kr\u00e4mer, and Frank No\u00e9. Smooth normalizing flows. Advances in Neural Information Processing Systems, 34:2796\u20132809, 2021.   \n[55] Matthew D Hoffman, David M Blei, Chong Wang, and John Paisley. Stochastic variational inference. Journal of Machine Learning Research, 2013.   \n[56] Peter W Glynn. Likelihood ratio gradient estimation for stochastic systems. Communications of the ACM, 33(10):75\u201384, 1990.   \n[57] Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8:229\u2013256, 1992.   \n[58] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.   \n[59] Valentin De Bortoli, Michael Hutchinson, Peter Wirnsberger, and Arnaud Doucet. Target score matching. arXiv preprint arXiv:2402.08667, 2024.   \n[60] Alex Graves. Stochastic backpropagation through mixture density distributions. arXiv preprint arXiv:1607.05690, 2016.   \n[61] Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. arXiv preprint arXiv:1611.01144, 2016.   \n[62] Thomas M Cover. Elements of information theory. John Wiley & Sons, 1999.   \n[63] Oleg Arenz, Gerhard Neumann, and Mingjun Zhong. Efficient gradient-free variational inference using policy search. In International conference on machine learning, pages 234\u2013243. PMLR, 2018.   \n[64] Arthur P Dempster, Nan M Laird, and Donald B Rubin. Maximum likelihood from incomplete data via the em algorithm. Journal of the royal statistical society: series B (methodological), 39 (1):1\u201322, 1977.   \n[65] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. Advances in neural information processing systems, 32, 2019.   \n[66] Todd K Moon. The expectation-maximization algorithm. IEEE Signal processing magazine, 13 (6):47\u201360, 1996.   \n[67] Abhishek Gupta, Vikash Kumar, Corey Lynch, Sergey Levine, and Karol Hausman. Relay policy learning: Solving long-horizon tasks via imitation and reinforcement learning. arXiv preprint arXiv:1910.11956, 2019.   \n[68] Pete Florence, Corey Lynch, Andy Zeng, Oscar A Ramirez, Ayzaan Wahid, Laura Downs, Adrian Wong, Johnny Lee, Igor Mordatch, and Jonathan Tompson. Implicit behavioral cloning. In Conference on Robot Learning, pages 158\u2013168. PMLR, 2022. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A VDD Architecture and Algorithm Box ", "text_level": 1, "page_idx": 14}, {"type": "image", "img_path": "iiYadgKHwo/tmp/5fcae392d08d489ba4b8d7d9f518c2897ab8edd86a1055d01c9c1e464e1a4cb5.jpg", "img_caption": [], "img_footnote": [], "page_idx": 14}, {"type": "image", "img_path": "iiYadgKHwo/tmp/5620afece4afbe83b0313b3b3461a506f225a0c680432f9a809aabf98b2fd484.jpg", "img_caption": [], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "The architecture of VDD is depicted in Fig. 5 and the algorithm box is given in Algorithm 1. Align with the SoTA diffusion policies\u2019 architecture, such as [9, 12], we leverage a transformer architecture for VDD to encode a short observation history, seen as a sequence of states $\\{s_{k}\\}$ . The state sequences are first encoded using a state encoder: a linear layer for state-based tasks or a pre-trained ResNet-18 for image-based tasks. Positional encodings are added to the encoded sequence, which is then fed into a decoder-only transformer. The last output token predicts the parameters of the MoE, including the mean and covariance of each expert, as well as the gating distribution for expert selection. This parameterization enables VDD to predict all experts and the gating distribution with a single forward pass, enhancing inference time efficiency. ", "page_idx": 14}, {"type": "text", "text": "B Derivations ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "B.1 Derivation of the Variational Decomposition (Eq. 8) ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Recall that the marginal likelihood of the Mixture of Experts is given as ", "page_idx": 14}, {"type": "equation", "text": "$$\nq^{\\phi}(\\mathbf{a}|\\mathbf{s})=\\sum_{z}q^{\\xi}(z|\\mathbf{s})q^{\\nu_{z}}(\\mathbf{a}|\\mathbf{s},z),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $z$ denotes the latent variable, $q^{\\xi}(z|\\mathbf{s})$ and $q^{\\nu_{z}}(\\mathbf{a}|\\mathbf{s},z)$ are referred to as gating and experts respectively. ", "page_idx": 14}, {"type": "text", "text": "The expected reverse Kullback-Leibler (KL) divergence between $q^{\\phi}$ and $\\pi$ is given as ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\phi}{\\operatorname*{min}}\\mathbb{E}_{\\mu(\\mathbf{s})}\\,D_{\\mathrm{KL}}(q^{\\phi}(\\mathbf{a}|\\mathbf{s})\\|\\pi(\\mathbf{a}|\\mathbf{s}))=\\underset{\\phi}{\\operatorname*{min}}\\,\\mathbb{E}_{\\mu(\\mathbf{s})}\\,\\mathbb{E}_{q^{\\phi}(\\mathbf{a}|\\mathbf{s})}\\left[\\log q^{\\phi}(\\mathbf{a}|\\mathbf{s})-\\log\\pi(\\mathbf{a}|\\mathbf{s})\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\underset{\\phi}{\\operatorname*{min}}\\,J(\\phi),}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where ", "page_idx": 14}, {"type": "equation", "text": "$$\nJ(\\phi)=\\int_{\\mathbf{s}}\\mu(\\mathbf{s})\\int_{\\mathbf{a}}q^{\\phi}(\\mathbf{a}|\\mathbf{s})\\left[\\log q^{\\phi}(\\mathbf{a}|\\mathbf{s})-\\log\\pi(\\mathbf{a}|\\mathbf{s})\\right]d\\mathbf{a}d\\mathbf{s}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We can write ", "page_idx": 14}, {"type": "equation", "text": "$$\nJ(\\phi)=\\int_{\\mathbf{s}}\\mu(\\mathbf{s})\\sum_{z}q^{\\xi}(z|\\mathbf{s})\\int_{\\mathbf{a}}q^{\\nu_{z}}(\\mathbf{a}|\\mathbf{s},z)\\left[\\log q^{\\phi}(\\mathbf{a}|\\mathbf{s})-\\log\\pi(\\mathbf{a}|\\mathbf{s})\\right]d\\mathbf{a}d\\mathbf{s},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where we have used the definition of the marginal likelihood of the Mixture of Experts in Eq. 3. With the identity ", "page_idx": 14}, {"type": "equation", "text": "$$\nq^{\\phi}(\\mathbf{a}|\\mathbf{s})=\\frac{q^{\\xi}(z|\\mathbf{s})q^{\\nu_{z}}(\\mathbf{a}|\\mathbf{s},z)}{q^{\\phi}(z|\\mathbf{s},\\mathbf{a})}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "we can further write ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\cal J}(\\phi)=\\int_{\\bf s}\\mu({\\bf s})\\sum_{z}q^{\\xi}(z|{\\bf s})\\int_{\\bf a}q^{\\nu_{z}}({\\bf a}|{\\bf s},z)\\left[\\log q^{\\xi}(z|{\\bf s})+\\log q^{\\nu_{z}}({\\bf a}|{\\bf s},z)-\\log q(z|{\\bf s},{\\bf a})\\right.}\\ ~}\\\\ {{\\displaystyle~\\left.-\\log\\pi({\\bf a}|{\\bf s})\\right]d{\\bf a}d{\\bf s}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We can now introduce the auxiliary distribution $\\tilde{q}(z|\\mathbf{a},\\mathbf{s})$ by adding and subtracting it as ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\cal J}(\\phi)=\\int_{\\bf s}\\mu({\\bf s})\\sum_{z}q^{\\xi}(z|{\\bf s})\\int_{\\bf a}q^{\\nu_{z}}({\\bf a}|{\\bf s},z)\\left[\\log q^{\\xi}(z|{\\bf s})+\\log q^{\\nu_{z}}({\\bf a}|{\\bf s},z)-\\log q^{\\phi}(z|{\\bf s},{\\bf a})\\right.}~}\\\\ {{\\displaystyle{\\quad\\quad-\\log\\pi({\\bf a}|{\\bf s})+\\log\\tilde{q}(z|{\\bf a},{\\bf s})-\\log\\tilde{q}(z|{\\bf a},{\\bf s})\\right]d{\\bf a}d{\\bf s}}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We can rearrange the terms such that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{J(\\phi)=\\displaystyle\\int_{\\mathbf{s}}\\mu(\\mathbf{s})\\sum_{z}q^{\\xi}(z|\\mathbf{s})\\int_{\\mathbf{a}}q^{\\nu_{z}}(\\mathbf{a}|\\mathbf{s},z)\\left[\\log q^{\\xi}(z|\\mathbf{s})+\\log q^{\\nu_{z}}(\\mathbf{a}|\\mathbf{s},z)-\\log\\pi(\\mathbf{a}|\\mathbf{s})\\right.}\\\\ &{\\displaystyle\\qquad-\\log\\tilde{q}(z|\\mathbf{a},\\mathbf{s})]\\,d\\mathbf{a}d\\mathbf{s}+\\int_{\\mathbf{s}}\\mu(\\mathbf{s})\\sum_{z}q^{\\xi}(z|\\mathbf{s})\\int_{\\mathbf{a}}q^{\\nu_{z}}(\\mathbf{a}|\\mathbf{s},z)\\left[\\log\\tilde{q}(z|\\mathbf{a},\\mathbf{s})\\right.}\\\\ &{\\displaystyle\\qquad\\left.-\\log q^{\\phi}(z|\\mathbf{s},\\mathbf{a})\\right]d\\mathbf{a}d\\mathbf{s}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "With can plug in the identity ", "page_idx": 15}, {"type": "equation", "text": "$$\nq^{\\nu_{z}}(\\mathbf{a}|\\mathbf{s},z)=\\frac{q^{\\phi}(\\mathbf{a}|\\mathbf{s})q^{\\phi}(z|\\mathbf{s},\\mathbf{a})}{q^{\\xi}(z|\\mathbf{s})}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "into the second sum and obtain ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\cal I}(\\phi)=\\int_{\\bf s}\\mu({\\bf s})\\sum_{z}q^{\\xi}(z|{\\bf s})\\int_{\\bf a}q^{\\nu_{z}}({\\bf a}|{\\bf s},z)\\left[\\log q^{\\xi}(z|{\\bf s})+\\log q^{\\nu_{z}}({\\bf a}|{\\bf s},z)-\\log\\pi({\\bf a}|{\\bf s})-\\log\\tilde{q}(z|{\\bf a},{\\bf s})\\right]}~}\\\\ {{\\displaystyle~~~~+\\int_{\\bf s}\\sum_{z}\\int_{\\bf a}q^{\\phi}({\\bf a}|{\\bf s})q^{\\phi}(z|{\\bf s},{\\bf a})\\left[\\log\\tilde{q}(z|{\\bf a},{\\bf s})-\\log q^{\\phi}(z|{\\bf s},{\\bf a})\\right]d{\\bf a}d{\\bf s}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "which is the expected negative $\\mathrm{KL}$ as ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{I(\\phi)=\\displaystyle\\int_{\\mathbf{s}}\\mu(\\mathbf{s})\\sum_{z}q^{\\xi}(z|\\mathbf{s})\\int_{\\mathbf{a}}q^{\\nu_{z}}(\\mathbf{a}|\\mathbf{s},z)\\left[\\log q^{\\xi}(z|\\mathbf{s})+\\log q^{\\nu_{z}}(\\mathbf{a}|\\mathbf{s},z)-\\log\\pi(\\mathbf{a}|\\mathbf{s})-\\log\\tilde{q}(z|\\mathbf{a},\\mathbf{s})\\right]}&{{}}&{}\\\\ {-\\displaystyle\\mathbb{E}_{\\mu(\\mathbf{s})}\\mathbb{E}_{q^{\\phi}(\\mathbf{a}|\\mathbf{s})}D_{\\mathrm{KL}}(q^{\\phi}(z|\\mathbf{s},\\mathbf{a})\\|\\tilde{q}(z|\\mathbf{a},\\mathbf{s})).}&{{}}&{(28)}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We note that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{U(\\phi,\\widetilde{q})=\\displaystyle\\int_{\\mathbf{s}}\\mu(\\mathbf{s})\\sum_{z}q^{\\xi}(z|\\mathbf{s})\\int_{\\mathbf{a}}q^{\\nu_{z}}(\\mathbf{a}|\\mathbf{s},z)\\left[\\log q^{\\xi}(z|\\mathbf{s})+\\log q^{\\nu_{z}}(\\mathbf{a}|\\mathbf{s},z)-\\log\\pi(\\mathbf{a}|\\mathbf{s})\\right.}\\\\ &{\\displaystyle\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\left.-\\log\\widetilde{q}(z|\\mathbf{a},\\mathbf{s})\\right]d\\mathbf{a}d\\mathbf{s},}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "such that we arrive to the identical expression as in Eq. 8 ", "page_idx": 15}, {"type": "equation", "text": "$$\nJ(\\phi)=U(\\phi,\\tilde{q})-\\mathbb{E}_{\\mu(\\mathbf{s})}\\mathbb{E}_{q^{\\phi}(\\mathbf{a}\\mid\\mathbf{s})}D_{\\mathrm{KL}}\\left(q^{\\phi}(z|\\mathbf{a},\\mathbf{s})\\|\\tilde{q}(z|\\mathbf{a},\\mathbf{s})\\right).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "B.2 Derivation of the Gating Update (Eq. 13) ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "First, we note that ", "page_idx": 15}, {"type": "equation", "text": "$$\nD_{\\mathrm{KL}}(\\pi(\\mathbf{a}|\\mathbf{s})\\|q^{\\phi}(\\mathbf{a}|\\mathbf{s}))=\\mathbb{E}_{\\pi(\\mathbf{a}|\\mathbf{s})}\\left[\\log\\pi(\\mathbf{a}|\\mathbf{s})\\right]-\\mathbb{E}_{\\pi(\\mathbf{a}|\\mathbf{s})}\\left[\\log q^{\\phi}(\\mathbf{a}|\\mathbf{s})\\right]\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "as we are optimizing w.r.t. $\\phi$ , we can write $c o n s t.=\\mathbb{E}_{\\pi(\\mathbf{a}|\\mathbf{s})}\\left[\\log\\pi(\\mathbf{a}|\\mathbf{s})\\right]$ ", "page_idx": 15}, {"type": "equation", "text": "$$\nD_{\\mathrm{KL}}(\\pi(\\mathbf{a}|\\mathbf{s})\\|q^{\\phi}(\\mathbf{a}|\\mathbf{s}))=-\\mathbb{E}_{\\pi(\\mathbf{a}|\\mathbf{s})}\\left[\\log q^{\\phi}(\\mathbf{a}|\\mathbf{s})\\right]+c o n s t.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We can now introduce the latent variable $z$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{D_{\\mathrm{KL}}(\\pi(\\mathbf{a}|\\mathbf{s})\\|q^{\\phi}(\\mathbf{a}|\\mathbf{s}))=-\\mathbb{E}_{\\pi(\\mathbf{a}|\\mathbf{s})}\\left[\\sum_{z}\\tilde{q}(z|\\mathbf{a},\\mathbf{s})\\log q^{\\phi}(\\mathbf{a}|\\mathbf{s})\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\,c o n s t.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We use the identity in Eq. 22 to arrive at ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{D_{\\mathrm{KL}}(\\pi(\\mathbf{a}|s)||q^{\\delta}(\\mathbf{a}|s))=-\\mathbb{E}_{\\pi(\\mathbf{a}|s)}\\left[\\sum_{z}\\tilde{q}(z|\\mathbf{a},\\mathbf{s})\\left(\\log q^{\\nu_{z}}(\\mathbf{a}|\\mathbf{s},z)+\\log q^{\\xi}(z|\\mathbf{s})-\\log q^{\\phi}(z|\\mathbf{s},\\mathbf{a})\\right)\\right.}\\\\ &{}&{\\left.(3\\pi^{2}\\!+\\!\\mathcal{O}^{\\delta})\\right]}\\\\ &{}&{+\\mathcal{O}\\!n^{\\delta},}\\\\ &{}&{=-\\mathbb{E}_{\\pi(\\mathbf{a}|s)}\\left[\\sum_{z}\\tilde{q}(z|\\mathbf{a},\\mathbf{s})\\left(\\log q^{\\nu_{z}}(\\mathbf{a}|\\mathbf{s},z)+\\log q^{\\xi}(z|\\mathbf{s})-\\log q^{\\phi}(z|\\mathbf{s},\\mathbf{a})\\right)\\right.}\\\\ &{}&{\\left.+\\log\\tilde{q}(z|\\mathbf{a},\\mathbf{s})-\\log\\tilde{q}(z|\\mathbf{a},\\mathbf{s})\\right]+\\mathcal{O}\\!n^{\\delta},}\\\\ &{}&{=-\\mathbb{E}_{\\pi(\\mathbf{a}|s)}\\left[\\sum_{z}\\tilde{q}(z|\\mathbf{a},\\mathbf{s})\\left(\\log q^{\\nu_{z}}(\\mathbf{a}|\\mathbf{s},z)+\\log q^{\\xi}(z|\\mathbf{s})-\\log\\tilde{q}(z|\\mathbf{s},\\mathbf{s})\\right)\\right]}\\\\ &{}&{-D_{\\mathrm{KL}}(\\tilde{q}(z|\\mathbf{a},\\mathbf{s})||q^{\\phi}(z|\\mathbf{s},\\mathbf{a}))+\\mathcal{O}\\!n^{\\delta}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Since $D_{\\mathrm{KL}}(\\tilde{q}(z|\\mathbf{a},\\mathbf{s})\\|q^{\\phi}(z|\\mathbf{s},\\mathbf{a}))\\geq0$ we have the upper bound ", "page_idx": 16}, {"type": "equation", "text": "$$\nU(\\phi,\\widetilde{q})=-\\mathbb{E}_{\\pi(\\mathbf{a}|\\mathbf{s})}\\left[\\sum_{z}\\widetilde{q}(z|\\mathbf{a},\\mathbf{s})\\left(\\log q^{\\nu_{z}}(\\mathbf{a}|\\mathbf{s},z)+\\log q^{\\xi}(z|\\mathbf{s})-\\log\\widetilde{q}(z|\\mathbf{a},\\mathbf{s})\\right)\\right].\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We can now write ", "page_idx": 16}, {"type": "equation", "text": "$$\nU(\\phi,\\tilde{q})=-\\mathbb{E}_{\\pi(\\mathbf{a}|\\mathbf{s})}\\left[\\sum_{z}\\tilde{q}(z|\\mathbf{a},\\mathbf{s})\\log q^{\\nu_{z}}(\\mathbf{a}|\\mathbf{s},z)\\right]+\\mathbb{E}_{\\pi(\\mathbf{a}|\\mathbf{s})}\\left[D_{\\mathrm{KL}}(\\tilde{q}(z|\\mathbf{a},\\mathbf{s})\\|q^{\\xi}(z|\\mathbf{s}))\\right].\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Hence optimizing $U(\\phi,\\tilde{q})$ with respect to $\\phi\\;\\;\\;=\\;\\;\\xi\\;\\cup\\;\\{\\nu_{z}\\}_{z}$ is equivalent to optimizing $D_{\\mathrm{KL}}(\\pi(\\mathbf{a}|\\mathbf{s})\\|q^{\\phi}(\\mathbf{a}|\\mathbf{s}))$ . Specifically optimizing with respect to $\\xi$ boils down to ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\xi}\\;U(\\phi,\\widetilde{q})=\\operatorname*{min}_{\\xi}\\;\\mathbb{E}_{\\pi({\\bf a}|{\\bf s})}D_{\\mathrm{KL}}(\\widetilde{q}(z|{\\bf a},{\\bf s})|q^{\\xi}(z|{\\bf s}))=\\operatorname*{max}_{\\xi}\\;\\mathbb{E}_{\\pi({\\bf a}|{\\bf s})}\\mathbb{E}_{\\widetilde{q}(z|{\\bf a},{\\bf s})}[\\log q^{\\xi}(z|{\\bf s})].\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Noting that the expectation concerning $\\mu(\\mathbf{s})$ does not affect the minimizer, concludes the derivation. ", "page_idx": 16}, {"type": "text", "text": "C Environments", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Relay Kitchen: A multi-task kitchen environment with long-horizon manipulation tasks such as moving kettle, open door, and turn on/off lights. The dataset consists of 566 human-collected trajectories with sequences of 4 executed skills. We used the same experiment settings and the pre-trained diffusion models from [9]. ", "page_idx": 16}, {"type": "text", "text": "XArm Block Push: We used the adapted goal-conditioned variant from [9]. The Block-Push Environment consists of an XARm robot that must push two blocks, a red and a green one, into a red and green squared target area. The dataset consists of 1000 demonstrations collected by a deterministic controller with 4 possible goal configurations. The methods got 0.5 credit for every block pushed into one of the targets with a maximum score of 1.0. We use the pretrained Beso model from [9]. ", "page_idx": 16}, {"type": "text", "text": "D3IL [13] is a simulation benchmark with diverse human demonstrations, which aims to evaluate imitation learning models\u2019 ability to capture multi-modal behaviors. D3IL provides 7 simulation tasks consisting of a 7DoF Franka Emika Panda robot and various objects, where each task has different solutions and the robot is required to acquire all behaviors. Except for success rate, D3IL proposes to use task behavior entropy to quantify the policy\u2019s capability of learning multi-modal distributions. Given the predefined behaviors $\\beta$ for each task, the task behavior entropy is defined as, ", "page_idx": 16}, {"type": "text", "text": "where $s_{0}$ refers to the initial state and $S_{0}$ refers to the number of samples from the initial state distribution $p(s_{0})$ . During the simulation, we rollout the policy multiple times for each $s_{0}$ and use a Monte Carlo estimation to compute the expectation of the behavior entropy. ", "page_idx": 17}, {"type": "image", "img_path": "iiYadgKHwo/tmp/57d6f895ce5f3555664fa9ea48f41b58b6358a3db378ee354af19605f42ca572.jpg", "img_caption": ["Figure 7: Visualization of D3IL tasks. We further provide the figure of demonstrations for the Avoiding task, which indicates 24 solutions of it. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "In this paper, we evaluate our algorithm in Avoiding, Aligning, Pushing, and Stacking with statebased representations and Sorting and Stacking with image-based representations. The simulation environments can be found in Fig. 7. The Avoiding task requires the robot to reach the green line without colliding with any obstacles. The task contains 96 demonstrations, consisting of 24 solutions with 4 trajectories for each solution. The Aligning task requires the robot to push the box to match the target position and orientation. The robot can either push the box from inside or outside, thus resulting in 2 solutions. This task contains 1000 demonstrations, 500 for each solution with uniformly sampled initial states. The Pushing task requires the robot to push two blocks to the target areas. The robot can push the blocks in different orders and to different target areas, which gives the task 4 solutions. This task contains 2000 demonstrations, 500 for each solution with uniformly sampled initial states. The Sorting task requires the robot to sort red and blue blocks to the corresponding box. The number of solutions is determined by the sorting order. D3IL provides Sorting 2, 4, and 6 boxes, here we only use the Sorting-4 task, which contains around 1054 demonstrations with 18 solutions. The Stacking task requires the robot to stack three blocks in the target zone. Additionally, the blue block needs to be stacked upright which makes it more challenging. This task contains 1095 demonstrations with 6 solutions. ", "page_idx": 17}, {"type": "text", "text": "D Baselines Implementation ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "BESO [9] is a continuous time diffusion policy that uses a continuous stochastic-differential equation to represent the denoising process. We implement this method from the D3IL benchmark, following the default which predicts one-step action conditional on the past five-step observations. ", "page_idx": 17}, {"type": "text", "text": "DDPM [2, 10] is a discrete diffusion policy. We do not directly use the code from DiffusionPolicy [10] which implements an encoder-decoder transformer structure. For fair comparison, we evaluate this model from the D3IL benchmark which shares the same architecture as in BESO. ", "page_idx": 17}, {"type": "text", "text": "Consistency Distillation [42] is designed to overcome the slow generation of diffusion models. Consistency models can directly map noise to data using one-step and few-step generation and they can be trained either through distilling pre-trained diffusion models or as an independent generative model. Our implementation takes the main training part of the model by integrating a GPT-based diffusion policy as the backbone. ", "page_idx": 17}, {"type": "text", "text": "Consistency Trajectory Models [44] is an extension of the CD model originally used in image generation. It augments the performance by integrating additional CTM and GAN loss terms in consideration. Later, it is used in robot policy prediction in [45] without taking the GAN loss. Our implementation of CTM is extended from our CD implementation, by modifying the loss computation. ", "page_idx": 18}, {"type": "text", "text": "IMC [14] is a curriculum-based approach that uses a curriculum to assign weights to the training data so that the policy can select samples to learn, aiming to address the mode-averaging problem in multimodal distributions. We implement the model using the official IMC code with a GPT structure. ", "page_idx": 18}, {"type": "text", "text": "EM [66] is based on the maximum likelihood objective and follows an iterative optimization scheme, where the algorithm switches between the M-step and the E-step in each iteration. ", "page_idx": 18}, {"type": "text", "text": "E Hyper Parameters ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "E.1 Hyperparameter Selection ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We executed a large-scale grid search to fine-tune key hyperparameters for each baseline method. For other hyperparameters, we choose the value specified in their respective original papers. Below is a list summarizing the key hyperparameters that we swept during the experiment phase. ", "page_idx": 18}, {"type": "text", "text": "BESO: None ", "page_idx": 18}, {"type": "text", "text": "DDPM: None ", "page_idx": 18}, {"type": "text", "text": "Consistency Distillation: $\\mu$ : EMA decay rate, N: see Algorithm 2 in [42]. All the other hyperparameters reuse the ones from the diffusion policy (BESO), as CD requires to be initialized using a pre-trained diffusion model. ", "page_idx": 18}, {"type": "text", "text": "Consistency Trajectory Models: Same as Consistency Distillation. ", "page_idx": 18}, {"type": "text", "text": "IMC-GPT: Eta [14], Number of components ", "page_idx": 18}, {"type": "text", "text": "EM-GPT: Number of components ", "page_idx": 18}, {"type": "text", "text": "VDD-DDPM: Number of components, tmin, tmax ", "page_idx": 18}, {"type": "text", "text": "VDD-BESO: Number of components, \u03c3min, \u03c3max ", "page_idx": 18}, {"type": "table", "img_path": "iiYadgKHwo/tmp/56c1f0a0a6058184a2ab79fcbef123b1521e44eaf9b31e16d98535e44c713d4a.jpg", "table_caption": ["E.2 Hyperparameter List "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "Table 4: Hyperparameter algorithms proposed method and baselines. The \u2018Grid Serach\u2019 column indicates the values over which we performed a grid search. The values in the column which are marked with task names indicate which values were chosen for the reported results. ", "page_idx": 19}, {"type": "text", "text": "F Evaluation of Teacher Diffusion Models with Different Denoising Steps ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Unlike DDPM, which uses a fixed set of timesteps, BESO learns a continuous-time representation of the scores. This continuous representation enables the use of various numerical integration schemes, which can impact the performance of the diffusion model. We conducted an evaluation on the BESO teacher we used with different denoising steps. The results are presented in Table 5. ", "page_idx": 19}, {"type": "table", "img_path": "iiYadgKHwo/tmp/cbb05b4b45e270f762e6598be3f78f68bf648810eab39b30fbf9920e58f369f3.jpg", "table_caption": ["Table 5: BESO with varies denoising steps. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "G Compute Resources ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We train and evaluate all the models based on our private clusters. Each node contains 4 NVIDIA A100 and we use one GPU for each method. We report the average training time in Table 6. ", "page_idx": 20}, {"type": "table", "img_path": "iiYadgKHwo/tmp/c61c1c7a24411d97b1a9e69128f30b6a7259f45b044aa6089821ecfce1d718eb.jpg", "table_caption": [], "table_footnote": ["Table 6: Training time for each method. "], "page_idx": 20}, {"type": "text", "text": "In addition, we evaluate how the number of trainable parameters and training time scale with different numbers of components. The results are presented in Table 7. ", "page_idx": 20}, {"type": "table", "img_path": "iiYadgKHwo/tmp/cf1a562379d7f1e8cff1ad1567bb6790e764c290b123c0bc8271818492964dae.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "Table 7: Adding more experts does not significantly increase the number of neural network parameters or the training time. We conducted the evaluation on the state-based avoiding task, using a machine with an RTX 3070 GPU and an i7-13700 CPU. This result is due to the optimized network architecture of the VDD model, as shown in Figure 5. Adding more experts will only increase the number of output linear layers, i.e., the mean and covariance nets, while the transformer backbone which contains most of the parameters remains unchanged. ", "page_idx": 20}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The papers not including the checklist will be desk rejected. The checklist should follow the references and precede the (optional) supplemental material. The checklist does NOT count towards the page limit. ", "page_idx": 21}, {"type": "text", "text": "Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist: ", "page_idx": 21}, {"type": "text", "text": "\u2022 You should answer [Yes] , [No] , or [NA] .   \n\u2022 [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.   \n\u2022 Please provide a short (1\u20132 sentence) justification right after your answer (even for NA). ", "page_idx": 21}, {"type": "text", "text": "The checklist answers are an integral part of your paper submission. They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper. ", "page_idx": 21}, {"type": "text", "text": "The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a proper justification is given (e.g., \"error bars are not reported because it would be too computationally expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found. ", "page_idx": 21}, {"type": "text", "text": "IMPORTANT, please: ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "\u2022 Delete this instruction block, but keep the section heading \u201cNeurIPS paper checklist\", \u2022 Keep the checklist subsection headings, questions/answers and guidelines below. \u2022 Do not modify the questions and only use the provided macros for your answers. ", "page_idx": 21}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: Our main contribution is a novel method for distilling diffusion models into mixture of experts, which is outlined and described in the abstract and the introduction and the method section. Claims wrt to the performance of the distilled policies are verified in the experiment section. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 21}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: We discuss the limitations of this work in the conclusion section Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 22}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: We provide a description of our learning objective in the method section, and a full derivation in the Appendix. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 22}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We describe how the baselines are implemented in Appendix C and corresponding hyperparameters to reproduce the experiment results in the appendix D ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 23}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: We will open source the codes in the near future once they are cleaned up and anomnymity is not a concern anymore. All the experiments we conducted were using open-source datasets. In the experiments section and appendix C we provide information to get access to the data. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We provide hyperparameter lists for each of the algorithms, how they were chosen and type of optimizer in appendix E. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 24}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: The experiments describe the number of trials and show the deviations in the result tables. There are no deviations for the origin models as they are the used as the base of the distillation and hence set to a fixed seed 0. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 24}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: The used compute resources are described in appendix G. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 25}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: Yes ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 25}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: We discussed the broader impact in the conclusion section ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 25}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: This paper poses no such risks ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 26}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: Yes, we do used pretrained models for diffusion model and open source code base for baselines, which is clearly stated in both experiment section and appendix. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 26}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: We plan to open source the code in the future ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used. ", "page_idx": 26}, {"type": "text", "text": "\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 27}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 27}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 27}]