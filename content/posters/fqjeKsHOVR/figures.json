[{"figure_path": "fqjeKsHOVR/figures/figures_1_1.jpg", "caption": "Figure 1: Figure (a) illustrates the different types of image-text generation models: visual text comprehension models can only generate text, visual text generation models can only generate images, and TextHarmony can generate both text and images. Figure (b) illustrates the versatility of TextHarmony in generating different modalities for various text-centric tasks.", "description": "This figure showcases three different types of image-text generation models and their capabilities.  Figure 1(a) compares TextMonkey (text comprehension only), TextDiffusor (image generation only), and TextHarmony (both text and image generation). Figure 1(b) demonstrates TextHarmony's versatility across various text-centric tasks, including comprehension, perception, generation, and editing of visual text.", "section": "1 Introduction"}, {"figure_path": "fqjeKsHOVR/figures/figures_2_1.jpg", "caption": "Figure 2: Comparison of single-modal and multi-modal output performance in text generation and image generation Tasks. \u201cUni-Modal Output\u201d represents the results achieved by modality-specific supervised fine-tuning. \u201cMulti-Modal Output\" represents the results achieved by modal-independent supervised fine-tuning. Compared to the multi-modal output, a major performance degradation in the uni-modal output is observed for both text generation and image generation tasks.", "description": "This figure compares the performance of single-modal and multi-modal outputs in text and image generation tasks. The single-modal output uses modality-specific fine-tuning, while the multi-modal output uses modality-independent fine-tuning. The results show that the multi-modal output outperforms the single-modal output in both text and image generation tasks, indicating that a unified model is more effective than separate models for multimodal generation.", "section": "4.2 Quantitative Analysis"}, {"figure_path": "fqjeKsHOVR/figures/figures_4_1.jpg", "caption": "Figure 3: Pipeline of TextHarmony. TextHarmony generates both textual and visual content by concatenating a vision encoder, an LLM, and an image decoder. The proposed Slide-LoRA module mitigates the problem of inconsistency in multi-modal generation by partially separating the parameter space.", "description": "This figure illustrates the architecture of TextHarmony, a multimodal generation model.  It shows how a vision encoder, a large language model (LLM), and an image decoder are integrated to generate both text and images. The key innovation, Slide-LoRA, is highlighted as a method to address the common issue of inconsistency between vision and language modalities in multimodal generation, achieving this by partially separating the parameter space during training.", "section": "3 Methodology"}, {"figure_path": "fqjeKsHOVR/figures/figures_5_1.jpg", "caption": "Figure 4: Captions from DetailedTextCaps-100K and MARIO-LAION for the same image. DetailedTextCaps-100K can better depict the textual elements in the image.", "description": "This figure shows two sets of image captions for the same two images.  The first set of captions comes from the MARIO-LAION dataset, which provides relatively short and general descriptions. The second set comes from the newly created DetailedTextCaps-100K dataset. These captions are significantly more detailed and accurately reflect the textual elements present within the images. This comparison demonstrates the superior quality of the DetailedTextCaps-100K dataset for visual text generation tasks.", "section": "3.3 Multi-Modal Pre-Training and Comprehensive Fine-Tuning"}, {"figure_path": "fqjeKsHOVR/figures/figures_8_1.jpg", "caption": "Figure 5: Visualisation of visual text generation.", "description": "This figure shows examples of visual text generation results from different models including SD-XL, PixArt-a, GlyphControl, AnyText, TextDiffuser-2, and TextHarmony.  For each model, there are three examples with the prompts: \"A cake of \\\"Good Time\\\"\", \"A T-shirt of \\\"Keep Focused\\\"\", and \"Photo of A book cover of \\\"Summer Love\\\"\".  The images generated show the variety in style and quality of text rendering achieved by each model.", "section": "4.4 Qualitative Analysis"}, {"figure_path": "fqjeKsHOVR/figures/figures_9_1.jpg", "caption": "Figure 6: Visualisation of visual text editing.", "description": "This figure shows a comparison of visual text editing results between the original image and those generated by AnyText, TextDiffuer-2, and TextHarmony.  The three example images demonstrate the models' abilities to edit text within images,  preserving the overall image quality.  Differences in text clarity, accuracy, and overall aesthetic are readily apparent across different models.  The figure highlights TextHarmony's capacity to perform visual text editing effectively.", "section": "4.4 Qualitative Analysis"}, {"figure_path": "fqjeKsHOVR/figures/figures_14_1.jpg", "caption": "Figure 4: Captions from DetailedTextCaps-100K and MARIO-LAION for the same image. DetailedTextCaps-100K can better depict the textual elements in the image.", "description": "This figure shows four examples of image captions generated using two different methods: MARIO-LAION and DetailedTextCaps-100K.  Each example shows the same image and its caption from both methods. The captions from DetailedTextCaps-100K are longer and more detailed, and they better capture the text contained within each image than those from MARIO-LAION.", "section": "3.3 Multi-Modal Pre-Training and Comprehensive Fine-Tuning"}, {"figure_path": "fqjeKsHOVR/figures/figures_15_1.jpg", "caption": "Figure 8: Visualisation of TextHarmony's visual text comprehension and perception capabilities.", "description": "This figure demonstrates TextHarmony's ability to perform visual text comprehension and perception tasks.  The left side shows an example image with a book cover.  The right side illustrates how the model answers questions about the image, demonstrating its capacity to extract and locate text within images and answer questions based on their contents.", "section": "A.1.2 Visualisation of visual text comprehension and perception"}]