[{"figure_path": "fqjeKsHOVR/tables/tables_6_1.jpg", "caption": "Table 1: Results of visual text comprehension. TextHarmony is compared with both uni-modal generation models and multi-modal generation models. We employ the Accuracy metric for all methods. TextHarmony* is trained without Slide-LoRA.", "description": "This table presents a quantitative comparison of TextHarmony's performance on visual text comprehension tasks against other uni-modal and multi-modal generation models.  It evaluates various models across multiple benchmarks including Document-Oriented VQA, Table VQA, and Scene Text-Centric VQA.  The \"Accuracy\" metric is used for comparison. A version of TextHarmony trained without the Slide-LoRA module (TextHarmony*) is also included as a baseline.", "section": "4.2 Quantitative Analysis"}, {"figure_path": "fqjeKsHOVR/tables/tables_6_2.jpg", "caption": "Table 2: Text grounding performance on MARIO-Eval. The Acc@0.5 metric is employed.", "description": "This table presents the results of the TextHarmony model on the MARIO-Eval benchmark for text grounding.  The Acc@0.5 metric, which measures the accuracy of text grounding at a 0.5 IoU threshold, is used to evaluate the model's performance.  The table compares TextHarmony's performance to two other state-of-the-art models, TGDoc and DocOwl 1.5, demonstrating that TextHarmony achieves superior performance on this task.", "section": "4.2 Quantitative Analysis"}, {"figure_path": "fqjeKsHOVR/tables/tables_7_1.jpg", "caption": "Table 3: Results of visual text editing and generation. TextHarmony is compared with both uni-modal generation models and multi-modal generation models. TextHarmony* is trained without Slide-LoRA.", "description": "This table presents the results of visual text editing and generation tasks.  It compares the performance of TextHarmony against both unimodal (models specializing in either image or text generation) and multimodal (models generating both) generation models.  A version of TextHarmony trained without the Slide-LoRA module (TextHarmony*) is included as a baseline for comparison. The metrics used for evaluation are NED (higher is better), FID (lower is better), and CLIP Score (higher is better).", "section": "4.2.2 Visual Text Generation and Editing"}, {"figure_path": "fqjeKsHOVR/tables/tables_8_1.jpg", "caption": "Table 4: Ablation studies of the config choices of Slide-LoRA and the places to insert Slide-LoRA.", "description": "This table presents the ablation study results for different configurations of Slide-LoRA, a module in TextHarmony responsible for harmonizing visual text comprehension and generation.  It shows how changes in the number of LoRA modules (n) and scaling factor (s) impact the performance metrics on various tasks.  It also compares the impact of placing Slide-LoRA in different locations within the model architecture (Vision Encoder, LLM, or both). The performance metrics include the accuracy (TextVQA, InfoVQA, OCRBench) and image generation metrics (NED, CLIP Score).", "section": "4.3 Ablation Studies"}, {"figure_path": "fqjeKsHOVR/tables/tables_8_2.jpg", "caption": "Table 5: Ablation studies of DetailedTextCaps.", "description": "This table presents the results of an ablation study evaluating the impact of using the DetailedTextCaps-100K dataset on the performance of TextHarmony in visual text generation and editing tasks.  The table compares the performance metrics (NED, FID, and CLIP score) when using the DetailedTextCaps-100K dataset ('w/') against the performance when not using it ('w/o'). The results show that including DetailedTextCaps-100K improves the performance across all three metrics.", "section": "4.2.2 Visual Text Generation and Editing"}, {"figure_path": "fqjeKsHOVR/tables/tables_15_1.jpg", "caption": "Table 1: Results of visual text comprehension. TextHarmony is compared with both uni-modal generation models and multi-modal generation models. We employ the Accuracy metric for all methods. TextHarmony* is trained without Slide-LoRA.", "description": "This table presents a comparison of TextHarmony's performance on visual text comprehension tasks against various unimodal and multimodal generation models.  It shows accuracy scores across several datasets categorized by task type (Document-Oriented VQA, Table VQA, Scene Text-Centric VQA, and OCRBench).  The table highlights TextHarmony's competitive performance, especially when compared to models that only generate text or images, demonstrating the effectiveness of the integrated approach.", "section": "4.2 Quantitative Analysis"}]