{"references": [{"fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-00-00", "reason": "This paper introduces CLIP, a foundational model for vision-language tasks, which heavily influences the current work."}, {"fullname_first_author": "Haotian Liu", "paper_title": "Visual instruction tuning", "publication_date": "2024-00-00", "reason": "This paper introduces Visual Instruction Tuning (VIT), a key method for improving image-text alignment, which is directly compared against in this paper."}, {"fullname_first_author": "Junnan Li", "paper_title": "BLIP-2: Bootstrapping language-image pre-training with frozen image encoders and large language models", "publication_date": "2023-00-00", "reason": "This paper introduces BLIP-2, a significant advancement in vision-language models, which is compared to the current work."}, {"fullname_first_author": "Yanwei Li", "paper_title": "Mini-gemini: Mining the potential of multi-modality vision language models", "publication_date": "2024-00-00", "reason": "This paper introduces Mini-Gemini, a leading VLM architecture, which is used as a baseline for the current work."}, {"fullname_first_author": "Haotian Liu", "paper_title": "LLaVA-NeXT: Improved reasoning, OCR, and world knowledge", "publication_date": "2024-00-00", "reason": "This paper introduces LLaVA-NeXT, another state-of-the-art VLM architecture, which serves as a baseline and is improved by the proposed method."}]}