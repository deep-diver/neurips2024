[{"type": "text", "text": "Seeing the Image: Prioritizing Visual Correlation by Contrastive Alignment ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Xin Xiao1,2\u2217\u2020, Bohong $\\mathbf{W}\\mathbf{u}^{2*}$ , Jiacong $\\mathbf{Wang}^{2,3\\,\\dagger}$ , Chunyuan $\\mathbf{Li^{2}}$ , Xun Zhou2, Haoyuan Guo2 ", "page_idx": 0}, {"type": "text", "text": "1School of Computer Science, Wuhan University 2ByteDance Inc. 3School of Artificial Intelligence, University of Chinese Academy of Sciences https://github.com/foundation-multimodal-models/CAL ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Existing image-text modality alignment in Vision Language Models (VLMs) treats each text token equally in an autoregressive manner. Despite being simple and effective, this method results in sub-optimal cross-modal alignment by over-emphasizing the text tokens that are less correlated with or even contradictory with the input images. In this paper, we advocate for assigning distinct contributions for each text token based on its visual correlation. Specifically, we present by contrasting image inputs, the difference in prediction logits on each text token provides strong guidance of visual correlation. We therefore introduce Contrastive ALignment $(C A L)$ , a simple yet effective re-weighting strategy that prioritizes visually correlated tokens. Our experimental results demonstrate that $C A L$ consistently improves different types of VLMs across different resolutions and model sizes on various benchmarks. Importantly, our method incurs minimal additional computational overhead, rendering it highly efficient compared to alternative data scaling strategies. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Recent advancements in Large Language Models (LLMs) [1\u20134] have opened up new avenues in multimodal understanding, giving rise to a novel model category known as Vision Language Models (VLMs) [5\u20138]. Many recent studies on VLMs are centered around enhancing their capabilities, either through increasing the resolution of input images [9\u201312] or incorporating higher-quality training datasets [13\u201316]. Additionally, research efforts have been directed towards exploring variations of vision models, such as replacing or augmenting vision encoders beyond CLIP [17, 18], including approaches like SigLIP [19], DINO [20, 21] or ConvNeXt [22, 23]. The integration of these techniques has spurred the development of VLMs, continually enhancing their performance across various benchmarks including visual question answering [24\u201328], image captioning [29, 30], and visual grounding [31, 32]. ", "page_idx": 0}, {"type": "text", "text": "Despite these advancements, whether the current alignment strategy on existing image-text datasets performs satisfactorily is often less studied. Existing alignment strategies simply treat all text tokens equally in an auto-regressive manner. Although such a method has been proven to be simple and effective, many text tokens exhibit limited relevance to the visual inputs, which contribute little to image-text modality alignment. Figure 1a presents a sample drawn from the ShareGPT4V [16] dataset, where a large proportion of text tokens including unique, context presents little visual correlation. Treating these text tokens in equal weights results in ineffective training and can introduce negative effects by prioritizing more on fitting the distribution of these visually irrelevant tokens, rather than the image-text modality alignment. ", "page_idx": 0}, {"type": "image", "img_path": "NsxthTVpqA/tmp/b357d8a9c07921da458cdf7098d71dacb986e472927d5bcc61361dbf2bfa1a67.jpg", "img_caption": ["Figure 1: Figure 1a is one sample drawn from the ShareGPT4V dataset, which contains text tokens that are even contradictory with the given image. Figure 1b further presents our human evaluation results on the proportion of noisy samples that contain contradictory tokens. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Moreover, there also exists a proportion of tokens that contradicts visual conditions, which is inevitable in model generated datasets [7, 16, 13, 15, 14], presented in Figure 1a. In particular, we conduct human evaluations on the broadly used GPT-assisted datasets including ShareGPT4V and the detail caption subset of LLaVA-Instruct in Figure 1b by sampling 100 samples from each dataset. We score each sample by 0 and 1 based on whether there exist text tokens that contradict with visual input, and the score is averaged by three annotators. We found approximately half of the sampled datasets contain visually contradictory tokens in both datasets. Imitating the text distribution on these contradictory tokens further harms the image-text modality alignment. Consequently, recent evaluations on existing VLMs [33, 34, 5, 35, 6, 8] present the shortcomings of current alignment strategy from various aspects, including hallucination [36, 37, 33] and responding without depending on visual conditions [27, 38]. ", "page_idx": 1}, {"type": "text", "text": "Fortunately, inspired by recent training-free visual contrastive decoding researches [36, 33, 39], we present that the visual correlation can be directly indicated by contrasting input image conditions. In particular, we investigate the change in prediction logits of text tokens with or without the image input and observe strong relevance between the logit change of each text token and its visual correlation. We therefore propose Contrastive ALignment $(C A L)$ , which is a surprisingly simple re-weighting strategy to prioritize the training of text tokens that are highly correlated with the input image to enhance imagetext modality alignment. Experiments have shown that our proposed method can improve leading VLMs of different kinds including LLaVA-1.5/LLaVA-NeXT [7, 6, 10], MiniGemini(MGM)/MGMHD [11], across different resolution and model size on various types of benchmarks including visual question answering, captioning and grounding. Especially, $C A L$ on LLaVA-Next-13B [10] can bring an impressive performance of 1.7 ANLS on $\\mathrm{\\dot{V}Q}\\mathrm{\\dot{A}}^{\\mathrm{Doc}}$ [26], 3.4 relaxed accuracy on VQAChart [25], 2.2 CIDEr [40] on COCO [30] and 6.3 CIDEr on TextCaps [29], $0.6/0.7$ IoU on validation/test set of RefCOCOg [32]. Moreover, our method introduces little computational overhead, with one auxiliary gradient-free forward operation in each training step. The lightweight feature while the impressive performance of $C A L$ brings current VLMs to a new stage, highlighting the importance of a delicate image-text modality alignment strategy design. We further conduct extensive qualitative analysis for $C A L$ and present the improved ability of OCR recognition and image-captioning. ", "page_idx": 1}, {"type": "text", "text": "In summary, our contributions are listed as follows. ", "page_idx": 1}, {"type": "text", "text": "\u2022 We present that by contrasting image inputs, existing VLMs are able to distinguish visually correlated tokens both visually irrelevant and visually contradictory ones.   \n\u2022 We propose $C A L$ , a contrastive image-text alignment method via token re-weighting, which is lightweight and effective. $C A L$ requires little additional training cost and no additional inference cost.   \n\u2022 Experiments show that our $C A L$ can consistently improve VLMs of different kinds, across different resolutions and sizes in various types of benchmarks. ", "page_idx": 1}, {"type": "image", "img_path": "NsxthTVpqA/tmp/42042cc367a697f8a447acb2f5eece22368057c75304a5dad78387e3fc038e64.jpg", "img_caption": ["Figure 2: Overview of $C A L$ . Figure 2a presents a sample drawn from the ShareGPT4V dataset. We calculate the logit difference w/ or $\\mathrm{w/o}$ image inputs and plot the heat map on partial text tokens. Figure 2b presents the training procedure of $C A L$ , which re-weights the importance of label tokens based on the contrasting logits. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "2 Contrastive Alignment ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we describe the detailed design of $C A L$ . First of all, we review the existing image-text modality alignment method and provide the notations in Section 2.1. Secondly, we show the token discrepancy in cross-modal datasets can be inferred via contrasting image inputs in Section 2.2. Finally, we present a detailed description of our proposed $C A L$ in Section 2.3. ", "page_idx": 2}, {"type": "text", "text": "2.1 Preliminary and Notations ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Preliminary Most existing VLMs adopt a two-stage strategy to align pre-trained image features with text embeddings in Large Language Models, i.e., a PreTraining (PT) stage that uses quantitative while noisy datasets for rough alignment, and an Instruction-Tuning (IT) stage that uses high-quality datasets to enhance the alignment. Both stages treat all tokens equally in an auto-regressive generation manner, i.e., the Maximum Likelihood Estimation (MLE) objective. ", "page_idx": 2}, {"type": "text", "text": "Notations In this paper, we denote the alignment dataset $\\mathbf{D}$ consisting paired image-text samples $\\mathbf{D}=\\{(I^{1},T^{1}),(I^{2},\\bar{T^{2}}),...,(I^{n},T^{n})\\}$ , and denote the logit computation function as $f(\\theta)$ where $\\theta$ is the weight of VLMs. For the $i^{t h}$ sample $(I^{i},T^{i})$ in the training dataset, where $T^{i}$ consists of a sequence of $l$ tokens $T^{i}=[t^{i,1},t^{i,2},...,t^{i,l}]$ , we denote the prediction logit distribution without input $I$ as $\\mathbf{o}$ , and prediction logit distribution with input $I$ as \u02dco, depicted in the following equation: ", "page_idx": 2}, {"type": "equation", "text": "$$\n{\\bf o}^{i,j}=f_{\\theta}(T^{i,<j}),\\tilde{\\bf o}^{i,j}=f_{\\theta}(I^{i},T^{i,<j})\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $T^{i,<j}$ represents all previous tokens before position $j$ in the $i^{t h}$ sample. We further use $\\mathbf{o}_{[t_{j}]}^{i,j}$ or $\\tilde{\\mathbf{o}}_{[t_{j}]}^{i,j}$ to represent the prediction logit in the $i^{t h}$ sample at token $t_{j}$ . As a result, the MLE loss objective for the given $i^{t h}$ sample at token $t_{j}$ is written in the following equation by treating the weight $c$ of each token equally, where $c$ is set to 1: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{L}_{M L E}^{i,t_{j}}=c\\cdot\\log_{\\mathrm{softmax}}(\\tilde{\\mathbf{o}}_{[t_{j}]}^{i,j})\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "2.2 Tokens Differ in Image-text Modality alignment ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we present the necessity of token re-weighting in Section 2.2.1, and show that the re-weighting guidance could be naturally inferred by contrasting image inputs in Section 2.2.2. ", "page_idx": 2}, {"type": "text", "text": "2.2.1 Discrepancy exists in text tokens ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Our proposed method begins with the discrepancy in the training label tokens. For image-text modality alignment, the training labels are usually natural texts, where not all text tokens have a strong correlation with the image inputs. Moreover, due to the existence of model generated datasets, there also exist noisy text tokens that harm the alignment process, which is depicted in Figure 1. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Based on the relevance between corresponding input images, text tokens can be naturally divided into three kinds. (1) Visually correlated tokens, which contain clear visual concepts and movements depicted in the image. (2) Visually irrelevant tokens, which contain either irrelevant to the image inputs or could be easily inferred by previous text tokens. (3) Visually contradictory tokens, which contain hallucinated objects, especially in the model generated datasets. ", "page_idx": 3}, {"type": "text", "text": "2.2.2 Visually correlation can be inferred by contrasting image inputs. ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Inspired by VCD [36] and IBD [33], which both enhance the generation by contrasting image inputs, we further present that the contrastive method can also provide clear guidance for visually correlation on each token. ", "page_idx": 3}, {"type": "text", "text": "We take the prediction logit of each label token under two circumstances, i.e., with or without the image inputs, which we denote as $\\tilde{\\mathbf{o}}_{[t_{j}]}^{i,j}$ and $\\mathbf{o}_{[t_{j}]}^{i,j}$ . Then denote $\\Delta\\mathbf{o}_{[t_{j}]}^{i,j}=\\tilde{\\mathbf{o}}_{[t_{j}]}^{i,j}-\\mathbf{o}_{[t_{j}]}^{i,j}$ as the difference between the predictions logits across two circumstances, we plot \u2206o[it,jj] on each token in Figure 2a to visualize the effect of image conditions. ", "page_idx": 3}, {"type": "text", "text": "From Figure 2a, $\\Delta\\mathbf{o}_{[t_{j}]}^{i,j}$ performs impressively in distinguishing text tokens of three kinds. By $\\Delta\\mathbf{o}_{[t_{j}]}^{i,j}$ the visually correlated tokens the traffic lights tree, busy street, red truck are specially high-lighted while other tokens, especially the visually contradictory tokens a black car are light-colored. ", "page_idx": 3}, {"type": "text", "text": "In summary, the discrepancy in the label tokens motivates us to apply the token-wise dynamics on loss to enhance the image-text modality alignment. By contrasting the image inputs, the difference in the prediction logits \u2206o[it,jj] a ids us with clear guidance for visually correlation of each text token. ", "page_idx": 3}, {"type": "text", "text": "2.3 Contrastive Alignment (CAL) ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we present the details of our proposed CAL. CAL proposed to re-assign the contribution of each token based on their visually correlation weights. The overview of our method is shown in Figure 2b and the detailed algorithm is depicted in Algorithm 1. ", "page_idx": 3}, {"type": "text", "text": "Following the previous section, $C A L$ first dynamically computes the visually correlation weight $\\bar{\\Delta}\\mathbf{o}_{[t_{j}]}^{i,j}$ of each token $t_{j}$ by contrasting the image conditions. To avoid the effects of extreme values, we additionally introduce post-processing methods including clamping and average pooling. We clamp \u2206logit by setting the upper bound to $\\beta$ and the lower bound to $\\alpha(\\stackrel{\\cdot}{\\alpha}>=0)$ . By setting $\\alpha$ to the extreme value 0, $C A L$ neglects the visually irrelevant tokens and visually contradictory tokens. By setting $\\beta$ to the extreme value $+\\infty$ , $C A L$ tolerates the circumstances where some visually correlated tokens occupy most of the importance weights in all label tokens: ", "page_idx": 3}, {"type": "table", "img_path": "NsxthTVpqA/tmp/9cfff6a9aab91a1637c78a8ef1195ac1403671292f8280853d63dc14e1c13475.jpg", "table_caption": [], "table_footnote": [], "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{w}^{i,t_{j}}=c l a m p_{\\alpha,\\beta}(\\Delta\\mathbf{o}_{[t_{j}]}^{i,j})\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "We further introduce average pooling with a window size of $W$ to smooth the visually correlation weights of each token, where we denote as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\tilde{\\mathbf{w}}^{i,t_{j}}=p o o l i n g_{W}(\\mathbf{w}^{i,t_{j}})\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The final loss objective of $C A L$ is the weighted average of the original MLE objective based on $w$ and is defined as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{L}_{C A L}^{i,t_{j}}=-\\frac{1}{\\sum_{k=1}^{l}\\tilde{\\mathbf{w}}^{i,t_{k}}}\\tilde{\\mathbf{w}}^{i,t_{j}}\\cdot\\log_{\\mathrm{softmax}}f_{\\theta}(\\tilde{\\mathbf{o}}_{[t_{j}]}^{i,j})\n$$", "text_format": "latex", "page_idx": 3}, {"type": "table", "img_path": "NsxthTVpqA/tmp/772e48f1df403af2404a5eeca6d196ed7560c9a2737e52ea62fbbee06a790d37.jpg", "table_caption": ["Table 1: Visual Question Answering benchmarks of $C A L$ on leading methods including LLaVA-1.5, LLaVA-NeXT1, and MGM/MGM-HD. Our results are marked with \u25a0. $\\mathrm{VQA}^{\\mathrm{Text}}$ is evaluated without OCR tokens. Abbreviations: OCRB. (OCR-Bench), MMS. (MMStar), MMT. (MMT-Bench). "], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "3 Experiments ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "3.1 Experimental Setup ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Implementation Details In this paper, we verify our proposed $C A L$ on two leading model structures: LLaVA-1.5/LLaVA-NeXT [6, 10] and Mini-Gemini/Mini-Gemini-HD [11]. LLaVA-1.5 uses CLIPpretrained ViT-L as the visual encoder. For resolution scaling, LLaVA-NeXT employs a simple while adaptive image cropping strategy, encodes each image and concatenates them in one single sequence. Mini-Gemini (MGM) further introduces a LAION-pretrained ConvNeXt-L [22, 41] for high-resolution refinement. For MGM/MGM-HD/LLaVA-1.5, we follow the same setting as the original paper as it is public available, where the learning rate for the PT stage is set to $1\\bar{e}^{-3}$ and the IT stage is set to $2\\dot{e}^{-5}$ for both Vicuna-7B and Vicuna-13B. For LLaVA-NeXT, where only the evaluation code is made public, we reproduce LLaVA-NeXT with the same learning rate as MGM, and set the learning rate of ViT to 1/10 of the base learning rate (our reproduction presents on-par performance with the original paper/blog. We present a comparison of our reproduction results with those of the original papers in Appendix A.1). We also set the lower bound $\\alpha$ and upper bound $\\beta$ in Equation (3) to 1 and 5 respectively, and we set $l$ in Equation (4) to 3 for all experiments. We use 16 A100 for experiments, except for 8 GPUs in LLavA-1.5/Gemma-2B and 32 GPUs in MGM-HD-13B. ", "page_idx": 4}, {"type": "text", "text": "Datasets For experiments on LLaVA-NeXT [10], since the detailed composition of training datasets is not publicly available, we use a slightly different training dataset combination, where we include the mixture of $\\mathrm{LLaVA}_{665k}$ [6], VQADoc [26], VQAChart [25] and the ShareGPT4V [16]. For experiments of LLaVA-1.5 [6] and MGM/MGM-HD [11], we use the same dataset combination with original paper. The training datasets include LLaVA-filtered CC3M [42], ALLaVA [14], ShareGPT4V [16], LAION-GPT-4V [43], LIMA [44], OpenAssistant2 [45], VQADoc [26], VQAChart [25], DVQA [46] and AI2D [47]. Finally, we report results on widely-adopted VLM benchmarks, including $\\mathrm{VQA}^{\\mathrm{Text}}$ [48](without providing OCR tokens), VQADoc [26], VQAChart [25], OCR-Bench [49], MMT [28], MMStar [27], SQAI [50], COCO Caption [30], TextCaps [29], and RefCOCOg [32] in our main experiments which observe significant improvement in majority settings, and additional benchmarks MME [24], POPE [37], SEED-I [51], VQAText\\* [48](with OCR tokens given), with comparable performance in Appendix A.2. ", "page_idx": 4}, {"type": "table", "img_path": "NsxthTVpqA/tmp/47a0f3e7df6865b5f1a0cd20a67c91247e696d977b2ae09c2d3847d04e045b10.jpg", "table_caption": ["Table 2: Image captioning and visual grounding benchmarks on LLaVA-1.5, LLaVA-NeXT, and MGM/MGM- $\\mathrm{\\nabla{HD^{2}}}$ . Our results are marked with \u25a0. "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "3.2 Main Results ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "CAL effectively improves various VLMs in Visual Question Answering scenarios Table 1 presents the performance of VLMs on Visual Question Answering. Our $C A L$ consistently improves the performance on most of the understanding benchmarks with impressive margin, on different resolution settings on both MGM/MGM-HD and LLaVA-1.5/LLaVA-NeXT which have different vision model architectures. Especially on the high resolution setting, our $C A L$ presents impressive performance improvement on OCR centric benchmarks. $C A L$ can bring improvement in 7 out of 8 benchmarks on LLaVA-NeXT-7B, MGM-HD-7B/13B, and 6 out of 8 benchmarks on LLaVANeXT-13B. Especially, on LLaVA-NeXT-13B, CAL improves $\\mathrm{VQA}^{\\mathrm{Doc}}$ by 1.7 ANLS, VQAChart by 3.4 relaxed accuracy, and OCR-Bench by 21 points. ", "page_idx": 5}, {"type": "text", "text": "Effectiveness of $C A L$ is consistent on other benchmarks including Image captioning and Grounding More promisingly, we found the improvement of $C A L$ is not limited to Visual Question Answering benchmarks, but even more impressive on image-caption and visual grounding benchmarks. Table 2 presents the performance of VLMs on COCO Caption, TextCaps and both validation and test set of RefCOCOg. Especially, $C A L$ improves LLaVA-NeXT-13B 2.1 CIDEr score on COCO Caption benchmark, 6.2 CIDEr score on TextCaps, 0.6/0.7 IoU on the validation/test set of RefCOCOg. ", "page_idx": 5}, {"type": "text", "text": "3.3 Ablation Studies ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Stages of Conducting CAL within VLM Training $C A L$ can be integrated into both the PreTraining (PT) stage and the Instruction Tuning (IT) stage in existing VLMs. In this section, we investigate which stage benefits the most from $C A L$ in Table 3. Integrating $C A L$ into the IT stage contributes most of the improvement in all listed benchmarks, and $C A L$ in the PT stage further enhances the performance with notable improvement on MMT-Bench and OCR-Bench. ", "page_idx": 5}, {"type": "text", "text": "$C A L$ relieve the effect of noisy labels in the training data $C A L$ can impressively reduce the effect of the contradictory label tokens in training data. To distinctly presents the denoising ability of $C A L$ , we pollute the original training data with a ratio of $10\\,\\%$ , $20\\;\\%$ and $30\\,\\%$ by exchanging the training images and their labels, i.e., we select $(I^{i},T^{i})$ and $(I^{j},T^{j})$ with a probability of $p$ and replace them with $(I^{i},T^{j})$ and $(I^{j},T^{i})$ . We plot the performance difference when the noise rate is varied on four different benchmarks, including COCO caption, $\\mathrm{VQA}^{\\mathrm{Doc}}$ , $\\mathrm{VQA}^{\\mathrm{Text}}$ and OCR-Bench. From Figure 3, the performance of baseline drops significantly when the noise rate is increased, while $C A L$ performs more robust to noise. ", "page_idx": 5}, {"type": "image", "img_path": "NsxthTVpqA/tmp/d18251dc2f14453ebb35a42da09d0dd7ae57f24ebf102a2d13dacb263e8f1457.jpg", "img_caption": ["Figure 3: Accuracy difference when different noise ratios applied. The performance of the baseline is marked with red lines, and $C A L$ is marked with green lines. The dashed line represents the asymptote. "], "img_footnote": [], "page_idx": 6}, {"type": "image", "img_path": "NsxthTVpqA/tmp/ae40ea369352dd8b5e5ecef16632d8097f8463b4b8968e7a6be893322b7e4a20.jpg", "img_caption": ["Figure 4: \u2206o distribution for LLaVA models on 100 random sampled cases. "], "img_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "NsxthTVpqA/tmp/3a1bf73a0264fd39e6e6ed679851b09801ee9c97968c303c6f5b6b7f88ae7b8d.jpg", "table_caption": ["Table 3: Performance difference when $C A L$ is applied at different training stages. "], "table_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "NsxthTVpqA/tmp/883fc96c57c8f2ee46b2fd27c2bbbaa175a24fadf843fba8d8c8a66369c40185.jpg", "table_caption": ["Table 4: Performance difference when applying different weights $[\\alpha,\\beta]$ for clamping. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Hyper-parameters for $[\\alpha,\\beta]$ in clamping We further conduct ablation study on $\\alpha$ and $\\beta$ to study the effect of the hyperparameters in our clamping operation. ", "page_idx": 6}, {"type": "text", "text": "First, we plot the $\\Delta\\mathbf{o}$ distribution on LLaVA series in Figure 4. Tokens whose $\\Delta\\mathbf{o}$ is lower than 5 nearly occupy approximately $90\\%$ of the total label sequences. Therefore, we select 5 as the upper bound $\\beta$ . To prevent the context dependent tokens being totally neglected, we set the lower bound $\\alpha$ to 1. We then extend both lower bound and upper bound to extreme values, i.e., 0 and $+\\infty$ . The results are shown in Table 4. (1) Both setting the lower bound to 0 and setting the upper bound to $+\\infty$ causes performance degradation. (2) Even when setting the lower bound to 0, $C A L$ still achieves better performance compared with the original baseline, indicating that totally neglecting the context dependent/visually contradictory tokens while focusing on the visually dependent ones still brings steady improvement. ", "page_idx": 6}, {"type": "text", "text": "Complementary Ablations We further provide ablations on the image contrasting conditions in Appendix A.3, where we compare $C A L$ when different kinds of augmentation strategy is used. ", "page_idx": 6}, {"type": "text", "text": "3.4 Analysis ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "$C A L$ presents better capability in OCR recognition and capturing details We provide qualitative analysis in Table 5 to analyze the improved caption ability of $C A L$ . In the table, $C A L$ presents much better ability in OCR recognition by accurately distinguishing bewildering OCR cases, e.g., CAL accurately tells the difference between hand-written consciousness and construction, world and would. Such ability contributes to the superior performance of $C A L$ on the TextCaps benchmark. ", "page_idx": 6}, {"type": "text", "text": "Meanwhile, $C A L$ also presents better ability in capturing visually-conditioned details. For instance, compared with baseline, $C A L$ captures the material details cd cover, and the numerical details by telling two men on horses from a man on a horse. The capability of $C A L$ to capture intricate details leads to sustained enhancements in the COCO caption benchmark. $C A L$ empowers the model to identify more accurate elements within images, including objects like a trolley and the number 8, which might otherwise be incorrectly recognized or overlooked. ", "page_idx": 7}, {"type": "text", "text": "Complementary Analysis We first provide statistics for computational overhead in Appendix A.6. And we further provide more qualitative analysis on studying the quality of image-text modality alignment. The attention map scores are visualized in Appendix B.1, and the aligned image features are visualized in Appendix B.2. ", "page_idx": 7}, {"type": "text", "text": "4 Related Work ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Vision Language Models LLMs [1, 52, 2, 53, 4] have made significant strides in Natural Language Processing (NLP) tasks, including text generation and question-answering, paving the way for VLMs that integrate vision ability with LLMs. In the realm of visual language learning, CLIP [17, 54] has set a milestone by employing extensive image-text pair contrastive learning to achieve multimodal alignment. Recently, numerous VLMs [5, 7, 8, 55, 34, 56\u201359, 35, 60, 61] have leveraged the robust capabilities of LLMs for cross-modal understanding and generation tasks. Models like BLIP-2 [5] and MiniGPT-4 [8] have improved cross-modal alignment through comprehensive image-text pair pre-training. LLaVA [7] has further advanced its comprehension of complex prompts via refined instruction fine-tuning. Additionally, recent research [9\u201311] has incorporated higher resolution input images and longer sequences to enhance VLMs\u2019 understanding capabilities. Mini-Gemini (MGM) [11] introduces a LAION-pretrained ConvNeXt-L [22] for high-resolution refinement. ", "page_idx": 7}, {"type": "text", "text": "Image-text Modality Alignment Image-text modality alignment has long been regarded as the core problem in cross-modal understanding and generation tasks. Traditional image-text alignment strategies include both contrastive learning across different modalities and generative learning that train text tokens in an autoregressive manner [17, 62\u201364]. The combination of both techniques is also proven to be effective in the early era of VLMs, where BLIP [5] proposes a multi-stage alignment strategy, with contrastive learning in early alignment and generative learning in the latter stage. However, in recent researches [7, 10], contrastive learning is discarded for being redundant in image-text modality alignment of VLMs, and researchers propose to enhance the cross-modal alignment through dataset scaling and image resolution scaling [53, 9, 65, 10]. Despite being simple in application, the existing generative alignment method simply treats each text token with equal importance, resulting in sub-optimal alignment performance. ", "page_idx": 7}, {"type": "text", "text": "More recently, due to the great success in Reinforcement Learning (RL) in the alignment of LLMs, many recent works [66, 37, 67] have also integrated Reinforcement Learning methods to align existing VLMs with human preference. However, these RL-based methods require high-quality human-labeled pair-wise data and focus more on aligning with human preference rather than modality alignment. ", "page_idx": 7}, {"type": "text", "text": "Training-free Contrastive Decoding Recently, many researchers have proposed to improve the generation quality via contrastive decoding [68\u201370, 36, 33]. Especially, in the field of LLMs, CID [71] utilizes contrastive decoding on paired text inputs for model de-biasing. Such method is also proven to be effective in enhancing the reasoning ability of LLMs in various aspects [72\u201374]. Similar investigations have also been taking in VLMs. Recently, both VCD [36] and IBD [33] propose to enhance the generation of VLMs by contrasting the prediction logits between the original visual input and the perturbed ones. CRG [39] further proposes to improve the grounding ability without training via contrasting differently masked images. However, these training-free methods require additional computation during the decoding stage, making it highly ineffective for application. ", "page_idx": 7}, {"type": "text", "text": "5 Limitation ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Despite the superiority of $C A L$ in various model structures, resolution settings and model scales on various benchmarks, limitations still exist in our proposed method. First of all, there lacks a clear and ", "page_idx": 7}, {"type": "table", "img_path": "NsxthTVpqA/tmp/46cbeb6eec491b07f7f263e046592d32e10e355a20fa209b1e199dd14fdf022d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Short caption cases ", "text_level": 1, "page_idx": 8}, {"type": "image", "img_path": "NsxthTVpqA/tmp/62cfe16ed1a2a03ad1d87792a97cf099bbd17e983a19d599c168c6e8ffa21ad9.jpg", "img_caption": [], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "NsxthTVpqA/tmp/9015b888dd0b33f4b2e43a02b6536d798e42e5c303a86af2b5d6d4e5b0a8cd17.jpg", "img_caption": [], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "uestion: Provide a one-sentence caption for the provided image.   \nseline: A red background with a sun and birds and the words Sibelius Symphonies No 2 & 5.   \nAL : A cd cover for Sibelius Symphonies Nos Provide a one-sentence caption for the provided image.   \nA book is open to a page with a picture of a man on a horse. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "A book is open to a page with a picture of two men on horses. ", "page_idx": 8}, {"type": "image", "img_path": "NsxthTVpqA/tmp/24c5cc3504135e5fccbdd9a45b38c495db37e6d208d0df8a94ae880ba07114ed.jpg", "img_caption": [], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "NsxthTVpqA/tmp/22c49377f7f41e5ef4cf3d12ce0da4a0064bc45def857a1af9f66def66946f81.jpg", "img_caption": [], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "tion: Provide a one-sentence caption for the provided image.   \nline: A silver car is parked in front of a fence and a bus. A silver car is parked behind a fence in front of a trolley. Provide a one-sentence caption for the provided image.   \nA person riding a horse with a cart attached to it.   \nA horse pulling a cart with the number 8 on it. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "quantitative discrepancy between the three kinds of label tokens. More discussion of the importance weights guidance can be further investigated in future works. ", "page_idx": 9}, {"type": "text", "text": "The selection of lower bounds and upper bounds in Equation (3) are empirically decided based on the frequency of the prediction logits, which could be extended to more adaptive settings in further explorations. Nevertheless, the simple while broadly effective nature of $C A L$ indicates the importance of a delicate image-text modality alignment strategy for leading VLM structures. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we investigate the in-completeness of current image-text alignment in leading VLMs by treating all text tokens with equal weights. We present by contrasting input images, the difference in the prediction logits for each token naturally reveals their visual correlation. We therefore propose a token re-weighting strategy that prioritize the training of highly visually correlated tokens. Our proposed strategy, $C A L$ is simple while impressively effective, achieving consistent performance gain across various benchmarks including visual question answering, image-captioning and grounding. ", "page_idx": 9}, {"type": "text", "text": "Our work raises a question about the potential optimal learning strategy of image-text modality alignment. Both the imperfectness of training data and over concentration on visually irrelevant/visually contradictory tokens hinder the performance of current VLMs. We hope the proposed $C A L$ can inspire more investigation on better alignment strategy to enhance the capabilities of existing VLMs. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] OpenAI. Chatgpt. https://openai.com/blog/chatgpt/, 2023. 1, 8 ", "page_idx": 10}, {"type": "text", "text": "[2] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. 8 [3] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022. [4] Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024. 1, 8 [5] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning, pages 19730\u201319742. PMLR, 2023. 1, 2, 8 [6] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following, 2023. 2, 5 [7] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024. 2, 8 [8] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing visionlanguage understanding with advanced large language models. In The Twelfth International Conference on Learning Representations, 2023. 1, 2, 8 [9] Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke Ouyang, Songyang Zhang, Haodong Duan, Wenwei Zhang, Yining Li, et al. Internlm-xcomposer2-4khd: A pioneering large visionlanguage model handling resolutions from 336 pixels to 4k hd. arXiv preprint arXiv:2404.06512, 2024. 1, X [10] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, January 2024. URL https://llava-vl.github.io/ blog/2024-01-30-llava-next/. 2, 5, 8 [11] Yanwei Li, Yuechen Zhang, Chengyao Wang, Zhisheng Zhong, Yixin Chen, Ruihang Chu, Shaoteng Liu, and Jiaya Jia. Mini-gemini: Mining the potential of multi-modality vision language models. arXiv preprint arXiv:2403.18814, 2024. 2, 5, 8 [12] Anwen Hu, Haiyang Xu, Jiabo Ye, Ming Yan, Liang Zhang, Bo Zhang, Chen Li, Ji Zhang, Qin Jin, Fei Huang, et al. mplug-docowl 1.5: Unified structure learning for ocr-free document understanding. arXiv preprint arXiv:2403.12895, 2024. 1 [13] Weiyun Wang, Min Shi, Qingyun Li, Wenhai Wang, Zhenhang Huang, Linjie Xing, Zhe Chen, Hao Li, Xizhou Zhu, Zhiguo Cao, et al. The all-seeing project: Towards panoptic visual recognition and understanding of the open world. In The Twelfth International Conference on Learning Representations,   \n2023. 1, 2 [14] Guiming Hardy Chen, Shunian Chen, Ruifei Zhang, Junying Chen, Xiangbo Wu, Zhiyi Zhang, Zhihong Chen, Jianquan Li, Xiang Wan, and Benyou Wang. Allava: Harnessing gpt4v-synthesized data for a lite vision-language model. arXiv preprint arXiv:2402.11684, 2024. 2, 5 [15] Weiyun Wang, Yiming Ren, Haowen Luo, Tiantong Li, Chenxiang Yan, Zhe Chen, Wenhai Wang, Qingyun Li, Lewei Lu, Xizhou Zhu, et al. The all-seeing project v2: Towards general relation comprehension of the open world. arXiv preprint arXiv:2402.19474, 2024. 2 [16] Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v: Improving large multi-modal models with better captions. arXiv preprint arXiv:2311.12793,   \n2023. 1, 2, 5 [17] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748\u20138763. PMLR,   \n2021. 1, 8   \n[18] Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue Cao. Eva-clip: Improved training techniques for clip at scale. arXiv preprint arXiv:2303.15389, 2023. 1   \n[19] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 11975\u201311986, 2023. 1   \n[20] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00e9 J\u00e9gou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 9650\u20139660, 2021. 1   \n[21] Maxime Oquab, Timoth\u00e9e Darcet, Th\u00e9o Moutakanni, Huy V Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel HAZIZA, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. Transactions on Machine Learning Research, 2023. 1   \n[22] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. A convnet for the 2020s. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 11976\u201311986, 2022. 1, 5, 8   \n[23] Sanghyun Woo, Shoubhik Debnath, Ronghang Hu, Xinlei Chen, Zhuang Liu, In So Kweon, and Saining Xie. Convnext v2: Co-designing and scaling convnets with masked autoencoders. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16133\u201316142, 2023. 1   \n[24] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, Yunsheng Wu, and Rongrong Ji. Mme: A comprehensive evaluation benchmark for multimodal large language models, 2024. 1, 5   \n[25] Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: A benchmark for question answering about charts with visual and logical reasoning. arXiv preprint arXiv:2203.10244, 2022. 2, 5   \n[26] Rub\u00e8n Tito, Dimosthenis Karatzas, and Ernest Valveny. Document collection visual question answering. In Document Analysis and Recognition\u2013ICDAR 2021: 16th International Conference, Lausanne, Switzerland, September 5\u201310, 2021, Proceedings, Part II 16, pages 778\u2013792. Springer, 2021. 2, 5   \n[27] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, et al. Are we on the right way for evaluating large vision-language models? arXiv preprint arXiv:2403.20330, 2024. 2, 5   \n[28] Kaining Ying, Fanqing Meng, Jin Wang, Zhiqian Li, Han Lin, Yue Yang, Hao Zhang, Wenbo Zhang, Yuqi Lin, Shuo Liu, et al. Mmt-bench: A comprehensive multimodal benchmark for evaluating large vision-language models towards multitask agi. arXiv preprint arXiv:2404.16006, 2024. 1, 5   \n[29] Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and Amanpreet Singh. Textcaps: a dataset for image captioning with reading comprehension. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part II 16, pages 742\u2013758. Springer, 2020. 1, 2, 5   \n[30] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco captions: Data collection and evaluation server. arXiv preprint arXiv:1504.00325, 2015. 1, 2, 5   \n[31] Bryan A Plummer, Liwei Wang, Chris M Cervantes, Juan C Caicedo, Julia Hockenmaier, and Svetlana Lazebnik. Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models. In Proceedings of the IEEE international conference on computer vision, pages 2641\u20132649, 2015.   \n[32] Junhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan L Yuille, and Kevin Murphy. Generation and comprehension of unambiguous object descriptions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 11\u201320, 2016. 1, 2, 5   \n[33] Lanyun Zhu, Deyi Ji, Tianrun Chen, Peng Xu, Jieping Ye, and Jun Liu. Ibd: Alleviating hallucinations in large vision-language models via image-biased decoding. arXiv preprint arXiv:2402.18476, 2024. 2, 4, 8   \n[34] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al. mplug-owl: Modularization empowers large language models with multimodality. arXiv preprint arXiv:2304.14178, 2023. 2, 8   \n[35] Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, et al. Openflamingo: An open-source framework for training large autoregressive vision-language models. arXiv preprint arXiv:2308.01390, 2023. 2, 8   \n[36] Sicong Leng, Hang Zhang, Guanzheng Chen, Xin Li, Shijian Lu, Chunyan Miao, and Lidong Bing. Mitigating object hallucinations in large vision-language models through visual contrastive decoding. arXiv preprint arXiv:2311.16922, 2023. 2, 4, 8   \n[37] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in large vision-language models. arXiv preprint arXiv:2305.10355, 2023. 2, 5, 8   \n[38] Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Peng Gao, et al. Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems? arXiv preprint arXiv:2403.14624, 2024. 2   \n[39] David Wan, Jaemin Cho, Elias Stengel-Eskin, and Mohit Bansal. Contrastive region guidance: Improving grounding in vision-language models without training. arXiv preprint arXiv:2403.02325, 2024. 2, 8   \n[40] Ramakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image description evaluation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4566\u20134575, 2015. 2   \n[41] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in Neural Information Processing Systems, 35:25278\u201325294, 2022. 5   \n[42] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2556\u20132565, 2018. 5   \n[43] LAION eV. Laion/gpt4v-dataset $\\begin{array}{r l}\\end{array}.$ datasets at hugging face. URL https://huggingface.co/datasets/ laion/gpt4v-dataset. 5   \n[44] Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. Lima: Less is more for alignment. Advances in Neural Information Processing Systems, 36, 2024. 5   \n[45] Andreas K\u00f6pf, Yannic Kilcher, Dimitri von R\u00fctte, Sotiris Anagnostidis, Zhi Rui Tam, Keith Stevens, Abdullah Barhoum, Duc Nguyen, Oliver Stanley, Rich\u00e1rd Nagyf,i et al. Openassistant conversationsdemocratizing large language model alignment. Advances in Neural Information Processing Systems, 36, 2024. 5   \n[46] Kushal Kafle, Brian Price, Scott Cohen, and Christopher Kanan. Dvqa: Understanding data visualizations via question answering. In CVPR, 2018. 5   \n[47] Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi. A diagram is worth a dozen images. In ECCV, 2016. 5   \n[48] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. Towards vqa models that can read. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 8317\u20138326, 2019. 5   \n[49] Yuliang Liu, Zhang Li, Hongliang Li, Wenwen Yu, Mingxin Huang, Dezhi Peng, Mingyu Liu, Mingrui Chen, Chunyuan Li, Lianwen Jin, et al. On the hidden mystery of ocr in large multimodal models. arXiv preprint arXiv:2305.07895, 2023. 5   \n[50] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. Advances in Neural Information Processing Systems, 35:2507\u20132521, 2022. 5   \n[51] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. Seed-bench: Benchmarking multimodal llms with generative comprehension. arXiv preprint arXiv:2307.16125, 2023. 5   \n[52] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 8   \n[53] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. 8   \n[54] Yunhao Gou, Tom Ko, Hansi Yang, James Kwok, Yu Zhang, and Mingxuan Wang. Leveraging per image-token consistency for vision-language pre-training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19155\u201319164, 2023. 8   \n[55] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei. Kosmos-2: Grounding multimodal large language models to the world. arXiv preprint arXiv:2306.14824, 2023. 8   \n[56] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Fanyi Pu, Jingkang Yang, Chunyuan Li, and Ziwei Liu. Mimic-it: Multi-modal in-context instruction tuning. arXiv preprint arXiv:2306.05425, 2023. 8   \n[57] Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechun Liu, Pengchuan Zhang, Raghuraman Krishnamoorthi, Vikas Chandra, Yunyang Xiong, and Mohamed Elhoseiny. Minigpt-v2: large language model as a unified interface for vision-language multi-task learning. arXiv preprint arXiv:2310.09478, 2023.   \n[58] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale N Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language models with instruction tuning. Advances in Neural Information Processing Systems, 36, 2024.   \n[59] Xi Chen, Xiao Wang, Lucas Beyer, Alexander Kolesnikov, Jialin Wu, Paul Voigtlaender, Basil Mustafa, Sebastian Goodman, Ibrahim Alabdulmohsin, Piotr Padlewski, et al. Pali-3 vision language models: Smaller, faster, stronger. arXiv preprint arXiv:2310.09199, 2023. 8   \n[60] Pan Zhang, Xiaoyi Dong Bin Wang, Yuhang Cao, Chao Xu, Linke Ouyang, Zhiyuan Zhao, Shuangrui Ding, Songyang Zhang, Haodong Duan, Hang Yan, et al. Internlm-xcomposer: A vision-language large model for advanced text-image comprehension and composition. arXiv preprint arXiv:2309.15112, 2023. 8   \n[61] Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke Ouyang, Xilin Wei, Songyang Zhang, Haodong Duan, Maosong Cao, et al. Internlm-xcomposer2: Mastering free-form text-image composition and comprehension in vision-language large model. arXiv preprint arXiv:2401.16420, 2024. 8   \n[62] Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui Wu. Coca: Contrastive captioners are image-text foundation models. arXiv preprint arXiv:2205.01917, 2022. 8   \n[63] Weicheng Kuo, AJ Piergiovanni, Dahun Kim, Benjamin Caine, Wei Li, Abhijit Ogale, Luowei Zhou, Andrew M Dai, Zhifeng Chen, Claire Cui, et al. Mammut: A simple architecture for joint learning for multimodal tasks. Transactions on Machine Learning Research, 2023.   \n[64] Zhenghao Lin, Zhibin Gou, Yeyun Gong, Xiao Liu, Yelong Shen, Ruochen Xu, Chen Lin, Yujiu Yang, Jian Jiao, Nan Duan, et al. Rho-1: Not all tokens are what you need. arXiv preprint arXiv:2404.07965, 2024. 8   \n[65] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond. 2023. 8   \n[66] Tianyu Yu, Yuan Yao, Haoye Zhang, Taiwen He, Yifeng Han, Ganqu Cui, Jinyi Hu, Zhiyuan Liu, Hai-Tao Zheng, Maosong Sun, et al. Rlhf-v: Towards trustworthy mllms via behavior alignment from fine-grained correctional human feedback. arXiv preprint arXiv:2312.00849, 2023. 8   \n[67] Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, LiangYan Gui, Yu-Xiong Wang, Yiming Yang, et al. Aligning large multimodal models with factually augmented rlhf. arXiv preprint arXiv:2309.14525, 2023. 8   \n[68] Xiang Lisa Li, Ari Holtzman, Daniel Fried, Percy Liang, Jason Eisner, Tatsunori Hashimoto, Luke Zettlemoyer, and Mike Lewis. Contrastive decoding: Open-ended text generation as optimization. arXiv preprint arXiv:2210.15097, 2022. 8   \n[69] Timo Schick, Sahana Udupa, and Hinrich Sch\u00fctze. Self-diagnosis and self-debiasing: A proposal for reducing corpus-based bias in nlp. Transactions of the Association for Computational Linguistics, 9: 1408\u20131424, 2021.   \n[70] Mian Zhang, Lifeng Jin, Linfeng Song, Haitao Mi, Wenliang Chen, and Dong Yu. Safeconv: Explaining and correcting conversational unsafe behavior. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 22\u201335, 2023. 8   \n[71] Gal Yona, Or Honovich, Itay Laish, and Roee Aharoni. Surfacing biases in large language models using contrastive input decoding. arXiv preprint arXiv:2305.07378, 2023. 8   \n[72] Sean O\u2019Brien and Mike Lewis. Contrastive decoding improves reasoning in large language models. arXiv preprint arXiv:2309.09117, 2023. 8   \n[73] Yung-Sung Chuang, Yujia Xie, Hongyin Luo, Yoon Kim, James Glass, and Pengcheng He. Dola: Decoding by contrasting layers improves factuality in large language models. arXiv preprint arXiv:2309.03883, 2023.   \n[74] Alisa Liu, Xiaochuang Han, Yizhong Wang, Yulia Tsvetkov, Yejin Choi, and Noah A Smith. Tuning language models by proxy. arXiv preprint arXiv:2401.08565, 2024. 8 ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "Outline ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A. Complementary Experimental Analysis ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "\u2022 A.1. Comparison of our reproduced results with the official results in Table 6.   \n\u2022 A.2 . Results of $C A L$ on other Visual Question Answering benchmarks in Table 7.   \n\u2022 A.3. Ablations of $C A L$ when different contrasting method is used in Table 8 and Table 9.   \n\u2022 A.4. Ablation for w/ or w/o average pooling in Table 10.   \n\u2022 A.5. Ablation for pre-trained model in Table 11.   \n\u2022 A.6. Training time comparision w/ or w/o CAL on LLaVA-NeXT-7B-13B in Table 12. ", "page_idx": 15}, {"type": "text", "text": "B. Complementary Qualitative Analysis ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "\u2022 B.1. Visualization of attention map between image tokens and text tokens in Figure 5.   \n\u2022 B.2. Visualization of image-text modality alignment quality in Figure 6. ", "page_idx": 15}, {"type": "text", "text": "A Complementary Experimental Analysis ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "A.1 Ablations for Comparison with Official Results ", "text_level": 1, "page_idx": 16}, {"type": "table", "img_path": "NsxthTVpqA/tmp/e0f00b19a986484b41b84fc693bf9aeb3941b59a0c47718a539ad40810955872.jpg", "table_caption": ["Table 6: Comparisons between our reproduced results and official results. $^*$ denotes providing OCR tokens for evaluation. Official results are marked with $\\dagger$ . "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "We compare our reproduced results with those reported in the paper or from reliable sources in Table 6. The results for Mini-Gemini are taken from their paper, while the results for LLaVA are sourced from this sheet provided by the LLaVA authors. We can observe that our reproduced results are comparable to the official ones. ", "page_idx": 16}, {"type": "text", "text": "A.2 Results on other Visual Question Answering benchmarks. ", "text_level": 1, "page_idx": 16}, {"type": "table", "img_path": "NsxthTVpqA/tmp/fc7eb10dce3d886d7aaf12d68be7bcce13c9938c3e37f3f1a0a2be8c74c9fd00.jpg", "table_caption": ["Table 7: Results on additional Visual Question Answering benchmarks. $^*$ denotes providing OCR tokens for $\\mathrm{VQA}^{\\mathrm{Text}}$ . Our results are marked with \u25a0. "], "table_footnote": [], "page_idx": 16}, {"type": "table", "img_path": "NsxthTVpqA/tmp/a1e64677c6e8b1200caf188c91cf606ce7304a34d44e59ddaa066f542ad3b85c.jpg", "table_caption": ["Table 8: Ablations for contrasting image conditions on Visual Question Answering benchmarks using LLaVA-NeXT/13B. "], "table_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "NsxthTVpqA/tmp/1c11a90978768f9c67c904f3c6aebabc4b0b79519628450ca93b092e0b22788c.jpg", "table_caption": ["Table 9: Ablations for image contrasting conditions on image captioning and visual grounding benchmarks using LLaVA-NeXT/13B. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "In the primary sections of this manuscript, $C A L$ achieves better performance on benchmarks of OCR centric VQA, image-captioning, visual grounding, and other image-dependent benchmarks. In this section, we extend our analysis by presenting additional results on MME, POPE and SEED-I and $\\mathrm{VQA}^{\\mathrm{Text}}$ (with OCR token given). $C A L$ presents comparable performance on these benchmarks, without a clear distinction on performance in different training settings. With steady improvement on OCR centric or image dependent VQA benchmarks, $C A L$ also presents satisfying quality on the remaining VQA tasks. ", "page_idx": 17}, {"type": "text", "text": "A.3 Ablations for Image Contrasting Conditions ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In this section, we further conduct additional analysis on the image contrasting conditions. Except from masking the whole image token sequence, which is the method we use in our paper, we further study two kinds of contrasting conditions, including random patch masking and Gaussian blurring. For random patch masking, we cut images into $20\\times10$ grids, with 10 pieces in the width and 20 pieces in the height. Therefore each images is cut into 200 pieces. We then randomly select $50\\,\\%$ , $70\\;\\%$ and $90\\;\\%$ of the patches to mask, and contrast them with the original image. For Gaussian blurring, we set the kernel $\\sigma$ to 1 and 10 to simulate the extreme blurring circumstances. By using 1, we adopt light blurring which only brings significant effect on OCR centric images. By using 10, all images will be heavily affected. ", "page_idx": 17}, {"type": "text", "text": "Table 8 presents the results on Visual Question Answering benchmarks of different image contrasting conditions on LLaVA-NeXT-13B, and Table 9 further presents results on image-caption benchmarks and visual grounding benchmarks. From these two tables, either random patch masking or adding Gaussian blurring performs sub-optimal in different benchmarks. By $C A L_{\\mathrm{\\;mask}}$ , except for the significant improvement on the test split of $\\mathbf{RefCOCOg}$ , performance of $C A L$ Gaussian drops significantly, especially on the OCR-centric benchmarks. By $C A L$ Gaussian, although the performance on OCR centric benchmarks (VQADoc, VQAChart, OCR-Bench) is less affected, it performs less effective in nearly all benchmarks than $C A L$ . ", "page_idx": 17}, {"type": "text", "text": "A.4 Ablations for Average Pooling ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We fix the window size (W) to 3 in Equation (4) to smooth the weight distribution. An additional experiment in Table 10, where we removed the average pooling step, shows slightly inferior performance, supporting our chosen. ", "page_idx": 17}, {"type": "table", "img_path": "NsxthTVpqA/tmp/9ca8a5b04b69e8438266b2793fdf47458e40febb11ac917a3842fd6369ccd715.jpg", "table_caption": ["Table 10: Comparison of benchmarks with and without Average Pooling. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "A.5 Ablations for Pre-trained Model ", "text_level": 1, "page_idx": 18}, {"type": "table", "img_path": "NsxthTVpqA/tmp/0144b888bcf46facaa82cc5f89570b3e52d9a45909c03bc640e7f896b5a402ec.jpg", "table_caption": ["Table 11: Comparison of pre-trained models for CAL on LLaVA-Next-13B. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "One assumption of $C A L$ is that after training a simple projector only, the VLM is capable of distinguishing visually correlated tokens. In the first phase of VLM training, it is common practice to freeze the ViT and the LLM, and only train a projector to align their features. In the second phase, we finetune the model using high-quality data to enable it to answer image-related questions. To validate this assumption, we finetune $C A L$ models using two types of pre-trained versions: one with the original pre-trained model (only train the projector) and one with the fully trained baseline model (after the finetuning phase). As shown in Table 11, we compare the performance of these two versions and found no significant differences in performance. ", "page_idx": 18}, {"type": "text", "text": "A.6 Computational Overhead. ", "text_level": 1, "page_idx": 18}, {"type": "table", "img_path": "NsxthTVpqA/tmp/507ce8efaad44c3970b468883db49ccab4c6d8171b982812c7f53322d43745b2.jpg", "table_caption": ["Table 12: Training time comparison with and without $C A L$ in the instruction-tuning stage on LLaVANeXT. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "Each iteration of our proposed method requires forwarding text tokens twice to effectively enhance image-text modality alignment. Table 12 presents the training time for the instruct-tuning stage on LLaVA-NeXT, with each experiment conducted on 16 A100 GPUs. $C A L$ introduces approximately $20\\%$ computational overhead on both LLaVA-NeXT-7B and LLaVA-NeXT-13B. Our implementation currently uses an attention mask to neutralize the effect of the image tokens, meaning that the image tokens are still forwarded twice. This additional burden could be further reduced by completely removing image tokens. ", "page_idx": 18}, {"type": "text", "text": "B Complementary Qualitative Analysis ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "B.1 Attention Map Visualization ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In this section, we visualize the attention maps generated by both the baseline model and our proposed model. These visualizations provide insight into how each model focuses on different parts of the input image. The attention weights are calculated by accumulating the attention score between image tokens and text tokens across all layers. As shown in Figure 5, model w/ $C A L$ provides clearer attention maps with less noisy points in the background area, indicating a more precise focus on relevant regions. ", "page_idx": 19}, {"type": "image", "img_path": "NsxthTVpqA/tmp/c763cf0c014e9c5a8bdb59f7c309c4f1728ff156710930e3a647a5d62c7b2788.jpg", "img_caption": ["Figure 5: Comparison of attention maps with and without $C A L$ on LLaVA-NeXT-13B. The left side of each sub-figure shows LLaVA-NeXT-13B without $C A L$ , while the right side shows LLaVA-NeXT13B with $C A L$ . "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "B.2 Image-text Modality Alignment Visualization ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In this section, we visualize the image-text modality alignment by retrieving the nearest text words to each image patch feature from the LLM vocabulary. We plot the results in Figure 6. CAL brings better image-text modality by accurately retrieving the OCR information from the language vocabulary. E.g., LLaVA1 $.5{-}7\\mathrm{B}+C A L$ correctly retrieves Prices, expert, 0, shadow, which is exactly the OCR information in the original image. We do not select LLaVA-NeXT in this section due to the presence of token overlaps from sub-crops. ", "page_idx": 19}, {"type": "image", "img_path": "NsxthTVpqA/tmp/677c3fdba6c1f1951e730ca7a0150a76388571a027f409686ee345a21dd7646c.jpg", "img_caption": ["Figure 6: Visualization of image-text modality alignment for each image patch. We flitered out some nonsensical patches for better visualization. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: The main claims fully reflect our paper\u2019s contributions and scope. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 20}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Justification: We discuss the limitations of our work in the paper. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 20}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: Our paper does not include theoretical results. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 21}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: We provide the specific parameters used in the experiment and the information needed to reproduce it. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 21}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: We will release our code after the paper is accepted. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 22}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We show all the training and test details in our paper. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 22}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [No] ", "page_idx": 22}, {"type": "text", "text": "Justification: We follow the same guideline in our experiment as those previous works. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 23}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: The research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 23}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: There is no societal impact of our work performed for our paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: Our paper poses no such risks. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 24}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: All the creators or original owners of assets used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected. All datasets we use are from internet open source datasets under CC-BY licenses and are cited properly. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: Our paper does not release new assets. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 25}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: Our paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 25}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: Our paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 25}]