[{"figure_path": "NsxthTVpqA/figures/figures_1_1.jpg", "caption": "Figure 1: Figure 1a is one sample drawn from the ShareGPT4V dataset, which contains text tokens that are even contradictory with the given image. Figure 1b further presents our human evaluation results on the proportion of noisy samples that contain contradictory tokens.", "description": "Figure 1(a) shows an example from the ShareGPT4V dataset illustrating how some tokens in the caption are not visually correlated or even contradict the image content.  Figure 1(b) presents the results of a human evaluation assessing the percentage of samples in ShareGPT4V and LLaVA-Instruct-DetailCap datasets that contain visually contradictory tokens, demonstrating a significant proportion of such noisy samples.", "section": "1 Introduction"}, {"figure_path": "NsxthTVpqA/figures/figures_2_1.jpg", "caption": "Figure 2: Overview of CAL. Figure 2a presents a sample drawn from the ShareGPT4V dataset. We calculate the logit difference w/ or w/o image inputs and plot the heat map on partial text tokens. Figure 2b presents the training procedure of CAL, which re-weights the importance of label tokens based on the contrasting logits.", "description": "This figure illustrates the Contrastive Alignment (CAL) method.  Figure 2a shows a heatmap visualizing the difference in prediction logits for each text token with and without image input.  Tokens with higher logit differences (stronger visual correlation) are highlighted. Figure 2b depicts the CAL training process, which uses these logit differences to re-weight the importance of each text token, prioritizing visually correlated tokens during training.", "section": "2 Contrastive Alignment"}, {"figure_path": "NsxthTVpqA/figures/figures_6_1.jpg", "caption": "Figure 3: Accuracy difference when different noise ratios applied. The performance of the baseline is marked with red lines, and CAL is marked with green lines. The dashed line represents the asymptote.", "description": "This figure shows the impact of noisy labels in the training dataset on the performance of the baseline model and the proposed CAL method. The x-axis represents the noise rate (percentage of noisy labels), while the y-axis represents the accuracy on four different benchmarks (COCO Caption, VQA Doc, VQA Text, and OCR-Bench). As the noise rate increases, the accuracy of the baseline model decreases significantly, while the accuracy of CAL model decreases at a much slower rate, demonstrating its robustness to noisy labels.", "section": "3.3 Ablation Studies"}, {"figure_path": "NsxthTVpqA/figures/figures_6_2.jpg", "caption": "Figure 4: \u0394o distribution for LLaVA models on 100 random sampled cases.", "description": "The figure shows the distribution of the difference in prediction logits (\u0394o) for various text tokens in four different LLaVA models (LLaVA-1.5-7B, LLaVA-1.5-13B, LLaVA-NeXT-7B, and LLaVA-NeXT-13B).  The x-axis represents the \u0394o value, and the y-axis shows the frequency.  The vertical dashed line indicates the threshold where \u0394o is less than or equal to 5, showing the proportion of tokens with low \u0394o values.  This distribution visualization is used to support the argument that contrasting image inputs helps distinguish between visually correlated, irrelevant, and contradictory text tokens.", "section": "2.2 Tokens Differ in Image-text Modality alignment"}, {"figure_path": "NsxthTVpqA/figures/figures_8_1.jpg", "caption": "Figure 1: Figure 1a is one sample drawn from the ShareGPT4V dataset, which contains text tokens that are even contradictory with the given image. Figure 1b further presents our human evaluation results on the proportion of noisy samples that contain contradictory tokens.", "description": "The figure shows an example from the ShareGPT4V dataset illustrating how some text tokens are contradictory to the image content (Figure 1a).  It also presents a human evaluation demonstrating that approximately half of the samples in ShareGPT4V and LLaVA-Instruct contain visually contradictory tokens (Figure 1b), highlighting the problem of existing image-text alignment strategies.", "section": "1 Introduction"}, {"figure_path": "NsxthTVpqA/figures/figures_8_2.jpg", "caption": "Figure 1: Figure 1a is one sample drawn from the ShareGPT4V dataset, which contains text tokens that are even contradictory with the given image. Figure 1b further presents our human evaluation results on the proportion of noisy samples that contain contradictory tokens.", "description": "This figure shows an example from the ShareGPT4V dataset illustrating how some tokens in the caption are not visually correlated with the image, and even contradict it.  It also presents results from a human evaluation, showing the proportion of samples with visually contradictory tokens in ShareGPT4V and LLaVA-Instruct datasets.", "section": "1 Introduction"}, {"figure_path": "NsxthTVpqA/figures/figures_8_3.jpg", "caption": "Figure 1: Figure 1a is one sample drawn from the ShareGPT4V dataset, which contains text tokens that are even contradictory with the given image. Figure 1b further presents our human evaluation results on the proportion of noisy samples that contain contradictory tokens.", "description": "The figure shows an example from the ShareGPT4V dataset illustrating how some text tokens are not visually correlated with the image, and even contradict it.  It also includes the results of a human evaluation showing a significant percentage of samples containing contradictory tokens.", "section": "1 Introduction"}, {"figure_path": "NsxthTVpqA/figures/figures_8_4.jpg", "caption": "Figure 1: Figure 1a is one sample drawn from the ShareGPT4V dataset, which contains text tokens that are even contradictory with the given image. Figure 1b further presents our human evaluation results on the proportion of noisy samples that contain contradictory tokens.", "description": "The figure shows an example from the ShareGPT4V dataset illustrating how some tokens in the caption are not visually correlated with the image (Figure 1a).  It also presents a bar chart summarizing the results of human evaluation, which demonstrates that a significant portion of samples in both ShareGPT4V and LLaVA-Instruct-DetailCap datasets contain visually contradictory tokens (Figure 1b). This highlights the issue of existing image-text alignment strategies in VLMs that treat all text tokens equally, leading to sub-optimal cross-modal alignment.", "section": "1 Introduction"}, {"figure_path": "NsxthTVpqA/figures/figures_19_1.jpg", "caption": "Figure 5: Comparison of attention maps with and without CAL on LLaVA-NeXT-13B. The left side of each sub-figure shows LLaVA-NeXT-13B without CAL, while the right side shows LLaVA-NeXT-13B with CAL.", "description": "The figure demonstrates the attention maps generated by the baseline model and the proposed model with contrastive alignment (CAL). The attention weights are calculated by accumulating the attention score between image tokens and text tokens across all layers. The figure shows that the model with CAL produces clearer attention maps with less noisy points in the background, which indicates that CAL helps the model focus on the relevant regions of the image.", "section": "B.1 Attention Map Visualization"}, {"figure_path": "NsxthTVpqA/figures/figures_19_2.jpg", "caption": "Figure 6: Visualization of image-text modality alignment for each image patch. We filtered out some nonsensical patches for better visualization.", "description": "The figure visualizes the image-text modality alignment by finding the nearest text words to each image patch feature from the LLM vocabulary.  The top shows a sample image patch with identified text. The bottom shows the results for baseline and CAL methods, comparing the nearest text words (from the LLM vocabulary) found for each image patch. CAL shows improved alignment by more accurately identifying relevant OCR information from the language vocabulary.", "section": "B.2 Image-text Modality Alignment Visualization"}]