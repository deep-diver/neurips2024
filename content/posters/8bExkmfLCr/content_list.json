[{"type": "text", "text": "Block Coordinate Descent Methods for Optimization under J-Orthogonality Constraints with Applications ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 The J-orthogonal matrix, also referred to as the hyperbolic orthogonal matrix, is   \n2 a class of special orthogonal matrix in hyperbolic space, notable for its advanta  \n3 geous properties. These matrices are integral to optimization under J-orthogonal   \n4 constraints, which have widespread applications in statistical learning and data   \n5 science. However, addressing these problems is generally challenging due to   \n6 their non-convex nature and the computational intensity of the constraints. Cur  \n7 rently, algorithms for tackling these challenges are limited. This paper introduces   \n8 JOBCD, a novel Block Coordinate Descent method designed to address opti  \n9 mizations with J-orthogonality constraints. We explore two specific variants of   \n10 JOBCD: one based on a Gauss-Seidel strategy (GS-JOBCD), the other on a   \n11 variance-reduced and Jacobi strategy (VR-J-JOBCD). Notably, leveraging the   \n12 parallel framework of a Jacobi strategy, VR-J-JOBCD integrates variance reduc  \n13 tion techniques to decrease oracle complexity in the minimization of finite-sum   \n14 functions. For both GS-JOBCD and VR-J-JOBCD, we establish the oracle com  \n15 plexity under mild conditions and strong limit-point convergence results under the   \n6 Kurdyka-Lojasiewicz inequality. To demonstrate the effectiveness of our method,   \n17 we conduct experiments on hyperbolic eigenvalue problems, hyperbolic structural   \n18 probe problems, and the ultrahyperbolic knowledge graph embedding problem.   \n19 Extensive experiments using both real-world and synthetic data demonstrate that   \n20 JOBCD consistently outperforms state-of-the-art solutions, by large margins. ", "page_idx": 0}, {"type": "text", "text": "21 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "22 A matrix $\\mathbf{X}\\in\\mathbb{R}^{n\\times n}$ is a J-orthogonal matrix if $\\mathbf{X}^{\\mathsf{T}}\\mathbf{J}\\mathbf{X}=\\mathbf{J}$ , where $\\mathbf{J}=\\bigl[\\begin{array}{c c}{\\mathbf{I}_{p}}&{\\mathbf{0}}\\\\ {\\mathbf{0}}&{\\!\\!-\\mathbf{I}_{n-p}}\\end{array}\\bigr]$ 0p \u2212I0  ], and Ip is a p\u00d7p   \n23 identity matrix. Here, $\\mathbf{J}\\in\\mathbb{R}^{n\\times n}$ is the signature matrix with signature $(p,n-p)$ . In this paper, we   \n24 mainly focus on the following optimization problem under J-orthogonality constraints: ", "page_idx": 0}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\operatorname*{min}_{\\mathbf{X}\\in\\mathbb{R}^{n\\times n}}f(\\mathbf{X})\\triangleq\\frac{1}{N}\\sum_{i=1}^{N}f_{i}(\\mathbf{X}),\\mathrm{~s.t.~}\\mathbf{X}^{\\mathsf{T}}\\mathbf{J}\\mathbf{X}=\\mathbf{J}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "25 Here, $f(\\mathbf{X})$ could have a finite-sum structure, each component function $f_{i}(\\mathbf{X})$ is assumed to be   \n26 differentiable, and $N$ is the number of data points. For brevity, the $\\mathbf{J}.$ -orthogonality constraint   \n27 $\\mathbf{X}^{\\mathsf{T}}\\mathbf{J}\\mathbf{X}=\\mathbf{J}$ in Problem (1) is rewritten as $\\mathbf{X}\\in{\\mathcal{I}}$ .   \n28 We impose the following assumptions on Problem (1) throughout this paper. (A-i) For any matrices   \n29 $\\mathbf{X}$ and ${{\\bf X}}^{+}$ , we assume $f_{i}:\\mathbb{R}^{n\\times n}\\mapsto\\mathbb{R}$ is continuously differentiable for some symmetric positive   \n30 semidefinite matrix $\\mathbf{H}\\in\\mathbb{R}^{n n\\times n n}$ that: ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "equation", "text": "$$\n\\begin{array}{r}{f_{i}(\\mathbf{X}^{+})\\leq f_{i}(\\mathbf{X})+\\langle\\mathbf{X}^{+}-\\mathbf{X},\\nabla f_{i}(\\mathbf{X})\\rangle+\\frac{1}{2}\\|\\mathbf{X}^{+}-\\mathbf{X}\\|_{\\mathbf{H}}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "31 for all $i\\,\\in\\,[N]$ , where $\\|\\mathbf{H}\\|\\leq L_{f}$ for some constant $L_{f}\\,>\\,0$ and $\\|\\mathbf{X}\\|_{\\mathbf{H}}^{2}\\triangleq\\operatorname{vec}(\\mathbf{X})^{\\top}\\mathbf{H}\\operatorname{vec}(\\mathbf{X})$ .   \n32 This further implies that: $\\|\\nabla f_{i}(\\mathbf{\\bar{X}})-\\nabla f_{i}(\\mathbf{X}^{+})\\|_{\\mathsf{F}}\\,\\le\\,\\bar{L}_{f}\\|\\mathbf{X}-\\mathbf{X}^{+}\\|_{\\mathsf{F}}$ for all $i~\\in~[N]$ . Impor  \n33 tantly, the function $\\begin{array}{r}{f(\\mathbf{X})=\\frac{1}{2}\\operatorname{tr}(\\mathbf{X}^{\\top}\\mathbf{C}\\mathbf{X}\\mathbf{D})=\\frac{1}{2}\\|\\mathbf{X}\\|_{\\mathbf{H}}^{2}}\\end{array}$ with $\\mathbf{H}=\\mathbf{D}\\otimes\\mathbf{C}$ satisfies the equality   \n34 $\\forall\\mathbf{X}$ , ${{\\bf X}}^{+}$ , $f(\\mathbf{X}^{+})=\\mathcal{Q}(\\mathbf{X}^{+};\\mathbf{X})$ in (2), where $\\mathbf{C}\\in\\mathbb{R}^{n\\times n}$ and $\\mathbf{D}\\in\\mathbb{R}^{n\\times n}$ are arbitrary symmetric   \n35 matrices. (A-ii) The function ${f}_{i}(\\mathbf{X})$ is coercive for all $i\\in N$ , that is, $\\operatorname*{lim}_{\\|\\mathbf{x}\\|_{\\mathsf{F}}\\to\\infty}f_{i}(\\mathbf{X})=\\infty$ , $\\forall i$ .   \n36 Problem (1) defines an optimization framework that is fundamental to a wide range of models in   \n37 statistical learning and data science, including hyperbolic eigenvalue problem [6, 43, 40], hyperbolic   \n38 structural probe problem [20, 7], and ultrahyperbolic knowledge graph embedding [48]. Additionally,   \n39 it is closely related to machine learning in hyperbolic spaces, including Lorentz model learning   \n40 [35, 50, 8] and ultrahyperbolic neural networks [27, 54, 42]. It also intersects with hyperbolic linear   \n41 algebra [3, 21], addressing problems such as the indefinite least squares problem, hyperbolic QR   \n42 factorization, and indefinite polar decomposition. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "43 1.1 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "44 \u25b6Block Coordinate Descent Methods. Block Coordinate Descent (BCD) is a well-established   \n45 iterative algorithm that sequentially minimizes along block coordinate directions. Its simplicity   \n46 and efficiency have led to its widespread adoption in structured convex applications [37]. Recently,   \n47 BCD has gained traction in non-convex problems due to its robust optimality guarantees and/or   \n48 excellent empirical performance in areas including optimal transport [22], matrix optimization [12],   \n49 fractional minimization [52], deep neural networks [5, 53, 32], federated learning[47], black-box   \n50 optimization [4], and optimization with orthogonality constraints [51, 14]. To our knowledge, this is   \n51 the first application of BCD methods to optimization under J-orthoginality constraints, with a focus   \n52 on analyzing their theoretical guarantees and empirical efficacy.   \n53 \u25b6Minimizing Smooth Functions under J-Orthogonality Constraints. The J-orthogonal matrix   \n54 belongs to a subset of generalized orthogonal matrices [16, 36, 23]. However, projecting onto the   \n55 J-orthogonality constraint poses challenges, complicating the extension of conventional optimization   \n56 algorithms to address optimization problems under these constraints [1, 16]. This contrasts with   \n57 computing orthogonal projections using methods such as polar or SVD decomposition, or approxi  \n58 mating them via QR factorization. Existing methods for addressing Problem (1) can be categorized   \n59 into three classes. (i) CS-Decomposition Based Methods. These approaches involve parameterizing   \n60 four orthogonal matrices (as described in Proposition 2.2) and subsequently minimizing a smooth   \n61 function over these matrices in an alternating fashion. The involvement of $3\\times3$ block matrices makes   \n62 the implementation of these methods very challenging. Consequently, the work of [48] focuses on   \n63 optimizing a reduced subspace of the CS decomposition parameters, albeit at the expense of losing   \n64 some degrees of freedom. (ii) Unconstrained Multiplier Correction Methods [31, 13, 14]. These   \n65 methods leverage the symmetry and explicit closed-form expression of the Lagrangian multiplier at   \n66 the first-order optimality condition. Consequently, they address an unconstrained problem, resulting   \n67 in efficient first-order infeasible approaches. (iii) Alternating Direction Method of Multipliers [19].   \n68 This method reformulates the original problem into a bilinear constrained optimization problem by   \n69 introducing auxiliary variables. It employs dual variables to handle bilinear constraints, iteratively   \n70 optimizing primal variables while keeping other primal and dual variables fixed, and using a gradient   \n71 ascent strategy to update the dual variables. This approach has become widely adopted for solving   \n72 general nonconvex and nonsmooth composite optimization problems. Notably, all the aforementioned   \n73 methods solely identify critical points of Problem (1).   \n74 \u25b6Finite-Sum Problems via Stochastic Gradient Descent. The finite-sum structure is prevalent in   \n75 machine learning and statistical modeling, facilitating decomposition into smaller, more manageable   \n76 components. This property is advantageous for developing efficient algorithms for large-scale prob  \n77 lems, such as Stochastic Gradient Descent (SGD). Reducing variance is crucial in SGD because it can   \n78 lead to more stable and faster convergence. Various techniques, such as mini-batch SGD, momentum   \n79 methods, and variance reduction methods like SAGA [10], SVRG [25], SARAH [34], SPIDER   \n80 [11, 44], SNVRG [55], and PAGE [30], have been developed to address this issue. Additionally, SGD   \n81 for minimizing composite functions has also been investigated by the authors [15, 24, 29]. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "82 1.2 Contributions ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "83 This paper makes the following contributions. (i) Algorithmically: We introduce the JOBCD   \n84 algorithm, a novel Block Coordinate Descent method specifically designed to tackle optimizations   \n85 constrained by J-orthogonality. We explore two specific variants of JOBCD, one based on a   \n86 Gauss-Seidel strategy (GS-JOBCD), the other on a variance-reduced and Jacobi strategy (VR  \n87 J-JOBCD). Notably, VR-J-JOBCD incorporates a variance-reduction technique into a parallel   \n88 framework to reduce oracle complexity in the minimization of finite-sum functions (See Section   \n89 2). (ii) Theoretically: We provide comprehensive optimality and convergence analyses for both   \n90 algorithms (see Sections 3 and 4). (iii) Empirically: Extensive experiments across hyperbolic   \n91 eigenvalue problems, structural probe problems, and ultrahyperbolic knowledge graph embedding,   \n92 using both real-world and synthetic data, consistently show the significant superiority of JOBCD   \n93 over state-of-the-art solutions (see Section 5). ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "94 2 The Proposed JOBCD Algorithm ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "95 This section proposes JOBCD for solving optimization problems under J-orthogonality constraints   \n96 in Problem (1), which is based on randomized block coordinate descent. Two variants of JOBCD are   \n97 explored, one based on a Gauss-Seidel strategy (GS-JOBCD), the other on a variance-reduced and   \n98 Jocobi strategy (VR-J-JOBCD).   \n99 Notations. We define $[n]\\triangleq\\{1,2,\\ldots,n\\}$ . We denote $\\Omega\\triangleq\\{\\mathcal{B}_{1},\\mathcal{B}_{2},\\ldots,\\mathcal{B}_{\\mathrm{C}_{n}^{2}}\\}$ as all the possible   \n100 combinations of the index vectors choosing 2 items from $n$ without repetition. For any $\\mathbb{B}\\in\\Omega$ , we   \n101 define $\\mathbf{U_{B}}\\in\\mathbb{R}^{n\\times2}$ as $(\\mathbf{U}_{\\mathtt{B}})_{j i}\\,=\\,1$ if $\\mathsf{B}_{i}\\,=\\,j$ , else 0 for all $j$ and $i$ , leading to $\\mathbf{U}_{\\mathtt{B}}^{\\mathsf{T}}\\mathbf{X}\\overset{\\cdot}{=}\\mathbf{X}(\\mathtt{B},\\colon)\\in$   \n102 $\\mathbb{R}^{2\\times n}$ . We denote $\\mathcal{J}_{\\mathtt{B}}\\triangleq\\{\\mathbf{V}\\,|\\,\\mathbf{V}^{\\mathsf{T}}\\mathbf{J}_{\\mathtt{B B}}\\mathbf{V}=\\mathbf{J}_{\\mathtt{B B}}\\}$ , where $\\mathbf{J}_{\\mathtt{B B}}\\in\\mathbb{R}^{2\\times2}$ is the sub-matrix of $\\mathbf{J}$ indexed by   \n103 B. Further notations are provided in Appendix A.1. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "104 2.1 Gauss-Seidel Block Coordinate Descent Algorithm ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "105 This subsection describes the proposed GS-JOBCD algorithm. We consider Problem (1) with $N=1$   \n106 only, without utilizing its finite-sum structure.   \n107 GS-JOBCD is an iterative algorithm that, in each iteration $t$ , randomly and uniformly (with replace  \n108 ment) selects a coordinate B from the set $\\Omega$ and then solves a small-sized subproblem. The row index   \n109 $[n]$ of the decision variable $\\mathbf{X}$ are separated to two sets $\\mathtt{B}$ and $\\mathtt{B}^{c}$ , where $\\mathtt{B}\\in\\Omega$ with $|\\mathtt{B}|=2$ is the work  \n110 ing set and $\\mathtt{B}^{c}=[n]\\backslash\\mathtt{B}$ . For simplicity, we use B instead of $\\mathtt{B}^{t}$ . Following [51], we consider the follow  \n111 ing block coordinate update rule: $\\begin{array}{r}{[{\\mathbf{X}}^{t+1}({\\mathbf{B}},:)={\\mathbf{V}}{\\mathbf{X}}^{t}({\\mathbf{B}},:)]\\Leftrightarrow[{\\mathbf{X}}^{t+1}={\\mathbf{X}}^{t}+{\\mathbf{U}}_{\\mathrm{B}}({\\mathbf{V}}-{\\mathbf{I}}){\\mathbf{U}}_{\\mathrm{B}}^{\\top}{\\mathbf{X}}^{t}]}\\end{array}$ ,   \n112 where $\\mathbf{V}\\in\\mathbb{R}^{2\\times2}$ is some suitable matrix.   \n113 The following lemma illustrates matrix selection for enforcing J-orthogonality constraints via the   \n114 update rule $\\mathbf{X}^{+}\\Leftarrow\\boldsymbol{\\mathcal{X}}_{\\mathsf{B}}(\\mathbf{V})\\triangleq\\mathbf{X}+\\mathbf{U}_{\\mathsf{B}}(\\mathbf{V}-\\mathbf{I})\\mathbf{U}_{\\mathsf{B}}^{\\mathsf{T}}\\mathbf{X}$ , and presents associated properties.   \n115 Lemma 2.1. (Proof in Section $C.I$ ) For any $\\mathsf{B}\\in\\Omega$ , we define ${{\\bf X}}^{+}$ \u225c $:\\mathcal{X}_{\\mathrm{B}}(\\mathbf{V})\\triangleq\\mathbf{X}+\\mathbf{U}_{\\mathrm{B}}(\\mathbf{V}-\\mathbf{I})\\mathbf{U}_{\\mathrm{B}}^{\\top}\\mathbf{X}.$ .   \n116 We have: (a) If $\\mathbf{V}\\in\\mathcal{T}_{\\mathtt{B}}$ and $\\mathbf{X}\\in{\\mathcal{I}}$ , then $\\mathbf{X}^{+}\\in\\mathcal{I}$ . (b) $\\|\\mathbf{X}^{+}-\\mathbf{X}\\|_{\\mathsf{F}}^{2}\\,\\leq\\,\\|\\mathbf{X}\\|_{\\mathsf{F}}^{2}\\cdot\\|\\mathbf{V}-\\mathbf{I}\\|_{\\mathsf{F}}^{2}$ . (c)   \n117 $\\lVert\\mathbf{X}^{+}-\\mathbf{X}\\rVert_{\\mathbf{H}}^{2}\\leq\\lVert\\mathbf{V}-\\mathbf{I}\\rVert_{\\mathbf{Q}}^{2}$ for all $\\mathbf{Q}\\succcurlyeq\\mathbf{Q}\\triangleq(\\mathbf{Z}^{\\top}\\otimes\\mathbf{U}_{\\mathrm{B}})^{\\top}\\mathbf{H}(\\mathbf{Z}^{\\top}\\otimes\\mathbf{U}_{\\mathrm{B}})$ , $\\mathbf{Z}\\triangleq\\mathbf{U}_{\\mathtt{B}}^{\\top}\\mathbf{X}\\in\\mathbb{R}^{k\\times n}$ .   \n118 $\\blacktriangleright$ The Main Algorithm. Using the above update rule, we consider the following iterative procedure:   \n119 $\\mathbf{X}^{t+1}\\Leftarrow\\mathcal{X}_{\\mathrm{B}}^{t}(\\bar{\\mathbf{V}}^{\\tilde{t}})$ , where $\\bar{\\mathbf{V}}^{t}\\in\\bar{\\mathbf{\\Omega}}$ arg minV $\\bar{f}(\\mathcal{X}_{\\mathrm{B}}^{t}(\\mathbf{V}))$ . However, the resulting subproblem could be   \n120 still difficult to solve. This inspires us to use sequential majorization minimization [38, 33] to address   \n121 it. This technique iteratively constructs a surrogate function that upper-bounds the objective function,   \n122 allowing for effective optimization and gradual reduction of the objective function. We derive: ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{f(\\mathcal{X}_{\\mathtt{B}}^{t}(\\mathbf{V}))}&{\\overset{\\triangledown}{\\leq}\\begin{array}{l}{f(\\mathbf{X}^{t})+\\frac{1}{2}\\|\\mathcal{X}_{\\mathtt{B}}^{t}(\\mathbf{V})-\\mathbf{X}^{t}\\|_{\\mathbf{H}}^{2}+\\langle\\mathcal{X}_{\\mathtt{B}}^{t}(\\mathbf{V})-\\mathbf{X}^{t},\\nabla f(\\mathbf{X}^{t})\\rangle}\\\\ &{\\overset{\\triangledown}{\\leq}\\begin{array}{l}{f(\\mathbf{X}^{t})+\\frac{1}{2}\\|\\mathbf{V}-\\mathbf{I}\\|_{\\mathbf{Q}+\\theta\\mathbf{I}}^{2}+\\langle\\mathbf{V}-\\mathbf{I},[\\nabla f(\\mathbf{X}^{t})(\\mathbf{X}^{t})^{\\top}]_{\\mathtt{B}\\mathtt{B}}\\rangle\\triangleq\\mathcal{G}(\\mathbf{V};\\mathbf{X}^{t},\\mathtt{B}^{t}),}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "123 where step $\\textcircled{1}$ uses Inequality (2); step $\\circledcirc$ uses Claim $(c)$ of Lemma 2.1, $\\theta\\ge0$ and the fact that   \n124 $\\langle\\mathbf{U}_{\\mathrm{B}}(\\mathbf{V}-\\mathbf{\\dot{I}})\\mathbf{U}_{\\mathrm{B}}^{\\mathsf{T}}\\mathbf{X},\\nabla f(\\mathbf{\\dot{X}})\\rangle=\\langle\\mathbf{V}-\\mathbf{\\dot{I}},[\\nabla f(\\mathbf{X})\\mathbf{X}^{\\mathsf{T}}]_{\\mathrm{BB}}\\rangle$ , and the choice of $\\mathbf{Q}\\in\\mathbb{R}^{4\\times4}$ that: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{Q}=\\underline{{\\mathbf{Q}}},\\mathrm{~or~}\\mathbf{Q}=\\varsigma\\mathbf{I}_{2},\\mathrm{~with~}\\|\\underline{{\\mathbf{Q}}}\\|\\leq\\varsigma\\leq L_{f}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "125 Therefore, the function $\\mathcal{G}(\\mathbf{V};\\mathbf{X}^{t},\\mathbf{B}^{t})$ becomes a majorization function of $f(\\mathbf{X})$ at $\\mathbf{X}^{t}\\in\\mathcal{I}$ for all $\\mathtt{B}^{t}\\in$   \n126 $\\Omega$ . We can consider the following optimization problem to find $\\overline{{\\mathrm{V}}}^{t}$ : $\\overline{{\\mathrm{V}}}^{t}\\in$ arg minV $\\mathcal{G}(\\mathbf{V};\\mathbf{X}^{t},\\mathbf{B}^{t})$ . ", "page_idx": 2}, {"type": "text", "text": "127 We summarize the proposed GS-JOBCD in Algorithm 1. ", "page_idx": 2}, {"type": "text", "text": "128 Although the J-orthogonality constraint typically has a sorted diagonal with $\\mathrm{diag}(\\mathbf{J})\\in\\{-1,+1\\}^{n}$ ,   \n129 GS-JOBCD is also applicable to problems with more general constraints $\\mathbf{X}^{\\mathsf{T}}\\mathbf{J}\\mathbf{X}\\,=\\,\\mathbf{J}$ where   \n130 $\\mathrm{diag}(\\mathbf{J})\\in\\{\\pm1\\}^{n}$ is unsorted.   \n131 $\\blacktriangleright$ Solving the Small-Sized Subproblem. We now elaborate on how to find the global op  \n132 timal solution of Problem (6). We notice that $\\mathbf{V}\\ \\in\\ {\\mathcal{T}}_{\\mathtt{B}}\\ \\triangleq\\ \\{\\mathbf{\\{V}\\,\\lvert\\,}\\mathbf{V}^{\\mathsf{T}}\\mathbf{J}_{\\mathtt{B B}}\\mathbf{V}\\ =\\ \\mathbf{J}_{\\mathtt{B B}}\\}$ , where   \n133 $\\mathbf{J}_{\\mathtt{B B}}\\in\\,\\big\\{\\big(\\,\\mathtt{l}\\,_{0}^{1}\\,\\lrcorner\\,\\big),\\,\\big(\\,\\mathtt{l}\\,_{0}^{1}\\,\\lrcorner\\,\\mathtt{l}\\,,\\,\\mathtt{m}\\,\\in\\,\\mathtt{N}\\big)\\big\\}$ . We now concentrate on the first case where $\\mathbf{J}_{\\mathtt{B B}}=\\left(\\O_{0}^{1}\\ \\O_{-1}^{0}\\right)$ . The   \n134 following proposition provides a strategy to decompose any J-orthogonal matrix. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Algorithm 1: GS-JOBCD: Block Coordinate Descent Methods using a Gauss-Seidel Strategy for Solving Problem (1) ", "page_idx": 3}, {"type": "text", "text": "Init.: Set ${\\bf X}^{0}$ to satisfy J-orthogonality constraints (e.g., via Hyperbolic CS Decomposition).   \nfor $t$ from 0 to $T$ do (S1) Choose a coordinate $\\mathtt{B}^{t}$ with $|\\mathtt{B}^{t}|=2$ from the set $\\Omega$ randomly and uniformly (with replacement) for the $t$ -th iteration. Denote $\\mathtt{B}=\\mathtt{B}^{t}$ . (S2) Choose a matrix $\\mathbf{Q}\\in\\mathbb{R}^{4\\times4}$ using Formula (4). (S3) Solve the following small-size subproblem globally. $\\begin{array}{r l r}{\\overline{{\\mathbf{V}}}^{t}}&{\\in}&{\\arg\\underset{\\mathbf{V}\\in\\mathcal{T}_{\\mathbb{B}}}{\\operatorname*{min}}\\ \\frac{1}{2}\\|\\mathbf{V}-\\mathbf{I}\\|_{\\mathbf{Q}+\\theta\\mathbf{I}}^{2}+\\langle\\mathbf{V}-\\mathbf{I},[\\nabla f(\\mathbf{X}^{t})(\\mathbf{X}^{t})^{\\top}]_{\\mathbb{B}\\mathbb{B}}\\rangle+f(\\mathbf{X}^{t})}\\\\ &{=}&{\\arg\\underset{\\mathbf{V}\\in\\mathcal{T}_{\\mathbb{B}}\\in\\mathbb{R}^{2\\times2}}{\\operatorname*{min}}\\ \\frac{1}{2}\\|\\mathbf{V}\\|_{\\dot{\\mathbf{Q}}}^{2}+\\langle\\mathbf{V},\\mathbf{P}\\rangle+c}\\end{array}$ (5) (6 where $\\mathbf{P}\\triangleq[\\nabla f(\\mathbf{X}^{t})(\\mathbf{X}^{t})^{\\top}]_{\\mathbb{B}}-\\operatorname*{mat}(\\dot{\\mathbf{Q}}\\operatorname{vec}(\\mathbf{I}_{2}))$ , $\\dot{\\mathbf{Q}}=\\mathbf{Q}+\\theta\\mathbf{I}$ and $\\begin{array}{r}{c\\triangleq f(\\mathbf{X}^{t})-\\langle\\mathbf{I}_{2},[\\nabla f(\\mathbf{X}^{t})(\\mathbf{X}^{t})^{\\mathsf{T}}]_{\\mathbb{B}\\mathbb{B}}\\rangle+\\frac{1}{2}\\|\\mathbf{I}\\|_{\\dot{\\mathbf{Q}}}^{2}}\\end{array}$ is a constant. (S4) $\\mathbf{X}^{t+1}({\\mathsf{B}},:)=\\overline{{\\mathbf{V}}}^{t}\\mathbf{X}^{t}({\\mathsf{B}},:)$ ", "page_idx": 3}, {"type": "text", "text": "end ", "page_idx": 3}, {"type": "text", "text": "135 Proposition 2.2. (Hyperbolic CS Decomposition [41]) Let $\\mathbf{V}$ be $\\textbf{J}$ -orthogonal with signature   \n136 $(p,n\\mathrm{~-~}p)$ . Assume that $n\\,-\\,p\\ \\leq\\ p$ . Then there exist vectors $\\dot{c},\\dot{s}\\;\\in\\;\\mathbf{\\dot{R}}^{n-p}$ with $\\dot{c}\\odot\\dot{c}-$   \n137 $\\dot{s}\\odot\\dot{s}=1$ , and orthogonal matrices $\\mathbf{U}_{1},\\mathbf{V}_{1}\\;\\in\\;\\mathbb{R}^{p\\times p}$ and $\\mathbf{U}_{2},\\mathbf{V}_{2}\\;\\in\\;\\mathbb{R}^{(n-p)\\times(n-p)}$ such that:   \n138 $\\mathbf{V}=\\bigl[\\begin{array}{c c}{\\mathbf{U}_{1}}&{\\mathbf{\\Sigma}_{0}}\\\\ {\\mathbf{\\Sigma}_{0}}&{\\mathbf{U}_{2}}\\end{array}\\bigr]\\bigl[\\begin{array}{c c c}{\\mathrm{Diag}(\\boldsymbol{\\dot{c}})}&{0}&{\\mathrm{Diag}(\\boldsymbol{\\dot{s}})}\\\\ {0}&{I_{p-(n-p)}}&{0}\\end{array}\\bigr]\\bigl[\\begin{array}{c c}{\\mathbf{V}_{1}^{\\top}}&{\\mathbf{\\Sigma}_{0}}\\\\ {\\mathbf{\\Sigma}_{0}}&{\\mathbf{V}_{2}^{\\top}}\\end{array}\\bigr].$   \n114309 $\\tilde{\\mathrm{c}},\\tilde{\\mathrm{s}}\\in\\mathbb{R}$ ,g  wPer opparoasimtieotnri z2e. $\\mathbf{V}$ iatsh: $n=2$ $\\mathbf{V}=\\big(\\begin{array}{c c}{\\pm1}&{0}\\\\ {0}&{\\pm1}\\end{array}\\big)\\cdot\\big(\\begin{array}{c c}{\\tilde{\\mathbf{c}}}&{\\tilde{\\mathbf{s}}}\\\\ {\\tilde{\\mathbf{s}}}&{\\tilde{\\mathbf{c}}}\\end{array}\\big)\\cdot\\big(\\begin{array}{c c}{\\pm1}&{0}\\\\ {0}&{\\pm1}\\end{array}\\big)$ $p=1$ $\\mathbf{U}_{1}=\\mathbf{U}_{2}=\\mathbf{V}_{1}=\\mathbf{V}_{2}=\\pm1,\\,\\tilde{\\mathrm{c}}^{2}-\\tilde{\\mathrm{s}}^{2}=1$ $\\sinh(\\mu)$ , wc\u02dc itahs   \n141 $\\cosh(\\mu)$ , and $\\widetilde{\\mathfrak{t}}$ as $\\operatorname{tanh}(\\mu)$ for some $\\mu\\in\\mathbb{R}$ , for simplicity of notation. It is not difficult to show that   \n142 Problem (6) reduces to the following one-dimensional search problem: ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\bar{\\mu}\\in\\operatorname*{min}_{\\mu}\\frac{1}{2}\\operatorname{vec}(\\mathbf{V})^{\\top}\\dot{\\mathbf{Q}}\\operatorname{vec}(\\mathbf{V})+\\langle\\mathbf{V},\\mathbf{P}\\rangle,\\,\\mathrm{s.t.}\\,\\mathbf{V}\\in\\{\\big(\\array}{\\frac{\\bar{\\kappa}}{\\hat{\\mathbf{s}}}}\\big),\\big(\\textstyle\\frac{\\bar{\\kappa}}{-\\hat{\\mathbf{s}}}}&{-\\frac{\\bar{s}}{\\hat{\\mathbf{c}}}\\big),\\big(\\textstyle\\frac{-\\bar{\\kappa}}{\\hat{\\mathbf{s}}}}&{\\bar{\\kappa}}\\big),\\big(\\textstyle\\frac{\\bar{\\kappa}}{\\hat{\\mathbf{s}}}}&{-\\frac{\\bar{s}}{\\hat{\\mathbf{c}}}\\big)\\}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "143 We apply a breakpoint search method to solve Problem (7). For simplicity, we provide an analysis   \n144 only for the first case. A detailed discussion of all four cases can be found in Appendix Section B.1.   \n145 For the case where $\\mathbf{V}=\\left(\\frac{\\widetilde{\\mathbf{c}}}{\\widetilde{\\mathbf{s}}}\\,\\frac{\\widetilde{\\mathbf{s}}}{\\widetilde{\\mathbf{c}}}\\right)$ , Problem (7) reduces to the following problem: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\tilde{\\mathfrak{c}},\\tilde{\\mathfrak{s}}}a\\,\\tilde{\\mathfrak{c}}+b\\,\\tilde{\\mathfrak{s}}+c\\,\\tilde{\\mathfrak{c}}^{2}+d\\,\\tilde{\\mathfrak{c}}\\,\\tilde{\\mathfrak{s}}+e\\,\\tilde{\\mathfrak{s}}^{2},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "146 where $a=\\mathbf{P}_{11}+\\mathbf{P}_{22}$ , $b=\\mathbf{P}_{12}+\\mathbf{P}_{21}$ , $c=\\textstyle{\\frac{1}{2}}(\\dot{\\bf Q}_{11}+\\dot{\\bf Q}_{41}+\\dot{\\bf Q}_{14}+\\dot{\\bf Q}_{44})$ , $d={\\textstyle\\frac{1}{2}}(\\dot{\\bf Q}_{21}+\\dot{\\bf Q}_{31}+$   \n147 $\\dot{\\bf Q}_{12}+\\dot{\\bf Q}_{42}+\\dot{\\bf Q}_{13}+\\dot{\\bf Q}_{43}+\\dot{\\bf Q}_{24}+\\dot{\\bf Q}_{34})$ , and $e={\\textstyle{\\frac{1}{2}}}(\\dot{\\bf Q}_{22}+\\dot{\\bf Q}_{32}+\\dot{\\bf Q}_{23}+\\dot{\\bf Q}_{33})$ . Then we perform   \n148 a substitution to convert Problem (8) into an equivalent problem that depends on the trigonometric   \n149 functions: $\\begin{array}{r}{(\\pmb{i})\\,\\tilde{\\mathrm{c}}^{2}=\\frac{1}{1-\\tilde{\\mathrm{t}}^{2}};(\\pmb{i}\\pmb{i})\\,\\tilde{\\mathrm{s}}^{2}=\\frac{\\tilde{\\mathrm{t}}^{2}}{1-\\tilde{\\mathrm{t}}^{2}};(\\pmb{i}\\pmb{i})\\,\\tilde{\\mathrm{t}}=\\frac{\\tilde{\\mathrm{s}}}{\\tilde{\\mathrm{c}}}}\\end{array}$ = 1t\u02dc\u22122t\u02dc2 ; (iii) \u02dct = \u02dcc\u02dcs. The following lemma provides a characterization   \n150 of the global optimal solution for Problem (8).   \n151 Lemma 2.3. (Proof in Section $C.2$ ) We let $\\check{F}(\\tilde{c},\\tilde{s})$ $\\hat{\\equiv}~a\\tilde{c}+b\\tilde{s}+c\\tilde{c}^{2}+d\\tilde{c}\\tilde{s}+e\\tilde{s}^{2}$ . The optimal so  \n152 lution $\\bar{\\mu}$ to Problem (8) can be computed as: $[\\cosh(\\tilde{\\mu}),\\sinh(\\tilde{\\mu})]\\in\\arg\\operatorname*{min}_{[c,s]}\\,\\breve{F}(c,s)$ , s. t. $[c,s]\\in$   \n153 $\\begin{array}{r}{\\{[\\frac{1}{\\sqrt{1-(\\bar{t}_{+})^{2}}},\\frac{\\bar{t}_{+}}{\\sqrt{1-(\\bar{t}_{+})^{2}}}],[\\frac{-1}{\\sqrt{1-(\\bar{t}_{-})^{2}}},\\frac{-\\bar{t}_{-}}{\\sqrt{1-(\\bar{t}_{-})^{2}}}]\\},}\\end{array}$ , where $\\begin{array}{r}{\\bar{t}_{+}\\ \\in\\ \\arg\\operatorname*{min}_{t}\\ p(t)\\ \\triangleq\\ \\frac{a+b t}{\\sqrt{1-t^{2}}}\\,+\\,\\frac{w+d t}{1-t^{2}},}\\end{array}$ ;   \n154 $\\begin{array}{r}{\\bar{t}_{-}\\in\\arg\\operatorname*{min}_{t}\\tilde{p}(t)\\triangleq\\frac{-a-b t}{\\sqrt{1-t^{2}}}+\\frac{w+d t}{1-t^{2}}}\\end{array}$ . Here $w=c+e$ .   \n155 We now describe how to find the optimal solution $\\bar{t}_{+}$ , where $\\begin{array}{r}{\\bar{t}_{+}\\;\\in\\;\\arg\\operatorname*{min}_{t}\\;p(t)\\;\\triangleq\\;\\frac{a+b t}{\\sqrt{1-t^{2}}}\\;+}\\end{array}$   \n156 $\\scriptstyle{\\frac{w+d t}{1-t^{2}}}$ ; this strategy can naturally be extended to find $\\bar{t}_{-}$ . Initially, we have the \u221afollowing   \n157 first-order optimality conditions for the problem: $0=\\nabla p(t)=[b(1-t^{2})+(a+b t)t]\\sqrt{1-t^{2}}\\ +$   \n158 $\\begin{array}{r}{[d(1-t^{2})+(w+d t)(2t)]\\Leftrightarrow d t^{2}+\\underline{{{2}}}w t+d=-[b+a t]\\sqrt{1-t^{2}}}\\end{array}$ . Squaring both sides yields the   \n159 following quartic equation: $c_{4}t^{4}+c_{3}t^{3}+c_{2}t^{2}+c_{1}t+c_{0}=0.$ , where $c_{4}\\dot{=}\\:d^{2}\\,\\bar{+}\\,a^{2}$ , $c_{3}=4w d+2a b$ ,   \n160 $c_{2}=4w^{2}+2d^{2}-a^{2}+b^{2}$ , $c_{1}=4w d-2a b$ , $c_{0}=d^{2}-b^{2}$ . This equation can be solved analytically   \n161 by Lodovico Ferrari\u2019s method [46], resulting in all its real roots $\\{\\bar{t_{1}},\\bar{t}_{2},\\ldots,\\bar{t_{j}}\\}$ with $1\\le j\\le4$ .   \n162 For the second and third cases, Problem (6) essentially boils down to optimization under orthogonality   \n163 constraints. The work of [51] derives a breakpoint search method for finding the optimal solution for   \n164 Problem (6) with $\\mathbf{J}_{\\mathtt{B B}}\\in\\dot{\\{\\big(\\,0\\,1\\,\\big)},\\,\\big(\\begin{array}{c c}{-1}&{0}\\\\ {0}&{-1}\\end{array}\\big)\\big\\}}$ using the Givens rotation and Jacobi reflection matrices. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "165 2.2 Variance-Reduced Jacobi Block Coordinate Descent Algorithm ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "166 This subsection proposes the VR-J-JOBCD algorithm, a randomized block coordinate descent   \n167 method derived from GS-JOBCD. Importantly, by leveraging the parallel framework of a Jacobi   \n168 strategy [17, 9], VR-J-JOBCD integrates variance reduction techniques [39, 30, 18] to decrease   \n169 oracle complexity in the minimization of finite-sum functions. This makes the algorithm effective for   \n170 minimizing large-scale problems under J-orthogonality constraints.   \n171 Notations. We assume $n$ is an even number in this paper. We create $(n/2)$ pairs by non-overlapping   \n172 grouping of the numbers in any arbitrary combination, with each pair containing two distinct numbers   \n173 from the set $[n]$ . It is not hard to verify that such grouping yields $\\mathrm{C}_{J}=(n!)/(2^{n/2}\\frac{n}{2}!)$ possible   \n174 combinations. The set of these combinations is denoted as $\\Upsilon\\triangleq\\{\\tilde{\\mathcal{B}}_{i}\\}_{i=1}^{\\mathrm{C}_{J}}\\triangleq\\{\\tilde{\\mathcal{B}}_{1},\\tilde{\\mathcal{B}}_{2},\\dotsc,\\tilde{\\mathcal{B}}_{\\mathrm{C}_{J}}\\}:$ 1.   \n175 $\\blacktriangleright$ Variance Reduction Strategy. We incorporate state-of-the-art variance reduction strategies from   \n176 the literature [30, 5] into our algorithm to solve Problem (1). These methods iteratively generate a   \n177 stochastic gradient estimator as follows: ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\tilde{\\mathbf{G}}^{t}=\\left\\{\\begin{array}{l l}{\\frac{1}{b}\\sum_{i\\in\\mathtt{S}_{+}^{t}}\\nabla f_{i}(\\mathbf{X}^{t}),}&{\\mathrm{~with~probability~}\\,p;}\\\\ {\\tilde{\\mathbf{G}}^{t-1}+\\frac{1}{b^{\\prime}}\\sum_{i\\in\\mathtt{S}_{+}^{t}}(\\nabla f_{i}(\\mathbf{X}^{t})-\\nabla f_{i}(\\mathbf{X}^{t-1})),}&{\\mathrm{~with~probability~}\\,1-p.}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "178 Here, $\\{\\mathbf{S}_{+}^{t},\\mathbf{S}_{*}^{t}\\}$ are uniform random minibatch samples with $|\\mathtt{S}_{+}^{t}|~=~b$ , $|\\mathbf{S}_{*}^{t}|\\;=\\;b^{\\prime}$ , and $\\tilde{\\bf G}^{0}\\,=$   \n179 $\\begin{array}{r}{\\frac{1}{b}\\sum_{i\\in\\mathbf{S}_{+}^{0}}\\nabla f_{i}(\\mathbf{X}^{0})}\\end{array}$ . We drop the superscript $t$ for $\\{\\mathbf{S}_{+}^{t},\\mathbf{S}_{*}^{t}\\}$ as $t$ can be inferred from context. We   \n180 only focus on the default setting that [30, 5]: $b=N$ , $b^{\\prime}={\\sqrt{b}}$ and $\\begin{array}{r}{p=\\frac{b^{\\prime}}{b+b^{\\prime}}}\\end{array}$   \n181 \u25b6Jacobi Block Coordinate Descent Method. The proposed algorithm is built upon the parallel   \n182 framework of a Jacobi strategy. In each iteration $t$ , we randomly and uniformly (with replacement)   \n183 select a coordinate set $\\mathsf{B}^{t}\\triangleq\\{\\mathsf{B}_{(1)}^{t},\\mathsf{B}_{(2)}^{t},\\cdots,\\mathsf{B}_{(\\frac{n}{2})}^{t}\\}$ from the set $\\Upsilon$ with $\\mathtt{B}^{t}\\in\\mathbb{N}^{\\frac{n}{2}\\times2}$ and $\\mathtt{B}_{(i)}^{t}\\in\\mathbb{N}^{2}$ .   \n184 For all $t$ , we have: $\\mathsf{B}_{(i)}^{t}\\cap\\mathsf{B}_{(j)}^{t}\\,=\\,\\emptyset$ and $\\mathsf{U}_{i=1}^{n/2}(\\mathsf{B}_{(i)}^{t})\\,=\\,[n]$ . We drop the superscript $t$ if $t$ can be   \n185 inferred from context.   \n186 The following lemma shows how to choose a suitable matrix $\\mathbf{Q}$ so that the Jacobi strategy can be   \n187 applied.   \n188 Lemma 2.4. (Proof in Section C.3) We let $\\mathsf{B}^{t}\\triangleq\\{\\mathsf{B}_{(1)}^{t},\\mathsf{B}_{(2)}^{t},\\cdots,\\mathsf{B}_{(\\frac{n}{2})}^{t}\\}\\in\\Upsilon$ for all $t$ . We let $\\mathbf{Q}={\\varsigma}\\mathbf{I}_{4}$ ,   \n189 where $\\varsigma$ is some suitable constant with $\\varsigma\\le L_{f}$ . For any $\\mathsf{B}_{(i)}^{t}$ and $\\mathsf{B}_{(j)}^{t}$ with $i\\neq j$ , their corresponding   \n190 objective functions as in Equation (3) are independent.   \n191 We consider the following block coordinate update rule in VR-J-JOBCD: $\\mathbf{X}^{t+1}\\,\\Leftarrow\\,\\tilde{\\mathcal{X}}_{\\mathtt{B}}^{t}(\\mathbf{V}_{:})\\,\\triangleq$   \n192 $\\begin{array}{r}{\\mathbf{X}^{t}+[\\sum_{i=1}^{n/2}\\mathbf{U}_{\\mathrm{B}_{(i)}}(\\mathbf{V}_{i}-\\mathbf{I}_{2})\\mathbf{U}_{\\mathrm{B}_{(i)}}^{\\top}]\\mathbf{X}^{t}}\\end{array}$ . The following lemma provides properties of this rule.   \n193 Lemma 2.5. (Proof in Section $C.4$ ) We let $\\mathrm{~{~\\sf~B~}~}\\in\\mathrm{~\\sf~T~}$ , $\\begin{array}{r}{\\mathbf{V}_{i}\\;\\;\\in\\;\\;{\\mathcal{I}}_{\\mathtt{B}_{(i)}}}\\end{array}$ , $\\mathbf{X}\\ \\in\\ {\\mathcal{I}}_{}$ , and $i\\in$   \n194 $\\left[{\\frac{n}{2}}\\right]$ . We define $\\begin{array}{r l r}{{\\mathbf{X}}^{+}}&{\\triangleq}&{{\\tilde{\\mathcal{X}}}_{\\mathtt{B}}({\\mathbf{V}}_{:})\\;\\;\\triangleq\\;\\;{\\mathbf{X}}\\;+\\;[\\sum_{i=1}^{n/2}{\\mathbf{U}}_{\\mathtt{B}_{(i)}}({\\mathbf{V}}_{i}\\;-\\;{\\mathbf{I}}_{2}){\\mathbf{U}}_{\\mathtt{B}_{(i)}}^{\\top}]{\\mathbf{X}}}\\end{array}$ . We have: (a)   \n195 $\\begin{array}{r}{\\sum_{i=1}^{\\frac{n}{2}}\\|\\mathbf{U}_{\\mathbf{B}_{(i)}}(\\mathbf{V}_{i}-\\mathbf{I}_{2})\\mathbf{U}_{\\mathbf{B}_{(i)}}^{\\top}\\mathbf{X}\\|_{\\mathsf{F}}^{2}=\\|\\sum_{i=1}^{\\frac{n}{2}}\\mathbf{U}_{\\mathbf{B}_{(i)}}(\\mathbf{V}_{i}-\\mathbf{I}_{2})\\mathbf{U}_{\\mathbf{B}_{(i)}}^{\\top}\\mathbf{X}\\|_{\\mathsf{F}}^{2}}\\end{array}$ . (b) $\\lVert\\mathbf{X}^{+}-\\mathbf{X}\\rVert_{\\mathsf{F}}^{2}\\leq\\lVert\\mathbf{X}\\rVert_{\\mathsf{F}}^{2}\\,.$   \n196 $\\begin{array}{r}{\\sum_{i=1}^{n/2}\\|\\mathbf{V}_{i}-\\mathbf{I}_{2}\\|_{\\mathsf{F}}^{2}}\\end{array}$ . (c) $\\begin{array}{r}{\\|\\mathbf{X}^{+}-\\mathbf{X}\\|_{\\mathbf{H}}^{2}\\leq\\sum_{i=1}^{n/2}\\|\\mathbf{V}_{i}-\\mathbf{I}_{2}\\|_{\\mathbf{Q}}^{2}}\\end{array}$ with $\\mathbf{Q}={\\varsigma}\\mathbf{I}_{4}$ . (d) For all $\\tilde{\\mathbf{G}}\\in\\mathbb{R}^{n\\times n}$ , it   \n197 follows that: $\\begin{array}{r}{2\\sum_{i=1}^{n/2}\\langle\\mathbf{V}_{i}-\\mathbf{I}_{2},[(\\nabla f(\\mathbf{X})-\\tilde{\\mathbf{G}})\\mathbf{X}^{\\top}]_{\\mathbf{B}(i)}\\mathbf{B}_{(i)}\\rangle\\leq\\|\\mathbf{X}\\|_{\\mathsf{F}}^{2}\\sum_{i=1}^{n/2}\\|\\mathbf{V}_{i}-\\mathbf{I}_{2}\\|_{\\mathsf{F}}^{2}+\\|[\\nabla f(\\mathbf{X})-\\mathbf{X}^{\\top}]\\|_{\\mathsf{F}}^{2}\\|\\mathbf{X}\\|_{\\mathsf{X}^{2}}.}\\end{array}$   \n198 $\\tilde{\\mathbf{G}}]\\Vert_{\\mathtt{F}}^{2}$ .   \n199 $\\blacktriangleright$ The Main Algorithm. Using the update rule above, we consider the following iterative procedure:   \n200 $\\mathbf{X}^{t+1}\\Leftarrow\\tilde{\\mathcal{X}}_{\\mathtt{B}}^{t}(\\mathbf{V}_{:})$ , where $\\bar{\\mathbf{V}}_{:}^{t}\\in\\arg\\operatorname*{min}_{\\mathbf{V}}$ : $f(\\tilde{\\mathcal{X}}_{\\mathtt{B}}^{t}(\\mathbf{V}_{:}))$ . We establish the majorization function for ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "201 $f(\\tilde{\\mathcal{X}}_{\\mathtt{B}}^{t}(\\mathbf{V}_{:}))$ , as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{f(\\tilde{\\mathcal{X}}_{\\mathtt{B}}^{t}(\\mathbf{V}_{\\cdot}))}&{\\overset{\\mathtt{()}}{\\leq}\\;\\;\\;f(\\mathbf{X}^{t})+\\langle\\tilde{\\mathcal{X}}_{\\mathtt{B}}^{t}(\\mathbf{V}_{\\cdot})-\\mathbf{X}^{t},\\nabla f(\\mathbf{X}^{t})\\rangle+\\frac{1}{2}\\|\\tilde{\\mathcal{X}}_{\\mathtt{B}}^{t}(\\mathbf{V}_{\\cdot})-\\mathbf{X}^{t}\\|_{\\mathbf{H}}^{2}}\\\\ &{\\overset{\\mathtt{()}}{\\leq}\\;\\;\\;f(\\mathbf{X}^{t})+\\sum_{i=1}^{n/2}\\{\\langle\\mathbf{V}_{i}-\\mathbf{I}_{2},[\\nabla f(\\mathbf{X})(\\mathbf{X})^{\\top}]_{\\mathtt{B}_{(i)}\\mathtt{B}_{(i)}}\\rangle+\\frac{1}{2}\\|\\mathbf{V}_{i}-\\mathbf{I}_{2}\\|_{(\\theta+\\varsigma)\\mathbf{I}}^{2}\\}\\,0}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "202 where step $\\textcircled{1}$ uses the results of telescoping Inequality (2) over $i$ from 1 to $N$ ; step $\\circledcirc$ uses $\\mathbf{X}^{t+1}-$   \n203 $\\begin{array}{r}{\\mathbf{X}^{t}=[\\sum_{i=1}^{n/2}\\mathbf{U}_{\\mathbb{B}_{(i)}}(\\mathbf{V}_{i}-\\mathbf{I}_{2})\\mathbf{U}_{\\mathbb{B}_{(i)}}^{\\top}]\\mathbf{X}^{t}}\\end{array}$ , Claim $(c)$ of Lemma 2.5, $\\theta\\ge0$ , and $\\mathbf{Q}=\\varsigma\\mathbf{I}$ .   \n204 Instead of computing the exact Euclidean gradient $\\nabla f(\\mathbf{X}^{t})$ as GS-JOBCD, VR-J-JOBCD maintains   \n205 and updates a recursive gradient estimator $\\tilde{\\mathbf{G}}^{t}$ using a variance-reduced strategy as in Formula (9).   \n206 We consider minimizing the following function instead of the one on the right-hand side of Inequality   \n207 (10): ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{T}(\\mathbf{V}_{:};\\mathbf{X}^{t},\\mathbf{B}^{t})\\triangleq f(\\mathbf{X}^{t})+\\sum_{i=1}^{n/2}\\langle\\mathbf{V}_{i}-\\mathbf{I}_{2},[\\tilde{\\mathbf{G}}^{t}(\\mathbf{X}^{t})^{\\top}]_{\\mathbb{B}_{(i)}\\mathbb{B}_{(i)}}\\rangle+\\frac{1}{2}\\|\\mathbf{V}_{i}-\\mathbf{I}_{2}\\|_{\\dot{\\mathbf{Q}}}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "208 Here, $\\mathcal{T}(\\mathbf{V}_{:};\\mathbf{X}^{t},\\mathbf{B}^{t})$ can be termed as a stochastic majorization function of $f(\\tilde{\\mathcal{X}}_{\\mathtt{B}}^{t}(\\mathbf{V}_{:}))$ at the current   \n209 solution $\\mathbf{X}^{t}$ . Therefore, we can consider the following optimization problem to find $\\{\\mathbf{V}_{:}\\}$ using:   \n210 $\\bar{\\mathbf{V}}_{:}^{t}\\in\\mathrm{arg}\\,\\mathrm{min}\\mathbf{v}$ : $\\mathcal{T}(\\mathbf{V}_{:};\\mathbf{X}^{t},\\mathbf{B}^{t})$ , which can be decomposed into $(n/2)$ independent subproblems and   \n211 solved in parallel. It is important to note that each $\\mathbf{V}_{i}$ in Problem (12) is identical to Problem (6),   \n212 which can be efficiently solved in $\\mathcal{O}(1)$ using the breakpoint search method, as in GS-JOBCD.   \n213 We summarize the proposed VR-J-JOBCD in Algorithm 2. Notably, when $N=1$ , VR-J-JOBCD   \n214 simplifies to a direct Jacobi strategy for solving Problem (1), which we refer to as J-JOBCD. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Algorithm 2: VR-J-JOBCD: Block Coordinate Descent Methods using a variance-reduced and   \nJacobi strategy for Solving Problem 1   \nInit.: Set ${\\bf X}^{0}$ to satisfy J-orthogonality constraints (e.g., via Hyperbolic CS Decomposition).   \nfor $t$ from 0 to $T$ do (S1) Choose a coordinate $\\mathtt{B}^{t}$ from the set $\\Upsilon$ randomly and uniformly (with replacement) for the $t$ -th iteration. Denote $\\mathtt{B}=\\mathtt{B}^{t}$ . In our implementation, we simply randomly permute the set $\\{1,2,...,n\\}$ and then output the grouping $;\\{[1,2],[3,4],[5,6],\\cdots,,[n-1,n]\\}$ . (S2) Use a variance-reduced strategy (9) to obtain $\\tilde{\\mathbf{G}}^{t}$ . (S3) Solve small-sized subproblems in parallel with $\\mathbf{Q}=\\varsigma\\mathbf{I}\\in\\mathbb{R}^{4\\times4}$ . for $i=1$ to $\\underline{{n}}/2$ in parallel do $\\begin{array}{r l}{\\overline{{\\mathbf{V}}}_{i}^{t}\\in\\mathrm{~arg}_{\\mathbf{V}_{i}\\in\\mathcal{I}_{\\mathbb{R}_{(i)}}}\\!\\!\\!\\!\\!\\!}&{\\frac{1}{2}\\|\\mathbf{V}_{i}-\\mathbf{I}\\|_{\\dot{\\mathbf{Q}}}^{2}+\\langle\\mathbf{V}_{i}-\\mathbf{I},[\\nabla f(\\mathbf{X}^{t})(\\mathbf{X}^{t})^{\\mathsf{T}}]_{\\mathbf{B}_{(i)}\\mathbf{B}_{(i)}}\\rangle+f(\\mathbf{X}^{t})}\\\\ {=}&{\\arg_{\\mathbf{V}_{i}\\in\\mathcal{I}_{\\mathbb{B}_{(i)}}}\\frac{1}{2}\\|\\mathbf{V}_{i}\\|_{\\dot{\\mathbf{Q}}}^{2}+\\langle\\mathbf{V}_{i},\\mathbf{P}_{i}\\rangle}\\end{array}$ (12) where $\\mathbf{P}_{i}\\triangleq[\\nabla f(\\mathbf{X}^{t})(\\mathbf{X}^{t})^{\\top}]_{\\mathbf{B}_{(i)}\\mathbf{B}_{(i)}}-\\mathrm{mat}(\\ddot{\\mathbf{Q}}\\operatorname{vec}(\\mathbf{I}_{2}))-\\theta\\mathbf{I}_{2},\\ddot{\\mathbf{Q}}=$ $\\ddot{\\mathbf{Q}}=(\\zeta+\\theta)\\mathbf{I}$ . (S4) Update the solution $\\mathbf{X}^{t+1}$ in parallel as follows: for $i=1$ to $\\underline{{n}}/2$ in parallel do L $\\overline{{\\mathbf{X}^{t+1}(\\mathbf{B}_{(i)},:)=\\mathbf{\\bar{V}}_{i}^{t}\\mathbf{X}^{t}(\\mathbf{B}_{(i)},:)}}$   \nend ", "page_idx": 5}, {"type": "text", "text": "215 3 Optimality Analysis ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "216 This section provides an optimality analysis for the proposed algorithms. ", "page_idx": 5}, {"type": "text", "text": "217 Initially, we define the first-order optimality condition for Problem (1). Since the matrix $\\mathbf{X}^{\\mathsf{T}}\\mathbf{J}\\mathbf{X}$   \n218 is symmetric, the Lagrangian multiplier $\\Lambda$ corresponding to the constraints $\\mathbf{X}^{\\mathsf{T}}\\mathbf{J}\\mathbf{X}=\\mathbf{J}$ is also a   \n219 symmetric matrix. The Lagrangian function of problem (1) is $\\begin{array}{r}{\\mathcal{L}(\\mathbf{X},\\boldsymbol{\\Lambda})=f(\\mathbf{X})-\\frac{1}{2}\\langle\\boldsymbol{\\Lambda},\\mathbf{X}^{\\mathsf{T}}\\mathbf{J}\\mathbf{X}-\\mathbf{J}\\rangle}\\end{array}$ . ", "page_idx": 5}, {"type": "text", "text": "220 We obtain the following lemma for the first-order optimality condition for Problem (1). ", "page_idx": 5}, {"type": "text", "text": "221 Lemma 3.1. (Proof in Section $D.I$ , First-Order Optimality Condition) We let ${\\mathcal{I}}\\triangleq\\{\\mathbf{X}\\,|\\,\\mathbf{X}^{\\mathsf{T}}\\mathbf{J}\\mathbf{X}=$   \n222 ${\\bf J}\\}$ . We have ${\\bf\\Pi}({\\pmb a})$ A solution $\\check{\\textbf{X}}\\in\\mathbf{\\mathcal{I}}$ is a critical point of problem (1) if and only if: ${\\textbf{0}}=$   \n223 $\\nabla_{\\mathcal{I}}f(\\check{\\mathbf{X}})\\triangleq\\nabla f(\\check{\\mathbf{X}})-\\mathbf{J}\\check{\\mathbf{X}}[\\nabla f(\\check{\\mathbf{X}})]^{\\intercal}\\check{\\mathbf{X}}\\mathbf{J}$ . The associated Lagrangian multiplier can be computed as   \n224 $\\boldsymbol{\\Lambda}=\\mathbf{J}\\check{\\mathbf{X}}^{\\mathsf{T}}\\nabla f(\\check{\\mathbf{X}})$ . $(b)$ The critical point condition is equivalent to the requirement that the matrix   \n225 $\\mathbf{X}{\\boldsymbol{\\nabla}}f(\\Breve{\\mathbf{X}})^{\\top}\\mathbf{J}$ is symmetric, which is expressed as $\\mathbf{X}\\mathbf{G}^{\\mathsf{T}}\\mathbf{J}=[\\mathbf{X}\\mathbf{G}^{\\mathsf{T}}\\mathbf{J}]^{\\mathsf{T}}$ .   \n226 Remarks. While our results in Lemma 3.1 show similarities to existing works focusing on problems   \n227 under orthogonality constraints [45], this study marks the first investigation into the first-order   \n228 optimality condition for optimization problems under J-orthogonality constraints. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "229 The following definition is useful in our subsequent analysis of the proposed algorithms. ", "page_idx": 6}, {"type": "text", "text": "230 Definition 3.2. (Block Stationary Point, abbreviated as BS-point) Let $\\theta>0$ . A solution $\\ddot{\\mathbf{X}}\\in\\mathcal{I}$ is   \n231 termed as a block stationary point if, for all $\\mathsf{B}\\in\\Omega\\triangleq\\{\\mathcal{B}_{1},\\mathcal{B}_{2},\\hdots,\\mathcal{B}_{\\mathrm{C}_{n}^{2}}\\}$ , the following condition is   \n232 satisfied: $\\begin{array}{r}{\\mathbf{I}_{2}\\in\\arg\\operatorname*{min}_{\\mathbf{V}\\in\\mathcal{I}_{\\mathtt{B}}}\\mathcal{G}(\\mathbf{V};\\ddot{\\mathbf{X}},\\mathtt{B})}\\end{array}$ . ", "page_idx": 6}, {"type": "text", "text": "233 The following theorem shows the relation between critical points and BS-points. ", "page_idx": 6}, {"type": "text", "text": "234 Theorem 3.3. (Proof in Section $D.2$ ) Any BS-point is a critical point, while the reverse is not   \n235 necessarily true. ", "page_idx": 6}, {"type": "text", "text": "236 4 Convergence Analysis ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "237 This section provides a convergence analysis for GS-JOBCD and VR-J-JOBCD. ", "page_idx": 6}, {"type": "text", "text": "238 For GS-JOBCD, the randomness of output $(\\overline{{\\mathbf{V}}}^{t},\\mathbf{X}^{t+1})$ for all $t$ are influenced by the random variable   \n239 $\\xi^{t}\\triangleq\\left(\\mathtt{B}^{1};\\mathtt{B}^{2};\\cdot\\cdot\\cdot;\\mathtt{B}^{t}\\right)$ . For VR-J-JOBCD, the randomness of output $(\\bar{\\mathbf{V}}_{:}^{t},\\mathbf{X}^{t+1})$ are influenced by   \n240 the random variables $\\iota^{t}\\triangleq\\left(\\mathtt{B}^{1},\\mathtt{S}_{+}^{1},\\mathtt{S}_{*}^{1};\\mathtt{B}^{2},\\mathtt{S}_{+}^{2},\\mathtt{S}_{*}^{2};\\cdot\\cdot\\cdot;\\mathtt{B}^{t},\\mathtt{S}_{+}^{t},\\mathtt{S}_{*}^{t}\\right)$ .   \n241 We denote $\\bar{\\mathbf X}$ as the global optimal solution of Problem (1). To simplify notations, we define:   \n242 $u^{t}=\\|\\tilde{\\mathbf{G}}^{t}-\\nabla f(\\mathbf{X}^{t})\\|_{\\mathsf{F}}^{2}$ , and $\\Delta_{i}=f(\\mathbf{X}^{i})-f(\\bar{\\mathbf{X}})$ . ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "243 We impose the following additional assumptions on the proposed algorithms. ", "page_idx": 6}, {"type": "text", "text": "244 Assumption 4.1. There exists constants $\\{\\overline{{\\mathbf{X}}},\\overline{{\\mathbf{V}}}\\}$ that: $\\|\\mathbf{X}^{t}\\|_{\\mathsf{F}}\\leq\\overline{{\\mathbf{X}}}$ , and $\\|\\mathbf{V}^{t}\\|_{\\mathsf{F}}\\leq\\overline{{\\mathbf{V}}}$ for all $t$ . ", "page_idx": 6}, {"type": "text", "text": "245 Assumption 4.2. There exists a constant $\\overline{{\\mathbf{G}}}$ that: $\\|\\nabla f(\\mathbf{X}^{t})\\|_{\\mathsf{F}}\\leq\\overline{{\\mathbf{G}}}$ , and $\\|\\tilde{\\mathbf{G}}^{t}\\|_{\\mathsf{F}}\\leq\\overline{{\\mathbf{G}}}$ for all $t$ . ", "page_idx": 6}, {"type": "text", "text": "246 Assumption 4.3. For any $\\mathsf{C}\\in\\mathbb{R}^{n\\times n},\\mathbb{E}_{i}[\\|\\nabla f_{i}(\\mathbf{X}^{t})-\\nabla f(\\mathbf{X}^{t})\\|_{\\mathsf{F}}^{2}]\\le\\sigma^{2}$ , where $i$ is drawn uniformly   \n247 at random from $[N]$ .   \n248 Remarks. $(i)$ Assumption 4.1 is satisfied as the function $f_{i}(\\mathbf{X})$ is coercive for all $i$ . (ii) Assumption   \n249 4.2 imposes a bound on the (stochastic) gradient, a fairly moderate condition frequently employed in   \n250 nonconvex optimization [26]. $(i i i)$ Assumption 4.3 ensures that the variance of the stochastic gradient   \n251 is bounded, which is a common requirement in stochastic optimization [30, 5]. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "252 4.1 Global Convergence ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "253 We define the $\\epsilon$ -BS-point as follows. ", "page_idx": 6}, {"type": "text", "text": "254 Definition 4.4. ( $\\epsilon$ -BS-point) Given any constant $\\epsilon>0$ , a point $\\ddot{\\mathbf{X}}$ is called an $\\epsilon$ -BS-point if: $\\mathcal{E}(\\ddot{\\mathbf{X}})\\leq\\epsilon$ .   \n255 Here, E(X) is defined as E(X) \u225cC12n iC=n1 $\\begin{array}{r}{\\mathcal{E}(\\mathbf{X})\\triangleq\\frac{1}{C_{\\mathrm{-}}^{2}}\\sum_{i=1}^{C_{n}^{2}}\\operatorname{dist}(\\mathbf{I}_{2},\\arg\\operatorname*{min}_{\\mathbf{V}}\\mathcal{G}(\\mathbf{V};\\mathbf{X},\\mathcal{B}_{i}))^{2}}\\end{array}$ for For GS-JOBCD,   \n256 while it is defined as $\\begin{array}{r}{\\mathcal{E}(\\mathbf{X})\\triangleq\\frac{1}{\\mathrm{C}_{J}}\\sum_{i=1}^{\\mathrm{C}_{J}}\\mathbb{E}_{\\iota^{t}}[\\mathrm{dist}(\\mathbf{I}_{2},\\mathrm{arg}\\operatorname*{min}_{\\mathbf{V}_{\\iota}}\\mathcal{T}(\\mathbf{V}_{:};\\mathbf{X},\\tilde{\\mathscr{B}}_{i}))^{2}]}\\end{array}$ for VR-J-JOBCD,   \n257 where the expectation is with respect to the randomness inherent in the algorithm [30].   \n258 We have the following useful lemma for VR-J-JOBCD.   \n259 Lemma 4.5. (Proof in Section $E.l$ ) Suppose Assumption 4.3 holds, then the variance $\\mathbb{E}_{\\iota^{t}}[u_{k}]$ of the   \n260 gradient estimators $\\{\\tilde{\\mathbf{G}}^{t}\\}$ of Algorithm 2 is bounded by: $\\begin{array}{r}{\\mathbb{E}_{\\iota^{t}}[u^{t}]\\leq\\frac{p(N-b)}{b(N-1)}\\sigma^{2}+(1-p)\\mathbb{E}_{\\iota^{t-1}}\\[u^{t-1}]+}\\end{array}$   \n261 L2f Xb(\u20321\u2212p)E\u03b9t\u22121[ in=/12 \u2225Vit\u22121\u2212I2\u22252F]   \n262 The following two theorems establish the iteration complexity (or oracle complexity) for GS-JOBCD   \n263 and VR-J-JOBCD.   \n264 Theorem 4.6. (Proof in Section E.2) GS-JOBCD finds an $\\epsilon$ -BS-point of Problem (1) within $\\mathcal{O}(\\frac{\\Delta_{0}N}{\\epsilon})$   \n265 arithmetic operations.   \n266 Theorem 4.7. (Proof in Section $E.3$ ) Let $b=N$ , $\\begin{array}{r}{b^{\\prime}=\\sqrt{N},}\\end{array}$ , and $\\begin{array}{r}{p=\\frac{b^{\\prime}}{b+b^{\\prime}}}\\end{array}$ . VR-J-JOBCD finds an   \n267 $\\epsilon$ -BS-point of Problem (1) within $\\mathcal{O}(n N+\\frac{\\Delta_{0}\\sqrt{N}}{\\epsilon})$ arithmetic operations.   \n268 Remark. Theorems 4.6 and 4.7 demonstrate that the arithmetic operation com\u221aplexity of GS-JOBCD   \n269 is linearly dependent on $N$ , while VR-J-JOBCD is linearly dependent on $\\sqrt{N}$ . Therefore, VR-J  \n270 JOBCD reduces the iteration complexity significantly. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "271 4.2 Strong Convergence under KL Assumption ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "272 We prove algorithms achieve strong convergence based on a non-convex analysis tool called Kurdyka  \n273 \u0141ojasiewicz inequality[2].   \n274 We impose the following assumption on Problem (1).   \n275 Assumption 4.8. (Kurdyka-\u0141ojasiewicz Property). Assume that $f^{\\circ}(\\mathbf{X})=f(\\mathbf{X})+{\\mathcal{I}}_{\\mathcal{I}}(\\mathbf{X})$ is a KL   \n276 function. For all $\\mathbf{X}\\in\\operatorname{dom}f^{\\circ}$ , there exists $\\sigma\\in[0,1),\\eta\\in(0,+\\infty]$ a neighborhood $\\Upsilon$ of $\\mathbf{X}$ and a   \n277 concave and continuous function $\\varphi(t)=c t^{1-\\sigma},c>0,t\\in[0,\\eta)$ such that for all $\\mathbf{X}^{\\prime}\\in\\Upsilon$ and satisfies   \n278 $f^{\\circ}(\\mathbf{X}^{\\prime})\\in(f^{\\circ}(\\mathbf{X}),f^{\\circ}(\\mathbf{X})+\\eta)$ , the following holds: $\\mathrm{dist}(\\mathbf{0},\\dot{\\nabla}f^{\\circ}(\\mathbf{X}^{\\prime}))\\varphi^{\\prime}(f^{\\circ}(\\mathbf{X}^{\\prime})-f^{\\circ}(\\mathbf{X}))\\geq1$ . ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "279 We establish strong limit-point convergence for VR-J-JOBCD and GS-JOBCD. ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "280 Theorem 4.9. (Proof in Section E.5, a Finite Length Property). The sequence $\\{\\mathbf{X}^{t}\\}_{t=0}^{\\infty}$ of GS  \n281 JOBCD has finite length property that: \u2200t, $\\begin{array}{r}{,\\sum_{i=1}^{t}\\mathbb{E}_{\\xi^{t}}[\\|\\mathbf{X}^{t+1}-\\mathbf{X}^{t}\\|_{\\mathsf{F}}]\\le\\mathcal{O}(\\varphi(\\Delta_{1}))<+\\infty,}\\end{array}$ , where   \n282 $\\varphi(\\cdot)$ is the desingularization function defined in Proposition 4.8.   \n283 Theorem 4.10. (Proof in Section E.4, a Finite Length Property). Choosing $b\\,=\\,N,$ , $b^{\\prime}=\\sqrt{N}$   \n284 and $\\begin{array}{r}{p=\\frac{b^{\\prime}}{b+b^{\\prime}}}\\end{array}$ , then the sequence $\\{\\mathbf{X}^{t}\\}_{t=0}^{\\infty}$ of VR-J-JOBCD has finite length property that:   \n285 $\\begin{array}{r}{\\forall t,\\!\\!\\sum_{i=1}^{t}\\mathbb{E}_{{t}^{t}}[\\|{\\mathbf{X}}^{t+1}-{\\mathbf{X}}^{t}\\|_{\\mathsf{F}}]\\,\\leq\\,\\mathcal{O}(\\frac{\\varphi(\\Delta_{1})}{N^{1/4}})\\,<\\,+\\infty,}\\end{array}$ , where $\\varphi(\\cdot)$ is the desingularization function   \n286 defined in Assumption 4.8. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "287 5 Applications and Numerical Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "288 This section demonstrates the effectiveness and efficiency of JOBCD on three optimization tasks:   \n289 $(i)$ the hyperbolic eigenvalue problem, $(\\romannumeral2)$ structural probe problem, and $(i i i)$ Ultra-hyperbolic   \n290 Knowledge Graph Embedding problem. We provide experiments for the last problem in Section F.2.   \n291 $\\blacktriangleright$ Application to the Hyperbolic Eigenvalue Problem (HEVP). The hyperbolic eigenvalue problem   \n292 refers to the generalized eigenvalue problem in hyperbolic spaces [40]. This problem is a fundamental   \n293 component in machine learning models, such as Hyperbolic PCA [43, 6]. Given a data matrix   \n294 $\\mathbf{D}\\in\\mathbb{R}^{m\\times n}$ and a signature matrix $\\mathbf{J}$ with signature $(p,n-p)$ , HEVP can be formulated as the   \n295 following optimization problem: minX $-\\;\\mathbf{tr}(\\mathbf{X^{\\mathsf{T}}D^{\\mathsf{T}}D X})$ , s. t. $\\mathbf{X}^{\\mathsf{T}}\\mathbf{J}\\mathbf{X}=\\mathbf{J}$ .   \n296 $\\blacktriangleright$ Application to the Hyperbolic Structural Probe Problem (HSPP). The Structure Probe (SP) is   \n297 a metric learning model aimed at understanding the intrinsic semantic information of large language   \n298 models [20] [7]. Given a data matrix $\\mathbf{D}\\,\\in\\,\\bar{\\mathbb{R}}^{m\\times n}$ and its associated Euclidean distance metric   \n299 matrix $\\mathbf{T}\\,\\in\\,\\mathbb{R}^{m\\times m}$ , HSPP employs a smooth homeomorphic mapping function $\\varphi(\\cdot)$ to project   \n300 the data $\\mathbf{D}$ into ultra-hyperbolic space. Subsequently, it seeks an appropriate linear transformation   \n301 $\\mathbf{X}\\,\\in\\,\\mathbb{R}^{n\\times n}$ constrained within a specific structure $\\mathbf{X}\\in{\\mathcal{I}}$ , such that the resulting transformed   \n302 2 data ${\\bf Q}\\triangleq\\varphi({\\bf D}){\\bf X}\\in\\mathbb{R}^{m\\times n}$ exhibits similarity to the original distance metric matrix $\\mathbf{T}$ under the   \n303 ultra-hyperbolic geodesic distance $d_{\\alpha}(\\mathbf{Q}_{i:},\\mathbf{Q}_{j:})$ , expressed as $\\mathbf{T}_{i,j}\\approx d_{\\alpha}(\\mathbf{Q}_{i:},\\mathbf{Q}_{j:})$ for all $i,j\\in[m]$ ,   \n304 4 where $\\mathbf{Q}_{i}$ : is $i$ -th row of the matrix $\\mathbf{Q}\\in\\mathbb{R}^{m\\times n}$ . This can be formulated as the following optimization   \n305 problem: minX $\\begin{array}{r}{\\frac{1}{m^{2}}\\sum_{i,j\\in m}(\\mathbf{T}_{i,j}-d_{\\alpha}(\\mathbf{Q}_{i:},\\mathbf{Q}_{j:}))^{2}}\\end{array}$ , s. t. $\\mathbf{Q}\\triangleq\\varphi(\\mathbf{D})\\mathbf{X}$ , $\\mathbf{X}\\in{\\mathcal{I}}$ . For more details   \n306 on the functions $\\varphi(\\cdot)$ and $d_{\\alpha}(\\cdot,\\cdot)$ , please refer to Appendix Section F.1.   \n307 $\\blacktriangleright$ Datasets. To generate the matrix $\\mathbf{D}\\in\\mathbb{R}^{m\\times n}$ , we use 8 real-world or synthetic data sets for both   \n308 HEVP and HSPP tasks: \u2018Cifar\u2019, \u2018CnnCaltech\u2019, \u2018Gisette\u2019, \u2018Mnist\u2019, \u2018randn\u2019, \u2018Sector\u2019, \u2018TDT2\u2019, \u2018w1a\u2019.   \n309 We randomly extract a subset from the original data sets for the experiments.   \n310 $\\blacktriangleright$ Compared Methods. We compare GS-JOBCD and VR-J-JOBCD with 3 state-of-the-art   \n311 optimization algorithms under $\\mathbf{J}.$ -orthogonality constraints. (i) The CS Decomposition Method   \n312 (CSDM) [48]. $(\\romannumeral2)$ Stardard ADMM (ADMM) [19]. UMCM: Unconstrained Multiplier Correction   \n313 Method [31, 13]. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Table 1: Comparisons of the objectives for HEVP across all the compared methods. The time limit is set to 90s. The notation $\\mathbf{\\dot{\\rho}}(+)^{\\prime}$ indicates that GS-JOBCD significantly improves upon the initial solution provided by CSDM. The $1^{s t}$ , $2^{\\mathrm{nd}}$ , and $3^{\\mathrm{rd}}$ best results are colored with red, green and blue, respectively. The value in (\u00b7) stands for $\\begin{array}{r}{\\sum_{i j}^{n}|\\mathbf{X}^{\\top}\\mathbf{J}\\mathbf{X}-\\mathbf{J}|_{i j}}\\end{array}$ . ", "page_idx": 8}, {"type": "image", "img_path": "8bExkmfLCr/tmp/2ec2eaf07c3bfa787516dcef00d9b81a6805b527c58f65e2ffd013402935f50f.jpg", "img_caption": ["Figure 1: The convergence curve for the HEVP across various datasets with different parameters $(m,n,p)$ . "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "8bExkmfLCr/tmp/af8099db119e2adf5a18d96b1b6f5f5ef8505ab00ffae3770d4d642e67f6070e.jpg", "img_caption": ["Figure 2: The convergence curve for HEVP across various datasets with different parameters $(m,n,p)$ . "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "314 $\\blacktriangleright$ Experiment Settings. All methods are implemented using Pytorch on an Intel $2.6\\:\\mathrm{GHz}$ processor   \n315 with an A40 (48GB). For HSPP, we fix $\\alpha$ to 1. Each method employs the same random J-orthogonal   \n316 matrix. The built-in solver Admm is used to solve the unconstrained minimization problem in CSDM.   \n317 We provide our code in the supplemental material.   \n318 \u25b6Experiment Results. Table 1 and Figure 1 display the accuracy and computational efficiency   \n319 for HEVP, while Figure 2 presents the results for HSPP, leading to the following observations: (i)   \n320 GS-JOBCD and JJOBCD consistently deliver better performance than the other methods. (ii) Other   \n321 methods frequently encounter poor local minima, whereas GS-JOBCD effectively escapes these   \n322 minima and typically achieves lower objective values, aligning with our theory that our methods   \n323 locate stronger stationary points. (iii) VR-J-JOBCD outperforms both J-JOBCD and CSDM when   \n324 dealing with a large dataset characterized by an infinite-sum structure. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "325 6 Conclusions ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "326 In this paper, we propose a new approach JOBCD, which is based on block coordinate descent, for   \n327 solving the optimization problem under J-orthogonality constraints. We discuss two specific variants   \n328 of JOBCD: one based on a Gauss-Seidel strategy (GS-JOBCD), the other on a variance-reduced   \n329 Jacobi strategy. Both algorithms capitalize on specific structural characteristics of the constraints to   \n330 converge to more favorable stationary solutions. Notably, VR-J-JOBCD incorporates a variance  \n331 reduction technique into a parallel framework to reduce oracle complexity in the minimization of   \n332 finite-sum functions. For both GS-JOBCD and VR-J-JOBCD, we establish the oracle complexity   \n333 under mild conditions and strong limit-point convergence results under the Kurdyka-Lojasiewicz   \n334 inequality. Some experiments on the hyperbolic eigenvalue problem and structural probe problem   \n335 show the efficiency and efficacy of the proposed methods. ", "page_idx": 8}, {"type": "text", "text": "336 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "337 [1] P-A Absil, Robert Mahony, and Rodolphe Sepulchre. Optimization algorithms on matrix   \n338 manifolds. Princeton University Press, 2008.   \n339 [2] H\u00e9dy Attouch, J\u00e9r\u00f4me Bolte, Patrick Redont, and Antoine Soubeyran. Proximal alternating   \n340 minimization and projection methods for nonconvex problems: An approach based on the   \n341 kurdyka-\u0142ojasiewicz inequality. Mathematics of operations research, 35(2):438\u2013457, 2010.   \n342 [3] Adam Bojanczyk, Nicholas J Higham, and Harikrishna Patel. Solving the indefinite least squares   \n343 problem by hyperbolic qr factorization. SIAM Journal on Matrix Analysis and Applications,   \n344 24(4):914\u2013931, 2003.   \n345 [4] HanQin Cai, Yuchen Lou, Daniel McKenzie, and Wotao Yin. A zeroth-order block coordinate   \n346 descent algorithm for huge-scale black-box optimization. In International Conference on   \n347 Machine Learning, pages 1193\u20131203. PMLR, 2021.   \n348 [5] Xufeng Cai, Chaobing Song, Stephen Wright, and Jelena Diakonikolas. Cyclic block coordinate   \n349 descent with variance reduction for composite nonconvex optimization. In International   \n350 Conference on Machine Learning, pages 3469\u20133494. PMLR, 2023.   \n351 [6] Ines Chami, Albert Gu, Dat P Nguyen, and Christopher Re. Horopca: Hyperbolic dimensionality   \n352 reduction via horospherical projections. In International Conference on Machine Learning   \n353 (ICML), volume 139, pages 1419\u20131429, 2021.   \n354 [7] Boli Chen, Yao Fu, Guangwei Xu, Pengjun Xie, Chuanqi Tan, Mosha Chen, and Liping Jing.   \n355 Probing bert in hyperbolic spaces. ICLR, 2021.   \n356 [8] Weize Chen, Xu Han, Yankai Lin, Hexu Zhao, Zhiyuan Liu, Peng Li, Maosong Sun, and Jie   \n357 Zhou. Fully hyperbolic neural networks. arXiv preprint arXiv:2105.14686, 2021.   \n358 [9] Ashok Cutkosky and Francesco Orabona. Momentum-based variance reduction in non-convex   \n359 sgd. Advances in neural information processing systems, 32, 2019.   \n360 [10] Aaron Defazio, Francis Bach, and Simon Lacoste-Julien. Saga: A fast incremental gradi  \n361 ent method with support for non-strongly convex composite objectives. Advances in neural   \n362 information processing systems, 27, 2014.   \n363 [11] Cong Fang, Chris Junchi Li, Zhouchen Lin, and Tong Zhang. Spider: Near-optimal non  \n364 convex optimization via stochastic path-integrated differential estimator. Advances in neural   \n365 information processing systems, 31, 2018.   \n366 [12] Hamza Fawzi and Harry Goulbourne. Faster proximal algorithms for matrix optimization   \n367 using jacobi-based eigenvalue methods. Advances in Neural Information Processing Systems,   \n368 34:11397\u201311408, 2021.   \n369 [13] Bin Gao, Xin Liu, Xiaojun Chen, and Ya-xiang Yuan. A new first-order algorithmic framework   \n370 for optimization problems with orthogonality constraints. SIAM Journal on Optimization,   \n371 28(1):302\u2013332, 2018.   \n372 [14] Bin Gao, Xin Liu, and Ya-xiang Yuan. Parallelizable algorithms for optimization problems with   \n373 orthogonality constraints. SIAM Journal on Scientific Computing, 41(3):A1949\u2013A1983, 2019.   \n374 [15] Saeed Ghadimi, Guanghui Lan, and Hongchao Zhang. Mini-batch stochastic approximation   \n375 methods for nonconvex stochastic composite optimization. Mathematical Programming, 155(1-   \n376 2):267\u2013305, 2016.   \n377 [16] Gene H Golub and Charles F Van Loan. Matrix computations. JHU press, 2013.   \n378 [17] Eldon R Hansen. On cyclic jacobi methods. Journal of the Society for Industrial and Applied   \n379 Mathematics, 11(2):448\u2013459, 1963.   \n380 [18] Vjeran Hari and Erna Begovic\u00b4 Kovac\u02c7. On the convergence of complex jacobi methods. Linear   \n381 and multilinear algebra, 69(3):489\u2013514, 2021.   \n382 [19] Bingsheng He and Xiaoming Yuan. On the ${\\mathcal{O}}(1/n)$ convergence rate of the douglas-rachford   \n383 alternating direction method. SIAM Journal on Numerical Analysis, 50(2):700\u2013709, 2012.   \n384 [20] John Hewitt and Christopher D. Manning. A structural probe for finding syntax in word   \n385 representations. In Jill Burstein, Christy Doran, and Thamar Solorio, editors, Proceedings of   \n386 the 2019 Conference of the North American Chapter of the Association for Computational   \n387 Linguistics: Human Language Technologies (NAACL-HLT), pages 4129\u20134138, 2019.   \n388 [21] Nicholas J Higham. J-orthogonal matrices: Properties and generation. SIAM review, 45(3):504\u2013   \n389 519, 2003.   \n390 [22] Minhui Huang, Shiqian Ma, and Lifeng Lai. A riemannian block coordinate descent method for   \n391 computing the projection robust wasserstein distance. In International Conference on Machine   \n392 Learning, pages 4446\u20134455. PMLR, 2021.   \n393 [23] Bo Hui and Wei-Shinn Ku. Low-rank nonnegative tensor decomposition in hyperbolic space. In   \n394 Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining,   \n395 pages 646\u2013654, 2022.   \n396 [24] Sashank J Reddi, Suvrit Sra, Barnabas Poczos, and Alexander J Smola. Proximal stochastic   \n397 methods for nonsmooth nonconvex finite-sum optimization. Advances in neural information   \n398 processing systems, 29, 2016.   \n399 [25] Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance   \n400 reduction. In C.J. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.Q. Weinberger, editors,   \n401 Advances in Neural Information Processing Systems, volume 26. Curran Associates, Inc., 2013.   \n402 [26] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua   \n403 Bengio and Yann LeCun, editors, International Conference on Learning Representations (ICLR),   \n404 2015.   \n405 [27] Marc Law. Ultrahyperbolic neural networks. Advances in Neural Information Processing   \n406 Systems, 34:22058\u201322069, 2021.   \n407 [28] Marc Law and Jos Stam. Ultrahyperbolic representation learning. Advances in neural informa  \n408 tion processing systems, 33:1668\u20131678, 2020.   \n409 [29] Qunwei Li, Yi Zhou, Yingbin Liang, and Pramod K Varshney. Convergence analysis of proximal   \n410 gradient with momentum for nonconvex optimization. In International Conference on Machine   \n411 Learning, pages 2111\u20132119. PMLR, 2017.   \n412 [30] Zhize Li, Hongyan Bao, Xiangliang Zhang, and Peter Richt\u00e1rik. Page: A simple and optimal   \n413 probabilistic gradient estimator for nonconvex optimization. In International conference on   \n414 machine learning, pages 6286\u20136295. PMLR, 2021.   \n415 [31] Wei Liu, Yinyu Zhang, Hongqiao Yang, and Shuzhong Zhang. A class of smooth exact penalty   \n416 function methods for optimization problems with orthogonality constraints. Optimization,   \n417 69(3):399\u2013426, 2020.   \n418 [32] Wan-Duo Kurt Ma, JP Lewis, and W Bastiaan Kleijn. The hsic bottleneck: Deep learning   \n419 without back-propagation. In Proceedings of the AAAI conference on artificial intelligence,   \n420 volume 34, pages 5085\u20135092, 2020.   \n421 [33] Julien Mairal. Optimization with first-order surrogate functions. In International Conference   \n422 on Machine Learning (ICML), volume 28, pages 783\u2013791, 2013.   \n423 [34] Lam M Nguyen, Jie Liu, Katya Scheinberg, and Martin Tak\u00e1\u02c7c. Sarah: A novel method for   \n424 machine learning problems using stochastic recursive gradient. In International conference on   \n425 machine learning, pages 2613\u20132621. PMLR, 2017.   \n426 [35] Maximillian Nickel and Douwe Kiela. Learning continuous hierarchies in the lorentz model   \n427 of hyperbolic geometry. In International Conference on Machine Learning, pages 3779\u20133788.   \n428 PMLR, 2018.   \n429 [36] Vedran Novakovi\u00b4c and Sanja Singer. A kogbetliantz-type algorithm for the hyperbolic svd.   \n430 Numerical algorithms, 90(2):523\u2013561, 2022.   \n431 [37] Julie Nutini, Issam Laradji, and Mark Schmidt. Let\u2019s make block coordinate descent converge   \n432 faster: faster greedy rules, message-passing, active-set complexity, and superlinear convergence.   \n433 Journal of Machine Learning Research, 23(131):1\u201374, 2022.   \n434 [38] Meisam Razaviyayn, Mingyi Hong, and Zhi-Quan Luo. A unified convergence analysis of block   \n435 successive minimization methods for nonsmooth optimization. SIAM Journal on Optimization,   \n436 23(2):1126\u20131153, 2013.   \n437 [39] Mark Schmidt, Nicolas Le Roux, and Francis Bach. Minimizing finite sums with the stochastic   \n438 average gradient. Mathematical Programming, 162:83\u2013112, 2017.   \n439 [40] Ivan Slapnicar and Ninoslav Truhar. Relative perturbation theory for hyperbolic eigenvalue   \n440 problem. Linear Algebra and its Applications, 309(1):57\u201372, 2000.   \n441 [41] Michael Stewart and Paul Van Dooren. On the factorization of hyperbolic and unitary trans  \n442 formations into rotations. SIAM Journal on Matrix Analysis and Applications, 27(3):876\u2013890,   \n443 2005.   \n444 [42] Puoya Tabaghi and Ivan Dokmani\u00b4c. Hyperbolic distance matrices. In Proceedings of the   \n445 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages   \n446 1728\u20131738, 2020.   \n447 [43] Puoya Tabaghi, Michael Khanzadeh, Yusu Wang, and Sivash Mirarab. Principal component   \n448 analysis in space forms. ArXiv, abs/2301.02750, 2023.   \n449 [44] Zhe Wang, Kaiyi Ji, Yi Zhou, Yingbin Liang, and Vahid Tarokh. Spiderboost and momentum:   \n450 Faster variance reduction algorithms. Advances in Neural Information Processing Systems, 32,   \n451 2019.   \n452 [45] Zaiwen Wen and Wotao Yin. A feasible method for optimization with orthogonality constraints.   \n453 Mathematical Programming, 142:397 \u2013 434, 2012.   \n454 [46] WikiContributors. Quartic equation. https: // en. wikipedia. org/ wiki/ Quartic_   \n455 equation .   \n456 [47] Ruiyuan Wu, Anna Scaglione, Hoi-To Wai, Nurullah Karakoc, Kari Hreinsson, and Wing-Kin   \n457 Ma. Federated block coordinate descent scheme for learning global and personalized models. In   \n458 Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 10355\u201310362,   \n459 2021.   \n460 [48] Bo Xiong, Shichao Zhu, Mojtaba Nayyeri, Chengjin Xu, Shirui Pan, Chuan Zhou, and Steffen   \n461 Staab. Ultrahyperbolic knowledge graph embeddings. In Proceedings of the 28th ACM SIGKDD   \n462 Conference on Knowledge Discovery and Data Mining, pages 2130\u20132139, 2022.   \n463 [49] Bo Xiong, Shichao Zhu, Nico Potyka, Shirui Pan, Chuan Zhou, and Steffen Staab. Semi  \n464 riemannian graph convolutional networks. ArXiv, abs/2106.03134, 2021.   \n465 [50] Tao Yu and Christopher M De Sa. Numerically accurate hyperbolic embeddings using tiling  \n466 based models. Advances in Neural Information Processing Systems, 32, 2019.   \n467 [51] Ganzhao Yuan. A block coordinate descent method for nonsmooth composite optimization   \n468 under orthogonality constraints. ArXiv, abs/2304.03641, 2023.   \n469 [52] Ganzhao Yuan. Coordinate descent methods for fractional minimization. In International   \n470 Conference on Machine Learning, pages 40488\u201340518, 2023.   \n471 [53] Jinshan Zeng, Tim Tsz-Kit Lau, Shaobo Lin, and Yuan Yao. Global convergence of block   \n472 coordinate descent in deep learning. In International conference on machine learning, pages   \n473 7313\u20137323. PMLR, 2019.   \n474 [54] Yiding Zhang, Xiao Wang, Chuan Shi, Nian Liu, and Guojie Song. Lorentzian graph convolu  \n475 tional networks. In Proceedings of the Web Conference 2021, pages 1249\u20131261, 2021.   \n476 [55] Dongruo Zhou, Pan Xu, and Quanquan Gu. Stochastic nested variance reduction for nonconvex   \n477 optimization. The Journal of Machine Learning Research, 21(1):4130\u20134192, 2020. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "478 Appendix ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "479 The appendix is organized as follows. ", "page_idx": 12}, {"type": "text", "text": "480 Appendix A introduces some notations, technical preliminaries, and relevant lemmas.   \n481 Appendix B concludes some additional discussions.   \n482 Appendix C presents the proofs for Section 2.   \n483 Appendix D offers the proofs for Section 3.   \n484 Appendix E contains the proofs for Section 4.   \n485 Appendix F contains several extra experiments, extensions and discussions of the proposed methods. ", "page_idx": 12}, {"type": "text", "text": "486 A Notations, Technical Preliminaries, and Relevant Lemmas ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "487 A.1 Notations ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "488 In this paper, we denote the Lowercase boldface letters represent vectors, while uppercase letters   \n489 represent real-valued matrices. We use the Matlab colon notation to denote indices that describe   \n490 submatrices. The following notations are used throughout this paper.   \n491 \u2022 $\\mathbb{N}:S\\mathbf{e}$ t of natural numbers   \n492 \u2022 $\\mathbb{R}$ : Set of real numbers   \n493 \u2022 $[n]$ : {1, 2, ..., n}   \n494 \u2022 $\\|\\mathbf{x}\\|$ : Euclidean norm: $\\|\\mathbf{x}\\|=\\|\\mathbf{x}\\|_{2}=\\sqrt{\\langle\\mathbf{x},\\mathbf{x}\\rangle}$   \n495 \u2022 $\\mathbf{x}_{i}$ : the $i$ -th element of vector x   \n496 \u2022 $\\mathbf{X}_{i,j}$ or $\\mathbf{X}_{i j}$ : the $(i^{\\mathrm{th}},j^{\\mathrm{th}})$ element of matrix $\\mathbf{X}$   \n497 \u2022 $\\mathrm{vec}(\\mathbf{X}):\\mathrm{vec}(\\mathbf{X})\\in\\mathbb{R}^{n n\\times1}$ , the vector formed by stacking the column vectors of $\\mathbf{X}$   \n498 \u2022 $\\mathrm{mat}(\\mathbf{x})\\in\\mathbb{R}^{n\\times n}$ , Convert $\\mathbf{x}\\in\\mathbb{R}^{n n\\times1}$ into a matrix with $\\operatorname{mat}(\\operatorname{vec}(\\mathbf{X}))=\\mathbf{X}$   \n499 \u2022 ${\\bf X^{\\top}}$ : the transpose of the matrix $\\mathbf{X}$   \n500 \u2022 $\\mathrm{sign}(t)$ : the signum function, $\\mathrm{sign}(t)=1$ if $t\\geq0$ and $\\mathrm{sign}(t)=-1$ otherwise   \n501 \u2022 $\\mathbf{X}\\otimes\\mathbf{Y}$ : Kronecker product of $\\mathbf{X}$ and $\\mathbf{Y}$   \n502 \u2022 $\\operatorname*{det}(\\mathbf{D})$ : Determinant of a square matrix $\\mathbf{D}\\in\\mathbb{R}^{n\\times n}\\mathbf{D}\\in\\mathbb{R}^{n\\times n}$   \n503 \u2022 ${\\mathbf{C}}_{n}^{2}$ : the number of possible combinations choosing $k$ items from $n$ without repetition.   \n504 \u2022 $\\mathbf{0}_{n,r}$ : A zero matrix of size $n\\times r$ ; the subscript is omitted sometimes   \n505 \u2022 $\\mathbf{I}_{r}:\\mathbf{I}_{r}\\in\\mathbb{R}^{r\\times r}$ , Identity matrix   \n506 \u2022 $\\mathbf{X}\\succeq\\mathbf{0}(\\mathrm{or}\\,\\succ\\mathbf{0})$ : the Matrix $\\mathbf{X}$ is symmetric positive semidefinite (or definite)   \n507 \u2022 $\\operatorname{Diag}(\\mathbf{x})$ : Diagonal matrix with $\\mathbf{x}$ as the main diagonal entries.   \n508 \u2022 $\\mathbf{tr}(\\mathbf{A}):\\operatorname{Sum}$ of the elements on the main diagonal A: $\\begin{array}{r}{\\mathbf{tr}(\\mathbf{A})=\\sum_{i}\\mathbf{A}_{i,i}}\\end{array}$   \n509 \u2022 $\\|\\mathbf{X}\\|_{*}$ : Nuclear norm: sum of the singular values of matrix $\\mathbf{X}$   \n510 \u2022 $\\lVert\\bf X\\rVert$ : Operator/Spectral norm: the largest singular value of $\\mathbf{X}$   \n511 \u2022 $\\lVert\\mathbf{X}\\rVert_{\\mathsf{F}}$ : Frobenius norm: $(\\sum_{i j}\\mathbf{X}_{i j}^{2})^{1/2}$   \n512 \u2022 $\\nabla f(\\mathbf{X})$ : classical (limiting) Euclidean gradient of $f(\\mathbf{X})$ at $\\mathbf{X}$   \n513 \u2022 $\\nabla_{\\mathcal{I}}f(\\mathbf{X})$ : Riemannian gradient of $f(\\mathbf{X})$ at $\\mathbf{X}$   \n514 \u2022 ${\\mathcal{T}}_{\\boldsymbol{\\xi}}(\\mathbf{X})$ : the indicator function of a set $\\xi$ with $\\mathcal{T}_{\\xi}(\\mathbf{X})=0$ if $\\mathbf{X}\\in\\xi$ and otherwise $+\\infty$   \n515 \u2022 $\\mathrm{dist}(\\xi,\\xi^{\\prime})$ : the distance between two sets with $\\operatorname{dist}(\\xi,\\xi^{\\prime})\\triangleq\\operatorname*{inf}_{\\mathbf{X}\\in\\xi,\\mathbf{X^{\\prime}}\\in\\xi^{\\prime}}\\|\\mathbf{X}-\\mathbf{X^{\\prime}}\\|_{\\mathsf{F}}$   \n516 \u2022 $\\mathcal{T}_{\\xi}(\\mathbf{x})$ : the indicator function of a set $\\xi$ with $\\mathcal{T}_{\\xi}(\\mathbf{x})=0$ if $\\mathbf{x}\\in\\xi$ and otherwise $+\\infty$ . ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "517 A.2 Relevant Lemmas ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "518 Lemma A.1. (Lemma 6.6 of [51]) For any $\\textbf{W}\\in\\,\\mathbb{R}^{n\\times n}$ , we have: $\\begin{array}{r l r}{\\sum_{i=1}^{C_{n}^{k}}\\|\\mathbf{W}(B_{i},B_{i})\\|_{\\mathsf{F}}^{2}\\!\\!}&{{}=}&{}\\end{array}$   \n519 $\\begin{array}{r}{\\frac{k}{n}C_{n}^{k}\\sum_{i}\\mathbf{W}_{i i}^{2}+C_{n-2}^{k-2}\\sum_{i}\\sum_{j,j\\neq i}\\mathbf{W}_{i j}^{2}}\\end{array}$ . H re, the set $\\{B_{1},B_{2},\\cdots,B_{C_{n}^{k}}\\}$ represents all possible   \n520 combinations of the index vectors choosing $k$ items from $n$ without repetition. ", "page_idx": 13}, {"type": "text", "text": "Lemma A.2. We have $\\mathtt{S}_{+}$ be the set of $|\\mathsf{S}_{+}|=b$ samples from $[N]$ , drawn with replacement and uniformly at random. Then, $\\b{\\mathscr{n}},\\mathbf{X}^{t}\\in\\bar{\\mathbb{R}^{n\\times n}}$ , we have: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{t^{t}}[\\|\\frac{1}{b}\\sum_{i\\in\\mathtt{S}_{+}}\\nabla f_{i}(\\mathbf{X}^{t})-\\nabla f(\\mathbf{X}^{t})\\|_{\\mathsf{F}}^{2}]=\\frac{N-b}{b(N-1)}\\mathbb{E}_{t^{t}}[\\|\\nabla f_{i}(\\mathbf{X}^{t})-\\nabla f(\\mathbf{X}^{t})\\|_{\\mathsf{F}}^{2}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "521 Proof. The proof is exactly the same as in Lemma 2.8 of [5]. ", "page_idx": 13}, {"type": "text", "text": "522 Lemma A.3. The tangent space $\\mathbf{T}\\mathbf{x}\\mathcal{I}$ of manifold constructed by $\\mathbf{X}^{\\top}\\mathbf{J}\\mathbf{X}=\\mathbf{J}$ , with $\\mathbf{X}\\in\\mathbb{R}^{n\\times n}$ , is : ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbf{T}_{\\mathbf{X}}\\mathcal{I}\\triangleq\\{\\mathbf{Y}\\in\\mathbb{R}^{n\\times n}\\mid\\mathbf{X}^{\\top}\\mathbf{J}\\mathbf{Y}+\\mathbf{Y}^{\\top}\\mathbf{J}\\mathbf{X}=0\\},\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "523 where $\\mathbf{Y}=t\\tilde{\\mathbf{Y}}$ with $t$ is a positive scalar approaching $\\boldsymbol{O}$ . ", "page_idx": 13}, {"type": "text", "text": "524 Proof. Assuming point $\\mathbf{X}\\,\\in\\,\\mathbb{R}^{n\\times n}$ lies on manifold $\\mathcal{I}$ , we have: $h(\\mathbf{X})=\\mathbf{X}^{\\top}\\mathbf{J}\\mathbf{X}-\\mathbf{J}$ . Moving   \n525 along $\\mathbf{Y}\\in\\mathbb{R}^{n\\times\\bar{n}}$ in the tangent space of $\\mathbf{X}$ , we obtain: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{h({\\mathbf X}+{\\mathbf Y})=({\\mathbf X}+{\\mathbf Y})^{\\top}{\\mathbf J}({\\mathbf X}+{\\mathbf Y})-{\\mathbf J}}\\\\ &{\\qquad\\qquad={\\mathbf X}^{\\top}{\\mathbf J}{\\mathbf X}+{\\mathbf X}^{\\top}{\\mathbf J}{\\mathbf Y}+{\\mathbf Y}^{\\top}{\\mathbf J}{\\mathbf X}+{\\mathbf Y}^{\\top}{\\mathbf J}{\\mathbf Y}-{\\mathbf J}}\\\\ &{\\qquad\\qquad\\overset{\\textregistered}{=}{\\mathbf X}^{\\top}{\\mathbf J}{\\mathbf Y}+{\\mathbf Y}^{\\top}{\\mathbf J}{\\mathbf X}+{\\mathbf Y}^{\\top}{\\mathbf J}{\\mathbf Y}}\\\\ &{\\qquad\\qquad\\triangleq_{t}{\\mathbf X}^{\\top}{\\mathbf J}\\tilde{\\mathbf Y}+t\\tilde{\\mathbf Y}^{\\top}{\\mathbf J}{\\mathbf X}+t^{2}\\tilde{\\mathbf Y}^{\\top}{\\mathbf J}\\tilde{\\mathbf Y}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "526 where step $\\textcircled{1}$ uses $\\mathbf{X}^{\\top}\\mathbf{J}\\mathbf{X}=\\mathbf{J}$ ; step $\\circledcirc$ uses $\\mathbf{Y}=t\\tilde{\\mathbf{Y}}$ . ", "page_idx": 13}, {"type": "text", "text": "527 Since $t$ is a positive scalar approaching 0, we can ignore the higher-order term: $t^{2}\\tilde{\\mathbf{Y}}^{\\top}\\mathbf{J}\\tilde{\\mathbf{Y}}$ . Ac  \n528 cording to the properties of the tangent space of any manifold, we have: $h(\\mathbf{X}+\\mathbf{Y})=0$ , In   \n529 other words, $\\mathbf{X}^{\\mp}\\mathbf{J}\\mathbf{\\bar{Y}}+\\mathbf{Y}^{\\top}\\mathbf{J}\\mathbf{X}=\\,0$ , i.e. we obtain the defining equation for the tangent space:   \n530 ${\\bf T}{\\bf x}\\mathcal{I}\\triangleq\\{{\\bf Y}\\in\\mathbb{R}^{n\\times n}\\mid{\\bf X}^{\\top}{\\bf J}{\\bf Y}+{\\bf Y}^{\\top}{\\bf J}{\\bf X}=0\\}$ . \u53e3 ", "page_idx": 13}, {"type": "text", "text": "531 B Additional Discussions ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "532 B.1 On the Global Optimal Solution for Problem (7) ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "533 In Section 2.1, we have demonstrated how to use the breakpoint search method to obtain an optimal   \n534 solution for the case of $\\mathbf{V}\\,=\\,\\left(\\begin{array}{l}{\\tilde{\\mathbf{c}}~\\tilde{\\mathbf{s}}}\\\\ {\\tilde{\\mathbf{s}}~\\tilde{\\mathbf{c}}}\\end{array}\\right)$ of Problem (7). Since the structure of the other three cases   \n535 $\\mathbf{V}\\in\\big\\{\\big(\\begin{array}{l l}{\\tilde{\\mathrm{~c~}}}&{-\\tilde{\\mathrm{s}}}\\\\ {-\\,\\tilde{\\mathrm{s}}}&{\\tilde{\\mathrm{c}}}\\end{array}\\big),\\big(\\begin{array}{l l}{-\\,\\tilde{\\mathrm{c}}}&{-\\tilde{\\mathrm{s}}}\\\\ {\\tilde{\\mathrm{s}}}&{\\tilde{\\mathrm{c}}}\\end{array}\\big),\\big(\\begin{array}{l l}{\\tilde{\\mathrm{c}}}&{-\\tilde{\\mathrm{s}}}\\\\ {\\tilde{\\mathrm{s}}}&{-\\tilde{\\mathrm{c}}}\\end{array}\\big)\\big\\}$ is exactly the same except for the coefficients of Problem (8), we   \n536 will provide the corresponding coefficients in Problem (8): $\\mathrm{min_{\\tilde{c},\\tilde{s}}}\\,a\\,\\tilde{\\mathrm{c}}\\,\\mathrm{+}b\\,\\tilde{\\mathrm{s}}\\,\\mathrm{+}c\\,\\tilde{\\mathrm{c}}^{2}\\,\\mathrm{+}d\\,\\tilde{\\mathrm{c}}\\,\\tilde{\\mathrm{s}}\\,\\mathrm{+}e\\,\\tilde{\\mathrm{s}}^{2}$ , and   \n537 omit the specific analysis process.   \n538 Case (a). $\\mathbf{V}=\\left(\\begin{array}{c c}{\\tilde{\\mathbf{c}}}&{-\\tilde{\\mathbf{s}}}\\\\ {-\\tilde{\\mathbf{s}}}&{\\tilde{\\mathbf{c}}}\\end{array}\\right)$ : $a=\\mathbf{P}_{11}+\\mathbf{P}_{22}$ , $b=-\\mathbf{P}_{12}-\\mathbf{P}_{21},$ , $c=\\textstyle{\\frac{1}{2}}(\\dot{\\bf Q}_{11}+\\dot{\\bf Q}_{41}+\\dot{\\bf Q}_{14}+\\dot{\\bf Q}_{44})$ ,   \n539 $\\begin{array}{r}{d=-\\frac{1}{2}(\\dot{\\bf Q}_{21}+\\dot{\\bf Q}_{31}+\\dot{\\bf Q}_{12}+\\dot{\\bf Q}_{42}+\\dot{\\bf Q}_{13}+\\dot{\\bf Q}_{43}+\\dot{\\bf Q}_{24}+\\dot{\\bf Q}_{34}),}\\end{array}$ , and $e={\\textstyle{\\frac{1}{2}}}(\\dot{\\bf Q}_{22}\\!+\\!\\dot{\\bf Q}_{32}\\!+\\!\\dot{\\bf Q}_{23}\\!+\\!\\dot{\\bf Q}_{33})$ .   \n540 Case (b). $\\mathbf{V}=\\big(\\begin{array}{c c}{-\\tilde{\\mathbf{c}}}&{-\\tilde{\\mathbf{s}}}\\\\ {\\tilde{\\mathbf{s}}}&{\\tilde{\\mathbf{c}}}\\end{array}\\big)\\mathbf{:}a=-\\mathbf{P}_{11}+\\mathbf{P}_{22}$ , $b=-\\mathbf{P}_{12}+\\mathbf{P}_{21},$ , $c=\\textstyle{\\frac{1}{2}}(\\dot{\\bf Q}_{11}-\\dot{\\bf Q}_{41}-\\dot{\\bf Q}_{14}+\\dot{\\bf Q}_{44})$ ,   \n541 $\\begin{array}{r}{d=\\frac{1}{2}(\\dot{\\bf Q}_{21}-\\dot{\\bf Q}_{31}+\\dot{\\bf Q}_{12}-\\dot{\\bf Q}_{42}-\\dot{\\bf Q}_{13}+\\dot{\\bf Q}_{43}-\\dot{\\bf Q}_{24}+\\dot{\\bf Q}_{34})}\\end{array}$ and $e=\\textstyle{\\frac{1}{2}}(\\dot{\\bf Q}_{22}-\\dot{\\bf Q}_{32}-\\dot{\\bf Q}_{23}+\\dot{\\bf Q}_{33})$ .   \n542 Case (c). $\\mathbf{V}\\,=\\,{\\bigl(}{\\tilde{\\bar{\\mathbf{c}}}}\\ -{\\tilde{\\bar{\\mathbf{c}}}}{\\bigr)}{\\colon}a\\,=\\,\\mathbf{P}_{11}\\,-\\,\\mathbf{P}_{22}$ , $b\\,=\\,-\\mathbf{P}_{12}+\\mathbf{P}_{21}$ , $c=\\textstyle{\\frac{1}{2}}(\\dot{\\bf Q}_{11}-\\dot{\\bf Q}_{41}-\\dot{\\bf Q}_{14}+\\dot{\\bf Q}_{44})$ ,   \n543 $\\begin{array}{r}{d=\\frac{1}{2}(-\\dot{\\bf Q}_{21}+\\dot{\\bf Q}_{31}-\\dot{\\bf Q}_{12}+\\dot{\\bf Q}_{42}+\\dot{\\bf Q}_{13}-\\dot{\\bf Q}_{43}+\\dot{\\bf Q}_{24}-\\dot{\\bf Q}_{34})}\\end{array}$ , and $e={\\textstyle{\\frac{1}{2}}}(\\dot{\\bf Q}_{22}-\\dot{\\bf Q}_{32}-\\dot{\\bf Q}_{23}+\\dot{\\bf Q}_{33})$ . ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "544 C Proofs for Section 2 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "545 C.1 Proof of Lemma 2.1 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "546 Proof. Defining $\\mathbf{J}_{\\mathtt{B B}}=\\mathbf{J}(\\mathbf{U}_{\\mathtt{B}},\\mathbf{U}_{\\mathtt{B}})$ , then we have: $\\mathbf{JU}_{\\mathtt{B}}=\\mathbf{U}_{\\mathtt{B}}\\mathbf{J}_{\\mathtt{B B}}$ , $\\mathbf{U}_{\\mathtt{B}}^{\\top}\\mathbf{J}=\\mathbf{J}_{\\mathtt{B B}}\\mathbf{U}_{\\mathtt{B}}^{\\top}$ , and $\\mathbf{U}_{\\mathtt{B}}^{\\top}\\mathbf{J}\\mathbf{U}_{\\mathtt{B}}=$   \n547 $\\mathbf{J}_{\\mathtt{B B}}$ . ", "page_idx": 13}, {"type": "text", "text": "548 Part (a). For any $\\mathbf{V}\\in\\mathbb{R}^{2\\times2}$ and $\\mathbb{B}\\in\\{B_{i}\\}_{i=1}^{\\mathbf{C}_{n}^{2}}$ , we have: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{[\\mathbf{X}^{+}]^{\\top}\\mathbf{J}\\mathbf{X}^{+}-\\mathbf{X}^{\\top}\\mathbf{J}\\mathbf{X}}\\\\ {\\stackrel{\\mathrm{\\scriptsize()}}{=}}&{\\mathbf{X}^{\\top}\\mathbf{J}\\mathbf{U}_{\\mathrm{B}}(\\mathbf{V}-\\mathbf{I}_{2})\\mathbf{U}_{\\mathrm{B}}^{\\top}\\mathbf{X}+[\\mathbf{U}_{\\mathrm{B}}(\\mathbf{V}-\\mathbf{I}_{2})\\mathbf{U}_{\\mathrm{B}}^{\\top}\\mathbf{X}]^{\\top}\\mathbf{J}\\mathbf{X}}\\\\ &{+[\\mathbf{U}_{\\mathrm{B}}(\\mathbf{V}-\\mathbf{I}_{2})\\mathbf{U}_{\\mathrm{B}}^{\\top}\\mathbf{X}]^{\\top}\\mathbf{J}[\\mathbf{U}_{\\mathrm{B}}(\\mathbf{V}-\\mathbf{I}_{2})\\mathbf{U}_{\\mathrm{B}}^{\\top}\\mathbf{X}]}\\\\ {=}&{\\mathbf{X}^{\\top}[\\mathbf{J}\\mathbf{U}_{\\mathrm{B}}(\\mathbf{V}-\\mathbf{I}_{2})\\mathbf{U}_{\\mathrm{B}}^{\\top}+\\mathbf{U}_{\\mathrm{B}}(\\mathbf{V}-\\mathbf{I}_{2})^{\\top}\\mathbf{U}_{\\mathrm{B}}^{\\top}\\mathbf{J}+\\mathbf{U}_{\\mathrm{B}}(\\mathbf{V}-\\mathbf{I}_{2})^{\\top}\\mathbf{U}_{\\mathrm{B}}^{\\top}\\mathbf{J}\\mathbf{U}_{\\mathrm{B}}(\\mathbf{V}-\\mathbf{I}_{2})\\mathbf{U}_{\\mathrm{B}}^{\\top}]\\mathbf{X}}\\\\ {=}&{\\mathbf{X}^{\\top}[\\mathbf{U}_{\\mathrm{B}}\\mathbf{J}_{\\mathrm{B}}(\\mathbf{V}-\\mathbf{I}_{2})\\mathbf{U}_{\\mathrm{B}}^{\\top}+\\mathbf{U}_{\\mathrm{B}}(\\mathbf{V}-\\mathbf{I}_{2})^{\\top}\\mathbf{J}_{\\mathrm{B}\\mathrm{B}}\\mathbf{U}_{\\mathrm{B}}^{\\top}+\\mathbf{U}_{\\mathrm{B}}(\\mathbf{V}-\\mathbf{I}_{2})^{\\top}\\mathbf{J}_{\\mathrm{B}}\\mathbf{(V}-\\mathbf{I}_{2})\\mathbf{U}_{\\mathrm{B}}^{\\top}]\\mathbf{X}}\\\\ {=}&{\\mathbf{X}^{\\top}\\mathbf{U}_{\\mathrm{B}}[\\mathbf{J}_{\\mathrm{B}}(\\mathbf{V}-\\mathbf{I} \n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "549 Part (b). Using the update rule for $\\mathbf{X}^{+}=\\mathbf{X}+\\mathbf{U}_{\\mathtt{B}}(\\mathbf{V}-\\mathbf{I}_{2})\\mathbf{U}_{\\mathtt{B}}^{\\top}\\mathbf{X}\\in\\mathbb{R}^{n\\times n}$ , we derive: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\|\\mathbf{X}^{+}-\\mathbf{X}\\|_{\\mathsf{F}}}&{=\\phantom{+}\\|\\mathbf{U}_{\\mathtt{B}}(\\mathbf{V}-\\mathbf{I}_{2})\\mathbf{U}_{\\mathtt{B}}^{\\top}\\mathbf{X}\\|_{\\mathsf{F}}}\\\\ &{\\overset{\\mathtt{()}}{\\le}\\phantom{+}\\|\\mathbf{U}_{\\mathtt{B}}\\|_{\\mathsf{F}}\\cdot\\|(\\mathbf{V}-\\mathbf{I}_{2})\\mathbf{U}_{\\mathtt{B}}^{\\top}\\mathbf{X}\\|_{\\mathsf{F}},}\\\\ &{\\overset{\\mathtt{()}}{\\le}\\phantom{+}\\|\\mathbf{U}_{\\mathtt{B}}\\|_{\\mathsf{F}}\\cdot\\|(\\mathbf{V}-\\mathbf{I}_{2})\\|_{\\mathsf{F}}\\cdot\\|\\mathbf{U}_{\\mathtt{B}}^{\\top}\\|_{\\mathsf{F}}\\cdot\\|\\mathbf{X}\\|_{\\mathsf{F}},}\\\\ &{\\overset{\\mathtt{()}}{=}\\phantom{+}\\|\\mathbf{V}-\\mathbf{I}_{2}\\|_{\\mathsf{F}}\\cdot\\|\\mathbf{X}\\|_{\\mathsf{F}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "550 where step $\\textcircled{1}$ and step $\\circledcirc$ use the norm inequality that $\\|\\mathbf{AX}\\|_{\\mathsf{F}}\\leq\\|\\mathbf{A}\\|_{\\mathsf{F}}\\cdot\\|\\mathbf{X}\\|_{\\mathsf{F}}$ for any $\\mathbf{A}$ and $\\mathbf{X}$ ;   \n551 step $\\circled{3}$ uses $\\|\\mathbf{U}_{\\mathtt{B}}\\|=\\|\\mathbf{U}_{\\mathtt{B}}^{\\top}\\|=1$ . ", "page_idx": 14}, {"type": "text", "text": "552 Part (c). We define $\\mathbf{Z}\\triangleq\\mathbf{U}_{\\mathtt{B}}^{\\top}\\mathbf{X}$ . We derive: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\|\\mathbf{X}^{+}-\\mathbf{X}\\|_{\\mathbf{H}}^{2}}&{=\\phantom{\\sum_{\\mathbf{\\theta}}}\\|\\mathbf{U}_{\\mathrm{B}}(\\mathbf{V}-\\mathbf{I}_{2})\\mathbf{Z}\\|_{\\mathbf{H}}^{2}}\\\\ &{\\overset{\\mathrm{()}}{=}\\phantom{\\sum_{\\mathbf{\\theta}}}\\mathrm{vec}(\\mathbf{U}_{\\mathrm{B}}(\\mathbf{V}-\\mathbf{I}_{2})\\mathbf{Z})^{\\top}\\mathbf{H}\\mathrm{vec}(\\mathbf{U}_{\\mathrm{B}}(\\mathbf{V}-\\mathbf{I}_{2})\\mathbf{Z})}\\\\ &{\\overset{\\mathrm{()}}{=}\\phantom{\\sum_{\\mathbf{\\theta}}}\\mathrm{vec}(\\mathbf{V}-\\mathbf{I}_{2})^{\\top}(\\mathbf{Z}^{\\top}\\otimes\\mathbf{U}_{\\mathrm{B}})^{\\top}\\mathbf{H}(\\mathbf{Z}^{\\top}\\otimes\\mathbf{U}_{\\mathrm{B}})\\mathrm{vec}(\\mathbf{V}-\\mathbf{I}_{2})}\\\\ &{=\\phantom{\\sum_{\\mathbf{\\theta}}}\\|\\mathbf{V}-\\mathbf{I}_{2}\\|_{(\\mathbf{Z}^{\\top}\\otimes\\mathbf{U}_{\\mathrm{B}})^{\\top}\\mathbf{H}(\\mathbf{Z}^{\\top}\\otimes\\mathbf{U}_{\\mathrm{B}})}^{2}}\\\\ &{\\overset{\\mathrm{()}}{\\leq}\\phantom{\\sum_{\\mathbf{\\theta}}}\\|\\mathbf{V}-\\mathbf{I}_{2}\\|_{\\mathbf{Q}}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "553 where step $\\textcircled{1}$ uses $\\|\\mathbf{X}\\|_{\\mathbf{H}}^{2}=\\operatorname{vec}(\\mathbf{X})^{\\top}\\mathbf{H}\\operatorname{vec}(\\mathbf{X})$ ; step $\\circledcirc$ uses $(\\mathbf{Z}^{\\top}\\otimes\\mathbf{R})\\mathrm{vec}(\\mathbf{U})=\\mathrm{vec}(\\mathbf{R}\\mathbf{U}\\mathbf{Z})$ for   \n554 all $\\mathbf{R}$ , $\\mathbf{Z}$ and $\\mathbf{U}$ of suitable dimensions; step $\\circled{3}$ uses the choice of $\\mathbf{Q}\\succcurlyeq\\mathbf{\\underline{{Q}}}\\triangleq(\\mathbf{Z}^{\\top}\\otimes\\mathbf{U_{\\mathrm{B}}})^{\\top}\\mathbf{H}(\\mathbf{Z}^{\\top}\\otimes$   \n555 $\\mathbf{U_{B}}_{\\mathrm{.}}$ ). \u53e3 ", "page_idx": 14}, {"type": "text", "text": "556 C.2 Proof of Lemma 2.3 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "557 Proof. We denote $w=c+e$ . According to the properties of trigonometric functions, we have: (i)   \n558 $\\begin{array}{r}{\\tilde{\\mathrm{c}}^{2}=\\frac{1}{1-\\tilde{\\mathrm{t}}^{2}}}\\end{array}$ ; ( $\\begin{array}{r}{\\ddot{u})\\,\\tilde{\\mathrm{s}}^{2}=\\frac{\\tilde{\\mathrm{t}}^{2}}{1-\\tilde{\\mathrm{t}}^{2}}}\\end{array}$ 1t\u02dc\u2212t\u02dc2 ; (iii)\u02dct = \u02dcc\u02dcs, leading to: c\u02dc = \u221a $\\begin{array}{r}{\\tilde{\\mathrm{c}}=\\frac{\\pm1}{\\sqrt{1-\\tilde{\\mathrm{t}}^{2}}},\\tilde{\\mathrm{s}}=\\frac{\\pm\\,\\tilde{\\mathrm{t}}}{\\sqrt{1-\\tilde{\\mathrm{t}}^{2}}}}\\end{array}$ =\u221a1\u00b1\u2212t\u02dct\u02dc2 with |\u02dct | < 1. ", "page_idx": 14}, {"type": "text", "text": "559 We discuss two cases for Problem (8). ", "page_idx": 14}, {"type": "text", "text": "560 Case ${\\bf\\Pi}({\\pmb a})$ . $\\begin{array}{r}{\\tilde{\\mathrm{c}}=\\frac{1}{\\sqrt{1-\\tilde{\\mathrm{t}}^{2}}},\\tilde{\\mathrm{s}}=\\frac{\\tilde{\\mathrm{t}}}{\\sqrt{1-\\tilde{\\mathrm{t}}^{2}}}}\\end{array}$ Problem (8) is equivalent to the following problem: $\\bar{\\mu}_{+}\\ =$ $\\begin{array}{r}{\\arg\\operatorname*{min}_{\\mu}\\frac{a+\\tilde{\\mathbf{t}}\\,b}{\\sqrt{1-\\tilde{\\mathbf{t}}^{2}}}+\\frac{w+\\tilde{\\mathbf{t}}\\,d}{1-\\tilde{\\mathbf{t}}^{2}}-e}\\end{array}$ . Therefore, the optimal solution $\\bar{\\mu}_{+}$ can be computed as: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\cosh(\\bar{\\mu}_{+})=\\frac{1}{\\sqrt{1-(\\bar{t}_{+})^{2}}},\\ \\mathrm{{and}}\\ \\sinh(\\bar{\\mu}_{+})=\\frac{\\bar{t}_{+}}{\\sqrt{1-(\\bar{t}_{+})^{2}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "562 Case $(b)$ . $\\begin{array}{r}{\\tilde{\\mathrm{c}}=\\frac{-1}{\\sqrt{1-\\tilde{\\mathrm{t}}^{2}}},\\tilde{\\mathrm{s}}=\\frac{-\\tilde{\\mathrm{t}}}{\\sqrt{1-\\tilde{\\mathrm{t}}^{2}}}}\\end{array}$ . Problem (8) is equivalent to the following problem: $\\bar{\\mu}_{-}\\,=$ $\\begin{array}{r}{\\arg\\operatorname*{min}_{\\mu}{\\frac{-a-\\tilde{\\mathbf{t}}\\,b}{\\sqrt{1-\\tilde{\\mathbf{t}}^{2}}}}+{\\frac{w+\\tilde{\\mathbf{t}}\\,d}{1-\\tilde{\\mathbf{t}}^{2}}}-e}\\end{array}$ . Therefore, the optimal solution $\\bar{\\mu}_{-}$ can be computed as: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\cosh(\\bar{\\mu}_{-})=\\frac{-1}{\\sqrt{1-(\\bar{t}_{-})^{2}}},\\;\\mathrm{and}\\;\\sinh(\\bar{\\mu}_{-})=\\frac{-\\bar{t}_{-}}{\\sqrt{1-(\\bar{t}_{-})^{2}}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "564 We define the objective function as: $\\check{F}(\\tilde{c},\\tilde{s})\\triangleq a\\tilde{c}+b\\tilde{s}+c\\tilde{c}^{2}+d\\tilde{c}\\tilde{s}+e\\tilde{s}^{2}$ . In view of (14) and (15),   \n565 the optimal solution pair $[c o s h(\\bar{\\mu},s i n h(\\bar{\\mu})]$ for problem (8) can be computed as: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{sh}(\\bar{\\mu}),\\mathrm{sinh}(\\bar{\\mu})]=\\arg\\underset{[c,s]}{\\operatorname*{min}}\\,\\breve{F}(c,s),}\\\\ &{\\mathrm{t.}\\,\\,[c,s]\\in\\{[\\mathrm{cosh}(\\bar{\\mu}_{+}),\\mathrm{sinh}(\\bar{\\mu}_{+})],[\\mathrm{cosh}(\\bar{\\mu}_{-}),\\mathrm{sinh}(\\bar{\\mu}_{-})]\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "566 Importantly, it is not necessary to compute the values $\\bar{\\mu}_{+}$ for (14) and $\\bar{\\mu}_{-}$ for (15).   \n567 ", "page_idx": 15}, {"type": "text", "text": "568 C.3 Proof of Lemma 2.4 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Proof. The objective function for $\\mathsf{B}_{(i)}^{t}$ as in Equation (3) is formulated as : ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{f(\\mathbf{X}^{t})+\\frac{1}{2}\\|\\mathbf{V}_{i}-\\mathbf{I}\\|_{\\mathbf{Q}+\\theta\\mathbf{I}}^{2}+\\langle\\mathbf{V}_{i}-\\mathbf{I},[\\nabla f(\\mathbf{X}^{t})(\\mathbf{X}^{t})^{\\mathsf{T}}]_{\\mathbf{B}_{(i)}^{t}\\mathbf{B}_{(i)}^{t}}\\rangle}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "569 Part (1). For the part of $\\begin{array}{r}{\\frac{1}{2}\\|\\mathbf{V}_{i}-\\mathbf{I}\\|_{\\mathbf{Q}+\\theta\\mathbf{I}}^{2}}\\end{array}$ , it is obviously irrelevant. ", "page_idx": 15}, {"type": "text", "text": "570 Part (2). For the part of $\\langle\\mathbf{V}_{i}-\\mathbf{I},[\\nabla f(\\mathbf{X}^{t})(\\mathbf{X}^{t})^{\\mathsf{T}}]_{\\mathtt{B}_{(i)}^{t}\\mathtt{B}_{(i)}^{t}}\\rangle$ , we note that $\\begin{array}{r l}{[\\nabla f(\\mathbf{X}^{t})(\\mathbf{X}^{t})^{\\top}]_{\\mathbf{B}_{(i)}^{t}\\mathbf{B}_{(i)}^{t}}=}&{{}}\\end{array}$   \n571 $[\\nabla f({\\mathbf{X}}^{t})](\\mathbf{B}_{(i)}^{t},:)[({\\mathbf{X}}^{t})^{\\top}](:,\\mathbf{B}_{(i)}^{t})=[\\nabla f({\\mathbf{X}}^{t})](\\mathbf{B}_{(i)}^{t},:)[({\\mathbf{X}}^{t})(\\mathbf{B}_{(i)}^{t},:)]^{\\top}$ , which just use the informa  \n572 tion of block $\\mathsf{B}_{(i)}^{t}$ . The proof ends. \u53e3 ", "page_idx": 15}, {"type": "text", "text": "573 C.4 Proof of Lemma 2.5 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "574 Proof. Part ${\\bf\\Psi}({\\pmb a})$ . For the purpose of analysis, we define the following: $\\begin{array}{r}{\\forall i\\in[\\frac{n}{2}],\\mathbf{K}_{i}=\\mathbf{U}_{\\mathtt{B}_{(i)}}(\\mathbf{V}_{i}-}\\end{array}$   \n575 $\\mathbf{I}_{2})\\mathbf{U}_{\\mathtt{B}_{(i)}}^{\\top}\\mathbf{X}$ . ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{||\\sum_{i=1}^{\\frac{n}{2}}[\\mathbf{U}_{\\mathbf{B}_{(i)}}(\\mathbf{V}_{i}-\\mathbf{I}_{2})\\mathbf{U}_{\\mathbf{B}_{(i)}}^{\\top}\\mathbf{X}]||_{\\mathsf{F}}^{2}}&{\\triangleq}\\\\ {||\\sum_{i=1}^{\\frac{n}{2}}[\\mathbf{U}_{\\mathbf{B}_{(i)}}(\\mathbf{V}_{i}-\\mathbf{I}_{2})\\mathbf{U}_{\\mathbf{B}_{(i)}}^{\\top}\\mathbf{X}]||_{\\mathsf{F}}^{2}}&{\\triangleq}\\\\ {\\triangleq}&{\\sum_{i=1}^{\\frac{n}{2}}[||\\mathbf{U}_{\\mathbf{B}_{(i)}}(\\mathbf{V}_{i}-\\mathbf{I}_{2})\\mathbf{U}_{\\mathbf{B}_{(i)}}^{\\top}\\mathbf{X}||_{\\mathsf{F}}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "576 where step $\\textcircled{1}$ uses the definition of $\\mathbf{K}_{i}$ and the assumption that $\\mathtt{B}\\in\\Upsilon$ ; step $\\circledcirc$ uses the definition of   \n577 Squared Frobenius Norm; step $\\circled{3}$ uses the definition of $\\mathbf{K}_{i}$ .   \n578 Part $\\mathbf{\\eta}(b)$ . Using the update rule for $\\begin{array}{r}{{\\mathbf{X}}^{+}={\\mathbf{X}}+[\\sum_{i=1}^{n/2}{\\mathbf{U}}_{\\mathbf{B}_{(i)}}({\\mathbf{V}}_{i}-{\\mathbf{I}}_{2}){\\mathbf{U}}_{\\mathbf{B}_{(i)}}^{\\top}]{\\mathbf{X}}\\in\\mathbb{R}^{n\\times n},}\\end{array}$ , we have the   \n579 following inequalities: ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\|{\\bf X}^{+}-{\\bf X}\\|_{\\mathsf F}^{2}}&{=}&{\\|[\\sum_{i=1}^{n/2}{\\bf U}_{{\\tt B}_{(i)}}({\\bf V}_{i}-{\\bf I}_{2}){\\bf U}_{{\\tt B}_{(i)}}^{\\top}]{\\bf X}\\|_{\\mathsf F}^{2}}\\\\ &{\\overset\\textcircled{\\Updownarrow}{=}}&{\\sum_{i=1}^{n/2}\\|[{\\bf U}_{{\\tt B}_{(i)}}({\\bf V}_{i}-{\\bf I}_{2}){\\bf U}_{{\\tt B}_{(i)}}^{\\top}]{\\bf X}\\|_{\\mathsf F}^{2}}\\\\ &{\\overset{\\textcircled{\\circ}}{\\leq}}&{\\sum_{i=1}^{n/2}\\|{\\bf V}_{i}-{\\bf I}_{2}\\|_{\\mathsf F}^{2}\\cdot\\|{\\bf X}\\|_{\\mathsf F}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "580 where step $\\textcircled{1}$ uses the conclusion of Part ${\\bf\\Psi}({\\pmb a})$ ; step $\\circledcirc$ uses the same proof process of Part $(b)$ of lemma   \n581 2.1. ", "page_idx": 15}, {"type": "text", "text": "Part $(c)$ . We derive the following results: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{2}\\|\\mathbf{X}^{+}-\\mathbf{X}\\|_{\\mathbf{H}}^{2}=\\frac{1}{2}\\|\\big[\\sum_{i=1}^{n/2}\\mathbf{U}_{\\mathtt{B}_{(i)}}(\\mathbf{V}_{i}-\\mathbf{I}_{2})\\mathbf{U}_{\\mathtt{B}_{(i)}}^{\\top}\\big]\\mathbf{X}\\|_{\\mathbf{H}}^{2}}\\\\ &{\\qquad\\qquad\\qquad\\overset{\\textcircled{3}}{=}\\frac{1}{2}\\sum_{i=1}^{n/2}\\|\\big[\\mathbf{U}_{\\mathtt{B}_{(i)}}(\\mathbf{V}_{i}-\\mathbf{I}_{2})\\mathbf{U}_{\\mathtt{B}_{(i)}}^{\\top}\\big]\\mathbf{X}\\|_{\\mathbf{H}}^{2}}\\\\ &{\\qquad\\qquad\\qquad\\overset{\\textcircled{2}}{\\leq}\\frac{1}{2}\\sum_{i=1}^{n/2}\\|\\mathbf{V}_{i}-\\mathbf{I}_{2}\\|_{\\mathbf{Q}}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "582 where step $\\textcircled{1}$ uses the conclusion of Part ${\\bf\\Psi}({\\pmb a})$ ; step $\\circledcirc$ uses the same proof process of Part $(c)$ of lemma   \n583 2.1. ", "page_idx": 16}, {"type": "text", "text": "584 Part $(\\pmb{d})$ . We derive the following results: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\sum_{i=1}^{n/2}\\langle\\mathbf{V}_{i}-\\mathbf{I}_{2},[(\\nabla f(\\mathbf{X})-\\tilde{\\mathbf{G}})\\mathbf{X}^{\\top}]_{\\mathrm{B}_{i}\\mathrm{B}_{i}}\\rangle}\\\\ {=}&{\\sum_{i=1}^{n/2}\\langle[\\mathbf{U}_{\\mathrm{B}_{(i)}}(\\mathbf{V}_{i}-\\mathbf{I}_{2})\\mathbf{U}_{\\mathrm{B}_{(i)}}^{\\top}]\\mathbf{X},[(\\nabla f(\\mathbf{X})-\\tilde{\\mathbf{G}})]\\rangle}\\\\ {=}&{\\langle\\mathbf{X}^{+}-\\mathbf{X},[(\\nabla f(\\mathbf{X})-\\tilde{\\mathbf{G}})]\\rangle}\\\\ {\\stackrel{\\mathrm{O}}{\\leq}}&{\\frac{1}{2}\\|\\mathbf{X}^{+}-\\mathbf{X}\\|_{\\mathsf{F}}^{2}+\\frac{1}{2}\\|[\\nabla f(\\mathbf{X})-\\tilde{\\mathbf{G}}]\\|_{\\mathsf{F}}^{2}}\\\\ {\\stackrel{\\mathrm{O}}{\\leq}}&{\\frac{1}{2}\\|\\mathbf{X}\\|_{\\mathsf{F}}^{2}\\sum_{i=1}^{n/2}\\|\\mathbf{V}_{i}-\\mathbf{I}_{2}\\|_{\\mathsf{F}}^{2}+\\frac{1}{2}\\|[\\nabla f(\\mathbf{X})-\\tilde{\\mathbf{G}}]\\|_{\\mathsf{F}}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "585 where step $\\textcircled{1}$ uses \u2200A, B, $\\begin{array}{r}{,\\frac{1}{2}\\|\\mathbf{A}-\\mathbf{B}\\|_{\\mathsf{F}}^{2}=\\frac{1}{2}\\|\\mathbf{A}\\|_{\\mathsf{F}}^{2}+\\frac{1}{2}\\|\\mathbf{B}\\|_{\\mathsf{F}}^{2}-\\langle\\mathbf{A},\\mathbf{B}\\rangle\\ge0}\\end{array}$ , with $\\mathbf{A}=\\lVert\\mathbf{X}^{+}-\\mathbf{X}\\rVert_{\\mathsf{F}}^{2}$   \n586 and $\\mathbf{B}=\\|[\\nabla f(\\mathbf{X})-\\tilde{\\mathbf{G}}]\\|_{\\mathsf{F}}^{2}$ ; step $\\circledcirc$ uses the conclusion of Part $(b)$ . \u53e3 ", "page_idx": 16}, {"type": "text", "text": "587 D Proofs for Section 3 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "588 D.1 Proof of Lemma 3.1 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "589 Proof. We consider the Lagrangian function of problem (1): ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}(\\mathbf{X},\\boldsymbol{\\Lambda})=f(\\mathbf{X})-\\frac{1}{2}\\langle\\boldsymbol{\\Lambda},\\mathbf{X}^{\\mathsf{T}}\\mathbf{J}\\mathbf{X}-\\mathbf{J}\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "590 Setting the gradient of $\\mathcal{L}(\\mathbf{X},\\boldsymbol{\\Lambda})\\ w.r.t.\\ \\mathbf{X}$ to zero yields: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\nabla f(\\mathbf{X})-\\mathbf{J}\\mathbf{X}\\boldsymbol{\\Lambda}=\\mathbf{0}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "591 Part (a). Multiplying both sides by $\\mathbf{X^{\\top}}$ and using the fact that $\\mathbf{X}^{\\mathsf{T}}\\mathbf{J}\\mathbf{X}\\,=\\,\\mathbf{J},$ , we have $\\mathbf{J}\\Lambda\\,=$   \n592 $\\mathbf{X}^{\\top}\\nabla f(\\mathbf{X})$ . Multiplying both sides by $\\mathbf{J}^{\\mathsf{T}}$ and using $\\mathbf{\\breve{J}}^{\\top}\\mathbf{J}=\\mathbf{I},$ , we have $\\Lambda=\\mathbf{J}\\mathbf{X}^{\\top}\\nabla f(\\mathbf{X})$ . Since $\\Lambda$   \n593 is symmetric, we have $\\boldsymbol{\\Lambda}=\\nabla f(\\mathbf{X})^{\\sf T}\\mathbf{X}\\mathbf{J}$ . Putting this equality into Equality (21) yields the following   \n594 first-order optimality condition for Problem (1): ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\nabla f(\\mathbf{X})=\\mathbf{JX}[\\nabla f(\\mathbf{X})]^{\\mathsf{T}}\\mathbf{X}\\mathbf{J}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "595 Part (b). We let $\\mathbf{G}=\\nabla f(\\mathbf{X})$ . We derive the following results: ", "page_idx": 16}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}{\\mathbf{G}=\\mathbf{JXG}^{\\mathsf{T}}\\mathbf{X}\\mathbf{J}}&{{\\stackrel{\\mathrm{()}}{=}}}&{\\mathbf{JX}^{\\mathsf{T}}\\cdot\\mathbf{G}=\\mathbf{JX}^{\\mathsf{T}}\\cdot\\mathbf{JXG}^{\\mathsf{T}}\\mathbf{XJ}}\\\\ &{{\\stackrel{\\mathrm{()}}{=}}}&{\\mathbf{JX}^{\\mathsf{T}}\\mathbf{G}=\\mathbf{G}^{\\mathsf{T}}\\mathbf{XJ}}\\\\ &{{\\stackrel{\\mathrm{()}}{\\to}}}&{\\mathbf{X}(\\mathbf{JX}^{\\mathsf{T}}\\mathbf{G})\\mathbf{X}^{\\mathsf{T}}=\\mathbf{X}(\\mathbf{G}^{\\mathsf{T}}\\mathbf{X}\\mathbf{J})\\mathbf{X}^{\\mathsf{T}}}\\\\ &{{\\stackrel{\\mathrm{()}}{\\to}}}&{\\mathbf{X}\\underbrace{\\mathbf{JX}^{\\mathsf{T}}\\mathbf{G}\\mathbf{X}^{\\mathsf{T}}\\mathbf{J}}_{\\triangleq\\mathbf{G}^{\\mathsf{T}}}\\mathbf{J}=\\mathbf{J}\\underbrace{\\mathbf{JXG}^{\\mathsf{T}}\\mathbf{X}\\mathbf{J}}_{\\triangleq\\mathbf{G}}\\mathbf{X}^{\\mathsf{T}}}\\\\ &{{\\stackrel{\\mathrm{()}}{\\to}}}&{(\\mathbf{XG}^{\\mathsf{T}}\\mathbf{J})\\cdot\\mathbf{JX}=(\\mathbf{JG}\\mathbf{X}^{\\mathsf{T}})\\cdot\\mathbf{JX}}\\\\ &{{\\stackrel{\\mathrm{()}}{\\to}}}&{\\mathbf{XG}^{\\mathsf{T}}\\mathbf{X}=\\mathbf{JG}}\\\\ &{{\\stackrel{\\mathrm{()}}{\\to}}}&{\\mathbf{JXG}^{\\mathsf{T}}\\mathbf{X}\\mathbf{J}=\\mathbf{G},}\\end{array}}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "596 where step $\\textcircled{1}$ uses the results of left-multiplying both sides by $\\mathbf{JX^{\\top}}$ ; step $\\circledcirc$ uses $\\mathbf{J}\\!\\cdot\\!\\mathbf{X}^{\\mathsf{T}}\\mathbf{J}\\mathbf{X}=\\mathbf{J}\\mathbf{J}=\\mathbf{I}$ ;   \n597 step $\\circled{3}$ uses the results of left-multiplying both sides by $\\mathbf{X}$ and subsequently right-multiplying them   \n598 by $\\mathbf{\\dot{X}^{\\top}}$ ; $\\circledast$ uses $\\mathbf{G}=\\mathbf{J}\\mathbf{X}\\mathbf{G}^{\\mathsf{T}}\\mathbf{X}\\mathbf{J}$ ; step $\\textcircled{5}$ uses the the results of right-multiplying both sides by JX;   \n599 step $\\circled{6}$ uses $\\mathbf{J}\\mathbf{J}=\\mathbf{I}$ and $\\mathbf{X}^{\\mathsf{T}}\\mathbf{J}\\mathbf{X}=\\mathbf{\\bar{J}}$ ; step $\\circledcirc$ uses the results of left-multiply both sides by $\\mathbf{J}$ and   \n600 right-multiplied by $\\mathbf{J}$ .   \n601 Given Equality (23), we conclude that the critical point condition is equivalent to the requirement   \n602 that the matrix $\\mathbf{X}\\nabla f(\\Breve{\\mathbf{X}})^{\\top}\\mathbf{J}$ is symmetric, which is expressed as $\\mathbf{X}\\mathbf{G}^{\\mathsf{T}}\\mathbf{J}=[\\mathbf{X}\\mathbf{G}^{\\mathsf{T}}\\mathbf{J}]^{\\mathsf{T}}$ . \u53e3 ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "603 D.2 Proof of Theorem 3.3 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "604 Proof. We use $\\ddot{\\mathbf{X}}$ and $\\check{\\mathbf{X}}$ to denote any BS-point and critical point, respectively. ", "page_idx": 16}, {"type": "text", "text": "605 For all $\\mathsf{B}\\in\\Omega\\triangleq\\{\\beta_{1},\\beta_{2},\\hdots,\\beta_{\\mathrm{C}_{n}^{2}}\\}$ , we have: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbf{I}_{2}\\in\\arg\\operatorname*{min}_{\\mathbf{V}\\in\\mathcal{I}_{\\mathtt{B}}}\\mathcal{G}(\\mathbf{V};\\ddot{\\mathbf{X}},\\mathtt{B}).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{G}(\\mathbf{V};\\mathbf{X},\\mathbf{B})\\triangleq f(\\mathbf{X})+\\frac{1}{2}\\|\\mathbf{V}-\\mathbf{I}_{2}\\|_{\\mathbf{Q}+\\theta\\mathbf{I}}^{2}+\\langle\\mathbf{V}-\\mathbf{I},[\\nabla f(\\mathbf{X})(\\mathbf{X})^{\\mathsf{T}}]_{\\mathbf{B}\\mathbf{B}}\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "607 The Euclidean gradient of $\\mathcal{G}(\\mathbf{V};\\ddot{\\mathbf{X}},\\mathtt{B})$ can be computed as: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\ddot{\\mathbf{G}}\\triangleq\\mathrm{mat}((\\mathbf{Q}+\\theta\\mathbf{I}_{2})\\,\\mathrm{vec}(\\mathbf{V}-\\mathbf{I}_{2}))+[\\nabla f(\\ddot{\\mathbf{X}})(\\ddot{\\mathbf{X}})^{\\top}]\\mathbf{\\mathfrak{x}}\\mathbf{.}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "608 Given Lemma 3.1, we set the Riemannian gradient of $\\mathcal{G}(\\mathbf{V};\\ddot{\\mathbf{X}},\\mathtt{B})$ w.r.t. $\\mathbf{V}$ to zero, leading to the   \n609 following first-order optimality condition: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbf{0}=\\nabla_{\\mathcal{I}}\\mathcal{G}(\\mathbf{V};\\ddot{\\mathbf{X}},\\mathbf{B})=\\ddot{\\mathbf{G}}-\\mathbf{U}_{\\mathrm{B}}^{\\top}\\mathbf{J}\\mathbf{V}\\ddot{\\mathbf{G}}^{\\top}\\mathbf{V}\\mathbf{J}\\mathbf{U}_{\\mathrm{B}}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "610 Letting $\\mathbf{V}=\\mathbf{I}_{2}$ , and using the definition of G\u00a8, we have: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{0}_{2,2}=[\\nabla f(\\mathbf{X})(\\mathbf{X})^{\\top}]_{\\mathbb{B}8}-\\mathbf{J}_{\\mathbb{B}8}\\ddot{\\mathbf{G}}^{\\top}\\mathbf{J}_{\\mathbb{B}8},\\;\\forall\\mathbb{B}\\in\\{\\mathcal{B}_{i}\\}_{i=1}^{\\mathbf{C}_{n}^{2}}}\\\\ {\\Rightarrow}&{\\mathbf{0}_{2,2}=\\mathbf{U}_{\\mathbb{B}}^{\\top}[\\nabla f(\\ddot{\\mathbf{X}})\\ddot{\\mathbf{X}}^{\\top}]\\mathbf{U}_{\\mathbb{B}}-\\mathbf{J}_{\\mathbb{B}8}\\mathbf{U}_{\\mathbb{B}}^{\\top}[\\ddot{\\mathbf{X}}\\nabla f(\\ddot{\\mathbf{X}})^{\\top}]\\mathbf{U}_{\\mathbb{B}}\\mathbf{J}_{\\mathbb{B}8},\\;\\forall\\mathbb{B}\\in\\{\\mathcal{B}_{i}\\}_{i=1}^{\\mathbf{C}_{n}^{2}}}\\\\ {\\overset{\\textcircledast}{\\Rightarrow}}&{\\mathbf{0}_{2,2}=\\mathbf{U}_{\\mathbb{B}}^{\\top}[\\nabla f(\\ddot{\\mathbf{X}})\\ddot{\\mathbf{X}}^{\\top}]\\mathbf{U}_{\\mathbb{B}}-\\mathbf{U}_{\\mathbb{B}}^{\\top}\\mathbf{J}[\\ddot{\\mathbf{X}}\\nabla f(\\ddot{\\mathbf{X}})^{\\top}]\\mathbf{J}\\mathbf{U}_{\\mathbb{B}},\\;\\forall\\mathbb{B}\\in\\{\\mathcal{B}_{i}\\}_{i=1}^{\\mathbf{C}_{n}^{2}}}\\\\ {\\overset{\\textcircledast}{\\Rightarrow}}&{\\mathbf{0}_{n,n}=[\\nabla f(\\ddot{\\mathbf{X}})\\ddot{\\mathbf{X}}^{\\top}]-\\mathbf{J}[\\ddot{\\mathbf{X}}\\nabla f(\\ddot{\\mathbf{X}})^{\\top}]\\mathbf{J},}\\\\ {\\overset{\\textcircledast}{\\Rightarrow}}&{[\\mathbf{J}\\nabla f(\\ddot{\\mathbf{X}})\\ddot{\\mathbf{X}}^{\\top}]=[\\mathbf{J}\\nabla f(\\ddot{\\mathbf{X}})\\ddot{\\mathbf{X}}^{\\top}]^{\\top},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "611 where step $\\textcircled{1}$ uses $\\mathbf{U}_{\\mathtt{B}}^{\\top}\\mathbf{J}=\\mathbf{J}_{\\mathtt{B B}}\\mathbf{U}_{\\mathtt{B}}^{\\top}$ and $\\mathbf{JU}_{\\mathtt{B}}=\\mathbf{U}_{\\mathtt{B}}\\mathbf{J}_{\\mathtt{B B}}$ ; step $\\circledcirc$ uses the the following results for any   \n612 $\\mathbf{W}\\in\\mathbb{R}^{n\\times n}$ : ", "page_idx": 17}, {"type": "equation", "text": "$$\n(\\forall\\mathbb{B}\\in\\{\\mathcal{B}_{i}\\}_{i=1}^{\\mathbf{C}_{n}^{2}},\\mathbf{0}_{2,2}=\\mathbf{U}_{\\mathtt{B}}^{\\top}\\mathbf{W}\\mathbf{U}_{\\mathtt{B}}=\\mathbf{W}_{\\mathtt{B B}})\\Rightarrow(\\mathbf{W}=\\mathbf{0}_{n,n});\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "613 step $\\circled{3}$ uses the fact that both sides are left-multiplied by $\\mathbf{J}$ . We conclude that the matrix $\\mathbf{J}\\nabla f({\\ddot{\\mathbf{X}}}){\\ddot{\\mathbf{X}}}^{\\top}$   \n614 is symmetric. Using Claim $(b)$ of Lemma 3.1, we conclude that $\\ddot{\\mathbf{X}}$ is a also a critical point.   \n615 Notably, the condition in Equation (25) is a necessary but not sufficient condition. This is because   \n616 BS-point is the global minimum of Problem: $\\arg\\operatorname*{min}_{\\mathbf{V}\\in\\mathcal{I}_{\\mathtt{B}}}\\mathcal{G}(\\mathbf{V};\\ddot{\\mathbf{X}},\\mathtt{B})$ , according to Definition   \n617 3.2. \u53e3 ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "618 E Proofs for Section 4 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "619 E.1 Proof of Lemma 4.5 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "620 Proof. By the definition of $\\tilde{\\mathbf{G}}^{t}$ , we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\iota^{t}}[\\|\\tilde{\\mathbf{G}}^{t}-\\nabla f(\\mathbf{X}^{t})\\|_{\\mathsf{F}}^{2}]}\\\\ {\\overset{\\textcircled{\\geq}}{=}}&{p\\mathbb{E}_{\\iota^{t}}[\\|\\frac{1}{b}\\sum_{i=1}^{b}\\nabla f_{i}(\\mathbf{X}^{t})-\\nabla f(\\mathbf{X}^{t})\\|_{\\mathsf{F}}^{2}]+}\\\\ &{(1-p)\\mathbb{E}_{\\iota^{t}}[\\|\\tilde{\\mathbf{G}}^{t-1}+\\frac{1}{b^{\\prime}}\\sum_{i=1}^{b^{\\prime}}(\\nabla f_{i}(\\mathbf{X}^{t})-\\nabla f_{i}(\\mathbf{X}^{t-1}))-\\nabla f(\\mathbf{X}^{t})\\|_{\\mathsf{F}}^{2}]}\\\\ {\\overset{\\textcircled{\\geq}}{=}}&{p\\mathbb{E}_{\\iota^{t}}[\\|\\frac{1}{b}\\sum_{i=1}^{b}\\nabla f_{i}(\\mathbf{X}^{t})-\\nabla f(\\mathbf{X}^{t})\\|_{\\mathsf{F}}^{2}]+(1-p)\\mathbb{E}_{\\iota^{t-1}}[\\|\\tilde{\\mathbf{G}}^{t-1}-\\nabla f(\\mathbf{X}^{t-1})\\|_{\\mathsf{F}}^{2}]}\\\\ &{+(1-p)\\mathbb{E}_{\\iota^{t}}[\\|\\frac{1}{b^{\\prime}}\\sum_{i=1}^{b^{\\prime}}(\\nabla f_{i}(\\mathbf{X}^{t})-\\nabla f_{i}(\\mathbf{X}^{t-1}))-\\nabla f(\\mathbf{X}^{t})+\\nabla f(\\mathbf{X}^{t-1})\\|_{\\mathsf{F}}^{2}]}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "621 where step $\\textcircled{1}$ uses formula (9); step $\\circledcirc$ uses that $\\tilde{\\mathbf{G}}^{t-1}-\\nabla f(\\mathbf{X}^{t-1})$ is measurable w.r.t. $\\iota^{t-1}$ and   \n622 $\\begin{array}{r l}&{\\mathbb{E}_{\\iota^{t}}[\\|\\frac{1}{b^{\\prime}}\\sum_{i=1}^{b^{\\prime}}(\\nabla f_{i}(\\mathbf{X}^{t})-\\nabla f_{i}(\\mathbf{X}^{t-1}))-\\nabla f(\\mathbf{X}^{t})+\\nabla f(\\mathbf{X}^{t-1})\\|_{\\mathsf{F}}^{2}]=0}\\end{array}$ . We further have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\varepsilon^{\\ell}}[\\|\\widetilde{\\mathbf{G}}^{t}-\\nabla f(\\mathbf{X}^{t})\\|_{\\mathsf{F}}^{2}]}\\\\ {\\overset{\\circ}{\\leq}}&{p\\mathbb{E}_{\\varepsilon^{\\ell}}[\\|\\frac{1}{b}\\sum_{i=1}^{b}\\nabla f_{i}(\\mathbf{X}^{t})-\\nabla f(\\mathbf{X}^{t})\\|_{\\mathsf{F}}^{2}]+(1-p)\\mathbb{E}_{\\varepsilon^{-1}}[\\|\\widetilde{\\mathbf{G}}^{t-1}-\\nabla f(\\mathbf{X}^{t-1})\\|_{\\mathsf{F}}^{2}]}\\\\ &{+(1-p)\\mathbb{E}_{\\varepsilon^{\\ell}}[\\|\\frac{1}{b^{\\ell}}\\sum_{i=1}^{b^{\\ell}}(\\nabla f_{i}(\\mathbf{X}^{t})-\\nabla f_{i}(\\mathbf{X}^{t-1}))\\|_{\\mathsf{F}}^{2}]}\\\\ {\\overset{\\circ}{\\leq}}&{\\frac{p(N-b)}{b(N-1)}\\mathbb{E}_{\\varepsilon^{\\ell}}[\\|\\nabla f_{i}(\\mathbf{X}^{t})-\\nabla f(\\mathbf{X}^{t})\\|_{\\mathsf{F}}^{2}]+(1-p)\\mathbb{E}_{\\varepsilon^{-1}}\\|\\widetilde{\\mathbf{G}}^{t-1}-\\nabla f(\\mathbf{X}^{t-1})\\|_{\\mathsf{F}}^{2}]}\\\\ &{+\\frac{1-p}{b^{\\ell}}\\mathbb{E}_{\\varepsilon^{-1}}[\\|\\nabla f_{i}(\\mathbf{X}^{t})-\\nabla f_{i}(\\mathbf{X}^{t-1})\\|_{\\mathsf{F}}^{2}]}\\\\ {\\overset{\\circ}{\\leq}}&{p(N-b)\\sigma^{2}+(1-p)\\mathbb{E}_{\\varepsilon^{\\ell-1}}[\\|\\widetilde{\\mathbf{G}}^{t-1}-\\nabla f(\\mathbf{X}^{t-1})\\|_{\\mathsf{F}}^{2}]}\\\\ &{+\\frac{L_{f}^{2}\\overline{{\\mathbb{X}}}^{2}(1-p)}{N}\\mathbb{E}_{\\varepsilon^{\\ell-1}}[\\sum_{i=1}^{n/2}\\|\\nabla_{i}^{t-1}-\\mathbf{I}_{2}\\|_{\\mathsf{F}}^{2}]}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "623 where step $\\textcircled{1}$ uses that for any random variable $\\mathbf{X},\\mathbb{E}[(\\mathbf{X}-\\mathbb{E}[\\mathbf{X}])^{2}]\\leq\\mathbb{E}[\\mathbf{X}^{2}]$ ; step $\\circledcirc$ uses lemma   \n624 A.2; step $\\circled{3}$ uses assumption 4.3, Inequality (2) and Part $(b)$ of lemma 2.5. \u53e3 ", "page_idx": 18}, {"type": "text", "text": "625 E.2 Proof of theorem 4.6 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "626 Proof. For simplicity, we use B instead of $\\mathtt{B}^{t}$ . We will show that the following inequality holds : ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{\\theta}{2}\\|\\bar{\\mathbf V}^{t}-\\mathbf I_{2}\\|_{\\mathsf F}^{2}\\leq f(\\mathbf X^{t})-f(\\mathbf X^{t+1}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Since ${\\bar{\\mathbf{V}}}^{t}$ is the global optimal solution of Problem (5), we have: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathcal{G}(\\bar{\\mathbf{V}}^{t};\\mathbf{X}^{t},\\mathbf{B})\\leq\\mathcal{G}(\\mathbf{V};\\mathbf{X}^{t},\\mathbf{B}),\\mathbf{V}\\in\\mathcal{I}_{\\mathtt{B}}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "627 Letting $\\mathbf{V}=\\mathbf{I}_{2}$ , we have: $\\mathcal{G}(\\bar{\\mathbf{V}}^{t};\\mathbf{X}^{t},\\mathbf{B})\\leq\\mathcal{G}(\\mathbf{I}_{2};\\mathbf{X}^{t},\\mathbf{B})$ . We further obtain: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{1}{2}\\|\\bar{\\mathbf{V}}^{t}-\\mathbf{I}_{2}\\|_{\\mathbf{Q}+\\theta\\mathbf{I}}^{2}+\\langle\\bar{\\mathbf{V}}^{t}-\\mathbf{I},[\\nabla f(\\mathbf{X}^{t})(\\mathbf{X}^{t})^{\\top}]_{\\mathtt{B B}}\\rangle\\le0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "628 Using Inequality (2) with $N=1$ and Part $(c)$ of Lemma 2.1, we have: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{f({\\mathbf{X}}^{t+1})\\leq f({\\mathbf{X}}^{t})+\\langle\\bar{{\\mathbf{V}}}^{t}-\\mathbf{I}_{2},[\\nabla f({\\mathbf{X}}^{t})({\\mathbf{X}}^{t})^{\\top}]_{\\mathbb{B}}\\rangle+\\frac{1}{2}\\|\\bar{{\\mathbf{V}}}^{t}-\\mathbf{I}_{2}\\|_{\\mathbf{Q}}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "629 Adding Inequality (29) and (30) together, we obtain the inequality in (28). Using the result of Part $\\mathbf{\\eta}(b)$   \n630 in Lemma 2.1 that $\\begin{array}{r}{\\frac{\\|\\mathbf{X}^{+}-\\mathbf{X}\\|_{\\mathsf{F}}^{2}}{\\|\\mathbf{X}\\|_{\\mathsf{F}}^{2}}\\leq\\|\\mathbf{V}-\\mathbf{I}_{2}\\|_{\\mathsf{F}}^{2}}\\end{array}$ , we have the following sufficient decrease condition: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{f(\\mathbf{X}^{t+1})-f(\\mathbf{X}^{t})\\leq-\\frac{\\theta}{2}\\|\\bar{\\mathbf{V}}^{t}-\\mathbf{I}_{2}\\|_{\\mathsf{F}}^{2}\\leq-\\frac{\\theta}{2}\\frac{\\|\\mathbf{X}^{t+1}-\\mathbf{X}^{t}\\|_{\\mathsf{F}}^{2}}{\\|\\mathbf{X}^{t}\\|_{\\mathsf{F}}^{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We now prove the global convergence. Taking the expectation for Inequality (31), we obtain a lower bound on the expected progress made by each iteration for Algorithm 1: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\xi^{t+1}}[f({\\mathbf{X}}^{t+1})]-\\mathbb{E}_{\\xi^{t}}[f({\\mathbf{X}}^{t})]\\leq-\\mathbb{E}_{\\xi^{t}}[\\frac{\\theta}{2}\\|\\bar{{\\mathbf{V}}}^{t}-{\\mathbf{I}}_{2}\\|_{\\mathsf{F}}^{2}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Summing up the inequality above over $t=0,1,\\ldots,T$ , we have: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\xi^{t}}[\\frac{\\theta}{2}\\sum_{t=0}^{T}\\|\\bar{\\mathbf{V}}^{t}-\\mathbf{I}_{2}\\|_{\\mathsf{F}}^{2}]\\le f(\\mathbf{X}^{0})-\\mathbb{E}_{\\xi^{T+1}}[f(\\mathbf{X}^{T+1})]\\le f(\\mathbf{X}^{0})-f(\\bar{\\mathbf{X}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "631 As a result, there exists an index $\\bar{t}$ with $0\\leq\\bar{t}\\leq T$ such that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\xi^{\\bar{t}}}[\\|\\bar{\\mathbf{V}}^{\\bar{t}}-\\mathbf{I}_{2}\\|_{\\mathsf{F}}^{2}]\\le\\frac{2}{\\theta(T+1)}[f(\\mathbf{X}^{0})-f(\\bar{\\mathbf{X}})].}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "632 Furthermore, for any $t$ , we have: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{E}(\\mathbf{X}^{t})\\triangleq\\frac{1}{C_{n}^{2}}\\sum_{i=1}^{C_{n}^{2}}\\operatorname{dist}(\\mathbf{I}_{2},\\operatorname{arg}\\operatorname*{min}_{\\mathbf{V}}\\mathcal{G}(\\mathbf{V};\\mathbf{X}^{t},\\mathcal{B}_{i}))^{2}=\\mathbb{E}_{\\xi^{t}}[\\|\\bar{\\mathbf{V}}^{t}-\\mathbf{I}_{2}\\|_{\\mathsf{F}}^{2}]}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "633 Combining Inequality (32) and equality (33), we have the following result: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\xi^{t}}[\\|\\bar{\\mathbf{V}}^{t}-\\mathbf{I}_{2}\\|_{\\mathsf{F}}^{2}]=\\mathcal{E}\\big(\\mathbf{X}^{\\bar{t}}\\big)\\leq\\frac{2(f(\\mathbf{X}^{0})-f(\\bar{\\mathbf{X}}))}{\\theta(T+1)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We will give the arithmetic operations of GS-JOBCD. By the chosen parameters and Inequality (34), we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{E}(\\mathbf{X}^{\\bar{t}})\\leq\\frac{2(f(\\mathbf{X}^{0})-f(\\bar{\\mathbf{X}}))}{\\theta(T+1)}\\leq\\epsilon.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We define $\\Delta_{0}=f(\\mathbf{X}_{0})-f(\\bar{\\mathbf{X}})$ and set $\\begin{array}{r}{T+1=\\frac{2\\Delta_{0}}{\\epsilon\\theta}}\\end{array}$ . Denoting $m_{t}$ to be the number of arithmetic operations at $t$ -th iteration, we have for $t\\geq1$ : ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\xi^{t}}[m_{t}]=\\mathcal{O}(2N).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Then we have for $t\\geq1$ , the total number of arithmetic operations $M^{T}$ in $T$ iterations to obtain $\\epsilon$ -BS-point is ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\xi^{T}}[M^{T}]=\\mathbb{E}_{\\xi^{t}}[\\sum_{t=0}^{T}m_{t}]=2(T+1)N=\\mathcal{O}((T+1)N).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "634 We have (T + 1)N = N 2\u03f5\u2206\u03b80 $\\begin{array}{r}{(T+1)N=N\\frac{2\\Delta_{0}}{\\epsilon\\theta}=\\mathcal{O}(\\frac{\\Delta_{0}N}{\\epsilon})}\\end{array}$ . ", "page_idx": 19}, {"type": "text", "text": "635 E.3 Proof of Theorem 4.7 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Proof. For simplicity, we use B instead of $\\mathtt{B}^{t}$ . Defining $\\bar{\\bf V}$ :t as the global optimal solution of arg minV: $\\mathcal{T}(\\mathbf{V}_{:};\\mathbf{X}^{t},\\mathtt{B})$ , we have: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{T}(\\bar{\\mathbf{V}}_{:}^{\\ t};\\mathbf{X}^{t},\\mathbb{B})\\leq\\mathcal{T}(\\mathbf{V}_{:};\\mathbf{X}^{t},\\mathbb{B}),\\forall i,\\mathbf{V}_{i}\\in\\mathcal{J}_{\\mathtt{B}_{(i)}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "636 Letting $\\mathbf{V}_{i}=\\mathbf{I}_{2},\\forall i$ , we have: $T(\\bar{\\mathbf{V}}_{:}^{\\ t};\\mathbf{X}^{t},\\mathbb{B})\\leq\\mathcal{T}(\\mathbf{I}_{2};\\mathbf{X}^{t},\\mathbb{B})$ . We further obtain: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{1}{2}\\sum_{i=1}^{n/2}\\|\\bar{\\mathbf{V}}_{i}^{t}-\\mathbf{I}_{2}\\|_{(\\zeta+\\theta)\\mathbf{I}}^{2}+\\sum_{i=1}^{n/2}\\langle\\bar{\\mathbf{V}}_{i}^{t}-\\mathbf{I},[\\tilde{\\mathbf{G}}^{t}(\\mathbf{X}^{t})^{\\top}]_{\\mathbb{B}_{(i)}\\mathbb{B}_{(i)}}\\rangle\\leq0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "637 Using the results of telescoping Inequality (2) over $i$ from 1 to $N$ with Part $(c)$ of Lemma 2.5, we   \n638 have: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{f({\\mathbf{X}}^{t+1})\\leq f({\\mathbf{X}}^{t})+\\sum_{i=1}^{n/2}\\langle\\bar{\\mathbf{V}}_{i}-\\mathbf{I}_{2},[\\nabla f({\\mathbf{X}}){\\mathbf{X}}^{\\top}]_{\\mathbb{B}(i)}\\mathbb{B}_{(i)}\\rangle+\\frac{1}{2}\\sum_{i=1}^{n/2}\\|\\bar{\\mathbf{V}}_{i}-\\mathbf{I}_{2}\\|_{\\zeta\\mathbf{I}}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "639 Adding inequality (35), and (36) together, we obtain the inequality in (37). ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\theta}{2}\\sum_{i=1}^{n/2}\\|\\bar{\\mathbf{V}}_{i}^{t}-\\mathbf{I}_{2}\\|_{\\mathsf{F}}^{2}}\\\\ {\\le}&{f(\\mathbf{X}^{t})-f(\\mathbf{X}^{t+1})+\\sum_{i=1}^{n/2}\\langle\\bar{\\mathbf{V}}_{i}^{t}-\\mathbf{I},[(\\nabla f(\\mathbf{X}^{t})-\\tilde{\\mathbf{G}}^{t})(\\mathbf{X}^{t})^{\\top}]_{\\mathbb{B}_{(i)}\\mathbb{B}_{(i)}}\\rangle}\\\\ {\\overset{\\mathrm{()}}{\\le}}&{f(\\mathbf{X}^{t})-f(\\mathbf{X}^{t+1})+\\frac{1}{2}\\|\\mathbf{X}^{t}\\|_{\\mathsf{F}}^{2}\\sum_{i=1}^{n/2}\\|\\bar{\\mathbf{V}}_{i}^{t}-\\mathbf{I}_{2}\\|_{\\mathsf{F}}^{2}+\\frac{1}{2}\\|[\\nabla f(\\mathbf{X}^{t})-\\tilde{\\mathbf{G}}^{t}]\\|_{\\mathsf{F}}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "640 where step $\\textcircled{1}$ uses Part $(\\pmb{d})$ of Lemma 2.5. ", "page_idx": 19}, {"type": "text", "text": "641 Taking expectation on both sides of inequality (37) with respect to all randomness of the algorithm,   \n642 and adding the inequality in Lemma $4.5^{-}\\!\\times\\!\\frac{1}{2p}$ to (37), we have: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(\\frac{\\theta-\\overline{{\\mathbf{X}}}^{2}}{2}-\\frac{L_{f}^{2}\\overline{{\\mathbf{X}}}^{2}(1-p)}{2p b^{\\prime}})\\mathbb{E}_{\\iota^{t}}[\\sum_{i=1}^{n/2}\\|\\bar{\\mathbf{V}}_{i}^{t}-\\mathbf{I}_{2}\\|_{\\mathsf{F}}^{2}]}\\\\ {\\leq}&{\\mathbb{E}_{\\iota^{t}}[f(\\mathbf{X}^{t})]-\\mathbb{E}_{\\iota^{t+1}}[f(\\mathbf{X}^{t+1})]+\\frac{(N-b)}{2b(N-1)}\\sigma^{2}+\\frac{1-p}{2p}(\\mathbb{E}_{\\iota^{t}}[u^{t}]-\\mathbb{E}_{\\iota^{t+1}}[u^{t+1}])}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "643 Summing up the inequality above over $t=0,1,\\ldots,T$ , we have: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(\\frac{\\theta-\\overline{{{\\bf X}}}^{2}}{2}-\\frac{L_{f}^{2}\\overline{{{\\bf X}}}^{2}(1-p)}{2p b^{\\prime}})\\mathbb{E}_{t}T\\left[\\sum_{t=0}^{T}\\sum_{i=1}^{n/2}\\|\\bar{\\bf V}_{i}^{t}-{\\bf I}_{2}\\|_{\\mathsf{F}}^{2}\\right]}\\\\ {\\le}&{f({\\bf X}^{0})-\\mathbb{E}_{t}{\\boldsymbol{r}}\\left[f({\\bf X}^{T})\\right]+\\frac{(T+1)(N-b)}{2b(N-1)}\\sigma^{2}+\\frac{1-p}{2p}(u^{0}-\\mathbb{E}_{t}{\\boldsymbol{r}}+[u^{T+1}])}\\\\ {\\le}&{f({\\bf X}^{0})-f(\\bar{\\bf X})+\\frac{(T+1)(N-b)}{2b(N-1)}\\sigma^{2}+\\frac{1-p}{2p}(u^{0}-\\mathbb{E}_{t}{\\boldsymbol{r}}+[u^{T+1}])}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "644 As a result, there exists an index $\\bar{t}$ with $0\\leq\\bar{t}\\leq T$ such that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(\\frac{\\theta-\\overline{{\\mathbf{X}}}^{2}}{2}-\\frac{L_{f}^{2}\\overline{{\\mathbf{X}}}^{2}(1-p)}{2p b^{\\prime}})(T+1)\\mathbb{E}_{\\iota^{\\bar{\\tau}}}[\\sum_{i=1}^{n/2}\\|\\bar{\\mathbf{V}}_{i}^{\\bar{t}}-\\mathbf{I}_{2}\\|_{\\mathsf{F}}^{2}]}\\\\ {\\leq}&{f(\\mathbf{X}^{0})-f(\\bar{\\mathbf{X}})+\\frac{(T+1)(N-b)}{2b(N-1)}\\sigma^{2}+\\frac{1-p}{2p}(u^{0}-\\mathbb{E}_{\\iota^{T+1}}[u^{T+1}])}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "645 Defining \u03d6 = \u03b8\u2212X2 $\\begin{array}{r}{\\varpi=\\frac{\\theta-\\overline{{\\mathbf{X}}}^{2}}{2}-\\frac{L_{f}^{2}\\overline{{\\mathbf{X}}}^{2}(1-p)}{2p b^{\\prime}}}\\end{array}$ L2f X2p(b1\u2032\u2212p), furthermore, for any t and \u2200i, we have: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{E}(\\mathbf{X}^{t})=\\frac{1}{\\mathrm{C}J}\\sum_{i=1}^{\\mathrm{C}_{J}}\\mathbb{E}_{\\iota}\\boldsymbol{\\ t}[\\mathrm{dist}(\\mathbf{I}_{2},\\mathrm{arg}\\operatorname*{min}_{\\mathbf{V}_{\\iota}}\\mathcal{T}(\\mathbf{V}_{:};\\mathbf{X}^{t},\\tilde{B}_{i}))^{2}]=\\mathbb{E}_{\\iota^{t}}[\\sum_{i=1}^{n/2}\\|\\bar{\\mathbf{V}}_{i}^{t}-\\mathbf{I}_{2}\\|_{\\mathsf{F}}^{2}],}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "646 Combining inequality (40) and (41) , we have the following result: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{E}(\\mathbf{X}^{\\bar{t}})\\leq\\frac{1}{(T+1)\\varpi}(f(\\mathbf{X}^{0})-f(\\bar{\\mathbf{X}})+\\frac{(T+1)(N-b)}{2b(N-1)}\\sigma^{2}+\\frac{1-p}{2p}(u^{0}-\\mathbb{E}_{\\iota^{T+1}}[u^{T+1}]))}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "By the chosen parameters and Inequality (42), we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal E({\\mathbf X}^{\\bar{t}})\\leq\\frac{1}{(T+1)\\varpi}(f({\\mathbf X}^{0})-f(\\bar{{\\mathbf X}})+\\frac{(T+1)(N-b)}{2b(N-1)}\\sigma^{2}+\\frac{1-p}{2p}(u^{0}-\\mathbb{E}_{t^{T+1}}[u^{T+1}]))\\leq\\epsilon.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We define $\\Delta_{0}=f(\\mathbf{X}_{0})-f(\\bar{\\mathbf{X}})$ and set $\\begin{array}{r}{T+1=\\frac{\\Delta_{0}}{\\epsilon\\varpi}}\\end{array}$ . Denoting $m_{t}^{i}$ to be the number of arithmetic operations to update the $i$ -th block at $t$ -th iteration, we have for $t\\geq1$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\iota^{t}}[m_{t}^{i}]=\\mathcal{O}\\big(2(p b+(1-p)b^{\\prime})\\big).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Letting $m_{t}$ be the number of arithmetic operations in the $t$ -the iteration, we have for $t\\geq1$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{t^{t}}[m_{t}]=\\mathbb{E}_{t^{t}}[\\sum_{i=1}^{n/2}m_{t}^{i}]=\\mathcal{O}((p b+(1-p)b^{\\prime})n/2\\times2)=\\mathcal{O}(n(p b+(1-p)b^{\\prime})).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Hence, the total number of arithmetic operations $M^{T}$ in $T$ iterations to obtain $\\epsilon$ -BS-point is ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{t}\\boldsymbol{r}[M]=\\mathbb{E}_{t}\\boldsymbol{t}\\left[\\sum_{t=0}^{T}m_{t}\\right]=\\mathcal{O}(b n)+\\mathbb{E}_{t}\\boldsymbol{t}\\left[\\sum_{t=1}^{T}m_{t}\\right]=\\mathcal{O}(b n+T n(p b+(1-p)b^{\\prime})).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\! \n$$", "text_format": "latex", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{n T(p b+(1-p)b^{\\prime})=n\\frac{\\Delta_{0}}{\\epsilon(\\theta-\\overline{{\\mathbf{X}}}^{2}-L_{f}^{2}\\overline{{\\mathbf{X}}}^{2})}\\frac{2b b^{\\prime}}{b+b^{\\prime}}\\le\\frac{n\\Delta_{0}}{\\epsilon(\\theta-\\overline{{\\mathbf{X}}}^{2}-L_{f}^{2}\\overline{{\\mathbf{X}}}^{2})}2b^{\\prime}=\\mathcal{O}(\\frac{\\Delta_{0}\\sqrt{N}}{\\epsilon}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "647 ", "page_idx": 20}, {"type": "text", "text": "648 E.4 Proof of Theorem 4.10 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "649 Proof. For simplicity, we use B instead of $\\mathtt{B}^{t}$ . We notice that the Riemannian gradient of $\\mathcal{T}(\\mathbf{V}_{:};\\mathbf{X}^{t},\\mathtt{B})$   \n650 at the point ${\\bf V}_{i}={\\bf I}_{2},\\forall i$ . Defining $\\mathbf{G}=\\tilde{\\mathbf{G}}^{t}[\\mathbf{X}^{t}]^{\\top}$ and using $\\mathbf{JU}_{\\mathtt{B}}=\\mathbf{U}_{\\mathtt{B}}\\mathbf{J}_{\\mathtt{B B}}$ , $\\mathbf{U}_{\\mathtt{B}}^{\\top}\\mathbf{J}=\\mathbf{J}_{\\mathtt{B B}}\\mathbf{U}_{\\mathtt{B}}^{\\top}$ ,we   \n651 have: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\nabla_{\\mathcal{I}}\\mathcal{T}(\\mathbf{V}_{:}=\\mathbf{I}_{2};\\mathbf{X}^{t},\\mathtt{B})=\\sum_{i=1}^{n/2}\\mathbf{U}_{\\mathtt{B}_{(i)}}^{\\top}\\mathbf{G}\\mathbf{U}_{\\mathtt{B}_{(i)}}-\\mathbf{U}_{\\mathtt{B}_{(i)}}^{\\top}\\mathbf{J}\\mathbf{G}^{\\top}\\mathbf{J}\\mathbf{U}_{\\mathtt{B}_{(i)}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "652 Then, we prove the following important lemmas. ", "page_idx": 20}, {"type": "text", "text": "653 Lemma E.1. We have the following result for VR-J-JOBCD: $\\mathbb{E}_{\\iota^{t+1}}[\\|\\tilde{\\mathbf{G}}^{t}-\\tilde{\\mathbf{G}}^{t+1}\\|_{\\mathsf{F}}]\\leq p\\mathbb{E}_{\\iota^{t}}[\\sqrt{u^{t}}]+$   \n654 $L_{f}\\mathbb{E}_{\\iota^{t+1}}[\\|\\mathbf{X}^{t}-\\mathbf{X}^{t+1}\\|_{\\mathsf{F}}]$ ", "page_idx": 20}, {"type": "text", "text": "655 Proof. By the definition of $\\tilde{\\mathbf{G}}^{t}$ , with the choice of $b=N$ , $b^{\\prime}={\\sqrt{b}}$ and $\\begin{array}{r}{p=\\frac{b^{\\prime}}{b+b^{\\prime}}}\\end{array}$ \u2032 , we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{t^{+}}[|\\hat{\\mathbf{U}}^{t}-\\hat{\\mathbf{G}}^{t+1}||]}\\\\ {\\overset{\\mathrm{()}}{=}}&{\\mathbb{E}_{t^{+}}[|\\hat{\\mathbf{U}}^{t}-\\frac{\\nu}{b}\\sum_{i=1}^{b}\\nabla f_{i}(\\mathbf{X}^{t+1})-\\frac{1-\\nu}{b^{\\prime}}\\sum_{i=1}^{b^{\\prime}}(\\nabla f_{i}(\\mathbf{X}^{t+1})-\\nabla f_{i}(\\mathbf{X}^{t}))-(1-p)\\tilde{\\mathbf{G}}^{t}||_{F}]}\\\\ {=}&{\\mathbb{E}_{t^{+}}[||\\hat{\\mathbf{U}}\\tilde{G}^{t}-\\frac{\\nu}{b}\\sum_{i=1}^{b}\\nabla f_{i}(\\mathbf{X}^{t+1})-\\frac{1-\\nu}{b^{\\prime}}\\sum_{i=1}^{b^{\\prime}}(\\nabla f_{i}(\\mathbf{X}^{t+1})-\\nabla f_{i}(\\mathbf{X}^{t}))||\\mathbf{f}]}\\\\ {\\overset{\\mathrm{()}}{\\leq}}&{p\\mathbb{E}_{t^{+}}[||\\tilde{\\mathbf{G}}^{t}-\\nabla f(\\mathbf{X}^{t+1})||]\\Big.+\\frac{1-\\nu}{b^{\\prime}}\\mathbb{E}_{t^{+}}[||\\sum_{i=1}^{b}\\nabla f_{i}(\\mathbf{X}^{t+1})-\\nabla f_{i}(\\mathbf{X}^{t})||_{F}]}\\\\ {\\overset{\\mathrm{()}}{\\leq}}&{p\\mathbb{E}_{t^{+}}[||\\tilde{\\mathbf{G}}^{t}-\\nabla f(\\mathbf{X}^{t})||_{F}]+p\\mathbb{E}_{t^{++}}[||\\nabla f(\\mathbf{X}^{t})-\\nabla f(\\mathbf{X}^{t+1})||_{F}]}\\\\ &{\\left.+\\frac{1-\\nu}{b}\\mathbb{E}_{t^{+}}[||\\sum_{i=1}^{b^{\\prime}}\\nabla f_{i}(\\mathbf{X}^{t+1})-\\nabla f_{i}(\\mathbf{X}^{t})||_{F}]\\right.}\\\\ {\\overset{\\mathrm{()}}{\\leq}}&{\\left.p\\mathbb{E}_{t^{+}}[\\sqrt{u^{\\prime}}]+p\\mathbb{E}_{t^{++}}[||\\nabla f \n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "656 where step $\\textcircled{1}$ uses formula (9); step $\\circledcirc$ uses norm inequality and $\\begin{array}{r}{\\frac{1}{b}\\sum_{i=1}^{b}\\nabla f_{i}(\\mathbf{X}^{t+1})=\\nabla f(\\mathbf{X}^{t+1})}\\end{array}$   \n657 with $b=N$ and norm inequality; step uses triangle inequality that $\\|\\mathbf{\\bar{A}}-\\mathbf{B}\\|_{\\mathsf{F}}\\leq\\|\\mathbf{A}-\\mathbf{C}\\|_{\\mathsf{F}}+$   \n658 $\\|\\mathbf{C}-\\mathbf{B}\\|_{\\mathsf{F}}$ , for any A, $\\mathbf{B}$ and $\\mathbf{C}$ ; step $\\circledast$ the definition of $u^{t}$ ; step $\\textcircled{5}$ uses Inequality (2) and the results   \n659 of telescoping it over $i$ from 1 to $N$ . \u53e3   \n660 Lemma E.2. (Riemannian gradient Lower Bound for the Iterates Gap) We de  \n661 fine $\\begin{array}{r}{\\phi\\triangleq(3\\overline{{\\mathbf{X}}}+\\overline{{\\mathbf{V}\\mathbf{X}}})\\overline{{\\mathbf{G}}}+(1+\\overline{{\\mathbf{V}}}^{2}+\\frac{n}{2}(\\overline{{\\mathbf{X}}}^{2}+\\overline{{\\mathbf{V}}}^{2}\\overline{{\\mathbf{X}}}^{2}))L_{f}+(1+\\overline{{\\mathbf{V}}}^{2})\\theta.}\\end{array}$ . $I t$ holds that:   \n662 $\\begin{array}{r}{\\mathbb{E}_{t^{t+1}}[\\mathrm{dist}(\\mathbf{0},\\nabla_{\\mathcal{I}}\\mathcal{T}(\\mathbf{I}_{2};\\mathbf{X}^{t+1},\\mathbf{B}^{t+1}))]\\leq\\phi\\cdot\\mathbb{E}_{t^{t}}[\\sum_{i=1}^{n/2}\\|\\bar{\\mathbf{V}}_{i}^{t}-\\mathbf{I}_{2}\\|_{\\mathsf{F}}]+\\frac{n p\\sqrt{u^{t}}}{2}(\\overline{{\\mathbf{X}}}+\\overline{{\\mathbf{V}}}^{2}\\overline{{\\mathbf{X}}}).}\\end{array}$ . ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "663 Proof. For notation simplicity, we define: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Omega_{i0}\\triangleq\\mathbf{U}_{\\mathtt{B}_{(i)}}^{\\top}[\\tilde{\\mathbf{G}}^{t+1}][\\mathbf{X}^{t+1}]^{\\top}\\mathbf{U}_{\\mathtt{B}_{(i)}},\\forall i}\\\\ &{\\Omega_{i1}\\triangleq\\!\\mathbf{U}_{\\mathtt{B}_{(i)}}^{\\top}[\\tilde{\\mathbf{G}}^{t+1}][\\mathbf{X}^{t}]^{\\top}\\mathbf{U}_{\\mathtt{B}_{(i)}},\\forall i,}\\\\ &{\\Omega_{i2}\\triangleq\\!\\mathbf{U}_{\\mathtt{B}_{(i)}}^{\\top}[\\tilde{\\mathbf{G}}^{t}-\\tilde{\\mathbf{G}}^{t+1}][\\mathbf{X}^{t}]^{\\top}\\mathbf{U}_{\\mathtt{B}_{(i)}},\\forall i.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "664 First, using the optimality of $\\bar{\\mathbf{V}}_{i}^{t},i\\in\\{1,\\cdots,\\frac{n}{2}\\}$ for the subproblem, we have: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathrm{where}\\ \\tilde{\\mathbf{G}}_{i}=\\underbrace{\\mathrm{mat}((\\mathbf{Q}+\\theta\\mathbf{I}_{2})\\operatorname{vec}(\\bar{\\mathbf{V}}_{i}^{t}-\\mathbf{I}_{2}))}_{\\triangleq\\Upsilon_{i1}}+\\underbrace{\\mathbf{U}_{8(i)}^{\\top}\\tilde{\\mathbf{G}}^{t}(\\mathbf{X}^{t})^{\\top}\\mathbf{U}_{8(i)}}_{\\triangleq\\Upsilon_{i2}}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "665 Using the relation that $\\tilde{\\mathbf{G}}_{i}=\\Upsilon_{i1}+\\Upsilon_{i2}$ , we obtain the following results from the above equality: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbf{0}_{2,2}=(\\Upsilon_{i1}+\\Upsilon_{i2})-\\mathbf{J}_{\\mathbb{B}_{(i)}}\\bar{\\mathbf{V}}_{i}^{t}(\\Upsilon_{i1}+\\Upsilon_{i2})^{\\top}\\bar{\\mathbf{V}}_{i}^{t}\\mathbf{J}_{\\mathbb{B}_{(i)}}}\\\\ &{\\stackrel{\\mathrm{\\tiny{(\\widehat{\\mathbb{D}})}}}{\\Rightarrow}\\mathbf{0}_{2,2}=\\Upsilon_{i1}+\\Omega_{i1}+\\Omega_{i2}-\\mathbf{J}_{\\mathbb{B}_{(i)}}\\bar{\\mathbf{V}}_{i}^{t}(\\Upsilon_{i1}+\\Omega_{i1}+\\Omega_{i2})^{\\top}\\bar{\\mathbf{V}}_{i}^{t}\\mathbf{J}_{\\mathbb{B}_{(i)}}}\\\\ &{\\Rightarrow\\Omega_{i1}=\\mathbf{J}_{\\mathbb{B}_{(i)}}\\bar{\\mathbf{V}}_{i}^{t}(\\Upsilon_{i1}+\\Omega_{i1}+\\Omega_{i2})^{\\top}\\bar{\\mathbf{V}}_{i}^{t}\\mathbf{J}_{\\mathbb{B}_{(i)}}-\\Upsilon_{i1}-\\Omega_{i2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "666 where step $\\textcircled{1}$ uses $\\Upsilon_{i2}=\\Omega_{i1}+\\Omega_{i2}$ . Then we derive the following results: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{t\\rightarrow t+1}[\\operatorname*{inf}(0,\\mathbb{P}_{t}^{T}\\nabla_{u}^{\\lambda}\\mathrm{L}_{2}\\mathbb{X}^{\\lambda}+\\mathbb{I}_{t}^{(s)})]=\\mathbb{E}_{t\\rightarrow u}[\\vert\\nabla_{x}^{\\lambda}\\nabla_{u}T^{\\lambda}+\\vert\\nabla_{x}^{\\lambda}(\\vert u\\vert^{2},\\vert u)+1]\\vert}\\\\ {\\tilde{\\mathbf{J}}\\otimes_{t\\rightarrow t}\\Vert\\sum_{i=0}^{T-1}\\mathbb{E}_{t\\rightarrow u}[\\vert\\nabla_{u}u\\vert^{2}+\\vert\\nabla_{u}^{\\lambda}(\\vert u\\vert^{2}+\\vert\\nabla_{u}^{\\lambda}(\\vert u\\vert^{2}+\\vert\\nabla_{u}^{\\lambda}(\\vert u\\vert^{2}+\\vert\\nabla_{u}^{\\lambda})\\vert)\\vert)]}\\\\ {\\tilde{\\mathbf{J}}\\otimes_{t\\rightarrow t}\\Vert\\sum_{i=0}^{T-1}\\mathbb{E}_{t\\rightarrow u}[\\langle\\nabla_{u}u^{\\lambda}+\\vert\\nabla_{u}T^{\\lambda}\\vert-\\mathbf{X}^{\\lambda}(u^{\\lambda}+\\vert\\nabla_{u}T^{\\lambda}\\vert)\\nabla_{t}u\\vert^{2}]}\\\\ {\\tilde{\\mathbf{J}}\\otimes_{t\\rightarrow t}\\Vert\\sum_{i=0}^{T-1}\\mathbb{E}_{t\\rightarrow u}[\\vert u\\vert^{2}+\\vert\\nabla_{u}u^{\\lambda}\\vert]\\nabla_{u}[\\vert u\\vert^{2}}\\\\ {\\tilde{\\mathbf{J}}\\otimes_{t\\rightarrow t}\\Vert\\sum_{i=0}^{T-1}(\\mathbb{I}_{0}^{T}-\\mathbb{I}_{0}^{T})+\\mathbb{I}_{t\\rightarrow u}[\\langle u_{u}\\vert^{2},\\vert u\\vert^{2}-\\lambda_{0}+\\vert\\nabla_{u}\\vert_{u}^{\\lambda}\\vert]\\nabla_{u}-\\texttt{J}_{0}(\\vert u_{u}\\vert^{2},\\vert u\\vert^{2})}\\\\ {\\tilde{\\mathbf{J}}\\otimes_{t\\rightarrow t}\\Vert\\sum_{i=0}^{T-1}\\mathbb{I}_{t\\rightarrow u}[\\vert u\\vert^{2}+\\vert\\nabla_{u}^{\\lambda}(\\vert u\\vert^{2}+\\vert\\nabla_{u}^{\\lambda}\\vert)\\vert_{L}\\nabla_{u}-\\texttt{J}_{0}(\\vert u_{u}\\vert^{2},\\vert u\\vert^{2})}\\\\ &{+\\mathbb{E}_{t\\rightarrow u}[\\vert\\nabla_{u}u\\vert^{\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "667 where step $\\textcircled{1}$ uses Equality (43) ; step $\\circledcirc$ uses the fact that both the working set $\\mathtt{B}^{t}$ and $\\mathsf{B}^{t+1}$ are selected   \n668 randomly and uniformly; step $\\circled{3}$ uses the definition of $\\Omega_{i0}$ in (44); step $\\circledast$ uses $-\\Omega_{i1}+\\Omega_{i1}=\\mathbf{0}$   \n669 and $-\\Omega_{i1}^{\\dag}+\\Omega_{i1}^{\\top}=\\mathbf{0}$ ; step $\\textcircled{5}$ uses the norm inequality; step $\\circled{6}$ uses the norm inequality; step $\\circledcirc$ uses   \n670 the norm inequality; step $\\circled{8}$ uses Equality (49); step $\\circledcirc$ uses the norm inequality. We now establish   \n671 individual bounds for each term for Inequality (50). ", "page_idx": 21}, {"type": "text", "text": "672 For the first term $\\begin{array}{r}{2\\mathbb{E}_{\\iota^{t}}[\\|\\sum_{i=1}^{n/2}\\Omega_{i0}-\\Omega_{i1}\\|_{\\mathsf{F}}]}\\end{array}$ in (50): ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{2\\mathbb{E}_{t}t[\\|\\sum_{i=1}^{n/2}\\Omega_{i0}-\\Omega_{i1}\\|_{\\mathsf{F}}]}&{=}&{2\\mathbb{E}_{t}t[\\|\\sum_{i=1}^{n/2}\\mathbf{U}_{\\mathtt{B}_{(i)}}^{\\top}[\\tilde{\\mathbf{G}}^{t}][\\mathbf{X}^{t}-\\mathbf{X}^{t}]^{\\top}\\mathbf{U}_{\\mathtt{B}_{(i)}}\\|_{\\mathsf{F}}]}\\\\ &{\\overset{\\mathrm{()}}{=}}&{2\\mathbb{E}_{t}t[\\|\\sum_{i=1}^{n/2}[\\tilde{\\mathbf{G}}^{t}][\\mathbf{U}_{\\mathtt{B}_{(i)}}(\\bar{\\mathbf{V}}_{i}^{t}-\\mathbf{I}_{2})\\mathbf{U}_{\\mathtt{B}_{(i)}}\\mathbf{X}^{t}]^{\\top}\\|_{\\mathsf{F}}]}\\\\ &{\\overset{\\mathrm{()}}{\\leq}}&{2\\overline{{\\mathbf{X}\\mathbf{G}}}\\mathbb{E}_{t}t[\\|\\sum_{i=1}^{n/2}\\bar{\\mathbf{V}}_{i}^{t}-\\mathbf{I}_{2}\\|_{\\mathsf{F}}]}\\\\ &{\\overset{\\mathrm{()}}{\\leq}}&{2\\overline{{\\mathbf{X}\\mathbf{G}}}\\mathbb{E}_{t}t\\sum_{i=1}^{n/2}\\|\\bar{\\mathbf{V}}_{i}^{t}-\\mathbf{I}_{2}\\|_{\\mathsf{F}}]}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "673 where step $\\textcircled{1}$ uses $[\\mathbf{X}^{t}-\\mathbf{X}^{t}]_{\\mathbb{B}_{i}\\mathbb{B}_{i}}=\\mathbf{U}_{\\mathbb{B}_{(i)}}(\\bar{\\mathbf{V}}_{i}^{t}-\\mathbf{I}_{2})\\mathbf{U}_{\\mathbb{B}_{(i)}}^{\\top}\\mathbf{X}^{t}$ ; step $\\circledcirc$ uses the inequality $\\lVert\\mathbf{XY}\\rVert_{\\mathsf{F}}\\leq$   \n674 $\\|\\mathbf{X}\\|_{\\mathsf{F}}\\|\\mathbf{Y}\\|_{\\mathsf{F}}$ for all $\\mathbf{X}$ and $\\mathbf{Y}$ repeatedly and the fact that $\\forall t$ , $\\|\\tilde{\\mathbf{G}}^{t}\\|_{\\mathsf{F}}\\leq\\overline{{\\mathbf{G}}}$ and $\\forall t,\\|\\mathbf{X}^{t}\\|_{\\mathsf{F}}\\leq\\overline{{\\mathbf{X}}}$ ; step $\\circled{3}$   \n675 uses the norm inequality. ", "page_idx": 22}, {"type": "text", "text": "676 For the second term $\\begin{array}{r}{\\mathbb{E}_{\\iota^{t}}[\\|\\sum_{i=1}^{n/2}\\mathbf{J}_{\\mathtt{B}_{(i)}}\\bar{\\mathbf{V}}_{i}^{t}\\Upsilon_{i1}^{\\top}\\bar{\\mathbf{V}}_{i}^{t}\\mathbf{J}_{\\mathtt{B}_{(i)}}-\\Upsilon_{i1}\\|_{\\mathsf{F}}]}\\end{array}$ in (50): ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{t}\\{\\|\\sum_{i=1}^{n/2}\\mathbf{J}_{\\mathbb{B}_{i}}\\bar{\\mathbf{V}}_{i}^{t}\\Upsilon_{i1}^{\\top}\\bar{\\mathbf{V}}_{i}^{t}\\mathbf{J}_{\\mathbb{B}_{(i)}}-\\Upsilon_{i1}\\|_{\\mathsf{F}}\\}}\\\\ {\\overset{\\textmd{()}}{\\le}}&{\\mathbb{E}_{t}t[\\|\\sum_{i=1}^{n/2}\\bar{\\mathbf{V}}_{i}^{t}\\Upsilon_{i1}^{\\top}\\bar{\\mathbf{V}}_{i}^{t}\\|_{\\mathsf{F}}]+\\mathbb{E}_{t}\\{\\|\\sum_{i=1}^{n/2}\\bar{\\mathbf{Y}}_{i1}\\|_{\\mathsf{F}}\\}}\\\\ {\\overset{\\textmd{()}}{\\le}}&{(1+\\overline{{\\mathbf{V}}}^{2})\\mathbb{E}_{t}\\{\\|\\sum_{i=1}^{n/2}\\Upsilon_{i1}^{\\top}\\|_{\\mathsf{F}}\\}}\\\\ {\\overset{\\textmd{()}}{=}}&{(1+\\overline{{\\mathbf{V}}}^{2})\\mathbb{E}_{t}\\{\\|\\sum_{i=1}^{n/2}\\mathbf{mat}((\\mathbf{Q}+\\theta\\mathbf{I}_{2})\\,\\mathrm{vec}(\\bar{\\mathbf{V}}_{i}^{t}-\\mathbf{I}_{2}))\\|_{\\mathsf{F}}\\}}\\\\ {\\le}&{(1+\\overline{{\\mathbf{V}}}^{2})\\|\\mathbf{Q}+\\theta\\mathbf{I}_{2}\\|_{\\mathsf{F}}\\cdot\\mathbb{E}_{t}\\{\\|\\sum_{i=1}^{n/2}\\bar{\\mathbf{V}}_{i}^{t}-\\mathbf{I}_{2}\\|_{\\mathsf{F}}\\}}\\\\ {\\overset{\\textmd{()}}{\\le}}&{(1+\\overline{{\\mathbf{V}}}^{2})(L_{f}+\\theta)\\cdot\\mathbb{E}_{t}\\{\\sum_{i=1}^{n/2}\\|\\bar{\\mathbf{V}}_{i}^{t}-\\mathbf{I}_{2}\\|_{\\mathsf{F}}\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "677 where step $\\textcircled{1}$ uses the triangle inequality; step $\\circledcirc$ uses the inequality $\\|\\mathbf{X}\\mathbf{Y}\\|_{\\mathsf{F}}\\leq\\|\\mathbf{X}\\|_{\\mathsf{F}}\\|\\mathbf{Y}\\|_{\\mathsf{F}}$ for all   \n678 $\\mathbf{X}$ and $\\mathbf{Y}$ and $\\forall t$ , $\\|\\mathbf{V}^{t}\\|_{\\mathsf{F}}\\leq\\overline{{\\mathbf{V}}}$ ; step $\\circled{3}$ uses the definition of $\\Upsilon_{i1}$ ; step $\\circledast$ uses the choice of $\\mathbf{Q}\\preceq L_{f}\\mathbf{I}$   \n679 and the norm inequality. ", "page_idx": 22}, {"type": "text", "text": "680 For the third term $\\begin{array}{r}{\\mathbb{E}_{\\boldsymbol{\\iota}^{t}}[\\|\\sum_{i=1}^{n/2}\\bar{\\mathbf{V}}_{i}^{t}\\Omega_{i1}^{\\top}\\bar{\\mathbf{V}}_{i}^{t}-\\Omega_{i1}^{\\top}\\|_{\\mathsf{F}}]}\\end{array}$ in (50), we have: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{t}t[\\|\\sum_{i=1}^{n/2}\\bar{\\mathbf{V}}_{i}^{t}\\Omega_{i1}^{\\top}\\bar{\\mathbf{V}}_{i}^{t}-\\Omega_{i1}^{\\top}\\|_{\\mathsf{F}}]}\\\\ {\\overset\\textcircled{\\geq}}&{\\mathbb{E}_{t}t[\\|\\sum_{i=1}^{n/2}\\bar{\\mathbf{V}}_{i}^{t}\\Omega_{i1}^{\\top}(\\bar{\\mathbf{V}}_{i}^{t}-\\mathbf{I}_{2})+(\\bar{\\mathbf{V}}_{i}^{t}-\\mathbf{I}_{2})\\Omega_{i1}^{\\top}\\|_{\\mathsf{F}}]}\\\\ {\\overset\\textcircled{\\geq}}&{(1+\\overline{{\\mathbf{V}}})\\mathbb{E}_{t}t\\bigl[\\sum_{i=1}^{n/2}\\|\\Omega_{i1}\\|_{\\mathsf{F}}\\cdot\\|\\bar{\\mathbf{V}}_{i}^{t}-\\mathbf{I}_{2}\\|_{\\mathsf{F}}\\bigr]}\\\\ {\\overset{\\textcircled{\\leq}}{\\leq}}&{(\\overline{{\\mathbf{X}}}+\\overline{{\\mathbf{V}}}\\overline{{\\mathbf{X}}})\\mathbb{E}_{t}t\\bigl[\\sum_{i=1}^{n/2}\\|\\tilde{\\mathbf{G}}^{t}\\|_{\\mathsf{F}}\\cdot\\|\\bar{\\mathbf{V}}_{i}^{t}-\\mathbf{I}_{2}\\|_{\\mathsf{F}}\\bigr]}\\\\ {\\overset{\\textcircled{\\leq}}{\\leq}}&{(\\overline{{\\mathbf{X}}}+\\overline{{\\mathbf{V}}}\\overline{{\\mathbf{X}}})\\overline{{\\mathbf{G}}}\\mathbb{E}_{t}t\\bigl[\\sum_{i=1}^{n/2}\\|\\bar{\\mathbf{V}}_{i}^{t}-\\mathbf{I}_{2}\\|_{\\mathsf{F}}\\bigr]}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "681 where step $\\textcircled{1}$ uses the fact that $-\\bar{\\mathbf{V}}_{i}^{t}\\boldsymbol{\\Omega}_{i1}^{\\top}\\mathbf{I}_{2}+\\bar{\\mathbf{V}}_{i}^{t}\\boldsymbol{\\Omega}_{i1}^{\\top}=\\mathbf{0}$ ; step $\\circledcirc$ uses the norm inequality and   \n682 $\\forall t,\\|\\mathbf{V}^{t}\\|_{\\mathsf{F}}\\leq\\overline{{\\mathbf{V}}}$ ; step $\\circled{3}$ uses the fact that $\\|\\Omega_{i1}\\|_{\\mathsf{F}}=\\|\\mathbf{U}_{\\mathtt{B}_{(i)}}^{\\top}\\tilde{\\mathbf{G}}^{t}[\\mathbf{X}^{t}]^{\\top}\\mathbf{U}_{\\mathtt{B}_{(i)}}\\|_{\\mathsf{F}}\\leq\\overline{{\\mathbf{X}}}\\|\\tilde{\\mathbf{G}}^{t}\\|_{\\mathsf{F}},\\forall i$ which   \n683 can be derived using the norm inequality ; step $\\circledast$ uses the fact that $\\forall\\mathbf{X}$ , $\\|\\tilde{\\mathbf{G}}^{t}\\|_{\\mathsf{F}}\\leq\\overline{{\\mathbf{G}}}$ . ", "page_idx": 22}, {"type": "text", "text": "684 For the fourth term $\\begin{array}{r}{\\mathbb{E}_{\\iota^{t}}[\\|\\sum_{i=1}^{n/2}\\mathbf{J}_{\\mathtt{B}_{(i)}}\\bar{\\mathbf{V}}_{i}^{t}\\Omega_{i2}^{\\top}\\bar{\\mathbf{V}}_{i}^{t}\\mathbf{J}_{\\mathtt{B}_{(i)}}-\\Omega_{i2}\\|_{\\mathsf{F}}]}\\end{array}$ in (50), we have: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\boldsymbol{\\iota}^{\\prime}}[\\big\\lVert\\sum_{i=1}^{n/2}\\mathbf{\\hat{J}}_{\\mathbf{B}_{i}}\\hat{\\nabla}_{i}^{t}\\Omega_{i}^{\\top}\\bar{\\mathbf{J}}_{i}^{t}\\mathbf{\\hat{J}}_{\\mathbf{B}_{i}}^{\\top}\\mathbf{\\hat{J}}_{\\mathbf{B}_{i}}^{\\top}]\\mathbb{E}_{\\boldsymbol{\\iota}^{\\prime}}\\,\\mathcal{\\bar{\\mathbf{J}}}_{i}]\\,}\\\\ {\\stackrel{()}{\\le}}&{\\mathbb{E}_{\\boldsymbol{\\iota}^{\\prime}}[\\big\\lVert\\sum_{i=1}^{n/2}\\bar{\\nabla}_{i}^{t}\\Omega_{i}^{\\top}\\bar{\\nabla}_{i}^{t}\\|_{\\boldsymbol{\\ell}}]+\\mathbb{E}_{\\boldsymbol{\\iota}^{\\prime}}[\\big\\lVert\\sum_{i=1}^{n/2}\\Omega_{i2}]\\|_{\\boldsymbol{\\ell}}]}\\\\ {\\stackrel{()}{\\le}}&{(1+\\overline{{\\nabla}}^{2})\\mathbb{E}_{\\boldsymbol{\\iota}^{\\prime}}[\\big\\lVert\\sum_{i=1}^{n/2}\\Omega_{i2}]\\mathbb{E}_{\\boldsymbol{\\iota}}]}\\\\ {\\stackrel{()}{=}}&{(1+\\overline{{\\nabla}}^{2})\\mathbb{E}_{\\boldsymbol{\\iota}^{\\prime}}[\\big\\lVert\\sum_{i=1}^{n/2}\\mathbf{\\hat{J}}_{\\mathbf{B}_{i}}^{\\top}(\\bar{\\mathbf{X}}^{t}-\\bar{\\mathbf{G}}^{t})[\\mathbf{X}^{t}]^{\\top}\\mathbf{U}_{\\mathbf{B}_{\\mathcal{O}}_{i}})\\left\\lVert\\boldsymbol{\\ell}\\right\\rVert}\\\\ {\\stackrel{()}{\\le}}&{\\frac{n}{2}(\\overline{{\\mathbf{X}}}+\\overline{{\\nabla}}^{2}\\overline{{\\mathbf{X}}})\\mathbb{E}_{\\boldsymbol{\\iota}^{\\prime}}[\\big\\lVert[\\bar{\\mathbf{G}}^{t}-\\bar{\\mathbf{G}}^{t}]\\big\\rVert_{\\boldsymbol{\\ell}}]}\\\\ {\\stackrel{()}{\\le}}&{\\frac{n}{2}(\\overline{{\\mathbf{X}}}+\\overline{{\\nabla}}^{2}\\overline{{\\mathbf{X}}})(p\\mathbb{E}_{\\boldsymbol{\\iota}^{\\prime}}[\\sqrt{\\boldsymbol{\\iota}^{\\prime}}]+L_{2}\\mathbb{\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "685 where step $\\textcircled{1}$ uses the triangle inequality; step $\\circledcirc$ uses the norm inequality and $\\forall t$ , $\\|\\mathbf{V}^{t}\\|_{\\mathsf{F}}\\leq\\overline{{\\mathbf{V}}}$ ; step $\\circled{3}$   \n686 uses the definition of $\\forall i$ $\\mathbf{\\Xi}^{\\prime}\\boldsymbol{i},\\Omega_{i2}=\\mathbf{U}_{\\mathtt{B}_{(i)}}^{\\top}[\\tilde{\\mathbf{G}}^{t}-\\tilde{\\mathbf{G}}^{t}][\\mathbf{X}^{t}]^{\\top}\\mathbf{U}_{\\mathtt{B}_{(i)}}$ in (46); step $\\circledast$ uses the norm inequality   \n687 and $\\forall t$ $\\mathbf{\\boldsymbol{\\ell}},\\mathbf{\\boldsymbol{\\|}X^{t}\\|}_{\\mathsf{F}}\\leq\\overline{{\\mathbf{X}}}$ ; step $\\textcircled{5}$ uses Lemma E.1; step $\\circled{6}$ uses Part $(b)$ in Lemma 2.5 and $\\forall t$ , $\\|\\mathbf{X}^{t}\\|_{\\mathsf{F}}\\leq\\overline{{\\mathbf{X}}}$ .   \n688 In view of( 51), (52), (53), (54), and (50), we have: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{t^{t+1}}[\\|\\nabla_{\\mathcal{I}}\\mathcal{T}(\\mathbf{I}_{2};\\mathbf{X}^{t+1},\\mathbf{B}^{t+1})\\|_{\\mathsf{F}}]}\\\\ {\\leq}&{\\frac{n p}{2}(\\overline{{\\mathbf{X}}}+\\overline{{\\mathbf{V}}}^{2}\\overline{{\\mathbf{X}}})\\mathbb{E}_{t^{t}}[\\sqrt{u^{t}}]+(c_{1}+c_{2}+c3+c4)\\cdot\\mathbb{E}_{t^{t}}[\\sum_{i=1}^{n/2}\\|\\bar{\\mathbf{V}}_{i}^{t}-\\mathbf{I}_{2}\\|_{\\mathsf{F}}]}\\\\ {=}&{\\frac{n p}{2}(\\overline{{\\mathbf{X}}}+\\overline{{\\mathbf{V}}}^{2}\\overline{{\\mathbf{X}}})\\mathbb{E}_{t^{t}}[\\sqrt{u^{t}}]+\\phi\\mathbb{E}_{t^{t}}[\\sum_{i=1}^{n/2}\\|\\bar{\\mathbf{V}}_{i}^{t}-\\mathbf{I}_{2}\\|_{\\mathsf{F}}]}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "689 where $c_{1}=2\\overline{{\\mathrm{XG}}}$ , $c_{2}=(1+\\overline{{\\mathbf{V}}}^{2})(L_{f}+\\theta),c_{3}=(\\overline{{\\mathbf{X}}}+\\overline{{\\mathbf{V}}}\\overline{{\\mathbf{X}}})\\overline{{\\mathbf{G}}}$ , and $c_{4}=\\textstyle{\\frac{n}{2}}({\\overline{{\\mathbf{X}}}}^{2}+{\\overline{{\\mathbf{V}}}}^{2}{\\overline{{\\mathbf{X}}}}^{2})L_{f}$ . ", "page_idx": 23}, {"type": "text", "text": "690 Lemma E.3. We have the following results: $\\begin{array}{r}{\\mathrm{dist}(\\mathbf{0},\\nabla_{\\mathcal{I}}f(\\mathbf{X}^{t}))\\;\\leq\\;\\gamma\\,\\cdot\\,\\|\\nabla_{\\mathcal{I}}\\mathcal{T}(\\mathbf{I}_{2};\\mathbf{X}^{t},\\mathbf{B})\\|_{\\mathsf{F}}\\;+}\\end{array}$   \n691 $2\\overline{{\\mathbf{X}}}^{2}\\sqrt{\\mathbb{E}_{\\iota^{t}}[u^{t}]}$ with $\\gamma\\triangleq\\overline{{\\mathbf{X}}}\\sqrt{C_{n}^{2}}$ . ", "page_idx": 23}, {"type": "text", "text": "692 Proof. We have the following inequalities: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\|\\nabla_{\\mathcal{I}}f(\\mathbf{X}^{t})\\|_{\\mathsf{F}}}&{\\overset{\\Omega}{=}\\;\\;\\|\\nabla f(\\mathbf{X}^{t})-\\mathbf{J}\\mathbf{X}^{t}(\\nabla f(\\mathbf{X}^{t}))^{\\top}\\mathbf{X}^{t}\\mathbf{J}\\|_{\\mathsf{F}}}\\\\ &{\\overset{\\Omega}{=}\\;\\;\\|\\nabla f(\\mathbf{X}^{t})(\\mathbf{X}^{t})^{\\top}\\mathbf{J}\\mathbf{X}^{t}\\mathbf{J}-\\mathbf{J}\\mathbf{X}^{t}(\\nabla f(\\mathbf{X}^{t}))^{\\top}\\mathbf{J}\\mathbf{J}\\mathbf{X}^{t}\\mathbf{J}\\|_{\\mathsf{F}}}\\\\ &{\\overset{\\Omega}{\\leq}\\;\\;\\|\\nabla f(\\mathbf{X}^{t})(\\mathbf{X}^{t})^{\\top}-\\mathbf{J}\\mathbf{X}^{t}(\\nabla f(\\mathbf{X}^{t}))^{\\top}\\mathbf{J}\\|_{\\mathsf{F}}\\|\\mathbf{J}\\mathbf{X}^{t}\\mathbf{J}\\|_{\\mathsf{F}}}\\\\ &{\\overset{\\Theta}{\\leq}\\;\\;\\overline{{\\mathbf{X}}}\\|\\nabla f(\\mathbf{X}^{t})(\\mathbf{X}^{t})^{\\top}-\\mathbf{J}\\mathbf{X}^{t}(\\nabla f(\\mathbf{X}^{t}))^{\\top}\\mathbf{J}\\|_{\\mathsf{F}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "693 where step $\\textcircled{1}$ uses the definition of $\\nabla_{\\mathcal{I}}f(\\mathbf{X}^{t})$ ; step $\\circledcirc$ uses $\\mathbf{J}\\mathbf{J}=\\mathbf{I}$ and $\\mathbf{X}^{\\top}\\mathbf{J}\\mathbf{X}=\\mathbf{J}\\Rightarrow\\mathbf{X}^{\\top}\\mathbf{J}\\mathbf{X}\\mathbf{J}=$   \n694 $\\mathbf{J}\\mathbf{J}=\\mathbf{I}$ ; step $\\circled{3}$ uses the norm inequality and ; step $\\circledast$ uses $\\forall t$ , $\\|\\mathbf{X}^{t}\\|_{\\mathsf{F}}\\leq\\overline{{\\mathbf{X}}}$ . ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Consider~}\\|\\nabla f({\\mathbf{X}}^{t})({\\mathbf{X}}^{t})^{\\top}-\\mathbf{J}{\\mathbf{X}}^{t}(\\nabla f({\\mathbf{X}}^{t}))^{\\top}\\mathbf{J}\\|{\\mathbf{\\Xi}}:}\\\\ &{\\qquad\\|\\nabla f({\\mathbf{X}}^{t})({\\mathbf{X}}^{t})^{\\top}-\\mathbf{J}{\\mathbf{X}}^{t}(\\nabla f({\\mathbf{X}}^{t}))^{\\top}\\mathbf{J}\\|{\\mathbf{\\Xi}}}\\\\ {\\overset{\\textregistered}{\\leq}}&{\\|\\tilde{\\mathbf{G}}^{t}({\\mathbf{X}}^{t})^{\\top}-\\mathbf{J}{\\mathbf{X}}^{t}(\\tilde{\\mathbf{G}}^{t})^{\\top}\\mathbf{J}\\|{\\mathbf{\\Xi}}_{t}+\\|(\\nabla f({\\mathbf{X}}^{t})-\\tilde{\\mathbf{G}}^{t})({\\mathbf{X}}^{t})^{\\top}-\\mathbf{J}{\\mathbf{X}}^{t}(\\nabla f({\\mathbf{X}}^{t})-\\tilde{\\mathbf{G}}^{t})^{\\top}\\mathbf{J}\\|{\\mathbf{\\Xi}}}\\\\ {\\overset{\\textregistered}{\\leq}}&{\\|\\tilde{\\mathbf{G}}^{t}({\\mathbf{X}}^{t})^{\\top}-\\mathbf{J}{\\mathbf{X}}^{t}(\\tilde{\\mathbf{G}}^{t})^{\\top}\\mathbf{J}\\|{\\mathbf{\\Xi}}_{t}+\\|\\nabla f({\\mathbf{X}}^{t})-\\tilde{\\mathbf{G}}^{t}\\|_{\\mathsf{F}}\\cdot\\|{\\mathbf{X}}^{t}\\|_{\\mathsf{F}}+\\|{\\mathbf{X}}^{t}\\|_{\\mathsf{F}}\\cdot\\|\\nabla f({\\mathbf{X}}^{t})-\\tilde{\\mathbf{G}}^{t}\\|{\\mathsf{F}}}\\\\ {\\overset{\\textregistered}{\\leq}}&{\\|\\tilde{\\mathbf{G}}^{t}({\\mathbf{X}}^{t})^{\\top}-\\mathbf{J}{\\mathbf{X}}^{t}(\\tilde{\\mathbf{G}}^{t})^{\\top}\\mathbf{J}\\|_{\\mathsf{F}}+2\\overline{{\\mathbf{X}}}\\sqrt{\\mathbb{E}_{t}[u^{t}]}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "696 where step $\\begin{array}{r}{\\mathbb{\\Phi}{\\mathrm{~uses~}}\\forall\\mathbf{A},\\mathbf{B},\\|\\mathbf{A}\\|_{\\mathsf{F}}-\\|\\mathbf{B}\\|_{\\mathsf{F}}\\leq\\|\\mathbf{A}-\\mathbf{B}\\|_{\\mathsf{F}}}\\end{array}$ ; step $\\circledcirc$ uses the norm inequality; step $\\circled{3}$   \n697 uses $\\forall t$ , $\\|\\mathbf{X}^{t}\\|_{\\mathsf{F}}\\leq\\overline{{\\mathbf{X}}}$ . Thus, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\|\\nabla_{\\mathcal{I}}F({\\mathbf x}^{t})\\|_{\\mathsf{F}}}&{\\leq}&{\\overline{{\\mathbf x}}\\|\\tilde{\\mathbf G}^{t}({\\mathbf x}^{t})^{\\top}-\\mathbf J{\\mathbf X}^{t}(\\tilde{\\mathbf G}^{t})^{\\top}\\mathbf J\\|_{\\mathsf{F}}+2\\overline{{\\mathbf X}}^{2}\\sqrt{\\mathbb{E}_{t}\\mathbf\\Xi[u^{t}]}\\,}\\\\ &{\\overset{\\mathrm{()}}{\\leq}}&{\\overline{{\\mathbf x}}\\sqrt{C_{n}^{2}}\\cdot\\|\\sum_{i=1}^{n/2}\\mathbf U_{\\mathbb{B}_{(i)}}^{\\top}[\\tilde{\\mathbf G}^{t}({\\mathbf X}^{t})^{\\top}-\\mathbf J{\\mathbf X}^{t}(\\tilde{\\mathbf G}^{t})^{\\top}\\mathbf J\\mathbf U_{\\mathbb{B}_{(i)}}\\|_{\\mathsf{F}}]+2\\overline{{\\mathbf X}}^{2}\\sqrt{\\mathbb{E}_{t}\\mathbf\\Xi[u^{t}]}\\,}\\\\ &{\\overset{\\mathrm{()}}{=}}&{\\overline{{\\mathbf X}}\\sqrt{C_{n}^{2}}\\cdot\\|\\nabla_{\\mathcal{I}}\\mathcal{T}(\\mathbf I_{2};{\\mathbf X}^{t},{\\mathbf B})\\|_{\\mathsf{F}}+2\\overline{{\\mathbf X}}^{2}\\sqrt{\\mathbb{E}_{t}\\mathbf\\Xi[u^{t}]}\\,}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "698 where step $\\textcircled{1}$ uses Lemma A.1 with $\\mathbf{W}=\\tilde{\\mathbf{G}}^{t}(\\mathbf{X}^{t})^{\\top}-\\mathbf{J}\\mathbf{X}^{t}(\\tilde{\\mathbf{G}}^{t})^{\\top}\\mathbf{J}$ and $k\\,=\\,2$ ; step $\\circledcirc$ uses the   \n699 definition of $\\nabla_{\\mathcal{I}}\\mathcal{T}(\\mathbf{I}_{2};\\mathbf{X}^{t},\\mathtt{B})$ . \u53e3 ", "page_idx": 23}, {"type": "text", "text": "700 We now present the following useful lemma. ", "page_idx": 23}, {"type": "text", "text": "Lemma E.4. We define $\\mathbf{T}_{\\mathbf{X}}{\\mathcal{I}}\\triangleq\\{\\mathbf{Y}\\in\\mathbb{R}^{n\\times n}\\mid A_{X}(\\mathbf{Y})=\\mathbf{0}\\}$ and $\\mathcal{A}_{\\mathbf{X}}(\\mathbf{Y})\\triangleq\\mathbf{X}^{\\top}\\mathbf{J}\\mathbf{Y}+\\mathbf{Y}^{\\top}\\mathbf{J}\\mathbf{X}$ . For any $\\mathbf{G}\\in\\mathbb{R}^{n\\times n}$ and $\\mathbf{X}^{\\top}\\mathbf{J}\\mathbf{X}=\\mathbf{J}$ , the unique minimizer of the following optimization problem: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\bar{\\mathbf{Y}}=\\arg\\operatorname*{min}_{\\mathbf{Y}\\in\\mathbf{T}_{\\mathbf{X}M}\\mathcal{I}}h(\\mathbf{Y})=\\frac{1}{2}\\|\\mathbf{Y}-\\mathbf{G}\\|_{\\mathsf{F}}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "701 satisify $h(\\bar{\\mathbf Y})\\leq h(\\mathbf G-\\mathbf J\\mathbf X\\mathbf G^{\\top}\\mathbf X\\mathbf J)$ . ", "page_idx": 23}, {"type": "text", "text": "702 Proof. We note that $\\begin{array}{r l r}{\\bar{\\bf Y}=\\arg\\operatorname*{min}_{{\\bf Y}\\in{\\bf T}_{\\bf X},\\mathcal{T}}\\frac{1}{2}\\|{\\bf Y}-{\\bf G}\\|_{\\mathsf{F}}^{2}}&{{}}&{=}&{\\quad\\arg\\operatorname*{min}_{{\\bf Y}}\\frac{1}{2}\\|{\\bf Y}-{\\bf G}\\|_{\\mathsf{F}}^{2}}\\end{array}$ ,   \n703 s.t. $\\mathbf{X}^{\\top}\\mathbf{J}\\mathbf{Y}+\\mathbf{Y}^{\\top}\\mathbf{J}\\mathbf{X}=\\mathbf{0}$ . Introducing a multiplier $\\mathbf{A}\\in\\mathbb{R}^{n\\times n}$ for the linear con  \n704 straints $\\mathbf{X}^{\\top}\\mathbf{J}\\mathbf{Y}+\\mathbf{Y}^{\\top}\\mathbf{J}\\mathbf{X}=\\mathbf{0}$ , we have following Lagrangian function: $\\tilde{\\mathcal{L}}(\\mathbf{Y};\\mathbf{A})=$   \n705 $\\begin{array}{r}{\\frac{1}{2}\\|\\mathbf{Y}-\\mathbf{G}\\|_{\\mathrm{F}}^{2}+\\langle\\mathbf{X}^{\\top}\\mathbf{J}\\mathbf{Y}+\\mathbf{Y}^{\\top}\\mathbf{J}\\mathbf{X},\\mathbf{A}\\rangle}\\end{array}$ . We naturally derive the following first-order optimality   \n706 condition: $\\mathbf{Y}-\\mathbf{G}+\\mathbf{J}\\mathbf{X}\\mathbf{A}=\\mathbf{0}$ , $\\mathbf{X}^{\\top}\\mathbf{J}\\mathbf{Y}+\\mathbf{Y}^{\\top}\\mathbf{J}\\mathbf{X}=\\mathbf{0}$ . Incorporating the term $\\mathbf{Y}=\\mathbf{G}-\\mathbf{J}\\mathbf{X}\\mathbf{A}$   \n707 into $\\mathbf{X}^{\\top}\\mathbf{J}\\mathbf{Y}+\\mathbf{Y}^{\\top}\\mathbf{J}\\mathbf{X}=\\mathbf{0}$ , we obtain: ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbf{X}^{\\top}\\mathbf{X}\\mathbf{A}+\\mathbf{A}^{\\top}\\mathbf{X}^{\\top}\\mathbf{X}=\\mathbf{G}^{\\top}\\mathbf{J}\\mathbf{X}+\\mathbf{X}^{\\top}\\mathbf{J}\\mathbf{G}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "708 Any $\\Lambda$ satisfying formula (55) is a feasible point, so we can easily find : ", "page_idx": 24}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}&{\\mathbf{X}^{\\mathsf{T}}\\mathbf{X}\\mathbf{A}=\\mathbf{X}^{\\mathsf{T}}\\mathbf{J}\\mathbf{G}}\\\\ {{\\overset{\\textregistered}{\\Rightarrow}}}&{\\mathbf{X}\\mathbf{A}=\\mathbf{J}\\mathbf{G}}\\\\ {{\\overset{\\textregistered}{\\Rightarrow}}}&{\\mathbf{X}^{\\mathsf{T}}\\mathbf{J}\\mathbf{X}\\mathbf{A}=\\mathbf{X}^{\\mathsf{T}}\\mathbf{J}\\mathbf{J}\\mathbf{G}}\\\\ {{\\overset{\\textregistered}{\\Rightarrow}}}&{\\mathbf{J}\\mathbf{\\boldsymbol{\\Lambda}}=\\mathbf{X}^{\\mathsf{T}}\\mathbf{G}}\\\\ {{\\overset{\\textregistered}{\\Rightarrow}}}&{\\mathbf{\\boldsymbol{\\Lambda}}=\\mathbf{J}\\mathbf{X}^{\\mathsf{T}}\\mathbf{G}}\\\\ {{\\overset{\\textregistered}{\\Rightarrow}}}&{\\mathbf{\\boldsymbol{\\Lambda}}=\\mathbf{G}^{\\mathsf{T}}\\mathbf{X}\\mathbf{J}}\\end{array}}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "709 where step $\\textcircled{1}$ uses the fact that any matrix $\\mathbf{X}$ satisfying the J-orthogonality constraint has a determinant   \n710 of 1 or $^{-1}$ , thus $\\operatorname{inv}(\\mathbf{X})$ exists; step $\\circledcirc$ multiply both sides of the equation by XJ;step $\\circled{3}$ uses   \n711 $\\mathbf{X}^{T}\\mathbf{J}\\mathbf{X}=\\mathbf{J}$ and $\\mathbf{J}\\mathbf{J}=\\mathbf{I}$ ; step $\\circledast$ multiply both sides of the equation by $\\mathbf{J}$ and uses $\\mathbf{J}\\mathbf{J}=\\mathbf{I}$ ; step $\\textcircled{5}$   \n712 uses the fact that $\\Lambda$ is a symmetric matrix.   \n713 Therefore, a feasible solution $\\mathbf{Y}$ can be computed as $\\mathbf{Y}=\\mathbf{G}-\\mathbf{J}\\mathbf{X}\\mathbf{A}=\\mathbf{G}-\\mathbf{J}\\mathbf{X}\\mathbf{G}^{\\top}\\mathbf{X}\\mathbf{J}$ . Since $\\bar{\\mathbf Y}$   \n714 is the optimal solution, there must be $h(\\bar{\\mathbf Y})\\stackrel{*}{\\leq}h(\\mathbf G-\\mathbf J\\mathbf X\\mathbf G^{\\top}\\mathbf X\\mathbf J)$ . \u53e3 ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "715 We now present the proof of this lemma. ", "page_idx": 24}, {"type": "text", "text": "716 Lemma E.5. For any $\\mathbf{X}\\in\\mathbb{R}^{n\\times n}$ , it holds that $\\operatorname{dist}(\\mathbf{0},\\nabla f^{\\circ}(\\mathbf{X}))\\leq\\operatorname{dist}(\\mathbf{0},\\nabla_{\\mathcal{I}}f(\\mathbf{X})).$ ", "page_idx": 24}, {"type": "text", "text": "717 Proof. For the purpose of analysis, we define the nearest J orthogonal matrix to an arbitrary matrix   \n718 $\\mathbf{Y}\\in\\mathbb{R}^{n\\times n}$ is given by $\\mathcal{P}_{\\mathcal{I}}(\\mathbf{X})$ . Similarly, we have $\\mathcal{P}_{\\mathbf{T}_{\\mathbf{X}}\\mathcal{I}}(\\nabla f(\\bar{\\mathbf{X}}))$ for projecting gradient $\\nabla f(\\mathbf{X})$   \n719 into space $\\mathbf{T}\\mathbf{x}\\mathcal{I}$ . ", "page_idx": 24}, {"type": "text", "text": "720 We recall that the following first-order optimality conditions are equivalent for all $\\mathbf{X}\\in\\mathbb{R}^{n\\times n}$ : ", "page_idx": 24}, {"type": "equation", "text": "$$\n(\\mathbf{0}\\in\\nabla f^{\\circ}(\\mathbf{X}))\\Leftrightarrow(\\mathbf{0}\\in\\mathcal{P}_{\\mathbf{T}_{\\mathbf{X}}\\mathcal{I}}(\\nabla f(\\mathbf{X}))).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "721 Therefore, we derive the following results: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{l l l}{\\mathrm{dist}(\\mathbf{0},\\nabla f^{\\circ}(\\mathbf{X}))}&{=}&{\\operatorname*{inf}_{\\mathbf{Y}\\in\\nabla f^{\\circ}(\\mathbf{X})}\\|\\mathbf{Y}\\|_{\\mathsf{F}}}\\\\ &{=}&{\\operatorname*{inf}_{\\mathbf{Y}\\in\\mathcal{P}(\\mathbf{T}_{\\mathbf{X}}\\mathcal{I})}(\\nabla f(\\mathbf{X}))\\left\\|\\mathbf{Y}\\right\\|_{\\mathsf{F}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "722 We let $\\mathbf{G}\\in\\nabla f(\\mathbf{X})$ and obtain the following results from the above equality: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathrm{dist}(\\mathbf{0},\\nabla f^{\\circ}(\\mathbf{X}))}&{\\overset{\\mathbb{\\Omega}}{\\leq}\\quad\\|\\mathbf{G}-\\mathbf{JXG}^{\\top}\\mathbf{X}\\mathbf{J}\\|_{\\mathsf{F}},}\\\\ &{\\overset{\\mathbb{\\Omega}}{=}\\quad\\|\\nabla_{\\mathcal{I}}f(\\mathbf{X})\\|_{\\mathsf{F}}\\triangleq\\mathrm{dist}(\\mathbf{0},\\nabla_{\\mathcal{I}}f(\\mathbf{X})).}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "723 where step $\\textcircled{1}$ uses Lemma E.4; step $\\circledcirc$ uses $\\nabla_{\\mathcal{I}}f(\\mathbf{X})=\\mathbf{G}-\\mathbf{J}\\mathbf{X}\\mathbf{G}^{\\top}\\mathbf{X}\\mathbf{J}$ with $\\mathbf{G}\\in\\nabla f(\\mathbf{X})$ . ", "page_idx": 24}, {"type": "text", "text": "724 First of all, since $f^{\\circ}(\\mathbf{X})\\triangleq f(\\mathbf{X})+{\\mathcal{Z}}_{\\mathcal{I}}(\\mathbf{X})$ is a $\\mathrm{KL}$ function, we have from Proposition 4.8 that: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l r c l}{\\frac{1}{\\varphi^{\\prime}(f^{\\circ}(\\mathbf{X}^{\\prime})-f^{\\circ}(\\mathbf{X}))}}&{\\leq}&{\\mathrm{dist}(0,\\nabla f^{\\circ}(\\mathbf{X}^{\\prime}))}\\\\ &{\\stackrel{\\oslash}{=}}&{\\|\\nabla_{\\mathcal{I}}f(\\mathbf{X}^{\\prime})\\|_{\\mathsf{F}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "725 where step $\\textcircled{1}$ uses Lemma E.5. Here, $\\varphi(\\cdot)$ is some certain concave desingularization function. Since   \n726 $\\varphi(\\cdot)$ is concave, we have: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\forall\\Delta\\in\\mathbb{R},\\Delta^{+}\\in\\mathbb{R},\\varphi(\\Delta^{+})+(\\Delta-\\Delta^{+})\\varphi^{\\prime}(\\Delta)\\leq\\varphi(\\Delta).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "727 Applying the inequality above with $\\Delta=f(\\mathbf{X}^{t})-f(\\bar{\\mathbf{X}})$ and $\\Delta^{+}=f(\\mathbf{X}^{t+1})-f(\\bar{\\mathbf{X}})$ , we have: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(f(\\mathbf{X}^{t})-f(\\mathbf{X}^{t+1}))\\varphi^{\\prime}(f(\\mathbf{X}^{t})-f(\\bar{\\mathbf{X}}))}\\\\ {\\leq}&{\\varphi(f(\\mathbf{X}^{t})-f(\\bar{\\mathbf{X}}))-\\varphi(f(\\mathbf{X}^{t+1})-f(\\bar{\\mathbf{X}}))\\triangleq\\mathcal{E}^{t}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "728 With the sufficient descent condition as shown in Theorem 4.7, we derive the following inequalities: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\iota}t\\left[\\frac{\\theta}{2}\\sum_{i=1}^{n/2}\\|\\bar{\\mathbf{V}}_{i}^{t}-\\mathbf{I}_{2}\\|_{\\mathsf{F}}^{2}\\right]}\\\\ {\\leq}&{\\mathbb{E}_{\\iota}t\\left[f(\\mathbf{X}^{t})-f(\\mathbf{X}^{t+1})\\right]+\\frac{1}{2}\\mathbb{E}_{\\iota}t\\left[\\|\\mathbf{X}^{t}\\|_{\\mathsf{F}}^{2}\\right]\\!\\mathbb{E}_{\\iota}t\\left[\\sum_{i=1}^{n/2}\\|\\bar{\\mathbf{V}}_{i}^{t}-\\mathbf{I}_{2}\\|_{\\mathsf{F}}^{2}\\right]+\\frac{1}{2}\\mathbb{E}_{\\iota}t\\left[u^{t}\\right]}\\\\ {\\overset{\\mathrm{(i)}}{\\Rightarrow}}&{\\mathbb{E}_{\\iota}t\\left[\\frac{\\theta-\\overline{{\\mathbf{X}}}^{2}}{2}\\sum_{i=1}^{n/2}\\|\\bar{\\mathbf{V}}_{i}^{t}-\\mathbf{I}_{2}\\|_{\\mathsf{F}}^{2}\\right]\\!\\!\\leq\\!\\!\\mathbb{E}_{\\iota}t\\left[f(\\mathbf{X}^{t})-f(\\mathbf{X}^{t+1})\\right]+\\frac{1}{2}\\mathbb{E}_{\\iota}t\\left[u^{t}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "729 where step $\\textcircled{1}$ uses $\\forall t$ , $\\|\\mathbf{X}^{t}\\|_{\\mathsf{F}}\\leq\\overline{{\\mathbf{X}}}$ . ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\mathbf{x}}\\left\\{\\frac{\\mathbf{x}^{2}-\\sum_{j=1}^{K-1}\\mathbf{x}_{j}^{(2)}\\prod_{s=1}^{(1)}\\mathbf{y}_{j}^{(1)}}{\\mathbf{x}_{j}}\\right\\}}\\\\ {\\leq}&{\\mathbb{E}_{\\mathbf{x}}\\left\\{\\frac{\\mathbf{y}^{2}}{\\rho(T)}[\\frac{\\mathbf{y}^{2}}{\\gamma-\\mathbf{x}_{j}^{(1)}}]+\\frac{\\mathbf{z}}{\\mathbf{z}_{j}}[\\mathbf{u}^{\\prime}]\\right.}\\\\ {\\leq}&{\\mathbb{E}_{\\mathbf{x}},[\\varepsilon^{(1)}\\nabla_{\\mathbf{x}}f(\\mathbf{X})]\\Big\\}+\\frac{1}{2}\\mathbb{E}_{\\mathbf{x}},[u^{\\prime}]}\\\\ {\\leq}&{\\mathbb{E}_{\\mathbf{x}},[\\varepsilon^{(2)}\\nabla_{\\mathbf{x}}f(\\mathbf{u}_{2})\\mathbf{X},[\\mathbf{b}]]\\in\\mathcal{X}^{C}\\mathbb{Y}^{C}\\mathbb{Y}^{C}\\mathbb{Y}_{\\mathbf{x}}[u^{\\prime}]+\\frac{1}{2}\\mathbb{E}_{\\mathbf{x}},[u^{\\prime}]}\\\\ {\\leq}&{\\mathbb{E}_{\\mathbf{x}},[\\varepsilon^{(3)}\\nabla_{\\mathbf{x}}f(\\mathbf{u}_{2})\\mathbf{X}_{\\mathbf{x}}^{\\top}-\\mathbf{I}_{\\mathbf{x}}]\\in[\\mathbf{x}^{\\prime}-\\mathbf{y}_{2}]\\mathbb{E}_{\\mathbf{x}}\\left(\\mathbb{X}^{\\top}\\mathbb{Y}_{\\mathbf{x}}\\right)\\mathbb{C}\\mathbb{Y}_{\\mathbf{x}}[\\mathbf{u}^{\\prime}]}\\\\ {+}&{2\\varepsilon^{C}\\mathbb{X}^{C}\\mathbb{Y}_{\\mathbf{x}}[\\mathbf{u}^{\\prime}]+\\frac{1}{2}\\mathbb{E}_{\\mathbf{x}}[u^{\\prime}]}\\\\ {\\leq}&{\\mathbb{E}_{\\mathbf{x}},[\\varepsilon^{(1)}\\gamma\\delta]\\mathbb{Y}_{\\mathbf{x}}[\\sum_{j=1}^{(2)}\\mathbb{Y}_{\\mathbf{x}}^{\\top}-\\mathbf{I}_{\\mathbf{x}}]\\mathbb{I}_{\\mathbf{y}}^{2}}\\\\ &{+\\delta^{C}\\mathbb{U}\\mathbb{X}^{C}\\mathbb{Y}^{C}\\mathbb{Y}^{\\top}\\mathbb{\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "730 where step $\\textcircled{1}$ uses the sufficient descent condition as shown in Theorem 4.7; step   \n731 $\\circledcirc$ uses Inequality (64) and (62) with $\\begin{array}{r l r}{{\\bf X}^{\\prime}}&{{}=}&{{\\bf X}^{t}}\\end{array}$ and $\\begin{array}{r l r}{{\\bf X}}&{{}=}&{\\bar{\\bf X}}\\end{array}$ ; step $\\circled{3}$ uses lemma   \n732 ${\\begin{array}{r l r l}{{3}.3}&{;}&{{\\mathrm{step~}}^{\\cdot}\\;{\\mathrm{~?~}}{\\mathrm{~uses~\\Gamma~Lemma~\\underbrace{E.2}~}}\\;{\\mathrm{~step~}}\\;{\\mathrm{~step~}}\\;{\\mathrm{~5~}}\\;{\\mathrm{~uses~\\underbrace{\\nabla[x_{i}~}~\\in~\\mathrm{~\\mathbb{R}},~{\\frac{~x_{i}~}{~\\mu}}~}}\\times{\\mathrm{~\\mathbb{R}}},{\\frac{\\mathrm{~]}}{n}}+{\\sqrt{\\frac{x_{1}^{2}+\\cdots+x_{n}^{2}}{n}}}}\\\\ &{}&{{\\mathrm{step~\\Gamma~6}}\\;\\;{\\mathrm{~applies~\\Psi~the~}}\\;{\\mathrm{inequality~\\Psi~that~}}\\;\\forall\\theta^{\\prime}\\;\\quad>\\;0,a,b,a b\\quad\\leq\\;\\;\\;{\\frac{\\theta^{\\prime}a^{2}}{2}}\\;+\\;{\\frac{b^{2}}{2\\theta^{\\prime}}}\\;{\\mathrm{~with~}}}\\\\ &{}&{\\quad\\quad:={\\sqrt{\\sum_{i=1}^{n/2}\\|{\\bar{\\mathbf{V}}}_{i}^{t-1}-\\mathbf{I}_{2}\\|_{\\mathsf{F}}^{2}}},b=\\mathcal{E}^{t}\\gamma\\phi{\\sqrt{\\frac{n}{2}}};a={\\sqrt{\\mathbb{E}_{\\imath}\\!\\!\\!\\!{\\ell}}}{\\mathrm{~[}u^{t}]},b=\\mathcal{E}^{t}(2{\\overline{{\\mathbf{X}}}}^{2}+\\gamma{\\frac{n p}{2}}{\\overline{{\\mathbf{X}}}}+\\gamma{\\frac{n p}{2}}{\\overline{{\\nabla^{2}}}}{\\overline{{\\mathbf{X}}}});}\\end{array}}$   \n733 ;   \n734   \n7 35 step \u2466 denote A2 \u225c (2X+\u03b3 2 $\\begin{array}{r l r}{\\mathfrak{A}^{2}\\!}&{\\triangleq}&{\\!\\frac{(2\\overline{{\\Lambda}}^{2}+\\gamma\\frac{n p}{2}\\overline{{\\Lambda}}+\\gamma\\frac{n p}{2}\\overline{{\\mathrm{V}}}^{2}\\overline{{\\mathrm{X}}})^{2}}{2\\bar{\\theta}}\\;+\\;\\frac{n\\gamma^{2}\\phi^{2}}{4\\theta^{\\prime}}}\\end{array}$ To simplify the formula, we define   \n736 $\\begin{array}{r}{\\aleph^{t}=\\sum_{i=1}^{n/2}\\|\\bar{\\mathbf{V}}_{i}^{t}-\\mathbf{I}_{2}\\|_{\\mathsf{F}}^{2}}\\end{array}$ . ", "page_idx": 25}, {"type": "text", "text": "737 Multiplying both sides by 2 and taking the square root of both sides, we have: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\mathbb{E}_{\\iota^{t}}[\\sqrt{\\theta-\\overline{{\\mathbf{X}}}^{2}}\\sqrt{\\mathsf{N}t}]}&{\\leq}&{\\sqrt{\\mathbb{E}_{\\iota^{t}}[\\mathscr{E}^{t^{2}}\\mathfrak{A}^{2}+\\theta^{\\prime}\\mathbb{N}^{t-1}]+(\\bar{\\theta}+1)\\mathbb{E}_{\\iota^{t}}[u^{t}]}}\\\\ &{\\leq}&{\\sqrt{\\mathbb{E}_{\\iota^{t}}[\\mathscr{E}^{t^{2}}\\mathfrak{A}^{2}]}+\\mathbb{E}_{\\iota^{t-1}}[\\sqrt{\\theta^{\\prime}\\mathbb{N}^{t-1}}]+\\sqrt{(\\bar{\\theta}+1)\\mathbb{E}_{\\iota^{t}}[u^{t}]}}\\\\ &{\\leq}&{\\mathscr{E}^{t}\\mathfrak{A}+\\sqrt{\\theta^{\\prime}}\\mathbb{E}_{\\iota^{t-1}}[\\sqrt{\\mathbb{N}^{t-1}}]+\\sqrt{(\\bar{\\theta}+1)}\\sqrt{\\mathbb{E}_{\\iota^{t}}[u^{t}]}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "738 To recursively eliminate term $\\sqrt{(\\bar{\\theta}+1)\\mathbb{E}_{\\iota^{t}}[u^{t}]}$ , we take the root of both sides of the Inequality in   \n739 Lemma 4.5: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\sqrt{\\mathbb{E}_{\\iota^{t}}[u^{t}]}}&{\\leq}&{\\sqrt{\\frac{p(N-b)}{b(N-1)}\\sigma^{2}}+\\sqrt{(1-p)\\mathbb{E}_{\\iota^{t-1}}[u^{t-1}]}+\\sqrt{\\frac{L_{f}^{2}\\overline{{X}}^{2}(1-p)}{b^{\\prime}}\\mathbb{E}_{\\iota^{t-1}}[{\\mathbb{N}}^{t-1}]}}\\\\ &{\\leq}&{\\sqrt{\\frac{p(N-b)}{b(N-1)}\\sigma^{2}}+\\sqrt{(1-p)}\\sqrt{\\mathbb{E}_{\\iota^{t-1}}[u^{t-1}]}+\\sqrt{\\frac{L_{f}^{2}\\overline{{X}}^{2}(1-p)}{b^{\\prime}}}\\sqrt{\\mathbb{E}_{\\iota^{t-1}}[{\\mathbb{N}}^{t-1}]}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "740 Adding Inequality ${\\frac{\\sqrt{\\bar{\\theta}\\!+\\!1}}{1\\!-\\!{\\sqrt{1\\!-\\!p}}}}\\times(70)$ to (69) ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\mathbb{E}_{\\iota^{\\varepsilon}}[\\sqrt{\\theta-\\overline{{\\mathbf{X}}}^{2}}\\sqrt{\\mathbb{N}\\iota}]}&{\\leq}&{\\mathcal{E}^{t}\\mathfrak{A}+(\\sqrt{\\theta^{\\prime}}+\\sqrt{\\frac{L_{f}^{2}\\overline{{X}}^{2}(1-p)}{b^{\\prime}}\\frac{\\sqrt{\\theta+1}}{1-\\sqrt{1-p}}})\\mathbb{E}_{\\iota^{t-1}}[\\sqrt{\\mathbb{N}^{t-1}}]\\;+}\\\\ &&{\\frac{\\sqrt{1-p}\\sqrt{(\\overline{{\\theta}}+1)}}{1-\\sqrt{1-p}}(\\sqrt{\\mathbb{E}_{\\iota^{t-1}}[u^{t-1}]}-\\sqrt{\\mathbb{E}_{\\iota^{t}}[u^{t}]})+\\frac{\\sqrt{\\theta+1}}{1-\\sqrt{1-p}}\\sqrt{\\frac{p(N-b)}{b(N-1)}\\sigma^{2}}(71)}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "741 With the choice \u03b8\u2032 = \u03b8\u2212X2 $\\begin{array}{r}{\\sqrt{\\theta^{\\prime}}=\\frac{\\sqrt{\\theta-\\overline{{\\mathrm{X}}}^{2}}}{2}-\\sqrt{\\frac{L_{f}^{2}\\overline{{\\mathrm{X}}}^{2}(1-p)}{b^{\\prime}}}\\frac{\\sqrt{\\bar{\\theta}+1}}{1-\\sqrt{1-p}}}\\end{array}$ 1\u2212\u221a1\u2212p, we have: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\mathbb{E}_{\\boldsymbol\\iota^{t}}[\\sqrt{\\theta-\\overline{{\\mathbf{X}}}^{2}}\\sqrt{\\mathbb{N}t}]}&{\\leq}&{\\mathcal{E}^{t}\\mathfrak{A}+(\\frac{\\sqrt{\\theta-\\overline{{\\mathbf{X}}}^{2}}}{2})\\mathbb{E}_{\\boldsymbol\\iota^{t-1}}[\\sqrt{\\mathbb{N}^{t-1}}]+}\\\\ &{}&{\\frac{\\sqrt{1-p}\\sqrt{(\\bar{\\theta}+1)}}{1-\\sqrt{1-p}}(\\sqrt{\\mathbb{E}_{\\boldsymbol\\iota^{t-1}}[u^{t-1}]}-\\sqrt{\\mathbb{E}_{\\boldsymbol\\iota^{t}}[u^{t}]})+\\frac{\\sqrt{\\bar{\\theta}+1}}{1-\\sqrt{1-p}}\\sqrt{\\frac{p(N-b)}{b(N-1)}\\sigma^{2}}(7}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "742 Rearranging terms, we have: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\iota^{t}}[\\sqrt{\\theta-\\overline{{{X}}}^{2}}\\sqrt{\\mathbb{N}^{t}}]-\\mathbb{E}_{\\iota^{t-1}}[\\frac{\\sqrt{\\theta-\\overline{{{X}}}^{2}}}{2}\\sqrt{\\mathbb{N}^{t-1}}]}\\\\ {\\leq}&{\\mathcal{E}^{t}\\mathfrak{A}+\\frac{\\sqrt{1-p}\\sqrt{(\\bar{\\theta}+1)}}{1-\\sqrt{1-p}}(\\sqrt{\\mathbb{E}_{\\iota^{t-1}}[u^{t-1}]}-\\sqrt{\\mathbb{E}_{\\iota^{t}}[u^{t}]})+\\frac{\\sqrt{\\bar{\\theta}+1}}{1-\\sqrt{1-p}}\\sqrt{\\frac{p(N-b)}{b(N-1)}\\sigma^{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "743 Summing the inequality above over $t=1,2\\ldots,T$ , we have: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\iota^{T}}[\\sqrt{\\theta-\\overline{{\\mathbf{X}}}^{2}}\\sqrt{\\mathbb{N}^{T}}]+\\mathbb{E}_{\\iota^{T-1}}[\\frac{\\sqrt{\\theta-\\overline{{\\mathbf{X}}}^{2}}}{2}\\sum_{t=1}^{T-1}\\sqrt{\\mathbb{N}^{t}}]}\\\\ {\\leq}&{\\mathfrak{A}\\sum_{t=1}^{T}\\mathcal{E}^{t}+\\frac{\\sqrt{1-p}\\sqrt{(\\theta+1)}}{1-\\sqrt{1-p}}(\\sqrt{\\mathbb{E}_{\\iota^{0}}[u^{0}]}-\\sqrt{\\mathbb{E}_{\\iota^{T}}[u^{T}]})+\\frac{T\\sqrt{\\bar{\\theta}+1}}{1-\\sqrt{1-p}}\\sqrt{\\frac{p(N-b)}{b(N-1)}\\sigma^{2}}+\\frac{\\sqrt{\\theta-\\overline{{\\mathbf{X}}}^{2}}}{2}\\sqrt{\\mathbb{N}^{0}}}\\\\ {\\leq}&{\\mathfrak{A}\\sum_{t=1}^{T}\\mathcal{E}^{t}+\\frac{\\sqrt{1-p}\\sqrt{(\\bar{\\theta}+1)}}{1-\\sqrt{1-p}}\\sqrt{\\frac{N-b}{b(N-1)}\\sigma^{2}}+\\frac{T\\sqrt{\\bar{\\theta}+1}}{1-\\sqrt{1-p}}\\sqrt{\\frac{p(N-b)}{b(N-1)}\\sigma^{2}}+\\mathbb{E}_{\\iota^{t}}[\\frac{\\sqrt{\\theta-\\overline{{\\mathbf{X}}}^{2}}}{2}\\sqrt{\\mathbb{N}^{0}}]}\\\\ {\\leq}&{\\mathfrak{A}\\sum_{t=1}^{T}\\mathcal{E}^{t}+\\frac{\\sqrt{1-p}\\sqrt{(\\bar{\\theta}+1)}}{1-\\sqrt{1-p}}\\sqrt{\\frac{N-b}{b(N-1)}\\sigma^{2}}+\\frac{T\\sqrt{\\bar{\\theta}+1}}{1-\\sqrt{1-p}}\\sqrt{\\frac{p(N-b)}{b(N-1)}\\sigma^{2}}+\\frac{\\sqrt{\\theta-\\overline{{\\mathbf{X}}}^{2}}}{2}\\sqrt{\\frac{n}{2}(\\overline{{\\mathbf{V}}}+\\sqrt{2})^{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "744 where step $\\textcircled{1}$ uses the fact that $\\mathbb{E}_{\\iota^{T}}[u^{T}]\\geq0$ and $\\begin{array}{r}{\\mathbb{E}_{\\iota^{0}}[u^{0}]\\leq\\frac{N-b}{b(N-1)}\\sigma^{2}}\\end{array}$ ; step $\\circledcirc$ uses $\\forall t,\\|\\mathbf{V}\\|_{\\mathsf{F}}\\leq\\overline{{\\mathbf{V}}}$   \n745 then, $\\|\\mathbf{V}_{i}-\\mathbf{I}_{2}\\|_{\\mathsf{F}}^{2}\\;\\leq\\;(\\|\\mathbf{V}_{i}\\|_{\\mathsf{F}}+\\|\\mathbf{I}_{2}\\|_{\\mathsf{F}})^{2}\\;\\leq\\;(\\overline{{\\mathbf{X}}}+\\sqrt{2})^{2}$ and $\\begin{array}{r}{\\sum_{i=1}^{n/2}\\|\\bar{\\mathbf{V}}_{i}^{0}-\\mathbf{I}_{2}\\|_{\\mathsf{F}}^{2}\\ \\leq\\ \\frac{n}{2}(\\overline{{\\mathbf{V}}}+\\sqrt{2})^{2}}\\end{array}$ .   \n746 Define $\\begin{array}{r}{\\mathfrak{C}=\\frac{\\sqrt{1-p}\\sqrt{(\\bar{\\theta}+1)}}{1-\\sqrt{1-p}}\\sqrt{\\frac{N-b}{b(N-1)}\\sigma^{2}}+\\frac{T\\sqrt{\\bar{\\theta}+1}}{1-\\sqrt{1-p}}\\sqrt{\\frac{p(N-b)}{b(N-1)}\\sigma^{2}}}\\end{array}$ + 1T\u2212\u221a\u03b8\u00af1+\u22121p bp((NN\u2212\u22121b))\u03c32 and rearrange terms, we have: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\iota^{t}}[\\frac{\\theta-\\overline{{\\mathrm{X}}}^{2}}{2}\\sum_{t=1}^{T}\\sqrt{\\mathrm{N}^{t}}]\\leq\\mathfrak{A}\\sum_{t=1}^{T}\\mathcal{E}^{t}+\\mathfrak{C}+\\frac{\\sqrt{\\theta-\\overline{{\\mathrm{X}}}^{2}}}{2}\\sqrt{\\frac{n}{2}(\\overline{{\\mathrm{V}}}+\\sqrt{2})^{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "747 Considering $\\mathfrak{A}\\sum_{t=1}^{T}\\mathcal{E}^{t}$ , we have: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathfrak{A}\\sum_{t=1}^{T}\\mathcal{E}^{t}}&{\\overset{\\oplus}{=}}&{\\mathfrak{A}\\sum_{t=1}^{T}\\varphi(f(\\mathbf{X}^{t})-f(\\bar{\\mathbf{X}}))-\\varphi(f(\\mathbf{X}^{t+1})-f(\\bar{\\mathbf{X}}))}\\\\ &{\\overset{\\cong}{=}}&{\\mathfrak{A}[\\varphi(f(\\mathbf{X}^{1})-f(\\bar{\\mathbf{X}}))-\\varphi(f(\\mathbf{X}^{T+1})-f(\\bar{\\mathbf{X}}))]}\\\\ &{\\overset{\\cong}{\\leq}}&{\\mathfrak{A}\\varphi(f(\\mathbf{X}^{1})-f(\\bar{\\mathbf{X}}))}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "748 where step $\\textcircled{1}$ uses the definition of $\\mathcal{E}^{i}$ in (64); step $\\circledcirc$ uses a basic recursive reduction; step $\\circled{3}$ uses the   \n749 fact the desingularization function $\\varphi(\\cdot)$ is positive. Combining Inequality (74) and (75), we obtain : ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{t^{t}}[\\frac{\\theta-\\overline{{\\mathbf{X}}}^{2}}{2}\\sum_{t=1}^{T}\\sqrt{\\aleph^{t}}]\\leq2\\mathbb{1}\\varphi(f(\\mathbf{X}^{1})-f(\\bar{\\mathbf{X}}))+\\mathfrak{C}+\\frac{\\sqrt{\\theta-\\overline{{\\mathbf{X}}}^{2}}}{2}\\sqrt{\\frac{n}{2}(\\overline{{\\mathbf{V}}}+\\sqrt{2})^{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "750 Using the inequality that $\\begin{array}{r}{\\frac{\\|\\mathbf{X}^{+}-\\mathbf{X}\\|_{\\mathsf{F}}^{2}}{\\overline{{\\mathbf{X}}}^{2}}\\leq\\frac{\\|\\mathbf{X}^{+}-\\mathbf{X}\\|_{\\mathsf{F}}^{2}}{\\|\\mathbf{X}\\|_{\\mathsf{F}}^{2}}\\leq\\sum_{i=1}^{n/2}\\|\\bar{\\mathbf{V}}_{i}-\\mathbf{I}_{2}\\|_{\\mathsf{F}}^{2}}\\end{array}$ as shown in Part $(b)$ in Lemma   \n751 2.5, we have: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{t^{t}}[\\frac{\\theta-\\overline{{{\\bf X}}}^{2}}{2\\overline{{{\\bf X}}}}\\sum_{t=1}^{T}\\|{\\bf X}^{t+1}-{\\bf X}^{t}\\|_{\\mathsf{F}}]\\le\\mathfrak{A}\\varphi(f({\\bf X}^{1})-f(\\bar{\\bf X}))+\\mathfrak{C}+\\frac{\\sqrt{\\theta-\\overline{{{\\bf X}}}^{2}}}{2}\\sqrt{\\frac{n}{2}(\\overline{{{\\bf V}}}+\\sqrt{2})^{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Since b = N, b\u2032 =\u221ab and p =bb+\u2032b\u2032 , C = 11\u2212\u2212p\u221a1(\u2212\u03b8\u00afp+1) b(NN\u2212\u2212b1)\u03c3 $\\begin{array}{r}{\\mathfrak{C}=\\frac{\\sqrt{1-p}\\sqrt{(\\bar{\\theta}+1)}}{1-\\sqrt{1-p}}\\sqrt{\\frac{N-b}{b(N-1)}\\sigma^{2}}+\\frac{T\\sqrt{\\bar{\\theta}+1}}{1-\\sqrt{1-p}}\\sqrt{\\frac{p(N-b)}{b(N-1)}\\sigma^{2}}=0,}\\end{array}$ we can get the expression for : ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\iota^{t}}[\\sum_{j=1}^{t}\\|\\mathbf{X}^{j+1}-\\mathbf{X}^{j}\\|_{\\mathsf{F}}]\\leq C}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{C\\triangleq\\!\\frac{2\\overline{{\\mathbf{X}}}}{\\theta-\\overline{{\\mathbf{X}}}^{2}}(\\mathfrak{A}\\varphi(f(\\mathbf{X}^{1})-f(\\bar{\\mathbf{X}}))+\\frac{\\sqrt{\\theta-\\overline{{\\mathbf{X}}}^{2}}}{2}\\sqrt{\\frac{n}{2}(\\overline{{\\mathbf{V}}}+\\sqrt{2})^{2}})}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "7 53 Considering that: \u03b8\u2032 = \u03b82\u2212X2 $\\begin{array}{r}{\\sqrt{\\theta^{\\prime}}=\\frac{\\sqrt{\\theta-\\overline{{\\mathrm{X}}}^{2}}}{2}-\\sqrt{\\frac{L_{f}^{2}\\overline{{\\mathrm{X}}}^{2}(1-p)}{b^{\\prime}}}\\frac{\\sqrt{\\theta+1}}{1-\\sqrt{1-p}}=\\frac{\\sqrt{\\theta-\\overline{{\\mathrm{X}}}^{2}}}{2}-\\sqrt{L_{f}^{2}\\overline{{\\mathrm{X}}}^{2}(1+\\overline{{\\theta}})}((1+N^{\\frac{1}{2}})^{\\frac{1}{2}}+N^{\\frac{1}{2}}).}\\end{array}$   \n754 $N^{\\frac{1}{4}})\\,=\\,\\mathcal{O}(N^{\\frac{1}{4}})$ , we have: $\\begin{array}{r}{\\mathfrak{A}=\\sqrt{\\frac{(2\\overline{{{X}}}^{2}+\\gamma\\frac{n p}{2}\\overline{{{X}}}+\\gamma\\frac{n p}{2}\\overline{{{\\nabla}}}^{2}\\overline{{{X}}})^{2}}{2\\overline{{\\theta}}}+\\frac{n\\gamma^{2}\\phi^{2}}{4\\theta^{\\prime}}}=\\mathcal{O}(\\frac{1}{N^{1/4}})}\\end{array}$ . Finally, we have   \n755 C = O( \u03c6(f(X1)\u2212f( X\u00af))) \u53e3 ", "page_idx": 27}, {"type": "text", "text": "756 E.5 Proof of Theorem 4.9 ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "757 Proof. For simplicity, we use B instead of $\\mathtt{B}^{t}$ . Initially, we prove the following important lemmas. ", "page_idx": 27}, {"type": "text", "text": "758 Lemma E.6. (Riemannian gradient Lower Bound for the Iterates Gap) We define $\\phi\\triangleq(3\\overline{{\\mathbf{X}}}+\\overline{{\\mathbf{V}\\mathbf{X}}})\\overline{{\\mathbf{G}}}+$   \n759 $(1+\\overline{{\\mathbf{X}}}^{2}+\\overline{{\\mathbf{V}}}^{2}+\\overline{{\\mathbf{V}}}^{2}\\overline{{\\mathbf{X}}}^{2})L_{f}+(1+\\overline{{\\mathbf{V}}}^{2})\\theta$ . It holds that: $\\begin{array}{r l}{\\mathbb{E}_{\\xi^{t+1}}[\\mathrm{dist}(\\mathbf{0},\\nabla_{\\mathcal{I}}\\mathcal{G}(\\mathbf{I}_{2};\\mathbf{X}^{t+1},\\mathbf{B}^{t+1}))]\\le}&{{}}\\end{array}$   \n760 $\\phi\\cdot\\mathbb{E}_{\\xi^{t}}[\\|\\bar{\\mathbf{V}}^{t}-\\mathbf{I}_{2}\\|_{\\mathsf{F}}]$ . ", "page_idx": 27}, {"type": "text", "text": "761 Proof. The proof process is exactly the same as in lemma E.2 and will not be repeated here. ", "page_idx": 27}, {"type": "text", "text": "762 The following lemma is useful to outline the relation of $\\|\\nabla_{\\mathcal{I}}f(\\mathbf{X}^{t})\\|_{\\mathsf{F}}$ and $\\|\\nabla_{\\mathcal{I}}\\mathcal{G}(\\mathbf{I}_{2};\\mathbf{X}^{t},\\mathtt{B})\\|_{\\mathsf{F}}$ . ", "page_idx": 27}, {"type": "text", "text": "763 Lemma E.7. We have the following results: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{dist}(\\mathbf{0},\\nabla_{\\mathcal{I}}f(\\mathbf{X}^{t}))\\leq\\gamma\\cdot\\vec{\\mathbb{E}}_{\\xi^{t-1}}[\\mathrm{dist}(\\mathbf{0},\\nabla_{\\mathcal{I}}\\mathcal{G}(\\mathbf{I}_{2};\\mathbf{X}^{t},\\mathbf{B}))]\\;w i t h\\;\\gamma\\triangleq\\overline{{\\mathbb{X}}}\\sqrt{C_{n}^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "765 Proof. We have the following inequalities: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\|\\nabla_{\\mathcal{I}}f({\\mathbf{X}}^{t})\\|_{\\mathsf{F}}^{2}}&{\\overset{\\Omega}{=}}&{\\|{\\mathbf{G}}^{t}-{\\mathbf{J}}{\\mathbf{X}}^{t}({\\mathbf{G}}^{t})^{\\top}{\\mathbf{X}}^{t}\\mathbf{J}\\|_{\\mathsf{F}}^{2}}\\\\ &{\\overset{\\ge}=}&{\\|{\\mathbf{G}}^{t}({\\mathbf{X}}^{t})^{\\top}{\\mathbf{J}}{\\mathbf{X}}^{t}{\\mathbf{J}}-{\\mathbf{J}}{\\mathbf{X}}^{t}({\\mathbf{G}}^{t})^{\\top}{\\mathbf{J}}{\\mathbf{J}}{\\mathbf{X}}^{t}{\\mathbf{J}}\\|_{\\mathsf{F}}^{2}}\\\\ &{\\overset{\\ge}\\le}&{\\|{\\mathbf{G}}^{t}({\\mathbf{X}}^{t})^{\\top}-{\\mathbf{J}}{\\mathbf{X}}^{t}({\\mathbf{G}}^{t})^{\\top}{\\mathbf{J}}\\|_{\\mathsf{F}}^{2}\\|{\\mathbf{J}}{\\mathbf{X}}^{t}{\\mathbf{J}}\\|_{\\mathsf{F}}^{2}}\\\\ &{\\overset{\\ge}}&{\\overline{{\\mathbf{X}}}^{2}\\|{\\mathbf{W}}\\|_{\\mathsf{F}}^{2},\\mathrm{~with~}{\\mathbf{W}}\\overset{\\ge}{\\mathbf{G}}^{t}({\\mathbf{X}}^{t})^{\\top}-\\mathbf{J}{\\mathbf{X}}^{t}({\\mathbf{G}}^{t})^{\\top}{\\mathbf{J}}}\\\\ &{\\overset{\\ge}}&{\\overline{{\\mathbf{X}}}^{2}C_{n}^{2}\\cdot\\mathbb{R}_{\\xi^{t-1}}[\\|{\\mathbf{U}}_{\\mathsf{F}}^{\\top}[{\\mathbf{G}}^{t}({\\mathbf{X}}^{t})^{\\top}-\\mathbf{J}{\\mathbf{X}}^{t}({\\mathbf{G}}^{t})^{\\top}{\\mathbf{J}}]{\\mathbf{U}}_{\\mathsf{B}}\\|_{\\mathsf{F}}^{2}]}\\\\ &{\\overset{\\ge}=}&{\\overline{{\\mathbf{X}}}^{2}C_{n}^{2}\\cdot\\mathbb{R}_{\\xi^{t-1}}[\\|\\nabla_{\\mathcal{I}}\\mathcal{G}({\\mathbf{I}}_{2};{\\mathbf{X}}^{t},{\\mathbf{B}}) \n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "766 where step $\\textcircled{1}$ uses the definition of $\\nabla_{\\mathcal{I}}f(\\mathbf{X}^{t})$ ; step $\\circledcirc$ uses $\\mathbf{J}\\mathbf{J}=\\mathbf{I}$ and $\\mathbf{X}^{\\top}\\mathbf{J}\\mathbf{X}=\\mathbf{J}\\Rightarrow\\mathbf{X}^{\\top}\\mathbf{J}\\mathbf{X}\\mathbf{J}=$   \n767 $\\mathbf{J}\\mathbf{J}=\\mathbf{I}$ ; step $\\circled{3}$ uses the norm inequality and ; step $\\circledast$ uses the definition of $\\mathbf{W}\\,\\triangleq\\,\\mathbf{G}^{t}(\\mathbf{X}^{t})^{\\top}\\,-$   \n768 $\\mathbf{J}\\mathbf{X}^{t}(\\mathbf{G}^{t})^{\\top}\\mathbf{J}$ and $\\forall t$ , $\\|\\mathbf{X}^{t}\\|_{\\mathsf{F}}\\leq\\overline{{\\mathbf{X}}}$ ; step $\\textcircled{5}$ uses Lemma (A.1) with $k=2$ ; step $\\circled{6}$ uses the definition   \n769 of $\\nabla_{\\mathcal{I}}\\mathcal{G}(\\mathbf{I}_{2};\\mathbf{X}^{t},\\mathtt{B})$ . Taking the square root of both sides, we finish the proof of this lemma. $\\sqsqcup$   \n770 Finally, we obtain our main convergence results. First of all, since $f^{\\circ}(\\mathbf{X})\\triangleq f(\\mathbf{X})+{\\mathcal{Z}}_{\\mathcal{I}}(\\mathbf{X})$ is a KL   \n771 function, we have from Proposition 4.8 that: ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{1}{\\varphi^{\\prime}(f^{\\circ}(\\mathbf{X}^{\\prime})-f^{\\circ}(\\mathbf{X}))}\\leq\\mathrm{dist}(0,\\nabla f^{\\circ}(\\mathbf{X}^{\\prime}))\\overset{\\textregistered}{\\leq}\\|\\nabla_{\\mathcal{I}}f(\\mathbf{X}^{\\prime})\\|_{\\mathsf{F}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where step $\\textcircled{1}$ uses Lemma E.5. Here, $\\varphi(\\cdot)$ is some certain concave desingularization function. Since $\\varphi(\\cdot)$ is concave, we have: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\forall\\Delta\\in\\mathbb{R},\\Delta^{+}\\in\\mathbb{R},\\varphi(\\Delta^{+})+(\\Delta-\\Delta^{+})\\varphi^{\\prime}(\\Delta)\\leq\\varphi(\\Delta).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "772 Applying the inequality above with $\\Delta=f(\\mathbf{X}^{t})-f(\\bar{\\mathbf{X}})$ and $\\Delta^{+}=f(\\mathbf{X}^{t+1})-f(\\bar{\\mathbf{X}})$ , we have: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(f(\\mathbf{X}^{t})-f(\\mathbf{X}^{t+1}))\\varphi^{\\prime}(f(\\mathbf{X}^{t})-f(\\bar{\\mathbf{X}}))}\\\\ {\\leq}&{\\varphi(f(\\mathbf{X}^{t})-f(\\bar{\\mathbf{X}}))-\\varphi(f(\\mathbf{X}^{t+1})-f(\\bar{\\mathbf{X}}))\\triangleq\\mathcal{E}^{t}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "773 We derive the following inequalities: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}_{\\xi^{t}}[\\frac{\\theta}{2}\\|\\bar{\\mathbf{V}}^{t}-\\mathbf{I}_{2}\\|_{\\bar{\\mathbf{F}}}^{2}]}&{\\overset{(0)}{\\leq}}&{\\mathbb{E}_{\\xi^{t}}[f(\\mathbf{X}^{t})-f(\\mathbf{X}^{t+1})]}\\\\ &{\\overset{()}{\\leq}}&{\\mathbb{E}_{\\xi^{t}}[\\frac{\\xi^{t}}{\\rho(f(\\mathbf{X}^{t})-f(\\mathbf{X}))}]}\\\\ &{\\overset{()}{\\leq}}&{\\mathbb{E}_{\\xi^{t}}[\\xi^{t}\\|\\nabla_{\\mathcal{I}}f(\\mathbf{X}^{t})\\|_{\\mathrm{F}}]}\\\\ &{\\overset{(0)}{\\leq}}&{\\mathbb{E}_{\\xi^{t}}[\\xi^{t}\\gamma]\\|\\nabla_{\\mathcal{I}}\\mathcal{G}(\\mathbf{I}_{2};\\mathbf{X}^{t},\\mathbf{B})\\|_{\\mathrm{F}}]}\\\\ &{\\overset{(0)}{\\leq}}&{\\mathbb{E}_{\\xi^{t-1}}[\\xi^{t}\\gamma\\phi|\\bar{\\mathbf{V}}^{t-1}-\\mathbf{I}_{2}\\|_{\\mathrm{F}}]}\\\\ &{\\overset{(0)}{\\leq}}&{\\mathbb{E}_{\\xi^{t-1}}[\\xi^{t}\\gamma\\phi|\\bar{\\mathbf{V}}^{t-1}-\\mathbf{I}_{2}\\|_{\\mathrm{F}}]}\\\\ &{\\overset{(0)}{\\leq}}&{\\mathbb{E}_{\\xi^{t-1}}[\\frac{\\beta^{t}}{2}\\|\\bar{\\mathbf{V}}^{t-1}-\\mathbf{I}_{2}\\|_{\\mathrm{F}}^{2}+\\frac{(\\xi^{t}\\gamma\\phi)^{2}}{2\\theta^{t}}],\\forall\\theta^{\\prime}>0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "774 where step $\\textcircled{1}$ uses the sufficient descent condition as shown in Theorem 4.6; step $\\circledcirc$ uses Inequality   \n775 (77); step $\\circled{3}$ uses Inequality (76) with $\\mathbf{X}^{\\prime}=\\mathbf{X}^{t}$ and $\\mathbf X=\\bar{\\mathbf X}$ ; step $\\circledast$ uses Lemma E.7; step $\\textcircled{5}$ uses   \n776 Lemma E.6; step $\\circled{6}$ applies the inequality that $\\begin{array}{r}{\\forall\\theta^{\\prime}>0,a,b,a b\\leq\\frac{\\theta^{\\prime}a^{2}}{2}+\\frac{b^{2}}{2\\theta^{\\prime}}}\\end{array}$ \u2264\u03b82a + 2b\u03b8\u2032 with a = \u2225V\u00aft\u22121 \u2212I2\u2225F   \n777 and $b=\\mathcal{E}^{t}\\gamma\\phi$ . ", "page_idx": 28}, {"type": "text", "text": "778 Multiplying both sides by 2 and taking the square root of both sides, we have: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\sqrt{\\theta}\\mathbb{E}_{\\xi^{t}}[\\|\\bar{\\mathbf{V}}^{t}-\\mathbf{I}_{2}\\|_{\\mathsf{F}}]}&{\\le}&{\\sqrt{\\frac{(\\mathcal{E}^{t}\\gamma\\phi)^{2}}{\\theta^{\\prime}}+\\theta^{\\prime}\\mathbb{E}_{\\xi^{t-1}}[\\|\\bar{\\mathbf{V}}^{t-1}-\\mathbf{I}_{2}\\|_{\\mathsf{F}}^{2}]},\\forall\\theta^{\\prime}>0}\\\\ &{\\overset{\\oslash}{\\le}}&{\\sqrt{\\theta^{\\prime}}\\mathbb{E}_{\\xi^{t-1}}[\\|\\bar{\\mathbf{V}}^{t-1}-\\mathbf{I}_{2}\\|_{\\mathsf{F}}]+\\frac{\\mathcal{E}^{t}\\gamma\\phi}{\\sqrt{\\theta^{\\prime}}},\\forall\\theta^{\\prime}>0,~~~~}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "779 where step $\\textcircled{1}$ uses the inequality that ${\\sqrt{a+b}}\\leq{\\sqrt{a}}+{\\sqrt{b}}$ for all $a\\geq0$ and $b\\geq0$ . Summing the   \n780 inequality above over $i=1,2\\dots,t$ , we have: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\sqrt{\\theta}\\mathbb{E}_{\\xi^{t}}[\\|\\bar{\\mathbf{V}}^{t}-\\mathbf{I}_{2}\\|_{\\mathsf{F}}]-\\sqrt{\\theta^{\\prime}}\\mathbb{E}_{\\xi^{0}}[\\|\\bar{\\mathbf{V}}^{0}-\\mathbf{I}_{2}\\|_{\\mathsf{F}}]+\\sum_{i=1}^{t-1}(\\sqrt{\\theta}-\\sqrt{\\theta^{\\prime}})\\mathbb{E}_{\\xi^{i}}[\\|\\bar{\\mathbf{V}}^{i}-\\mathbf{I}_{2}\\|_{\\mathsf{F}}]}\\\\ {\\le}&{\\frac{\\gamma\\phi}{\\sqrt{\\theta^{\\prime}}}\\sum_{i=1}^{t}\\mathcal{E}^{i}}\\\\ {\\overset{\\mathrm{()}}{=}}&{\\frac{\\gamma\\phi}{\\sqrt{\\theta^{\\prime}}}\\sum_{i=1}^{t}\\varphi(f(\\mathbf{X}^{i})-f(\\bar{\\mathbf{X}}))-\\varphi(f(\\mathbf{X}^{i+1})-f(\\bar{\\mathbf{X}}))}\\\\ {\\overset{\\mathrm{()}}{=}}&{\\frac{\\gamma\\phi}{\\sqrt{\\theta^{\\prime}}}[\\varphi(f(\\mathbf{X}^{1})-f(\\bar{\\mathbf{X}}))-\\varphi(f(\\mathbf{X}^{t+1})-f(\\bar{\\mathbf{X}}))]}\\\\ {\\overset{\\mathrm{()}}{\\le}}&{\\frac{\\gamma\\phi}{\\sqrt{\\theta^{\\prime}}}\\varphi(f(\\mathbf{X}^{1})-f(\\bar{\\mathbf{X}})),}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "781 where step $\\textcircled{1}$ uses the definition of $\\mathcal{E}^{i}$ in (77); step $\\circledcirc$ uses a basic recursive reduction; step $\\circled{3}$ uses   \n782 the fact the desingularization function $\\varphi(\\cdot)$ is positive. With the choice $\\begin{array}{r}{\\theta^{\\prime}=\\frac{\\theta}{4}}\\end{array}$ , we have: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\sqrt{\\theta}\\mathbb{E}_{\\xi^{t}}[\\|\\bar{\\mathbf{V}}^{t}-\\mathbf{I}_{2}\\|\\mathsf{r}]+\\frac{\\sqrt{\\theta}}{2}\\sum_{i=1}^{t-1}\\mathbb{E}_{\\xi^{i}}[\\|\\bar{\\mathbf{V}}^{i}-\\mathbf{I}_{2}\\|\\mathsf{r}]}\\\\ {\\le}&{\\frac{2\\gamma\\phi}{\\sqrt{\\theta}}\\varphi\\big(f\\big(\\mathbf{X}^{1}\\big)-f(\\bar{\\mathbf{X}})\\big)+\\frac{\\sqrt{\\theta}}{2}\\mathbb{E}_{\\xi^{0}}[\\|\\bar{\\mathbf{V}}^{0}-\\mathbf{I}_{2}\\|\\mathsf{r}]}\\\\ {\\overset{\\textregistered}{\\le}}&{\\frac{2\\gamma\\phi}{\\sqrt{\\theta}}\\varphi\\big(f\\big(\\mathbf{X}^{1}\\big)-f(\\bar{\\mathbf{X}})\\big)+\\frac{\\sqrt{\\theta}}{2}(\\overline{{\\mathbf{V}}}+\\sqrt{2}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "783 where step $\\textcircled{1}$ uses $\\forall t$ , $\\lVert\\mathbf{V}\\rVert_{\\mathsf{F}}\\leq\\overline{{\\mathbf{V}}}$ ,then, $\\|\\mathbf{V}-\\mathbf{I}_{2}\\|_{\\mathsf{F}}\\leq\\|\\mathbf{V}\\|_{\\mathsf{F}}+\\|\\mathbf{I}\\|_{\\mathsf{F}}\\leq\\overline{{\\mathbf{V}}}+\\sqrt{2}.$ . Finally, we obtain   \n784 from Inequality (79): ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{2}\\!\\sum_{i=1}^{t}\\mathbb{E}_{\\xi^{i}}[\\|\\bar{\\mathbf{V}}^{i}-\\mathbf{I}_{2}\\|_{\\mathsf{F}}]\\leq\\frac{2\\gamma\\phi}{\\theta}\\varphi(f(\\mathbf{X}^{1})-f(\\bar{\\mathbf{X}}))+\\frac{1}{2}(\\overline{{\\mathbf{V}}}+\\sqrt{2})}\\\\ {\\overset{\\textregistered}{\\Rightarrow}}&{\\frac{1}{2}\\sum_{i=1}^{t}\\mathbb{E}_{\\xi^{i}}[\\|\\mathbf{X}^{i+1}-\\mathbf{X}^{i}\\|_{\\mathsf{F}}]\\leq(\\frac{2\\overline{{X}}\\gamma\\phi}{\\theta}\\varphi(f(\\mathbf{X}^{1})-f(\\bar{\\mathbf{X}}))+\\frac{\\overline{{X}}}{2}(\\overline{{\\mathbf{V}}}+\\sqrt{2}))}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where step $\\textcircled{1}$ uses the inequality that $\\begin{array}{r}{\\frac{\\|\\mathbf{X}^{i+1}-\\mathbf{X}^{i}\\|_{\\mathsf{F}}}{\\overline{{\\mathbf{X}}}}\\leq\\|\\bar{\\mathbf{V}}^{i}-\\mathbf{I}_{2}\\|_{\\mathsf{F}}}\\end{array}$ as shown in Part $(b)$ in Lemma 2.1. Finally, we can get the expression for $C$ : ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{C\\triangleq\\frac{4\\overline{{\\Lambda}}\\gamma\\phi}{\\theta}\\varphi(F(\\mathbf{X}^{1})-F(\\bar{\\mathbf{X}}))+\\overline{{\\mathbf{X}}}(\\overline{{\\mathbf{V}}}+\\sqrt{2})=n\\mathcal{O}(\\varphi(f(\\mathbf{X}^{1})-f(\\bar{\\mathbf{X}})))}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "786 F Additional Experiment Details and Results ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "787 F.1 Additional Details for Hyperbolic Structural Probe Problem ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "788 To begin with, we give the definition of the Ultrahyperbolic manifold $\\mathbb{U}_{\\alpha}^{p,q}$ , which will be used in   \n789 Ultra-hyperbolic geodesic distance $\\mathbf{d}_{\\alpha}(\\mathbf{x},\\mathbf{y})$ and Diffeomorphism $\\varphi(\\cdot)$ .   \n790 $\\blacktriangleright$ Ultrahyperbolic manifold. Vectors in an ultrahyperbolic manifold is defined as $\\mathbb{U}_{\\alpha}^{p,q}=\\{\\mathbf{x}=$   \n791 $(x_{1},x_{2},\\cdot\\cdot\\cdot\\cdot,x_{p+q})^{\\top}\\in\\mathbb{R}^{p,q}:\\|{\\bf x}\\|_{q}^{2}=-\\alpha^{2}\\}[48]$ , where $\\alpha$ is a non-negative real number denoting   \n792 the radius of curvature. $\\lVert\\mathbf{x}\\rVert_{q}^{2}=\\langle\\mathbf{x},\\mathbf{x}\\rangle_{q}$ , $\\forall\\mathbf{x},\\mathbf{y}\\in\\mathbb{R}^{p,q}$ , $\\begin{array}{r}{\\langle\\mathbf{x},\\mathbf{y}\\rangle_{q}=\\sum_{i=1}^{p}\\mathbf{x}_{i}\\mathbf{y}_{i}-\\sum_{j=p+1}^{p+q}\\mathbf{x}_{j}\\mathbf{y}_{j}}\\end{array}$ j=p+1 xjyj is   \n793 a norm of the induced scalar product. The hyperbolic and spherical manifolds can be defined as   \n794 : $\\mathbb{H}_{\\alpha}=\\mathbb{U}_{\\alpha}^{p,1}$ , $\\mathbb{S}_{\\alpha}=\\mathbb{U}_{\\alpha}^{0,q}$ .   \n795 $\\blacktriangleright$ Ultra-hyperbolic geodesic distance. The ultra-hyperbolic geodesic distance [27][28] $\\mathbf{d}_{\\gamma}(\\cdot,\\cdot)$ is   \n796 formulated: \u2200x \u2208Up\u03b1, q, y \u2208Up\u03b1,qand \u03b1 > 0, d\u03b1(x, y) = { \u03b1\u03b1  ccoossh\u22121\u2212(1| (\u27e8| x\u27e8,xy,\u03b1\u27e9y2q\u27e9|q)|) ioft |h \u27e8exr,\u03b1wy2i\u27e9sqe|. \u22651 .   \n797 $\\blacktriangleright$ Diffeomorphism. [Theorem 1 Diffeomorphism of [49]]: Any vector $\\mathbf{x}\\in\\mathbb{R}^{p}\\times\\mathbb{R}_{*}^{q}$ can be mapped   \n798 into $\\mathbb{U}_{\\alpha}^{p,q}$ by a double projection $\\varphi=\\phi^{-1}\\circ\\phi$ , with $\\psi(\\mathbf{x})=(\\begin{array}{c}{\\mathbf{s}}\\\\ {\\alpha\\frac{\\mathbf{t}}{\\|\\mathbf{t}\\|}}\\end{array}),\\quad\\psi^{-1}(\\mathbf{z})=(\\begin{array}{c}{\\mathbf{v}}\\\\ {\\frac{\\sqrt{\\alpha^{2}+\\|\\mathbf{v}\\|^{2}}}{\\alpha}\\mathbf{u}}\\end{array}),$   \n799 where $\\mathbf{x}=(\\begin{array}{l}{\\mathbf{s}}\\\\ {\\mathbf{t}}\\end{array})\\in\\mathbb{U}_{\\alpha}^{p,q}$ with $\\mathbf{s}\\in\\mathbb{R}^{p}$ and $\\mathbf{t}\\in\\mathbb{R}_{*}^{q}\\cdot\\mathbf{z}=(\\mathbf{\\mu_{u}^{v}})\\in\\mathbb{R}^{p}\\times\\mathbb{S}_{\\alpha}^{q}$ with $\\mathbf{v}\\in\\mathbb{R}^{p}$ and $\\mathbf{u}\\in\\mathbb{S}_{\\alpha}^{q}$ . ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "800 F.2 Additional application: Ultra-hyperbolic Knowledge Graph Embedding ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "801 The J orthogonal matrix can be used as an isometric linear operator in the Ultrahyperbolic manifold,   \n802 [48] et al. extended the knowledge graph model from hyperbolic space to Ultra-hyperbolic space   \n803 (named as UltraE) by this property. The UltraE model is formulated as follows: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\mathbf{R},\\mathbf{E},\\mathbf{b}}{\\operatorname*{min}}\\mathcal{L}(\\mathbf{R},\\mathbf{E},\\mathbf{b})\\triangleq-\\frac{1}{N}\\sum_{(h,r,t)\\in\\Delta}(\\log s(h,r,t)+\\underset{(h^{\\prime},r^{\\prime},r^{\\prime})\\in\\Delta_{(h,r,t)}^{\\prime}}{\\sum}\\log(1-s(h^{\\prime},r^{\\prime},t^{\\prime})))}\\\\ &{\\qquad s.t.\\left\\{\\begin{array}{l l}{s(h,r,t)=\\sigma(-d_{\\alpha}^{2}(\\mathbf{R}_{r}\\mathbf{E}_{h},\\mathbf{E}_{t})+\\mathbf{b}_{h}+\\mathbf{b}_{t}+\\delta)}\\\\ {\\mathbf{R}_{r}^{\\top}\\mathbf{J}\\mathbf{R}_{r}=\\mathbf{J}}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "804 where $\\mathbf{E}\\in\\mathbb{R}^{n_{e}\\times n}$ with $\\mathbf{E}_{h}=\\mathbf{E}(h,:)\\in\\mathbb{U}_{\\alpha}^{p,q}$ , $\\mathbf{b}\\in\\mathbb{R}^{n_{r}}$ with $\\mathbf{b}_{h}=\\mathbf{b}(r)\\in\\mathbb{R}$ , $\\mathbf{R}\\in\\mathbb{R}^{n_{r}\\times n\\times n}$ with   \n805 ${\\bf R}_{r}\\,=\\,{\\bf R}(r,:,:)\\,\\in\\,\\mathbb{R}^{n\\times n}$ and $\\mathbf{J}\\,=\\,\\bigl[\\begin{array}{c c}{\\mathbf{I}_{p}}&{\\mathbf{0}}\\\\ {\\mathbf{0}}&{\\!\\!\\!-\\mathbf{I}_{q}}\\end{array}\\bigr]$ ; $\\Delta\\in\\mathbb{N}^{N\\times3}$ is the set of positive triplets, $\\Delta_{(h,r,t)}^{\\prime}\\in$   \n806 $\\mathbb{N}^{N\\times k\\times3}$ denotes the set of negative triples constructed by corrupting $(h,r,t)$ ; $\\delta$ is a global margin   \n807 hyper-parameter, $\\sigma(\\cdot)$ is the sigmoid function, $n_{e}$ represents the number of entities and $n_{r}$ represents   \n808 the number of relations; $d_{\\alpha}(\\cdot)$ stands for the Ultra-hyperbolic geodesic distance (refer to F.1).   \n809 \u25b6Experiment Details. We selected a batch of FB15K and WN18RR respectively as the data set for   \n810 the Ultra-hyperbolic Knowledge Graph Embedding problem, (training set size, test set size, number   \n811 of entities, number of relations) are (719,308,135,22) and (545,233,208,5) respectively. $n\\,=\\,36$ ,   \n812 $p\\,=\\,18$ , $\\delta\\,=\\,5$ , $\\alpha\\,=\\,1$ and $k\\,=\\,50$ . In order to highlight the difference between J orthogonal   \n813 optimization, in the UltraE model, all entities and biases of the optimization algorithm are optimized   \n814 using ADMM by Pytorch, $l r=5e-4$ . We use the Adagrad optimizer in Pytorch to optimize the   \n815 J-orthogonality constraint variable in the CS model. ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "816 F.3 Experiment result ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "817 $\\blacktriangleright$ Hyperbolic Eigenvalue Problem. Table 2 and Figure 3, 4, 5 are supplementary experiments for   \n818 HEVP. Several conclusions can be drawn. (i) GS-JOBCD often greatly improves upon UMCM,   \n819 ADMM and CSDM. This is because our methods find stronger stationary points than them. (ii)   \n820 J-JOBCD is a parallel version of GS-JOBCD and thus exhibits significantly faster convergence. (iii)   \n821 The proposed methods generally give the best performance.   \n822 \u25b6Hyperbolic Structural Probe Problem. Table 3 and Figure 6, 7 are supplementary experiments   \n823 for HSPP. Several conclusions can be drawn. (i) J-JOBCD often greatly improves upon UMCM,   \n824 ADMM and CSDM (ii) VR-J-JOBCD is a reduced variance version of J-JOBCD and thus exhibits   \n825 significantly faster convergence for problems with large samples. (iii) The proposed methods generally   \n826 give the best performance.   \n827 \u25b6Ultra-hyperbolic Knowledge Graph Embedding Problem. Figure 8, 9, 10 and 11 are supplemen  \n828 tary experiments for UltraE. Several conclusions can be drawn. (i) In terms of Epoch performance,   \n829 J-JOBCD and VR-J-JOBCD often greatly improves upon CSDM, thus they show better MRR and   \n830 hits results. (ii) In models with limited sample sizes, the computational efficiency of VR-J-JOBCD   \n831 is inferior to that of J-JOBCD. This discrepancy arises because each iteration in VR-J-JOBCD   \n832 necessitates two instances of backpropagation, thus consuming substantial computational resources.   \n833 (iii) The proposed methods generally give the best performance. ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "Table 2: The convergence curve of the compared methods for solving HEVP. $(+)$ indicates that after the convergence of the CSDM, UMCM and ADMM, utilizing the GS-JOBCD for optimization markedly enhances the objective value. The $1^{s t}$ , $2^{\\mathrm{nd}}$ , and $3^{\\mathrm{rd}}$ best results are colored with red, green and blue, respectively. $(n,p)$ represents the dimension and $\\mathbf{p}$ -value of the J orthogonal matrix (square matrix). The value in () stands for $\\begin{array}{r}{\\sum_{i j}^{n}|\\mathbf{X}^{\\top}\\mathbf{J}\\mathbf{X}-\\mathbf{J}|_{i j}}\\end{array}$ . ", "page_idx": 31}, {"type": "image", "img_path": "8bExkmfLCr/tmp/3e14c25fff2899d0b6edcc2a4be83ddf4a16167e294b01ebb0b6d00c0ded60dd.jpg", "img_caption": [], "img_footnote": [], "page_idx": 31}, {"type": "image", "img_path": "8bExkmfLCr/tmp/124db6b6bc5925489366e800bb31bfee0a5bcc4ff56d737983319e856b269712.jpg", "img_caption": ["Figure 3: The convergence curve of the compared methods for solving HEVP with varying $(m,n,p)$ . ", "(j) w1a(2470-290-145) "], "img_footnote": [], "page_idx": 32}, {"type": "image", "img_path": "8bExkmfLCr/tmp/33b55b62d56ecc85494ad1646ef933a4c6eaf53b78e7bba306a06e26c4309327.jpg", "img_caption": ["Figure 4: The convergence curve of the compared methods for solving HEVP with varying $(m,n,p)$ . ", "(j) w1a(2470-290-200) "], "img_footnote": [], "page_idx": 33}, {"type": "image", "img_path": "8bExkmfLCr/tmp/3036c6fc591544fabb891542ee01b9ef91df6f200131c8e3b1153efcd6d588b4.jpg", "img_caption": ["Figure 5: The convergence curve of the compared methods for solving HEVP with varying $(m,n,p)$ . ", "(j) w1a(2470-290-250) "], "img_footnote": [], "page_idx": 34}, {"type": "text", "text": "835 F.3.2 Hyperbolic Structural Probe Problem ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Table 3: The convergence curve of the compared methods for solving HSPP. $(+)$ indicates that after the convergence of the CSDM, utilizing the J-OBCD for optimization markedly enhances the objective value. The $1^{s t},2^{\\mathrm{nd}}$ , and $3^{\\mathrm{rd}}$ best results are colored with red, green and blue, respectively. $(n,p)$ represents the dimension and $\\mathbf{p}$ -value of the J orthogonal matrix (square matrix). The value in () stands for $\\begin{array}{r}{\\sum_{i j}^{n}|\\mathbf{X}^{\\top}\\mathbf{J}\\mathbf{X}-\\mathbf{J}|_{i j}}\\end{array}$ . ", "page_idx": 35}, {"type": "table", "img_path": "8bExkmfLCr/tmp/58f2b3254a5663219f60ebc350eb9b283167ad97fcf5b21a187b2da4716ebbdf.jpg", "table_caption": [], "table_footnote": [], "page_idx": 35}, {"type": "image", "img_path": "8bExkmfLCr/tmp/0cf53edf75a9312f9f73befda682493b17a538f1fe4f5780c2fd26d110e37b34.jpg", "img_caption": ["(a) 20News(9423-50-45) "], "img_footnote": [], "page_idx": 36}, {"type": "image", "img_path": "8bExkmfLCr/tmp/047aa818dcb8fceb815ec60cd90461ec89e2c484445eb4770b7b73a49b824dc4.jpg", "img_caption": ["(b) Cifar(10000-50-45)"], "img_footnote": [], "page_idx": 36}, {"type": "image", "img_path": "8bExkmfLCr/tmp/18e8dcc592a785edab80a586703a5e41a0dbbb522fd4fa879ea0a66e6ecdff54.jpg", "img_caption": ["(c) CnnCaltech(3000-96-85) "], "img_footnote": [], "page_idx": 36}, {"type": "image", "img_path": "8bExkmfLCr/tmp/35f8d36f21bb44959f79ea04aba53858113bad461d247b3854dcbd145b5c1131.jpg", "img_caption": ["(d) E2006(5000-100-90) "], "img_footnote": [], "page_idx": 36}, {"type": "image", "img_path": "8bExkmfLCr/tmp/e0a6ce8c528475272847cb6dac36c615a4f354b440dbb13cb684786ae66dddfa.jpg", "img_caption": ["(e) Gisette(6000-50-45) "], "img_footnote": [], "page_idx": 36}, {"type": "image", "img_path": "8bExkmfLCr/tmp/cf0e39c58e63d01d0069f6411911de3c344215a2c91684d7c0150d8513564301.jpg", "img_caption": ["(f) Mnist(6000-92-85) "], "img_footnote": [], "page_idx": 36}, {"type": "image", "img_path": "8bExkmfLCr/tmp/4cbd45ce07fd746dad3c941c1576042dbb49f2149a13000c6f8927d8ecb13b34.jpg", "img_caption": ["(g) News20(7967-50-45) "], "img_footnote": [], "page_idx": 36}, {"type": "image", "img_path": "8bExkmfLCr/tmp/1163b524da1cf3d13b8b7d0f1b4c7dea57d1c46fb2c4322d77b213d39a68e429.jpg", "img_caption": ["(h) randn(5000-100-85) "], "img_footnote": [], "page_idx": 36}, {"type": "image", "img_path": "8bExkmfLCr/tmp/120df206e2e9e0114045486a0003863c7ac63850de0cec32760936aa2c83b41b.jpg", "img_caption": ["(i) randn(10000-50-45) "], "img_footnote": [], "page_idx": 36}, {"type": "image", "img_path": "8bExkmfLCr/tmp/857e4de970b48a640a099867619c68f69db55fba1c17570cbe59c16d967063e5.jpg", "img_caption": ["(j) W1a(2477-100-90) "], "img_footnote": [], "page_idx": 36}, {"type": "text", "text": "Figure 6: The convergence curve of the compared methods for solving HSPP by epochs with varying $(m,n,p)$ . ", "page_idx": 36}, {"type": "image", "img_path": "8bExkmfLCr/tmp/bb85416dd58d42bc38d38b43fa6bfa86428afe390bb58681b82797d733cd5b72.jpg", "img_caption": ["(a) 20News(9423-50-45) "], "img_footnote": [], "page_idx": 37}, {"type": "image", "img_path": "8bExkmfLCr/tmp/5f9c5570c8a9eba68f51147009c42a588c1255f9cf33d782d24526f79a9f8214.jpg", "img_caption": ["(b) Cifar(10000-50-45)"], "img_footnote": [], "page_idx": 37}, {"type": "image", "img_path": "8bExkmfLCr/tmp/11a0c12149f256a785a5f7771d6eac6aae148ce39e68eb157cea6897c90ca324.jpg", "img_caption": ["(c) CnnCaltech(3000-96-85) "], "img_footnote": [], "page_idx": 37}, {"type": "image", "img_path": "8bExkmfLCr/tmp/f8133040c2a4c8df7c50c84b244031f1ec2279d18e4ffb0712b063fd95937d6a.jpg", "img_caption": ["(d) E2006(5000-100-90) "], "img_footnote": [], "page_idx": 37}, {"type": "image", "img_path": "8bExkmfLCr/tmp/f6ae26a8d4d2d366dedc5add322e47824c28ab67eaabf4a7857b754fb9c62678.jpg", "img_caption": ["(e) Gisette(6000-50-45) "], "img_footnote": [], "page_idx": 37}, {"type": "image", "img_path": "8bExkmfLCr/tmp/dc41145c38f1b1b0bc896fb9be99a775293228e95e10fd43794b2c60adbbe768.jpg", "img_caption": ["(f) Mnist(6000-92-85) "], "img_footnote": [], "page_idx": 37}, {"type": "image", "img_path": "8bExkmfLCr/tmp/a7fd82f7b8fcdea7d3dc459c5626fbe5e47e5485ed5bc19f07fcccd1ebe20f68.jpg", "img_caption": ["(g) News20(7967-50-45) "], "img_footnote": [], "page_idx": 37}, {"type": "image", "img_path": "8bExkmfLCr/tmp/a6fef2d265e26a81df739b58375b360c3d77cc4272b746595b16d9275314323c.jpg", "img_caption": ["(h) randn(5000-100-85) "], "img_footnote": [], "page_idx": 37}, {"type": "image", "img_path": "8bExkmfLCr/tmp/f39353976ff5e805d540132ce94f91eac0d0848b0d7b8c9ea819ba0f7bf88532.jpg", "img_caption": ["(i) randn(10000-50-45) "], "img_footnote": [], "page_idx": 37}, {"type": "image", "img_path": "8bExkmfLCr/tmp/4266dc818cadcb4b9416df0fb427eb868094ab14732c585d2cf8470c4e7a2b78.jpg", "img_caption": ["(j) W1a(2477-100-90) "], "img_footnote": [], "page_idx": 37}, {"type": "text", "text": "Figure 7: The convergence curve of the compared methods for solving HSPP by time with varying $(m,n,p)$ . ", "page_idx": 37}, {"type": "image", "img_path": "8bExkmfLCr/tmp/d9fda4c5fb46d19a5962d25a4942c347a0d52ef29c4c18957dfac8e5ce19b33c.jpg", "img_caption": [], "img_footnote": [], "page_idx": 38}, {"type": "text", "text": "Figure 8: Epoch performance of CS, J-JOBCD, and VR-J-JOBCD in training UltraE on FB15k. ", "page_idx": 38}, {"type": "image", "img_path": "8bExkmfLCr/tmp/961eceb67aa05fcc7729937399533a64524016290699900fe8fc817e3249ffe4.jpg", "img_caption": [], "img_footnote": [], "page_idx": 38}, {"type": "text", "text": "Figure 9: Time performance of CS, J-JOBCD, and VR-J-JOBCD in training UltraE on FB15k. ", "page_idx": 38}, {"type": "image", "img_path": "8bExkmfLCr/tmp/684f8e509999cfe02e4e5b78b2111263e6fb765314d076148192afd43690e110.jpg", "img_caption": [], "img_footnote": [], "page_idx": 38}, {"type": "text", "text": "Figure 10: Epoch performance of CSDM, J-JOBCD, and VR-J-JOBCD in training UltraE on WN18RR. ", "page_idx": 38}, {"type": "image", "img_path": "8bExkmfLCr/tmp/1df6eb00472fd7c1c5d5b80404a989f7fbc0710dc8f799a2835bb87d9b8b6348.jpg", "img_caption": [], "img_footnote": [], "page_idx": 38}, {"type": "text", "text": "Figure 11: Time performance of CSDM, J-JOBCD, and VR-J-JOBCD in training UltraE on WN18RR. ", "page_idx": 38}, {"type": "text", "text": "837 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Justification: In the abstract, we highlighted our contributions, including algorithm development, theoretical analysis, and empirical study. ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 39}, {"type": "text", "text": "854 2. Limitations ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: Please refer to the assumptions made for the optimization problem outlined in the introduction and Section 4. ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 39}, {"type": "text", "text": "886 3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "7 Question: For each theoretical result, does the paper provide the full set of assumptions and   \n88 a complete (and correct) proof? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes]   \nJustification: We have added a hyperlink before each theoretical result, which points to the complete proof located in the appendix.   \n\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 40}, {"type": "text", "text": "", "page_idx": 40}, {"type": "text", "text": "903 4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "904 Question: Does the paper fully disclose all the information needed to reproduce the main ex  \n905 perimental results of the paper to the extent that it affects the main claims and/or conclusions   \n906 of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: We have provided sufficient details for reproducing the results of the paper, such as parameter settings, runtime environments, and dataset descriptions. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 40}, {"type": "text", "text": "942 5. Open access to data and code ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "943 Question: Does the paper provide open access to the data and code, with sufficient instruc  \n944 tions to faithfully reproduce the main experimental results, as described in supplemental   \n945 material?   \n946 Answer: [Yes]   \n947 Justification: We have included all the code and data in the supplemental materials.   \n948 Guidelines:   \n949 \u2022 The answer NA means that paper does not include experiments requiring code.   \n950 \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/   \n951 public/guides/CodeSubmissionPolicy) for more details.   \n952 \u2022 While we encourage the release of code and data, we understand that this might not be   \n953 possible, so \u201dNo\u201d is an acceptable answer. Papers cannot be rejected simply for not   \n954 including code, unless this is central to the contribution (e.g., for a new open-source   \n955 benchmark).   \n956 \u2022 The instructions should contain the exact command and environment needed to run to   \n957 reproduce the results. See the NeurIPS code and data submission guidelines (https:   \n958 //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n959 \u2022 The authors should provide instructions on data access and preparation, including how   \n960 to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n961 \u2022 The authors should provide scripts to reproduce all experimental results for the new   \n962 proposed method and baselines. If only a subset of experiments are reproducible, they   \n963 should state which ones are omitted from the script and why.   \n964 \u2022 At submission time, to preserve anonymity, the authors should release anonymized   \n965 versions (if applicable).   \n966 \u2022 Providing as much information as possible in supplemental material (appended to the   \n967 paper) is recommended, but including URLs to data and code is permitted.   \n968 6. Experimental Setting/Details   \n969 Question: Does the paper specify all the training and test details (e.g., data splits, hyper  \n970 parameters, how they were chosen, type of optimizer, etc.) necessary to understand the   \n971 results?   \n972 Answer: [Yes]   \n973 Justification: We have provided sufficient details for solving the optimization problem,   \n974 encompassing hyperparameter settings and dataset generation.   \n975 Guidelines:   \n976 \u2022 The answer NA means that the paper does not include experiments.   \n977 \u2022 The experimental setting should be presented in the core of the paper to a level of detail   \n978 that is necessary to appreciate the results and make sense of them.   \n979 \u2022 The full details can be provided either with the code, in appendix, or as supplemental   \n980 material.   \n981 7. Experiment Statistical Significance   \n982 Question: Does the paper report error bars suitably and correctly defined or other appropriate   \n983 information about the statistical significance of the experiments?   \n984 Answer: [No]   \n985 Justification: For simplicity, we only demonstrate the convergence behavior of the objective   \n986 function by varying the time or iterations. Our methods exhibit clear advantages over the   \n987 compared methods. Such results have demonstrated significance in the experiments.   \n988 Guidelines:   \n989 \u2022 The answer NA means that the paper does not include experiments.   \n990 \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confi  \n991 dence intervals, or statistical significance tests, at least for the experiments that support   \n992 the main claims of the paper. ", "page_idx": 41}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 42}, {"type": "text", "text": "9 8. Experiments Compute Resources ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "10 Question: For each experiment, does the paper provide sufficient information on the com  \n11 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n12 the experiments? ", "page_idx": 42}, {"type": "text", "text": "Justification:   \nGuidelines: We have outlined the types of compute workers, detailing CPU and memory specifications.   \n\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 42}, {"type": "text", "text": "", "page_idx": 42}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 42}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 42}, {"type": "text", "text": "29 Justification: Our research aligns with the ethical guidelines outlined by NeurIPS.   \n30 Guidelines:   \n031 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n032 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n033 deviation from the Code of Ethics.   \n034 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid  \n035 eration due to laws or regulations in their jurisdiction). ", "page_idx": 42}, {"type": "text", "text": "", "page_idx": 42}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "037 Question: Does the paper discuss both potential positive societal impacts and negative   \n038 societal impacts of the work performed?   \n1044 \u2022 If the authors answer NA or No, they should explain why their work has no societal   \n1045 impact or why the paper does not address societal impact.   \n1046 \u2022 Examples of negative societal impacts include potential malicious or unintended uses   \n1047 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n1048 (e.g., deployment of technologies that could make decisions that unfairly impact specific   \n1049 groups), privacy considerations, and security considerations.   \n1050 \u2022 The conference expects that many papers will be foundational research and not tied   \n1051 to particular applications, let alone deployments. However, if there is a direct path to   \n1052 any negative applications, the authors should point it out. For example, it is legitimate   \n1053 to point out that an improvement in the quality of generative models could be used to   \n1054 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n1055 that a generic algorithm for optimizing neural networks could enable people to train   \n1056 models that generate Deepfakes faster.   \n1057 \u2022 The authors should consider possible harms that could arise when the technology is   \n1058 being used as intended and functioning correctly, harms that could arise when the   \n1059 technology is being used as intended but gives incorrect results, and harms following   \n1060 from (intentional or unintentional) misuse of the technology.   \n1061 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n1062 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n1063 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n1064 feedback over time, improving the efficiency and accessibility of ML).   \n1065 11. Safeguards   \n1066 Question: Does the paper describe safeguards that have been put in place for responsible   \n1067 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n1068 image generators, or scraped datasets)?   \n1069 Answer: [NA]   \n1070 Justification: The paper poses no such risks.   \n1071 Guidelines:   \n1072 \u2022 The answer NA means that the paper poses no such risks.   \n1073 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n1074 necessary safeguards to allow for controlled use of the model, for example by requiring   \n1075 that users adhere to usage guidelines or restrictions to access the model or implementing   \n1076 safety filters.   \n1077 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n1078 should describe how they avoided releasing unsafe images.   \n1079 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n1080 not require this, but we encourage authors to take this into account and make a best   \n1081 faith effort.   \n1082 12. Licenses for existing assets   \n1083 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n1084 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n1085 properly respected?   \n1086 Answer: [Yes]   \n1087 Justification: The dataset used in the experiments is published on an open site without   \n1088 license.   \n1089 Guidelines:   \n1090 \u2022 The answer NA means that the paper does not use existing assets.   \n1091 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n1092 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n1093 URL.   \n1094 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n1095 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n1096 service of that source should be provided.   \n1097 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n1098 package should be provided. For popular datasets, paperswithcode.com/datasets   \n1099 has curated licenses for some datasets. Their licensing guide can help determine the   \n1100 license of a dataset.   \n1101 \u2022 For existing datasets that are re-packaged, both the original license and the license of   \n1102 the derived asset (if it has changed) should be provided.   \n1103 \u2022 If this information is not available online, the authors are encouraged to reach out to   \n1104 the asset\u2019s creators.   \n1105 13. New Assets   \n1106 Question: Are new assets introduced in the paper well documented and is the documentation   \n1107 provided alongside the assets?   \n1108 Answer: [NA]   \n1109 Justification: The experiments do not involve new datasets.   \n1110 Guidelines:   \n1111 \u2022 The answer NA means that the paper does not release new assets.   \n1112 \u2022 Researchers should communicate the details of the dataset/code/model as part of their   \n1113 submissions via structured templates. This includes details about training, license,   \n1114 limitations, etc.   \n1115 \u2022 The paper should discuss whether and how consent was obtained from people whose   \n1116 asset is used.   \n1117 \u2022 At submission time, remember to anonymize your assets (if applicable). You can either   \n1118 create an anonymized URL or include an anonymized zip file.   \n1119 14. Crowdsourcing and Research with Human Subjects   \n1120 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n1121 include the full text of instructions given to participants and screenshots, if applicable, as   \n1122 well as details about compensation (if any)?   \n1123 Answer: [NA]   \n1124 Justification: No crowdsourcing or human object is involved.   \n1125 Guidelines:   \n1126 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n1127 human subjects.   \n1128 \u2022 Including this information in the supplemental material is fine, but if the main contribu  \n1129 tion of the paper involves human subjects, then as much detail as possible should be   \n1130 included in the main paper.   \n1131 \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation,   \n1132 or other labor should be paid at least the minimum wage in the country of the data   \n1133 collector.   \n1134 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human   \n1135 Subjects   \n1136 Question: Does the paper describe potential risks incurred by study participants, whether   \n1137 such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)   \n1138 approvals (or an equivalent approval/review based on the requirements of your country or   \n1139 institution) were obtained?   \n1140 Answer: [NA]   \n1141 Justification: No crowdsourcing or human object is involved.   \n1142 Guidelines:   \n1143 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n1144 human subjects.   \n1145 \u2022 Depending on the country in which research is conducted, IRB approval (or equivalent)   \n1146 may be required for any human subjects research. If you obtained IRB approval, you   \n1147 should clearly state this in the paper. ", "page_idx": 42}, {"type": "text", "text": "", "page_idx": 43}, {"type": "text", "text": "", "page_idx": 44}, {"type": "text", "text": "\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 45}]