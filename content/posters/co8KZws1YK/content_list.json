[{"type": "text", "text": "Light Unbalanced Optimal Transport ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Milena Gazdieva Skolkovo Institute of Science and Technology Moscow, Russia milena.gazdieva@skoltech.ru ", "page_idx": 0}, {"type": "text", "text": "Arip Asadulaev ITMO University Artificial Intelligence Research Institute Moscow, Russia asadulaev@airi.net ", "page_idx": 0}, {"type": "text", "text": "", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Evgeny Burnaev Alexander Korotin Skolkovo Institute of Science and Technology Skolkovo Institute of Science and Technology Artificial Intelligence Research Institute Artificial Intelligence Research Institute Moscow, Russia Moscow, Russia e.burnaev@skoltech.ru a.korotin@skoltech.ru ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "While the continuous Entropic Optimal Transport (EOT) field has been actively developing in recent years, it became evident that the classic EOT problem is prone to different issues like the sensitivity to outliers and imbalance of classes in the source and target measures. This fact inspired the development of solvers that deal with the unbalanced EOT (UEOT) problem \u2212the generalization of EOT allowing for mitigating the mentioned issues by relaxing the marginal constraints. Surprisingly, it turns out that the existing solvers are either based on heuristic principles or heavyweighted with complex optimization objectives involving several neural networks. We address this challenge and propose a novel theoretically-justified, lightweight, unbalanced EOT solver. Our advancement consists of developing a novel view on the optimization of the UEOT problem yielding tractable and a non-minimax optimization objective. We show that combined with a light parametrization recently proposed in the field our objective leads to a fast, simple, and effective solver which allows solving the continuous UEOT problem in minutes on CPU. We prove that our solver provides a universal approximation of UEOT solutions and obtain its generalization bounds. We give illustrative examples of the solver\u2019s performance. The code is publicly available at ", "page_idx": 0}, {"type": "text", "text": "https://github.com/milenagazdieva/LightUnbalancedOptimalTransport ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The computational optimal transport (OT) has proven to be a powerful tool for solving various popular tasks, e.g., image-to-image translation [68, 17, 50, 28], image generation [67, 13, 7] and biological data transfer [5, 42, 66]. Historically, the majority of early works in the field were built upon solving the OT problem between discrete probability measures [10, 53]. Only recently the advances in the field of generative models have led to explosive interest from the ML community in developing the continuous OT solvers, see [38] for a survey. The setup of this problem assumes that the learner needs to estimate the OT plan between continuous measures given only empirical samples of data from them. Due to convexity-related issues of OT problem [40], many works consider the EOT problem, i.e., use entropy regularizers which guarantee, e.g., the uniqueness of learned plans. ", "page_idx": 0}, {"type": "text", "text": "Meanwhile, researches attract attention to other shortcomings of the classic OT problem. It enforces hard constraints on the marginal measures and, thus, does not allow for mass variations. As a result, OT shows high sensitivity to an imbalance of classes and outliers in the source and target measures [4] which are almost inevitable for large-scale datasets. To overcome these issues, it is common to consider extensions of the OT problem, e.g., unbalanced OT/EOT (UOT/UEOT) [8, 43]. The unbalanced OT/EOT formulations allow for variation of total mass by relaxing the marginal constraints through the use of divergences. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "The scope of our paper is the continuous UEOT problem. It seems that in this field, a solver that is fast, light, and theoretically justified has not yet been developed. Indeed, many of the existing solvers follow a kind of heuristical principles and are based on the solutions of discrete OT. For example, [45] uses a regression to interpolate the discrete solutions, and [16, 36] build a flow matching upon them. Almost all of the other solvers [9, 70] employ several neural networks with many hyper-parameters and require time-consuming optimization procedures. We solve the aforementioned shortcomings by introducing a novel lightweight solver that can play the role of a simple baseline for unbalanced EOT. ", "page_idx": 1}, {"type": "text", "text": "Contributions. We develop a novel lightweight solver to estimate continuous unbalanced EOT couplings between probability measures ( 4). Our solver has a non-minimax optimization objective and employs the Gaussian mixture parametrization for the UEOT plans. We provide the generalization bounds for our solver ( 4.4) and experimentally test it on several tasks ( 5.1, 5.2). ", "page_idx": 1}, {"type": "text", "text": "Notations. We work in the Euclidian space $(\\mathbb{R}^{d},\\|\\cdot\\|)$ . We use $\\mathcal{P}_{2,a c}(\\mathbb{R}^{d})$ (or $\\mathcal{M}_{2,+}(\\mathbb{R}^{d}))$ to denote the set of absolutely continuous Borel probability (or non-negative) measures on $\\mathbb{R}^{d}$ with finite second moment and differential entropy. We use $\\dot{C}_{2}(\\mathbb{R}^{d})$ to denote the space of all continuous functions $\\zeta:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ for which $\\exists a=a(\\zeta),b=b(\\zeta)$ such that $\\forall x\\in\\mathbb{R}^{d}:|\\zeta(x)|\\leq a+b\\|x\\|^{2}$ . Its subspace of functions which are additionally bounded from above is denoted as $\\mathcal{C}_{2,b}(\\mathbb{R}^{d})$ . For any $p\\in\\mathcal{P}_{2,a c}(\\mathbb{R}^{d})$ (or $\\mathcal{M}_{2,+}(\\mathbb{R}^{d}))$ , we use $p(x)$ to denote its density at a point $x\\in\\mathbb{R}^{d}$ . For a given measure $\\gamma\\in\\mathcal{M}_{2,+}(\\mathbb{R}^{d}\\times\\mathbb{R}^{d})$ , we denote its total mass by $\\|\\gamma\\|_{1}\\stackrel{\\mathrm{def}}{=}\\int_{\\mathbb{R}^{d}\\times\\mathbb{R}^{d}}\\gamma(x,y)d x d y$ . We use $\\gamma_{x}(x),\\gamma_{y}(y)$ to denote the marginals of $\\gamma(x,y)$ . They statisfy the equality $\\lVert\\gamma_{x}\\rVert_{1}=\\lVert\\gamma_{y}\\rVert_{1}=\\lVert\\gamma\\rVert_{1}$ . We write $\\gamma(\\cdot|x)$ to denote the conditional probability measure. Each such measure has a unit total mass. We use $\\overline{{f}}$ to denote the Fenchel conjugate of a function $f!\\;{\\overline{{f}}}(t)\\;{\\stackrel{\\mathrm{def}}{=}}\\;\\operatorname*{sup}_{u\\in\\mathbb{R}}\\{u t-f(u)\\}$ . We use $\\mathbb{I}_{A}$ to denote the convex indicator of a set $A$ , i.e., $\\mathbb{I}_{A}(x)=0$ if $x\\in A$ ; $\\mathbb{I}_{A}(x)=+\\infty$ if $x\\notin A$ . ", "page_idx": 1}, {"type": "text", "text": "2 Background ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Here we give an overview of the relevant entropic optimal transport (EOT) concepts. For additional details on balanced EOT, we refer to [10, 24, 53], unbalanced EOT - to [8, 43]. ", "page_idx": 1}, {"type": "text", "text": "$f$ -divergences for positive measures. For positive measures $\\mu_{1},\\mu_{2}\\,\\in\\,\\mathcal{M}_{2,+}(\\mathbb{R}^{d^{\\prime}})$ and a lower semi-continuous function $f:\\mathbb{R}\\rightarrow\\mathbb{R}\\cup\\{\\infty\\}$ , the $f$ -divergence between $\\mu_{1},\\mu_{2}$ is defined by: ", "page_idx": 1}, {"type": "equation", "text": "$$\n{\\mathcal{D}}_{f}(\\mu_{1}\\|\\mu_{2})\\ {\\stackrel{\\mathrm{def}}{=}}\\ \\int_{\\mathbb{R}^{d^{\\prime}}}f\\left({\\frac{\\mu_{1}(x)}{\\mu_{2}(x)}}\\right)\\mu_{2}(x)d x\\,{\\mathrm{if}}\\,\\mu_{1}\\ll\\mu_{2}{\\mathrm{~and~}}+\\infty\\,{\\mathrm{otherwise.}}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "We consider $f(t)$ which are convex, non-negative and attain zero uniquely when $t=1$ . In this case, $D_{f}$ is a valid measure of dissimilarity between two positive measures (see Appendix C for details). This means that $D_{f}(\\mu_{1}\\|\\mu_{2})\\geq0$ and $D_{f}(\\mu_{1}\\|\\mu_{2})\\!=\\!0$ if and only if $\\mu_{1}=\\mu_{2}$ . In our paper, we also assume that all $f$ that we consider satisfy the property that $\\overline{f}$ is a non-decreasing function. ", "page_idx": 1}, {"type": "text", "text": "Kullback-Leibler divergence $\\mathbf{D}_{\\mathrm{KL}}$ [8, 62], is a particular case of such $f$ -divergence for positive measures. It has a generator function $f_{\\mathrm{KL}}(t)\\,{\\stackrel{\\mathrm{def}}{=}}\\,t\\log t{-t}{+1}$ . Its convex conjugate $\\overline{{f_{\\mathrm{KL}}}}(t)\\!=\\!\\exp(t)-1$ . Another example is the $\\chi^{2}$ -divergence $\\mathbf{D}_{\\chi^{2}}$ which is generated by $f_{\\chi^{2}}(t)\\stackrel{\\mathrm{def}}{=}(t-1)^{2}$ when $t\\geq0$ and $\\infty$ otherwise. The convex conjugate of this function is $\\overline{{f_{\\chi^{2}}}}(t)=-1$ if $t\\!<\\!-2$ and ${\\textstyle\\frac{1}{4}}t^{2}+t$ when $t\\!\\geq\\!2$ . Remark. By the definition, convex conjugates of $f_{\\mathrm{KL}}$ and $f_{\\chi^{2}}$ divergences are proper, non-negative and non-decreasing. These properties are used in some of our theoretical results. ", "page_idx": 1}, {"type": "text", "text": "Entropy for positive measures. For $\\mu\\in\\mathcal{M}_{2,+}(\\mathbb{R}^{d^{\\prime}})$ , its entropy [8] is given by ", "page_idx": 1}, {"type": "equation", "text": "$$\nH(\\mu){\\stackrel{\\mathrm{def}}{=}}-\\int_{\\mathbb{R}^{d^{\\prime}}}\\mu(x)\\mathrm{log}\\,\\mu(x)d x+\\|\\mu\\|_{1}.\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "When $\\mu\\in\\mathcal{P}_{2,a c}(\\mathbb{R}^{d^{\\prime}})$ , i.e., $\\|\\mu\\|_{1}\\!=\\!1$ , equation (1) is the usual differential entropy minus 1. ", "page_idx": 1}, {"type": "text", "text": "Classic EOT formulation (with the quadratic cost). Consider two probability measures $p\\in$ $\\mathcal{P}_{2,a c}(\\mathbb{R}^{d})$ , $q\\!\\in\\!\\mathcal{P}_{2,a c}(\\mathbb{R}^{d})$ . For $\\varepsilon>0$ , the EOT problem between $p$ and $q$ is ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\pi\\in\\Pi(p,q)}\\int_{\\mathbb{R}^{d}}\\int_{\\mathbb{R}^{d}}\\frac{\\|x-y\\|^{2}}{2}\\pi(x,y)d x d y-\\varepsilon H(\\pi),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\Pi(p,q)$ is the set of probability measures $\\pi\\!\\in\\!\\mathcal{P}_{2,a c}(\\mathbb{R}^{d}\\!\\times\\!\\mathbb{R}^{d})$ with marginals $p,q$ (transport plans). Plan $\\pi^{*}$ attaining the minimum exists, it is unique and called the EOT plan. ", "page_idx": 2}, {"type": "text", "text": "Classic EOT imposes hard constraints on the marginals which leads to several issues, e.g., sensitivity to outliers [4], inability to handle potential measure shifts such as class imbalances in the measures $p,q$ . The UEOT problem [70, 9] overcomes these issues by relaxing the marginal constraints [62]. ", "page_idx": 2}, {"type": "text", "text": "Unbalanced EOT formulation (with the quadratic cost). Let $D_{f_{1}}$ and $D_{f_{2}}$ be two $f$ -divergences over $\\mathbb{R}^{d}$ . For two probability measures $p\\in\\mathcal{P}_{2,a c}(\\mathbb{R}^{d})$ , $q\\in\\mathcal{P}_{2,a c}(\\mathbb{R}^{d})$ and $\\varepsilon>0$ , the unbalanced EOT problem between $p$ and $q$ consists of finding a minimizer of ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{\\substack{\\gamma\\in\\mathcal{M}_{2,+}\\,(\\ensuremath{\\mathbb{R}}^{d}\\times\\ensuremath{\\mathbb{R}}^{d})}}\\int_{\\ensuremath{\\mathbb{R}}^{d}}\\int_{\\ensuremath{\\mathbb{R}}^{d}}\\frac{\\|x-y\\|^{2}}{2}\\gamma(x,y)d x d y-\\varepsilon H(\\gamma)+D_{f_{1}}\\left(\\gamma_{x}\\|p\\right)+D_{f_{2}}\\left(\\gamma_{y}\\|q\\right).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Here the minimum is attained for a unique $\\gamma^{*}$ which is called the unbalanced optimal entropic (UEOT) plan. Typical choices of $f_{i}$ $_{i}\\ (i\\in[1,2])$ ) are $f_{i}(t)=\\tau_{i}f_{\\mathrm{KL}}(t)$ or $f_{i}(t)=\\bar{\\tau_{i}}f_{\\chi^{2}}(t)\\;(\\tau_{i}>0)$ yielding the scaled $\\mathbf{D}_{\\mathrm{KL}}$ and $\\mathbf{D}_{\\chi^{2}}$ , respectively. In this case, the bigger $\\tau_{1}\\left(\\tau_{2}\\right)$ is, the more $\\gamma_{x}\\left(\\gamma_{y}\\right)$ is penalized for not matching the corresponding marginal distribution $p\\left(q\\right)$ . ", "page_idx": 2}, {"type": "text", "text": "Remark. The balanced EOT problem (2) is a special case of (3). Indeed, let $f_{1}$ and $f_{2}$ be the convex indicators of $\\{1\\}$ , i.e., $f_{1}=f_{2}=\\mathbb{I}_{x=1}$ . Then the $f$ -divergences $D_{f_{1}}\\left(\\gamma_{x}\\Vert p\\right)$ and $D_{f_{2}}\\left(\\gamma_{y}\\Vert q\\right)$ become infinity if $p\\neq\\gamma_{x}$ or $q\\neq\\gamma_{y}$ , and become zeros otherwise. ", "page_idx": 2}, {"type": "text", "text": "Dual form of unbalanced EOT problem (3) is formulated as follows ", "text_level": 1, "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{(\\phi,\\psi)}\\left\\{-\\varepsilon\\!\\int_{\\mathbb{R}^{d}}\\!\\int_{\\mathbb{R}^{d}}\\!\\exp\\{\\frac{1}{\\varepsilon}(\\phi(x)+\\psi(y)-\\frac{\\|x-y\\|^{2}}{2})\\}d x d y\\!-\\!\\int_{\\mathbb{R}^{d}}\\!\\overline{{f}}_{1}(-\\phi(x))p(x)d x\\!-\\!\\int_{\\mathbb{R}^{d}}\\!\\overline{{f}}_{2}(-\\psi(y))q(y)d x d y\\right\\},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "It is known that there exist two measurable functions $\\phi^{*}$ , $\\psi^{*}$ delivering maximum to (4). They have the following connection with the solution of the primal unbalanced problem (3): ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\gamma^{*}(x,y)\\!=\\!\\exp\\{\\frac{\\phi^{*}(x)}{\\varepsilon}\\}\\!\\exp\\{-\\frac{||x-y||^{2}}{2\\varepsilon}\\}\\!\\exp\\{\\frac{\\psi^{*}(y)}{\\varepsilon}\\}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Remark. For some of our results, we will need to restrict potentials $(\\phi,\\psi)$ in problem (4) to the space $C_{2,b}(\\mathbb{R}^{d})\\times C_{2,b}(\\mathbb{R}^{d})$ . Since established variants of dual forms [8] typically correspond to other functional spaces, we derive and theoretically justify a variant of the dual problem (4) in Appendix A.3. Note that it may hold that optimal potentials $\\psi^{*},\\phi^{*}\\notin C_{2,b}(\\mathbb{R}^{\\overline{{D}}})$ , i.e., the supremum is not achieved within our considered spaces. This is not principle for our subsequent derivations. ", "page_idx": 2}, {"type": "text", "text": "Computational UEOT setup. Analytical solution for the unbalanced EOT problem is, in general, not known.1 Moreover, in real-world setups where unbalanced EOT is applicable, the measures $p,q$ are typically not available explicitly but only through their empirical samples (datasets). ", "page_idx": 2}, {"type": "text", "text": "We assume that data measures $p,q\\in\\mathcal{P}_{2,a c}(\\mathbb{R}^{d})$ are unknown and accessible only by a limited number of i.i.d. empirical samples $\\{x_{0},...,x_{N}\\}\\!\\sim\\!p$ , $\\left\\{y_{0},...,y_{M}\\right\\}\\sim q$ . We aim to approximate the optimal UEOT plan $\\gamma^{*}$ solving (3) between the entire measures $p,q$ . The recovered plans should allow the out-of-sample estimation, i.e., generation of samples from $\\gamma^{*}(\\cdot|x^{\\mathrm{new}})$ where $x^{\\mathrm{new}}$ is a new test point (not necessarily present in the train data). Optionally, one may require the ability to sample from $\\gamma_{x}^{*}$ ", "page_idx": 2}, {"type": "text", "text": "The described setup is typically called the continuous EOT and should not be twisted up with the discrete EOT [53, 10]. There the aim is to recover the (unbalanced) EOT plan between the empirical measures $\\begin{array}{r}{\\hat{p}\\!=\\!\\!\\frac{1}{N}\\!\\sum_{i=1}^{N}\\delta_{x_{i}}}\\end{array}$ , $\\scriptstyle{\\hat{q}}={\\frac{1}{M}}\\sum_{j=1}^{M}\\delta_{y_{j}}$ and out-of-sample estimations are typically not needed. ", "page_idx": 2}, {"type": "table", "img_path": "co8KZws1YK/tmp/c711e8319bac27a6f3a4e5a873f39f1c3c706ef1d650748f5ce8e927792af99e.jpg", "table_caption": [], "table_footnote": ["Table 1: Comparison of the principles of existing UOT/UEOT solvers and our proposed light solver. "], "page_idx": 3}, {"type": "text", "text": "3 Related Work ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Nowadays, the sphere of continuous OT/EOT solvers is actively developing. Some of the early works related to this topic utilize OT cost as the loss function [27, 25, 2]. These approaches are not relevant to us as they do not learn OT/EOT maps (or plans). We refer to [39] for a detailed review. ", "page_idx": 3}, {"type": "text", "text": "At the same time, there exist a large amount of works within the discrete OT/EOT setup [10, 15, 69, 51], see [53] for a survey. We again emphasize that solvers of this type are not relevant to us as they construct discrete matching between the given (train) samples and typically do not provide a generalization to the new unseen (test) data. Only recently ML community started developing out-ofsample estimation procedures based on discrete/batched OT. For example, [19, 56, 32, 14, 48, 57] mostly develop such estimators using the barycentric projections of the discrete EOT plans. Though these procedures have nice theoretical properties, their scalability remains unclear. ", "page_idx": 3}, {"type": "text", "text": "Balanced OT/EOT solvers. There exists a vast amount of neural solvers for continuous OT problem. Most of them learn the OT maps (or plans) via solving saddle point optimization problems [3, 18, 37, 22, 60, 50]. Though the recent works [28, 61, 11, 41, 30] tackle the EOT problem (2), they consider its balanced version. Hence they are not relevant to us. Among these works, only [41, 30] evade non-trivial training/inference procedures and are ideologically the closest to ours. The difference between them consists of the particular loss function used. In fact, our paper proposes the solver which subsumes these solvers and generalizes them for the unbalanced case. The derivation of our solver is non-trivial and requires solid mathematical apparatus, see 4. ", "page_idx": 3}, {"type": "text", "text": "Unbalanced OT/EOT solvers. A vast majority of early works in this field tackle the discrete UOT/UEOT setup [6, 20, 54] but the principles behind their construction are not easy to generalize to the continuous setting. Thus, many of the recent papers that tackle the continuous unbalanced OT/EOT setup employ discrete solutions in the construction of their solvers. For example, [45] regress neural network on top of scaling factors obtained using the discrete UEOT while simultaneously learning the continuous OT maps using an ICNN method [47]. In [16] and [36], the authors implement Flow Matching [44, FM] and conditional FM on top of the discrete UEOT plans, respectively. The algorithm of the latter consists of regressing neural networks on top of scaling factors and simultaneously learning a conditional vector field to transport the mass between re-balanced measures. Despite the promising practical performance of these solvers, it is still unclear to what extent such approximations of UEOT plans are theoretically justified. ", "page_idx": 3}, {"type": "text", "text": "The recent papers [70, 9] are more related to our study as they do not rely on discrete OT approximations of the transport plan. However, they have non-trivial minimax optimization objectives solved using complex GAN-style procedures. Thus, these GANs often lean on heavy neural parametrization, may incur instabilities during training, and require careful hyperparameter selection [46]. ", "page_idx": 3}, {"type": "text", "text": "For completeness, we also mention other papers which are only slightly relevant to us. For example, [23] considers incomplete OT which relaxes only one of the OT marginal constraints and is less general than the unbalanced OT. Other works [12, 4] incorporate unbalanced OT into the training objective of GANs aimed at generating samples from noise. ", "page_idx": 3}, {"type": "text", "text": "In contrast to the listed works, our paper proposes a theoretically justified and lightweight solver to the UEOT problem, see Table 1 for the detailed comparison of solvers. ", "page_idx": 3}, {"type": "text", "text": "4 Light Unbalanced OT Solver ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we derive the optimization objective ( 4.1) of our U-LightOT solver, present practical aspects of training and inference procedures ( 4.2) and derive the generalization bounds for our solver ( 4.4). We provide the proofs of all theor etical results in Appendix A. ", "page_idx": 4}, {"type": "text", "text": "4.1 Derivation of the Optimization Objective ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Following the learning setup described above, we aim to get a parametric approximation $\\gamma_{\\theta,w}$ of the UEOT plan $\\gamma^{*}$ . Here $\\theta,\\omega$ are the model parameters to learn, and it will be clear later why we split them into two groups. To recover $\\gamma_{\\theta,\\omega}\\approx\\gamma^{*}$ , our aim is to learn $\\theta,\\omega$ by directly minimizing the $\\mathrm{D}_{\\mathrm{KL}}$ divergence between $\\gamma_{\\theta,w}$ and $\\gamma^{*}$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathrm{D}_{\\mathrm{KL}}\\left(\\gamma^{*}\\Vert\\gamma_{\\theta,w}\\right)\\to\\operatorname*{min}_{(\\theta,w)}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The main difficulty of this optimizing objective (6) is obvious: the UEOT plan $\\gamma^{*}$ is unknown.   \nFortunately, below we show that one still can optimize (6) without knowing $\\gamma^{*}$ . ", "page_idx": 4}, {"type": "text", "text": "Recall that the optimal UEOT plan $\\gamma^{*}$ has the form (5). We first make some changes of the variables. Specifically, we define $\\begin{array}{r}{v^{*}(y)\\stackrel{\\mathrm{def}}{=}\\exp\\bigl\\{\\frac{2\\psi^{*}(y)-\\|y\\|^{2}}{2\\varepsilon}\\bigr\\}}\\end{array}$ . Formula (5) now reads as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\gamma^{*}(x,y)=\\exp\\{\\frac{2\\phi^{*}(x)-\\|x\\|^{2}}{2\\varepsilon}\\}\\mathrm{exp}\\{\\frac{\\langle x,y\\rangle}{\\varepsilon}\\}v^{*}(y)\\mathrm{\\Longrightarrow}\\gamma^{*}(y|x)\\propto\\mathrm{exp}\\{\\frac{\\langle x,y\\rangle}{\\varepsilon}\\}v^{*}(y).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Since the conditional plan has the unit mass, we may write ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\gamma^{*}(y|x)=\\exp\\{\\frac{\\langle x,y\\rangle}{\\varepsilon}\\}\\frac{v^{*}(y)}{c_{v^{*}}(x)}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Consider the decomposition $\\gamma^{*}(x,y)=\\gamma_{x}^{*}(x)\\gamma^{*}(y|x)$ . It shows that to obtain parametrization for the entire plan $\\gamma^{*}(x)$ , it is sufficient to consider parametrizations for its left marginal $\\gamma_{x}^{*}$ and the conditional measure $\\gamma^{*}(y|x)$ . Meanwhile, equation (8) shows that conditional measures $\\gamma^{*}(\\cdot|x)$ are entirely described by the variable $v^{*}$ . We use these observations to parametrize $\\gamma_{\\theta,w}$ . We set ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\gamma_{\\theta,w}(x,y)\\stackrel{\\mathrm{def}}{=}u_{w}(x)\\gamma_{\\theta}(y|x)\\!=\\!u_{w}(x)\\frac{\\exp\\bigl\\{\\langle x,y\\rangle\\bigl/\\varepsilon\\bigr\\}v_{\\theta}(y)}{c_{\\theta}(x)},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $u_{w}$ and $v_{\\theta}$ parametrize marginal measure $\\gamma_{x}^{*}$ and the variable $v^{*}$ , respectively. In turn, the constant $\\begin{array}{r}{c_{\\theta}(x)\\underset{-}{\\overset{\\mathrm{def}}{=}}\\int_{\\mathbb{R}^{d}}\\exp\\{\\frac{\\langle x,y\\rangle}{\\varepsilon}\\}v_{\\theta}(y)d y}\\end{array}$ is the parametrization of $c_{v^{*}}(x)$ . Next, we demonstrate our main result which shows that the optimization of (6) can be done without the access to $\\gamma^{*}$ . ", "page_idx": 4}, {"type": "text", "text": "Theorem 4.1 (Tractable form of $\\mathrm{D}_{\\mathrm{KL}}$ minimization). Assume that $\\gamma^{*}$ is parametrized using (9). Then the following bound holds: $\\varepsilon D_{K L}\\left(\\gamma^{*}\\|\\gamma_{\\theta,w}\\right)\\leq\\mathcal{L}(\\theta,w)-\\mathcal{L}^{*}$ , where ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathscr{L}(\\theta,w)\\frac{d\\varphi}{d\\mathfrak{t}}\\!\\!\\int_{\\mathbb{R}^{d}}\\!\\!\\!\\!\\overline{{f}}_{1}(-\\varepsilon\\log\\frac{u_{w}(x)}{c_{\\theta}(x)}-\\frac{\\|x\\|^{2}}{2})p(x)d x+\\!\\!\\int_{\\mathbb{R}^{d}}\\!\\!\\!\\overline{{f}}_{2}(-\\varepsilon\\log v_{\\theta}(y)-\\frac{\\|y\\|^{2}}{2})q(y)d y+\\varepsilon\\|u_{w}\\|_{1},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "and constant $(-\\mathcal{L}^{\\ast})$ is the optimal value of the dual form (4). The bound is tight in the sence that it turns to $0=0$ when $v_{\\theta}(y)\\!=\\!\\!\\exp\\!\\left\\{2\\psi^{\\ast}(y)\\!-\\!\\|y\\|^{2}\\!/\\!2\\varepsilon\\right\\}$ and $u_{\\omega}(x)\\!=\\!\\exp\\!\\{2\\phi^{\\ast}(x)\\!-\\!|x||^{2}\\!/2\\varepsilon\\}$ . In fact, (10) is the dual form (4) but with potentials $(\\phi,\\psi)$ expressed through $u_{\\omega},v_{\\theta}$ (and $c_{\\theta}$ ): ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\phi(x)\\!\\gets\\!\\phi_{\\theta,\\omega}(x)\\!=\\!\\varepsilon\\mathrm{log}\\frac{u_{w}(x)}{c_{\\theta}(x)}\\!+\\!\\frac{\\|x\\|^{2}}{2},\\quad\\psi(y)\\!\\gets\\!\\psi_{\\theta}(y)\\!=\\!\\varepsilon\\log v_{\\theta}(y)\\!+\\!\\frac{\\|y\\|^{2}}{2}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Our result can be interpreted as the bound on the quality of approximate solution $\\gamma_{\\theta,\\omega}$ (3) recovered from the approximate solution to the dual problem (4). It can be directly proved using the original $(\\phi,\\psi)$ notation of the dual problem, but we use $(u_{\\omega},v_{\\theta})$ instead as with this change of variables the form of the recovered plan $\\gamma_{\\theta,\\omega}$ is more interpretable ( $\\,\\!u_{w}$ defines the first marginal, $v_{\\theta}$ \u2013 conditionals). ", "page_idx": 4}, {"type": "text", "text": "Instead of optimizing (6) to get $\\gamma_{\\theta,\\omega}$ , we may optimize the upper bound (10) which is more tractable. Indeed, (10) is a sum of the expectations w.r.t. the probability measures $p,q$ . We can obtain Monte-Carlo estimation of (10) from random samples and optimize it with stochastic gradient descent procedure w.r.t. $(\\theta,\\omega)$ . The main challenge here is the computation of the variable $c_{\\theta}$ and term $\\Vert u_{\\omega}\\Vert_{1}$ . Below we propose a smart parametrization by which both variables can be derived analytically. ", "page_idx": 4}, {"type": "text", "text": "4.2 Parameterization and the Optimization Procedure ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Paramaterization. Recall that $u_{w}$ parametrizes the density of the marginal $\\gamma_{x}^{*}$ which is unnormalized. Setting $x=0$ in equation (7), we get $\\gamma^{*}(y|x=0)\\propto v^{*}(\\dot{y})$ which means that $v^{*}$ also corresponds to an unnormalized density of a measure. These motivate us to use the unnormalized Gaussian mixture parametrization for the potential $v_{\\theta}(y)$ and measure $u_{w}(x)$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\nv_{\\theta}(y)\\!=\\!\\sum_{k=1}^{K}\\!\\!\\alpha_{k}\\mathcal{N}(y|r_{k},\\!\\varepsilon S_{k});\\;\\;u_{\\omega}(x)\\!\\!=\\!\\!\\sum_{l=1}^{L}\\!\\!\\beta_{l}\\mathcal{N}(x|\\mu_{l},\\!\\varepsilon\\Sigma_{l}).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "$\\theta\\ {\\stackrel{\\mathrm{def}}{=}}\\ \\{\\alpha_{k},r_{k},S_{k}\\}_{k=1}^{K}$ , $w\\ {\\stackrel{\\mathrm{def}}{=}}\\ \\{\\beta_{l},\\mu_{l},\\Sigma_{l}\\}_{l=1}^{L}$ with $\\alpha_{k},\\beta_{l}\\,\\geq\\,0,\\,r_{k},\\mu_{l}\\,\\in\\,\\mathbb{R}^{d}$ and $0\\prec S_{k},\\Sigma_{l}\\in$ $\\mathbb{R}^{d\\times d}$ . The covariance matrices are scaled by $\\varepsilon$ just for convenience. ", "page_idx": 5}, {"type": "text", "text": "For this type of parametrization, it holds that $\\begin{array}{r}{\\|u_{w}\\|_{1}=\\sum_{l=1}^{L}\\beta_{l}}\\end{array}$ . Moreover, there exist closed-from expressions for the normalization constant $c_{\\theta}(x)$ and conditional plan $\\gamma_{\\theta}(y|x)$ , see [41, Proposition 3.2]. Specifically, define $r_{k}(x)\\stackrel{\\mathrm{def}}{=}r_{k}+S_{k}x$ and $\\widetilde{\\alpha}_{k}(x)\\ {\\stackrel{\\mathrm{def}}{=}}\\ \\alpha_{k}\\exp\\{{\\frac{x^{T}S_{k}x+2r_{k}^{T}x}{2\\varepsilon}}\\}$ xTSkx+2rkT x}. It holds that ", "page_idx": 5}, {"type": "equation", "text": "$$\nc_{\\theta}(x)=\\sum_{k=1}^{K}\\widetilde{\\alpha}_{k}(x);\\;\\;\\gamma_{\\theta}(y|x)=\\frac{1}{c_{\\theta}(x)}\\sum_{k=1}^{K}\\widetilde{\\alpha}_{k}(x)\\mathcal{N}(y|r_{k}(x),\\varepsilon S_{k}).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Using this result and (11), we get the expression for $\\gamma_{\\theta,w}$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\gamma_{\\theta,w}(x,y)=u_{\\omega}(x)\\cdot\\gamma_{\\theta}(y|x)=\\underbrace{\\sum_{l=1}^{L}\\beta_{l}\\mathcal{N}(x|\\mu_{l},\\varepsilon\\Sigma_{l})}_{u_{\\omega}(x)}\\cdot\\underbrace{\\frac{\\sum_{k=1}^{K}\\widetilde{\\alpha}_{k}(x)\\mathcal{N}(y|r_{k}(x),\\varepsilon S_{k})}{\\sum_{k=1}^{K}\\widetilde{\\alpha}_{k}(x)}}_{\\gamma_{\\theta}(y|x)}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Training. We recall that the measures $p,q$ are accessible only by a number of empirical samples (see the learning setup in 2). Thus, given samples $\\{x_{1},...,x_{N}\\}$ and $\\{y_{1},...,y_{M}\\}$ , we optimize the empirical analog of (10): ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\widehat{\\mathcal{L}}(\\theta,w)\\overset{\\mathrm{def}}{=}\\frac{1}{N}\\sum_{i=1}^{N}\\overline{{f}}_{1}(-\\varepsilon\\log\\frac{u_{w}(x_{i})}{c_{\\theta}(x_{i})}-\\frac{\\|x_{i}\\|^{2}}{2})+\\frac{1}{M}\\sum_{j=1}^{M}\\overline{{f}}_{2}(-\\varepsilon\\log v_{\\theta}(y_{j})-\\frac{\\|y_{j}\\|^{2}}{2})+\\varepsilon\\|u_{w}\\|_{1}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "using minibatch gradient descent procedure w.r.t. parameters $(\\theta,w)$ . In the parametrization of $v_{\\theta}$ and $u_{w}$ (11), we utilize the diagonal matrices $S_{k},\\Sigma_{l}$ . This allows decreasing the number of learnable parameters in $\\theta$ and speeding up the computation of inverse matrices $S_{k}^{-1},\\ \\Sigma_{l}^{-1}$ . ", "page_idx": 5}, {"type": "text", "text": "Inference. Sampling from the conditional and marginal measures $\\gamma_{\\theta,w}(y|x)\\!\\approx\\!\\gamma^{*}(y|x)$ , $u_{w}\\!\\approx\\!\\gamma_{x}^{*}$ is easy and lightweight since they are explicitly parametrized as Gaussian mixtures, see (12), (11). ", "page_idx": 5}, {"type": "text", "text": "4.3 Connection to Related Prior Works ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The idea of using Gaussian Mixture parametrization for dual potentials in EOT-related tasks first appeared in the EOT/SB benchmark [29]. There it was used to obtain the benchmark pairs of probability measures with the known EOT solution between them. In [41], the authors utilized this type of parametrization to obtain a light solver (LightSB) for the balanced EOT. ", "page_idx": 5}, {"type": "text", "text": "Our solver for unbalanced EOT (10) subsumes their solver for balanced EOT as well as one problem subsumes the other for the special case of divergences, see the remark in 2. Let $f_{1}$ , $f_{2}$ be convex indicators of $\\{1\\}$ . Then $\\overline{{f_{1}}}(t)=t$ and $\\overline{{f_{2}}}(t)=t$ and objective (10) becomes ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}(\\theta,w)\\!=\\!\\int_{\\mathbb{R}^{d}}(-\\varepsilon\\log\\frac{u_{w}(x)}{c_{\\theta}(x)}\\!-\\!\\frac{\\|x\\|^{2}}{2})p(x)d x+\\int_{\\mathbb{R}^{d}}(-\\varepsilon\\log v_{\\theta}(y)\\!-\\!\\frac{\\|y\\|^{2}}{2})q(y)d y\\!+\\!\\varepsilon\\|u_{w}\\|_{1}=}\\\\ {-\\varepsilon\\Big(\\displaystyle\\int_{\\mathbb{R}^{d}}\\frac{u_{w}(x)}{c_{\\theta}(x)}p(x)d x\\!+\\!\\!\\int_{\\mathbb{R}^{d}}\\!\\log v_{\\theta}(y)q(y)d y\\!-\\!\\|u_{w}\\|_{1}\\Big)\\!-\\!\\underbrace{\\!-\\!\\!\\int_{\\mathbb{R}^{d}}\\!\\frac{\\|x\\|^{2}}{2}p(x)d x\\!-\\!\\displaystyle\\int_{\\mathbb{R}^{d}}\\!\\frac{\\|y\\|^{2}}{2}q(y)d y}_{\\stackrel{\\mathrm{def}}{=}\\!\\ensuremath{\\mathrm{Const}}(p,q)}\\!=\\!\\!}\\\\ {\\varepsilon(\\displaystyle\\int_{\\mathbb{R}^{d}}\\log c_{\\theta}(x)p(x)d x)-\\displaystyle\\int_{\\mathbb{R}^{d}}\\log v_{\\theta}(y)q(y)d y)-\\!\\displaystyle\\int_{\\mathbb{R}^{d}}\\log v_{\\theta}(y)q(y)d y)-}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "equation", "text": "$$\n\\varepsilon\\int_{\\mathbb{R}^{d}}\\log u_{w}(x)p(x)d x+\\varepsilon\\|u_{w}\\|_{1}-{\\mathrm{Const}}(p,q).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Here line (15) depends exclusively on $\\theta$ and exactly coincides with the LightSB\u2019s objective, see [41, Proposition 8]. At the same time, line (16) depends only on $\\omega$ , and its minimum is attained when $u_{w}=p$ . Thus, this part is not actually needed in the balanced case, see [41, Appendix C]. ", "page_idx": 6}, {"type": "text", "text": "4.4 Generalization Bounds and Universal Approximation Property ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Theoretically, to recover the UEOT plan $\\gamma^{*}$ , one needs to solve the problem $\\begin{array}{r}{\\mathcal{L}(\\theta,\\omega)\\rightarrow\\mathrm{min}_{\\theta,\\omega}}\\end{array}$ as stated in our Theorem 4.1. In practice, the measures $p$ and $q$ are accessible via empirical samples X d=ef {x1, ..., xN} and Y d=ef {y1, ..., yM}, thus, one needs to optimize the empirical counterpart $\\widehat{\\mathcal{L}}(\\theta,\\omega)$ of $\\mathcal{L}(\\boldsymbol{\\theta},\\boldsymbol{\\omega})$ , see (14). Besides, the available potentials $u_{\\omega}$ , $v_{\\theta}$ over which one optimizes the objective come from the restricted class of functions. Specifically, we consider unnormalized Gaussian mixtures $u_{\\omega},v_{\\theta}$ with $K$ and $L$ components respectively. Thus, one may naturally wonder: how close is the recovered plan $\\gamma_{\\widehat{\\theta},\\widehat{\\omega}}$ to the UEOT plan $\\gamma^{*}$ given that $\\begin{array}{r}{(\\widehat{\\theta},\\widehat{\\omega})=\\operatorname*{arg\\,min}_{\\theta,\\omega}\\widehat{\\mathcal{L}}(\\theta,\\omega)?}\\end{array}$ To address this question, we study the generalization error $\\mathbb{E}\\mathrm{D}_{\\mathrm{KL}}\\left(\\gamma^{*}\\Vert\\gamma_{\\widehat{\\theta},\\widehat{\\omega}}\\right)$ , i.e., the expectation of $\\mathbf{D}_{\\mathrm{KL}}$ between $\\gamma^{*}$ and $\\gamma_{\\widehat{\\theta},\\widehat{\\omega}}$ taken w.r.t. the random realization of the train datasets $X\\sim p,Y\\sim q$ . Let $(\\overline{{\\theta}},\\overline{{\\omega}})=\\arg\\operatorname*{min}_{(\\theta,\\omega)\\in\\Theta\\times\\Omega}\\mathcal{L}(\\theta,\\omega)$ denote the best approximators of $\\mathcal{L}(\\boldsymbol{\\theta},\\boldsymbol{\\omega})$ in the admissible class. From Theorem 4.1 it follows that that $\\mathbb{E}\\mathrm{D}_{\\mathrm{KL}}\\left(\\gamma^{*}\\Vert\\gamma_{\\widehat{\\theta},\\widehat{\\omega}}\\right)$ can be upper bounded using $\\mathbb{E}(\\mathcal{L}(\\widehat{\\theta},\\widehat{\\omega})-\\mathcal{L}^{*})$ . The latter can be decomposed into the estimation and approximation errors: $\\mathbb{E}(\\mathcal{L}(\\widehat{\\theta},\\widehat{\\omega})\\!-\\!\\mathcal{L}^{*})\\!=\\!\\mathbb{E}[\\mathcal{L}(\\widehat{\\theta},\\widehat{\\omega})\\!-\\!\\mathcal{L}(\\overline{{\\theta}},\\overline{{\\omega}})]\\!+\\!\\mathbb{E}[\\mathcal{L}(\\overline{{\\theta}},\\overline{{\\omega}})\\!-\\!\\mathcal{L}^{*}]\\!=\\!\\underbrace{\\mathbb{E}[\\mathcal{L}(\\widehat{\\theta},\\widehat{\\omega})\\!-\\!\\mathcal{L}(\\overline{{\\theta}},\\overline{{\\omega}})]}_{\\mathrm{Esimation~error~}}\\!+\\!\\underbrace{[\\mathcal{L}(\\overline{{\\theta}},\\overline{{\\omega}})\\!-\\!\\mathcal{L}^{*}]}_{\\mathrm{Approvimation~error~}}.$ (17) ", "page_idx": 6}, {"type": "text", "text": "We establish the quantitative bound for the estimation error in the proposition below. ", "page_idx": 6}, {"type": "text", "text": "Proposition 4.2 (Bound for the estimation error). Let $p,q$ be compactly supported and assume that $\\overline{{f}}_{1},\\ \\overline{{f}}_{2}$ are Lipshitz. Assume that the considered parametric classes $\\Theta$ , \u2126 $(\\ni\\left(\\theta,\\omega\\right))$ consist of unnormalized Gaussian mixtures with $K$ and $L$ components respectively with bounded means $\\|r_{k}\\|,\\|\\mu_{l}\\|\\leq R$ (for some $R\\,>\\,0,$ ), covariances $s I\\,\\preceq\\,S_{k},\\Sigma_{l}\\,\\preceq\\,S I$ (for some $0\\,<\\,s\\,\\leq\\,S_{\\!\\;\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!S\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!$ ) and weights $a\\leq\\alpha_{k},\\beta_{l}\\leq A$ (for some $0<a\\le A_{\\mathrm{,}}$ ). Then the following holds: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbb{E}\\big[\\mathcal{L}(\\widehat{\\theta},\\widehat{\\omega})-\\mathcal{L}(\\overline{{\\theta}},\\overline{{\\omega}})\\big]\\leq O(\\frac{1}{\\sqrt{N}})+O(\\frac{1}{\\sqrt{M}}),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $O(\\cdot)$ hides the constants depending only on $K,L,R,s,S,a,A,p,q,\\varepsilon$ but not on sizes $M,N$ . ", "page_idx": 6}, {"type": "text", "text": "This proposition allows us to conclude that the estimation error converges to zero when $N$ and $M$ tend to infinity at the usual parametric rate. It remains to clarify the question: can we make the approximation error arbitrarily small? We answer this question positively in our Theorem below. ", "page_idx": 6}, {"type": "text", "text": "Theorem 4.3 (Gaussian mixture parameterization for the variables provides the universal approximation of UEOT plans). Let $p$ and $q$ be compactly supported and assume that ${\\overline{{f}}}_{1},\\ {\\overline{{f}}}_{2}$ are Lipshitz. Then for all $\\delta>0$ there exist Gaussian mixtures $v_{\\theta}$ , $u_{\\omega}$ (11) with scalar covariances $S_{k}=\\lambda_{k}I_{d}\\succ0$ , $\\Sigma_{l}=\\zeta_{l}I_{d}\\succ0$ of their components that satisfy $D_{K L}\\left(\\gamma^{*}\\|\\gamma_{\\theta,\\omega}\\right)\\leq\\varepsilon^{-1}(\\mathcal{L}(\\theta,\\omega)-\\mathcal{L}^{*})<\\delta$ . ", "page_idx": 6}, {"type": "text", "text": "In summary, results of this section show that one can make the generalization error arbitrarily small given a sufficiently large amount of samples and components in the Gaussian parametrization. It means that theoretically our solver can recover the UEOT plan arbitrarily well. ", "page_idx": 6}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we test our U-LightOT solver on several setups from the related works. The code is written using PyTorch framework and is publicly available at ", "page_idx": 6}, {"type": "text", "text": "https://github.com/milenagazdieva/LightUnbalancedOptimalTransport. ", "page_idx": 6}, {"type": "text", "text": "The experiments are issued in the convenient form of $^*$ .ipynb notebooks. Each experiment requires several minutes of training on CPU with 4 cores. Technical training details are given in Appendix B. ", "page_idx": 6}, {"type": "image", "img_path": "co8KZws1YK/tmp/7b231c6eb60a6516b6efe15ecb4cde56d3ddd0d759c2a0df05c973e920bdf1d5.jpg", "img_caption": ["(a) Input, target measures (b) U-LightOT, $\\tau=10^{2}$ (c) U-LightOT, $\\tau=10^{1}$ (d) U-LightOT, $\\tau=10^{0}$ Figure 1: Conditional plans $\\gamma_{\\theta,\\omega}(y|x)$ learned by our solver in Gaussians Mixture experiment with unbalancedness parameter $\\tau\\in[10^{0},10^{1},10^{2}]$ . Here $p_{\\omega}$ denotes the normalized first marginal $u_{w}$ , i.e., $p_{\\omega}=u_{\\omega}/\\|u_{\\omega}\\|_{1}$ . "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "5.1 Example with the Mixture of Gaussians ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We provide an illustrative \u2019Gaussians Mixture\u2019 example in 2D to demonstrate the ability of our solver to deal with the imbalance of classes in the source and target measures. We follow the experimental setup proposed in [16, Figure 2] and define the probability measures $p,q$ as follows (Fig. 1a): ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{p(x)\\!=\\!\\displaystyle\\frac{1}{4}\\!\\mathcal{N}(x|(-3,3),0.1\\cdot I_{2})\\!+\\!\\displaystyle\\frac{3}{4}\\!\\mathcal{N}(x|(1,3),0.1\\cdot I_{2}),}}\\\\ {{q(y)\\!=\\!\\displaystyle\\frac{3}{4}\\!\\mathcal{N}(y|(-3,0),0.1\\cdot I_{2})\\!+\\!\\displaystyle\\frac{1}{4}\\!\\mathcal{N}(y|(1,0),0.1\\cdot I_{2}).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "We test our U-LightOT solver with scaled $\\mathrm{D}_{\\mathrm{KL}}$ divergences, i.e., $f_{1}(t),f_{2}(t)$ are defined by $\\tau$ \u00b7 $f_{\\mathrm{KL}}(t)=\\tau(t\\log(\\bar{t})-t+1)$ where $\\tau>0$ . We provide the learned plans for $\\tau\\,\\in\\,[1,\\,\\,10^{1},\\,\\,\\dot{1}0^{2}]$ . The results in Fig. 1 show that parameter $\\tau$ can be used to control the level of unbalancedness of the learned plans. For $\\tau\\,=\\,1$ , our U-LightOT solver truly learns the UEOT plans, see Fig. 1d. When $\\tau$ increases, the solver fails to transport the mass from the input Gaussians to the closest target ones. Actually, for $\\tau=10^{2}$ , our solutions are similar to the solutions of [41, LightSB] solver which approximates balanced EOT plans between the measures. Hereafter, we treat $\\tau$ as the unbalancedness parameter. In Appendix C, we test the performance of our solver with $\\mathbf{D}_{\\chi^{2}}$ divergence. ", "page_idx": 7}, {"type": "text", "text": "Remark. Here we conduct all experiments with the entropy regularization parameter $\\varepsilon=0.05$ . The parameter $\\varepsilon$ is responsible for the stochasticity of the learned transport $\\gamma_{\\theta}(\\cdot|x)$ . Since we are mostly interested in the correct transport of the mass (controlled by $f_{1},f_{2})$ rather than the stochasticity, we do not pay much attention to $\\varepsilon$ throughout the paper. ", "page_idx": 7}, {"type": "text", "text": "5.2 Unpaired Image-to-Image Translation ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Another popular testbed which is usually considered in OT/EOT papers is the unpaired image-toimage translation [71] task. Since our solver uses the parametrization based on Gaussian mixture, it may be hard to apply U-LightOT for learning translation directly in the image space. Fortunately, nowadays it is common to use autoencoders [59] for more efficient translation. We follow the setup of [41, Section 5.4] and use pre-trained ALAE autoencoder [55] for $1024\\times1024$ FFHQ dataset [34] of human faces. We consider different subsets of FFHQ dataset (Adult, Young, Man, Woman) and all variants of translations between them: $A d u l t{\\leftrightarrow}Y o u n g$ and Woman $\\leftrightarrow M a n$ . ", "page_idx": 7}, {"type": "table", "img_path": "co8KZws1YK/tmp/143e84aa6213edac20f270732150bca0da5ad3fcedd13a8f04970b4c3f14fe8e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "The main challenge of the described translations is the imbalance of classes in the images from source and target subsets, see Table 2. Let us consider in more detail $A d u l t{\\rightarrow}Y o u n g$ translation. In the FFHQ dataset, the amount of adult men significantly outnumbers the adult women, while the amount of young men is smaller than that of young women. Thus, balanced OT/EOT solvers are ", "page_idx": 7}, {"type": "text", "text": "Table 2: Number of train FFHQ images for each subset. ", "page_idx": 7}, {"type": "text", "text": "expected to translate some of adult man representatives to young woman ones. At the same time, solvers based on unbalanced transport are supposed to alleviate this issue. ", "page_idx": 7}, {"type": "text", "text": "Baselines. We perform a comparison with the recent procedure [16, UOT-FM] which considers roughly the same setup and demonstrates good performance. This method interpolates the results of unbalanced discrete OT to the continuous setup using the flow matching [44]. For completeness, we include the comparison with LightSB and balanced optimal transport-based flow matching (OTFM) [44] to demonstrate the issues of the balanced solvers. We also consider neural network based solvers relying on the adversarial learning such as UOT-SD [9] and UOT-GAN[70]. ", "page_idx": 7}, {"type": "image", "img_path": "co8KZws1YK/tmp/ff47414ee89a83a213da36dfa93729a497d2e5096c3a337006065bf23c57a78b.jpg", "img_caption": ["Figure 2: Visualization of pairs of accuracies (keep-target) for our U-LightOT solver and other OT/EOT methods in the image translation experiment. The values of unbalancedness parameters for our U-LightOT solver $(\\tau)$ and [16, UOT-FM] $\\lambda=r e g_{-}m)$ ) are specified directly on the plots. "], "img_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "co8KZws1YK/tmp/854b188898a364e2a882a078a74aca91242d5131b2789d453feadc1f6667ddf6.jpg", "table_caption": ["Table 3: Comparison of wall-clock running times of unbalanced OT/EOT solvers in Young $\\rightarrow$ Adult translation. The best results are in bold, second best are underlined. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Metrics. We aim to assess the ability of the solvers to perform the translation of latent codes keeping the class of the input images unchanged, e.g., keeping the gender in $A d u l t{\\rightarrow}Y o u n g$ translation. Thus, we train a $99\\%$ MLP classifier for gender using the latent codes of images. We use it to compute the accuracy of preserving the gender during the translation. We denote this number by accuracy (keep). ", "page_idx": 8}, {"type": "text", "text": "Meanwhile, it is also important to ensure that the generated images belong to the distribution of the target images rather than the source ones, e.g., belong to the distribution of young people in our example. To monitor this property, we use another $99\\%$ MLP classifier to identify whether each of the generated images belong to the target domain or to the source one. Then we calculate the fraction of the generated images belonging to the target domain which we denote as accuracy (target). ", "page_idx": 8}, {"type": "text", "text": "Results. Unbalanced OT/EOT approaches are equipped with some kind of unbalancedness parameter (like $\\tau$ in our solver) which influences the methods\u2019 results: with the increase of unbalancedness, accuracy of keeping the class increases but the accuracy of mapping to the target decreases. The latter is because of the relaxation of the marginal constraints in UEOT. For a fair comparison, we aim to compute the trade-offs between the keep/target accuracies for different values of the unbalancedness. We do this for our solver and UOT-FM. We found that GAN-based unbalanced approaches show unstable behaviour, so we report UOT-SD and UOT-GAN\u2019s results only for one parameter value. ", "page_idx": 8}, {"type": "text", "text": "For convenience, we visualize the accuracies\u2019 pairs for our solver and its competitors in Fig. 2. The evaluation shows that our U-LightOT method can effectively solve translation tasks in high dimensions $d=512$ ) and outperforms its alternatives in dealing with class imbalance issues. Namely, we see that our solver allows for achieving the best accuracy of keeping the attributes of the input images than other methods while it provides good accuracy of mapping to the target class. While balanced methods and some of the unbalanced ones provide really high accuracy of mapping to the target, their corresponding accuracies of keeping the attributes are worse than ours meaning that we are better in dealing with class imbalance issues. As expected, for large parameter $\\tau$ , the results of our U-LightOT solver coincide with those for LightSB which is a balanced solver. Meanwhile, our solver has the lowest wall-clock running time among the existing unbalanced solvers, see Table 3 for comparison. We demonstrate qualitative results of our solver and baselines in Fig. 3. The choice of unbalancedness parameter for visualization of our method and UOT-FM is detailed in Appendix B. ", "page_idx": 8}, {"type": "text", "text": "We present the results of the quantitative comparison in the form of tables in Appendix D.1. In Appendix C, we perform the ablation study of our solver focusing on the selection of parameters $\\tau,\\varepsilon$ and number of Gaussian mixtures\u2019 components. ", "page_idx": 8}, {"type": "image", "img_path": "co8KZws1YK/tmp/1ca8e694b05e0e6dbd534a132cd4ff2cb83808b589435d495a2efbc41670a8c8.jpg", "img_caption": ["Figure 3: Unpaired translation with LightSB, OT-FM, UOT-FM and our U-LightOT solvers applied in the latent space of ALAE [55] for FFHQ images [34] $\\left\\langle1024\\!\\times\\!1024\\right\\rangle$ . "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "6 Discussion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Potential impact. Our light and unbalanced solver has a lot of advantages in comparison with the other existing UEOT solvers. First, it does not require complex max-min optimization. Second, it provides the closed form of the conditional measures $\\gamma_{\\theta,\\omega}(y|x)\\approx\\gamma^{*}(y|x)$ of the UEOT plan. Moreover, it allows for sampling both from the conditional measure $\\gamma_{\\theta,\\omega}(y|x)$ and marginal measure $u_{w}(x)\\approx\\gamma_{x}^{*}(x)$ . Besides, the decisive superiority of our lightweight and unbalanced solver is its simplicity and convenience of use. Indeed, it has a straightforward and non-minimax optimization objective and avoids heavy neural parametrization. As a result, our lightweight and unbalanced solver converges in minutes on CPU. We expect that these advantages could boost the usage of our solver as a standard and easy baseline for UEOT task with applications in different spheres. ", "page_idx": 9}, {"type": "text", "text": "The limitations and broader impact of our solver are discussed in Appendix E. ", "page_idx": 9}, {"type": "text", "text": "ACKNOWLEDGEMENTS. The work of Skoltech was supported by the Analytical center under the RF Government (subsidy agreement 000000D730321P5Q0002, Grant No. 70-2021-00145 02.11.2021). We thank Kirill Sokolov and Mikhail Persiianov for providing a valuable feedback and suggestions for improving the proofs and clarity of our paper. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] R. Agrawal and T. Horel. Optimal bounds between f-divergences and integral probability metrics. Journal of Machine Learning Research, 22(128):1\u201359, 2021.   \n[2] M. Arjovsky, S. Chintala, and L. Bottou. Wasserstein generative adversarial networks. In International conference on machine learning, pages 214\u2013223. PMLR, 2017.   \n[3] A. Asadulaev, A. Korotin, V. Egiazarian, and E. Burnaev. Neural optimal transport with general cost functionals. In The Twelfth International Conference on Learning Representations, 2024.   \n[4] Y. Balaji, R. Chellappa, and S. Feizi. Robust optimal transport with applications in generative modeling and domain adaptation. Advances in Neural Information Processing Systems, 33: 12934\u201312944, 2020.   \n[5] C. Bunne, S. G. Stark, G. Gut, J. S. Del Castillo, M. Levesque, K.-V. Lehmann, L. Pelkmans, A. Krause, and G. R\u00e4tsch. Learning single-cell perturbation responses using neural optimal transport. Nature Methods, 20(11):1759\u20131768, 2023.   \n[6] L. Chapel, R. Flamary, H. Wu, C. F\u00e9votte, and G. Gasso. Unbalanced optimal transport through non-negative penalized linear regression. Advances in Neural Information Processing Systems, 34:23270\u201323282, 2021.   \n[7] T. Chen, G.-H. Liu, and E. Theodorou. Likelihood training of schr\u00f6dinger bridge using forwardbackward sdes theory. In International Conference on Learning Representations, 2021.   \n[8] L. Chizat. Unbalanced optimal transport: Models, numerical methods, applications. PhD thesis, Universit\u00e9 Paris sciences et lettres, 2017.   \n[9] J. Choi, J. Choi, and M. Kang. Generative modeling through the semi-dual formulation of unbalanced optimal transport. In Advances in Neural Information Processing Systems, volume 36, 2024.   \n[10] M. Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. In Advances in neural information processing systems, pages 2292\u20132300, 2013.   \n[11] M. Daniels, T. Maunu, and P. Hand. Score-based generative neural networks for large-scale optimal transport. Advances in neural information processing systems, 34:12955\u201312965, 2021.   \n[12] Q. Dao, B. Ta, T. Pham, and A. Tran. Robust diffusion gan using semi-unbalanced optimal transport. arXiv preprint arXiv:2311.17101, 2023.   \n[13] V. De Bortoli, J. Thornton, J. Heng, and A. Doucet. Diffusion schr\u00f6dinger bridge with applications to score-based generative modeling. Advances in Neural Information Processing Systems, 34:17695\u201317709, 2021.   \n[14] N. Deb, P. Ghosal, and B. Sen. Rates of estimation of optimal transport maps using plug-in estimators via barycentric projections. Advances in Neural Information Processing Systems, 34: 29736\u201329753, 2021.   \n[15] P. Dvurechenskii, D. Dvinskikh, A. Gasnikov, C. Uribe, and A. Nedich. Decentralize and randomize: Faster algorithm for Wasserstein barycenters. In Advances in Neural Information Processing Systems, pages 10760\u201310770, 2018.   \n[16] L. Eyring, D. Klein, T. Uscidda, G. Palla, N. Kilbertus, Z. Akata, and F. Theis. Unbalancedness in neural monge maps improves unpaired domain translation. In The Twelfth International Conference on Learning Representations, 2024.   \n[17] J. Fan, A. Taghvaei, and Y. Chen. Scalable computations of Wasserstein barycenter via input convex neural networks. arXiv preprint arXiv:2007.04462, 2020.   \n[18] J. Fan, S. Liu, S. Ma, H.-M. Zhou, and Y. Chen. Neural monge map estimation and its applications. Transactions on Machine Learning Research, 2023. ISSN 2835-8856. URL https://openreview.net/forum?id $=$ 2mZSlQscj3. Featured Certification.   \n[19] K. Fatras, Y. Zine, R. Flamary, R. Gribonval, and N. Courty. Learning with minibatch wasserstein: asymptotic and gradient properties. In AISTATS 2020-23nd International Conference on Artificial Intelligence and Statistics, volume 108, pages 1\u201320, 2020.   \n[20] K. Fatras, T. S\u00e9journ\u00e9, R. Flamary, and N. Courty. Unbalanced minibatch optimal transport; applications to domain adaptation. In International Conference on Machine Learning, pages 3186\u20133197. PMLR, 2021.   \n[21] R. Flamary, N. Courty, A. Gramfort, M. Z. Alaya, A. Boisbunon, S. Chambon, L. Chapel, A. Corenflos, K. Fatras, N. Fournier, et al. Pot: Python optimal transport. The Journal of Machine Learning Research, 22(1):3571\u20133578, 2021.   \n[22] M. Gazdieva, L. Rout, A. Korotin, A. Kravchenko, A. Filippov, and E. Burnaev. An optimal transport perspective on unpaired image super-resolution. arXiv preprint arXiv:2202.01116, 2022.   \n[23] M. Gazdieva, A. Korotin, D. Selikhanovych, and E. Burnaev. Extremal domain translation with neural optimal transport. In Advances in Neural Information Processing Systems, volume 36, 2023.   \n[24] A. Genevay. Entropy-Regularized Optimal Transport for Machine Learning. Theses, PSL University, Mar. 2019. URL https://theses.hal.science/tel-02319318.   \n[25] A. Genevay, G. Peyr\u00e9, and M. Cuturi. Learning generative models with sinkhorn divergences. In International Conference on Artificial Intelligence and Statistics, pages 1608\u20131617. PMLR, 2018.   \n[26] N. Gozlan, C. Roberto, P.-M. Samson, and P. Tetali. Kantorovich duality for general transport costs and applications. Journal of Functional Analysis, 273(11):3327\u20133405, 2017.   \n[27] I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and A. C. Courville. Improved training of Wasserstein GANs. In Advances in Neural Information Processing Systems, pages 5767\u20135777, 2017.   \n[28] N. Gushchin, A. Kolesov, A. Korotin, D. Vetrov, and E. Burnaev. Entropic neural optimal transport via diffusion processes. In Advances in Neural Information Processing Systems, 2023.   \n[29] N. Gushchin, A. Kolesov, P. Mokrov, P. Karpikova, A. Spiridonov, E. Burnaev, and A. Korotin. Building the bridge of schr\\\" odinger: A continuous entropic optimal transport benchmark. arXiv preprint arXiv:2306.10161, 2023.   \n[30] N. Gushchin, S. Kholkin, E. Burnaev, and A. Korotin. Light and optimal schr\\\"odinger bridge matching. In arXiv preprint arXiv:2402.03207, 2024.   \n[31] G. H. Hardy, J. E. Littlewood, and G. P\u00f3lya. Inequalities. Cambridge university press, 1952.   \n[32] J.-C. H\u00fctter and P. Rigollet. Minimax estimation of smooth optimal transport maps. 2021.   \n[33] H. Janati, B. Muzellec, G. Peyr\u00e9, and M. Cuturi. Entropic optimal transport between unbalanced gaussian measures has a closed form. Advances in neural information processing systems, 33: 10468\u201310479, 2020.   \n[34] T. Karras, S. Laine, and T. Aila. A style-based generator architecture for generative adversarial networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 4401\u20134410, 2019.   \n[35] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.   \n[36] D. Klein, T. Uscidda, F. Theis, and M. Cuturi. Generative entropic neural optimal transport to map within and across spaces. arXiv preprint arXiv:2310.09254, 2023.   \n[37] A. Korotin, L. Li, A. Genevay, J. M. Solomon, A. Filippov, and E. Burnaev. Do neural optimal transport solvers work? a continuous wasserstein-2 benchmark. Advances in Neural Information Processing Systems, 34:14593\u201314605, 2021.   \n[38] A. Korotin, L. Li, J. Solomon, and E. Burnaev. Continuous wasserstein-2 barycenter estimation without minimax optimization. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id $=$ 3tFAs5E-Pe.   \n[39] A. Korotin, A. Kolesov, and E. Burnaev. Kantorovich strikes back! wasserstein gans are not optimal transport? In Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2022.   \n[40] A. Korotin, D. Selikhanovych, and E. Burnaev. Kernel neural optimal transport. In International Conference on Learning Representations, 2023. URL https://openreview.net/forum? id=Zuc_MHtUma4.   \n[41] A. Korotin, N. Gushchin, and E. Burnaev. Light schr\\\" odinger bridge. In The Twelfth International Conference on Learning Representations, 2024.   \n[42] T. Koshizuka and I. Sato. Neural lagrangian $\\operatorname{schr}\\backslash\"\\{0\\}$ dinger bridge: Diffusion modeling for population dynamics. In The Eleventh International Conference on Learning Representations, 2022.   \n[43] M. Liero, A. Mielke, and G. Savar\u00e9. Optimal entropy-transport problems and a new hellinger\u2013 kantorovich distance between positive measures. Inventiones mathematicae, 211(3):969\u20131117, 2018.   \n[44] Y. Lipman, R. T. Chen, H. Ben-Hamu, M. Nickel, and M. Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022.   \n[45] F. L\u00fcbeck, C. Bunne, G. Gut, J. S. del Castillo, L. Pelkmans, and D. Alvarez-Melis. Neural unbalanced optimal transport via cycle-consistent semi-couplings. arXiv preprint arXiv:2209.15621, 2022.   \n[46] M. Lucic, K. Kurach, M. Michalski, S. Gelly, and O. Bousquet. Are GANs created equal? a large-scale study. In Advances in neural information processing systems, pages 700\u2013709, 2018.   \n[47] A. Makkuva, A. Taghvaei, S. Oh, and J. Lee. Optimal transport mapping via input convex neural networks. In International Conference on Machine Learning, pages 6672\u20136681. PMLR, 2020.   \n[48] T. Manole, S. Balakrishnan, J. Niles-Weed, and L. Wasserman. Plugin estimation of smooth optimal transport maps. arXiv preprint arXiv:2107.12364, 2021.   \n[49] M. Mohri, A. Rostamizadeh, and A. Talwalkar. Foundations of machine learning. MIT press, 2018.   \n[50] P. Mokrov, A. Korotin, and E. Burnaev. Energy-guided entropic neural optimal transport. In The Twelfth International Conference on Learning Representations, 2024.   \n[51] Q. M. Nguyen, H. H. Nguyen, Y. Zhou, and L. M. Nguyen. On unbalanced optimal transport: Gradient methods, sparsity and approximation error. arXiv preprint arXiv:2202.03618, 2022.   \n[52] T. T. Nguyen, H. D. Nguyen, F. Chamroukhi, and G. J. McLachlan. Approximation by finite mixtures of continuous density functions that vanish at infinity. Cogent Mathematics & Statistics, 7(1):1750861, 2020.   \n[53] G. Peyr\u00e9, M. Cuturi, et al. Computational optimal transport. Foundations and Trends\u00ae in Machine Learning, 11(5-6):355\u2013607, 2019.   \n[54] K. Pham, K. Le, N. Ho, T. Pham, and H. Bui. On unbalanced optimal transport: An analysis of sinkhorn algorithm. In International Conference on Machine Learning, pages 7673\u20137682. PMLR, 2020.   \n[55] S. Pidhorskyi, D. A. Adjeroh, and G. Doretto. Adversarial latent autoencoders. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14104\u201314113, 2020.   \n[56] A.-A. Pooladian and J. Niles-Weed. Entropic estimation of optimal transport maps. arXiv preprint arXiv:2109.12004, 2021.   \n[57] P. Rigollet and A. J. Stromme. On the sample complexity of entropic optimal transport. arXiv preprint arXiv:2206.13472, 2022.   \n[58] R. Rockafellar. Duality and stability in extremum problems involving convex functions. Pacific Journal of Mathematics, 21(1):167\u2013187, 1967.   \n[59] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10684\u201310695, 2022.   \n[60] L. Rout, A. Korotin, and E. Burnaev. Generative modeling with optimal transport maps. In International Conference on Learning Representations, 2022.   \n[61] V. Seguy, B. B. Damodaran, R. Flamary, N. Courty, A. Rolet, and M. Blondel. Large scale optimal transport and mapping estimation. In International Conference on Learning Representations, 2018.   \n[62] T. S\u00e9journ\u00e9, G. Peyr\u00e9, and F.-X. Vialard. Unbalanced optimal transport, from theory to numerics. arXiv preprint arXiv:2211.08775, 2022.   \n[63] S. Shalev-Shwartz and S. Ben-David. Understanding machine learning: From theory to algorithms. Cambridge university press, 2014.   \n[64] A. Taghvaei and A. Jalali. 2-Wasserstein approximation via restricted convex potentials with application to improved training for GANs. arXiv preprint arXiv:1902.07197, 2019.   \n[65] A. Tong, J. Huang, G. Wolf, D. Van Dijk, and S. Krishnaswamy. Trajectorynet: A dynamic optimal transport network for modeling cellular dynamics. In International conference on machine learning, pages 9526\u20139536. PMLR, 2020.   \n[66] F. Vargas, P. Thodoroff, A. Lamacraft, and N. Lawrence. Solving schr\u00f6dinger bridges via maximum likelihood. Entropy, 23(9):1134, 2021.   \n[67] G. Wang, Y. Jiao, Q. Xu, Y. Wang, and C. Yang. Deep generative learning via schr\u00f6dinger bridge. In International Conference on Machine Learning, pages 10794\u201310804. PMLR, 2021.   \n[68] Y. Xie, M. Chen, H. Jiang, T. Zhao, and H. Zha. On scalable and efficient computation of large scale optimal transport. In International Conference on Machine Learning, pages 6882\u20136892. PMLR, 2019.   \n[69] Y. Xie, Y. Luo, and X. Huo. An accelerated stochastic algorithm for solving the optimal transport problem. arXiv preprint arXiv:2203.00813, 2022.   \n[70] K. D. Yang and C. Uhler. Scalable unbalanced optimal transport using generative adversarial networks. In International Conference on Learning Representations, 2018.   \n[71] J.-Y. Zhu, R. Zhang, D. Pathak, T. Darrell, A. A. Efros, O. Wang, and E. Shechtman. Toward multimodal image-to-image translation. Advances in neural information processing systems, 30, 2017. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Proofs ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.1 Proof of Theorem 4.1 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Proof. To begin with, we derive the useful property of the optimal UEOT plans. We recall that there exist measurable functions $\\phi^{*}$ and $\\psi^{*}$ ( 2) which minimize dual form (4), i.e., $(\\phi^{*},\\psi^{*})\\in$ arg $\\operatorname*{min}_{(\\phi,\\psi)}\\mathcal{I}(\\phi,\\psi)$ where ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{I}(\\phi,\\psi)\\overset{\\mathrm{def}}{=}}\\\\ {\\varepsilon\\displaystyle\\int_{\\mathbb{R}^{d}}\\!\\!\\int_{\\mathbb{R}^{d}}\\!\\exp\\{\\frac{1}{\\varepsilon}(\\phi(x)+\\psi(y)-\\frac{\\|x-y\\|^{2}}{2})\\}d x d y+}\\\\ {\\displaystyle\\int_{\\mathbb{R}^{d}}\\!\\overline{{f}}_{1}(-\\phi(x))p(x)d x+\\!\\int_{\\mathbb{R}^{d}}\\!\\!\\overline{{f}}_{2}(-\\psi(y))q(y)d y.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Then, by the first order optimality condition $\\begin{array}{r}{\\frac{d}{d\\phi}\\mathcal{J}(\\phi,\\psi)|_{\\phi=\\phi^{*}}\\,=\\,0}\\end{array}$ and $\\begin{array}{r}{\\frac{d}{d\\psi}\\mathcal{I}(\\phi,\\psi)|_{\\psi=\\psi^{*}}=0}\\end{array}$ . It means that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\displaystyle\\frac{d}{d\\phi}\\mathcal{I}(\\phi,\\psi)|_{\\phi=\\phi^{*}}(x)=}\\\\ &{}&{\\displaystyle\\varepsilon\\mathrm{-}\\bar{\\varepsilon}^{-\\dagger}\\int_{\\mathbb{R}^{d}}\\exp\\{\\varepsilon^{-1}(\\phi^{*}(x)+\\psi^{*}(y)-\\|x-y\\|^{2}/2)\\}d y-\\nabla\\overline{{f}}_{1}(-\\phi^{*}(x))p(x)=0\\Longrightarrow}\\\\ &{}&{\\nabla\\overline{{f}}_{1}(-\\phi^{*}(x))=\\frac{\\gamma_{x}^{*}(x)}{p(x)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "holds for all $x$ , s.t. $p(x)>0$ . Analogously, $\\begin{array}{r}{\\nabla\\overline{{f}}_{2}(-\\psi^{*}(y))=\\frac{\\gamma_{y}^{*}(y)}{q(y)}}\\end{array}$ for all $y$ , s.t. $q(y)>0$ . ", "page_idx": 14}, {"type": "text", "text": "For convenience of our further derivations, we consider the change of variables to simplify the expression for L(\u03b8, \u03c9). Let \u03d5\u03b8,\u03c9(x)d=ef \u03b5 log uc\u03b8\u03c9((xx)) + \u2225x2\u22252 and $\\begin{array}{r}{\\psi_{\\theta}(y)\\stackrel{\\mathrm{def}}{=}\\varepsilon\\log v_{\\theta}(y){+}\\frac{\\|y\\|^{2}}{2}}\\end{array}$ . Then ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\theta,\\omega)=\\int_{\\mathbb{R}^{d}}\\overline{{f}}_{1}(-\\phi_{\\theta,\\omega}(x))p(x)d x+\\int_{\\mathbb{R}^{d}}\\overline{{f}}_{2}(-\\psi_{\\theta}(y))q(y)d y+\\varepsilon\\|u_{w}\\|_{1}=\\mathcal{I}(\\phi_{\\theta,\\omega},\\psi_{\\theta}).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Next, we note that: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{D_{KL}}\\left(\\gamma^{*}\\Vert\\gamma_{\\theta,\\omega}\\right)\\!=\\int_{\\mathbb{R}^{d}\\times\\mathbb{R}^{d}}\\gamma^{*}(x,y)\\log\\gamma^{*}(x,y)d x d y\\;-}\\\\ &{\\int_{\\mathbb{R}^{d}\\times\\mathbb{R}^{d}}\\!\\!\\!\\gamma^{*}(x,y)\\log\\gamma_{\\theta,\\omega}(x,y)d x d y+\\!\\Vert\\gamma_{\\theta,\\omega}\\Vert_{1}-\\Vert\\gamma^{*}\\Vert_{1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Now we recall that the ground-truth UEOT plan $\\gamma^{*}(x,y)$ and the optimal dual variables $\\phi^{*}(x),\\psi^{*}(y)$ are connected via equation (5). Similarly, our parametrized plan $\\gamma_{\\theta,\\omega}(x,y)$ can be expressed using $\\phi_{\\theta,\\omega}(x),\\psi_{\\theta}(y)$ as $\\gamma_{\\theta,\\omega}(x,y)=\\exp\\{\\varepsilon^{-1}(\\phi_{\\theta,\\omega}(x)+\\psi_{\\theta}(y)-\\|x\\mathrm{-}y\\|^{2}/2)\\}$ . Then ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(20)=\\varepsilon^{-1}\\int_{\\mathbb{R}^{d}\\times\\mathbb{R}^{d}}\\gamma^{*}(x,y)\\big(\\phi^{*}(x)+\\psi^{*}(y)-\\|x-y\\|^{2}/2\\big)d x d y-}\\\\ {\\varepsilon^{-1}\\int_{\\mathbb{R}^{d}\\times\\mathbb{R}^{d}}\\gamma^{*}(x,y)\\big(\\phi_{\\theta,\\omega}(x)+\\psi_{\\theta}(y)-\\|x-y\\|^{2}/2\\big)d x d y+\\|\\gamma_{\\theta,\\omega}\\|_{1}-\\|\\gamma^{*}\\|_{1}=}\\\\ {\\varepsilon^{-1}\\int_{\\mathbb{R}^{d}\\times\\mathbb{R}^{d}}\\gamma^{*}(x,y)\\big(\\phi^{*}(x)+\\psi^{*}(y)-\\phi_{\\theta,\\omega}(x)-\\psi_{\\theta}(y)\\big)d x d y+\\|\\gamma_{\\theta,\\omega}\\|_{1}-\\|\\gamma^{*}\\|_{1}=}\\\\ {\\varepsilon^{-1}\\int_{\\mathbb{R}^{d}}\\gamma_{x}^{*}(x)\\big(\\phi^{*}(x)-\\phi_{\\theta,\\omega}(x)\\big)d x+\\varepsilon^{-1}\\int_{\\mathbb{R}^{d}}\\gamma_{y}^{*}(y)\\big(\\psi^{*}(y)-\\psi_{\\theta}(y)\\big)d y+\\|\\gamma_{\\theta,\\omega}\\|_{1}-\\|\\gamma^{*}\\|_{1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Finally, we derive ", "page_idx": 14}, {"type": "equation", "text": "$$\n(21)=\\varepsilon^{-1}\\!\\int_{\\mathbb{R}^{d}}\\underbrace{\\frac{\\gamma_{x}^{*}(x)}{p(x)}}_{\\nabla\\overline{{f}}_{1}(-\\phi^{*}(x))}\\big(\\phi^{*}(x)\\!-\\!\\phi_{\\theta,\\omega}(x)\\big)p(x)d x+\n$$", "text_format": "latex", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{c^{-1}\\displaystyle\\int_{\\mathbb{R}^{+}}\\frac{\\partial_{x}^{0}}{\\partial y}(y)}&{(v^{*}(y)-\\mathrm{vig}(y))\\Big(\\eta(y)\\Big)\\phi(y)+\\lvert\\mathrm{v}_{x}\\rvert\\|_{1}-\\mathrm{l}\\tau^{*}\\|_{1}-}&{}\\\\ {\\displaystyle v_{x}^{-1}\\displaystyle(-\\epsilon_{x}(y))}&{}\\\\ {\\displaystyle v^{*}\\displaystyle\\int_{\\mathbb{R}^{+}}^{\\infty}\\int_{(0,T)}\\gamma_{x}\\zeta_{y}\\big(-\\epsilon_{y}^{*}(x)\\big)\\Big(\\phi^{*}(x)-\\phi_{x}(x)\\Big)\\rho(y)d z+}&{}\\\\ {\\displaystyle\\varepsilon^{-1}\\displaystyle\\int_{\\mathbb{R}^{+}}\\nabla_{x}^{2}\\zeta_{x}(-\\mathrm{vig}(y))\\big(\\psi^{*}(y)-\\mathrm{vig}(y)\\big)\\Big(\\eta(y)+\\ln\\epsilon_{x}(y)-\\mathrm{l}\\tau^{*}\\|_{1}\\leq\\epsilon_{x}(y)\\Big)\\Big(\\eta(y)\\Big)\\,\\mathrm{d}x+}&{}\\\\ {\\displaystyle\\varepsilon^{-1}\\displaystyle\\int_{\\mathbb{R}^{+}}^{\\infty}\\int_{(0,T)}\\gamma_{x}\\Big(\\zeta_{x}-\\mathrm{vig}(x)\\Big(x)\\Big)\\Big[\\eta\\Big(y\\Big)+\\Gamma_{1}\\Big(-\\mathrm{v}^{*}\\Big(x)\\Big)\\Big]\\eta(x)d z+}&{}\\\\ {\\displaystyle\\varepsilon^{-1}\\displaystyle\\int_{\\mathbb{R}^{+}}^{\\infty}\\int_{(0,T)}\\gamma_{x}\\Big(-\\mathrm{vig}(y)\\Big)\\Big(\\eta(y)+\\mathrm{fig}(y)-\\mathrm{fig}(z)\\Big)\\Big(\\eta(x)-\\mathrm{fig}(y)-\\mathrm{fig}(z)\\Big)\\,\\mathrm{d}z}&{}\\\\ {\\displaystyle\\varepsilon^{-1}\\displaystyle\\int_{\\mathbb{R}^{+}}^{\\infty}\\Big(-\\mathrm{fig}\\Big(x)\\Big)\\Big(x^{*}\\Big)\\displaystyle\\varepsilon^{-1}\\displaystyle\\int_{\\mathbb{R}^{+}}^{\\infty}\\Big(-\\mathrm{vig}(y)\\Big)\\Big(\\eta(y)+\\mathrm{fig}(z)\\Big)\\|-}&{}\\\\ {\\displaystyle\\varepsilon^{-1}\\displaystyle\\Bigg(\\displaystyle\\int_{0}^{\\infty}\\int_{\\mathbb{R}^{+}}(-\\mathrm{vig}(x \n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The inequality (22) follows from the convexity of the functions $\\overline{{f_{1}}}$ and $\\overline{{f_{2}}}$ . ", "page_idx": 15}, {"type": "text", "text": "A.2 Proof of Proposition 4.2 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We begin with proving the auxiliary theoretical results (Propositions A.1, A.2) which are needed to prove the main proposition of this section. ", "page_idx": 15}, {"type": "text", "text": "Proposition A.1 (Rademacher bound on the estimation error). It holds that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\big[\\mathcal{L}(\\widehat{\\theta})-\\mathcal{L}(\\overline{{\\theta}})\\big]\\leq4\\mathcal{R}_{N}(\\mathcal{F}_{1},p)+4\\mathcal{R}_{M}(\\mathcal{F}_{2},q),}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\begin{array}{r}{\\mathcal F_{1}=\\{\\overline{{f}}_{1}(-\\phi_{\\theta,\\omega})|(\\theta,\\omega)\\in\\Theta\\times\\Omega\\},\\mathcal F_{2}=\\{\\overline{{f}}_{2}(-\\psi_{\\theta})|\\theta\\in\\Theta\\}\\,f o r\\,\\phi_{\\theta,\\omega}(x)=\\varepsilon\\log\\frac{u_{w}(x)}{c_{\\theta}(x)}+\\varepsilon\\log\\frac{\\mathcal F(\\theta,\\omega)}{c_{\\theta}(x)}\\}.}\\end{array}$ $\\frac{\\|x\\|^{2}}{2}$ , $\\begin{array}{r}{\\psi_{\\theta}(y)=\\varepsilon\\log v_{\\theta}(y)+\\frac{\\|y\\|^{2}}{2}}\\end{array}$ , and $\\mathcal{R}_{N}(\\mathcal{F}_{1},p)$ , $\\mathcal{R}_{M}(\\mathcal{F}_{2},q)$ denote the Rademacher complexity [63, 26] of the functional classes $\\boldsymbol{\\mathcal{U}},~\\boldsymbol{\\mathcal{V}}$ w.r.t. to the sample sizes $N$ , $M$ of distributions $p,\\,q.$ . ", "page_idx": 15}, {"type": "text", "text": "Proof of Proposition A.1. The derivation of this fact is absolutely analogous to [41, Proposition H.2], [50, Theorem 4] or [64, Theorem 3.4]. \u53e3 ", "page_idx": 15}, {"type": "text", "text": "Proposition A.2 (Bound on the Rademacher complexity of the considered classes). Let $0<a\\le A$ , let $0<u\\leq U$ , let $0<w\\le W$ and $V>0$ . Consider the class of functions ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\mathcal{F}_{1}=\\displaystyle\\{x\\mapsto\\overline{{f}}_{1}(-\\varepsilon\\log u_{w}(x)+\\varepsilon\\log c_{\\theta}(x)-\\frac{\\|x\\|^{2}}{2})\\},}}\\\\ {{\\mathcal{F}_{2}=\\displaystyle\\{y\\mapsto\\overline{{f}}_{2}(-\\varepsilon\\log v_{\\theta}(y)-\\frac{\\|y\\|^{2}}{2})\\}\\,w h e r e}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathcal{V}=\\left\\{x\\mapsto\\sum_{k=1}^{K}\\alpha_{k}\\exp\\left(x^{T}U_{k}x+v_{k}^{T}x+w_{k}\\right)w i t h\n$$", "text_format": "latex", "page_idx": 15}, {"type": "equation", "text": "$$\nu I\\preceq U_{k}=U_{k}^{T}\\preceq U I;\\|v_{k}\\|\\leq V;w\\leq w_{k}\\leq W;a\\leq\\alpha_{k}\\leq A\\}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Following [41, Proposition $H.3J,$ , we call the functions of the class $\\mathcal{V}$ as constrained log-sum-exp quadratic functions. We assume that $\\overline{{f}}_{1}$ , ${\\overline{{f}}}_{2}$ are Lipshitz functions and measures $p$ , q are compactly supported with the supports lying in a zero-centered ball of a radius $R>0$ . Then ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathcal{R}_{N}(\\mathcal{F}_{1},p)\\leq\\frac{C_{0}}{\\sqrt{N}},\\;\\mathcal{R}_{M}(\\mathcal{F}_{2},q)\\leq\\frac{C_{1}}{\\sqrt{M}}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where the constants $C_{0}$ , $C_{1}$ do not depend on sizes $N$ , $M$ of the empirical samples from $p,\\ q$ . ", "page_idx": 15}, {"type": "text", "text": "Proof of Proposition A.2. Thanks to [41, Proposition H.3], the Rademacher complexities of constrained log-sum-exp quadratic functions $x\\mapsto\\log u_{\\omega}(x)$ , $x\\mapsto\\log c_{\\theta}(x)$ and $y\\mapsto\\log v_{\\theta}(y)$ are known to be bounded by $\\textstyle O({\\frac{1}{\\sqrt{N}}})$ or $\\textstyle O({\\frac{1}{\\sqrt{M}}})$ respectively. According to the definition of Rademacher complexity, for single quadratic functions $\\begin{array}{r}{x\\mapsto\\frac{x^{T}x}{2}\\;(y\\mapsto\\frac{y^{T}y}{2})}\\end{array}$ it is just equal to zero. Then, using the well-known scaling and additivity properties of the Rademacher complexity [63], we get that $\\begin{array}{r}{x\\mapsto-\\varepsilon\\log u_{w}(x)+\\bar{\\varepsilon^{\\mathrm{log}}}\\,c_{\\theta}(x)-\\frac{\\|\\bar{x}\\|^{\\bar{z}}}{2}}\\end{array}$ and y  \u2192\u2212\u03b5 log v\u03b8(y) \u2212\u2225y2\u2225 are bounded by $\\textstyle O({\\frac{1}{\\sqrt{N}}})$ and $\\textstyle O({\\frac{1}{\\sqrt{M}}})$ respectively. The remaining step is to recall that $\\overline{{f}}_{1}(x)$ and $\\overline{{f}}_{2}(y)$ are Lipschitz. Therefore, according to Talagrand\u2019s contraction principle [49], the Rademaher complexities of ${\\mathcal{F}}_{1}$ and ${\\mathcal{F}}_{2}$ are also bounded by $\\begin{array}{r}{\\bar{O}(\\frac{1}{\\sqrt{N}})}\\end{array}$ and $\\textstyle O({\\frac{1}{\\sqrt{M}}})$ , respectively. \u53e3 ", "page_idx": 16}, {"type": "text", "text": "Proof of Proposition 4.2. The proof directly follows from Propositions A.1 and A.2. ", "page_idx": 16}, {"type": "text", "text": "A.3 Proof of Theorem 4.3 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "To begin with, we provide a quick reminder about the Fenchel-Rockafellar theorem which is needed to derive the dual form of problem (3). ", "page_idx": 16}, {"type": "text", "text": "Theorem A.3 (Fenchel-Rockafellar [58]). Let $(E,E^{\\prime})$ and $(F,F^{\\prime})$ be two couples of topologically paired spaces. Let $A:E\\mapsto F$ be a continuous linear operator and ${\\overline{{A}}}:F^{\\prime}\\mapsto E^{\\prime}$ be its adjoint. Let $f$ and $g$ be lower semicontinuous and proper convex functions defined on $E$ and $F$ respectively. If there exists $x\\in$ domf s.t. $g$ is continuous at $A x$ , then ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{x\\in E}-f(-x)-g(A x)=\\operatorname*{min}_{\\overline{{y}}\\in F^{\\prime}}\\overline{{f}}(\\overline{{A}}\\;\\overline{{y}})+\\overline{{g}}(\\overline{{y}})\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and the min is attained. Moreover, if there exists a maximizer $x\\in E$ then there exists ${\\overline{{y}}}\\in{\\overline{{F}}}$ satisfying $A x\\in\\partial\\overline{{g}}(\\overline{{y}})$ and $\\overline{{A}}\\overline{{y}}\\in\\partial f(-x)$ . ", "page_idx": 16}, {"type": "text", "text": "We note that in the below theorem $\\mathcal{C}_{2}(\\mathbb{R}^{d})$ does not denote the space of twice differentiable continuous functions. The exact definition of this space and $\\mathcal{C}_{2,b}(\\mathbb{R}^{d})$ is given in notations part. ", "page_idx": 16}, {"type": "text", "text": "Theorem A.4 (Dual form of problem (3)). The primal UEOT problem (3) has the dual counterpart (4) where the potentials $(\\phi,\\psi)$ belong to the space $\\mathring{C}_{2,b}(\\mathbb{R}^{d})^{\\star}\\times\\mathring{C}_{2,b}(\\mathbb{R}^{d})$ . The minimum of (3) is attained for a unique $\\gamma^{*}\\in\\mathcal{M}_{2,+}(\\mathbb{R}^{d}\\times\\mathbb{R}^{d})$ . ", "page_idx": 16}, {"type": "text", "text": "Proof. We recall that in the primal form, the minimization is performed over functions $\\gamma$ belonging to $\\bar{\\mathcal{M}}_{2,+}(\\mathbb{R}^{d}\\times\\mathbb{R}^{d})$ . In this proof, we suppose that this space is endowed with a coarsest topology $\\sigma(\\mathcal{M}_{2,+}(\\mathbb{R}^{d}\\times\\mathbb{R}^{d}))$ which makes continuous the linear functionals $\\gamma\\mapsto\\textstyle\\int\\zeta d\\gamma$ , $\\forall\\zeta\\in\\mathcal{C}_{2}(\\mathbb{R}^{d}\\times$ $\\mathbb{R}^{d})$ . Then the topological space $\\left(\\boldsymbol{\\mathcal{M}}_{2,+}(\\mathbb{R}^{d}\\times\\mathbb{R}^{d}),\\sigma(\\boldsymbol{\\mathcal{M}}_{2,+}(\\mathbb{R}^{d}\\times\\mathbb{R}^{d}))\\right)$ has a topological dual $\\left(\\mathcal{M}_{2,+}(\\mathbb{R}^{d}\\times\\mathbb{R}^{d}),\\sigma(\\mathcal{M}_{2,+}(\\mathbb{R}^{d}\\times\\mathbb{R}^{d}))\\right)^{\\prime}$ which, actually, is (linear) isomorphic to the space $\\mathcal{C}_{2}(\\mathbb{R}^{d}\\times$ $\\mathbb{R}^{d}$ ), see [26, Lemma 9.9]. This fact opens an opportunity to apply the well-celebrated FenchelRockafellar theorem. For this purpose, we will consider the following spaces: $E\\stackrel{\\mathrm{def}}{=}C_{2}(\\mathbb{R}^{d})\\times C_{2}(\\mathbb{R}^{d})$ , $F\\stackrel{\\mathrm{def}}{=}C_{2}(\\mathbb{R}^{d}\\times\\mathbb{R}^{d})$ and their duals $E^{\\prime}\\,{\\stackrel{\\mathrm{def}}{=}}\\,{\\mathcal{M}}_{2,+}(\\mathbb{R}^{d})\\times{\\mathcal{M}}_{2,+}(\\mathbb{R}^{d})$ and $F^{\\prime}\\stackrel{\\mathrm{def}}{=}\\mathcal{M}_{2,+}(\\mathbb{R}^{d}\\times\\mathbb{R}^{d})$ . ", "page_idx": 16}, {"type": "text", "text": "Step 1. Recall that the convex conjugate of any function $g:\\mathcal{M}_{2,+}(\\mathbb{R}^{d}\\times\\mathbb{R}^{d})\\rightarrow\\mathbb{R}\\cup\\{+\\infty\\}$ is defined for each $\\zeta\\,\\in\\,(\\mathcal{M}_{2,+}(\\mathbb{R}^{d}\\times\\mathbb{R}^{d}))^{\\prime}\\cong\\mathcal{C}_{2}(\\mathbb{R}^{d}\\times\\mathbb{R}^{d})$ as $\\begin{array}{r}{\\overline{{g}}(\\zeta)\\,=\\,\\operatorname*{sup}_{\\gamma\\in\\mathcal{M}_{2,+}(\\mathbb{R}^{d}\\times\\mathbb{R}^{d})}\\bigl\\{\\langle\\gamma,\\zeta\\rangle-g(\\gamma)\\bigr\\}}\\end{array}$ . For the convenience of further derivations, we introduce additional functionals corresponding to the summands in the primal UEOT problem (3): ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{}}&{}&{P(\\gamma)\\stackrel{\\mathrm{def}}{=}\\int_{\\mathbb{R}^{d}}\\int_{\\mathbb{R}^{d}}\\frac{\\|x-y\\|^{2}}{2}\\gamma(x,y)d x d y-\\varepsilon H(\\gamma);\\quad}\\\\ &{}&{F_{1}(\\gamma_{x})\\stackrel{\\mathrm{def}}{=}D_{f_{1}}\\left(\\gamma_{x}\\|p\\right);\\;\\;\\;F_{2}(\\gamma_{y})\\stackrel{\\mathrm{def}}{=}D_{f_{2}}\\left(\\gamma_{y}\\|q\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "For our purposes, we need to calculate the convex conjugates of these functionals. Fortunately, convex conjugates of $f$ -divergences $F_{1}(\\gamma_{x})$ and $F_{2}(\\gamma_{y})$ are well-known, see [1, Proposition 23], and equal to ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\overline{{F_{1}}}(\\phi)\\overset{\\mathrm{def}}{=}\\int_{\\mathbb{R}^{d}}\\overline{{f_{1}}}(\\phi(x))p(x)d x,\\quad\\overline{{F_{2}}}(\\psi)\\overset{\\mathrm{def}}{=}\\int_{\\mathbb{R}^{d}}\\overline{{f_{2}}}(\\psi(y))q(y)d y.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "To proceed, we calculate the convex conjugate of $P(\\gamma)$ : ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\mathcal{P}(\\zeta)=\\displaystyle\\int_{a_{0}}^{a_{1}}\\int_{a_{1}}^{\\infty}\\int_{a_{2}}^{a_{3}}\\int_{a_{3}}^{a_{4}}d a\\,p-i(\\zeta)=}\\\\ &{}&{=\\displaystyle\\operatorname*{sup}_{\\forall\\in a_{3},\\dots\\nrightarrow\\infty^{+}\\nu}\\Bigg\\{\\int_{a_{4}}\\int_{a_{3}}\\int_{a_{4}}\\int_{a_{5}}(z,y)d(z,y)d\\mu=-\\int_{a_{4}}\\int_{a_{3}}\\int_{a_{5}}^{\\infty}\\int_{a_{3}}^{a_{4}}\\int_{\\mathbb{Z}}(z,y)d\\mu d\\eta=}\\\\ &{}&{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad}\\\\ &{}&{\\displaystyle\\int_{a_{4}}\\int_{a_{3}}^{a_{4}}\\int_{\\mathbb{Z}}\\int_{a_{4}}^{a_{4}}\\int_{\\mathbb{Z}}\\left(\\zeta(x,y)\\frac{1-\\zeta^{2}}{2}\\right)^{2}\\rangle\\langle\\eta(z,y)\\,d\\mu d\\eta+}\\\\ &{}&{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad}\\\\ &{}&{\\displaystyle\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad}\\\\ &{}&{\\displaystyle\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad}\\\\ &{}&{\\displaystyle\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\times\\frac{1}{2}\\int_{a_{3}}^{a_{4}}d\\eta\\int_{a_{4\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Here in the transition from (25) to (26), we keep in mind our prior calculations of $\\mathrm{D}_{\\mathrm{KL}}$ in (20). Recall that $\\mathrm{D}_{\\mathrm{KL}}$ is non-negative and attains zero at the unique point ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\gamma(x,y)=\\exp\\{\\frac{\\zeta(x,y)-\\frac{\\|x-y\\|^{2}}{2}}{\\varepsilon}\\}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Thus, we get ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\overline{{P}}(\\zeta)=\\varepsilon\\int_{\\mathbb{R}^{d}}\\int_{\\mathbb{R}^{d}}\\exp\\{\\frac{\\zeta(x,y)-\\frac{\\|x-y\\|^{2}}{2}}{\\varepsilon}\\}d x d y.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Step 2. Now we are ready to apply the Fenchel-Rockafellar theorem in our case. To begin with, we show that this theorem is applicable to problem (3), i.e., that the functions under consideration satisfy the necessary conditions. Indeed, it is known that the convex conjugate of any functional (e.g., $\\overleftarrow{F_{1}}(\\cdot),\\overline{{F_{2}}}(\\cdot),\\dot{\\overline{{P}}}(\\cdot))$ is lower semi-continuous and convex. Besides, the listed functionals are proper convex2. Indeed, the properness of $\\overline{{F_{1}}}(\\cdot)$ and $\\overline{{F_{2}}}(\\cdot)$ follows from the fact that $f$ -divergences are known to be lower-semicontinuous and proper themselves, while properness of $\\overline{{P}}(\\cdot)$ is evident from (26). ", "page_idx": 17}, {"type": "text", "text": "Now we consider the linear operator $A:C_{2}(\\mathbb{R}^{d})\\times C_{2}(\\mathbb{R}^{d})\\mapsto C_{2}(\\mathbb{R}^{d}\\times\\mathbb{R}^{d})$ which is defined as $A(\\phi,\\psi):(x,y)\\mapsto\\phi(x)+\\psi(y)$ . It is continuous, and its adjoint is defined on $\\mathcal{M}_{2,+}(\\mathbb{R}^{d}\\times\\mathbb{R}^{d})$ ", "page_idx": 17}, {"type": "text", "text": "as $\\overline{{A}}(\\gamma)=(\\gamma_{x},\\gamma_{y})$ . Indeed, $\\begin{array}{r}{\\langle\\overline{{A}}(\\gamma),(u,v)\\rangle=\\langle\\gamma,A(u,v)\\rangle=\\int_{\\mathbb{R}^{d}}\\int_{\\mathbb{R}^{d}}\\gamma(x,y)(u(x)+v(y))d x d y=}\\end{array}$ $\\begin{array}{r}{\\int_{\\mathbb{R}^{d}}\\gamma_{x}(x)u(x)d x+\\int_{\\mathbb{R}^{d}}\\gamma_{y}(y)v(y)d y}\\end{array}$ . Thus, the strong duality and the existence of minimizer for (3) follows from the Fenchel-Rockafellar theorem which states that problems ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{(\\phi,\\psi)\\in C_{2}(\\mathbb{R}^{d})\\times C_{2}(\\mathbb{R}^{d})}\\{-\\overline{{P}}(A(\\phi,\\psi))-\\overline{{F_{1}}}(-\\phi)-\\overline{{F_{2}}}(-\\psi)\\}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\gamma\\in\\mathcal{M}_{2,+}(\\mathbb{R}^{d}\\times\\mathbb{R}^{d})}\\{\\varepsilon P(\\gamma)+F_{1}(\\gamma_{x})+F_{2}(\\gamma_{y})\\}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "are equal. Uniqueness of the minimizer for (3) comes from the strict convexity of $P(\\cdot)$ (which holds thanks to the entropy term). Note that the conjugate of the sum of $F_{1}$ and $F_{2}$ is equal to the sum of their conjugates since they are defined for separate non-intersecting groups of parameters. ", "page_idx": 18}, {"type": "text", "text": "Next we prove that the supremum can be restricted to $\\mathcal{C}_{2,b}(\\mathbb{R}^{d}\\times\\mathbb{R}^{d})$ . Here we use $\\vee$ to denote the operation of taking maximum between the function $f:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ and real value $k$ : $(f\\vee k)(x)~{\\stackrel{\\mathrm{def}}{=}}$ $\\operatorname*{max}(f(x),k)$ . Then analogously to [26, Theorem 9.5], we get: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\operatorname*{sup}_{(\\phi,\\psi)\\in C_{2}(\\mathbb{R}^{d})\\times C_{2}(\\mathbb{R}^{d})}\\{-\\overline{{P}}(A(\\phi,\\psi))-\\overline{{F_{1}}}(-\\phi)-\\overline{{F_{2}}}(-\\psi)\\}=}\\\\ {\\operatorname*{lim}_{(\\phi,\\psi)\\in C_{2}(\\mathbb{R}^{d})\\times C_{2}(\\mathbb{R}^{d})}\\{-\\overline{{P}}(A(\\phi\\vee k_{1},\\psi\\vee k_{2}))-\\overline{{F_{1}}}(-\\phi\\vee k_{1})-\\overline{{F_{2}}}(-\\psi\\vee k_{2})\\}\\leq}\\\\ {\\mathrm{~sup~}_{(\\phi,\\psi)\\in C_{2,b}(\\mathbb{R}^{d})\\times C_{2}(\\mathbb{R}^{d})\\times C_{2,b}(\\mathbb{R}^{d})}\\{-\\overline{{P}}(A(\\phi,\\psi))-\\overline{{F_{1}}}(-\\phi)-\\overline{{F_{2}}}(-\\psi)\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Since the another inequality is obvious, the two quantities are equal which completes the proof. ", "page_idx": 18}, {"type": "text", "text": "Proof of Theorem 4.3. Our aim is to prove that for all $\\delta\\,>\\,0$ there exist unnormalized Gaussian mixtures $u_{\\omega}$ and $v_{\\theta}$ s.t. $\\mathcal{L}(\\theta,\\omega)-\\mathcal{L}^{*}<\\delta\\varepsilon$ . Following (18), we define ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\int_{\\mathbb{R}^{d}}\\int_{\\mathbb{R}^{d}}\\exp\\{\\frac{1}{\\varepsilon}(\\phi(x)+\\psi(y)-\\frac{\\|x-y\\|^{2}}{2})\\}d x d y+\\int_{\\mathbb{R}^{d}}\\overline{{f}}_{1}(-\\phi(x))p(x)d x+\\int_{\\mathbb{R}^{d}}\\overline{{f}}_{2}(-\\psi(y))q(y)d y.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Then from (4), it follows that $\\begin{array}{r}{\\mathcal{L}^{*}=\\operatorname*{inf}_{(\\phi,\\psi)\\in\\mathcal{C}_{2,b}(\\mathbb{R}^{d})\\times\\mathcal{C}_{2,b}(\\mathbb{R}^{d})}\\mathcal{I}(\\phi,\\psi)}\\end{array}$ . Finally, using the definition of the infimum, we get that for all $\\delta^{\\prime}>0$ there exist some functions $(\\widehat{\\phi},\\ \\widehat{\\psi})\\in\\mathcal{C}_{2,b}(\\mathbb{R}^{d})\\times\\mathcal{C}_{2,b}(\\mathbb{R}^{d})$ such that $\\mathcal{I}(\\widehat{\\phi},\\widehat{\\psi}){<}\\mathcal{L}^{*}{+}\\delta^{\\prime}$ . For further derivations, we set $\\delta^{\\prime}\\ {\\stackrel{\\mathrm{def}}{=}}\\ {\\frac{\\delta\\varepsilon}{2}}$ and pick  the corresponding $(\\widehat\\phi,\\;\\widehat\\psi)$ . Step 1. We start with the derivation of some inequalities useful for future steps. Since $(\\phi,\\psi)\\in\\mathcal{C}_{2,b}(\\mathbb{R}^{d})\\times\\mathcal{C}_{2,b}(\\mathbb{R}^{d})$ , they have upper bounds $\\widehat{a}$ and $\\widehat{b}$ such that for all $x,y\\in\\mathbb{R}^{d}\\colon\\widehat{\\phi}(x)\\leq a$ and $\\widehat{\\psi}(y)\\leq b$ respectively. We recall that by the assumption of the theorem, measures $p$ and $q$ are compactly supported. Thus, there exist balls centered at $x=0$ and $y=0$ and having some radius $R>0$ which contain the supports of $p$ and $q$ respectively. Then we define ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widetilde\\phi(x)\\overset{\\mathrm{def}}{=}\\widehat\\phi(x)-\\operatorname*{max}\\{0,\\operatorname*{max}\\{\\|x\\|^{2}-R^{2},\\|x\\|^{4}-R^{4}\\}\\}\\le\\widehat\\phi(x)\\le\\widehat a;}\\\\ {\\widetilde\\psi(y)\\overset{\\mathrm{def}}{=}\\widehat\\psi(y)-\\operatorname*{max}\\{0,\\|y\\|^{2}-R^{2}\\}\\le\\widehat\\psi(y)\\le\\widehat b.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We get that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widetilde{\\phi}(x)\\leq\\widehat{\\phi}(x),\\widetilde{\\psi}(y)\\leq\\widehat{\\psi}(y)\\Longrightarrow}\\\\ {\\varepsilon\\displaystyle\\int_{\\mathbb{R}^{d}}\\int_{\\mathbb{R}^{d}}\\exp\\{\\frac{1}{\\varepsilon}(\\widetilde{\\phi}(x)+\\widetilde{\\psi}(y)\\!-\\!\\frac{\\|x-y\\|^{2}}{2})\\}d x d y\\leq}\\\\ {\\varepsilon\\displaystyle\\int_{\\mathbb{R}^{d}}\\int_{\\mathbb{R}^{d}}\\exp\\{\\frac{1}{\\varepsilon}(\\widehat{\\phi}(x)+\\widehat{\\psi}(y)\\!-\\!\\frac{\\|x-y\\|^{2}}{2})\\}d x d y}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Importantly, for all $x$ and $y$ within the supports of $p$ and $q$ it holds that $\\widetilde{\\phi}(x)=\\widehat{\\phi}(x)$ and $\\widetilde{\\psi}(y)=\\widehat{\\psi}(y)$ , respectively. Then ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\int_{\\mathbb{R}^{d}}\\overline{{f}}_{1}(-\\widehat{\\phi}(x))p(x)d x=\\int_{\\mathbb{R}^{d}}\\overline{{f}}_{1}(-\\widetilde{\\phi}(x))p(x)d x,}\\\\ &{\\displaystyle\\int_{\\mathbb{R}^{d}}\\overline{{f}}_{2}(-\\widehat{\\psi}(y))q(y)d y=\\int_{\\mathbb{R}^{d}}\\overline{{f}}_{2}(-\\widetilde{\\psi}(y))q(y)d y.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Combining (32) and (33), we get that $\\mathcal{J}(\\widetilde{\\phi},\\widetilde{\\psi})\\leq\\mathcal{J}(\\widehat{\\phi},\\widehat{\\psi})<\\mathcal{L}^{*}+\\delta.$ . ", "page_idx": 19}, {"type": "text", "text": "Before moving on, we note that functions $\\exp\\{\\widetilde{\\phi}(x)/\\varepsilon\\}$ and $\\exp\\{\\widetilde{\\psi}(y)/\\varepsilon\\}$ are continuous and nonnegative. Therefore, since measures $p$ and $q$ are compactly supported, there exist some constants $e_{\\mathrm{min}}$ , $h_{\\operatorname*{min}}>0$ such that $\\exp\\{\\widetilde{\\phi}(x)\\big/\\varepsilon\\}>\\,e_{\\mathrm{min}}$ and $\\mathrm{exp}\\big\\{\\tilde{\\psi}(y)\\big/\\varepsilon\\big\\}>h_{\\mathrm{min}}$ for all $x$ and $y$ from the supports of measures $p$ and $q$ respectively. We keep these constants for future steps. ", "page_idx": 19}, {"type": "text", "text": "Step 2. This step of our proof is similar to [41, Theorem 3.4]. We get that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\exp\\big(\\widetilde\\phi(x)/\\varepsilon\\big)\\leq\\exp\\left(\\displaystyle\\frac{\\widehat a-\\operatorname*{max}\\{0,\\|x\\|^{2}-R^{2}\\}}{\\varepsilon}\\right)\\leq\\exp\\big(\\displaystyle\\frac{\\widehat a+R^{2}}{\\varepsilon}\\big)\\cdot\\exp(-\\|x\\|^{2}/\\varepsilon),}\\\\ {\\exp\\big(\\widetilde\\psi(y)/\\varepsilon\\big)\\leq\\exp\\left(\\displaystyle\\frac{\\widehat b-\\operatorname*{max}\\{0,\\|y\\|^{2}-R^{2}\\}}{\\varepsilon}\\right)\\leq\\exp\\big(\\displaystyle\\frac{\\widehat b+R^{2}}{\\varepsilon}\\big)\\cdot\\exp(-\\|y\\|^{2}/\\varepsilon).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "From this we can deduce that $y\\mapsto\\exp\\bigl(\\tilde{\\psi}(y)\\big/\\varepsilon\\bigr)$ is a normalizable density since it is bounded by the unnormalized Gaussian density. Moreover, we see that it vanishes at the infinity. Thus, using the result [52, Theorem 5a], we get that for all $\\delta^{\\prime\\prime}>0$ there exists an unnormalized Gaussian mixture v\u03b8 = v\u03b8(y) such that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\|v_{\\widetilde{\\theta}}-\\exp(\\widetilde{\\psi}/\\varepsilon)\\|_{\\infty}=\\operatorname*{sup}_{y\\in\\mathbb{R}^{D}}|v_{\\widetilde{\\theta}}(y)-\\exp(\\widetilde{\\psi}(y)/\\varepsilon)|<\\delta^{\\prime\\prime}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Following the mentioned theorem, we can set all the covariances in $v_{\\widetilde{\\theta}}$ to be scalar, i.e., define $\\begin{array}{r}{v_{\\widetilde{\\theta}}(y)=\\sum_{k=1}^{K}\\widetilde{\\alpha}_{k}{\\mathcal N}(y|\\widetilde{r}_{k},\\varepsilon\\widetilde{\\lambda}_{k}I_{d})}\\end{array}$ for some $K$ and $\\widetilde{\\alpha}_{k}\\in\\mathbb{R}_{+},\\widetilde{r}_{k}\\in\\mathbb{R}^{d},\\widetilde{\\lambda}_{k}\\in\\mathbb{R}_{+}\\;(k\\in\\{1,\\ldots,K\\})$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\delta^{\\prime\\prime}=\\frac{\\delta\\varepsilon}{2}\\cdot\\Bigg[L_{1}\\cdot\\frac{\\varepsilon}{e_{\\mathrm{min}}}+L_{2}\\cdot\\frac{\\varepsilon}{h_{\\mathrm{min}}}+\\varepsilon(2\\pi\\varepsilon)^{\\frac{d}{2}}\\Big(1+(\\pi\\varepsilon)^{\\frac{d}{2}}\\exp\\big\\{\\frac{\\widehat{a}+R^{2}}{\\varepsilon}\\}\\Big)\\Bigg]^{-1}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "For simplicity, we consider the other mixture $v_{\\theta}(y)\\stackrel{\\mathrm{def}}{=}v_{\\widetilde{\\theta}}(y)\\exp(-\\frac{\\|y\\|^{2}}{2\\varepsilon})$ which is again unnormalized and has scalar covariances, see the proof of [41, Th eorem 3.4] for explanation. We denote the weights, means, and covariances of this mixture by $\\alpha_{k}\\in\\mathbb{R}_{+}$ , $r_{k}\\in\\mathbb{R}^{d}$ and $\\lambda_{k}\\in\\mathbb{R}_{+}$ , respectively. ", "page_idx": 19}, {"type": "text", "text": "We derive that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\varepsilon\\int_{\\mathbb{R}^{d}}\\int_{\\mathbb{R}^{d}}\\exp\\{\\frac{1}{\\varepsilon}(\\tilde{\\phi}(x)+\\widetilde{\\psi}(y)-\\frac{\\|x-y\\|^{2}}{2})\\}d x d y=}\\\\ {\\varepsilon\\int_{\\mathbb{R}^{d}}\\int_{\\mathbb{R}^{d}}\\exp\\{\\frac{\\widetilde{\\phi}(x)}{\\varepsilon}\\}\\exp\\{-\\frac{\\|x-y\\|^{2}}{2\\varepsilon}\\}\\exp\\{\\frac{\\widetilde{\\psi}(y)}{\\varepsilon}\\}d x d y>}\\\\ {\\varepsilon\\int_{\\mathbb{R}^{d}}\\int_{\\mathbb{R}^{d}}\\exp\\{\\frac{\\widetilde{\\phi}(x)}{\\varepsilon}\\}\\exp\\{-\\frac{\\|x-y\\|^{2}}{2\\varepsilon}\\}(v_{\\widetilde{\\theta}}(y)-\\delta^{\\prime\\prime})d x d y=}\\\\ {\\varepsilon\\int_{\\mathbb{R}^{d}}\\int_{\\mathbb{R}^{d}}\\exp\\{\\frac{\\widetilde{\\phi}(x)}{\\varepsilon}\\}\\exp\\{-\\frac{\\|x-y\\|^{2}}{2\\varepsilon}\\}v_{\\widetilde{\\theta}}(y)d x d y-}\\\\ {\\varepsilon\\int_{\\mathbb{R}^{d}}\\int_{\\mathbb{R}^{d}}\\exp\\{\\frac{\\widetilde{\\phi}(x)}{\\varepsilon}\\}\\exp\\{-\\frac{\\|x-y\\|^{2}}{2\\varepsilon}\\}v_{\\widetilde{\\theta}}(y)d x d y-}\\\\ {\\delta^{\\prime\\prime}\\varepsilon\\int_{\\mathbb{R}^{d}}\\int_{\\mathbb{R}^{d}}\\exp\\{\\frac{\\widetilde{\\phi}(x)}{\\varepsilon}\\}\\exp\\{-\\frac{\\|x-y\\|^{2}}{2\\varepsilon}\\}d x d y=}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{c\\int_{\\mathbb{R}^{3}}\\!\\!\\!\\int_{0}^{\\infty}\\!\\!\\!\\!\\int_{0}^{\\infty}\\!\\!\\!\\!\\int_{0}^{1}\\!\\!\\!\\!\\int_{0}^{\\infty}\\!\\!\\!\\!\\left(\\frac{1-\\frac{D}{\\theta}}{r}\\right)\\!\\!\\exp\\left(\\frac{\\!\\!-\\frac{D}{\\theta}\\!}{r}\\right)\\!\\exp\\left(\\!-\\frac{|\\frac{D}{\\theta}|^{2}}{r^{3}}\\right)\\!\\exp\\left(j\\!\\frac{\\!-\\frac{D}{\\theta}}{r}\\right)\\!\\exp\\left(j\\!\\frac{\\!-\\frac{D}{\\theta}}{r}\\right)\\!\\exp\\left(j\\!\\frac{\\!-\\frac{D}{\\theta}}{r}\\right)}\\\\ {c\\int_{0}^{\\infty}\\!\\!\\!\\int_{0}^{\\infty}\\!\\!\\!\\int_{0}^{\\infty}\\!\\!\\!\\int_{0}^{\\infty}\\!\\!\\!\\exp\\left(\\frac{\\!-\\frac{D}{\\theta}\\!}{r}\\right)\\!\\exp\\left(\\!-\\frac{|\\frac{D}{\\theta}|^{2}}{r^{3}}\\right)\\!\\exp\\left(j\\!\\frac{\\!-\\!\\frac{D}{\\theta}}{r}\\right)\\!\\exp\\left(k\\!\\theta\\right)\\!-\\!\\!\\pi}\\\\ {c\\int_{0}^{\\infty}\\!\\!\\!\\exp\\left(\\frac{\\!-\\frac{D}{\\theta}}{r}\\right)\\!\\exp\\left(-\\frac{\\frac{1-\\theta}{\\theta}}{r^{3}}\\right)\\!\\left(\\!\\frac{\\int_{0}^{\\infty}\\!\\!\\!-\\!\\frac{D}{\\theta}\\!}{r^{3}}\\!\\exp\\left(j\\!\\frac{\\!-\\frac{D}{\\theta}}{r}\\right)\\!\\exp\\left(k\\!\\theta\\right)\\!\\right)d\\!z-}\\\\ {c\\int_{0}^{\\infty}\\!\\!\\!\\int_{0}^{\\infty}\\!\\!\\!\\int_{0}^{\\infty}\\!\\!\\!\\int_{0}^{\\infty}\\!\\!\\!\\exp\\left(\\frac{\\!-\\frac{D}{\\theta}\\!}{r}\\right)\\!\\left(\\!\\frac{\\int_{0}^{\\infty}\\!\\!\\!-\\!\\frac{D}{\\theta}\\!}{r^{3}}\\!\\!-\\!\\frac{|\\frac{D}{\\theta}|^{2}}{r^{3}}\\!\\right)\\!d\\!z-}\\\\ {c\\int_{0}^{\\infty}\\!\\!\\exp\\left(\\frac{\\!-\\frac{D}{\\theta}}{r}\\right)\\!\\exp\\left(-\\frac{\\frac{\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Step 3. At this point, we will show that for every $\\delta^{\\prime\\prime}>0$ , there exists an unnormalized Gaussian mixture $u_{\\widetilde{\\omega}}$ which is $\\delta^{\\prime\\prime}$ -close to $\\exp\\{\\widetilde{\\phi}(x)/\\varepsilon\\}c_{\\theta}(x)$ . Using the closed-form expression for $c_{\\theta(x)}$ from [41, Propo sition 3.2], we get that ", "page_idx": 20}, {"type": "equation", "text": "$$\nc_{\\theta}(x)=\\sum_{k=1}^{K}\\alpha_{k}\\exp\\{-\\frac{\\|r_{k}\\|^{2}}{2\\varepsilon\\lambda_{k}}\\}\\exp\\{\\frac{\\|r_{k}+x\\lambda_{k}\\|^{2}}{2\\varepsilon\\lambda_{k}}\\}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Then ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\exp\\big(\\widehat{\\phi}(x)/\\varepsilon\\big)c_{\\theta}(x)\\leq}\\\\ &{}&{\\exp\\bigg(\\displaystyle\\frac{\\widehat{a}-\\operatorname*{max}\\{0,\\|x\\|^{4}-R^{4}\\}}{\\varepsilon}\\bigg)c_{\\theta}(x)\\leq\\exp\\big(\\displaystyle\\frac{\\widehat{a}+R^{4}}{\\varepsilon}\\big)\\cdot\\exp(-\\|x\\|^{4}/\\varepsilon)c_{\\theta}(x)=}\\\\ &{}&{\\displaystyle\\sum_{k=1}^{K}\\alpha_{k}\\exp\\big(\\displaystyle\\frac{\\widehat{a}+R^{4}}{\\varepsilon}\\big)\\exp\\{-\\displaystyle\\frac{\\|r_{k}\\|^{2}}{2\\varepsilon\\lambda_{k}}\\}\\cdot\\exp(-\\displaystyle\\frac{\\|x\\|^{4}}{\\varepsilon})\\exp\\{\\displaystyle\\frac{\\|r_{k}+x\\lambda_{k}\\|^{2}}{2\\varepsilon\\lambda_{k}}\\}=}\\\\ &{}&{\\displaystyle\\sum_{k=1}^{K}\\alpha_{k}\\exp\\big(\\displaystyle\\frac{\\widehat{a}+R^{4}}{\\varepsilon}\\big)\\exp\\{-\\displaystyle\\frac{\\|r_{k}\\|^{2}}{2\\varepsilon\\lambda_{k}}\\}\\cdot\\exp\\{\\displaystyle\\frac{\\|r_{k}+x\\lambda_{k}\\|^{2}-2\\lambda_{k}\\|x\\|^{4}}{2\\varepsilon\\lambda_{k}}\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "From this, we see that $\\exp\\left(\\widetilde{\\phi}(x)/\\varepsilon\\right)\\!c_{\\theta}(x)$ tends to zero while $x$ approaches infinity. It means that $x\\mapsto\\exp\\big(\\widetilde{\\phi}(x)/\\varepsilon\\big)c_{\\theta}(x)$ corresponds to the normalizable density. Using [52, Theorem 5a], we get that for all $\\delta^{\\prime\\prime}>0$ there exists an unnormalized Gaussian mixture $u_{\\omega}^{\\sim}$ such that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\|u_{\\widetilde{\\omega}}-\\exp(\\widetilde{\\phi}/\\varepsilon)c_{\\theta}\\|_{\\infty}=\\operatorname*{sup}_{x\\in\\mathbb{R}^{D}}|u_{\\widetilde{\\omega}}(x)-\\exp(\\widetilde{\\phi}(x)/\\varepsilon)c_{\\theta}(x))|<\\delta^{\\prime\\prime}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Analogously with $v_{\\widetilde{\\theta}}$ , we can set all the covariances in $u_{\\omega}^{\\sim}$ to be scalar, i.e., define $u_{\\omega}^{\\mathrm{~\\,~}}=$ $\\begin{array}{r}{\\sum_{l=1}^{L}\\widetilde{\\beta}_{l}\\mathcal{N}(x|\\widetilde{\\mu}_{l},\\varepsilon\\widetilde{\\zeta_{l}}\\bar{I}_{d})}\\end{array}$ for some $L$ , $\\widetilde{\\mu}_{l}\\,\\in\\,\\mathbb{R}^{d}$ , $\\widetilde{\\zeta}_{l}\\,\\in\\,\\mathbb{R}_{+}$ $(l\\in\\{1,...,L\\})$ ). Moreover, we consider ", "page_idx": 20}, {"type": "text", "text": "$\\begin{array}{r}{u_{\\omega}(x)=u_{\\widetilde{\\omega}}(x)\\exp\\{-\\frac{\\|x\\|^{2}}{2\\varepsilon}\\}}\\end{array}$ which is again an unnormalized density with scalar covariances. We denote the weights, means, covariances of this mixture by $\\beta_{l}\\in\\mathbb{R}_{+}$ , $\\mu_{l}\\in\\mathbb{R}^{d}$ , $\\zeta_{l}\\in\\mathbb{R}_{+}$ respectively. Next we recall the equation (36) and get that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\varepsilon\\int_{\\mathbb{R}^{d}}\\underbrace{\\exp\\{-\\frac{\\|x\\|^{2}}{2\\varepsilon}\\}u_{\\tilde{\\omega}}}_{=u_{\\omega}(x)}\\varepsilon-\\varepsilon\\delta^{\\prime\\prime}\\underbrace{\\int_{\\mathbb{R}^{d}}\\exp\\{-\\frac{\\|x\\|^{2}}{2\\varepsilon}\\}(u_{\\tilde{\\omega}}(x)-\\delta^{\\prime\\prime})d x}_{=\\delta^{\\prime\\prime}}-\\delta^{\\prime\\prime}2^{d/2}\\pi^{d}\\varepsilon^{(d+1)}\\exp\\{\\frac{\\widehat{a}+R^{2}}{\\varepsilon}\\}=\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Step 4. Now we turn to other expressions. Using the property that a function $t\\mapsto\\log t$ is $\\frac{1}{t_{\\operatorname*{min}}}$ -Lipshitz on interval $[t_{\\mathrm{min}},+\\infty)$ we get ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\log(\\exp\\{\\frac{\\widetilde{\\psi}(y)}{\\varepsilon}\\})-\\log(\\exp\\{\\frac{\\widetilde{\\psi}(y)}{\\varepsilon}\\}-\\delta^{\\prime\\prime})\\leq\\frac{\\delta^{\\prime\\prime}}{h_{\\operatorname*{min}}}\\Longrightarrow}\\\\ {\\log(\\exp\\{\\frac{\\widetilde{\\psi}(y)}{\\varepsilon}\\}-\\delta^{\\prime\\prime})\\geq\\frac{\\widetilde{\\psi}(y)}{\\varepsilon}-\\frac{\\delta^{\\prime\\prime}}{h_{\\operatorname*{min}}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Similarly, we get that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\log(\\exp\\{\\frac{\\widetilde{\\phi}(y)}{\\varepsilon}\\}c_{\\theta}(x)-\\delta^{\\prime\\prime})\\geq\\frac{\\widetilde{\\phi}(y)}{\\varepsilon}+\\log c_{\\theta}(x)-\\frac{\\delta^{\\prime\\prime}}{e_{\\operatorname*{min}}}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "We use this inequality, monotonicity of logarithm function, and (34), to derive ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\bar{\\gamma}(y)^{\\frac{(34)}{>}}\\exp\\{\\frac{\\widetilde\\psi(y)}{\\varepsilon}\\}-\\delta^{\\prime\\prime}\\Longrightarrow\\log v_{\\bar{\\theta}}(y)>\\log(\\exp\\{\\frac{\\widetilde\\psi(y)}{\\varepsilon}\\}-\\delta^{\\prime\\prime})\\overset{(39)}{\\geq}\\frac{\\widetilde\\psi(y)}{\\varepsilon}-\\frac{\\delta^{\\prime\\prime}}{h_{\\operatorname*{min}}}\\Longrightarrow}\\\\ {-\\widetilde\\psi(y)>-\\varepsilon\\log v_{\\bar{\\theta}}(y)-\\frac{\\varepsilon\\delta^{\\prime\\prime}}{h_{\\operatorname*{min}}};}\\\\ {u_{\\tilde{\\omega}}(x)^{\\frac{(37)}{>}}\\exp\\{\\frac{\\widetilde\\phi(x)}{\\varepsilon}\\}c_{\\theta}(x)-\\delta^{\\prime\\prime}\\Longrightarrow\\log u_{\\tilde{\\omega}}(x)>\\log(\\exp\\{\\frac{\\widetilde\\phi(x)}{\\varepsilon}\\}c_{\\theta}(x)-\\delta^{\\prime\\prime})\\overset{(40)}{\\geq}}\\\\ {\\frac{\\widetilde\\phi(y)}{\\varepsilon}+\\log c_{\\theta}(x)-\\frac{\\delta^{\\prime\\prime}}{e_{\\operatorname*{min}}}\\Longrightarrow-\\widetilde\\phi(y)>-\\varepsilon\\log\\frac{u_{\\tilde{\\omega}}(x)}{c_{\\theta}(x)}-\\frac{\\varepsilon\\delta^{\\prime\\prime}}{e_{\\operatorname*{min}}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Recall that $\\overline{{f_{1}}}$ and $\\overline{{f_{2}}}$ are non-decreasing functions. Moreover, they are Lipshitz with the constants $L_{1},L_{2}$ respectively. Thus, we get ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\overline{{f_{2}}}(-\\widetilde{\\psi}(y))>\\overline{{f_{2}}}(-\\varepsilon\\log v_{\\widetilde{\\theta}}(y)-\\frac{\\varepsilon\\delta^{\\prime\\prime}}{h_{\\operatorname*{min}}})=\\overline{{f_{2}}}(-\\varepsilon(\\log v_{\\theta}(y)+\\frac{\\|y\\|^{2}}{2\\varepsilon})-\\frac{\\varepsilon\\delta^{\\prime\\prime}}{h_{\\operatorname*{min}}})=}\\\\ {\\overline{{f_{2}}}(-\\varepsilon\\log v_{\\theta}(y)-\\frac{\\|y\\|^{2}}{2}-\\frac{\\varepsilon\\delta^{\\prime\\prime}}{h_{\\operatorname*{min}}})\\geq\\overline{{f_{2}}}(-\\varepsilon\\log v_{\\theta}(y)-\\frac{\\|y\\|^{2}}{2})-L_{2}\\cdot\\frac{\\varepsilon\\delta^{\\prime\\prime}}{h_{\\operatorname*{min}}};}\\\\ {\\overline{{f_{1}}}(-\\widetilde{\\phi}(y))>\\overline{{f_{1}}}(-\\varepsilon\\log\\frac{u_{\\widetilde{\\omega}}(x)}{c_{\\theta}(x)}-\\frac{\\varepsilon\\delta^{\\prime\\prime}}{e_{\\operatorname*{min}}})=\\overline{{f_{1}}}(-\\varepsilon\\log u_{\\widetilde{\\omega}}(x)+\\varepsilon\\log c_{\\theta}(x)-\\frac{\\varepsilon\\delta^{\\prime\\prime}}{e_{\\operatorname*{min}}})=}\\\\ {\\overline{{f_{1}}}(-\\varepsilon\\log u_{\\omega}(x)-\\frac{\\|x\\|^{2}}{2}+\\varepsilon\\log c_{\\theta}(x)-\\frac{\\varepsilon\\delta^{\\prime\\prime}}{e_{\\operatorname*{min}}})=\\overline{{f_{1}}}(-\\varepsilon\\log\\frac{u_{\\omega}(x)}{c_{\\theta}(x)}-\\frac{\\|x\\|^{2}}{2}-\\frac{\\varepsilon\\delta^{\\prime\\prime}}{e_{\\operatorname*{min}}})\\geq}\\\\ {\\overline{{f_{1}}}(-\\varepsilon\\log\\frac{u_{\\omega}(x)}{c_{\\theta}(x)}-\\frac{\\|x\\|^{2}}{2})-L_{1}\\cdot\\frac{\\varepsilon\\delta^{\\prime\\prime}}{e_{\\operatorname*{min}}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Integrating these inequalities over all $x$ and $y$ in supports of $p$ and $q$ respectively, we get ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\int_{\\mathbb{R}^{d}}\\overline{{f}}_{2}(-\\psi(y))q(y)d y\\geq\\int_{\\mathbb{R}^{d}}\\overline{{f_{2}}}(-\\varepsilon\\log v_{\\theta}(y)-\\frac{\\|y\\|^{2}}{2})q(y)d y-L_{2}\\cdot\\frac{\\varepsilon\\delta^{\\prime\\prime}}{h_{\\operatorname*{min}}};}\\\\ &{\\displaystyle\\int_{\\mathbb{R}^{d}}\\overline{{f}}_{1}(-\\phi(x))p(x)d x\\geq\\int_{\\mathbb{R}^{d}}\\overline{{f_{1}}}(-\\varepsilon\\log\\frac{u_{\\omega}(x)}{c_{\\theta}(x)}-\\frac{\\|x\\|^{2}}{2})p(x)d x-L_{1}\\cdot\\frac{\\varepsilon\\delta^{\\prime\\prime}}{e_{\\operatorname*{min}}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Finally, combining (38) and (46), we get ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}[\\mathfrak{a},\\omega]=\\mathcal{J}(\\mathfrak{a}_{\\omega,\\infty},\\mathfrak{w}_{0})=}\\\\ {\\mathcal{L}[\\mathfrak{a}_{\\omega}]_{1}+\\overline{{f_{1}}}(-\\varepsilon\\log\\frac{u_{\\omega}(x)}{c_{0}(x)}-\\frac{\\|x\\|^{2}}{2})\\mathfrak{p}(x)d x+\\frac{\\bar{\\lambda}}{\\hat{Z}_{2}}(-\\varepsilon\\log v_{0}(y)-\\frac{\\|y\\|^{2}}{2})\\eta(y)d y<}\\\\ {\\varepsilon\\int_{\\mathbb{R}^{d}}\\int_{\\mathbb{R}^{d}}\\exp\\Big(\\frac{1}{\\hat{\\omega}}(\\hat{\\omega})+\\bar{\\psi}(y)-\\frac{\\|x-y\\|^{2}}{2}\\Big)\\mathrm{d}x d y+}\\\\ {\\int_{\\mathbb{R}^{d}}\\int_{\\mathbb{R}^{d}}\\exp\\Big(\\frac{1}{\\hat{\\omega}}(\\hat{\\omega})\\Big)d x+\\int_{\\mathbb{R}^{d}}\\bar{f}_{2}(-\\hat{\\nu}(y))d y d y+}\\\\ {\\int_{\\mathbb{R}^{d}}\\int_{(1-\\hat{\\nu}(\\hat{\\omega}))\\backslash\\left(x+\\hat{\\nu}^{\\perp}(2y)\\right)}\\eta(x)d x+\\int_{\\mathbb{R}^{d}}\\bar{f}_{2}(-\\hat{\\nu}(y))\\eta(y)d y+}\\\\ {L_{1}\\cdot\\frac{\\varepsilon\\hat{\\nu}^{\\perp\\prime}}{e_{\\mathrm{min}}}+L_{2}\\cdot\\frac{\\varepsilon\\hat{\\nu}^{\\perp\\prime}}{\\hat{h}_{\\mathrm{min}}}+\\varepsilon\\delta^{\\mu}(2\\pi\\varepsilon)^{\\frac{3}{4}}\\Big(1+(\\pi\\varepsilon)^{4}\\exp\\left(\\frac{\\hat{\\alpha}+R^{2}}{\\varepsilon}\\right)\\Big)<}\\\\ {\\mathcal{L}^{*}+\\delta^{\\prime}+\\delta^{\\prime\\prime}\\left[L_{1}\\cdot\\frac{\\varepsilon}{e_{\\mathrm{min}}}+L_{2}\\cdot\\frac{\\varepsilon}{\\hat{h}_{\\mathrm{min}}}+\\varepsilon(2\\pi\\varepsilon)^{\\frac{4}{2}}\\Big(1+(\\pi\\varepsilon)^{\\frac{3}{4}}\\exp\\left(\\frac{\\hat{\\alpha}+R^{2}}{\\varepsilon}\\right)\\Big)\\right]\\leq}\\\\ {\\varepsilon^{*}+\\frac{\\delta^{2}}{2}+\\frac{\\delta^{2}}{2 \n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "which completes the proof. ", "page_idx": 22}, {"type": "text", "text": "Remark. In fact, the assumption of the Lishitzness of ${\\overline{{f_{1}}}}(x),{\\overline{{f_{2}}}}(x)$ can be omitted. Indeed, under the \"everything is compact\" assumptions of Proposition 4.2 and Theorem 4.3, inputs to $\\overline{{f_{1}}}(\\cdot)$ , $\\overline{{f_{2}}}(\\cdot)$ also always belong to certain compact sets. The convex functions are known to be Lipshitz on compact subsets of $\\mathbb{R}$ , see [31, Chapter 3, 18], and we actually do not need the Lipschitzness on the entire $\\mathbb{R}$ . ", "page_idx": 22}, {"type": "text", "text": "B Experiments Details ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "B.1 General Details ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "To minimize our objective (14), we parametrize $\\alpha_{k},r_{k},S_{k}$ of $v_{\\theta}$ and $\\beta_{l},\\mu_{l},\\Sigma_{l}$ of $u_{\\omega}$ in (11). Here we follow the standard practices in deep learning and parametrize logarithms $\\log\\alpha_{k},\\,\\log\\beta_{l}$ instead of directly parameterizing $\\alpha_{k},\\beta_{l}$ . In turn, variables $r_{k}$ , $\\mu_{l}$ are parametrized directly as multidimensional vectors. We consider diagonal matrices $S_{k}$ , $\\Sigma_{l}$ and parametrize them via their diagonal values $\\log(S_{k})_{i,i}$ and $\\log(\\Sigma_{l})_{i,i}$ respectively. We initialize the parameters following the scheme in [41]. In all our experiments, we use the Adam optimizer. ", "page_idx": 22}, {"type": "text", "text": "B.2 Details of the Experiment with Gaussian Mixtures ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We use $K=L=5$ , $\\varepsilon=0.05$ , $l r=3e-4$ and batchsize 128. We do $2\\cdot10^{4}$ gradient steps. For the LightSB algorithm, we use the parameters presented by the authors in the official repository. ", "page_idx": 22}, {"type": "text", "text": "B.3 Details of the Image Translation Experiment ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We use the code and decoder model from ", "page_idx": 22}, {"type": "text", "text": "We download the data and neural network extracted attributes for the FFHQ dataset from ", "page_idx": 22}, {"type": "text", "text": "In the Adult class we include the images with the attribute $A g e\\ge44$ ; in the Young class - with the $A g e\\in$ [16, 44]. We excluded the images with faces of children to increase the accuracy of classification per gender attribute. For the experiments with our solver, we use weighted $\\mathbf{D}_{\\mathrm{KL}}$ divergence with parameters $\\tau$ specified in Appendix C, and set $K=L=10$ , $\\varepsilon=0.05$ , $l r=1$ , and batch size to 128. We do $5\\cdot10^{3}$ gradient steps using Adam optimizer [35] and MultiStepLR scheduler with parameter $\\gamma=0.1$ and milestone $\\mathsf{s}=[500,1000]$ . For testing [41, LightSB] solver, we use the official code (see the link above) and instructions provided by the authors. ", "page_idx": 23}, {"type": "text", "text": "Baselines. For the OT-FM and UOT-FM methods, we parameterize the vector field $(v_{t,\\theta})_{t\\in[0,1]}$ for mass transport using a 2-layer feed-forward network with 512 hidden neurons and ReLU activation. An additional sinusoidal embedding[65] was applied for the parameter $t$ . The learning rate for the Adam optimizer was set to 1e-4. To obtain an optimal transport plan $\\pi^{*}(x,y)$ discrete OT solvers from the POT [21] package were used. These methods are built on the solutions (plans $\\pi^{*}(x,y)]$ ) of discrete OT problems, to obtain them we use the POT [21] package. Especially for the UOT-FM, we use the ot.unbalanced.sinkhorn with the regularization equal to 0.05. We set the number of training and inference time steps equal to 100. To obtain results of UOT-FM for Fig. 2, we run this method for 3K epochs with parameter $r e g\\_m\\in[5e-4,5e-3,5e-2,0.5,1,10,10^{2},10^{3},10^{4},10^{5},10^{6}]$ and reported the mean values of final metrics for 3 independent launches with different seeds. In Tables 20, 21, 22, for each translation we report the results for one chosen parameter specified in Appendix D.1. We use the corresponding checkpoints of UOT-FM to visualize its performance in Fig. 3. For [9, UOT-SD], [70, UOT-GAN] we use the official code provided by the authors, see the links: ", "page_idx": 23}, {"type": "text", "text": "https://github.com/Jae-Moo/Unbalanced-Optimal-Transport-Generative-Model ", "page_idx": 23}, {"type": "text", "text": "https://github.com/uhlerlab/unbalanced_ot ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "While both UOT-SB, UOT-GAN methods were not previously applied to the FFHQ dataset, we set up a grid search for the parameters and followed the instructions provided by the authors for parameter settings. Both for UOT-SB, UOT-GAN, we used a 3-layer neural network with 512 hidden neurons, and ReLU activation was used for the generator networks and the potential and discriminator, respectively. Adam optimizer [35] with $l r=\\bar{10}^{-5}$ and $l r=10^{-4}$ was used to train the networks in UOT-SB and UOT-GAN, respectively. We train the methods for 10K iterations and set a batch size to 128. For UOT-SD, we used $\\mathbf{D}_{\\mathrm{KL}}$ divergence with their unbalancedness parameter $\\tau\\!=\\!0.002$ . For other parameters, we used the default values provided by the authors for CIFAR-10 generation tasks. For all baseline models which use entropy regularization, we set $\\varepsilon=0.05$ . ", "page_idx": 23}, {"type": "text", "text": "C Additional Discussion & Experiments with $f$ -divergences ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Details about $f$ -divergences between positive measures. In the classic form, $f$ -divergences are defined as measures of dissimilarity between two probability measures. This definition should be revised when dealing with measures of arbitrary masses. In the paragraph below we show that if the function $f$ is convex, non-negative, and attains zero uniquely at point $\\{1\\}$ then $D_{f}\\left(\\mu_{1}\\middle|\\vert\\mu_{2}\\right)$ is a valid measure of dissimilarity between two positive measures. ", "page_idx": 23}, {"type": "text", "text": "Let $\\mu_{1},\\mu_{2}\\in\\mathcal{M}_{2,+}(\\mathbb{R}^{d^{\\prime}})$ be two positive measures. The $f$ -divergence satisfies $D_{f}\\left(\\mu_{1}\\|\\mu_{2}\\right)\\geq0$ which is obvious from the non-negativity of $f$ . From the definition of $)_{f}\\left(\\mu_{1}\\|\\mu_{2}\\right)$ and the fact that function $f$ attains zero uniquely at a point $\\{1\\}$ , we obtain that $D_{f}\\left(\\mu_{1}\\|\\mu_{2}\\right)=0$ if and only if $\\mu_{1}(x)=\\mu_{2}(\\dot{x})$ holds $\\mu_{2}$ -everywhere. Actually, $\\dot{\\mu}_{1}\\dot{(x)}=\\mu_{2}(x)$ should hold for all $x$ as $\\mu_{1}$ must be absolutely continuous w.r.t. $\\mu_{2}$ (otherwise $D_{f}\\left(\\mu_{1}\\|\\mu_{2}\\right)$ is assumed to be equal $+\\infty$ ). ", "page_idx": 23}, {"type": "text", "text": "The usage of $D_{\\chi^{2}}$ divergence. We tested the performance of our solver with scaled $\\mathbf{D}_{\\mathrm{KL}}$ divergences in the main text, see 5.1, 5.2. For completeness, here we evaluate our solver with $\\mathbf{D}_{\\chi^{2}}$ divergence in Gaussian Mixture experiment. We use the same experimental setup as in $\\S5.1$ and present the qualitative results in Fig. 4. Interestingly, the solver\u2019s results differ from those   which we obtain for $\\mathrm{D}_{\\mathrm{KL}}$ divergence. For $\\mathbf{D}_{\\chi^{2}}$ divergence, supports of learned plans\u2019 marginals constitute only parts of source and target measures\u2019 supports when $\\tau=1$ . The issue disappears with a slight increase of $\\tau$ , i.e., for $\\tau=2$ . At the same time, a further increase of $\\tau$ is useless, since the learned plans fail to deal with class imbalance issue. Thus, parameter $\\tau$ should be adjusted heuristically. In the case of $\\mathrm{D}_{\\mathrm{KL}}$ divergence, supports coincide for all $\\tau$ , see Fig. 1. This motivates us to use $\\mathrm{D}_{\\mathrm{KL}}$ divergences in our main experiments. ", "page_idx": 23}, {"type": "image", "img_path": "co8KZws1YK/tmp/f1fdd49bf23f161efa73b8a438ff73c2e919d60415b4d0916a7a74a758bc7a3e.jpg", "img_caption": ["Gaussians Mixture experiment $(\\tau\\in[1,2,5,10])$ . "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "Parameter $\\tau$ in Gaussian Mixture experiment. An ablation study on unbalancedness parameter $\\tau$ in Gaussian Mixture experiment is conducted in $\\S5.1$ and above in this section. For completeness, we also perform the quantitative assessment of o u r solver with $\\mathbf{D}_{\\mathrm{KL}}$ divergence for different unbalancedness parameters $\\tau$ . We compute the normalized OT cost $(\\mathbb{E}_{x\\sim p}\\mathbb{E}_{y\\sim\\gamma(y|x)}\\frac{(x-y)^{2}}{2})$ (x\u2212y)2) between the source and generated distributions, and the Wasserstein distance between the generated and target distributions (computed by a discrete OT solver). For completeness, we additionally calculate the metrics for the balanced [41, LightSB] approach. The results are presented in Table 4. ", "page_idx": 24}, {"type": "table", "img_path": "co8KZws1YK/tmp/e800b88d1292c36df711c6cfef4aa82aad1852e0096ba626b0dfbc3b8f57f4c0.jpg", "table_caption": [], "table_footnote": ["Table 4: Normalized OT cost between the input and learned distributions, and Wasserstein distance between the learned and target distributions in Gaussian mixture experiment. "], "page_idx": 24}, {"type": "text", "text": "Results. Recall that the unbalanced nature of our solver leads to two important properties. Firstly, our solver better preserves the properties of the input objects than the balanced approaches \u2212indeed, it allows for preserving object attributes (classes) even in the case of class imbalance. Secondly, due to the relaxed boundary condition for the target distribution, the distribution generated by our solver is naturally less similar to the target distribution than for balanced methods. ", "page_idx": 24}, {"type": "text", "text": "The above intuitive reasoning is confirmed by the metrics we obtained. Indeed, as the $\\tau$ parameter increases, when our method becomes more and more similar to balanced approaches, the normalized OT cost between the source and generated distributions increases, and the Wasserstein distance between learned and target distributions decreases. LightSB [1] baseline, which is a purely balanced approach, shows the best quality in terms of Wasserstein distance and the worst in terms of OT cost. ", "page_idx": 24}, {"type": "text", "text": "Parameters $\\tau,\\varepsilon$ in image experiments. The effect of entropy regularization parameter $\\varepsilon$ is well studied, see, e.g., [28, 41]. Namely, increasing the parameter $\\varepsilon$ stimulates the conditional distributions $\\gamma_{\\theta}(y|x)$ to become more dispersed. Still, below we provide an additional quantitative analysis of its influence on the learned translation. Besides, we address the question how does the parameter $\\tau$ influence the performance of our solver in image translation experiments? To address this question, we learn the translations $Y o u n g{\\rightarrow}\\,A d u l t$ , Man $\\rightarrow$ Woman on FFHQ dataset varying the parameters $\\tau$ , $\\varepsilon$ , see $\\mathbb{S}5.2$ for the experimental setup details. We test our solver with scaled $\\mathrm{D}_{\\mathrm{KL}}$ divergence training it for 5K iterations. Other hyperparameters are in Appendix B. In Tables 5, 8, 11, 14, we report the accuracy of keeping the attributes of the source images (e.g., gender in $Y o u n g{\\rightarrow}A d u l t$ translation). In Tables 6, 9, 12, 15, we report the accuracy of mapping to the correct target class (e.g., adult people in Young $\\leftrightarrow$ Adult translation). In Tables 7, 10, 13, 16, we report FD metrics which is defined as Frechet distance between means and covariances of the learned and the target measures. For convenience, we additionally illustrate the results of ablation studies on Fig. 5. ", "page_idx": 24}, {"type": "text", "text": "Results show that increase of $\\varepsilon$ negatively influences both accuracy of keeping the attributes of the source images and FD of generated latent codes which is caused by an increased dispersity of $\\gamma_{\\theta}(y|x)$ . Interestingly, accuracy of mapping to the correct target class does not have an evident dynamics w.r.t. $\\varepsilon$ . At the same time, when $\\tau$ increases, the learned plans provide worse accuracy for keeping the input latents\u2019 class but better $F D$ of generated latent codes and accuracy of mapping to the target class. It is an expected behavior since for bigger $\\tau$ , the constraints on the marginals of the learned plans become more strict. That is, we enforce the marginals of the learned plans to be closer to source and target measures which allows learning more accurate mappings to target measure but does not allow keeping the source classes in the case of imbalance issues. Interestingly, in $A d u l t{\\rightarrow}Y o u n g$ translation, FD of learned latents and accuracy of mapping to the target do not change much for $\\tau\\geq10^{2}$ while accuracy of keeping the attributes exhibits a significant drop between $\\bar{\\tau}=10^{2}$ and $\\tau=10^{3}$ . Thus, we can treat $\\tau=10^{2}$ as optimal since it provides the best trade-off between the quality of learned translations and their ability to keep the features of input latents. In the case of $Y o u n g{\\rightarrow}A d u l t$ translation, the values of accuracy and FD exhibit significant differences for considered $\\tau$ . Thus, we may consider a more detailed scale and choose $\\tau={\\bar{2}}.5\\cdot10^{2}$ as the optimal one. ", "page_idx": 24}, {"type": "table", "img_path": "co8KZws1YK/tmp/0e4a979e24ad8a3c24640a55839b33137fb34d2048c94d4f1921ba8f7942561b.jpg", "table_caption": [], "table_footnote": ["Table 5: Test accuracy (\u2191) of keeping the class in Young $\\rightarrow$ Adult translation. "], "page_idx": 25}, {"type": "table", "img_path": "co8KZws1YK/tmp/7c27ea352366a4bb70413d87eb2ef6d102a0919e27ec80bd724186630a09de60.jpg", "table_caption": [], "table_footnote": ["Table 6: Test accuracy (\u2191) of mapping to the target in Young $\\rightarrow$ Adult translation. "], "page_idx": 25}, {"type": "table", "img_path": "co8KZws1YK/tmp/8fee137575e88bbdf4caadb5ec1aed7042dfbc5abf7d027f4b484de75f204152.jpg", "table_caption": [], "table_footnote": ["Table 7: Test FD (\u2193) of generated latent codes in Young \u2192Adult translation. "], "page_idx": 25}, {"type": "table", "img_path": "co8KZws1YK/tmp/11b1bb1f06eecb3bfd7715e22e21a0b59042a07cb75c76cdfd282c04866129ff.jpg", "table_caption": [], "table_footnote": ["Table 8: Test accuracy (\u2191) of keeping the class in Adult $\\rightarrow$ Young translation. "], "page_idx": 25}, {"type": "table", "img_path": "co8KZws1YK/tmp/25b25b02a86f516d1ebe53eb97cbca6e89118f15ff800dae625d5df20775d241.jpg", "table_caption": [], "table_footnote": ["Table 9: Test accuracy (\u2191) of mapping to the target in Adult \u2192Young translation. "], "page_idx": 25}, {"type": "table", "img_path": "co8KZws1YK/tmp/821412e33f9332451bfb2e2f56371b5effdbacd78431c2d56af8cfbc85c29407.jpg", "table_caption": [], "table_footnote": ["Table 10: Test FD (\u2193) of generated latent codes in Adult \u2192Young translation. "], "page_idx": 25}, {"type": "table", "img_path": "co8KZws1YK/tmp/2500c30bee2a70a545b02e9a2b9bdf6315be832c7af6db9d230c4dca92d33ceb.jpg", "table_caption": [], "table_footnote": ["Table 11: Test accuracy (\u2191) of keeping the class in Man $\\rightarrow$ Woman translation. "], "page_idx": 25}, {"type": "text", "text": "", "page_idx": 25}, {"type": "table", "img_path": "co8KZws1YK/tmp/70aaeb782f136f63a5424b391e7cef1bcc923767be9e7b9ea997f319982f2d08.jpg", "table_caption": [], "table_footnote": ["Table 12: Test accuracy (\u2191) of mapping to the target in Man $\\rightarrow$ Woman translation. "], "page_idx": 26}, {"type": "table", "img_path": "co8KZws1YK/tmp/b833f293a14f7851c4130c268ff9b51673f4d62bb489410f7fcf31d550072626.jpg", "table_caption": [], "table_footnote": ["Table 13: Test FD (\u2193) of generated latent codes in Man $\\rightarrow$ Woman translation. "], "page_idx": 26}, {"type": "table", "img_path": "co8KZws1YK/tmp/f500b5995d39f1760ee63999fa85cf479fdeb202860abff6c4470d84ee848245.jpg", "table_caption": [], "table_footnote": ["Table 14: Test accuracy (\u2191) of keeping the class in Woman \u2192Man translation. "], "page_idx": 26}, {"type": "table", "img_path": "co8KZws1YK/tmp/8fd95651b1f0c7f35db70b00ea821b7889c6f17331149430bba18d43e752fb44.jpg", "table_caption": [], "table_footnote": ["Table 15: Test accuracy (\u2191) of mapping to the target in Woman \u2192Man translation. "], "page_idx": 26}, {"type": "table", "img_path": "co8KZws1YK/tmp/449977db1b58074ad86711e7734bd2172c4bf95b6f2bffb581a82b14c30dbedd.jpg", "table_caption": [], "table_footnote": ["Table 16: Test FD (\u2193) of generated latent codes in Woman \u2192Man translation. "], "page_idx": 26}, {"type": "image", "img_path": "co8KZws1YK/tmp/78880570d1bbbdcc9fb7fbfcf163e51200cc368f696816b9a799bf61ee6b9292.jpg", "img_caption": ["Figure 5: Visualization of ablation studies on parameters $\\tau$ , $\\varepsilon$ in image translation experiment. "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "It is important to note that our method offers a flexible way to select a domain translation configuration that allows for better preserving the properties of the original objects or generating a distribution ", "page_idx": 26}, {"type": "text", "text": "closer to the target one. The final optimal configuration selection remains at the discretion of the user.   \nThe highlighted values in the Tables are used for comparison with other approaches in $\\mathrm{\\SD.1}$ . ", "page_idx": 27}, {"type": "text", "text": "Remark. FD should be treated as a relative measure of similarity between learned and target measures. The results obtained by balanced solver [41, LightSB] (equivalent to ours for big $\\tau$ ) are considered as a gold standard. ", "page_idx": 27}, {"type": "text", "text": "Number of Gaussian components in potentials. For completeness, we perform an ablation study of our U-LightOT solver with different number on Gaussian components $(K,L)$ in potentials $v_{\\theta}$ and $u_{\\omega}$ , respectively. We run the solver in $Y o u n g{\\rightarrow}A d u l t$ translation with 5K steps, $\\varepsilon=0.05$ and set $\\tau=250$ . The quantitative results (accuracy of keeping the class, accuracy of mapping to the target, FD of generated latents vs target latents) are presented in the Tables below. ", "page_idx": 27}, {"type": "table", "img_path": "co8KZws1YK/tmp/6d4a5c0ee3c38a1bb3f7168306c2fd201de67bd779308cd940882f5885cbd757.jpg", "table_caption": [], "table_footnote": [], "page_idx": 27}, {"type": "table", "img_path": "co8KZws1YK/tmp/b39a3f882961451d5bd1af24070a6438f9da924dccd4259700a8569e647e80f3.jpg", "table_caption": ["Table 17: Test accuracy (\u2191) of keeping the attributes in Young $\\rightarrow$ Adult translation for our U-LightOT solver with different number of Gaussian components in potentials. "], "table_footnote": [], "page_idx": 27}, {"type": "table", "img_path": "co8KZws1YK/tmp/16aace1afec2fcaa018cc834e3c0239dde11108c8f32adf8168e17075f3385c5.jpg", "table_caption": ["Table 18: Test accuracy (\u2191) of mapping to the target in Young\u2192Adult translation for our U-LightOT solver with different number of Gaussian components in potentials. "], "table_footnote": ["Table 19: Test FD (\u2193) of generated latent codes in Young $\\rightarrow$ Adult translation for our U-LightOT solver with different number of Gaussian components in potentials. "], "page_idx": 27}, {"type": "text", "text": "The results show that in the considered task, our solver provides good performance even for small number of Gaussian components. This can be explained by the smoothness of the latent representations of data in ALAE autoencoder. ", "page_idx": 27}, {"type": "text", "text": "D Additional Experimental Results ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "D.1 Quantitative comparison with other methods in Image-to-Image translation experiment ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "In this section, we provide additional results of quantitative comparison of our U-LightOT solver and other unbalanced and balanced OT/EOT solvers in image translation experiment. Tables 20, 21, 22, provide values used for plotting Fig. 2 in the main text. The unbalancedness parameters used for our U-LightOT solver and [16, UOT-FM] are specified in th Tables below. For obtaining the result of [9, UOT-SD], we use their unbalancedness parameter $\\tau=0.002$ . For other details on methods\u2019 parameters used to obtain the results below, see Appendix B. Note that we do not include FID metric for assessing the quality of generated images since we found that it is not a representative metric for assessing the performance of models performing the translation of ALAE latent codes. ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}, {"type": "table", "img_path": "co8KZws1YK/tmp/5adcbb2b6185190c401e50c37edf9b6d55e1653a35774c2de58d7b91f58220f3.jpg", "table_caption": [], "table_footnote": [], "page_idx": 28}, {"type": "table", "img_path": "co8KZws1YK/tmp/8ef464d138d9a303f4b3696ab6133b3702d16ab1b213636729e48a2a49213eed.jpg", "table_caption": ["Table 20: Comparison of accuracies of keeping the attributes of the source images. "], "table_footnote": [], "page_idx": 28}, {"type": "table", "img_path": "co8KZws1YK/tmp/b4012c2f95a7011d754ab2682d4470cb7d1f02308f4a48b00c71f062ce2eeb6a.jpg", "table_caption": ["Table 21: Comparison of accuracies of mapping to the target. ", "Table 22: Comparison of FD between the generated and learned latents. "], "table_footnote": [], "page_idx": 28}, {"type": "text", "text": "D.2 Outlier robustness property of U-LightOT solver ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "To show the outlier robustness property of our solver, we conduct the experiment on Gaussian Mixtures with added outliers and visualize the results in Fig. 6. The setup of the experiment, in general, follows the Gaussian mixtures experiment setup described in section 5.2 of our paper. The difference consists in outliers (small gaussians) added to the input and output measures. ", "page_idx": 28}, {"type": "image", "img_path": "co8KZws1YK/tmp/dc4295d68d1844244bab1b6e6308d19c22c2dd759718a3a26b929f87b33eba38.jpg", "img_caption": ["Figure 6: Conditional plans $\\gamma_{\\theta,\\omega}(y|x)$ learned by our solver $\\left[\\tau=1\\right]$ ) and LightSB in Gaussian Mixtures with otliers experiment. "], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "The results show that our U-LightOT solver successfully eliminates the outliers and manages to simultaneously handle the class imbalance issue. At the same time, the balanced LightSB [41] solver fails to deal with either of these problems. ", "page_idx": 28}, {"type": "text", "text": "E Limitations and Broader Impact ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Limitations. One limitation of our solver is the usage of the Gaussian Mixture parametrization which might restrict the scalability of our solver. This points to the necessity for developing ways to optimize objective (4) with more general parametrization, e.g., neural networks. ", "page_idx": 28}, {"type": "text", "text": "Broader impact. Our work aims to advance the field of Machine Learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here. ", "page_idx": 28}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes]   \nJustification: For each contribution listed in abstract and introduction we provide the links to the sections about them. ", "page_idx": 29}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes]   \nJustification: We discuss the limitations of our solver in 6. ", "page_idx": 29}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?   \nAnswer: [Yes]   \nJustification: We provide the proofs and the assumptions in Appendix A. ", "page_idx": 29}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?   \nAnswer: [Yes]   \nJustification: We provide a full list of the experimental details in Appendix B. We use publicly available datasets. The code for the experiments is provided in supplementary material. ", "page_idx": 29}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: The code for our solver is included in supplementary material and will be made public after acceptance of our paper. We use publicly available datasets. ", "page_idx": 29}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?   \nAnswer: [Yes]   \nJustification: We provide the experimental details in Appendix B. ", "page_idx": 29}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 29}, {"type": "text", "text": "Answer: [No] ", "page_idx": 29}, {"type": "text", "text": "Justification: The focus of the papers is mostly theoretical. The experiments are provided just for illustration of the derived theoretical results. ", "page_idx": 29}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] Justification: The computational resources are discussed in 5. ", "page_idx": 29}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes]   \nJustification: The research conforms with NeurIPS Code of Ethics. We discuss the societal impact of our paper in $\\S6$ . ", "page_idx": 29}, {"type": "text", "text": "10. Broader Impacts ", "page_idx": 30}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?   \nAnswer: [Yes]   \nJustification: We discuss the societal impact of our paper in 6. ", "page_idx": 30}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?   \nAnswer: [NA]   \nJustification: The research does not release data or models that have a high risk for misuse and does not need safeguards. ", "page_idx": 30}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?   \nAnswer: [Yes]   \nJustification: We cited the creators all of the used assets. ", "page_idx": 30}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes]   \nJustification: The code is included in supplementary material. The license for the code will be provided after the paper acceptance. ", "page_idx": 30}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)?   \nAnswer: [NA]   \nJustification: The paper does not include crowdsourcing experiments or research with human subjects. ", "page_idx": 30}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 30}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA]   \nJustification: The paper does not include crowdsourcing experiments or research with human subjects. ", "page_idx": 30}]