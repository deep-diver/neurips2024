[{"figure_path": "s2hA6Bz3LE/figures/figures_1_1.jpg", "caption": "Figure 1: Overview of feature repulsive loss construction: Starting with a batch of examples (left), optionally including synthetic outliers, ensemble features at each layer l are used to construct centered Gram matrices projected onto the unit hypersphere (middle). The hyperspherical energy is then calculated between models, weighted by layer, and incorporated into the loss function (right).", "description": "This figure illustrates the process of constructing a feature repulsive loss for enhancing diversity in Bayesian deep learning. It starts with a batch of input examples, optionally including synthetic outliers.  Ensemble features from each layer are used to create centered Gram matrices. These matrices are then projected onto a unit hypersphere, and hyperspherical energy is calculated between the models. Finally, the layer-weighted hyperspherical energy is incorporated into the loss function to encourage diversity among models.", "section": "1 Introduction"}, {"figure_path": "s2hA6Bz3LE/figures/figures_4_1.jpg", "caption": "Figure 2: Comparison between optimizing cosine similarity (cossim) or HE on a sphere. (a) initial random set of points placed on sphere. (b-c) the final set of points after 50 iterations either cossim or HE as the similarity metric. (d-e) the value of cossim/HE with respect to the number of iterations. The orange line indicates that cossim is minimized and the black line indicates that HE with s = 2 is minimized. Both methods used gradient descent with a learning rate of 0.75 and momentum 0.9.", "description": "This figure compares the results of minimizing cosine similarity and hyperspherical energy (HE) on a unit hypersphere.  Panel (a) shows the initial random distribution of points on the sphere. Panels (b) and (c) display the final point distributions after 50 iterations of optimization using cosine similarity and HE, respectively.  The visualizations highlight how HE promotes a more uniform distribution of points, unlike cosine similarity which leads to clustering. Panels (d) and (e) plot the values of cosine similarity and HE, respectively, against the number of iterations during the optimization process, demonstrating HE's faster convergence and achievement of uniformity.", "section": "3 Measurements of Network Diversity"}, {"figure_path": "s2hA6Bz3LE/figures/figures_5_1.jpg", "caption": "Figure 3: Predictive entropies (PE) on a four-cluster 2D classification task. Darker values indicate higher entropy, lower confidence regions, and lighter values indicate higher confidence regions. (b) and (d) use an RBF kernel on ensemble member weights, whereas (c) and (e) use an RBF kernel on ensemble member outputs. (f) and (g) use the HE-CKA, RBF feature kernel, for feature diversity on inlier points. Both (h) and (j) use HE-CKA and OOD entropy terms. All methods were trained on an ensemble of 30 four layer MLPs for 1k iterations with the same seeds.", "description": "This figure compares the predictive entropy on a 2D four-class classification task using different methods: ensembles, SVGD with RBF kernel, and HE-CKA with and without OOD terms. Each subfigure shows a heatmap representing the predictive entropy, with darker colors indicating higher entropy (lower confidence) and lighter colors indicating lower entropy (higher confidence).", "section": "Experiments"}, {"figure_path": "s2hA6Bz3LE/figures/figures_6_1.jpg", "caption": "Figure 4: Hypernetwork h(z) model architecture example on a four layer CNN", "description": "This figure illustrates the architecture of a hypernetwork used to generate the weights of a four-layer convolutional neural network (CNN). A latent vector z, sampled from a normal distribution N(0,I), is passed through the hypernetwork h(z). The output of the hypernetwork, denoted by c, is then used as input to four separate layer generators, g1(c), g2(c), g3(c), and g4(c). Each layer generator produces the weights for its corresponding layer in the CNN. The weights generated by the hypernetwork are used to make predictions on input data X. The features from each layer are then used to construct kernel matrices, which are in turn used to calculate the hyperspherical energy (HE).", "section": "4.1 Diverse Generative Ensemble with Hypernetworks"}, {"figure_path": "s2hA6Bz3LE/figures/figures_6_2.jpg", "caption": "Figure 5: 1D regression task comparing uncertainty estimation between different approaches", "description": "The figure compares the uncertainty estimation of four different methods on a 1D regression task: (a) Ensemble, (b) Ensemble + HE-CKA, (c) Hypernetwork, and (d) Hypernetwork + HE-CKA. The x-axis represents the input values, and the y-axis represents the output values. The red line represents the true function, the blue shaded area represents the posterior mean, and the red plus signs represent the training data.  The figure shows that adding HE-CKA to both ensembles and hypernetworks improves uncertainty estimation, especially in regions with low data density.", "section": "5.1 Synthetic Data"}, {"figure_path": "s2hA6Bz3LE/figures/figures_7_1.jpg", "caption": "Figure 6: Predictive softmax entropy between MNIST, Dirty-MNIST (with aleatoric uncertainty), and OOD Fashion-MNIST. Utilizing an ensemble of 5 LeNets. It can be seen that HE-CKA and OOD HE-CKA better separates the inlier Dirty-MNIST from outlier Fashion-MNIST.", "description": "This figure shows the predictive softmax entropy for three different datasets: MNIST, Dirty-MNIST (MNIST with added noise representing aleatoric uncertainty), and Fashion-MNIST (out-of-distribution data).  The results are shown for four different methods: a standard ensemble, SVGD with an RBF kernel, SVGD with HE-CKA, and an ensemble with HE-CKA and an OOD term.  The HE-CKA methods, especially when combined with the OOD term, show significantly better separation between the in-distribution (Dirty-MNIST) and out-of-distribution (Fashion-MNIST) data. This illustrates the improved uncertainty quantification achieved by incorporating HE-CKA and the OOD term.", "section": "5 Experiments"}, {"figure_path": "s2hA6Bz3LE/figures/figures_16_1.jpg", "caption": "Figure 7: Effect of smoothing term when using a cosine similarity based HE-CKAsmooth kernel with SVGD on inlier points only. All methods were trained with AdamW (lr=0.05, wd=0.0075), HE-CKAsmooth s = 2, and Edist = 0.00025, and w = [0.2, 0.35, 0.85, 0.05] for 1k steps.", "description": "This figure shows the effect of the smoothing term (\\(\\epsilon_{arc}\\)) on the HE-CKA kernel's performance when used with SVGD.  As \\(\\epsilon_{arc}\\) increases from 0.010 to 0.030, the resulting distribution of points on the hypersphere changes. Smaller values lead to more concentrated clusters, while larger values encourage a more uniform distribution.  The smoothing term helps to manage the gradients during optimization, preventing the model from getting stuck in local optima. The plot shows that with increasing values of \\(\\epsilon_{arc}\\), the distribution of points becomes more uniform, suggesting that the smoothing term is effective in improving the performance of the algorithm.", "section": "C Training Details"}, {"figure_path": "s2hA6Bz3LE/figures/figures_17_1.jpg", "caption": "Figure 3: Predictive entropies (PE) on a four-cluster 2D classification task. Darker values indicate higher entropy, lower confidence regions, and lighter values indicate higher confidence regions. (b) and (d) use an RBF kernel on ensemble member weights, whereas (c) and (e) use an RBF kernel on ensemble member outputs. (f) and (g) use the HE-CKA, RBF feature kernel, for feature diversity on inlier points. Both (h) and (j) use HE-CKA and OOD entropy terms. All methods were trained on an ensemble of 30 four layer MLPs for 1k iterations with the same seeds.", "description": "This figure compares the predictive entropy of various Bayesian deep learning methods on a 2D four-class classification task. Each subplot shows a heatmap representing the predictive entropy, with darker colors indicating higher entropy (lower confidence) and lighter colors indicating lower entropy (higher confidence). Different methods are compared, including those using RBF kernels on weights or outputs, and methods incorporating HE-CKA and OOD entropy terms. The figure demonstrates that incorporating diversity in the ensemble significantly improves uncertainty estimation. ", "section": "Experiments"}, {"figure_path": "s2hA6Bz3LE/figures/figures_19_1.jpg", "caption": "Figure 3: Predictive entropies (PE) on a four-cluster 2D classification task. Darker values indicate higher entropy, lower confidence regions, and lighter values indicate higher confidence regions. (b) and (d) use an RBF kernel on ensemble member weights, whereas (c) and (e) use an RBF kernel on ensemble member outputs. (f) and (g) use the HE-CKA, RBF feature kernel, for feature diversity on inlier points. Both (h) and (j) use HE-CKA and OOD entropy terms. All methods were trained on an ensemble of 30 four layer MLPs for 1k iterations with the same seeds.", "description": "This figure compares the predictive entropy from different methods on a synthetic 2D four-class classification task. It shows how different methods generate different levels of uncertainty in different regions. The results highlight the effectiveness of HE-CKA in improving uncertainty quantification compared to baselines.", "section": "Experiments"}, {"figure_path": "s2hA6Bz3LE/figures/figures_20_1.jpg", "caption": "Figure 10: CIFAR generated OOD set.", "description": "This figure shows a 5x5 grid of images that are generated as out-of-distribution (OOD) examples for the CIFAR dataset. These images are synthetically generated using various transformations and augmentations to make them different from the in-distribution images. The purpose of using these synthetic OOD examples is to enhance the diversity of features in the ensemble of neural networks.", "section": "4.2 Synthetic OOD Feature Diversity"}, {"figure_path": "s2hA6Bz3LE/figures/figures_21_1.jpg", "caption": "Figure 1: Overview of feature repulsive loss construction: Starting with a batch of examples (left), optionally including synthetic outliers, ensemble features at each layer l are used to construct centered Gram matrices projected onto the unit hypersphere (middle). The hyperspherical energy is then calculated between models, weighted by layer, and incorporated into the loss function (right).", "description": "This figure illustrates the process of constructing a feature repulsive loss for enhancing diversity in deep learning ensembles.  It shows how ensemble features from each layer are used to compute centered Gram matrices, which are then projected onto a unit hypersphere.  The hyperspherical energy (HE) between the models on the hypersphere is then computed and incorporated into the loss function, along with inlier and outlier loss terms.  Synthetic outliers are optionally included to help push models apart in feature space.", "section": "1 Introduction"}, {"figure_path": "s2hA6Bz3LE/figures/figures_22_1.jpg", "caption": "Figure 3: Predictive entropies (PE) on a four-cluster 2D classification task. Darker values indicate higher entropy, lower confidence regions, and lighter values indicate higher confidence regions. (b) and (d) use an RBF kernel on ensemble member weights, whereas (c) and (e) use an RBF kernel on ensemble member outputs. (f) and (g) use the HE-CKA, RBF feature kernel, for feature diversity on inlier points. Both (h) and (j) use HE-CKA and OOD entropy terms. All methods were trained on an ensemble of 30 four layer MLPs for 1k iterations with the same seeds.", "description": "This figure visualizes predictive entropies on a 2D four-cluster classification task using different methods. Darker colors represent higher entropy (lower confidence), while lighter colors indicate lower entropy (higher confidence). It compares various methods such as ensembles, SVGD with RBF kernels on weights or outputs, and the proposed HE-CKA method, with and without out-of-distribution (OOD) data and entropy terms. The results show that HE-CKA with OOD terms can better estimate uncertainty and identify confidence regions effectively.", "section": "3 Measurements of Network Diversity"}, {"figure_path": "s2hA6Bz3LE/figures/figures_22_2.jpg", "caption": "Figure 3: Predictive entropies (PE) on a four-cluster 2D classification task. Darker values indicate higher entropy, lower confidence regions, and lighter values indicate higher confidence regions. (b) and (d) use an RBF kernel on ensemble member weights, whereas (c) and (e) use an RBF kernel on ensemble member outputs. (f) and (g) use the HE-CKA, RBF feature kernel, for feature diversity on inlier points. Both (h) and (j) use HE-CKA and OOD entropy terms. All methods were trained on an ensemble of 30 four layer MLPs for 1k iterations with the same seeds.", "description": "This figure compares predictive entropy for different methods on a 2D four-class classification task. It shows that HE-CKA significantly improves uncertainty estimation, especially when combined with out-of-distribution (OOD) entropy terms. The results demonstrate improved uncertainty quantification in low-density regions of the feature space.", "section": "Experiments"}, {"figure_path": "s2hA6Bz3LE/figures/figures_23_1.jpg", "caption": "Figure 3: Predictive entropies (PE) on a four-cluster 2D classification task. Darker values indicate higher entropy, lower confidence regions, and lighter values indicate higher confidence regions. (b) and (d) use an RBF kernel on ensemble member weights, whereas (c) and (e) use an RBF kernel on ensemble member outputs. (f) and (g) use the HE-CKA, RBF feature kernel, for feature diversity on inlier points. Both (h) and (j) use HE-CKA and OOD entropy terms. All methods were trained on an ensemble of 30 four layer MLPs for 1k iterations with the same seeds.", "description": "This figure compares predictive entropy visualizations for different methods on a 2D four-cluster classification task.  It shows how different methods (ensemble, SVGD with different kernels, HE-CKA with and without OOD) handle uncertainty estimation by visualizing the entropy in the feature space. Darker colors represent higher entropy and lower confidence, while lighter colors show lower entropy and higher confidence. The results highlight the superior uncertainty quantification achieved by the proposed HE-CKA approach, particularly when incorporating out-of-distribution (OOD) data.", "section": "Experiments"}, {"figure_path": "s2hA6Bz3LE/figures/figures_23_2.jpg", "caption": "Figure 3: Predictive entropies (PE) on a four-cluster 2D classification task. Darker values indicate higher entropy, lower confidence regions, and lighter values indicate higher confidence regions. (b) and (d) use an RBF kernel on ensemble member weights, whereas (c) and (e) use an RBF kernel on ensemble member outputs. (f) and (g) use the HE-CKA, RBF feature kernel, for feature diversity on inlier points. Both (h) and (j) use HE-CKA and OOD entropy terms. All methods were trained on an ensemble of 30 four layer MLPs for 1k iterations with the same seeds.", "description": "The figure shows the predictive entropies of different methods on a four-cluster 2D classification task.  Darker colors represent higher entropy (less confidence), while lighter colors represent lower entropy (higher confidence).  The figure compares several methods, highlighting the impact of different kernels (RBF vs. HE-CKA) and the inclusion of out-of-distribution (OOD) data and entropy terms.  The results demonstrate the improved uncertainty estimation by the proposed method (HE-CKA) across diverse model ensembles.", "section": "Experiments"}, {"figure_path": "s2hA6Bz3LE/figures/figures_24_1.jpg", "caption": "Figure 3: Predictive entropies (PE) on a four-cluster 2D classification task. Darker values indicate higher entropy, lower confidence regions, and lighter values indicate higher confidence regions. (b) and (d) use an RBF kernel on ensemble member weights, whereas (c) and (e) use an RBF kernel on ensemble member outputs. (f) and (g) use the HE-CKA, RBF feature kernel, for feature diversity on inlier points. Both (h) and (j) use HE-CKA and OOD entropy terms. All methods were trained on an ensemble of 30 four layer MLPs for 1k iterations with the same seeds.", "description": "This figure compares the predictive entropy results of different methods on a 2D four-cluster classification task.  It illustrates how different methods (ensemble, SVGD with RBF kernel, SVGD with HE-CKA kernel, etc.) produce different uncertainty estimations, visualized as heatmaps. Darker colors represent higher uncertainty (lower confidence), and lighter colors show lower uncertainty (higher confidence). The impact of using an RBF kernel on weights vs. outputs, and the effect of adding HE-CKA and out-of-distribution (OOD) terms are shown.", "section": "Experiments"}, {"figure_path": "s2hA6Bz3LE/figures/figures_24_2.jpg", "caption": "Figure 3: Predictive entropies (PE) on a four-cluster 2D classification task. Darker values indicate higher entropy, lower confidence regions, and lighter values indicate higher confidence regions. (b) and (d) use an RBF kernel on ensemble member weights, whereas (c) and (e) use an RBF kernel on ensemble member outputs. (f) and (g) use the HE-CKA, RBF feature kernel, for feature diversity on inlier points. Both (h) and (j) use HE-CKA and OOD entropy terms. All methods were trained on an ensemble of 30 four layer MLPs for 1k iterations with the same seeds.", "description": "This figure compares the predictive entropy of different methods on a 2D four-cluster classification task.  Each subfigure shows the predictive entropy for a different approach: ensemble methods, SVGD with RBF kernel (on weights and outputs), KDE with RBF kernel, and finally, the proposed method HE-CKA with and without OOD examples. Darker colors mean higher entropy (lower confidence), illustrating the effectiveness of HE-CKA in uncertainty quantification.", "section": "Experiments"}, {"figure_path": "s2hA6Bz3LE/figures/figures_25_1.jpg", "caption": "Figure 3: Predictive entropies (PE) on a four-cluster 2D classification task. Darker values indicate higher entropy, lower confidence regions, and lighter values indicate higher confidence regions. (b) and (d) use an RBF kernel on ensemble member weights, whereas (c) and (e) use an RBF kernel on ensemble member outputs. (f) and (g) use the HE-CKA, RBF feature kernel, for feature diversity on inlier points. Both (h) and (j) use HE-CKA and OOD entropy terms. All methods were trained on an ensemble of 30 four layer MLPs for 1k iterations with the same seeds.", "description": "This figure compares different methods' uncertainty estimation performance on a 2D four-cluster classification task.  The color intensity represents predictive entropy, with darker shades indicating higher entropy (uncertainty) and lighter shades indicating lower entropy (certainty). The results show that using HE-CKA, along with OOD entropy terms, improves the accuracy of the uncertainty estimations. Different kernels (RBF on weights vs. outputs) and the inclusion of out-of-distribution (OOD) data are also considered.", "section": "Experiments"}, {"figure_path": "s2hA6Bz3LE/figures/figures_25_2.jpg", "caption": "Figure 3: Predictive entropies (PE) on a four-cluster 2D classification task. Darker values indicate higher entropy, lower confidence regions, and lighter values indicate higher confidence regions. (b) and (d) use an RBF kernel on ensemble member weights, whereas (c) and (e) use an RBF kernel on ensemble member outputs. (f) and (g) use the HE-CKA, RBF feature kernel, for feature diversity on inlier points. Both (h) and (j) use HE-CKA and OOD entropy terms. All methods were trained on an ensemble of 30 four layer MLPs for 1k iterations with the same seeds.", "description": "This figure compares predictive entropy of different methods on a 2D four-cluster classification task.  The darker the color, the higher the entropy (uncertainty), and lighter colors indicate higher confidence.  It shows the effect of using different kernels (RBF, HE-CKA) and incorporating out-of-distribution (OOD) examples on uncertainty estimation. The results illustrate the improvement in uncertainty quantification provided by the proposed HE-CKA method, especially when combined with OOD examples.", "section": "5.1 Synthetic Data"}]