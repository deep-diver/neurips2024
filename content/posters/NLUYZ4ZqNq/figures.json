[{"figure_path": "NLUYZ4ZqNq/figures/figures_4_1.jpg", "caption": "Figure 1: Domain adaptation method model for turning a Mixtral to a SaulLM-141B. Training involves different stages: legal domain pretraining, instruction filtering, and preference filtering.", "description": "This figure illustrates the domain adaptation process used to create the SaulLM-141B model.  It starts with a Mixtral-base 141B model, which undergoes three main stages of adaptation.  First, legal-focused continued pretraining expands the model's knowledge of legal concepts and language using a large corpus of legal data supplemented by general-purpose data (Web, Math, Replay). Second, legal-enhanced instruction fine-tuning refines the model's ability to follow instructions related to legal tasks, using a combination of legal and generic instructions.  Finally, legal-enhanced preference fine-tuning aligns the model's outputs with human preferences for legal interpretations through preference filtering.  The figure shows the progression from the initial Mixtral model, through the intermediate SaulLM-base and SaulLM-IFT stages, to the final SaulLM-Large model, highlighting the various data sources and adaptation techniques used in each phase.", "section": "4 Implementation Details & Evaluation Protocol"}, {"figure_path": "NLUYZ4ZqNq/figures/figures_5_1.jpg", "caption": "Figure 2: Overall Results. Comparison of SaulLM-large and SaulLM-medium with existing models.", "description": "This figure presents a bar chart comparing the mean balanced accuracy achieved by various large language models (LLMs) on the LegalBench-Instruct benchmark. The LLMs compared include Mixtral-54B, GPT3.5, Mixtral-140B, Llama3, Saul-medium, GPT4, and Saul-large.  Saul-large and Saul-medium are the models introduced in this paper.  The chart visually demonstrates the performance improvement of SaulLM models over existing LLMs in legal reasoning tasks.", "section": "5.1 Global Results"}, {"figure_path": "NLUYZ4ZqNq/figures/figures_6_1.jpg", "caption": "Figure 2: Overall Results. Comparison of SaulLM-large and SaulLM-medium with existing models.", "description": "This figure presents the overall performance comparison of SaulLM-large and SaulLM-medium against other existing models such as Mixtral-54B, GPT3.5, Mixtral-140B, Llama3, GPT4 and Saul-medium on the mean balanced accuracy metric.  The bar chart visually represents the performance of each model, allowing for easy comparison of their effectiveness in legal reasoning tasks. The results show that SaulLM-medium outperforms Mixtral-54B, and SaulLM-large outperforms Mixtral-141B.  Interestingly, the smaller SaulLM models outperform larger models like GPT-4 and Llama3-70B. This highlights the effectiveness of the domain adaptation strategy employed.", "section": "5.1 Global Results"}, {"figure_path": "NLUYZ4ZqNq/figures/figures_6_2.jpg", "caption": "Figure 2: Overall Results. Comparison of SaulLM-large and SaulLM-medium with existing models.", "description": "This figure displays a bar chart comparing the mean balanced accuracy scores achieved by various large language models (LLMs) on LegalBench-Instruct.  The models compared include SaulLM-large, SaulLM-medium,  Mixtral-54B, Mixtral-141B, GPT-3.5, Llama-3, and GPT-4.  The chart visually demonstrates the relative performance of each model, showcasing the improved accuracy of SaulLM-large and SaulLM-medium compared to other models. This comparison highlights the effectiveness of the domain adaptation strategy used in developing SaulLM.", "section": "5.1 Global Results"}, {"figure_path": "NLUYZ4ZqNq/figures/figures_7_1.jpg", "caption": "Figure 5: Continue Pretraining Analysis. Training loss for SaulLM-141B-base and SaulLM-54B-base over normalized epochs.", "description": "This figure displays the training loss curves for both SaulLM-141B-base and SaulLM-54B-base models plotted against normalized epochs.  The curves show a consistent downward trend indicating that continued pretraining could potentially yield further performance improvements. Both raw and smoothed loss curves are presented.", "section": "5 Experimental Results"}, {"figure_path": "NLUYZ4ZqNq/figures/figures_7_2.jpg", "caption": "Figure 6: Energy Consumption Analysis. Mean Power per Node for training jobs on the ADASTRA Supercomputer.", "description": "This figure shows the mean power consumption per node for each training job on the ADASTRA supercomputer. The weighted mean power per node is 1361.97W. The data points are scattered around the mean, showing the variability in power consumption for each job. The x-axis represents the job ID, and the y-axis represents the mean power per node in Watts (W).", "section": "5 Experimental Results"}, {"figure_path": "NLUYZ4ZqNq/figures/figures_21_1.jpg", "caption": "Figure 7: Energy Analysis. GPU Load vs Elapsed Time for Different Numbers of Nodes.", "description": "This figure displays the relationship between GPU load and elapsed time during the training process, categorized by the number of nodes used.  It visually represents the computational efficiency and resource utilization at different scales of parallelization. Higher GPU load generally indicates more intensive computation, while elapsed time shows the duration of the training process. The variation in both GPU load and time likely reflects differences in the training workload and resource allocation across various node configurations.", "section": "Energy Analysis"}, {"figure_path": "NLUYZ4ZqNq/figures/figures_21_2.jpg", "caption": "Figure 8: Energy Analysis. Number of Jobs vs Number of Nodes.", "description": "This figure shows the relationship between the number of jobs and the number of nodes used in the energy consumption analysis of the SaulLM model training.  The x-axis represents the number of nodes, and the y-axis represents the number of jobs run on those nodes. The data is presented as a bar chart, showing the distribution of jobs across different node counts.  This provides insights into the scalability and resource utilization during the training process.", "section": "5.1 Global Results"}, {"figure_path": "NLUYZ4ZqNq/figures/figures_21_3.jpg", "caption": "Figure 5: Continue Pretraining Analysis. Training loss for SaulLM-141B-base and SaulLM-54B-base over normalized epochs.", "description": "This figure shows the training loss curves for both the SaulLM-141B-base and SaulLM-54B-base models over normalized epochs.  The curves demonstrate a consistent downward trend throughout the training, indicating that the models haven't yet reached their full potential and that further pretraining could lead to performance improvements. The raw and smoothed loss curves are presented for both models.", "section": "5.4 Can We Achieve Further Improvements by Continuing Pretraining?"}, {"figure_path": "NLUYZ4ZqNq/figures/figures_21_4.jpg", "caption": "Figure 10: Energy Analysis. Log-Scaled Total Power Consumption vs GPU Load for Different Numbers of Nodes.", "description": "This figure displays the relationship between total power consumption (log-scaled) and GPU load for various numbers of nodes used in the training process.  Different colors represent the different numbers of nodes used, allowing for a visual comparison of energy efficiency across varying computational scales.  This analysis is crucial for understanding cost-effectiveness and resource allocation during large-scale model training.", "section": "Energy Analysis"}, {"figure_path": "NLUYZ4ZqNq/figures/figures_21_5.jpg", "caption": "Figure 2: Overall Results. Comparison of SaulLM-large and SaulLM-medium with existing models.", "description": "This figure presents a bar chart comparing the mean balanced accuracy achieved by different large language models (LLMs) on the LegalBench-Instruct benchmark.  The LLMs compared are SaulLM-large, SaulLM-medium, GPT4, Mixtral-54B, and Mixtral-140B. The chart shows that SaulLM-medium outperforms Mixtral-54B, and SaulLM-large outperforms Mixtral-140B, demonstrating the effectiveness of the domain adaptation strategy employed in the SaulLM models.  It also highlights that the SaulLM models (medium and large) achieve higher accuracy than Llama3-70B and GPT-3.5 models.", "section": "5.1 Global Results"}, {"figure_path": "NLUYZ4ZqNq/figures/figures_21_6.jpg", "caption": "Figure 2: Overall Results. Comparison of SaulLM-large and SaulLM-medium with existing models.", "description": "This figure compares the performance of SaulLM-large and SaulLM-medium against other existing models (GPT3.5, GPT4, Llama3, Mixtral-54, and Mixtral-140) on LegalBench-Instruct.  It shows the mean balanced accuracy achieved by each model across the six legal reasoning tasks. The figure highlights that SaulLM-medium outperforms Mixtral-54B, and SaulLM-large outperforms Mixtral-141B, demonstrating the effectiveness of the domain adaptation strategy. Interestingly, SaulLM's smaller models even surpass larger models like GPT-4 and Llama3-70B, further supporting the value of the proposed domain adaptation approach.", "section": "5.1 Global Results"}, {"figure_path": "NLUYZ4ZqNq/figures/figures_22_1.jpg", "caption": "Figure 1: Domain adaptation method model for turning a Mixtral to a SaulLM-141B. Training involves different stages: legal domain pretraining, instruction filtering, and preference filtering.", "description": "This figure shows a schematic representation of the domain adaptation process used to create the SaulLM-141B model.  It starts with a base Mixtral model, which is then subjected to three main stages: 1) Legal Domain Pretraining: enhancing the model with a large corpus of legal text; 2) Instruction Fine-tuning: aligning model outputs with human-provided instructions, focusing on legal tasks; and 3) Preference Fine-tuning: refining the model's outputs to align with human preferences regarding legal reasoning. The figure illustrates the flow of data and the transformation of the model at each stage.", "section": "Implementation Details & Evaluation Protocol"}, {"figure_path": "NLUYZ4ZqNq/figures/figures_23_1.jpg", "caption": "Figure 1: Domain adaptation method model for turning a Mixtral to a SaulLM-141B. Training involves different stages: legal domain pretraining, instruction filtering, and preference filtering.", "description": "This figure illustrates the domain adaptation process used to transform Mixtral models into SaulLM-141B. It highlights the three key stages involved in this process: 1. Legal Domain Pretraining: This stage involves training the model on a large corpus of legal data. 2. Instruction Filtering: This stage involves refining the model's ability to follow instructions. 3. Preference Filtering: This stage involves aligning the model's outputs with human preferences. The figure shows how these stages are combined to create a model that is better suited for legal tasks.", "section": "4 Implementation Details & Evaluation Protocol"}]