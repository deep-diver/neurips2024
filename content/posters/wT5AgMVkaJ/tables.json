[{"figure_path": "wT5AgMVkaJ/tables/tables_6_1.jpg", "caption": "Table 1: Performance on traditional retrieval benchmarks and our proposed aesthetic alignment dataset. PT indicates pre-training, and RLFT indicates our alignment fine-tuning.", "description": "This table presents a comparison of the performance of different models on standard image retrieval benchmarks (ImageNet1K zero-shot classification, MSCOCO T2I retrieval, and Flickr30K T2I retrieval) and a newly proposed aesthetic alignment dataset (HPIR).  The models compared are CLIP, DataComp, and the authors' model, both before (PT) and after (RLFT) the proposed reinforcement learning fine-tuning for aesthetic alignment.  The table highlights the impact of the fine-tuning on both traditional retrieval metrics and the aesthetic scores, showing improvements in the latter after the fine-tuning.", "section": "4.1 Details and Evaluations of Alignment Fine-tuning"}, {"figure_path": "wT5AgMVkaJ/tables/tables_7_1.jpg", "caption": "Table 2: System level comparison results. Mark \u2020 indicates using LLM rephrasing. DataComp-15M is a 15 million subset of DataComp-1B dataset [8].", "description": "This table presents a system-level comparison of the proposed method against other models and commercial search engines.  It compares the win and similar rates (percentage of times system A is judged better than system B by GPT-4V, considering similar results as half a win) across accuracy and aesthetic metrics, using both a smaller (8M images) and larger (15M images) subset of the DataComp dataset.  Human labeler judgments are included for comparison.", "section": "Experiments"}, {"figure_path": "wT5AgMVkaJ/tables/tables_7_2.jpg", "caption": "Table 3: Evaluation of different method prompts for LLM rephrasing on HPIR and GPT-4V win rate. Scores from aesthetic models are also provided.", "description": "This table presents the results of an experiment evaluating the effectiveness of different LLM (Large Language Model) rephrasing methods on improving the aesthetic quality of image retrieval.  It compares several prompt types ('original', '<detail>', '<k list>', '<kw dict>', 'repeat', '<reorg>') for LLM query rephrasing, assessing their impact on HPIR (Human Preference of Image Retrieval) metrics\u2014both accuracy and aesthetic scores\u2014as well as GPT-4V win rates (accuracy and aesthetic).  The table also includes the average aesthetic scores generated by three different aesthetic models (CLIPIQA, IAP, MANIQA) for each prompt type, providing a multi-faceted evaluation of the aesthetic enhancement achieved through LLM rephrasing.", "section": "4.2 Effect of LLM Rephrasing"}, {"figure_path": "wT5AgMVkaJ/tables/tables_14_1.jpg", "caption": "Table 4: The composition and amount of data during pretraining. SCVL indicates our self-collected image-text pair dataset.", "description": "This table shows the datasets used for pre-training the vision-language model, including the number of samples in each dataset, the number of samples used for training, and the number of epochs used for training each dataset. The table also shows the total number of samples and epochs used for pre-training. SCVL refers to a self-collected image-text pair dataset.", "section": "2.1 Model Pretraining"}, {"figure_path": "wT5AgMVkaJ/tables/tables_15_1.jpg", "caption": "Table 5: Details of pre-training hyper-parameters.", "description": "This table lists the hyperparameters used during the pre-training phase of the vision-language model.  It details settings for the training process (learning rate, batch size, optimizer, etc.) and loss function (NCE loss, label smoothing), as well as data augmentation techniques (auto augmentation, color jitter, etc.) and model-specific parameters (tau init, project size, etc.).  The table provides a comprehensive overview of the configurations employed to pre-train the model.", "section": "A Pre-training Details"}, {"figure_path": "wT5AgMVkaJ/tables/tables_16_1.jpg", "caption": "Table 6: Details of fine-tuning hyper-parameters.", "description": "This table lists the hyperparameters used during the fine-tuning phase of the model. It includes settings for the learning rate, batch size, weight decay, optimizer, dropout, and other regularization techniques.  It also specifies the loss function used (Ranked DPO) and its hyperparameters, including label smoothing and the weight given to the pre-training loss. Finally, it shows the data augmentation techniques employed during fine-tuning.", "section": "B Alignment Fine-tuning Details and Ablations"}, {"figure_path": "wT5AgMVkaJ/tables/tables_17_1.jpg", "caption": "Table 7: Experiments on the effect of the construction of partial order set Dpo. Q is the number of queries.", "description": "This table presents ablation studies on the construction of the partially ordered dataset Dpo used in the reinforcement learning fine-tuning.  It shows the impact of varying the parameters *u* (semantic dimension) and *v* (aesthetic dimension) on the model's performance, measured by Accuracy and Aesthetic scores on the HPIR benchmark. Different stride values are also tested to see their effect on the final performance. The size of the dataset |Dpo| is shown for each experiment. ", "section": "B.2 Effect of 2-D sampling of Dpo"}, {"figure_path": "wT5AgMVkaJ/tables/tables_17_2.jpg", "caption": "Table 1: Performance on traditional retrieval benchmarks and our proposed aesthetic alignment dataset. PT indicates pre-training, and RLFT indicates our alignment fine-tuning.", "description": "This table presents a comparison of the performance of different models on standard image retrieval benchmarks and a newly proposed aesthetic alignment dataset.  The benchmarks assess the models' abilities in ImageNet1K zero-shot classification, MSCOCO T2I retrieval, and Flickr30K T2I retrieval.  The new dataset evaluates the models' alignment with human aesthetic preferences.  The table shows results for the original CLIP and DataComp models (both pre-trained), and those same models after undergoing the proposed reinforcement learning fine-tuning for aesthetic alignment (RLFT).  The \"Accuracy\" and \"Aesthetic\" columns in the HPIR (Human Preference of Image Retrieval) section shows the performance on the newly proposed dataset.", "section": "4.1 Details and Evaluations of Alignment Fine-tuning"}, {"figure_path": "wT5AgMVkaJ/tables/tables_18_1.jpg", "caption": "Table 9: Ablation of different data augmentations. Different strategies are added to the base transform separately.", "description": "This table presents the results of an ablation study on the impact of various data augmentation techniques on the performance of the proposed model.  The study focuses on the HPIR (Human Preference of Image Retrieval) metric's Accuracy and Aesthetic aspects. The \"Eval-transform\" row shows the baseline performance with only basic transformations. Subsequent rows add individual augmentation methods (auto-augmentation, random erase, color jitter) to assess their effects. The results reveal how each data augmentation strategy influences the accuracy and aesthetic scores, highlighting the complex interplay between data augmentation and the model's aesthetic alignment.", "section": "B.2 Effect of 2-D sampling of Dpo"}, {"figure_path": "wT5AgMVkaJ/tables/tables_21_1.jpg", "caption": "Table 10: Evaluation of GPT-4V with different prompts (<ranker>, <scorer>, and <cp-scorer>) on HPIR. With order-consistency (OC), the <ranker> performs the best.", "description": "This table presents the results of evaluating GPT-4V's ability to judge the aesthetic quality of image retrieval results using three different prompting methods: <ranker>, <scorer>, and <cp-scorer>.  The <ranker> method presents two sets of images side-by-side and asks GPT-4V to choose the better set. The <scorer> method provides a scoring rubric to GPT-4V for evaluating each image set individually, and the <cp-scorer> method combines aspects of both. The results show the accuracy and aesthetic scores for each method, both with and without order consistency (OC), and are compared to human expert judgments.  The key finding is that the <ranker> method, when order consistency is applied, yields results most closely aligned with human expert evaluations.", "section": "3.2 GPT-4V Win Rate"}, {"figure_path": "wT5AgMVkaJ/tables/tables_21_2.jpg", "caption": "Table 2: System level comparison results. Mark \u2020 indicates using LLM rephrasing. DataComp-15M is a 15 million subset of DataComp-1B dataset [8].", "description": "This table presents a system-level comparison of the proposed fine-tuned model against other models and commercial search engines. It shows the win rate and win-and-similar rate of the proposed model compared to other systems across different metrics, including accuracy and aesthetics, using two different datasets (DataComp-15M and internal-8M). The table helps to evaluate the performance and efficiency of the proposed model in a real-world scenario.", "section": "Experiments"}, {"figure_path": "wT5AgMVkaJ/tables/tables_22_1.jpg", "caption": "Table 3: Evaluation of different method prompts for LLM rephrasing on HPIR and GPT-4V win rate. Scores from aesthetic models are also provided.", "description": "This table presents the results of experiments evaluating different prompt methods for Large Language Model (LLM) rephrasing on the Human Preference of Image Retrieval (HPIR) dataset.  It compares the performance using various prompts:  `original` (no rephrasing), `<detail>`, `<k list>`, `<kw dict>`, `repeat`, and `<reorg>`.  The metrics used are HPIR accuracy, HPIR aesthetic scores, and GPT-4V win rates.  Scores from four different aesthetic models (CLIPIQA, IAP, MANIQA, and Accuracy) are also included to provide a more comprehensive assessment of the aesthetic quality of the results.", "section": "4.2 Effect of LLM Rephrasing"}, {"figure_path": "wT5AgMVkaJ/tables/tables_22_2.jpg", "caption": "Table 1: Performance on traditional retrieval benchmarks and our proposed aesthetic alignment dataset. PT indicates pre-training, and RLFT indicates our alignment fine-tuning.", "description": "This table presents a comparison of the performance of different models on standard image retrieval benchmarks and a newly proposed aesthetic alignment dataset.  It shows the performance of the original CLIP and DataComp models, as well as the performance after pre-training (PT) and after fine-tuning with reinforcement learning from human feedback (RLFT). The benchmarks include ImageNet1K zero-shot classification, MSCOCO T2I retrieval Recall@1, and Flickr30K T2I retrieval Recall@1, along with the new aesthetic alignment dataset (HPIR). The table demonstrates that while fine-tuning did not significantly impact traditional retrieval performance, it substantially improved performance on the aesthetic alignment dataset, outperforming both the original CLIP and DataComp models.", "section": "4.1 Details and Evaluations of Alignment Fine-tuning"}]