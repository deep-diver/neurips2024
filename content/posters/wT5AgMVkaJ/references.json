{"references": [{"fullname_first_author": "A. Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-07-11", "reason": "This paper introduces CLIP, a foundational vision-language model that is extensively used and adapted in the current work for aesthetic alignment."}, {"fullname_first_author": "S. Y. Gadre", "paper_title": "DataComp: In search of the next generation of multimodal datasets", "publication_date": "2023-12-01", "reason": "DataComp is a large-scale multimodal dataset leveraged for pre-training and evaluation, serving as a crucial resource for the research presented."}, {"fullname_first_author": "L. Ouyang", "paper_title": "Training language models to follow instructions with human feedback", "publication_date": "2022-12-01", "reason": "This paper details RLHF, a critical technique used to align language models with human preferences, which is adapted for aligning vision models with human aesthetics in the current work."}, {"fullname_first_author": "R. Rafailov", "paper_title": "Direct preference optimization: Your language model is secretly a reward model", "publication_date": "2023-12-01", "reason": "This paper introduces DPO, a preference-based reinforcement learning method, adapted in this research for aesthetic alignment fine-tuning."}, {"fullname_first_author": "J. Li", "paper_title": "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models", "publication_date": "2023-07-11", "reason": "BLIP-2 is a powerful vision-language model used to benchmark the aesthetic performance of the proposed system, highlighting its importance in the evaluation strategy."}]}