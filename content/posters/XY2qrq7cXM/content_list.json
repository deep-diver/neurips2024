[{"type": "text", "text": "Gradient Rewiring for Editable Graph Neural Network Training ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Zhimeng Jiang1, Zirui Liu2, Xiaotian $\\mathbf{Han^{3}}$ , Qizhang Feng1, Hongye $\\mathbf{Jin}^{1}$ , Qiaoyu $\\mathbf{Tan^{4}}$ , Kaixiong Zhou5, Na $\\mathbf{Zou}^{6}$ , Xia $\\mathbf{H}\\mathbf{u}^{7}$ ", "page_idx": 0}, {"type": "text", "text": "1Texas A&M University, 2University of Minnesota, 3Case Western Reserve University,   \n4NYU Shanghai, 5North Carolina State University, 6University of Houston, 7Rice University ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Deep neural networks are ubiquitously adopted in many applications, such as computer vision, natural language processing, and graph analytics. However, welltrained neural networks can make prediction errors after deployment as the world changes. Model editing involves updating the base model to correct prediction errors with less accessible training data and computational resources. Despite recent advances in model editors in computer vision and natural language processing, editable training in graph neural networks (GNNs) is rarely explored. The challenge with editable GNN training lies in the inherent information aggregation across neighbors, which can lead model editors to affect the predictions of other nodes unintentionally. In this paper, we first observe the gradient of crossentropy loss for the target node and training nodes with significant inconsistency, which indicates that directly fine-tuning the base model using the loss on the target node deteriorates the performance on training nodes. Motivated by the gradient inconsistency observation, we propose a simple yet effective Gradient Rewiring method for Editable graph neural network training, named GRE. Specifically, we first store the anchor gradient of the loss on training nodes to preserve the locality. Subsequently, we rewire the gradient of the loss on the target node to preserve performance on the training node using anchor gradient. Experiments demonstrate the effectiveness of GRE on various model architectures and graph datasets in terms of multiple editing situations. The source code is available at https://github.com/zhimengj0326/Gradient_rewiring_editing. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Graph Neural Networks (GNNs) have demonstrated exemplary performance for graph learning tasks, such as recommendation, link prediction, molecule property analysis [1, 2, 3, 4, 5, 6, 7]. With message passing, GNNs learn node representations by recursively aggregating the neighboring nodes\u2019 representations. Once trained, GNN models are deployed to handle various high-stake tasks, such as credit risk assessment in financial networks [8] and fake news detection in social networks [9]. However, the impact of erroneous decisions in such influential applications can be substantial. For instance, misplaced credit trust in undetected fake news can lead to severe financial loss. ", "page_idx": 0}, {"type": "text", "text": "An ideal approach to tackle such errors should possess the following properties: 1) the ability to rectify severe errors in the model\u2019s predictions, 2) the capacity to generalize these corrections to other similar instances of misclassified samples, and 3) the ability to preserve the model\u2019s prediction accuracy for all other unrelated inputs. To achieve these goals, various model editing frameworks have been developed to rectify errors by dynamically adjusting the model\u2019s behavior when errors are detected [10, 11]. The core principle is to implement minimal changes to the model to correct the error while keeping the rest of the model\u2019s behavior intact. However, model editing is not a simple plug-and-play solution. These frameworks often require an additional training phase to prepare for editing before they can be used effectively for editing [10, 11, 12, 13]. Although model editing techniques have shown significant utility in computer vision and language models, there is rare work focused on rectifying critical errors in graph data. The unique challenge arises from the inherent message-passing mechanism in GNNs when edits involve densely interconnected nodes [14, 15]. Specifically, editing the behavior of a single node can unintentionally induce a ripple effect, causing changes that propagate throughout the entire graph. [14] theoretically and empirically demonstrate the complexity of editing GNNs through the lens of the loss landscape of the Kullback-Lieber divergence between the pre-trained node features and edited final node embeddings. Moreover, a simple yet effective model structure, named EGNN, is proposed with stitched peer multi-layer perception (MLP), where only the stitched MLP is trained during model editing. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In this work, we investigate the model editing problem for GNNs from a brand-new gradient perspective, which is compatible with existing work [14]. Specifically, we first found a considerable inconsistency between the gradients of the cross-entropy loss for the target node and the training nodes for GNNs. Such inconsistency implies that direct fine-tuning of the base model using the loss of the target node can lead to a deterioration in the performance on the training nodes. Motivated by the above observation, we propose a simple yet effective Gradient Rewiring method for Editable graph neural network training, named GRE. Specifically, we first calculate and store the anchor gradient of the loss on the training nodes. This anchor gradient represents the original learning direction that we wish to preserve. Then, during the editing process, we adjust the gradient of the loss on the target node based on the stored anchor gradient. This adjustment, or \u201crewiring\u201d, ensures that the changes made to the target node do not adversely affect the performance on the training nodes. Experiments demonstrate the effectiveness of our proposed method for various model structures and graph datasets. Moreover, the proposed method is compatible with the existing EGNN baseline and further improves the performance. ", "page_idx": 1}, {"type": "image", "img_path": "XY2qrq7cXM/tmp/7161c544b50e25957c1dfb65d96ff355711902a28c611c7261dce622e9525987.jpg", "img_caption": ["Figure 1: (a) Top: RMSE distance between the gradients of cross-entropy loss over training datasets and over the targeted sample for different architectures. (b) Middle: Cross-entropy loss over training datasets when the model is updated using target loss. (c) Bottom: Cross-entropy loss over the targeted sample when the model is updated using target loss. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "2 Preliminary and Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We first introduce the notations used throughout this paper. A graph is given by $\\mathcal{G}=(\\mathcal{V},\\mathcal{E})$ ,where $\\mathcal{V}=(v_{1},\\cdots,v_{N})$ is the set of nodes indexed from 1 to $n$ , and $\\mathcal{E}=(e_{1},\\cdot\\cdot\\cdot\\,,e_{m})\\subseteq\\mathcal{V}\\times\\mathcal{V}$ is the set of edges. $n=|\\gamma|$ and $m=|\\mathcal{E}|$ are the numbers of nodes and edges, respectively. Let $\\pmb{X}\\in\\mathbb{R}^{n\\times d}$ be the node feature matrix, where $d$ is the dimension of node features. $A\\in\\mathbb{R}^{n\\times n}$ is the graph adjacency matrix, where $A_{i,j}=1$ if $(v_{i},v_{j})\\in\\mathcal{E}$ else $A_{i,j}=0$ . $\\tilde{\\cal A}=\\tilde{D}^{-\\frac{1}{2}}(A+I)\\tilde{D}^{-\\frac{1}{2}}$ is the normalized adjacency matrix, where $\\tilde{D}$ is the degree matrix of $A+I$ . The node label is defined as $y_{i}$ for node $v_{i}$ . We consider node classification tasks with $C$ classes in this paper. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "2.1 Graph Neural Networks ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Graph Neural Networks have been successfully applied across various domains/tasks, including knowledge graphs [16, 17], graph condensation [18, 19, 20], event extraction [21], and entity relation tasks [22]. Most graph neural networks follow a neighborhood aggregation procedure to learn node representation via propagating representations of neighbors and then follow up with feature transformation [23]. The $l$ -th layer of graph neural networks is given by: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\mathbf{a}_{i}^{(l)}}&{=}&{\\mathsf{P R O P A G A T I O N}^{(l)}\\Big(\\big\\{\\mathbf{x}_{i}^{(l-1)},\\mathbf{x}_{j}^{(l-1)}|j\\in\\mathcal{N}_{i}\\big\\}\\Big),}\\\\ {\\mathbf{x}_{i}^{(l)}}&{=}&{\\mathsf{T R A N S F O R M A T I O N}^{(l)}\\Big(\\mathbf{a}_{i}^{(l)}\\Big),\\quad}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\mathbf{x}_{i}^{(l)}$ is the representation of node $v_{i}$ at $l$ -th layer and $x_{i}^{(0)}$ is initialized as node feature $\\mathbf{x}_{i}$ i..e, the $i$ -th row at node feature matrix $\\mathbf{\\deltaX}$ . Many GNNs, such as GCN [24], GraphSAGE [25], and GAT [26], can be defined under this computation paradigm via adopting the different propagation and transformation operations. For example, the $l$ -th layer in GCN can be defined as: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbf{\\boldsymbol{X}}^{(l)}=\\sigma(\\tilde{\\mathbf{\\boldsymbol{A}}}\\mathbf{\\boldsymbol{X}}^{(l-1)}\\mathbf{\\boldsymbol{W}}^{(l)}),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\pmb{X}^{(l)}\\in\\mathbb{R}^{n\\times d}$ and $\\pmb{X}^{(l-1)}\\in\\mathbb{R}^{n\\times d}$ are the node representation matrix containing the $h_{v}$ for each node $v$ at the layer $l$ and layer $l-1$ , respectively. $\\bar{W}^{(l)}\\in\\mathbb{R}^{d\\times d}$ is a layer-specific trainable weight matrix, and $\\sigma(\\cdot)$ is a non-linear activation function (e.g., ReLU). ", "page_idx": 2}, {"type": "text", "text": "2.2 Model Editing ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Model editing aims to modify a base model\u2019s responses for a misclassified sample $x_{t g}$ and its analogs. This is typically achieved by fine-tuning the model using only a single pair of input $x_{t g}$ and the desired output $y_{t g}$ , while preserving the model\u2019s responses to unrelated inputs [10, 11, 12, 14]. Our contribution lies in the novel application of model editing to graph data, a domain where misclassifications on a few pivotal nodes can trigger substantial financial losses, fairness issues, or even the propagation of adversarial attacks. Consider the scenario of node classification where a well-trained GNN incorrectly predicts a particular node. Model editing can be employed to rectify this erroneous prediction. By leveraging the node\u2019s characteristics and the desired label, the model can be updated to correct such behavior. The ideal outcome of model editing is twofold: first, the updated model should correctly predict the specific node and its similar instances; second, the model should maintain its original behavior on unrelated inputs. It is important to note that some model editors require a preparatory training phase before they can be applied effectively [10, 13, 12, 11]. This crucial step ensures that the model editing process is both precise and effective in its application. ", "page_idx": 2}, {"type": "text", "text": "3 Methodology ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we first provide the preliminary experimental results as the motivation to rewire gradients for model editing. Subsequently, we propose our gradient rewiring method for editable graph neural networks training (GRE) and an advanced version $\\mathrm{(GRE+)}$ to improve the effectiveness of model editing, respectively. ", "page_idx": 2}, {"type": "text", "text": "3.1 Motivation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In the preliminary experiments, we first pre-train GCN, GraphSAGE, and MLP on the training dataset $\\mathcal{V}_{t r a i n}$ (e.g., Cora, Flickr, ogbn-arxiv, and Amazon Photo datasets) using cross-entropy loss. Subsequently, we find the misclassified samples in the validation dataset and randomly select one sample as the target sample $(\\mathbf{x}_{t g},y_{t g})$ . During the model editing, we update the pre-trained model using cross-entropy loss over the target sample using gradient descent, i.e., the models are trained inductively. Following previous work [10, 12, 14], we perform 50 independent edits and report the averaged metrics. ", "page_idx": 2}, {"type": "text", "text": "It is well-known that model editing incurs training performance degradation [11, 10, 12] 1 for many model architectures. To deeply delve into the underlying reason, we investigate performance degradation from a model gradient perspective. We further define the training loss as $\\mathcal{L}_{t r a i n}\\,=$ $\\begin{array}{r}{\\frac{\\bar{\\mathbf{\\check{\\Gamma}}}_{1}}{|\\mathcal{V}_{t r a i n}|}\\sum_{i\\in\\mathcal{V}_{t r a i n}}C E(f_{\\theta}(\\Breve{\\mathbf{x}}_{i}),y_{i})}\\end{array}$ , where $f_{\\theta}(\\cdot)\\,\\in\\,\\mathbb{R}^{C}$ is a prediction model parameterized with $\\theta\\in\\mathbb{R}^{L}$ , $C$ and $L$ are the number of classes and model parameters, $C E(\\cdot,\\cdot)$ is the cross-entropy loss, the target loss is given by $\\mathcal{L}_{t g}=C E(f_{\\theta}(\\mathbf{x}_{t g}),y_{t g})$ . For example, model $f_{\\theta}(\\cdot)$ can be instantiated by GNNs with the number of layers defined in Eq. (1) or a simple MLP. For model editing, the gradient for training and target loss is given by $\\begin{array}{r}{g_{t r a i n}=\\frac{\\partial\\mathcal{L}_{t r a i n}}{\\partial\\theta}\\in\\mathbb{R}^{L}}\\end{array}$ \u2202Ltrain \u2208RL and gtg = \u2202\u2202L\u03b8tg \u2208RL, respectively. To investigate why the model editing leads to training performance degradation, we use gradient RMSE (Root-Mean-Squared-Error), i.e., $\\mathrm{Grad}_{R M S E}=\\sqrt{\\|g_{t r a i n}-g_{t g}\\|_{2}^{2}}$ , to measure the model editing discrepancy for training datasets and target sample. ", "page_idx": 3}, {"type": "text", "text": "The model editing curves for gradient RMSE 2, training loss, and target loss across various model architectures (GCN, GraphSAGE, and MLP) are shown in Figure 1. Although the gradient RMSE for training datasets and target sample is close to 0, the model parameters demonstrate significant inconsistent behavior in terms of training loss due to large gradient discrepancy in the initial editing stage. We observe that: 1) Even though the target loss decreases during model editing, the training loss increases significantly. 2) The increasing rates of training loss for GCN and GraphSAGE are significantly higher than that of MLP. The above observations imply that editing training for graph neural networks is more challenging due to higher gradient discrepancy between the training dataset and the target sample. ", "page_idx": 3}, {"type": "text", "text": "3.2 Gradient Rewiring Approach ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Preliminary results show a high discrepancy in training loss and target loss for GNNs, which implies that the vanilla model editing hampers the performance on the overall training dataset and thus results in a high accuracy drop for node classification tasks. Therefore, we aim to tackle the training dataset performance degradation from the gradient rewiring approach. ", "page_idx": 3}, {"type": "text", "text": "GRE We propose a simple yet effective gradient rewiring approach for editable graph neural network training, named GRE. We first formulate a constrained optimization problem to regulate model editing and then solve the constrained optimization problem via gradient rewiring. ", "page_idx": 3}, {"type": "text", "text": "Model editing aims to correct the prediction for the target sample while maintaining the prediction accuracy on the training nodes. The objective function focuses on minimizing the loss at the target node. To preserve the predictions on the training nodes, we introduce two constraints: (1) the training loss should not exceed its value prior to model editing (see Eq. (3)); and (2) the differences in model predictions after editing should remain within a predefined range (see Eq. (4)). Define $\\theta_{0}$ and $\\theta^{\\prime}$ as the model parameters before and after model editing. Then we have the following constrained optimization problem: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{\\theta}{\\operatorname*{min}}\\quad}&{\\mathcal{L}_{t g}\\big(f_{\\theta}(\\mathbf{x}_{t g}),y_{t g}\\big)}\\\\ {\\mathrm{s.t.}\\quad}&{\\mathcal{L}_{t r a i n}\\big(f_{\\theta^{\\prime}},\\mathcal{V}_{t r a i n}\\big)\\leq\\mathcal{L}_{t r a i n}\\big(f_{\\theta_{0}},\\mathcal{V}_{t r a i n}\\big)}\\\\ &{\\quad\\|\\frac{1}{|\\mathcal{V}_{t r a i n}|}\\sum_{i\\in\\mathcal{V}_{t r a i n}}f_{\\theta^{\\prime}}(\\mathbf{x}_{i})-f_{\\theta_{0}}(\\mathbf{x}_{i})\\|^{2}\\leq\\delta^{\\prime},}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\theta$ and $\\theta^{\\prime}$ represent the model parameters before and after model editing, respectively, the hyperparameter $\\delta^{\\prime}$ represents the maximum average prediction difference on training nodes. Notice that the model parameters update adopts gradient descent using target loss without any constraints, i.e., $\\theta^{\\prime}=\\theta_{0}-\\alpha g_{t g}$ , where $\\alpha$ is step size in model editing. The key idea of our proposed solution is to rewire gradient $g_{t g}$ as $g$ , which is obtained by satisfying the involved constraints. Note that the model editing usually corrects the model prediction on the target sample within a few steps, i.e. there are no significant model parameter differences, thus we adopt Taylor expansion to tackle such constrained optimization problem. For target loss $\\mathcal{L}_{t g}$ , we can approximate it as: ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\mathcal{L}_{t g}\\big(f_{\\theta^{\\prime}}(\\mathbf{x}_{t g}),y_{t g}\\big)}&{\\approx}&{\\mathcal{L}_{t g}\\big(f_{\\theta_{0}}(\\mathbf{x}_{t g}),y_{t g}\\big)+g_{t g}^{\\top}(\\theta^{\\prime}-\\theta_{0})}\\\\ &{=}&{\\mathcal{L}_{t g}\\big(f_{\\theta_{0}}(\\mathbf{x}_{t g}),y_{t g}\\big)-\\alpha g_{t g}^{\\top}g.\\ \\ \\ \\ }\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "To optimize the objective function Eq. (2), it is easy to conclude that the gradient cosine similarity $g_{t g}^{\\top}g$ should be maximized. Given the gradient before/after model editing is fixed, the maximization of gradient cosine similarity $g_{t g}^{\\top}g$ is equivalent to the minimization of $\\|g_{t g}-g\\|^{2}$ . To satisfy Eq. (3), we also adopt Taylor expansion on $\\mathcal{L}_{t r a i n}$ and it is easy to obtain that the gradient cosine similarity should be positive, i.e., $\\dot{g}_{t g}^{\\top}g\\geq0$ . As for the constraint in Eq. (4), similarly, a Taylor expansion is used to express the relationship between the model predictions before and after the model editing, as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\nf_{\\theta^{\\prime}}(\\mathbf{x}_{i})\\approx f_{\\theta_{0}}(\\mathbf{x}_{i})+{\\frac{\\partial f_{\\theta_{0}}(\\mathbf{x}_{i})}{\\partial\\theta}}^{\\top}(\\theta^{\\prime}-\\theta)=f_{\\theta_{0}}(\\mathbf{x}_{i})-{\\frac{\\partial f_{\\theta_{0}}(\\mathbf{x}_{i})}{\\partial\\theta}}^{\\top}\\cdot\\alpha g,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Therefore, we can obtain the following approximation on Eq. (4): ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\|\\frac{1}{|\\mathcal{V}_{t r a i n}|}\\sum_{i\\in\\mathcal{V}_{t r a i n}}f_{\\theta^{\\prime}}(\\mathbf{x}_{i})-f_{\\theta_{0}}(\\mathbf{x}_{i})\\|^{2}\\approx\\|\\hat{g}_{t r a i n}^{\\top}(-\\alpha g)\\|^{2}\\le\\delta^{\\prime},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where gradient for a model prediction is defined as $\\begin{array}{r}{\\hat{g}_{t r a i n}\\,=\\,\\frac{\\partial\\frac{1}{|\\mathcal{V}_{t r a i n}|}\\sum_{i\\in\\mathcal{V}_{t r a i n}}f_{\\theta_{0}}(\\mathbf{x}_{i})}{\\partial\\theta}\\Big\\vert\\Big\\vert_{\\theta=\\theta_{0}}\\,\\in\\,}\\end{array}$ $\\mathbb{R}^{L\\times C}$ . Therefore, the model prediction difference constraint can be transformed into $\\|\\hat{g}_{t r a i n}^{\\top}g\\|^{2}\\leq$ $\\|\\hat{g}_{t r a i n}\\|_{s p e c t}^{2}\\|g\\|^{2}\\leq\\delta$ , where $\\|\\cdot\\|_{s p e c t}$ represents matrix spectrum norm and $\\|\\hat{g}_{t r a i n}\\|_{s p e c t}$ is fixed in model editing, and $\\begin{array}{r}{\\delta=\\frac{\\delta^{\\prime}}{\\alpha^{2}}}\\end{array}$ . In a nutshell, our goal is to correct the target sample (i.e., minimize $\\|g_{t g}-g\\|^{2})$ and minimize gradient discrepancy for model prediction among training dataset and target sample (i.e., $\\|g\\|^{2})$ , while guaranteeing non-increased training loss (i.e., $g_{t r a i n}^{\\top}g\\geq0)$ . The original constraint optimization problem is simplified as gradient rewiring, i.e., ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{g}\\frac{1}{2}\\|g-g_{t g}\\|^{2}+\\frac{\\lambda}{2}\\|g\\|^{2}=\\operatorname*{min}_{g}\\frac{1+\\lambda}{2}g^{\\top}g-g_{t g}^{\\top}g+\\frac{1}{2}g_{t g}^{\\top}g_{t g}\\qquad\\mathrm{~s.t.~}\\quad g_{t r a i n}^{\\top}g\\ge0,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\lambda\\geq0$ is the hyperparameter to control the balance between target sample correction and gradient discrepancy for model prediction. It is easy to obtain that Eq.(8) is a quadratic program (QP) in $L$ -variables (the number of model parameters is usually high in neural networks). Fortunately, we can effectively solve this problem in the dual space via transforming as a smaller QP problem with only one variable $v$ [27], where the relation between primal and dual variable is $g_{t r a i n}v-(1\\!+\\!\\lambda)g=-g_{t g}$ Then we have the following problem: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{v}\\frac{(1+\\lambda)^{-1}}{2}(g_{t r a i n}v+g_{t g})^{\\top}(g_{t r a i n}v+g_{t g})\\qquad\\mathrm{s.t.}\\ v\\geq0.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "It is easy to obtain the optimal dual variable $\\begin{array}{r}{v^{*}=-\\operatorname*{min}\\{\\frac{g_{t r a i n}^{\\top}g_{t g}}{g_{t r a i n}\\top},0\\}}\\end{array}$ and the optimal rewired gradient . In other words, the gradient rewiring procedure is quite simple: for the gradient of the target loss $g_{t g}$ , reduce its projection component on $g_{t r a i n}$ and then scale it by $(1+\\lambda)^{-1}$ . ", "page_idx": 4}, {"type": "text", "text": "Additionally, we highlight that the gradient for training loss $g_{t r a i n}$ must be stored before model editing. In this way, gradient rewiring can be conducted to remove the harmful gradient component on target loss that increases training loss. Since shallow GNNs model performs well in practice [28], the model size of GNNs is small and the memory cost $O(L)$ for storing anchor gradient is negligible. ", "page_idx": 4}, {"type": "text", "text": "$\\mathbf{GRE+}$ In GRE, the training loss after model editing is required not to be larger than that before model editing. However, it is still possible that the training loss on specific sub-training sets performs worse after model editing. At the same time, the training loss for the whole training dataset, after model editing, is on par with or even lower than that of before editing. To tackle this issue, we proposed an advanced gradient rewiring approach, named $\\mathrm{GRE+}$ , via applying loss constraint on multiple disjoint sub-training sets. Specifically, we split training dataset $\\mathcal{V}_{t r a i n}$ into $K$ sub-training sets $\\{\\gamma_{t r a i n}^{1},\\gamma_{t r a i n}^{2},\\cdot\\cdot\\cdot,\\gamma_{t r a i n}^{K}\\}$ . Similarly, we define $\\begin{array}{r}{g_{t r a i n}^{k}=\\frac{\\partial\\mathcal{L}_{t r a i n}^{k}}{\\partial\\theta}\\in\\mathbb{R}^{L}}\\end{array}$ rain\u2208RL, where $\\begin{array}{r}{\\mathcal{L}_{t r a i n}^{k}=\\frac{1}{|\\mathcal{V}_{t r a i n}^{k}|}\\sum_{i\\in\\mathcal{V}_{t r a i n}^{k}}C E(f_{\\theta}(\\mathbf{x}_{i}),y_{i})}\\end{array}$ . ", "page_idx": 4}, {"type": "text", "text": "Following the derivative clue in GRE, we can replace the training loss constraint on the whole training dataset with multiple training loss constraints on training subsets, and obtain the advanced gradient rewiring approach as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\operatorname*{min}_{g}\\frac{1}{2}\\|g-g_{t g}\\|^{2}+\\frac{\\lambda}{2}\\|g\\|^{2}=\\operatorname*{min}_{g}\\frac{1+\\lambda}{2}g^{\\top}g-g_{t g}^{\\top}g+\\frac{1}{2}g_{t g}^{\\top}g_{t g}}\\\\ &{\\mathrm{~s.t.~}(g_{t r a i n}^{k})^{\\top}g\\ge0,\\mathrm{~for~any~}1\\le k\\le K.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Notice that Eq.(10) is a quadratic program (QP) in $L$ -variables (the number of model parameters are usually high in neural networks), and we can effectively solve this problem in the dual space via transforming as a smaller QP problem with only $K$ variables ${\\bf v}\\in\\mathbb{R}^{K}$ [27]. Define gradient matrix as G = [gt1rain, gt2rain, \u00b7 \u00b7 \u00b7 , gtKrain]\u22a4 , then the relation between primal and dual variable is given by $G\\mathbf{v}-(1+\\lambda)g=-g_{t g}$ . The original optimization problem can be transformed into the following dual problem: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{\\mathbf v}{\\operatorname*{min}}}&{\\quad\\frac{(1+\\lambda)^{-1}}{2}\\mathbf v^{\\top}G G^{\\top}\\mathbf v+(1+\\lambda)^{-1}g_{t g}^{\\top}G^{\\top}\\mathbf v+(1+\\lambda)^{-1}g_{t g}^{\\top}g_{t g}}\\\\ {\\mathrm{s.t.}}&{\\quad\\mathbf v_{k}\\geq0,\\;\\mathrm{for}\\;\\mathrm{any}\\;1\\leq k\\leq K.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The dual problem is a QP with $K\\ll L$ variables, and we usually consider the value of $K$ to be smaller than 5 in practice. Once we tackle dual QP problem (11) for $\\mathbf{v}^{*}$ , we can recover the rewired gradient as $g=(\\mathring{1}+\\lambda)^{-1}(G\\mathbf{v}+g_{t g})$ . Similarly, the gradient for training loss $g_{t r a i n}^{k}$ , where $1\\le k\\le K$ , is required to be stored before model editing. The corresponding memory cost is given by $O(K L)$ . ", "page_idx": 5}, {"type": "text", "text": "Table 1: The results on four small-scale datasets after applying one single edit. The reported number is averaged over 50 independent edits. SR is the edit success rate, Acc is the test accuracy after editing, and DD are the test drawdown, respectively. \u201cOOM\u201d is the out-of-memory error. The best/second-best results are highlighted in boldface/underlined, respectively. ", "page_idx": 5}, {"type": "table", "img_path": "XY2qrq7cXM/tmp/62e1cb15c87cf3a0cd5ddc8d9849d34008d933f1f9cb84ac3b9081d7903a56c0.jpg", "table_caption": [], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we conduct experiments to evaluate the effectiveness of our proposed GRE and $\\mathrm{GRE+}$ , with the goal of answering the following three research questions. RQ1: Can the proposed solution correct the wrong model prediction with a lower accuracy drop after model editing in the independent and sequential editing setting? RQ2: What\u2019s the tradeoff performance between accuracy drop and success rate in the independent editing setting? RQ3: How sensitive are the proposed GRE and $\\mathrm{GRE+}$ methods to the key hyperparameter $\\lambda?$ ", "page_idx": 5}, {"type": "text", "text": "4.1 Experimental Setting ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We follow the standard experimental setting for GNNs [14]. Specifically, we first randomly split the train/validation/test dataset. Then, we ensure that each class has 20 samples in the training and 30 samples in the validation sets. The remaining samples are used for the test set. The target node is randomly selected 50 times from the validation set and the well-trained model makes the wrong prediction. The average model editing performance (e.g., success rate, drawdown) is reported for evaluation. ", "page_idx": 5}, {"type": "text", "text": "Datasets and Models. In our experiments, we utilize a selection of eight graph datasets from diverse domains, split evenly between small-scale and large-scale datasets. The small-scale datasets include Cora, A-computers [29], A-photo [29], and Coauthor-CS [29]. On the other hand, the large-scale datasets encompass Reddit [25], Flickr [2], ogbn-arxiv [3], and ogbn-products [3]. Note that our approach is based on gradient rewiring, which is orthogonal to model architectures. We adopt two prevalent models GCN [24] and GraphSAGE [25], where both of them are trained with the entire graph at each step. We evaluate our method under the inductive setting, which means the model is trained on a subgraph containing only the training node, and evaluated on the whole graph. ", "page_idx": 6}, {"type": "text", "text": "Baselines. Our methods are evaluated against three notable baselines: the traditional gradient descent editor (GD), the Editable Neural Network editor (ENN) [10], and editable training for GNNs[14]. 3 The GD editor is a straightforward application using gradient descent on the target loss with respect to the GNNs model parameters until the desired prediction outcome is achieved. ENN adopts a different approach by initially training the GNN parameters for a few steps to prime the model for subsequent edits. After this preparatory phase, ENN, like GD, applies the gradient descent on the parameters of GNN until the correct prediction is attained. EGNN [14] stitches a peer MLP and only trains MLP during model editing. Note that our method is compatible with EGNN, and different GNN architectures integrated with EGNN (e.g., EGNN-GCN, EGNN-GraphSAGE) are treated as distinct architectures. ", "page_idx": 6}, {"type": "text", "text": "Independent, sequential, and batch editing. All independent, sequential, and batch editing processes involve well-trained GNN models using training datasets, with target samples randomly selected multiple times from misclassified instances in the validation dataset. The key differences lie in the base model that needs to be edited. For independent editing, the same well-trained model using the training datasets is edited multiple times. In contrast, for sequential editing, the model is edited iteratively, with each editing step using the previously edited model from the last target sample, incorporating both the training datasets and partial samples from the validation dataset. For batch editing, all batched samples are edited simultaneously in one editing process. 4 ", "page_idx": 6}, {"type": "text", "text": "Evaluation Metrics. Consistent with preceding studies [10, 12, 11], we assess the effectiveness of the various methods using two primary metrics: (1) Accuracy (Acc): We use accuracy for the test dataset to evaluate the effectiveness after model editing. (2) DrawDown (DD): This metric measures the mean absolute difference in test accuracy before and after model editing. A lower drawdown value signifies a superior editor locality. (3) Success Rate (SR): This metric evaluates the proportion of edits in which the editor successfully amends the model\u2019s prediction. Both metrics offer a different perspective on the effectiveness of the editing process. ", "page_idx": 6}, {"type": "text", "text": "Table 2: The results on four large-scale datasets after applying one single edit. \u201cOOM\u201d is the out-of-memory error. The best/second-best results are highlighted in boldface/underlined, respectively. The results for more backbones (e.g., MLP, EGNN-GCN, EGNN-SAGE) are in Appendix D.1. ", "page_idx": 6}, {"type": "table", "img_path": "XY2qrq7cXM/tmp/489b40c99423d5b66801270332e52cb0fb26699a592874fa529171f5f167a919.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "4.2 Experimental Results in the Independent and Sequential Editing Setting ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In many real-world scenarios, well-trained models often produce inaccurate predictions on unseen data. To evaluate the practical effectiveness of editors for independent editing (RQ1), we randomly choose nodes from the validation set that were misclassified during the training. The editor is then applied to rectify the model\u2019s predictions for these misclassified nodes, and we evaluate the drawdown and edit success rate on the test set. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "We edit one random single node 50 times and report the mean and standard deviation results in Tables 1 and 2 for small-scale and large-scale graph datasets, respectively. Our observations are made below: ", "page_idx": 7}, {"type": "image", "img_path": "XY2qrq7cXM/tmp/d13ff5f704de7c031b46a75f16b52ca802821a703cb4643cb1d0e6b92f8d70c0.jpg", "img_caption": ["Figure 2: The test accuracy drawdown in sequential editing setting for GCN and GraphSAGE on various datasets. The units for y-axis are percentages $(\\%)$ . "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "$\\pmb{\\mathrm{\\Sigma}}$ Contrasting model editing on textual data [11, 12, 30], all editors can effectively rectify model predictions in the graph domain. As shown in Table 1, all editors achieve a high success rate (typically from $96\\%\\sim100\\%$ ) after editing GNNs, which is highly different from transformers with below $50\\%$ SR. This finding indicates that GNNs, unlike transformers, can be more easily adjusted to produce correct predictions. However, this improvement comes at the expense of substantial drawdown on other unrelated nodes, underscoring the key challenge of maintaining prediction locality for unrelated nodes before and after editing. ", "page_idx": 7}, {"type": "text", "text": "$\\pmb{\\varphi}$ Our proposed GRE and $G R E+$ notably surpass both GD and ENN in terms of test drawdown. This advantage stems mainly from the rewired gradient based on the pre-stored training loss gradient, which facilitates target sample correction while preserving the training loss. GD and ENN attempt to rectify model predictions by updating the parameters of GNNs without incorporating training loss information. In contrast, GRE and $\\mathrm{GRE+}$ maintain much better test accuracy after model editing. For example, for Amazon-photos, the accuracy drop dwindles from roughly $65.\\dot{0}8\\%$ to around $43.8\\bar{7}\\%$ , a $43.9\\bar{\\%}$ improvement over the baseline. This is due to the gradient rewiring approach that facilitates target sample correction while preserving the training loss. Interestingly, when applied to GNNs, ENN performs markedly worse than the basic editor GD. Moreover, GD performs well in MLP, which is consistent with the low gradient discrepancy of MLP. ", "page_idx": 7}, {"type": "text", "text": "$\\pmb{\\otimes}$ Our proposed GRE and $G R E+$ are compatible with EGNN and further improve the performance. We observe that while GRE occasionally underperforms, $\\mathrm{GRE+}$ consistently shows better performance than GD in reducing accuracy. For instance, when the A-computers dataset is evaluated with EGNNGCN, GRE, and $\\mathrm{GRE+}$ exhibit an average accuracy drop of $4.62\\%$ and $0.51\\%$ , respectively, whereas GD shows a decrease of $0.73\\%$ . Notably, we find that for 7 out of 8 datasets, $\\mathrm{GRE+}$ with EGNNSAGE shows a negative drop in accuracy, meaning that the test accuracy actually increases after model editing. This points towards the superior performance of the EGNN-SAGE model architecture. ", "page_idx": 7}, {"type": "text", "text": "In the sequential editing setting, we select a sequence of nodes from the validation set that were misclassified during the training phase. The editor is then used to iteratively correct the model\u2019s predictions for these sequentially misclassified nodes, and we measure the resulting drawdown and success rate of edits on the test set. ", "page_idx": 7}, {"type": "text", "text": "In Figure 2, we report the test accuracy drawdown in the sequential setting, a more challenging scenario that warrants further investigation. In particular, we plot the test accuracy drawdown compared to GD across various GNN architectures and graph datasets. Our observations are as follows: \u2779The proposed GRE and $G R E+$ consistently outperform GD in the sequential setting. However, the drawdown is significantly higher than in the single edit setting. For instance, $\\mathrm{GRE+}$ exhibits a $43.87\\%$ drawdown for GCN on the A-photo dataset in the single edit setting, which escalates up to a $65\\%$ drawdown in the sequential edit setting. These results also highlight the challenge of maintaining the locality of GNN prediction in sequential editing. $\\pmb{\\mathcal{\\Theta}}$ The improvement of $G R E+$ over GRE is quite limited in the sequential setting. For example, $\\mathrm{GRE+}$ exhibits a $24.52\\%$ drawdown over GRE for GCN on the A-photo dataset in the single edit setting while is on par with GRE in the sequential edit setting. These results further verify the difficulty of sequential editing and indicate more comprehensive training subset selection may be promising. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "image", "img_path": "XY2qrq7cXM/tmp/b0458a8e1d70096d527a5b945aa7ced176a83422192774c5ca3c1aad57820383.jpg", "img_caption": ["Figure 3: The success rate and test accuracy drawdown tradeoff in independent editing setting for GCN and GraphSAGE on various datasets. The trade-off curve close to the top left corner means better trade-off performance. The units for $\\Chi-$ and y-axis are percentages $(\\%)$ . "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "4.3 Trade-off Performance Comparison ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We further compare the trade-off between the accuracy drawdown and the success rate of our method on various GNN architectures and graph datasets. As shown in Figure 3, we plot Pareto front curves by assigning different hyperparameters for the proposed methods. The upper-left corner point represents the ideal performance, i.e., the highest SR and lowest accuracy drawdown. The results show that $\\mathrm{GRE+}$ achieves better trade-off results compared to GRE, and all methods consistently maintain a high success rate on various GNN architectures and graph datasets. ", "page_idx": 8}, {"type": "image", "img_path": "XY2qrq7cXM/tmp/69a685a2bdbe02aff77a8581ebcfad74ba6de956eaca5c670076940ad76b56d6.jpg", "img_caption": ["Figure 4: The hyperparameter study on test accuracy drawdown in independent editing setting w.r.t. $\\lambda$ . "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "4.4 Hyperparameter Study ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this experiment, we investigate the sensitivity of our proposed method w.r.t. $\\lambda$ across a variety of GNN architectures and graph datasets. Specifically, we search for $\\lambda$ from the set of $\\{0.{\\dot{0}},0.1,1.0,10.0,50.0\\}$ . As shown in Figure 4, the test accuracy drop remains relatively stable despite variations in $\\lambda$ , suggesting that meticulous tuning of this parameter may not be crucial. For the ogbn-arxiv dataset, an uptick in accuracy drop corresponds with an increase in $\\lambda$ , reflecting the inherent difficulty of this dataset. Intriguingly, in the case of $\\mathrm{GRE+5}$ with 5 training subsets, the test accuracy drop exceeds that of $\\mathrm{GRE}{+2}$ and $\\mathrm{GRE}{+3}$ , a pattern that diverges from the trend observed in other datasets. ", "page_idx": 8}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we explore the editing of graph neural networks from a new gradient perspective. Through empirical observations, we discover that conventional model editing techniques often underperform due to the gradient discrepancy between the training loss and target loss in GNNs. To address this issue, we propose a gradient rewiring approach. Specifically, we formulate a constrained optimization problem to regulate the model performance during model editing and identify a simple yet effective gradient rewiring approach to explicitly satisfy the constraints. In this way, the proposed approach can correct the target sample while preventing an increase in training loss. Experiments demonstrate the effectiveness of our approach, and our proposed method is also compatible with the existing baseline EGNN and can further improve performance. Future work includes more comprehensive training subset selection in $\\mathrm{GRE+}$ and a tailed approach for editable graph neural networks training in the sequential editing setting. ", "page_idx": 9}, {"type": "text", "text": "6 Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The authors thank the anonymous reviewers for their helpful comments. This work is in part supported by NSF grants NSF IIS-2310260, IIS-2224843, IIS-2450662, IIS-2431515 and IIS-2239257. The views and conclusions contained in this paper are those of the authors and should not be interpreted as representing any funding agencies. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L Hamilton, and Jure Leskovec. Graph convolutional neural networks for web-scale recommender systems. In Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining, pages 974\u2013983, 2018.   \n[2] Hanqing Zeng, Hongkuan Zhou, Ajitesh Srivastava, Rajgopal Kannan, and Viktor Prasanna. Graphsaint: Graph sampling based inductive learning method. In International Conference on Learning Representations, 2020.   \n[3] Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. arXiv preprint arXiv:2005.00687, 2020.   \n[4] Zhimeng Jiang, Xiaotian Han, Chao Fan, Zirui Liu, Na Zou, Ali Mostafavi, and Xia Hu. Fmp: Toward fair graph message passing against topology bias. arXiv preprint arXiv:2202.04187, 2022.   \n[5] Keyu Duan, Zirui Liu, Peihao Wang, Wenqing Zheng, Kaixiong Zhou, Tianlong Chen, Xia Hu, and Zhangyang Wang. A comprehensive study on large-scale graph training: Benchmarking and rethinking. arXiv preprint arXiv:2210.07494, 2022.   \n[6] Xiaotian Han, Zhimeng Jiang, Ninghao Liu, and Xia Hu. G-mixup: Graph data augmentation for graph classification. In International Conference on Machine Learning, pages 8230\u20138248. PMLR, 2022.   \n[7] Hongyi Ling, Zhimeng Jiang, Meng Liu, Shuiwang Ji, and Na Zou. Graph mixup with soft alignments. In International Conference on Machine Learning. PMLR, 2023.   \n[8] Daniele Petrone and Vito Latora. A dynamic approach merging network theory and credit risk techniques to assess systemic risk in financial networks. Scientific Reports, 8(1):5561, 2018.   \n[9] Kai Shu, Amy Sliva, Suhang Wang, Jiliang Tang, and Huan Liu. Fake news detection on social media: A data mining perspective. ACM SIGKDD explorations newsletter, 19(1):22\u201336, 2017.   \n[10] Anton Sinitsin, Vsevolod Plokhotnyuk, Dmitriy Pyrkin, Sergei Popov, and Artem Babenko. Editable neural networks. arXiv preprint arXiv:2004.00345, 2020.   \n[11] Eric Mitchell, Charles Lin, Antoine Bosselut, Chelsea Finn, and Christopher D Manning. Fast model editing at scale. arXiv preprint arXiv:2110.11309, 2021.   \n[12] Eric Mitchell, Charles Lin, Antoine Bosselut, Christopher D Manning, and Chelsea Finn. Memory-based model editing at scale. In International Conference on Machine Learning, pages 15817\u201315831. PMLR, 2022.   \n[13] Nicola De Cao, Wilker Aziz, and Ivan Titov. Editing factual knowledge in language models. arXiv preprint arXiv:2104.08164, 2021.   \n[14] Zirui Liu, Zhimeng Jiang, Shaochen Zhong, Kaixiong Zhou, Li Li, Rui Chen, Soo-Hyun Choi, and Xia Hu. Editable graph neural network for node classifications. arXiv preprint arXiv:2305.15529, 2023.   \n[15] Shaochen Zhong, Duy Le, Zirui Liu, Zhimeng Jiang, Andrew Ye, Jiamu Zhang, Jiayi Yuan, Kaixiong Zhou, Zhaozhuo Xu, Jing Ma, et al. Gnns also deserve editing, and they need it more than once. In International Conference on Machine Learning, 2024.   \n[16] Yezi Liu, Qinggang Zhang, Mengnan Du, Xiao Huang, and Xia Hu. Error detection on knowledge graphs with triple embedding. In 2023 31st European Signal Processing Conference (EUSIPCO), pages 1604\u20131608. IEEE, 2023.   \n[17] Junnan Dong, Qinggang Zhang, Xiao Huang, Keyu Duan, Qiaoyu Tan, and Zhimeng Jiang. Hierarchy-aware multi-hop question answering over knowledge graphs. In Proceedings of the ACM Web Conference 2023, pages 2519\u20132527, 2023.   \n[18] Wei Jin, Lingxiao Zhao, Shichang Zhang, Yozen Liu, Jiliang Tang, and Neil Shah. Graph condensation for graph neural networks. arXiv preprint arXiv:2110.07580, 2021.   \n[19] Yezi Liu and Yanning Shen. Tinygraph: Joint feature and node condensation for graph neural networks. arXiv preprint arXiv:2407.08064, 2024.   \n[20] Qizhang Feng, Zhimeng Stephen Jiang, Ruiquan Li, Yicheng Wang, Na Zou, Jiang Bian, and Xia Hu. Fair graph distillation. Advances in Neural Information Processing Systems, 36:80644\u201380660, 2023.   \n[21] Pei Chen, Hang Yang, Kang Liu, Ruihong Huang, Yubo Chen, Taifeng Wang, and Jun Zhao. Reconstructing event regions for event extraction via graph attention networks. In Kam-Fai Wong, Kevin Knight, and Hua Wu, editors, Proceedings of the 1st Conference of the AsiaPacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing, pages 811\u2013820, Suzhou, China, December 2020. Association for Computational Linguistics.   \n[22] Pei Chen, Haibo Ding, Jun Araki, and Ruihong Huang. Explicitly capturing relations between entity mentions via graph neural networks for domain-specific named entity recognition. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers), pages 735\u2013742, Online, August 2021. Association for Computational Linguistics.   \n[23] Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message passing for quantum chemistry. In International conference on machine learning, pages 1263\u20131272. PMLR, 2017.   \n[24] Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In International Conference on Learning Representations, 2017.   \n[25] William L Hamilton, Rex Ying, and Jure Leskovec. Inductive representation learning on large graphs. In Proceedings of the 31st International Conference on Neural Information Processing Systems, pages 1025\u20131035, 2017.   \n[26] Petar Velic\u02c7kovic\u00b4, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li\u00f2, and Yoshua Bengio. Graph attention networks. In International Conference on Learning Representations, 2018.   \n[27] Jorge Nocedal and Stephen J Wright. Quadratic programming. Numerical optimization, pages 448\u2013492, 2006.   \n[28] Felix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, and Kilian Weinberger. Simplifying graph convolutional networks. In International conference on machine learning, pages 6861\u20136871. PMLR, 2019.   \n[29] Oleksandr Shchur, Maximilian Mumme, Aleksandar Bojchevski, and Stephan G\u00fcnnemann. Pitfalls of graph neural network evaluation. arXiv preprint arXiv:1811.05868, 2018.   \n[30] Zeyu Huang, Yikang Shen, Xiaofeng Zhang, Jie Zhou, Wenge Rong, and Zhang Xiong. Transformer-patcher: One mistake worth one neuron. In The Eleventh International Conference on Learning Representations, 2023.   \n[31] Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina EliassiRad. Collective classification in network data. AI magazine, 29(3):93\u201393, 2008.   \n[32] Mehrdad Farajtabar, Navid Azizan, Alex Mott, and Ang Li. Orthogonal gradient descent for continual learning. In International Conference on Artificial Intelligence and Statistics, pages 3762\u20133773. PMLR, 2020.   \n[33] Gobinda Saha, Isha Garg, and Kaushik Roy. Gradient projection memory for continual learning. In International Conference on Learning Representations, 2021.   \n[34] Cheng Chen, Ji Zhang, Jingkuan Song, and Lianli Gao. Class gradient projection for continual learning. In Proceedings of the 30th ACM International Conference on Multimedia, pages 5575\u20135583, 2022.   \n[35] Christian Simon, Piotr Koniusz, Richard Nock, and Mehrtash Harandi. On modulating the gradient for meta-learning. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part VIII 16, pages 556\u2013572. Springer, 2020.   \n[36] Aravind Rajeswaran, Chelsea Finn, Sham M Kakade, and Sergey Levine. Meta-learning with implicit gradients. Advances in neural information processing systems, 32, 2019.   \n[37] Mikhail Khodak, Maria-Florina F Balcan, and Ameet S Talwalkar. Adaptive gradient-based meta-learning methods. Advances in Neural Information Processing Systems, 32, 2019.   \n[38] Zhekai Du, Jingjing Li, Hongzu Su, Lei Zhu, and Ke Lu. Cross-domain gradient discrepancy minimization for unsupervised domain adaptation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 3937\u20133946, 2021.   \n[39] Zhiqiang Gao, Shufei Zhang, Kaizhu Huang, Qiufeng Wang, and Chaoliang Zhong. Gradient distribution alignment certificates better adversarial domain adaptation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 8937\u20138946, 2021. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Experimental Setting ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A.1 More details on EGNN ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We provide more details on editable graph neural networks (EGNN), a method free of neighborhood propagation, designed specifically for correcting misclassified node predictions [14]. EGNN uniquely integrates a peer MLP (matching in the number of hidden units and layers) with GNNs (such as GCN and GraphSAGE), and trains solely the MLP model using the target loss during model editing. This strategy enables EGNN to leverage the propagation-free advantages of MLPs for model editing. However, it\u2019s important to note that EGNN is incompatible with EGNN, as the model editing in EGNN refines a stitched MLP, which is not employed during model training. Our proposed methodologies, GRE and $\\mathrm{GRE+}$ , are founded on gradient rewiring, which is orthogonal to the EGNN approach. In the appendix, we illustrate how our proposed methodologies can further augment the effectiveness of the EGNN approach and MLP models. ", "page_idx": 13}, {"type": "table", "img_path": "XY2qrq7cXM/tmp/a85a33ed7769e1545ad5a80b60b520e6053d26e3a5d6084f2c91df2c1307e805.jpg", "table_caption": ["Table 3: Statistics information for datasets used for node classification. "], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "A.2 Datasets ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "The statistical information of all datasets is summarized in Table 3. The details of the datasets utilized for node classification are described as follows: ", "page_idx": 13}, {"type": "text", "text": "\u2022 Cora [31]: This citation network comprises 2,708 publications interconnected by 5,429 links. Each publication is characterized by a 1,433-dimensional binary vector that signifies the presence or absence of specific words from a predetermined vocabulary. ", "page_idx": 13}, {"type": "text", "text": "\u2022 A-computers [29]: This dataset is a segment of the Amazon co-purchase graph. In this network, nodes denote goods, and GREes represent frequent co-purchases of two goods. Node features are encoded as bag-of-words product reviews. ", "page_idx": 13}, {"type": "text", "text": "\u2022 A-photo [29]: Similar to A-computers, this is another segment of the Amazon co-purchase graph. Node features are also bag-of-words encoded product reviews. ", "page_idx": 13}, {"type": "text", "text": "\u2022 Coauthor-CS [29]: Derived from the Microsoft Academic Graph from the KDD Cup 2016 challenge 3, this co-authorship graph has nodes representing authors who are linked if they have co-authored a paper. Node features denote paper keywords for each author\u2019s publications, while class labels indicate an author\u2019s most active fields of study. ", "page_idx": 13}, {"type": "text", "text": "\u2022 Reddit [25]: This dataset is formulated from Reddit posts, with each node representing a post associated with different communities. ", "page_idx": 13}, {"type": "text", "text": "\u2022 ogbn-arxiv [3]: This dataset represents the citation network among all arXiv papers. Each node denotes a paper, and each GREe signifies a citation between two papers. Node features are generated from the average 128-dimensional word vector of each paper\u2019s title and abstract. ", "page_idx": 13}, {"type": "text", "text": "\u2022 ogbn-products [3]: This is an Amazon product co-purchasing network, where nodes represent Amazon products and GREes denote co-purchases of two products. Node features are created from low-dimensional representations of product description text. ", "page_idx": 13}, {"type": "text", "text": "A.3 Implementation Details ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "The hyperparameters for model architecture, learning rate, dropout rate, and training epochs are shown in Table 4. For EGNN, we also adopt GNNs and MLPs with hyperparameters in Table 4. For GRE, we use the hyperparameters $\\gamma=\\{0.0,0.1,1.0,10.0,50.0\\}$ . For $\\mathrm{GRE+}$ , we also select hyperparameters $\\gamma=\\{0.0,0.1,1.0,10.0,50.0\\}$ and $K=\\{1,2,3,5\\}$ . As for QP problem Eq. (11), we use a standard package qpsolvers with version 3.4.0 to tackle this QP problem with ecos solver. ", "page_idx": 13}, {"type": "table", "img_path": "XY2qrq7cXM/tmp/ebb1e55457eebfede8103e43a1f76ba06cddfd1dff7d520663e79808aef577a8.jpg", "table_caption": ["Table 4: Training hyperparameters configurations in the experiments "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "A.4 Running Environment ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "For hardware configuration, all experiments are executed on a server with 251GB main memory, 24 AMD EPYC 7282 16-core processor CPUs, and a single NVIDIA GeForce-RTX 3090 (24GB). For software configuration, we use ${\\mathrm{CUDA}}{=}11.3.1$ , python $=\\!3.8.0$ , pytorch $1{=}1.12.1$ , higher $=\\!0.2.1$ , torch-geometric $=1.7.2$ , torch-sparse $=0.6.16$ in the software environment. Additionally, we use the package of higher in https://github.com/eric-mitchell/mend for ENN implementation. ", "page_idx": 14}, {"type": "text", "text": "B Limitations and Discussions ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "While our proposed GRE and $\\mathrm{GRE+}$ methods effectively mitigate the accuracy dropdown compared to conventional gradient descent algorithms, the success of our approaches is largely contingent on the precision of the pre-stored gradient for training loss. Despite the relatively few required model edit steps for single node editing, the accuracy of the pre-stored gradient may not sustain over long-term model editing, as the pre-stored gradient for training loss could exhibit significant discrepancy from the gradient of training loss for the edited model. To address such discrepancy, a straightforward strategy could involve leveraging critical training samples to estimate the true gradient of training loss for the edited model. Another possible direction is to identify critical samples instead of random samples for $\\mathrm{GRE+}$ with the aim of further constraining the model\u2019s behavior before and after model editing. ", "page_idx": 14}, {"type": "text", "text": "Notice that the proposed gradient rewiring method is not inherently specific to graphs, the gradient rewiring method is particularly suitable in the graph domain due to the small model size. Specifically, graph models are typically a few layers and thus are smaller in model size compared to models (e.g., Transformers) used in NLP and CV tasks. This results in lower computational and storage costs for gradients, making our strategy particularly suitable for the graph domain. Additionally, it is more challenging to edit nodes in a graph due to the inherent propagation process within neighborhoods. Such propagation may lead to significant gradient discrepancies within the graph domain. ", "page_idx": 14}, {"type": "text", "text": "C Algorithms ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We show the algorithms of GRE and $\\mathrm{GRE+}$ during model editing in Algorithm 1 and 2, respectively. ", "page_idx": 14}, {"type": "text", "text": "D More Experimental Results ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this section, we present experimental results to showcase the improved efficacy of our proposed methods, GRE and $\\mathrm{GRE+}$ . These techniques enhance the performance of EGNN, a specifically designed editable graph neural network, across both independent and sequential editing settings. ", "page_idx": 14}, {"type": "text", "text": "Algorithm 1 Gradient Rewiring Editable (GRE) Graph Neural Networks Training ", "page_idx": 15}, {"type": "text", "text": "1: Input: Target samples $\\left(\\mathbf{x}_{\\mathrm{tg}},\\mathbf{y}_{\\mathrm{tg}}\\right)$ , hyperparameter $\\lambda$ , well-trained GNN model $f_{\\theta}(\\cdot)$ , and its   \ncorresponding gradient for the training subgraph.   \n2: Output: Updated GNN model $f_{\\theta^{\\prime}}(\\cdot)$ .   \n3: while $f_{\\theta}(\\mathbf{x}_{\\mathrm{tg}}^{-})\\neq\\mathbf{y}_{\\mathrm{tg}}\\:\\mathbf{d}$ o   \n4: Compute the model gradient $g_{\\mathrm{tg}}$ for the target loss $\\mathcal{L}_{\\mathrm{tg}}$ .   \n5: Rewire the target loss gradient $g_{\\mathrm{tg}}$ by reducing the projection component on $g_{\\mathrm{train}}$ , then scale   \nwith $(1+\\lambda)^{-1}$ :   \n6: $g^{*}=(1+\\lambda)^{-1}\\,(g_{\\mathrm{tg}}-v^{*}g_{\\mathrm{train}}).$ .   \n7: Replace $g_{\\mathrm{tg}}$ with $g^{\\ast}$ and update the model parameters using the optimizer to obtain $\\theta^{\\prime}$ .   \n8: end while ", "page_idx": 15}, {"type": "text", "text": "Algorithm 2 Gradient Rewiring Editable Plus (GRE+) Graph Neural Networks Training ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "1: Input: Target samples $\\left(\\mathbf{x}_{t g},\\mathbf{y}_{t g}\\right)$ , hyperparameters $\\lambda$ , well-trained GNNs model $f_{\\theta}(\\cdot)$ , and its   \ncorresponding model gradient for training subgraph.   \n2: Output: Editable GNNs model $f_{\\theta^{'}}(\\cdot)$ .   \n3: while $f_{\\theta}(\\mathbf{x}_{t g})!=\\mathbf{y}_{t g}$ do   \n4: Compute model gradient $g_{t g}$ for target loss $\\mathcal{L}_{t g}$ .   \n5: Solve QP problem Eq. (11) via standard QP solver package and obtain the optimal dual   \nvariable $\\mathbf{v}^{*}$ .   \n6: Calculate the rewired gradient using $g^{*}=(1+\\lambda)^{-1}(G\\mathbf{v}^{*}+g_{t g})$ .   \n7: Replace $g_{t g}$ with $g^{*}$ and then adopt optimizer to update model parameters as $\\theta^{'}$ .   \n8: end while ", "page_idx": 15}, {"type": "text", "text": "D.1 More results on Model Architectures for Independent Editing ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this section, we conduct a sequence of 50 single-node edits and present the mean and standard deviation results in Tables 5 for large-scale graph datasets. Similarly, GRE occasionally underperforms, and $\\mathrm{GRE+}$ consistently shows better performance than GD with respect to the reduction in accuracy. For instance, when the Reddit dataset is evaluated with EGNN-GCN, GRE and $\\mathrm{GRE+}$ exhibit an average accuracy drop of $1.48\\%$ and $-0.21\\%$ , respectively, whereas GD shows a decrease of $1.28\\%$ . Moreover, $\\mathrm{GRE+}$ with EGNN-SAGE shows a negative drop in accuracy among 6 out of 8 datasets, i.e., the test accuracy actually increases after model editing. ", "page_idx": 15}, {"type": "table", "img_path": "XY2qrq7cXM/tmp/fe4fbe43c6bfd43c9f68ae38fcf1d740fb05999915b45b43609f2a064ccf97d5.jpg", "table_caption": ["Table 5: The results on four large scale datasets after applying one single edit. \u201cOOM\u201d is the out-of-memory error. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "D.2 Experimental Results on other Model Architectures for Sequential Editing Setting ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In the sequential editing setting, we take a sequence of 50 misclassified nodes and use the editor to iteratively correct the model\u2019s predictions for EGNN across different GNN architectures. The test accuracy drop associated with various model editing methods for different graph datasets is reported in Figure 7. Our observations indicate that the proposed GRE and $\\mathrm{GRE+}$ methods consistently outshine GD in this sequential setting. For instance, with the Coauthor-CS dataset and EGNN-GCN, our proposed methods achieve virtually no decrease in accuracy, while GD exhibits a drop of over $7\\%$ . Another compelling observation is that the improvement demonstrated by our methods over GD for EGNN is markedly larger than for GNNs. This suggests potential synergies between optimizer selection and model architecture design. ", "page_idx": 15}, {"type": "table", "img_path": "XY2qrq7cXM/tmp/08b496303d2e27d2242b27e367a2d5dc09762f6e0019369210068e5d04d7d6f2.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "D.3 Trade-off Performance Comparison on other Model Architectures ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We extend our evaluation by comparing the trade-off between the accuracy drop and success rate of our method on EGNN across various graph datasets. By adjusting different hyperparameters for the proposed methods, we construct Pareto front curves as shown in Figure 5. The results underscore that both $\\mathrm{GRE+}$ and GRE outperform GD in achieving superior trade-off outcomes. Importantly, our proposed methods exhibit robust preservation of the success rate across various GNN architectures and graph datasets. ", "page_idx": 16}, {"type": "text", "text": "D.4 More Experimental results on Batch Editing ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this section, we present the experimental results of applying batch editing on four small-scale datasets (Table 6) and four large-scale datasets (Table 7). ", "page_idx": 16}, {"type": "text", "text": "For the small-scale datasets, our proposed methods, GRE and $\\mathrm{GRE+}$ , consistently outperform the baseline methods. For example, on the Cora dataset, $\\mathrm{GRE+}$ achieves the highest accuracy with a minimal drawdown and a high success rate. Specifically, $\\mathrm{GRE+}$ can reduce $56.9\\%$ and $\\mathrm{{30.3\\%}}$ drawdown compared with 2nd best baseline in GCN and GraphSAGE architectures, respectively. For the large-scale datasets, $\\mathrm{GRE+}$ again demonstrates superior performance. On ogbn-products datasets $\\mathrm{GRE+}$ can reduce $2.5\\%$ and $0.5\\%$ drawdown compared with 2nd best baseline GRE in GCN and GraphSAGE architectures, respectively, while maintaining a high success rate. ", "page_idx": 16}, {"type": "text", "text": "Table 6: The results on four small-scale datasets after applying batch edit. SR is the edit success rate, Acc is the test accuracy after editing, and DD are the test drawdown, respectively. The best/second-best results are highlighted in boldface/underlined, respectively. ", "page_idx": 16}, {"type": "table", "img_path": "XY2qrq7cXM/tmp/20a689db8af506c0c2138b861dde74dec716cf8cbd47bacdee028c5023ce0e9a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "table", "img_path": "XY2qrq7cXM/tmp/1b95f21b6b541f55da7b15fca9fa431ce8f60ddb39693edee2af43ef3e6a2b83.jpg", "table_caption": ["Table 7: The results on four large-scale datasets after applying batch edit. \u201cOOM\u201d is the out-of-memory error. The best/second-best results are highlighted in boldface/underlined, respectively. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "D.5 The Edit Time and Memory Comparison for Editing Methods ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In this section, we present the experimental results of the edit time and memory required for editing across four large-scale datasets (Table 8). ", "page_idx": 17}, {"type": "text", "text": "We observe that $\\mathrm{GRE+}$ takes $1.5\\;2.5\\times$ wall-clock editing time than the GD/GRE editor in terms of the wall-clock edit time. This is because $\\mathrm{GRE+}$ requires QP solver to obtain the rewired gradient. In terms of memory consumption, the overall memory overhead is insignificant. For example, $\\mathrm{GRE+}$ (5) requires $17.9\\%$ GPU memory than GD editor in obgn-products dataset and GCN architecture. The reason is that the anchor gradient is required to store in memory and QP solver computation in memory. ", "page_idx": 17}, {"type": "table", "img_path": "XY2qrq7cXM/tmp/ebc3d6bdd0986fc7a2f8e72d9a8fe2480a1f3d0e45c70ebd5db388d85af0a329.jpg", "table_caption": ["Table 8: The edit time and memory required for editing. ET (ms) and PM (MB) represent the edit time in milliseconds and peak memory in megabytes, respectively. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "D.6 More Test Accuracy Results on Sequential Editing ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In this subsection, we present the after-editing test accuracy results of applying sequential editing on various datasets for both GCN and GraphSAGE models in Figure 6. The test accuracy is reported as a percentage for each dataset. ", "page_idx": 17}, {"type": "text", "text": "Overall, our proposed methods, GRE and $\\mathrm{GRE+}$ , consistently outperform the baseline methods GD and ENN across all datasets in terms of test accuracy. For example, on the Reddit dataset, the proposed methods can achieve more than $100\\%$ improvement over GD and ENN in terms of accuracy. Besides, compared with GRE and $\\mathrm{GRE+}$ , the improvement is marginal on most occasions except A-computer dataset in GraphSAGE, which indicates the limited effectiveness of the fine-grained gradient rewiring in $\\mathrm{GRE+}$ . ", "page_idx": 17}, {"type": "text", "text": "E More Related Work ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Gradient-based method for other tasks. The existing literature on gradient modification mainly incorporates continual learning and meta learning. In continual learning, work [32] proposes gradient projection methods to update the model with gradients in the orthogonal directions of old tasks, without access to old task data. GPM [33] identifies the bases of these subspaces by examining network representations after learning each task using Singular Value Decomposition (SVD) in a single-shot manner and stores them in memory as gradient projection memory. Class gradient projection is proposed in [34] to address the class deviation in gradient projection. In meta-learning, work [35] proposes a meta-learning algorithm to learn to modulate the gradient in the absence of abundant data. The implicit model-agnostic meta-learning (iMAML) algorithm is developed in [36] for optimization-based meta-learning with deep neural networks that remove the need for differentiating through the optimization path. [37] provides a theoretical framework for designing and understanding practical meta-learning methods that integrate sophisticated formalizations of task-similarity. ", "page_idx": 17}, {"type": "image", "img_path": "XY2qrq7cXM/tmp/f093ad6e5e25aa14ae07614b3e64fa42fb563b5b6cf7e78679ffc2bf6d2fa303.jpg", "img_caption": ["Figure 6: The test accuracy in sequential editing setting for GCN and GraphSAGE on various datasets. The units for y-axis are percentages $(\\bar{\\%})$ . "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "F More discussion ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Comparison with Curriculum Learning. Curriculum learning and model editing are two distinct approaches in the field of machine learning. Curriculum learning is an approach where the network is trained in a structured manner, starting with simpler tasks and gradually introducing more complex ones. This method aims to improve the learning process by mimicking how humans learn. Model editing is a fast and efficient approach to patch the well-trained model prediction for several failed test cases. Although both are multi-stage training stages, there are several key differences: (1) Goals: Curriculum Learning aims to improve the overall learning process by structuring the training data in a way that mimics human learning. In contrast, model editing aims to make targeted adjustments to a pre-trained model to correct undesirable behaviors. (2) Approach: curriculum learning mainly focuses on the sequence and complexity of the training data. Model editing typically modifies the model\u2019s parameters or architecture to correct undesirable behavior goals. (3) Additional information in the multi-stage process. Model editing requires failure feedback for well-trained models as the target samples to patch, e.g., test failure cases after production is launched. In other words, such feedback can only be obtained after model pertaining. In curriculum learning, all information is given in multi-stage training. In summary, curriculum learning focuses on structuring the training process to improve overall learning, while model editing focuses on making targeted adjustments to a pre-trained model to correct specific behaviors. Both approaches can be complementary and used together to achieve better model performance. ", "page_idx": 18}, {"type": "text", "text": "Comparison with Domain Adaptation. To the best of our knowledge, many existing methods in domain adaptation (DA) [38, 39] integrate source and target gradients in the loss function. For example, [38] aims to minimize the gradient discrepancy for unsupervised DA, and [39] aligns gradient distribution for better adversarial DA. However, these methods can not be applied in graph model editing since (1) gradient discrepancy is required to successfully edit model prediction; (2) model editing collapses (i.e., no gradient discrepancy) at the initial stage; (3) regulating gradient behavior is insufficient for model editing task since the main problem is how to get an edited model instead of cross-domain generalization. To this end, we rewire the gradient before the model parameters update, and derive a closed-form, instead of a learning-based, gradient rewiring method to accelerate model editing. ", "page_idx": 18}, {"type": "image", "img_path": "XY2qrq7cXM/tmp/0b9b30e5692ce3bfcdf95e937ddd772349e1585ce04ed64b7d966abd476f4f60.jpg", "img_caption": ["Figure 7: The test accuracy dropdown in sequential editing setting for EGNN-GCN and EGNN-SAGE on various datasets. The units for the y-axis are percentages $(\\%)$ . "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": ". Claims ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: We list our main claims and contributions as the bullet items at the end of the introduction ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 19}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Justification: Please check Appendix B ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. \u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper. \u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. ", "page_idx": 19}, {"type": "text", "text": "\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 20}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Justification: In this paper, we don\u2019t include theoretical proofs. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 20}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Justification: We provide a clear algorithm in the method section. Besides, we provide implemental details in the experiment section and appendix. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often ", "page_idx": 20}, {"type": "text", "text": "one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. ", "page_idx": 21}, {"type": "text", "text": "\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 21}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We plan to clean up and release the code during the camera-ready stage. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/ guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 21}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We include all implemented details in the appendix. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 22}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: For the experimental results, we include the standard deviation. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 22}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We provide compute resources in Appendix. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 22}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We confirm the paper meets the NeurIPS Code of Ethics in every respect. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 23}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 23}, {"type": "text", "text": "Answer: [No] ", "page_idx": 23}, {"type": "text", "text": "Justification: This paper has no positive societal impacts or negative societal impacts. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 23}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: The paper poses no safeguard risk ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 23}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We follow the license of the existing paper and assets ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 24}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: This paper does not release new assets. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 24}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 24}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 24}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjec Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 25}]