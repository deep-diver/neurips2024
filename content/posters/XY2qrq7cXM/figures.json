[{"figure_path": "XY2qrq7cXM/figures/figures_1_1.jpg", "caption": "Figure 1: (a) Top: RMSE distance between the gradients of cross-entropy loss over training datasets and over the targeted sample for different architectures. (b) Middle: Cross-entropy loss over training datasets when the model is updated using target loss. (c) Bottom: Cross-entropy loss over the targeted sample when the model is updated using target loss.", "description": "This figure shows the results of preliminary experiments that motivate the gradient rewiring method.  The top row displays the RMSE (Root Mean Squared Error) between the gradients of the cross-entropy loss for training nodes and the target node across different model architectures (GCN, GraphSAGE, and MLP). The middle row shows the cross-entropy loss over the training datasets during model editing, and the bottom row shows the cross-entropy loss on the target sample. The results illustrate the significant gradient inconsistency between training and target nodes, highlighting the challenges of directly fine-tuning the base model with the target loss.", "section": "3 Methodology"}, {"figure_path": "XY2qrq7cXM/figures/figures_7_1.jpg", "caption": "Figure 1: (a) Top: RMSE distance between the gradients of cross-entropy loss over training datasets and over the targeted sample for different architectures. (b) Middle: Cross-entropy loss over training datasets when the model is updated using target loss. (c) Bottom: Cross-entropy loss over the targeted sample when the model is updated using target loss.", "description": "This figure shows three subplots visualizing the results of a preliminary experiment on model editing. The top subplot shows the RMSE distance between gradients of cross-entropy loss for training data and the target sample. The middle subplot shows the training loss, and the bottom subplot shows the target loss during model editing for different architectures (GCN, GraphSAGE, and MLP). This experiment highlights inconsistencies in gradients that motivate the proposed gradient rewiring method.", "section": "3.1 Motivation"}, {"figure_path": "XY2qrq7cXM/figures/figures_8_1.jpg", "caption": "Figure 1: (a) Top: RMSE distance between the gradients of cross-entropy loss over training datasets and over the targeted sample for different architectures. (b) Middle: Cross-entropy loss over training datasets when the model is updated using target loss. (c) Bottom: Cross-entropy loss over the targeted sample when the model is updated using target loss.", "description": "This figure displays the results of preliminary experiments that compare the gradients of cross-entropy loss for different models (GCN, GraphSAGE, MLP) over both the training datasets and the target sample.  The top section (a) shows the RMSE (Root Mean Squared Error) distance between these gradients, illustrating their inconsistency.  The middle (b) and bottom (c) sections show the training loss and target loss, respectively, during the model editing process.  The inconsistency in gradients motivates the introduction of the GRE method proposed in the paper.", "section": "3.1 Motivation"}, {"figure_path": "XY2qrq7cXM/figures/figures_8_2.jpg", "caption": "Figure 1: (a) Top: RMSE distance between the gradients of cross-entropy loss over training datasets and over the targeted sample for different architectures. (b) Middle: Cross-entropy loss over training datasets when the model is updated using target loss. (c) Bottom: Cross-entropy loss over the targeted sample when the model is updated using target loss.", "description": "This figure presents a comparison of gradients and loss functions before and after model editing. It demonstrates that there is a large discrepancy between the gradients of cross-entropy loss over training datasets and the target sample, especially for GCN and GraphSAGE.  The middle and bottom plots shows that direct fine-tuning using only target node loss deteriorates the performance on training nodes. This observation motivates the gradient rewiring approach proposed in the paper.", "section": "3.1 Motivation"}, {"figure_path": "XY2qrq7cXM/figures/figures_18_1.jpg", "caption": "Figure 1: (a) Top: RMSE distance between the gradients of cross-entropy loss over training datasets and over the targeted sample for different architectures. (b) Middle: Cross-entropy loss over training datasets when the model is updated using target loss. (c) Bottom: Cross-entropy loss over the targeted sample when the model is updated using target loss.", "description": "This figure presents the results of a preliminary experiment designed to show the gradient discrepancy between training and target nodes in different GNN architectures. The top row shows RMSE (root mean squared error) of the gradient difference between training and target samples.  The middle row shows the training loss and the bottom row shows the target loss.  The experiment demonstrates that there is a significant difference in gradients between training and target nodes, highlighting the challenges of directly fine-tuning the base model using target node loss.", "section": "3.1 Motivation"}, {"figure_path": "XY2qrq7cXM/figures/figures_19_1.jpg", "caption": "Figure 2: The test accuracy drawdown in sequential editing setting for GCN and GraphSAGE on various datasets. The units for y-axis are percentages (%).", "description": "This figure displays the test accuracy drawdown for both GCN and GraphSAGE models across various datasets in a sequential editing setting.  The x-axis represents the datasets (Cora, A-computers, A-photo, Coauthor-CS, Flickr, Reddit, Arxiv), while the y-axis represents the test accuracy drawdown (in percentages).  The figure demonstrates the performance of different model editing approaches (GD, GRE, and GRE+) in maintaining test accuracy across multiple sequential edits. Lower drawdown values indicate better performance in preserving test accuracy during the editing process.", "section": "3.2 Gradient Rewiring Approach"}]