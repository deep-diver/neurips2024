{"references": [{"fullname_first_author": "Sen Na", "paper_title": "Hessian averaging in stochastic Newton methods achieves superlinear convergence", "publication_date": "2022-XX-XX", "reason": "This paper is the most directly related work, proposing a Hessian averaging scheme to achieve superlinear convergence in stochastic Newton methods, which this paper improves upon."}, {"fullname_first_author": "Yu Nesterov", "paper_title": "Accelerating the cubic regularization of Newton's method on convex problems", "publication_date": "2008-XX-XX", "reason": "This paper introduces the cubic regularization of Newton's method, a key second-order method which this paper builds upon and adapts to the stochastic setting."}, {"fullname_first_author": "Renato DC Monteiro", "paper_title": "An accelerated hybrid proximal extragradient method for convex optimization and its implications to second-order methods", "publication_date": "2013-XX-XX", "reason": "This foundational paper introduces the Hybrid Proximal Extragradient (HPE) method, which forms the basis of the novel method proposed in this paper."}, {"fullname_first_author": "Mert Pilanci", "paper_title": "Newton sketch: A near linear-time optimization algorithm with linear-quadratic convergence", "publication_date": "2017-XX-XX", "reason": "This paper introduces the Newton Sketch method, a key technique for efficiently approximating the Hessian in large-scale problems which is relevant to the setting of this paper."}, {"fullname_first_author": "Richard H Byrd", "paper_title": "On the use of stochastic Hessian information in optimization methods for machine learning", "publication_date": "2011-XX-XX", "reason": "This early paper explores the use of stochastic Hessian information in optimization, laying groundwork for many of the later stochastic second-order methods that this paper builds upon."}]}