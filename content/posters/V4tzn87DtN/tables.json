[{"figure_path": "V4tzn87DtN/tables/tables_1_1.jpg", "caption": "Table 1: Comparison between Algorithm 1 and the stochastic Newton method in [1], in terms of how many iterations it takes to transition to each phase, and the convergence rates achieved. We drop constant factors as well as logarithmic dependence and 1/8, and assume 1/poly(\u03ba) < \u03b3 < O(\u03ba).", "description": "This table compares the performance of the proposed Stochastic Newton Proximal Extragradient (SNPE) method (Algorithm 1) with the stochastic Newton method from a previous work [1] in terms of the number of iterations required to reach different convergence phases (linear, initial superlinear, and final superlinear) and the corresponding convergence rates.  It shows that SNPE requires significantly fewer iterations to transition between phases and achieves faster convergence rates, particularly in the initial superlinear phase. The comparison is made for both uniform and non-uniform weight averaging schemes for Hessian estimates.", "section": "3 Stochastic Newton Proximal Extragradient"}, {"figure_path": "V4tzn87DtN/tables/tables_4_1.jpg", "caption": "Table 1: Comparison between Algorithm 1 and the stochastic Newton method in [1], in terms of how many iterations it takes to transition to each phase, and the convergence rates achieved. We drop constant factors as well as logarithmic dependence and 1/8, and assume 1/poly(\u03ba) < Y < O(\u03ba).", "description": "This table compares the number of iterations required to reach different convergence phases (linear, initial superlinear, and final superlinear) for Algorithm 1 (Stochastic NPE) and the stochastic Newton method from a previous work [1].  It shows how the convergence rates differ between the two methods under both uniform and non-uniform Hessian averaging schemes. The table highlights the improved iteration complexity of Algorithm 1, particularly in transitioning to the superlinear convergence phases.", "section": "3 Stochastic Newton Proximal Extragradient"}, {"figure_path": "V4tzn87DtN/tables/tables_8_1.jpg", "caption": "Table 1: Comparison between Algorithm 1 and the stochastic Newton method in [1], in terms of how many iterations it takes to transition to each phase, and the convergence rates achieved. We drop constant factors as well as logarithmic dependence and 1/8, and assume 1/poly(\u03ba) < Y < O(\u03ba).", "description": "The table compares the number of iterations required to reach different convergence phases (linear, initial superlinear, and final superlinear) for Algorithm 1 (the proposed Stochastic Newton Proximal Extragradient method) and the stochastic Newton method from a previous work [1].  It shows how the convergence rates and the number of iterations needed to transition between phases depend on the condition number (\u03ba) and the noise level (Y) for both methods, using different weighting schemes (uniform and non-uniform).", "section": "3 Stochastic Newton Proximal Extragradient"}]