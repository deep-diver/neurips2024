[{"type": "text", "text": "Overcoming the Sim-to-Real Gap: Leveraging Simulation to Learn to Explore for Real-World RL ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Andrew Wagenmaker\u2217 Kevin Huang Liyiming Ke University of California, Berkeley University of Washington University of Washington ", "page_idx": 0}, {"type": "text", "text": "Kevin Jamieson Abhishek Gupta University of Washington University of Washington ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In order to mitigate the sample complexity of real-world reinforcement learning, common practice is to first train a policy in a simulator where samples are cheap, and then deploy this policy in the real world, with the hope that it generalizes effectively. Such direct sim2real transfer is not guaranteed to succeed, however, and in cases where it fails, it is unclear how to best utilize the simulator. In this work, we show that in many regimes, while direct sim2real transfer may fail, we can utilize the simulator to learn a set of exploratory policies which enable efficient exploration in the real world. In particular, in the setting of low-rank MDPs, we show that coupling these exploratory policies with simple, practical approaches\u2014least-squares regression oracles and naive randomized exploration\u2014yields a polynomial sample complexity in the real world, an exponential improvement over direct sim2real transfer, or learning without access to a simulator. To the best of our knowledge, this is the first evidence that simulation transfer yields a provable gain in reinforcement learning in settings where direct sim2real transfer fails. We validate our theoretical results on several realistic robotic simulators and a real-world robotic sim2real task, demonstrating that transferring exploratory policies can yield substantial gains in practice as well. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Over the last decade, reinforcement learning (RL) techniques have been deployed to solve a variety of real-world problems, with applications in robotics, the natural sciences, and beyond [27, 54, 52, 26, 46, 23]. While promising, the broad application of RL methods has been severely limited by its large sample complexity\u2014the number of interactions with the environment required for the algorithm to learn to solve the desired task. In applications of interest, it is often the case that collecting samples is very costly, and the number of samples required by RL algorithms is prohibitively expensive. ", "page_idx": 0}, {"type": "text", "text": "In many domains, while collecting samples in the desired deployment environment may be very costly, we have access to a simulator where the cost of samples is virtually nonexistent. As a concrete example, in robotic applications where the goal is real-world deployment, directly training in the real world typically requires an infeasibly large number of samples. However, it is often possible to obtain a simulator\u2014derived from first principles or knowledge of the robot\u2019s actuation\u2014which provides an approximate model of the real-world deployment environment. Given such a simulator, common practice is to first train a policy to accomplish the desired task in the simulator, and then deploy it in the real world, with the hope that the policy generalizes effectively from the simulator to the goal deployment environment. Indeed, such \u201csim2real\u201d transfer has become a key piece in the application of RL to robotic settings, as well as many other domains of interest such as the natural sciences [12, 15], and is a promising approach towards reducing the sample complexity of RL in real-world deployment [19, 4, 18]. ", "page_idx": 0}, {"type": "image", "img_path": "JjQl8hXJAS/tmp/462ac60685db74c6fe09a248577349cf37112fe7c564712f8f10f771ab09f2b8.jpg", "img_caption": ["Figure 1: Left: Overview of our approach compared to standard sim2real transfer on puck pushing task. Standard sim2real transfer first trains a policy to solve the goal task in sim and then transfers this policy to real. This policy may fail to solve the task in real due to the sim2real gap, and furthermore may not provide sufficient data coverage to successfully learn a policy that does solve the goal task in real. In contrast, our approach trains a set of exploratory policies in sim which achieve high-coverage data when deployed in real, even if they are unable to solve the task 0-shot. This high-coverage data can then be used to successfully learn a policy that solves the goal task in real. Right: Quantitative results running our approach on the puck pushing task illustrated on left, compared to standard sim2real transfer. Over 6 real-world trials, our approach solves the task 6/6 times while standard sim2real transfer solves the task 0/6 times. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Effective sim2real transfer can be challenging, however, as there is often a non-trivial mismatch between the simulated and real environments. The real world is difficult to model perfectly, and some discrepancy is inevitable. As such, directly transferring the policy trained in the simulator to the real world often fails, the mismatch between sim and real causing the policy\u2014which may perfectly solve the task in sim\u2014to never solve the task in real. While some attempts have been made to address this\u2014for example, utilizing domain randomization to extend the space of environments covered by simulator [60, 49], or finetuning the policy learned in sim in the real world [50, 73]\u2014these approaches are not guaranteed to succeed. In settings where such methods fail, can we still utilize a simulator to speed up real-world RL? ", "page_idx": 1}, {"type": "text", "text": "In this work we take steps towards developing principled approaches to sim2real transfer that addresses this question. Our key intuition is that it is often easier to learn to explore than to learn to solve the goal task. While solving the goal task may require very precise actions, collecting high-quality exploratory data can require significantly less precision. For example, successfully solving a complex robotic manipulation task requires a particular sequence of motions, but obtaining a policy that will interact with the object of interest in some way, providing useful exploratory data on its behavior, would require significantly less precision. ", "page_idx": 1}, {"type": "text", "text": "Formally, we show that, in the setting of low-rank MDPs where there is a mismatch in the dynamics between the \u201csim\u201d and \u201creal\u201d environments, even when this mismatch is such that direct sim2real transfer fails, under certain conditions we can still effectively transfer a set of exploratory policies from sim to real. In particular, we demonstrate that access to such exploratory policies, coupled with random exploration and a least-squares regression oracle\u2014which are insufficient for efficient learning on their own, but often still favored in practice due to their simplicity\u2014enable provably efficient learning in real. Our results therefore demonstrate that simulators, when carefully applied, can yield a provable\u2014exponential\u2014gain over both naive sim2real transfer and learning without a simulator, and enable algorithms commonly used in practice to learn efficiently. ", "page_idx": 1}, {"type": "text", "text": "Furthermore, our results motivate a simple, easy-to-implement algorithmic principle: rather than training and transferring a policy that solves the task in the simulator, utilize the simulator to train a set of exploratory policies, and transfer these, coupled with random exploration, to generate high quality exploratory data in real. We show experimentally\u2014through a realistic robotic simulator and real-world sim2real transfer problem on the Franka robot platform\u2014that this principle of transferring exploratory policies from sim to real yields a significant practical gain in sample efficiency, often enabling efficient learning in settings where naive transfer fails completely (see Figure 1). ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Provable Transfer in RL. Perhaps the first theoretical result on transfer in RL is the \u201csimulation lemma\u201d, which transforms a bound on the total-variation distance between the dynamics to a bound on policy value [24, 25, 6, 22]\u2014we argue that we can do significantly better with exploration transfer. More recent work has considered transfer in the setting of block MDPs [34], but requires relatively strong assumptions on the similarity between source and target MDPs, or the meta-RL setting [69], but only consider tabular MDPs, and assume the target MDP is covered by the training distribution. Perhaps most relevant to this work is the work of [36], which presents several lower bounds showing that efficient transfer in RL is not feasible in general. In relation to this work, our work can be seen as providing a set of sufficient conditions that do enable efficient transfer; the lower bounds presented in [36] do not hold in the low-rank MDP setting we consider. Several other works exist, but either consider different types of transfer than what we consider (e.g., observation space mismatch), or only learn a policy that has suboptimality bounded by the sim2real mismatch [37, 56, 58]. Another somewhat tangential line of work considers representation transfer in RL, where it is assumed the source and target tasks share a common representation [35, 10, 2]. We remark as well that the formal sim2real setting we consider is a special case of the MF-MDP setting of [53]. ", "page_idx": 2}, {"type": "text", "text": "Simulators and Low-Rank MDPs. Several existing works show that there are provable benefits to training a policy in \u201csimulation\u201d due to the ability to reset on command [67, 33, 5, 68, 70, 42]. These works do not consider the transfer problem, however. The setting of linear and low-rank MDPs which we consider has seen a significant amount of attention over the last several years, and many provably efficient algorithms exist [21, 1, 62, 63, 43, 41]. These works typically assume access to powerful oracles which enable efficient learning; we only consider access to a simple regression oracle. Beyond the theory literature, recent work has also shown that low-rank MDPs can effectively model a variety of standard RL settings in practice [72]. ", "page_idx": 2}, {"type": "text", "text": "Sim2Real Transfer in Practice. The sim2real literature is vast and we only highlight particularly relevant works here; see [74] for a full survey. To mitigate the inconsistency between the simulator and real world\u2019s physical parameters and modeling, domain randomization creates a variety of simulated environments with randomized properties to develop a robust policy [60, 49, 44, 8, 39]. Domain adaptation instead constructs encoding of deployment conditions (e.g., physical condition or past histories) and adapts to the deployment environment by matching the encoding [29, 9, 66, 55, 38, 40]. In contrast, our work assumes a fundamental sim2real mismatch where we do not expect the real system to match the simulator for any parameter settings. A related line of work shows that policies trained with robust exploration strategies generalize better to disturbed or unseen environments [13, 20]. Our work is complimentary to this work in that our goal is not to transfer a policy that solves the task in new environment, but rather explores the environment. ", "page_idx": 2}, {"type": "text", "text": "3 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We let $\\triangle_{\\mathcal{X}}$ denote the set of distributions over set $\\mathcal{X}$ $\\langle\\sp{\\prime},[H]:=\\{1,2,\\ldots,H\\}$ , and $\\|P-Q\\|_{\\mathrm{TV}}$ the total-variation distance between distributions $P$ and $Q$ . ", "page_idx": 2}, {"type": "text", "text": "Markov Decision Processes. We consider the setting of episodic Markov Decision Processes (MDPs). An MDP is denoted by a tuple $\\mathcal{M}=(\\mathcal{S},\\mathcal{A},\\{\\bar{P}_{h}\\}_{h=1}^{H},\\{r_{h}\\}_{h=1}^{H},s_{1},H)$ , where $\\boldsymbol{S}$ denotes the set of states, $\\boldsymbol{\\mathcal{A}}$ the set of actions, $P_{h}:S\\times A\\to\\triangle_{S}$ the transition function, $r_{h}:S\\times A\\to[0,1]$ the reward (which we assume is deterministic and known), $s_{1}$ the initial state, and $H$ the horizon. We assume $\\boldsymbol{\\mathcal{A}}$ is finite and denote $A:=|A|$ . Interaction with an MDP starts from state $s_{1}$ , the agent takes some action $a_{1}$ , transitions to state $s_{2}\\sim P_{1}(\\cdot\\mid s_{1},a_{1})$ , and receives reward $r_{1}(s_{1},a_{1})$ . This process continues for $H$ steps at which points the episode terminates, and the process resets. ", "page_idx": 2}, {"type": "text", "text": "The goal of the learner is to find a policy $\\pi=\\{\\pi_{h}\\}_{h=1}^{H}$ , $\\pi_{h}:{\\mathcal{S}}\\to\\triangle_{{\\mathcal{A}}}$ , that achieves maximum reward. We can quantify the reward received by some policy $\\pi$ in terms of the value and $Q$ -value functions. The $Q$ -value function is defined as $\\begin{array}{r}{Q_{h}^{\\pi}(s,a):=\\mathbb{E}^{\\pi}[\\sum_{h^{\\prime}=h}^{H}r_{h^{\\prime}}(s_{h^{\\prime}},a_{h^{\\prime}})\\mid s_{h}=s,a_{h}=}\\end{array}$ , and value function is defined in terms of the $Q$ -value function as $V_{h}^{\\pi}(s):=\\mathbb{E}_{a\\sim\\pi_{h}(\\cdot|s)}[Q_{h}^{\\pi}(s,a)]$ . The value of policy $\\pi$ , its expected reward, is denoted by $V_{0}^{\\pi}:=V_{1}^{\\pi}(s_{1})$ , and the value of the optimal policy, the maximum achievable reward, by $V_{0}^{\\star}:=\\operatorname*{sup}_{\\pi}V_{0}^{\\pi}$ . ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "In this work we are interested in the setting where we wish to solve some task in the \u201creal\u201d environment, represented as an MDP, and we have access to a simulator which approximates the real environment in some sense. We denote the real MDP as $\\mathcal{M}^{\\tt r e a l}$ , and the simulator as $\\mathcal{M}^{\\mathsf{s i m}}$ . We assume that $\\mathcal{M}^{\\tt r e a l}$ and $\\mathcal{M}^{\\mathsf{s i m}}$ have the same state and actions spaces, reward function, and initial state, but different transition functions, $P^{\\mathsf{r e a l}}$ and $P^{\\mathsf{s i m}}$ . We denote value functions in $\\mathcal{M}^{\\tt r e a l}$ and $\\mathcal{M}^{\\sf s i m}$ as $V_{h}^{\\mathsf{r e a l},\\pi}(s)$ and $V_{h}^{\\mathsf{s i m},\\pi}(s)$ , respectively. We make the following assumption. ", "page_idx": 3}, {"type": "text", "text": "Assumption 1. For all $(s,a,h)\\in S\\times A\\times[H]$ and some $\\epsilon_{\\mathrm{sim}}>0$ , we have: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|P_{h}^{\\mathsf{r e a l}}(\\cdot\\mid s,a)-P_{h}^{\\mathsf{s i m}}(\\cdot\\mid s,a)\\|_{\\mathrm{TV}}\\leq\\epsilon_{\\mathrm{sim}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "We do not assume that the value of $\\epsilon_{\\mathrm{sim}}$ is known, simply that there exists some such $\\epsilon_{\\mathrm{sim}}$ . ", "page_idx": 3}, {"type": "text", "text": "Function Approximation. In order to enable efficient learning, some structure on the MDPs of interest is required. We will assume that $\\mathcal{M}^{\\tt r e a l}$ and $\\mathcal{M}^{\\sf s i m}$ are low-rank MDPs, as defined below. ", "page_idx": 3}, {"type": "text", "text": "Definition 3.1 (Low-Rank MDP). We say an MDP is a low-rank MDP with dimension $d$ if there exists some featurization $\\phi:S\\times\\mathcal{A}\\rightarrow\\mathbb{R}^{\\dot{d}}$ and measure $\\pmb{\\mu}:[H]\\times\\mathcal{S}\\rightarrow\\mathbb{R}^{d}$ such that: ", "page_idx": 3}, {"type": "equation", "text": "$$\nP_{h}(\\cdot\\mid s,a)=\\langle\\phi(s,a),\\mu_{h}(\\cdot)\\rangle,\\quad\\forall s,a,h.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "We assume that $\\|\\phi(s,a)\\|_{2}\\leq1$ for all $(s,a)$ , and for all $\\begin{array}{r}{h,\\||\\pmb{\\mu}_{h}|(S)\\|_{2}=\\|\\int_{s\\in S}|\\mathrm{d}\\pmb{\\mu}_{h}(s)|\\|_{2}\\leq\\sqrt{d}.}\\end{array}$ ", "page_idx": 3}, {"type": "text", "text": "Formally, we make the following assumption on the structure of $\\mathcal{M}^{\\mathsf{s i m}}$ and $\\mathcal{M}^{\\tt r e a l}$ . ", "page_idx": 3}, {"type": "text", "text": "Assumption 2. Both $\\mathcal{M}^{\\mathsf{s i m}}$ and $\\mathcal{M}^{\\tt r e a l}$ satisfy Definition 3.1 with feature maps and measures $(\\phi^{\\mathrm{s}},\\mu^{\\mathrm{s}})$ and $(\\phi^{\\mathsf{r}},\\mu^{\\mathsf{r}})$ , respectively. Furthermore, $\\phi^{\\mathrm{s}}$ is known, but all of $\\pmb{\\mu}^{\\mathsf{s}},\\phi^{\\mathsf{r}}$ , and $\\pmb{\\mu}^{r}$ are unknown. ", "page_idx": 3}, {"type": "text", "text": "In the literature, MDPs satisfying Definition 3.1 but where $\\phi$ is known are typically referred to as \u201clinear\u201d MDPs, while MDPs satisfying Definition 3.1 but with $\\phi$ unknown are typically referred to as \u201clow-rank\u201d MDPs. Given this terminology, we have that $\\mathcal{M}^{\\sf s i m}$ is a linear $\\mathrm{MDP}^{2}$ , while $\\mathcal{M}^{\\tt r e a l}$ is a low-rank MDP. We assume the following reachability condition on $\\mathcal{M}^{\\sf s i m}$ . ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\exists\\lambda_{\\operatorname*{min}}^{\\star}>0\\;w i t h\\operatorname*{min}_{h}\\operatorname*{sup}_{\\pi}\\lambda_{\\operatorname*{min}}(\\mathbb{E}^{{\\mathcal{M}}^{\\sin},\\pi}[\\phi^{s}(s_{h},a_{h})\\phi^{s}(s_{h},a_{h})^{\\top}])\\geq\\lambda_{\\operatorname*{min}}^{\\star}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Assumption 3 posits that each direction in the feature space in our simulator can be activated by some policy, and can be thought of as a measure of how easily each direction can be reached. Similar assumptions have appeared before in the literature on linear and low-rank MDPs [71, 3, 2]. Note that we only require this reachability assumption in $\\mathcal{M}^{\\sf s i m}$ . We also assume we are given access to function classes $\\bar{\\mathcal{F}_{h}}:S\\times\\mathcal{A}\\rightarrow[0,H]$ and let $\\mathcal{F}:=\\mathcal{F}_{1}\\times\\mathcal{F}_{2}\\times...\\times\\mathcal{F}_{H}$ . Since no reward is collected in the $(H+1)\\mathrm{th}$ step we take $f_{H+1}=0$ . For any $f:S\\times A\\rightarrow\\mathbb{R}$ , we let $\\pi_{h}^{f}(s):=\\arg\\operatorname*{max}_{a\\in\\mathcal{A}}f_{h}(s,a)$ We define the Bellman operator on some function $f_{h+1}:S\\times A\\to\\mathbb{R}$ as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{T}f_{h+1}(s,a):=r_{h}(s,a)+\\mathbb{E}_{s^{\\prime}\\sim P_{h}(\\cdot\\vert s,a)}\\lbrack\\operatorname*{max}_{a^{\\prime}}f_{h+1}(s^{\\prime},a^{\\prime})\\rbrack.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "We make the following standard assumption on $\\mathcal{F}$ . ", "page_idx": 3}, {"type": "text", "text": "Assumption 4 (Bellman Completeness). For all $f_{h+1}\\in\\mathcal{F}_{h+1}$ , we have $\\mathcal{T}^{\\mathrm{real}}f_{h+1}$ , $\\mathcal{T}^{\\sin}f_{h+1}\\in\\mathcal{F}_{h}$ , where $\\mathcal{T}^{\\mathrm{real}}$ and $\\mathcal{T}^{\\mathrm{sim}}$ denote the Bellman operators on $\\mathcal{M}^{\\tt r e a l}$ and $\\mathcal{M}^{\\sf s i m}$ , respectively. ", "page_idx": 3}, {"type": "text", "text": "PAC Reinforcement Learning. Our goal is to find a policy $\\widehat{\\pi}$ that achieves maximum reward in $\\mathcal{M}^{\\tt r e a l}$ . Formally, we consider the PAC (Probably-Approximate l y-Correct) RL setting. ", "page_idx": 3}, {"type": "text", "text": "Definition 3.2 (PAC Reinforcement Learning). Given some $\\epsilon>0$ and $\\delta>0$ , with probability at least $1-\\delta$ identify some policy $\\widehat{\\pi}$ such that: $\\bar{V}_{0}^{\\mathsf{r e a l},\\widehat{\\pi}}\\geq V_{0}^{\\mathsf{r e a l},\\star}-\\epsilon$ . ", "page_idx": 3}, {"type": "text", "text": "We will be particularly interested in solving the PAC RL problem with the aid of a simulator, using the minimum number of samples from $\\mathcal{M}^{\\tt r e a l}$ possible, as we will formalize in the following. As we will see, while it is straightforward to achieve this objective using $\\mathcal{M}^{\\sf s i m}$ if $\\epsilon=\\mathcal{O}(\\epsilon_{\\mathrm{sim}})$ , naive transfer methods can fail to achieve this completely if $\\epsilon\\ll\\epsilon_{\\mathrm{sim}}$ . As such, our primary focus will be on developing efficient sim2real methods in this regime. ", "page_idx": 3}, {"type": "text", "text": "4 Theoretical Results ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section we provide our main theoretical results. We first present two negative results: in Section 4.1 showing that \u201cnaive exploration\u201d\u2014utilizing only a least-squares regression oracle and random exploration approaches such as $\\zeta$ -greedy3\u2014is provably inefficient, and in Section 4.2 showing that directly transferring the optimal policy from $\\mathcal{M}^{\\mathsf{s i m}}$ to $\\mathcal{M}^{\\tt r e a l}$ is unable to efficiently obtain a policy with suboptimality better than $\\mathcal{O}(\\epsilon_{\\mathrm{sim}})$ in real. Then in Section 4.3 we present our main positive result, showing that by utilizing the same oracles as in Sections 4.1 and 4.2\u2014a least-squares regression oracle, simulator access, and the ability to take actions randomly\u2014we can efficiently learn an $\\epsilon$ -optimal policy for $\\epsilon\\ll\\epsilon_{\\mathrm{sim}}$ in $\\mathcal{M}^{\\tt r e a l}$ by carefully utilizing the simulator to learn exploration policies. ", "page_idx": 4}, {"type": "text", "text": "4.1 Naive Exploration is Provably Inefficient ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "While a variety of works have developed provably efficient methods for solving PAC RL in low-rank MDPs [1, 62, 43, 41], these works typically either rely on complex computation oracles or carefully directed exploration strategies which are rarely utilized in practice. In contrast, RL methods utilized in practice typically rely on \u201csimple\u201d computation oracles and exploration strategies. Before considering the sim2real setting, we first show that such \u201csimple\u201d strategies are insufficient for efficient PAC RL. To instantiate such strategies, we consider a least-squares regression oracle, often available in practice. ", "page_idx": 4}, {"type": "text", "text": "Oracle 4.1 (Least-Squares Regression Oracle). We assume access to a least-squares regression oracle such that, for any $h$ and dataset $\\begin{array}{r c l}{\\mathfrak{D}}&{=}&{\\{(s^{t},a^{t},y^{t})\\}_{t=1}^{T}}\\end{array}$ , we can compute $\\begin{array}{r}{\\arg\\operatorname*{min}_{f\\in\\mathcal{F}_{h}}\\sum_{t=1}^{T}(f(s^{t},a^{t})-y^{t})^{2}}\\end{array}$ . ", "page_idx": 4}, {"type": "text", "text": "We couple this oracle with \u201cnaive exploration\u201d, which here we use to refer to any method that explores by randomly perturbing the action recommended by the current estimate of the optimal policy. While a variety of instantiations of naive exploration exist (see e.g. [11]), we consider a particularly common formulation, $\\zeta$ -greedy exploration. ", "page_idx": 4}, {"type": "text", "text": "Protocol 4.1 ( $\\zeta$ -Greedy Exploration). Given access to a regression oracle, any $\\zeta\\in[0,1]$ , and time horizon $T$ , consider the following protocol: ", "page_idx": 4}, {"type": "text", "text": "1. Interact with $\\mathcal{M}^{\\tt r e a l}$ for $T$ episodes. At every step of episode $t+1$ , play $\\pi_{h}^{f^{t}}(s)$ with probability $1-\\zeta$ , and $a\\sim\\operatorname{unif}(A)$ otherwise, where: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{f_{h}^{t}=\\arg\\operatorname*{min}_{f\\in\\mathcal{F}_{h}}\\sum_{t^{\\prime}=1}^{t}(f(s_{h}^{t^{\\prime}},a_{h}^{t^{\\prime}})-r_{h}^{t^{\\prime}}-\\operatorname*{max}_{a^{\\prime}}f_{h+1}^{t}(s_{h+1}^{t^{\\prime}},a^{\\prime}))^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "2. Using collected data in any way desired, propose a policy $\\widehat{\\pi}$ . ", "page_idx": 4}, {"type": "text", "text": "Protocol 4.1 forms the backbone of many algorithms used in practice. Despite its common application, as existing work [11] and the following result show, it is provably inefficient. ", "page_idx": 4}, {"type": "text", "text": "Proposition 1. For any $H>1$ , $\\zeta\\in[0,1]$ , and $c\\leq1/6$ , there exist some $\\mathcal{M}^{\\tt r e a l,1}$ and $\\mathcal{M}^{\\tt r e a l,2}$ such that both $\\mathcal{M}^{\\tt r e a l,1}$ and $\\mathcal{M}^{\\tt r e a l,2}$ satisfy Assumptions 2 and 4, and unless $T\\geq\\Omega(2^{H/2})$ , when running Protocol 4.1 we have: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\operatorname*{sup}_{\\mathcal{M}^{\\mathrm{real}}\\in\\{\\mathcal{M}^{\\mathrm{real},1},\\mathcal{M}^{\\mathrm{real},2}\\}}\\mathbb{E}^{\\mathcal{M}^{\\mathrm{real}}}[V_{0}^{\\mathcal{M}^{\\mathrm{real}},\\star}-V_{0}^{\\mathcal{M}^{\\mathrm{real}},\\widehat{\\pi}}]\\ge c/32.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Proposition 1 shows that, in a minimax sense, $\\zeta$ -greedy exploration is insufficient for provably efficient reinforcement learning: on one of $\\mathcal{M}^{\\tt r e a l,1}$ and $\\mathcal{M}^{\\tt r e a l,2}$ , $\\zeta$ -greedy exploration will only be able to find a policy that is suboptimal by a constant factor, unless we take an exponentially large number of samples. While we focus on $\\zeta$ -greedy exploration in Proposition 1, this result extends to other types of naive exploration, for example, those given in [11]. See Section 5.2 for further discussion of the construction for Proposition 1. ", "page_idx": 4}, {"type": "text", "text": "4.2 Understanding the Limits of Direct sim2real Transfer ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Proposition 1 shows that in general utilizing a least-squares regression oracle with $\\zeta$ -greedy exploration is insufficient for provably efficient RL. Can this be made efficient with access to a simulator $\\mathcal{M}^{\\mathsf{s i m\\,\\gamma}}$ In practice, standard sim2real methodology typically trains a policy to accomplish the goal task in $\\mathcal{M}^{\\bar{\\mathsf{s i m}}}$ , and then transfers this policy to $\\mathcal{M}^{\\tt r e a l}$ . We refer to this methodology as direct sim2real transfer. The following canonical result, usually referred to as the \u201csimulation lemma\u201d [24, 25, 6, 22], provides a sufficient guarantee for direct sim2real transfer to succeed under Assumption 1. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Proposition 2 (Simulation Lemma). Let $\\pi^{\\mathsf{s i m,\\star}}$ denote an optimal policy in $\\mathcal{M}^{\\sf s i m}$ . Then under Assumption 1 we have $V_{0}^{\\mathsf{r e a l},\\pi^{\\mathsf{s i m},\\star}}\\geq V_{0}^{\\mathsf{r e a l},\\star}-2H^{2}\\epsilon_{\\mathrm{sim}}.$ . ", "page_idx": 5}, {"type": "text", "text": "Proposition 2 shows that, as long as $\\epsilon\\,\\geq\\,2H^{2}\\epsilon_{\\mathrm{sim}}$ , direct sim2real transfer succeeds in obtaining an $\\epsilon$ -optimal policy in $\\mathcal{M}^{\\tt r e a l}$ . While this justifies direct sim2real transfer in settings where $\\mathcal{M}^{\\sf s i m}$ and $\\bar{\\mathcal{M}}^{\\mathsf{r e a l}}$ are sufficiently close, we next show that given access only to $\\pi^{\\mathsf{s i m,\\star}}$ and a least-squares regression oracle\u2014even when coupled with random exploration\u2014we cannot hope to efficiently obtain a policy with suboptimality less than $\\mathcal{O}(\\epsilon_{\\mathrm{sim}})$ on $\\bar{\\mathcal{M}}^{\\mathsf{r e a l}}$ using naive exploration. To formalize this, we consider the following interaction protocol. ", "page_idx": 5}, {"type": "text", "text": "Protocol 4.2 (Direct sim2real Transfer with Naive Exploration). Given access to $\\pi^{\\mathsf{s i m,\\star}}$ , an optimal policy in $\\mathcal{M}^{\\sf s i m}$ , any $\\zeta\\in[0,1]$ , and time horizon $T$ , consider the following protocol: ", "page_idx": 5}, {"type": "text", "text": "1. Interact with Mreal for T episodes, and at each step h and state s play \u03c0shim,\u22c6(\u00b7 | s) with probability $1-\\zeta$ , and $a\\sim\\operatorname{unif}(A)$ with probability $\\zeta$ . ", "page_idx": 5}, {"type": "text", "text": "2. Using collected data in any way desired, propose a policy $\\widehat{\\pi}$ . ", "page_idx": 5}, {"type": "text", "text": "Protocol 4.2 is a standard instantiation of direct sim2real transfer commonly found in the literature, and couples playing the optimal policy from $\\mathcal{M}^{\\sf s i m}$ with naive exploration. We have the following. ", "page_idx": 5}, {"type": "text", "text": "Proposition 3. With the same choice of $\\mathcal{M}^{\\tt r e a l,1}$ and $\\mathcal{M}^{\\tt r e a l,2}$ as in Proposition $^{\\,l}$ , there exists some $\\mathcal{M}^{\\sf s i m}$ such that both $\\mathcal{M}^{\\tt r e a l,1}$ and $\\mathcal{M}^{\\sf r e a l,\\dot{2}}$ satisfy Assumption 1 with $\\mathcal{M}^{\\sf s i m}$ for $\\epsilon_{\\mathrm{sim}}\\leftarrow c,$ , Assumptions 2 to 4 hold, and unless $T\\geq\\Omega(2^{H})$ when running Protocol 4.2, we have: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{\\mathcal{M}^{\\mathrm{real}}\\in\\{\\mathcal{M}^{\\mathrm{real},1},\\mathcal{M}^{\\mathrm{real},2}\\}}\\mathbb{E}^{\\mathcal{M}^{\\mathrm{real}}}[V_{0}^{\\mathcal{M}^{\\mathrm{real}},\\star}-V_{0}^{\\mathcal{M}^{\\mathrm{real}},\\widehat{\\pi}}]\\ge\\epsilon_{\\mathrm{sim}}/32.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Proposition 3 shows that there exists a setting where there are two possible $\\mathcal{M}^{\\tt r e a l}$ satisfying Assumption 1 with $\\mathcal{M}^{\\sf s i m}$ , and where, using direct policy transfer, unless we interact with $\\mathcal{M}^{\\tt r e a l}$ for exponentially many episodes (in $H$ ), we cannot determine a better than $\\Omega(\\epsilon_{\\mathrm{sim}})$ -optimal policy for the worst-case $\\mathcal{M}^{\\tt r e a l}$ . Together, Propositions 2 and 3 show that, while we can utilize direct sim2real transfer to learn a policy that is $\\bar{\\mathcal{O}}(\\bar{\\epsilon_{\\mathrm{sim}}})$ -optimal in $\\mathcal{M}^{\\tt r e a l}$ , if our goal is to learn an $\\epsilon_{}$ -optimal policy for $\\epsilon\\ll\\epsilon_{\\mathrm{sim}}$ , direct sim2real transfer is unable to efficiently achieve this. ", "page_idx": 5}, {"type": "text", "text": "4.3 Efficient sim2real Transfer via Exploration Policy Transfer ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Does there exist some way to utilize $\\mathcal{M}^{\\sf s i m}$ and a least-squares regression oracle to enable efficient learning in $\\mathcal{M}^{\\tt r e a l}$ , even when $\\epsilon\\ll{\\epsilon_{\\mathrm{sim}}}?$ Our key insight is that, rather than transferring the policy that optimally solves the task in $\\mathcal{M}^{\\mathsf{s i m}}$ , we should instead transfer policies that explore effectively in $\\dot{\\mathcal{M}}^{\\mathsf{s i m}}$ . While learning to solve a task may require very precise actions, we can often obtain sufficiently rich data with relatively imprecise actions\u2014it is easier to learn to explore than learn to solve a task. In such settings, directly transferring a policy to solve the task will likely fail due to imprecision in the simulator, but it may be possible to still transfer a policy that generates exploratory data. To formalize this, we consider the following access model to $\\mathcal{M}^{\\mathsf{s i m}}$ . ", "page_idx": 5}, {"type": "text", "text": "Oracle 4.2 ( $\\mathbf{\\mathcal{M}^{\\sin}}$ Access). We may interact with $\\mathcal{M}^{\\mathsf{s i m}}$ by either: ", "page_idx": 5}, {"type": "text", "text": "1. (Trajectory Sampling) For any policy $\\pi$ , sampling a trajectory $\\{(s_{h},a_{h},r_{h},s_{h+1})\\}_{h=1}^{H}$ generated by playing $\\pi$ on . ", "page_idx": 5}, {"type": "text", "text": "2. (Policy Optimization) For any reward $\\widetilde r$ , computing a policy $\\pi^{\\mathsf{s i m}}(\\widetilde{r})$ maximizing $\\widetilde{r}$ on $\\mathcal{M}^{\\mathsf{s i m}}$ . ", "page_idx": 5}, {"type": "text", "text": "While access to such a policy optimization oracle is unrealistic in $\\mathcal{M}^{\\tt r e a l}$ , where we want to minimize the number of samples collected, given cheap access to samples in $\\mathcal{M}^{\\mathsf{s i m}}$ , such an oracle can often be (approximately) implemented in practice4. Note that under Oracle 4.2 we only assume black-box access to our simulator\u2014rather than allowing the behavior of the simulator to be queried at arbitrary states, we are simply allowed to roll out policies on $\\mathcal{M}^{\\mathsf{s i m}}$ , and compute optimal policies. Given Oracle 4.2, as well as our least-squares regression oracle, Oracle 4.1, we propose the following algorithm. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "table", "img_path": "JjQl8hXJAS/tmp/f633b83fed1495560d627e6b6db81848717383a87930e320ab3e72e9c7745d87.jpg", "table_caption": [""], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Algorithm 1 first calls a subroutine LEARNEXPPOLICIES, which learns a set of policies that provide rich data coverage on $\\mathcal{M}^{\\sf s i m}$ \u2014precisely, LEARNEXPPOLICIES returns policies $\\{\\Pi_{\\mathrm{exp}}^{h}\\}_{h\\in[H]}$ which induce covariates with lower-bounded minimum eigenvalue on $\\mathcal{M}^{\\mathsf{s i m}}$ and relies only on Oracle 4.2 (as well as knowledge of the featurization of $\\mathcal{M}^{\\sf s i m},\\phi^{\\sf s})$ to find such policies. Algorithm 1 then plays these exploration policies in $\\mathcal{M}^{\\tt r e a l}$ , coupled with random exploration, and applies the regression oracle to the data they collect. Finally, it estimates the value of the policy learned by the regression oracle and $\\pi^{\\mathsf{s i m,\\star}}$ , and returns whichever is best. We have the following. ", "page_idx": 6}, {"type": "text", "text": "Theorem 1. If Assumptions 1 to 4 hold and ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\epsilon_{\\mathrm{sim}}\\leq\\frac{\\lambda_{\\operatorname*{min}}^{\\star}}{64d H A^{3}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "then as long as ", "page_idx": 6}, {"type": "equation", "text": "$$\nT\\geq c\\cdot\\frac{d^{2}H^{16}}{\\epsilon^{8}}\\cdot\\log\\frac{H|\\mathcal{F}|}{\\delta},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "with probability at least $1-\\delta$ , Algorithm $^{\\,l}$ returns a policy $\\widehat{\\pi}$ such that $V_{0}^{\\mathsf{r e a l,\\star}}-V_{0}^{\\mathsf{r e a l,\\widehat{\\pi}}}\\leq\\epsilon$ , and Oracles 4.1 and 4.2 are invoked at most poly $\\left(d,H,\\epsilon^{-1},\\log\\textstyle{\\frac{1}{\\delta}}\\right)$ ) times. ", "page_idx": 6}, {"type": "text", "text": "Theorem 1 shows that, as long as $\\epsilon_{\\mathrm{sim}}$ satisfies (4.1), utilizing a simulator and least-squares regression oracle, Oracles 4.1 and 4.2, allows for efficient learning in $\\mathcal{M}^{\\tt r e a l}$ , achieving a complexity scaling polynomially in problem parameters. This yields an exponential improvement over learning without a simulator using naive exploration or direct sim2real transfer\u2014which Propositions 1 and 3 show have complexity scaling exponentially in the horizon\u2014despite utilizing the same practical computation oracles. To the best of our knowledge, this result provides the first theoretical evidence that sim2real transfer can yield provable gains in RL beyond trivial settings where direct transfer succeeds. ", "page_idx": 6}, {"type": "text", "text": "Note that the condition in (4.1) is independent of $\\epsilon$ \u2014unlike direct sim2real transfer, which requires $\\epsilon=\\mathcal{O}(\\epsilon_{\\mathrm{sim}})$ , we simply must assume $\\epsilon_{\\mathrm{sim}}$ is small enough that (4.1) holds, and Theorem 1 shows that we can efficiently learn an $\\epsilon_{}$ -optimal policy in $\\mathcal{M}^{\\tt r e a l}$ for any $\\epsilon>0$ . In Appendix B.4, we also present an extended version of Theorem 1, Theorem 3, which utilizes data from $\\mathcal{M}^{\\sf s i m}$ to reduce the dependence on $\\log\\left|{\\mathcal{F}}\\right|$ . In particular, instead of scaling with $\\log\\left|{\\mathcal{F}}\\right|$ , it only scales with the log-cardinality of functions that are (approximately) Bellman-consistent on $\\mathcal{M}^{\\sf s i m}$ . To illustrate the effectiveness of Theorem 1, we return to the instance of Propositions 1 and 3, where naive exploration and direct sim2real transfer fails. We have the following. ", "page_idx": 6}, {"type": "text", "text": "Proposition 4. In the setting of Propositions $^{\\,l}$ and 3 and assuming that $\\begin{array}{r}{\\epsilon_{\\mathrm{sim}}\\leq\\frac{1}{8192}\\cdot\\frac{1}{H}}\\end{array}$ , running Algorithm 1 will require p $\\mathrm{{oly}}(H,\\epsilon^{-1})\\cdot\\log\\frac{1}{\\delta}$ samples from $\\mathcal{M}^{\\tt r e a l}$ in order to identify an $\\epsilon$ -optimal policy in $\\mathcal{M}^{\\tt r e a l}$ with probability at least $1-\\delta$ , for any $\\epsilon>0$ . ", "page_idx": 6}, {"type": "text", "text": "Note that the condition required by Proposition 4 is simply that $\\epsilon_{\\mathrm{sim}}\\lesssim1/H$ \u2014as long as our simulator satisfies this condition, we can efficiently transfer exploration policies to learn an $\\epsilon$ -optimal policy, for any $\\epsilon>0$ , while naive methods would be limited to only obtaining an $\\Omega(1/H)$ -optimal policy. ", "page_idx": 7}, {"type": "text", "text": "bRye lmeaarrnki n4.g1  p(oNlieccieesss $\\bar{\\Pi}_{\\mathrm{exp}}^{h}$ iRna $\\mathcal{M}^{\\mathsf{s i m}}$ Ethxaptl osrpaatino tnh).e  fAelagtourriet hsmp a1c ea cohfi $\\mathcal{M}^{\\sf s i m}$ f(fLiciiennet  2e)x, palnorda ttihoenn i np $\\mathcal{M}^{\\tt r e a l}$ these policies in $\\mathcal{M}^{\\tt r e a l}$ , coupled with random exploration (Line 4). This use of random exploration is critical to obtaining Theorem 1. As we show in Proposition 5, if we omit the random exploration, Assumption 1 is not sufficient to guarantee $\\Pi_{\\mathrm{exp}}^{h}$ explores effectively in $\\mathcal{M}^{\\tt r e a l}$ , even when (4.1) holds. ", "page_idx": 7}, {"type": "text", "text": "Remark 4.2 (Computational Efficiency). Algorithm 1, as well as its main subroutine LEARNEXPPOLICIES, relies only on calls to Oracle 4.1 and Oracle 4.2. Thus, assuming we can efficiently implement these oracles, which is often the case in problem settings of interest, Algorithm 1 can be run in a computationally efficient manner. ", "page_idx": 7}, {"type": "text", "text": "5 Practical Algorithm and Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We next validate the effectiveness of our proposal in practice: can a set of diverse exploration policies obtained from simulation improve the efficiency of real-world reinforcement learning? We start by showing that this holds for a simple, didactic, tabular environment in Section 5.2. From here, we consider several more realistic task domains: simulators inspired by real-world robotic manipulation tasks (sim2sim transfer, Section 5.3); and an actual real-world sim2real experiment on a Franka robotic platform (sim2real transfer, Section 5.4). Further details on all experiments, including additional baselines, can be found in Appendix E. Before stating our experimental results, we first provide a practical instantiation of Algorithm 1 that we can apply with real robotic systems and neural network function approximators. ", "page_idx": 7}, {"type": "text", "text": "5.1 Practical Instantiation of Exploration Policy Transfer ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "The key idea behind Algorithm 1 is quite simple: learn a set of exploratory policies in $\\mathcal{M}^{\\mathsf{s i m}}-$ policies which provide rich data coverage in $\\mathcal{M}^{\\mathsf{s i m}}$ \u2014and transfer these policies to $\\mathcal{M}^{\\tt r e a l}$ , coupled with random exploration, using the collected data to determine a near-optimal policy for $\\bar{\\mathcal{M}}^{\\mathsf{r e a l}}$ . Algorithm 1 provides a particular instantiation of this principle, learning exploratory policies in $\\mathcal{M}^{\\sf s i m}$ via the LEARNEXPPOLICIES subroutine, which aims to cover the feature space of $\\mathcal{M}^{\\mathsf{s i m}}$ , and utilizing a least-squares regression oracle to compute an optimal policy given the data collected in $\\mathcal{M}^{\\tt r e a l}$ . In practice, however, other instantiations of this principle are possible by replacing LEARNEXPPOLICIES with any procedure which generates exploratory policies in $\\mathcal{M}^{\\sf s i m}$ , and replacing the regression oracle with any RL algorithm able to learn from off-policy data. We consider a general meta-algorithm instantiating this in Algorithm 2. ", "page_idx": 7}, {"type": "text", "text": "Algorithm 2 Practical sim2real Exploration Policy Transfer Meta-Algorithm ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "1: Input: Simulator $\\mathcal{M}^{\\mathsf{s i m}}$ , real environment $\\mathcal{M}^{\\tt r e a l}$ , simulator budget $T_{\\mathsf{s i m}}$ , real budget $T$ , algorithm to generate exploratory policies in sim ${\\mathfrak{A}}_{\\mathrm{exp}}$ , algorithm to solve policy optimization in real ${\\mathfrak{A}}_{\\mathrm{po}}$ // Learn exploratory policies in $\\mathcal{M}^{\\mathrm{sim}}$   \n2: Run ${\\mathfrak{A}}_{\\mathrm{exp}}$ for $T_{\\mathsf{s i m}}$ steps in $\\mathcal{M}^{\\mathsf{s i m}}$ to generate set of exploratory policies $\\Pi_{\\mathrm{exp}}$ // Deploy exploratory policies in $\\mathcal{M}^{\\tt r e a l}$   \n3: for $t=1,2,\\dots,T/2$ do   \n4: Draw $\\pi_{\\mathrm{exp}}\\sim\\operatorname{unif}(\\Pi_{\\mathrm{exp}})$ , play in $\\mathcal{M}^{\\tt r e a l}$ for one episode, add data to replay buffer of ${\\mathfrak{A}}_{\\mathrm{po}}$   \n5: Run ${\\mathfrak{A}}_{\\mathrm{po}}$ for one episode // optional if ${\\mathfrak{A}}_{\\mathrm{po}}$ learns fully offline ", "page_idx": 7}, {"type": "text", "text": "In practice, ${\\mathfrak{A}}_{\\mathrm{exp}}$ and ${\\mathfrak{A}}_{\\mathrm{po}}$ can be instantiated with a variety of algorithms. For example, we might take ${\\mathfrak{A}}_{\\mathrm{exp}}$ to be an RND [7] or bootstrapped Q-learning-style [45, 31] algorithm, or any unsupervised RL procedure [48, 14, 32, 47], and ${\\mathfrak{A}}_{\\mathrm{po}}$ to be an off-policy policy optimization algorithm such as soft actor-critic (SAC) [16] or implicit $Q$ -learning (IQL) [28]. ", "page_idx": 7}, {"type": "text", "text": "For the following experiments, we instantiate Algorithm 2 by setting ${\\mathfrak{A}}_{\\mathrm{exp}}$ to an algorithm inspired by recent work on inducing diverse behaviors in RL [14, 30], and ${\\mathfrak{A}}_{\\mathrm{po}}$ to SAC. In particular, ${\\mathfrak{A}}_{\\mathrm{exp}}$ simultaneously trains an ensemble of policies $\\Pi_{\\mathrm{exp}}\\;=\\;\\{\\pi_{\\mathrm{exp}}^{i}\\}_{i=1}^{n}$ and a discriminator $d_{\\theta}:\\mathcal{S}\\times$ [n] \u2192R, where d\u03b8 is trained to discriminate between the behaviors of each policy \u03c0iexp, and \u03c0iexp is optimized on a weighting of the true task reward and the exploration reward induced by the discriminator, $\\begin{array}{r}{r_{e}(s,i):=\\log\\frac{\\exp(d_{\\theta}(s,i))}{\\sum_{j\\in[n]}\\exp(d_{\\theta}(s,j))}}\\end{array}$ . As shown in existing work [14, 30], this simple training objective effectively induces diverse behavior with temporally correlated exploration while remaining within the vicinity of the optimal policy, using standard optimization techniques. Note that the particular choice of algorithm is less critical here than abiding by the recipes laid out in the meta-algorithm (Algorithm 2). The particular instantiation that we run for our experiments is detailed in Algorithm 6, along with further details in Appendix E.2. ", "page_idx": 7}, {"type": "image", "img_path": "JjQl8hXJAS/tmp/0c1873687172abb254e5504ea01334f6894c6a79f26dad2955f6f0da6b54a38d.jpg", "img_caption": ["Figure 2: Left: Illustration ofCombination Lock Example. Right: Results on Combination Lock. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "5.2 Didactic Combination Lock Experiment ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We first consider a variant of the construction used to prove Propositions 1 and 3, itself a variant of the classic combination lock instance. We illustrate this instance in Figure 2. Unless noted, all transitions occur with probability 1, and rewards are 0. Here, in $\\mathcal{M}^{\\mathsf{s i m}}$ the optimal policy, $\\pi^{\\mathsf{s i m,\\star}}$ , plays action $a_{2}$ for all steps $h<H-1$ , while in $\\mathcal{M}^{\\tt r e a l}$ , the optimal policy plays action $a_{1}$ at every step. Which policy is optimal is determined by the outgoing transition from $s_{1}$ at the $(H-1)\\mathrm{th}$ step and, as such, to identify the optimal policy, any algorithm must reach $s_{1}$ at the $(H-1)\\mathrm{th}$ step. As $s_{1}$ will only be reached at step $H-1$ by playing $a_{1}$ for $H-1$ consecutive times, any algorithm relying on naive exploration will take exponentially long to identify the optimal policy. Furthermore, playing $\\pi^{\\mathsf{s i m,\\star}}$ coupled with random exploration will similarly take an exponential number of episodes, since $\\pi^{\\mathsf{s i m,\\star}}$ always plays $a_{2}$ . As such, both direct sim2real policy transfer as well as $Q$ -learning with naive exploration (Protocol 4.1) will fail to find the optimal policy in $\\mathcal{M}^{\\tt r e a l}$ . However, if we transfer exploratory policies from $\\mathcal{M}^{\\sf s i m}$ , since $\\mathcal{M}^{\\mathsf{s i m}}$ and $\\mathcal{M}^{\\tt r e a l}$ behave identically up to step $H-1$ , these policies can efficiently traverse $\\mathcal{M}^{\\tt r e a l}$ , reach $s_{1}$ at step $H-1$ , and identify the optimal policy. We compare our approach of exploration policy transfer to these baselines methods and illustrate the performance of each in Figure 2. As this is a simple tabular instance, we implement Algorithm 1 directly here. As Figure 2 shows, the intuition described above leads to real gains in practice\u2014exploration policy transfer quickly identifies the optimal policy, while more naive approach fail completely over the time horizon we considered. ", "page_idx": 8}, {"type": "text", "text": "5.3 Realistic Robotics sim2sim Experiment ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "To test the ability of our proposed method to scale to more complex problems, we next experiment on a sim2sim transfer setting with a realistic robotic simulator. We consider TychoEnv, a simulator of the 7DOF Tycho robotics platform introduced by [73], and shown in Figure 3. We test sim2sim transfer on a reaching task where the goal is to touch a small ball hanging in the air with the tip of the chopstick end effector. The agent perceives the ball and its own end effector pose and outputs a delta in its desired end effector pose as a command. We set $\\mathcal{M}^{\\mathsf{s i m}}$ and $\\mathcal{M}^{\\tt r e a l}$ to be two instances of TychoEnv with slightly different parameters to model real-world sim2real transfer. Precisely, we change the action bounds and control frequency from $\\mathcal{M}^{\\mathsf{s i m}}$ to $\\mathcal{M}^{\\tt r e a l}$ . ", "page_idx": 8}, {"type": "image", "img_path": "JjQl8hXJAS/tmp/225963aa1c5fdb9e53b5cd7380773866cf7ec335e3062eba0d8b80ac0f76f831.jpg", "img_caption": ["Figure 3: TychoEnv Reach Task Setup "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "We aim to compare our approach of exploration policy transfer with direct sim2real policy transfer. To this end, we first train a policy in $\\mathcal{M}^{\\sf s i m}$ that solves the task in $\\mathcal{M}^{\\sf s i m}$ , $\\pi^{\\mathsf{s i m,\\star}}$ , and then utilize this policy in place of $\\Pi_{\\mathrm{exp}}$ in Algorithm 2. We instantiate our approach of exploration policy transfer as outlined above. Our aim in this experiment is to illustrate how the quality of the data provided by direct policy transfer vs. exploration policy transfer affects learning. As such, for both approaches we simply initialize our SAC agent in $\\bar{\\mathcal{M}}^{\\mathsf{r e a l}}$ , ${\\mathfrak{A}}_{\\mathrm{po}}$ , from scratch, and set the reward in $\\bar{\\mathcal{M}}^{\\mathsf{r e a l}}$ to be sparse: the agent only receives a non-zero reward if it successfully touches the ball. For each approach, we repeat the process of training in $\\mathcal{M}^{\\mathsf{s i m}}$ four times, and for each of these run them for two trials in $\\mathcal{M}^{\\tt r e a l}$ . ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "We illustrate our results in Figure 4. As this figure illustrates, direct policy transfer fails to learn completely, while exploration policy transfer successfully solves the task. Investigating the behavior of each method, we find that the policies transferred via exploration policy transfer, while failing to solve the task with perfect accuracy, when coupled with naive exploration are able to successfully make contact with the ball on occasion. This provides sufficiently rich data for SAC to ultimately learn to solve the task. In contrast, direct policy transfer fails to collect any reward when run in $\\mathcal{M}^{\\tt r e a l}$ , and, given the sparse reward nature of the task, SAC is unable to locate any reward and learn.We include an additional sim2sim experiment on the Franka Emika Panda Robot Arm in Appendix E.4. ", "page_idx": 9}, {"type": "image", "img_path": "JjQl8hXJAS/tmp/bb5928ff3b03aec4772e2dcf611692d2f2ab7ab8a7b86ef088c16b11a2621097.jpg", "img_caption": ["Figure 4: Results on sim2sim Transfer in TychoEnv Simulator "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "5.4 Real-World Robotic sim2real Experiment ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Finally, we demonstrate our algorithm for actual sim2real policy transfer for a manipulation task on a real-world Franka Emika Panda robot arm [17] with a parallel gripper. Our task is to push a $75\\mathrm{mm}$ diameter cylindrical \u201cpuck\" from the center to the edge of the surface, as shown in Figure 1, with the arm initialized at random locations. The observed state $s=[\\mathbf{p}_{\\mathrm{ee}},\\mathbf{p}_{\\mathrm{obj}}]\\in\\mathbb{R}^{4}$ consists of the planar Cartesian coordinate of the end effector $\\mathbf{p}_{\\mathrm{ee}}$ along with the center of mass of the puck $\\mathbf{p}_{\\mathrm{{obj}}}$ . Our policy outputs planar end effector position deltas $a=\\Delta\\mathbf{p}_{\\mathrm{ee}}\\in\\mathbb{R}^{2}$ , evaluated at $8\\,\\mathrm{Hz}$ , which are passed into a lower-level joint position PID controller. We use an Intel Realsense D435 depth camera to track the location of the puck. Our reward function is a sum of a success indicator (indicating when the puck has been pushed to the edge of the surface) and terms which give negative reward if the distance from the end effector to the puck, or puck to the goal, are too large (see (E.1)); in particular, a reward greater than 0 indicates success. ", "page_idx": 9}, {"type": "text", "text": "We run the instantiation of Algorithm 2 outlined above. In particular, we train an ensemble of $n=15$ exploration policies, training for 20 million steps in $\\mathcal{M}^{\\sf s i m}$ . In addition, we train a policy that solves the task in $\\bar{\\mathcal{M}}^{\\mathsf{s i m}}$ , $\\pi^{\\mathsf{s i m,\\star}}$ . We use a custom simulator of the arm, where during training the friction of the table is randomized and noise is added to the observations. ", "page_idx": 9}, {"type": "text", "text": "We observe a substantial sim2real gap between our simulator and the real robot, with policies trained in simulation failing to complete the pushing task zero shot in real, even when trained with domain randomization. We compare direct sim2real policy transfer against our method of transferring exploration policies. For direct policy transfer, we simply run SAC to finetune $\\pi^{\\mathsf{s i m,\\star}}$ in the real world, using the current policy to collect data. For exploration policy transfer, we instead utilize $\\Pi_{\\mathrm{exp}}$ , our ensemble of exploration policies, to collect data in the real world. We run this in tandem with an SAC agent, feeding the data from the exploration policies into the SAC agent\u2019s replay buffer. Unlike in Section 5.3, rather than initializing the SAC policy from scratch, we set the initial policy as $\\pi^{\\mathsf{s i m,\\star}}$ , and fine-tune from this on the data collected from playing $\\Pi_{\\mathrm{exp}}$ . See Appendix E.5 for additional details. ", "page_idx": 9}, {"type": "text", "text": "Our results are shown on the right side of Figure 1. Statistics are computed over 6 runs for each method. Direct policy transfer with finetuning is unable to solve the task in real in each of the 6 runs, and converges to a suboptimal solution. However, our method is able to solve the task successfully each time and achieve a substantially higher reward. ", "page_idx": 9}, {"type": "text", "text": "6 Discussion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we have demonstrated that simulators can make naive exploration efficient even in settings where direct sim2real transfer fails, if they are used to train a set of exploration policies. We highlight several limitations of this work, which we believe are interesting future research questions: ", "page_idx": 9}, {"type": "text", "text": "\u2022 Our focus is purely on dynamics shift\u2014where the dynamics of sim and real differ, but the environments are otherwise the same. While dynamics shift is common in many scenarios, other types of shift can exist as well, for example perceptual shift. How can we best handle these types of shift? \u2022 How can we utilize a simulator in sim2real transfer if we can reset it arbitrarily, rather than just allowing for black-box access? Does the ability to reset allow us to improve sample efficiency further? \u2022 Is the reachability condition, Assumption 3, necessary for successful exploration transfer? ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "The work of AW and KJ was partially supported by the NSF through the University of Washington Materials Research Science and Engineering Center, DMR-2308979, and awards CCF 2007036 and CAREER 2141511. The work of LK was partially supported by Toyota Research Institute URP. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Alekh Agarwal, Sham Kakade, Akshay Krishnamurthy, and Wen Sun. Flambe: Structural complexity and representation learning of low rank mdps. Advances in neural information processing systems, 33:20095\u201320107, 2020.   \n[2] Alekh Agarwal, Yuda Song, Wen Sun, Kaiwen Wang, Mengdi Wang, and Xuezhou Zhang. Provable benefits of representational transfer in reinforcement learning. In The Thirty Sixth Annual Conference on Learning Theory, pages 2114\u20132187. PMLR, 2023.   \n[3] Naman Agarwal, Syomantak Chaudhuri, Prateek Jain, Dheeraj Nagaraj, and Praneeth Netrapalli. Online target q-learning with reverse experience replay: Efficiently finding the optimal policy for linear mdps. arXiv preprint arXiv:2110.08440, 2021.   \n[4] Ilge Akkaya, Marcin Andrychowicz, Maciek Chociej, Mateusz Litwin, Bob McGrew, Arthur Petron, Alex Paino, Matthias Plappert, Glenn Powell, Raphael Ribas, et al. Solving rubik\u2019s cube with a robot hand. arXiv preprint arXiv:1910.07113, 2019.   \n[5] Philip Amortila, Nan Jiang, Dhruv Madeka, and Dean P Foster. A few expert queries suffices for sample-efficient rl with resets and linear value approximation. Advances in Neural Information Processing Systems, 35:29637\u201329648, 2022.   \n[6] Ronen I Brafman and Moshe Tennenholtz. R-max-a general polynomial time algorithm for near-optimal reinforcement learning. Journal of Machine Learning Research, 3(Oct):213\u2013231, 2002.   \n[7] Yuri Burda, Harrison Edwards, Amos Storkey, and Oleg Klimov. Exploration by random network distillation. arXiv preprint arXiv:1810.12894, 2018. [8] Yevgen Chebotar, Ankur Handa, Viktor Makoviychuk, Miles Macklin, Jan Issac, Nathan Ratliff, and Dieter Fox. Closing the sim-to-real loop: Adapting simulation randomization with real world experience. In ICRA, 2019.   \n[9] Tao Chen, Megha Tippur, Siyang Wu, Vikash Kumar, Edward Adelson, and Pulkit Agrawal. Visual dexterity: In-hand reorientation of novel and complex object shapes. Science Robotics, 8 (84):eadc9244, 2023.   \n[10] Yuan Cheng, Songtao Feng, Jing Yang, Hong Zhang, and Yingbin Liang. Provable benefit of multitask representation learning in reinforcement learning. Advances in Neural Information Processing Systems, 35:31741\u201331754, 2022.   \n[11] Chris Dann, Yishay Mansour, Mehryar Mohri, Ayush Sekhari, and Karthik Sridharan. Guarantees for epsilon-greedy reinforcement learning with function approximation. In International conference on machine learning, pages 4666\u20134689. PMLR, 2022.   \n[12] Jonas Degrave, Federico Felici, Jonas Buchli, Michael Neunert, Brendan Tracey, Francesco Carpanese, Timo Ewalds, Roland Hafner, Abbas Abdolmaleki, Diego de Las Casas, et al. Magnetic control of tokamak plasmas through deep reinforcement learning. Nature, 602(7897): 414\u2013419, 2022.   \n[13] Benjamin Eysenbach and Sergey Levine. Maximum entropy rl (provably) solves some robust rl problems. arXiv preprint arXiv:2103.06257, 2021.   \n[14] Benjamin Eysenbach, Abhishek Gupta, Julian Ibarz, and Sergey Levine. Diversity is all you need: Learning skills without a reward function. arXiv preprint arXiv:1802.06070, 2018.   \n[15] Raj Ghugare, Santiago Miret, Adriana Hugessen, Mariano Phielipp, and Glen Berseth. Searching for high-value molecules using reinforcement learning and transformers. arXiv preprint arXiv:2310.02902, 2023.   \n[16] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Offpolicy maximum entropy deep reinforcement learning with a stochastic actor. In International conference on machine learning, pages 1861\u20131870. PMLR, 2018.   \n[17] Sami Haddadin, Sven Parusel, Lars Johannsmeier, Saskia Golz, Simon Gabl, Florian Walch, Mohamadreza Sabaghian, Christoph J\u00e4hne, Lukas Hausperger, and Simon Haddadin. The franka emika robot: A reference platform for robotics research and education. IEEE Robotics & Automation Magazine, 29(2):46\u201364, 2022. doi: 10.1109/MRA.2021.3138382.   \n[18] Sebastian H\u00f6fer, Kostas Bekris, Ankur Handa, Juan Camilo Gamboa, Melissa Mozifian, Florian Golemo, Chris Atkeson, Dieter Fox, Ken Goldberg, John Leonard, et al. Sim2real in robotics and automation: Applications and challenges. IEEE transactions on automation science and engineering, 18(2):398\u2013400, 2021.   \n[19] Stephen James, Paul Wohlhart, Mrinal Kalakrishnan, Dmitry Kalashnikov, Alex Irpan, Julian Ibarz, Sergey Levine, Raia Hadsell, and Konstantinos Bousmalis. Sim-to-real via sim-to-sim: Data-efficient robotic grasping via randomized-to-canonical adaptation networks. arxiv e-prints, page. arXiv preprint arXiv:1812.07252, 2018.   \n[20] Yiding Jiang, J Zico Kolter, and Roberta Raileanu. On the importance of exploration for generalization in reinforcement learning. arXiv preprint arXiv:2306.05483, 2023.   \n[21] Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I Jordan. Provably efficient reinforcement learning with linear function approximation. In Conference on Learning Theory, pages 2137\u2013 2143. PMLR, 2020.   \n[22] Sham Kakade, Michael J Kearns, and John Langford. Exploration in metric state spaces. In Proceedings of the 20th International Conference on Machine Learning (ICML-03), pages 306\u2013312, 2003.   \n[23] Elia Kaufmann, Leonard Bauersfeld, Antonio Loquercio, Matthias M\u00fcller, Vladlen Koltun, and Davide Scaramuzza. Champion-level drone racing using deep reinforcement learning. Nature, 620(7976):982\u2013987, 2023.   \n[24] Michael Kearns and Daphne Koller. Efficient reinforcement learning in factored mdps. In IJCAI, volume 16, pages 740\u2013747, 1999.   \n[25] Michael Kearns and Satinder Singh. Near-optimal reinforcement learning in polynomial time. Machine learning, 49:209\u2013232, 2002.   \n[26] B Ravi Kiran, Ibrahim Sobh, Victor Talpaert, Patrick Mannion, Ahmad A Al Sallab, Senthil Yogamani, and Patrick P\u00e9rez. Deep reinforcement learning for autonomous driving: A survey. IEEE Transactions on Intelligent Transportation Systems, 23(6):4909\u20134926, 2021.   \n[27] Jens Kober, J Andrew Bagnell, and Jan Peters. Reinforcement learning in robotics: A survey. The International Journal of Robotics Research, 32(11):1238\u20131274, 2013.   \n[28] Ilya Kostrikov, Ashvin Nair, and Sergey Levine. Offline reinforcement learning with implicit q-learning. arXiv preprint arXiv:2110.06169, 2021.   \n[29] Ashish Kumar, Zipeng Fu, Deepak Pathak, and Jitendra Malik. Rma: Rapid motor adaptation for legged robots. arXiv preprint arXiv:2107.04034, 2021.   \n[30] Saurabh Kumar, Aviral Kumar, Sergey Levine, and Chelsea Finn. One solution is not all you need: Few-shot extrapolation via structured maxent rl. Advances in Neural Information Processing Systems, 33:8198\u20138210, 2020.   \n[31] Kimin Lee, Michael Laskin, Aravind Srinivas, and Pieter Abbeel. Sunrise: A simple unified framework for ensemble learning in deep reinforcement learning. In International Conference on Machine Learning, pages 6131\u20136141. PMLR, 2021.   \n[32] Lisa Lee, Benjamin Eysenbach, Emilio Parisotto, Eric Xing, Sergey Levine, and Ruslan Salakhutdinov. Efficient exploration via state marginal matching. arXiv preprint arXiv:1906.05274, 2019.   \n[33] Gen Li, Yuxin Chen, Yuejie Chi, Yuantao Gu, and Yuting Wei. Sample-efficient reinforcement learning is feasible for linearly realizable mdps with limited revisiting. Advances in Neural Information Processing Systems, 34:16671\u201316685, 2021.   \n[34] Yao Liu, Dipendra Misra, Miro Dud\u00edk, and Robert E Schapire. Provably sample-efficient rl with side information about latent dynamics. Advances in Neural Information Processing Systems, 35:33482\u201333493, 2022.   \n[35] Rui Lu, Gao Huang, and Simon S Du. On the power of multitask representation learning in linear mdp. arXiv preprint arXiv:2106.08053, 2021.   \n[36] Dhruv Malik, Yuanzhi Li, and Pradeep Ravikumar. When is generalizable reinforcement learning tractable? Advances in Neural Information Processing Systems, 34:8032\u20138045, 2021.   \n[37] Timothy A Mann and Yoonsuck Choe. Directed exploration in reinforcement learning with transferred knowledge. In European Workshop on Reinforcement Learning, pages 59\u201376. PMLR, 2013.   \n[38] Gabriel B Margolis, Xiang Fu, Yandong Ji, and Pulkit Agrawal. Learning physically grounded robot vision with active sensing motor policies. In CoRL, 2023.   \n[39] Bhairav Mehta, Manfred Diaz, Florian Golemo, Christopher J Pal, and Liam Paull. Active domain randomization. In CoRL, 2020.   \n[40] Marius Memmel, Andrew Wagenmaker, Chuning Zhu, Patrick Yin, Dieter Fox, and Abhishek Gupta. Asid: Active exploration for system identification in robotic manipulation. arXiv preprint arXiv:2404.12308, 2024.   \n[41] Zak Mhammedi, Adam Block, Dylan J Foster, and Alexander Rakhlin. Efficient model-free exploration in low-rank mdps. Advances in Neural Information Processing Systems, 36, 2024.   \n[42] Zakaria Mhammedi, Dylan J Foster, and Alexander Rakhlin. The power of resets in online reinforcement learning. arXiv preprint arXiv:2404.15417, 2024.   \n[43] Aditya Modi, Jinglin Chen, Akshay Krishnamurthy, Nan Jiang, and Alekh Agarwal. Modelfree representation learning and exploration in low-rank mdps. Journal of Machine Learning Research, 25(6):1\u201376, 2024.   \n[44] Fabio Muratore, Michael Gienger, and Jan Peters. Assessing transferability from simulation to reality for reinforcement learning. IEEE transactions on pattern analysis and machine intelligence, 2019.   \n[45] Ian Osband, Charles Blundell, Alexander Pritzel, and Benjamin Van Roy. Deep exploration via bootstrapped dqn. Advances in neural information processing systems, 29, 2016.   \n[46] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:27730\u201327744, 2022.   \n[47] Seohong Park, Oleh Rybkin, and Sergey Levine. Metra: Scalable unsupervised rl with metricaware abstraction. arXiv preprint arXiv:2310.08887, 2023.   \n[48] Deepak Pathak, Pulkit Agrawal, Alexei A Efros, and Trevor Darrell. Curiosity-driven exploration by self-supervised prediction. In International conference on machine learning, pages 2778\u2013 2787. PMLR, 2017.   \n[49] Xue Bin Peng, Marcin Andrychowicz, Wojciech Zaremba, and Pieter Abbeel. Sim-to-real transfer of robotic control with dynamics randomization. In 2018 IEEE international conference on robotics and automation (ICRA), pages 3803\u20133810. IEEE, 2018.   \n[50] Xue Bin Peng, Erwin Coumans, Tingnan Zhang, Tsang-Wei Lee, Jie Tan, and Sergey Levine. Learning agile robotic locomotion skills by imitating animals. arXiv preprint arXiv:2004.00784, 2020.   \n[51] Antonin Raffin, Ashley Hill, Adam Gleave, Anssi Kanervisto, Maximilian Ernestus, and Noah Dormann. Stable-baselines3: Reliable reinforcement learning implementations. Journal of Machine Learning Research, 22(268):1\u20138, 2021.   \n[52] Aravind Rajeswaran, Vikash Kumar, Abhishek Gupta, Giulia Vezzani, John Schulman, Emanuel Todorov, and Sergey Levine. Learning complex dexterous manipulation with deep reinforcement learning and demonstrations. arXiv preprint arXiv:1709.10087, 2017.   \n[53] FL Silva, Jiachen Yang, Mikel Landajuela, Andre Goncalves, Alexander Ladd, Daniel Faissol, and Brenden Petersen. Toward multi-fidelity reinforcement learning for symbolic optimization. Technical report, Lawrence Livermore National Laboratory (LLNL), Livermore, CA (United States), 2023.   \n[54] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. nature, 529(7587):484\u2013489, 2016.   \n[55] Rohan Sinha, James Harrison, Spencer M Richards, and Marco Pavone. Adaptive robust model predictive control with matched and unmatched uncertainty. In 2022 American Control Conference (ACC), 2022.   \n[56] Yuda Song, Aditi Mavalankar, Wen Sun, and Sicun Gao. Provably efficient model-based policy adaptation. arXiv preprint arXiv:2006.08051, 2020.   \n[57] Yuda Song, Yifei Zhou, Ayush Sekhari, J Andrew Bagnell, Akshay Krishnamurthy, and Wen Sun. Hybrid rl: Using both offline and online data can make rl efficient. arXiv preprint arXiv:2210.06718, 2022.   \n[58] Yanchao Sun, Ruijie Zheng, Xiyao Wang, Andrew Cohen, and Furong Huang. Transfer rl across observation feature spaces via model-based regularization. arXiv preprint arXiv:2201.00248, 2022.   \n[59] Andrea Tirinzoni, Matteo Pirotta, and Alessandro Lazaric. A fully problem-dependent regret lower bound for finite-horizon mdps. arXiv preprint arXiv:2106.13013, 2021.   \n[60] Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, and Pieter Abbeel. Domain randomization for transferring deep neural networks from simulation to the real world. In 2017 IEEE/RSJ international conference on intelligent robots and systems (IROS), pages 23\u201330. IEEE, 2017.   \n[61] Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 5026\u20135033, 2012. doi: 10.1109/IROS.2012.6386109.   \n[62] Masatoshi Uehara, Xuezhou Zhang, and Wen Sun. Representation learning for online and offline rl in low-rank mdps. arXiv preprint arXiv:2110.04652, 2021.   \n[63] Andrew Wagenmaker and Kevin G Jamieson. Instance-dependent near-optimal policy identification in linear mdps via online experiment design. Advances in Neural Information Processing Systems, 35:5968\u20135981, 2022.   \n[64] Andrew Wagenmaker, Guanya Shi, and Kevin Jamieson. Optimal exploration for model-based rl in nonlinear systems. arXiv preprint arXiv:2306.09210, 2023.   \n[65] Andrew J Wagenmaker, Yifang Chen, Max Simchowitz, Simon Du, and Kevin Jamieson. Reward-free rl is no harder than reward-aware rl in linear markov decision processes. In International Conference on Machine Learning, pages 22430\u201322456. PMLR, 2022.   \n[66] Jane X Wang, Zeb Kurth-Nelson, Dhruva Tirumala, Hubert Soyer, Joel Z Leibo, Remi Munos, Charles Blundell, Dharshan Kumaran, and Matt Botvinick. Learning to reinforcement learn. arXiv preprint arXiv:1611.05763, 2016.   \n[67] Gellert Weisz, Philip Amortila, Barnab\u00e1s Janzer, Yasin Abbasi-Yadkori, Nan Jiang, and Csaba Szepesv\u00e1ri. On query-efficient planning in mdps under linear realizability of the optimal state-value function. In Conference on Learning Theory, pages 4355\u20134385. PMLR, 2021.   \n[68] Gell\u00e9rt Weisz, Andr\u00e1s Gy\u00f6rgy, Tadashi Kozuno, and Csaba Szepesv\u00e1ri. Confident approximate policy iteration for efficient local planning in $q^{\\pi}$ -realizable mdps. Advances in Neural Information Processing Systems, 35:25547\u201325559, 2022.   \n[69] Haotian Ye, Xiaoyu Chen, Liwei Wang, and Simon Shaolei Du. On the power of pre-training for generalization in rl: provable beneftis and hardness. In International Conference on Machine Learning, pages 39770\u201339800. PMLR, 2023.   \n[70] Dong Yin, Botao Hao, Yasin Abbasi-Yadkori, Nevena Lazic\u00b4, and Csaba Szepesv\u00e1ri. Efficient local planning with linear function approximation. In International Conference on Algorithmic Learning Theory, pages 1165\u20131192. PMLR, 2022.   \n[71] Andrea Zanette, Alessandro Lazaric, Mykel J Kochenderfer, and Emma Brunskill. Provably efficient reward-agnostic navigation with linear value iteration. Advances in Neural Information Processing Systems, 33:11756\u201311766, 2020.   \n[72] Tianjun Zhang, Tongzheng Ren, Mengjiao Yang, Joseph Gonzalez, Dale Schuurmans, and Bo Dai. Making linear mdps practical via contrastive representation learning. In International Conference on Machine Learning, pages 26447\u201326466. PMLR, 2022.   \n[73] Yunchu Zhang, Liyiming Ke, Abhay Deshpande, Abhishek Gupta, and Siddhartha Srinivasa. Cherry-picking with reinforcement learning. arXiv preprint arXiv:2303.05508, 15, 2023. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "[74] Wenshuai Zhao, Jorge Pe\u00f1a Queralta, and Tomi Westerlund. Sim-to-real transfer in deep reinforcement learning for robotics: a survey. In 2020 IEEE symposium series on computational intelligence (SSCI), pages 737\u2013744. IEEE, 2020. ", "page_idx": 14}, {"type": "text", "text": "A Technical Results ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We denote the state-visitations for some policy $\\pi$ as $w_{h}^{\\pi}(s,a):=\\mathbb{P}^{\\pi}[(s_{h},a_{h})=(s,a)]$ , $w_{h}^{\\pi}({\\mathcal{Z}}):=$ $\\mathbb{P}^{\\pi}[(s_{h},a_{h})\\in\\mathcal{Z}]$ , for $\\mathcal{Z}\\subseteq\\mathcal{S}\\times\\mathcal{A}$ . For $\\mathcal{X}\\subseteq\\mathbb{R}^{d}$ , we denote $w_{h}^{\\pi}(\\mathcal{X}):=\\mathbb{P}^{\\pi}[\\phi(s_{h},a_{h})\\in\\mathcal{X}]$ , for $\\phi$ the featurization of the environment. ", "page_idx": 15}, {"type": "text", "text": "Lemma A.1. Consider MDPs $M$ and $\\widetilde{M}$ with transition kernels $P$ and $\\widetilde{P}$ . Assume that both $M$ and $M$ start in the same state $s_{0}$ and that,  for each $(s,a,h)$ : ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|P_{h}(\\cdot\\mid s,a)-\\widetilde{P}_{h}(\\cdot\\mid s,a)\\|_{\\mathrm{TV}}\\leq\\epsilon_{\\mathrm{sim}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Consider some reward function $r$ such that $\\begin{array}{r}{\\sum_{h=1}^{H}r_{h}(s_{h},a_{h})~\\le~R}\\end{array}$ for all possible sequences $\\{(s_{h},a_{h})\\}_{h=1}^{H}$ . Then it follows that, for any $\\pi$ and $(s,a,h)$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|Q_{h}^{M,\\pi}(s,a)-Q_{h}^{\\widetilde{M},\\pi}(s,a)|\\leq H R\\cdot\\epsilon_{\\mathrm{sim}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof. We prove this by induction. First, assume that for some $h$ and all $s,a$ , we have $|Q_{h+1}^{M,\\pi}(s,a)-$ $Q_{h+1}^{\\widetilde M,\\pi}(s,a)|\\leq\\epsilon_{h+1}$ . By definition we have ", "page_idx": 15}, {"type": "equation", "text": "$$\nQ_{h}^{M,\\pi}(s,a)=r_{h}(s,a)+\\mathbb{E}^{M,\\pi}[Q_{h+1}^{M,\\pi}(s_{h+1},a_{h+1})\\mid s_{h}=s,a_{h}=a]\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and similarly for $Q_{h+1}^{\\widetilde M,\\pi}(s,a)$ . Thus: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{Q_{h}^{M,\\pi}(s,a)-Q_{h}^{\\widetilde{M},\\pi}(s,a)|}\\\\ &{\\overset{(a)}{\\leq}|\\mathbb{E}^{M,\\pi}[Q_{h+1}^{M,\\pi}(s_{h+1},a_{h+1})\\mid s_{h}=s,a_{h}=a]-\\mathbb{E}^{\\widetilde{M},\\pi}[Q_{h+1}^{M,\\pi}(s_{h+1},a_{h+1})\\mid s_{h}=s,a_{h}}\\\\ &{\\quad\\quad\\quad\\quad\\quad+\\mathbb{E}^{\\widetilde{M},\\pi}[|Q_{h+1}^{M,\\pi}(s_{h+1},a_{h+1})-Q_{h+1}^{\\widetilde{M},\\pi}(s_{h+1},a_{h+1})|\\mid s_{h}=s,a_{h}=a]}\\\\ &{\\overset{(b)}{\\leq}|\\mathbb{E}^{M,\\pi}[Q_{h+1}^{M,\\pi}(s_{h+1},a_{h+1})\\mid s_{h}=s,a_{h}=a]-\\mathbb{E}^{\\widetilde{M},\\pi}[Q_{h+1}^{M,\\pi}(s_{h+1},a_{h+1})\\mid s_{h}=s,a_{h}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $(a)$ follows from the triangle inequality and $(b)$ follows from the inductive hypothesis. Under (A.1), we can bound ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{E}^{M,\\pi}[Q_{h+1}^{M,\\pi}(s_{h+1},a_{h+1})\\mid s_{h}=s,a_{h}=a]-\\mathbb{E}^{\\widetilde M,\\pi}[Q_{h+1}^{M,\\pi}(s_{h+1},a_{h+1})\\mid s_{h}=s,a_{h}=a]|\\le\\epsilon_{\\mathrm{sim}}\\cdot R.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "It follows that for any $(s,a),|Q_{h}^{M,\\pi}(s,a)-Q_{h}^{\\widetilde{M},\\pi}(s,a)|\\leq\\epsilon_{h}=:\\epsilon_{\\mathrm{sim}}R+\\epsilon_{h+1}.$ ", "page_idx": 15}, {"type": "text", "text": "The base case follows trivially with $\\epsilon_{H}~=~0$ since for any MDP we have that $Q_{H}^{M,\\pi}(s,a)\\;=\\;$ $r_{H}(s,a)=Q_{H}^{\\widetilde{M},\\pi}(s,a)$ . ", "page_idx": 15}, {"type": "text", "text": "Lemma A.2. Under the same setting as Lemma A.1 and for any $h,\\,\\pi$ , and ${\\mathcal{Z}}\\subseteq S\\times A,$ , we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n|w_{h}^{M,\\pi}(\\mathcal{Z})-w_{h}^{\\widetilde{M},\\pi}(\\mathcal{Z})|\\leq H\\epsilon_{\\mathrm{sim}}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof. This is an immediate consequence of Lemma A.1 since, setting the reward $r_{h^{\\prime}}(s,a)\\;=$ $\\mathbb{I}\\{(s,a)\\in\\mathcal{Z},h^{\\prime}=h\\}$ , we can set $R=1$ and have $V_{0}^{M,\\pi}=w_{h}^{M,\\pi}(\\mathcal{Z})$ . \u53e3 ", "page_idx": 15}, {"type": "text", "text": "Lemma A.3 (Proposition 2). Under Assumption $^{l}$ , we have that ", "page_idx": 15}, {"type": "equation", "text": "$$\nV_{0}^{r\\mathbf{eal},\\star}-V_{0}^{r\\mathbf{eal},\\pi^{\\mathrm{sim},\\star}}\\leq2H^{2}\\epsilon_{\\mathrm{sim}}\\quad a n d\\quad V_{0}^{\\mathrm{sim},\\star}-V_{0}^{\\mathrm{sim},\\pi^{\\mathrm{real},\\star}}\\leq2H^{2}\\epsilon_{\\mathrm{sim}}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof. We prove the result for real\u2014the result for sim follows analogously. We have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\int_{0}^{r\\in\\beth|,\\star}-V_{0}^{r\\mathrm{eal},\\pi^{\\mathrm{sm},\\star}}=V_{0}^{r\\mathrm{eal},\\pi^{\\mathrm{eal},\\star}}-V_{0}^{s\\mathrm{im},\\pi^{\\mathrm{eal},\\star}}+\\underbrace{V_{0}^{\\ s\\mathrm{im},\\pi^{\\mathrm{eal},\\star}}-V_{0}^{\\ s\\mathrm{im},\\pi^{\\mathrm{eal},\\star}}}_{\\leq0}+V_{0}^{s\\mathrm{im},\\pi^{\\mathrm{sm},\\star}}-V_{0}^{r\\mathrm{eal},\\pi^{\\mathrm{im},\\star}}-V_{0}^{r\\mathrm{eal},\\pi^{\\mathrm{im},\\star}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\leq0}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\leq|V_{0}^{r\\mathrm{eal},\\pi^{\\mathrm{eal},\\star}}-V_{0}^{s\\mathrm{im},\\pi^{\\mathrm{eal},\\star}}|+|V_{0}^{s\\mathrm{im},\\pi^{\\mathrm{eal},\\star}}-V_{0}^{r\\mathrm{eal},\\pi^{\\mathrm{eal},\\star}}|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The result then follows by applying Lemma A.1 to bound each of these terms by $H^{2}\\epsilon_{\\mathrm{sim}}$ . ", "page_idx": 15}, {"type": "text", "text": "Lemma A.4. For any $f\\in\\mathcal F$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\nV_{0}^{\\star}-V_{0}^{\\pi^{f}}\\leq\\operatorname*{max}_{\\pi\\in\\{\\pi^{f},\\pi^{\\star}\\}}\\sum_{h=0}^{H-1}2\\left|\\mathbb{E}^{\\pi}[f_{h}(s_{h},a_{h})-T f_{h+1}(s_{h},a_{h})]\\right|.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof. We write ", "page_idx": 16}, {"type": "equation", "text": "$$\nV_{0}^{\\star}-V_{0}^{\\pi^{f}}=\\underbrace{V_{0}^{\\star}-\\operatorname*{max}_{a}f_{0}(s_{0},a)}_{(a)}+\\underbrace{\\operatorname*{max}_{a}f_{0}(s_{0},a)-V_{0}^{\\pi^{f}}}_{(b)}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and then bound each of these terms separately. By Lemma 5 of [57] we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(a)\\le\\displaystyle\\sum_{h=0}^{H}\\Big|\\mathbb{E}^{\\pi^{\\star}}[f_{h}(s_{h},a_{h})-r_{h}-\\underset{a^{\\prime}}{\\operatorname*{max}}\\,f_{h+1}(s_{h+1},a^{\\prime})]\\Big|}\\\\ &{\\quad=\\displaystyle\\sum_{h=0}^{H-1}\\Big|\\mathbb{E}^{\\pi^{\\star}}[f_{h}(s_{h},a_{h})-\\mathbb{E}[r_{h}+\\underset{a^{\\prime}}{\\operatorname*{max}}\\,f_{h+1}(s_{h+1},a^{\\prime})\\mid s_{h},a_{h}]]\\Big|\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Similarly, by Lemma 4 of [57] we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(b)\\le\\displaystyle\\sum_{h=0}^{H-1}\\Big|\\mathbb{E}^{\\pi^{f}}[f_{h}(s_{h},a_{h})-r_{h}-\\operatorname*{max}_{a^{\\prime}}f_{h+1}(s_{h+1},a^{\\prime})]\\Big|}\\\\ &{\\quad=\\displaystyle\\sum_{h=0}^{H-1}\\Big|\\mathbb{E}^{\\pi^{f}}[f_{h}(s_{h},a_{h})-\\mathbb{E}[r_{h}+\\operatorname*{max}_{a^{\\prime}}f_{h+1}(s_{h+1},a^{\\prime})\\mid s_{h},a_{h}]]\\Big|\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "B Proof of Main Results ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In Appendix B.1 we first provide a general result on learning in real when collecting data via a fixed set of exploration policies, given a particular coverage assumption. Then in Appendix B.2, we show that by playing a set of policies which induce full-rank covariates in sim, these policies provide sufficient coverage for learning in real. Finally in Appendices B.3 and B.4, we use these results to prove Theorems 1 and 3. Throughout the appendix we develop the supporting lemmas for our more general result, Theorem 3, which utilizes the simulator to restrict the version space (i.e. the dependence on $|{\\mathcal{F}}|)$ in addition to utilizing the simulator to aid in exploration. ", "page_idx": 16}, {"type": "text", "text": "Throughout this and the following section we assume that Assumption 4 holds. We also assume that $f_{h}\\in[0,V_{\\mathrm{max}}]$ instead of $f_{h}\\in[0,H]$ , for some $V_{\\mathrm{max}}>0$ . For any $f\\in\\mathcal F$ , we denote the Bellman residual as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathscr{E}_{h}(f)(s,a):=T f_{h+1}(s,a)-f_{h}(s,a).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Note that by assumption on $\\mathcal{F}$ , we have $\\mathcal{E}_{h}(f)(s,a)\\in[-V_{\\operatorname*{max}},V_{\\operatorname*{max}}].$ . ", "page_idx": 16}, {"type": "text", "text": "For any policy $\\pi$ , we denote $\\begin{array}{r l r}{\\Lambda_{\\pi,h}^{s}}&{{}:=}&{\\mathbb{E}^{\\sin,\\pi}[\\phi^{s}(s_{h},a_{h})\\phi^{s}(s_{h},a_{h})^{\\top}]}\\end{array}$ and $\\mathbf{A}_{\\pi,h}^{\\mathsf{r}}\\quad:=\\quad$ $\\mathbb{E}^{\\mathrm{real},\\pi}[\\phi^{\\sf r}(s_{h},a_{h})\\phi^{\\sf r}(s_{h},a_{h})^{\\sf T}]$ . ", "page_idx": 16}, {"type": "text", "text": "Necessity of Random Exploration. Algorithm 1 achieves efficient exploration in $\\mathcal{M}^{\\sf r e a l}$ by first learning a set of policies $\\bar{\\Pi}_{\\mathrm{exp}}^{h}$ in $\\mathcal{M}^{\\mathsf{s i m}}$ that span the feature space of $\\mathcal{M}^{\\sf s i m}$ (Line 2), achieving ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\lambda_{\\operatorname*{min}}\\bigg(\\frac{1}{|\\Pi_{\\mathrm{exp}}^{h}|}\\sum_{\\pi\\in\\Pi_{\\mathrm{exp}}^{h}}\\mathbb{E}^{\\mathcal{M}^{\\mathrm{sim}},\\pi}[\\phi^{\\mathrm{s}}(s_{h},a_{h})\\phi^{\\mathrm{s}}(s_{h},a_{h})^{\\top}]\\bigg)\\gtrsim\\lambda_{\\operatorname*{min}}^{\\star},}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and then playing these policies in $\\mathcal{M}^{\\tt r e a l}$ , coupled with random exploration (Line 4). In particular, Algorithm 1 plays policies from $\\widetilde{\\Pi}_{\\mathrm{exp}}^{h}$ , where each $\\widetilde{\\pi}_{\\mathrm{exp}}\\in\\widetilde{\\Pi}_{\\mathrm{exp}}^{h}$ is defined as the policy which plays some $\\pi_{\\mathrm{exp}}\\in\\Pi_{\\mathrm{exp}}^{h}$ up to step $h$ , and then for steps $h^{\\prime}=h+1,\\ldots,H$ chooses actions uniformly at random. This use of random exploration is critical to obtaining Theorem 1. Indeed, under our transfer model, condition (4.1) of Theorem 1 is not strong enough to ensure that policies satisfying (B.1) collect rich enough data in $\\mathcal{M}^{\\tt r e a l}$ to allow for learning a near-optimal policy. While (4.1) is sufficient to guarantee that playing $\\Pi_{\\mathrm{exp}}^{h}$ on $\\mathcal{M}^{\\tt r e a l}$ collects data which spans the feature space of $\\mathcal{M}^{\\sf s i m}$ \u2014that is, satisfying (B.1) but with the expectation over $\\mathcal{M}^{\\sf s i m}$ replaced by an expectation of $\\mathcal{M}^{\\sf r e a l}.$ \u2014 this is insufficient for learning, as the following result shows. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "Proposition 5. For any $\\epsilon_{\\mathrm{sim}}\\leq1/2$ , there exist some $\\mathcal{M}^{\\sf s i m}$ , $\\mathcal{M}^{\\tt r e a l,1}$ , and $\\mathcal{M}^{\\sf r e a l,2}$ such that: ", "page_idx": 17}, {"type": "text", "text": "1. Both $\\mathcal{M}^{\\tt r e a l,1}$ and $\\mathcal{M}^{\\tt r e a l,2}$ satisfy Assumption $^{l}$ with $\\mathcal{M}^{\\mathsf{s i m}}$ and Assumptions 2 to $^{4}$ hold. 2. There exists some policy $\\pi_{\\mathrm{exp}}$ such that $\\lambda_{\\operatorname*{min}}(\\mathbb{E}^{\\mathcal{M}^{\\mathrm{sim}},\\pi_{\\mathrm{exp}}}[\\phi^{\\mathsf{s}}(s_{h},a_{h})\\phi^{\\mathsf{s}}(s_{h},a_{h})^{\\top}])=1/2$ $\\forall h\\in[H]$ , and for any $T\\geq0$ , if we play $\\pi_{\\mathrm{exp}}$ on $\\mathcal{M}^{\\tt r e a l}$ for $T$ steps, we have: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\operatorname*{inf}_{\\widehat\\pi}\\operatorname*{sup}_{\\mathcal{M}^{\\mathrm{real}}\\in\\{\\mathcal{M}^{\\mathrm{real},1},\\mathcal{M}^{\\mathrm{real},2}\\}}\\mathbb{E}^{\\mathcal{M}^{\\mathrm{real}},\\pi_{\\mathrm{exp}}}[V_{0}^{\\mathcal{M}^{\\mathrm{real}},\\star}-V_{0}^{\\mathcal{M}^{\\mathrm{real}},\\widehat\\pi}]\\geq\\epsilon_{\\sin}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proposition 5 holds because two MDPs may be \u201cclose\u201d in the sense of Assumption 1 but admit very different feature representations. As a result, transferring a policy that covers the feature space of $\\mathcal{M}^{\\sf s i m}$ is not necessarily sufficient for covering the feature space of $\\mathcal{M}^{\\tt r e a l}$ , which ultimately means that data collected from $\\pi_{\\mathrm{exp}}$ is unable to identify the optimal policy in $\\mathcal{M}^{\\tt r e a l}$ . Our key technical result, Lemma B.4, shows, however, that under Assumption 1 and (4.1), policies which achieve high coverage in $\\mathcal{M}^{\\sf s i m}$ (i.e. satisfy (B.1)) are able to reach within a logarithmic number of steps of relevant states in $\\mathcal{M}^{\\tt r e a l}$ . While the sample complexity of random exploration typically scales exponentially in the horizon, if the horizon over which we must explore is only logarithmic, the total complexity is then only polynomial. Theorem 1 critically relies on these facts\u2014by playing policies in $\\Pi_{\\mathrm{exp}}^{h}$ up to step $h$ and then exploring randomly, and repeating this for each $h\\in[H]$ , we show that sufficiently rich data is collected in $\\mathcal{M}^{\\tt r e a l}$ for learning an $\\epsilon\\cdot$ -optimal policy. ", "page_idx": 17}, {"type": "text", "text": "B.1 Learning in real with Fixed Exploration Policies ", "text_level": 1, "page_idx": 17}, {"type": "table", "img_path": "JjQl8hXJAS/tmp/b40261ac5025a8133779af6ab0ec20e973ae25e759211b8969998681c10bfb0c.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "Lemma B.1. Consider running Algorithm 3. Assume that $\\mathfrak{D}_{\\mathsf{s i m}}$ was generated as in Assumption $^{5}$ , via the procedure of Lemma C.3 run with some parameter $\\beta$ , and $\\gamma$ satisfies ", "page_idx": 17}, {"type": "equation", "text": "$$\n2V_{\\mathrm{max}}^{2}\\epsilon_{\\mathrm{sim}}^{2}+\\frac{43V_{\\mathrm{max}}^{2}\\beta^{2}}{d H}\\cdot\\log\\frac{8H|\\mathcal{F}_{h}|}{\\delta}+6V_{\\mathrm{max}}^{2}\\beta\\sqrt{\\frac{\\log\\frac{8H|\\mathcal{F}_{h}|}{\\delta}}{d H}}\\le\\gamma.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Furthermore, assume that there exists some $\\mathfrak{C},\\epsilon>0$ such that, for any $\\pi$ , $h\\in[H]$ , and ${\\mathcal{Z}}^{\\prime}\\subseteq S\\times A$ we have: ", "page_idx": 17}, {"type": "equation", "text": "$$\nw_{h}^{\\mathsf{r e a l},\\pi}(\\mathcal{Z}^{\\prime})\\leq\\mathfrak{C}\\cdot w_{h}^{\\mathsf{r e a l},\\pi_{\\mathrm{exp}}}(\\mathcal{Z}^{\\prime})+\\epsilon.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Then with probability at least $1-2\\delta$ , the policy $\\pi^{\\widehat{f}}$ generated by Algorithm 3 satisfies ", "page_idx": 18}, {"type": "equation", "text": "$$\nV_{0}^{\\star}-V_{0}^{\\pi^{\\widehat{f}}}\\leq4\\mathfrak{C}H\\sqrt{\\frac{256V_{\\operatorname*{max}}^{2}\\log(4H|\\widetilde{\\mathcal{F}}(\\pi_{\\mathrm{exp}}^{\\mathrm{sim}})|/\\delta)}{T}}+4H V_{\\operatorname*{max}}\\epsilon\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "for ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widetilde{\\mathcal{F}}(\\pi_{\\mathrm{exp}}^{\\mathrm{sim}}):=\\{f\\in\\mathcal{F}\\,:\\,\\mathbb{E}^{\\mathrm{sim},\\pi_{\\mathrm{exp}}^{\\mathrm{sim}}}[(f_{h}(s_{h},a_{h})-7^{\\mathrm{sim}}f_{h+1}(s_{h},a_{h}))^{2}]\\le2\\gamma,\\forall h\\in[H]\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof. Let $\\mathcal{E}$ denote the good event of Lemma B.2, which holds with probability at least $1-2\\delta$ . By Lemma A.4 we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V_{0}^{\\mathrm{real},\\star}-V_{0}^{\\mathrm{real},\\pi^{\\widehat{f}}}\\leq\\underset{\\pi\\in\\{\\pi^{\\widehat{f}},\\pi^{\\mathrm{real},\\star}\\}}{\\operatorname*{max}}\\sum_{h=0}^{H-1}2\\left|\\mathbb{E}^{\\mathrm{real},\\pi}[\\widehat{f}_{h}(s_{h},a_{h})-T^{\\mathrm{real}}\\widehat{f}_{h+1}(s_{h},a_{h})]\\right|}\\\\ &{\\qquad\\qquad\\qquad\\leq\\underset{\\pi}{\\operatorname*{max}}\\sum_{h=0}^{H-1}2\\mathbb{E}^{\\mathrm{real},\\pi}[|\\mathcal{E}_{h}^{\\mathrm{real}}(\\widehat{f})(s_{h},a_{h})|].}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Let ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathcal{Z}_{h,i}:=\\{(s,a)\\ :\\ |\\mathcal{E}_{h}^{\\mathsf{r e a l}}(\\widehat{f})(s,a)|\\in[V_{\\operatorname*{max}}\\cdot2^{-i},V_{\\operatorname*{max}}\\cdot2^{-i+1})\\}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Then we have, for any $\\pi$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\mathbb{E}^{{\\mathrm{real}},\\pi}[|\\mathcal{E}_{h}^{\\mathrm{real}}(\\widehat{f})(s_{h},a_{h})|]\\leq\\sum_{i=1}^{\\infty}w_{h}^{\\mathrm{real},\\pi}(\\mathcal{Z}_{h,i})\\cdot V_{\\mathrm{max}}2^{-i+1}}}\\\\ &{\\leq\\mathfrak{C}\\cdot\\displaystyle\\sum_{i=1}^{\\infty}w_{h}^{\\mathrm{real},\\pi_{\\mathrm{exp}}}(\\mathcal{Z}_{h,i})\\cdot V_{\\mathrm{max}}2^{-i+1}+2V_{\\mathrm{max}}6}\\\\ &{\\leq2\\mathfrak{C}\\cdot\\mathbb{E}^{{\\mathrm{real}},\\pi_{\\mathrm{exp}}}[|\\mathcal{E}_{h}^{\\mathrm{real}}(\\widehat{f})(s_{h},a_{h})|]+2V_{\\mathrm{max}}\\epsilon}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the second inequality follows from (B.3). On $\\mathcal{E}$ , by Lemma B.2 and Jensen\u2019s inequality, we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathrm{g}^{\\mathrm{real},\\pi_{\\mathrm{exp}}}\\big[\\big|\\mathcal{E}_{h}^{\\mathrm{real}}(\\widehat{f})(s_{h},a_{h})\\big|\\big]\\leq\\sqrt{\\mathbb{E}^{\\mathrm{real},\\pi_{\\mathrm{exp}}}[\\mathcal{E}_{h}^{\\mathrm{real}}(\\widehat{f})(s_{h},a_{h})^{2}]}\\leq\\sqrt{\\frac{1}{T}\\cdot256V_{\\mathrm{max}}^{2}\\log\\frac{2H|\\widetilde{\\mathcal{F}}_{h}(\\pi_{\\mathrm{exp}}^{\\mathrm{sim}})|}{\\delta}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "As this holds for each $h$ and $\\pi$ , we have therefore shown that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V_{0}^{\\mathrm{real},\\star}-V_{0}^{\\mathrm{real},\\pi^{\\hat{f}}}\\leq4\\mathfrak{C}\\cdot\\displaystyle\\sum_{h=0}^{H-1}\\sqrt{\\frac{1}{T}\\cdot256V_{\\mathrm{max}}^{2}\\log\\frac{2H|\\widetilde{\\mathcal{F}}_{h}(\\pi_{\\mathrm{exp}}^{\\mathrm{sim}})|}{\\delta}}+4H V_{\\mathrm{max}}\\epsilon}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\le4\\mathfrak{C}H\\sqrt{\\frac{1}{T}\\cdot256V_{\\mathrm{max}}^{2}\\log\\frac{2H|\\widetilde{\\mathcal{F}}(\\pi_{\\mathrm{exp}}^{\\mathrm{sim}})|}{\\delta}}+4H V_{\\mathrm{max}}\\epsilon.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "This proves the result. ", "page_idx": 18}, {"type": "text", "text": "Lemma B.2. With probability at least 1 \u22122\u03b4, for each $h\\,\\in\\,[H]$ simultaneously, as long as the conditions on $\\gamma$ given in Lemma B.3 hold, we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}^{{\\mathrm{real}},\\pi_{\\mathrm{exp}}}\\big[(\\widehat{f}_{h}(s_{h},a_{h})-T^{\\mathrm{real}}\\widehat{f}_{h+1}(s_{h},a_{h}))^{2}\\big]\\leq\\frac{1}{T}\\cdot256V_{\\mathrm{max}}^{2}\\log(2H|\\widetilde{\\mathcal{F}}_{h}(\\pi_{\\mathrm{exp}}^{\\mathrm{sim}})|/\\delta),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and $\\widehat{f}_{h}\\in\\widetilde{\\mathcal{F}}_{h}(\\pi_{\\mathrm{exp}}^{\\mathsf{s i m}})$ for all $h\\in[H]$ , where ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\tilde{\\boldsymbol{\\kappa}}_{h}(\\pi_{\\mathrm{exp}}^{\\mathsf{s i m}}):=\\{f_{h}\\in\\mathcal{F}_{h}\\ :\\ \\exists f_{h+1}\\in\\mathcal{F}_{h+1}\\ s.t.\\ \\mathbb{E}^{\\mathsf{s i m},\\pi_{\\mathrm{exp}}^{\\mathsf{s i m}}}[(f_{h}(s_{h},a_{h})-7^{\\mathsf{s i m}}f_{h+1}(s_{h},a_{h}))^{2}]\\leq2\\gamma\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof. Let ${\\widehat{\\mathcal{F}}}_{h}$ denote the feasible set of (B.2) at step $h$ . By Lemma B.3, with probability at least $1-\\delta$ , $\\widehat{\\mathcal{F}}_{h}^{t}\\subseteq\\widetilde{\\mathcal{F}}_{h}$ , and, furthermore, that $\\mathcal{T}^{\\mathrm{real}}\\widehat{f}_{h+1}$ is feasible. The result then follows from Lemma 3 of [57] , since the constraint on the regression problem restricts the version space. \u53e3 ", "page_idx": 18}, {"type": "text", "text": "Lemma B.3. Assume that data in $\\mathfrak{D}_{\\mathsf{s i m}}$ is generated as in Assumption 5 via the procedure of Lemma C.3 run with some parameter $\\beta$ , and $\\gamma$ satisfies ", "page_idx": 19}, {"type": "equation", "text": "$$\n2V_{\\mathrm{max}}^{2}\\epsilon_{\\mathrm{sim}}^{2}+\\frac{43V_{\\mathrm{max}}^{2}\\beta^{2}}{d H}\\cdot\\log\\frac{8H|\\mathcal{F}_{h}|}{\\delta}+6V_{\\mathrm{max}}^{2}\\beta\\sqrt{\\frac{\\log\\frac{8H|\\mathcal{F}_{h}|}{\\delta}}{d H}}\\le\\gamma.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Then with probability at least $1-\\delta$ we have, for each $h\\in[H]$ : ", "page_idx": 19}, {"type": "text", "text": "1. T real $\\widehat{f}_{h+1}$ is feasible for (B.2).   \n2. The set of feasible $f$ for (B.2) is a subset of ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\{f\\in\\mathcal{F}\\ :\\ \\mathbb{E}^{\\mathrm{sim},\\pi_{\\mathrm{exp}}^{\\mathrm{sim}}}[(f_{h}(s_{h},a_{h})-\\mathcal{T}^{\\mathrm{sim}}\\widehat{f}_{h+1}(s_{h},a_{h}))^{2}]\\leq2\\gamma\\}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof. By Lemma C.1, we have that with probability at least $1-\\delta/2H$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{1}{l_{\\sin}}\\sum_{t=1}^{T_{\\sin}}(T^{\\mathrm{real}}\\widehat{f}_{h+1}(\\widetilde{s}_{h}^{t},\\widetilde{a}_{h}^{t})-\\widetilde{f}_{h}(\\widetilde{s}_{h}^{t},\\widetilde{a}_{h}^{t}))^{2}\\leq2V_{\\operatorname*{max}}^{2}\\epsilon_{\\sin}^{2}+\\frac{512V_{\\operatorname*{max}}^{2}}{T_{\\sin}}\\cdot\\log\\frac{8H|\\mathcal{F}_{h}|}{\\delta}+V_{\\operatorname*{max}}^{2}\\sqrt{\\frac{2\\log\\frac{4}{T}}{T_{\\sin}}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "By Lemma C.3, we have $\\begin{array}{r}{\\frac{12d H}{\\beta^{2}}\\leq T_{\\mathrm{sim}}}\\end{array}$ , which implies ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{1}{I_{\\sin}}\\sum_{t=1}^{T_{\\sin n}}(T^{\\mathrm{real}}\\widehat{f}_{h+1}(\\widetilde{s}_{h}^{t},\\widetilde{a}_{h}^{t})-\\widetilde{f}_{h}(\\widetilde{s}_{h}^{t},\\widetilde{a}_{h}^{t}))^{2}\\leq2V_{\\operatorname*{max}}^{2}\\epsilon_{\\sin}^{2}+\\frac{43V_{\\operatorname*{max}}^{2}\\beta^{2}}{d H}\\cdot\\log\\frac{8H|\\mathcal{F}_{h}|}{\\delta}+V_{\\operatorname*{max}}^{2}\\beta\\sqrt{\\frac{\\log\\sqrt{\\epsilon_{\\operatorname*{max}}}}{6}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Part 1 then follows given our assumption on $\\gamma$ ", "page_idx": 19}, {"type": "text", "text": "To bound the feasible set for (B.2) we appeal to Lemma C.2 which states that with probability at least $1-\\delta/2H$ we have that the feasible set of (B.2) is a subset of ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\left\\{f_{h}\\in\\mathcal{F}_{h}\\;:\\;\\mathbb{E}^{\\mathrm{sim},\\pi_{\\mathrm{exp}}^{\\mathrm{sim}}}[(f_{h}(s_{h},a_{h})-7^{\\mathrm{sim}}\\widehat{f}_{h+1}(s_{h},a_{h}))^{2}]\\leq\\gamma+18V_{\\mathrm{max}}^{2}\\sqrt{\\frac{\\log\\frac{8H|\\mathcal{F}_{h}|}{\\delta}}{T_{\\mathrm{sim}}}}\\right\\}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Again using that $\\begin{array}{r}{\\frac{12d H}{\\beta^{2}}\\leq T_{\\mathsf{s i m}}}\\end{array}$ , we have have that this is a subset of ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\{f_{h}\\in\\mathcal{F}_{h}\\ :\\ {\\mathbb{E}}^{\\mathrm{sim},\\pi_{\\mathrm{exp}}^{\\mathrm{sim}}}[(f_{h}(s_{h},a_{h})-\\mathcal{T}^{\\mathrm{sim}}\\widehat{f}_{h+1}(s_{h},a_{h}))^{2}]\\leq\\gamma+18V_{\\mathrm{max}}^{2}\\beta\\sqrt{\\frac{\\log\\frac{8H\\lvert\\mathcal{F}_{h}\\rvert}{\\delta}}{12d H}}\\right\\}}\\\\ &{\\subseteq\\left\\{f_{h}\\in\\mathcal{F}_{h}\\ :\\ {\\mathbb{E}}^{\\mathrm{sim},\\pi_{\\mathrm{exp}}^{\\mathrm{sim}}}[(f_{h}(s_{h},a_{h})-\\mathcal{T}^{\\mathrm{sim}}\\widehat{f}_{h+1}(s_{h},a_{h}))^{2}]\\leq2\\gamma\\right\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where the inclusion follows from our assumption on $\\gamma$ . The result then follows from a union bound. \u53e3 ", "page_idx": 19}, {"type": "text", "text": "B.2 Performance of Full-Rank sim Policies in real ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Lemma B.4. Consider policies $\\{\\pi_{\\mathrm{exp}}^{h}\\}_{h=1}^{H}$ , and assume that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\lambda_{\\operatorname*{min}}\\left(\\Lambda_{\\pi_{\\exp}^{h},h}^{s}\\right)\\geq\\bar{\\lambda}_{\\operatorname*{min}},\\quad\\forall h\\in[H]\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and that $\\pi_{\\mathrm{exp}}^{h}$ plays actions uniformly at random for $h^{\\prime}>h.$ Let $\\pi_{\\mathrm{exp}}=\\operatorname{unif}\\!\\left(\\{\\pi_{\\mathrm{exp}}^{h}\\}_{h=1}^{H}\\right)$ . Then, for any $\\pi$ $;\\,\\kappa>0,\\,\\gamma>0,\\,h\\in[H]$ , and $\\mathcal{Z}^{\\prime}\\subseteq\\mathcal{S}\\times\\mathcal{A}$ , we have ", "page_idx": 19}, {"type": "equation", "text": "$$\nw_{h}^{\\mathsf{r e a l},\\pi}(\\mathcal{Z}^{\\prime})\\leq\\frac{4H\\gamma A^{k^{\\star}-2}}{\\kappa}\\cdot w_{h}^{\\mathsf{r e a l},\\pi_{\\mathrm{exp}}}(\\mathcal{Z}^{\\prime})+4\\kappa,\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\xi:=2\\sqrt{\\frac{A}{\\bar{\\lambda}_{\\operatorname*{min}}}\\left(\\frac{d}{\\gamma}+H\\epsilon_{\\sin}\\right)}\\quad a n d\\quad k^{\\star}:=\\lceil\\frac{\\log1/\\kappa}{\\log1/\\xi}\\rceil.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof. Denote ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\widetilde{\\mathcal{Z}}_{h+1}:=\\{(s,a)\\ :\\ \\phi^{\\mathsf{r}}(s,a)^{\\top}(\\Delta_{\\pi_{\\mathrm{exp}}^{h},h+1}^{\\mathsf{r}})^{-1}\\phi^{\\mathsf{r}}(s,a)>\\gamma\\}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "for some $\\gamma>0$ . We have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{v_{h+1}^{\\mathrm{eq,\\eta}_{\\mathrm{esp}}^{h}}(\\widetilde{Z}_{h+1})=\\mathbb{E}^{\\mathrm{eq,\\eta}_{\\mathrm{esp}}^{h}}[\\mathbb{I}\\{(s_{h+1},a_{h+1})\\in\\widetilde{Z}_{h+1}\\}]}\\\\ &{\\overset{(a)}{\\leq}\\mathbb{E}^{\\mathrm{eq,\\eta}_{\\mathrm{esp}}^{h}}\\left[\\frac{\\phi^{r}\\left(s_{h+1},a_{h+1}\\right)^{\\top}\\left(({\\Lambda}_{\\tau_{\\mathrm{esp}}^{h},h+1})^{-1}\\phi^{r}\\left(s_{h+1},a_{h+1}\\right)\\right.}{\\gamma}\\cdot\\mathbb{I}\\{(s_{h+1},a_{h+1})\\in\\widetilde{Z}\\}\\right.}\\\\ &{\\leq\\mathbb{E}^{\\mathrm{eq,\\eta}_{\\mathrm{esp}}^{h}}\\left[\\frac{\\phi^{r}\\left(s_{h+1},a_{h+1}\\right)^{\\top}\\left({\\Lambda}_{\\tau_{\\mathrm{esp}}^{h},h+1}^{\\tau}\\right)^{-1}\\phi^{r}\\left(s_{h+1},a_{h+1}\\right)}{\\gamma}\\right]}\\\\ &{=\\frac{1}{\\gamma}\\cdot\\mathrm{tr}\\left(\\mathbb{E}^{\\mathrm{eq,\\eta}_{\\mathrm{esp}}^{h}}[\\phi^{r}(s_{h+1},a_{h+1})\\phi^{r}(s_{h+1},a_{h+1})^{\\top}]({\\Lambda}_{\\tau_{\\mathrm{esp}}^{h},h+1}^{\\tau})^{-1}\\right)}\\\\ &{=\\frac{d}{\\gamma}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $(a)$ follows since for all $(s,a)\\in\\widetilde{\\mathcal{Z}}_{h+1}$ , we have $1<\\phi^{\\boldsymbol{r}}(s,a)^{\\top}(\\mathbf{A}_{\\pi_{\\exp}^{h},h+1}^{\\boldsymbol{r}})^{-1}\\phi^{\\boldsymbol{r}}(s,a)/\\gamma$ . By Lemma A.2, we then have that ", "page_idx": 20}, {"type": "equation", "text": "$$\nw_{h+1}^{\\sin,\\pi_{\\mathrm{exp}}^{h}}(\\widetilde{\\mathcal{Z}}_{h+1})\\leq\\frac{d}{\\gamma}+H\\epsilon_{\\mathrm{sim}}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Let $\\widetilde{S}_{h+1}:=\\{s\\,:\\,\\exists a\\}$ s.t. $(s,a)\\in\\widetilde{\\mathcal{Z}}_{h+1}\\}$ and note that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{w_{h+1}^{\\sin,\\pi_{\\mathrm{exp}}^{h}}(\\widetilde{Z}_{h+1})=\\mathbb{E}^{\\sin,\\pi_{\\mathrm{exp}}^{h}}\\left[\\int_{\\widetilde{S}_{h+1}}\\sum_{\\substack{a:\\left(s,a\\right)\\in\\widetilde{Z}_{h+1}}}\\pi_{\\mathrm{exp}}^{h}(a\\mid s,h+1)\\mathrm{d}P_{h}^{\\sin}(s\\mid s_{h},a_{h})\\right]}}\\\\ &{\\ge\\frac{1}{A}\\mathbb{E}^{\\sin,\\pi_{\\mathrm{exp}}^{h}}\\left[\\int_{\\widetilde{S}_{h+1}}\\mathrm{d}\\mu_{h}^{s}(s)^{\\top}\\phi^{s}(s_{h},a_{h})\\right]}\\\\ &{=\\frac{1}{A}\\mathbb{E}^{\\sin,\\pi_{\\mathrm{exp}}^{h}}[P_{h}^{\\sin}(\\widetilde{S}_{h+1}\\mid s_{h},a_{h})]}\\\\ &{\\ge\\frac{1}{A}\\mathbb{E}^{\\sin,\\pi_{\\mathrm{exp}}^{h}}[P_{h}^{\\sin}(\\widetilde{S}_{h+1}\\mid s_{h},a_{h})^{2}]}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where we have used the fact that $\\pi_{\\mathrm{exp}}^{h}(a\\mid s,h+1)=1/A$ for all $(s,a)$ by assumption, and define $\\begin{array}{r}{P_{h}^{\\sin}\\!\\left(\\widetilde{S}_{h+1}\\mid s,a\\right):=\\mathbb{P}^{\\sin}\\!\\left[s_{h+1}\\in\\widetilde{S}_{h+1}\\mid s_{h}=s,a_{h}=a\\right]=\\int_{\\widetilde{S}_{h+1}}\\!\\mathrm{d}\\mu_{h}^{s}(s)^{\\top}\\phi^{s}(s,a)}\\end{array}$ , where the last equality follows from the definition of a linear MDP. Letting $\\begin{array}{r}{\\pmb{\\mu}_{h}^{\\mathsf{s}}(\\widetilde{S}_{h+1}):=\\int_{\\widetilde{S}_{h+1}}\\mathrm{d}\\pmb{\\mu}_{h}^{\\mathsf{s}}(s)}\\end{array}$ , note that: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\small}&{\\frac{1}{A}\\mathbb{E}^{\\mathrm{sim},\\pi_{\\mathrm{exp}}^{h}}[P_{h}^{\\mathrm{sim}}(\\widetilde{S}_{h+1}\\mid s_{h},a_{h})^{2}]=\\frac{1}{A}\\mu_{h}^{\\mathsf{s}}(\\widetilde{S}_{h+1})^{\\top}\\mathbb{E}^{\\mathrm{sim},\\pi_{\\mathrm{exp}}^{h}}[\\phi^{\\mathsf{s}}(s_{h},a_{h})\\phi^{\\mathsf{s}}(s_{h},a_{h})^{\\top}]\\mu_{h}^{\\mathsf{s}}(\\widetilde{S}_{h+1})}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad=\\frac{1}{A}\\mu_{h}^{\\mathsf{s}}(\\widetilde{S}_{h+1})^{\\top}\\Lambda_{\\pi_{\\mathrm{exp}}^{h},h}^{\\mathsf{s}}\\mu_{h}^{\\mathsf{s}}(\\widetilde{S}_{h+1})}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\geq\\frac{\\bar{\\lambda}_{\\mathrm{min}}}{A}\\|\\mu_{h}^{\\mathsf{s}}(\\widetilde{S}_{h+1})\\|_{2}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the last inequality follows from (B.4). Combining this with (B.5), we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\frac{d}{\\gamma}+H\\epsilon_{\\mathrm{sim}}\\geq\\frac{\\bar{\\lambda}_{\\mathrm{min}}}{A}\\|\\pmb{\\mu}_{h}^{s}(\\widetilde{S}_{h+1})\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Now note that, for any $z\\in{\\mathcal{S}}\\times{\\mathcal{A}}$ : ", "page_idx": 20}, {"type": "equation", "text": "$$\nP_{h}^{\\sin}(\\widetilde{S}_{h+1}\\mid z)=\\int_{\\widetilde{S}_{h+1}}\\mathrm{d}P_{h}^{\\sin}(s\\mid z)=\\left(\\int_{\\widetilde{S}_{h+1}}\\mathrm{d}\\mu_{h}^{s}(s)\\right)^{\\top}\\phi^{s}(z)\\leq\\|\\mu_{h}^{s}(\\widetilde{S}_{h+1})\\|_{2}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and we also have that $P_{h}^{\\sin}(\\widetilde{S}_{h+1}\\mid z)\\ge P_{h}^{\\mathsf{r e a l}}(\\widetilde{S}_{h+1}\\mid z)-\\epsilon_{\\mathrm{sim}}$ under Assumption 1. Putting this together we have that for all $z\\in{\\mathcal{S}}\\times{\\mathcal{A}}$ : ", "page_idx": 21}, {"type": "equation", "text": "$$\nP_{h}^{\\mathsf{r e a l}}(\\widetilde{S}_{h+1}\\mid z)\\leq\\sqrt{\\frac{A}{\\bar{\\lambda}_{\\operatorname*{min}}}\\left(\\frac{d}{\\gamma}+H\\epsilon_{\\mathrm{sim}}\\right)}+\\epsilon_{\\mathrm{sim}}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Note that we can always take $\\epsilon_{\\mathrm{sim}}\\leq1$ , and will always have $\\bar{\\lambda}_{\\operatorname*{min}}\\leq1$ . This implies that $\\epsilon_{\\mathrm{sim}}\\leq$ $\\sqrt{\\frac{A}{\\bar{\\lambda}_{\\mathrm{min}}}\\left(\\frac{d}{\\gamma}+H\\epsilon_{\\mathrm{sim}}\\right)}$ . Thus, ", "page_idx": 21}, {"type": "equation", "text": "$$\nP_{h}^{\\mathsf{r e a l}}(\\widetilde{S}_{h+1}\\mid z)\\leq2\\sqrt{\\frac{A}{\\bar{\\lambda}_{\\operatorname*{min}}}\\left(\\frac{d}{\\gamma}+H\\epsilon_{\\mathrm{sim}}\\right)}=:\\xi.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Coverage of $\\pi_{\\mathrm{exp}}$ in real. Let $\\begin{array}{r}{k^{\\star}:=\\lceil\\frac{\\log1/\\kappa}{\\log1/\\xi}\\rceil}\\end{array}$ , so that $\\xi^{k^{\\star}}\\leq\\kappa$ . Let $\\bar{\\mathcal{Z}}_{h}:=(S\\times A)\\backslash\\widetilde{\\mathcal{Z}}_{h}$ . Fix some ${\\mathcal{Z}}^{\\prime}\\subseteq(S\\times A)$ , $h\\in[H]$ , and policy $\\pi$ . ", "page_idx": 21}, {"type": "text", "text": "Consider some $z\\in\\bar{\\mathcal{Z}}_{h}$ , and some $S^{\\prime}\\subseteq S$ . Then note that5 ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{P_{h}^{\\mathrm{real}}(S^{\\prime}\\mid z)=\\mu_{h}^{r}(S^{\\prime})^{\\top}\\phi^{r}(z)=\\mu_{h}^{r}(S^{\\prime})^{\\top}(\\Lambda_{\\pi_{\\exp{\\bar{\\mathcal{h}}}}^{h-1},h}^{r})^{1/2}(\\Lambda_{\\pi_{\\exp{\\bar{\\mathcal{h}}}}^{h-1},h}^{r})^{-1/2}\\phi^{r}(z)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\Vert\\mu_{h}^{r}(S^{\\prime})\\Vert_{\\Lambda_{\\pi_{\\exp{\\bar{\\mathcal{h}}}}^{h-1},h}^{r}}\\Vert\\phi^{r}(z)\\Vert_{(\\Lambda_{\\pi_{\\exp{\\bar{\\mathcal{h}}}}^{h-1},h}^{r})^{-1}}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\sqrt{\\gamma}\\Vert\\mu_{h}^{r}(S^{\\prime})\\Vert_{\\Lambda_{\\pi_{\\exp{\\mathcal{h}}}^{h-1},h}^{r}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where the last inequality follows from the definition of $\\bar{\\mathcal{Z}}_{h}$ . Note, though, that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\rVert\\mu_{h}^{\\boldsymbol{r}}(\\mathcal{S}^{\\prime})\\rVert_{\\boldsymbol{\\Lambda}_{\\pi_{\\exp}^{h-1},h}^{r}}^{2}=\\mathbb{E}^{\\mathrm{real},\\pi_{\\exp}^{h-1}}[(\\mu_{h}^{r}(\\mathcal{S}^{\\prime})^{\\top}\\phi^{r}(z_{h}))^{2}]=\\mathbb{E}^{\\mathrm{real},\\pi_{\\exp}^{h-1}}[P_{h}^{\\mathrm{real}}(\\mathcal{S}^{\\prime}\\mid z_{h})^{2}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "This implies that for all $z\\in\\bar{\\mathcal{Z}}_{h}$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{E}^{{\\sf r e a l},\\pi_{\\mathtt{e x p}}^{h-1}}[P_{h}^{\\sf r e a l}(S^{\\prime}\\mid z_{h})^{2}]\\ge\\frac{1}{\\gamma}\\cdot P_{h}^{{\\sf r e a l}}(S^{\\prime}\\mid z)^{2}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "For $h^{\\prime}<h$ , define ", "page_idx": 21}, {"type": "equation", "text": "$$\nS_{h^{\\prime},i}:=\\{s\\ :\\ w_{h}^{\\mathsf{r e a l},\\pi}({\\mathcal{Z}}^{\\prime}\\mid s_{h^{\\prime}}=s)\\in[2^{-i+1},2^{-i})\\}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "for $w_{h}^{\\mathsf{r e a l},\\pi}(\\mathcal{Z}\\mid s_{h^{\\prime}}=s):=\\mathbb{P}^{\\mathsf{r e a l},\\pi}[z_{h}\\in\\mathcal{Z}\\mid s_{h^{\\prime}}=s].$ . Note that we then have wrheal,\u03c0(Z\u2032 | Sh\u2032,i) \u2208 $[2^{-i+1},2^{-i})$ . By what we have just shown, we have that for $z\\in\\bar{\\mathcal{Z}}_{h^{\\prime}}$ ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{E}^{\\mathrm{{real}},\\pi_{\\mathrm{exp}}^{h^{\\prime}-1}}[P_{h^{\\prime}}^{\\mathrm{real}}(S_{h^{\\prime}+1,i}\\mid z_{h^{\\prime}})^{2}]\\ge\\frac{1}{\\gamma}\\cdot P_{h^{\\prime}}^{\\mathrm{real}}(S_{h^{\\prime}+1,i}\\mid z)^{2}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "which implies that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{E}^{\\mathrm{real},\\pi_{\\mathrm{exp}}^{h^{\\prime}-1}}[P_{h^{\\prime}}^{\\mathrm{real}}(S_{h^{\\prime}+1,i}\\mid z_{h^{\\prime}})]\\ge\\frac{1}{\\gamma}\\cdot P_{h^{\\prime}}^{\\mathrm{real}}(S_{h^{\\prime}+1,i}\\mid z)^{2}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Fix $z\\in\\bar{\\mathcal{Z}}_{h^{\\prime}}$ . Note that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{w_{h}^{\\varepsilon\\lefteqn{\\|_{\\mathcal{S}}},x}(z^{\\prime}\\mid\\mathcal{Z}_{h^{\\prime}}=z)=\\mathbb{E}_{s\\sim h_{h}^{(\\#)}(x)}[w_{h}^{\\varepsilon\\log(1)}(z^{\\prime}\\mid s_{h^{\\prime}+1}=s)]}\\\\ &{=\\displaystyle\\sum_{i=1}^{\\infty}\\mathbb{E}_{s\\sim h_{h}^{(\\#)}(x)}[w_{h}^{\\varepsilon\\log(1)}(z^{\\prime}\\mid s_{h^{\\prime}+1}=s)\\cdot\\mathbb{I}\\{s\\in\\mathcal{S}_{h^{\\prime}+1,\\varepsilon}\\}]}\\\\ &{\\le\\displaystyle\\sum_{i=1}^{\\infty}z^{-i+1}P_{h}^{\\varepsilon\\log(1)}(\\mathcal{S}_{h^{\\prime}+1,\\varepsilon}\\mid z)}\\\\ &{=\\displaystyle\\sum_{i=1}^{\\lfloor\\lfloor\\eta\\rfloor}z^{\\frac{\\delta}{\\delta}\\rfloor+1}P_{h}^{\\varepsilon\\log(1)}(\\mathcal{S}_{h^{\\prime}+1,\\varepsilon}\\mid z)+\\kappa}\\\\ &{\\le\\displaystyle\\sum_{i=1}^{\\lfloor\\eta\\rfloor+1}2^{-i+1}P_{h}^{\\varepsilon\\log(1)}(\\mathcal{S}_{h^{\\prime}+1,\\varepsilon}\\mid z)\\cdot\\mathbb{I}\\{P_{h}^{\\varepsilon\\log(1)}(\\mathcal{S}_{h^{\\prime}+1,\\varepsilon}\\mid z)\\ge\\kappa\\}+3\\kappa}\\\\ &{\\le\\displaystyle\\sum_{i=1}^{\\lfloor\\eta\\rfloor+1}2^{-i+1}P_{h}^{\\varepsilon\\log(1)}(\\mathcal{S}_{h^{\\prime}+1,\\varepsilon}\\mid z)\\cdot\\mathbb{I}\\{P_{h}^{\\varepsilon\\log(1)}(\\mathcal{S}_{h^{\\prime}+1,\\varepsilon}\\mid z)\\ge\\kappa\\}}\\\\ &{\\le2\\displaystyle\\sum_{i=1}^{\\lfloor\\eta\\rfloor+1}\\mathbb{E}_{s\\sim h_{h}^{(\\eta\\log(1),\\varepsilon}\\mid z^{\\prime}\\mid s_{h^{\\prime}+1}=s)}\\lvert P_{h}^{\\varepsilon\\log(1)}(\\mathcal{S}_{h^{\\prime}+1,\\varepsilon}\\mid z)\\cdot\\mathbb{I}\\{P_{h}^{\\varepsilon\\log(1)}(\\mathcal{S}_{h^{\\prime}+1,\\varepsilon}\\mid z)\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "for any $\\lambda_{i}\\in\\triangle_{S_{h^{\\prime}+1,i}}$ . Note also that, since $\\pi_{\\mathrm{exp}}^{h^{\\prime}-1}$ plays randomly for all $h^{\\prime\\prime}\\geq h^{\\prime}$ , we have: ", "page_idx": 22}, {"type": "equation", "text": "$$\nw_{h}^{\\mathrm{real},\\pi_{\\mathrm{exp}}^{h^{\\prime}-1}}(\\mathcal{Z}^{\\prime}\\mid s_{h^{\\prime}+1}=s)\\geq\\frac{1}{A^{h-h^{\\prime}}}\\cdot w_{h}^{\\mathrm{real},\\pi}(\\mathcal{Z}^{\\prime}\\mid s_{h^{\\prime}+1}=s),\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "since with probability $1/A^{h-h^{\\prime}}$ on any given episode, $\\pi_{\\mathrm{exp}}^{h^{\\prime}-1}$ will play the same sequence of actions as $\\pi$ from steps $h^{\\prime}$ to $h$ . It follows that we can bound the above as: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\leq2A^{h-h^{\\prime}}\\cdot\\underset{\\rightharpoonup}{\\sum}\\ \\underset{i=1}{\\overset{{\\mathrm{lim}}}{\\sum}}(\\gamma_{k})\\underset{s\\to h,\\uph}{\\sum}\\Big[w_{h}^{\\mathrm{edri}}\\frac{n_{h}^{\\prime\\prime-1}}{\\alpha_{h}}\\big({Z^{\\prime}}\\big\\vert\\ {s}_{h^{\\prime}+1}=s\\big)\\Big]P_{h^{\\prime}}^{\\mathrm{edri}}(S_{h^{\\prime}+1,i}\\big\\vert\\ {z})\\cdot\\big[\\big\\{P_{h^{\\prime}}^{\\mathrm{edri}}(S_{h^{\\prime}+1,i}\\big\\vert\\ {z})\\geq\\kappa\\big\\}\\Big]}\\\\ &{\\stackrel{(a)}{\\leq}\\frac{2A^{h-h^{\\prime}}\\gamma}{\\kappa}\\cdot\\underset{\\rightharpoonup}{\\sum}\\ \\underset{i=1}{\\overset{{\\mathrm{lim}}}{\\sum}}\\ \\underset{s\\to h,\\uph}{\\sum}\\Big[w_{h}^{\\mathrm{edri}}\\frac{n_{h}^{\\prime\\prime-1}}{\\alpha_{h}}\\big({Z^{\\prime}}\\big\\vert\\ {s}_{h^{\\prime}+1}=s\\big)\\Big]\\mathbb{E}^{\\mathrm{edri},\\mathbf{n}^{\\prime\\prime-1}}\\big[P_{h^{\\prime}}^{\\mathrm{edri}}(S_{h^{\\prime}+1,i}\\big\\vert\\ {z}_{h^{\\prime}})\\Big]\\big[\\big\\{P_{h^{\\prime}}^{\\mathrm{edri}}(S_{h^{\\prime}})-\\big\\}\\big(\\big\\vert\\ {s}_{h}+\\big\\vert\\ {z}_{h^{\\prime}}\\big\\vert\\big)\\Big]}\\\\ &{\\leq\\frac{2\\gamma A^{h-h^{\\prime}}}{\\kappa}\\cdot\\underset{\\rightharpoonup}{\\sum}\\ \\underset{i=1}{\\overset{{\\mathrm{lim}}}{\\sum}}\\ \\underset{s\\to h,\\uplus}{\\sum}\\Big[w_{h}^{\\mathrm{edri}}\\frac{n_{h}^{\\prime\\prime-1}}{\\alpha_{h}}\\big({Z^{\\prime}}\\big\\vert\\ {s}_{h^{\\prime}+1}=s\\big)\\Big]\\cdot w_{h^{\\prime}+1}^{\\mathrm{edri},\\mathbf{n}^{\\prime\\prime-1}}\\big(S_{h^{\\prime}+1,i}\\big)+3\\kappa}\\\\ &{\\stackrel{(b)}{=}\\frac{2\\gamma A^{h-h^{\\prime}}}{\\kappa}\\cdot\\underset \n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $(a)$ follows from (B.6) and since $P_{h^{\\prime}}^{\\mathsf{r e a l}}(S_{h^{\\prime},i}\\mid z)\\,\\geq\\,\\kappa$ , and $(b)$ follows choosing $\\lambda_{i}(s)\\,=$ ", "page_idx": 22}, {"type": "text", "text": "$w_{h^{\\prime}+1}^{\\mathsf{r e a l},\\pi_{\\exp}^{h^{\\prime}-1}}(s)/w_{h^{\\prime}+1}^{\\mathsf{r e a l},\\pi_{\\exp}^{h^{\\prime}-1}}(S_{h^{\\prime}+1,i})\\cdot\\mathbb{I}\\{s\\in\\bar{S}_{h^{\\prime}+1,i}\\}.$ . We therefore have that, for all $z\\in\\bar{\\mathcal{Z}}_{h^{\\prime}}$ : ", "page_idx": 22}, {"type": "equation", "text": "$$\nw_{h}^{\\mathsf{r e a l},\\pi}(\\mathcal{Z}^{\\prime}\\mid z_{h^{\\prime}}=z)\\leq\\frac{2\\gamma A^{h-h^{\\prime}}}{\\kappa}\\cdot w_{h}^{\\mathsf{r e a l},\\pi_{\\exp}^{h^{\\prime}-1}}(\\mathcal{Z}^{\\prime})+3\\kappa.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Controlling events. Consider events $\\mathcal{E}:=\\left\\{z_{h}\\in\\mathcal{Z}^{\\prime}\\right\\}$ and $\\mathcal{E}_{h^{\\prime}}:=\\{z_{h^{\\prime}}\\in\\bar{\\mathcal{Z}}_{h^{\\prime}}\\}$ . We then have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{w_{h}^{\\mathrm{real},\\pi}(\\mathcal{Z}^{\\prime})=\\mathbb{P}^{\\mathrm{real},\\pi}[\\mathcal{E}]}\\\\ &{\\quad\\quad\\quad\\quad\\quad=\\mathbb{P}^{\\mathrm{real},\\pi}[\\mathcal{E}\\cap\\mathcal{E}_{h-1}]+\\mathbb{P}^{\\mathrm{real},\\pi}[\\mathcal{E}\\cap\\mathcal{E}_{h-1}^{c}]}\\\\ &{\\quad\\quad\\quad\\quad=\\displaystyle\\sum_{h^{\\prime}=h-k^{\\star}+1}^{h}\\mathbb{P}^{\\mathrm{real},\\pi}[\\mathcal{E}\\cap\\mathcal{E}_{h^{\\prime}-1}\\cap\\bigcap_{i=h^{\\prime}}^{h-1}\\mathcal{E}_{i}^{c}]+\\mathbb{P}^{\\mathrm{real},\\pi}[\\mathcal{E}\\cap\\mathcal{E}_{h-k^{\\star}-1}\\cap\\bigcap_{i=h-k^{\\star}}^{h-1}\\mathcal{E}_{i}^{c}]}\\\\ &{\\quad\\quad\\quad\\quad\\leq\\displaystyle\\sum_{h^{\\prime}=h-k^{\\star}+1}^{h}\\mathbb{P}^{r e a l,\\pi}[\\mathcal{E}\\cap\\mathcal{E}_{h^{\\prime}-1}]+\\mathbb{P}^{r e a l,\\pi}[\\mathcal{E}\\cap\\mathcal{E}_{h-k^{\\star}-1}\\cap\\bigcap_{i=h-k^{\\star}}^{h-1}\\mathcal{E}_{i}^{c}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We now analyze each of these terms. First, note that ", "page_idx": 23}, {"type": "text", "text": "Prea $\\begin{array}{r}{^{\\pi}[\\mathcal{E}\\cap\\mathcal{E}_{h^{\\prime}-1}]=\\mathbb{P}^{{\\mathrm{real}},\\pi}[\\mathcal{E}~|~\\mathcal{E}_{h^{\\prime}-1}]\\mathbb{P}^{{\\mathrm{real}},\\pi}[\\mathcal{E}_{h^{\\prime}-1}]\\leq\\mathbb{P}^{{\\mathrm{real}},\\pi}[\\mathcal{E}~|~\\mathcal{E}_{h^{\\prime}-1}]=w_{h}^{{\\mathrm{real}},\\pi}(\\mathcal{Z}^{\\prime}~|~z_{h^{\\prime}-1}\\in\\bar{\\mathcal{Z}}_{h^{\\prime}-1}).}\\end{array}$ We can then bound ", "page_idx": 23}, {"type": "equation", "text": "$$\nw_{h}^{\\mathrm{real},\\pi}(\\mathcal{Z}^{\\prime}\\mid z_{h^{\\prime}-1}\\in\\bar{\\mathcal{Z}}_{h^{\\prime}-1})\\leq\\frac{2\\gamma A^{h-h^{\\prime}-1}}{\\kappa}\\cdot w_{h}^{\\mathrm{real},\\pi_{\\mathrm{exp}}^{h^{\\prime}-2}}(\\mathcal{Z}^{\\prime})+3\\kappa\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where the inequality follows from (B.7). For the second term, we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}^{\\mathrm{real},\\pi}\\bigl[\\mathcal{E}\\cap\\mathcal{E}_{h-k^{\\star}-1}\\cap\\displaystyle\\bigcap_{i=h-k^{\\star}}^{h-1}\\mathcal{E}_{i}^{c}\\bigr]\\leq\\mathbb{P}^{\\mathrm{real},\\pi}\\bigl[\\mathcal{E}\\cap\\displaystyle\\bigcap_{i=h-k^{\\star}}^{h-1}\\mathcal{E}_{i}^{c}\\bigr]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\displaystyle\\prod_{h=1}^{h-1}\\mathcal{E}_{i}^{c}\\bigr]\\cdot\\displaystyle\\prod_{j=1}^{k^{\\star}}\\mathbb{P}^{\\mathrm{real},\\pi}\\bigl[\\mathcal{E}_{h-j}^{c}\\mid\\displaystyle\\bigcap_{i=h-k^{\\star}}^{h-j-1}\\mathcal{E}_{i}^{c}\\bigr].}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Note, however, that $\\mathbb{P}^{\\mathtt{r e a l,\\pi}}[\\mathcal{E}\\mid\\bigcap_{i=h-k^{\\star}}^{h-1}\\mathcal{E}_{i}^{c}]\\leq\\xi$ and $\\mathbb{P}^{\\mathrm{real},\\pi}[\\mathcal{E}_{h-j}^{c}\\ |\\ \\bigcap_{i=h-k^{\\star}}^{h-j-1}\\mathcal{E}_{i}^{c}]\\leq\\xi$ for all $j$ . We therefore can bound the above as ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\xi^{k^{\\star}+1}\\leq\\kappa.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Altogether, then, we have that ", "page_idx": 23}, {"type": "equation", "text": "$$\nw_{h}^{\\mathrm{real},\\pi}(\\mathcal{Z}^{\\prime})\\leq\\sum_{h^{\\prime}=h-k^{\\star}+1}^{h}\\frac{2\\gamma A^{h-h^{\\prime}-1}}{\\kappa}\\cdot w_{h}^{\\mathrm{real},\\pi_{\\mathrm{exp}}^{h^{\\prime}-2}}(\\mathcal{Z}^{\\prime})+4\\kappa.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Furthermore, since \u03c0exp = unif({\u03c0ehxp}hH=1), we have wh $\\begin{array}{r}{w_{h}^{\\mathsf{r e a l},\\pi_{\\exp}^{h^{\\prime}-2}}(\\mathcal{Z}^{\\prime})\\,\\le\\,H w_{h}^{\\mathsf{r e a l},\\pi_{\\exp}}(\\mathcal{Z}^{\\prime})}\\end{array}$ wrheal,\u03c0exp(Z\u2032), so we conclude that ", "page_idx": 23}, {"type": "equation", "text": "$$\nw_{h}^{\\mathsf{r e a l},\\pi}(\\mathcal{Z}^{\\prime})\\leq\\frac{4H\\gamma A^{k^{\\star}-2}}{\\kappa}\\cdot w_{h}^{\\mathsf{r e a l},\\pi_{\\mathrm{exp}}}(\\mathcal{Z}^{\\prime})+4\\kappa.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "B.3 Proof of Unconstrained Upper Bound ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Theorem 2. Assume that one of the two conditions is met: ", "page_idx": 23}, {"type": "text", "text": "1. For each $h$ , $\\pi_{\\mathrm{exp}}^{h}$ plays actions uniformly at random for $h^{\\prime}>h$ , ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\lambda_{\\operatorname*{min}}\\left(\\Lambda_{\\pi_{\\mathrm{exp}}^{h},h}^{\\mathsf{s}}\\right)\\ge\\bar{\\lambda}_{\\operatorname*{min}},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "and ", "page_idx": 23}, {"type": "equation", "text": "$$\nT\\ge c\\cdot\\frac{V_{\\mathrm{max}}^{4}H^{4}d^{2}A^{2(k^{\\star}-2)}\\log(2H|\\mathcal{F}|/\\delta)}{\\epsilon^{4}\\epsilon_{\\mathrm{sim}}^{2}},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "for ", "page_idx": 23}, {"type": "equation", "text": "$$\nk^{\\star}=\\lceil\\frac{\\log_{A}\\frac{64H V_{\\mathrm{max}}}{\\epsilon}}{\\log_{A}1/\\xi}\\rceil,\\quad\\xi=2\\sqrt{\\frac{2H A}{\\bar{\\lambda}_{\\mathrm{min}}}\\cdot\\epsilon_{\\mathrm{sim}}}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "2. $\\epsilon_{\\mathrm{sim}}\\le\\epsilon/4H^{2}$ and ", "page_idx": 23}, {"type": "equation", "text": "$$\nT\\geq\\frac{16H^{2}\\log\\frac{4}{\\delta}}{\\epsilon^{2}}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Then with probability at least 1 \u2212\u03b4, Algorithm 1 returns a \u03c0  such that V 0real,\u03c0real,\u22c6\u2212 $V_{0}^{\\mathsf{r e a l},\\pi^{\\mathsf{r e a l},\\star}}-V_{0}^{\\mathsf{r e a l},\\widehat{\\pi}}\\leq\\epsilon$ ", "page_idx": 23}, {"type": "text", "text": "Proof. We consider each of the conditions above. ", "page_idx": 23}, {"type": "text", "text": "Condition 1. First, note that by our assumption on $\\pi_{\\mathrm{exp}}$ and applying Lemma B.4 with $\\kappa\\,=$ $\\frac{\\epsilon}{64H V_{\\mathrm{max}}}$ and \u03b3 =H\u03f5sim , for any $\\pi$ and $\\mathcal{Z}^{\\prime}\\subseteq\\mathcal{S}\\times\\mathcal{A}$ , we have ", "page_idx": 24}, {"type": "equation", "text": "$$\nw_{h}^{\\mathrm{real,}\\pi}(\\mathcal{Z}^{\\prime})\\leq\\frac{256d H V_{\\mathrm{max}}A^{k^{\\star}-2}}{\\epsilon\\epsilon_{\\mathrm{sim}}}\\cdot w_{h}^{\\mathrm{real,}\\pi_{\\mathrm{exp}}}(\\mathcal{Z}^{\\prime})+\\frac{\\epsilon}{16H V_{\\mathrm{max}}}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "for ", "page_idx": 24}, {"type": "equation", "text": "$$\nk^{\\star}=\\lceil\\frac{\\log_{A}\\frac{64H V_{\\mathrm{max}}}{\\epsilon}}{\\log_{A}1/\\xi}\\rceil,\\quad\\xi=2\\sqrt{\\frac{2H A}{\\bar{\\lambda}_{\\mathrm{min}}}\\cdot\\epsilon_{\\mathrm{sim}}}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "By Lemma B.1 we then have that, with probability at least $1-\\delta^{6}$ , ", "page_idx": 24}, {"type": "equation", "text": "$$\nV_{0}^{r\\mathbf{eal},\\pi^{r\\mathbf{eal},\\star}}-V_{0}^{r\\mathbf{eal},\\widehat{\\pi}}\\leq\\frac{256d H V_{\\operatorname*{max}}A^{k^{\\star}-2}}{\\epsilon\\epsilon_{\\sin}}\\cdot4H\\sqrt{\\frac{256V_{\\operatorname*{max}}^{2}\\log(2H|\\mathcal{F}|/\\delta)}{T}}+\\epsilon/4\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where the last inequality follows under our condition on $T$ . ", "page_idx": 24}, {"type": "text", "text": "Condition 2. By Lemma A.3, we have that V 0real,\u22c6\u2212V 0real,\u03c0sim,\u22c6 $\\begin{array}{r}{V_{0}^{\\mathsf{r e a l},\\star}-V_{0}^{\\mathsf{r e a l},\\pi^{\\mathsf{s i m},\\star}}\\le2H^{2}\\epsilon_{\\mathrm{sim}}}\\end{array}$ . Thus, if $\\epsilon_{\\mathrm{sim}}\\le\\epsilon/4H^{2}$ , we have $V_{0}^{\\mathsf{r e a l,\\star}}-V_{0}^{\\mathsf{r e a l},\\pi^{\\mathsf{s i m,\\star}}}\\le\\epsilon/2$ . ", "page_idx": 24}, {"type": "text", "text": "Concluding the Proof. By what we have shown, as long as one of our conditions is met, we will have that with probability least $1-\\delta/2$ , there exists $\\pi\\in\\{\\pi^{\\widehat{f}},\\pi^{\\mathsf{s i m},\\star}\\}$ such that $V_{0}^{\\mathsf{r e a l,\\star}}-V_{0}^{\\mathsf{r e a l,\\pi}}\\leq$ . Denote this policy as $\\widetilde{\\pi}$ . ", "page_idx": 24}, {"type": "text", "text": "Note that V 0real,\u03c0= Ereal,\u03c0[ and that $\\textstyle\\sum_{h=0}^{H-1}r_{h}\\in[0,H]$ almost surely. Consider playing $\\pi$ for $T/4$ episodes in real and let $R^{i}$ denote the total return of the ith episode. Let ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\widehat V_{0}^{\\pi}:=\\frac{4}{T}\\sum_{i=1}^{T/4}R^{i}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "By Hoeffding\u2019s inequality we have that, with probability at least $1-\\delta/4$ : ", "page_idx": 24}, {"type": "equation", "text": "$$\n|\\widehat{V}_{0}^{\\pi}-V_{0}^{\\mathsf{r e a l},\\pi}|\\leq H\\sqrt{\\frac{4\\log\\frac{4}{\\delta}}{T}}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Thus, if ", "page_idx": 24}, {"type": "equation", "text": "$$\nT\\geq\\frac{16H^{2}\\log\\frac{4}{\\delta}}{\\epsilon^{2}},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "we have that $|\\widehat{V}_{0}^{\\pi}-V_{0}^{\\mathsf{r e a l},\\pi}|\\leq\\epsilon/2$ . Union bounding over this for both $\\pi\\in\\{\\pi^{\\widehat{f}},\\pi^{\\mathsf{s i m},\\star}\\}$ , we have that with probability at least $1-\\delta/2$ : ", "page_idx": 24}, {"type": "equation", "text": "$$\nV_{0}^{r e a l,\\widehat{\\pi}}\\geq\\widehat{V}_{0}^{\\widehat{\\pi}}-\\epsilon/4\\geq\\widehat{V}_{0}^{\\widetilde{\\pi}}-\\epsilon/4\\geq V_{0}^{r e a l,\\widetilde{\\pi}}-\\epsilon/2.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "It follows that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{V^{\\mathsf{r e a l},\\star}-V_{0}^{\\mathsf{r e a l},\\widehat{\\pi}}\\le V^{\\mathsf{r e a l},\\star}-V_{0}^{\\mathsf{r e a l},\\widetilde{\\pi}}+\\epsilon/2\\le\\epsilon.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "The proof follows from a union bound and our condition on $T$ (note that (B.9) is satisfied in both cases). ", "page_idx": 24}, {"type": "text", "text": "Proof of Theorem 1. We first assume that $\\zeta\\,\\leq\\,\\frac{\\lambda_{\\operatorname*{min}}^{\\star}}{4d}$ , for $\\zeta$ the input regularization value \u22c6given to Algorithm 5 by Algorithm 1, and Condition 1 of Theorem 2, and show that in this case $A^{k^{\\star}-2}$ is at most polynomial in problem parameters. ", "page_idx": 25}, {"type": "text", "text": "First, by Lemma C.7 we have that, under the assumption that $\\zeta\\leq\\frac{\\lambda_{\\operatorname*{min}}^{\\star}}{4d}$ , the policy $\\pi_{\\mathrm{exp}}^{h}$ given by the uniform mixture of policies returned by Algorithm 5 will, with probability at least $1-\\delta$ , satisfy $\\begin{array}{r}{\\lambda_{\\operatorname*{min}}(\\Lambda_{\\pi_{\\mathrm{exp}}^{h},h}^{\\mathfrak{s}})\\geq\\frac{\\lambda_{\\operatorname*{min}}^{\\star}}{8d}}\\end{array}$ under Assumption 3. Plugging $\\begin{array}{r}{\\bar{\\lambda}_{\\mathrm{min}}\\leftarrow\\frac{\\lambda_{\\mathrm{min}}^{\\star}}{8d}}\\end{array}$ \u03bb8mdin into Theorem 2, we have that $\\begin{array}{r}{\\xi=2\\sqrt{\\frac{16d H A}{\\lambda_{\\operatorname*{min}}^{\\star}}\\cdot\\epsilon_{\\mathrm{sim}}}}\\end{array}$ . Now note that ", "page_idx": 25}, {"type": "equation", "text": "$$\nA^{k^{\\star}-2}\\le A^{\\frac{\\log_{A}64H V_{\\operatorname*{max}}/\\epsilon}{\\log_{A}1/\\xi}}=\\left(\\frac{64H V_{\\operatorname*{max}}}{\\epsilon}\\right)^{1/\\log_{A}1/\\xi}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "It then suffices that we show $1/\\log_{A}1/\\xi\\leq1\\iff1/A\\geq\\xi$ . However, this is clearly met by our condition on $\\epsilon_{\\mathrm{sim}}$ . Thus, as long as ", "page_idx": 25}, {"type": "equation", "text": "$$\nT\\geq c\\cdot\\frac{V_{\\mathrm{max}}^{6}H^{6}d^{2}\\log(2H T|\\mathcal{F}|/\\delta)}{\\epsilon^{6}\\epsilon_{\\mathrm{sim}}^{2}},\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "by Theorem 2 we have that $\\widehat{\\pi}$ is $\\epsilon$ -optimal. ", "page_idx": 25}, {"type": "text", "text": "Now, if \u03f5sim \u2264\u03f5/4H2 and T \u226516H2 l\u03f5og 4/\u03b4, we also have that $\\widehat{\\pi}$ is $\\epsilon$ -optimal, by Theorem 2. Thus, in the first case, we at most will require ", "page_idx": 25}, {"type": "equation", "text": "$$\nT\\geq c\\cdot\\frac{V_{\\mathrm{max}}^{6}H^{10}d^{2}\\log(2H T|\\mathcal{F}|/\\delta)}{\\epsilon^{8}}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "to produce a policy that is $\\epsilon$ -optimal, since otherwise we will be in the second case. ", "page_idx": 25}, {"type": "text", "text": "It remains to justify the assumption that $\\zeta\\leq\\frac{\\lambda_{\\operatorname*{min}}^{\\star}}{4d}$ . Note that the condition of (4.1) is only required in the first case. Furthermore, if $\\epsilon_{\\mathrm{sim}}\\le\\epsilon/4H^{2}$ we will be in the second case. Thus, in the first case, we will have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\frac{\\epsilon}{4H^{2}}\\leq\\epsilon_{\\mathrm{sim}}\\leq\\frac{\\lambda_{\\operatorname*{min}}^{\\star}}{64d H A^{3}}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Rearranging this we obtain that, to be in the first case, we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\frac{16d A^{3}\\epsilon}{H}\\leq\\lambda_{\\mathrm{min}}^{\\star}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "By our choice of \u03b6 = 4AH3 \u03f5 , we then have that $\\zeta\\leq\\frac{\\lambda_{\\operatorname*{min}}^{\\star}}{4d}$ . By Lemma C.7 and our choice of $\\zeta$ , we have that Oracle 4.2 is called at most $\\mathrm{poly}(d,H,\\epsilon^{-1},\\log\\textstyle{\\frac{1}{\\delta}})$ times, and we call the oracle of Oracle 4.1 only $H$ times. The result the follows from a union bound and rescaling $\\delta$ . \u53e3 ", "page_idx": 25}, {"type": "text", "text": "B.4 Reducing the Version Space ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "As we noted, in general, given that we do not assume that $\\phi^{\\mathsf{r}}$ is unknown, $\\log\\left|{\\mathcal{F}}\\right|$ could be significantly greater than the dimension. One might hope that, given access to $\\mathcal{M}^{\\sf s i m}$ , we can reduce this dependence somewhat. We next show that this is possible given access to the following constrained regression oracle. ", "page_idx": 25}, {"type": "text", "text": "Oracle B.1 (Constrained Regression Oracle). We assume access to a regression oracle such that, for any $h$ and datasets $\\{(s^{t},a^{t},y^{t})\\}_{t=1}^{T}$ and $\\{(\\widetilde{s}^{t},\\widetilde{a}^{t},\\widetilde{y}^{t})\\}_{t=1}^{\\widetilde{T}}$ , we can compute: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\widehat{f}_{h}=\\underset{f\\in\\mathcal{F}_{h}}{\\arg\\operatorname*{min}}\\sum_{t=1}^{T}(f(s^{t},a^{t})-y^{t})^{2}\\quad\\mathrm{s.t.}\\quad\\sum_{t=1}^{\\widetilde T}(f(\\widetilde{s}^{t},\\widetilde{a}^{t})-\\widetilde{y}^{t})^{2}\\leq\\gamma.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "While in general the oracle of Oracle B.1 cannot be reduced to the oracle of Oracle 4.1, under certain conditions on $\\mathcal{F}$ this is possible. Given this oracle, we have the following result. ", "page_idx": 25}, {"type": "text", "text": "Theorem 3. Assume that $\\begin{array}{r}{\\epsilon_{\\mathrm{sim}}\\leq\\frac{\\lambda_{\\operatorname*{min}}^{\\star}}{64d H A^{3}}}\\end{array}$ Then if ", "page_idx": 26}, {"type": "equation", "text": "$$\nT\\geq\\tilde{\\mathcal{O}}\\left(\\frac{d^{2}H^{16}}{\\epsilon^{8}}\\cdot\\log\\frac{H|\\tilde{\\mathcal{F}}|}{\\delta}\\right),\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "with probability at least $1-\\delta$ , Algorithm $^{4}$ returns policy $\\widehat{\\pi}$ such that $\\begin{array}{r}{V_{0}^{\\mathsf{r e a l},\\pi^{\\mathrm{real},\\star}}-V_{0}^{\\mathsf{r e a l},\\widehat{\\pi}}\\leq\\epsilon,}\\end{array}$ where ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\widetilde{\\mathcal{F}}:=\\left\\{f\\in\\mathcal{F}\\ :\\ \\operatorname*{sup}_{\\pi}\\big(\\mathbb{E}^{\\mathrm{sim},\\pi}[f_{h}(s_{h},a_{h})-T^{\\mathrm{sim}}f_{h+1}(s_{h},a_{h})]\\big)^{2}\\leq\\alpha\\cdot\\epsilon_{\\mathrm{sim}}^{2}\\right\\}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "for $\\begin{array}{r}{\\alpha=\\widetilde{\\mathcal{O}}(A d H^{3}{\\cdot}\\log^{2}\\frac{\\log|\\mathcal{F}|/\\delta}{\\epsilon_{\\mathrm{sim}}})}\\end{array}$ . Furthermore, the computation oracles of Oracle 4.2 and Oracle B.1 are called at most $\\begin{array}{r}{\\mathrm{?oly}(d,\\widehat{A},H,\\epsilon^{-1},\\log\\frac{|\\mathcal{F}|}{\\delta})}\\end{array}$ times. ", "page_idx": 26}, {"type": "text", "text": "Theorem 3 shows that, rather than paying for the full complexity of $\\mathcal{F}$ , we can pay only for the subset of $\\mathcal{F}$ that is Bellman-consistent on $\\mathcal{M}^{\\sf s i m}$ . ", "page_idx": 26}, {"type": "text", "text": "B.4.1 Algorithm and Proof ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Algorithm 4 sim-to-real transfer via simulated exploration (SIM2EXPLORE)   \n1: input: tolerance $\\epsilon$ , confidence $\\delta$ , budget $T$ , $Q.$ -value function class $\\mathcal{F}$   \n2: $\\begin{array}{r}{\\Pi_{\\mathrm{exp}}^{h}\\leftarrow\\mathrm{LEARNExPPOLICIES}(\\mathcal{M}^{\\mathrm{sim}},\\delta,\\frac{4A^{3}\\epsilon}{H},h)}\\end{array}$ for all $h\\in[H]$   \n3: $\\iota\\gets\\mathcal{O}(\\log_{2}\\frac{V_{\\operatorname*{max}}A d H}{\\epsilon})$   \n4: for $\\ell=1,2,\\ldots,\\iota\\,\\mathfrak{$ do   \n5: $\\bar{\\epsilon}^{\\ell}\\leftarrow2^{-\\ell}$ , $T^{\\ell}\\gets T/2\\iota$ , $\\gamma^{\\ell}\\leftarrow10V_{\\mathrm{max}}^{2}(\\bar{\\epsilon}^{\\ell})^{2}$   \n6: $\\begin{array}{r}{\\beta_{\\ell}\\leftarrow\\frac{\\gamma^{\\ell}}{20V_{\\mathrm{max}}^{2}\\log\\frac{8H|\\mathcal{F}|}{\\delta}}}\\end{array}$ $\\mathfrak{D}_{\\mathsf{s i m}}^{\\ell}$   \n7: $\\widehat{\\pi}^{\\ell}\\gets$ EXPLOREREAL $(\\{\\operatorname{unif}(\\Pi_{\\mathrm{exp}}^{h})\\}_{h\\in[H]},T^{\\ell},\\mathfrak{D}_{\\mathsf{s i m}}^{\\ell},\\gamma^{\\ell})$ (Algorithm 3)   \n8: $\\widehat{V}_{0}^{\\widehat{\\pi}^{\\ell}}\\leftarrow$ average return running $\\widehat{\\pi}^{\\ell}$ in real $T^{\\ell}/2$ times   \n9: retur n $\\widehat\\pi\\leftarrow\\arg\\operatorname*{max}_{\\ell\\in[\\iota]}\\widehat V_{0}^{\\widehat\\pi^{\\ell}}$ ", "page_idx": 26}, {"type": "text", "text": "Theorem 4. Assume that one of the two conditions is met: ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "1. For each $h$ , $\\pi_{\\mathrm{exp}}^{h}$ plays actions uniformly at random for $h^{\\prime}>h$ , ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\lambda_{\\operatorname*{min}}\\left(\\Lambda_{\\pi_{\\mathrm{exp}}^{h},h}^{\\mathsf{s}}\\right)\\ge\\bar{\\lambda}_{\\operatorname*{min}},\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "and ", "page_idx": 26}, {"type": "equation", "text": "$$\nT\\ge c\\cdot\\frac{V_{\\mathrm{max}}^{4}H^{4}d^{2}A^{2(k^{\\star}-2)}\\iota\\log(16H|\\widetilde{\\mathcal{F}}|/\\delta)}{\\epsilon^{4}\\epsilon_{\\mathrm{sim}}^{2}},\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "for ", "page_idx": 26}, {"type": "equation", "text": "$$\nk^{\\star}=\\lceil\\frac{\\log_{A}\\frac{64H V_{\\mathrm{max}}}{\\epsilon}}{\\log_{A}1/\\xi}\\rceil,\\quad\\xi=2\\sqrt{\\frac{2H A}{\\bar{\\lambda}_{\\mathrm{min}}}}\\cdot\\epsilon_{\\mathrm{sim}}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "and ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widetilde{\\boldsymbol{\\mathscr{F}}}:=\\left\\{f\\in\\mathscr{F}\\ :\\ \\operatorname*{sup}_{\\pi}\\,(\\mathbb{E}^{\\mathrm{sim},\\pi}[f_{h}(s_{h},a_{h})-\\mathcal{T}^{\\mathrm{sim}}f_{h+1}(s_{h},a_{h})])^{2}\\right.}\\\\ &{\\qquad\\qquad\\qquad\\leq c\\left(\\log\\frac{\\log\\left.32H\\vert\\mathscr{F}\\right\\vert}{V_{\\operatorname*{max}}\\epsilon_{\\mathrm{sim}}^{2}}+1\\right)A d H V_{\\operatorname*{max}}^{2}\\log\\frac{48d\\log\\left.32H\\vert\\mathscr{F}\\right\\vert}{V_{\\operatorname*{max}}\\epsilon_{\\mathrm{sim}}^{2}}\\cdot\\epsilon_{\\sin}^{2}\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "2. $\\epsilon_{\\mathrm{sim}}\\leq\\epsilon/16H^{2}$ and ", "page_idx": 26}, {"type": "equation", "text": "$$\nT\\geq c\\cdot{\\frac{H^{2}\\iota\\log{\\frac{16\\iota}{\\delta}}}{\\epsilon^{2}}}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Then with probability at least 1\u2212\u03b4, Algorithm 4 returns a policy \u03c0  such that V 0real,\u03c0real,\u22c6\u2212 $V_{0}^{\\mathsf{r e a l},\\pi^{\\mathsf{r e a l},\\star}}-V_{0}^{\\mathsf{r e a l},\\widehat{\\pi}}\\leq\\epsilon$ ", "page_idx": 26}, {"type": "text", "text": "Proof. We break the proof into two cases. ", "page_idx": 26}, {"type": "text", "text": "Case 1: $\\epsilon_{\\mathrm{sim}}\\,\\geq\\,\\epsilon/16H^{2}$ . Let $\\bar{\\ell}=\\,\\lfloor\\log_{2}\\epsilon_{\\mathrm{sim}}^{-1}\\rfloor$ and note that $\\bar{\\ell}\\leq\\,\\iota$ in this case and that this is a deterministic quantity. Further, note that $\\gamma^{\\bar{\\ell}}\\in[10V_{\\mathrm{max}}^{2}\\epsilon_{\\mathrm{sim}}^{2}$ , $40V_{\\mathrm{max}}^{2}\\epsilon_{\\mathrm{sim}}^{2}]$ and $\\bar{\\epsilon}^{\\bar{\\ell}}\\in[\\epsilon_{\\mathrm{sim}},2\\epsilon_{\\mathrm{sim}}]$ . Note that by our assumption on \u03c0exp and applying Lemma B.4 with \u03ba =64H\u03f5Vmax and \u03b3 = $\\begin{array}{r}{\\gamma=\\frac{d}{H\\epsilon_{\\mathrm{sim}}}}\\end{array}$ for any $\\pi$ and $\\mathcal{Z}^{\\prime}\\subseteq\\mathcal{S}\\times\\mathcal{A}$ , we have ", "page_idx": 27}, {"type": "equation", "text": "$$\nw_{h}^{\\mathrm{real,}\\pi}(\\mathcal{Z}^{\\prime})\\leq\\frac{256d H V_{\\mathrm{max}}A^{k^{\\star}-2}}{\\epsilon\\epsilon_{\\mathrm{sim}}}\\cdot w_{h}^{\\mathrm{real,}\\pi_{\\mathrm{exp}}}(\\mathcal{Z}^{\\prime})+\\frac{\\epsilon}{16H V_{\\mathrm{max}}}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "for ", "page_idx": 27}, {"type": "equation", "text": "$$\nk^{\\star}=\\lceil\\frac{\\log_{A}\\frac{64H V_{\\mathrm{max}}}{\\epsilon}}{\\log_{A}1/\\xi}\\rceil,\\quad\\xi=2\\sqrt{\\frac{2H A}{\\bar{\\lambda}_{\\mathrm{min}}}\\cdot\\epsilon_{\\mathrm{sim}}}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "By Lemma B.1, as long as $\\beta^{\\bar{\\ell}}$ and $\\gamma^{\\bar{\\ell}}$ satisfy ", "page_idx": 27}, {"type": "equation", "text": "$$\n2V_{\\mathrm{max}}^{2}\\epsilon_{\\mathrm{sim}}^{2}+\\frac{43V_{\\mathrm{max}}^{2}\\beta_{\\bar{\\ell}}^{2}}{d H}\\cdot\\log\\frac{8H|\\mathcal{F}_{h}|}{\\delta}+6V_{\\mathrm{max}}^{2}\\beta_{\\bar{\\ell}}\\sqrt{\\frac{\\log\\frac{8H|\\mathcal{F}_{h}|}{\\delta}}{d H}}\\le\\gamma^{\\bar{\\ell}},\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "we have that with probability at least $1-2\\delta$ , ", "page_idx": 27}, {"type": "equation", "text": "$$\nV_{0}^{r\\mathrm{eal},\\pi^{\\mathrm{real},\\star}}-V_{0}^{r\\mathrm{eal},\\widehat{\\pi}^{\\ell}}\\leq\\frac{256d H V_{\\mathrm{max}}A^{k^{\\star}-2}}{\\epsilon\\epsilon_{\\mathrm{sim}}}\\cdot4H\\sqrt{\\frac{256V_{\\mathrm{max}}^{2}\\log(4H|\\widetilde{\\mathcal{F}}^{\\widetilde{\\ell}}|/\\delta)}{T^{\\ell}}+\\epsilon/4}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widetilde{{\\mathscr F}}^{\\bar{\\ell}}:=\\{f\\in{\\mathscr F}\\ :\\ {\\mathbb R}^{\\mathrm{sim},\\pi_{\\exp}^{\\mathrm{sim}}}[(f_{h}(s_{h},a_{h})-{\\mathscr T}^{\\mathrm{sim}}f_{h+1}(s_{h},a_{h}))^{2}]\\leq2\\gamma^{\\bar{\\ell}},\\forall h\\in[H]\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "However, since $\\begin{array}{r}{V_{\\mathrm{max}}^{2}\\epsilon_{\\mathrm{sim}}\\leq\\frac{1}{10}\\gamma^{\\bar{\\ell}}}\\end{array}$ , and by our choice of 20V 2  l\u03b3og 8H|F|, we see that (B.11) is met, so the conclusion holds. Note that, by Lemma C.5, we have that with probability at least $1-\\delta$ : ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widetilde{\\mathcal{F}}^{\\ell}\\subseteq\\Bigg\\{f\\in\\mathcal{F}\\ :\\ \\operatorname*{sup}\\ (\\mathbb{E}^{\\mathsf{s i m},\\pi}[f_{h}(s_{h},a_{h})-\\mathcal{T}^{\\mathsf{s i m}}f_{h+1}(s_{h},a_{h})])^{2}}\\\\ &{\\qquad\\qquad\\le\\left(4\\log\\frac{1}{\\beta_{\\ell}}+6\\right)A\\cdot\\left[48d H\\log\\frac{48d}{\\beta_{\\ell}^{2}}\\cdot2\\gamma^{\\ell}+V_{\\operatorname*{max}}^{2}\\sqrt{96d H\\log\\frac{48d}{\\beta_{\\ell}^{2}}\\log\\frac{1}{\\delta}}\\cdot\\beta_{\\bar{\\ell}}\\right]\\Bigg\\}}\\\\ &{\\subseteq\\Bigg\\{f\\in\\mathcal{F}\\ :\\ \\operatorname*{sup}\\ (\\mathbb{E}^{\\mathsf{s i m},\\pi}[f_{h}(s_{h},a_{h})-\\mathcal{T}^{\\mathsf{s i m}}f_{h+1}(s_{h},a_{h})])^{2}}\\\\ &{\\qquad\\qquad\\le c\\left(\\log\\frac{\\log\\frac{8d H|\\mathcal{F}|}{\\delta}}{V_{\\operatorname*{max}}\\epsilon_{\\operatorname*{sim}}^{2}}+1\\right)A d H V_{\\operatorname*{max}}^{2}\\log\\frac{48d\\log\\frac{8H|\\mathcal{F}|}{\\delta}}{V_{\\operatorname*{max}}\\epsilon_{\\operatorname*{sim}}^{2}}\\cdot\\epsilon_{\\sin}^{2}\\Bigg\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where the second inclusion follows from our setting of $\\beta_{\\bar{\\ell}}$ , and bounds on $\\gamma^{\\bar{\\ell}}$ . ", "page_idx": 27}, {"type": "text", "text": "Since $T^{\\ell}\\gets T/2\\iota$ , it follows that if ", "page_idx": 27}, {"type": "equation", "text": "$$\nT\\ge c\\cdot\\frac{d^{2}H^{4}V_{\\mathrm{max}}^{4}A^{2(k^{\\star}-2)}\\iota\\log(4H|\\widetilde{\\mathcal{F}}|/\\delta)}{\\epsilon^{4}\\epsilon_{\\mathrm{sim}}^{2}},\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "then we have that V 0real,\u03c0real,\u22c6 $V_{0}^{\\mathsf{r e a l},\\pi^{\\mathrm{real},\\star}}-V_{0}^{\\mathsf{r e a l},\\widehat{\\pi}^{\\ell}}\\leq\\epsilon/2$ . ", "page_idx": 27}, {"type": "text", "text": "Case 2: $\\epsilon_{\\mathrm{sim}}\\le\\epsilon/16H^{2}$ . By Lemma B.5 and our choice of $T_{\\mathrm{sim}}^{\\ell}$ , we have that with probability at least $1-\\delta$ , ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\gamma_{0}^{r\\mathbf{e}\\mathbf{a}\\mathrm{l},\\star}-V_{0}^{r\\mathbf{e}\\mathbf{a}\\mathrm{l},\\widehat{\\pi}^{\\prime}}\\leq6H\\left(2\\log\\frac{20V_{\\operatorname*{max}}^{2}\\log\\frac{8H|\\mathcal{F}|}{\\delta}}{\\gamma^{\\prime}}+3\\right)\\cdot\\sqrt{192A d H\\log\\frac{960d V_{\\operatorname*{max}}^{2}\\log\\frac{8H|\\mathcal{F}|}{\\delta}}{\\gamma^{\\prime}}\\cdot\\gamma^{\\prime}+3}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "$\\iota=\\mathcal{O}(\\log_{2}\\frac{V_{\\operatorname*{max}}A d H}{\\epsilon})$ and since $\\gamma^{\\iota}=10V_{\\mathrm{max}}^{2}(\\bar{\\epsilon}^{\\iota})^{2}=10V_{\\mathrm{max}}^{2}\\cdot2^{-2\\iota}$ , we can bound $V_{0}^{\\mathsf{r e a l,\\star}}-V_{0}^{\\mathsf{r e a l,\\widehat{\\pi}^{\\iota}}}\\leq\\epsilon/2$ ", "page_idx": 27}, {"type": "text", "text": "Completing the Proof. In either case, we have that with probability at least $1-\\delta$ , there exists some $\\widehat{i}\\in[l]$ such that $V_{0}^{\\mathsf{r e a l,\\star}}-V_{0}^{\\mathsf{r e a l,\\widehat{\\pi}}^{\\widehat{i}}}\\leq\\epsilon/2.$ . ", "page_idx": 28}, {"type": "text", "text": "Note that $V_{0}^{\\mathsf{r e a l},\\pi}=\\mathbb{E}^{\\mathsf{r e a l},\\pi}[\\sum_{h=0}^{H-1}r_{h}]$ and that $\\textstyle\\sum_{h=0}^{H-1}r_{h}\\in[0,H]$ almost surely. Consider playing $\\pi$ for $n$ episodes in real and let $R^{i}$ denote the total return of the $i$ th episode. Let ", "page_idx": 28}, {"type": "equation", "text": "$$\n{\\widehat{V}}_{0}^{\\pi}:={\\frac{1}{n}}\\sum_{i=1}^{n}R^{i}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "By Hoeffding\u2019s inequality we have that, with probability at least $1-\\delta/\\iota$ : ", "page_idx": 28}, {"type": "equation", "text": "$$\n|\\widehat{V}_{0}^{\\pi}-V_{0}^{\\mathrm{real},\\pi}|\\leq H\\sqrt{\\frac{\\log\\frac{2\\iota}{\\delta}}{n}}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Thus, if ", "page_idx": 28}, {"type": "equation", "text": "$$\nn\\ge\\frac{16H^{2}\\log\\frac{2\\iota}{\\delta}}{\\epsilon^{2}},\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "we have that $|\\widehat{V}_{0}^{\\pi}-V_{0}^{\\mathsf{r e a l},\\pi}|\\leq\\epsilon/2$ . However, as we run each $\\pi\\in\\widehat{\\Pi}^{\\ell}\\;T_{\\ell}/2=T/2\\iota$ times, and in either case we assume T \u2265c\u03b9\u03f5H22\u00b7 $\\begin{array}{r}{T\\ge\\frac{c\\iota H^{2}}{\\epsilon^{2}}\\cdot\\log\\frac{4\\iota}{\\delta}}\\end{array}$ , this will be met. Union bounding over this for all $\\widehat{\\pi}^{\\ell}$ , we have that with probability at least $1-\\delta$ : ", "page_idx": 28}, {"type": "equation", "text": "$$\nV_{0}^{r\\mathsf{e a l},\\widehat{\\pi}}\\geq\\widehat{V}_{0}^{\\widehat{\\pi}}-\\epsilon/4\\geq\\widehat{V}_{0}^{\\widehat{\\pi}^{\\widehat{i}}}-\\epsilon/4\\geq V_{0}^{r\\mathsf{e a l},\\widehat{\\pi}^{\\widehat{i}}}-\\epsilon/2.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "It follows that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{V^{\\mathsf{r e a l},\\star}-V_{0}^{\\mathsf{r e a l},\\widehat{\\pi}}\\le V^{\\mathsf{r e a l},\\star}-V_{0}^{\\mathsf{r e a l},\\widehat{\\pi}^{i}}+\\epsilon/2\\le\\epsilon.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "The result then follows from a union bound and rescaling $\\delta$ . ", "page_idx": 28}, {"type": "text", "text": "Proof of Theorem 3. The argument follows analogously to the proof of Theorem 1, but using Theorem 4 in place of Theorem 2. The bound on the number of oracle calls follows from Lemma C.3 and our choice of $\\beta_{\\ell}$ . \u53e3 ", "page_idx": 28}, {"type": "text", "text": "Lemma B.5. With probability at least $1-\\delta$ , for some $\\ell$ , we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V_{0}^{\\mathrm{sim},\\star}-V_{0}^{\\mathrm{sim},\\widehat{\\pi}^{\\ell}}\\leq6H\\left(2\\log\\frac{20V_{\\mathrm{max}}^{2}\\log\\frac{8H|\\mathcal{F}|}{\\delta}}{\\gamma^{\\ell}}+3\\right)\\cdot\\sqrt{192A d H\\log\\frac{960d V_{\\mathrm{max}}^{2}\\log\\frac{8H|\\mathcal{F}|}{\\delta}}{\\gamma^{\\ell}}\\cdot\\gamma^{\\ell}},}\\\\ &{\\overset{\\gamma^{\\mathrm{real},\\star}}{\\underset{0}{\\sim}}-V_{0}^{\\mathrm{real},\\widehat{\\pi}^{\\ell}}\\leq6H\\left(2\\log\\frac{20V_{\\mathrm{max}}^{2}\\log\\frac{8H|\\mathcal{F}|}{\\delta}}{\\gamma^{\\ell}}+3\\right)\\cdot\\sqrt{192A d H\\log\\frac{960d V_{\\mathrm{max}}^{2}\\log\\frac{8H|\\mathcal{F}|}{\\delta}}{\\gamma^{\\ell}}\\cdot\\gamma^{\\ell}}+}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Proof. By Lemma C.4 we have, with probability at least $1-\\delta$ , ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\gamma_{0}^{\\sin,\\star}-V_{0}^{\\sin,\\widehat{\\pi}^{\\ell}}\\leq2H\\left(2\\log\\frac{1}{\\beta_{\\ell}}+3\\right)\\cdot\\left[\\beta_{\\ell}\\sqrt{512V_{\\operatorname*{max}}^{2}A\\log\\frac{8H|\\mathcal{F}|}{\\delta}}+\\sqrt{96A d H\\log\\frac{48d}{\\beta_{\\ell}^{2}}\\cdot\\gamma^{\\ell}}\\right.}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\left.+\\sqrt{2A V_{\\operatorname*{max}}^{2}\\sqrt{96d H\\log\\frac{48d}{\\beta_{\\ell}^{2}}\\log\\frac{2}{\\delta}}\\cdot\\beta_{\\ell}}\\right]}\\\\ &{\\qquad\\qquad\\leq6H\\left(2\\log\\frac{20V_{\\operatorname*{max}}^{2}\\log\\frac{8H|\\mathcal{F}|}{\\delta}}{\\gamma^{\\ell}}+3\\right)\\cdot\\sqrt{192A d H\\log\\frac{960d V_{\\operatorname*{max}}^{2}\\log\\frac{8H|\\mathcal{F}|}{\\delta}}{\\gamma^{\\ell}}\\cdot\\gamma^{\\ell}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where the second inequality holds by our setting of $\\beta_{\\ell}$ . ", "page_idx": 28}, {"type": "text", "text": "We have ", "page_idx": 28}, {"type": "equation", "text": "$$\n-\\b{V}_{0}^{r\\mathrm{eal},\\hat{\\pi}^{t}}=\\b{V}_{0}^{r\\mathrm{eal},\\star}-\\b{V}_{0}^{r\\mathrm{eal},\\pi^{\\mathrm{in},\\star}}+\\b{V}_{0}^{r\\mathrm{eal},\\pi^{\\mathrm{in},\\star}}-\\b{V}_{0}^{\\ s\\mathrm{im},\\pi^{\\mathrm{in},\\star}}+\\b{V}_{0}^{\\ s\\mathrm{im},\\pi^{\\mathrm{in},\\star}}-\\b{V}_{0}^{\\ s\\mathrm{im},\\hat{\\pi}^{t}}+\\b{V}_{0}^{\\ s\\mathrm{im},\\hat{\\pi}^{t}}-\\b{V}_{0}^{r\\mathrm{eal},\\pi^{\\mathrm{in},\\star}},\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "By Lemma A.3, we can bound ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r}{V_{0}^{\\mathsf{r e a l},\\star}-V_{0}^{\\mathsf{r e a l},\\pi^{\\mathsf{s i m},\\star}}\\le2H^{2}\\epsilon_{\\mathrm{sim}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "and by Lemma A.1 we can bound ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r}{V_{0}^{\\mathrm{real},\\pi^{\\mathrm{sim},\\star}}-V_{0}^{\\mathrm{sim},\\pi^{\\mathrm{sim},\\star}}\\leq H^{2}\\epsilon_{\\mathrm{sim}},\\quad V_{0}^{\\mathrm{sim},\\widehat\\pi^{\\ell}}-V_{0}^{\\mathrm{real},\\widehat\\pi^{\\ell}}\\leq H^{2}\\epsilon_{\\mathrm{sim}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Combining this with our bound on $V_{0}^{\\mathsf{s i m,\\star}}-V_{0}^{\\mathsf{s i m,\\widehat{\\pi}}^{\\ell}}$ gives the result. ", "page_idx": 29}, {"type": "text", "text": "C Learning in sim ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "In this section we provide additional supporting lemmas for our main results and in particular, we focus on linear in sim. In Appendix C.1 we provide several technical results critical to showing that sim can be utilized to restrict the version space, as is done in Theorem 4. In order to restrict the version space using sim, sufficiently rich data must be collected from sim, and in Appendix C.2 we provide results on this data collection. Finally, in Appendix C.3 we provide a procedure to compute the exploration policies in sim which we ultimately transfer to real. ", "page_idx": 29}, {"type": "text", "text": "In Appendices C.1 and C.2, we let hypothesis $\\widetilde{f}$ and $\\widehat{f}$ be defined recursively as: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\widetilde{f}_{h}:=\\underset{f_{h}\\in\\mathcal{F}_{h}}{\\arg\\operatorname*{min}}\\,\\frac{1}{T_{\\sin}}\\sum_{t=1}^{T_{\\sin}}\\bigl(f_{h}\\big(\\widetilde{s}_{h}^{t},\\widetilde{a}_{h}^{t}\\big)-\\widetilde{r}_{h}^{t}-\\operatorname*{max}_{a^{\\prime}}\\widehat{f}_{h+1}\\big(\\widetilde{s}_{h+1}^{t},a^{\\prime}\\big)\\big)^{2}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "and $\\widehat{f}_{h}\\in\\mathcal{F}_{h}$ some hypothesis satisfying ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\frac{1}{T_{\\sin}}\\sum_{t=1}^{T_{\\sin}}(\\widehat{f}_{h}(\\widetilde{s}_{h}^{t},\\widetilde{a}_{h}^{t})-\\widetilde{f}_{h}(\\widetilde{s}_{h}^{t},\\widetilde{a}_{h}^{t}))^{2}\\le\\gamma\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "for parameter $\\gamma>0$ . ", "page_idx": 29}, {"type": "text", "text": "In Appendix C.1 we make the following assumption on the data generating process. ", "page_idx": 29}, {"type": "text", "text": "Assumption 5. Consider the dataset $\\mathfrak{D}_{\\mathsf{s i m}}=\\{(\\widetilde{s}_{0}^{t},\\widetilde{a}_{0}^{t},\\widetilde{r}_{0}^{t},\\ldots,\\widetilde{s}_{H-1}^{t},\\widetilde{a}_{H-1}^{t},\\widetilde{r}_{H-1}^{t})\\}_{t=1}^{T_{\\mathrm{sim}}}$ . We assume that episode $t$ in $\\mathfrak{D}_{\\mathsf{s i m}}$ was generated by playi ng   an $\\mathcal{F}_{t-1}$ - m easur a ble p o licy $\\widetilde{\\pi}_{\\mathrm{exp}}^{t}$ , and denote $\\pi_{\\mathrm{exp}}^{\\mathsf{s i m}}=\\operatorname{unif}\\bigl(\\{\\widetilde{\\pi}_{\\mathrm{exp}}^{t}\\}_{t=1}^{T_{\\mathrm{sim}}}\\bigr)$ . ", "page_idx": 29}, {"type": "text", "text": "We provide a specific instantiation of $\\pi_{\\mathrm{exp}}^{\\mathsf{s i m}}$ in Appendix C.2. In Appendix C.3, we provide a procedure for learning a set of policies which induce full-rank covariates in sim, a crucial piece in obtaining good exploration performance in real. ", "page_idx": 29}, {"type": "text", "text": "C.1 Regularizing with Data from sim ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Lemma C.1. With probability at least $1-\\delta$ : ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\frac{1}{l_{\\sin}}\\sum_{t=1}^{T_{\\sin}}(T^{\\mathrm{real}}\\widehat{f}_{h+1}(\\widetilde{s}_{h}^{t},\\widetilde{a}_{h}^{t})-\\widetilde{f}_{h}(\\widetilde{s}_{h}^{t},\\widetilde{a}_{h}^{t}))^{2}\\leq2V_{\\operatorname*{max}}^{2}\\epsilon_{\\sin}^{2}+\\frac{512V_{\\operatorname*{max}}^{2}}{T_{\\sin}}\\cdot\\log\\frac{4|\\mathcal{F}_{h}|}{\\delta}+V_{\\operatorname*{max}}^{2}\\sqrt{\\frac{2\\log\\frac{2|\\mathcal{F}_{h}|}{\\delta}}{T_{\\sin}}}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Proof. First, note that $\\mathcal{T}^{\\mathsf{r e a l}}\\widehat{f}_{h+1}\\in\\mathcal{F}_{h}$ by Assumption 4. ", "page_idx": 29}, {"type": "text", "text": "By Azuma-Hoeffding and a union bound, we have that, with probability at least $1-\\delta$ , for each $f,f^{\\prime}\\in\\mathcal{F}_{h}$ , ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=1}^{T_{\\mathrm{sim}}}(f_{h}(\\tilde{s}_{h}^{t},\\widetilde{a}_{h}^{t})-f_{h}^{\\prime}(\\widetilde{s}_{h}^{t},\\widetilde{a}_{h}^{t}))^{2}\\leq\\displaystyle\\frac{1}{T_{\\mathrm{sim}}}\\sum_{t=1}^{T_{\\mathrm{sim}}}\\mathbb{E}^{\\mathrm{sim},\\tilde{\\pi}_{\\mathrm{exp}}^{t}}\\big[(f_{h}(\\widetilde{s}_{h},\\widetilde{a}_{h})-f_{h}^{\\prime}(\\widetilde{s}_{h},\\widetilde{a}_{h}))^{2}\\big]+V_{\\operatorname*{max}}^{2}\\sqrt{\\frac{2\\log|\\mathcal{F}|}{T_{\\mathrm{sim}}}}}\\\\ &{\\displaystyle\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=\\mathbb{E}^{\\mathrm{sim},\\tau_{\\mathrm{exp}}^{\\mathrm{sim}}}\\big[(f_{h}(s_{h},a_{h})-f_{h}^{\\prime}(s_{h},a_{h}))^{2}\\big]+V_{\\operatorname*{max}}^{2}\\sqrt{\\frac{2\\log|\\mathcal{F}_{h}|/\\delta}{T_{\\mathrm{sim}}}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "In particular, this implies that ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\frac{1}{l_{\\sin}}\\sum_{t=1}^{T_{\\sin}}(T^{\\mathrm{real}}\\widehat{f}_{h+1}(\\widetilde{s}_{h}^{t},\\widetilde{a}_{h}^{t})-\\widetilde{f}_{h}(\\widetilde{s}_{h}^{t},\\widetilde{a}_{h}^{t}))^{2}\\leq\\mathbb{E}^{\\sin}\\!\\operatorname*{max}_{\\tau\\in\\mathbb{N}}^{\\infty}\\!\\left[(T^{\\mathrm{real}}\\widehat{f}_{h+1}(s_{h},a_{h})-\\widetilde{f}_{h}(s_{h},a_{h}))^{2}\\right]+V_{\\operatorname*{max}}^{2}\\!\\left[(\\widetilde{f}_{h+1}(s_{h},a_{h})-\\widetilde{f}_{h}(s_{h},a_{h}))^{2}\\right]\\!,\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "We can bound ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\begin{array}{r l}&{z^{\\mathrm{sim},\\pi_{\\mathrm{exp}}^{\\mathrm{sim}}}[(T^{\\mathrm{rea}})\\widehat{f}_{h+1}(s_{h},a_{h})-\\widetilde{f}_{h}(s_{h},a_{h}))^{2}]\\leq\\underbrace{2\\mathbb{E}^{\\mathrm{sim},\\pi_{\\mathrm{exp}}^{\\mathrm{sim}}}[(T^{\\mathrm{rea}})\\widehat{f}_{h+1}(s_{h},a_{h})-T^{\\mathrm{sim}}\\widehat{f}_{h+1}(s_{h},a_{h}))^{2}]}_{(a)}}\\\\ &{\\phantom{z^{\\mathrm{sim},\\pi_{\\mathrm{exp}}^{\\mathrm{sim}}}[(T^{\\mathrm{sim}},\\pi_{\\mathrm{exp}}^{\\mathrm{sim}}[(T^{\\mathrm{sim}}\\widehat{f}_{h+1}(s_{h},a_{h})-\\widetilde{f}_{h}(s_{h},a_{h}))^{2}]}.}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "To bound $(a)$ , we note that ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{T^{\\mathrm{real}}\\widehat{f}_{h+1}(s_{h},a_{h})-\\widetilde{f}_{h}(s_{h},a_{h})=\\mathbb{E}^{\\mathrm{real}}[\\underset{a^{\\prime}}{\\mathrm{max}}\\,\\widehat{f}_{h+1}(s^{\\prime},a^{\\prime})\\mid s,a]-\\mathbb{E}^{\\mathrm{sim}}[\\underset{a^{\\prime}}{\\mathrm{max}}\\,\\widehat{f}_{h+1}(s^{\\prime},a^{\\prime})\\mid s,a]}\\\\ {=\\underset{s^{\\prime}}{\\sum}(P_{h}^{\\mathrm{real}}(s^{\\prime}\\mid s,a)-P_{h}^{\\mathrm{sim}}(s^{\\prime}\\mid s,a))\\cdot\\underset{a^{\\prime}}{\\mathrm{max}}\\,\\widehat{f}_{h+1}(s^{\\prime},a^{\\prime})}\\\\ {\\leq V_{\\mathrm{max}}\\cdot\\underset{s^{\\prime}}{\\sum}|P_{h}^{\\mathrm{real}}(s^{\\prime}\\mid s,a)-P_{h}^{\\mathrm{sim}}(s^{\\prime}\\mid s,a)|}\\\\ {\\leq V_{\\mathrm{max}}\\epsilon_{\\mathrm{sim}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where the last inequality follows under Assumption 1. This gives that $(a)\\leq2V_{\\mathrm{max}}^{2}\\epsilon_{\\mathrm{sim}}^{2}$ . To bound $(b)$ , we apply Lemma 3 of [57], which gives that with probability at least $1-\\delta$ , ", "page_idx": 30}, {"type": "equation", "text": "$$\n(b)\\leq\\frac{512V_{\\mathrm{max}}^{2}}{T_{\\mathrm{sim}}}\\cdot\\log\\frac{4|\\mathcal{F}_{h}|}{\\delta}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Combining these with a union bound gives the result. ", "page_idx": 30}, {"type": "text", "text": "Lemma C.2. Consider the set ", "text_level": 1, "page_idx": 30}, {"type": "equation", "text": "$$\n\\widehat{\\mathcal{F}}_{h}:=\\left\\{f_{h}\\in\\mathcal{F}_{h}\\ :\\ \\frac{1}{T_{\\mathrm{sim}}}\\sum_{t=1}^{T_{\\mathrm{sim}}}(f_{h}(\\widetilde{s}_{h}^{t},\\widetilde{a}_{h}^{t})-\\widetilde{f}_{h}(\\widetilde{s}_{h}^{t},\\widetilde{a}_{h}^{t}))^{2}\\leq\\gamma\\right\\}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Then with probability $1-2\\delta$ we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\widehat{\\mathcal{F}}_{h}\\subseteq\\left\\{f_{h}\\in\\mathcal{F}_{h}\\ :\\ \\mathbb{E}^{\\mathrm{sim},\\pi_{\\mathrm{exp}}^{\\mathrm{sim}}}[(f_{h}(s_{h},a_{h})-7^{\\mathrm{sim}}\\widehat{f}_{h+1}(s_{h},a_{h}))^{2}]\\leq\\gamma+18V_{\\mathrm{max}}^{2}\\sqrt{\\frac{\\log\\frac{4|\\mathcal{F}_{h}|}{\\delta}}{T_{\\mathrm{sim}}}}\\right\\}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Proof. By Azuma-Hoeffding, we have that with probability at least $1-\\delta$ , for each $f_{h},f_{h}^{\\prime}\\in\\mathcal{F}_{h}$ , ", "page_idx": 30}, {"type": "equation", "text": "$$\nz\\mathrm{sim},\\pi_{\\mathrm{exp}}^{\\mathrm{sim}}\\big[\\big(f_{h}(s_{h},a_{h})-f_{h}^{\\prime}(s_{h},a_{h})\\big)^{2}\\big]-V_{\\mathrm{max}}^{2}\\sqrt{\\frac{2\\log|\\mathcal{F}_{h}|/\\delta}{T_{\\mathrm{sim}}}}\\le\\frac{1}{T_{\\mathrm{sim}}}\\sum_{t=1}^{T_{\\mathrm{sim}}}\\bigl(f_{h}\\bigl(\\tilde{s}_{h}^{t},\\tilde{a}_{h}^{t}\\bigr)-f_{h}^{\\prime}\\bigl(\\tilde{s}_{h}^{t},\\tilde{a}_{h}^{t}\\bigr)\\bigr)^{2}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "which implies in particular that, for any $f_{h}\\in\\mathcal{F}_{h}$ , ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathrm{g}\\mathrm{sim},\\pi_{\\mathrm{exp}}^{\\mathrm{sim}}\\big[\\big(f_{h}(s_{h},a_{h})-\\widetilde{f}_{h}(s_{h},a_{h})\\big)^{2}\\big]-V_{\\mathrm{max}}^{2}\\sqrt{\\frac{2\\log|\\mathcal{F}_{h}|/\\delta}{T_{\\mathrm{sim}}}}\\le\\frac{1}{T_{\\mathrm{sim}}}\\sum_{t=1}^{T_{\\mathrm{sim}}}\\!\\!\\!\\!(f_{h}(\\widetilde{s}_{h}^{t},\\widetilde{a}_{h}^{t})-\\widetilde{f}_{h}(\\widetilde{s}_{h}^{t},\\widetilde{a}_{h}^{t}))^{2}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "We can write ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\xi^{\\mathrm{sim},\\pi_{\\mathrm{exp}}^{\\mathrm{sim}}}\\big[(f_{h}(s_{h},a_{h})-\\widetilde{f}_{h}(s_{h},a_{h}))^{2}\\big]}\\\\ &{\\qquad\\qquad=\\mathbb{E}^{\\mathrm{sim},\\pi_{\\mathrm{exp}}^{\\mathrm{sim}}}\\big[(f_{h}(s_{h},a_{h})-\\mathcal{T}^{\\mathrm{sim}}\\widehat{f}_{h+1}(s_{h},a_{h}))^{2}\\big]+\\mathbb{E}^{\\mathrm{sim},\\pi_{\\mathrm{exp}}^{\\mathrm{sim}}}\\big[(\\widetilde{f}_{h}(s_{h},a_{h})-\\mathcal{T}^{\\mathrm{sim}}\\widehat{f}_{h+1}(s_{h},a_{h}))^{2}\\big]}\\\\ &{\\qquad\\qquad-\\ 2\\mathbb{E}^{\\mathrm{sim},\\pi_{\\mathrm{exp}}^{\\mathrm{sim}}}\\big[(\\widetilde{f}_{h}(s_{h},a_{h})-\\mathcal{T}^{\\mathrm{sim}}\\widehat{f}_{h+1}(s_{h},a_{h}))(f_{h}(s_{h},a_{h})-\\mathcal{T}^{\\mathrm{sim}}\\widehat{f}_{h+1}(s_{h},a_{h}))\\big]}\\\\ &{\\qquad\\qquad\\geq\\mathbb{E}^{\\mathrm{sim},\\pi_{\\mathrm{exp}}^{\\mathrm{sim}}}\\big[(f_{h}(s_{h},a_{h})-\\mathcal{T}^{\\mathrm{sim}}\\widehat{f}_{h+1}(s_{h},a_{h}))^{2}\\big]}\\\\ &{\\qquad\\qquad\\qquad-\\ 2\\mathbb{E}^{\\mathrm{sim},\\pi_{\\mathrm{exp}}^{\\mathrm{sim}}}\\big[(\\widetilde{f}_{h}(s_{h},a_{h})-\\mathcal{T}^{\\mathrm{sim}}\\widehat{f}_{h+1}(s_{h},a_{h}))(f_{h}(s_{h},a_{h})-\\mathcal{T}^{\\mathrm{sim}}\\widehat{f}_{h+1}(s_{h},a_{h}))\\big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "By Lemma 3 of [57], with probability at least $1-\\delta$ , ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathbb{E}^{\\mathrm{{sim}},\\pi_{\\mathrm{exp}}^{\\mathrm{sim}}}[(\\widetilde{f}_{h}(s_{h},a_{h})-7^{\\mathrm{{sim}}}\\widehat{f}_{h+1}(s_{h},a_{h}))^{2}]\\leq\\frac{256V_{\\mathrm{max}}^{2}}{T_{\\mathrm{sim}}}\\cdot\\log\\frac{2|\\mathcal{F}_{h}|}{\\delta}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "We can therefore bound the final term as ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}^{\\mathrm{sim},\\pi_{\\mathrm{exp}}^{\\mathrm{sim}}}\\bigl[\\bigl(\\widetilde{f}_{h}(s_{h},a_{h})-\\mathcal{T}^{\\mathrm{sim}}\\widehat{f}_{h+1}(s_{h},a_{h})\\bigr)(f_{h}(s_{h},a_{h})-\\mathcal{T}^{\\mathrm{sim}}\\widehat{f}_{h+1}(s_{h},a_{h}))\\bigr]}\\\\ &{\\qquad\\leq V_{\\mathrm{max}}\\cdot\\mathbb{E}^{\\mathrm{sim},\\pi_{\\mathrm{exp}}^{\\mathrm{sim}}}[\\bigl|\\widetilde{f}_{h}(s_{h},a_{h})-\\mathcal{T}^{\\mathrm{sim}}\\widehat{f}_{h+1}(s_{h},a_{h})\\bigr|]}\\\\ &{\\qquad\\leq V_{\\mathrm{max}}\\cdot\\sqrt{\\mathbb{E}^{\\mathrm{sim},\\pi_{\\mathrm{exp}}^{\\mathrm{sim}}}\\bigl[(\\widetilde{f}_{h}(s_{h},a_{h})-\\mathcal{T}^{\\mathrm{sim}}\\widehat{f}_{h+1}(s_{h},a_{h}))^{2}\\bigr]}}\\\\ &{\\qquad\\leq V_{\\mathrm{max}}\\cdot\\sqrt{\\frac{256V_{\\mathrm{max}}^{2}}{T_{\\mathrm{sim}}}\\cdot\\log\\frac{2|\\mathcal{F}_{h}|}{\\delta}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Altogether then we have shown that, for any $f_{h}\\in\\mathcal{F}_{h}$ , with probability at least $1-2\\delta$ : ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\frac{1}{I_{\\mathrm{sim}}}\\sum_{t=1}^{T_{\\mathrm{sim}}}(f_{h}(\\widetilde{s}_{h}^{t},\\widetilde{a}_{h}^{t})-\\widetilde{f}_{h}(\\widetilde{s}_{h}^{t},\\widetilde{a}_{h}^{t}))^{2}\\geq\\mathbb{E}^{\\mathrm{sim},\\pi_{\\mathrm{exp}}^{\\mathrm{sim}}}[(f_{h}(s_{h},a_{h})-T^{\\mathrm{sim}}\\widehat{f}_{h+1}(s_{h},a_{h}))^{2}]-18V_{\\mathrm{max}}^{2}\\sqrt{\\log\\left(\\frac{I_{\\mathrm{sim}}}{\\delta_{1}}\\widehat{f}_{h+1}(s_{h},a_{h})\\right)^{2}}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Thus, if ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\frac{1}{T_{\\sin}}\\sum_{t=1}^{T_{\\sin}}(f_{h}(\\widetilde{s}_{h}^{t},\\widetilde{a}_{h}^{t})-\\widetilde{f}_{h}(\\widetilde{s}_{h}^{t},\\widetilde{a}_{h}^{t}))^{2}\\le\\gamma,\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "then ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathbb{E}^{\\mathrm{{sim}},\\pi_{\\mathrm{exp}}^{\\mathrm{sim}}}[(f_{h}(s_{h},a_{h})-\\mathcal{T}^{\\mathrm{sim}}\\widehat{f}_{h+1}(s_{h},a_{h}))^{2}]\\leq\\gamma+18V_{\\mathrm{max}}^{2}\\sqrt{\\frac{\\log2|\\mathcal{F}_{h}|/\\delta}{T_{\\mathrm{{sim}}}}}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "The result follows from a union bound. ", "page_idx": 31}, {"type": "text", "text": "C.2 Data Collection with CoverTraj ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Lemma C.3. Consider running the COVERTRAJ algorithm of [65] for each $h\\in[H]$ with parameters $m\\leftarrow\\lceil\\log_{2}1/\\beta\\rceil$ and $\\gamma_{i}\\leftarrow2^{i}\\cdot\\beta$ for some $\\beta\\in[0,1]$ , and with REGMIN set to the policy optimization oracle of Oracle 4.2. Then this procedure collects ", "page_idx": 31}, {"type": "equation", "text": "$$\nT_{\\mathsf{s i m}}:=H\\cdot\\sum_{i=1}^{m}\\left\\lceil\\frac{24d}{2^{i}\\cdot\\beta^{2}}\\log\\frac{48d}{2^{i}\\cdot\\beta^{2}}\\right\\rceil\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "episodes, calls the policy optimization oracle at most $T_{\\mathsf{s i m}}$ times, and produces covariates ${\\mathbf{}}{\\Lambda}_{h,i}$ and sets $\\chi_{h,i}$ such that, for each $i\\in[m]$ , ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{\\pi}w_{h}^{\\mathrm{sim},\\pi}(\\mathcal{X}_{h,i})\\leq2^{-i+1}\\quad a n d\\quad\\phi^{\\top}\\mathbf{A}_{h,i}^{-1}\\phi\\leq2^{2i}\\cdot\\beta^{2},\\forall\\phi\\in\\mathcal{X}_{h,i},\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "and sup\u03c0 wshim,\u03c0(Bd\\ \u222aim=1 Xh,i) \u2264\u03b2. Furthermore, we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\frac{12d H}{\\beta^{2}}\\leq T_{\\mathsf{s i m}}\\leq\\frac{48d H}{\\beta^{2}}\\log\\frac{48d}{\\beta^{2}}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Proof. Instantiating REGMIN with the oracle of Oracle 4.2, we have that Definition 5.1 of [65] is met with $\\mathcal{C}_{1}=\\mathcal{C}_{2}=0$ . Therefore, we have that at each stage $i$ we collect exactly (using the precise form for $K_{i}$ given in the appendix of [65]) ", "page_idx": 31}, {"type": "equation", "text": "$$\nK_{i}=\\lceil2^{i}\\cdot\\frac{24d}{\\gamma_{i}^{2}}\\log\\frac{48\\cdot2^{i}d}{\\gamma_{i}^{2}}\\rceil\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "episodes. The result then follows by Theorem 3 of [65]. ", "page_idx": 31}, {"type": "text", "text": "Lemma C.4. Consider running the procedure of Lemma C.3 to collect data. Then with probability at least $1-2\\delta$ , we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V_{0}^{\\sin,\\star}-V_{0}^{\\sin,\\pi^{\\hat{J}}}\\leq2H\\left(2\\log\\frac{1}{\\beta}+3\\right)\\cdot\\left[\\beta\\sqrt{512V_{\\operatorname*{max}}^{2}A\\log(4H|\\mathcal{F}|/\\delta)}+\\sqrt{96A d H\\log\\frac{48d}{\\beta^{2}}\\cdot\\gamma}\\right.}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+\\left.\\sqrt{2A V_{\\operatorname*{max}}^{2}\\sqrt{96d H\\log\\frac{48d}{\\beta^{2}}\\log\\frac{1}{\\delta}}\\cdot\\beta}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Proof. By Lemma A.4: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r}{V_{0}^{\\mathrm{sim},\\star}-V_{0}^{\\mathrm{sim},\\pi^{\\widehat{f}}}\\leq\\underset{\\pi\\in\\{\\pi^{\\widehat{f}},\\pi^{\\mathrm{sim},\\star}\\}}{\\operatorname*{max}}\\sum_{h=0}^{H-1}2\\left|\\mathbb{E}^{\\mathrm{sim},\\pi}[\\widehat{f}_{h}(s_{h},a_{h})-T^{\\mathrm{sim}}\\widehat{f}_{h+1}(s_{h},a_{h})]\\right|}\\\\ {\\leq\\underset{\\pi\\in\\{\\pi^{\\widehat{f}},\\pi^{\\mathrm{sim},\\star}\\}}{\\operatorname*{max}}\\sum_{h=0}^{H-1}2\\mathbb{E}^{\\mathrm{sim},\\pi}[|\\widehat{f}_{h}(s_{h},a_{h})-T^{\\mathrm{sim}}\\widehat{f}_{h+1}(s_{h},a_{h})|].}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Denote $g(z_{h}):=|\\widehat{f}_{h}(s_{h},a_{h})-T^{\\sin}\\widehat{f}_{h+1}(s_{h},a_{h})|$ and $\\begin{array}{r}{\\mathbf{A}_{h-1}=\\sum_{i=1}^{m}\\mathbf{A}_{h,i}+I}\\end{array}$ , for ${\\bf\\cal A}_{h,i}$ collected as in Lemma C.3, and note that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\mathbb{E}^{\\mathrm{sim},\\pi}[g(z_{h})]=\\mathbb{E}^{\\mathrm{sim},\\pi}[\\int g(z)\\mathrm{d}P_{h}^{\\pi}(z:|\\,z_{h-1})]}}\\\\ &{=\\mathbb{E}^{\\mathrm{sim},\\pi}[\\int\\int g(z)\\pi(a\\mid s)\\mathrm{d}a\\mathrm{d}\\mu_{h-1}^{s}(s)^{\\top}\\phi^{s}(z_{h-1})]}\\\\ &{=\\mathbb{E}^{\\mathrm{sim},\\pi}[\\int\\int g(z)\\pi(a\\mid s)\\mathrm{d}a\\mathrm{d}\\mu_{h-1}^{s}(s)^{\\top}\\Lambda_{h-1}^{1/2}\\phi^{s}(z_{h-1})]}\\\\ &{\\leq\\mathbb{E}^{\\mathrm{sim},\\pi}[\\|\\int\\int g(z)\\pi(a\\mid s)\\mathrm{d}a\\mathrm{d}\\mu_{h-1}^{s}(s)\\|_{\\Lambda_{h-1}}\\cdot\\|\\phi^{s}(z_{h-1})\\|_{\\Lambda_{h-1}^{-1}}]}\\\\ &{=\\|\\int\\int g(z)\\pi(a\\mid s)\\mathrm{d}a\\mathrm{d}\\mu_{h-1}^{s}(s)\\|_{\\Lambda_{h-1}}\\cdot\\mathbb{E}^{\\mathrm{sim},\\pi}[\\|\\phi^{s}(z_{h-1})\\|_{\\Lambda_{h-1}^{-1}}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "We bound each of these terms separately. First, we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\mathbb{E}^{\\mathrm{sim},\\pi}[\\|\\phi^{s}(z_{h-1})\\|_{\\mathbf{A}_{h-1}^{-1}}]\\leq\\sum_{i=1}^{m}\\operatorname*{max}_{\\phi\\in\\mathcal{X}_{h-1,i}}\\|\\phi\\|_{\\mathbf{A}_{h-1}^{-1}}\\cdot\\operatorname*{sup}_{\\pi}\\mathbb{E}^{\\mathrm{sim},\\pi}[\\mathbb{I}\\{\\phi^{s}(z_{h-1})\\in\\mathcal{X}_{h-1,i}\\}]}}\\\\ &{\\leq\\operatorname*{max}_{\\phi\\in\\mathcal{B}^{d}\\backslash\\cup_{i=1}^{m}\\mathcal{X}_{h-1,i}}\\|\\phi\\|_{\\mathbf{A}_{h-1}^{-1}}\\cdot\\operatorname*{sup}_{\\pi}\\mathbb{E}^{\\mathrm{sim},\\pi}[\\mathbb{I}\\{\\phi^{s}(z_{h-1})\\in\\mathcal{X}_{h-1,i}\\}]}\\\\ &{\\overset{(a)}{\\leq}\\displaystyle\\sum_{i=1}^{\\iota}\\gamma_{i}\\cdot2^{-i+1}+\\beta}\\\\ &{\\leq(2m+1)\\beta}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where $(a)$ follows from Lemma C.3 and since $\\|\\phi\\|_{\\mathbf{A}_{h-1}^{-1}}\\leq1$ always. ", "page_idx": 32}, {"type": "text", "text": "We turn now to bounding the first term. Note that ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\int\\hat{\\rho}(z)e^{\\theta}(z)e^{\\theta}(z)\\right\\|d z\\right\\|_{z=0}}\\\\ &{=\\sqrt{\\frac{\\displaystyle{\\sum_{i=1}^{n}\\int_{z}^{\\theta}\\int_{0}^{z}(z)e^{\\theta}(z)e^{\\theta}(z-\\theta_{i})\\theta}\\,d z}{\\displaystyle{\\sum_{i=1}^{n}\\int_{z}^{\\theta}\\int_{0}^{z}\\int_{z}^{z}\\left(z\\right)e^{\\theta}(z-\\theta_{i})\\theta\\left(z-\\theta_{i}\\right)^{2}\\left(z-\\theta_{i}\\right)^{2}}}}}\\\\ &{=\\sqrt{\\frac{\\displaystyle{\\sum_{i=1}^{n}\\sum_{i=1}^{n}\\sum_{i=1}^{n}\\sum_{i=1}^{n}\\theta}}{\\displaystyle{\\sum_{i=1}^{n}\\sum_{i=1}^{n}\\sum_{i=1}^{n}\\theta}}}}\\\\ &{\\lesssim\\sqrt{\\frac{\\displaystyle{\\sum_{i=1}^{n}\\sum_{i=1}^{n}e^{\\theta}(z)e^{\\theta}(z)\\,|z_{i-1}|}}{\\displaystyle{\\sum_{i=1}^{n}\\sum_{i=1}^{n}\\sum_{i=1}^{n}\\theta}}}}\\\\ &{\\lesssim\\sqrt{\\frac{\\displaystyle{\\sum_{i=1}^{n}\\sum_{i=1}^{n}e^{\\theta}(z)\\left(\\sum_{i=1}^{n}\\theta\\left(z\\right)e^{\\theta}\\left(z\\right)\\right)\\left(z-\\theta_{i}\\right)^{2}}}{\\displaystyle{\\sum_{i=1}^{n}\\sum_{i=1}^{n}\\sum_{i=1}^{n}e^{\\theta}\\left(z\\right)\\left(z\\right)\\theta\\left(z_{i-1}\\right)-\\theta^{2}\\left(z\\right)\\left(z_{i-1}\\right)^{2}\\left(z\\right)}}}}\\\\ &{\\lesssim\\sqrt{\\frac{\\displaystyle{\\sum_{i=1}^{n}\\sum_{i=1}^{n}e^{\\theta}(z)e^{\\theta}(z)\\left(z\\right)\\theta\\left(z_{i-1}\\right)-\\theta^{2}e^{-\\theta}\\left(z\\right)\\left(z_{i-1}\\right)}}{\\displaystyle{\\sum_{i=1}^{n}\\sum_{i=1}^{n}\\sum_{i=1}^{n}e^{\\theta}\\left(z\\right)\\theta\\left(z_{i-1}\\right)-\\theta^{2}e^{-\\theta}\\left(z\\right)\\left(z_{i-1}\\right)}} \n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where $(a)$ uses the fact that $\\pi_{\\mathrm{exp}}^{h-1,t}$ plays actions randomly at step $h$ and $(b)$ holds with probability at least $1-\\delta$ by Lemma C.6. By Azuma-Hoeffding, we have with probability $1-\\delta$ : ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\sum_{=1}^{\\overset{\\land}{\\sum}}\\mathbb{E}^{\\pi_{\\mathrm{exp}}^{h-1,t}}[(\\widetilde{f}_{h}(s_{h},a_{h})-\\widehat{f}_{h}(s_{h},a_{h}))^{2}\\mid z_{h-1}^{t}]\\leq\\sum_{t=1}^{T_{\\mathrm{sm}}}(\\widetilde{f}_{h}(s_{h}^{t},a_{h}^{t})-\\widehat{f}_{h}(s_{h}^{t},a_{h}^{t}))^{2}+\\sqrt{2V_{\\operatorname*{max}}^{4}T_{\\mathrm{sm}}\\log}}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where the last inequality follows from the definition of $\\widehat{f}_{h}$ ", "page_idx": 33}, {"type": "text", "text": "Altogether then we have shown that, with probability at least $1-2\\delta$ : ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\iota^{,\\star}-V_{0}^{\\mathrm{sim},\\pi^{\\hat{J}}}\\leq2H(2m+1)\\beta\\cdot\\sqrt{512V_{\\mathrm{max}}^{2}A\\log(4H|\\mathcal{F}|/\\delta)+2A T_{\\mathrm{sim}}\\gamma+2A V_{\\mathrm{max}}^{2}\\sqrt{2T_{\\mathrm{sim}}\\log1/\\delta}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Using that Tsim \u2264 $\\begin{array}{r}{T_{\\mathsf{s i m}}\\leq\\frac{48d H}{\\beta^{2}}\\log\\frac{48d}{\\beta^{2}}}\\end{array}$ as given in Lemma C.3, we can bound this as ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\leq2H(2m+1)\\bigg[\\beta\\sqrt{512V_{\\operatorname*{max}}^{2}A\\log(4H|\\mathcal{F}|/\\delta)}+\\sqrt{96A d H\\log\\frac{48d}{\\beta^{2}}\\cdot\\gamma}}\\\\ &{\\qquad+\\,\\sqrt{2A V_{\\operatorname*{max}}^{2}\\sqrt{96d H\\log\\frac{48d}{\\beta^{2}}\\log\\frac{1}{\\delta}}\\cdot\\beta}\\bigg].}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "The result follows. ", "page_idx": 33}, {"type": "text", "text": "Lemma C.5. Assume that ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}^{\\sin,\\pi_{\\mathrm{exp}}^{\\mathrm{sim}}}[(f_{h}(s_{h},a_{h})-\\mathcal{T}^{\\sin}f_{h+1}(s_{h},a_{h}))^{2}]\\leq\\gamma.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Then this implies that, with probability at least $1-\\delta$ , ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\operatorname*{sup}_{\\pi}\\big(\\mathbb{E}^{\\mathrm{sim},\\pi}[f_{h}(s_{h},a_{h})-\\mathcal{T}^{\\mathrm{sim}}f_{h+1}(s_{h},a_{h})]\\big)^{2}}\\quad}&{}\\\\ &{\\leq\\left(4\\log\\frac{1}{\\beta}+6\\right)A\\cdot\\left[48d\\log\\frac{48d}{\\beta^{2}}\\cdot\\gamma+V_{\\operatorname*{max}}\\sqrt{96d\\log\\frac{48d}{\\beta^{2}}\\log\\frac{1}{\\delta}}\\cdot\\beta\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Therefore, ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\{f\\in\\mathcal{F}\\ :\\ \\mathbb{E}^{\\mathrm{sim},\\pi_{\\mathrm{exp}}^{\\mathrm{sim}}}[(f_{h}(s_{h},a_{h})-\\mathcal{T}^{\\mathrm{sim}}f_{h+1}(s_{h},a_{h}))^{2}]\\leq\\gamma\\}}\\\\ &{\\subseteq\\Bigg\\{f\\in\\mathcal{F}\\ :\\ \\operatorname*{sup}_{\\pi}\\,(\\mathbb{E}^{\\mathrm{sim},\\pi}[f_{h}(s_{h},a_{h})-\\mathcal{T}^{\\mathrm{sim}}f_{h+1}(s_{h},a_{h})])^{2}}\\\\ &{\\quad\\quad\\quad\\leq\\bigg(4\\log\\frac{1}{\\beta}+6\\bigg)\\,A\\cdot\\Bigg[48d H\\log\\frac{48d}{\\beta^{2}}\\cdot\\gamma+V_{\\operatorname*{max}}^{2}\\sqrt{96d H\\log\\frac{48d}{\\beta^{2}}\\log\\frac{1}{\\delta}}\\cdot\\beta\\Bigg]\\,\\Bigg\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Proof. We follow a similar argument as the proof of Lemma C.4. Denoting $g(z_{h}):=f_{h}(s_{h},a_{h})\\:-\\:$ $\\tau^{\\sin}f_{h+1}(s_{h},a_{h})$ , by the same calculation as (C.1) we have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathbb{E}^{\\mathrm{sim},\\pi}[g(z_{h})]\\leq\\|\\int\\int g(z)\\pi(a\\mid s)\\mathrm{d}a\\mathrm{d}\\mu_{h-1}^{s}(s)\\|_{\\Lambda_{h-1}}\\cdot\\mathbb{E}^{\\mathrm{sim},\\pi}[\\|\\phi^{s}(z_{h-1})\\|_{\\Lambda_{h-1}^{-1}}]\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "and as in the proof of Lemma C.4, we can bound ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathbb{E}^{\\mathrm{{sim},\\pi}}[\\|\\phi^{\\mathrm{s}}(z_{h-1})\\|_{\\Lambda_{h-1}^{-1}}]\\leq(2m+1)\\beta\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "and ", "text_level": 1, "page_idx": 34}, {"type": "equation", "text": "$$\n\\int\\int g(z)\\pi(a\\mid s)\\mathrm{d}a\\mathrm{d}\\mu_{h-1}^{s}(s)\\|_{\\boldsymbol{\\Lambda}_{h-1}}\\leq\\sqrt{A\\cdot\\sum_{t=1}^{T_{\\mathrm{sm}}}\\mathbb{E}^{\\pi_{\\mathrm{exp}}^{h-1,t}}\\big[(f_{h}(s_{h},a_{h})-\\mathcal{T}^{\\mathrm{sm}}f_{h+1}(s_{h},a_{h}))^{2}\\mid z_{h-1}^{t}}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "By Azuma-Hoeffding, with probability at least $1-\\delta$ we can then bound ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\sum_{=1}^{\\infty}\\mathbb{E}^{\\pi_{\\mathrm{exp}}^{h-1,t}}\\bigl[(f_{h}(s_{h},a_{h})-\\mathcal{T}^{\\mathrm{sim}}f_{h+1}(s_{h},a_{h}))^{2}\\mid z_{h-1}^{t}\\bigr]\\leq T_{\\mathrm{sim}}\\cdot\\mathbb{E}^{\\pi_{\\mathrm{exp}}^{\\mathrm{sim}}}\\bigl[(f_{h}(s_{h},a_{h})-\\mathcal{T}^{\\mathrm{sim}}f_{h+1}(s_{h},a_{h}))^{2}\\bigr]}\\qquad}&{}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad+\\ \\sqrt{2V_{\\mathrm{max}}^{4}T_{\\mathrm{sim}}\\log1/\\delta}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\leq T_{\\mathrm{sim}}\\gamma+\\sqrt{2V_{\\mathrm{max}}^{4}T_{\\mathrm{sim}}\\log1/\\delta}}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where the last inequality follows by assumption, and where $\\pi_{\\mathrm{exp}}^{\\mathsf{s i m}}=\\operatorname{unif}\\!\\left(\\{\\pi_{\\mathrm{exp}}^{h-1,t}\\}_{t=1}^{T_{\\mathsf{s i m}}}\\right)$ . Altogether then, for all $\\pi$ , we have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathbb{E}^{\\mathrm{sim},\\pi}[f_{h}(s_{h},a_{h})-\\mathcal{T}^{\\mathrm{sim}}f_{h+1}(s_{h},a_{h})]\\leq(2m+1)\\beta\\cdot\\sqrt{A T_{\\mathrm{sim}}\\gamma+A V_{\\mathrm{max}}^{2}\\sqrt{2T_{\\mathrm{sim}}\\log1/\\delta}}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Using that Tsim \u2264 $\\begin{array}{r}{T_{\\mathsf{s i m}}\\leq\\frac{48d H}{\\beta^{2}}\\log\\frac{48d}{\\beta^{2}}}\\end{array}$ as given in Lemma C.3, we can bound this as ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\leq(2m+1)\\sqrt{48A d H\\log\\frac{48d}{\\beta^{2}}\\cdot\\gamma}+(2m+1)\\sqrt{A V_{\\operatorname*{max}}^{2}\\sqrt{96d H\\log\\frac{48d}{\\beta^{2}}\\log\\frac{1}{\\delta}}\\cdot\\beta}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "The result follows from some algebra. ", "page_idx": 34}, {"type": "text", "text": "Lemma C.6. With probability at least $1-\\delta_{i}$ , for each $h\\in[H]$ simultaneously, we have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T_{\\mathrm{sim}}}\\mathbb{E}^{\\mathrm{sim},\\pi_{\\mathrm{exp}}^{h-1,t}}[(\\widetilde f_{h}(s_{h},a_{h})-T^{\\mathrm{sim}}\\widehat f_{h+1}(s_{h},a_{h}))^{2}\\mid s_{h-1}^{t},a_{h-1}^{t}]\\le256V_{\\mathrm{max}}^{2}\\log(4H|\\mathcal F|/\\delta).\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Proof. This follows from Lemma 3 of [57]. ", "page_idx": 34}, {"type": "text", "text": "Algorithm 5 Learn Exploration Policies in $\\mathcal{M}^{\\mathsf{s i m}}$ (LEARNEXPPOLICIES) ", "page_idx": 35}, {"type": "text", "text": "1: input: environment $\\mathcal{M}$ , confidence $\\delta$ , regularization $\\zeta$ , step $h$   \n2: $\\mathbb{A}_{\\mathcal{R}}\\leftarrow$ policy optimization oracle of Oracle 4.2   \n3: for $\\begin{array}{r}{j=1,2,3,\\ldots,\\mathcal{O}(\\log_{2}(\\frac{d}{\\zeta}\\cdot\\log\\frac{1}{\\delta}+\\zeta^{-9}\\cdot\\log^{3/2}\\frac{1}{\\delta}))}\\end{array}$ do   \n4: $\\begin{array}{r}{N_{j}\\gets\\lceil2^{j/3}\\rceil-1,K_{j}\\gets\\lceil2^{2j/3}\\rceil,T_{j}^{'}\\gets(N_{j}+1)K_{j},\\delta_{j}\\gets\\frac{\\delta}{4j^{2}}}\\end{array}$   \n// DynamicOED algorithm from [64]   \n5: $\\Sigma_{j},\\Pi_{j}\\gets\\mathrm{DYNAMICOED}(\\Phi,N_{j},K_{j},\\delta_{j},\\mathbb{A}_{\\mathcal{R}})$ for $\\Phi(\\mathbf{A}_{h})\\leftarrow\\operatorname{tr}((\\mathbf{A}_{h}+\\zeta\\cdot I)^{-1})$   \n6: if $\\begin{array}{r}{\\lambda_{\\operatorname*{min}}(\\Sigma_{j})\\geq12544d\\log{\\frac{4+64T_{j}}{\\delta}}}\\end{array}$ and $T_{j}\\ge c\\cdot\\zeta^{-9}\\cdot\\log^{3/2}\\frac{j T_{j}}{\\delta}$ then   \n7: break   \n8: return $\\Pi_{j}$ ", "page_idx": 35}, {"type": "text", "text": "C.3 Learning Full-Rank Policies ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "We consider running the MINEIG algorithm (Algorithm 6) of [64] in sim. For a fixed $h$ , we instantiate the setting of Appendix C of [64] with $\\psi(\\pmb{\\tau})\\bar{=\\phi}(s_{h},a_{h})\\phi(s_{h},a_{h})^{\\top}$ , $D=1$ , and $\\mathbb{A}_{\\mathcal{R}}$ the policy optimization oracle of Oracle 4.2 (and so $C_{\\mathcal{R}}=0$ ), and set $N=1$ for MINEIG. We note that this algorithm is computationally efficient, given a policy optimization oracle. ", "page_idx": 35}, {"type": "text", "text": "Lemma C.7. For $\\mathcal{M}\\gets\\mathcal{M}^{\\sf s i m}$ , Algorithm 5 will call Oracle 4.2 at most $\\begin{array}{r}{\\widetilde{\\mathcal{O}}(\\frac{d}{\\zeta}\\cdot\\log\\frac{1}{\\delta}+\\zeta^{-9}\\cdot\\log^{3/2}\\frac{1}{\\delta})}\\end{array}$ times, and with probability at least $1-\\delta$ , under Assumption 3 and $i f\\zeta\\le\\frac{\\lambda_{\\mathrm{min}}^{\\star}}{4d}$ , will return policies \u03a0 such that ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\lambda_{\\operatorname*{min}}\\left(\\frac{1}{|\\Pi|}\\sum_{\\pi\\in\\Pi}\\mathbf{A}_{\\pi,h}^{s}\\right)\\geq\\frac{\\lambda_{\\operatorname*{min}}^{\\star}}{8d}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "and each $\\pi\\in\\Pi$ plays actions randomly for $h^{\\prime}>h$ . ", "page_idx": 35}, {"type": "text", "text": "Proof. We first argue that, if $\\zeta\\leq\\frac{\\lambda_{\\operatorname*{min}}^{\\star}}{4d}$ , then with probability at least $1-\\delta$ , (C.2) holds. Let $\\mathcal{E}$ denote the success event of each call to DYNAMICOED, and note that by our choice of $\\delta_{j}$ , we have $\\mathbb{P}[\\mathcal{E}]\\ge1-\\delta/2$ . Let $j^{\\star}$ denote the minimal value of $j$ such that ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\frac{\\lambda_{\\operatorname*{min}}^{\\star}}{4d}T_{j}\\geq12544d\\log\\frac{4+64T_{j}}{\\delta}\\quad\\mathrm{and}\\quad T_{j}\\geq c\\cdot\\zeta^{-9}\\cdot\\log^{3/2}\\frac{j T_{j}}{\\delta}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "By Lemma C.4 of [64] and if $\\zeta\\,\\leq\\,\\frac{\\lambda_{\\operatorname*{min}}^{\\star}}{4d}$ , we then have that, on $\\mathcal{E}$ , $\\begin{array}{r}{\\lambda_{\\operatorname*{min}}(\\Sigma_{j^{\\star}})\\,\\ge\\,\\frac{\\lambda_{\\operatorname*{min}}^{\\star}}{4d}T_{j^{\\star}}}\\end{array}$ , which implies that the termination criteria of Algorithm 5 will be met. By Lemma C.5 of [64], it follows that with probability at least $1-\\delta/2$ , we have $\\begin{array}{r}{\\lambda_{\\mathrm{min}}\\big(\\frac{1}{|\\Pi_{j^{\\star}}|}\\sum_{\\pi\\in\\Pi_{j^{\\star}}}\\mathbf{\\boldsymbol{\\Lambda}}_{\\pi,h}^{s}\\big)\\geq\\frac{{\\boldsymbol{\\lambda}}_{\\mathrm{min}}^{\\star}}{8d}}\\end{array}$ (since $T_{j^{\\star}}=|\\Pi_{j^{\\star}}|)$ , the desired conclusion. ", "page_idx": 35}, {"type": "text", "text": "Assume that Algorithm 5 terminates for some $j<j^{\\star}$ . This implies that $\\begin{array}{r}{\\frac{\\lambda_{\\operatorname*{min}}^{\\star}}{4d}T_{j}<12544d\\log{\\frac{4+64T_{j}}{\\delta}}}\\end{array}$ However, in this case, we then have that ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\lambda_{\\mathrm{min}}(\\Sigma_{j})\\geq12544d\\log\\frac{4+64T_{j}}{\\delta}\\geq\\frac{\\lambda_{\\mathrm{min}}^{\\star}}{4d}T_{j}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "From Lemma C.5 of [64], it then follows that with probability at least $1\\,-\\,\\delta/2$ , we have $\\begin{array}{r}{\\lambda_{\\operatorname*{min}}\\big(\\frac{1}{|\\Pi_{j}|}\\sum_{\\pi\\in\\Pi_{j}}\\pmb{\\Lambda}_{\\pi,h}^{s}\\big)\\geq\\frac{\\lambda_{\\operatorname*{min}}^{\\star}}{8d}}\\end{array}$ ", "page_idx": 35}, {"type": "text", "text": "It follows that, assuming $T_{j}$ is large enough that (C.3) is met, and we are in the case when $\\zeta\\leq$ \u03bb4\u22c6mdin holds, then Algorithm 5 will terminate and return a set of policies satisfying (C.2), with probability at least $1-\\delta$ . Note that $T_{j}\\,=\\,\\mathcal{O}(2^{j})$ . Given that Algorithm 5 does not terminate until $\\begin{array}{r}{j\\,=\\,\\mathcal{O}(\\log_{2}(\\frac{d}{\\zeta}\\cdot\\log\\frac{1}{\\delta}+\\zeta^{-9}\\cdot\\log^{\\frac{3}{3}/2}\\frac{1}{\\delta})\\,\\geq\\,\\mathcal{O}(\\log_{2}(\\frac{d^{2}}{\\lambda_{\\operatorname*{min}}^{\\star}}\\cdot\\log\\frac{1}{\\delta}+\\zeta^{-9}\\cdot\\log^{3/2}\\frac{1}{\\delta}))}\\end{array}$ , we will hDaYveN AthMaIt $T_{j}$ EwDil lc ablel sl aOrrgaec leen o4.u2g hat  thmaot s(t $T_{j}$ )t iism ems eatt,  rifo $\\zeta\\ \\leq\\ \\frac{\\lambda_{\\operatorname*{min}}^{\\star}}{4d}$ $j$ \u03bb4\u22c6mdin . The proof then follows since the total sum of $T_{j}$ is bounded as $\\begin{array}{r}{\\widetilde{\\mathcal{O}}(\\frac{d}{\\zeta}\\cdot\\log\\frac{1}{\\delta}+\\zeta^{-9}\\cdot\\log^{3/2}\\frac{1}{\\delta})}\\end{array}$ by the maximum of $j$ , and since the actions chosen by $\\pi\\in\\Pi$ for $h^{\\prime}>\\bar{h}$ are irrelevant for the operation of DYNAMICOED, so they can be set to random. \u53e3 ", "page_idx": 35}, {"type": "text", "text": "D Lower Bound Proofs ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "D.1 Proof of Propositions 1, 3 and 4 ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Construction. Consider the following variation of the combination lock. We let the action space $A=\\{1,2\\}$ , and assume there are two states, $S=\\{s_{1},s_{2}\\}$ , and horizon $H$ . We start in state $s_{1}$ . The sim dynamics are given as: ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\forall h<H-1:\\quad P_{h}^{\\mathsf{s i m}}(s_{1}\\mid s_{1},a_{1})=1,\\quad P_{h}^{\\mathsf{s i m}}(s_{2}\\mid s_{1},a_{2})=1}\\\\ &{P_{H-1}^{\\mathsf{s i m}}(s_{1}\\mid s_{1},a_{1})=P_{H-1}^{\\mathsf{s i m}}(s_{2}\\mid s_{1},a_{1})=P_{H-1}^{\\mathsf{s i m}}(s_{1}\\mid s_{1},a_{2})=P_{H-1}^{\\mathsf{s i m}}(s_{2}\\mid s_{1},a_{2})=1/2}\\\\ &{\\forall h\\in[H]:\\quad P_{h}^{\\mathsf{s i m}}(s_{2}\\mid s_{2},a)=1,a\\in\\{a_{1},a_{2}\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "We define two real instances, $\\mathcal{M}_{1}:=\\mathcal{M}^{\\sf r e a l,1}$ and $\\mathcal{M}_{2}:=\\mathcal{M}^{\\sf r e a l,2}$ , where for both we have: ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\forall h<H-1:\\quad P_{h}^{r\\mathsf{e a l}}(s_{1}\\mid s_{1},a_{1})=1,\\quad P_{h}^{r\\mathsf{e a l}}(s_{2}\\mid,s_{1},a_{2})=1}\\\\ &{\\forall h\\in[H]:\\quad P_{h}^{r\\mathsf{e a l}}(s_{2}\\mid s_{2},a)=1,a\\in\\{a_{1},a_{2}\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "for $\\mathcal{M}_{1}$ : ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{P_{H-1}^{\\mathrm{real}}(s_{1}\\mid s_{1},a_{1})=1/2+\\epsilon_{\\mathrm{sim}},P_{H-1}^{\\mathrm{real}}(s_{2}\\mid s_{1},a_{1})=1/2-\\epsilon_{\\mathrm{sim}},}\\\\ &{P_{H-1}^{\\mathrm{real}}(s_{1}\\mid s_{1},a_{2})=1/2-\\epsilon_{\\mathrm{sim}},P_{H-1}^{\\mathrm{real}}(s_{2}\\mid s_{1},a_{2})=1/2+\\epsilon_{\\mathrm{sim}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "and for $\\mathcal{M}_{2}$ ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{P_{H-1}^{\\mathrm{real}}(s_{1}\\mid s_{1},a_{1})=1/2-\\epsilon_{\\mathrm{sim}},P_{H-1}^{\\mathrm{real}}(s_{2}\\mid s_{1},a_{1})=1/2+\\epsilon_{\\mathrm{sim}},}\\\\ &{P_{H-1}^{\\mathrm{real}}(s_{1}\\mid s_{1},a_{2})=1/2+\\epsilon_{\\mathrm{sim}},P_{H-1}^{\\mathrm{real}}(s_{2}\\mid s_{1},a_{2})=1/2-\\epsilon_{\\mathrm{sim}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Note then that $\\mathcal{M}_{1},\\mathcal{M}_{2}$ , and sim only differ at step $H-1$ in state $s_{1}$ . Furthermore, it is easy to see that both $\\mathcal{M}_{1}$ and $\\mathbf{\\mathcal{M}}_{2}$ satisfy Assumption 1 with misspecification $\\epsilon_{\\mathrm{sim}}$ . It is easy to see that Assumption 2 holds as well with $d=4$ since this is a tabular MDP, and furthermore Assumption 3 also holds with $\\lambda_{\\mathrm{min}}^{\\star}=1/4$ . We define the reward function as (note that this is deterministic, and the same for all instances): ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\forall h\\in[H]:\\quad r_{h}(s_{1},a_{2})=1/2+\\epsilon_{\\mathrm{sim}}(1/2-h/4H)}\\\\ &{r_{H}(s_{1},a)=1,a\\in\\{a_{1},a_{2}\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "and all other rewards are taken to be 0. ", "page_idx": 36}, {"type": "text", "text": "In sim, we see that the optimal policy always plays $a_{2}$ . In both $\\mathcal{M}_{1}$ and $\\mathcal{M}_{2}$ , the optimal policy plays $a_{1}$ for all $h<H-1$ , for $\\mathcal{M}_{1}$ plays $a_{1}$ at $H-1$ , and for $\\mathcal{M}_{2}$ plays $a_{2}$ at $H-1$ . Note that for both $\\mathcal{M}_{1}$ and $\\mathcal{M}_{2}$ , we have $V_{0}^{\\star}=1/2+\\epsilon_{\\mathrm{sim}}$ . ", "page_idx": 36}, {"type": "text", "text": "The most natural choice of $\\mathcal{F}$ would be the set of all tabular $Q$ -value functions, however, this set   \nis infinite, and would require a covering argument to incorporate. For simplicity, consider ${\\mathcal{F}}_{H}$ the $\\{0,1\\}$ $\\mathcal{F}_{h}$ .a ppNiontge  ttoh aat  fsinuicteh  sae ts ecto nstaatiisnifniegs   \n$\\{0,1/2\\,-\\,\\epsilon_{\\mathrm{sim}},1/2+\\epsilon_{\\mathrm{sim}}\\}\\stackrel{}{\\cup}\\{\\mathrm{i}/2+\\epsilon_{\\mathrm{sim}}(1/2\\,-\\,h^{\\prime}/4H)\\}_{h^{\\prime}=0}^{H}$   \nAssumption 4 and we can construct it such that $\\log|\\mathcal{F}|\\leq\\mathcal{O}(H)$ . ", "page_idx": 36}, {"type": "text", "text": "Lower Bound for Direct Policy Transfer (Proposition 3). We consider direct sim2real transfer with randomized exploration. In particular, as noted, the optimal policy in sim always plays $a_{2}$ , so we consider the $\\zeta$ -greedy policy that at every state plays $a_{2}$ with probability $1-\\zeta$ , and plays $\\operatorname{unif}(\\{a_{1},a_{2}\\})$ with probability $\\zeta$ . Denote this policy as $\\widetilde{\\pi}$ . We then wish to lower bound: ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{\\widehat{\\pi}}\\operatorname*{sup}_{i\\in\\{1,2\\}}\\mathbb{E}^{\\mathcal{M}_{i},\\widetilde{\\pi}}[V_{0}^{\\mathcal{M}_{i},\\star}-V_{0}^{\\mathcal{M}_{i},\\widehat{\\pi}}]\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "after running our procedure for $T$ episodes. Note that on $\\mathcal{M}_{1}$ , regardless of the actions $\\widehat{\\pi}$ chooses in other states, we have ", "page_idx": 36}, {"type": "equation", "text": "$$\nV_{0}^{\\mathcal{M}_{1},\\star}-V_{0}^{\\mathcal{M}_{1},\\widehat{\\pi}}\\geq\\frac{\\epsilon_{\\mathrm{sim}}}{2}(1-\\widehat{\\pi}_{H-1}(a_{1}\\mid s_{1})),\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "since the only way $\\widehat{\\pi}$ can achieve a reward of $1/2+\\epsilon_{\\mathrm{sim}}$ is by playing $a_{1}$ in $s_{1}$ at step $H-1$ , and all other sequences of   actions obtain a reward of at most $1/2+\\epsilon_{\\mathrm{sim}}/2$ . Similarly for $\\mathcal{M}_{2}$ we have ", "page_idx": 37}, {"type": "equation", "text": "$$\nV_{0}^{\\mathcal{M}_{2},\\star}-V_{0}^{\\mathcal{M}_{2},\\widehat{\\pi}}\\geq\\frac{\\epsilon_{\\mathrm{sim}}}{2}(1-\\widehat{\\pi}_{H-1}(a_{2}\\mid s_{1})).\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Using this, and replacing the max over $i\\in\\{1,2\\}$ with the average of them, we obtain ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\underset{,2)}{\\mathbb{P}}\\ \\mathbb{E}^{M_{1},\\tilde{\\pi}}[V_{0}^{M_{i},\\star}-V_{0}^{M_{i},\\widehat{\\pi}}]\\geq\\underset{\\tilde{\\pi}}{\\operatorname*{inf}}\\ \\frac{1}{2}\\mathbb{E}^{M_{1},\\tilde{\\pi}}[\\frac{\\epsilon_{\\sin1}}{2}(1-\\widehat{\\pi}_{H-1}(a_{1}\\ |\\ s_{1}))]+\\frac{1}{2}\\mathbb{E}^{M_{2},\\tilde{\\pi}}[\\frac{\\epsilon_{\\sin1}}{2}(1-\\widehat{\\pi}_{H-1}(a_{2}\\ |\\ s_{1}))]}\\\\ &{}&{=\\frac{\\epsilon_{\\sin1}}{2}\\left[1-\\frac{1}{2}\\cdot\\underset{\\tilde{\\pi}}{\\operatorname*{sup}}\\left(\\mathbb{E}^{M_{1},\\tilde{\\pi}}[\\widehat{\\pi}_{H-1}(a_{1}\\ |\\ s_{1})]+\\mathbb{E}^{M_{2},\\tilde{\\pi}}[\\widehat{\\pi}_{H-1}(a_{2}\\ |\\ s_{1})]\\right)\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Since $\\widehat{\\pi}_{H-1}(a_{1}\\mid s_{1})=1-\\widehat{\\pi}_{H-1}(a_{2}\\mid s_{1})$ , we have ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Xi^{M_{1},\\widetilde{\\pi}}\\big[\\widehat{\\pi}_{H-1}(a_{1}\\mid s_{1})\\big]+\\mathbb{E}^{M_{2},\\widetilde{\\pi}}[\\widehat{\\pi}_{H-1}(a_{2}\\mid s_{1})]=1+\\mathbb{E}^{M_{1},\\widetilde{\\pi}}[\\widehat{\\pi}_{H-1}(a_{1}\\mid s_{1})]-\\mathbb{E}^{M_{2},\\widetilde{\\pi}}[\\widehat{\\pi}_{H-1}(a_{1}\\mid s_{1})]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\leq1+\\mathrm{TV}(\\mathbb{P}^{M_{1},\\widetilde{\\pi}},\\mathbb{P}^{M_{2},\\widetilde{\\pi}})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\leq1+\\sqrt{\\frac{1}{2}\\mathrm{KL}(\\mathbb{P}^{M_{1},\\widetilde{\\pi}}\\parallel\\mathbb{P}^{M_{2},\\widetilde{\\pi}})}}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where TV denotes the total-variation distance, KL the KL-divergence, and the last inequality follows from Pinsker\u2019s inequality. We therefore have ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{\\widehat{\\pi}}\\operatorname*{sup}_{i\\in\\{1,2\\}}\\mathbb{E}^{M_{i},\\widetilde{\\pi}}[V_{0}^{M_{i},\\star}-V_{0}^{M_{i},\\widehat{\\pi}}]\\ge\\frac{\\epsilon_{\\dim}}{4}\\left(1-\\sqrt{\\frac{1}{2}\\mathrm{KL}(\\mathbb{P}^{M_{1},\\widetilde{\\pi}}\\parallel\\mathbb{P}^{M_{2},\\widetilde{\\pi}})}\\right).\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Now note that, since $\\mathcal{M}_{1}$ and $\\mathcal{M}_{2}$ only differ at state $s_{1}$ and step $H-1$ , we have ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{KL}(\\mathbb{P}^{M_{1},\\widetilde{\\pi}}\\parallel\\mathbb{P}^{M_{2},\\widetilde{\\pi}})=\\mathbb{E}^{M_{1},\\widetilde{\\pi}}[T_{H-1}(s_{1},a_{1})]\\mathrm{KL}(P_{H-1}^{M_{1}}(\\cdot\\mid s_{1},a_{1})\\parallel P_{H-1}^{M_{2}}(\\cdot\\mid s_{1},a_{1}))}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+\\,\\mathbb{E}^{M_{1},\\widetilde{\\pi}}[T_{H-1}(s_{1},a_{2})]\\mathrm{KL}(P_{H-1}^{M_{1}}(\\cdot\\mid s_{1},a_{2})\\parallel P_{H-1}^{M_{2}}(\\cdot\\mid s_{1},a_{2})),}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where $T_{H-1}(s_{1},a_{i})$ denotes the total number of visits to $(s_{1},a_{i})$ at step $H-1$ after $T$ episodes (see e.g. [59]). We have ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{KL}(P_{H-1}^{M_{1}}(\\cdot\\mid s_{1},a_{1})\\parallel P_{H-1}^{M_{2}}(\\cdot\\mid s_{1},a_{1}))=\\mathrm{KL}(P_{H-1}^{M_{1}}(\\cdot\\mid s_{1},a_{2})\\parallel P_{H-1}^{M_{2}}(\\cdot\\mid s_{1},a_{2}))}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=\\frac{1}{4}\\log\\frac{1/4}{3/4}+\\frac{3}{4}\\log\\frac{3/4}{1/4}\\leq\\frac{3}{5}}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where the last inequality holds as long as $\\epsilon_{\\mathrm{sim}}\\leq1/6$ . Note that the only way for a policy to reach $s_{1}$ at step $H-1$ is to play action $a_{1}\\ H-1$ consecutive times. Since $\\widetilde{\\pi}$ only plays $a_{1}$ at any given step with probability $\\zeta/2$ , it follows that the probability that $\\widetilde{\\pi}$ reaches $s_{1}$ at step $H-1$ on any given episode is only $(\\zeta/2)^{\\dot{H}-1}$ . Thus, ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{KL}(\\mathbb{P}^{M_{1},\\widetilde{\\pi}}\\parallel\\mathbb{P}^{M_{2},\\widetilde{\\pi}})\\le\\frac{3}{5}\\left(\\mathbb{E}^{M_{1},\\widetilde{\\pi}}[T_{H-1}(s_{1},a_{1})]+\\mathbb{E}^{M_{1},\\widetilde{\\pi}}[T_{H-1}(s_{1},a_{2})]\\right)}\\\\ &{\\qquad\\qquad\\qquad=\\frac{3}{5}\\mathbb{E}^{M_{1},\\widetilde{\\pi}}[T_{H-1}(s_{1})]}\\\\ &{\\qquad\\qquad\\qquad=\\frac{3}{5}\\left(\\frac{\\zeta}{2}\\right)^{H-1}\\cdot T.}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "We thus have: ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{\\hat{\\pi}}\\operatorname*{sup}_{i\\in\\{1,2\\}}\\mathbb{E}^{\\mathcal{M}_{i},\\tilde{\\pi}}[V_{0}^{\\mathcal{M}_{i},\\star}-V_{0}^{\\mathcal{M}_{i},\\hat{\\pi}}]\\geq\\frac{\\epsilon_{\\mathrm{sim}}}{4}\\left(1-\\sqrt{\\frac{3}{10}\\left(\\frac{\\zeta}{2}\\right)^{H-1}\\cdot T}\\right)\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "and we therefore have $\\begin{array}{r}{\\operatorname*{inf}_{\\widehat{\\pi}}\\operatorname*{sup}_{i\\in\\{1,2\\}}\\mathbb{E}^{\\mathcal{M}_{i},\\widetilde{\\pi}}[V_{0}^{\\mathcal{M}_{i},\\star}-V_{0}^{\\mathcal{M}_{i},\\widehat{\\pi}}]\\geq\\epsilon_{\\sin}/8}\\end{array}$ unless ", "page_idx": 37}, {"type": "equation", "text": "$$\nT\\geq\\frac{5}{6}\\cdot\\left(\\frac{2}{\\zeta}\\right)^{H-1}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Lower Bound for $\\zeta$ -Greedy Without sim (Proposition 1). In order to quantify the performance of a $\\zeta$ -greedy algorithm, we must specify how it chooses $\\widehat{f}$ when it has not yet observed any samples from a given $(s,a,h)$ . Following the lead of Theorem  2 of [11], to avoid an overly optimistic or pessimistic initialization, we assume that the replay buffer is initialized with a single sample from each $(s,a,h)$ . Note that the conclusion would hold with other initializations, however, e.g. initializing $\\widehat{f}_{h}(s,a)=0$ or randomly if we have no observations from $(s,a,h)$ . ", "page_idx": 38}, {"type": "text", "text": "Assume that the observation from $(s_{1},a_{1},H-1)$ transitions to $s_{2}$ , which occurs with probability at least $1/4$ . In this case, we then have that, for each $h$ , ${\\widehat{f}}_{h}^{0}(s_{1},a_{2})\\geq{\\widehat{f}}_{h}^{0}(s_{1},a_{1})$ . Thus, following the $\\zeta$ -greedy policy, we have that $\\pi_{h}^{0}(a_{1}~|~s_{1})\\leq1/2$ . Denote this event on $\\mathcal{E}_{0}$ . Furthermore, the only way we will have $\\widehat{f}_{h}^{0}(s_{1},a_{2})\\<\\widehat{f}_{h}^{0}(s_{1},a_{1})$ is if we visit $(s_{1},a_{1},H-1)$ again and observe a transition to $s_{1}$ . For this  to occur, how ever, we must play action $a_{1}\\ H-1$ times consecutively which, in this case, will occur with probability at most $\\operatorname*{max}\\{1/2,\\zeta/2\\}^{H-1}\\leq1/2^{H-1}$ . ", "page_idx": 38}, {"type": "text", "text": "Following the argument in the direct policy transfer case, we have ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\underset{\\widehat{\\pi}}{\\operatorname*{inf}}\\ \\underset{i\\in\\{1,2\\}}{\\operatorname*{sup}}\\mathbb{E}^{{M_{i}},\\widetilde{\\pi}}[V_{0}^{{M_{i}},\\star}-V_{0}^{{M_{i}},\\widehat{\\pi}}]\\ge\\underset{\\widehat{\\pi}}{\\operatorname*{inf}}\\ \\underset{i\\in\\{1,2\\}}{\\operatorname*{sup}}\\frac{1}{4}\\mathbb{E}^{{M_{i}},\\widetilde{\\pi}}[V_{0}^{{M_{i}},\\star}-V_{0}^{{M_{i}},\\widehat{\\pi}}\\mid\\mathcal{E}_{0}]}\\\\ {\\ge\\frac{\\epsilon_{\\sin}}{16}\\left(1-\\sqrt{\\frac{3}{10}\\mathbb{E}^{{M_{1}}}[T_{H-1}(s_{1})\\mid\\mathcal{E}_{0}]}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where $\\mathbb{E}^{\\mathcal{M}_{1}}[T_{H-1}(s_{1})\\mid\\mathcal{E}_{0}]$ is the expected number of visitations to $(s_{1},H-1)$ after $T$ episodes of running the $\\zeta$ -greedy policy. We can rewrite ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\mathbb{E}^{\\mathcal{M}_{1}}[T_{H-1}(s_{1})\\mid\\mathcal{E}_{0}]=\\sum_{t=1}^{T}\\mathbb{E}^{\\mathcal{M}_{1}}[\\mathbb{I}\\{s_{H-1}=s_{1}\\}\\mid\\mathcal{E}_{0}].\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Let $\\mathcal{E}$ be the event that we have reached $(s_{1},H-1)$ in the first $T$ rounds. Then, ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{\\mathbb{C}}^{M_{1}}[\\mathbb{I}\\{s_{H-1}=s_{1}\\}\\mid\\mathcal{E}_{0}]=\\mathbb{E}^{M_{1}}[\\mathbb{I}\\{s_{H-1}=s_{1}\\}\\mid\\mathcal{E},\\mathcal{E}_{0}]\\mathbb{P}^{M_{1}}[\\mathcal{E}\\mid\\mathcal{E}_{0}]+\\mathbb{E}^{M_{1}}[\\mathbb{I}\\{s_{H-1}=s_{1}\\}\\mid\\mathcal{E}^{c},\\mathcal{E}_{0}]}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\leq\\mathbb{P}^{M_{1}}[\\mathcal{E}\\mid\\mathcal{E}_{0}]+\\mathbb{E}^{M_{1}}[\\mathbb{I}\\{s_{H-1}=s_{1}\\}\\mid\\mathcal{E}^{c},\\mathcal{E}_{0}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "By what we have just argued, we have $\\mathbb{P}^{\\mathcal{M}_{1}}[\\mathcal{E}\\mid\\mathcal{E}_{0}]\\le T\\cdot\\frac{1}{2^{H-1}}$ , and $\\mathbb{E}^{\\mathcal{M}_{1}}[\\mathbb{I}\\{s_{H-1}=s_{1}\\}\\ |\\ \\mathcal{E}^{c},\\mathcal{E}_{0}]\\le$ $\\frac{1}{2^{H-1}}$ . Thus, $\\begin{array}{r}{\\mathbb{E}^{\\mathcal{M}_{1}}[T_{H-1}(s_{1})\\mid\\mathcal{E}_{0}]\\le\\frac{2T^{2}}{2^{H-1}}}\\end{array}$ . It follows that, ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{\\widehat{\\pi}}\\operatorname*{sup}_{i\\in\\{1,2\\}}\\mathbb{E}^{\\mathcal{M}_{i},\\widetilde{\\pi}}[V_{0}^{\\mathcal{M}_{i},\\star}-V_{0}^{\\mathcal{M}_{i},\\widehat{\\pi}}]\\ge\\frac{\\epsilon_{\\mathrm{sim}}}{16}\\left(1-\\sqrt{\\frac{3}{10}\\frac{2T^{2}}{2^{H-1}}}\\right)\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "and we therefore have $\\operatorname*{inf}_{\\widehat{\\pi}}\\operatorname*{sup}_{i\\in\\{1,2\\}}\\mathbb{E}^{\\mathcal{M}_{i},\\widetilde{\\pi}}[V_{0}^{\\mathcal{M}_{i},\\star}-V_{0}^{\\mathcal{M}_{i},\\widehat{\\pi}}]\\ge\\epsilon_{\\sin}/32$ unless ", "page_idx": 38}, {"type": "equation", "text": "$$\nT\\geq{\\sqrt{{\\frac{5}{8}}\\cdot2^{H-1}}}.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Upper Bound for Exploration Policy Transfer (Proposition 4). To obtain an upper bound for Algorithm 1, we can apply Theorem 1, so long as ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\epsilon_{\\mathrm{sim}}\\leq\\frac{\\lambda_{\\operatorname*{min}}^{\\star}}{64d H A^{3}}.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Note that in our setting we have $d=4,A=2,\\lambda_{\\operatorname*{min}}^{\\star}=1/4$ , so this condition reduces to $\\begin{array}{r}{\\epsilon_{\\mathrm{sim}}\\leq\\frac{1}{8192H}}\\end{array}$ . Taking to simply be the set of $Q$ -functions defined above ( $\\mathrm{{so}}\\left.V_{\\mathrm{{max}}}=H\\right)$ ), Theorem 1 then gives that with probability at least $1-\\delta$ , Algorithm 1 learns an $\\epsilon$ -optimal policy as long as $\\begin{array}{r}{T\\geq c\\cdot\\frac{H^{1\\breve{7}}}{\\epsilon^{8}}\\cdot\\log\\frac{H}{\\delta}}\\end{array}$ . ", "page_idx": 38}, {"type": "text", "text": "D.2 Proof of Proposition 5 ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "We define three MDPs: $\\mathcal{M}^{\\mathsf{s i m}}$ , and two possible real MDPs, $\\mathcal{M}_{1}:=\\mathcal{M}^{\\sf r e a l,1}$ and $\\mathcal{M}_{2}:=\\mathcal{M}^{\\sf r e a l,2}$ . In all cases we have states $S=\\{s_{1},s_{2}\\}$ , actions $\\mathcal{A}=\\{a_{1},a_{2},a_{3},a_{4}\\}$ , and $H=2$ , and set the starting state to $s_{1}$ . We define ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r}{P_{1}^{\\sin}\\!\\left(s_{1}\\mid s_{1},a_{1}\\right)=1,\\quad P_{1}^{\\sin}\\!\\left(s_{1}\\mid s_{1},a\\right)=P_{1}^{\\sin}\\!\\left(s_{2}\\mid s_{1},a\\right)=1/2,a\\in\\{a_{2},a_{3},a_{4}\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "For both $\\mathcal{M}_{1}$ and $\\mathcal{M}_{2}$ , we have: ", "page_idx": 39}, {"type": "equation", "text": "$$\nP_{1}^{\\mathsf{r e a l}}(s_{1}\\mid s_{1},a_{1})=1,P_{1}^{\\mathsf{r e a l}}(s_{1}\\mid s_{1},a_{4})=P_{1}^{\\mathsf{r e a l}}(s_{2}\\mid s_{1},a_{4})=1/2\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "for $\\mathcal{M}_{1}$ , we have ", "page_idx": 39}, {"type": "equation", "text": "$$\n^{\\mathrm{prel}}_{1}(s_{2}\\mid s_{1},a_{2})=1+\\epsilon_{\\mathrm{sim}},P_{1}^{\\mathrm{real}}(s_{1}\\mid s_{1},a_{2})=1-\\epsilon_{\\mathrm{sim}},P_{1}^{\\mathrm{real}}(s_{1}\\mid s_{1},a_{3})=P_{1}^{\\mathrm{real}}(s_{2}\\mid s_{1},a_{3})=1\n$$", "text_format": "latex", "page_idx": 39}, {"type": "equation", "text": "$$\n^{\\mathrm{\\tiny{real}}}_{1}(s_{2}\\mid s_{1},a_{3})=1+\\epsilon_{\\mathrm{\\tiny{sim}}},P_{1}^{{\\mathrm{\\tiny{real}}}}(s_{1}\\mid s_{1},a_{3})=1-\\epsilon_{\\mathrm{\\tiny{sim}}},P_{1}^{{\\mathrm{\\tiny{real}}}}(s_{1}\\mid s_{1},a_{2})=P_{1}^{{\\mathrm{\\tiny{real}}}}(s_{2}\\mid s_{1},a_{2})=1\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "We take the reward to be 0 everywhere, except $r_{2}(s_{2},a)=1$ for all $a$ . ", "page_idx": 39}, {"type": "text", "text": "Note that each of these can be represented as a linear MDP in $d=2$ dimensions, so Assumption 2 holds. In particular, for $\\mathcal{M}^{\\mathsf{s i m}}$ we can take: ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\phi^{s}(s,a_{1})=e_{1},\\phi^{s}(s,a)=e_{2},a\\in\\{a_{2},a_{3},a_{4}\\},s\\in S,}\\\\ &{\\pmb{\\mu}_{1}^{s}(s_{1})=[1,1/2],\\pmb{\\mu}_{1}^{s}(s_{2})=[0,1/2].}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "For $\\mathcal{M}_{1}$ we can instead take: ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\phi^{r}(s,a_{1})=e_{1},\\phi^{r}(s,a)=[1/2,1/2],a\\in\\{a_{3},a_{4}\\},s\\in\\mathcal{S},}\\\\ &{\\phi^{r}(s,a_{2})=[1/2-\\epsilon_{\\mathrm{sim}},1/2+\\epsilon_{\\mathrm{sim}}],s\\in\\mathcal{S},}\\\\ &{\\mu_{1}^{r}(s_{1})=[1,0],\\mu_{1}^{r}(s_{2})=[0,1].}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "$\\mathcal{M}_{2}$ follows similarly with the role of $a_{2}$ and $a_{3}$ flipped. ", "page_idx": 39}, {"type": "text", "text": "It is easy to see that Assumption 1 is met on this instance for both choices of $\\mathcal{M}^{\\tt r e a l}$ . On $\\mathcal{M}^{\\sf s i m}$ , the policy $\\pi_{\\mathrm{exp}}$ which in every states plays action $a_{1}$ with probability $1/2$ and action $a_{4}$ with probability $1/2$ satisfies $\\lambda_{\\operatorname*{min}}(\\mathbb{E}^{\\mathrm{sim},\\pi_{\\mathrm{exp}}}[\\phi^{\\mathsf{s}}(s_{h},a_{h})\\phi^{\\mathsf{s}}(s_{h},a_{h})^{\\top}])\\geq1/2$ (which shows that Assumption 3 holds). ", "page_idx": 39}, {"type": "text", "text": "Note, however, that $\\pi_{\\mathrm{exp}}$ does not play action $a_{2}$ or $a_{3}$ . As $\\mathcal{M}_{1}$ and $\\mathcal{M}_{2}$ differ only on $a_{2}$ and $a_{3}$ , playing $\\pi_{\\mathrm{exp}}$ will not allow for $\\mathcal{M}_{1}$ and $\\mathcal{M}_{2}$ to be distinguished. As $a_{2}$ is the optimal action on $\\mathcal{M}_{1}$ and $a_{3}$ the optimal action on $\\mathcal{M}_{2}$ , it follows that playing $\\pi_{\\mathrm{exp}}$ will not allow for the identification of the optimal policy on $\\mathcal{M}_{1}$ and $\\mathcal{M}_{2}$ . This can be formalized identically to Appendix D.1, yielding the stated result. ", "page_idx": 39}, {"type": "text", "text": "E Experimental Details ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "E.1 Didactic Tabular Example ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Consider the following variation of the combination lock. We let the action space $A=\\{1,2\\}$ , and assume there are two states, $S=\\{s_{1},s_{2}\\}$ , and horizon $H$ . We start in state $s_{1}$ . The sim dynamics are given as: ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\forall h<H-1:\\quad P_{h}^{\\sin}(s_{1}\\mid s_{1},a_{1})=1,\\quad P_{h}^{\\sin}(s_{2}\\mid,s_{1},a_{2})=1}\\\\ &{P_{H-1}^{\\sin}(s_{1}\\mid s_{1},a_{1})=1/4,P_{H-1}^{\\sin}(s_{2}\\mid s_{1},a_{1})=3/4,P_{H-1}^{\\sin}(s_{2}\\mid s_{1},a_{2})=1}\\\\ &{\\forall h\\in[H]:\\quad P_{h}^{\\sin}(s_{2}\\mid s_{2},a)=1,a\\in\\{a_{1},a_{2}\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "and the real dynamics are given as: ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\forall h<H-1:\\quad P_{h}^{r\\mathsf{e a l}}(s_{1}\\mid s_{1},a_{1})=1,\\quad P_{h}^{r\\mathsf{e a l}}(s_{2}\\mid,s_{1},a_{2})=1}\\\\ &{P_{H-1}^{r\\mathsf{e a l}}(s_{1}\\mid s_{1},a_{1})=3/4,P_{H-1}^{r\\mathsf{e a l}}(s_{2}\\mid s_{1},a_{1})=1/4,P_{H-1}^{r\\mathsf{e a l}}(s_{2}\\mid s_{1},a_{2})=1}\\\\ &{\\forall h\\in[H]:\\quad P_{h}^{r\\mathsf{e a l}}(s_{2}\\mid s_{2},a)=1,a\\in\\{a_{1},a_{2}\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Note that these only differ on $(s_{1},a_{1})$ at $h=H-1$ , and we have $\\epsilon_{\\mathrm{sim}}=1/2$ . We define the reward function as (note that this is deterministic, and the same for both sim and real): ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\forall h\\in[H]:\\quad r_{h}(s_{1},a_{2})=1/8-h/8H}\\\\ &{r_{H}(s_{1},a)=1/5,a\\in\\{a_{1},a_{2}\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "and all other rewards are taken to be 0. ", "page_idx": 39}, {"type": "text", "text": "The intuition for this example is as follows. In both sim and real, the only way the agent can get reward is to either end up in state $s_{1}$ at step $H$ , or to take action $a_{2}$ in state $s_{1}$ at any point. In sim, the probability of ending up in state $s_{1}$ at step $H$ , even if the optimal sequence of actions to do this is taken, is only $1/4$ , due to the final transition, and thus the average reward obtained by the policy which aims to end up in $s_{1}$ is only $1/4$ . In contrast, if we take action $a_{2}$ in $s_{1}$ , we will always collect reward of at least $3/8$ (and the earlier we take action $a_{2}$ the more reward we collect, up to 1/2). Thus, in sim the optimal thing to do in $s_{1}$ is always to play $a_{2}$ . However, if we play $a_{2}$ even once, we will transition out of $s_{1}$ and never return, so there is no chance we will reach $s_{1}$ at step $H$ . ", "page_idx": 40}, {"type": "text", "text": "In real, the transitions at the final step are flipped, so that now the probability of finishing in $s_{1}$ , if we take the optimal sequences of actions to do this, is $3/4$ , and the expected reward for this is then also $3/4$ . Since the reward for taking $a_{2}$ in $s_{1}$ does not change, and is bounded as $1/2$ , then in real the optimal policy is to seek to end up in $s_{1}$ at the final step. ", "page_idx": 40}, {"type": "text", "text": "The challenge with ending up in $s_{1}$ at the end is that it requires playing action $a_{1}$ at every step. In this sense it is then a classic combination lock instance, and randomized exploration will fail, requiring $\\Omega(2^{H})$ episodes to reach the final state (since the probability of randomly taking $a_{1}$ at every state decreases exponentially with the horizon). Similarly, if we transfer the optimal policy from sim to real, it will never take action $a_{1}$ , so will never reach $s_{1}$ at the end, and if we transfer the optimal policy from sim with some random exploration, it will fail for the same reason random exploration from scratch fails. ", "page_idx": 40}, {"type": "text", "text": "However, note that we can transfer a policy from sim that is able to reach $s_{1}$ at the second-to-last step with probability 1, i.e. the policy that takes action $a_{1}$ at every step. Thus, if in sim we aim to learn exploration policies that can traverse the MDP, and we transfer these exploration policies, they will transfer, and will allow us to easily reach $s_{1}$ at the final step, and quickly determine that it is indeed the optimal thing in real. ", "page_idx": 40}, {"type": "text", "text": "We provide additional experimental results on this instance in Appendix E.1. ", "page_idx": 40}, {"type": "image", "img_path": "JjQl8hXJAS/tmp/97c140ad6cdef5d26f7f08fad2e5df5b45f9f9f99cf43d2c89ecce95f15f8ac8.jpg", "img_caption": ["Figure 5: Performance of Exploration Policy Transfer on instance from Section 5.2, varying number of states, actions, and horizon. We plot the number of samples required to achieve a reward of 0.35, which is approximately solving the task. All results are averaged across 20 trials. When increasing the number of states, we add additional 0-reward states (i.e. states given in yellow in Figure 2), and when adding additional actions we add additional low-reward actions (i.e. actions that have the same behavior as action $a_{2}$ in Figure 2). We observe that increasing the number of states and horizon increases the number of samples needed, while increasing the number of actions does not substantially. We emphasize, however, that this is for a particular example, and this scaling may not be the same for all examples\u2014Theorem 1, however, gives an upper bound on all examples. "], "img_footnote": [], "page_idx": 40}, {"type": "text", "text": "E.2 Practical Algorithm Details ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "The core of our work is to decouple the optimal policy training from exploration strategies in reinforcement learning fine-tuning. Specifically, we propose a framework that uses a set of diverse exploration policies to collect samples from the environment. These exploration policies are fixed while we run off policy RL updates on the collected samples to extract an optimal policy. Our theoretical derivation suggests that this decoupling can improve sample efficiency and overall learning performance. ", "page_idx": 40}, {"type": "text", "text": "1: Input: Simulator $\\mathcal{M}^{\\sf s i m}$ , real environment $\\mathcal{M}^{\\tt r e a l}$ , simulator training budget $N$ , exploration   \nreward balancing $\\alpha$ , reward threshold $\\epsilon$ , exploration set size $n$ .   \n2: Pre-train Exploration Policies in $\\mathcal{M}^{\\mathsf{s i m}}$ :   \n3: Initialize $\\bar{\\Pi_{e x p}}=\\{\\pi_{\\theta}(\\cdot|z)|z\\in\\{1\\ldots n\\}\\}$   \n4: Initialize discriminator $D_{\\phi}$   \n5: for $i=1$ to $N$ do $\\triangleright$ Learn diverse exploration policies   \n6: Sample latent $z\\sim\\operatorname{unif}(1,n)$ and initial state $s_{0}$ .   \n7: for $t=1$ to max_steps_per_episode do   \n8: Sample action $a_{t}\\sim\\pi_{\\theta}(a_{t}|s_{t},z)$ .   \n9: Step environment: $s_{t+1}\\sim p\\big(s_{t+1}\\big|s_{t},a_{t}\\big)$ .   \n10: Compute discriminator score $d_{t}=D(s_{t+1},z)$   \n11: Compute exploration reward re(st+1, z) = log z\u2032 exepx(pd((dstt)+1,z\u2032)).   \n12: if $R_{\\pi}\\geq\\epsilon$ then   \n13: Compute reward $r_{t}=r(s_{t},a_{t})+\\alpha\\cdot r_{e}(s_{t+1},z)$ .   \n14: else   \n15: Compute reward $r_{t}=r(s_{t},a_{t})$   \n16: Let $\\mathcal{D}\\leftarrow\\mathcal{D}\\cup\\left\\{\\left(s_{t},a_{t},r_{t},s_{t+1},z\\right)\\right\\}$ .   \n17: Update $\\pi_{\\theta}$ to maximize $J_{\\pi}$ with SAC.   \n18: Update $\\phi$ to maximize $J_{u}$ , $\\phi\\leftarrow\\phi+\\eta\\nabla_{\\phi}\\mathbb{E}_{s,z\\sim\\mathcal{D}}\\left[\\log D_{\\phi}(s,z)\\right]$   \n19: Compute $\\textstyle R_{\\pi}=\\sum_{t}r_{t}$   \n20: Explore in $\\mathcal{M}^{\\tt r e a l}$ and Estimate Optimal Policy :   \n21: Initialize SAC agent (either from scratch or to weights of optimal sim policy).   \n22: while not converged do   \n23: Sample $z\\sim\\operatorname{unif}(1,n)$ , play $\\pi_{\\theta}(\\cdot\\mid z)$ in $\\mathcal{M}^{\\tt r e a l}$ , add data to replay buffer of SAC.   \n24: Roll out SAC policy for one step, perform standard SAC update. ", "page_idx": 41}, {"type": "text", "text": "Our framework is complementary to (a) RL works on diversity or exploration that generate diverse policies and (b) off policy RL algorithms that optimize for policies. One can plug in (a) to extract a set of exploration policy from a simulator and use them for data collect in the real world but use (b) to optimize for the final policy. The design choice to use simulator to extract a set of exploration policies where each policy is not necessarily optimizing for the task at hand marks our distinction from previous works in (a) and (b). ", "page_idx": 41}, {"type": "text", "text": "We provide a practical instantiation of our framework using an approach inspired by One Solution is Not All You Need (OS) [30] to extract exploration policies and Soft Actor Critic (SAC) [30] to optimize for the optimal policy. We details the instantiation in Algorithm 6. OS trains a set of policy to optimize not only the task reward but also a discriminator reward where the discriminator encourages each policy to achieve different state. Unlike OS which carefully balances the task and exploration rewards to ensure all policies have a chance at solving the desired task, we emphasize only on having diverse policies. With a known sim2real gap, we posit that some sub-optimal policies that are not solving the task in the simulator is actually helpful for exploration in the real world, which allows us to simplify the balance between task and exploration. We uses standard off-shelf SAC update to optimize for the policy. ", "page_idx": 41}, {"type": "text", "text": "E.3 TychoEnv sim2sim Experiment Details ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "For the TychoEnv experiment we run a variant of Algorithm 6. We set $n=20$ , and set the reward to $r_{t}^{i}=(1-\\alpha_{i})r+\\alpha_{i}\\bar{r}_{e}$ where we vary $\\alpha_{i}$ from 0 to 0.5. While we use a sparse reward in $\\mathcal{M}^{\\tt r e a l}$ , to speed up training in $\\mathcal{M}^{\\mathsf{s i m}}$ we use a dense reward that penalizes the agent for its distance to the target. We train in $\\mathcal{M}^{\\sf s i m}$ for 7M steps to obtain exploration policies. Rather than simply transferring the converged version of the exploration policies trained in $\\mathcal{M}^{\\sf s i m}$ , we found it most effective to save the weights of the policies throughout training, and transfer all of these policies. As the majority of these policies do not collect any reward in $\\mathcal{M}^{\\sf s i m}$ , we run an initial flitering stage where we identify several policies from this set that find reward (this can be seen in Figure 4 with the initial region of 0 reward). We then run SAC in $\\mathcal{M}^{\\tt r e a l}$ , initialized from scratch, feeding in the data collected by these refined exploration policies into the replay buffer. We found it most effective to only inject data from the exploration policies in the replay buffer on episodes where they observe reward. We run vanilla SAC with $\\mathrm{UTD}=3$ and target entropy of -3. We rely on the implementation of SAC from stable-baselines3 [51]. ", "page_idx": 41}, {"type": "table", "img_path": "JjQl8hXJAS/tmp/08f29524e3a6bd82c30d49a9d1c266c97edfe7ba8f8b395152b20fa8f00eaf39.jpg", "table_caption": [], "table_footnote": ["Table 1: Hyperparameters used in Tycho training and finetuning "], "page_idx": 42}, {"type": "image", "img_path": "JjQl8hXJAS/tmp/e534ee0a96997a6bb97cc9182f718b2e9f0768b7e5535da663eec747f0df5729.jpg", "img_caption": ["Figure 6: Additional results on Tycho, including baselines training from scratch in $\\mathcal{M}^{\\tt r e a l}$ , and training exploration policies in $\\mathcal{M}^{\\mathsf{s i m}}$ with reward as stated above but with $\\alpha_{i}=1$ (which is equivalent to simply training exploration policies with DIAYN [14]). As can be seen, while training from scratch in $\\mathcal{M}^{\\tt r e a l}$ is able to learn, it learns at a much slower rate than exploration policy transfer, and achieves a much lower final value. Furthermore, training the exploration policies to maximize a mix of the task and diversity reward yields a substantial gain over simply training them to be diverse. "], "img_footnote": [], "page_idx": 42}, {"type": "text", "text": "", "page_idx": 42}, {"type": "text", "text": "For direct policy transfer, we train a policy to convergence in $\\mathcal{M}^{\\sf s i m}$ that solves the task (using SAC), and then transfer this single policy, otherwise following the same procedure as above. ", "page_idx": 42}, {"type": "text", "text": "In $\\mathcal{M}^{\\tt r e a l}$ , our reward is chosen to have a value of 50 if the end effector makes contact with the ball, and otherwise 0. If the robot successfully makes contact with the ball the episode terminates. To generate a realistic transfer environment, we change the control frequency (doubling it in $\\mathcal{M}^{\\tt r e a l}$ ) and the action bounds. ", "page_idx": 42}, {"type": "text", "text": "For both methods, we run the $\\mathcal{M}^{\\sf s i m}$ training procedure 4 times, and then with each of these run it in $\\mathcal{M}^{\\tt r e a l}$ twice. Error bars in our plot denote one standard error. ", "page_idx": 42}, {"type": "text", "text": "All experiments were run on two Nvidia V100 GPUs, and 32 Intel(R) Xeon(R) CPU E5-2620 v4 $@$ 2.10GHz CPUs. Additional hyperparameters in given in Table 1. ", "page_idx": 42}, {"type": "text", "text": "We provide results on several additional baselines for the Tycho setup in Figure 6. ", "page_idx": 42}, {"type": "image", "img_path": "JjQl8hXJAS/tmp/787c526985f5edcf7e2d74c7d6d7bef52f26e8413e57b6a30d3bf4327c029f02.jpg", "img_caption": ["Figure 7: Results on Franka sim2real experiment, comparing to training from scratch in real. "], "img_footnote": [], "page_idx": 42}, {"type": "table", "img_path": "JjQl8hXJAS/tmp/8d16e0fc2b638fc41babd5888a23f8aee9ed1a8fed8ef75b0c5311be6dbdc31e.jpg", "table_caption": [], "table_footnote": ["Table 2: Hyperparameters used in Franka training and finetuning "], "page_idx": 43}, {"type": "image", "img_path": "JjQl8hXJAS/tmp/e979c9adb0d4e41655b5cbedd08acdf93803aff3395ab9b6efd25845d72d6601.jpg", "img_caption": ["Figure 8: Franka Hammering Task Setup "], "img_footnote": [], "page_idx": 43}, {"type": "image", "img_path": "JjQl8hXJAS/tmp/a03a5c4f64cd8aed537cfa65e6fc7bc393a8777df21e9a3d9ee26d084148f528.jpg", "img_caption": ["Figure 9: Results on sim2sim Transfer in Franka Simulator "], "img_footnote": [], "page_idx": 43}, {"type": "text", "text": "E.4 sim2sim Transfer on Franka Emika Panda Robot Arm ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "We next turn to the Franka Emika Panda robot arm [17], for which we use a realistic custom simulator built using the MuJoCo simulation engine [61]. We consider a hammering task, where the Franka arm holds a hammer, and the goal is to hammer a nail into the board (see Figure 8). Success is obtained when the nail is fully inserted. We simulate sim2real transfer by setting $\\mathcal{M}^{\\tt r e a l}$ to be a version of the simulator with nail location and stiffness significantly beyond the range seen during training in $\\mathcal{M}^{\\mathsf{s i m}}$ . ", "page_idx": 43}, {"type": "text", "text": "We compare exploration policy transfer with direct sim2real policy transfer. Unlike the Tycho experiment, where we trained policies from scratch in $\\mathcal{M}^{\\tt r e a l}$ and simply used the policies trained in $\\mathcal{M}^{\\mathsf{s i m}}$ to explore, here we initialize the task policy in $\\mathcal{M}^{\\tt r e a l}$ to $\\pi^{\\mathsf{s i m,\\star}}$ , which we then finetune on the data collected in $\\mathcal{M}^{\\tt r e a l}$ by running SAC. For direct sim2real transfer, we collect data in $\\mathcal{M}^{\\tt r e a l}$ by simply rolling out $\\pi^{\\mathsf{s i m,\\star}}$ and feeding this data to the replay buffer of SAC. For exploration policy transfer, we train an ensemble of $n\\,=\\,10$ exploration policies in $\\mathcal{M}^{\\mathsf{s i m}}$ and run these policies in $\\mathcal{M}^{\\tt r e a l}$ , again feeding this data to the replay buffer of SAC to finetune $\\pi^{\\mathsf{s i m,\\star}}$ . During training in $\\mathcal{M}^{\\sf s i m}$ , we utilize domain randomization for both methods, randomizing nail stiffness, location, radius, mass, board size, and damping. ", "page_idx": 43}, {"type": "text", "text": "The results of this experiment are shown in Figure 9. We see that, while direct policy transfer is able to learn, it learns at a significantly slower rate than our exploration policy transfer approach, and achieves a much smaller final success rate. ", "page_idx": 43}, {"type": "text", "text": "E.5 Franka sim2real Experiment Details ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "We use Algorithm 6 to train a policy on the Franka robot with $n=15$ . ", "page_idx": 43}, {"type": "text", "text": "The reward of the pushing task is given by: ", "page_idx": 43}, {"type": "equation", "text": "$$\nr(s_{t},a_{t})=-\\|\\mathbf{p_{ee}}-\\mathbf{p_{\\mathrm{obj}}}\\|^{2}-\\|\\mathbf{p_{\\mathrm{obj}}}-\\mathbf{p_{\\mathrm{goal}}}\\|^{2}+\\mathbb{I}_{\\mathbf{p_{\\mathrm{obj}}}-\\mathbf{p_{\\mathrm{goal}}}\\leq0.025}-\\mathbb{I}_{\\mathbf{p_{\\mathrm{obj}}}\\mathrm{offable}}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "where $\\mathbf{p}_{\\mathrm{goal}}$ is the desired position of the puck by the edge of the surface. ", "page_idx": 43}, {"type": "text", "text": "The network architecture of the actor and critic networks are identical, consisting of a 2-layer MLP, each of size 256 and ReLU activations. ", "page_idx": 43}, {"type": "text", "text": "We use stable-baselines3 [51] for our SAC implementation, using all of their default hyperparameters. The implemention of OS is built on top of this SAC implementation. Values of hyperparameters ", "page_idx": 43}, {"type": "text", "text": "are shown in Table 2. Gaussian noise with mean 0 and standard deviation 0.005 meters is added in simulation to the position of the puck. Hyperparameters are identical between exploration policy transfer and direct transfer methods.   \nFor finetuning in real, we start off by sampling exclusively from the buffer used during simulation. Then, as finetuning proceeds, we gradually start taking more samples from the real buffer, with the proportion of samples taken from sim equal to $1-s/3000$ , where $s$ is the current number of steps. After 3000 steps, all samples are taken from the real buffer.   \nExperiments were run using a standard Nvidia RTX 4090 GPU. Training in simulation takes about 3 hours, while finetuning was ran for about 90 minutes.   \nIn Figure 7, we provide results on this setup running the additional baseline of training a policy from scratch in real. As can be seen, this is significantly worse than either transfer method. ", "page_idx": 44}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 45}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 45}, {"type": "text", "text": "Justification: We validate all our claims with theoretical results and experiments. ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 45}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 45}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Justification: Please see Discussion section. ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 45}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 45}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 45}, {"type": "text", "text": "Justification: All results are precisely proved in the supplemental, and all assumptions clearly stated. ", "page_idx": 46}, {"type": "text", "text": "Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 46}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 46}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 46}, {"type": "text", "text": "Justification: To the extent possible, given that we are working with real-world systems, we have described our setup and implementation details. ", "page_idx": 46}, {"type": "text", "text": "Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 46}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 47}, {"type": "text", "text": "Answer: [No] ", "page_idx": 47}, {"type": "text", "text": "Justification: We have not currently released our code but hope to in the future. Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 47}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 47}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Justification: We have stated all parameters and algorithm details to the best of our knowledge (please see Experimental Details section in supplemental). ", "page_idx": 47}, {"type": "text", "text": "Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 47}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 47}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Justification: For all our experimental results, we provide error bars corresponding to 1 standard error. ", "page_idx": 47}, {"type": "text", "text": "Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 47}, {"type": "text", "text": "", "page_idx": 48}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 48}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 48}, {"type": "text", "text": "Justification: Please see Experimental Details section in appendix. ", "page_idx": 48}, {"type": "text", "text": "Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 48}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 48}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 48}, {"type": "text", "text": "Justification: This paper does not violate any ethical guidelines. ", "page_idx": 48}, {"type": "text", "text": "Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 48}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 48}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 48}, {"type": "text", "text": "Justification: This is work is related to the advancement of our fundamental understanding of machine learning. As such, we do not believe there are any direct societal impacts from this work. ", "page_idx": 48}, {"type": "text", "text": "Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 48}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 49}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 49}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 49}, {"type": "text", "text": "Justification: We are not releasing high-risk models. ", "page_idx": 49}, {"type": "text", "text": "Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 49}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 49}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Justification: We have cited the creators of the code used in this project. Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 49}, {"type": "text", "text": "", "page_idx": 50}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 50}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 50}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 50}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 50}, {"type": "text", "text": "Answer: [NA] Justification: This work does not involve human subjects research. ", "page_idx": 50}, {"type": "text", "text": "Guidelines: ", "page_idx": 50}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 50}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 50}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 50}, {"type": "text", "text": "Justification: This work does not involve human subjects. ", "page_idx": 50}, {"type": "text", "text": "Guidelines: ", "page_idx": 50}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 50}]