[{"figure_path": "GkzrVxs9LS/tables/tables_5_1.jpg", "caption": "Table 1: Performance comparisons between LRFL models and SOTA baselines on CheXpert. The best result is highlighted in bold, and the second-best result is underlined. This convention is followed by all the tables in this paper. DN represents DenseNet.", "description": "This table compares the performance of the proposed Low-Rank Feature Learning (LRFL) method with state-of-the-art (SOTA) baselines on the CheXpert dataset for thorax disease classification.  It shows the mAUC and accuracy for several diseases, highlighting the improved performance of the LRFL models (ViT-S-LR and ViT-B-LR) over the baseline models (ViT-S and ViT-B).  DN refers to DenseNet.", "section": "4.2 Stanford CheXpert"}, {"figure_path": "GkzrVxs9LS/tables/tables_7_1.jpg", "caption": "Table 1: Performance comparisons between LRFL models and SOTA baselines on CheXpert. The best result is highlighted in bold, and the second-best result is underlined. This convention is followed by all the tables in this paper. DN represents DenseNet.", "description": "This table compares the performance of the proposed Low-Rank Feature Learning (LRFL) method with several state-of-the-art (SOTA) baselines on the CheXpert dataset for thorax disease classification.  It shows the mAUC and Accuracy scores for different diseases (Atelectasis, Cardiomegaly, Consolidation, Edema, Effusion).  The best-performing model for each metric is highlighted.", "section": "4.2 Stanford CheXpert"}, {"figure_path": "GkzrVxs9LS/tables/tables_7_2.jpg", "caption": "Table 2: Performance comparisons between LRFL models and SOTA baselines on COVIDx (in accuracy). DN represents DenseNet.", "description": "This table compares the performance of the proposed Low-Rank Feature Learning (LRFL) models with state-of-the-art (SOTA) baselines on the COVIDx dataset.  It shows the Covid-19 Sensitivity and Accuracy for different models, including both CNN-based (DenseNet) and Transformer-based (ViT) architectures. The LRFL models consistently outperform their corresponding baselines, demonstrating the effectiveness of the proposed method.", "section": "4.3 COVIDX"}, {"figure_path": "GkzrVxs9LS/tables/tables_8_1.jpg", "caption": "Table 3: Performance comparison of baseline models and LRFL models on the CheXpert and COVIDx datasets, with and without synthetic data. n denotes the number of training images in the respective dataset.", "description": "This table presents a comparison of the performance of baseline models and LRFL models on the CheXpert and COVIDx datasets.  It shows the results with and without the addition of synthetic data generated using a diffusion model. The table highlights the improvement in mAUC (multi-class Area Under the Receiver Operating Characteristic Curve) for CheXpert and accuracy for COVIDx achieved by the LRFL models, particularly when augmented with synthetic data.", "section": "4.4 Improved Results using Diffusion Model"}, {"figure_path": "GkzrVxs9LS/tables/tables_8_2.jpg", "caption": "Table 4: Average feature distance between original features and disease features of images with a ground-truth bounding box for the disease in the NIH ChestX-ray 14.", "description": "This table presents a comparison of the average feature distance between original features and disease features for ViT-S, ViT-S-LR, ViT-B, and ViT-B-LR models.  The average feature distance is calculated using KL-divergence on the softmaxed features for images in the NIH ChestX-ray14 dataset with ground truth bounding boxes for disease areas.  Lower values indicate that the original features are closer to the disease features, suggesting better robustness to background noise.", "section": "4.5.1 Study of LRFL in Reducing the Adverse Effects of Background"}, {"figure_path": "GkzrVxs9LS/tables/tables_9_1.jpg", "caption": "Table 5: AP25 and AP50 scores for different diseases using ViT-S and ViT-S-LR models.", "description": "This table presents the Average Precision (AP) at 25% and 50% Intersection over Union (IoU) for different thorax diseases detected by ViT-S and ViT-S-LR models.  It shows that the low-rank feature learning method (LRFL) significantly improves the accuracy of disease localization.", "section": "4.5.2 Disease Localization"}, {"figure_path": "GkzrVxs9LS/tables/tables_18_1.jpg", "caption": "Table 6: Performance comparisons between LRFL models and SOTA baselines on NIH ChestX-ray14. RN, DN, and SwinT represent ResNet, DenseNet, and Swin Transformer.", "description": "This table compares the performance of the proposed Low-Rank Feature Learning (LRFL) method with various state-of-the-art (SOTA) methods on the NIH ChestX-ray14 dataset for thorax disease classification.  It shows the mAUC scores achieved by different models (ResNet, DenseNet, Swin Transformer, and ViT variants) with and without LRFL.  The table highlights the improvement in mAUC achieved by LRFL across different models and pre-training strategies.  Pre-training methods include ImageNet-1K and Masked Autoencoders (MAE) on chest X-rays.", "section": "4.1 Implementation Details"}, {"figure_path": "GkzrVxs9LS/tables/tables_18_2.jpg", "caption": "Table 1: Performance comparisons between LRFL models and SOTA baselines on CheXpert. The best result is highlighted in bold, and the second-best result is underlined. This convention is followed by all the tables in this paper. DN represents DenseNet.", "description": "This table compares the performance of the proposed Low-Rank Feature Learning (LRFL) method with several state-of-the-art (SOTA) baseline models on the CheXpert dataset for thorax disease classification.  It shows the mAUC and accuracy for each of five diseases (Atelectasis, Cardiomegaly, Consolidation, Edema, and Effusion), as well as the overall mAUC across all five diseases.  The best and second-best results are highlighted for easy comparison.", "section": "4.2 Stanford CheXpert"}, {"figure_path": "GkzrVxs9LS/tables/tables_18_3.jpg", "caption": "Table 8: Optimal values of rank ratio \u03b3, weighting parameter \u03b7, and learning rate \u03bc decided by cross-validation for different models on different datasets.", "description": "This table presents the hyperparameters obtained through cross-validation for different models (ViT-S and ViT-B) on three datasets (NIH ChestX-ray, COVIDx, and CheXpert). The hyperparameters include the rank ratio (\u03b3), the weighting parameter for the truncated nuclear norm (\u03b7), and the learning rate (\u03bc). These values were determined using a 5-fold cross-validation process to optimize the performance of the low-rank feature learning method.", "section": "B.3 Cross-Validation Results"}, {"figure_path": "GkzrVxs9LS/tables/tables_19_1.jpg", "caption": "Table 9: Time Spent for cross-validation on NIH ChestX-ray14, CheXpert, and CovidX. All the results are reported in minutes.", "description": "This table presents the time taken for cross-validation on three different datasets (NIH ChestX-ray14, CheXpert, and CovidX) using two different Vision Transformer models (ViT-S-LR and ViT-B-LR).  The cross-validation was used to determine optimal hyperparameters (rank ratio \u03b3, weighting parameter \u03b7, and learning rate \u03bc) for the LRFL method. The time is reported in minutes for each model and dataset. Note that only 20% of the training data is used for the cross-validation and the models were trained for only 40% of the total training epochs.", "section": "B.3 Cross-Validation Results"}, {"figure_path": "GkzrVxs9LS/tables/tables_20_1.jpg", "caption": "Table 11: The table evaluates the performance of various models under low data regimes on the NIH ChestX-rays14 dataset. Models trained with low-rank features effectively combat overfitting in scenarios with limited data availability, thereby enhancing the quality of representations for downstream tasks.", "description": "This table shows the performance of different models (ViT-S, ViT-S-LR, ViT-B, ViT-B-LR) with varying amounts of training data (5%, 10%, 15%, 20%, 25%, 50%) on the NIH ChestX-ray14 dataset.  The results demonstrate the effectiveness of the LRFL method in handling small datasets by mitigating overfitting and improving the quality of learned representations.", "section": "B.4.2 Experiments in Small Data Regimes"}, {"figure_path": "GkzrVxs9LS/tables/tables_20_2.jpg", "caption": "Table 1: Performance comparisons between LRFL models and SOTA baselines on CheXpert. The best result is highlighted in bold, and the second-best result is underlined. This convention is followed by all the tables in this paper. DN represents DenseNet.", "description": "This table compares the performance of the proposed Low-Rank Feature Learning (LRFL) method against state-of-the-art (SOTA) baselines on the CheXpert dataset for thorax disease classification.  It shows the mAUC and accuracy for several diseases, highlighting the superior performance of LRFL models (ViT-S-LR and ViT-B-LR) compared to the baseline models (ViT-S and ViT-B) and other SOTA methods.  The table indicates improvements in both overall mAUC and individual disease classification accuracy for LRFL.", "section": "4.2 Stanford CheXpert"}, {"figure_path": "GkzrVxs9LS/tables/tables_21_1.jpg", "caption": "Table 1: Performance comparisons between LRFL models and SOTA baselines on CheXpert. The best result is highlighted in bold, and the second-best result is underlined. This convention is followed by all the tables in this paper. DN represents DenseNet.", "description": "This table compares the performance of the proposed Low-Rank Feature Learning (LRFL) models against state-of-the-art (SOTA) baselines on the CheXpert dataset for thorax disease classification.  It shows the Area Under the Curve (AUC) and accuracy for multiple diseases, highlighting the superior performance of the LRFL models.", "section": "4.2 Stanford CheXpert"}, {"figure_path": "GkzrVxs9LS/tables/tables_22_1.jpg", "caption": "Table 14: Selected optimal percentage of images \u03b1 on different datasets and models.", "description": "This table shows the optimal percentage of synthetic images (\u03b1) determined through 5-fold cross-validation for different models and datasets (CheXpert and COVIDx). The optimal percentage of synthetic images that yields the best performance is reported for each model and dataset.", "section": "4.4 Improved Results using Diffusion Model"}]