[{"type": "text", "text": "Diff-PCC: Diffusion-based Neural Compression for 3D Point Clouds ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 Stable diffusion networks have emerged as a groundbreaking development for   \n2 their ability to produce realistic and detailed visual content. This characteristic   \n3 renders them ideal decoders, capable of producing high-quality and aesthetically   \n4 pleasing reconstructions. In this paper, we introduce the first diffusion-based point   \n5 cloud compression method, dubbed Diff-PCC, to leverage the expressive power of   \n6 the diffusion model for generative and aesthetically superior decoding. Different   \n7 from the conventional autoencoder fashion, a dual-space latent representation   \n8 is devised in this paper, in which a compressor composed of two independent   \n9 encoding backbones is considered to extract expressive shape latents from distinct   \n10 latent spaces. At the decoding side, a diffusion-based generator is devised to   \n11 produce high-quality reconstructions by considering the shape latents as guidance   \n12 to stochastically denoise the noisy point clouds. Experiments demonstrate that the   \n13 proposed Diff-PCC achieves state-of-the-art compression performance (e.g., 7.711   \n14 dB BD-PSNR gains against the latest G-PCC standard at ultra-low bitrate) while   \n15 attaining superior subjective quality. Source code will be made publicly available. ", "page_idx": 0}, {"type": "text", "text": "16 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "17 Point clouds, composed of numerous discrete points with coordinates (x, y, z) and optional attributes,   \n18 offer a flexible representation of diverse 3D shapes and are extensively applied in various fields such   \n19 as autonomous driving [8], game rendering [35], robotics [7], and others. With the rapid advancement   \n20 of point cloud acquisition technologies and 3D applications, effective point cloud compression   \n21 techniques have become indispensable to reduce transmission and storage costs. ", "page_idx": 0}, {"type": "text", "text": "22 1.1 Background ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "23 Prior to the widespread adoption of deep learning techniques, the most prominent traditional point   \n24 cloud compression methods were the G-PCC [39] and V-PCC [40] proposed by the Moving Picture   \n25 Experts Group(MPEG). G-PCC compresses point clouds by converting them into a compact tree   \n26 structure, whereas V-PCC projects point clouds onto a 2D plane for compression. In recent years,   \n27 numerous deep learning-based methods have been proposed [50, 45, 11, 12, 7, 30, 46, 14, 42],   \n28 which primarily employ the Variational Autoencoder (VAE) [1, 2] architecture. By learning a prior   \n29 distribution of the data, the VAE projects the original input into a higher-dimensional latent space,   \n30 and reconstructs the latent representation effectively using a posterior distribution. However, previous   \n31 VAE-based point cloud compression architectures still face recognized limitations: 1) Assuming a   \n32 single Gaussian distribution $\\bar{N}(\\mu,\\sigma^{2})$ in the latent space may prove inadequate to capture the intricate   \n33 diversity of point cloud shapes, yielding blurry and detail-deficient reconstructions [56, 10]; 2) The   \n34 Multilayer Perceptron (MLP) based decoders [50, 45, 11, 12, 46] suffer from feature homogenization,   \n35 which leads to point clustering and detail degradations in the decoded point cloud surfaces, lacking the ", "page_idx": 0}, {"type": "image", "img_path": "1CssOqRYyz/tmp/7545aa192e0116ca7e4e0a3506ba417d0be445eab46303c4a425ea4a75e537fa.jpg", "img_caption": [], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Figure 1: Diff-PCC pipeline. $X_{t}$ and $\\bar{X_{t}}$ represents the tth original point cloud and noisy point cloud, respectively; $p$ refers to the forward process and $q$ refers to the reverse process; $N(0,I)$ means the pure noise. Entropy model and arithmetic coding is omitted for a concise explanation. ", "page_idx": 1}, {"type": "text", "text": "36 ability to produce high-quality reconstructions. Recently, Diffusion models (DMs) [5] have attracted   \n37 considerable attention in the field of generative modeling [34, 48, 41, 19] due to their outstanding   \n38 performance in generating high-quality samples and adapting to intricate data distributions, thus   \n39 presenting a novel and exciting opportunity within the domain of neural compression [33, 44, 25].   \n40 By generating a more refined and realistic 3D point cloud shape, DMs offer a distinctive approach to   \n41 reduce the heavy dependence of reconstruction quality on the information loss of bottleneck layers. ", "page_idx": 1}, {"type": "text", "text": "42 1.2 Our Approach ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "43 Building on the preceding discussion, we introduce Diff-PCC, a novel lossy point cloud compression   \n44 framework that leverages diffusion models to achieve superior rate-distortion performance with   \n45 exceptional reconstruction quality. Specifically, to enhance the representation ability of simplistic   \n46 Gaussian priors in VAEs, this paper devises a dual-space latent representation that employs two   \n47 independent encoding backbones to extract complementary shape latents from distinct latent spaces.   \n48 At the decoding side, a diffusion-based generator is devised to produce high-quality reconstructions by   \n49 considering the shape latents as guidance to stochastically denoise the noisy point clouds. Experiments   \n50 demonstrate that the proposed Diff-PCC achieves state-of-the-art compression performance (e.g.,   \n51 7.711 dB BD-PSNR gains against the latest G-PCC standard at ultra-low bitrate) while attaining   \n52 superior subjective quality. ", "page_idx": 1}, {"type": "text", "text": "53 1.3 Contribution ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "54 Main contributions of this paper are summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "55 \u2022 We propose Diff-PCC, a novel diffusion-based lossy point cloud compression framework.   \n56 To the best of our knowledge, this study presents the first exploration of diffusion-based   \n57 neural compression for 3D point clouds.   \n58 \u2022 We introduce a dual-space latent representation to enhance the representation ability of the   \n59 conventional Gaussian priors in VAEs, enabling the Diff-PCC to extract expressive shape   \n60 latents and facilitate the following diffusion-based decoding process.   \n61 \u2022 We devise an effective diffusion-based generator to produce high-quality noises by consider  \n62 ing the shape latents as guidance to stochastically denoise the noisy point clouds. ", "page_idx": 1}, {"type": "text", "text": "63 2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "64 2.1 Point Cloud Compression ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "65 Classic point cloud compression standards, such as G-PCC, employ octree[29] to compress point   \n66 cloud geometric information. In recent years, inspired by deep learning methods in point cloud   \n67 analysis[26, 27] and image compression[1, 2, 22], researchers have turned their attention to learning  \n68 based point cloud compression. Currently, point cloud compression methods can be primarily divided   \n69 into two branches: voxel-based and point-based approaches. Voxel-based methods further branch into   \n70 sparse convolution[36, 37, 38, 49, 51, 52] and octree[9, 24, 31]. Among them, sparse convolution de  \n71 rives from 2D-pixel representations but optimizes for voxel sparsity. On the other hand, octree-based   \n72 methods, utilize tree structures to eliminate redundant voxels, representing only the occupied ones.   \n73 Point-based methods[11, 50, 45, 46] are draw inspiration from PointNet [26], utilizing symmetric   \n74 operators (max pooling, average pooling, attention pooling) to handle permutation-invariant point   \n75 clouds and capture geometric shapes. For compression, different quantization operations categorize   \n76 point cloud compression into lossy and lossless types. In this paper, we focus on lossy compression   \n77 to achieve higher compression ratios by sacrificing some precision in the original data. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "78 2.2 Diffusion Models for Point Cloud ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "79 Recently, diffusion models have ignited the image generation field[58, 17, 32], inspiring researchers   \n80 to explore their potential in point cloud applications. DPM[20] pioneered the introduction of diffusion   \n81 models in this domain. Starting from DPM, PVD[57] combines the strengths of point cloud and   \n82 voxel representations, establishing a baseline based on PVCNN. LION[47] employs two diffusion   \n83 models to separately learn shape representations in latent space and point representations in 3D   \n84 space. Dit-3D[23] innovates by integrating transformers into DDPM, directly operating on voxelized   \n85 point clouds during the denoising process. PDR[21] employs diffusion model twice during the   \n86 process of generating coarse point clouds and refined point clouds. Point\u00b7E[] utilizes three diffusion   \n87 models for the following processes: text-to-image generation, image-to-point cloud generation, and   \n88 point cloud upsampling. PointInfinity[13] utilizes cross-attention mechanism to decouple fixed-size   \n89 shape latent and variable-size position latent, enabling the model to train on low-resolution point   \n90 clouds while generating high-resolution point clouds during inference. DiffComplete[4] enhances   \n91 control over the denoising process by incorporating ControlNet[53], achieving new state-of-the-art   \n92 performances. These advancements demonstrate the promise of DMs in point cloud generation tasks,   \n93 which motivates our exploring its applicability in point cloud compression. Our research objective is   \n94 to explore the effective utilization of diffusion models for point cloud compression while preserving   \n95 its critical structural features. ", "page_idx": 2}, {"type": "text", "text": "96 3 Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "97 Figure 1 illustrates the pipeline of the proposed Diff-PCC, which can also represent the general work  \n98 flow of diffusion-based neural compression. A concise review for Denoising Diffusion Probabilistic   \n99 Models (DDPMs) and Neural Network (NN) based point cloud compression is first provided in   \n100 Sec. 3.1; The proposed Diff-PCC is detailed in Sec. 3.2. ", "page_idx": 2}, {"type": "text", "text": "101 3.1 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "102 Denoising Diffusion Probabilistic Models (DDPMs) comprise two Markov chains of length T:   \n103 diffusion process and denoising process. Diffusion process adds noise to clean data $x_{0}$ , resulting in   \n104 a series of noisy samples $\\{x_{1},x_{2}...x_{T}\\}$ . When $T$ is large enough, $x_{T}\\sim N(0,I)\\,$ . The denoising   \n105 process is the reverse process, gradually removing the noise added during the diffusion process. We   \n106 formulate them as follows: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle q(\\pmb{x}_{1},\\cdots,\\pmb{x}_{T}|\\pmb{x}_{0})=\\prod_{t=1}^{T}q(\\pmb{x}_{t}|\\pmb{x}_{t-1}),\\mathrm{~where~}q(\\pmb{x}_{t}|\\pmb{x}_{t-1})=\\mathcal{N}(\\pmb{x}_{t};\\sqrt{1-\\beta_{t}}\\pmb{x}_{t-1},\\beta_{t}{I})}\\\\ {\\displaystyle p_{\\theta}(\\pmb{x}_{0},\\cdots,\\pmb{x}_{T-1}|\\pmb{x}_{T})=\\prod_{t=1}^{T}p_{\\theta}(\\pmb{x}_{t-1}|\\pmb{x}_{t}),\\mathrm{~where~}p_{\\theta}(\\pmb{x}_{t-1}|\\pmb{x}_{t})=\\mathcal{N}(\\pmb{x}_{t-1};\\mu_{\\theta}(\\pmb{x}_{t},t),\\sigma_{t}^{2}{I})}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "107 where $\\beta$ is a hyperparameter representing noise level. $t\\sim\\mathrm{Unif}\\{1,\\ldots,T\\}$ represents time step. Via   \n108 reparameterization trick, we can sample from $q(x_{t}|x_{t-1})$ and $p_{\\theta}(\\mathbf{x}_{t-1}|x_{t})$ as following: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle x_{t}=\\sqrt{1-\\beta_{t}}x_{t-1}+\\sqrt{\\beta_{t}}\\epsilon}}\\\\ {{\\displaystyle x_{t-1}=\\mu_{\\theta}(x_{t},t)+\\sigma_{t}\\epsilon=\\frac{1}{\\sqrt{\\alpha_{t}}}\\left(x_{t}-\\frac{\\beta_{t}}{\\sqrt{1-\\bar{\\alpha}_{t}}}\\epsilon_{\\theta}(x_{t},t)\\right)+\\sqrt{\\frac{1-\\bar{\\alpha}_{t-1}}{1-\\bar{\\alpha}_{t}}\\beta_{t}}\\epsilon}}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "image", "img_path": "1CssOqRYyz/tmp/bbb65a0c7e793b718e9e1893a2a445680b690ffc6822c8a02cd1e97cdbf148e6.jpg", "img_caption": ["Figure 2: Detailed Structure of the Utilized Compressor and Generator. $y_{l}$ and $y_{h}$ refer to the low-frequency shape latent and high-frequency detail latent, respectively; $z$ means hyperprior latent; $Q$ refers to the quantization; AE and AD represents the arithmetic encoding and decoding. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "109 where $\\begin{array}{r}{\\alpha_{t}\\,=\\,1\\,-\\,\\beta_{t},\\bar{\\alpha}_{t}\\,=\\,\\prod_{i=1}^{t}\\alpha_{i}}\\end{array}$ , $\\epsilon$ denotes random noise sampled from $N(0,I)$ . Note that   \n110 $\\epsilon_{\\theta}(\\boldsymbol{x}_{t},t)$ is a neural netwo\u221a rk used to\u221a predict noise during the denoising process, and $\\pmb{x}_{t}$ can be   \n111 directly sampled via $x_{t}=\\sqrt{\\bar{\\alpha}_{t}}x_{0}+\\sqrt{1-\\bar{\\alpha}_{t}}\\epsilon$ .   \n112 DDPMs train the reverse process by optimizing the model parameters $\\theta$ through noise distortion. The   \n113 loss function $L(\\theta,x_{0})$ is defined as the expected squared difference between the predicted noise and   \n114 the actual noise, with the mathematical expression as follows: ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\nL(\\theta,\\mathbf{x}_{0})=E_{t,\\epsilon}||\\epsilon-\\epsilon_{\\theta}(\\mathbf{x}_{t},t)||^{2}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "115 3.2 DIFF-PCC ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "116 3.2.1 Overview ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "117 As shown in Fig. 2, two key components, i.e., compressor and generator, are respectively utilized   \n118 in the diffusion process and denoising process. In Diff-PCC, the diffusion process is identified as   \n119 the encoding, in which a compressor extracts latents from the point cloud and compresses latents   \n120 into bitstreams; at the decoding side, the generator accepts the latents as a condition and gradually   \n121 restoring point cloud shape from noisy samples. ", "page_idx": 3}, {"type": "text", "text": "122 3.2.2 Dual-Space Latent Encoding ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "123 Several research have demonstrated that a simplistic Gaussian distribution in the latent space may   \n124 prove inadequate to capture the complex visual signals [56, 3, 6, 10]. Although previous works have   \n125 proposed to solve these problems using different technologies such as non-gaussian prior [15] or   \n126 coupling between the prior and the data distribution [10], these techniques may not be able to directly   \n127 employed on neural compression tasks.   \n128 In this paper, a simple yet effective compressor is introduced, which composed of two independent   \n129 encoding backbones to extract expressive shape latents from distinct latent spaces. Motivated by   \n130 PointPN [55], which excels in capturing high-frequency 3D point cloud structures characterized by   \n131 sharp variations, we design a dual-space latent encoding approach that utilizes PointNet to extract   \n132 low-frequency shape latent and leverages PointPN to characterize complementary latent from high   \n133 frequency domain. Let $x$ be the original input point cloud, we formulate the above process as: ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\{y_{l},y_{h}\\}=\\{E_{l}(x),E_{h}(x)\\}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "134 where $y_{l}\\in\\mathbb{R}^{1\\times C}$ and $y_{h}\\in\\mathbb{R}^{S\\times C}$ represent the low-frequency and high-frequency latent features,   \n135 respectively; $E_{l}$ and $E_{h}$ refer to the PointNet and PointPN backbones, respectively. Next, the   \n136 quantization process $Q$ is applied on the obtained features $\\bar{y}_{l}$ and $\\bar{y}_{h}$ , i.e., ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\{\\bar{y}_{l},\\bar{y}_{h}\\}=\\{Q(y_{l}),Q(y_{h})\\}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "137 where function $Q$ refers to the operation of adding uniform noise during training [1] and the rounding   \n138 operation during test.   \n139 Then, fully factorized density model [1] and the hyperprior density model [2] are employed to fti the   \n140 distribution of quantized features $\\bar{y}\\bar{\\iota}$ and $\\bar{y_{h}}$ , respectively. Particularly, the hyperprior density model   \n141 $p_{\\varphi}(\\bar{y}_{h})$ can be described as: ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\np_{\\varphi}(\\bar{y}_{h})=\\left(N(\\mu,\\sigma^{2})*\\mathcal{U}\\left(-\\frac{1}{2},\\frac{1}{2}\\right)\\right)(\\bar{y}_{h})\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "142 where $\\mathcal{U}\\left(-\\frac{1}{2},\\frac{1}{2}\\right)$ refers to the uniform noise ranging from $-\\,{\\frac{1}{2}}$ to $\\frac{1}{2}$ ; $N(\\mu,\\sigma^{2})$ refers to the normal   \n143 distribution with expectation $\\mu$ and standard deviation $\\sigma$ , which can be further estimated by a   \n144 hyperprior encoder $E_{h y p e r}$ and decoder $D_{h y p e r}$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n(\\mu,\\sigma^{2})=D_{h y p e r}(\\bar{z})=D_{h y p e r}(Q(z))=D_{h y p e r}(Q(E_{h y p e r}(y_{h})))\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "145 In this way, a triplet containing quantized low-frequency feature $\\bar{y}_{l}$ , quantized high-frequency feature   \n146 $\\bar{y}_{h}$ , and quantized hyperprior $\\bar{z}$ will be compressed into three separate streams. Let $p(\\cdot)$ and $p_{(\\dots)}(\\cdot)$   \n147 respectively represents the actual distribution and estimated distribution of latent features, then the   \n148 bitrate $\\mathcal{R}$ can be estimated as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{R}=\\mathbb{E}_{\\bar{y}_{l}\\sim p(\\bar{y}_{l})}\\left[-\\log_{2}p_{\\theta}(\\bar{y}_{l})\\right]+\\mathbb{E}_{\\bar{y}_{h}\\sim p(\\bar{y}_{h})}\\left[-\\log_{2}p_{\\varphi}(\\bar{y}_{h})\\right]+\\mathbb{E}_{\\bar{z}\\sim p(\\bar{z})}\\left[-\\log_{2}p_{\\phi}(\\bar{z})\\right]\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "149 3.2.3 Diffusion-based Generator ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "150 The generator takes noisy point cloud $x_{t}$ at time $t$ and necessary conditional information $C$ as input.   \n151 We hope generator to learn positional distribution $F$ of $x_{t}$ and fully integrate $F$ with $C$ to predict   \n152 noise $\\epsilon_{t}$ at time $t$ . In this paper, we consider all information that could potentially guide the generator   \n153 as conditional information, including time $t$ , class label $l$ , noise coefficient $\\beta_{t}$ , and decoded latent   \n154 features $\\langle\\bar{y}_{l}$ and $\\bar{y}_{h}$ ).   \n155 DiffComplete [4] uses ControlNet [54] to achieve refined noise generation. However, the denoiser of   \n156 DiffComplete is a 3D-Unet, adapted from its 2D version [16]. This structure is not suitable for our   \n157 method, because we directly deal with points, instead of voxels. We embraced this idea and specially   \n158 designed a hierarchical feature fusion mechanism to adapt to our method. Note that 3D-Unet can   \n159 directly downsample features $F$ through 3D convolution with a stride greater than one. It is very   \n160 complex for point-based methods to achieve equivalent processing. Therefore, we did not replicate   \n161 the same structure as DiffComplete does, but directly used AdaLN to inject conditional information,   \n162 formulated as: ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\nA d a L N(F_{i n},C)=N o r m(F_{i n})\\odot L i n e a r(C)+L i n e a r(C)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "163 where $F_{i n}$ denotes the original features in the Generator and $C$ denotes the condition information. ", "page_idx": 4}, {"type": "text", "text": "164 Now we detail the structure: First, we need to exact the shape latent of noise point cloud $x_{t}$ and we   \n165 choose PointNet for structural consistency. However, in the early stages of the denoising process,   \n166 $x_{t}$ lacks a regular surface shape for the generator to learn. Therefore, we adopt the suggestion from   \n167 PDR [23], adding positional encoding to each noise point so that the generator can understand the   \n168 absolute position of each point in 3D space. Then we inject shape latent $\\bar{y}_{l}$ from the compressor via   \n169 ADaLN. We formulate the above process as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{l}{F_{x_{t}}=P o i n t N e t(x_{t})+P E(x_{t})}\\\\ {F_{x t}^{'}=A d a L N(F_{x_{t}},C)}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "170 Next, we need to fuse high-frequency features. We extract the local high-frequency features of $x_{t}$   \n171 using PointPN and add them to $F$ from the previous step, Then we inject the high-frequency features   \n172 from the compressor via AdaLN. We use K-Nearest Neighbor (KNN) operation to partition locally   \n173 and set the number of neighbor points to 8, which allows the generator to learn local details. We   \n174 formulate the above process as: ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\boldsymbol{F}^{'}=\\boldsymbol{P o i n t P N}(\\boldsymbol{x}_{t})+F P S(F_{i n})}\\\\ {F_{o u t}=\\boldsymbol{A d a L N}(\\boldsymbol{F}^{'},\\boldsymbol{C})}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "175 After that, we use the self-attention mechanism to interact with information from different local areas.   \n176 And through a feature up-sampling module, we generate features for $\\mathbf{n}$ points. Finally, we output   \n177 noise through a linear layer. We formulate the above process as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\boldsymbol{F}^{'}=\\boldsymbol{S}\\boldsymbol{A}(F_{i n})}\\\\ &{\\boldsymbol{F}^{'\\prime}=\\boldsymbol{U}\\boldsymbol{P}(\\boldsymbol{F}^{'})}\\\\ &{\\epsilon_{t}=L i n e a r(\\boldsymbol{F}^{'\\prime})}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "178 3.2.4 Training Objective ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "179 We follow the conventional rate-distortion trade-off as our loss function as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{{\\mathcal{L}}={\\mathcal{D}}+\\lambda{\\mathcal{R}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "180 where $\\mathcal{D}$ refers to the evaluated distortion; $\\mathcal{R}$ represents bitrate as shown in Eq. 10; $\\lambda$ serves as the   \n181 balance the distortion and bitrate. Specifically, a combined form of distortion $\\mathcal{D}$ is used in this paper,   \n182 which considers both intermediate noises $(\\epsilon,\\bar{\\epsilon})$ and global shapes $(x_{0},{\\bar{x}}_{0})$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{D}=\\mathcal{D}_{M S E}(\\epsilon,\\bar{\\epsilon})+\\gamma\\mathcal{D}_{C D}(x_{0},\\bar{x}_{0})\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "183 where $\\mathcal{D}_{M S E}$ denotes the Mean Squared Error (MSE) distance; $\\mathcal{D}_{C D}$ refers to the Chamfer Distance;   \n184 $\\gamma$ means the weighting factor. Here, the overall point cloud shape is additively supervised under the   \n185 Chamfer Distance $\\bar{D_{C D}}(x_{0},\\bar{x}_{0})$ to provide a global optimization. The following function is utilized   \n186 to predict the reconstructed point cloud $\\scriptstyle{\\bar{x}}_{0}$ in practice: ", "page_idx": 5}, {"type": "equation", "text": "$$\nx_{0}=\\frac{1}{\\sqrt{\\bar{\\alpha}_{t}}}\\left(x_{t}-\\sqrt{1-\\bar{\\alpha}_{t}}\\epsilon_{\\theta}\\left(x_{t},t,c\\right)\\right)\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "187 where $\\bar{\\alpha}_{t}$ means the noise level; $x_{t}$ refers to the noisy point cloud at time step t; $\\epsilon_{\\theta}$ denotes the   \n188 predicted noise from the generator; $c$ represent the conditional information we inject into the generator. ", "page_idx": 5}, {"type": "text", "text": "189 4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "190 4.1 Experimental Setup ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "191 Datasets Based on previous work, we used ShapeNet as our training set, sourced from [20]. This   \n192 dataset contains 51,127 point clouds, across 55 categories, which we allocated in an 8:1:1 ratio for   \n193 training, validation, and testing. Each point cloud has 15K points, and following the suggestions from   \n194 [28], we randomly select 2K points from each for training. Additionally, we also used ModelNet10   \n195 and ModelNet40 as our test sets, sourced from [43]. These datasets contain 10 categories and   \n196 40 categories respectively, totaling 10,582 point clouds. During training and testing, we perform   \n197 individual normalization on the shape of each point cloud.   \n198 Baselines & Metric We compare our method with the state-of-the-art non-learning-based method:   \n199 G-PCC, and the latest learning-based methods from the past two years: IPDAE, PCT-PCC, Following   \n200 [45, 46], we use point-to-point PSNR to measure the geometric accuracy and the number of bits per   \n201 point to measure the compression ratio.   \n202 Implementation Our model is implemented using PyTorch [27] and CompressAI [4], trained on the   \n203 NVIDIA 4090X GPU (24GB Memory) for 80,000 steps with a batch size of 48. We utilize the Adam   \n204 optimizer [21] with an initial learning rate of 1e-4 and a decay factor of 0.5 every 30,000 steps, with   \n205 $\\beta_{1}$ set to 0.9 and $\\beta_{2}$ set to 0.999. Since the positional encoding method requires the dimension (dim)   \n206 to be a multiple of 6, we designed the bottleneck layer size to be 288. For diffusion, we employ a   \n207 cosine preset noise parameter, setting the denoising steps T to 200, which is used for both training   \n208 and testing. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "table", "img_path": "1CssOqRYyz/tmp/aefc9ee87609cecc79fe20c5ae6a5c88ba2179cfa02607741d71ff8c0d1e927a.jpg", "table_caption": ["Table 1: Objective comparison using BD-PSNR and BD-Rate metrics. G-PCC serves as the anchor. The best and second-best results are highlighted in bold and underlined, respectively. "], "table_footnote": [], "page_idx": 6}, {"type": "image", "img_path": "1CssOqRYyz/tmp/89d2ee12ef220cdffb6abfcea1ce8611d78f1b88bf6a7e16b2e1b9924a82884a.jpg", "img_caption": ["Figure 3: Rate-distortion curves for performance comparison. From left to right: ShapeNet, ModelNet10, and ModelNet40 dataset. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "209 4.2 Baseline Comparisons ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "210 Objective Quality Comparison Table 1 shows the quantitative indicators using BD-Rate and BD  \n211 PSNR, and Fig. 3 demonstrates the rate-distortion curves of different methods. It can be seen   \n212 that, under identical reconstruction quality conditions, our method achieves superior rate-distortion   \n213 performance, conserving between $56\\%$ to $99\\%$ of the bitstream compared to G-PCC. At the most   \n214 minimal bit rates, point ot point PSNR of our proposed method surpasses that of G-PCC by 7.711 dB.   \n215 Subjective Quality Comparison Fig 4 presents the ground truth and decoded point clouds from   \n216 different methods. We choose three point cloud:airplane, chair ,and mug. to be tested across a   \n217 comparable bits per pixel (bpp) range. The comparative analysis reveals that at the lowest code rate,   \n218 our method preserves the ground truth\u2019s shape information to the greatest extent while simultaneously   \n219 achieving the highest Peak Signal-to-Noise Ratio (PSNR). ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "220 4.3 Ablation Studies ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "221 We conduct ablation studies to examine the impact of key components in the model. Specifically,   \n222 we investigate the effectiveness of low-frequency features, high-frequency features, and the loss   \n223 function designed in Sec. 3.2.4. As shown in Table 2, utilizing solely low-frequency features to   \n224 guide the reconstruction of the diffusion model results in a $20\\%$ reduction in the code rate, along   \n225 with a decrease in the reconstruction quality by 0.397dB. This indicates that high-frequency features   \n226 play an effective role in guiding the model during the reconstruction process. Conversely, discarding   \n227 the low-frequency features, which represent the shape of the point cloud, leads to a reduction in   \n228 the code rate and significantly diminishes the reconstruction quality. Therefore, we argue that the   \n229 loss of the shape variable is not worth it. Lastly, we ascertain the impact of $\\mathcal{D}_{C D}(x_{0},\\bar{x}_{0})$ , and the   \n230 results indicate that this loss marginally increases the bits per point (bpp) while diminishing the   \n231 reconstruction quality. ", "page_idx": 6}, {"type": "image", "img_path": "1CssOqRYyz/tmp/c852c6274cdd12be1aca20c052f83d154211614ee2793a43b116ed45999716ad.jpg", "img_caption": [], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Figure 4: Subjective quality comparison. Example point clouds are selected from the ShapeNet dataset, each with $2\\mathbf{k}$ points. ", "page_idx": 7}, {"type": "table", "img_path": "1CssOqRYyz/tmp/5a5a08ce5d40cac1056e0363b129a4202ced3601721f4a27530fccc37c50a6f6.jpg", "table_caption": ["Table 2: Ablation study of the proposed method. The original Diff-PCC serves as the anchor. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "232 5 Limitations ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "233 Although our method has achieved advanced rate distortion performance and excellent visual re  \n234 construction results, there are several limitations that warrant discussion. Firstly, the encoding and   \n235 decoding time are relatively long, which could potentially be improved by the acceleration techniques   \n236 employed in several explorations [18, 19]. Secondly, the model is currently limited to compressing   \n237 small-scale point clouds, and further research is required to enhance its capability to handle large-scale   \n238 instances. ", "page_idx": 7}, {"type": "text", "text": "239 6 Conclusion ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "240 We propose a diffusion-based point cloud compression method, dubbed Diff-PCC, to leverage the   \n241 expressive power of the diffusion model for generative and aesthetically superior decoding. We   \n242 introduce a dual-space latent representation to enhance the representation ability of the conventional   \n243 Gaussian priors in VAEs, enabling the Diff-PCC to extract expressive shape latents and facilitate   \n244 the following diffusion-based decoding process. At the decoding side, an effective diffusion-based   \n245 generator produces high-quality reconstructions by considering the shape latents as guidance to   \n246 stochastically denoise the noisy point clouds. The proposed method achieves state-of-the-art com  \n247 pression performance while attaining superior subjective quality. Future works may include reducing   \n248 the coding complexity and extending to large-scale point cloud instances. ", "page_idx": 7}, {"type": "text", "text": "249 References ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "250 [1] Johannes Ball\u00e9, Valero Laparra, and Eero P Simoncelli. End-to-end optimized image compression. arXiv   \n251 preprint arXiv:1611.01704, 2016.   \n252 [2] Johannes Ball\u00e9, David Minnen, Saurabh Singh, Sung Jin Hwang, and Nick Johnston. Variational image   \n253 compression with a scale hyperprior. arXiv preprint arXiv:1802.01436, 2018.   \n254 [3] Francesco Paolo Casale, Adrian Dalca, Luca Saglietti, Jennifer Listgarten, and Nicolo Fusi. Gaussian   \n255 process prior variational autoencoders. Advances in neural information processing systems, 31, 2018.   \n256 [4] Ruihang Chu, Enze Xie, Shentong Mo, Zhenguo Li, Matthias Nie\u00dfner, Chi-Wing Fu, and Jiaya Jia.   \n257 Diffcomplete: Diffusion-based generative 3d shape completion. Advances in Neural Information Processing   \n258 Systems, 36, 2024.   \n259 [5] Florinel-Alin Croitoru, Vlad Hondru, Radu Tudor Ionescu, and Mubarak Shah. Diffusion models in vision:   \n260 A survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(9):10850\u201310869, 2023.   \n261 [6] Bin Dai and David Wipf. Diagnosing and enhancing vae models. arXiv preprint arXiv:1903.05789, 2019.   \n262 [7] Kamak Ebadi, Lukas Bernreiter, Harel Biggie, Gavin Catt, Yun Chang, Arghya Chatterjee, Christopher E   \n263 Denniston, Simon-Pierre Desch\u00eanes, Kyle Harlow, Shehryar Khattak, et al. Present and future of slam in   \n264 extreme environments: The darpa subt challenge. IEEE Transactions on Robotics, 2023.   \n265 [8] Lili Fan, Junhao Wang, Yuanmeng Chang, Yuke Li, Yutong Wang, and Dongpu Cao. 4d mmwave radar for   \n266 autonomous driving perception: a comprehensive survey. IEEE Transactions on Intelligent Vehicles, 2024.   \n267 [9] Chunyang Fu, Ge Li, Rui Song, Wei Gao, and Shan Liu. Octattention: Octree-based large-scale contexts   \n268 model for point cloud compression. In Proceedings of the AAAI conference on artificial intelligence,   \n269 volume 36, pages 625\u2013633, 2022.   \n270 [10] Xiaoran Hao and Patrick Shafto. Coupled variational autoencoder. arXiv preprint arXiv:2306.02565, 2023.   \n271 [11] Yun He, Xinlin Ren, Danhang Tang, Yinda Zhang, Xiangyang Xue, and Yanwei Fu. Density-preserving   \n272 deep point cloud compression. In Proceedings of the IEEE/CVF Conference on Computer Vision and   \n273 Pattern Recognition, pages 2333\u20132342, 2022.   \n274 [12] Tianxin Huang, Jiangning Zhang, Jun Chen, Zhonggan Ding, Ying Tai, Zhenyu Zhang, Chengjie Wang,   \n275 and Yong Liu. 3qnet: 3d point cloud geometry quantization compression network. ACM Transactions on   \n276 Graphics (TOG), 41(6):1\u201313, 2022.   \n277 [13] Zixuan Huang, Justin Johnson, Shoubhik Debnath, James M Rehg, and Chao-Yuan Wu. Pointinfinity:   \n278 Resolution-invariant point diffusion models. arXiv preprint arXiv:2404.03566, 2024.   \n279 [14] Yiqi Jin, Ziyu Zhu, Tongda Xu, Yuhuan Lin, and Yan Wang. Ecm-opcc: Efficient context model for   \n280 octree-based point cloud compression. In ICASSP 2024 - 2024 IEEE International Conference on Acoustics,   \n281 Speech and Signal Processing (ICASSP), pages 7985\u20137989, 2024.   \n282 [15] Weonyoung Joo, Wonsung Lee, Sungrae Park, and Il-Chul Moon. Dirichlet variational autoencoder. Pattern   \n283 Recognition, 107:107514, 2020.   \n284 [16] M Krithika Alias AnbuDevi and K Suganthi. Review of semantic segmentation of medical images using   \n285 modified architectures of unet. Diagnostics, 12(12):3064, 2022.   \n286 [17] Jin Sub Lee, Jisun Kim, and Philip M Kim. Score-based generative modeling for de novo protein design.   \n287 Nature Computational Science, 3(5):382\u2013392, 2023.   \n288 [18] Xiuyu Li, Yijiang Liu, Long Lian, Huanrui Yang, Zhen Dong, Daniel Kang, Shanghang Zhang, and   \n289 Kurt Keutzer. Q-diffusion: Quantizing diffusion models. In Proceedings of the IEEE/CVF International   \n290 Conference on Computer Vision, pages 17535\u201317545, 2023.   \n291 [19] Qingguo Liu, Chenyi Zhuang, Pan Gao, and Jie Qin. Cdformer: When degradation prediction embraces   \n292 diffusion model for blind image super-resolution. arXiv preprint arXiv:2405.07648, 2024.   \n293 [20] Shitong Luo and Wei Hu. Diffusion probabilistic models for 3d point cloud generation. In Proceedings of   \n294 the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2021.   \n295 [21] Zhaoyang Lyu, Zhifeng Kong, Xudong Xu, Liang Pan, and Dahua Lin. A conditional point diffusion  \n296 refinement paradigm for 3d point cloud completion. ArXiv, abs/2112.03530, 2021.   \n297 [22] David Minnen, Johannes Ball\u00e9, and George D Toderici. Joint autoregressive and hierarchical priors for   \n298 learned image compression. Advances in neural information processing systems, 31, 2018.   \n299 [23] Shentong Mo, Enze Xie, Ruihang Chu, Lanqing Hong, Matthias Niessner, and Zhenguo Li. Dit-3d:   \n300 Exploring plain diffusion transformers for 3d shape generation. Advances in Neural Information Processing   \n301 Systems, 36, 2024.   \n302 [24] Dat Thanh Nguyen and Andr\u00e9 Kaup. Lossless point cloud geometry and attribute compression using a   \n303 learned conditional probability model. IEEE Transactions on Circuits and Systems for Video Technology,   \n304 2023.   \n305 [25] Francesco Pezone, Osman Musa, Giuseppe Caire, and Sergio Barbarossa. Semantic-preserving image   \n306 coding based on conditional diffusion models. In ICASSP 2024-2024 IEEE International Conference on   \n307 Acoustics, Speech and Signal Processing (ICASSP), pages 13501\u201313505. IEEE, 2024.   \n308 [26] Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas. Pointnet: Deep learning on point sets for 3d   \n309 classification and segmentation. In Proceedings of the IEEE conference on computer vision and pattern   \n310 recognition, pages 652\u2013660, 2017.   \n311 [27] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J Guibas. Pointne $^{++}$ : Deep hierarchical feature   \n312 learning on point sets in a metric space. Advances in neural information processing systems, 30, 2017.   \n313 [28] Guocheng Qian, Yuchen Li, Houwen Peng, Jinjie Mai, Hasan Hammoud, Mohamed Elhoseiny, and Bernard   \n314 Ghanem. Pointnext: Revisiting pointnet $^{++}$ with improved training and scaling strategies.   \n315 [29] Ruwen Schnabel and Reinhard Klein. Octree-based point-cloud compression. PBG@ SIGGRAPH,   \n316 3:111\u2013121, 2006.   \n317 [30] Rui Song, Chunyang Fu, Shan Liu, and Ge Li. Efficient hierarchical entropy model for learned point cloud   \n318 compression. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,   \n319 pages 14368\u201314377, 2023.   \n320 [31] Rui Song, Chunyang Fu, Shan Liu, and Ge Li. Efficient hierarchical entropy model for learned point cloud   \n321 compression. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,   \n322 pages 14368\u201314377, 2023.   \n323 [32] Yu Takagi and Shinji Nishimoto. High-resolution image reconstruction with latent diffusion models from   \n324 human brain activity. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern   \n325 Recognition, pages 14453\u201314463, 2023.   \n326 [33] Lucas Theis, Tim Salimans, Matthew D Hoffman, and Fabian Mentzer. Lossy compression with gaussian   \n327 diffusion. arXiv preprint arXiv:2206.08889, 2022.   \n328 [34] Anwaar Ulhaq, Naveed Akhtar, and Ganna Pogrebna. Efficient diffusion models for vision: A survey.   \n329 arXiv preprint arXiv:2210.09292, 2022.   \n330 [35] Juho-Pekka Virtanen, Sylvie Daniel, Tuomas Turppa, Lingli Zhu, Arttu Julin, Hannu Hyypp\u00e4, and Juha   \n331 Hyypp\u00e4. Interactive dense point clouds in a game engine. ISPRS Journal of Photogrammetry and Remote   \n332 Sensing, 163:375\u2013389, 2020.   \n333 [36] Jianqiang Wang, Dandan Ding, Zhu Li, and Zhan Ma. Multiscale point cloud geometry compression. In   \n334 2021 Data Compression Conference (DCC), pages 73\u201382. IEEE, 2021.   \n335 [37] Jianqiang Wang, Dandan Ding, and Zhan Ma. Lossless point cloud attribute compression using cross-scale,   \n336 cross-group, and cross-color prediction. In 2023 Data Compression Conference (DCC), pages 228\u2013237.   \n337 IEEE, 2023.   \n338 [38] Jianqiang Wang and Zhan Ma. Sparse tensor-based point cloud attribute compression. In 2022 IEEE   \n339 5th International Conference on Multimedia Information Processing and Retrieval (MIPR), pages 59\u201364.   \n340 IEEE, 2022.   \n341 [39] MPEG 3D Graphics WG 7 and Haptics Coding. G-pcc 2nd edition codec description. ISO/IEC JTC 1/SC   \n342 29/WG 7, 2023.   \n343 [40] MPEG 3D Graphics Coding WG 7. V-pcc codec description. ISO/IEC JTC 1/SC 29/WG 7, 2020.   \n344 [41] Yankun Wu, Yuta Nakashima, and Noa Garcia. Not only generative art: Stable diffusion for content-style   \n345 disentanglement in art analysis. In Proceedings of the 2023 ACM International conference on multimedia   \n346 retrieval, pages 199\u2013208, 2023.   \n347 [42] Ruixiang Xue, Jiaxin Li, Tong Chen, Dandan Ding, Xun Cao, and Zhan Ma. Neri: Implicit neural   \n348 representation of lidar point cloud using range image sequence. In ICASSP 2024-2024 IEEE International   \n349 Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 8020\u20138024. IEEE, 2024.   \n350 [43] Guandao Yang, Xun Huang, Zekun Hao, Ming-Yu Liu, Serge Belongie, and Bharath Hariharan. Pointflow:   \n351 3d point cloud generation with continuous normalizing flows. arXiv, 2019.   \n352 [44] Ruihan Yang and Stephan Mandt. Lossy image compression with conditional diffusion models. Advances   \n353 in Neural Information Processing Systems, 36, 2024.   \n354 [45] Kang You, Pan Gao, and Qing Li. Ipdae: Improved patch-based deep autoencoder for lossy point cloud   \n355 geometry compression. In Proceedings of the 1st International Workshop on Advances in Point Cloud   \n356 Compression, Processing and Analysis, pages 1\u201310, 2022.   \n357 [46] Kang You, Kai Liu, Li Yu, Pan Gao, and Dandan Ding. Pointsoup: High-performance and ex  \n358 tremely low-decoding-latency learned geometry codec for large-scale point cloud scenes. arXiv preprint   \n359 arXiv:2404.13550, 2024.   \n360 [47] Xiaohui Zeng, Arash Vahdat, Francis Williams, Zan Gojcic, Or Litany, Sanja Fidler, and Karsten Kreis.   \n361 Lion: Latent point diffusion models for 3d shape generation. In Advances in Neural Information Processing   \n362 Systems (NeurIPS), 2022.   \n363 [48] Chenshuang Zhang, Chaoning Zhang, Mengchun Zhang, and In So Kweon. Text-to-image diffusion model   \n364 in generative ai: A survey. arXiv preprint arXiv:2303.07909, 2023.   \n365 [49] Junteng Zhang, Tong Chen, Dandan Ding, and Zhan Ma. Yoga: Yet another geometry-based point cloud   \n366 compressor. In Proceedings of the 31st ACM International Conference on Multimedia, pages 9070\u20139081,   \n367 2023.   \n368 [50] Junteng Zhang, Gexin Liu, Dandan Ding, and Zhan Ma. Transformer and upsampling-based point cloud   \n369 compression. In Proceedings of the 1st International Workshop on Advances in Point Cloud Compression,   \n370 Processing and Analysis, pages 33\u201339, 2022.   \n371 [51] Junteng Zhang, Jianqiang Wang, Dandan Ding, and Zhan Ma. Scalable point cloud attribute compression.   \n372 IEEE Transactions on Multimedia, 2023.   \n373 [52] Junzhe Zhang, Tong Chen, Dandan Ding, and Zhan Ma. G-pcc $^{++}$ : Enhanced geometry-based point cloud   \n374 compression. In Proceedings of the 31st ACM International Conference on Multimedia, pages 1352\u20131363,   \n375 2023.   \n376 [53] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion   \n377 models.   \n378 [54] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion   \n379 models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 3836\u20133847,   \n380 2023.   \n381 [55] Renrui Zhang, Liuhui Wang, Ziyu Guo, Yali Wang, Peng Gao, Hongsheng Li, and Jianbo Shi. Parameter   \n382 is not all you need: Starting from non-parametric networks for 3d point cloud analysis. arXiv preprint   \n383 arXiv:2303.08134, 2023.   \n384 [56] Shengjia Zhao, Jiaming Song, and Stefano Ermon. Towards deeper understanding of variational autoen  \n385 coding models. arXiv preprint arXiv:1702.08658, 2017.   \n386 [57] Linqi Zhou, Yilun Du, and Jiajun Wu. 3d shape generation and completion through point-voxel diffusion.   \n387 In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 5826\u20135835,   \n388 October 2021.   \n389 [58] Yuanzhi Zhu, Kai Zhang, Jingyun Liang, Jiezhang Cao, Bihan Wen, Radu Timofte, and Luc Van Gool.   \n390 Denoising diffusion models for plug-and-play image restoration. In Proceedings of the IEEE/CVF   \n391 Conference on Computer Vision and Pattern Recognition, pages 1219\u20131229, 2023. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "392 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "393 1. Claims   \n394 Question: Do the main claims made in the abstract and introduction accurately reflect the   \n395 paper\u2019s contributions and scope?   \n396 Answer: [Yes]   \n397 Justification: Claims are clearly stated in abstract and introduction (Sec. 1). The experimental   \n398 results (Sec. 4) match with these claims and reflect how much the results can be expected to   \n399 generalize to other settings.   \n400 Guidelines:   \n401 \u2022 The answer NA means that the abstract and introduction do not include the claims   \n402 made in the paper.   \n403 \u2022 The abstract and/or introduction should clearly state the claims made, including the   \n404 contributions made in the paper and important assumptions and limitations. A No or   \n405 NA answer to this question will not be perceived well by the reviewers.   \n406 \u2022 The claims made should match theoretical and experimental results, and reflect how   \n407 much the results can be expected to generalize to other settings.   \n408 \u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals   \n409 are not attained by the paper.   \n410 2. Limitations   \n411 Question: Does the paper discuss the limitations of the work performed by the authors?   \n412 Answer: [Yes]   \n413 Justification: Limitations are discussed in Sec. 5.   \n414 Guidelines:   \n415 \u2022 The answer NA means that the paper has no limitation while the answer No means that   \n416 the paper has limitations, but those are not discussed in the paper.   \n417 \u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n418 \u2022 The paper should point out any strong assumptions and how robust the results are to   \n419 violations of these assumptions (e.g., independence assumptions, noiseless settings,   \n420 model well-specification, asymptotic approximations only holding locally). The authors   \n421 should reflect on how these assumptions might be violated in practice and what the   \n422 implications would be.   \n423 \u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was   \n424 only tested on a few datasets or with a few runs. In general, empirical results often   \n425 depend on implicit assumptions, which should be articulated.   \n426 \u2022 The authors should reflect on the factors that influence the performance of the approach.   \n427 For example, a facial recognition algorithm may perform poorly when image resolution   \n428 is low or images are taken in low lighting. Or a speech-to-text system might not be   \n429 used reliably to provide closed captions for online lectures because it fails to handle   \n430 technical jargon.   \n431 \u2022 The authors should discuss the computational efficiency of the proposed algorithms   \n432 and how they scale with dataset size.   \n433 \u2022 If applicable, the authors should discuss possible limitations of their approach to   \n434 address problems of privacy and fairness.   \n435 \u2022 While the authors might fear that complete honesty about limitations might be used by   \n436 reviewers as grounds for rejection, a worse outcome might be that reviewers discover   \n437 limitations that aren\u2019t acknowledged in the paper. The authors should use their best   \n438 judgment and recognize that individual actions in favor of transparency play an impor  \n439 tant role in developing norms that preserve the integrity of the community. Reviewers   \n440 will be specifically instructed to not penalize honesty concerning limitations.   \n441 3. Theory Assumptions and Proofs   \n442 Question: For each theoretical result, does the paper provide the full set of assumptions and   \n443 a complete (and correct) proof?   \n445 Justification: This paper does not include theoretical results.   \n446 Guidelines:   \n47 \u2022 The answer NA means that the paper does not include theoretical results.   \n48 \u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross  \n49 referenced.   \n50 \u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n51 \u2022 The proofs can either appear in the main paper or the supplemental material, but if   \n52 they appear in the supplemental material, the authors are encouraged to provide a short   \n53 proof sketch to provide intuition.   \n54 \u2022 Inversely, any informal proof provided in the core of the paper should be complemented   \n55 by formal proofs provided in appendix or supplemental material.   \n56 \u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "457 4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "458 Question: Does the paper fully disclose all the information needed to reproduce the main ex  \n459 perimental results of the paper to the extent that it affects the main claims and/or conclusions   \n460 of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 12}, {"type": "text", "text": "Justification: The proposed architecture is fully described in Sec. 3, detailed instructions for replication is provided in the experimental setup section (Sec. 4.1). ", "page_idx": 12}, {"type": "text", "text": "Guidelines: ", "page_idx": 12}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 12}, {"type": "text", "text": "496 5. Open access to data and code ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "497 Question: Does the paper provide open access to the data and code, with sufficient instruc  \n498 tions to faithfully reproduce the main experimental results, as described in supplemental   \n499 material?   \n500 Answer: [No]   \n501 Justification: Source code will be made publicly available once paper is accepted.   \n502 Guidelines:   \n503 \u2022 The answer NA means that paper does not include experiments requiring code.   \n504 \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/   \n505 public/guides/CodeSubmissionPolicy) for more details.   \n506 \u2022 While we encourage the release of code and data, we understand that this might not be   \n507 possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not   \n508 including code, unless this is central to the contribution (e.g., for a new open-source   \n509 benchmark).   \n510 \u2022 The instructions should contain the exact command and environment needed to run to   \n511 reproduce the results. See the NeurIPS code and data submission guidelines (https:   \n512 //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n513 \u2022 The authors should provide instructions on data access and preparation, including how   \n514 to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n515 \u2022 The authors should provide scripts to reproduce all experimental results for the new   \n516 proposed method and baselines. If only a subset of experiments are reproducible, they   \n517 should state which ones are omitted from the script and why.   \n518 \u2022 At submission time, to preserve anonymity, the authors should release anonymized   \n519 versions (if applicable).   \n520 \u2022 Providing as much information as possible in supplemental material (appended to the   \n521 paper) is recommended, but including URLs to data and code is permitted.   \n522 6. Experimental Setting/Details   \n523 Question: Does the paper specify all the training and test details (e.g., data splits, hyper  \n524 parameters, how they were chosen, type of optimizer, etc.) necessary to understand the   \n525 results?   \n526 Answer: [Yes]   \n527 Justification: Experimental setting and details are fully disclosed in Sec. 4.   \n528 Guidelines:   \n529 \u2022 The answer NA means that the paper does not include experiments.   \n530 \u2022 The experimental setting should be presented in the core of the paper to a level of detail   \n531 that is necessary to appreciate the results and make sense of them.   \n532 \u2022 The full details can be provided either with the code, in appendix, or as supplemental   \n533 material.   \n534 7. Experiment Statistical Significance   \n535 Question: Does the paper report error bars suitably and correctly defined or other appropriate   \n536 information about the statistical significance of the experiments?   \n537 Answer: [No]   \n538 Justification: Error bars are not reported due to the specificity of the compression task. The   \n539 rate-distortion curve (Fig. 3) and Bjontegaard metric (Tab. 1) could be convincing enough.   \n540 Guidelines:   \n541 \u2022 The answer NA means that the paper does not include experiments.   \n542 \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confi  \n543 dence intervals, or statistical significance tests, at least for the experiments that support   \n544 the main claims of the paper.   \n545 \u2022 The factors of variability that the error bars are capturing should be clearly stated (for   \n546 example, train/test split, initialization, random drawing of some parameter, or overall   \n547 run with given experimental conditions).   \n548 \u2022 The method for calculating the error bars should be explained (closed form formula,   \n549 call to a library function, bootstrap, etc.)   \n550 \u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n551 \u2022 It should be clear whether the error bar is the standard deviation or the standard error   \n552 of the mean.   \n553 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should   \n554 preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis   \n555 of Normality of errors is not verified.   \n556 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or   \n557 figures symmetric error bars that would yield results that are out of range (e.g. negative   \n558 error rates).   \n559 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how   \n560 they were calculated and reference the corresponding figures or tables in the text.   \n561 8. Experiments Compute Resources   \n562 Question: For each experiment, does the paper provide sufficient information on the com  \n563 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n564 the experiments?   \n565 Answer: [Yes]   \n566 Justification: Sufficient information on the computer resources is disclosed in the experiment   \n567 setting (Sec. 4.1).   \n568 Guidelines:   \n569 \u2022 The answer NA means that the paper does not include experiments.   \n570 \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,   \n571 or cloud provider, including relevant memory and storage.   \n572 \u2022 The paper should provide the amount of compute required for each of the individual   \n573 experimental runs as well as estimate the total compute.   \n574 \u2022 The paper should disclose whether the full research project required more compute   \n575 than the experiments reported in the paper (e.g., preliminary or failed experiments that   \n576 didn\u2019t make it into the paper).   \n577 9. Code Of Ethics   \n578 Question: Does the research conducted in the paper conform, in every respect, with the   \n579 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n580 Answer: [Yes]   \n581 Justification: This research does not involve human subjects or participants. This paper   \n582 conform with the Code of Ethics in every respect.   \n583 Guidelines:   \n584 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n585 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n586 deviation from the Code of Ethics.   \n587 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid  \n588 eration due to laws or regulations in their jurisdiction).   \n589 10. Broader Impacts   \n590 Question: Does the paper discuss both potential positive societal impacts and negative   \n591 societal impacts of the work performed?   \n592 Answer: [NA]   \n593 Justification: There is no societal impact of the work. The proposed method is limited to   \n594 compression and reconstruction and cannot be used to generate deepfakes or disinformation.   \n595 Guidelines:   \n596 \u2022 The answer NA means that there is no societal impact of the work performed.   \n597 \u2022 If the authors answer NA or No, they should explain why their work has no societal   \n598 impact or why the paper does not address societal impact.   \n599 \u2022 Examples of negative societal impacts include potential malicious or unintended uses   \n600 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n601 (e.g., deployment of technologies that could make decisions that unfairly impact specific   \n602 groups), privacy considerations, and security considerations.   \n603 \u2022 The conference expects that many papers will be foundational research and not tied   \n604 to particular applications, let alone deployments. However, if there is a direct path to   \n605 any negative applications, the authors should point it out. For example, it is legitimate   \n606 to point out that an improvement in the quality of generative models could be used to   \n607 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n608 that a generic algorithm for optimizing neural networks could enable people to train   \n609 models that generate Deepfakes faster.   \n610 \u2022 The authors should consider possible harms that could arise when the technology is   \n611 being used as intended and functioning correctly, harms that could arise when the   \n612 technology is being used as intended but gives incorrect results, and harms following   \n613 from (intentional or unintentional) misuse of the technology.   \n614 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n615 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n616 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n617 feedback over time, improving the efficiency and accessibility of ML).   \n618 11. Safeguards   \n619 Question: Does the paper describe safeguards that have been put in place for responsible   \n620 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n621 image generators, or scraped datasets)?   \n622 Answer: [NA]   \n623 Justification: This paper poses no safeguard risks.   \n624 Guidelines:   \n625 \u2022 The answer NA means that the paper poses no such risks.   \n626 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n627 necessary safeguards to allow for controlled use of the model, for example by requiring   \n628 that users adhere to usage guidelines or restrictions to access the model or implementing   \n629 safety filters.   \n630 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n631 should describe how they avoided releasing unsafe images.   \n632 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n633 not require this, but we encourage authors to take this into account and make a best   \n634 faith effort.   \n635 12. Licenses for existing assets   \n636 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n637 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n638 properly respected?   \n639 Answer: [Yes]   \n640 Justification: This paper follows the license of the datasets used. Original papers are properly   \n641 cited.   \n642 Guidelines:   \n643 \u2022 The answer NA means that the paper does not use existing assets.   \n644 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n645 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n646 URL.   \n647 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n648 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n649 service of that source should be provided.   \n650 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n651 package should be provided. For popular datasets, paperswithcode.com/datasets   \n652 has curated licenses for some datasets. Their licensing guide can help determine the   \n653 license of a dataset.   \n654 \u2022 For existing datasets that are re-packaged, both the original license and the license of   \n655 the derived asset (if it has changed) should be provided.   \n656 \u2022 If this information is not available online, the authors are encouraged to reach out to   \n657 the asset\u2019s creators.   \n658 13. New Assets   \n659 Question: Are new assets introduced in the paper well documented and is the documentation   \n660 provided alongside the assets?   \n661 Answer: [NA]   \n662 Justification: This paper does not release new assets.   \n663 Guidelines:   \n664 \u2022 The answer NA means that the paper does not release new assets.   \n665 \u2022 Researchers should communicate the details of the dataset/code/model as part of their   \n666 submissions via structured templates. This includes details about training, license,   \n667 limitations, etc.   \n668 \u2022 The paper should discuss whether and how consent was obtained from people whose   \n669 asset is used.   \n670 \u2022 At submission time, remember to anonymize your assets (if applicable). You can either   \n671 create an anonymized URL or include an anonymized zip file.   \n672 14. Crowdsourcing and Research with Human Subjects   \n673 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n674 include the full text of instructions given to participants and screenshots, if applicable, as   \n675 well as details about compensation (if any)?   \n676 Answer: [NA]   \n677 Justification: This paper does not involve crowdsourcing nor research with human subjects.   \n678 Guidelines:   \n679 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n680 human subjects.   \n681 \u2022 Including this information in the supplemental material is fine, but if the main contribu  \n682 tion of the paper involves human subjects, then as much detail as possible should be   \n683 included in the main paper.   \n684 \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation,   \n685 or other labor should be paid at least the minimum wage in the country of the data   \n686 collector.   \n687 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human   \n688 Subjects   \n689 Question: Does the paper describe potential risks incurred by study participants, whether   \n690 such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)   \n691 approvals (or an equivalent approval/review based on the requirements of your country or   \n692 institution) were obtained?   \n693 Answer: [NA]   \n694 Justification: This paper does not involve crowdsourcing nor research with human subjects.   \n695 Guidelines:   \n696 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n697 human subjects.   \n698 \u2022 Depending on the country in which research is conducted, IRB approval (or equivalent)   \n699 may be required for any human subjects research. If you obtained IRB approval, you   \n700 should clearly state this in the paper. ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 17}]