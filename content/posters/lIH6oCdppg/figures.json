[{"figure_path": "lIH6oCdppg/figures/figures_6_1.jpg", "caption": "Figure 1: Long-term behavior of tokens in the case of N = 2, d = 2. Without LayerNorm (left), both tokens collapse to the same point in R2; whereas with LayerNorm (right), such a collapse would not necessarily happen and token representations can maintain full rank in the long term (first token converges either to (0, 1) or (0, -1). Assuming convergence to (0, 1) for the first token, the second token converges to B, if it is initially located within the red segment).", "description": "This figure illustrates the long-term behavior of two tokens (N=2) in a two-dimensional space (d=2) under self-attention dynamics, with and without Layer Normalization (LayerNorm). The left panel shows that without LayerNorm, both tokens collapse to the same point, indicating rank collapse. In contrast, the right panel demonstrates that with LayerNorm, the tokens can maintain full rank and converge to different points.  The behavior with LayerNorm depends on the initial positions of the tokens; the second token will converge to either point A or point B depending on its initial location in the plane.", "section": "4.3.1 Illustrative Counterexample"}, {"figure_path": "lIH6oCdppg/figures/figures_8_1.jpg", "caption": "Figure 2. Evolution of \u03bc(X(t)) (in log-log scale) as the number of layers increases. Rank collapse happens exponentially for pure attention, despite different attention masks having different convergence rates. However, as soon as we solely add in LayerNorm, \u03bc(X(t)) no longer converge to zero in randomly initialized models; in pretrained models, LayerNorm helps prevent the issue together with other components and stabilize the representations.", "description": "This figure displays the evolution of token similarity (\u03bc(X(t))) over an increasing number of layers for different transformer architectures.  It illustrates the effects of attention masks (complete, causal, sliding window, uni-directional sliding window) and LayerNorm on rank collapse. The results show that without LayerNorm, rank collapse happens exponentially regardless of the attention mask, although local attention masks slow the rate.  With LayerNorm, however,  the token similarity stabilizes and does not converge to zero, especially in pretrained models where LayerNorm works synergistically with other components to prevent collapse.", "section": "Main Results: Attention with Masking and LayerNorm"}, {"figure_path": "lIH6oCdppg/figures/figures_8_2.jpg", "caption": "Figure 3. Evolution of \u03bc(X(t)) (in log-log scale) as the number of layers increases. Smaller temperature terms alleviate the rate of rank collapse, and effect is more significant with global attention than with sparser masked attention, and more in shallower layers than deeper layers.", "description": "This figure displays the evolution of the token similarity measure \u03bc(X) as the number of layers increases for different attention masks and temperature terms (dQK).  The results show that smaller temperature terms slow down the rate of rank collapse, especially with global attention masks and in earlier layers. This suggests a complex interplay between masking and temperature in controlling the rank collapse phenomenon.", "section": "Main Results: Attention with Masking and LayerNorm"}, {"figure_path": "lIH6oCdppg/figures/figures_9_1.jpg", "caption": "Figure 4: Evolution of token geometry as the number of layers increases. We see that tokens are indeed able to maintain full rank, while at the same time the representations are anisotropic, meaning that they concentrate in a narrow region, as indicated by the average pairwise absolute cosine similarities.", "description": "This figure shows the evolution of token geometry as the number of layers increases in pretrained transformer models.  It displays three key metrics: the percentage of full rank, the minimum singular value, and the average absolute cosine similarity between tokens. The results reveal that the models effectively maintain full rank (close to 100%), showcasing the ability of LayerNorm to prevent rank collapse.  However, the minimum singular value decreases with depth, and the average absolute cosine similarity increases, indicating an anisotropic representation where tokens cluster in a narrow region, aligning with empirical observations of token embeddings generated by transformers.", "section": "Numerical Experiments"}, {"figure_path": "lIH6oCdppg/figures/figures_22_1.jpg", "caption": "Figure 1: Long-term behavior of tokens in the case of N = 2, d = 2. Without LayerNorm (left), both tokens collapse to the same point in R2; whereas with LayerNorm (right), such a collapse would not necessarily happen and token representations can maintain full rank in the long term (first token converges either to (0, 1) or (0, -1). Assuming convergence to (0, 1) for the first token, the second token converges to B, if it is initially located within the red segment).", "description": "This figure shows a comparison of token behavior in a two-token (N=2), two-dimensional embedding space (d=2) scenario with and without LayerNorm.  The left panel illustrates that without LayerNorm, both tokens converge to the same point, resulting in rank collapse. The right panel demonstrates that LayerNorm prevents this collapse; even though one token converges to a fixed point, the other token's position depends on its initial location, potentially maintaining full rank in the long term.", "section": "4.3 Illustrative Counterexample"}, {"figure_path": "lIH6oCdppg/figures/figures_28_1.jpg", "caption": "Figure 2. Evolution of \u03bc(X(t)) (in log-log scale) as the number of layers increases. Rank collapse happens exponentially for pure attention, despite different attention masks having different convergence rates. However, as soon as we solely add in LayerNorm, \u03bc(X(t)) no longer converge to zero in randomly initialized models; in pretrained models, LayerNorm helps prevent the issue together with other components and stabilize the representations.", "description": "The figure shows the evolution of the token similarity measure \u03bc(X(t)) as the number of layers increases for different transformer model variants.  The results demonstrate an exponential rank collapse in models using only self-attention, even with different attention masks (complete, causal, sliding window, and unidirectional sliding window).  The addition of LayerNorm significantly changes the dynamics, preventing the convergence of \u03bc(X(t)) to zero in randomly initialized models. In pre-trained models, LayerNorm, in conjunction with other components, helps prevent rank collapse and stabilizes the token representations.", "section": "Main Results: Attention with Masking and LayerNorm"}, {"figure_path": "lIH6oCdppg/figures/figures_28_2.jpg", "caption": "Figure 3. Evolution of \u03bc(X(t)) (in log-log scale) as the number of layers increases. Smaller temperature terms alleviate the rate of rank collapse, and effect is more significant with global attention than with sparser masked attention, and more in shallower layers than deeper layers.", "description": "This figure shows how the token similarity metric \u03bc(X(t)) changes as the number of layers increases for different attention mask types (complete, causal, sliding window) and temperature terms (dQK = 1/64, 1, 64). The results indicate that smaller temperature terms and sparser attention masks lead to slower rank collapse, highlighting the complex interplay between these factors in determining the long-term behavior of transformers.", "section": "Main Results: Attention with Masking and LayerNorm"}, {"figure_path": "lIH6oCdppg/figures/figures_29_1.jpg", "caption": "Figure 1: Long-term behavior of tokens in the case of N = 2, d = 2. Without LayerNorm (left), both tokens collapse to the same point in R2; whereas with LayerNorm (right), such a collapse would not necessarily happen and token representations can maintain full rank in the long term (first token converges either to (0, 1) or (0, -1). Assuming convergence to (0, 1) for the first token, the second token converges to B, if it is initially located within the red segment).", "description": "This figure illustrates the long-term behavior of two tokens (N=2) in a two-dimensional space (d=2) under self-attention dynamics with and without LayerNorm.  The left panel shows that without LayerNorm, both tokens collapse to a single point, indicating rank collapse. In contrast, the right panel shows that with LayerNorm, the tokens do not necessarily collapse. The first token converges to either (0,1) or (0,-1), and the position of the second token depends on its initial location.", "section": "4.3 Illustrative Counterexample"}, {"figure_path": "lIH6oCdppg/figures/figures_29_2.jpg", "caption": "Figure 1: Long-term behavior of tokens in the case of N = 2, d = 2. Without LayerNorm (left), both tokens collapse to the same point in R2; whereas with LayerNorm (right), such a collapse would not necessarily happen and token representations can maintain full rank in the long term (first token converges either to (0, 1) or (0, -1). Assuming convergence to (0, 1) for the first token, the second token converges to B, if it is initially located within the red segment).", "description": "This figure illustrates the long-term behavior of two tokens (N=2) in a two-dimensional space (d=2) under self-attention dynamics with and without LayerNorm.  The left panel shows that without LayerNorm, both tokens converge to the same point, indicating rank collapse.  The right panel, however, demonstrates that with LayerNorm, rank collapse does not necessarily occur. The tokens can maintain full rank, converging to distinct points depending on their initial positions. The red segment highlights the region of initial positions for the second token that would lead to convergence to point B, given that the first token converges to (0,1).", "section": "4.3 Illustrative Counterexample"}]