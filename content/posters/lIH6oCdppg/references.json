{"references": [{"fullname_first_author": "Josh Alman", "paper_title": "Fast attention requires bounded entries", "publication_date": "2023", "reason": "This paper provides theoretical limitations on the computational complexity of attention mechanisms, which is highly relevant to the topic of the main paper."}, {"fullname_first_author": "Jimmy Ba", "paper_title": "Layer normalization", "publication_date": "2016-07-22", "reason": "This foundational paper introduces layer normalization, a crucial component in many transformer architectures, and its impact on model training and performance is analyzed in the main paper."}, {"fullname_first_author": "Iz Beltagy", "paper_title": "Longformer: The long-document transformer", "publication_date": "2020-04-16", "reason": "This paper introduces Longformer, a transformer architecture designed to handle long sequences efficiently, directly addressing the challenges of long-range dependencies discussed in the main paper."}, {"fullname_first_author": "Yihe Dong", "paper_title": "Attention is not all you need: Pure attention loses rank doubly exponentially with depth", "publication_date": "2021", "reason": "This paper provides a theoretical analysis of rank collapse in self-attention, which serves as a central motivation and comparison point for the main paper's investigation."}, {"fullname_first_author": "Borjan Geshkovski", "paper_title": "A mathematical perspective on transformers", "publication_date": "2023-12-22", "reason": "This paper offers a mathematical framework for analyzing the dynamics of transformers, providing theoretical underpinnings and a complementary analysis to the main paper's empirical investigation."}]}