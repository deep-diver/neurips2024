[{"figure_path": "IwNTiNPxFt/figures/figures_1_1.jpg", "caption": "Figure 1: Stable-Pose leverages the patch-wise attention of ViTs to address the complex pose conditioning problem in T2I generation, showing superior performance compared to current techniques.", "description": "This figure demonstrates Stable-Pose's ability to handle complex pose conditions in text-to-image generation.  It compares Stable-Pose's results to those of other methods (T2I-Adapter and ControlNet) on several example images, showcasing the improved accuracy and detail of human poses generated by Stable-Pose, especially in challenging poses like side or rear views. The examples show that Stable-Pose is more robust to complex pose conditions than other techniques.", "section": "1 Introduction"}, {"figure_path": "IwNTiNPxFt/figures/figures_2_1.jpg", "caption": "Figure 2: The Stable Diffusion architecture with Stable-Pose: operating on the pose skeleton image, Stable-Pose integrates a trainable ViT unit into the frozen-weight Stable Diffusion [33] to improve the generation of pose-guided human images.", "description": "This figure shows the Stable Diffusion architecture with the Stable-Pose module integrated. Stable-Pose is a trainable Vision Transformer (ViT) unit added to improve pose-guided human image generation.  The figure illustrates the flow of information: starting with the input image encoded into a latent space, then through the denoising U-Net (with the Stable-Pose module), and finally decoded into the output image.  Text and pose information are also input to condition the generation. The Stable-Pose module specifically works with the pose skeleton image to refine the attention mechanism and ensure accurate alignment between the generated image and the input pose.", "section": "3 Proposed Method"}, {"figure_path": "IwNTiNPxFt/figures/figures_3_1.jpg", "caption": "Figure 3: Stable-Pose consists of a pose encoder B\u03b8 and a coarse-to-fine Pose-Masked Self-Attention (PMSA) ViT F\u03b8 for seeking the patch-wise relationship of human parts. PMSA restricts attention to embedding tokens within a specific attention mask to ensure that each embedding token can only attend to pose embedding tokens, not non-pose ones.", "description": "This figure shows the architecture of Stable-Pose, which is composed of two main blocks: a pose encoder and a coarse-to-fine Pose-Masked Self-Attention (PMSA) Vision Transformer (ViT).  The pose encoder processes the pose image to extract high-level features. The PMSA ViT leverages a self-attention mechanism to explore the interconnections among different anatomical parts in human pose skeletons. A coarse-to-fine masking strategy is employed to smoothly refine the attention maps based on target pose-related features in a hierarchical manner, transitioning from coarse to fine levels. The output of the PMSA ViT is then combined with the output of the pose encoder and fed into the pre-trained Stable Diffusion model to generate images.", "section": "3 Proposed Method"}, {"figure_path": "IwNTiNPxFt/figures/figures_6_1.jpg", "caption": "Figure 1: Stable-Pose leverages the patch-wise attention of ViTs to address the complex pose conditioning problem in T2I generation, showing superior performance compared to current techniques.", "description": "This figure showcases Stable-Pose's ability to generate images based on text prompts and human pose skeletons, even with challenging poses like side or rear views.  It compares Stable-Pose's results to other state-of-the-art methods (T2I-Adapter and ControlNet), highlighting Stable-Pose's improved accuracy and handling of complex poses. Each row depicts different scenarios.  For each scenario, the pose skeleton is shown along with the resulting images generated by each technique.", "section": "1 Introduction"}, {"figure_path": "IwNTiNPxFt/figures/figures_9_1.jpg", "caption": "Figure 4: Qualitative results of SOTA techniques and our Stable-Pose on Human-Art (first two rows) and LAION-Human (last two rows). An illustration of the pose input is shown in Figure A.1.", "description": "This figure shows a qualitative comparison of Stable-Pose with other state-of-the-art (SOTA) methods for pose-guided text-to-image generation.  The top two rows display results from the Human-Art dataset, while the bottom two rows show results from the LAION-Human dataset. Each set of images shows a pose (a), and then the generated images from T2I-Adapter, ControlNet, Uni-ControlNet, GLIGEN, HumanSD and Stable-Pose.  The results demonstrate Stable-Pose's superior performance in pose accuracy and image fidelity across various complex pose scenarios.", "section": "4.1 Results"}, {"figure_path": "IwNTiNPxFt/figures/figures_13_1.jpg", "caption": "Figure 1: Stable-Pose leverages the patch-wise attention of ViTs to address the complex pose conditioning problem in T2I generation, showing superior performance compared to current techniques.", "description": "This figure showcases the effectiveness of Stable-Pose in addressing complex pose conditions during text-to-image generation.  It compares the results of Stable-Pose with other existing techniques (T2I-Adapter and ControlNet) on various examples of human poses, highlighting Stable-Pose's superior performance in accurately capturing pose details, even in challenging perspectives such as side or rear views. The images demonstrate Stable-Pose's ability to generate photorealistic images with precise pose alignment.", "section": "1 Introduction"}, {"figure_path": "IwNTiNPxFt/figures/figures_18_1.jpg", "caption": "Figure 1: Stable-Pose leverages the patch-wise attention of ViTs to address the complex pose conditioning problem in T2I generation, showing superior performance compared to current techniques.", "description": "This figure shows a comparison of different methods for pose-guided text-to-image generation. The top row shows examples where current techniques struggle with complex poses (side or rear perspectives). The bottom row shows how Stable-Pose achieves better results by using a coarse-to-fine attention masking strategy in a vision transformer to accurately align the pose representation during image synthesis.", "section": "1 Introduction"}, {"figure_path": "IwNTiNPxFt/figures/figures_18_2.jpg", "caption": "Figure 1: Stable-Pose leverages the patch-wise attention of ViTs to address the complex pose conditioning problem in T2I generation, showing superior performance compared to current techniques.", "description": "This figure showcases comparative results of Stable-Pose against other techniques in pose-guided text-to-image generation.  It highlights Stable-Pose's improved ability to handle complex pose conditions, such as side or rear views, compared to existing methods. The images illustrate how Stable-Pose more accurately generates images that match the given pose information.", "section": "1 Introduction"}, {"figure_path": "IwNTiNPxFt/figures/figures_19_1.jpg", "caption": "Figure 1: Stable-Pose leverages the patch-wise attention of ViTs to address the complex pose conditioning problem in T2I generation, showing superior performance compared to current techniques.", "description": "This figure showcases a comparison of different methods for pose-guided text-to-image generation.  It demonstrates Stable-Pose's ability to handle challenging poses, such as side or rear views of human figures, where other methods struggle. The examples highlight Stable-Pose's superior performance in generating images that accurately reflect the input pose compared to existing techniques like T2I-Adapter and ControlNet.", "section": "1 Introduction"}, {"figure_path": "IwNTiNPxFt/figures/figures_19_2.jpg", "caption": "Figure 1: Stable-Pose leverages the patch-wise attention of ViTs to address the complex pose conditioning problem in T2I generation, showing superior performance compared to current techniques.", "description": "This figure showcases the results of Stable-Pose compared to other state-of-the-art techniques for pose-guided text-to-image generation.  It highlights Stable-Pose's ability to accurately generate images even under complex pose conditions (side or rear perspectives), which other methods struggle with. The figure displays various examples of input poses and the resulting images generated by each method, highlighting the improved accuracy and detail of Stable-Pose's output.", "section": "1 Introduction"}, {"figure_path": "IwNTiNPxFt/figures/figures_19_3.jpg", "caption": "Figure 1: Stable-Pose leverages the patch-wise attention of ViTs to address the complex pose conditioning problem in T2I generation, showing superior performance compared to current techniques.", "description": "This figure showcases the results of Stable-Pose compared to other techniques (T2I-Adapter and ControlNet) in handling complex human poses during text-to-image generation.  The image demonstrates that Stable-Pose produces more accurate and realistic images by leveraging the patch-wise attention mechanism of Vision Transformers.  The examples illustrate various poses, highlighting Stable-Pose's ability to address challenges like side or rear perspectives, often missed by other methods.", "section": "1 Introduction"}, {"figure_path": "IwNTiNPxFt/figures/figures_20_1.jpg", "caption": "Figure 1: Stable-Pose leverages the patch-wise attention of ViTs to address the complex pose conditioning problem in T2I generation, showing superior performance compared to current techniques.", "description": "This figure showcases the effectiveness of Stable-Pose in addressing the challenges of pose-guided text-to-image (T2I) generation.  It compares the results of Stable-Pose with other state-of-the-art methods, demonstrating its superior performance in handling complex pose conditions such as side or rear views.  The comparison highlights Stable-Pose's ability to generate images with more accurate and natural poses. Each row presents an example pose, followed by the images produced by various T2I models.", "section": "1 Introduction"}, {"figure_path": "IwNTiNPxFt/figures/figures_20_2.jpg", "caption": "Figure 4: Qualitative results of SOTA techniques and our Stable-Pose on Human-Art (first two rows) and LAION-Human (last two rows). An illustration of the pose input is shown in Figure A.1.", "description": "This figure shows a qualitative comparison of Stable-Pose with other state-of-the-art methods for pose-guided text-to-image generation.  The top two rows display results using the Human-Art dataset, and the bottom two rows show results using the LAION-Human dataset.  Each set of images shows the input pose (skeleton), the generated images produced by different methods, and the ground truth image. This allows for a visual comparison of the effectiveness of each method in terms of pose accuracy and image quality.", "section": "4.1 Results"}, {"figure_path": "IwNTiNPxFt/figures/figures_20_3.jpg", "caption": "Figure 1: Stable-Pose leverages the patch-wise attention of ViTs to address the complex pose conditioning problem in T2I generation, showing superior performance compared to current techniques.", "description": "This figure showcases the results of Stable-Pose and other state-of-the-art (SOTA) methods in generating images based on pose input.  The comparison highlights Stable-Pose's improved performance in handling complex poses, such as those from side or rear perspectives, where previous methods often struggle.  The figure demonstrates Stable-Pose's ability to accurately generate images with correct body proportions and pose details, even in challenging scenarios.", "section": "1 Introduction"}, {"figure_path": "IwNTiNPxFt/figures/figures_20_4.jpg", "caption": "Figure 1: Stable-Pose leverages the patch-wise attention of ViTs to address the complex pose conditioning problem in T2I generation, showing superior performance compared to current techniques.", "description": "This figure showcases the results of Stable-Pose compared to other state-of-the-art techniques on a pose-guided text-to-image generation task.  It highlights Stable-Pose's ability to handle complex pose conditions, such as side or rear views, which were previously challenging for existing methods.  The images demonstrate that Stable-Pose produces more accurate and realistic results than its predecessors.", "section": "1 Introduction"}, {"figure_path": "IwNTiNPxFt/figures/figures_21_1.jpg", "caption": "Figure A.1: Illustration of pose input, where each body part is marked in different color. The pose-image pair is from UBC Fashion dataset [43].", "description": "This figure shows an example of pose input used in the Stable-Pose model.  The image on the left displays a stick figure representation of a person's pose, with each body part (keypoint) depicted in a different color. The image on the right shows a photograph of a fashion model that corresponds to that pose. The caption indicates that this particular pose-image pair originates from the UBC Fashion dataset.", "section": "A.1 Datasets and Preprocessing"}, {"figure_path": "IwNTiNPxFt/figures/figures_21_2.jpg", "caption": "Figure 1: Stable-Pose leverages the patch-wise attention of ViTs to address the complex pose conditioning problem in T2I generation, showing superior performance compared to current techniques.", "description": "This figure shows several examples of images generated by different methods (T2I-Adapter, ControlNet, and Stable-Pose) given the same human pose and text prompt.  The goal is to highlight Stable-Pose's improved ability to accurately generate images reflecting the input pose, even in challenging scenarios like side or rear perspectives, where other methods struggle. The superior performance of Stable-Pose demonstrates its effectiveness in addressing the complexities of pose-guided text-to-image generation.", "section": "1 Introduction"}, {"figure_path": "IwNTiNPxFt/figures/figures_21_3.jpg", "caption": "Figure 1: Stable-Pose leverages the patch-wise attention of ViTs to address the complex pose conditioning problem in T2I generation, showing superior performance compared to current techniques.", "description": "This figure shows a comparison of different methods for pose-guided text-to-image generation, highlighting the improved performance of Stable-Pose in handling complex poses. The figure shows several examples of generated images, where each example includes the input pose, the images generated by the T2I-adapter, ControlNet, and Stable-Pose. The results demonstrate that Stable-Pose generates images that are more closely aligned with the input pose compared to other methods. ", "section": "1 Introduction"}]