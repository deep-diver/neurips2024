[{"figure_path": "Hd2EOwKItm/tables/tables_4_1.jpg", "caption": "Table 1: Comparison of the Linear probing top-1 accuracy on ImageNet-1K dataset.", "description": "This table compares the linear probing top-1 accuracy on the ImageNet-1K dataset for various self-supervised and vision-language pre-training methods.  It shows the number of seen samples and the achieved accuracy for both ViT-Base and ViT-Large models.  The methods are categorized into contrastive or clustering-based, reconstruction-based, and vision-language pre-training-based approaches.  The table highlights the performance of SuperClass in comparison to other state-of-the-art methods.", "section": "4.3 Main results"}, {"figure_path": "Hd2EOwKItm/tables/tables_4_2.jpg", "caption": "Table 2: Performance of frozen visual representations on different classification datasets. 10-shot linear evaluation accuracy on the pre-logit representation. *results from the paper.", "description": "This table presents the performance of the frozen visual representations (features extracted from the image encoder before the classification head) of several models on three different image classification datasets: ImageNet, Pets, and Cars.  The evaluation metric is 10-shot linear evaluation accuracy, indicating how well a linear classifier can distinguish the categories when using the frozen features as input.  The table shows that SuperClass achieves competitive results on ImageNet and Pets and a similar performance on Cars compared to other models, including those using contrastive learning.", "section": "4.2 Evaluation protocols"}, {"figure_path": "Hd2EOwKItm/tables/tables_4_3.jpg", "caption": "Table 3: Zero-shot Top-1 acc. and CIDEr are tested on ImageNet-1k dataset and COCO captions, respectively. The zero-shot accuracy of SuperClass is obtained after lock-image tuning [77].", "description": "This table compares the zero-shot classification accuracy on ImageNet-1k and the CIDEr score for COCO captioning of various vision-language pre-training models.  It shows that SuperClass achieves competitive performance to other models, especially when using a ViT-L/16 backbone after lock-image tuning. The results highlight the effectiveness of SuperClass in zero-shot settings.", "section": "4.2 Evaluation protocols"}, {"figure_path": "Hd2EOwKItm/tables/tables_7_1.jpg", "caption": "Table 4: Word tokenizer vs. Subword tokenizer. The performance of classification and LLaVA downstream tasks with different tokenizers. SuperClass use a subword-level tokenizer to map text into category labels. All models are trained in the same settings with a batch size 16k and 512M seen samples.", "description": "This table compares the performance of using word-level and subword-level tokenizers for both classification and vision & language downstream tasks using the SuperClass model.  The results show the impact of tokenizer choice on model performance across different model sizes (ViT-S/16, ViT-B/16, ViT-L/16).  All models were trained using the same parameters (batch size 16k, 512M seen samples).", "section": "4.4 Ablations"}, {"figure_path": "Hd2EOwKItm/tables/tables_7_2.jpg", "caption": "Table 5: The performance of classification and LLaVA downstream tasks with different subword-level tokenizers. All models are trained in the same settings with a batch size 16k, 512M seen samples and ViT-L/16 as Backbone.", "description": "This table compares the performance of different subword-level tokenizers (OpenaiCLIP, WordPiece, SentencePiece) on classification and vision & language downstream tasks.  The results highlight the impact of tokenizer choice on the model's ability to learn effective representations for both image classification and downstream tasks that leverage both vision and language modalities. All models were trained using the same hyperparameters and data, using a ViT-L/16 backbone.", "section": "4.4 Ablations"}, {"figure_path": "Hd2EOwKItm/tables/tables_8_1.jpg", "caption": "Table 6: The performance on classification tasks with different classification losses. All models are trained in the same settings with a batch size 16k, 512M seen samples and ViT-B/16 as Backbone.", "description": "This table shows the performance of different classification losses (Softmax, BCE, ASL, SoftMargin, Two-way) on classification and zero-shot tasks using ViT-B/16 backbone.  The models were trained with a batch size of 16k and 512M seen samples. The results highlight the performance differences between various loss functions and help determine the best-performing loss for this specific pre-training task.", "section": "4.4 Ablations"}, {"figure_path": "Hd2EOwKItm/tables/tables_8_2.jpg", "caption": "Table 7: The effect of IDF weight in the loss and removing stopwords.", "description": "This table presents the ablation study results on the performance of SuperClass model with and without IDF (Inverse Document Frequency) weighting in the loss function and with and without removing stop words from the text. It shows the impact of these techniques on both classification tasks and several vision and language downstream tasks, providing a quantitative evaluation of their contribution to the overall performance of the model.", "section": "4.4 Ablations"}, {"figure_path": "Hd2EOwKItm/tables/tables_14_1.jpg", "caption": "Table 8: The performance of classification and LLaVA downstream tasks with different seen samples. \"LP\" means linear probing and \"ZS\" means zero-shot classification, these two are tested on ImageNet-1K dataset. The results of vision&language downstream tasks are obtained by combine the frozen vision models and Vicuna-7B [13], following the settings in LLaVA [47].", "description": "This table shows the performance comparison between CLIP and SuperClass on various tasks with different amounts of training data.  The tasks include image classification (linear probing and zero-shot) and several vision-language downstream tasks using the LLaVA framework. The results show how both methods scale with larger amounts of training data.", "section": "4.3 Main results"}, {"figure_path": "Hd2EOwKItm/tables/tables_14_2.jpg", "caption": "Table 10: Performance of frozen visual representations trained via image classification (SuperClass) and constrastively (CLIP). Linear probing and zero-shot classification are both tested on ImageNet-1k dataset. Captioning is conducted on COCO captions and CIDEr is reported in the table. The zero-shot accuracy of SuperClass is obtained after lock-image tuning [77].", "description": "This table compares the performance of SuperClass and CLIP on various downstream tasks, including zero-shot and linear probing classification on ImageNet-1k and captioning on COCO.  It shows the performance using different backbones (RN-50 and ConvNext-tiny) and highlights SuperClass's superior performance in zero-shot and linear probing classification.", "section": "4.3 Main results"}, {"figure_path": "Hd2EOwKItm/tables/tables_15_1.jpg", "caption": "Table 10: Performance of frozen visual representations trained via image classification (SuperClass) and constrastively (CLIP). Linear probing and zero-shot classification are both tested on ImageNet-1k dataset. Captioning is conducted on COCO captions and CIDEr is reported in the table. The zero-shot accuracy of SuperClass is obtained after lock-image tuning [77].", "description": "This table compares the performance of SuperClass and CLIP on various tasks using different backbones (RN-50 and ConvNext-tiny).  It shows the zero-shot accuracy, linear probing accuracy on ImageNet-1k, and captioning performance (CIDEr score) on COCO captions.  The results highlight the improved performance of SuperClass over CLIP across these tasks and backbones.", "section": "4.3 Main results"}, {"figure_path": "Hd2EOwKItm/tables/tables_15_2.jpg", "caption": "Table 11: The performance of vision & language downstream tasks with different pretrained models.", "description": "This table compares the performance of various vision-language pretrained models (OpenCLIP, MAE, DINOv2, and SuperClass) on several downstream tasks. The tasks include VQAv2, GQA, VizWiz, T-VQA, SciQA, MME, MMB, PoPE, and MMMU.  The results show the performance of each model on each task, providing a comprehensive comparison of their abilities in various vision and language applications.", "section": "4.3 Main results"}, {"figure_path": "Hd2EOwKItm/tables/tables_15_3.jpg", "caption": "Table 12: Comparison of the Fine-tuning top-1 accuracy on ImageNet-1K dataset. *number from the paper.", "description": "This table compares the fine-tuning top-1 accuracy on the ImageNet-1K dataset for three different methods: OpenCLIP, CatLIP, and Superclass. All models used the Datacomp-1B dataset for pretraining.  The table highlights the fine-tuning performance achieved by each method, showing Superclass achieving the highest accuracy (87.8). The asterisk (*) indicates that the CatLIP result is taken from their paper.", "section": "4.3 Main results"}]