{"references": [{"fullname_first_author": "Shafi Goldwasser", "paper_title": "Planting undetectable backdoors in machine learning models", "publication_date": "2022-00-00", "reason": "This paper lays the theoretical foundation for undetectable backdoors, a concept central to the current work."}, {"fullname_first_author": "Christian Szegedy", "paper_title": "Intriguing properties of neural networks", "publication_date": "2013-12-00", "reason": "This seminal work highlights the vulnerability of deep learning models to adversarial attacks, providing context for the backdoor threat."}, {"fullname_first_author": "Boaz Barak", "paper_title": "Can we obfuscate programs", "publication_date": "2002-00-00", "reason": "This foundational paper introduces the concept of indistinguishability obfuscation, a crucial cryptographic tool used for constructing undetectable backdoors."}, {"fullname_first_author": "Aayush Jain", "paper_title": "Indistinguishability obfuscation from well-founded assumptions", "publication_date": "2021-00-00", "reason": "This paper provides a construction of indistinguishability obfuscation under well-founded assumptions, strengthening the theoretical basis of the main result."}, {"fullname_first_author": "Micah Goldblum", "paper_title": "Dataset security for machine learning: Data poisoning, backdoor attacks, and defenses", "publication_date": "2022-00-00", "reason": "This survey paper provides a comprehensive overview of dataset security threats in machine learning, including backdoor attacks and defenses, which helps to contextualize this research."}]}