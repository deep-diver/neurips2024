[{"type": "text", "text": "Node-Level Topological Representation Learning on Point Clouds ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Topological Data Analysis (TDA) allows us to extract powerful topological, and higher-order information on the global shape of a data set or point cloud. Tools like Persistent Homology or the Euler Transform give a single complex description of the global structure of the point cloud. However, common machine learning applications like classification require point-level information and features to be available. In this paper, we bridge this gap and propose a novel method to extract node-level topological features from complex point clouds using discrete variants of concepts from algebraic topology and differential geometry. We verify the effectiveness of these topological point features (TOPF) on both synthetic and real-world data and study their robustness under noise. ", "page_idx": 0}, {"type": "image", "img_path": "jwE1dgOox1/tmp/02df1d2c5b738f3d1b7f3bf75baeb9c72e7835d55bcefd7cf305238dd8fbefc1.jpg", "img_caption": ["Figure 1: Schematic of Computing Topological Point Features (TOPF). Input. A point cloud $X$ in $n$ -dimensional space. Step 1. To extract global topological information, the persistent homology is computed on an $\\alpha/\\mathrm{VR}$ -filtration. The most significant topological features $\\mathcal{F}$ across all specified dimensions are selected. Step 2. $k$ -homology generators associated to all features $f_{i,k}\\,\\in\\,{\\mathcal{F}}$ are computed. For every feature, a simplicial complex is built at a step of the filtration where $f_{i,k}$ is alive. Step 3. The homology generators are projected to the harmonic space of the simplices. Step 4. The vectors are normalised to obtain vectors ${\\bf e_{k}^{i}}^{-}$ indexed over the $k$ -simplices. For every point $x$ and feature $f\\in\\mathcal F$ , we compute the mean of the entries of $\\mathbf{e_{k}^{i}}$ corresponding to simplices containing $x$ The output is a $|X|\\times|{\\mathcal{F}}|$ matrix which can be used for downstream ML tasks. Optional. We weigh the simplicial complexes resulting in a topologically more faithful harmonic representative in Step 3. "], "img_footnote": [], "page_idx": 0}, {"type": "text", "text": "11 1 Introduction ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "12 In modern machine learning [39], objects are described by feature vectors within a high-dimensional   \n13 space. However, the coordinates of a single vector can often only be understood in relation to the   \n14 entire data set: if the value $x$ is small, average, large, or even an outlier depends on the remaining   \n15 data. In a 1-dimensional (or low-dimensional) case this issue can be addressed simply by normalising   \n16 the data points according to the global mean and standard deviation or similar procedures. We can   \n17 interpret this as the most straight-forward way to construct local features informed by the global   \n18 structure of the data set.   \n19 In the case where not all data dimensions are equally relevant, or contain correlated and redundant   \n20 information, we can apply (sparse) PCA to project the data points to a lower dimensional space   \n21 using information about the global structure of the point cloud [51]. For even more complex data,   \n22 we may first have to learn the encoded structure itself: indeed, a typical assumption underpinning   \n23 many unsupervised learning methods is the so-called \u201cmanifold hypothesis\u201d which posits that   \n24 real world data can be described well via submanifolds of $n$ -dimensional space [36, 21]. Using   \n25 eigenvectors of some Laplacian, we can then obtain a coordinate system intrinsic to the point cloud   \n26 (see e.g. [47, 4, 15]). Common to all these above examples is the goal is to construct locally   \n27 interpretable point-level features that encode globally meaningful positional information robust to   \n28 local perturbations of the data. However, none of these approaches is able to represent higher-order   \n29 topological information, making point clouds with these kind of structure inaccessible to point-level   \n30 machine learning algorithms.   \n31 Instead of focussing on the interpretation of individual points, topological data analysis (TDA), [9],   \n32 follows a different approach. TDA extracts a global description of the shape of data, which is typically   \n33 considered in the form of a high-dimensional point cloud. This is done measuring topological features   \n34 like persistence homology, which counts the number of generalised \u201choles\u201d in the point cloud on   \n35 multiple scales. Due to their flexibility and robustness these global topological features have been   \n36 shown to contain relevant information in a broad range of application scenarios: In medicine, TDA   \n37 has provided methods to analyse cancer progression [33]. In biology, persistent homology has been   \n38 used to analyse knotted protein structures [5], and the spectrum of the Hodge Laplacian has been   \n39 used for predicting protein behaviour [50].   \n40 This success of topological data analysis is a testament to the fact that relevant information is encoded   \n41 in the global topological structure of point cloud data. Such higher-order topological information is   \n42 however invisible to standard tools of data analysis like PCA or $k$ -means clustering, and can also not   \n43 be captured by graph models of the point cloud. We are now faced by a situation where (i) important   \n44 parts of the global structure of a complex point cloud can only be described by the language of   \n45 applied topology, however (ii) most standard methods to obtain positional point-level information are   \n46 not sensitive to the higher-order topology of the point cloud.   \n47 Contributions We introduce TOPF (Figure 1), a novel method to compute node-level topological   \n48 features relating individual points to global topological structures of point clouds. TOPF (i) outper  \n49 forms other methods and embeddings for clustering downstream tasks on topologically structured data,   \n50 returns (ii) provably meaningful representations, and is (iii) robust to noise. Finally, we introduce the   \n51 topological clustering benchmark suite, the first benchmark for topological clustering.   \n52 Related Work The intersection of topological data analysis, topological signal processing and   \n53 geometry processing has many interesting related developments in the past few years. On the side   \n54 of homology and TDA, the authors in [16] and [41] use harmonic cohomology representatives to   \n55 reparametrise point clouds based on circular coordinates. This implicitly assumes that the underlying   \n56 structure of the point cloud is amenable to such a characterization. In [2, 26], the authors develop and   \n57 use harmonic persistent homology for data analysis. However, among other differences their focus   \n58 is not on providing robust topological point features. [24] uses the harmonic space of the Hodge   \n59 Laplacians to cluster point clouds respecting topology, but is unstable against some form of noise,   \n60 has no possibility for features selection across scales and is computationally far more expensive than   \n61 TOPF. For a more in-depth review of related work, see Appendix A   \n62 Organisation of the paper In Section 2, we give an overview over the main ideas and concepts   \n63 behind of TOPF. In Section 3, we describe how to compute TOPF. In Section 4, we give a theoretical   \n64 result guaranteeing the correctness of TOPF. Finally, we will apply TOPF on synthetic and real-world   \n65 data in Section 5. Furthermore, Appendix A contains a brief history of topology and a detailed   \n66 discussion of related work. Appendix B contains additional theoretical considerations, Appendix C   \n67 describes the novel topological clustering benchmark suite, Appendix D contains details on the   \n68 implementation and the choice of hyperparameters, Appendix E gives a detailed treatment of feature   \n69 selection, Appendix F discusses simplicial weights, and Appendix G discusses limitations in detail. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "70 2 Main Ideas of TOPF ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "71 A main goal of algebraic topology is to capture the shape of spaces. Techniques from topology   \n72 describe globally meaningful structures that are indifferent to local perturbations and deformations.   \n73 This robustness of topological features to local perturbations is particularly useful for the analysis   \n74 of large-scale noisy datasets. To apply the ideas of algebraic topology in our TOPF pipeline, we   \n75 need to formalise and explain the notion of topological features. An important observation for   \n76 this is that high-dimensional point clouds and data may be seen as being sampled from topological   \n77 spaces \u2014 most of the time, even low-dimensional submanifolds of $\\mathbb{R}^{n}$ [21].   \n78 In this section we provide a broad overview over the most important concepts of topology and TDA   \n79 for our context, prioritising intuition over technical formalities. The interested reader is referred   \n80 to [7, 27, 49] for a complete technical account of topology and [38] for an overview over TDA.   \n81 Simplicial Complexes Spaces in topology are continuous, consist of infinitely many points, and   \n82 often live in abstract space. Our input data sets however consist of finitely many points embedded   \n83 in real space $\\mathbb{R}^{n}$ . In order to bridge this gap and open up topology to computational methods, we   \n84 need a notion of discretised topological spaces consisting of finitely many base points with finite   \n85 description length. A Simplicial Complex is the simplest discrete model that can still approximate   \n86 any topological space occuring in practice [43]:   \n87 Definition 2.1 (Simplicial complexes). A simplicial complex (SC) $\\boldsymbol{S}$ consists of a set of vertices $V$   \n88 and a set of finite non-empty subsets (simplices, $S$ ) of $V$ closed under taking non-empty subsets, such   \n89 that the union over all simplices $\\textstyle\\bigcup_{\\sigma\\in S}\\sigma$ is $V$ . In the following, we will often identify $\\boldsymbol{S}$ with its set   \n90 of simplicies $S$ and denote by $\\scriptstyle S_{k}$ the set of simplices $\\sigma\\in S$ with $|\\sigma|=k+1$ , called $k$ -simplices. We   \n91 say that $\\boldsymbol{S}$ is $n$ -dimensional, where $n$ is the largest $k$ such that the set of $k$ -simplices $\\ensuremath{\\mathcal{S}}_{k}$ is non-empty.   \n92 The $k$ -skeleton of SC contains the simplices of dimension at most $k$ . If the vertices $V$ lie in real space   \n93 $\\mathbb{R}^{n}$ , we call the convex hull in $\\mathbb{R}^{n}$ of a simplex $\\sigma$ its geometric realisation $|\\sigma|$ . When doing this for   \n94 every simplex of $\\boldsymbol{S}$ , we call this the geometric realisation of $\\boldsymbol{S}$ , $\\left\\vert S\\right\\vert\\subset\\mathbb{R}^{n}$ .   \n95 Concretely, we can construct an $n$ -dimensional SC $\\boldsymbol{S}$ in $n+1$ steps: First, we start with a set of   \n96 vertices $V$ which we can identify with the 0-simplices ${\\mathcal{S}}_{0}$ . Second, we connect certain pairs of   \n97 vertices with edges, which constitute the set of 1-simplices. We can then choose to flil in some triples   \n98 of vertices which are fully connected by 1-simplices with triangles, i.e. 2-simplices. More generally,   \n99 in the $k^{\\mathrm{th}}$ step, we can add a $k$ -simplex for every set $\\sigma_{k}$ of $k+1$ vertices such that every $k$ -element   \n100 subset $\\sigma_{k-1}$ of $\\sigma_{k}$ is already a $(k-1)$ -simplex.   \n101 Vietoris\u2013Rips and $\\alpha$ -complexes We now need a way to construct a simplicial complex that   \n102 approximates the topological structure inherent in our data set $X\\subset\\mathbb{R}^{n}$ . Such a construction will   \n103 always depend on the scale of the structures we are interested in. When looking from a very large   \n104 distance, the point cloud will appear as a singular connected blob in the otherwise empty and infinite   \n105 real space, on the other hand when we continue to zoom in, the point cloud will at some point appear   \n106 as a collection of individual points separated by empty continuous space; all interesting information   \n107 can be found in-between these two extreme scales where some vertices are joined by simplices and   \n108 others are not. Instead of having to pick a single scale, the Vietoris\u2013Rips $\\left(V\\!R\\right)$ filtration and the   \n109 $\\alpha$ -flitration take as input a point cloud and return a nested sequence of simplicial complexes indexed   \n110 by a scale parameter $\\varepsilon$ approximating the topology of the data across all possible scales.   \n111 Definition 2.2 (VR complex). Given a finite point cloud $X$ in a metric space $(\\mathcal{M},d)$ and a non  \n112 negative real number $\\varepsilon\\in\\mathbb{R}_{\\geq0}$ , the associated VR complex $V R_{\\varepsilon}(X)$ is given by the vertex set $X$ and   \n113 the set of simplices $S=\\{\\sigma\\overset{-}{\\subset}X\\mid\\sigma\\neq\\emptyset,\\forall x,y\\in\\sigma:d(x,y)\\leq\\varepsilon\\}$   \n114 Intuitively, a VR complex with parameter $\\varepsilon$ consists of all simplices $\\sigma$ where all vertices $x\\in\\sigma$ have a   \n115 pair-wise distance of at most $\\varepsilon$ . For $r\\le r^{\\prime}$ , we obtain the canonical inclusions $i_{r,r^{\\prime}}(X)\\colon V R_{r}(X)\\hookrightarrow$   \n116 $V R_{r^{\\prime}}(X)$ . The set of VR complexes on $X$ for all possible $r\\in\\mathbb{R}_{\\geq0}$ together with the inclusions then   \n117 form the VR flitration on $X$ . For large point clouds, using the VR complex for computations becomes   \n118 expensive due to its large number of simplices. In contrast, the more sophisticated $\\alpha$ -complex   \n119 approximates the topology of a point cloud using far fewer simplices and thus we will make use of it.   \n120 For a complete account and definition of $\\alpha$ -complexes and our reason to use them, see Appendix B.   \n121 Boundary matrices So far, we have discussed a discretised version of topological spaces in the   \n122 form of SCs and a way to turn point clouds into a sequence of SCs indexed by a scale parameter.   \n123 However, we still need an algebraic representation of simplicial complexes that is capable of encoding   \n124 the structure of the SC and enables extraction of the topological features: The boundary matrices   \n125 $\\boldsymbol{{\\beta}}_{k}$ associated to an $\\mathbf{\\boldsymbol{S}}\\mathbf{\\boldsymbol{C}}\\mathbf{\\boldsymbol{\\mathcal{S}}}$ store all structural information of SC. The rows of $\\boldsymbol{{\\beta}}_{k}$ are indexed by the   \n126 $k$ -simplices of $\\boldsymbol{S}$ and the columns are indexed by the $(k+1)$ -simplices.   \n127 Definition 2.3 (Boundary matrices). Let $\\boldsymbol{S}$ be a simplicial complex and $\\preceq\\mathbf{a}$ total order on its vertices   \n128 $V$ . Then, the $i$ -th face map in dimension $n$ $f_{i}^{n}\\colon S_{n}\\to S_{n-1}$ is given by ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\nf_{i}^{n}\\colon\\{v_{0},v_{1},\\ldots,v_{n}\\}\\mapsto\\{v_{0},v_{1},\\ldots,\\widehat{v}_{i},\\ldots,v_{n}\\}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "129 with $v_{0}\\,\\preceq\\,v_{1}\\,\\preceq\\,\\cdots\\,\\preceq\\,v_{n}$ and ${\\widehat{v_{i}}}$ denoting the omission of $v_{i}$ . Now, the $n$ -th boundary operator   \n130 $B_{n}\\colon\\mathbb{R}[S_{n+1}]\\to\\mathbb{R}[S_{n}]$ with $\\mathbb{R}[S_{n}]$ being the real vector space over the basis ${\\mathcal{S}}_{n}$ is given by ", "page_idx": 3}, {"type": "equation", "text": "$$\n{\\mathcal{B}}_{n}\\colon{\\boldsymbol{\\sigma}}\\mapsto\\sum_{i=0}^{n+1}(-1)^{i}f_{i}^{n+1}({\\boldsymbol{\\sigma}}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "131 When lexicographically ordering the simplex basis, we can view $B_{n}$ as a matrix. We call $\\mathbb{R}[S_{n}]$ the   \n132 space of $n$ -chains. Now, $B_{0}$ is the vertex-edge incidence matrix of the associated graph consisting of   \n133 the 0- and 1-simplices of $\\boldsymbol{S}$ and $B_{1}$ is the edge-triangle incidence matrix of $\\boldsymbol{S}$   \n134 Betti Numbers and Persistent Homology We now turn to the notion of   \n135 topological features and how to extract them. Homology is one of the main   \n136 algebraic invariants to capture the shape of topological spaces and SC. From   \n137 a technical point of view, the $k$ -th homology module $H_{k}(S)$ of an SC $\\boldsymbol{S}$   \n138 with boundary operators $\\boldsymbol{{\\beta}}_{k}$ is defined as $H_{k}(S):=\\ker B_{k-1}/\\operatorname{Im}{\\cal B}_{k}$ . The   \n139 generator or representative of a homology class is an element of the kernel   \n140 $\\ker\\boldsymbol{B}_{k-1}$ . In dimension 1, these are given by formal sums of 1-simplices   \n141 forming closed loops in the SC. Importantly, the rank rk $H_{k}(S)$ is called   \n142 the $k$ -th Betti number $B_{k}$ of $\\boldsymbol{S}$ . In dimension 0, $B_{0}$ counts the number of   \n143 connected components, $B_{1}$ counts the number of loops around \u2018holes\u2019 of   \n144 the space, $B_{2}$ counts the number of 3-dimensional voids with 2-dimensional   \n145 boundary, and so on.   \n146 If we are now given a filtration of simplicial complexes instead of a single   \n147 SC, we can track how the homology modules evolve as the simplicial   \n148 complex grows. The mathematical formalisation, persistent homology, thus   \n149 turns a point cloud via a simplicial filtration into an algebraic object summarising the topological   \n150 feature of the point cloud. For better computational performance, the computations are usually done   \n151 in one of the small finite fields $\\mathbb{Z}/p\\mathbb{Z}$ . Because we will later be interested in the sign of numbers   \n152 to distinguish different simplex orientations, we will use $\\mathbb{Z}/3\\mathbb{Z}$ -coefficients, with $\\mathbb{Z}/3\\mathbb{Z}$ being the   \n153 smallest field being able to distinguish 1 and $-1$ .   \n154 The Hodge Laplacian and the Harmonic Space In the previous part, we have introduced a   \n155 language to characterise the global shape of spaces and point clouds. However, we still need to find   \n156 a way to relate these global characterisations back to local properties of the point cloud. We will   \n157 do so by using ideas and concepts from differential geometry and topology: The simplicial Hodge   \n158 Laplacian is a discretisation of the Hodge\u2013Laplace operator acting on differential forms of manifolds:   \n159 Definition 2.4 (Hodge Laplacian). Given a simplicial complex $\\boldsymbol{S}$ with boundary operators $\\boldsymbol{{\\beta}}_{k}$ , we   \n160 define the $n$ -th Hodge Laplacian $L_{n}\\colon\\mathbb{R}[S_{n}]\\to{\\bar{\\mathbb{R}}}[S_{n}]$ by setting ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "image", "img_path": "jwE1dgOox1/tmp/73e74239efba2685f96006d3581030ef2f7609efa36b476f680b33c984453b98.jpg", "img_caption": ["Figure 2: Sketch of Persistent Homology, [23] "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\nL_{n}:=\\mathcal{B}_{n-1}^{\\top}\\mathcal{B}_{n-1}+\\mathcal{B}_{n}\\mathcal{B}_{n}^{\\top}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "161 The Hodge Laplacian gives rise to the Hodge decomposition theorem: ", "page_idx": 3}, {"type": "text", "text": "Input: Point cloud $X\\in\\mathbb{R}^{n}$ , maximum homology dimension $d\\in\\mathbb{N}$ , interpolation coeff. $\\lambda$ .   \n1. Compute persistent homology with generators in dimension $k\\leq d$ .   \n2. Select set of significant features $\\left(b_{i},d_{i},g_{i}\\right)$ with birth, death, and generator in $\\mathbb{F}_{3}$ coordinates.   \n3. Embed $g_{i}$ into real space and project into harmonic subspace of SC at step $t=\\lambda b_{i}+(1-\\lambda)d_{i}$ .   \n4. Normalise projections to ${\\bf e}_{i}^{k}$ and compute $F_{k}^{i}(x):=\\arg_{x\\in\\sigma}(\\mathbf{e}_{i}^{k}l(\\sigma))$ for all points $x\\in X$ .   \nOutput: Features of $x\\in X$ ", "page_idx": 4}, {"type": "image", "img_path": "jwE1dgOox1/tmp/eb59f32c182a014271fa38c8dee12ccd1052c9ff1dd768dd19b6bda064ab87af.jpg", "img_caption": ["Figure 3: TOPF pipeline applied to NALCN channelosome, a membran protein [32]. Left: Steps 1&2a, when computing persistent 1-homology, three classes are more prominent than the rest. Centre: Step 2b: The selected homology generators. Right: Step 3: The projections of the generators into (weighted) harmonic are now each supported on one of the three rings. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "162 Theorem 2.5 (Hodge Decomposition [34, 46, 44]). For an SC $\\boldsymbol{S}$ with boundary matrices $(\\boldsymbol{B}_{i})$ and   \n163 Hodge Laplacians $\\left(L_{i}\\right)$ , we have in every dimension $k$ ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbb{R}[S_{k}]=\\underbrace{\\operatorname{Im}\\mathscr{B}_{k-1}^{\\top}}_{g r a d i e n t\\,s p a c e}\\oplus\\underbrace{\\operatorname{ker}L_{k}}_{h a r m o n i c\\,s p a c e}\\oplus\\underbrace{\\operatorname{Im}\\mathscr{B}_{k}}_{c u r l\\,s p a c e}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "164 This, together with the fact that the $k$ -th harmonic space is isomorphic to the $k$ -th real-valued   \n165 homology group $\\ker L_{k}\\cong H_{k}(\\mathbb{R})$ means that we can associate a unique harmonic representative   \n166 to every homology class. The harmonic space encodes higher-order generalisations of smooth flow   \n167 around the holes of the simplicial complex. Intuitively, this means that for every abstract global   \n168 homology class of persistent homology from above we can now compute one unique harmonic   \n169 representative in $\\ker L_{k}$ that assigns every simplex a value based on how much it contributes to the   \n170 homology class. Thus, the Hodge Laplacian is a gateway between the global topological features   \n171 and the local properties of our SC. It is easy to show that the kernel of the Hodge Laplacian is the   \n172 intersection of the kernel of the boundary and the coboundary map $\\ker L_{k}=\\ker\\dot{B_{n-1}}\\cap\\ker\\ensuremath{B_{n}^{\\top}}$ .   \n173 Because we have finite SCs we can identify the spaces of chains and cochains. This leads to another   \n174 characterisation of the harmonic space: The space of chains that are simultaneously homology and   \n175 cohomology representatives. ", "page_idx": 4}, {"type": "text", "text": "176 3 How to Compute Topological Point Features ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "177 In this section, we will combine the ideas and insights of the previous section to give a complete   \n178 account of how to compute Topological point features (TOPF). A pseudo-code version can be found   \n179 in Algorithm 1 and an overview in Figure 1. We start with a finite point cloud $X\\subset\\mathbb{R}^{n}$ .   \n180 Step 1: Computing the persistent homology First, we need to determine the most significant   \n181 persistent homology classes which determine the shape of the point cloud. By doing this, we can   \n182 also extract the \u201cinteresting\u201d scales of the data set. We will later use this to construct SCs to derive   \n183 local variants of the global homology features. Thus we first compute the persistent $k$ -homology   \n184 modules $P_{k}$ including a set of homology representatives $R_{k}$ of $X$ using an $\\alpha$ -flitration for $n\\leq3$ and   \n185 a VR flitration for $n>3$ . We use $\\mathbb{Z}/3\\mathbb{Z}$ coefficients to be sensitive to simplex orientations. In case we   \n186 have prior knowledge on the data set, we can choose a real number $R\\in\\mathbb{R}_{>0}$ and only compute the   \n187 filtration and persistent homology connecting points up to a distance of at most $R$ . In data sets like   \n188 protein atom coordinates, this might be useful as we have prior knowledge on what constitutes the   \n189 \u201cinteresting\u201d scale, reducing computational complexity. See Figure 3 left for a persistent homology   \n190 diagram.   \n191 Step 2: Selecting the relevant topological features We now need to select the relevant homology   \n192 classes which carry the most important global information. The persistent homology $P_{k}$ module in   \n193 dimension $k$ is given to us as a list of pairs of birth and death times $(b_{i}^{k},d_{i}^{k})$ . We can assume these   \n194 pairs are ordered in non-increasing order of the durations $l_{i}^{k}=d_{i}^{k}-\\dot{b}_{i}^{k}$ . This list is typically very   \n195 long and consists to a large part of noisy homological features which vanish right after they appear.   \n196 In contrast, we are interested in connected components, loops, cavities, etc. that persist over a long   \n197 time, indicating that they are important for the shape of the point cloud. Distinguishing between the   \n198 relevant and the irrelevant features is in general difficult and may depend on additional insights on   \n199 tahses udmopmtiaoinn so fo na ptphlei cnautimonb.e r Ionf  orreldeevr atnot  pfreoatvuirdees  a whee upriicskti tc hwe hsimcha lldeoset s qnuoott ideenpt $q_{i}^{k}:=l_{i+1}^{k}/l_{i}^{k}>0$   \n201 as the point of cut-off $N_{k}:=\\arg\\operatorname*{min}_{i}q_{i}^{k}$ . The only underlying assumption of this approach is that   \n202 the band of \u201crelevant\u201d features is separated from the \u201cnoisy\u201d homological features by a drop in   \n203 persistence. If this assumption is violated, the only possible way to do meaningful feature selection   \n204 depends on application-specific domain knowledge. We found that our proposed heuristics work well   \n205 across a large scale of applications. See Figure 3 left and centre for an illustration and Appendix E   \n206 for more technical details and ways to improve and adapt the feature selection module of TOPF. We   \n207 call the chosen $k$ -homology classes including $k$ -homology generators in dimension $f_{k}^{i}$ .   \n208 Step 3: Projecting the features into harmonic space and normalising In this step, we need to   \n209 relate the global topology extracted in the previous step to the simplices which we will use to compute   \n210 the local topological point feature. Every selected feature $f_{k}^{i}$ of the previous step comes with a birth   \n211 time $b_{i,k}$ and a death time $d_{i,k}$ . This means that the homology class $f_{k}^{i}$ is present in every SC of   \n212 the filtration between step $\\varepsilon=b_{i,k}$ and $\\varepsilon=d_{i,k}$ and we could choose any of the SCs for the next   \n213 step. Picking a small $\\varepsilon$ will lead to fewer simplices in the SC and thus to a very localised harmonic   \n214 representative. Picking a large $\\varepsilon$ will lead to many simplices in the SC and thus to a very smooth   \n215 and \u201cblurry\u201d harmonic representative with large support. Finding a middle ground between these   \n221176 rsiegmipmliecsi arle tcuornms poleptxi $S^{t_{i,k}}(X)$ sa.t  Fstoer pt $t_{i,k}:=\\stackrel{\\cdot}{b}_{i,k}^{1-\\gamma}d_{i,k}^{\\gamma}$ p faorar $k>0$ $\\gamma\\in(0,1)$ e, pw $t_{i,k}:=\\gamma d_{i,k}$ o fnosri $k=0$   \n218 of the simplicial filtration. At this point, the homology class $f_{k}^{i}$ is still alive. We then consider the   \n219 real vector space $\\mathbb{R}[S_{k}^{t_{i,k}}(X)]$ with formal basis consisting of the $k$ -simplices of the SC $S^{t_{i,k}}$ . From   \n220 the persistent homology computation of the first step, we also obtain a generator of the feature $f_{k}^{i}$ ,   \n221 consisting of a list \u03a3ik of simplices \u03c3\u02c6j \u2208Skbi, kand coefficients cj \u2208Z/3Z. We need to turn this   \n222 formal sum of simplices with $\\mathbb{Z}/3\\mathbb{Z}$ -coefficients into a vector in the real vector space $\\mathbb{R}[S_{k}^{t_{i,k}}(X)]$ :   \n223 Let $\\iota\\colon\\mathbb{Z}/3\\mathbb{Z}$ be the map induced by the canonical inclusion of $\\{-1,0,1\\}\\hookrightarrow\\mathbb{R}$ . We can now define   \n224 an indicator vector $e_{k}^{i}\\overset{\\mathbf{\\alpha}}{\\in}\\mathbb{R}[S_{k}^{t_{i,k}}(\\overset{\\cdot}{X})]$ associated to the feature $f_{k}^{i}$ . ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\ne_{k}^{i}(\\sigma):=\\left\\{\\!\\!\\begin{array}{l l}{\\iota(c_{j})}&{\\exists\\hat{\\sigma}_{j}\\in\\Sigma_{k}^{i}:\\sigma=\\hat{\\sigma}_{j}}\\\\ {0}&{\\mathrm{else}}\\end{array}\\!\\!\\right..\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "225 While this homology representative lives in a real vector space, it is not unique, has a small support,   \n226 and can differ largely between close simplices. All of these problems can be solved by projecting   \n227 the homology representative to the harmonic subspace $\\ker L_{k}$ of $\\mathbb{R}[S_{k}^{t_{i,k}}(X)]$ . Rather than directly   \n228 projecting $e_{k}^{i}$ to the harmonic subspace, we make use of the Hodge decomposition theorem (The  \n229 orem 2.5) which allows us to compute the gradient and curl projections solving computationally   \n230 efficient least square problems: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\boldsymbol{e}_{k,\\mathrm{grad}}^{i}:=\\mathcal{B}_{k-1}^{\\top}\\underset{\\boldsymbol{x}\\in\\mathbb{R}[S_{k-1}]}{\\arg\\operatorname*{min}}\\ \\left\\|\\boldsymbol{e}_{k}^{i}-\\mathcal{B}_{k-1}^{\\top}\\boldsymbol{x}\\right\\|_{2}^{2}\\quad\\mathrm{and}\\quad\\boldsymbol{e}_{k,\\mathrm{cut}}^{i}:=\\mathcal{B}_{k}\\underset{\\boldsymbol{x}\\in\\mathbb{R}[S_{k+1}]}{\\arg\\operatorname*{min}}\\ \\left\\|\\boldsymbol{e}_{k}^{i}-\\boldsymbol{e}_{k,\\mathrm{grad}}^{i}-\\mathcal{B}_{k}\\boldsymbol{x}\\right\\|_{2}^{2}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "231 and then setting $\\hat{e}_{k}^{i}:=e_{k}^{i}-e_{k,\\mathrm{grad}}^{i}-e_{k,\\mathrm{curl}}^{i}$ . (Cf. Figure 3 right for a visualisation.) Because homology   \n232 representatives are gradient-free, we only need to consider the projection of $e_{k}^{i}$ into the curl space.   \n233 Step 4: Processing and aggregation at a point level In the previous step, we have computed   \n234 a set of simplex-valued harmonic representatives of homology classes. However, these simplices   \n235 likely have no real-world meaning and the underlying simplicical complexes differ depending   \n236 on the birth and death times of the homology classes. Hence in this step, we will collect the   \n237 features on the point-level after performing some necessary preprocessing. Given a simplex-valued   \n238 vector $\\hat{e}_{k}^{i}$ and a hyperparameter $\\delta$ , we now construct $\\mathbf{e}_{k}^{i}\\colon\\dot{S_{k}^{t_{i,k}}}(X)\\to[\\bar{0},1]$ by setting $\\mathbf{e}_{k}^{i}\\colon\\sigma\\mapsto\\in$   \n239 $\\{|\\hat{e}_{k}^{i}(\\sigma)|/(\\delta\\operatorname*{max}_{\\sigma^{\\prime}\\in S_{k}^{t_{i,k}}(X)}|\\hat{e}_{k}^{i}(\\sigma^{\\prime})|),1\\}$ such that $\\hat{e}_{k}^{i}$ is normalised to $[0,1]$ , the values of $[0,\\delta]$ are   \n240 mapped linearly to $[0,1]$ and everything above is sent to 1. We found empirically that a thresholding   \n241 parameter of $\\delta=0.07$ works best across at the range of applications considered below. However,   \n242 TOPF is not sensitive to small changes to $\\delta$ because entries of $\\hat{e}_{k}^{i}$ are concentrated around 0.   \n243 For every feature $f_{k}^{i}$ in dimension $k$ with processed simplicial feature vector ${\\bf e}_{k}^{i}$ and simplicial   \n244 complex $S^{t_{i,k}}$ , we define the point-level feature map $F_{i}^{k}\\colon X\\to\\mathbb{R}$ mapping from the initial point   \n245 cloud $X$ to $\\mathbb{R}$ by setting ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "equation", "text": "$$\nF_{i}^{k}\\colon v\\mapsto\\frac{\\sum_{\\sigma_{k}\\in S_{k}^{t_{i,k}}\\colon v\\in\\sigma_{k}}\\mathbf{e}_{k}^{i}(\\sigma_{k})}{\\operatorname*{max}(1,|\\{\\sigma_{k}\\in S_{k}^{t}:v\\in\\sigma_{k}\\}|)}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "246 For every point $v$ , we can thus view the vector $(F_{i}^{k}(v)\\colon f_{i}^{k}\\in\\mathcal{F})$ as a feature vector for $v$ . We call   \n247 this collection of features Topological Point Features (TOPF). (Cf. Figure 4 for an example).   \n248 Choosing Simplicial Weights By default, the simplicial complexes of $\\alpha$ - and VR filtrations are   \n249 unweighted. However, the weights determine the entries of the harmonic representatives, increasing   \n250 and decreasing the influence of certain simplices and parts of the simplicial complex. We can use this   \n251 observation to increase the robustness of TOPF against the influence of heterogeneous point cloud   \n252 structure, which is present in virtually all real-world data sets. For a complete technical account of   \n253 how and why we do this, see Appendix F. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "254 4 Theoretical guarantees ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "255 In this section, we prove the relationship between TOPF and actual topological structure in datasets: ", "page_idx": 6}, {"type": "text", "text": "256 Theorem 4.1 (Topological Point Features of Spheres). Let $X$ consist of at least $(n+2)$ points   \n257 (denoted by $S$ ) sampled uniformly at random from a unit $n$ -sphere in $\\mathbb{R}^{n+1}$ and an arbitrary number   \n258 of points with distance of at least 2 to $S$ . When we now consider the $\\alpha$ -filtration on this point   \n259 cloud, with probability 1 we have that $(i)$ there exists an $n$ -th persistent homology class generated   \n260 by the 2-simplices on the convex hull hull of $S$ , (ii) the associated unweighted harmonic homology   \n261 representative takes values in $\\{0,\\pm1\\}$ where the 2-simplices on the boundary of the convex hull are   \n262 assigned a value of $\\pm1$ , and (iii) the support of the associated topological point feature (TOPF) ${\\mathcal{F}}_{n}^{*}$   \n263 is precisely $S$ : $\\operatorname{supp}(\\mathcal{F}_{n}^{*})=S$ . (iv) The same holds true for point clouds sampled from multiple   \n264 $n_{i}$ -spheres if the above conditions are met on each individual sphere. ", "page_idx": 6}, {"type": "text", "text": "265 We will give a proof of this theorem in Appendix B. ", "page_idx": 6}, {"type": "text", "text": "266 Remark 4.2. In practice, datasets with topological structure consist in a majority of cases of points   \n267 sampled with noise from deformed $n$ -spheres. The theorem thus guarantees that TOPF will recover   \n268 these structural information in an idealised setting. Experimental evidence suggests that this holds   \n269 under the addition of noise as well which is plausible as harmonic persistent homology is robust   \n270 against some noise [2]. ", "page_idx": 6}, {"type": "text", "text": "271 5 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "272 In this section, we conduct experiments on real world and synthetic data, compare the clustering   \n273 results with clustering by TPCC, other classical clustering algorithms, and other point features, and   \n274 demonstrate the robustness of TOPF against noise.   \n275 Topological Point Cloud Clustering Benchmark We introduce the topological clustering bench  \n276 mark suite (Appendix C) and report running times and the accuracies of clustering based on TOPF   \n277 and other methods and point embeddings, see Table 1. We see that TOPF outperforms all classical   \n278 clustering algorithms on all but one dataset by a wide margin. We also see that TOPF closely matches   \n279 the performance of the only other higher-order topological clustering algorithm, TPCC on two datasets   \n280 with clear topological features, whereas TOPF outperforms TPCC on datasets with more complex   \n281 structure. In addition, TOPF has a consistently lower running time with better scaling for the more ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Table 1: Quantitative performance comparison of clustering with TOPF and other features/clustering algorithms. Four $2D$ and three $3D$ data sets of the topological clustering benchmark suite (Appendix C, cf. Figure 6 for ground truth labels and Figure 7 for clustering results of TOPF). We ran each algorithm 20 times and list the mean adjusted rand index (ARI) with standard deviation $\\sigma$ and mean running time. We omit $\\sigma$ for algorithms with $\\sigma=0$ on every dataset. TOPF consistently outperforms or almost matches the other algorithms while having significantly better run time than the second best performing algorithm TPCC. Spectral Clustering (SC), DBSCAN, and Agglomerative Clustering $(\\mathrm{AgC})$ are standard clustering algorithms, ToMATo is a topological clustering algorithm [11], Geo clusters using 12-dimensional point geometric features extracted by pgeof and the normal point coordinates, whereas node2vec [25] produces node embeddings on a $k$ -nearest neighbour graph built upon an affinity matrix. We highlight all ARI scores within $\\pm0.05$ of the best ARI score. ", "page_idx": 7}, {"type": "table", "img_path": "jwE1dgOox1/tmp/44efdfe510f168ca750add3e9a2476002666d1456eb72f6ba44e8d13868962e0.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "jwE1dgOox1/tmp/d0a142c47a249e41da2f2e15268980883c7aee8f230b650b3eb03990aae277b6.jpg", "img_caption": ["Figure 4: TOPF on $3D$ real-world and synthetic point clouds. For every point, we highlight the largest corresponding topological feature, where colour stands for the different features and saturation for the value of the feature. (a): Atoms of mutated Cys123 of E. coli [29]. We added auxiliary points on the convex hull and considered 2-homology, to detect the protein pockets which are crucial for protein-environment interactions (Cf. [40]). (b): Atoms of NALCN Channelosome [32] display three distinct loops. $(c)$ : Points sampled in the state space of a Lorentz attractor. The two features correspond to the two lobes of the attractor. (d): Point cloud spaceship of our newly introduced topological clustering benchmark suite (See Appendix C). "], "img_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "jwE1dgOox1/tmp/6dd09eb40394d9e17aa50044e67c7a47fb01e74dc19e30909e25e07cbdfbbfc2.jpg", "img_caption": ["Figure 5: Performance of Clustering based on TOPF features in increasing noise/outlier levels with ${\\bf95\\%}$ CI. Left: We add i.i.d. Gaussian noise to every point with standard deviation indicated by the noise parameter. We see that even when compared with TPCC on a data set specifically crafted for TPCC, TOPF requires significantly less information and delivers almost equal performance. When tuned for datasets with a high noise level, the TOPF even outperform TPCC and drastically outperform all classical clustering algorithms. Right: We add outliers with the same standard deviation as the point cloud to the data set. We then measure the adjusted rand index obtained restricted on the original points. We see that even when compared with TPCC on a data set specifically crafted for TPCC, TOPF requires significantly less information and delivers matching to superior performance, significantly outperforming all other classical clustering algorithms. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "282 complex datasets, while also not requiring prior knowledge on the best topological scale. As for the   \n283 other point embeddings, Node2Vec is not able to capture any meaningful topological information,   \n284 whereas the performance of clustering using geometric features depends on the data set.   \n285 Feature Generation In Figure 4, we show qualitatively that TOPF constructs meaningful topological   \n286 features on data sets from Biology and Physics, and synthetic data, corresponding to for example   \n287 rings and pockets in proteins or trajectories around different attractors in dynamical systems. (For   \n288 individual heatmaps see Figure 8)   \n289 Robustness against noise We have evaluated the robustness of TOPF against Gaussian noise on   \n290 the dataset introduced in [24] and compared the results against TPCC, Spectral Clustering, Graph   \n291 Spectral Clustering on the graph constructed by TPCC, and against $k$ -means in Figure 5 Left. We have   \n292 also analysed the robustness of TOPF against the addition of outliers in Figure 5 Right. We see that   \n293 TOPF performs well in both cases, underlining our claim of robustness. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "294 6 Discussion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "295 Limitations TOPF can \u2014 by design \u2014 only produce meaningful output on point clouds with a   \n296 topological structure quantifiable by persistent homology. In practice it is thus desirable to combine   \n297 TOPF with some geometric or other point-level feature extractor. As TOPF relies on the computation of   \n298 persistent homology, its runtime increases on very large point clouds, especially in higher dimensions   \n299 where $\\alpha$ -flitrations are computationally infeasible. However, subsampling, either randomly or using   \n300 landmarks, usually preserves relevant topological features while improving run time [41]. Finally,   \n301 selection of the relevant features is a very hard problem. While our proposed heuristics work well   \n302 across a variety of domains and application scenarios, only domain- and problem-specific knowledge   \n303 makes correct feature selection feasible.   \n304 Future Work The integration of higher-order TOPF features into ML pipelines that require point  \n305 level features potentially leads to many new interesting insights across the domains of biology, drug   \n306 design, graph learning and computer vision. Furthermore, efficient computation of simplicial weights   \n307 leading to the provably most faithful topological point features is an exciting open problem.   \n308 Conclusion We introduced point-level features TOPF founded on algebraic topology relating global   \n309 structural features to local information. We gave theoretical guarantees for the correctness of their   \n310 construction and evaluated them quantitatively and qualitatively on synthetic and real-world data sets.   \n311 Finally, we introduced the novel topological clustering benchmark suite and showed that clustering   \n312 using TOPF outperforms other available clustering methods and features extractors. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "313 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "314 [1] Michael Atiyah. K-theory. CRC press, 1989.   \n315 [2] Saugata Basu and Nathanael Cox. Harmonic persistent homology. In 2021 IEEE 62nd Annual   \n316 Symposium on Foundations of Computer Science (FOCS), pages 1112\u20131123. IEEE, 2022.   \n317 [3] Ulrich Bauer. Ripser: efficient computation of vietoris\u2013rips persistence barcodes. Journal of   \n318 Applied and Computational Topology, 5(3):391\u2013423, 2021.   \n319 [4] Mikhail Belkin and Partha Niyogi. Laplacian eigenmaps for dimensionality reduction and data   \n320 representation. Neural computation, 15(6):1373\u20131396, 2003.   \n321 [5] Katherine Benjamin, Lamisah Mukta, Gabriel Moryoussef, Christopher Uren, Heather A   \n322 Harrington, Ulrike Tillmann, and Agnese Barbensi. Homology of homologous knotted proteins.   \n323 Journal of the Royal Society Interface, 20(201):20220727, 2023.   \n324 [6] A. K. Bousfield. The localization of spaces with respect to homology. Topology, 14(2):133\u2013150,   \n325 1975.   \n326 [7] G.E. Bredon, J.H. Ewing, F.W. Gehring, and P.R. Halmos. Topology and Geometry. Graduate   \n327 Texts in Mathematics. Springer, New York, 1993.   \n328 [8] Peter Bubenik et al. Statistical topological data analysis using persistence landscapes. J. Mach.   \n329 Learn. Res., 16(1):77\u2013102, 2015.   \n330 [9] Gunnar Carlsson and Mikael Vejdemo-Johansson. Topological Data Analysis with Applications.   \n331 Cambridge University Press, 2021.   \n332 [10] Charu Chaudhry, Arthur L Horwich, Axel T Brunger, and Paul D Adams. Exploring the struc  \n333 tural dynamics of the e. coli chaperonin groel using translation-libration-screw crystallographic   \n334 refinement of intermediate states. Journal of molecular biology, 342(1):229\u2013245, 2004.   \n335 [11] Fr\u00e9d\u00e9ric Chazal, Leonidas J. Guibas, Steve Y. Oudot, and Primoz Skraba. Persistence-based   \n336 clustering in riemannian manifolds. J. ACM, 60(6), nov 2013.   \n337 [12] Fr\u00e9d\u00e9ric Chazal and Bertrand Michel. An introduction to topological data analysis: fundamental   \n338 and practical aspects for data scientists. Frontiers in artificial intelligence, 4:108, 2021.   \n339 [13] Yu-Chia Chen and Marina Meila\u02d8. The decomposition of the higher-order homology embedding   \n340 constructed from the k-laplacian. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and   \n341 J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, volume 34,   \n342 pages 15695\u201315709. Curran Associates, Inc., 2021.   \n343 [14] Yu-Chia Chen, Marina Meila\u02d8, and Ioannis G Kevrekidis. Helmholtzian eigenmap: Topological   \n344 feature discovery & edge flow learning from point cloud data. arXiv preprint arXiv:2103.07626,   \n345 2021.   \n346 [15] Ronald R Coifman and St\u00e9phane Lafon. Diffusion maps. Applied and computational harmonic   \n347 analysis, 21(1):5\u201330, 2006.   \n348 [16] Vin De Silva and Mikael Vejdemo-Johansson. Persistent cohomology and circular coordinates.   \n349 In Proceedings of the twenty-fifth annual symposium on Computational geometry, pages 227\u2013   \n350 236, 2009.   \n351 [17] Richard Dedekind. Was sind und was sollen die Zahlen? Verlag Friedrich Vieweg und Sohn,   \n352 Braunschweig, 1888.   \n353 [18] Boris Delaunay et al. Sur la sphere vide. Izv. Akad. Nauk SSSR, Otdelenie Matematicheskii i   \n354 Estestvennyka Nauk, 7(793-800):1\u20132, 1934.   \n355 [19] Stefania Ebli and Gard Spreemann. A notion of harmonic clustering in simplicial complexes.   \n356 In 2019 18th IEEE International Conference On Machine Learning And Applications (ICMLA),   \n357 pages 1083\u20131090, 2019.   \n358 [20] Samuel Eilenberg and Saunders MacLane. General theory of natural equivalences. Transactions   \n359 of the American Mathematical Society, 58:231\u2013294, 1945.   \n360 [21] Charles Fefferman, Sanjoy Mitter, and Hariharan Narayanan. Testing the manifold hypothesis.   \n361 Journal of the American Mathematical Society, 29(4):983\u20131049, Oct 2016.   \n362 [22] David Chin-Lung Fong and Michael Saunders. Lsmr: An iterative algorithm for sparse least  \n363 squares problems. SIAM Journal on Scientific Computing, 33(5):2950\u20132971, 2011.   \n364 [23] Vincent P. Grande and Michael T Schaub. Non-isotropic persistent homology: Leveraging the   \n365 metric dependency of ph. In Learning on Graphs Conference, pages 17\u20131. PMLR, 2023.   \n366 [24] Vincent P. Grande and Michael T. Schaub. Topological point cloud clustering. In Proceedings   \n367 of the 40th International Coference on Machine Learning, ICML\u201923, 2023.   \n368 [25] Aditya Grover and Jure Leskovec. node2vec: Scalable feature learning for networks. In   \n369 Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and   \n370 data mining, pages 855\u2013864, 2016.   \n371 [26] Davide Gurnari, Aldo Guzm\u00e1n-S\u00e1enz, Filippo Utro, Aritra Bose, Saugata Basu, and Laxmi   \n372 Parida. Probing omics data via harmonic persistent homology. arXiv preprint arXiv:2311.06357,   \n373 2023.   \n374 [27] Allen Hatcher. Algebraic Topology. Cambridge University Press, Cambridge, 2002.   \n375 [28] Felix Hausdorff. Grundz\u00fcge einer theorie der geordneten mengen. Mathematische Annalen,   \n376 65:435\u2013505, 1908.   \n377 [29] Esther Hidber, Edward R Brownie, Koto Hayakawa, and Marie E Fraser. Participation of   \n378 cys $123\\alpha$ of escherichia coli succinyl-coa synthetase in catalysis. Acta Crystallographica   \n379 Section D: Biological Crystallography, 63(8):876\u2013884, 2007.   \n380 [30] David Hilbert. Grundlagen der Geometrie. Wissenschaft und Hypothese. B. G. Teubner,   \n381 Leipzig, 1899.   \n382 [31] Sze-tsen Hu. Homotopy theory. Academic press, 1959.   \n383 [32] Marc Kschonsak, Han Chow Chua, Claudia Weidling, Nourdine Chakouri, Cameron L. Noland,   \n384 Katharina Schott, Timothy Chang, Christine Tam, Nidhi Patel, Christopher P. Arthur, Alexan  \n385 der Leitner, Manu Ben-Johny, Claudio Ciferri, Stephan Alexander Pless, and Jian Payandeh.   \n386 Structural architecture of the human nalcn channelosome. Nature, 603(7899):180\u2013186, Mar   \n387 2022.   \n388 [33] Peter Lawson, Andrew B Sholl, J Quincy Brown, Brittany Terese Fasy, and Carola Wenk.   \n389 Persistent homology for the quantitative evaluation of architectural features in prostate cancer   \n390 histology. Scientific reports, 9(1):1139, 2019.   \n391 [34] Lek-Heng Lim. Hodge laplacians on graphs. SIAM Review, 62(3):685\u2013715, 2020.   \n392 [35] Jacob Lurie. Stable infinity categories. arXiv preprint math/0608228, 2006.   \n393 [36] Yunqian Ma and Yun Fu. Manifold learning theory and applications, volume 434. CRC press   \n394 Boca Raton, 2012.   \n395 [37] Facundo M\u00e9moli, Zhengchao Wan, and Yusu Wang. Persistent laplacians: Properties, algorithms   \n396 and implications. SIAM Journal on Mathematics of Data Science, 4(2):858\u2013884, 2022.   \n397 [38] Elizabeth Munch. A user\u2019s guide to topological data analysis. Journal of Learning Analytics,   \n398 4(2):47\u201361, 2017.   \n399 [39] Kevin P. Murphy. Probabilistic Machine Learning: An introduction. MIT Press, 2022.   \n400 [40] Haruhisa Oda, Mayuko Kida, Yoichi Nakata, and Hiroki Kurihara. Novel definition and quantita  \n401 tive analysis of branch structure with topological data analysis. arXiv preprint arXiv:2402.07436,   \n402 2024.   \n403 [41] Jose A. Perea. Sparse circular coordinates via principal $\\mathbb{Z}$ -bundles. In Nils A. Baas, Gunnar E.   \n404 Carlsson, Gereon Quick, Markus Szymik, and Marius Thaule, editors, Topological Data   \n405 Analysis, pages 435\u2013458, Cham, 2020. Springer International Publishing.   \n406 [42] Henri Poincar\u00e9. Analysis situs. J. de l\u2019Ecole Poly., 1, 1895.   \n407 [43] Daniel G. Quillen. Homotopical Algebra, volume 43 of Lecture Notes in Mathematics. Springer,   \n408 Berlin, 1967.   \n409 [44] T Mitchell Roddenberry, Nicholas Glaze, and Santiago Segarra. Principled simplicial neural   \n410 networks for trajectory prediction. In International Conference on Machine Learning, pages   \n411 9020\u20139029. PMLR, 2021.   \n412 [45] Michael T Schaub, Austin R Benson, Paul Horn, Gabor Lippner, and Ali Jadbabaie. Random   \n413 walks on simplicial complexes and the normalized hodge 1-laplacian. SIAM Review, 62(2):353\u2013   \n414 391, 2020.   \n415 [46] Michael T. Schaub, Yu Zhu, Jean-Baptiste Seby, T. Mitchell Roddenberry, and Santiago Segarra.   \n416 Signal processing on higher-order networks: Livin\u2019 on the edge... and beyond. Signal Processing,   \n417 187:108149, 2021.   \n418 [47] Jianbo Shi and Jitendra Malik. Normalized cuts and image segmentation. IEEE Transactions   \n419 on pattern analysis and machine intelligence, 22(8):888\u2013905, 2000.   \n420 [48] The GUDHI Project. GUDHI User and Reference Manual. GUDHI Editorial Board, 2015.   \n421 [49] Tammo tom Dieck. Algebraic topology, volume 8. European Mathematical Society, Z\u00fcrich,   \n422 2008.   \n423 [50] JunJie Wee, Jiahui Chen, Kelin Xia, and Guo-Wei Wei. Integration of persistent laplacian and   \n424 pre-trained transformer for protein solubility changes upon mutation. Computers in Biology   \n425 and Medicine, page 107918, 2024.   \n426 [51] Hui Zou, Trevor Hastie, and Robert Tibshirani. Sparse principal component analysis. Journal   \n427 of computational and graphical statistics, 15(2):265\u2013286, 2006.   \n428 [52] Matija \u02c7Cufar. Ripserer.jl: flexible and efficient persistent homology computation in julia.   \n429 Journal of Open Source Software, 5(54):2614, 2020. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "430 A Extended Background ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "431 A brief history of topology and machine learning Algebraic topology is a discipline of Mathe  \n432 matics dating back roughly to the late $19^{\\mathrm{th}}$ century [42]. Starting with Henri Poincar\u00e9 and continuing   \n433 in the early $\\breve{2}0^{\\mathrm{th}}$ century, the mathematical community became interested in developing a framework   \n434 to capture the global shapes of manifolds and topological spaces in concise algebraic terms. This de  \n435 velopment was partly made possible by the push towards a formalisation of mathematics and analysis,   \n436 in particular, which took place inside the mathematical community in the 1800\u2019s and early 1900\u2019s (e.g.   \n437 [17, 30, 28]). The axiomatisation of analysis in the early $20^{\\mathrm{th}}$ century is an important result of this   \n438 process. These abstract ideas made it possible for Topologists to talk about the now common notions   \n439 of Euler characteristics, Betti number, simplicial homology of manifolds, topological spaces, and   \n440 simplicial and CW complexes. Over the course of the last 100 years, branching into many sub-areas   \n441 like low-dimensional topology, differential topology, K-theory or homotopy theory [1, 31], algebraic   \n442 topology has resolved many of the important questions and provides a comprehensive tool-box for   \n443 the study of topological spaces. These achievements were tied to an abstraction and generalisation of   \n444 concepts: topological spaces turned into spectra, diffeomorphism to homotopy equvialences and later   \n445 weak equivalences, and Topologists turned to category theory [20], model categories [6] and recently   \n446 $\\infty$ -categories [35] as the language of choice.   \n447 The $21^{\\mathrm{st}}$ century saw the advent and rise of topological data analysis (TDA, [8, 12]). In short,   \n448 mathematicians realised that the same notions of shape and topology that their predecessors carefully   \n449 defined a century earlier were now characterising the difference between healthy and unhealthy   \n450 tissue, between normal and abnormal behaviour protein behaviour, or more general between different   \n451 categories in their complex data sets.   \n452 Related Work The intersection of topological data analysis, topological signal processing and   \n453 geometry processing has many interesting related developments in the past few years. On the side   \n454 of homology and TDA, the authors in [16] and [41] use harmonic cohomology representatives to   \n455 reparametrise point clouds based on circular coordinates. This implicitly assumes that the underlying   \n456 structure of the point cloud is amenable to such a characterization. Although circular coordinates are   \n457 orthogonal to the core goal of TOPF, the approaches share many key ideas and insights. In [2, 26],   \n458 the authors develop and use harmonic persistent homology and provide a way to pool features to   \n459 the point-level. However, their focus is not on providing robust topological point features and their   \n460 approach includes no tunable homology feature selection across dimensions, no support for weighted   \n461 simplicial complexes, and they only construct the simplicial complex at birth. In their paper on   \n462 topological mode analysis, [11] use persistent homology to cluster point clouds. However, they only   \n463 consider 0-dimensional homology to base the clustering on densities and there is no clear way to   \n464 generalise this to higher dimensions.   \n465 On the more geometric-centred side, [19] already provide a notion of harmonic clustering on simplices,   \n466 [13, 14] analyse the notion of geometry and topology encoded in the Hodge Laplacian and its relation   \n467 to homology decompositions, [45] study the normalised and weighted Hodge Laplacian in the context   \n468 of random walks, and [24] use the harmonic space of the Hodge Laplacians to cluster point clouds   \n469 respecting topology. Finally, a persistent variant of the Hodge Laplacian is used to study flitrations of   \n470 simplicial complexes [37].   \n471 In [24], the authors have introduced TPCC, the first method to cluster a point cloud based on the   \n472 higher-order topological features encoded in the data set. However, TPCC is (i) computationally   \n473 expensive due to extensive eigenvector computations, (ii) depending on high-dimensional subspace   \n474 clustering algorithms, which are prone to instabilities and errors, (iii) sensitive to the correct choice   \n475 of hyperparameters, (iv) requiring the topological true features and noise to occur in different steps   \n476 of the simplicial filtration, and it (v) solely focussed on clustering the points rather than extracting   \n477 relevant node-level features. This paper solves all the above by completely revamping the TPCC   \n478 pipeline, introducing several new ideas from applied algebraic topology and differential geometry.   \n479 The core insight is: When you have the time to compute persistent homology with generators on a   \n480 data set, you get the topological node features with similar computational effort. ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "481 B Theoretical Considerations ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "482 More details on VR and $\\alpha$ -filtrations Vietoris\u2013Rips complexes are easy to define, approximate   \n483 the topological properties of a point cloud across all scales and computationally easy to implement.   \n484 However for moderately large $r$ , the associated VR complex contains a large number of simplices \u2014   \n485 up to n $n$ -simplices for large enough $r$ \u2014 leading to poor computational performance for any   \n486 downstream task on some large point clouds. One way to see this is the following: After adding the   \n487 first edge that connects two components or the final simplex that flils a hole in the simplicial complex   \n488 the VR complex keeps adding more and more simplices in the same area that keep the topology   \n489 unchanged. One way to mitigate this problem is to pre-compute a set of simplices that are able to   \n490 express the entire topology of the point cloud. For a point cloud $X\\subset\\mathbb{R}^{n}$ , the $\\alpha$ -flitration consists of   \n491 the intersection of the simplicial complexes of the VR filtration on $X$ with the (higher-dimensional)   \n492 Delaunay triangulation of $X$ in $\\mathbb{R}$ . Due to algorithmic reasons, the filtration value of a simplex is   \n493 then the radius of the circumscribed sphere instead of the maximum pair-wise distance of vertices.   \n494 This reduces the number of required simplices across all dimensions to $O(|X|^{\\lceil n/2\\rceil})$ . However, the   \n495 Delaunay triangulation becomes computationally infeasible for larger $n$ .   \n496 Definition B.1 ( $\\mathit{\\check{n}}$ -dimensional Delaunay triangulation). Given a set of vertices $V\\in\\mathbb{R}^{n}$ , a Delaunay   \n497 triangulation $D T(V)$ is a triangulation of $V$ such that for any $n$ -simplex $\\sigma_{n}\\in D T(V)$ the interior   \n498 of the circum-hypersphere of $\\sigma_{n}$ contains no point of $D T(V)$ . A triangulation of $V$ is a SC $\\boldsymbol{S}$ with   \n499 vertex set $V$ such that its geometric realisation covers the convex hull of $V\\operatorname{hull}(V)=|S|$ and we   \n500 have for any two simplices $\\sigma,\\sigma^{\\prime}$ that the intersection of geometric realisations $|\\sigma|\\cap|\\sigma^{\\prime}|$ is either   \n501 empty or the geometric realisation $|\\hat{\\sigma}|$ of a common sub-simplex $\\hat{\\sigma}\\subset\\sigma,\\sigma^{\\prime}$ . ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "502 If $V$ is in general position, the Delaunay triangulation is unique and guaranteed to exist [18]. ", "page_idx": 12}, {"type": "text", "text": "503 Definition B.2 ( $\\alpha$ -complex of a point cloud). Given a finite point cloud $X$ in real space $\\mathbb{R}^{n}$ , the   \n504 $\\alpha$ -complex $\\alpha_{\\varepsilon}(X)$ is the subset of the $n$ -dimensional Delaunay triangulation $D T(X)$ consisting of   \n505 all $\\sigma\\in D T(X)$ with a radius $r$ of its circumscribed sphere with $r\\leq\\varepsilon$ .   \n506 Proof of the main theorem We will now give the proof of the theorem that guarantees that TOPF   \n507 works. First, let us recall Theorem 4.1:   \n508 Theorem 4.1 (Topological Point Features of Spheres). Let $X$ consist of at least $(n+2)$ points   \n509 (denoted by $S$ ) sampled uniformly at random from a unit $n$ -sphere in $\\mathbb{R}^{n+1}$ and an arbitrary number   \n510 of points with distance of at least 2 to $S$ . When we now consider the $\\alpha$ -filtration on this point   \n511 cloud, with probability 1 we have that $(i)$ there exists an $n$ -th persistent homology class generated   \n512 by the 2-simplices on the convex hull hull of $S$ , (ii) the associated unweighted harmonic homology   \n513 representative takes values in $\\{0,\\pm1\\}$ where the 2-simplices on the boundary of the convex hull are   \n514 assigned a value of $\\pm1$ , and (iii) the support of the associated topological point feature (TOPF) ${\\mathcal{F}}_{n}^{*}$   \n515 is precisely $S$ : $\\operatorname{supp}(\\mathcal{F}_{n}^{*})=S$ . (iv) The same holds true for point clouds sampled from multiple   \n516 $n_{i}$ -spheres if the above conditions are met on each individual sphere.   \n517 Proof. Assume that we are in the scenario of the theorem. Now because the $n$ -volume of $(n-1)$ -   \n518 submanifolds is zero, we have that with probability 1 the points of $S$ don\u2019t lie on a single $(n-1)$   \n519 sphere inside the $n$ -sphere. Let us now look at the $\\alpha$ -filtration of the simplices in $S$ : Recall that the   \n520 flitration values of a $k$ -simplex is given by the radius of the $(k-1)$ -sphere determined by its vertices.   \n521 Because all of the $(n+1)$ -simplices $\\sigma_{n+1}$ with vertices $V\\subset S$ in $S$ lie on the same unit $n$ -sphere $S_{n}$ ,   \n522 they all share the flitration value of $\\alpha(\\sigma_{n+1})=1$ . By the same argument as above, with probability 1   \n523 there are no $(n+1)$ points in $S$ that lie on an unit $(n-1)$ -sphere. Thus all of the $n$ -simplices $\\sigma_{n}$ lie   \n524 on $(n-1)$ -spheres $S_{n}$ with a radius $r<1$ smaller than 1 and hence have a filtration value $\\alpha(\\sigma_{n})$   \n525 smaller than 1. Let ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "equation", "text": "$$\nb:=\\operatorname*{max}\\big(\\{\\alpha(\\sigma_{n}):\\sigma_{n}\\subset\\partial\\operatorname{hull}(S)\\}\\big)\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "526 be the maximum flitration value of an $n$ -simplex on the boundary of the convex hull of $S$ . Then, then   \n527 a linear combination $g$ of the $n$ -simplices of the boundary of the convex hull of $S$ with coefficients in   \n528 $\\pm1$ is a generator of a persistent homology class with life time $(b,1)$ (this follows from the fact that   \n529 $n$ -spheres and their triangulations are orientable). This proves claim (i).   \n530 Because of the assumption that all points not contained in $S$ have a distance of at least 2 to the points   \n531 in $S$ , all $(n+1)$ -simplices $\\sigma_{n+1}$ with vertices both in $S$ and its complement in $X$ will have a flitration   \n532 value $\\alpha(\\sigma_{n+1})\\geq1$ of at least 1. Recall that all $(n+1)$ -simplices $\\sigma_{n+1}\\subset S$ with vertices inside $S$   \n533 have a filtration value of $\\alpha(\\sigma_{n+1})=1$ . Thus the adjoint of the $n$ -th boundary operator $B_{n}^{\\top}$ is trivial   \n534 on the homology generator $g$ . Thus, we have that for the $n$ -th Hodge Laplacian ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "equation", "text": "$$\nL_{n}g=\\beta_{n-1}^{\\top}\\beta_{n-1}g+\\beta_{n}\\beta_{n}^{\\top}g=0+0=0\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "535 and hence $g$ is a harmonic generator for the entire filtration range of $(b,1)$ , which proves claim (ii).   \n536 Claim (iii) and (iv) then follow from the construction of the TOPF values. \u25a1 ", "page_idx": 13}, {"type": "text", "text": "537 C Topological Clustering Benchmark Suite ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "538 We introduce seven point clouds for topological point cloud clustering in the topological clustering   \n539 benchmark suite (TCBS). The ground truth and the point clouds are depicted in Figure 6. The point   \n540 clouds represent a mix between 0-, 1- and 2-dimensional topological structures in noiseless and noisy   \n541 settings in ambient 2-dimensional and 3-dimensional space. The results of clustering according to   \n542 TOPF can be found in Figure 7. ", "page_idx": 13}, {"type": "text", "text": "543 D Implementation ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "544 We will release an implementation of TOPF and the code and data required to reproduce   \n545 the experimental results of this paper under https://anonymous.4open.science/r/topf_   \n546 submission-5C40/. In particular, we will release the topological clustering benchmark suite.   \n547 All experiments were run on a Apple M1 Pro chipset with 10 cores and $32\\ \\mathrm{GB}$ memory. TOPF   \n548 and the experiments are implemented in Python and Julia. For persistent homology computations,   \n549 we used GUDHI [48] ( $\\copyright$ The GUDHI developers, MIT license) and Ripserer [52] ( $\\copyright$ mtsch, MIT   \n550 license), which is a modified Julia implementation of [3]. For the least square problems, we used   \n551 the LSMR implementation of SciPy [22]. We used the Node2Vec python implementation https:   \n552 //github.com/eliorc/node2vec ( $\\copyright$ Elior Cohen, MIT License) based on the Node2Vec Paper   \n553 [25]. We used the pgeof Python package for computation of geometric features https://github.   \n554 com/drprojects/point_geometric_features $\\copyright$ Damien Robert, Loic Landrieu, Romain Jan  \n555 vier, MIT license). We use parts of the implementation of TPCC https://git.rwth-aachen.   \n556 de/netsci/publication-2023-topological-point-cloud-clustering ( $\\copyright$ Computational   \n557 Network Science Group, RWTH Aachen University, MIT license). ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "image", "img_path": "jwE1dgOox1/tmp/f5167fc6784968add9efc7dbc5aa2c1f8ed776b417afdd6f8fbb4ff8f1415df2.jpg", "img_caption": ["Figure 6: Data sets of the Topological Clustering Benchmark Suite (TCBS) with true labels. Top: $2D$ data sets. From left to right: 4Spheres (656 points), Ellipses (158 points), Spheres+Grid (866 points), Halved Circle (249 points). Bottom: $3D$ data sets. From left to right: 2Spheres2Circles (4600 points), SphereinCircle (267 points), spaceship (650 points). "], "img_footnote": [], "page_idx": 14}, {"type": "image", "img_path": "jwE1dgOox1/tmp/c0651f41158c65bd7efb34d455c18a3957bd04b9c4e7faefda180a27e6af264c.jpg", "img_caption": ["Figure 7: Data sets of the Topological Clustering Benchmark Suite (TCBS) with labels generated by TOPF. Top: $2D$ data sets. From left to right: 4Spheres (0.81 ARI), Ellipses (0.95 ARI), Spheres+Grid (0.70 ARI), Halved Circle (0.71 ARI). Bottom: $3D$ data sets. From left to right: 2Spheres2Circles (0.94 ARI), SphereinCircle (0.97 ARI), spaceship (0.92 ARI). "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "558 D.1 Hyperparameters ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "559 All the relevant hyperparameters are already mentioned in their respective sections. However, for   \n560 convenience we gather and briefly discuss them in this section. We note that TOPF is robust and   \n561 applicable in most scenarios when using the default parameters without tuning hyperparameters. The   \n562 hyperparameters should more be thought of as an additional way where detailed domain-knowledge   \n563 can enter the TOPF pipeline.   \n564 Maximum Homology Dimension $d$ The maximum homology dimension determines the dimen  \n565 sions of persistent homology the algorithm computes.   \n566 For the choice of the maximum homology degree $d$ to be considered there are mainly three heuristics   \n567 which we will list in decreasing importance (Cf. [24]):   \n568 I. In applications, we usually know which kind of topological features we are interested in, which   \n569 will then determine $d$ . This means that 1-dimensional homology and $d=1$ suffices when we   \n570 are looking at loops of protein chains. On the other hand, if we are working with voids and   \n571 cavities in 3d histological data, we need $d=2$ and thus compute 2-dimensional homology.   \n572 II. Algebraic topology tells us that there are no closed $n$ -dimensional submanifolds of $\\mathbb{R}^{n}$ . Hence   \n573 their top-homology will always vanish and all interesting homological activity will appear for   \n574 $d<n$ .   \n575 III. In the vast majority of cases, the choice will be between $d=1$ or $d=2$ because empirically   \n576 there are virtually no higher-dimensional topological features in practice.   \n578 Thresholding parameter $\\delta$ In step 4 of the algorithm, we normalise and threshold the harmonic   \n579 representatives. After normalising, the entries of the vectors lie in the interval of $[0,1]$ . The   \n580 thresholding parameter $\\delta$ now essentially determines an interval of $[0,\\delta]$ which we will linearly map   \n581 to $[0,1]$ , while mapping all entries above $\\delta$ to 1 as well. This is necessary as most of the entries in   \n582 the vector $e_{k}^{i}$ are very close to 0 with a very small number of entries being close to 1. Without this   \n583 thresholding, TOPF would now be almost entirely determined by these few large values. Thus this   \n584 step limits the maximum possible influence of a single entry. However, because most of the entries of   \n585 $e_{k}^{i}$ are concentrated around 0, small changes in $\\delta$ will not have a large effect and we chose $\\delta=0.07$   \n586 in all our experiments.   \n587 Interpolation coefficient $\\lambda$ The interpolation coefficient $\\lambda\\in[0,1)$ determines whether we build   \n588 our simplicial complexes close to the birth or the death of the relevant homological features at time   \n589 $t=b^{1-\\lambda}d$ . This then in turns controls how localised or smooth the harmonic representative will   \n590 be. In general, the noisier the ground data is the higher we should choose $\\lambda$ . However, TOPFis not   \n591 sensitive to small changes in $\\lambda$ . We have picked $\\lambda=0.3$ for all the quantitative experiments, which   \n592 empirically represents a good choice for a broad range of applications.   \n593 Feature selection factor $\\beta$ Increasing $\\beta$ leads to TOPF preferring to pick a larger number of relevant   \n594 topological features. Without specific domain-knowledge, $\\beta=0$ represents a good choice.   \n595 Feature selection quotients max_total_quot, min_rel_quot, and min_0_ratio These are   \n596 technical hyperparameters controlling the feature selection module of TOPF. For a technical account   \n597 of them, see Appendix E. In most of the cases without domain knowledge, they do not have an effect   \n598 on the performance of TOPF and should be kept at their default values.   \n599 Simplicial Complex Weights Although the simplicial weights are not technically a hyperparameter,   \n600 there are many potential ways to weigh the considers SCs that can highlight or suppress different   \n601 topological and geometric properties. In all our experiments, we use $w_{\\Delta}$ weights discussed in   \n602 Appendix F. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "603 E How to pick the most relevant topological features ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "604 Simplified heuristic The persistent homology $P_{k}$ module in dimension $k$ is given to us as a list of   \n605 pairs of birth and death times $(b_{i}^{k},d_{i}^{k})$ . We can assume these pairs are ordered in non-increasing order   \n606 of the durations $l_{i}^{k}=d_{i}^{k}-b_{i}^{k}$ . This list is typically very long and consists to a large part of noisy   \n607 homological features which vanish right after they appear. In contrast, we are interested in connected   \n608 components, loops, cavities, etc. that persist over a long time, indicating that they are important for   \n609 the shape of the point cloud. Distinguishing between the relevant and the irrelevant features is in   \n610 general difficult and may depend on additional insights on the domain of application. In order to   \n611 provide a heuristic which does not depend on any a-priori assumptions on the number of relevant   \n612 features we pick the smallest quotient $q_{i}^{k}:=l_{i+1}^{k}/l_{i}^{k}>0$ as the point of cut-off $N_{k}:=\\arg\\operatorname*{min}_{i}q_{i}^{k}$ .   \n613 The only underlying assumption of this approach is that the band of \u201crelevant\u201d features is separated   \n614 from the \u201cnoisy\u201d homological features by a drop in persistence.   \n615 Advanced Heuristic However, certain applications have a single very prominent feature, followed   \n616 by a range of still relevant features with significantly smaller life times, that are then followed by   \n617 the noisy features after another drop-off. This then could potentially lead the heuristic to find the   \n618 wrong drop-off. We propose to mitigate this issue by introducing a hyperparameter $\\beta\\in\\mathbb{R}_{>0}$ . We   \n619 then define the $i$ -th importance-drop-off quotient $q_{i}^{k}$ by ", "page_idx": 15}, {"type": "image", "img_path": "jwE1dgOox1/tmp/fa5ac562be5492cdd4d453fbc75445038001f8532624836e64407633f4f89cac.jpg", "img_caption": ["Figure 8: TOPF heatmaps for three proteins. Top left NALCN channelosome [32] Top right: Mutated Cys123 of E. coli [29], with convex hull added during computation, only 2-dimensional homology features Bottom: GroEL of E. coli [10] (Selected features). "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "equation", "text": "$$\nq_{i}^{k}:=l_{i+1}^{k}/l_{i}^{k}\\left(1+\\beta/i\\right).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "620 The basic idea is now to consider the most significant $N_{k}$ homology classes in dimension $k$ when   \n621 setting $N_{k}$ to be ", "page_idx": 16}, {"type": "equation", "text": "$$\nN_{k}:=\\arg\\operatorname*{min}_{i}q_{i}^{k}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "622 Increasing $\\beta$ leads the heuristic to prefer selections with more features than with fewer features.   \n623 Empirically, we still found $\\beta=0$ to work well in a broad range of application scenarios and used   \n624 it throughout all experiments. There are only a few cases where domain-specific knowledge could   \n625 suggest picking a larger $\\beta$ .   \n626 To catch edge cases with multiple steep drops or a continuous transition between real features and   \n627 noise, we introduce two more checks: We allow a minimal $q_{i}^{k}$ of $\\mathrm{min\\_re1\\_{9}u o t\\;=\\;0.1}$ and a   \n628 maximal quotient $q_{1}^{h}/q_{i}^{k}$ of max_total_quot $=10$ between any homology dimensions. Because   \n629 features in 0-dimensional homology are often more noisy than features in higher dimensions, we add   \n630 a minimum zero-dimensional homology ratio of min_0_ratio $=5$ , i.e. every chosen 0-dimensional   \n631 feature needs to be at least min_0_ratio more persistent then the minimum persistence of the   \n632 higher-dimensional features. Because these hyperparameters only deal with the edge cases of   \n633 feature selection, TOPF is not very sensitive to them. For all our experiments, we used the above   \n634 hyperparameters. We advise to change them only in cases where one has in-depth domain knowledge   \n635 about the nature of relevant topological features. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "image", "img_path": "jwE1dgOox1/tmp/5cf521d45e84cb25b1e70b08d7f1f5a1c034851cf53ee36a165e3b9fa15c351f.jpg", "img_caption": ["Figure 9: Effect of weighing a simplicial complex on harmonic representatives. Top: VR complex. Bottom: $\\alpha$ -complex Left: The base point cloud with different densities. $2^{n d}$ Left: Unweighted harmonic homology representative of the large loop. $3^{r d}$ Right: Effective resistance of the 1-simplices. $3^{r d}$ Right: Harmonic homology representative of the complex weighted by effective resistance. $2^{n d}$ Right: Inverse of number of incident triangles (Definition F.1). Right: Harmonic homology representative of the complex weighted by number of incident triangles. Up to a small threshold, the standard harmonic representative in the VR complex is almost exclusively supported in the low-density regions of the simplicial complex. This leads to poor and unpredictable classification performance in downstream tasks. In contrast, the harmonic homology representative of the weighted VR complex has a more homogenous support along the loop, while still being able to discriminate the edges not contributing to the loop. The $\\alpha$ -complex suffers less from this phenomenon (at least in dimension 2), and hence reweighing is not necessarily required. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "636 F Simplicial Weights ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "637 In an ideal world, the harmonic eigenvectors in dimension $k$ would be vectors assigning $\\pm1$ to all   \n638 $k$ -simplices contributing to $k$ -dimensional homological feature, a 0 to all $k$ -simplices not contributing   \n639 or orthogonal to the feature, and a value in $(-1,1)$ for all simplices based on the alignment of the   \n640 simplex with the boundary of the void. However, this is not the case: In dimension 1, we can for   \n641 example imagine a total flow of 1 circling around the hole. This flow is then split up between all   \n642 parallel edges which means $t w o$ things: I Edges where the loop has a larger diameter have smaller   \n643 harmonic values than edges in thin areas and $\\mathbf{II}$ in VR complexes, which are the most frequently   \n644 used simplicial complexes in TDA, edges in areas with a high point density have smaller harmonic   \n645 values than edges in low-density areas. Point $\\mathbf{II}$ is another advantage of $\\alpha$ -complexes: The expected   \n646 number of simplices per point does not scale with the point density in the same way as it does in the   \n647 VR complex, because only the simplices of the Delaunay triangulation can appear in the complex.   \n648 We address this problem by weighing the $k$ -simplices of the simplicial complex. The idea behind this   \n649 is to weigh the simplicial complex in such a way that it increases and decreases the harmonic values   \n650 of some simplices in an effort to make the harmonic eigenvectors more homogeneous. For weights   \n651 $w\\in\\mathbb{R}^{S_{k}}$ , $\\bar{W}=\\mathrm{diag}(w)$ , the symmetric weighted Hodge Laplacian [45] takes the form of ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "equation", "text": "$$\nL_{k}^{w}=W^{1/2}\\mathcal{B}_{k-1}\\mathcal{B}_{k-1}^{\\top}W^{1/2}+W^{-1/2}\\mathcal{B}_{k}\\mathcal{B}_{k}^{\\top}W^{-1/2}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "652 Because we want the homology representative to lie in the weighted gradient space, we have to scale   \n653 its entries with the weight and set $\\bar{e}_{k,w}^{i}:=W^{-1/2}e_{k}^{i}$ . With this, we have that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathcal{B}_{k-1}^{\\top}W^{1/2}e_{k,w}^{i}=\\mathcal{B}_{k-1}^{\\top}W^{1/2}W^{-1/2}e_{k}^{i}=\\mathcal{B}_{k-1}^{\\top}e_{k}^{i}=0\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "654 We propose two options to weigh the simplicial complex. The first option is to weigh a $k$ -simplex by   \n655 the square of the number of $k+1$ -simplices the simplex is contained in: ", "page_idx": 18}, {"type": "equation", "text": "$$\nw_{\\Delta}(\\sigma_{k})=1/(|\\{\\sigma_{k+1}\\in S_{k+1}^{t}:\\sigma_{k}\\subset\\sigma_{k+1}\\}|+1)^{2}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "656 where the $+1$ is to enforce good behaviour at simplices that are not contained in any higher-order   \n657 simplices. One of the advantages of the $\\alpha$ -complex is that we don\u2019t have large concentrations of   \n658 simplices in well-connected areas. The proposed weighting $w_{\\Delta}$ is computationally straightforward,   \n659 as it can be obtained as the column sums of the absolute value of the boundary matrix $|\\beta_{k}|$ . The   \n660 weights also deal with the previously mentioned problem $\\mathbf{II}$ : As the homology representative is scaled   \n661 inversely to the weight vector $w$ , the simplices in high-density regions will be assigned a low weight   \n662 and thus their weighted homology representative will have a larger entry. By the projection to the   \n663 orthogonal complement of the curl space, this large entry is then diffused among the high-density   \n664 region of the SC with many simplices, whereas the lower entries of the simplices in low-density   \n665 regions are only diffused among fewer adjacent simplices.   \n666 However, the first weight is not able to incorporate the number of parallel simplices into the weighting.   \n667 This is why we propose a second simplicial weight function based on generalised effective resistance.   \n668 Definition F.1 (Effective Hodge resistance weights). For a simplicial complex $\\boldsymbol{S}$ with boundary   \n669 matrices $(\\boldsymbol{B}_{k})$ , we define the effective Hodge resistance weights $w_{R}$ on $k$ -simplices to be: ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "equation", "text": "$$\nw_{R}:=\\mathrm{diag}\\left(\\mathcal{B}_{k-1}^{+}\\mathcal{B}_{k-1}\\right)^{2}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "670 where $\\mathrm{diag(-)}$ denotes the vector of diagonal entries and $(-)^{+}$ denotes taking the Moore\u2013Penrose   \n671 inverse.   \n672 Intuitively for $k=1$ , we can assume that every edge has a resistance of 1 and then the effective   \n673 resistance coincides with the notion from Physics. Thus simplices with many parallel simplices are   \n674 assigned a small effective resistance, whereas simplices with few parallel simplices are assigned an   \n675 effective resistance close to 1. However, computing the Moore\u2013Penrose inverse is computationally   \n676 expensive and only feasible for small simplicial complexes.   \n677 In Figure 9, we show that the weights $w_{\\Delta}$ are a good approximation of the effective resistance in   \n678 terms of the resulting harmonic representative. The standard form of TOPF used in all experiments   \n679 uses $w_{\\Delta}$ -weights. ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "680 G Limitations ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "681 Topological features are not everywhere The proposed topological point features take relevant   \n682 persistent homology generators and turn these into point-level features. As such, applying TOPF   \n683 only produces meaningful results on point clouds that have a topological structure. On these point   \n684 clouds, TOPF can extract structural information unobtainable by non-topological methods. Although   \n685 TDA has been successful in a wide range of applications, a large number of data sets does not   \n686 possess a meaningful topological structure. Applying TOPF in these cases will produce no additional   \n687 information. Other data sets require pre-processing before containing topological features. In Figure 4   \n688 left, the $2d$ topological features characterising protein pockets of Cys123 only appear after artificially   \n689 adding points sampled on the convex hull of the point cloud (Cf [40]).   \n690 Computing persistent homology can be computationally expensive As TOPF relies on the   \n691 computation of persistent homology including homology generators, its runtime increases on very   \n692 large point clouds. This is especially true when using VR instead of $\\alpha$ -filtrations, which become   \n693 computationally infeasible for higher-dimensional point clouds. Persistent homology computations   \n694 for dimensions above 2 are only feasible for very small point clouds. Because virtually all discovered   \n695 relevant homological features in applications appear in dimension 0, 1, or 2, this does not present   \n696 a large problem. Despite these computational challenges, subsampling, either randomly or using   \n697 landmarks, usually preserves relevant topological features and thus extends the applicability of TDA   \n698 in general and TOPF even to very large point clouds.   \n699 Automatic feature selection is difficult without domain knowledge While the proposed heuristics   \n700 works well across a variety of domains and application scenarios, only domain- and problem-specific   \n701 knowledge makes truthful feature selection feasible.   \n702 Experimental Evaluation There are no benchmark sets for topological point features in the   \n703 literature, which makes benchmarking TOPF not straightforward. On the level of clustering, we   \n704 introduced the topological clustering benchmark suite to make quantitative comparisons of TOPF   \n705 possible, and benchmarked TOPF on some of the point clouds of [24]. On both the level of point   \n706 features and real-world data sets, it is however hard to establish what a ground truth of topological   \n707 features would mean. Instead we chose to qualitatively report the results of TOPF on proteins and   \n708 real-world data, see Figure 4. ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "709 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "10 1. Claims   \n711 Question: Do the main claims made in the abstract and introduction accurately reflect the   \n712 paper\u2019s contributions and scope?   \n13 Answer: [Yes]   \n714 Justification: The claims about TOPF are supported by the theoretical background in Section 2   \n715 and Section 3, quantitatively and qualitatively validated and benchmarked in Section 5.   \n716 Furthermore, a theoretical guarantee can be found in Section 4.   \n717 Guidelines:   \n718 \u2022 The answer NA means that the abstract and introduction do not include the claims   \n719 made in the paper.   \n720 \u2022 The abstract and/or introduction should clearly state the claims made, including the   \n721 contributions made in the paper and important assumptions and limitations. A No or   \n22 NA answer to this question will not be perceived well by the reviewers.   \n723 \u2022 The claims made should match theoretical and experimental results, and reflect how   \n724 much the results can be expected to generalize to other settings.   \n725 \u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals   \n726 are not attained by the paper.   \n727 2. Limitations   \n728 Question: Does the paper discuss the limitations of the work performed by the authors?   \n29 Answer: [Yes]   \n730 Justification: We believe that being open about limitations is crucial for the practice of   \n731 doing good Science. We briefly discuss the main limitations in ??, and talk in detail about   \n732 limitations in Appendix G. Finally, we are open about limitations when talking about the   \n733 theoretical background and the algorithm in Section 2 and Section 3 and the remark in   \n734 Section 4.   \n735 Guidelines:   \n736 \u2022 The answer NA means that the paper has no limitation while the answer No means that   \n37 the paper has limitations, but those are not discussed in the paper.   \n738 \u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n739 \u2022 The paper should point out any strong assumptions and how robust the results are to   \n740 violations of these assumptions (e.g., independence assumptions, noiseless settings,   \n741 model well-specification, asymptotic approximations only holding locally). The authors   \n742 should reflect on how these assumptions might be violated in practice and what the   \n743 implications would be.   \n744 \u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was   \n745 only tested on a few datasets or with a few runs. In general, empirical results often   \n746 depend on implicit assumptions, which should be articulated.   \n747 \u2022 The authors should reflect on the factors that influence the performance of the approach.   \n748 For example, a facial recognition algorithm may perform poorly when image resolution   \n749 is low or images are taken in low lighting. Or a speech-to-text system might not be   \n750 used reliably to provide closed captions for online lectures because it fails to handle   \n751 technical jargon.   \n752 \u2022 The authors should discuss the computational efficiency of the proposed algorithms   \n753 and how they scale with dataset size.   \n754 \u2022 If applicable, the authors should discuss possible limitations of their approach to   \n755 address problems of privacy and fairness.   \n756 \u2022 While the authors might fear that complete honesty about limitations might be used by   \n757 reviewers as grounds for rejection, a worse outcome might be that reviewers discover   \n758 limitations that aren\u2019t acknowledged in the paper. The authors should use their best   \n759 judgment and recognize that individual actions in favor of transparency play an impor  \n760 tant role in developing norms that preserve the integrity of the community. Reviewers   \n761 will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 20}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 21}, {"type": "text", "text": "Justification: We provide a full set of assumptions for the theorem in Section 4 and a complete proof in Appendix B. We give references for all cited propositions and theorems exceeding basic common mathematical knowledge. ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 21}, {"type": "text", "text": "780 4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We list all the steps necessary to reproduce TOPF in Section 3, Appendix E, Appendix F and talk in detail about the hyperparameter choices in Appendix D.1. Furthermore, we will both release the Topological Clustering Benchmark Suite and the code necessary to reproduce all experiments in this paper. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). ", "page_idx": 21}, {"type": "text", "text": "816 (d) We recognize that reproducibility may be tricky in some cases, in which case   \n817 authors are welcome to describe the particular way they provide for reproducibility.   \n818 In the case of closed-source models, it may be that access to the model is limited in   \n819 some way (e.g., to registered users), but it should be possible for other researchers   \n820 to have some path to reproducing or verifying the results. ", "page_idx": 22}, {"type": "text", "text": "21 5. Open access to data and code ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We will release the full code necessary to reproduce all experimental results of this paper. Furthermore, we will release the topological clustering benchmark suite to the public. ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 22}, {"type": "text", "text": "849 6. Experimental Setting/Details ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We do not train neural networks in this paper. However, we will release the topological clustering benchmark suite. We talk in detail about how to reproduce the algorithm and the relevant choices of hyperparameters, and how we evaluate the experiments. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 22}, {"type": "text", "text": "63 7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "864 Question: Does the paper report error bars suitably and correctly defined or other appropriate   \n865 information about the statistical significance of the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "867 Justification: We provide standard deviations where applicable in Table 1, unless the   \n868 standard deviation is 0, which we talk about in the caption of the table. In Figure 5 we give   \n869 a confidence interval for all the experiments.   \n870 Guidelines:   \n871 \u2022 The answer NA means that the paper does not include experiments.   \n872 \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confi  \n873 dence intervals, or statistical significance tests, at least for the experiments that support   \n874 the main claims of the paper.   \n875 \u2022 The factors of variability that the error bars are capturing should be clearly stated (for   \n876 example, train/test split, initialization, random drawing of some parameter, or overall   \n877 run with given experimental conditions).   \n878 \u2022 The method for calculating the error bars should be explained (closed form formula,   \n879 call to a library function, bootstrap, etc.)   \n880 \u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n881 \u2022 It should be clear whether the error bar is the standard deviation or the standard error   \n882 of the mean.   \n883 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should   \n884 preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis   \n885 of Normality of errors is not verified.   \n886 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or   \n887 figures symmetric error bars that would yield results that are out of range (e.g. negative   \n888 error rates).   \n889 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how   \n890 they were calculated and reference the corresponding figures or tables in the text.   \n891 8. Experiments Compute Resources   \n892 Question: For each experiment, does the paper provide sufficient information on the com  \n893 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n894 the experiments?   \n895 Answer: [Yes]   \n896 Justification: We list the hardware used in Appendix D and list the required running times in   \n897 our quantitative experiments, see Table 1. Because we did not train neural networks, the   \n898 results are easily reproducible on any PC in reasonable time.   \n899 Guidelines:   \n900 \u2022 The answer NA means that the paper does not include experiments.   \n901 \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,   \n902 or cloud provider, including relevant memory and storage.   \n903 \u2022 The paper should provide the amount of compute required for each of the individual   \n904 experimental runs as well as estimate the total compute.   \n905 \u2022 The paper should disclose whether the full research project required more compute   \n906 than the experiments reported in the paper (e.g., preliminary or failed experiments that   \n907 didn\u2019t make it into the paper).   \n908 9. Code Of Ethics   \n909 Question: Does the research conducted in the paper conform, in every respect, with the   \n910 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n911 Answer: [Yes]   \n912 Justification: We have reviewed the NeurIPS Ethics guidelines to make sure our research   \n913 complies with them. (It does comply.)   \n914 Guidelines:   \n915 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n916 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n917 deviation from the Code of Ethics. ", "page_idx": 23}, {"type": "text", "text": "\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid", "page_idx": 24}, {"type": "text", "text": "19 eration due to laws or regulations in their jurisdiction).   \n920 10. Broader Impacts   \n921 Question: Does the paper discuss both potential positive societal impacts and negative   \n922 societal impacts of the work performed?   \n923 Answer: [NA]   \n924 Justification: As the paper is of foundational nature, we do not foresee any direct societal   \n925 impacts.   \n926 Guidelines:   \n927 \u2022 The answer NA means that there is no societal impact of the work performed.   \n928 \u2022 If the authors answer NA or No, they should explain why their work has no societal   \n929 impact or why the paper does not address societal impact.   \n30 \u2022 Examples of negative societal impacts include potential malicious or unintended uses   \n931 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n932 (e.g., deployment of technologies that could make decisions that unfairly impact specific   \n933 groups), privacy considerations, and security considerations.   \n934 \u2022 The conference expects that many papers will be foundational research and not tied   \n935 to particular applications, let alone deployments. However, if there is a direct path to   \n936 any negative applications, the authors should point it out. For example, it is legitimate   \n937 to point out that an improvement in the quality of generative models could be used to   \n938 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n939 that a generic algorithm for optimizing neural networks could enable people to train   \n40 models that generate Deepfakes faster.   \n941 \u2022 The authors should consider possible harms that could arise when the technology is   \n42 being used as intended and functioning correctly, harms that could arise when the   \n43 technology is being used as intended but gives incorrect results, and harms following   \n44 from (intentional or unintentional) misuse of the technology.   \n45 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n46 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n47 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n48 feedback over time, improving the efficiency and accessibility of ML).   \n953 Answer: [NA]   \n954 Justification: We do not foresee any such risks.   \n955 Guidelines:   \n971 Justification: We credit the creators and owners of code used in the model, and state the   \n972 licenses.   \n973 Guidelines:   \n974 \u2022 The answer NA means that the paper does not use existing assets.   \n975 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n976 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n977 URL.   \n978 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n979 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n980 service of that source should be provided.   \n981 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n982 package should be provided. For popular datasets, paperswithcode.com/datasets   \n983 has curated licenses for some datasets. Their licensing guide can help determine the   \n984 license of a dataset.   \n985 \u2022 For existing datasets that are re-packaged, both the original license and the license of   \n986 the derived asset (if it has changed) should be provided.   \n987 \u2022 If this information is not available online, the authors are encouraged to reach out to   \n988 the asset\u2019s creators.   \n989 13. New Assets   \n990 Question: Are new assets introduced in the paper well documented and is the documentation   \n991 provided alongside the assets?   \n992 Answer: [Yes]   \n993 Justification: The code and benchmark suite which we will release with the paper are   \n994 described and documented in the paper.   \n995 Guidelines:   \n996 \u2022 The answer NA means that the paper does not release new assets.   \n997 \u2022 Researchers should communicate the details of the dataset/code/model as part of their   \n998 submissions via structured templates. This includes details about training, license,   \n999 limitations, etc.   \n1000 \u2022 The paper should discuss whether and how consent was obtained from people whose   \n1001 asset is used.   \n1002 \u2022 At submission time, remember to anonymize your assets (if applicable). You can either   \n1003 create an anonymized URL or include an anonymized zip file.   \n1004 14. Crowdsourcing and Research with Human Subjects   \n1005 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n1006 include the full text of instructions given to participants and screenshots, if applicable, as   \n1007 well as details about compensation (if any)?   \n1008 Answer: [NA]   \n1009 Justification: This paper does not involve crowdsourcing nor research with human subjects.   \n1010 Guidelines:   \n1011 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n1012 human subjects.   \n1013 \u2022 Including this information in the supplemental material is fine, but if the main contribu  \n1014 tion of the paper involves human subjects, then as much detail as possible should be   \n1015 included in the main paper.   \n1016 \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation,   \n1017 or other labor should be paid at least the minimum wage in the country of the data   \n1018 collector.   \n1019 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "Subjects ", "page_idx": 25}, {"type": "text", "text": "1021 Question: Does the paper describe potential risks incurred by study participants, whether   \n1022 such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)   \n1023 approvals (or an equivalent approval/review based on the requirements of your country or   \n1024 institution) were obtained?   \n1025 Answer: [NA]   \n1026 Justification: This paper does not involve crowdsourcing nor research with human subjects.   \n1027 Guidelines:   \n1028 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n1029 human subjects.   \n1030 \u2022 Depending on the country in which research is conducted, IRB approval (or equivalent)   \n1031 may be required for any human subjects research. If you obtained IRB approval, you   \n1032 should clearly state this in the paper.   \n1033 \u2022 We recognize that the procedures for this may vary significantly between institutions   \n1034 and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the   \n1035 guidelines for their institution.   \n1036 \u2022 For initial submissions, do not include any information that would break anonymity (if   \n1037 applicable), such as the institution conducting the review. ", "page_idx": 26}]