[{"figure_path": "taI8M5DiXj/figures/figures_8_1.jpg", "caption": "Figure 1: Synthetic Data: Policy regret, lower policy regret is better. x-axis is levels of hidden confounding according to the MSM model. The true Ao is reported as a black vertical line. Human's Policy is the human expert's choices (A) as observed in the data. CRLogit Policy [Kallus and Zhou, 2020]: learn a policy with an IPW approach under hidden confounding, without deferral. ConfHAI Policy [Gao and Yin, 2023] similarly learns a policy with IPW approach under hidden confounding with deferral. CARED: our proposed method Pessimistic Policy and B-Learner Policy are based on CAPO bounds from the B-Learner [Oprescu et al., 2023] and are defined in Equation (2) and Equation (1), respectively. Oracle Policy assigns the best true treatment to each patient.", "description": "The figure shows the result of a synthetic data experiment comparing the proposed CARED policy to several baselines under varying levels of hidden confounding.  The x-axis represents the amount of hidden confounding, and the y-axis represents the policy regret (lower is better). CARED consistently outperforms all other methods, especially for a wide range of confounding levels. The plot illustrates the robustness of the proposed method compared to baselines that are sensitive to accurate specification of the confounding level.", "section": "7.1 Synthetic data"}, {"figure_path": "taI8M5DiXj/figures/figures_9_1.jpg", "caption": "Figure 1: Synthetic Data: Policy regret, lower policy regret is better. x-axis is levels of hidden confounding according to the MSM model. The true Ao is reported as a black vertical line. Human's Policy is the human expert's choices (A) as observed in the data. CRLogit Policy [Kallus and Zhou, 2020]: learn a policy with an IPW approach under hidden confounding, without deferral. ConfHAI Policy [Gao and Yin, 2023] similarly learns a policy with IPW approach under hidden confounding with deferral. CARED: our proposed method Pessimistic Policy and B-Learner Policy are based on CAPO bounds from the B-Learner [Oprescu et al., 2023] and are defined in Equation (2) and Equation (1), respectively. Oracle Policy assigns the best true treatment to each patient.", "description": "This figure displays the results of a synthetic data experiment comparing different policy learning methods under varying levels of hidden confounding (A).  Lower policy regret indicates better performance. The x-axis represents the MSM parameter, and the true value (A0) is marked.  Several baselines are included: the human expert's policy, an IPW approach without deferral (CRLogit), an IPW approach with deferral (ConfHAI), and policies derived from CAPO bounds. The oracle policy (optimal) provides a benchmark.  CARED shows superior performance across various A values.", "section": "7.1 Synthetic data"}, {"figure_path": "taI8M5DiXj/figures/figures_9_2.jpg", "caption": "Figure 2: IHDP Hidden Confounding: Figure 2a shows policy value for different levels of allowed hidden confounding in the data according to the MSM model Assumption 1. The x-axis represents different values of the uncertainty parameter A, and the true Ao is reported as a black vertical line. Figure 2b shows policy value for different rates of deferral. The x-axis represents different levels of practitioner caution by varying the percentage of recommendations deferred. The methods shown here are the same as in Figure 1, in addition to Random Deferral Policy that defers a randomly chosen fraction of samples to the expert at each deferral rate.", "description": "This figure displays the results of the IHDP Hidden Confounding experiment.  Figure 2a shows how the policy value of different methods (CARED, ConfHAI, CRLogit, etc.) changes with different levels of hidden confounding (parameter A). Figure 2b illustrates the relationship between policy value and deferral rate across multiple methods, demonstrating the trade-offs between machine learning and expert input.", "section": "7.2 IHDP Hidden Confounding"}, {"figure_path": "taI8M5DiXj/figures/figures_13_1.jpg", "caption": "Figure 3: No overlap between CAPOs intervals visualization", "description": "This figure illustrates two scenarios in the context of CAPO (Conditional Average Potential Outcome) intervals.  The top panel shows an example where the CAPO intervals for treatments 0 and 1 do not overlap. The bottom panel depicts the general case, again highlighting non-overlapping intervals. The figure serves to visually represent the conditions under which the proposed cost function's behavior is straightforward in guiding the model towards the optimal action and when deferral to an expert would be favored. These scenarios are discussed in Appendix A.", "section": "A Action Costs"}, {"figure_path": "taI8M5DiXj/figures/figures_14_1.jpg", "caption": "Figure 4: CAPOs intervals overlap visualization", "description": "This figure visualizes two examples where the CAPO (Conditional Average Potential Outcome) intervals overlap.  It illustrates how the overlap affects the cost calculations in the CARED (Causal Action Recommendation with Expert Deferral) model, specifically highlighting the scenarios where the model will choose to defer the decision to an expert or take action itself based on cost-sensitive analysis. The figure aids understanding of how the algorithm handles uncertainty in outcome estimations.", "section": "A.2 CAPOs intervals overlap"}]