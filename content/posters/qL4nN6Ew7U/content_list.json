[{"type": "text", "text": "Fantasy: Transformer Meets Transformer in Text-to-Image Generation ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 We present Fantasy, an efficient text-to-image generation model marrying the   \n2 decoder-only Large Language Models (LLMs) and transformer-based masked im  \n3 age modeling (MIM). While diffusion models are currently in a leading position in   \n4 this task, we demonstrate that with appropriate training strategies and high-quality   \n5 data, MIM can also achieve comparable performance. By incorporating pre-trained   \n6 decoder-only LLMs as the text encoder, we observe a significant improvement in   \n7 text fidelity compared to the widely used CLIP text encoder, enhancing the text  \n8 image alignment. Our training approach involves two stages: 1) large-scale concept   \n9 alignment pre-training, and 2) fine-tuning with high-quality instruction-image data.   \n10 Evaluations on FID, HPSv2 benchmarks, and human feedback demonstrate the   \n11 competitive performance of Fantasy against state-of-the-art diffusion and autore  \n12 gressive models. ", "page_idx": 0}, {"type": "text", "text": "13 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "14 Recent advances in text-to-image (T2I) models [3, 5, 12]   \n15 have become focal points within the computer vision field.   \n16 Most advances in T2I models, focused on generating high  \n17 quality images based on relatively short descriptions, strug  \n18 gle with intricate long-text semantic alignment due to in  \n19 herent structure constraints and data limitations. Text   \n20 encoders used for T2I fall into three categories: CLIP   \n21 [30], encoder-decoder LLMs, and decoder-only LLMs.   \n22 Models using encoder-decoder LLMs like T5-XXL [31]   \n23 have shown improved text-image alignment over CLIP   \n24 by exploiting enhanced text understanding, increasing to  \n25 ken capacity, yet without delving into the semantic align  \n26 ment for longer texts. ParaDiffusion [43] indicates that   \n27 directly aligning text embeddings with visual features with  \n28 out prior image-text knowledge is not the most effective   \n29 approach. Previous works [38, 45] have highlighted short  \n30 comings in existing text-image datasets [37], including   \n31 image-text mismatches, a lack of informative content, and   \n32 a pronounced long-tail effect. These deficiencies notably   \n33 impair training efficiency for T2I models and restrict their   \n34 ability to learn complex semantic alignment.   \n35 Existing diffusion-based T2I models [33, 5, 9, 26] have achieved unprecedented quality. However,   \n36 as detailed in Fig. 1, these advanced models come with significant computational demands. The   \n37 considerable expenses of these models create significant barriers for researchers and entrepreneurs.   \n38 Meanwhile, economical text-to-image models [25, 15, 48] compromise on image quality, yielding   \n39 lower resolution and diminished aesthetic appeal.   \n40 Given these challenges, a pivotal question arises: Can we develop a resource-efficient, high-quality   \n41 image generator for long instructions? In this paper, we present Fantasy, significantly reducing   \n42 training demands while maintaining the capability of instruction understanding and competitive   \n43 image generation quality, as shown in Fig. 2. To achieve this, we propose three core designs:   \n44 Efficient T2I netwrok. To leverage the powerful understanding ability of a decoder-only LLM,   \n45 we choose the lightweight Phi-2 [24] as our text encoder. We derive discrete image tokens from a   \n46 pre-trained VQGAN [27], and employ Transformer-based masked image modeling (MIM) as our T2I   \n47 architecture. We also utilize the pre-trained VQGAN decoder [27] for pixel space restoration.   \n48 Hierarchical Training strategy. We propose a thoughtfully two-stage training strategy to address the   \n49 high computational demands of current leading models while maintaining competitive performance:   \n50 (1) large-scale concept alignment pre-training, (2) high-quality instruction-image fine-tuning. To   \n51 facilitate a coarse image-text alignment, we initially train the T2I model from scratch using relatively   \n52 lower-quality data. We then fine-tune the pre-trained T2I model and LLM on text-image pair data   \n53 rich in information density with superior aesthetic quality.   \n54 High-quality data. To achieve rough alignment while pre-training, we select the large-scale dataset   \n55 LAION-2B [37] and employ the filtering strategy proposed by DataComp [14]. We collect long  \n56 text prompts and corresponding high-quality synthesized images for instruction tuning, including   \n57 DiffusionDB [42] and JourneyDB [39]. We further fliter and discard texts with special characters and   \n58 data containing violence or pornography, retaining only instructions exceeding 30 words. ", "page_idx": 0}, {"type": "image", "img_path": "qL4nN6Ew7U/tmp/b0b5bd76fdeeb5c205c805f4a8df24b81f48b8b89b0cb778ab1f3ca9a5107c0e.jpg", "img_caption": ["Figure 1: Comparison of data usage, training time and image quality. Colors from dark to light represent parameters increasing in size, and circles from small to large indicate improvements in image quality. "], "img_footnote": [], "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "image", "img_path": "qL4nN6Ew7U/tmp/99e130a46d1c34fc0507f88e3f765c470c607e5fe36e944f24ade6c78e5d733d.jpg", "img_caption": ["Figure 2: Samples produced by Fantasy $(512\\times512)$ . Each image, generated in 1.26 seconds (without super-resolution models), is accompanied by a descriptive caption showcasing diverse styles and comprehension. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "59 Our main contributions are summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "60 1. We present Fantasy, a novel framework that is the first to integrate a lightweight decoder-only   \n61 LLM and a Transformer-based MIM for text-to-image synthesis, allowing for long-form   \n62 text alignment.   \n63 2. We show that our two-stage training strategy with high-quality data enables MIM to achieve   \n64 comparable performance at a significantly reduced training cost.   \n65 3. We provide comprehensive validation of the model\u2019s efficacy based on automated metrics   \n66 and human feedback for visual appeal and text faithfulness. ", "page_idx": 1}, {"type": "image", "img_path": "qL4nN6Ew7U/tmp/15733758a4a4ceeb8ff972b0fbbae47e0afe51b27efaed39be1d688906735671.jpg", "img_caption": ["Figure 3: (Up) Overview of Fantasy featuring text encoder, VQGAN (encoder $\\mathcal{E}$ and decoder $\\mathcal{D}$ ), masked image generator $\\mathcal{G}$ , and super-resolution model. (Down) Our training pipeline involves two stages. The red parts are trainable and the blue parts are frozen; the yellow part is optionally utilized during inference. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "67 2 Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "68 2.1 Problem Formulation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "69 As depicted in Fig. 3, Fantasy consists of a pre-trained text encoder $\\tau$ , a transformer-based masked   \n70 image generator $\\mathcal{G}$ , a sampler $\\boldsymbol{S}$ , a frozen VQGAN, and a pre-trained super-resolution model. $\\tau$   \n71 maps a text prompt $t$ to a continuous embedding space. $\\mathcal{G}$ processes a text embedding $e$ to generate   \n72 logits $l$ for the visual token sequence. $\\boldsymbol{S}$ draws a sequence of visual tokens $v$ from logits via iterative   \n73 decoding [4], which runs $N$ steps of inference conditioned on the text embeddings $e$ and visual tokens   \n74 decoded from previous steps. Finally, $\\mathcal{D}$ maps the sequence of discrete tokens to pixel space $Z$ . To   \n75 summarize, given a text prompt $t$ , an image $\\hat{x}$ is synthesized as follows: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\hat{x}=\\mathcal{D}(\\mathcal{S}(\\mathcal{G},\\mathcal{T}(t))),\\quad l_{n}=\\mathcal{G}(v_{n},\\mathcal{T}(t)),\\quad v_{n}=\\mathcal{M}(\\mathcal{E}(x))\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "76 where $n$ is the synthesis step, and $l_{n}$ are logits, from which the next set of visual tokens $v_{n+1}$ are   \n77 sampled. $\\mathcal{M}$ denotes the masking operator that applies masks to the token in $v_{n}$ . We refer to [4, 3]   \n78 for details on the iterative decoding process. The Phi-2 [24] for $\\tau$ and VQGAN [8] for encoder $\\mathcal{E}$ and   \n79 decoder $\\mathcal{D}$ are used. $\\mathcal{G}$ is trained on a large text-image pairs $D$ using masked visual token modeling   \n80 loss: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}=\\mathbb{E}_{(\\boldsymbol{x},t)\\sim D}\\left[C E\\left(l_{N},\\mathcal{E}(\\boldsymbol{x})\\right)\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "81 where $C E$ is a weighted cross-entropy calculated by summing only over the unmasked tokens. ", "page_idx": 2}, {"type": "text", "text": "82 2.2 Model Architecture ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "83 2.2.1 VQGAN as Image Processor ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "84 VQGAN [8] is capable of transforming each image into discrete tokens with higher-level semantic   \n85 information from a learned codebook, while ignoring low level noise. The autoregressive tokens   \n86 prediction of VQGAN shares the same form as text tokens generated by LLMs. Prior research [46]   \n87 has shown that unifying vision and language by the same token space could enhance the coherency   \n88 for vision-text alignment. Furthermore, compared with RGB pixels, the visual token representation   \n89 has proven to reduce disk storage and improve the capability of robustness and generalization.   \n90 To reduce the computational burden, we initially compress an RGB image $v\\,\\in\\,\\mathbb{R}^{H\\times W\\times3}$ into a   \n91 diminished representation with a resolution of $h\\times w\\times3$ , where $h=H/f$ and $w=W/f$ , with   \n92 $f$ denoting the downsampling factor. We then employ a pre-trained $f16$ VQGAN [27] encoder $\\mathcal{E}$   \n93 to quantizate images $\\boldsymbol{x}\\in\\mathbb{R}^{3\\times256\\times256}$ into discrete tokens of spatial dimensions $16\\times16$ from a   \n94 pre-trained codebook $\\mathcal{Z}\\,=\\,\\{z_{k}\\}_{k=1}^{K}$ consisting of $K\\,=\\,8192$ vectors, resulting in the quantized   \n95 representation $z=\\mathcal{E}(x,\\mathcal{Z})$ .   \n97 Recent studies [10, 5, 3] tend to use encoder-decoder LLMs [31] for text encoding over CLIP [30],   \n98 which is adept at handling tasks that involve complex mappings between input and output sequences.   \n99 Due to the tremendous success of ChatGPT, attention has been drawn to models that consist solely of a   \n100 decoder. Also, [43] presents an insight that efficiently fine-tuning a more powerful decoder-only LLM   \n101 can yield stronger performance in long-text alignment. Consequently, to capitalize on the enhanced   \n102 semantic comprehension and generalization potential of LLMs while simultaneously reducing the   \n103 training burden, we employ Phi-2 [24], a state-of-the-art, lightweight LLM, as the text encoder.   \n104 Given the text prompt $t$ , Fantasy first passes it through Phi-2, extracting the text embedding from the   \n105 last hidden layer $L$ . However, typically, decoder-only architectures are not adept at feature extraction   \n106 and mapping tasks. [23] proposes that the conceptual representations learned by LLM\u2019s are roughly   \n107 linearly mappable to those learned by models trained on vision tasks. Therefore, the embedding   \n108 vectors are linearly projected to the hidden size of the image generator $\\mathcal{G}$ : ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\nc=\\mathcal{P}(\\mathcal{T}_{L}(t))\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "109 where $\\tau(\\cdot)$ denotes the decoder-only Phi-2 and $L$ is the index of the last hidden layer. $\\mathcal{P}$ represents   \n110 the projection from text space to visual space, and $c$ is the text feature suitable for the image generator. ", "page_idx": 3}, {"type": "text", "text": "111 2.2.3 MIM as Image Generator ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "112 MIM narrows the gap between its modeling and the extensively studied area of language modeling,   \n113 making it straightforward to leverage the findings of the LLMs research community. Therefore, we   \n114 adopt a masked transformer as the image generator backbone of Fantasy [46].   \n115 During training, we leave the projected text embeddings $c$ unmasked and the image tokens $z$ are   \n116 masked at a variable masking rate based on a Cosine scheduling $\\mathcal{M}$ as [4, 3]. Specifically, for   \n117 each training example, we sample a masking rate $r$ from $[0,1]$ from a truncated arccos distribution   \n118 with density function $\\begin{array}{r}{p(r)=\\frac{2}{\\pi}(1-r^{2})^{-\\frac{1}{2}}}\\end{array}$ . While autoregressive methods learn fixed-order token   \n119 distributions $P(z_{i}|\\boldsymbol{z}_{<i})$ , random masking with variable ratios enables learning $P(z_{i}|\\boldsymbol{z}_{\\neq i})$ for any   \n120 token subset, crucial for our parallel sampling scheme. The sampling of a new state $s_{n+1}$ at each   \n121 successive step is conditioned on the previous state and the specified text condition $c$ : ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\nP(s\\mid c)=\\int P(s_{N}\\mid s_{N-1},c)\\prod_{n=1}^{N-1}P(s_{n}\\mid s_{n-1},c)\\,d s_{1}\\dots d s_{N-1}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "122 For each training example, the most confidently predicted tokens are revealed at each step $n$ , main  \n123 taining cos $\\left({\\frac{n}{N}}\\cdot{\\frac{\\pi}{2}}\\right)$ masked until reaching $N$ total steps.   \n124 For the base model, we use a variant of MaskGiT [4], a masked image generative Transformer to   \n125 predict randomly masked tokens by attending to tokens in all directions. Leveraging the multi-layered   \n126 structure of the Transformer, we have developed scalable image generators with varying layer counts,   \n127 ranging in size from 257M parameters to 611M parameters (for the image generator; the Phi-2 model   \n128 has an additional 2.7B parameters). We first employ a series of Cross Attention blocks to optimize   \n129 text-driven feature extraction, before passing through $O$ layers of the masked image generator. Each   \n130 layer $o$ of the Transformer is again formed by Multi-Head Self-Attentuib(MSA), LayerNorm (LN),   \n131 Cross Attention (CA) and Multi-Layer Perceptron (MLP) blocks: ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\nY_{o}=\\mathbf{MSA}(\\mathbf{LN}(Z_{o})),\\quad Z_{o+1}=\\mathbf{MLP}(\\mathbf{CA}((\\mathbf{LN}(Y_{o}),c))).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "132 At the output layer, to reduce the training burden, ConvMLP [18] is utilized to transform masked   \n133 image embeddings into logits sets, aligning with the VQGAN codebook dimensions. Eventually, the   \n134 reconstructed lower-resolution tokens are restored with the pre-trained $256\\times256$ resolution VQGAN   \n135 decoder to the pixel space, resulting in the generated image $\\hat{x}$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\hat{x}=\\mathcal{D}(\\mathbf{ConvMLP}(Z_{O}),\\mathcal{Z})\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "136 2.3 Training Strategy ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "137 Fig. 3 illustrates Fantasy\u2019s two-stage training approach. Following prior works[43, 35, 9], we employ   \n138 large-scale pre-training to achieve general text-image concept alignment, and simultaneous fine-tuning   \n139 of Phi-2 [24] and the masked image generator using high-quality instruction-image pairs.   \n140 Pre-training Stage. To perform general text-image concept alignment, the VQGAN and LLM   \n141 weights are frozen, and only the image generator is pre-trained on deduplicated LAION-2B [37]   \n142 with images above a 4.5 aesthetic score. We exclusively preserve prompts in English, filter out   \n143 images above a $50\\%$ watermark probability or above a $45\\%$ NSFW probability, yielding a final set   \n144 of 9 million images. Since the computational cost of upsampling is much lower than training a   \n145 super-resolution model, Fantasy is started with training at a resolution of $256\\times256$ . Note that the   \n146 pre-training only needs approximate image-text alignment, substantially lowering the training costs.   \n147 Fine-tuning Stage. [43] has proven that LLMs trained solely on text data lack prior image-text   \n148 knowledge, and that merely aligning their text embeddings with visual features might not be optimal.   \n149 Therefore, in the second stage, we gather an internal dataset of 7 million high-quality instruction  \n150 image pairs to fine-tune both the Phi-2 model and the image generator of Fantasy, which ensures   \n151 enhanced compatibility of text embeddings within the text-image pair space, facilitating the use of   \n152 decoder-only LLMs in text-to-image generation tasks and harnessing their inherent advantages. To   \n153 prevent catastrophic forgetting in LLMs and preserve their understanding abilities during training, we   \n154 select questions from BIG-bench [2] and monitor the common sense question-answering ability of   \n155 Phi-2 in real-time throughout the training process. We construct our training dataset for the fine-tuning   \n156 stage by incorporating JourneyDB [39] and an internal synthetic dataset to enhance the aesthetic   \n157 quality of generated images beyond realistic photographs. To facilitate instruction-image alignment   \n158 learning, we retain only data with descriptions exceeding 30 words, as these provide enough detailed   \n159 insights into the image objects, including attributes and spatial relations.   \n160 With this approach, Fantasy trains a 0.6B parameter T2I model in about 69 A100 GPU days,   \n161 significantly reducing computation compared to existing diffusion-based methods, while maintaining   \n162 comparable visual and numerical fidelity. Throughout this paper, we present a comprehensive   \n163 evaluation of Fantasy\u2019s efficacy, showcasing the potential in training high-quality transformer-based   \n164 image synthesis models compared to diffusion-based models in future. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "165 2.4 High-quality Data Collection ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "166 To ensure rough alignment in the pre-training phase, we utilize the large-scale dataset LAION-2B   \n167 [37] and apply the filtering strategy developed by DataComp [14]. Furthermore, we gather long  \n168 text prompts and corresponding high-quality images to achieve finer-grained text-image alignment   \n169 through instruction tuning. CapsFusion [47] employs a fine-tuned LLaMA [40] for recaptioning   \n170 LAION-2B [37] and LAION-COCO [1]. However, this approach still results in suboptimal image   \n171 quality and occasional mismatches between images and text. SAM-LLAVA [5] utilizes LLaVA [20]   \n172 to recaption the SAM dataset [17], which leads to images with blurred faces, a consequence of the   \n173 dataset\u2019s inherent face-blurring. Therefore, we shift focus to synthesize images, mainly including   \n174 DiffusionDB [42] and JourneyDB [39], produced by Stable Diffusion and MidJourney, respectively.   \n175 To augment the diversity of the images, we minimize the use of datasets from specific domains, such   \n176 as gaming and anime. Furthermore, we implement filtering to discard texts with special characters   \n177 and data containing violence or pornography, retaining only instructions exceeding 30 words. ", "page_idx": 4}, {"type": "text", "text": "178 3 Experiments ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "179 In this section, we outline detailed training, inference, and evaluation protocols, followed by compre  \n180 hensive comparisons across three key metrics. ", "page_idx": 4}, {"type": "text", "text": "181 3.1 Implementation Details ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "182 Training Details. Different from the prior works [9, 43, 32, 34], we used a lightweight but powerful   \n183 decoder-only large language model Phi-2 [24] as the text encoder. Diverging from prior approaches   \n184 that extract a standard and fixed short text tokens, we extend the extraction to 256 tokens to master   \n185 long-term instruction-image alignment, ensuring precise alignment for more fine-grained prompts.   \n186 For the entire training process, we train Fantasy on $4\\!\\times\\!\\mathrm{A}100\\,80\\mathrm{G}$ GPUs and set the accumulation   \n187 step to 2. At different stages, we employ varying learning rate strategies with single-cycle cosine   \n188 annealing decay. Furthermore, the AdamW optimizer [22] is utilized with a weight decay of 0.01.   \n189 Fantasy trains a 0.6B parameter T2I model in about 84.5 A100 GPU days, significantly reducing   \n190 computation compared to existing diffusion-based methods as shown in Fig. 1.   \n191 Inference Details. We use $N=32$ sampling steps in all of our evaluation experiments. Since   \n192 Fantasy is trained at a resolution of $256\\times256$ , we employ the pre-trained diffusion-based super  \n193 resolution model StableSR [41] to upscale images to $512\\times512$ .   \n194 Evaluation Metrics. We comprehensively evaluate Fantasy via four primary metrics, i.e., alignment   \n195 on HPSv2 [44], FID [16] on MSCOCO dataset [19] and human evaluation on a collected dataset. ", "page_idx": 4}, {"type": "table", "img_path": "qL4nN6Ew7U/tmp/57d7100d6d21bb64cb6f6a4e17e5ea44dc1221138ca0271f90a9f5909b69cf3c.jpg", "table_caption": ["Table 1: Evaluation of diffusion (upper) and transformer (down) models on HPSv2. We underline the highest value and color the first above Fantasy in blue . "], "table_footnote": [], "page_idx": 5}, {"type": "table", "img_path": "qL4nN6Ew7U/tmp/5a8bc0d9bd4c66e30599675597d7d6940152f89412f3dd3dc37efa5d5213fa70.jpg", "table_caption": ["Table 2: Comparison with recent T2I models. \u2018Trained\u2019 indicates the model develops a text encoder from scratch, foregoing a pre-trained one. "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "196 3.2 Performance Comparisons and Analysis ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "97 Results on HPSv2. We utilize HPSv2 [44] as our primary automated metric, a preference prediction   \n98 model which can be used to compare images generated with the same prompt across five categories:   \n99 anime, concept art, paintings, photography, and DrawBench [36]. We present the results of HPSv2   \n00 between Fantasy and other state-of-the-art generative models in Tab. 1. Fantasy exhibited outstanding   \n01 performance across all key aspects among previous Transformer-based methods like CogView2   \n02 [12], which is expected. The results also reveal its competitive performance compared to prior   \n03 diffusion-based methods, especially in concept-art and painting, demonstrating similar performance   \n04 to DALL\u00b7E 2 [26]. This remarkable performance is primarily attributed to the text-image alignment   \n05 learning in fine-tuning stage, where high-quality text-image pairs were leveraged to achieve superior   \n06 alignment capabilities. In comparison, DeepFloyd-XL and other diffusion-based models achieve   \n07 better scores, while utilizing larger models with significantly higher compute budget.   \n208 Results on FID. We employ FID [16] to evaluate our models on COCO-30K [19]. To allow for   \n209 a fair comparison, all images are downsampled to $256\\times256$ pixels. The comparison between our   \n210 method and other methods in FID, and their training time is summarized in Tab. 2. We observe   \n211 that the FID of Fantasy is substantially higher compared to other state-of-the-art models. Visual   \n212 inspections reveal that images generated by Fantasy are smoother than those from other leading T2I   \n213 models. This discrepancy is most noticeable in real-world images like COCO, on which we compute   \n214 the FID-metric. Although the state-of-the-art models [43, 11, 29] exhibit lower FID, it relies on   \n215 unaffordable resources. Furthermore, prior studies [29, 5, 11] have demonstrated that FID may not   \n216 be an appropriate metric for image quality evaluation, as a lower score does not necessarily reflect   \n217 superior image generation, and it is more authoritative to use the evaluation of human users. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "image", "img_path": "qL4nN6Ew7U/tmp/0b0278c1e2f715f175d61f8022ca6131763b605026be96884aa6357418c25347.jpg", "img_caption": ["Figure 4: User study on prompts with different length. VC. , CV2. , FT. , SD. , and PA. refer to VQGAN $^+$ CLIP [8], CogView2 [12], our Fantasy, Stable Diffusion $\\mathrm{v}2.0$ [33], and Pixart- $\\alpha$ [5]. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "218 3.3 Results on Human Evaluation ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "219 Following prior works [5, 43, 28], we also conduct a study with human participants to supplement   \n220 our evaluation and provide a more intuitive assessment of Fantasy\u2019s performance. Participants are   \n221 asked to select a preference of the images based on the visual appeal of the generated images and the   \n222 precision of alignments between the text prompts and the corresponding images.   \n223 As involving human evaluators can be time-consuming, we choose the top-performing open-source   \n224 diffusion-based models (e.g., SD XL [33], and Pixart- $\\alpha$ [5]) and transformer-based models (e.g.,   \n225 VQGAN $^+$ CLIP [8] and CogView2 [12]) as our baseline, which are accessible through APIs and   \n226 capable of generating images. We randomly select a total of 600 prompts from existing prompt   \n227 sets (e.g., ParaPrompt [43], ViLG-300 [13], COCO Captions [6]). To comprehensively contrast the   \n228 capabilities of Fantasy and other models in interpreting text prompts of varying lengths, we allocate   \n229 one subset to consist of 300 prompts ranging from 10 to 30 characters and another subset comprising   \n230 300 prompts exceeding 30 characters. For each model, we use a consistent set to generate images,   \n231 which are then evaluated by 50 individuals.   \n232 Fig. 4a clearly demonstrates that images generated on relatively long text prompts (longer than 30   \n233 words) by Fantasy are distinctly favored among the four models in both two perspective, especially   \n234 for text-image alignment, aligning closely with the intended use case of Fantasy. As illustrated   \n235 in Fig. 4b, for text prompts shorter than 30 words, our model outperforms existing open-source   \n236 Transformer-based models in fidelity and alignment for shorter prompts. Our model slightly lags   \n237 behind diffusion-based models in visual appeal, limited by the 8,192 size of VQGAN\u2019s codebook   \n238 and not targeting visual appeal. Simultaneously, Fantasy lacks a distinct advantage in text-image   \n239 alignment in the short subset. We hypothesize that this is due to two main reasons: diffusion  \n240 based models\u2019 ability to handle shorter prompts, and vague prompts generating diverse images that   \n241 make preferences more subjective, thus biasing outcomes towards aesthetically superior images. In   \n242 summary, the human preference experiments confirm the observation made in the HPSv2 benchmarks. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "243 3.4 Case Study ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "244 Fig. 5 vividly illustrates Fantasy\u2019s supe  \n245 rior visual appeal and text-image alignment   \n246 over leading open-source transformer-based   \n247 T2I models [12, 8] and diffusion-based T2I   \n248 models [29, 26]. Fantasy significantly sur  \n249 passes existing transformer-based T2I mod  \n250 els, matches the performance of SDXL [29],   \n251 and qualitatively outperforms Dall\u00b7E 2 [26].   \n252 Despite being trained on images with a res  \n253 olution of $256\\times256$ , Fantasy ensures gener  \n254 ated low-resolution images contain sufficient   \n255 details, indirectly supporting long prompts.   \n256 Limited by computing resources, we haven\u2019t ", "page_idx": 6}, {"type": "image", "img_path": "qL4nN6Ew7U/tmp/05a029d8fba0f0e06cebc3e1752395008766df18eab1f872957d8e95dcf9788d.jpg", "img_caption": [], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Figure 6: Visual Comparison with ParaDiffusion [43]: Red markings and boxes highlight text misalignments in images generated by ParaDiffusion. ", "page_idx": 6}, {"type": "image", "img_path": "qL4nN6Ew7U/tmp/51ccfa60086157c7cb8adc13a595125928e88119a80c0b0f0d474e419f592745.jpg", "img_caption": ["Figure 5: Visual comparison with existing T2I models. (a) A hamster resembling a horse. (b) A frontal portrait of a anime girl with chin length pink hair wearing sunglasses and a white T-shirt smiling. (c) A colorful illustration of a suburban neighborhood on an ancient post-apocalyptic planet featuring creatures made by Jim Henson\u2019s workshop. (d) A blue-haired girl with soft features stares directly at the camera in an extreme close-up Instagram picture. (e) A building in a landscape by Ivan Aivazovsky. (f) Aoshima\u2019s masterpiece depicts a forest illuminated by morning light. (g) The image is a highly detailed portrait of an oak in GTA V, created using Unreal Engine and featuring fantasy artwork by various artists. "], "img_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "qL4nN6Ew7U/tmp/5b46da4d08c68239165f634ff4a0512dad71da9b0dc8b18fd889c308ac81010c.jpg", "table_caption": ["Table 3: Ablation study on two stages with the best bolded. \u2018Base\u2019 indicates the model after the pre-training stage. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "257 trained on higher resolutions like $512\\times512$ but aim to enhance Fantasy by training at higher   \n258 resolutions in the future.   \n259 ParaDiffusion [43] pioneers the use of decoder-only large language models as text encoders in   \n260 text-to-image generation. As illustrated in Fig. 6, our observations suggest that Fantasy more closely   \n261 aligns details with prompts than ParaDiffusion [43]. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "262 4 Ablation Study ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "263 This section analyzes the effects of LLMs fine-tuning, and model scale on Fantasy\u2019s performance   \n264 through ablation studies. More ablation study refers to appendix. ", "page_idx": 7}, {"type": "text", "text": "265 4.1 Effect of Language Model Fine-tuning ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "266 To assess the effect of training strategies on the comprehension of complex instructions, we perform   \n267 a human preference evaluation, as detailed in Sec. 3.3, using a subset of 300 prompts longer than   \n268 30 characters. \u2018Base\u2019 denotes general text-image alignment with filtered LAION-2B [1] in the   \n269 pre-training stage. Compared to the base model, our synergy fine-tuning with Phi-2 demonstrates a   \n270 notable improvement in all aspects in Tab. 3. ", "page_idx": 7}, {"type": "table", "img_path": "qL4nN6Ew7U/tmp/0941634023280d861223eadcbb3d560546b43a67ae24b6008883fe52919a059a.jpg", "table_caption": ["271 Table 4: Ablation study on models at different scales with the best bolded. DB. represents DrawBench [36]. "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "qL4nN6Ew7U/tmp/dee8d4c24d737a45dae51fbabc95785048c1c331eacde1f1c642a12b16e3cfe3.jpg", "table_caption": ["Table 5: Training cost for Fantasy at 3 different scales. BS. denotes batch size and LR. denotes learning rate. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "272 4.2 Scale of Image Generator ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "273 The hierarchical structure of the Transformer allows us   \n274 to train image generators with varying numbers of Trans  \n275 former layers. As shown in Tab. 4, we evaluate models   \n276 of different sizes on the HPSv2 benchmark. The insight   \n277 indicates that as trainable parameters increase from 257   \n278 million to 611 million, performance consistently improves.   \n279 Therefore, we set the number of Transformer layers to 22   \n280 with 611 million trainable parameters as the optimal set  \n281 ting. Tab. 5 showcases the required resources for models   \n282 of three different scales. Fig. 7 offers visual comparisons   \n283 across models of varying scales, illustrating a clear trend:   \n284 models with fewer parameters underperform on the HPSv2   \n285 benchmark, frequently resulting in distorted images and   \n286 omitted details, yet they may still generate acceptable   \n287 outcomes. Significantly, the visual quality diverges as   \n288 model size increases, highlighting the potential for scaling   \n289 up masked image modeling to enhance instruction-image   \n290 alignment and elevate generation quality. ", "page_idx": 8}, {"type": "image", "img_path": "qL4nN6Ew7U/tmp/2c4bd1279bf8810865fa219a6e89639d1530ae4c6710da42599b90276562b14c.jpg", "img_caption": ["Figure 7: Examples generated by models at different scales: $1^{s t}$ column for 6 layers, $2^{n d}$ column for 12 layers and $3^{r d}$ column for 22 layers. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "291 5 Limitations and Social Impact ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "292 Limitations. Despite Fantasy achieving competitive performance in text-image alignment and visual   \n293 appeal, it requires improvements in handling complex scenes. We propose two possible strategies to   \n294 overcome the challenge in future research: Firstly, augmenting the dataset with high-quality images   \n295 can enhance diversity and refine the model. Secondly, since the scale of the masked image generator   \n296 affects instruction-image alignment, training an upscale image generator based on higher resolution   \n297 left further explored.   \n298 Social Impact. Generative models for media bring both beneftis and challenges. They foster creativity   \n299 and make technology more accessible, yet pose risks by facilitating the creation of manipulated   \n300 content, spreading misinformation, and exacerbating biases, particularly affecting women with deep   \n301 fakes. Concerns also include the potential exposure of sensitive training data collected without   \n302 consent. Despite generative models potentially offering better data representation, the impact of   \n303 combining adversarial training with likelihood-based objectives on data distortion remains a crucial   \n304 research area. Ethical considerations of these models are significant and require thorough exploration. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "305 6 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "306 In this paper, we introduce Fantasy, a lightweight and efficient text-to-image model that combines   \n307 Large Language Models (LLMs) with a transformer-based masked image modeling (MIM), effec  \n308 tively transferring semantic understanding capabilities from LLMs to the text-to-image generation.   \n309 With our proposed two-stage training strategy and high-quality dataset, Fantasy significantly re  \n310 duces computational requirements while producing high-fidelity images. Extensive experiments   \n311 demonstrate that Fantasy achieves comparable performance to models trained with significantly more   \n312 computational resources, illustrating the viability of our approach and suggesting potential efficient   \n313 scalability to even larger masked image modeling for text-to-image generation. ", "page_idx": 8}, {"type": "text", "text": "314 References", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "315 [1] K\u00f6pf Andreas, Vencu Richard, Coombes Theo, and Beaumont Romain. Laion coco: $600\\mathrm{m}$ synthetic   \n316 captions from laion2b-en.[eb/ol], 2022.   \n317 [2] BIG bench authors. Beyond the imitation game: Quantifying and extrapolating the capabilities of language   \n318 models. Transactions on Machine Learning Research, 2023.   \n319 [3] Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, Jose Lezama, Lu Jiang, Ming-Hsuan Yang,   \n320 Kevin Murphy, William T Freeman, Michael Rubinstein, et al. Muse: Text-to-image generation via masked   \n321 generative transformers. arXiv preprint arXiv:2301.00704, 2023.   \n322 [4] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T Freeman. Maskgit: Masked generative image   \n323 transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,   \n324 pages 11315\u201311325, 2022.   \n325 [5] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James   \n326 Kwok, Ping Luo, Huchuan Lu, et al. Fast training of diffusion transformer for photorealistic text-to-image   \n327 synthesis. arXiv preprint arXiv:2310.00426, 2023.   \n328 [6] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Doll\u00e1r, and   \n329 C Lawrence Zitnick. Microsoft coco captions: Data collection and evaluation server. arxiv 2015. arXiv   \n330 preprint arXiv:1504.00325, 2015.   \n331 [7] Craiyon. Dall\u00b7e mini: Generate images from any text prompt. https://wandb.ai/dalle-mini/   \n332 dalle-mini/reports/DALL-E-mini-Generate-images-from-any-text-prompt--VmlldzoyMDE4N   \n333 2023. Accessed: 2024-02-27.   \n334 [8] Katherine Crowson, Stella Biderman, Daniel Kornis, Dashiell Stander, Eric Hallahan, Louis Castricato,   \n335 and Edward Raff. Vqgan-clip: Open domain image generation and editing with natural language guidance.   \n336 In European Conference on Computer Vision, pages 88\u2013105. Springer, 2022.   \n337 [9] Xiaoliang Dai, Ji Hou, Chih-Yao Ma, Sam Tsai, Jialiang Wang, Rui Wang, Peizhao Zhang, Simon   \n338 Vandenhende, Xiaofang Wang, Abhimanyu Dubey, et al. Emu: Enhancing image generation models using   \n339 photogenic needles in a haystack. arXiv preprint arXiv:2309.15807, 2023.   \n340 [10] Deepfloyd. Deepfloyd. https://www.deepfloyd.ai/, 2023.   \n341 [11] DeepFloyd. IF-I-XL-v1.0: A model by deepfloyd on hugging face models. https://huggingface.co/   \n342 DeepFloyd/IF-I-XL-v1.0, 2023. Accessed: 2024-02-28.   \n343 [12] Ming Ding, Wendi Zheng, Wenyi Hong, and Jie Tang. Cogview2: Faster and better text-to-image generation   \n344 via hierarchical transformers. Advances in Neural Information Processing Systems, 35:16890\u201316902,   \n345 2022.   \n346 [13] Zhida Feng, Zhenyu Zhang, Xintong Yu, Yewei Fang, Lanxin Li, Xuyi Chen, Yuxiang Lu, Jiaxiang   \n347 Liu, Weichong Yin, Shikun Feng, et al. Ernie-vilg 2.0: Improving text-to-image diffusion model with   \n348 knowledge-enhanced mixture-of-denoising-experts. In Proceedings of the IEEE/CVF Conference on   \n349 Computer Vision and Pattern Recognition, pages 10135\u201310145, 2023.   \n350 [14] Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen,   \n351 Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al. Datacomp: In search of the next   \n352 generation of multimodal datasets. Advances in Neural Information Processing Systems, 36, 2024.   \n353 [15] Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong Chen, Lu Yuan, and Baining Guo.   \n354 Vector quantized diffusion model for text-to-image synthesis. In Proceedings of the IEEE/CVF Conference   \n355 on Computer Vision and Pattern Recognition, pages 10696\u201310706, 2022.   \n356 [16] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans   \n357 trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural information   \n358 processing systems, 30, 2017.   \n359 [17] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete   \n360 Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. arXiv preprint   \n361 arXiv:2304.02643, 2023.   \n362 [18] Jiachen Li, Ali Hassani, Steven Walton, and Humphrey Shi. Convmlp: Hierarchical convolutional mlps for   \n363 vision. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages   \n364 6306\u20136315, 2023.   \n365 [19] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r,   \n366 and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision\u2013ECCV 2014:   \n367 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages   \n368 740\u2013755. Springer, 2014.   \n369 [20] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning, 2023.   \n370 [21] Xingchao Liu, Chengyue Gong, Lemeng Wu, Shujian Zhang, Hao Su, and Qiang Liu. Fusedream:   \n371 Training-free text-to-image generation with improved clip $^{1+}$ gan space optimization. arXiv preprint   \n372 arXiv:2112.01573, 2021.   \n373 [22] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint   \n374 arXiv:1711.05101, 2017.   \n375 [23] Jack Merullo, Louis Castricato, Carsten Eickhoff, and Ellie Pavlick. Linearly mapping from image to text   \n376 space. arXiv preprint arXiv:2209.15162, 2022.   \n377 [24] Microsoft. Phi-2. https://huggingface.co/microsoft/phi-2, 2023.   \n378 [25] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya   \n379 Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided   \n380 diffusion models. arXiv preprint arXiv:2112.10741, 2021.   \n381 [26] OpenAI. Dall-e 2. https://openai.com/dall-e-2, 2022.   \n382 [27] Suraj Patil, William Berman, Robin Rombach, and Patrick von Platen. amused: An open muse reproduction.   \n383 arXiv preprint arXiv:2401.01808, 2024.   \n384 [28] Pablo Pernias, Dominic Rampas, Mats Leon Richter, Christopher Pal, and Marc Aubreville. W\u00fcrstchen:   \n385 An efficient architecture for large-scale text-to-image diffusion models. In The Twelfth International   \n386 Conference on Learning Representations, 2023.   \n387 [29] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas M\u00fcller, Joe Penna,   \n388 and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv   \n389 preprint arXiv:2307.01952, 2023.   \n390 [30] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish   \n391 Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from   \n392 natural language supervision. In International conference on machine learning, pages 8748\u20138763. PMLR,   \n393 2021.   \n394 [31] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou,   \n395 Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer.   \n396 The Journal of Machine Learning Research, 21(1):5485\u20135551, 2020.   \n397 [32] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional   \n398 image generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3, 2022.   \n399 [33] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution   \n400 image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer   \n401 Vision and Pattern Recognition (CVPR), pages 10684\u201310695, June 2022.   \n402 [34] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution   \n403 image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer   \n404 vision and pattern recognition, pages 10684\u201310695, 2022.   \n405 [35] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed   \n406 Ghasemipour, Burcu Karagol Ayan, S Sara Mahdavi, Rapha Gontijo Lopes, et al. Photorealistic text-to  \n407 image diffusion models with deep language understanding, 2022. URL https://arxiv. org/abs/2205.11487,   \n408 4.   \n409 [36] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar   \n410 Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to  \n411 image diffusion models with deep language understanding. Advances in Neural Information Processing   \n412 Systems, 35:36479\u201336494, 2022.   \n413 [37] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti,   \n414 Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale   \n415 dataset for training next generation image-text models. Advances in Neural Information Processing   \n416 Systems, 35:25278\u201325294, 2022.   \n417 [38] Eyal Segalis, Dani Valevski, Danny Lumen, Yossi Matias, and Yaniv Leviathan. A picture is worth a   \n418 thousand words: Principled recaptioning improves image generation. arXiv preprint arXiv:2310.16656,   \n419 2023.   \n420 [39] Keqiang Sun, Junting Pan, Yuying Ge, Hao Li, Haodong Duan, Xiaoshi Wu, Renrui Zhang, Aojun Zhou,   \n421 Zipeng Qin, Yi Wang, et al. Journeydb: A benchmark for generative image understanding. Advances in   \n422 Neural Information Processing Systems, 36, 2024.   \n423 [40] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix,   \n424 Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard   \n425 Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. arXiv preprint   \n426 arXiv:2302.13971, 2023.   \n427 [41] Jianyi Wang, Zongsheng Yue, Shangchen Zhou, Kelvin CK Chan, and Chen Change Loy. Exploiting   \n428 diffusion prior for real-world image super-resolution. arXiv preprint arXiv:2305.07015, 2023.   \n429 [42] Zijie J. Wang, Evan Montoya, David Munechika, Haoyang Yang, Benjamin Hoover, and Duen Horng Chau.   \n430 DiffusionDB: A large-scale prompt gallery dataset for text-to-image generative models. arXiv:2210.14896   \n431 [cs], 2022.   \n432 [43] Weijia Wu, Zhuang Li, Yefei He, Mike Zheng Shou, Chunhua Shen, Lele Cheng, Yan Li, Tingting Gao, Di   \n433 Zhang, and Zhongyuan Wang. Paragraph-to-image generation with information-enriched diffusion model.   \n434 arXiv preprint arXiv:2311.14284, 2023.   \n435 [44] Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, and Hongsheng Li. Human   \n436 preference score v2: A solid benchmark for evaluating human preferences of text-to-image synthesis. arXiv   \n437 preprint arXiv:2306.09341, 2023.   \n438 [45] Ling Yang, Zhaochen Yu, Chenlin Meng, Minkai Xu, Stefano Ermon, and Bin Cui. Mastering text-to-image   \n439 diffusion: Recaptioning, planning, and generating with multimodal llms. arXiv preprint arXiv:2401.11708,   \n440 2024.   \n441 [46] Lijun Yu, Jos\u00e9 Lezama, Nitesh B Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng,   \n442 Agrim Gupta, Xiuye Gu, Alexander G Hauptmann, et al. Language model beats diffusion\u2013tokenizer is key   \n443 to visual generation. arXiv preprint arXiv:2310.05737, 2023.   \n444 [47] Qiying Yu, Quan Sun, Xiaosong Zhang, Yufeng Cui, Fan Zhang, Xinlong Wang, and Jingjing Liu.   \n445 Capsfusion: Rethinking image-text data at scale. arXiv preprint arXiv:2310.20550, 2023.   \n446 [48] Y Zhou, R Zhang, C Chen, C Li, C Tensmeyer, T Yu, J Gu, J Xu, and T Sun. Laftie: Towards language-free   \n447 training for text-to-image generation. arxiv 2021. arXiv preprint arXiv:2111.13792. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "448 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "50 Question: Do the main claims made in the abstract and introduction accurately reflect the   \n51 paper\u2019s contributions and scope? ", "page_idx": 12}, {"type": "text", "text": "Justification: The abstract and introduction clearly define the paper\u2019s contributions, which involve advancements in urban simulation accuracy and computational efficiency. These claims are backed by robust experimental validation detailed in the subsequent sections. ", "page_idx": 12}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 12}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 12}, {"type": "text", "text": "Justification: We have included a comprehensive discussion on limitations, particularly focusing on the scalability of our simulations in extremely large urban environments and potential biases in the modeling processes. ", "page_idx": 12}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 12}, {"type": "text", "text": "499 3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "500 Question: For each theoretical result, does the paper provide the full set of assumptions and   \n501 a complete (and correct) proof?   \n502 Answer: [Yes]   \n503 Justification: All theoretical results are accompanied by a clear statement of assumptions and   \n504 are supported by complete proofs provided in the supplementary materials. Each theorem   \n505 and lemma are properly referenced and numbered for clarity and ease of access.   \n506 Guidelines:   \n507 \u2022 The answer NA means that the paper does not include theoretical results.   \n508 \u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross  \n509 referenced.   \n510 \u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n511 \u2022 The proofs can either appear in the main paper or the supplemental material, but if   \n512 they appear in the supplemental material, the authors are encouraged to provide a short   \n513 proof sketch to provide intuition.   \n514 \u2022 Inversely, any informal proof provided in the core of the paper should be complemented   \n515 by formal proofs provided in appendix or supplemental material.   \n516 \u2022 Theorems and Lemmas that the proof relies upon should be properly referenced.   \n517 4. Experimental Result Reproducibility   \n518 Question: Does the paper fully disclose all the information needed to reproduce the main ex  \n519 perimental results of the paper to the extent that it affects the main claims and/or conclusions   \n520 of the paper (regardless of whether the code and data are provided or not)?   \n521 Answer: [Yes]   \n522 Justification: The paper provides detailed descriptions of the experimental setup, including   \n523 data splits, hyperparameters, and the type of optimizer used. We also provide access to the   \n524 source code and datasets in the supplementary materials to ensure full reproducibility.   \n525 Guidelines:   \n526 \u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 13}, {"type": "text", "text": "\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.   \n(b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). ", "page_idx": 13}, {"type": "text", "text": "52 (d) We recognize that reproducibility may be tricky in some cases, in which case   \n53 authors are welcome to describe the particular way they provide for reproducibility.   \n54 In the case of closed-source models, it may be that access to the model is limited in   \n55 some way (e.g., to registered users), but it should be possible for other researchers   \n56 to have some path to reproducing or verifying the results. ", "page_idx": 14}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 14}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 14}, {"type": "text", "text": "Justification: The paper does not propose a benchmark and we will release the code if the paper is accepted. The model depends on non-open-sourced dataset, and the copyright of the checkpoint belongs to the company. Detailed instructions for training our model, including command lines, are provided in the supplementary materials. ", "page_idx": 14}, {"type": "text", "text": "Guidelines: ", "page_idx": 14}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 14}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 14}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 14}, {"type": "text", "text": "Justification: The experimental section of the paper provides comprehensive details about the training and test setups, including the rationale behind choosing specific hyperparameters and the types of optimizers used. ", "page_idx": 14}, {"type": "text", "text": "Guidelines: ", "page_idx": 14}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 14}, {"type": "text", "text": "00 7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "601 Question: Does the paper report error bars suitably and correctly defined or other appropriate   \n602 information about the statistical significance of the experiments? ", "page_idx": 14}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 14}, {"type": "text", "text": "604 Justification: All experimental results are presented with error bars reflecting the standard   \n605 deviation across multiple runs. We provide a detailed explanation of how these were   \n606 calculated and the assumptions underlying our statistical tests.   \n607 Guidelines:   \n608 \u2022 The answer NA means that the paper does not include experiments.   \n609 \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confi  \n610 dence intervals, or statistical significance tests, at least for the experiments that support   \n611 the main claims of the paper.   \n612 \u2022 The factors of variability that the error bars are capturing should be clearly stated (for   \n613 example, train/test split, initialization, random drawing of some parameter, or overall   \n614 run with given experimental conditions).   \n615 \u2022 The method for calculating the error bars should be explained (closed form formula,   \n616 call to a library function, bootstrap, etc.)   \n617 \u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n618 \u2022 It should be clear whether the error bar is the standard deviation or the standard error   \n619 of the mean.   \n620 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should   \n621 preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis   \n622 of Normality of errors is not verified.   \n623 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or   \n624 figures symmetric error bars that would yield results that are out of range (e.g. negative   \n625 error rates).   \n626 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how   \n627 they were calculated and reference the corresponding figures or tables in the text.   \n628 8. Experiments Compute Resources   \n629 Question: For each experiment, does the paper provide sufficient information on the com  \n630 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n631 the experiments?   \n632 Answer: [Yes]   \n633 Justification: The paper details the computational resources required for each experiment,   \n634 including the types of GPUs used, the amount of memory, and the execution time. This   \n635 ensures that other researchers can allocate the appropriate resources to reproduce our results.   \n636 Guidelines:   \n637 \u2022 The answer NA means that the paper does not include experiments.   \n638 \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,   \n639 or cloud provider, including relevant memory and storage.   \n640 \u2022 The paper should provide the amount of compute required for each of the individual   \n641 experimental runs as well as estimate the total compute.   \n642 \u2022 The paper should disclose whether the full research project required more compute   \n643 than the experiments reported in the paper (e.g., preliminary or failed experiments that   \n644 didn\u2019t make it into the paper).   \n645 9. Code Of Ethics   \n646 Question: Does the research conducted in the paper conform, in every respect, with the   \n647 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n648 Answer: [Yes]   \n649 Justification: Our research adheres strictly to the NeurIPS Code of Ethics. We have consid  \n650 ered ethical implications, especially regarding the generation of images from text, and have   \n651 implemented measures to prevent misuse.   \n652 Guidelines:   \n653 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n654 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n655 deviation from the Code of Ethics. ", "page_idx": 15}, {"type": "text", "text": "\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 16}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 16}, {"type": "text", "text": "Justification: The paper includes content about broader impacts that discusses both the potential positive applications of our method in educational and creative industries, and potential negative impacts, such as the misuse of generated images. We also suggest mitigation strategies for potential negative uses. ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 16}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 16}, {"type": "text", "text": "Justification: Our paper poses no such risks. If then, we will describe the safeguards implemented in releasing our models, including usage guidelines and limitations to access, ensuring responsible use and mitigating risks of misuse. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 16}, {"type": "text", "text": "708 12. Licenses for existing assets ", "page_idx": 16}, {"type": "text", "text": "709 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n710 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n711 properly respected?   \n712 Answer: [Yes]   \n713 Justification: All third-party assets used in our research are properly credited, and we have   \n714 explicitly mentioned and complied with the licensing terms. URLs and version numbers of   \n715 datasets and code are clearly listed in the references.   \n716 Guidelines:   \n717 \u2022 The answer NA means that the paper does not use existing assets.   \n718 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n719 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n720 URL.   \n721 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n722 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n723 service of that source should be provided.   \n724 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n725 package should be provided. For popular datasets, paperswithcode.com/datasets   \n726 has curated licenses for some datasets. Their licensing guide can help determine the   \n727 license of a dataset.   \n728 \u2022 For existing datasets that are re-packaged, both the original license and the license of   \n729 the derived asset (if it has changed) should be provided.   \n730 \u2022 If this information is not available online, the authors are encouraged to reach out to   \n731 the asset\u2019s creators.   \n732 13. New Assets   \n733 Question: Are new assets introduced in the paper well documented and is the documentation   \n734 provided alongside the assets?   \n735 Answer: [Yes]   \n736 Justification: Any new datasets or models introduced in the paper are accompanied by   \n737 thorough documentation detailing their creation, intended use, limitations, and licensing   \n738 information.   \n739 Guidelines:   \n740 \u2022 The answer NA means that the paper does not release new assets.   \n741 \u2022 Researchers should communicate the details of the dataset/code/model as part of their   \n742 submissions via structured templates. This includes details about training, license,   \n743 limitations, etc.   \n744 \u2022 The paper should discuss whether and how consent was obtained from people whose   \n745 asset is used.   \n746 \u2022 At submission time, remember to anonymize your assets (if applicable). You can either   \n747 create an anonymized URL or include an anonymized zip file.   \n748 14. Crowdsourcing and Research with Human Subjects   \n749 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n750 include the full text of instructions given to participants and screenshots, if applicable, as   \n751 well as details about compensation (if any)?   \n752 Answer: [NA]   \n753 Justification: This paper does not involve crowdsourcing experiments or research with   \n754 human subjects.   \n755 Guidelines:   \n756 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n757 human subjects.   \n758 \u2022 Including this information in the supplemental material is fine, but if the main contribu  \n759 tion of the paper involves human subjects, then as much detail as possible should be   \n760 included in the main paper.   \n761 \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation,   \n762 or other labor should be paid at least the minimum wage in the country of the data   \n763 collector.   \n764 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human   \n765 Subjects   \n766 Question: Does the paper describe potential risks incurred by study participants, whether   \n767 such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)   \n768 approvals (or an equivalent approval/review based on the requirements of your country or   \n769 institution) were obtained?   \n770 Answer: [NA]   \n771 Justification: Our research did not involve human subjects, thus no IRB approval was   \n772 necessary.   \n773 Guidelines:   \n774 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n775 human subjects.   \n776 \u2022 Depending on the country in which research is conducted, IRB approval (or equivalent)   \n777 may be required for any human subjects research. If you obtained IRB approval, you   \n778 should clearly state this in the paper.   \n779 \u2022 We recognize that the procedures for this may vary significantly between institutions   \n780 and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the   \n781 guidelines for their institution.   \n782 \u2022 For initial submissions, do not include any information that would break anonymity (if   \n783 applicable), such as the institution conducting the review. ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}]