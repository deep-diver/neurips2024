[{"figure_path": "ioAlzcELTf/figures/figures_1_1.jpg", "caption": "Figure 1: Learnable router weight. The role of physics and AI at different lead times: major evolution and adaptive correction (details in Sec. 4.4).", "description": "This figure shows the learnable router weights in the HybridBlock of the WeatherGFT model at different forecast lead times.  The weights represent the proportion of the output from the physical (PDE) and AI (neural network) branches.  Initially, both branches are weighted equally (0.5:0.5). As the lead time increases, the weight of the physics branch decreases, indicating error accumulation in the PDE simulations. Conversely, the weight of the AI branch increases, showing that the AI component plays a more significant role in correcting for the errors at longer lead times. This demonstrates the adaptive nature of the hybrid model, where physics drives the primary evolution and AI performs adaptive bias correction.", "section": "3.2 WeatherGFT Overview"}, {"figure_path": "ioAlzcELTf/figures/figures_3_1.jpg", "caption": "Figure 2: Overview of WeatherGFT. HybridBlock serves as the fundamental unit of the model, consisting of three PDE kernels, a parallel Attention Block, and a subsequent learnable router. A lead time conditional decoder is employed to generate forecasts for different lead times.", "description": "The figure provides a detailed overview of the WeatherGFT model architecture.  It highlights the key components: the encoder for converting input weather data into tokens, the stacked HybridBlocks that form the core of the model, and the lead time conditional decoder that enables the model to predict forecasts at various lead times. Each HybridBlock contains a set of PDE (Partial Differential Equation) kernels for simulating physical evolution, a parallel Attention Block for bias correction, and a learnable router that adapts the fusion of physical and neural network outputs. This design allows the WeatherGFT model to generate fine-grained temporal resolution forecasts, exceeding the resolution of the training data.", "section": "3.2 WeatherGFT Overview"}, {"figure_path": "ioAlzcELTf/figures/figures_6_1.jpg", "caption": "Figure 4: Medium-Range Forecast. The x-axis represents the lead time in hours, while the y-axis represents the RMSE for different variables. The smaller RMSE the better.", "description": "The figure shows the root mean square error (RMSE) for different weather variables (t2m, u10, z500, t850) predicted by different models (FourCastNet, ClimODE, Keisler, ECMWF-IFS, WeatherGFT) at various lead times (0-125 hours).  A lower RMSE indicates better prediction accuracy.  WeatherGFT consistently demonstrates superior performance across all variables and lead times.", "section": "4.2 Skillful Medium-Range Forecasts by WeatherGFT"}, {"figure_path": "ioAlzcELTf/figures/figures_7_1.jpg", "caption": "Figure 5: Visualization of z500 Predictions.", "description": "This figure visualizes the predictions of the geopotential at the 500hpa pressure layer (z500) using three different methods: WeatherGFT, ECMWF-IFS (a physical dynamic model), and ground truth.  The visualization shows the predictions and their errors at two different time points (2017-01-01T06:00:00 and 2017-01-02T00:00:00). The red boxes highlight a region where WeatherGFT shows a smaller error compared to ECMWF-IFS, demonstrating improved accuracy in a specific region.", "section": "4.3 Generalizing to Fine-grained Time Scale for Nowcasting"}, {"figure_path": "ioAlzcELTf/figures/figures_8_1.jpg", "caption": "Figure 6: Visualization of Precipitation Nowcast. Precipitation in the area ranging from 34N to 50S and 148E to 128W during the time period from 00:00 to 02:00 on July 1, 2017.", "description": "This figure visualizes the precipitation nowcast results from different models at various time points (00:30:00, 01:00:00, 01:30:00, and 02:00:00).  It compares the ground truth precipitation data with predictions from FourCastNet combined with both Flavr and UPR interpolation methods, Keisler combined with both Flavr and UPR, ClimODE combined with both Flavr and UPR, and finally, the WeatherGFT model. The color scale represents the difference between predicted and ground truth precipitation, with green indicating accurate predictions and red/blue indicating underestimation and overestimation respectively.  The red boxes highlight specific areas where the WeatherGFT model's performance stands out.", "section": "4.3 Generalizing to Fine-grained Time Scale for Nowcasting"}, {"figure_path": "ioAlzcELTf/figures/figures_8_2.jpg", "caption": "Figure 7: The Weights in the Router of 24 HybridBlocks.", "description": "This figure visualizes the weights of the learnable router within each of the 24 HybridBlocks in the WeatherGFT model.  The x-axis represents time, broken down into 15-minute intervals across a 6-hour period. The y-axis shows the weight assigned to either the physics or AI branch within each HybridBlock's router.  The figure demonstrates how the contribution of the physics-based branch gradually decreases over time, while the contribution of the AI-based correction branch increases to compensate for accumulating errors from the PDE kernel's evolution.", "section": "4.4 Weather Forecasts can Benefit from Physics and AI via WeatherGFT"}, {"figure_path": "ioAlzcELTf/figures/figures_9_1.jpg", "caption": "Figure 4: Medium-Range Forecast. The x-axis represents the lead time in hours, while the y-axis represents the RMSE for different variables. The smaller RMSE the better.", "description": "The figure shows the Root Mean Square Error (RMSE) for different weather variables (t2m, u10, z500, t850) at different forecast lead times (0-125 hours).  It compares the performance of WeatherGFT against three other models (FourCastNet, ClimODE, Keisler) and a physical dynamic model (ECMWF-IFS).  Lower RMSE values indicate better forecast accuracy.  The graph illustrates how the RMSE changes for each variable as the lead time increases, demonstrating the model's skill in medium-range forecasting and its superiority to other methods for certain variables.", "section": "4.2 Skillful Medium-Range Forecasts by WeatherGFT"}, {"figure_path": "ioAlzcELTf/figures/figures_18_1.jpg", "caption": "Figure 9: Bias. The closer to 0 the better.", "description": "This figure shows the bias (the difference between predictions and ground truth) for three variables (t2m, t850, z500) with and without the PDE kernel.  Negative bias indicates underestimation. The results show that using the PDE kernel helps to reduce the bias, particularly for longer lead times.", "section": "D.1 Prediction Bias Evaluation"}, {"figure_path": "ioAlzcELTf/figures/figures_18_2.jpg", "caption": "Figure 10: Left. Energy: the more consistent the better. Right. The norms of the outputs from the two networks are similar and stable. This indicates: a) The two networks produce outputs on the same scale. b) The router is decoupled and dynamically selects the more crucial features from the two branches without affecting the scale of the two networks.", "description": "The figure demonstrates the consistency of energy in the model's prediction when using the PDE kernel. It also shows the normalization of the AI and physics outputs, which are similar, and that the router's role in selecting features doesn't affect the output scale.", "section": "D.2 Prediction Energy Evaluation"}]