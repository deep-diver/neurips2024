[{"figure_path": "PCgnTiGC9K/tables/tables_5_1.jpg", "caption": "Table 1: Test accuracy (%, \u2191) and ECE (\u2193) of DEs-5 and CreDEs-5 using CIFAR10, CIFAR100, and ImageNet as ID datasets over 15 runs. The better performance is marked in bold.", "description": "This table presents the results of test accuracy and expected calibration error (ECE) for both Deep Ensembles (DEs) and Credal Deep Ensembles (CreDEs) on three image datasets (CIFAR10, CIFAR100, and ImageNet).  The results are averaged over 15 independent runs and highlight the superior performance of CreDEs in terms of higher accuracy and lower ECE.", "section": "3 Experimental Validation"}, {"figure_path": "PCgnTiGC9K/tables/tables_6_1.jpg", "caption": "Table 1: Test accuracy (%, \u2191) and ECE (\u2193) of DEs-5 and CreDEs-5 using CIFAR10, CIFAR100, and ImageNet as ID datasets over 15 runs. The better performance is marked in bold.", "description": "This table presents the test accuracy and Expected Calibration Error (ECE) for both Deep Ensembles (DEs-5) and Credal Deep Ensembles (CreDEs-5) on three image classification datasets (CIFAR-10, CIFAR-100, and ImageNet).  The results are averaged over 15 independent runs, each starting from a different random seed.  Higher test accuracy and lower ECE indicate better performance. Bold values show where CreDEs-5 outperforms DEs-5.", "section": "3 Experimental Validation"}, {"figure_path": "PCgnTiGC9K/tables/tables_7_1.jpg", "caption": "Table 2: OOD detection AUROC and AUPRC performance (%, \u2191) between CreDEs-5 and DEs-5 based on ResNet50 using EU as uncertainty metrics on CIFAR10/100 vs. SVHN/Tiny-ImageNet and ImageNet vs. ImageNet-O. Results are averaged over 15 runs. Best results in bold.", "description": "This table presents the results of out-of-distribution (OOD) detection experiments comparing Credal Deep Ensembles (CreDEs) and Deep Ensembles (DEs).  The Area Under the Receiver Operating Characteristic curve (AUROC) and Area Under the Precision-Recall Curve (AUPRC) are reported for various dataset pairs (CIFAR10/100 vs. SVHN/Tiny-ImageNet, and ImageNet vs. ImageNet-O).  The results are based on ResNet50 architecture, using Epistemic Uncertainty (EU) as the uncertainty metric, and averaged over 15 runs.  The best performing model for each metric and dataset pair is shown in bold.", "section": "3 Experimental Validation"}, {"figure_path": "PCgnTiGC9K/tables/tables_8_1.jpg", "caption": "Table 1: Test accuracy (%, \u2191) and ECE (\u2193) of DEs-5 and CreDEs-5 using CIFAR10, CIFAR100, and ImageNet as ID datasets over 15 runs. The better performance is marked in bold.", "description": "This table presents the results of comparing the performance of Deep Ensembles (DEs) and Credal Deep Ensembles (CreDEs) on three image classification datasets: CIFAR10, CIFAR100, and ImageNet.  The metrics used are test accuracy (higher is better) and Expected Calibration Error (ECE) (lower is better).  The results are averaged over 15 independent runs, and the best performing model for each metric and dataset is highlighted in bold.  The table demonstrates the improved performance of CreDEs in terms of both accuracy and ECE compared to the standard DE baseline.", "section": "3 Experimental Validation"}, {"figure_path": "PCgnTiGC9K/tables/tables_9_1.jpg", "caption": "Table 1: Test accuracy (%, \u2191) and ECE (\u2193) of DEs-5 and CreDEs-5 using CIFAR10, CIFAR100, and ImageNet as ID datasets over 15 runs. The better performance is marked in bold.", "description": "This table presents the results of the test accuracy and expected calibration error (ECE) for both Deep Ensembles (DEs) and Credal Deep Ensembles (CreDEs) on three image datasets: CIFAR10, CIFAR100, and ImageNet.  The results are averaged over 15 runs with different random seeds, and the better performance (higher accuracy and lower ECE) is highlighted in bold.  This table demonstrates the improved performance of CreDEs over the baseline DEs in terms of accuracy and calibration.", "section": "3 Experimental Validation"}, {"figure_path": "PCgnTiGC9K/tables/tables_9_2.jpg", "caption": "Table 1: Test accuracy (%, \u2191) and ECE (\u2193) of DEs-5 and CreDEs-5 using CIFAR10, CIFAR100, and ImageNet as ID datasets over 15 runs. The better performance is marked in bold.", "description": "This table presents a comparison of the test accuracy and expected calibration error (ECE) between Deep Ensembles (DEs) and Credal Deep Ensembles (CreDEs) on three image datasets (CIFAR-10, CIFAR-100, and ImageNet).  The results are averaged over 15 independent runs, and the best performing model for each metric is highlighted in bold.  It demonstrates CreDEs' superior performance in terms of both accuracy and calibration.", "section": "3 Experimental Validation"}, {"figure_path": "PCgnTiGC9K/tables/tables_16_1.jpg", "caption": "Table 1: Test accuracy (%, \u2191) and ECE (\u2193) of DEs-5 and CreDEs-5 using CIFAR10, CIFAR100, and ImageNet as ID datasets over 15 runs. The better performance is marked in bold.", "description": "This table presents the results of testing the performance of Deep Ensembles (DEs) and Credal Deep Ensembles (CreDEs) on three image datasets (CIFAR10, CIFAR100, and ImageNet).  The metrics used to evaluate performance are test accuracy (higher is better) and Expected Calibration Error (ECE) (lower is better). The results are averaged across 15 runs for both methods, and the best-performing method for each metric and dataset is shown in bold.", "section": "3 Experimental Validation"}, {"figure_path": "PCgnTiGC9K/tables/tables_17_1.jpg", "caption": "Table 2: OOD detection AUROC and AUPRC performance (%, \u2191) between CreDEs-5 and DEs-5 based on ResNet50 using EU as uncertainty metrics on CIFAR10/100 vs. SVHN/Tiny-ImageNet and ImageNet vs. ImageNet-O. Results are averaged over 15 runs. Best results in bold.", "description": "This table presents the Area Under the Receiver Operating Characteristic curve (AUROC) and the Area Under the Precision-Recall Curve (AUPRC) for out-of-distribution (OOD) detection.  It compares the performance of Credal Deep Ensembles (CreDEs) with 5 models (CreDEs-5) against Deep Ensembles (DEs) with 5 models (DEs-5). The results are obtained using ResNet50 architecture and the epistemic uncertainty (EU) as the uncertainty metric.  The experiment is run 15 times with different random seeds, and the average performance and standard deviation are reported. The comparison is done across different dataset pairings: CIFAR10/CIFAR100 vs. SVHN/Tiny-ImageNet and ImageNet vs. ImageNet-O.  The best performance for each metric and dataset is shown in bold.", "section": "3 Experimental Validation"}, {"figure_path": "PCgnTiGC9K/tables/tables_17_2.jpg", "caption": "Table 2: OOD detection AUROC and AUPRC performance (%, \u2191) between CreDEs-5 and DEs-5 based on ResNet50 using EU as uncertainty metrics on CIFAR10/100 vs. SVHN/Tiny-ImageNet and ImageNet vs. ImageNet-O. Results are averaged over 15 runs. Best results in bold.", "description": "This table presents the results of out-of-distribution (OOD) detection experiments using Credal Deep Ensembles (CreDEs) and Deep Ensembles (DEs) as baselines.  The experiments were conducted on four different dataset pairings: CIFAR10/CIFAR100 vs. SVHN/Tiny-ImageNet, and ImageNet vs. ImageNet-O.  The performance is evaluated using the Area Under the Receiver Operating Characteristic curve (AUROC) and the Area Under the Precision-Recall curve (AUPRC) metrics.  Epistemic Uncertainty (EU), calculated as H(Q)-H(Q), is used as the uncertainty measure.  The table shows that CreDEs consistently outperforms DEs across all datasets and metrics.", "section": "3 Experimental Validation"}, {"figure_path": "PCgnTiGC9K/tables/tables_19_1.jpg", "caption": "Table 2: OOD detection AUROC and AUPRC performance (%, \u2191) between CreDEs-5 and DEs-5 based on ResNet50 using EU as uncertainty metrics on CIFAR10/100 vs. SVHN/Tiny-ImageNet and ImageNet vs. ImageNet-O. Results are averaged over 15 runs. Best results in bold.", "description": "This table presents the Area Under the Receiver Operating Characteristic (AUROC) and Area Under the Precision-Recall Curve (AUPRC) scores for out-of-distribution (OOD) detection.  It compares the performance of Credal Deep Ensembles (CreDEs) with 5 models (CreDEs-5) against Deep Ensembles (DEs) with 5 models (DEs-5).  The results are obtained using ResNet50 models and the EU (epistemic uncertainty) metric.  The experiments were run 15 times, and the average results are presented.  The table shows the performance on various datasets: CIFAR10/100 (in-distribution) versus SVHN/Tiny-ImageNet (out-of-distribution), and ImageNet (in-distribution) versus ImageNet-O (out-of-distribution).", "section": "3 Experimental Validation"}, {"figure_path": "PCgnTiGC9K/tables/tables_19_2.jpg", "caption": "Table 2: OOD detection AUROC and AUPRC performance (%, \u2191) between CreDEs-5 and DEs-5 based on ResNet50 using EU as uncertainty metrics on CIFAR10/100 vs. SVHN/Tiny-ImageNet and ImageNet vs. ImageNet-O. Results are averaged over 15 runs. Best results in bold.", "description": "This table presents the results of out-of-distribution (OOD) detection experiments using Credal Deep Ensembles (CreDEs) and Deep Ensembles (DEs) as baselines.  The Area Under the Receiver Operating Characteristic curve (AUROC) and Area Under the Precision-Recall curve (AUPRC) are reported for four different dataset pairings: CIFAR-10/CIFAR-100 vs. SVHN/Tiny-ImageNet, and ImageNet vs. ImageNet-O.  The results are based on ResNet50 architectures and averaged over 15 runs, with the best results highlighted in bold. The EU metric (epistemic uncertainty) is used for uncertainty quantification.", "section": "3 Experimental Validation"}, {"figure_path": "PCgnTiGC9K/tables/tables_19_3.jpg", "caption": "Table 1: Test accuracy (%, \u2191) and ECE (\u2193) of DEs-5 and CreDEs-5 using CIFAR10, CIFAR100, and ImageNet as ID datasets over 15 runs. The better performance is marked in bold.", "description": "This table presents the results of comparing Deep Ensembles (DEs) and Credal Deep Ensembles (CreDEs) on three image classification datasets (CIFAR10, CIFAR100, and ImageNet).  It shows the test accuracy and Expected Calibration Error (ECE) for both methods.  Higher accuracy and lower ECE indicate better model performance. The results are averaged over 15 independent runs for each method.  Bold values highlight the superior performance between DEs and CreDEs on each metric.", "section": "3 Experimental Validation"}, {"figure_path": "PCgnTiGC9K/tables/tables_19_4.jpg", "caption": "Table 2: OOD detection AUROC and AUPRC performance (%, \u2191) between CreDEs-5 and DEs-5 based on ResNet50 using EU as uncertainty metrics on CIFAR10/100 vs. SVHN/Tiny-ImageNet and ImageNet vs. ImageNet-O. Results are averaged over 15 runs. Best results in bold.", "description": "This table presents the results of out-of-distribution (OOD) detection experiments using two different ensemble methods: Credal Deep Ensembles (CreDEs) and Deep Ensembles (DEs).  The experiments were performed using the ResNet50 architecture on four different dataset pairs.  The table shows the Area Under the Receiver Operating Characteristic curve (AUROC) and the Area Under the Precision-Recall Curve (AUPRC) for each ensemble method and dataset pair. Higher AUROC and AUPRC values indicate better OOD detection performance.  The results are averaged across 15 runs, and the best result for each metric and dataset pair is highlighted in bold.  The uncertainty metric used is Epistemic Uncertainty (EU).", "section": "3 Experimental Validation"}, {"figure_path": "PCgnTiGC9K/tables/tables_20_1.jpg", "caption": "Table 1: Test accuracy (%, \u2191) and ECE (\u2193) of DEs-5 and CreDEs-5 using CIFAR10, CIFAR100, and ImageNet as ID datasets over 15 runs. The better performance is marked in bold.", "description": "This table presents the test accuracy and expected calibration error (ECE) for both Deep Ensembles (DEs) and Credal Deep Ensembles (CreDEs) on three image classification datasets (CIFAR10, CIFAR100, and ImageNet).  The results are averaged over 15 independent runs, and the best performing model (CreDEs-5) is highlighted in bold.  It demonstrates CreDEs' superior performance in terms of accuracy and calibration.", "section": "3 Experimental Validation"}, {"figure_path": "PCgnTiGC9K/tables/tables_20_2.jpg", "caption": "Table 2: OOD detection AUROC and AUPRC performance (%, \u2191) between CreDEs-5 and DEs-5 based on ResNet50 using EU as uncertainty metrics on CIFAR10/100 vs. SVHN/Tiny-ImageNet and ImageNet vs. ImageNet-O. Results are averaged over 15 runs. Best results in bold.", "description": "This table presents the Area Under the Receiver Operating Characteristic curve (AUROC) and the Area Under the Precision-Recall Curve (AUPRC) for out-of-distribution (OOD) detection.  It compares the performance of Credal Deep Ensembles (CreDEs) with 5 models (CreDEs-5) against Deep Ensembles (DEs) with 5 models (DEs-5), using ResNet50 architecture and the epistemic uncertainty (EU) as the uncertainty metric.  The results are averaged over 15 runs with different random seeds and presented for different dataset pairings: CIFAR10/CIFAR100 (in-distribution) versus SVHN/Tiny-ImageNet (out-of-distribution), and ImageNet (in-distribution) versus ImageNet-O (out-of-distribution).  The best-performing method for each metric and dataset is highlighted in bold.", "section": "3 Experimental Validation"}, {"figure_path": "PCgnTiGC9K/tables/tables_21_1.jpg", "caption": "Table 2: OOD detection AUROC and AUPRC performance (%, \u2191) between CreDEs-5 and DEs-5 based on ResNet50 using EU as uncertainty metrics on CIFAR10/100 vs. SVHN/Tiny-ImageNet and ImageNet vs. ImageNet-O. Results are averaged over 15 runs. Best results in bold.", "description": "This table presents the results of out-of-distribution (OOD) detection experiments using two different ensemble methods: Credal Deep Ensembles (CreDEs) and Deep Ensembles (DEs).  The experiments were performed using the ResNet50 architecture, with epistemic uncertainty (EU) as the metric for evaluating OOD detection performance.  The table shows AUROC (Area Under the Receiver Operating Characteristic curve) and AUPRC (Area Under the Precision-Recall curve) scores for different dataset pairings: CIFAR10/100 (in-distribution) versus SVHN/Tiny-ImageNet (out-of-distribution), and ImageNet (in-distribution) versus ImageNet-O (out-of-distribution).  The results are averages across 15 runs, and the best-performing method for each metric and dataset pair is highlighted in bold.", "section": "3 Experimental Validation"}, {"figure_path": "PCgnTiGC9K/tables/tables_22_1.jpg", "caption": "Table 1: Test accuracy (%, \u2191) and ECE (\u2193) of DEs-5 and CreDEs-5 using CIFAR10, CIFAR100, and ImageNet as ID datasets over 15 runs. The better performance is marked in bold.", "description": "This table presents a comparison of the performance of Deep Ensembles (DEs) and Credal Deep Ensembles (CreDEs) on three image classification datasets (CIFAR10, CIFAR100, and ImageNet).  The metrics used are test accuracy (higher is better) and Expected Calibration Error (ECE) (lower is better).  Results are averaged over 15 independent runs, and the best performing method for each metric on each dataset is highlighted in bold. This helps to demonstrate the improved accuracy and calibration of CreDEs compared to the baseline DEs.", "section": "3 Experimental Validation"}, {"figure_path": "PCgnTiGC9K/tables/tables_22_2.jpg", "caption": "Table 1: Test accuracy (%, \u2191) and ECE (\u2193) of DEs-5 and CreDEs-5 using CIFAR10, CIFAR100, and ImageNet as ID datasets over 15 runs. The better performance is marked in bold.", "description": "This table presents a comparison of the test accuracy and Expected Calibration Error (ECE) for Deep Ensembles (DEs) and Credal Deep Ensembles (CreDEs) on three image classification datasets (CIFAR-10, CIFAR-100, and ImageNet).  The results are averaged over 15 independent runs, each starting with a different random seed.  Bold values indicate superior performance for CreDEs.", "section": "3 Experimental Validation"}]