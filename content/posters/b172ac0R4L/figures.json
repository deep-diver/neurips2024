[{"figure_path": "b172ac0R4L/figures/figures_7_1.jpg", "caption": "Figure 1: (a), (b): For the hypothesis space of sparse decision trees and 0-1 loss, the number of leaves in optimal models for several datasets decreases with increased label noise. The solid lines depict the observed number of leaves in an optimal model over noisy data. The dashed lines depict the number of leaves of the optimal model over the clean data with regularization \u03bb/(1 \u2212 2p), (see Theorem 1). (c): For the hypothesis space of linear models and exponential loss, the sum of the squares of the weights corresponding to the continuous features decreases as additive noise with standard deviation \u03c3 is applied to the dataset.", "description": "Figure 1 shows the effect of noise (label and additive attribute noise) on model complexity for sparse decision trees and linear models.  Subfigures (a) and (b) demonstrate that for sparse decision trees, increasing label noise reduces the optimal model's number of leaves.  This aligns with the theoretical results showing that noise acts as an implicit regularizer. Subfigure (c) illustrates that for linear models using exponential loss, increasing additive attribute noise decreases the squared norm of relevant weights (a measure of model complexity).", "section": "7.1 Sparse Decision Trees, 0-1 Loss, Random Label Noise"}, {"figure_path": "b172ac0R4L/figures/figures_8_1.jpg", "caption": "Figure 2: Experimental results for Section 4 on recidivism datasets. In accuracy plots (left), blue is accuracy on clean test data, orange is accuracy on clean train data, and green is accuracy on noisy train data. In complexity plots, blue corresponds to leaves in optimal model over noisy data, orange to optimal model over clean data with higher regularization as in Figure 1. Green is the leaf upper bound from Corollary 10. Lambda is regularization parameter optimized via 5-fold CV.", "description": "This figure shows the experimental results on recidivism datasets for Section 4 of the paper. The left side shows accuracy plots, while the right side shows complexity plots. The accuracy plots compare the performance of models trained on clean data versus noisy data.  The complexity plots compare the number of leaves in optimal models trained under different conditions (noisy data, clean data with regularization).  The green line represents an upper bound on the number of leaves calculated using Corollary 10 from the paper. The regularization parameter (lambda) was optimized using 5-fold cross-validation.", "section": "7.1 Sparse Decision Trees, 0-1 Loss, Random Label Noise"}, {"figure_path": "b172ac0R4L/figures/figures_9_1.jpg", "caption": "Figure 3: A visual demonstration of the simplifying effect of noise on Rashomon sets. This shows a bar chart of the discrete probability distribution of the number of leaves among models in the Rashomon set. Results are shown aggregated in (a) for 23 real-world and synthetic datasets, and in (b), just real-world data, for 9 recidivism and finance datasets.", "description": "This figure shows the impact of noise on the complexity of models within the Rashomon set, which is a set of near-optimal models. The left panel (a) aggregates results over 23 datasets (both synthetic and real-world) while the right panel (b) focuses specifically on 9 real-world recidivism and finance datasets. For each noise level (0%, 10%, 20%, 30%), the number of leaves (model complexity) in the models of the Rashomon set is visualized using a bar chart. The figure empirically shows that as the noise level increases, the distribution of the number of leaves shifts towards lower values, indicating simpler models within the Rashomon set.", "section": "4.2 Noise Simplifies the Rashomon Set"}, {"figure_path": "b172ac0R4L/figures/figures_23_1.jpg", "caption": "Figure 1: (a), (b): For the hypothesis space of sparse decision trees and 0-1 loss, the number of leaves in optimal models for several datasets decreases with increased label noise. The solid lines depict the observed number of leaves in an optimal model over noisy data. The dashed lines depict the number of leaves of the optimal model over the clean data with regularization \u03bb/(1\u22122\u03c1), (see Theorem 1). (c): For the hypothesis space of linear models and exponential loss, the sum of the squares of the weights corresponding to the continuous features decreases as additive noise with standard deviation \u03c3 is applied to the dataset.", "description": "This figure shows the effect of label noise and additive attribute noise on model complexity for sparse decision trees and linear models, respectively.  The left and center plots demonstrate that the number of leaves in optimal decision trees decreases as label noise increases, aligning with theoretical predictions. The right plot illustrates that the sum of squared weights in linear models decreases with increased additive attribute noise, also supporting the paper's theoretical claims about noise acting as an implicit regularizer.", "section": "7.1 Sparse Decision Trees, 0-1 Loss, Random Label Noise"}, {"figure_path": "b172ac0R4L/figures/figures_25_1.jpg", "caption": "Figure 4: Feature AUC distribution before (left; blue) and after (right; orange) injecting 15% label noise, aggregated across 9 criminal recidivism and financial datasets with feature quality parameter \u03b3 = 0.05 (total of 109 features). All features degrade towards 0.5 AUC when label noise is introduced, and better features tend to degrade faster. This increases the size of the set of features within \u03b3 AUC of the best feature (green shaded regions) from 15 features with clean labels to 27 features with noisy labels, which aligns with our prediction in Corollary 7.", "description": "This figure shows the distribution of Area Under the ROC Curve (AUC) values for features in a set of datasets before and after adding 15% label noise.  The left panel shows the distribution for clean labels, while the right shows the distribution after adding noise.  The key observation is that, with the addition of noise, high-AUC features (those that strongly correlate with the labels) lose signal more quickly than low-AUC features. This effect is visualized by the shift in the distribution to the left and the widening of the green shaded area, which represents the set of features within a certain AUC threshold of the best feature. The expansion of this set demonstrates that the addition of noise increases the number of features that are relatively useful for prediction.", "section": "5.1 The Set of Good Features Increases under Noise"}, {"figure_path": "b172ac0R4L/figures/figures_25_2.jpg", "caption": "Figure 3: A visual demonstration of the simplifying effect of noise on Rashomon sets. This shows a bar chart of the discrete probability distribution of the number of leaves among models in the Rashomon set. Results are shown aggregated in (a) for 23 real-world and synthetic datasets, and in (b), just real-world data, for 9 recidivism and finance datasets.", "description": "This figure shows the effect of noise on the complexity of models within the Rashomon set (a set of near-optimal models).  The bar charts display the distribution of the number of leaves (a measure of model complexity) in decision trees for different noise levels.  Panel (a) aggregates results across 23 diverse datasets, while panel (b) focuses specifically on 9 real-world recidivism and finance datasets. The results demonstrate that as noise increases, the models within the Rashomon set tend to become simpler (fewer leaves).", "section": "4.2 Noise Simplifies the Rashomon Set"}, {"figure_path": "b172ac0R4L/figures/figures_26_1.jpg", "caption": "Figure 6: Experimental results for Section 4 on financial datasets. See experimental design in Appendix I.2 for a description of each line.", "description": "This figure shows experimental results for Section 4 (Random Label Noise and Regularized 0-1 Loss) applied to financial datasets.  For each dataset, it displays plots showing the accuracy (test, train, and noisy train) and model complexity (\"leaves\" in a decision tree) across different levels of label noise.  The dashed lines represent predictions of the number of leaves based on theoretical results, demonstrating an alignment between theory and experiment in how noise affects model complexity. The Appendix I.2 provides details on the experimental design.", "section": "Experimental Results"}, {"figure_path": "b172ac0R4L/figures/figures_27_1.jpg", "caption": "Figure 1: (a), (b): For the hypothesis space of sparse decision trees and 0-1 loss, the number of leaves in optimal models for several datasets decreases with increased label noise. The solid lines depict the observed number of leaves in an optimal model over noisy data. The dashed lines depict the number of leaves of the optimal model over the clean data with regularization \u03bb/(1\u22122\u03c1), (see Theorem 1). (c): For the hypothesis space of linear models and exponential loss, the sum of the squares of the weights corresponding to the continuous features decreases as additive noise with standard deviation \u03c3 is applied to the dataset.", "description": "Figure 1 presents experimental results that support the theoretical findings of the paper. The plots show how model complexity (number of leaves for decision trees, sum of squared weights for linear models) changes with increasing label noise or additive attribute noise, for several different datasets.  The results confirm that noise acts as an implicit regularizer, leading to simpler models.", "section": "7.1 Sparse Decision Trees, 0-1 Loss, Random Label Noise"}]