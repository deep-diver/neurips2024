[{"figure_path": "kV80nC1afE/figures/figures_3_1.jpg", "caption": "Figure 1: Adaptive learning scheme of APAS.", "description": "This figure illustrates the adaptive learning process of the proposed Adaptive Passive-Aggressive online regression framework with Side information (APAS). It starts with side information, ht(.), which is then used to calculate the proximal operator, prox\u03bbht(\u0175t+1(\u025bt)), to generate the weight vector, \u0175t+1(\u025bt), based on the passive-aggressive weight update. Then, Moreau Envelope, M\u03bbht(\u0175t+1(\u025bt)), is used to calculate the loss function ft(\u025bt), which finally determines the update of \u025bt+1. This iterative process shows how APAS adapts to different scenarios by adjusting the threshold parameter dynamically, ensuring balance between real-time tracking accuracy and side performance. ", "section": "3.1 PAS Framework"}, {"figure_path": "kV80nC1afE/figures/figures_7_1.jpg", "caption": "Figure 2: Comparison of tracking error and excess cumulative return on the synthetic dataset.", "description": "This figure shows the performance comparison of the proposed APAS framework against several benchmark methods on a synthetic dataset.  Subfigure (a) presents a trade-off curve between tracking error and excess cumulative return for APAS (with different lambda values) and the benchmarks. It demonstrates that APAS achieves higher excess cumulative returns for a given level of tracking error compared to the benchmarks. Subfigure (b) provides an ablation study comparing the adaptive APAS with a non-adaptive PAS (Passive-Aggressive with Side information) that uses fixed epsilon values. This highlights the benefit of APAS's adaptive epsilon selection in balancing tracking accuracy and side performance.", "section": "4.1 Synthetic Data Experiments"}, {"figure_path": "kV80nC1afE/figures/figures_7_2.jpg", "caption": "Figure 2: Comparison of tracking error and excess cumulative return on the synthetic dataset.", "description": "This figure compares the tracking error and excess cumulative return of different methods on a synthetic dataset.  Panel (a) shows a trade-off curve between tracking error and excess cumulative return for the proposed APAS method (with different lambda values) and other benchmark methods. Panel (b) shows an ablation study comparing APAS (adaptive threshold) with a non-adaptive version of PAS (fixed threshold) demonstrating improved performance of APAS.", "section": "4.1 Synthetic Data Experiments"}, {"figure_path": "kV80nC1afE/figures/figures_8_1.jpg", "caption": "Figure 2: Comparison of tracking error and excess cumulative return on the synthetic dataset.", "description": "This figure shows the performance comparison of different methods on a synthetic dataset.  The left subplot displays the trade-off between tracking error and excess cumulative return.  The adaptive APAS method achieves higher excess cumulative return for the same level of tracking error and lower tracking error for the same level of excess cumulative return, compared to the benchmarks (PA, SLAIT-ETE, and SLAIT-DR). The right subplot shows an ablation study comparing the adaptive APAS with different fixed threshold parameter settings (PAS). It highlights how the adaptive parameter selection in APAS leads to superior performance compared to manually setting the threshold parameter in PAS.", "section": "4.1 Synthetic Data Experiments"}, {"figure_path": "kV80nC1afE/figures/figures_8_2.jpg", "caption": "Figure 5: Tracking error and excess cumulative return over time T for different methods on NASDAQ 100 dataset.", "description": "This figure compares the performance of different online regression methods (APAS with different lambda values, SLAIT-ETE, SLAIT-DR, and PA) on the NASDAQ 100 dataset in terms of tracking error and excess cumulative return over time.  The x-axis represents the time step (T), while the y-axis shows the tracking error (left panel) and excess cumulative return (right panel).  The results illustrate the trade-off between tracking accuracy and excess return achieved by different methods and parameter settings.  APAS demonstrates a better balance between these two metrics, especially when a larger lambda value is used, indicating its effectiveness in achieving high excess returns without significantly sacrificing tracking accuracy.", "section": "4.2 Real Market Data Experiments"}, {"figure_path": "kV80nC1afE/figures/figures_9_1.jpg", "caption": "Figure 6: Average convergence speed and CPU time comparison of 100 randomized trials on N-dimensional datasets of Algorithm 2.", "description": "This figure compares the performance of four different algorithms (ADMM, CVXR, PGD, and the proposed algorithm) in terms of convergence speed and CPU time. The left panel shows the average convergence gap versus the number of iterations for a dataset with N = 1000 dimensions.  The right panel illustrates the average CPU time for each method across various problem dimensions (N). The proposed algorithm demonstrates significantly faster convergence and lower CPU time compared to the other algorithms, especially for high-dimensional data.", "section": "Speed Comparison of Acceleration Schemes"}, {"figure_path": "kV80nC1afE/figures/figures_14_1.jpg", "caption": "Figure 7: Illustration for curves of ft(\u03b5) with \u03bd < \u03b6t < D.", "description": "This figure illustrates two scenarios for the curves of the loss function ft(\u03b5) when \u03bd < \u03b6t < D.  The left panel (a) shows the case where the left derivative of ft(\u03b5) at \u03b6t is negative (\u2202_ft(\u03b6t) < 0), resulting in a convex function. The right panel (b) shows the case where the left derivative of ft(\u03b5) at \u03b6t is non-negative (\u2202_ft(\u03b6t) \u2265 0), resulting in a quasi-convex function.  These scenarios are analyzed to verify the inequalities used in the proof of Proposition 3, which establishes a bound used in the regret analysis of the APAS algorithm.", "section": "A.1 Proof of Proposition 3"}]