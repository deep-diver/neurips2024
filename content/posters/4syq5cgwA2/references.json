{"references": [{"fullname_first_author": "L\u00e9on Bottou", "paper_title": "Optimization methods for large-scale machine learning", "publication_date": "2018-00-00", "reason": "This paper provides foundational knowledge on optimization methods crucial for the gradient-based sampling methods discussed in the paper."}, {"fullname_first_author": "Arnak S Dalalyan", "paper_title": "Theoretical guarantees for approximate sampling from smooth and log-concave densities", "publication_date": "2017-00-00", "reason": "This paper provides theoretical guarantees for approximate sampling, which are essential for establishing the convergence properties of the proposed method."}, {"fullname_first_author": "Will Grathwohl", "paper_title": "Oops I took A gradient: Scalable sampling for discrete distributions", "publication_date": "2021-00-00", "reason": "This paper introduces gradient-based discrete sampling, which is the foundation of the proposed approach, providing a crucial starting point for the research."}, {"fullname_first_author": "Ruqi Zhang", "paper_title": "A langevin-like sampler for discrete distributions", "publication_date": "2022-00-00", "reason": "This paper presents a related gradient-based sampling method and its theoretical analysis, which is directly compared with the proposed approach in the experiments."}, {"fullname_first_author": "Haoran Sun", "paper_title": "Any-scale balanced samplers for discrete space", "publication_date": "2023-00-00", "reason": "This paper is closely related to the proposed method, offering a different approach to address the challenge of multimodal discrete distributions, thereby providing a strong benchmark for comparison."}]}