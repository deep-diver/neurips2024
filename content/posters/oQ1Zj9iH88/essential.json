{"importance": "This paper is crucial for researchers in bilevel optimization due to its novel penalization framework that provides **non-asymptotic convergence rates** for simple bilevel problems, addressing limitations of existing methods.  It offers **flexible algorithms** adaptable to various assumptions on problem structures and provides valuable **complexity results**, opening avenues for future work in handling nonsmooth and nonconvex cases.  Its adaptable methods enhance the applicability and efficiency of SBO across diverse applications.", "summary": "This paper proposes penalty-based methods for simple bilevel optimization, achieving (\u03b5, \u03b5\u03b2)-optimal solutions with improved complexity under H\u00f6lderian error bounds.", "takeaways": ["A novel penalization framework links approximate solutions of the original and penalized problems, enabling the application of various methods with different complexities.", "Under the a-H\u00f6lderian error bound condition, the proposed penalty-based algorithm achieves an (\u03b5, \u03b5\u03b2)-optimal solution within O(\u221a1/\u03b5max{\u03b1,\u03b2}) iterations.", "Adaptive algorithms with warm-start mechanisms provide similar complexity while demonstrating superior performance in numerical experiments."], "tldr": "Simple bilevel optimization (SBO) problems, minimizing an upper-level objective over the optimal solution set of a convex lower-level objective, pose significant challenges. Existing methods often suffer from slow convergence or strong assumptions. This paper addresses these challenges by proposing a penalization framework that connects approximate solutions of the original SBO problem and its reformulated counterparts.  This allows for flexibility in handling various smoothness and convexity assumptions, using different algorithms with distinct convergence results.\nThe paper introduces a penalty-based Accelerated Proximal Gradient (APG) algorithm.  Under an a-H\u00f6lderian error bound and mild assumptions, this algorithm achieves an (\u03b5, \u03b5\u03b2)-optimal solution.  The results improve further when strong convexity is present. The framework is extended to general nonsmooth functions, showcasing its versatility and effectiveness.  Numerical experiments validate the effectiveness of the algorithms, highlighting their superior performance compared to existing methods.", "affiliation": "Fudan University", "categories": {"main_category": "Machine Learning", "sub_category": "Optimization"}, "podcast_path": "oQ1Zj9iH88/podcast.wav"}