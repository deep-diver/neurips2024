{"references": [{"fullname_first_author": "Yurii Nesterov", "paper_title": "A method for solving the convex programming problem with convergence rate O(1/k\")", "publication_date": "1983", "reason": "This paper introduced Nesterov's accelerated gradient descent, a foundational algorithm in optimization that the current paper generalizes."}, {"fullname_first_author": "Mikhail Belkin", "paper_title": "Fit without fear: remarkable mathematical phenomena of deep learning through the prism of interpolation", "publication_date": "2021", "reason": "This paper provides the theoretical background for the multiplicative noise model used in the current paper, which is motivated by overparametrized machine learning."}, {"fullname_first_author": "Sharan Vaswani", "paper_title": "Fast and faster convergence of SGD for over-parametrized models and an accelerated perceptron", "publication_date": "2019", "reason": "This paper is directly related to the current work, proving that Nesterov's accelerated coordinate descent achieves acceleration with multiplicative noise, which motivates the current paper's approach."}, {"fullname_first_author": "Stephan Wojtowytsch", "paper_title": "Stochastic gradient descent with noise of machine learning type. Part I: Discrete time analysis", "publication_date": "2023", "reason": "This paper provides crucial theoretical tools used in the convergence analysis of the current paper and introduces important concepts about the nature of noise in overparametrized models."}, {"fullname_first_author": "Chaoyue Liu", "paper_title": "Accelerating SGD with momentum for over-parameterized learning", "publication_date": "2018", "reason": "This paper shows that the original Nesterov's method does not achieve acceleration with multiplicative noise, motivating the development of alternative accelerated methods in the current paper."}]}