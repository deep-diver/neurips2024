[{"figure_path": "kHXUb494SY/figures/figures_6_1.jpg", "caption": "Figure 2: To be able to quantify the gradient noise exactly, we choose relatively small models and data sets. Left: A ReLU network with four hidden layers of width 250 is trained by SGD to fit random labels yi (drawn from a 2-dimensional standard Gaussian) at 1,000 random data points Xi (drawn from a 500-dimensional standard Gaussian). The variance \u03c3\u00b2 of the gradient estimators is ~ 105 times larger than the loss function and ~ 106 times larger than the parameter gradient. This relationship is stable over approximately ten orders of magnitude. Right: A ReLU network with two hidden layers of width 50 is trained by SGD to fit the Runge function 1/(1 + x2) on equispaced data samples in the interval [-8, 8]. Also here, the variance in the gradient estimates is proportional to both the loss function and the magnitude of the gradient.", "description": "The figure shows empirical evidence supporting the multiplicative noise assumption.  Two experiments are presented: one fitting random labels to random data points using a larger network, and one fitting the Runge function to equispaced data points using a smaller network.  In both cases, the variance of the gradient estimator scales linearly with both the loss function and the magnitude of the gradient. This supports the paper's assumption that the noise intensity in the gradient estimates is proportional to the magnitude of the gradient.", "section": "Motivation for Multiplicative Noise"}, {"figure_path": "kHXUb494SY/figures/figures_7_1.jpg", "caption": "Figure 3: We plot E[fa(xn)] on a loglog scale for SGD (blue), AGNES (red), NAG (green), ACDM (orange) and CNM (maroon) with d = 4 (left) and d = 16 (right) for noise levels \u03c3 = 0 (solid line), \u03c3 = 10 (dashed) and \u03c3 = 50 (dotted). The initial condition is x0 = 1 in all simulations. Means are computed over 200 runs. After an initial plateau, AGNES, CNM and ACDM significantly outperform SGD in all settings, while NAG (green) diverges if \u03c3 is large. The length of the initial plateau increases with \u03c3.", "description": "The figure compares the performance of five optimization algorithms: SGD, AGNES, NAG, ACDM, and CNM on two convex functions with different dimensions (d=4 and d=16) and noise levels (\u03c3=0, 10, 50).  It shows that AGNES, CNM, and ACDM significantly outperform SGD, especially at higher noise levels, while NAG diverges for large \u03c3.", "section": "5 Numerical Experiments"}, {"figure_path": "kHXUb494SY/figures/figures_8_1.jpg", "caption": "Figure 4: We report the training loss as a running average with decay rate 0.99 (top row) and test loss (bottom row) for batch sizes 100 (left column), 50 (middle column), and 10 (right column) in the setting of Section 5.2. The horizontal axis represents the number of optimizer steps. The performance gap between AGNES and other algorithms widens for smaller batch sizes, where the gradient estimates are more stochastic and the two different parameters \u03b1, \u03b7 add the most benefit.", "description": "The figure shows the training and test loss for different batch sizes (100, 50, and 10) using several optimization algorithms: NAG, Adam, SGD with momentum, and AGNES.  The x-axis represents the number of optimization steps. The results show that AGNES generally outperforms other methods, especially for smaller batch sizes where the noise in gradient estimates is higher. The benefit of using two separate learning rate parameters in AGNES is highlighted for the cases where the gradient estimates have high stochasticity.", "section": "5.2 Neural network regression"}, {"figure_path": "kHXUb494SY/figures/figures_9_1.jpg", "caption": "Figure 4: We report the training loss as a running average with decay rate 0.99 (top row) and test loss (bottom row) for batch sizes 100 (left column), 50 (middle column), and 10 (right column) in the setting of Section 5.2. The horizontal axis represents the number of optimizer steps. The performance gap between AGNES and other algorithms widens for smaller batch sizes, where the gradient estimates are more stochastic and the two different parameters \u03b1, \u03b7 add the most benefit.", "description": "This figure compares the performance of AGNES against other optimizers (SGD with momentum, NAG, and Adam) for different batch sizes on a neural network regression task. The results are presented as running averages of the training and test losses. The figure shows that AGNES consistently outperforms other methods, especially with smaller batch sizes, where gradient estimates are noisier.", "section": "5.2 Neural network regression"}, {"figure_path": "kHXUb494SY/figures/figures_9_2.jpg", "caption": "Figure 6: We report the average training loss after each epoch for six epochs for training LeNet-5 on MNIST with AGNES for various combinations of the hyperparameters \u03b1 and \u03b7 to illustrate that \u03b1 is the algorithm\u2019s primary learning rate. Left: For a given \u03b1 (color coded), the difference in the trajectory for the three values of \u03b7 (line style) is marginal. On the other hand, choosing \u03b1 well significantly affects performance. Middle: For any given \u03b1, the largest value of \u03b7 performs much better than the other three values which have near-identical performance. Nevertheless, the worst performing value of \u03b7 with well chosen \u03b1 = 5\u00b710\u22123 performs better than the best performing value of \u03b7 with \u03b1 = 5\u00b710\u22124. Right: When \u03b1 is too large, the loss increases irrespective of the value of \u03b7.", "description": "This figure shows the impact of hyperparameters \u03b1 and \u03b7 on the performance of AGNES for training LeNet-5 on the MNIST dataset.  The left panel demonstrates that \u03b1 is the crucial hyperparameter, with variations in \u03b7 having a smaller effect. The middle panel shows that using the largest value of \u03b7 is optimal for a given \u03b1, while the right panel reveals that very large values of \u03b1 lead to increased loss regardless of \u03b7.", "section": "5.4 Hyperparameter comparison"}, {"figure_path": "kHXUb494SY/figures/figures_17_1.jpg", "caption": "Figure 7: We compare AGNES (red) and SGD (blue) for the optimization of f\u03bc,L with \u03bc = 1 and L = 500 (left) / L = 10\u2074 (right) for different noise levels \u03c3 = 0 (solid line), \u03c3 = 10 (dashed) and \u03c3 = 50 (dotted). In all cases, AGNES improves significantly upon SGD. The noise model used is isotropic Gaussian, but comparable results are obtained for different versions of multiplicatively scaling noise.", "description": "This figure compares the performance of AGNES and SGD on a strongly convex objective function with varying levels of multiplicative noise (\u03c3 = 0, 10, 50).  It shows that AGNES consistently outperforms SGD across different noise levels and problem parameters (L=500 and L=10000). The noise is isotropic Gaussian, but similar results were seen with other multiplicative noise models.", "section": "5.1 Convex optimization"}, {"figure_path": "kHXUb494SY/figures/figures_18_1.jpg", "caption": "Figure 8: We trained ResNet34 on CIFAR-10 with batch size 50 for 40 epochs using NAG. Training losses are reported as a running average with decay rate 0.999 in the left column and test accuracy after every epoch is reported in the right column. Each row represents a specific value of momentum used for NAG (from top to bottom: 0.99, 0.9, 0.8, 0.5, and 0.2) with learning rates ranging from 8.10-5 to 0.5. These hyperparameter choices for NAG were compared against AGNES with the default hyperparameters suggested a = 10-3 (learning rate), \u03b7 = 10-2 (correction step), and p = 0.99 (momentum) as well as AGNES with a slightly smaller learning rate 5\u00b710-4 (with p = 0.99, \u03b7 = 10-2 as well). The same two training trajectories with AGNES are shown in all the plots in shades of blue. The horizontal axes represent the number of optimizer steps.", "description": "This figure compares the performance of NAG and AGNES on the CIFAR-10 dataset using ResNet34.  It shows training loss and test accuracy for various momentum values in NAG, contrasted with the performance of AGNES using default and slightly modified hyperparameters.  The results highlight AGNES's consistent and superior performance across different momentum settings in NAG.", "section": "5.2 Neural network regression"}]