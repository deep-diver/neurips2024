[{"type": "text", "text": "LAMBDA: Learning Matchable Prior For Entity Alignment with Unlabeled Dangling Cases ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Hang Yin, Liyao Xiang\u2217 Dong Ding Shanghai Jiao Tong University Shanghai Jiao Tong University Shanghai, China Shanghai, China {yinhang_SJTU, xiangliyao08}@sjtu.edu.cn 18916162516@sjtu.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Yuheng He, Yihan Wu, Pengzhi Chu, Xinbing Wang ", "page_idx": 0}, {"type": "text", "text": "Shanghai Jiao Tong University Shanghai, China {heyuheng, caracalla, pzchu, xwang8}@sjtu.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Chenghu Zhou Chinese Academy of Sciences Beijing, China zhouchsjtu@gmail.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We investigate the entity alignment (EA) problem with unlabeled dangling cases, meaning that partial entities have no counterparts in the other knowledge graph (KG), yet these entities are unlabeled. The problem arises when the source and target graphs are of different scales, and it is much cheaper to label the matchable pairs than the dangling entities. To address this challenge, we propose the framework Lambda for dangling detection and entity alignment. Lambda features a GNNbased encoder called KEESA with a spectral contrastive learning loss for EA and a positive-unlabeled learning algorithm called iPULE for dangling detection. Our dangling detection module offers theoretical guarantees of unbiasedness, uniform deviation bounds, and convergence. Experimental results demonstrate that each component contributes to overall performances that are superior to baselines, even when baselines additionally exploit $30\\%$ of dangling entities labeled for training. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Entity alignment is a problem that seeks entities referring to the same real-world identity across different knowledge graphs (KGs), and is widely deployed in fields such as knowledge fusion, question-answering, web mining, etc. To address the issue, embedding-based methods have been proposed to capture entity similarity in the embedding space through translation-based [24, 42, 20] or graph neural network (GNN)-based [41, 37, 27, 26, 35] models. Particularly, if the entities do not have counterparts on another KG, the entities are referred to as dangling entities, as shown in Fig. 1. ", "page_idx": 0}, {"type": "text", "text": "In many real-world scenarios, the labels for the dangling entities on KGs are often missing, as those labels are much harder to acquire. For example, in KG plagiarism detection, it is relatively easy to align entity pairs that both exist in KGs, but one would have to traverse all possible pairs to conclude an entity is not paired. Hence EA with unlabeled dangling entities is a hard but realistic problem. The problem even worsens in EA on KGs of different scales where the dangling entities take a large proportion of all nodes. ", "page_idx": 0}, {"type": "text", "text": "Despite that many works have been investigating the EA problem with dangling entities, few have focused on EA with unlabeled dangling cases. We list closely related works in Table 1. The work of [33] extends the conventional EA methods MTransE [5] and AliNet [37] to the case with dangling entities, and thus require a portion of labeled dangling entities for training. Based on their works, MHP [19] has improved performance with additional knowledge, i.e., the high-order proximities information for alignment. Both UED [24] and SoTead [23] are unsupervised schemes that rely on side information such as entity name/attribute as global alignment information. Different from prior works, we consider a stricter case where neither side information nor any labeled dangling entities are known, as side information often leads to name-bias [21, 44] while labels for dangling entities are hard to obtain. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "image", "img_path": "AWFryOJaGi/tmp/e794b3edade62a8c62844353e434cc0a3fe118a5a9ff951d38f7af4440fba5c7.jpg", "img_caption": ["Figure 1: Examples of dangling entities. "], "img_footnote": [], "page_idx": 1}, {"type": "image", "img_path": "AWFryOJaGi/tmp/f8ec94d32c3c02c22920bf951b6f23e61aaddbda6199d65c1dd7982a069897aa.jpg", "img_caption": ["Table 1: Different EA models with dangling cases. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "The EA with unlabeled dangling entities faces unique challenges: first, the unlabeled dangling entities would cause erroneous information to propagate through neighborhood aggregation if applying conventional GNN-based embedding methods, negatively affecting the dangling detection and alignment of matchable entities. Second, the absence of labeled dangling entities makes its feature distribution non-observable, requiring the model to distinguish potential dangling entities while learning the representation of matchable entities. Hence the EA problem has to be solved with mere positive (matchable entities with labels) and unlabeled samples, yet without any prior knowledge of the distribution of the nodes. ", "page_idx": 1}, {"type": "text", "text": "We tackle the first challenge by proposing a novel GNN-based EA framework. To eliminate the \u2018pollution\u2019 of dangling entities, the adaptive dangling indicator has been applied globally for selective aggregation. Relation projection attention is designed to combine both entity and relation information for a more comprehensive representation. The designed spectral contrastive learning loss disentangles the matchable entities from dangling ones while portraying a unified embedding space for entity alignment. ", "page_idx": 1}, {"type": "text", "text": "As to the second challenge, we first derive an unbiased risk estimator and a tighter uniform deviation bound for the positive-unlabeled (PU) learning loss. However, such an estimator still requires prior knowledge of the proportion of positive entities among all nodes. Thus we develop an iterative strategy to estimate such prior knowledge while training the classifier with a PU learning loss. The prior estimation could also be seen as dangling entity detection; if too few entities are determined to be matchable, one can stop all subsequent procedures and decide the two KGs cannot be aligned. ", "page_idx": 1}, {"type": "text", "text": "Our framework Lambda is provided in Fig. 2 where there are two phases corresponding to two trained models \u2014 dangling detection and entity alignment. Both phases share one GNN-based encoder and the spectral contrastive learning loss. The dangling detection additionally uses a positiveunlabeled learning loss. The GNN-based encoder contains both the intra-graph and the cross-graph representation learning modules. After the dangling detection, the estimated proportion of matchable entities is figured for judging whether two KGs could be aligned. Only aligned KGs are sent for EA model training, and then only first-phase predicted matchable entity embeddings are obtained from the EA model for inference. Finally, we select pairs of entities that are mutually nearest by their embeddings as aligned pairs. ", "page_idx": 1}, {"type": "text", "text": "Highlights of our contributions are as follows: we raise the challenging problem of EA with unlabeled dangling entities for the first time. To resolve the issue, we propose the framework Lambda featured by a GNN-based encoder called KEESA with spectral contrastive learning for EA and a positiveunlabeled learning algorithm for dangling detection called iPULE. We provide a theoretical analysis of PU learning on the unbiasedness, uniform deviation bound, and convergence. Experiments on a variety of real-world datasets have demonstrated our alignment performance is superior to baselines, even the baselines with $30\\%$ labeled dangling entities. Our code is available on github2. ", "page_idx": 1}, {"type": "image", "img_path": "AWFryOJaGi/tmp/453b35eda5abaa1b3311f3a76d10e118205eb34241d6290d42c0c0ee5d683f68.jpg", "img_caption": ["Figure 2: The illustration of our framework. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "2 Preliminaries and Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 Entity Alignment ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Embedding-based entity alignment methods have evolved rapidly and are gradually becoming the mainstream approach of EA in recent years due to their flexibility and effectiveness [17], which aim to encode KGs into low-dimensional embedding space to capture the similarities of entities [18, 10]. It could be divided into translation-based [24, 42, 20] and GNN-based [41, 37, 27, 26, 35]. ", "page_idx": 2}, {"type": "text", "text": "Previous EA methods mostly assume a one-to-one correspondence between two KGs, but such an assumption does not always hold and thus leads to a performance drop in real-world cases [38]. Notably, Sun et al. [33] as a pioneering work modeled it upon a supervised setting, i.e., a small set of aligned entities and labeled dangling entities. On this basis, MHP [19] employed more dangling information concerning high-order proximities in both training and inference. UED [24] and SoTead [22] propose an unsupervised translation-based method for joint entity alignment and dangling entity detection without labeled dangling entities, while the practical problem of matching cost is ignored. ", "page_idx": 2}, {"type": "text", "text": "2.2 Positive-Unlabled Learning ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Let $X\\,\\in\\,\\mathbb{R}^{d},d\\,\\in\\,\\mathbb{N}$ , and $Y\\,\\in\\,\\{\\pm1\\}$ be the input and output random variables. We also define $p(x,y)$ to be the joint probability density of $(X,Y)$ , $p_{\\mathrm{p}}(x)={\\bar{p}}(x\\mid y=+1)$ , $p_{\\mathrm{n}}(x)=p(x\\mid y=-1)$ to be the $P$ (Positive) and $N$ (Negative) marginals (a.k.a., class-conditional densities), and $p_{\\mathrm{u}}(x)$ be the $U$ (Unlabeled) marginal. The class-prior probability is expressed as $\\pi_{p}=p(y=+1)$ , which is assumed to be known throughout the paper, and can be estimated from known datasets [13]. ", "page_idx": 2}, {"type": "text", "text": "The PU learning problem setting is as follows: the positive and unlabeled data are sampled independently from $p_{\\mathrm{p}}(x)$ and $p_{\\mathrm{u}}(x)$ as $\\chi_{\\mathrm{p}}=\\{x_{i}^{\\mathrm{p}}\\}_{i=1}^{n_{\\mathrm{p}}}\\sim p_{\\mathrm{p}}(x)$ and $\\mathcal{X}_{\\mathrm{u}}\\,=\\,\\{x_{i}^{\\mathrm{u}}\\}_{i=1}^{n_{\\mathrm{u}}}\\sim p_{\\mathrm{u}}\\bar{(x)}$ , and a classifier is trained from $\\chi_{\\mathrm{p}}$ and ${\\mathcal{X}}_{\\mathrm{u}}$ , in contrast to learning a classifier telling negative samples apart from positive ones. The general assumption of the previous work is to let the unlabeled distribution be equal to the overall data distribution, i.e., $p_{\\mathrm{u}}(x)=p(x)$ since $p_{\\mathrm{u}}(x)$ cannot be obtained, but the assumption hardly holds in many real-world scenarios, for example transductive learning, making methods in [13, 30] infeasible. ", "page_idx": 2}, {"type": "text", "text": "3 Selective Aggregation with Spectral Contrastive Learning ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Notation: Source and target KG $G_{s}\\,=\\,(E_{s},R_{s},T_{s})$ , $G_{t}=(E_{t},R_{t},T_{t})$ stored in triples <entity, relation, entity $>$ : entities $E$ , relations $R$ , and triples $T\\subseteq E\\times R\\times E$ , $E_{s}=D_{s}\\cup M_{s},E_{t}=D_{t}\\cup M_{t}$ , ", "page_idx": 2}, {"type": "text", "text": "where $D$ denotes dangling and $M$ denotes matchable. A set of pre-aligned anchor node pairs are $S=\\{(u,v)|u\\in M_{s},\\bar{v}\\in{\\mathbfcal{M}}_{t},u\\equiv v\\}$ . (see appendix A for more details). ", "page_idx": 3}, {"type": "text", "text": "We start by introducing the KEESA (KG Entity Encoder with Selective Aggregation). ", "page_idx": 3}, {"type": "text", "text": "3.1 KG Entity Encoder with Selective Aggregation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Adaptive Dangling Indicator $\\&$ Relation Projection Attention. Real-world EA tasks often involve graphs with dangling distortion [39, 3]. Conventional GNN aggregation will \u2018pollute\u2019 matchable entities\u2019 embeddings with dangling. However, a hard dangling indicator for the entity is overconfident as only approximate results can be obtained without labels. Incorrect indicators may lead to inappropriate aggregation and thus destruction of the KG structure. Instead, we apply a learnable scalar weight $r_{e_{j}}$ for each $e_{i}$ \u2019s neighboring message: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\pmb{h}_{e_{i}}^{l+1}=\\sigma\\left(\\sum_{e_{j}\\in\\mathcal{N}_{e_{i}}\\cup\\{e_{i}\\}}\\underbrace{\\operatorname{tanh}(r_{e_{j}})}_{\\mathrm{adaptive~dangling~indicator}}\\alpha_{i,j}W^{l+1}\\pmb{h}_{e_{j}}^{l}\\right),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where tanh serves to normalize $r_{e_{j}}$ to the range of $[-1,1]$ . The initialization of $r_{e_{j}}$ is critical, please see the implementation details for more. ", "page_idx": 3}, {"type": "text", "text": "As compressed feature of $e_{j}-r_{e_{j}}$ is a plain scalar, we link relation $r_{k}$ \u2019s embedding $h_{r_{k}}$ to the associated entity $e_{j}$ by $h_{r_{k}}^{\\rightarrow e_{j}}$ for capturing more comprehensive attention. A matrix $W_{r}\\in\\mathbb{R}^{d\\times d}$ with an orthogonal regularizer $L_{o}$ is applied to $h_{r_{k}}$ to perform projection while preserving its norm for better convergence: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{h}_{r_{k}}^{\\rightarrow e_{j}}=r_{e_{j}}W_{r}\\pmb{h}_{r_{k}}\\quad\\mathrm{and}\\quad L_{o}=\\left\\|\\pmb{W}_{r}^{\\top}\\pmb{W}_{r}-\\pmb{I}_{d\\times d}\\right\\|_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The attention coefficient is obtained by the following equation, where $\\pmb{v}^{\\top}$ is the attention vector: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\alpha_{i j k}^{l}=\\frac{\\exp(v^{\\top}h_{r_{k}}^{\\rightarrow e_{j}})}{\\sum_{e_{m}\\in{\\cal{N}}_{e_{i}},<e_{i},r_{n},e_{m}>\\in{\\cal{T}}_{s}\\cup{\\cal{T}}_{t}}\\exp(v^{\\top}h_{r_{n}}^{\\rightarrow e_{m}})}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Intra- & Cross-Graph Representation Learning. Based on the above, we can express the embedding of $e_{i}$ at the $(l+1)$ -th layer $h_{e_{i}}^{l+1}$ as Eq. 1, where $W^{l+1}$ is specified as $W^{l+1}=\\mathbf{\\dot{I}}_{d\\times d}-2h_{r_{k}}h_{r_{k}}^{\\top}$ by the triplet $<e_{i},r_{k},e_{j}>$ inclusive relation embedding $h_{r_{k}}$ . We adopt the $\\operatorname{tanh}(\\cdot)$ as the activation function. The Householder transformation $W^{l+1}$ is applied on the last layer embedding $h_{e_{i}}^{l}$ to restore the useful relative positions of KG entities at each layer recursively. ", "page_idx": 3}, {"type": "text", "text": "Overall, the intra-graph representation $h_{e_{i}}$ of $e_{i}$ is obtained by concatenating embeddings from all layers. Its cross-graph representation $h_{e_{i}}^{p r o x y}$ can be described by $h_{e_{i}}$ and proxy ${\\pmb q}_{j}$ , where the latter is generated by Proxy Matching Attention Layer [25] to align the embeddings across two graphs. With $S_{p}$ representing the set of proxy vectors, and $\\dim(\\cdot)$ denoting the cosine similarity, we have: ", "page_idx": 3}, {"type": "equation", "text": "$$\nh_{e_{i}}=[h_{e_{i}}^{0}||h_{e_{i}}^{1}||...||h_{e_{i}}^{l}]\\quad\\mathrm{~and~}\\quad h_{e_{i}}^{p r o x y}=\\sum_{q_{j}\\in S_{p}}\\frac{\\exp(\\sin(h_{e_{i}},q_{j}))}{\\sum_{q_{k}\\in S_{p}}\\exp(\\sin(h_{e_{i}},q_{k}))}(h_{e_{i}}-q_{j}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Finally, we employ a gating mechanism [31] to integrate both intra-graph representation $h_{e_{i}}$ and cross-graph representation $h_{e_{i}}^{p r o x y}$ into $h_{e_{i}}^{f}$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\theta_{e_{i}}=\\mathrm{sigmoid}(W_{g}h_{e_{i}}^{p r o x y}+b),\\qquad h_{e_{i}}^{f}=[(\\theta_{e_{i}}\\cdot h_{e_{i}}+(1-\\theta_{e_{i}})\\cdot h_{e_{i}}^{p r o x y})\\vert\\vert r_{e_{i}}],\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $W_{g}$ and $^{b}$ are the gate weight and gate bias, respectively. The learnable weight of $e_{i}$ is also attached to the embedding. It is worth noticing that for each entity on either $G_{s}$ or $G_{t}$ , they are encoded by one shared KEESA with below spectral contrastive learning for a unified representation space. ", "page_idx": 3}, {"type": "text", "text": "3.2 Spectral Contrastive Learning ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this part, we propose the spectral contrastive learning loss $\\mathcal{L}_{\\mathrm{info}}$ with high-quality negative sample mining, which serves both tasks (entity alignment and dangling detection) at the same time. ", "page_idx": 3}, {"type": "text", "text": "Specifically, given a pre-aligned matchable entity $e_{i}\\in\\mathcal{X}_{p}$ , let there be a paired positive sample entity $e_{+}^{i}\\in\\mathcal{X}_{p}$ , such that $(e_{i},e_{+}^{i})\\in S$ , and $N$ sampled entity $\\bar{\\{}e_{j}^{i}\\}^{N}$ as negative samples $(e_{i},e_{j}^{i})\\not\\in S$ . The spectral contrastive learning loss is one specific form of alignment loss $H(\\cdot)$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{info}}=\\sum_{e_{i}\\in\\mathcal{X}_{p}}\\log\\left[1+\\sum_{j}^{N}\\exp(\\lambda\\;H(e_{i},e_{+}^{i},e_{j}^{i}))\\right].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Unified Representation for Entity Alignment. We expect a unified feature space where the distance between aligned anchor node pairs is as close as possible, while the unaligned is on the contrary. To satisfy this intuition, we introduce an alignment loss: ", "page_idx": 4}, {"type": "equation", "text": "$$\nH(e_{i},e_{+}^{i},e_{j}^{i})=[\\sin(e_{i},e_{j}^{i})-\\sin(e_{i},e_{+}^{i})+\\gamma]_{+},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $[x]_{+}$ represents $\\operatorname*{max}(0,x)$ and $\\dim(\\cdot)$ indicates $L_{2}$ -norm distance between the embeddings. A soft margin $\\gamma$ is involved to discourage trivial solutions, e.g., $\\sin(e_{i},e_{j}^{i})=\\sin(e_{i},e_{+}^{i})=0$ . ", "page_idx": 4}, {"type": "text", "text": "Discrimination for Dangling Detection. For our proposed dangling detection, the vital task is to discriminate the dangling from the matchable ones with unlabeled dangling entities. Hence unsupervised method of spectral clustering is exploited to separate two types of entities. We design the loss function according to Lemma 1 to achieve its equivalent effect. ", "page_idx": 4}, {"type": "text", "text": "Lemma 1. Given one positive sample $p^{+}$ for $q_{\\mathrm{r}}$ , and $N$ negative samples $\\{p_{j}^{-}\\}^{N}$ (node set: $\\{q,p^{+}\\}\\cup$ $\\{p_{j}^{-}\\}^{N})$ , employing the following loss function is equivalent to conducting spectral clustering on similarity graph $\\pi$ with the temperature hyper-parameter $\\lambda$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\ni n f o r K C E(q,p^{+},\\{p^{-}\\}^{N})=-\\log\\frac{\\exp(\\lambda\\,s i m(q,p^{+}))}{\\exp(\\lambda\\,s i m(q,p^{+}))+\\sum_{j}^{N}\\exp(\\lambda\\,s i m(q,p_{j}^{-}))}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The equivalence has been discussed in previous studies [40, 1, 6, 32]. Regarding our proposed problem, the positive samples are the corresponding pairs whereas the negative samples are those sampled unaligned pairs. The equivalence is derived as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{i n f o N C E(q,p^{+},\\{p^{-}\\}^{N})=\\displaystyle\\log\\displaystyle\\frac{\\exp(\\lambda\\mathrm{sim}(q,p^{+}))+\\sum_{j}^{N}\\exp(\\lambda\\mathrm{sim}(q,p_{j}^{-}))}{\\exp(\\lambda\\mathrm{\\sim}(q,p^{+}))}}}\\\\ {{=\\log[1+\\displaystyle\\frac{\\sum_{j}^{N}\\exp(\\lambda\\mathrm{\\sim}(q,p_{j}^{-}))}{\\exp(\\lambda\\mathrm{\\sim}(q,p^{+}))}]}}\\\\ {{=\\log\\displaystyle\\left[1+\\sum_{j}^{N}\\exp(\\lambda\\mathrm{\\sim}(q,p_{j}^{-})-\\lambda\\mathrm{\\sim}(q,p^{+}))\\right].}}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The spectral contrastive learning loss could be obtained by replacing the exponent term with Eq. 4. ", "page_idx": 4}, {"type": "text", "text": "Remark. In the alignment loss function, we observe that dangling entities are only in the negative samples, and the entities in the positive samples are all matchable. Such a stark asymmetry provides an advantage in distinguishing between dangling and matchable entities. ", "page_idx": 4}, {"type": "text", "text": "We also prove that $\\mathcal{L}_{\\mathrm{info}}$ (Eq. 3) can promote model learning by Lemma 2 (see appendix B for proof). Lemma 2. The loss $\\mathcal{L}_{\\mathrm{info}}$ can mine high-quality negative samples, which we show has an equivalent effect to truncated uniform negative sampling (TUNS) in [35]. ", "page_idx": 4}, {"type": "text", "text": "Minimizing the spectral contrastive loss of Eq. (3) maps matchable and dangling entities into a unified but distinguishable feature space for improved entity alignment while facilitating dangling detection. In practice, we adopt the loss normalization trick [8] on $H(\\cdot)$ to speed up training. ", "page_idx": 4}, {"type": "text", "text": "4 Iterative Positive-Unlabeled Learning for Dangling Detection ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We expect to avoid any additional computational overhead for EA if few entities are matchable for the source and target KG. Thus, we address a more challenging problem in EA: given partial pre-aligned matchable entities as positive samples (i.e., $\\chi_{p,}$ ), how to jointly predict the proportion of matchable entities in the unlabeled nodes (i.e., $\\pi_{\\mathrm{p}.}^{\\mathrm{u}}$ ) and identify those entities? If $\\pi_{\\mathrm{p}}^{\\mathrm{u}}$ could be predicted, it could serve as an indicator whether we should proceed to EA. We propose to address the problem by PU learning. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Unbiased Risk Estimator. First, we propose a new unbiased estimation for PU learning without any constraint (i.e., $p_{\\mathrm{u}}(x)=p(x)$ in [13, 30]) on unlabeled samples distribution $p_{\\mathrm{u}}(x)$ concerning the overall distribution $p(x)$ . Assuming that $\\pi_{\\mathrm{p}}$ and $\\pi_{\\mathrm{p}}^{\\mathrm{u}}$ are known (estimation strategy would be given later), we have: ", "page_idx": 5}, {"type": "text", "text": "Theorem 1. Suppose that $g\\in\\mathcal{G}:\\mathbb{R}^{d}\\to\\mathbb{R}$ is a binary classifier, and $\\ell:\\mathbb{R}\\times\\{\\pm1\\}\\rightarrow\\mathbb{R}$ is the loss function by which $\\ell(t,y)$ means the loss incurred by predicting an output $t$ when the ground truth is y. $\\widehat{R}_{\\mathrm{pu}}(g)$ is the unbiased risk estimator of $R(g)$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\widehat{R}_{\\mathrm{pu}}(g)=\\pi_{\\mathrm{p}}\\widehat{R}_{\\mathrm{p}}^{+}(g)+\\frac{\\pi_{\\mathrm{n}}}{\\pi_{\\mathrm{n}}^{\\mathrm{u}}}\\cdot\\left[\\widehat{R}_{\\mathrm{u}}^{-}(g)-\\pi_{\\mathrm{p}}^{\\mathrm{u}}\\widehat{R}_{\\mathrm{p}}^{-}(g)\\right],\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\pi_{\\mathrm{n}}=p(y=-1)$ and $\\pi_{\\mathrm{n}}^{\\mathrm{u}}=p_{\\mathrm{u}}(y=-1)$ are estimable class priors given $\\pi_{\\mathrm{p}}$ and $\\pi_{\\mathrm{p}}^{\\mathrm{u}}$ , $R_{p}^{+}(g)=$ $\\mathbb{E}_{X\\sim p_{\\mathrm{p}}(x)}[\\ell(g(X),+1)]$ and $R_{n}^{-}(g)=\\mathbb{E}_{X\\sim p_{\\mathrm{n}}(x)}[\\ell(g(X),-1)]$ (see appendix $C$ for proof). ", "page_idx": 5}, {"type": "text", "text": "By our proof, $\\widehat{R}_{\\mathrm{pu}}(g)$ is an unbiased risk bound for the PU learning. More importantly, the bound provided by T hm. 2 is a tighter uniform deviation bound than the classic Non-negative Risk Estimator [13]: ", "page_idx": 5}, {"type": "text", "text": "Theorem 2. Let $\\mathrm{Var}(R)$ denote the uniform deviation bound of risk estimator $R$ , and Non-negative Risk Estimator be $\\widehat{R^{\\prime}}_{\\mathrm{pu}}(g)$ , then: $\\cdot\\mathrm{Var}(\\widehat{R}_{\\mathrm{pu}}(g))\\,<\\,\\mathrm{Var}(\\widehat{R^{\\prime}}_{\\mathrm{pu}}(g))$ (see appendix D for proof). ", "page_idx": 5}, {"type": "text", "text": "Positive Unlabeled Loss Function. Since it is evident that all negative samples exist in unlabeled data, we have $\\frac{\\pi_{\\mathrm{n}}}{\\pi_{\\mathrm{n}}^{\\mathrm{u}}}<1$ and thus we apply a hyper-parameter $\\begin{array}{r}{\\alpha=\\frac{\\bar{\\pi}_{\\mathrm{n}}^{\\mathrm{u}}}{\\pi_{\\mathrm{n}}}}\\end{array}$ to scale $\\pi_{\\mathrm{p}}\\widehat{R}_{\\mathrm{p}}^{+}(g)$ equivalently and $\\operatorname*{max}(\\cdot)$ to restrict the estimated $\\pi_{\\mathrm{n}}R_{\\mathrm{n}}^{-}(g)\\geq0$ . The PU learning loss function is formulated as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{pu}}=\\alpha\\pi_{\\mathrm{p}}\\widehat{R}_{\\mathrm{p}}^{+}(g)+\\operatorname*{max}\\{0,\\widehat{R}_{\\mathrm{u}}^{-}(g)-\\pi_{\\mathrm{p}}^{\\mathrm{u}}\\widehat{R}_{\\mathrm{p}}^{-}(g)\\},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "We specify the corresponding risk function using cross-entropy losses as below: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\hat{\\mathfrak{z}}_{\\mathrm{p}}^{+}(g)=\\frac{1}{|\\mathcal{X}_{p}|}\\sum_{e_{i}\\in\\mathcal{X}_{p}}\\log\\hat{y}_{i}(+1),\\widehat{R}_{\\mathrm{u}}^{-}(g)=\\frac{1}{|\\mathcal{X}_{u}|}\\sum_{e_{i}\\in\\mathcal{X}_{u}}\\log\\hat{y}_{i}(-1),\\widehat{R}_{\\mathrm{p}}^{-}(g)=\\frac{1}{|\\mathcal{X}_{p}|}\\sum_{e_{i}\\in\\mathcal{X}_{p}}\\log\\hat{y}_{i}(-1)\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where the output logit for $e_{i}~\\in~E_{s}\\cup E_{t}$ , being labeled as a state $u\\ \\in\\ \\{+1,-1\\}$ , is $\\hat{y}_{i}(u)\\;=\\;$ $s o f t m a x(\\mathbf{MLP}(h_{e_{i}}^{f}))$ , based on KEESA output $h_{e_{i}}^{f}$ . Hence each term in the final loss can be calculated or estimated without the negative labels. ", "page_idx": 5}, {"type": "text", "text": "Iterative PU Learning with Prior Estimator How could we estimate prior $\\pi_{\\mathrm{p}}$ and $\\pi_{\\mathrm{p}}^{\\mathrm{u}\\,\\cdot}$ Inspired by [11], we introduce a hidden variable in the model as well as an iterative approach. We adopt a variational approximation strategy and a warm-up phase to tackle the cold start problem, as shown in Alg. 1. First, we estimate and fix the class prior $\\pi_{\\mathrm{p}}$ and $\\pi_{\\mathrm{p}}^{\\mathrm{u}}$ by the ratio of the anchor points in the training set. ${\\mathcal{L}}_{\\mathrm{info}}$ is optimized together with $\\mathcal{L}_{\\mathrm{pu}}$ for a discriminative embedding space in the warm-up phase. Finally, we minimize $\\mathcal{L}_{\\mathrm{pu}}$ to update the class prior and the model parameters alternately till convergence. ", "page_idx": 5}, {"type": "text", "text": "The convergence guarantee is provided in Thm. 3, which mostly follows the convergence of EM algorithm. We collect the proof in appendix E. ", "page_idx": 5}, {"type": "text", "text": "Theorem 3. Given the assumptions of marginalization in Eq. 16 and Eq. 17, the objective function of $-\\mathcal{L}_{\\mathrm{pu}}$ is the same as the expectation function $Q$ of Eq. 13 where the loss function is the cross en$C E(\\bar{y}_{i},\\hat{y}_{i})=-\\bar{y}_{i}(+1)\\log\\hat{y}_{i}(+1)-\\bar{y}_{i}(\\stackrel{\\cdot}{-1})\\log\\hat{y}_{i}(-1)$ on the preference condition: $\\begin{array}{r}{\\sum_{j\\in\\mathcal{U}}\\frac{1}{|\\mathcal{U}|}\\log\\frac{\\hat{y}_{j}(+1)}{\\hat{y}_{j}(-1)}\\approx\\sum_{i\\in\\mathcal{P}}\\frac{1}{|\\mathcal{P}|}\\log\\frac{\\hat{y}_{i}(+1)}{\\hat{y}_{i}(-1)}}\\end{array}$ ", "page_idx": 5}, {"type": "text", "text": "The iterative process of our method is a special case of the EM algorithm. We hold the same assumptions as the EM algorithm and we further assume the training of $f$ is able to find the globally optimal $\\theta$ . Although the assumptions seem to be too strict, the algorithm typically converges in practice as we verified in the experimental section. ", "page_idx": 5}, {"type": "text", "text": "Algorithm 1 iPULE (iterative PU Learning with Prior Estimator) ", "page_idx": 6}, {"type": "text", "text": "Require: $G_{s}$ and $G_{t}$ are treated as one input graph $G\\;=\\;(\\mathcal{V},\\mathcal{E})$ , positive-node set ${\\mathcal P}\\;=\\;{\\mathcal X}_{p}$ , unlabeled-node set $\\mathcal{U}=\\mathcal{X}_{u}$ , classifier $f$ with initial parameters $\\theta_{0}$ , KEESA ${\\mathrm{Enc}}(G,\\psi)$ with initial parameters $\\psi_{0}$ and warm-up epoch $N$ . $\\mathcal{L}$ represents training loss. ", "page_idx": 6}, {"type": "text", "text": "Ensure: Model parameters $\\theta,\\psi$ and estimated prior ${\\hat{\\pi}}_{\\mathrm{p}}$ and $\\hat{\\pi}_{\\mathrm{p}}^{\\mathrm{u}}$ ", "page_idx": 6}, {"type": "text", "text": "1 $\\begin{array}{r}{:l\\gets\\infty,\\quad\\hat{\\pi}_{\\mathrm{p}}^{\\mathrm{u}}\\gets\\hat{\\pi}_{\\mathrm{p}}\\gets\\frac{|\\mathcal{P}|}{|\\mathcal{P}|+|\\mathcal{U}|},\\quad i\\gets0,\\quad\\beta=\\beta_{0};}\\end{array}$   \n2: $\\mathcal{L}\\leftarrow\\beta\\cdot\\mathcal{L}_{\\mathrm{info}}+(1-\\beta)\\cdot\\mathcal{L}_{\\mathrm{pu}}$ ; ", "page_idx": 6}, {"type": "text", "text": "3: repeat ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "//Entity embedding matrix $\\mathbf{X}$ //Optimize Enc(\u00b7) and $f$ jointly ", "page_idx": 6}, {"type": "text", "text": "7: until N epochs is over ", "page_idx": 6}, {"type": "text", "text": "//Warm-up phase to solve cold start ", "page_idx": 6}, {"type": "text", "text": "8: $\\mathcal{L}\\gets\\mathcal{L}_{\\mathrm{pu}}$ ; ", "page_idx": 6}, {"type": "text", "text": "9: repeat ", "text_level": 1, "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{\\dot{X}}\\leftarrow\\operatorname{Enc}(G,\\psi),\\quad\\hat{y}_{i}\\leftarrow f(\\mathbf{X},i;\\theta)\\mathrm{~for~all~}i\\in\\mathcal{V};}\\\\ &{\\hat{\\pi}_{\\mathrm{{p}}}^{\\mathrm{u}}\\leftarrow|\\mathcal{U}|^{-1}\\sum_{i\\in\\mathcal{U}}\\mathbb{I}[\\hat{y}_{i}(+1)>0.5],\\quad\\hat{\\pi}_{\\mathrm{{p}}}\\leftarrow\\frac{|\\mathcal{P}|+|\\mathcal{U}|\\cdot\\hat{\\pi}_{\\mathrm{{p}}}^{\\mathrm{u}}}{|\\mathcal{P}|+|\\mathcal{U}|};}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "13: $\\theta,\\psi\\gets\\arg\\operatorname*{max}_{\\theta,\\psi}-\\mathcal{L}(\\theta;\\mathbf{X},\\mathbf{y},\\mathcal{P},\\mathcal{U});$ ; ", "page_idx": 6}, {"type": "text", "text": "14: until $|l^{\\prime}-l|$ converge $\\mathbf{OR}\\;\\hat{\\pi}_{\\mathrm{p}}$ converge ", "page_idx": 6}, {"type": "text", "text": "15: return ", "page_idx": 6}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Our method is evaluated on datasets GA16K, DBP2.0, and GA-DBP15K. DBP2.0 and GA-DBP15K are used for the verification of iPULE. To address incomparability caused by inconsistent metrics, we adopt the GA16K dataset to enable compromised comparison of the Dangling-Entities-Unaware EA method. We further compare our method with dangling aware baselines on DBP2.0. Statistics of experimental dataset in appendix F, and additional experiment in appendix G. ", "page_idx": 6}, {"type": "text", "text": "Datasets. The training/test sets for each dataset are generated using a fixed random seed. For entity alignment, $30\\%$ of matchable entity pairs constitute the training set, while the remaining form the test set. For dangling entity detection, we did not utilize any labeled dangling entity data, in contrast to prior work which labels an extra $30\\%$ of the dangling entities for training [33, 19]. ", "page_idx": 6}, {"type": "text", "text": "Baselines. Since our work does not take advantage of any side information, we emphasize its comparison with the previous methods purely depending on graph structures. These works majorly incorporate two types: ", "page_idx": 6}, {"type": "text", "text": "Dangling-Entities-Unaware. We include advanced entity alignment methods in recent years: GCNAlign [41], RSNs [9], MuGNN [4], KECG [16]. Methods with bootstrapping to generate semisupervised structure data are also adopted: BootEA [35], TransEdge [36], MRAEA [26], AliNet [37], and Dual-AMN [25]. ", "page_idx": 6}, {"type": "text", "text": "Dangling-Entities-Aware. To the best of our knowledge, the method of [33] is the most fairly comparable baseline which is based on MTransE [5] and AliNet [37]. Because MHP [19] overemphasized more use of labeled dangling data like high-order similarity information which is also based on the above two methods, while SoTead [22] and UED [24] utilize additional sideinformation. SoTead [22] and UED [24] can only execute the degraded version on DBP2.0 cause no side-information is available on that. We exclude them from baselines for our methods. [33] introduces three techniques to address the dangling entity issue: nearest neighbor (NN) classification, marginal ranking (MR), and background ranking (BR). ", "page_idx": 6}, {"type": "text", "text": "Implementation Detail. We use the Keras framework for developing our approach. Our experiments are conducted on a workstation with an NVIDIA Tesla A100 GPU, and 80GB memory. ", "page_idx": 6}, {"type": "text", "text": "By default, the embedding dimension is set to 128 with the depth of GNN set to 2 and a dropout rate of 0.3. A total of 64 proxy vectors are used and margin $\\gamma=1$ . RMSprop optimizer is adopted with a learning rate of 0.005 and batch size of 5120. $\\lambda$ is set to be 30. $\\beta$ is set as 1e-3 for all datasets. CSLS [14] is adopted as the distance metric for alignment. As we found, the tanh function changes rapidly in the region close to 0 but stays stable in the region beyond $[-3,3]$ . Hence we initialize the $r_{e_{j}}$ to 1 to prevent gradients oscillation or near-zero gradients. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "5.1 Experiments of iPULE Convergence and Class Prior Estimation ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "$\\mathrm{DBP}2.0\\ \\pi_{\\mathrm{p}}$ between $20\\%{-}50\\%$ contains more entities to be aligned, trained by pre-aligned $9\\%{-}15\\%$ nodes then judged by iPULE as aligned KGs. GA-DBP15K $\\pi_{\\mathrm{p}}$ between $10\\%{-}25\\%$ are treated as unaligned KGs ignoring the pre-aligned part, trained by all pre-aligned $10\\%{-}25\\%$ nodes. We get accurate estimation and convergence results as shown in Fig. 3. As iPULE progresses, the estimated class prior gradually approaches the true value as the first row for GA-DBP15K and the second for DBP2.0. $\\pi_{\\mathrm{p}}^{\\mathrm{u}}=0$ for GA-DBP15K while DBP2.0\u2019s are given by red dotted line respectively. The $\\pi_{\\mathrm{p}}$ for GA-DBP15K is stably consistent as pre-aligned proportion due to accurate estimation of its $\\pi_{\\mathrm{p}}^{\\mathrm{u}}$ . As common in PU learning [43], iPULE treats more nodes as positive when $\\pi_{\\mathrm{p}}\\approx50\\%$ in FR-EN. ", "page_idx": 7}, {"type": "image", "img_path": "AWFryOJaGi/tmp/d305324823b027a294c606a2d3f3da2c622b10261065213cc3434636403838de.jpg", "img_caption": ["Figure 3: Prior estimation GA-DBP15K and DBP2.0. (loss convergence in appendix F). "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "5.2 Experiments Unaware of Dangling Entities ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We show the experiments on baselines without considering dangling entities in this section. ", "page_idx": 7}, {"type": "text", "text": "Dangling-Entities-Unaware Baselines Comparison. The direct comparison between our method and the dangling-entities-unaware baselines is unavailable due to inconsistent metrics used. Hence, we adopt the GA16K dataset as a compromise and do not remove any detected dangling entities for entity alignment. Thus the ranking list $S$ in Hits $@\\,\\mathrm{K}$ only contains (matchable) entities in the source graph since GA16K only contains dangling entities in the target KG. In Tab. 2, Dual-AMN demonstrates a competitive performance but is inferior to ours at $\\mathrm{Hits}(\\varpi1$ . MRAEA performs similarly to Dual-AMN since the latter is built on the former. TransEdge performs poorly since the method adopts semi-supervised bootstrapping to mine anchor entities iteratively. The presence of dangling entities could lead to false anchors and spread of error. Meanwhile, it is also a relation-centric approach that suffers from insufficient relation information on GA16K. Other baselines exhibit up-to-par performance but our method delivers consistently superior or state-of-the-art Hits $@\\,\\mathrm{Ks}$ . ", "page_idx": 7}, {"type": "text", "text": "5.3 Experiments Aware of Dangling Entities ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We provide a comparison of dangling detection and entity alignment with baselines aware of dangling. ", "page_idx": 7}, {"type": "text", "text": "Dangling Entities Detection Performance. We test our method\u2019s dangling detection performance compared with baselines aware of dangling entities. The results on DBP2.0 in the consolidated setting are reported in Tab. 12. Note that the comparison is unfair as we don\u2019t use $30\\%$ of the labeled dangling entities as the baselines. Nevertheless, our approach maintains SOTA performance across all six datasets, excelling in almost every metric except for a slightly inferior precision. ", "page_idx": 7}, {"type": "image", "img_path": "AWFryOJaGi/tmp/a19c794c1994b03fdaeb0d10f6ed0b36753044505e20a7441b1ac59145ced1ea.jpg", "img_caption": ["Figure 4: Visualization of entity representationsTable 2: Performance comparison with danglinglearned by our method on GA16K dataset. entities-unaware baselines on GA16K. "], "img_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "AWFryOJaGi/tmp/c258681ab396583ad6762136f5b0f910afac24df6e93df41adad450641be4f7a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "AWFryOJaGi/tmp/8faf50a3a0f9516b381ee4d51b6b9ea01b01c8af047c95e5c5040f133226f09d.jpg", "table_caption": ["Table 3: Dangling detection results on DBP2.0 in the consolidated setting. ", "Table 4: Entity alignment results on DBP2.0 in the consolidated setting. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Dangling-Entities-Aware Baselines Comparison. Tab. 4 reports the entity alignment performance comparison in the consolidated setting on DBP2.0. The precision, recall, and F1 scores are computed according to Eq. (19), (20), (21) in Metric part of appendix F, respectively. We test the entity alignment performance of our method in comparison with baselines that are aware of dangling entities. Our method still maintains almost state-of-the-art performance, but there is still a slightly inferior precision problem. It makes us wonder about the reasons behind it. ", "page_idx": 8}, {"type": "text", "text": "How does our method work? To understand why our method works and its precision slightly suffers, we visualized all entity embeddings of GA16K in Fig. 4. As shown above, matchable entities are denoted as red and green in source and target KG respectively, while dangling as blue. The distribution of three types of entity in Fig. 4(a) suggests our method maps all nodes into a unified embedding space where matchable entities exhibit considerable overlap and are appropriately aligned (shown in Fig. 4(b)). Fig. 4(c)(d) depicts that a part of the dangling entities is intertwined with the matchable ones, suggesting that this part resides at the decision boundary and easily leads to false positives which explain the lesser precision of our method. ", "page_idx": 8}, {"type": "text", "text": "5.4 Ablation Studies and Varying Anchor Nodes ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We conduct ablation studies to show each module\u2019s impact, and similarly pre-aligned entities\u2019 impact. ", "page_idx": 9}, {"type": "text", "text": "Ablation Studies The impact of adaptive dangling indicator and relation projection attention in our method are investigated. We denote the counterpart removing $r_{e_{i}}$ as $w/o\\ r_{e_{i}}$ , and replacing $h_{r_{k}}^{\\rightarrow e_{j}}$ with the original $h_{r_{k}}$ as $w/o\\ h_{r_{k}}^{\\rightarrow e_{j}}$ . Fig. 5 gives the ablation study results on DBP2.0, where \u2018Ours\u2019 represents an all-inclusive model. We observe that the $h_{r_{k}}^{\\rightarrow e_{j}}$ has a more substantial impact than $r_{e_{i}}$ to the alignment performance. As to why the $r_{e_{i}}$ has a minor impact on the alignment, we consider it may be attributed to the lower degrees of dangling entities on DBP2.0. The degrees of dangling entities are generally lower than that of matchable ones, indicating that the dangling is more isolated in the graph and thus has less impact on matchable nodes in the neighborhood aggregation. ", "page_idx": 9}, {"type": "image", "img_path": "AWFryOJaGi/tmp/a17a27a5972c523a031f88e5ecd8bced3e3a870ca58f6d87db3ff812f804ca81.jpg", "img_caption": ["Figure 5: The ablation study of entity alignment performance in the consolidated setting on DBP2.0. "], "img_footnote": [], "page_idx": 9}, {"type": "image", "img_path": "AWFryOJaGi/tmp/01abfe09aa91a457345699a84df2b3bc92fbaef1a3771526b7922885bfb38604.jpg", "img_caption": ["Figure 6: The entity alignment performance on varying pre-aligned anchor nodes ratios on DBP2.0. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Varying Anchor Nodes. Pre-aligned entities may be far scarce in reality. The sensitivity of our method to the proportion variation of anchor nodes is investigated. As the proportion increases, the alignment performance enhances as provided in Fig. 6. ", "page_idx": 9}, {"type": "text", "text": "Notably, even with an anchor ratio as low as $5\\%$ , our alignment accuracy still well exceeds $30\\%$ on most datasets except for FR-EN and EN-FR. Cause they contain twice as many entities and triples as ZH-EN and JA-EN, which introduces intricate dependencies among entities and thus greater challenges in alignment. Moreover, a larger graph may require a higher dimension of representations to learn, but the embedding dimension is restricted to merely 96 due to the out-of-memory problem. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We found that previous EA methods suffer from great performance decline if dangling entities are considered. Our goal is to address the EA problem with unlabeled dangling entities. A novel framework Lambda for detecting dangling entities and then pairing alignment is proposed. The core idea is to perform selective aggregation with spectral contrastive learning and to adopt theoretically guaranteed PU learning to relieve the dependence on the labeled dangling entities. Experimental results on multiple representative datasets demonstrate the effectiveness of our proposed approach. This work also has important implications for real-world applications, such as EA of different scales, KG plagiarism detection, etc. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "The research was supported in part by NSF China (No. 61960206002, 62272306, 62032020, 62136006). ", "page_idx": 10}, {"type": "text", "text": "The authors would like to thank the reviewers for their constructive comments and appreciate the Student Innovation Center of SJTU for providing GPUs. Hang Yin personally thanks Chenyu Liu, Yuting Feng, Jingyuan Zhou, and Qingyang Liu for feedbacks on early versions of this paper. Hang Yin would also like to thank Professor Yuan Luo for his inspiration in the information theory course (CS7317) of Shanghai Jiao Tong University. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Hugues Van Assel, Thibault Espinasse, Julien Chiquet, and Franck Picard. A probabilistic graph coupling view of dimension reduction. In Adcances in neural information processing systems (NeurIPS), pages 10696\u201310708, 2022.   \n[2] S\u00f6ren Auer, Christian Bizer, Georgi Kobilarov, Jens Lehmann, Richard Cyganiak, and Zachary G. Ives. Dbpedia: A nucleus for a web of open data. In international semantic web conference (ISWC), pages 722\u2013735, 2007.   \n[3] Khalid Belhajjame and Mohamed-Yassine Mejri. Online maintenance of evolving knowledge graphs with rdfs-based saturation and why-provenance support. Journal of Web Semantics (JoWS), 78:100796, 2023.   \n[4] Yixin Cao, Zhiyuan Liu, Chengjiang Li, Juanzi Li, and Tat-Seng Chua. Multi-channel graph neural network for entity alignment. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (ACL), pages 1452\u20131461, 2019.   \n[5] Muhao Chen, Yingtao Tian, Mohan Yang, and Carlo Zaniolo. Multilingual knowledge graph embeddings for cross-lingual knowledge alignment. In Proceedings of the 26th International Joint Conference on Artificial Intelligence (IJCAI), pages 1511\u20131517, 2017.   \n[6] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. A simple framework for contrastive learning of visual representations. In International conference on machine learning (ICML), pages 1597\u20131607, 2020.   \n[7] Cheng Deng, Yuting Jia, Hui Xu, Chong Zhang, Jingyao Tang, Luoyi Fu, Weinan Zhang, Haisong Zhang, Xinbing Wang, and Chenghu Zhou. Gakg: A multimodal geoscience academic knowledge graph. In Proceedings of the 30th ACM International Conference on Information & Knowledge Management (CIKM), pages 4445\u20134454, 2021.   \n[8] Yunjun Gao, Xiaoze Liu, Junyang Wu, Tianyi Li, Pengfei Wang, and Lu Chen. Clusterea: Scalable entity alignment with stochastic training and normalized mini-batch similarities. In ACM SIGKDD Conference on Knowledge Discovery and Data Mining (SIGKDD), pages 421\u2013 431, 2022.   \n[9] Lingbing Guo, Zequn Sun, and Wei Hu. Learning to exploit long-term relational dependencies in knowledge graphs. In International conference on machine learning (ICML), pages 2505\u20132514, 2019.   \n[10] Shaoxiong Ji, Shirui Pan, Erik Cambria, Pekka Marttinen, and S Yu Philip. A survey on knowledge graphs: Representation, acquisition, and applications. IEEE Transactions on Neural Networks and learning systems (TNNLS), 33(2):494\u2013514, 2021.   \n[11] Yibo Jiang and Bryon Aragam. Learning nonparametric latent causal graphs with unknown interventions. Advances in Neural Information Processing Systems, 36, 2024.   \n[12] Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks, 2016.   \n[13] Ryuichi Kiryo, Gang Niu, Marthinus C Du Plessis, and Masashi Sugiyama. Positive-unlabeled learning with non-negative risk estimator. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems (NIPS), pages 1675\u20131685, 2017.   \n[14] Guillaume Lample, Alexis Conneau, Marc\u2019Aurelio Ranzato, Ludovic Denoyer, and Herv\u00e9 J\u00e9gou. Word translation without parallel data. In International Conference on Learning Representations (ICLR), 2018.   \n[15] Kasper Green Larsen and Jelani Nelson. Optimality of the johnson-lindenstrauss lemma. In 2017 IEEE 58th Annual Symposium on Foundations of Computer Science (FOCS), pages 633\u2013638, 2017.   \n[16] Chengjiang Li, Yixin Cao, Lei Hou, Jiaxin Shi, Juanzi Li, and Tat-Seng Chua. Semi-supervised entity alignment via joint knowledge embedding model and cross-graph model. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP, pages 2723\u20132732, 2019.   \n[17] Yangning Li, Jiaoyan Chen, Yinghui Li, Yuejia Xiang, Xi Chen, and Hai-Tao Zheng. Vision, deduction and alignment: An empirical study on multi-modal knowledge graph alignment. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 1\u20135. IEEE, 2023.   \n[18] Yangning Li, Yinghui Li, Xi Chen, Hai-Tao Zheng, and Ying Shen. Active relation discovery: Towards general and label-aware open relation extraction, 2023.   \n[19] Juncheng Liu, Zequn Sun, Bryan Hooi, Yiwei Wang, Dayiheng Liu, Baosong Yang, Xiaokui Xiao, and Muhao Chen. Dangling-aware entity alignment with mixed high-order proximities. Findings of the Association for Computational Linguistics: NAACL 2022, 2022.   \n[20] Xiao Liu, Haoyun Hong, Xinghao Wang, Zeyi Chen, Evgeny Kharlamov, Yuxiao Dong, and Jie Tang. Selfkg: Self-supervised entity alignment in knowledge graphs. In ACM Web Conference (WWW), pages 860\u2013870, 2022.   \n[21] Xiaoze Liu, Junyang Wu, Tianyi Li, Lu Chen, and Yunjun Gao. Unsupervised entity alignment for temporal knowledge graphs. In Proceedings of the ACM Web Conference 2023, pages 2528\u20132538, 2023.   \n[22] Gongxu Luo, Jianxin Li, Hao Peng, Carl Yang, Lichao Sun, Philip S. Yu, and Lifang He. Graph entropy guided node embedding dimension selection for graph neural networks. In Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI, pages 2767\u20132774, 2021.   \n[23] Shengxuan Luo, Pengyu Cheng, and Sheng Yu. Semi-constraint optimal transport for entity alignment with dangling cases, 2022.   \n[24] Shengxuan Luo and Sheng Yu. An accurate unsupervised method for joint entity alignment and dangling entity detection. In Findings of the Association for Computational Linguistics (ACL), pages 2330\u20132339, 2022.   \n[25] Xin Mao, Wenting Wang, Yuanbin Wu, and Man Lan. Boosting the speed of entity alignment $10\\times$ : Dual attention matching network with normalized hard sample mining. In Proceedings of the Web Conference 2021 (WWW), pages 821\u2013832, 2021.   \n[26] Xin Mao, Wenting Wang, Huimin Xu, Man Lan, and Yuanbin Wu. Mraea: an efficient and robust entity alignment approach for cross-lingual knowledge graph. In Proceedings of the 13th International Conference on Web Search and Data Mining (WSDM), pages 420\u2013428, 2020.   \n[27] Xin Mao, Wenting Wang, Huimin Xu, Yuanbin Wu, and Man Lan. Relational reflection entity alignment. In ACM International Conference on Information & Knowledge Management (CIKM), pages 1095\u20131104, 2020.   \n[28] Xinnian Mao, Wenting Wang, Yuanbin Wu, and Man Lan. Lightea: A scalable, robust, and interpretable entity alignment framework via three-view label propagation. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 825\u2013838, 2022.   \n[29] Kevin P Murphy. Machine learning: a probabilistic perspective. MIT press, 2012.   \n[30] Gang Niu, Marthinus Christoffel Du Plessis, Tomoya Sakai, Yao Ma, and Masashi Sugiyama. Theoretical comparisons of positive-unlabeled learning against positive-negative learning. In Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems (NeurIPS), pages 1199\u20131207, 2016.   \n[31] Rupesh Kumar Srivastava, Klaus Greff, and J\u00fcrgen Schmidhuber. Highway networks, 2015.   \n[32] Yifan Sun, Changmao Cheng, Yuhan Zhang, Chi Zhang, Liang Zheng, Zhongdao Wang, and Yichen Wei. Circle loss: A unified perspective of pair similarity optimization. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (CVPR), pages 6398\u2013 6407, 2020.   \n[33] Zequn Sun, Muhao Chen, and Wei Hu. Knowing the no-match: Entity alignment with dangling cases. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (ACL-IJCNLP), pages 3582\u20133593, 2021.   \n[34] Zequn Sun, Wei Hu, and Chengkai Li. Cross-lingual entity alignment via joint attributepreserving embedding. In International Semantic Web Conference (ISWC), pages 628\u2013644, 2017.   \n[35] Zequn Sun, Wei Hu, Qingheng Zhang, and Yuzhong Qu. Bootstrapping entity alignment with knowledge graph embedding. In International Joint Conference on Artificial Intelligence (IJCAI), volume 18, pages 4396\u20134402, 2018.   \n[36] Zequn Sun, Jiacheng Huang, Wei Hu, Muhao Chen, Lingbing Guo, and Yuzhong Qu. Transedge: Translating relation-contextualized embeddings for knowledge graphs. In The Semantic Web\u2013 ISWC 2019: 18th International Semantic Web Conference (ISWC), pages 612\u2013629, 2019.   \n[37] Zequn Sun, Chengming Wang, Wei Hu, Muhao Chen, Jian Dai, Wei Zhang, and Yuzhong Qu. Knowledge graph alignment network with gated multi-hop neighborhood aggregation. In AAAI Conference on Artificial Intelligence (AAAI), volume 34, pages 222\u2013229, 2020.   \n[38] Zequn Sun, Qingheng Zhang, Wei Hu, Chengming Wang, Muhao Chen, Farahnaz Akrami, and Chengkai Li. A benchmarking study of embedding-based entity alignment for knowledge graphs. Proceedings of the VLDB Endowment, 13(12), 2020.   \n[39] Anil Surisetty, Deepak Chaurasiya, Nitish Kumar, Alok Singh, Gaurav Dhama, Aakarsh Malhotra, Ankur Arora, and Vikrant Dey. Reps: Relation, position and structure aware entity alignment. In Companion Proceedings of the Web Conference 2022 (WWW), pages 1083\u20131091, 2022.   \n[40] Zhiquan Tan, Yifan Zhang, Jingqin Yang, and Yang Yuan. Contrastive learning is spectral clustering on similarity graph, 2023.   \n[41] Zhichun Wang, Qingsong Lv, Xiaohan Lan, and Yu Zhang. Cross-lingual knowledge graph alignment via graph convolutional networks. In Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 349\u2013357, 2018.   \n[42] Kun Xu, Liwei Wang, Mo Yu, Yansong Feng, Yan Song, Zhiguo Wang, and Dong Yu. Crosslingual knowledge graph alignment via graph matching neural network. In Conference of the Association for Computational Linguistics (ACL), pages 3156\u20133161, 2019.   \n[43] Jaemin Yoo, Junghun Kim, Hoyoung Yoon, Geonsoo Kim, Changwon Jang, and U Kang. Graph-based pu learning for binary and multiclass classification without class prior. Knowledge and Information Systems, 64(8):2141\u20132169, 2022. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "[44] Ziheng Zhang, Hualuo Liu, Jiaoyan Chen, Xi Chen, Bo Liu, Yuejia Xiang, and Yefeng Zheng. An industry evaluation of embedding-based entity alignment. In Proceedings of the 28th International Conference on Computational Linguistics: Industry Track, pages 179\u2013189, 2020. ", "page_idx": 13}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A Notation ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.1 Definitions ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Definition 1 (Knowledge graph). Knowledge graph $(K G)$ is a directed graph $G\\;=\\;(E,R,T)$ comprising three distinct sets: entities $E$ , relations $R$ , and triples $T\\subseteq E\\times R\\times E$ . KG is stored in the form of triples <entity, relation, entity>, with entities denoted by nodes and the relation between entities defined by edges. ", "page_idx": 14}, {"type": "text", "text": "Definition 2 (Entity alignment). Given source $K G$ and target $K G$ , corresponding to $\\begin{array}{r l}{G_{s}}&{{}=}\\end{array}$ $(E_{s},R_{s},T_{s})$ and $G_{t}\\,=\\,(E_{t},R_{t},T_{t})$ respectively, and $A\\,=\\,\\{(u,v)|u\\in E_{s},v\\in E_{t},u\\equiv v\\}$ a set of pre-aligned anchor node pairs, where $\\equiv$ indicates equivalence, the goal of entity alignment is to identify additional pairs of potentially equivalent entities using information from $G_{s}$ , $G_{t}$ , and $A$ . This task typically assumes a one-to-one correspondence between $E_{s}$ and $E_{t}$ . ", "page_idx": 14}, {"type": "text", "text": "Definition 3 (Entity alignment with dangling cases). Let entities in the source and target graphs be composed of two types of nodes: $E_{s}=D_{s}\\cup M_{s},E_{t}=D_{t}\\cup M_{t}$ , where $D_{s},D_{t}$ denote dangling sets that contain entities that have no counterparts, and $M_{s},M_{t}$ are matchable sets. $A$ set of prealigned anchor node pairs are $S=\\{(u,v)|u\\in M_{s},v\\in M_{t},u\\equiv v\\}$ . The task seeks to discover the remaining aligned entities given $G_{s}$ , $G_{t}$ , and $S$ . ", "page_idx": 14}, {"type": "text", "text": "A.2 Transductive Learning: ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Transductive learning models are trained from observed, specific (training) cases to specific (test) cases, employing both training and test information except for test labels. In contrast, the inductive learning model is reasoning from observed training cases to general rules, which are then applied to the test cases. Let\u2019s get down to the EA task. As KG structure is accessible through given triples, which can accurately describe the connections between entities. When we attempt to figure out entity alignment tasks, the KG structure information of the whole source and target KGs is what we could exploit, covering potentially (test) equivalent entities\u2019 relative positions. That\u2019s why we confine the problem field to transductive learning. ", "page_idx": 14}, {"type": "text", "text": "A.3 Graph Convolutional Networks ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "For graph convolutional networks (GCN) [12], the embedding $h_{e_{i}}^{l+1}$ of node $e_{i}$ at the $l+1$ -th layer is updated iteratively by aggregating node features of the neighboring nodes $\\mathcal{N}_{e_{i}}$ from the prior layer: ", "page_idx": 14}, {"type": "equation", "text": "$$\nh_{e_{i}}^{l+1}=\\sigma\\left(\\sum_{e_{j}\\in\\mathcal{N}_{e_{i}}\\cup\\{e_{i}\\}}\\alpha_{i,j}W^{l+1}h_{e_{j}}^{l}\\right),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where each embedding $h_{e_{i}}^{l}$ represents the $d$ -dimensional embedding vector of $e_{i}$ , $\\alpha_{i,j}$ denotes the weight coefficient between $e_{i}$ and $e_{j}$ , $W^{l+1}$ being the transformation matrix of the $(l+1)$ -th GNN layer, and $\\sigma$ being the activation function. ", "page_idx": 14}, {"type": "text", "text": "B Proof for Lemma 2 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Proof. The $\\mathcal{L}_{\\mathrm{info}}$ also has a good effect on mining high-quality negative samples, which we show has an equivalent effect to truncated uniform negative sampling (TUNS) in [35]. TUNS points out that negative samples obtained by uniform random sampling are highly redundant since only high-quality negative samples improve the model. Thus TUNS chooses the K-nearest neighbors of the $e_{i}$ as the negative samples, which are most challenging to distinguish. In the special case of $K=1$ , the loss can be written as $\\begin{array}{r}{\\mathcal{L}_{\\mathrm{TUNS}}=\\sum_{e_{i}\\in\\mathcal{X}_{p}}\\operatorname*{max}_{j}(\\bar{H}(e_{i},e_{+}^{i},e_{j}^{\\bar{i}}))}\\end{array}$ . If we approximate the max function by the LogSumExp, the contrastive loss function turns to ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{TUNS}}\\approx\\sum_{e_{i}\\in\\mathcal{X}_{p}}\\frac{1}{\\lambda}\\log\\left(\\sum_{j}^{N}\\exp(\\lambda\\,H(e_{i},e_{+}^{i},e_{j}^{i}))\\right),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "minimizing which is equivalent to minimizing Eq. (3). Hence our contrastive learning loss is actually a special form of TUNS. For randomly sampled negative samples, ${\\mathcal{L}}_{\\mathrm{info}}$ can play a role in preferentially optimizing high-quality negative samples. ", "page_idx": 15}, {"type": "text", "text": "C Proof for Theorem 1 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Proof. The risk of $g$ is $R(g)\\ =\\ \\mathbb{E}_{(X,Y)\\sim p(x,y)}[\\ell(g(X),Y)]\\ =\\ \\pi_{p}R_{\\mathrm{p}}^{+}(g)\\,+\\,\\pi_{n}R_{\\mathrm{n}}^{-}(g)$ in positive negative learning problems. In positive-unlabeled learning where $\\chi_{\\mathrm{n}}$ is unavailable, we can only approximate $R(g)$ by positive samples and unlabeled samples. We represent the unlabeled distribution as $p_{\\mathrm{u}}(x)\\;=\\;\\pi_{\\mathrm{n}}^{\\mathrm{u}}p_{\\mathrm{n}}(x)\\,+\\,\\pi_{\\mathrm{p}}^{\\mathrm{u}}p_{\\mathrm{p}}(x)$ , so that the negative distribution can be written as $\\begin{array}{r}{\\pi_{\\mathrm{n}}p_{\\mathrm{n}}(x)\\,=\\,\\frac{\\pi_{\\mathrm{n}}}{\\pi_{\\mathrm{n}}^{u}}\\,\\cdot\\,\\left[p_{\\mathrm{u}}(x)-\\pi_{\\mathrm{p}}^{u}p_{\\mathrm{p}}(x)\\right]}\\end{array}$ . Provided $R_{\\mathrm{p}}^{-}(g)\\,=\\,\\mathbb{E}_{X\\sim p_{\\mathrm{p}}(x)}[\\ell(g(X),-1)]$ and $R_{\\mathrm{u}}^{-}(g)=\\mathbb{E}_{X\\sim p_{\\mathrm{u}}(x)}[\\ell(g(X),-1)]$ , we obtain that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\pi_{\\mathrm{n}}R_{\\mathrm{n}}^{-}(g)=\\frac{\\pi_{\\mathrm{n}}}{\\pi_{\\mathrm{n}}^{\\mathrm{u}}}\\cdot\\left[R_{\\mathrm{u}}^{-}(g)-\\pi_{\\mathrm{p}}^{\\mathrm{u}}R_{\\mathrm{p}}^{-}(g)\\right],\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\widehat{R}_{\\mathrm{pu}}(g)=\\pi_{\\mathrm{p}}\\widehat{R}_{\\mathrm{p}}^{+}(g)+\\frac{\\pi_{\\mathrm{n}}}{\\pi_{\\mathrm{n}}^{\\mathrm{u}}}\\cdot\\left[\\widehat{R}_{\\mathrm{u}}^{-}(g)-\\pi_{\\mathrm{p}}^{\\mathrm{u}}\\widehat{R}_{\\mathrm{p}}^{-}(g)\\right].\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Specifically, $\\pi_{\\mathrm{n}}=1-\\pi_{\\mathrm{p}},\\pi_{\\mathrm{n}}^{\\mathrm u}=1-\\pi_{\\mathrm{p}}^{\\mathrm u}$ could be derived as class-prior probability given $\\pi_{\\mathrm{p}}$ and $\\pi_{\\mathrm{p}}^{\\mathrm{u}}$ . In particular, the ratio of labeled positive samples could be precisely figured out as $\\pi_{\\mathrm{p}}^{t r}$ in transductive learning, given which \u03c0pu $\\begin{array}{r}{\\pi_{\\mathrm{p}}^{\\mathrm{u}}=\\frac{\\pi_{\\mathrm{p}}-\\pi_{\\mathrm{p}}^{t r}}{1-\\pi_{\\mathrm{p}}^{t r}}}\\end{array}$ $\\pi_{\\mathrm{n}}^{\\mathrm{u}}=1-\\pi_{\\mathrm{p}}^{\\mathrm{u}}$ could be derived as class-prior probability. ", "page_idx": 15}, {"type": "text", "text": "D Proof for Theorem 2 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Proof. $\\Re_{n,q}$ is defined as the Rademacher complexity of the class of classifiers $\\mathcal{G}$ for the sampling of size $n$ from distribution $q(x)$ . From [30] we have that with probability at least $1-\\delta/2$ , the uniform deviation bounds below hold separately: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\operatorname*{sup}_{g\\in\\mathcal{G}}|\\widehat{R}_{+}(g)-R_{+}(g)|\\leq2L_{\\ell}\\Re_{n_{+},p_{\\mathrm{p}}}(\\mathcal{G})+\\sqrt{\\frac{\\ln(4/\\delta)}{2n_{+}}}}}\\\\ &{}&{\\triangleq M_{+}>0,}\\\\ &{}&{\\underset{g\\in\\mathcal{G}}{\\operatorname*{sup}}\\,|\\widehat{R}_{\\mathrm{u,-}}(g)-R_{\\mathrm{u,-}}(g)|\\leq2L_{\\ell}\\Re_{n_{\\mathrm{u,}}p_{\\mathrm{u}}}(\\mathcal{G})+\\sqrt{\\frac{\\ln(4/\\delta)}{2n_{\\mathrm{u}}}}}\\\\ &{}&{\\triangleq M_{-}>0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $L_{\\ell}$ is the Lipschitz constant of loss $\\ell$ in its first parameter and $n_{+}$ is the number of positive samples while $n_{u}$ is that of unlabeled. Following the symmetric condition assumption in [30], it is obviously holds that: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Var}(\\widehat{R^{\\prime}}_{\\mathrm{pu}}(g))=2\\pi_{\\mathrm{p}}M_{+}+M_{-},\\mathrm{while}}\\\\ &{\\mathrm{Var}(\\widehat{R}_{\\mathrm{pu}}(g))=(\\pi_{\\mathrm{p}}+\\frac{\\pi_{\\mathrm{n}}\\cdot\\pi_{\\mathrm{p}}^{\\mathrm{u}}}{\\pi_{\\mathrm{n}}^{\\mathrm{u}}})M_{+}+\\frac{\\pi_{\\mathrm{n}}}{\\pi_{\\mathrm{n}}^{\\mathrm{u}}}M_{-}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "holds in our setting. Then it is evident that $\\begin{array}{r}{\\frac{\\pi_{\\mathrm{n}}}{\\pi_{\\mathrm{p}}}<\\frac{\\pi_{\\mathrm{n}}^{u}}{\\pi_{\\mathrm{p}}^{u}}}\\end{array}$ i.e. $\\frac{\\pi_{\\mathrm{n}}\\!\\cdot\\!\\pi_{\\mathrm{p}}^{\\mathrm{u}}}{\\pi_{\\mathrm{n}}^{\\mathrm{u}}}<\\pi_{\\mathrm{p}}$ and $\\frac{\\pi_{\\mathrm{n}}}{\\pi_{\\mathrm{n}}^{u}}<1$ . ", "page_idx": 15}, {"type": "text", "text": "Consequently, comparing the coefficients of $M_{+}$ and $M_{-}$ leads to the conclusion that $\\widehat{R}_{\\mathrm{pu}}(g)$ could possess tighter uniform deviation bound than that of Non-negative Risk Estimator [13 ]. ", "page_idx": 15}, {"type": "text", "text": "E Convergence Proof ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "The expectation\u2013maximization (EM) algorithm is an iterative approach to maximize the likelihood $p(\\mathbf{y}|\\mathbf{X};\\theta)$ of target variables $y$ over input variables $\\mathbf{X}$ and parameters $\\theta$ . The EM algorithm works iteratively, and each iteration consists of an expectation (E) step and a maximization (M) step. The E step computes the expectation of the log-likelihood concerning the conditional distribution of the latent variable ${\\bf z}$ given the current parameters $\\theta^{(t)}$ at the $t$ -th iteration: ", "page_idx": 16}, {"type": "equation", "text": "$$\nQ(\\theta\\mid\\theta^{(t)})=\\mathbb{E}_{\\mathbf{z}\\sim p(\\mathbf{z}\\mid\\mathbf{X},\\mathbf{y},\\theta^{(t)})}[\\log p(\\mathbf{y},\\mathbf{z}\\mid\\mathbf{X},\\theta)].\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Then, the $\\mathbf{M}$ step finds a set of parameters that maximizes the computed expectation: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\theta^{(t+1)}=\\arg\\operatorname*{max}_{\\theta}Q(\\theta\\mid\\theta^{(t)}).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Due to the maximization step, we get the following inequality naturally: ", "page_idx": 16}, {"type": "equation", "text": "$$\nQ(\\theta^{(t+1)}\\mid\\theta^{(t)})-Q(\\theta^{(t)}\\mid\\theta^{(t)})\\ge0.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Lemma 3 (Convergence of EM algorithm [29]). It is guaranteed that the EM algorithm always improves $\\log p(\\mathbf{y}\\mid\\mathbf{X},\\boldsymbol{\\theta})$ by increasing the value of $Q(\\theta\\mid\\theta^{(t)})$ . Since $\\log p(\\mathbf{y}\\mid\\mathbf{X},\\boldsymbol{\\theta})$ is monotonically bounded, the EM must converge. ", "page_idx": 16}, {"type": "text", "text": "Proof. The following equation holds for any ${\\bf Z}$ : ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\log p(\\mathbf{y}\\mid\\mathbf{X},\\boldsymbol{\\theta})=\\log p(\\mathbf{y},\\mathbf{z}\\mid\\mathbf{X},\\boldsymbol{\\theta})-\\log p(\\mathbf{z}\\mid\\mathbf{X},\\mathbf{y},\\boldsymbol{\\theta}).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We take the expectation over $p(\\mathbf{z}\\mid\\mathbf{X},\\mathbf{y},\\theta^{(t)})$ for both sides as follows: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}_{p(\\mathbf{z}|\\mathbf{X},\\mathbf{y},\\theta^{(t)})}[\\log p(\\mathbf{y}\\mid\\mathbf{X},\\theta)]}\\\\ &{=\\mathbb{E}_{p(\\mathbf{z}|\\mathbf{X},\\mathbf{y},\\theta^{(t)})}[\\log p(\\mathbf{y},\\mathbf{z}\\mid\\mathbf{X},\\theta)]-\\mathbb{E}_{p(\\mathbf{z}|\\mathbf{X},\\mathbf{y},\\theta^{(t)})}[\\log p(\\mathbf{z}\\mid\\mathbf{X},\\mathbf{y},\\theta)]}\\\\ &{=Q(\\theta\\mid\\theta^{(t)})+H(p(\\mathbf{z}\\mid\\mathbf{X},\\mathbf{y},\\theta)\\mid p(\\mathbf{z}\\mid\\mathbf{X},\\mathbf{y},\\theta^{(t)}))}\\\\ &{\\triangleq Q(\\theta\\mid\\theta^{(t)})+H(p_{\\theta}\\mid p_{\\theta^{(t)}})}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $H$ stands for the entropy. If we substitute $\\theta^{(t)}$ for $\\theta$ , we get the following: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\log p(\\mathbf{y}\\mid\\mathbf{X},\\boldsymbol{\\theta}^{(t)})=Q(\\boldsymbol{\\theta}^{(t)}\\mid\\boldsymbol{\\theta}^{(t)})+H(p_{\\boldsymbol{\\theta}^{(t)}}\\mid p_{\\boldsymbol{\\theta}^{(t)}}).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Gibbs\u2019 inequality states that $H(q\\mid p)-H(p\\mid p)\\geq0$ always holds for any distribution $p$ and $q$ . Hence we have: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\log p(\\mathbf{y}\\mid\\mathbf{X},\\boldsymbol{\\theta}^{(t+1)})-\\log p(\\mathbf{y}\\mid\\mathbf{X},\\boldsymbol{\\theta}^{(t)})}\\\\ &{=Q(\\boldsymbol{\\theta}^{(t+1)}\\mid\\boldsymbol{\\theta}^{(t)})+H(p_{\\boldsymbol{\\theta}^{(t+1)}}\\mid p_{\\boldsymbol{\\theta}^{(t)}})-Q(\\boldsymbol{\\theta}^{(t)}\\mid\\boldsymbol{\\theta}^{(t)})-H(p_{\\boldsymbol{\\theta}^{(t)}}\\mid p_{\\boldsymbol{\\theta}^{(t)}})}\\\\ &{=Q(\\boldsymbol{\\theta}^{(t+1)}\\mid\\boldsymbol{\\theta}^{(t)})-Q(\\boldsymbol{\\theta}^{(t)}\\mid\\boldsymbol{\\theta}^{(t)})+H(p_{\\boldsymbol{\\theta}^{(t+1)}}\\mid p_{\\boldsymbol{\\theta}^{(t)}})-H(p_{\\boldsymbol{\\theta}^{(t)}}\\mid p_{\\boldsymbol{\\theta}^{(t)}})}\\\\ &{\\ge Q(\\boldsymbol{\\theta}^{(t+1)}\\mid\\boldsymbol{\\theta}^{(t)})-Q(\\boldsymbol{\\theta}^{(t)}\\mid\\boldsymbol{\\theta}^{(t)})=Q(\\boldsymbol{\\theta}^{(t+1)}\\mid\\boldsymbol{\\theta}^{(t)})-Q(\\boldsymbol{\\theta}^{(t)})\\ge0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Now we provide the proof for Theorem 3. ", "page_idx": 16}, {"type": "text", "text": "Proof. According to Lemma 3, we have that the EM algorithm converges. Next, we model our problem and give the approximate equivalence between our algorithm and the EM algorithm to prove our algorithm converges. ", "page_idx": 16}, {"type": "text", "text": "The latent variables $\\mathbf{z}$ represent the true label distribution of unlabeled samples, where $\\hat{y}_{i}(u)\\,=$ $f(\\mathbf{X},i;\\theta)$ is the probability of node $i$ being labeled as $u\\in\\{+1,-1\\}$ by the current classifier $f$ Given $\\pi_{\\mathrm{p}}^{\\mathrm{u}}$ , the label distribution of all unlabeled samples is: ", "page_idx": 16}, {"type": "equation", "text": "$$\np(z_{i})=\\left\\{\\!\\!\\begin{array}{l l}{\\hat{\\pi}_{\\mathrm{p}}^{\\mathrm{u}}}&{\\mathrm{if}\\quad z_{i}=+1,}\\\\ {1-\\hat{\\pi}_{\\mathrm{p}}^{\\mathrm{u}}}&{\\mathrm{if}\\quad z_{i}=-1.}\\end{array}\\!\\!\\right.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "First, the conditional distribution $p(\\mathbf{z}\\mid\\mathbf{X},\\mathbf{y},\\theta^{(t)})$ of latent variables given the current parameters $\\theta^{(t)}$ is approximated by: ", "page_idx": 17}, {"type": "equation", "text": "$$\np(\\mathbf{z}\\mid\\mathbf{X},\\mathbf{y},\\theta^{(t)})\\approx\\prod_{i\\in\\mathcal{U}}p(z_{i}).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Second, the joint distribution $p(\\mathbf{y},\\mathbf{z}\\mid\\mathbf{X},\\boldsymbol{\\theta})$ of labeled and unlabeled nodes with new parameters $\\theta$ is approximated by the classifier $f$ , which is also considered as a marginalization function that gives the label distribution of each node based on all given information: ", "page_idx": 17}, {"type": "equation", "text": "$$\np(\\mathbf{y},\\mathbf{z}\\mid\\mathbf{X},\\theta)\\approx\\prod_{i\\in\\mathcal{P}}\\hat{y}_{i}(+1)\\prod_{j\\in\\mathcal{U}}\\hat{y}_{j}(z_{j})\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We propose to use the average log-likelihood differences to measure the classification preference of the model. It can be transformed into the following form by $\\Delta_{\\mathcal{U}}$ and $\\Delta_{\\mathcal{P}}$ , representing that on domain $\\boldsymbol{\\mathcal{U}}$ and $\\mathcal{P}$ : ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\Delta\\chi=\\displaystyle\\frac{1}{|\\mathcal{U}|}\\sum_{j\\in\\mathcal{U}}\\log\\hat{y}_{j}(+1)-\\log\\hat{y}_{j}(-1)}\\\\ {\\Delta\\phi=\\displaystyle\\frac{1}{|\\mathcal{P}|}\\sum_{i\\in\\mathcal{P}}\\log\\hat{y}_{i}(+1)-\\log\\hat{y}_{i}(-1)}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "If the mean value is positive, it means that the logarithmic probability of positive classes is higher than that of negative classes in the given domain, and vice versa. When we ignore the preference of the classification model, this actually describes the category feature bias in certain domains. ", "page_idx": 17}, {"type": "text", "text": "Let\u2019s rethink the meaning of preference condition. If the model has a similar classification preference in domain $\\boldsymbol{\\mathcal{U}}$ and $\\mathcal{P}$ , we can express as $\\Delta_{\\mathcal{U}}\\approx\\Delta_{\\mathcal{P}}$ . This condition can be arranged as the mathematical expression of the condition given by Theorem 3 through the properties of the log function. ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\Delta_{\\mathcal{U}}\\approx\\Delta_{\\mathcal{P}}\\equiv\\sum_{j\\in\\mathcal{U}}\\frac{1}{|\\mathcal{U}|}\\log\\frac{\\hat{y}_{j}(+1)}{\\hat{y}_{j}(-1)}\\approx\\sum_{i\\in\\mathcal{P}}\\frac{1}{|\\mathcal{P}|}\\log\\frac{\\hat{y}_{i}(+1)}{\\hat{y}_{i}(-1)}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We derive $-\\mathcal{L}_{\\mathrm{pu}}$ from Eq.( 7), since the goal of training is to minimize the objective function: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{1}{N}\\mathbb{E}_{x\\sim p(x,\\mathbf{X},g(t))}[\\log p(\\mathbf{y},\\,\\mathbf{z}\\mid\\mathbf{X},\\theta)]}\\\\ {\\displaystyle=\\frac{1}{N}\\sum_{\\underline{{\\tau}}}p(z\\mid\\mathbf{X},\\mathbf{y},g(t))\\log p(\\mathbf{y},\\,\\mathbf{z}\\mid\\mathbf{X},\\theta)}\\\\ {\\displaystyle\\approx\\frac{1}{N}\\sum_{\\underline{{\\tau}}}p(z\\mid\\mathbf{X},\\mathbf{y},g(t))\\Big(\\sum_{i\\in\\mathcal{P}}\\log\\hat{y}_{i}(+1)+\\sum_{j\\in\\mathcal{U}}\\log\\hat{y}_{j}(z_{j})\\Big)}\\\\ {\\displaystyle=\\frac{1}{N}\\sum_{i\\in\\mathcal{P}}\\log\\hat{y}_{i}(+1)+\\frac{1}{N}\\sum_{j\\in\\mathcal{U}}\\sum_{\\ell\\in\\mathcal{E}+1}p(z_{j})\\log\\hat{y}_{j}(z_{j})}\\\\ {\\displaystyle=\\frac{|\\mathcal{P}|}{N}\\cdot\\frac{1}{|\\mathcal{P}|}\\log\\hat{y}_{i}(+1)+\\frac{|\\mathcal{U}|}{N}\\cdot\\frac{1}{|\\mathcal{U}|}\\sum_{j\\in\\mathcal{U}}\\hat{y}_{j}(\\ell_{j}+1)+(1-\\frac{\\hat{w}_{0}^{\\top}}{N})\\log\\hat{y}_{j}(-1)}\\\\ {\\displaystyle=\\frac{|\\mathcal{P}|}{N}\\cdot\\frac{1}{|\\mathcal{P}|}\\log\\hat{y}_{i}(+1)+\\frac{|\\mathcal{U}|}{N}\\cdot\\frac{1}{|\\mathcal{U}|}\\sum_{j\\in\\mathcal{U}}\\log\\hat{y}_{j}(-1)+\\frac{\\hat{w}_{0}^{\\top}|\\mathcal{U}|}{N}\\Delta_{\\mathcal{U}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We replaced $\\Delta_{\\mathcal{U}}$ with $\\Delta_{\\mathcal{P}}$ , and this process is completed by the above preference condition. ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\approx\\displaystyle\\frac{|\\mathcal{P}|}{N}\\cdot\\frac{1}{|\\mathcal{P}|}\\sum_{i\\in\\mathcal{P}}\\log\\hat{y}_{i}(+1)+\\frac{|\\mathcal{U}|}{N}\\cdot\\frac{1}{|\\mathcal{U}|}\\sum_{j\\in\\mathcal{U}}\\log\\hat{y}_{j}(-1)+\\frac{|\\mathcal{U}|\\hat{\\pi}_{\\mathfrak{p}}^{\\mathfrak{u}}}{N}\\cdot\\frac{1}{|\\mathcal{P}|}\\sum_{i\\in\\mathcal{P}}\\log\\hat{y}_{i}(+1)-\\log\\hat{y}_{i}(-1)}\\\\ &{=-\\frac{|\\mathcal{P}|+|\\mathcal{U}|\\cdot\\hat{\\pi}_{\\mathfrak{p}}^{\\mathfrak{u}}}{|\\mathcal{P}|+|\\mathcal{U}|}\\sum_{i\\in\\mathcal{P}}\\frac{1}{|\\mathcal{P}|}\\log\\hat{y}_{i}(+1)-\\frac{\\frac{|\\mathcal{N}|}{|\\mathcal{P}|+|\\mathcal{U}|}}{\\frac{|\\mathcal{N}|}{|\\mathcal{U}|}}\\left(\\sum_{j\\in\\mathcal{U}}\\frac{1}{|\\mathcal{U}|}\\log\\hat{y}_{j}(-1)\\right.\\right.-\\hat{\\pi}_{\\mathfrak{p}}^{\\mathfrak{u}}\\!\\sum_{i\\in\\mathcal{P}}\\frac{1}{|\\mathcal{P}|}\\log\\hat{y}_{i}(-1)\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "table", "img_path": "AWFryOJaGi/tmp/2ae07109489ecfaba4795f2516a2c23fd0005cf1fe3bf2c450cd5478321a1ab2.jpg", "table_caption": [], "table_footnote": ["Table 5: Statistics of DBP2.0, DBP15K and GA16K. "], "page_idx": 18}, {"type": "table", "img_path": "AWFryOJaGi/tmp/e6a3015660ee90d2da263ddee82542d07e59f7d8ed7856c3dbca684f84a34ffc.jpg", "table_caption": [], "table_footnote": ["Table 6: Statistics of DBP2.0-Plus and DBP2.0-Minus "], "page_idx": 18}, {"type": "text", "text": "Denote \u03c0p = $\\begin{array}{r}{\\pi_{\\mathrm{p}}=\\frac{|\\mathcal{P}|+|\\mathcal{U}|\\cdot\\hat{\\pi}_{\\mathrm{p}}^{\\mathrm{u}}}{|\\mathcal{P}|+|\\mathcal{U}|}}\\end{array}$ , $\\begin{array}{r}{\\pi_{\\mathrm{n}}=\\frac{|\\mathcal{N}|}{|\\mathcal{P}|+|\\mathcal{U}|}}\\end{array}$ and $\\pi_{\\mathrm{n}}^{\\mathrm{u}}=\\frac{|\\mathcal{N}|}{|\\mathcal{U}|}$ . The above formula is equivalent to: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\arg\\underset{\\theta}{\\operatorname*{max}}\\,\\frac{1}{N}\\mathbb{E}_{\\mathbf{z}\\sim p(\\mathbf{z}|\\mathbf{X},\\mathbf{y},\\theta^{(t)})}[\\log p(\\mathbf{y},\\mathbf{z}\\mid\\mathbf{X},\\theta)]}\\\\ &{=\\arg\\underset{\\theta}{\\operatorname*{max}}\\underbrace{-\\pi_{\\mathrm{p}}\\hat{R}_{\\mathrm{p}}^{+}(g)-\\frac{\\pi_{\\mathrm{n}}}{\\pi_{\\mathrm{n}}^{\\mathrm{u}}}\\cdot\\left[\\hat{R}_{\\mathrm{u}}^{-}(g)-\\pi_{\\mathrm{p}}^{\\mathrm{u}}\\hat{R}_{\\mathrm{p}}^{-}(g)\\right]}_{-\\hat{R}_{\\mathrm{pu}}(g)}}\\\\ &{=\\arg\\underset{\\theta}{\\operatorname*{min}}\\,\\hat{R}_{\\mathrm{pu}}(g)=\\arg\\underset{\\theta}{\\operatorname*{max}}-\\mathcal{L}_{\\mathrm{pu}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "F Statistics of Experimental Dataset and Baselines ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Datasets. The training/test sets for each dataset are generated using a fixed random seed. For entity alignment, $30\\%$ of matchable entity pairs constitute the training set, while the remaining form the test set. For dangling entity detection, we did not utilize any labeled dangling entity data, in contrast to prior work which labels $30\\%$ of the dangling entities and matchable pairs respectively for training [33]. Hence our method imposes minimal restrictions on annotated data. All datasets are briefly introduced in the following and some statistics are provided in Tab. 7. ", "page_idx": 18}, {"type": "text", "text": "In addition to the existing datasets, we also constructed DBP2.0-minus & -plus as supplementary to DBP2.0, GA16K enabling comparison between Dangling-Entities-Unaware baselines, and GADBP15K for evaluation of iPULE. ", "page_idx": 18}, {"type": "text", "text": "DBP15K3 [34]: DBP15K consists of three cross-lingual subsets constructed from DBpedia: EnglishFrench $(\\mathrm{DBP}_{\\mathrm{FR-EN}})$ , English-Chinese $(\\mathrm{DBP_{ZH-EN}})$ ), English-Japanese $\\mathrm{\\DeltaDBP_{JA-EN}})$ . Each subset contains 15,000 pre-aligned entity pairs. This dataset includes a small proportion of dangling entity samples which is yet mostly ignored in previous entity alignment tasks. ", "page_idx": 18}, {"type": "table", "img_path": "AWFryOJaGi/tmp/b88b9c19a7b51b32b715fc7a8c20d344f1e47d2b9bbee64daff92eb5a29c8082.jpg", "table_caption": [], "table_footnote": ["Table 7: Statistics of GA-DBP15K. $\\mathrm{c}=[25\\%,20\\%,15\\%,10\\%]$ . "], "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "$\\mathbf{DBP2.0^{4}}$ [33]: DBP2.0 is an entity alignment dataset with a considerable proportion of dangling entities, constructed from the multilingual Infobox Data of DBpedia [2]. The dataset contains three pairs of crosslingual KGs, ZH-EN (Chinese to English), JA-EN (Japanese to English), and FR-EN (French to English). Since there are dangling nodes in both the source and target graphs, we separately test source-to-target and target-to-source alignment, consistent with the established approach. A representative feature of the dataset is that the matchable and dangling entities exhibit similar degree distributions which are hard to distinguish, displaying a real-world challenge in aligning knowledge graphs. Based on DBP2.0, we extend the following -minus & -plus datasets for verification of iPULE on different positive proportions between $20\\%{-}50\\%$ . ", "page_idx": 19}, {"type": "text", "text": "DBP2.0-plus: In the construction of the plus dataset, our goal is to construct the dataset that has a higher $\\pi_{p}$ , and we realize this by reducing a few existing dangling entities on ZH-EN and JA-EN. We randomly delete dangling entities from both source and target KG equally and remove triples containing them. The constructed DBP2.0-plus are reindexed and thus obtain a higher $\\pi_{\\mathrm{p}}$ value than the original dataset. ", "page_idx": 19}, {"type": "text", "text": "DBP2.0-minus: In contrast, to lower the $\\pi_{\\mathrm{p}}$ value. Given the constraint of preventing new dangling entities that could introduce false information to the KG, we can only reduce the number of matchable entities. Given source and target KG, removing one entity from a pair makes the remaining entity dangling. We randomly delete matchable entities from one side of the pair on both source and target KG uniformly. The constructed DBP2.0-minus are reindexed and thus obtain a lower $\\pi_{\\mathrm{p}}$ value than the original dataset. ", "page_idx": 19}, {"type": "text", "text": "GA16K: This dataset constructed by us exclusively contains dangling nodes in the target graph, facilitating a comparison between our work and baselines that neglect dangling entities. GA16K is extracted from GAKG5 [7], a Geoscience Academic Knowledge Graph. We first order each type of entity in GAKG according to their degrees and select the entities with a large degree into the entity set. A total of 16,363(16K) separate entities and their relations were extracted to compose the target graph. Then we extract 6,208 entities from the target graph to comprise the source graph. Hence there are 6,208 ground-truth matchable pairs between the source and the target. The remaining 10,155 entities in the target graph are regarded as dangling entities. ", "page_idx": 19}, {"type": "text", "text": "GA-DBP15K: The GA-DBP15K dataset is derived from a subset of entities within GA16K, along with their associated triples, which are then concatenated with the DBP15K dataset, such as EN, resulting in a new dataset pair that shares a proportion of common entities. To achieve the goal, we first extract a certain proportion of triples from GA16K. We then reindex all the entities from the extricated GA16K and DBP15K datasets. Finally, we update the entity and relation indices in the triples, replacing them with the newly assigned indices. ", "page_idx": 19}, {"type": "text", "text": "Baselines. Since our work does not take advantage of any side information, we emphasize its comparison with the previous methods purely depending on graph structures. These works majorly incorporate two types: ", "page_idx": 19}, {"type": "text", "text": "Dangling-Entities-Unaware. We include advanced entity alignment methods in recent years: GCNAlign [41], RSNs [9], MuGNN [4], KECG [16]. Methods with bootstrapping to generate semisupervised structure data are also adopted: BootEA [35], TransEdge [36], MRAEA [26], AliNet [37], and Dual-AMN [25]. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "Dangling-Entities-Aware. To the best of our knowledge, the method of [33] is the most fairly comparable baseline which is based on MTransE [5] and AliNet [37]. Because MHP [19] overemphasized more use of labeled dangling data like high-order similarity information which is also based on the above two methods, while SoTead [22] and UED [24] utilize additional sideinformation. SoTead [22] and UED [24] can only execute the degraded version on DBP2.0 cause no side-information is available on that. We exclude them from baselines for our methods. [33] introduces three techniques to address the dangling entity issue: nearest neighbor (NN) classification, marginal ranking (MR), and background ranking (BR). ", "page_idx": 20}, {"type": "text", "text": "Metrics are set for the dangling entity detection task and the entity alignment task separately. For the entity detection, we evaluate the detection performance by the standard precision, recall, and F1 score. To align the previous dangling detection baselines, we detect dangling entities as \u2018positive\u2019 samples and align matchable entities for entity alignment. ", "page_idx": 20}, {"type": "text", "text": "For the entity alignment, the metrics slightly differ in the dangling-entities-unaware and danglingentities-aware settings. We evaluate the baselines unaware of the dangling entities by following their assumptions and using their metric Hits $@\\,\\mathrm{K}$ $[\\!\\!\\!\\!\\!\\!\\!\\mathbf{K}\\!\\!\\!=\\!\\!\\!\\!\\!\\!\\!1\\!\\!\\!\\!\\!\\!$ , 10, 50, $\\mathrm{H}@\\mathrm{K}$ for short) on the ranking list $S$ . This setting is referred to as relaxed setting when $S$ is composed of all ground-truth entities without the dangling ones: ", "page_idx": 20}, {"type": "equation", "text": "$$\nH i t s\\ @K={\\frac{1}{|S|}}\\sum_{k=1}^{|S|}\\mathbb{1}(\\mathbf{rank}_{i}\\leq k).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "In contrast, we refer to a consolidated setting for baselines aware of dangling entities. In this setting, the ranking list $S$ also contains all dangling entities. We use $\\mathrm{H}@\\mathrm{K}$ in the consolidated setting to evaluate the performance of baselines aware of but not removing dangling entities in the alignment. For baselines where dangling entities are detected and removed before alignment, the direct use of $\\mathrm{H}@\\mathrm{K}$ to evaluate entity alignment may not be precise, since errors are introduced in the detection phase. Thus we follow the convention of [33] to apply a set of metrics for evaluating the accuracy of entity alignment in the consolidated setting. Each of them is derived and introduced as follows. ", "page_idx": 20}, {"type": "text", "text": "The standard precision and recall is given as ", "page_idx": 20}, {"type": "equation", "text": "$$\n{\\mathrm{precision}}={\\frac{T P}{T P+F P}},\\;\\;{\\mathrm{recall}}={\\frac{T P}{T P+F N}}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "for dangling entity detection. We denote the dangling entities as 0 and matchable ones as 1. The subscript $t_{1y}$ suggests that an entity with ground truth $y$ is classified as matchable. Likewise, $t_{y1}$ represents a matchable entity that is classified as $y$ by the detection classifier. If a source entity is dangling but not identified by the detection module, its alignment result is always considered incorrect, i.e., $H@K_{t_{10}}=0$ . Hence we have the precision for entity alignment as ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{H@1_{t_{1y}}=\\mathrm{precision}\\cdot H@1_{t_{11}}+(1-\\mathrm{precision})\\cdot H@1_{t_{10}}}\\\\ &{\\qquad\\qquad=\\mathrm{precision}\\cdot H@1_{t_{11}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Similarly, if a matchable entity is falsely excluded by the dangling detection module, this test case is also regarded as incorrect $\\dot{H}@K_{t_{01}}=0$ since the alignment model has no chance to search for alignment. Hence we have the recall for entity alignment as ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{H@1_{t_{y1}}=\\mathrm{recall}\\cdot H@1_{t_{11}}+(1-\\mathrm{recall})\\cdot H@1_{t_{01}}}}\\\\ {{=\\mathrm{recall}\\cdot H@1_{t_{11}}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "For the methods that are aware of dangling entities, we use $H\\mathbb{Q}1_{t_{1y}}$ and $H\\mathbb{Q}1_{t_{y1}}$ to denote the precision and recall of the entity alignment task. Similarly, we define the F1 score of the entity alignment as the harmonic average of precision and recall: ", "page_idx": 20}, {"type": "equation", "text": "$$\nF1=\\frac{2\\cdot H@1_{t_{1y}}\\cdot H@1_{t_{y1}}}{H@1_{t_{1y}}+H@1_{t_{y1}}}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Later, $H\\mathbb{Q}1_{t_{1y}}$ and $H\\mathbb{Q}1_{t_{y1}}$ are referred to as Prec. and Rec. in reporting alignment performance. ", "page_idx": 20}, {"type": "text", "text": "G Additional Experiment ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "RQ1: How do current network alignment methods perform in unlabeled dangling cases? (see appendix G.1) ", "page_idx": 21}, {"type": "text", "text": "RQ2: Loss convergence on GA-DBP15K and DBP2.0. (see appendix G.2) ", "page_idx": 21}, {"type": "text", "text": "RQ3: How do we select the embedding dimensions? (see appendix G.3) RQ4: What is the actual efficiency of our approach? (see appendix G.4) RQ5: Baseline comparison under different pre-aligned seeds? (see appendix G.5) RQ6: Additional experiments involved LightEA as a strong baseline? (see appendix G.6) ", "page_idx": 21}, {"type": "text", "text": "G.1 The Non-Negligibility of dangling Problem (RQ1). ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We investigated the performance degradation of various existing EA methods in the face of the dangling problem, which shows that this problem is worth considering. ", "page_idx": 21}, {"type": "table", "img_path": "AWFryOJaGi/tmp/0c246490951603c10a68aa1a4c4a1d70babffed2aa9f63e18f969f3ed58006bc.jpg", "table_caption": [], "table_footnote": ["Table 8: Network alignment performance on DBP15K in the consolidated setting. The blue numbers suggest the drop from the relaxed setting (as with their original implementation). "], "page_idx": 21}, {"type": "text", "text": "We reproduce the baselines unaware of dangling entities on DBP15K in the relaxed setting. On the same dataset, we rerun their methods but in a consolidated setting that takes the dangling entities into account. Even though DBP15K only comprises a small percentage of dangling entities, the drop in the consolidated setting is significant, as shown in Tab. 8. ", "page_idx": 21}, {"type": "text", "text": "The reason behind such a performance drop is mainly because most previous works remove dangling entities from the ground truth in measuring their alignment performance. In particular, Dual-AMN takes advantage of the bootstrapping module by incorporating labeled pairs in training. In the relaxed setting, such labeled pairs are ground-truth aligned pairs, but in the consolidated setting, the dangling entities could bring in erroneous alignment which contaminates the alignment of other pairs. ", "page_idx": 21}, {"type": "text", "text": "G.2 Class Prior Estimation Supplementary Experiment (RQ2). ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We hope further to verify the estimation and convergence results of iPULE of loss convergence. We list the corresponding loss convergence results in Fig. 7. The losses under different pre-aligned proportions (0.25, 0.2, 0.15, 0.1) on the GA-DBP15K constitute a group of statistical data, and the corresponding loss mean and standard deviation of this set of statistical data are displayed. ", "page_idx": 21}, {"type": "text", "text": "On the other hand, the loss difference is a direct indication of convergence in iPULE\u2019s implementation. Thus, we plot the histogram figure of the DBP2.0 (w/ -minus & -plus) of the corresponding loss difference for more comprehensive. With the progress of the algorithm, and the statistical number of the difference of the smaller loss function occupied the maximum. This shows the convergence of iPULE in this data set from another aspect. ", "page_idx": 21}, {"type": "text", "text": "It is worth noticing that, there are performance fluctuations during the constitution of an ideal embedding space during the cold start stage. The figure plotted covers only the cold start subsequent procedure. ", "page_idx": 21}, {"type": "text", "text": "G.3 Embedding Dimension Selection (RQ3). ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Although a higher embedding dimension may encode richer information, an overly high dimension leads to performance decline. We select the GNN dimension according to the principle of [22]. Let the dimension of embedding be $d$ and the number of entities is $N$ . According to the feature entropy in [22], it holds that $d>8.33\\log N$ by the Johnson-Lindenstrauss lemma [15] that the vector dimension is at ${\\mathcal{O}}(\\log N)$ order. In most of our settings, $N$ is approximately $10^{5}$ , and thus $d$ is set to 128. ", "page_idx": 21}, {"type": "image", "img_path": "AWFryOJaGi/tmp/d2664b91c3ac180b3aeff417a8e2b01db4ebe708c69635b66d418b618570f397.jpg", "img_caption": ["Figure 7: Visualization of loss convergence on DBP2.0 and GA-DBP15K. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "table", "img_path": "AWFryOJaGi/tmp/b595e337a5bed1f0bfdbda2ba7d5682feb94e6889954ce5e785262e3ff3d83e4.jpg", "table_caption": ["Table 9: The entity alignment performance over different embedding dimensions on DBP2.0. "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "As shown in Tab. 9, due to the varying number of entities in datasets, the embedding dimension at the optimal performance varies. For example, the top performance is achieved on ZH-EN and EN-ZH when the embedding dimension is 96 but is obtained on JA-EN, EN-JA, FR-EN, and EN-FR with an embedding dimension of 128. As a compromise, we simplify the embedding representation of edges to enable FR-EN and EN-FR to run with an embedding dimension of 128 with limited memory (For more details, please check our open source code). A higher alignment performance can be achieved if no compromise is made. As we observe, the optimal performance is typically achieved at the theoretically chosen $d$ . This also indicates our approach has a memory cost at the order of ${\\mathcal{O}}(\\log N)$ . ", "page_idx": 22}, {"type": "text", "text": "G.4 Efficiency (RQ4) ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "The previous works concerning the dangling problem have not analyzed its efficiency in their experiments. Thus we only report the efficiency of our methods without baseline comparison. We evaluate the efficiency of our work on Tab. 10 including alignment search time as \u2018Inference Time\u2019, KEESA average training time as \u2018Average Training Time\u2019, and GPU memory cost on three different datasets of DBP2.0. Data obtained from these three datasets with the top three node numbers is a robust indicator of the efficiency of our method. ", "page_idx": 22}, {"type": "text", "text": "We gathered the mean value of 5 inference time costs for each dataset with the corresponding CPU and GPU memory consumption. Meanwhile, the average training time for each period from early to late is calculated. We enumerate the average training time of epochs 1-20, 21-25, 26-30, 31-35, 36-40, and 41-45. ", "page_idx": 22}, {"type": "text", "text": "Cause GPU is employed for not only model training but also inference, as shown on Tab. 10, the inference speed is still very impressive. Specifically, we split the large similarity matrix into multiple independent row blocks to perform the nearest searches within each block, which are well suited for GPU parallel processing. ", "page_idx": 22}, {"type": "text", "text": "It\u2019s noteworthy that the average training time correspondingly increases as training progresses from early to late stages. More quasi-supervised information incorporated by us accounts for that. To be specific, as the training deepens, we repeatedly conduct preliminary alignment tests while we gather more and more entity pairs mutually closest under a given metric. The entity pairs serve as the pre-aligned anchor nodes, i.e., the quasi-supervisory information mentioned above. ", "page_idx": 22}, {"type": "text", "text": "Besides, we list the CPU and GPU memory consumption required for our work. Memory consumption is influenced by various factors such as complex allocation algorithms, model parameter scales, and hyperparameters. In this problem, we put more attention on triples which characterize one KG, revealing an approximate proportionality between the number of triples and memory consumption. ", "page_idx": 22}, {"type": "table", "img_path": "AWFryOJaGi/tmp/62015e1e61c6f7f0e0f7d53442e85cff30938cac7298fdad8dc0ff168b7ac32d.jpg", "table_caption": [], "table_footnote": ["Table 10: Efficiency performance of our work on DBP2.0. The measurement of average training time is \u2018s/it\u2019, which indicates seconds per iteration. One iteration here represents one training epoch. \u2018-\u2019 indicates the absence of data due to training termination. "], "page_idx": 23}, {"type": "text", "text": "G.5 Baseline Comparison Under Different Ratios of Pre-aligned Seeds (RQ5). ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Comparing the proposed method with strong baseline models under different ratios of pre-aligned seeds would better demonstrate Lambda\u2019s superiority. The experimental baseline includes MtransE w/ BR the SOTA method in previous works, which is also the only open-source method. The results are shown in the Table. 11. ", "page_idx": 23}, {"type": "table", "img_path": "AWFryOJaGi/tmp/d365007bf8efc4b49ff5d48a2b549c9a47ef62107f50416259e04b26f0f470e7.jpg", "table_caption": [], "table_footnote": ["Table 11: Performance of Lambda and MtransE w/ BR under different ratios of pre-aligned of $10\\%$ , $20\\%$ , and $30\\%$ . Bold indicates optimal performance. "], "page_idx": 23}, {"type": "text", "text": "G.6 LightEA as Strong Baseline for Comparison (RQ6). ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "LightEA [28] is recommended as a strong baseline for Lambda. We fixed LightEA\u2019s code to include dangling entities into the alignment candidates and evaluated its performance on DBP2.0. Hits $@1$ and Hits $@10$ are evaluated in a similar way to the dangling-unaware methods in our paper, as listed below. In comparison, Lambda still outperforms LightEA. ", "page_idx": 23}, {"type": "table", "img_path": "AWFryOJaGi/tmp/35912f2761e7bc8fdafa4c1bdf7f60164e0434f1abee1fa1e9c1721e237e5f4a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "Table 12: Comparison of Lambda and LightEA under relaxed setting. \u2018-\u2019 indicates the absence of data due to out of time. ", "page_idx": 23}, {"type": "text", "text": "H Discussion ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "H.1 Alignment Direction ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "As we found, the alignment problem with dangling cases has a deeper issue concerning the classification of imbalanced datasets. It originated from the observation that the alignment performance from the source to the target is different from the other direction. The work of [33] has observed that on DBP2.0, choosing the alignment direction from a less populated KG (e.g., ZH, JA, FR) to a more populated KG (e.g., EN) enjoys a higher alignment accuracy but the other way around would lead to a noticeable performance drop. Meanwhile, the dangling entity detection on EN-XX has a higher F1 score than XX-EN, as shown in Tab. 13. ", "page_idx": 23}, {"type": "text", "text": "By analysis, we think it may be attributed to an improper indication of the dangling entity detection power on imbalanced datasets. This error in removing the predicted dangling entity would accumulate hurting the alignment task. To verify the point, we introduce a trivial classifier that makes a simple choice to classify all entities as dangling (positive) ones, and the detection results are reported in ", "page_idx": 23}, {"type": "table", "img_path": "AWFryOJaGi/tmp/d1058b35868e92355fda6b2e959bea63459f031e83352485e79dedbede540461.jpg", "table_caption": [], "table_footnote": ["Table 13: Dangling entities detection by our classifier v.s. a trivial one on DBP2.0. "], "page_idx": 24}, {"type": "text", "text": "Tab. 13. As all unlabeled entities are trivially classified as dangling ones, the detection metrics of the trivial classifier are all falsely high. The more populated source KG usually has more dangling entities (positives) and thus yields a higher precision in detection. Meanwhile, since the detection classifier actually is not working, more dangling entities participate in the alignment phase, resulting in poor alignment performance. This has explained why EN-XX has a higher dangling detection performance but a lower alignment accuracy compared to the other direction. ", "page_idx": 24}, {"type": "text", "text": "The root of this issue is that matchable and dangling entities comprise imbalanced categories in the classification task, but the corresponding metric is inappropriate. Hence boosting the detection performance does not necessarily improve the alignment performance. We believe more practical indicators of imbalanced datasets should be introduced to the alignment problem. ", "page_idx": 24}, {"type": "text", "text": "H.2 The Similarity between Dual-AMN and Lambda ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "The differences between the proposed GNN and Dual-AMN include: ", "page_idx": 24}, {"type": "text", "text": "Aggregation: ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "1. The adaptive dangling indicator $r_{e_{j}}$ is included in Lambda for eliminating dangling pollution.   \n2. The indicator $r_{e_{j}}$ is concatenated as a part of the entity feature. ", "page_idx": 24}, {"type": "text", "text": "Attention: ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "1. The attention is scaled by $r_{e_{j}}$ to filter dangling information. 2. Relation $r_{k}$ \u2019s embedding $\\mathbf{h}_{\\mathbf{r}_{\\mathbf{k}}}$ is linked to the adaptive dangling indicator of the associated entity $r_{e_{j}}$ , and thus the attention in Eq. (2) models the relationship between the relation and the entity. ", "page_idx": 24}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: see Abstract and Introduction. 1. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 25}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We explained the trade-off of our method in How does our method work? 5.3 for a slightly inferior precision reported in Tab. 12 and Tab. 4. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 25}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: According to the order of appearance, we sort out and give the specific proof in the appendix. ", "page_idx": 26}, {"type": "text", "text": "\u2022 The problem setting about PU learning as sec. 2.2 \u2022 Proof for Lemma 3.   \n\u2022 Proof for Theorem 1.   \n\u2022 Proof for Theorem 2.   \n\u2022 Proof for Theorem 3.   \n\u2022 Proof for Lemma 2. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 26}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We introduce the method proposed in this paper in detail in two sections, Selective Aggregation with Spectral Contrastive Learning 3 and Iterative PositiveUnlabeled Learning for Dangling Detection 4, and use the Alg. 1 to describe the latter in pseudocode. Meanwhile, we gave implementation details at the beginning of the Experiment 5. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.   \n(c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).   \n(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We provide the code and data in supplemental material which is described in a documented readme.md file. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 27}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: We gave the main implementation details at the beginning of the Experiment 5. Statistics of the experimental dataset and baselines in appendix F and additional experiment in appendix G also cover that including dataset construction details and hyperparameter selection criteria. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 27}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We provide the corresponding mean and standard deviation curves in Fig. 7 by calculating the loss function of different alignment ratios 0.25, 0.2, 0.15, 0.1, and the corresponding mean and standard deviation are drawn. Other experimental data have also been measured many times to take the mean value. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 28}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We gave GPU and CPU resources needed for the experiment in Implementation Detail part at the beginning of the Experiment 5. Additionally, time of execution such as training $\\&$ inference time is provided in Efficiency. G.4. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 28}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: The dataset construction and usage do not contain any information that endangers personal privacy, and it is licensed. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 29}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: There is no societal impact of the work performed. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 29}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 29}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: In this paper, we give all the sufficient reference materials. We provide the code and data in the supplemental material and describe them in a documented readme.md file, where more required information is clarified. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 30}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Justification: We introduce the dataset GA16K, GA-DBP15K and DBP2.0-minus & -plus in detail in the appendix F. We provide the code and data in the supplemental material and describe them in a documented readme.md flie, where more required information is clarified. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 30}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. ", "page_idx": 30}, {"type": "text", "text": "\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 31}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 31}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 31}]