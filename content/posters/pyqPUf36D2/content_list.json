[{"type": "text", "text": "Pseudo-Private Data Guided Model Inversion Attacks ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Xiong Peng1 Bo Han1\u2020 Feng Liu2 Tongliang Liu3 Mingyuan Zhou4 ", "page_idx": 0}, {"type": "text", "text": "1TMLR Group, Department of Computer Science, Hong Kong Baptist University 2School of Computing and Information Systems, The University of Melbourne 3Sydney AI Centre, The University of Sydney 4McCombs School of Business, The University of Texas at Austin {csxpeng, bhanml}@comp.hkbu.edu.hk fengliu.ml $@$ gmail.com tongliang.liu $@$ sydney.edu.au mingyuan.zhou $@$ mccombs.utexas.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In model inversion attacks (MIAs), adversaries attempt to recover the private training data by exploiting access to a well-trained target model. Recent advancements have improved MIA performance using a two-stage generative framework. This approach first employs a generative adversarial network to learn a fixed distributional prior, which is then used to guide the inversion process during the attack. However, in this paper, we observed a phenomenon that such a fixed prior would lead to a low probability of sampling actual private data during the inversion process due to the inherent distribution gap between the prior distribution and the private data distribution, thereby constraining attack performance. To address this limitation, we propose increasing the density around high-quality pseudo-private data\u2014recovered samples through model inversion that exhibit characteristics of the private training data\u2014by slightly tuning the generator. This strategy effectively increases the probability of sampling actual private data that is close to these pseudo-private data during the inversion process. After integrating our method, the generative model inversion pipeline is strengthened, leading to improvements over state-of-the-art MIAs. This paves the way for new research directions in generative MIAs. Our source code is available at: https://github.com/tmlr-group/PPDG-MI. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Currently, machine learning (ML) models, especially deep neural networks (DNNs), have become prevalent in privacy-sensitive applications such as secure systems [Yin et al., 2020], personal chatbots [Ouyang et al., 2022] and healthcare services [Murdoch, 2021]. These applications inevitably rely on private and confidential datasets during model training, raising concerns about potential privacy leakages [Liu et al., 2021]. Unfortunately, recent studies reveal that ML models are vulnerable to various privacy attacks [Fredrikson et al., 2014, Krishna et al., 2019, Choquette-Choo et al., 2021]. Model inversion attacks (MIAs), a category of these attacks, pose significant privacy risks, which aim to infer and recover original training data by exploiting access to a well-trained model. ", "page_idx": 0}, {"type": "text", "text": "In the pioneering work [Fredrikson et al., 2015], MIAs were formulated as a gradient-based optimization problem in the raw data space. The goal was to seek synthetic features that maximize the prediction score for a targeted class under the target model, exploiting the strong dependency between inputs and labels established during training. For example, in attack scenarios where the target model is a facial recognition model trained on private facial images, traditional MIAs would optimize over the synthetic images to maximize the prediction score for a target identity. ", "page_idx": 0}, {"type": "image", "img_path": "pyqPUf36D2/tmp/0593cd1c1f21ac1e008f657df195f6c9bf748432d1067448265ef59e39efd28a.jpg", "img_caption": ["Figure 1: Impact of distribution discrepancies on MIAs. (a) The test power of maximum mean discrepancy (MMD) test increases with the sample number, indicating significant differences between the distributions of $\\mathcal{D}_{\\mathrm{private}}$ (CelebA) and $\\mathcal{D}_{\\mathrm{public}}$ (CelebA, FFHQ and FaceScrub). (b) & (c) The proxy public datasets $\\mathcal{D}_{\\mathrm{public}}^{\\prime}$ are crafted using the method outlined in Eq. (4). The attack performance consistently diminishes as the discrepancy between the $\\mathcal{D}_{\\mathrm{private}}$ (CelebA) and $\\mathcal{D}_{\\mathrm{public}}^{\\prime}$ increases. For detailed setups and additional results of the motivation-driven experiments, refer to Appx. C.6. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "However, when the target models are DNNs, and the private features to be reconstructed reside in high-dimensional and continuous data spaces (e.g., facial images), the direct optimization in the input space without any constraints is substantially ill-posed. Traditional MIAs could easily produce semantically meaningless adversarial examples [Szegedy et al., 2014], which nevertheless achieve high prediction scores under DNNs. Zhang et al. [2020] addressed this problem by employing a generative adversarial network (GAN) [Goodfellow et al., 2014, Radford et al., 2016] to learn a distributional prior, subsequently constraining the attack optimization space to a meaningful manifold during the inversion process. This methodology, called generative model inversion (MI), lays the groundwork for more effective model inversion of DNNs trained on high-dimensional data [Chen et al., 2021, Wang et al., 2021a, Kahla et al., 2022, Struppek et al., 2022, Nguyen et al., 2023]. ", "page_idx": 1}, {"type": "text", "text": "Generative MIAs have shown marked improvements by incorporating a fixed prior in the inversion pipeline (cf. left panel of Fig. 2). However, this approach is fundamentally limited due to the inherent distribution discrepancy between the prior distribution and the unknown private training data distribution (cf. Fig. 1(a)). This discrepancy arises because the public auxiliary dataset, used to learn the distributional prior, does not intersect in labels with the private training dataset. Therefore, there is a low probability that the original private training data can be accurately sampled during the inversion process, leading to suboptimal attack performance (cf. Figs. 1(b) and 1(c)). ", "page_idx": 1}, {"type": "text", "text": "Thus, we raise a critical research question: How can the discrepancy between the prior distribution and the unknown private training data distribution be mitigated? Addressing this distribution gap is challenging in the MI context, where we only have access to a well-trained target model. Nevertheless, the target model still encapsulates information about the private training data. By performing model inversion on the target model, we can generate what we called pseudo-private data, which are reconstructed samples that reveal the characteristics of the private training data and can serve as its surrogate. Consequently, enhancing the density of pseudo-private data under the prior distribution indirectly increases the density of the private training data. This, in turn, raises the probability of accurately sampling the actual private training data (cf. right panel of Fig. 2). ", "page_idx": 1}, {"type": "text", "text": "To this end, we propose a novel model inversion methodology, termed pseudo-private data guided MI (PPDG-MI). The efficacy of PPDG-MI is demonstrated through a simple example using a 2D dataset (Sec. 3.3), where we increase the density of the pseudo-private data by directly minimizing the distribution discrepancy between the prior distribution and empirical pseudo-private data distribution, as measured by conditional transport [Zheng and Zhou, 2021] that is amenable to mini-batch based optimization and straightforward to implement. For the density enhancement of high-dimensional data, we introduce a nuanced tuning strategy involving three iterative steps: $\\textcircled{\\scriptsize{1}}$ Conduct a round of MIAs to produce pseudo-private samples. $\\circledcirc$ Select high-quality pseudo-private samples based on prediction scores. $\\circledast$ Fine-tune the generator to increase the density around these high-quality samples, thereby increasing the probability of sampling the original private training data. ", "page_idx": 1}, {"type": "text", "text": "In summary, our contributions and findings are as follows: ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "\u2022 Conceptually, we identify a fundamental limitation common to state-of-the-art (SOTA) generative MIAs [Zhang et al., 2020, Chen et al., 2021, Kahla et al., 2022, Struppek et al., 2022, Han et al., 2023, Nguyen et al., 2023], i.e., the utilization of a fixed prior during the inversion process. We argue that this approach is sub-optimal for MIAs and introduce a novel strategy, termed pseudo-private data guided MI, to mitigate this limitation, thereby paving the way for future research and advancements in generative MIAs. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "\u2022 Technically, we provide multiple implementations of PPDG-MI to validate the effectiveness of our proposed strategy. For low-resolution MIAs, we introduce PPDG-vanilla. For more complex high-dimensional MIAs, we offer PPDG-PW, which employs point-wise tuning, and two batch-wise tuning strategies: PPDG-MI with conditional transport (PPDG-CT) and PPDG-MI with maximum mean discrepancy (PPDG-MMD) (Sec. 3). \u2022 Empirically, through extensive experimentation, we demonstrate that our solution significantly improves the performance of the SOTA MI methods across various settings, including white-box, black-box, and label-only MIAs (Sec. 4). Our findings emphasize the increasing risks associated with MIAs and further highlight the urgent need for more robust defenses against the leakage of private information from DNNs. ", "page_idx": 2}, {"type": "text", "text": "2 Problem Setup and Preliminary ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 Model Inversion Attacks ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Problem Setup. Let $\\mathcal{X}\\subset\\mathbb{R}^{d_{X}}$ be the feature space, and $\\mathcal{V}_{\\mathrm{private}}=\\{1,\\ldots,C\\}$ be the private label space. The target model, M: $\\mathcal{X}\\rightarrow[0,1]^{C}$ , is a classifier well-trained on the private training dataset $\\mathcal{D}_{\\mathrm{private}}$ sampled from $\\mathrm{P}(\\mathcal{X}_{\\mathrm{private}},\\mathcal{Y}_{\\mathrm{private}})$ ). In standard settings, for a specific class $y$ in $\\mathcal{V}_{\\mathrm{private}}$ , MIAs aim to reconstruct synthetic samples by exploiting access to the target model M to uncover sensitive features of class $y$ . In this context, the adversary is limited to querying $\\mathrm{M}$ , and also possesses knowledge of the target data domain but lacks specific details about $\\mathcal{D}_{\\mathrm{private}}$ . ", "page_idx": 2}, {"type": "text", "text": "Mathematically, MI is formulated as an optimization problem: Given a target class $y$ , the goal is to find a sample $\\mathbf{x}$ that maximizes the model M\u2019s prediction score for class $y$ . In high-dimensional data settings, traditional MIAs [Fredrikson et al., 2015] use direct input space optimization, often leading to adversarial samples [Szegedy et al., 2014] that, despite high prediction scores, lack meaningful features. To mitigate this issue, Zhang et al. [2020] propose a generative MI approach, which learns a distributional prior to constrain the optimization to a low-dimensional, meaningful manifold. ", "page_idx": 2}, {"type": "text", "text": "Current generative MIAs primarily concentrate on either the initial training process of GANs [Chen et al., 2021, Yuan et al., 2023, Nguyen et al., 2024] or the optimization techniques used in the attacks [Zhang et al., 2020, Wang et al., 2021a, Struppek et al., 2022, Kahla et al., 2022, Nguyen et al., 2023]. In this paper, we take another direction and introduce a novel approach by fine-tuning the GAN\u2019s generator based on the attack results from previous runs. This method introduces a dynamic and iterative dimension to model inversion attacks, expanding the current understanding and research direction of generative MIAs. For detailed related work, please refer to Appx. A.1. ", "page_idx": 2}, {"type": "text", "text": "Specifically, the generative MI approach consists of two stages. Initially, a GAN learns a prior from public auxiliary datasets, in which $\\mathcal{V}_{\\mathrm{public}}\\cap\\mathcal{V}_{\\mathrm{private}}=\\emptyset$ . This process involves a generator, denoted as $\\mathrm{\\bar{G}}(\\cdot;\\theta)\\colon\\mathcal{Z}\\to\\mathcal{X}_{\\mathrm{prior}}$ , parameterized by $\\pmb{\\theta}$ , that transforms a low-dimensional latent code, $\\mathbf{z}\\in{\\mathcal{Z}}$ , into a high-dimensional image, $\\mathbf{x}\\in\\mathcal{X}_{\\mathrm{prior}}$ . Concurrently, a discriminator $\\operatorname{D}(\\cdot;\\phi)\\colon\\mathcal{X}\\to\\mathbb{R}$ , which can distinguish between generated and real images. Subsequently, the MI optimization can be constrained to the latent space $\\mathcal{Z}$ of the fixed prior G, which can be formulated as: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{z}^{*}=\\underset{\\mathbf{z}}{\\arg\\operatorname*{min}}\\ \\mathcal{L}_{\\mathrm{id}}(\\mathbf{z};y,\\mathrm{M},\\mathrm{G})+\\lambda\\mathcal{L}_{\\mathrm{prior}}(\\mathbf{z};\\mathrm{G},\\mathrm{D}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\mathcal{L}_{\\mathrm{id}}(\\cdot)$ denotes the identity loss, e.g., the cross-entropy loss \u2212log $\\mathbb{P}_{\\mathrm{M}}\\!\\left(y|\\mathrm{G}(z)\\right)$ , which optimizes for an optimal synthetic sample $\\mathbf{x}^{*}=\\mathrm{G}(\\mathbf{z}^{*})$ . Additionally, $\\mathcal{L}_{\\mathrm{prior}}(\\cdot)$ serves as a regularizer for the latent code $\\mathbf{z}$ , and the parameter $\\lambda$ balances the trade-off between the identity loss and the regularizer. ", "page_idx": 2}, {"type": "text", "text": "2.2 Distribution Discrepancy Measure ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "To effectively align distributions in our methods, it is essential to introduce metrics that can accurately quantify the differences between them. Two commonly used measures for this purpose are maximum mean discrepancy (MMD) and conditional transport (CT). MMD focuses on mean differences ", "page_idx": 2}, {"type": "text", "text": "using kernel methods, while CT incorporates cost-based transport distances, offering complementary perspectives on distributional discrepancies. This section introduces the empirical estimation of MMD and CT. For more details on these discrepancy measures, please refer to Appx. A.2. ", "page_idx": 3}, {"type": "text", "text": "Estimation of MMD. Given distributions $\\mathrm{P}$ and $\\mathrm{Q}$ , and sample sets $S_{X}\\,=\\,\\{{\\bf x}_{i}\\}_{i=1}^{n}\\,\\sim\\,\\mathrm{P}$ and $S_{Y}=\\{\\mathbf{y}_{j}\\}_{j=1}^{m}\\sim\\mathrm{Q},$ , MMD can be estimated with the following estimator [Gretton et al., 2012b]: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{\\displaystyle\\widehat{\\mathrm{MMD}}_{u}^{2}(S_{X},S_{Y};k)=\\displaystyle\\frac{1}{n(n-1)}\\sum_{i=1}^{n}\\sum_{j=1,j\\neq i}^{n}k(\\mathbf{x}_{i},\\mathbf{x}_{j})+\\displaystyle\\frac{1}{m(m-1)}\\sum_{i=1}^{m}\\sum_{j=1,j\\neq i}^{m}k(\\mathbf{y}_{i},\\mathbf{y}_{j})}\\\\ {\\displaystyle}&{\\displaystyle-\\,\\frac{2}{m n}\\sum_{i=1}^{n}\\sum_{j=1}^{m}k(\\mathbf{x}_{i},\\mathbf{y}_{j}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $k$ is a kernel function, $\\mathbf{x}_{i},\\mathbf{x}_{j}\\in S_{X}$ and $\\mathbf{y}_{i},\\mathbf{y}_{j}\\in S_{Y}$ . ", "page_idx": 3}, {"type": "text", "text": "Estimation of CT. Similarly, for sample sets $S_{X}=\\{\\mathbf{x}_{i}\\}_{i=1}^{n}$ and $S_{Y}=\\{\\mathbf{y}_{j}\\}_{j=1}^{m}$ , the CT measure can be approximated as follows [Zheng and Zhou, 2021]: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{CT}(S_{X},S_{Y})=\\sum_{i=1}^{n}\\sum_{j=1}^{m}c(\\mathbf{x}_{i},\\mathbf{y}_{j})\\left(\\frac{e^{-d_{\\psi}(\\mathbf{x}_{i},\\mathbf{y}_{j})}}{\\sum_{j^{\\prime}=1}^{m}e^{-d_{\\psi}(\\mathbf{x}_{i},\\mathbf{y}_{j^{\\prime}})}}+\\frac{e^{-d_{\\psi}(\\mathbf{x}_{i},\\mathbf{y}_{j})}}{\\sum_{i^{\\prime}=1}^{n}e^{-d_{\\psi}(\\mathbf{x}_{i^{\\prime}},\\mathbf{y}_{j})}}\\right).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Here, $d_{\\psi}(\\mathbf{x},\\mathbf{y})$ is a function parameterized by $\\psi$ that measures the similarity between $\\mathbf{x}$ and $\\mathbf{y}$ , and $c(\\mathbf{x},\\mathbf{y})$ is a cost function that measures the distance between the points $\\mathbf{x}$ and $\\mathbf{y}$ . ", "page_idx": 3}, {"type": "text", "text": "3 Pseudo-private Data Guided Model Inversion ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "This section introduces our proposed methodology, i.e., pseudo-private data guided MI (PPDG-MI). First, we present and discuss the critical motivation that inspires our method (Sec. 3.1). Second, we introduce the general framework of PPDG-MI (Sec. 3.2). Third, to ease understanding, we demonstrate and illustrate the rationality of our solution on a simple toy dataset (Sec. 3.3). Fourth, we present a more nuanced and detailed strategy for tuning the generator to enhance density in high-dimensional image spaces, accompanied by multiple algorithmic implementations (Sec. 3.4). ", "page_idx": 3}, {"type": "text", "text": "3.1 Motivation: Effect of Distribution Discrepancies on MIAs ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Collecting public auxiliary datasets that closely resemble the private dataset remains challenging. This difficulty arises because the MI adversary lacks knowledge of specific class information, and only understands the general data domain about $\\mathrm{P}(\\lambda_{\\mathrm{private}})$ . Thus, we hypothesize a significant distribution discrepancy between the prior distribution $\\mathrm{P}({\\mathcal X}_{\\mathrm{prior}})$ and the private data distribution $\\mathrm{P}(\\lambda_{\\mathrm{private}})$ . This claim is supported by Fig. 1(a), where we quantify the distribution discrepancy between commonly adopted public auxiliary datasets and private training datasets using the MMD measure [Borgwardt et al., 2006], showcasing a substantial gap between these two distributions. ", "page_idx": 3}, {"type": "text", "text": "To evaluate the impact of this distribution discrepancy on MI performance, we create a series of proxy prior distributions through linear interpolation, where a mixing coefficient $\\alpha\\in[0,1]$ determines the proportion of samples drawn from each distribution. Specifically, a fraction $\\alpha$ of samples is drawn from $\\mathrm{P}(\\mathcal{X}_{\\mathrm{prior}})$ , and the remaining $(1-\\alpha)$ is drawn from $\\mathrm{P}(\\lambda_{\\mathrm{private}})$ . This process is represented as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{P}(\\mathcal{X}_{\\mathrm{prior}}^{\\prime})=\\alpha\\mathrm{P}(\\mathcal{X}_{\\mathrm{prior}})+(1-\\alpha)\\mathrm{P}(\\mathcal{X}_{\\mathrm{private}}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "We apply these proxy prior distributions to constrain the MI optimization as outlined in Eq. (1). As illustrated in Figs. 1(b) and 1(c), the MI performance decreases monotonically as the MMD value between $\\mathrm{P}(\\mathcal{X}_{\\mathrm{prior}}^{\\prime})$ and $\\mathrm{P}(\\lambda_{\\mathrm{private}})$ increases. This leads us to pose a critical research question: ", "page_idx": 3}, {"type": "text", "text": "How can the discrepancy between the prior distribution and the unknown private data distribution be mitigated to enhance MI performance? ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In response to this, a revised inversion pipeline is required, wherein G is dynamically adjusted throughout the inversion process. This adjustment aims to progressively narrow the distribution gap between $\\mathrm{P}({\\mathcal X}_{\\mathrm{prior}})$ and $\\bar{\\mathrm{P}}\\bar{(}\\lambda_{\\mathrm{private}}^{})$ , thereby potentially improving MI performance. ", "page_idx": 3}, {"type": "image", "img_path": "pyqPUf36D2/tmp/8e557690b59572866adb7e1997a27f66675935a69b23a1dd0ce4d9db016473dc.jpg", "img_caption": ["Figure 2: Overview of traditional generative MI framework vs. pseudo-private data guided MI (PPDG-MI) framework. PPDG-MI leverages pseudo-private data $\\hat{\\bf x}$ generated during the inversion process, which reveals the characteristics of the actual private data, to fine-tune the generator G. The goal is to enhance the density of $\\hat{\\bf x}$ under the learned distributional prior $\\mathrm{P}(\\mathcal{X}_{\\mathrm{prior}})$ , thereby increasing the probability of sampling actual private data $\\mathbf{x}^{*}$ during the inversion process. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "3.2 PPDG-MI Framework ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "This section presents a novel model inversion pipeline that dynamically adjusts the generator G to mitigate the distribution discrepancy between $\\bar{\\mathrm P}(\\bar{\\mathcal X}_{\\mathrm{prior}})$ and $\\dot{\\mathrm{P}}(\\chi_{\\mathrm{private}})$ (cf. Fig. 2 for the framework overview). As aforementioned, in the MI context, while the specific details of $\\mathcal{D}_{\\mathrm{private}}$ remain unknown, we have access to the target model M, which is well-trained on $\\mathcal{D}_{\\mathrm{private}}$ , still encapsulates information about $\\mathcal{D}_{\\mathrm{private}}$ . Therefore, by conducting MI on the target model M, we can generate a set of pseudo-private samples (i.e., reconstructed samples), denoted as Dsprivate, which reveal the characteristics of the private dataset $\\mathcal{D}_{\\mathrm{private}}$ and can serve as its surrogate. Thus, the key insight is that by enhancing the density of the prior distribution $\\mathrm{P}({\\mathcal X}_{\\mathrm{prior}})$ around Dsprivate, we indirectly increase the density around $\\mathcal{D}_{\\mathrm{private}}$ as well. Consequently, the probability of sampling data from $\\mathrm{P}(\\lambda_{\\mathrm{private}})$ could be increased. This strategy is termed pseudo-private data guided MI (PPDG-MI). ", "page_idx": 4}, {"type": "text", "text": "To this intuition, the proposed MI framework consists of the following three iterative steps: ", "page_idx": 4}, {"type": "text", "text": "Step-1: Pseudo-private Data Generation by Conducting MI on the Target Model. Specifically, in generative MI, optimization is restricted to the latent space $\\mathcal{Z}$ . Initially, we sample a set of latent codes, $\\mathbf{Z}=\\{\\mathbf{z}_{i}\\mid\\mathbf{z}_{i}\\in\\mathcal{Z},i=1,\\ldots,N\\}$ . Then, by leveraging Eq. (1), these initial latent codes are optimized to produce $\\hat{\\mathbf{Z}}=\\{\\hat{\\mathbf{z}}=\\arg\\operatorname*{min}\\mathcal{L}_{\\mathrm{id}}(\\mathbf{z})+\\lambda\\mathcal{L}_{\\mathrm{prior}}(\\mathbf{z})\\mid\\mathbf{z}\\in\\mathbf{Z}\\}$ . Subsequently, this optimized set $\\hat{\\textbf{Z}}$ is utilized to generate a pseudo-private dataset $\\mathcal{D}_{\\mathrm{private}}^{\\mathrm{s}}=\\{\\hat{\\mathbf{x}}=\\mathrm{G}(\\hat{\\mathbf{z}})\\mid\\hat{\\mathbf{z}}\\in\\hat{\\mathbf{Z}}\\}$ . ", "page_idx": 4}, {"type": "text", "text": "Step-2: Selection of High-Quality Pseudo-private Data. In this step, we aim to select high-quality data from $\\mathcal{D}_{\\mathrm{private}}^{\\mathrm{s}}$ that closely resemble the characteristics of samples in $\\mathcal{D}_{\\mathrm{private}}$ , serving as their proxy. An intuitive method is to select samples with high prediction scores. Thus, following Struppek et al. [2022], we opt to select samples with larger expected prediction scores $\\mathbb{E}[\\mathbb{P}_{\\mathrm{M}}(y|\\,T(\\mathbf{\\bar{x}}))]$ under random image transformations $T$ , indicating that $\\hat{\\bf x}$ represents the desired characteristics for target class $y$ more accurately. Specifically, we select a high-quality subset Dsprivate, consisting of samples with top $K$ expected prediction scores from $\\mathcal{D}_{\\mathrm{private}}^{\\mathrm{s}}$ . ", "page_idx": 4}, {"type": "text", "text": "Step-3: Density Enhancement around Pseudo-private Data. In this step, we focus on fine-tuning G to adjust the prior distribution $\\mathrm{P}({\\mathcal X}_{\\mathrm{prior}})$ , aiming to increase the probability of sampling data from $\\mathrm{P}(\\lambda_{\\mathrm{private}})$ . In the existing literature, MIAs can be categorized into two types: those targeting high-resolution tasks [Struppek et al., 2022] and those targeting low-resolution tasks [Zhang et al., 2020, Chen et al., 2021, Kahla et al., 2022, Nguyen et al., 2023]. In high-resolution MIAs, adversaries leverage pre-trained GANs without access to training specifics. In contrast, low-resolution MIAs involve adversaries training GANs from scratch using the public auxiliary dataset $\\mathcal{D}_{\\mathrm{public}}$ . This distinction enables the development of tuning strategies tailored to different attack settings. ", "page_idx": 4}, {"type": "text", "text": "For MIAs focusing on low-resolution tasks, where the generator G is less powerful and the lowresolution image manifold is more susceptible to disruption, we adopt a principled tuning strategy. Specifically, we fine-tune G and D using the original GAN training objective on $\\bar{D}_{\\mathrm{public}}\\cup\\bar{D}_{\\mathrm{private}}^{\\prime}$ , a strategy termed PPDG-vanilla (cf. Alg. 1). For MIAs focusing on high-resolution tasks, e.g., Plug & Play Attacks (PPA) [Struppek et al., 2022], we propose a tuning strategy that leverages only the high-quality pseudo-private dataset $\\mathcal{D}_{\\mathrm{private}}^{\\mathrm{s}^{\\prime}}$ , which can be formalized as follows: ", "page_idx": 4}, {"type": "image", "img_path": "pyqPUf36D2/tmp/3c91b7cad5372c169096813ab61a1a4f0815c35f507808ccd7d5ddfa45edd946.jpg", "img_caption": ["Figure 3: Illustration of the rationale behind PPDG-MI using a simple 2D example. Training samples from Class 0-2 are represented by purple, blue, and green, respectively, while public auxiliary data are shown in yellow. MIAs aim to recover training samples from Class 1, with reconstructed samples shown in red. (a) Results of the baseline attack with a fixed prior. (b) Left: Pseudo-private data generation. Middle: Density enhancement of pseudo-private data under prior distribution. Right: Final attack results of PPDG-MI with the tuned prior, where all the recovered points converge to the centroid of the class distribution, indicating the most representative features are revealed. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathrm{G},\\mathrm{D}\\gets\\mathtt{F i n e-t u n e}(\\mathrm{G},\\mathrm{D},\\mathcal{D}_{\\mathrm{private}}^{\\mathrm{s}^{\\prime}}).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "This adjustment aims to increase the density of the prior distribution around Dsprivate. The concrete realizations (cf. Alg. 2) are presented in Sec. 3.4. After fine-tuning the generator, return to Step-1 and repeat the attack process to further improve the MI performance. Our experiments primarily focus on PPA, which allows us to investigate a more realistic attack scenario with high-resolution data. ", "page_idx": 5}, {"type": "text", "text": "3.3 Understanding PPDG-MI with 2D Data ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "To illustrate the principles of PPDG-MI, we present a toy example using a 2D dataset with three classes, each sampled from a class-conditional Gaussian distribution, as shown in Fig. 3. Additionally, a public dataset is sampled from a separate Gaussian distribution to learn the distributional prior $\\mathrm{P}({\\mathcal X}_{\\mathrm{prior}})$ . We simulate a simple MIA by generating an initial set of samples using generator G and then optimizing these samples to maximize the model\u2019s prediction score for Class 1. The objective is to uncover the features of Class 1, primarily the coordinates of the training samples. The closer these optimized samples are to the centroid of Class 1\u2019s distribution, i.e., the high-density region, the more effective the attack. See Appx. C.7 for a larger version of Fig. 3 and the experimental details of the toy example. An animated illustration of the toy demo is available in the supplementary materials. ", "page_idx": 5}, {"type": "text", "text": "The baseline attack results are shown in Fig. 3(a), where a fixed G is adopted during the inversion process. The left panel of Fig. 3(b) illustrates the generation of dataset $\\mathcal{D}_{\\mathrm{private}}^{\\mathrm{s}}$ through a round of MI on model M. Middle panel of Fig. 3(b) shows the enhancement of density around Dsprivate under the prior distribution $\\mathrm{P}({\\mathcal X}_{\\mathrm{prior}})$ , achieved by fine-tuning $\\mathrm{G}(\\cdot;\\theta)$ to align with the empirical distribution of $\\mathcal{D}_{\\mathrm{private}}^{\\mathrm{s}}$ , using the CT measure. The final attack results of PPDG-MI are shown in the right panel of Fig. 3(b). It is evident that, in comparison to the baseline where only a small fraction of reconstructed samples fall within the high-density region of the training data distribution, all reconstructed samples from PPDG-MI are located in this high-density region. See Appx. C.7 for the quantitative results. ", "page_idx": 5}, {"type": "text", "text": "Although we initially applied a direct distribution match strategy in this simplified setting to implement PPDG-MI, the empirical results indicate that this approach is less effective for higher-dimensional image data, as it would destroy the generator\u2019s manifold (Appx. C.8). We address this issue by introducing a nuanced tuning strategy tailored for high-dimensional data settings, detailed in Sec. 3.4. ", "page_idx": 5}, {"type": "text", "text": "3.4 Nuanced Approach of PPDG-MI for High-Dimensional Image Data ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Considering the primary baseline PPA [Struppek et al., 2022] uses StyleGAN [Karras et al., 2020] as its distributional prior, our approach leverages StyleGAN\u2019s disentangled nature, allowing slight local changes to its produced appearance without disrupting the manifold. Specifically, we first identify a high-density neighbor for each pseudo-private sample \u02c6x (cf. Fig. 4(b)) and adjust this neighbor to be closer to $\\hat{\\bf x}$ , thereby enhancing density around $\\hat{\\bf x}$ in the generator\u2019s domain (cf. Fig. 4(c)). ", "page_idx": 5}, {"type": "image", "img_path": "pyqPUf36D2/tmp/59b62746a06fe9e50fed45ffc6140949160d0ec1cc907f0de233047aee137421.jpg", "img_caption": ["Figure 4: Illustration of PPDG-MI using a point-wise tuning approach. (a) The distribution of discriminator logit outputs for randomly generated samples by the generator G, showing that the discriminator can empirically reflect the density of generated samples. (b) Locating the high-density neighbor $\\mathbf{x}^{\\mathrm{p}}$ by optimizing Eq. (6). Darker colors represent regions with higher density. (c) Increasing density around the pseudo-private data $\\hat{\\bf x}$ by moving $\\mathbf{x}^{\\mathrm{p}}$ towards $\\hat{\\bf x}$ , i.e., optimizing Eq. (7). "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Instantiate PPDG-MI with Point-wise Tuning. To this intuition, we detail a two-step method to increase the density around pseudo-private samples. First, to locate a near neighbor $\\mathbf{x}^{\\mathrm{p}}$ of $\\hat{\\bf x}$ , we optimize the latent code $\\mathbf{z}$ to produce $\\mathbf{x}^{\\mathrm{p}}$ . The closeness between $\\mathbf{x}^{\\mathrm{p}}$ and $\\hat{\\bf x}$ is measured using the LPIPS perceptual loss function [Zhang et al., 2018]. Additionally, to ensure that $\\mathbf{x}^{\\mathrm{p}}$ is located in a high-density region of $\\mathrm{P}(\\mathcal{X}_{\\mathrm{prior}})$ , we leverage the discriminator D. Although GANs typically do not provide explicit probability density functions, empirical evidence suggests that D effectively indicates the density of generated samples (cf. Fig. 4(a)). Overall, the optimization objective is formulated as ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{z}^{\\mathsf{p}}=\\underset{\\mathbf{z}}{\\arg\\operatorname*{min}}\\ \\underbrace{\\mathcal{L}_{\\mathrm{LPIPS}}(\\hat{\\mathbf{x}},\\mathrm{G}(\\mathbf{z}))}_{\\mathrm{neighborhood\\constraint}}-\\underbrace{\\lambda_{1}\\mathrm{D}(\\mathrm{G}(\\mathbf{z}))}_{\\mathrm{high-density\\constraint}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\mathcal{L}_{\\mathrm{LPIPS}}$ represents the perceptual loss function, and $\\lambda_{1}$ is a tuning hyperparameter that balances the constraints. At this step, the generator remains frozen. After optimizing Eq. (6), we obtain a high-density neighbor point $\\mathbf{x}^{\\mathrm{p}}=\\mathrm{G}(\\mathbf{z}^{\\mathrm{p}};\\pmb{\\theta})$ of $\\hat{\\bf x}$ . We then aim to slightly alter $\\mathrm{G}(\\cdot;\\theta)$ to pull $\\mathbf{x}^{\\mathrm{p}}$ towards $\\hat{\\bf x}$ , thereby enhancing the local density around $\\hat{\\bf x}$ (cf. Fig. 4(c)). This is accomplished by fine-tuning the generator with the point-wise loss term: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{PPDG-PW}}(\\pmb{\\theta})=\\mathcal{L}_{\\mathrm{LPIPS}}(\\hat{\\mathbf{x}},\\mathrm{G}(\\mathbf{z}^{\\mathrm{p}};\\pmb{\\theta})).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "At this step, $\\mathbf{z}^{\\mathrm{p}}$ remains fixed, and the adjustment is applied exclusively to the generator G. Building on the point-wise density enhancement, we extend our approach to a batch-wise method using statistical distribution discrepancy measures [Borgwardt et al., 2006, Zheng and Zhou, 2021], aiming for a more principled local distribution alignment strategy. ", "page_idx": 6}, {"type": "text", "text": "Instantiate PPDG-MI with Batch-wise Tuning. Given the sets $\\mathcal{D}_{\\mathrm{private}}^{\\mathrm{s}^{\\prime}}~=~\\{\\hat{\\mathbf{x}}_{i}\\}_{i=1}^{m}$ and $\\{\\mathrm{G}(\\mathbf{z}_{j})\\}_{j=1\\atop j=2}^{n}\\sim\\mathrm{P}(\\mathcal{X}_{\\mathrm{prior}})$ , following the point-wise tuning settings, we initially map these samples to the LPIPS space using a feature extractor $f$ . Denote $\\delta$ as the distribution discrepancy measure, the batch-wise tuning strategy adapts the previous point-wise Eqs. (6) and (7) as follows: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\{\\mathbf{z}_{j}^{\\mathsf{p}}\\}_{j=1}^{n}=\\arg\\operatorname*{min}_{\\{\\mathbf{z}_{j}\\}_{j=1}^{n}}\\,\\underbrace{\\delta(\\{f(\\hat{\\mathbf{x}}_{i})\\}_{i=1}^{m},\\{f(\\mathrm{G}(\\mathbf{z}_{j}))\\}_{j=1}^{n})}_{\\mathrm{neishorhond~constraint}}-\\lambda_{1}\\frac{1}{n}\\sum_{i=1}^{n}\\mathrm{D}(\\mathrm{G}(\\mathbf{z}_{j})),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{\\mathrm{PDG-BW}}(\\pmb{\\theta})=\\delta(\\{f(\\hat{\\mathbf{x}}_{i})\\}_{i=1}^{m},\\{f(\\mathrm{G}(\\mathbf{z}_{j}^{\\mathrm{p}};\\pmb{\\theta}))\\}_{j=1}^{n}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "We present two realizations of the batch-wise tuning strategy: PPDG-MMD and PPDG-CT. When $\\delta$ is set as MMD with Gaussian kernel $k$ , the optimization objective in Eqs. (8a) and (8b) is realized as ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\{\\mathbf{z}_{j}^{\\mathsf{p}}\\}_{j=1}^{n}=\\underset{\\{\\mathbf{z}_{j}\\}_{j=1}^{n}}{\\mathrm{arg}\\,\\mathrm{min}}\\ \\widehat{\\mathrm{MMD}}_{u}^{2}(\\{f(\\hat{\\mathbf{x}}_{i})\\}_{i=1}^{m},\\{f(\\mathrm{G}(\\mathbf{z}_{j}))\\}_{j=1}^{n};k)-\\lambda_{1}\\frac{1}{n}\\sum_{i=1}^{n}\\mathrm{D}(\\mathrm{G}(\\mathbf{z}_{j})),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{\\mathrm{PPDG-MMD}}(\\pmb{\\theta})=\\widehat{\\mathrm{MMD}}_{u}^{2}(\\{\\boldsymbol{f}(\\hat{\\mathbf{x}}_{i})\\}_{i=1}^{m},\\{\\boldsymbol{f}(\\mathrm{G}(\\mathbf{z}_{j}^{\\mathrm{p}};\\pmb{\\theta}))\\}_{j=1}^{n};k).}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Similarly, when $\\delta$ is set as CT, the optimization objective in Eqs. (8a) and (8b) is realized as ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\{\\mathbf{z}_{j}^{\\mathrm{p}}\\}_{j=1}^{n}=\\underset{\\{\\mathbf{z}_{j}\\}_{j=1}^{n}}{\\mathrm{arg}\\,\\mathrm{min}}\\,\\,\\mathrm{CT}(\\{f(\\hat{\\mathbf{x}}_{i})\\}_{i=1}^{m},\\{f(\\mathrm{G}(\\mathbf{z}_{j}))\\}_{j=1}^{n})-\\lambda_{1}\\frac{1}{n}\\sum_{i=1}^{n}\\mathrm{D}(\\mathrm{G}(\\mathbf{z}_{j})),\n$$", "text_format": "latex", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{\\mathrm{PDG-CT}}(\\pmb{\\theta})=\\mathrm{CT}(\\{f(\\hat{\\mathbf{x}}_{i})\\}_{i=1}^{m},\\{f(\\mathrm{G}(\\mathbf{z}_{j}^{\\mathrm{p}};\\pmb{\\theta}))\\}_{j=1}^{n}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "The cost function in Eq. (3) is implemented as $c(\\mathbf{x},\\mathbf{y})=1-\\cos(f(\\mathbf{x}),f(\\mathbf{y}))$ , while the distance function is implemented as $d(\\mathbf{x},\\mathbf{y})=f(\\mathbf{x})^{T}f(\\mathbf{y})$ , which are commonly adopted realization choices in existing literature [Tanwisuth et al., 2021, 2023]. ", "page_idx": 7}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we evaluate the performance of SOTA MI methods before and after integrating them with PPDG-MI, as well as the robustness against SOTA MI defenses, including BiDO [Peng et al., 2022] and NegLS [Struppek et al., 2024], to assess the overall effectiveness of PPDG-MI. The evaluation primarily focuses on real-world face recognition tasks. For high-resolution $(224\\times224)$ tasks, we consider PPA [Struppek et al., 2022] in the white-box setting. For low-resolution $(64\\times64)$ tasks, we consider GMI [Zhang et al., 2020], KEDMI [Chen et al., 2021], LOM [Nguyen et al., 2023], and PLG-MI [Yuan et al., 2023] in the white-box setting, RLB-MI [Han et al., 2023] in the black-box setting, as well as BREP-MI [Kahla et al., 2022] in the label-only setting. ", "page_idx": 7}, {"type": "text", "text": "4.1 Experimental Setup ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "This section briefly introduces the experimental setups. For further details, please refer to Appx. C. ", "page_idx": 7}, {"type": "text", "text": "Datasets and Models. In line with existing MIA literature on face recognition, we use the CelebA [Liu et al., 2015], FaceScrub $[\\mathrm{Ng}$ and Winkler, 2014], and FFHQ datasets [Karras et al., 2019]. These datasets are divided into two parts: the private training dataset $\\mathcal{D}_{\\mathrm{private}}$ and the public auxiliary dataset $\\mathcal{D}_{\\mathrm{public}}$ , ensuring no identity overlap. For high-resolution tasks, we trained ResNet18 [He et al., 2016], DenseNet-121 [Huang et al., 2017] and ResNeSt-50 [Zhang et al., 2022] as target models. For low-resolution tasks, we trained VGG16 [Simonyan and Zisserman, 2015] and face.evoLVe [Wang et al., 2021b] as target models. The training details of these models are presented in Appx. C.3. We summarize the attack methods, target models, and datasets adopted in Tab. 4. ", "page_idx": 7}, {"type": "text", "text": "Attack Parameters. For all MIAs, we fine-tune the generator $\\mathbf{G}$ in an identity-wise manner, to minimize alterations to the generator\u2019s latent space. Thus, adjustments to the attack parameters in official implementations are required. Detailed attack parameters are provided in Appx. C.4. ", "page_idx": 7}, {"type": "text", "text": "Evaluation Metrics. To evaluate the performance of an MIA, we need to assess whether the reconstructed images reveal private information about the target identity. Following existing literature [Zhang et al., 2020], we adopt top-1 $(\\operatorname{Acc}\\@leftrightarrow1)$ and top-5 $(\\operatorname{Acc}\\!\\odot\\!5)$ attack accuracy, as well as K-Nearest Neighbors Distance (KNN Dist). Details for these metrics are provided in Appx. C.5. ", "page_idx": 7}, {"type": "text", "text": "4.2 Main Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In the main experiments, we integrate PPDG-MI for density enhancement while still employing the baseline attack method for MI. We conduct one round of fine-tuning of $\\mathbf{G}$ and present the resulting attack results to demonstrate the efficacy of PPDG-MI. The results of multi-round fine-tuning are reserved for the ablation study (cf. Sec. 4.3). Additional experimental results, including evaluations on various target models, assessments with PLG-MI, black-box, and label-only MIAs, as well as comparisons against SOTA MI defenses for low-resolution tasks, are presented in Appx. D. ", "page_idx": 7}, {"type": "text", "text": "Comparison with PPA in the high-resolution setting. For each baseline setup, we report results for three variants: PPDG-PW, PPDG-CT and PPDG-MMD. The results presented in Tab. 1 demonstrate that our proposed method significantly improves MI performance across all setups, validating its effectiveness. Notably, integrating our methods with the baseline substantially increases attack accuracy. The KNN distance results also confirm that our methods more accurately reconstruct data resembling the private training data. Qualitative results of reconstructed samples from all target models are provided in Figs. 9 and 10 in Appx. D.3. Additionally, among the three PPDG-MI variants, the batch-wise tuning strategy consistently outperforms the point-wise tuning strategy. Batch-wise tuning captures characteristics of the local data distribution by handling batches of pseudo-private data, whereas point-wise tuning focuses on individual data points. Furthermore, batch-wise tuning is more robust against outliers, leading to a more reliable adjustment of the prior distribution. ", "page_idx": 7}, {"type": "table", "img_path": "pyqPUf36D2/tmp/f25cb8de74fedffcd388db2604f0d71607bf2494694cf511b5375d2277697d55.jpg", "table_caption": ["Table 1: Comparison of MI performance with PPA in high-resolution settings. $\\mathcal{D}_{\\mathrm{private}}=\\mathrm{CelebA}$ or FaceScrub, GANs are pre-trained on $\\mathcal{D}_{\\mathrm{public}}=\\mathrm{FFHQ}$ . The symbol $\\downarrow$ (or $\\uparrow)$ indicates smaller (or larger) values are preferred, and the green numbers represent the attack performance improvement. The running time ratio (Ratio) between prior fine-tuning and MI reflects the overhead of fine-tuning. "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "pyqPUf36D2/tmp/d9bf2b6cec6dcf7ae9b832e9eafc68839192ee5ff62cb29bee9a17a327374ced.jpg", "table_caption": ["Table 2: Comparison of MI performance with white-box MIAs in low-resolution settings. Target model $\\mathrm{M}=\\mathrm{VGG}16$ trained on $\\mathcal{D}_{\\mathrm{private}}=\\mathrm{CelebA}$ . GANs are trained on $\\mathcal{D}_{\\mathrm{public}}=$ CelebA or FFHQ. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Comparison with white-box MIAs in the lowresolution setting. For each baseline setup, we report results for PPDG-vanilla. The results are shown in Tab. 2, where PPDG-vanilla consistently outperforms various baseline white-box attacks. The improvement is evident in both attack accuracy and KNN distance metrics. Notably, even with a significant distribution shift between the private training dataset (CelebA) and the public auxiliary dataset (FFHQ), the principled vanilla fine-tuning strategy with original GAN training objectives effectively enhances the density around pseudo-private samples. As a highlight, PPDGvanilla outperforms the baseline LOM (GMI) by achieving a $12.35\\%$ increase in top-1 attack accuracy and reducing KNN distance by approximately 75. Qualitative results of the reconstructed sample are provided in Figs. 11 and 12 in Appx. D.3. ", "page_idx": 8}, {"type": "text", "text": "Attacks against SOTA MI defense methods. We extend our evaluation to include state-of-theart model inversion defense methods, specifically BiDO-HSIC and NegLS, comparing the perfor", "page_idx": 8}, {"type": "table", "img_path": "pyqPUf36D2/tmp/5b454664ab325c6124755240f7427a559dab8103f6c10bb6975a02c95a30f1f6.jpg", "table_caption": ["Table 3: MI performance against SOTA defense methods in high-resolution settings. The target model $\\mathrm{M}=\\mathrm{ResNet}{-152}$ is trained on $\\ensuremath{\\mathcal{D}_{\\mathrm{private}}}=$ FaceScrub, GANs are pre-trained on $\\mathcal{D}_{\\mathrm{public}}=$ FFHQ. Bold numbers indicate superior results. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "mance of our proposed methods\u2014PPDG-PW, PPDG-CT, and PPDG-MMD\u2014with the baseline PPA. As summarized in Tab. 3, each proposed method consistently outperforms the baseline. Notably, PPDG-MMD achieves a $6.05\\%$ improvement in top-1 attack accuracy and reduces KNN distance by 0.0529 relative to the baseline against BiDO-HSIC. Similarly, against NegLS, PPDG-CT shows a $4.90\\%$ improvement in top-1 attack accuracy and a 0.0818 reduction in KNN distance compared to the baseline. Additional results for the low-resolution setting are provided in Appx. D.1. ", "page_idx": 8}, {"type": "image", "img_path": "pyqPUf36D2/tmp/5b8c6f86fb367d1f7a0238dd996a1b496060c51dc008fb57416a3e6865347ff0.jpg", "img_caption": ["Figure 5: Ablation study in the high-resolution setting. Left: Impact of iterative fine-tuning. Middle: Importance of selecting high-quality pseudo-private data for fine-tuning. Right: Effectiveness of using the discriminator as an empirical density estimator to locate high-density neighbors. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "4.3 Ablation Study ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this section, we present part of the ablation study on MIA in the high-resolution setting to further explore PPDG-MI. The target model is ResNet-18 trained on CelebA; GANs are pre-trained on FFHQ. Additional ablation results for high- and low-resolution settings are provided in Appx. D.2. Discussions (e.g., broader impact, failure case analysis and limitations) are provided in Appx. E. ", "page_idx": 9}, {"type": "text", "text": "Iterative fine-tuning. The iterative fine-tuning process is crucial for PPDG-MI (cf. Algs. 1 and 2). Its goal is to progressively increase the probability of sampling pseudo-private data with closer characteristics to the actual private training data. Ideally, if the classifier has learned all discriminative information of the target identity, this process can continue until it is capable of sampling the actual training data. As shown in left panel of Fig. 5, the attack performance consistently improves with additional rounds of fine-tuning, demonstrating the effectiveness of this approach. ", "page_idx": 9}, {"type": "text", "text": "Selecting high-quality pseudo-private data for density enhancement. The rationale behind enhancing the density around high-quality pseudo-private data, rather than random reconstructed ones, is that the former better reflect the characteristics of the private training data and are semantically closer. Thus, this increases the probability of sampling the actual training data. The middle panel of Fig. 5 compares the attack results of enhancing density around high-quality pseudo-private samples and randomly selected recovered samples, demonstrating the effectiveness of this strategy. ", "page_idx": 9}, {"type": "text", "text": "Locating high-density neighbors using the discriminator. We investigate the effect of using the discriminator D as an empirical density estimator to locate samples in high-density areas in Eqs. (6), (9a), and (10a). The comparison results, with and without the discriminator, are shown in the right panel of Fig. 5. The results indicate that MI performance decreases significantly without using the discriminator, with an approximate $13\u201322\\%$ reduction in attack accuracy across different fine-tuning methods. This demonstrates the effectiveness of incorporating D as a density estimator. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we identify a fundamental limitation common to state-of-the-art generative MIAs, i.e., the utilization of a fixed prior during the inversion phase. We argue that this approach is sub-optimal for MIAs. Accordingly, we introduce a novel inversion pipeline called pseudo-private data guided MI (PPDG-MI), which, for the first time, involves iteratively tuning the distributional prior during the inversion process using pseudo-private samples. This increases the probability of recovering actual private training data. We propose multiple realizations of PPDG-MI and demonstrate their effectiveness through extensive experiments. Our findings pave the way for future research on generative MIAs and highlight the urgent need for more robust defenses against MIAs. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "XP and BH were supported by NSFC General Program No. 62376235, Guangdong Basic and Applied Basic Research Foundation Nos. 2022A1515011652 and 2024A1515012399, HKBU Faculty Niche Research Areas No. RC-FNRA-IG/22-23/SCI/04, and HKBU CSD Departmental Incentive Scheme. TLL was partially supported by the following Australian Research Council projects: FT220100318, DP220102121, LP220100527, LP220200949, and IC190100031. FL is supported by the Australian Research Council (ARC) with grant numbers DP230101540 and DE240101089, and the NSF&CSIRO Responsible AI program with grant number 2303037. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Shengwei An, Guanhong Tao, Qiuling Xu, Yingqi Liu, Guangyu Shen, Yuan Yao, Jingwei Xu, and Xiangyu Zhang. Mirror: Model inversion for deep learning network with high fidelity. In NDSS, 2022.   \nKarsten M Borgwardt, Arthur Gretton, Malte J Rasch, Hans-Peter Kriegel, Bernhard Sch\u00f6lkopf, and Alex J Smola. Integrating structured biological data by kernel maximum mean discrepancy. Bioinformatics, 2006.   \nSi Chen, Mostafa Kahla, Ruoxi Jia, and Guo-Jun Qi. Knowledge-enriched distributional model inversion attacks. In ICCV, 2021.   \nChristopher A Choquette-Choo, Florian Tramer, Nicholas Carlini, and Nicolas Papernot. Label-only membership inference attacks. In ICML, 2021.   \nMatt Fredrikson, Somesh Jha, and Thomas Ristenpart. Model inversion attacks that exploit confidence information and basic countermeasures. In CCS, 2015.   \nMatthew Fredrikson, Eric Lantz, Somesh Jha, Simon Lin, David Page, and Thomas Ristenpart. Privacy in pharmacogenetics: An end-to-end case study of personalized warfarin dosing. In USENIX Security, 2014.   \nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In NeurIPS, 2014.   \nArthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bernhard Sch\u00f6lkopf, and Alexander Smola. A kernel two-sample test. The Journal of Machine Learning Research, 2012a.   \nArthur Gretton, Karsten M Borgwardt, Malte J. Rasch, Bernhard Sch\u00f6lkopf, and Alexander J. Smola. A kernel two-sample test. Journal of Machine Learning Research, 2012b.   \nIshaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Improved training of wasserstein gans. In NeurIPS, 2017.   \nGyojin Han, Jaehyun Choi, Haeil Lee, and Junmo Kim. Reinforcement learning-based black-box model inversion attacks. In CVPR, 2023.   \nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016.   \nGao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In CVPR, 2017.   \nMostafa Kahla, Si Chen, Hoang Anh Just, and Ruoxi Jia. Label-only model inversion attacks via boundary repulsion. In CVPR, 2022.   \nTero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In CVPR, 2019.   \nTero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and improving the image quality of stylegan. In CVPR, 2020.   \nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.   \nKalpesh Krishna, Gaurav Singh Tomar, Ankur P Parikh, Nicolas Papernot, and Mohit Iyyer. Thieves on sesame street! model extraction of bert-based apis. In ICLR, 2019.   \nBo Liu, Ming Ding, Sina Shaham, Wenny Rahayu, Farhad Farokhi, and Zihuai Lin. When machine learning meets privacy: A survey and outlook. ACM Computing Surveys, 2021.   \nZiwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In ICCV, 2015.   \nBlake Murdoch. Privacy and artificial intelligence: challenges for protecting health information in a new era. BMC Medical Ethics, 2021.   \nHong-Wei $\\mathrm{Ng}$ and Stefan Winkler. A data-driven approach to cleaning large face datasets. In ICIP, 2014.   \nBao-Ngoc Nguyen, Keshigeyan Chandrasegaran, Milad Abdollahzadeh, and Ngai-Man Man Cheung. Label-only model inversion attacks via knowledge transfer. In NeurIPS, 2024.   \nNgoc-Bao Nguyen, Keshigeyan Chandrasegaran, Milad Abdollahzadeh, and Ngai-Man Cheung. Re-thinking model inversion attacks against deep neural networks. In CVPR, 2023.   \nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. In NeurIPS, 2022.   \nXiong Peng, Feng Liu, Jingfeng Zhang, Long Lan, Junjie Ye, Tongliang Liu, and Bo Han. Bilateral dependency optimization: Defending against model-inversion attacks. In KDD, 2022.   \nAlec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. In ICLR, 2016.   \nFlorian Schroff, Dmitry Kalenichenko, and James Philbin. Facenet: A unified embedding for face recognition and clustering. In CVPR, 2015.   \nK Simonyan and A Zisserman. Very deep convolutional networks for large-scale image recognition. In ICLR, 2015.   \nLukas Struppek, Dominik Hintersdorf, Antonio De Almeida Correia, Antonia Adler, and Kristian Kersting. Plug & play attacks: Towards robust and flexible model inversion attacks. In ICML, 2022.   \nLukas Struppek, Dominik Hintersdorf, and Kristian Kersting. Be careful what you smooth for: Label smoothing can be a privacy shield but also a catalyst for model inversion attacks. In ICLR, 2024.   \nChristian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. In ICLR, 2014.   \nKorawat Tanwisuth, Xinjie Fan, Huangjie Zheng, Shujian Zhang, Hao Zhang, Bo Chen, and Mingyuan Zhou. A prototype-oriented framework for unsupervised domain adaptation. NeurIPS, 2021.   \nKorawat Tanwisuth, Shujian Zhang, Huangjie Zheng, Pengcheng He, and Mingyuan Zhou. Pouf: Prompt-oriented unsupervised fine-tuning for large pre-trained models. In ICML, 2023.   \nKuan-Chieh Wang, Yan Fu, Ke Li, Ashish Khisti, Richard Zemel, and Alireza Makhzani. Variational model inversion attacks. In NeurIPS, 2021a.   \nQingzhong Wang, Pengfei Zhang, Haoyi Xiong, and Jian Zhao. Face.evolve: A high-performance face recognition library. arXiv preprint arXiv:2107.08621, 2021b.   \nXiaosong Wang, Yifan Peng, Le Lu, Zhiyong Lu, Mohammadhadi Bagheri, and Ronald M Summers. Chestx-ray8: Hospital-scale chest x-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases. In CVPR, 2017.   \nXi Yin, Ying Tai, Yuge Huang, and Xiaoming Liu. Fan: Feature adaptation network for surveillance face recognition and normalization. In ACCV, 2020.   \nXiaojian Yuan, Kejiang Chen, Jie Zhang, Weiming Zhang, Nenghai Yu, and Yang Zhang. Pseudo label-guided model inversion attack via conditional generative adversarial network. In AAAI, 2023.   \nHang Zhang, Chongruo Wu, Zhongyue Zhang, Yi Zhu, Haibin Lin, Zhi Zhang, Yue Sun, Tong He, Jonas Mueller, R Manmatha, et al. Resnest: Split-attention networks. In CVPR, 2022.   \nRichard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In CVPR, 2018.   \nYuheng Zhang, Ruoxi Jia, Hengzhi Pei, Wenxiao Wang, Bo Li, and Dawn Song. The secret revealer: Generative model-inversion attacks against deep neural networks. In CVPR, 2020.   \nHuangjie Zheng and Mingyuan Zhou. Exploiting chain rule and bayes\u2019 theorem to compare probability distributions. In NeurIPS, 2021. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "A Detailed Related Work and Preliminary 14 ", "page_idx": 12}, {"type": "text", "text": "A.1 Model Inversion Attacks 14   \nA.2 Distribution Discrepancy Measure 14 ", "page_idx": 12}, {"type": "text", "text": "B The Algorithmic Realizations of PPDG-MI 17 ", "page_idx": 12}, {"type": "text", "text": "C Experimental Details 17 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "C.1 Hard- and Software Details 17   \nC.2 Evaluation Models 17   \nC.3 Target Models . . 17   \nC.4 Attack Parameters . 18   \nC.5 Evaluation Metrics 18   \nC.6 Experimental Details for Fig. 1 18   \nC.7 Experimental Details for Toy Example From Sec. 3.3 19   \nC.8 Investigate Distribution Alignment on High-dimensional Image Data . . 20 ", "page_idx": 12}, {"type": "text", "text": "D Additional Experimental Results 21 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "D.1 Additional Main Results 21   \nD.2 Additional Ablation Study 24   \nD.3 Visualization of Reconstructed Images . 25 ", "page_idx": 12}, {"type": "text", "text": "E Discussion 26 ", "page_idx": 12}, {"type": "text", "text": "A Detailed Related Work and Preliminary ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A.1 Model Inversion Attacks ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Model inversion attacks (MIAs) were first introduced by Fredrikson et al. [2014] as a method to reconstruct private data from outputs in simple regression tasks with shallow models. This groundbreaking research highlighted the privacy risks of exposing sensitive data via model predictions. Based on this foundation, Zhang et al. [2020] extended MIAs to more complex DNNs. They developed a methodology that involved learning a distributional prior from a publicly available auxiliary dataset, allowing for effective MIAs in a constrained latent space of the generator. Since these foundational studies, MIAs have received increased attention, particularly in the realm of high-dimensional image data. Recent research divides MIAs into three types based on the attacker\u2019s access to the model: white-box, black-box, and label-only settings. Each category represents varying levels of accessibility and potential risk, which informs the ongoing development of defensive strategies in this area. ", "page_idx": 13}, {"type": "text", "text": "In the white-box setting, attackers have full access to the model, including its architecture and weights. The first white-box attack on DNNs, generative model inversion attack [Zhang et al., 2020], utilized generative adversarial networks (GANs) to learn a distributional prior and optimize within the latent space. Subsequently, the knowledge-enriched distributional model inversion (KEDMI) attack [Chen et al., 2021] employed a specialized GAN with an advanced discriminator that leverages information from the target model. Wang et al. [2021a] proposed variational model inversion (VMI), which uses a probabilistic approach with a variational objective to ensure both diversity and accuracy. This evolution culminated in the Plug & Play Attack (PPA) [Struppek et al., 2022], which enhances the recovery of images ranging from low to high resolution. Additionally, Nguyen et al. [2023] introduced logit maximization (LOM) loss as an alternative to the cross-entropy (CE) identity loss previously used in [Zhang et al., 2020, Chen et al., 2021, Wang et al., 2021a] and addressed issues of model overfitting with the model augmentation technique. Moreover, Yuan et al. [2023] advanced MIAs with the pseudo label-guided model inversion (PLG-MI), employing a conditional GAN (cGAN) and max-margin loss, and using pseudo-labels to decouple the search space for different classes. ", "page_idx": 13}, {"type": "text", "text": "In the black-box setting, attackers lack direct access to the model\u2019s internals but can still query the target model a predetermined number of times to observe outputs for specific inputs, using this data to infer sensitive information indirectly. An et al. [2022] adopted a genetic search algorithm as an alternative to gradient descent in the black-box setting. While Han et al. [2023] developed the reinforcement learning-based model inversion (RLB-MI) algorithm, formulating the latent vector optimization as a Markov decision process (MDP) using reinforcement learning. ", "page_idx": 13}, {"type": "text", "text": "The label-only setting poses the greatest challenge in model inversion attacks, where attackers only have access to the hard labels produced by the model, without any confidence scores or other related information. Kahla et al. [2022] tackled this issue using the boundary-repelling model inversion (BREP-MI) algorithm, which effectively utilizes the labels to simulate a gradient through max-margin loss. This method effectively navigates toward the concentrated areas of the target class in the data distribution by estimating model predictions across a conceptual sphere. Inspired by transfer learning (TL), Nguyen et al. [2024] proposed the label-only via knowledge transfer (LOKT) method for label-only model inversion, transferring knowledge from the target model to a target model-assisted ACGAN (T-ACGAN), effectively turning the label-only scenario into a white-box setting. ", "page_idx": 13}, {"type": "text", "text": "A.2 Distribution Discrepancy Measure ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "This section introduces two measures used to evaluate the closeness between distributions $\\mathrm{P}$ and $\\mathrm{Q}$ : maximum mean discrepancy (MMD) and conditional transport (CT). ", "page_idx": 13}, {"type": "text", "text": "Maximum Mean Discrepancy (MMD). Given two random variables $X\\sim\\mathrm{P},Y\\sim\\mathrm{Q},$ the MMD measure is defined as follows: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathrm{MMD}(\\mathrm{P},\\mathrm{Q};{\\mathcal{F}}):=\\operatorname*{sup}_{f\\in{\\mathcal{F}}}|\\mathbb{E}[f(X)]-\\mathbb{E}[f(Y)]|,\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\mathcal{F}$ is a class of functions [Gretton et al., 2012a]. This class is often restricted to a unit ball in a reproducing kernel Hilbert space (RKHS) to facilitate analytical solutions [Gretton et al., 2012a], ", "page_idx": 13}, {"type": "text", "text": "leading to the kernel-based MMD defined in the following, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathrm{MMD}(\\mathrm{P},\\mathrm{Q};{\\mathcal{H}}_{k}):=\\operatorname*{sup}_{\\substack{f\\in{\\mathcal{H}},\\|f\\|_{\\mathcal{H}_{k}}\\leq1}}|\\mathbb{E}[f(X)]-\\mathbb{E}[f(Y)]|,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $k$ is a bounded kernel chosen based on the specific properties of the RKHS $\\mathcal{H}_{k}$ . The use of RKHS allows for the effective computation of MMD, leveraging kernel functions to measure the distance between the distributions in a high-dimensional feature space. ", "page_idx": 14}, {"type": "text", "text": "Estimation of MMD. Given sample sets $S_{X}=\\{\\mathbf{x}_{i}\\}_{i=1}^{n}\\sim\\mathrm{P}$ and $S_{Y}\\,=\\,\\{{\\bf y}_{j}\\}_{j=1}^{m}\\sim\\mathrm{Q},$ MMD (Eq. (12)) can be estimated with the $U$ -statistic estimator, which is unbiased for $\\mathrm{MMD^{2}}$ [Gretton et al., 2012a]: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{\\displaystyle\\widehat{\\mathrm{MMD}}_{u}^{2}(S_{X},S_{Y};k)=\\displaystyle\\frac{1}{n(n-1)}\\sum_{i=1}^{n}\\sum_{j=1,j\\neq i}^{n}k(x_{i},x_{j})+\\displaystyle\\frac{1}{m(m-1)}\\sum_{i=1}^{m}\\sum_{j=1,j\\neq i}^{m}k(y_{i},y_{j})}\\\\ &{\\displaystyle-\\,\\frac{2}{m n}\\sum_{i=1}^{n}\\sum_{j=1}^{m}k(x_{i},y_{j}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\mathbf{x}_{i},\\mathbf{x}_{j}\\in S_{X}$ and $\\mathbf{y}_{i},\\mathbf{y}_{j}\\in S_{Y}$ . This estimator efficiently computes the MMD by aggregating kernel evaluations over pairs of samples from both distributions. ", "page_idx": 14}, {"type": "text", "text": "Conditional Transport (CT). The CT measure provides a complementary approach to MMD by focusing on the transport cost between distributions. It consists of two components: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathrm{CT}(\\mathrm{P},\\mathrm{Q}):=\\mathcal{L}_{X\\rightarrow Y}+\\mathcal{L}_{Y\\rightarrow X},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where ${\\mathcal{L}}_{X\\rightarrow Y}$ and ${\\mathcal{L}}_{Y\\rightarrow X}$ represent the transport costs for the forward and backward CT, respectively. They are defined as follows: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{X\\to Y}:=\\mathbb{E}_{\\mathbf{x}\\sim\\mathrm{P}(X)}\\mathbb{E}_{\\mathbf{y}\\sim\\Pi(\\cdot\\,|\\,\\mathbf{x})}[c(\\mathbf{x},\\mathbf{y})],}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{Y\\to X}:=\\mathbb{E}_{\\mathbf{y}\\sim\\mathrm{Q}(Y)}\\mathbb{E}_{\\mathbf{x}\\sim\\Pi(\\cdot\\,|\\,\\mathbf{y})}[c(\\mathbf{x},\\mathbf{y})],}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\begin{array}{r}{\\Pi(Y\\,|\\,X)\\,=\\,\\frac{e^{-d_{\\psi}(X,Y)}Q(Y)}{\\int e^{-d_{\\psi}(X,Y)}Q(Y)\\mathrm{d}Y}}\\end{array}$ e\u2212d\u03c8(X,Y )Q(Y )dY , and \u03a0(X | Y ) = $\\begin{array}{r}{\\Pi(X\\,|\\,Y)\\,=\\,\\frac{e^{-d_{\\psi}(X,Y)}P(X)}{\\int e^{-d_{\\psi}(X,Y)}P(X)\\mathrm{d}X}}\\end{array}$ represent the conditional distributions of $Y$ given $X$ and $X$ given $Y$ , respectively. Here, $d_{\\psi}(X,Y)$ is a function parameterized by $\\psi$ that measures the distance between $X$ and $Y$ , and $c(\\mathbf{x},\\mathbf{y})$ is a cost function that measures the distance between the points $\\mathbf{x}$ and $\\mathbf{y}$ . ", "page_idx": 14}, {"type": "text", "text": "Estimation of CT. Given sample sets $S_{X}=\\{\\mathbf{x}_{i}\\}_{i=1}^{n}\\sim\\mathrm{P}$ and $S_{Y}=\\{\\mathbf{y}_{j}\\}_{j=1}^{m}\\sim\\mathrm{Q}$ , the CT measure can be approximated as follows: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\mathrm{CT}(S_{X},S_{Y})=\\mathcal{L}_{X\\to\\hat{Y}}+\\mathcal{L}_{Y\\to\\hat{X}}}}\\\\ &{=\\mathbb{E}_{\\mathbb{y}_{1:m}}\\frac{i\\omega}{\\kappa^{2}}\\mathrm{Q}(\\boldsymbol{Y})^{\\mathbb{E}_{\\mathbf{x}\\sim\\mathrm{P}(X)}}\\left[\\displaystyle\\sum_{j=1}^{m}c(\\mathbf{x},\\mathbf{y}_{j})\\hat{\\Pi}(\\mathbf{y}_{j}\\mid\\mathbf{x})\\right]}\\\\ &{+\\mathbb{E}_{\\mathbf{x}_{1:m}}\\frac{i\\omega}{\\kappa^{2}}\\mathrm{P}(\\boldsymbol{X})^{\\mathbb{E}_{\\mathbf{y}\\sim\\mathrm{Q}(Y)}}\\left[\\displaystyle\\sum_{i=1}^{n}c(\\mathbf{x}_{i},\\mathbf{y})\\hat{\\Pi}(\\mathbf{x}_{i}\\mid\\mathbf{y})\\right]}\\\\ &{=\\displaystyle\\sum_{i=1}^{n}\\sum_{j=1}^{m}c(\\mathbf{x}_{i},\\mathbf{y}_{j})\\left(\\frac{e^{-d_{\\psi}(\\mathbf{x}_{i},\\mathbf{y}_{j})}}{\\sum_{j^{\\prime}=1}^{m}e^{-d_{\\psi}(\\mathbf{x}_{i},\\mathbf{y}_{j^{\\prime}})}}+\\frac{e^{-d_{\\psi}(\\mathbf{x}_{i},\\mathbf{y}_{j})}}{\\sum_{i^{\\prime}=1}^{n}e^{-d_{\\psi}(\\mathbf{x}_{i^{\\prime}},\\mathbf{y}_{j})}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The CT measure evaluates the cost of transporting samples from one distribution to another, providing a detailed assessment of how closely the distributions align. ", "page_idx": 14}, {"type": "text", "text": "Algorithm 1 Pseudo-Private Data Guided Model Inversion with Vanilla Tuning.   \nInput: Target model M, pre-trained generator $\\mathrm{G}(\\cdot;\\pmb\\theta)$ , pre-trained discriminator $\\operatorname{D}(\\cdot;\\phi)$ , public auxiliary dataset $\\mathcal{D}_{\\mathrm{public}}$ , number of fine-tuning rounds $R$ , and the set of identities to be reconstructed $C$ .   \n1: $\\theta_{o l d}\\leftarrow\\theta$ ;   \n2: reconstructed_samples $\\mathbf{\\mu}=\\mathbf{\\mu}\\left[\\mathbf{\\Lambda}\\right]$ ;   \n3: for each target identity $y$ in $C$ do   \n4: $\\pmb{\\theta}\\leftarrow\\pmb{\\theta}_{o l d}$ ;   \n5: for round $=1,\\ldots,R$ do   \n6: # Step-1. Model inversion on target model M   \n7: Initialize latent codes: $\\mathbf{Z}=\\{\\mathbf{z}_{i}\\mid\\mathbf{z}_{i}\\in\\mathcal{Z},i=1,\\ldots,N\\}$ ;   \n8: Obtain optimized latent codes $\\hat{\\textbf{Z}}$ using Eq. (1);   \n9: # Step-2. Select high-quality pseudo-private samples   \n10: Select pseudo-private samples with top- $K$ stable prediction scores: $\\begin{array}{r l}{\\mathcal{D}_{\\mathrm{private}}^{\\mathrm{s}^{\\prime}}}&{{}=}\\end{array}$ $\\mathrm{TopK}\\{\\mathbb{E}[\\mathbb{P}_{\\mathrm{M}}(y\\vert\\,T(\\hat{\\mathbf{x}}))]\\ \\vert\\ \\hat{\\mathbf{x}}\\in\\mathcal{D}_{\\mathrm{private}}^{\\mathrm{s}}\\};$ ;   \n11: # Step-3. Enhance density around $\\mathcal{D}_{\\mathrm{pr}_{i}}^{\\mathrm{s}^{\\prime}}$ vate   \nFine-tune $\\mathrm{G}(\\cdot;\\theta)$ and $\\operatorname{D}(\\cdot;\\phi)$ to enhance density around $\\mathcal{D}_{\\mathrm{private}}^{\\mathrm{s}^{\\prime}}$ by directly fine-tuning them with the original GAN training objective on $D_{\\mathrm{public}}\\cup D_{\\mathrm{private}}^{\\mathrm{s}^{\\prime}}$ ;   \n12: end for   \n13: reconstructed_samples $+=\\ D_{\\mathtt{p r i v a t e}}^{\\mathtt{s}}$ ;   \n14: end for   \n15: Output: reconstructed_samples. ", "page_idx": 15}, {"type": "text", "text": "Algorithm 2 Pseudo-Private Data Guided Model Inversion with Point-wise or Batch-wise Tuning.   \nInput: Target model M, pre-trained generator $\\mathrm{G}(\\cdot;\\theta)$ , pre-trained discriminator D, number of fine  \ntuning rounds $R$ , identity set to be reconstructed $C$ ; point-wise tuning flag PW_Flag, distribution   \ndiscrepancy measure $\\delta$ .   \n1: $\\theta_{o l d}\\leftarrow\\theta$ ;   \n2: reconstructed_samples $\\mathbf{\\mu}=\\mathbf{\\mu}\\left[\\mathbf{\\Lambda}\\right]$ ;   \n3: for each target identity $y$ in $C$ do   \n4: $\\pmb{\\theta}\\leftarrow\\pmb{\\theta}_{o l d}$ ;   \n5: for round $=1,\\ldots,R$ do   \n6: # Step-1. Model inversion on target model M   \n7: Initialize latent codes: $\\mathbf{Z}=\\{\\mathbf{z}_{i}\\mid\\mathbf{z}_{i}\\in\\mathcal{Z},i=1,\\ldots,N\\}$ ;   \n8: Obtain optimized latent codes $\\hat{\\textbf{Z}}$ using Eq. (1);   \n9: Generate pseudo-private dataset: Dsprivate = {\u02c6x = G(z\u02c6) | z\u02c6 \u2208\u02c6Z};   \n10: # Step-2. Select high-quality pseudo-private samples   \n11: Select pseudo-private samples with top- $.K$ stable prediction scores: $\\begin{array}{r l}{\\mathcal{D}_{\\mathrm{private}}^{\\mathrm{s}^{\\prime}}}&{{}=}\\end{array}$   \n$\\Gamma o p\\mathbb{K}\\{\\mathbb{E}[\\mathbb{P}_{\\mathrm{M}}(y\\vert\\,T({\\hat{\\mathbf{x}}}))]\\ |\\ {\\hat{\\mathbf{x}}}\\in{\\mathcal{D}}_{\\mathrm{private}}^{\\mathrm{s}}\\};$   \n12: # Step-3. Enhance density around $\\mathcal{D}_{\\mathrm{private}}^{\\mathrm{s}^{\\prime}}$   \n13: if PW_Flag then   \n14: # Point-wise tuning   \n15: Locate high-density neighbors $\\mathbf{Z}^{\\mathrm{p}}$ by optimizing Eq. (6);   \n16: Fine-tune $\\mathrm{G}(\\cdot;\\theta)$ to enhance density around $\\ensuremath{\\mathcal{D}_{\\mathrm{private}}^{\\mathrm{s}^{\\prime}}}$ by optimizing Eq. (7);   \n17: else   \n18: # Batch-wise tuning   \n19: if $\\delta$ is MMD then   \n20: Locate high-density neighbors $\\mathbf{Z}^{\\mathrm{p}}$ by optimizing Eq. (9a);   \n21: Fine-tune $\\mathrm{G}(\\cdot;\\theta)$ to enhance density around $\\mathcal{D}_{\\mathrm{private}}^{\\mathrm{s}^{\\prime}}$ by optimizing Eq. (9b);   \n22: else if $\\delta$ is CT then   \n23: Locate high-density neighbors $\\mathbf{Z}^{\\mathrm{p}}$ by optimizing Eq. (10a);   \n24: Fine-tune $\\mathrm{G}(\\cdot;\\theta)$ to enhance density around Dsprivate by optimizing Eq. (10b);   \n25: end if   \n26: end if   \n27: end for   \n28: reconstructed_samples $+=~\\mathcal{D}_{\\mathtt{p r}}^{\\mathtt{s}}$ rivate;   \n29: end for   \n16   \n30: Output: reconstructed_samples. ", "page_idx": 15}, {"type": "text", "text": "B The Algorithmic Realizations of PPDG-MI ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "This section presents the detailed algorithmic realization of the pseudo-private data guided model inversion (PPDG-MI) method. We describe two variants of the PPDG-MI method, each tailored for different MI scenarios. The first variant utilizes vanilla tuning (cf. Alg. 1), applicable for lowresolution MIAs where the adversary trains a GAN from scratch. The second variant employs nuanced point-wise or batch-wise tuning (cf. Alg. 2), suitable for high-resolution MIAs (i.e., PPA) where pre-trained generators are provided without access to the original training details. These methods are designed to enhance the density of pseudo-private samples under the prior distribution, thereby increasing the probability of sampling from the private data distribution. ", "page_idx": 16}, {"type": "text", "text": "C Experimental Details ", "text_level": 1, "page_idx": 16}, {"type": "table", "img_path": "pyqPUf36D2/tmp/f09e3a16a3ca251ff600dd3817bb3b3a7f3a0c76dc3c2847cc088887c9cb5de9.jpg", "table_caption": ["Table 4: A summary of experimental setups. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "C.1 Hard- and Software Details ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In our experiments with Plug & Play Attacks (PPA), we conducted all of them on Oracle Linux Server 8.9 using NVIDIA Ampere A100-80G GPUs. The hardware operated under CUDA 11.7, Python 3.9.18, and PyTorch 1.13.1. For MIAs targeting low-resolution facial recognition tasks, we executed these experiments on Ubuntu 20.04.4 LTS, equipped with NVIDIA GeForce RTX 3090 GPUs. This setup utilized CUDA 11.6, Python 3.7.12, and PyTorch 1.13.1. ", "page_idx": 16}, {"type": "text", "text": "C.2 Evaluation Models ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "For experiments based on PPA, we train Inception-v3 evaluation models, following the code and guidelines available at the repository https://github.com/LukasStruppek/ Plug-and-Play-Attacks. For training details, please refer to [Struppek et al., 2022]. These models achieve test accuracies of $96.53\\%$ on FaceScrub and $94.87\\%$ on CelebA. In addition, We use the pretrained FaceNet [Schroff et al., 2015] from https://github.com/timesler/facenet-pytorch to compute the K-nearest neighbors distance, providing a measure of similarity between training samples and the reconstructed samples on the facial recognition tasks. ", "page_idx": 16}, {"type": "text", "text": "In experiments involving classifiers trained on the $64\\,\\times\\,64$ resolution CelebA dataset, we utilize an evaluation model available for download at repository https://github.com/ sutd-visual-computing-group/Re-thinking_MI. This model is built upon the face.evoLVe model [Wang et al., 2021b], incorporating a modified ResNet50 backbone, and achieves a stated test accuracy of $95.88\\%$ . For training details, please refer to Zhang et al. [2020]. ", "page_idx": 16}, {"type": "text", "text": "C.3 Target Models ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "For training target models on $224\\times224$ resolution CelebA and FaceScrub images, we adapt the training scripts and hyperparameters provided in the corresponding code repository and described in [Struppek et al., 2022]. The only training parameter we modify is the smoothing factor of the label smoothing loss. All models are trained for 100 epochs using the Adam optimizer [Kingma and Ba, 2015], with an initial learning rate of $10^{-3}$ and $\\beta=(0.9,0.999)$ , and a weight decay of $10^{-3}$ . We reduce the learning rate by a factor of 0.1 after 75 and 90 epochs. The batch size is set to 128. All data samples are normalized with $\\mu=\\sigma=0.5$ and resized to $224\\times224$ . The training samples are then augmented by random cropping with a scale of [0.85, 1.0] and a fixed ratio of 1.0. Crops are resized back to $224\\times224$ , and samples are horizontally flipped in $50\\%$ of the cases. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "For training target models on $64\\times64$ resolution CelebA images, we use the training script provided at https://github.com/sutd-visual-computing-group/Re-thinking_MI. These models are trained for 50 epochs using the SGD optimizer with an initial learning rate of $10^{-2}$ , a momentum term of 0.9, and a weight decay of $10^{-4}$ . The batch size is set to 64. The learning rate decay schedule varies depending on the model; please refer to the training script for details. ", "page_idx": 17}, {"type": "text", "text": "C.4 Attack Parameters ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "PPA consists of three stages: pre-attack latent code selection, MIA optimization, and final results selection. During pre-attack latent code selection, we choose 100 candidates for each target identity from a search space of 500 latent codes for both CelebA and FaceScrub. In the MIA optimization phase, we maintain an equal number of queries to the target model M to ensure a fair comparison for both the baseline attack and PPDG-MI. Thus, samples are optimized for 70 steps for both CelebA and FaceScrub in the baseline attack and 35 steps for each round of MIA in PPDG-MI. The final results selection stage is omitted. We target the first 100 identities and generate 100 samples per identity. Nonetheless, after some consideration, we believe that maintaining an equal number of queries to the target model M in each round of MIA for both PPDG-MI and the baseline attack may be a more reasonable approach to evaluate the performance of the proposed method. ", "page_idx": 17}, {"type": "text", "text": "For low-resolution attacks, we generate 1, 000 samples per identity for CelebA and 2, 000 samples per identity for FFHQ, as training GANs typically require more samples. Specifically, for low-resolution white-box attacks, we target the first 100 identities for CelebA and the first 50 identities for FFHQ. Samples in GMI and LOM (GMI) are optimized for 2, 400 steps on the VGG16 target model and 1, 200 steps on face.evoLVe, while KEDMI and LOM (KEDMI) optimize samples for 1, 200 steps on both target models. For PLG-MI, we target 50 identities of the CelebA dataset, generating 200 samples per identity. Samples are optimized for 80 iterations on VGG16 and face.evoLVe target models. For black-box and label-only attacks, we limit our selection to the first 10 identities due to the extremely time-consuming nature of point-wise optimization in these settings. In the black-box attack (i.e., RLB-MI), samples undergo 10, 000 optimization steps in the baseline attack and in each round of PPDG-MI. For the label-only attack (BREP-MI), the maximum number of optimization steps is set to $1,000$ for both the baseline and each round of PPDG-MI. ", "page_idx": 17}, {"type": "text", "text": "Due to the significant time required for MIAs, we perform a single attack against each target model.   \nTo reduce randomness, we generate at least 100 samples for each target class across various setups. ", "page_idx": 17}, {"type": "text", "text": "C.5 Evaluation Metrics ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Attack Accuracy (Attack Acc). Following Zhang et al. [2020], we use an evaluation model (typically more powerful than the target model) trained on the target model\u2019s training data to identify reconstructed images (cf. Tab. 4). Intuitively, it can be viewed as a proxy for a human evaluator. Attack accuracy is calculated as the proportion of predictions matching the target identity; top-1 $(\\operatorname{Acc}\\@leftrightarrow1)$ and top-5 $(\\operatorname{Acc}\\!\\odot\\!5)$ attack accuracy is adopted. ", "page_idx": 17}, {"type": "text", "text": "K-Nearest Neighbors Distance (KNN Dist). KNN Dist represents the $l_{2}$ distance between reconstructed images and the nearest samples from the target model\u2019s training data in the embedding space, indicating visual similarity between faces. For PPA [Struppek et al., 2022], we use the penultimate layer of pre-trained FaceNet [Schroff et al., 2015], whereas for other low-resolution MIAs, we adopt the penultimate layer of the evaluation model. Lower distances indicate a closer resemblance between the reconstructed samples and the training data. ", "page_idx": 17}, {"type": "text", "text": "C.6 Experimental Details for Fig. 1 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We present the experimental details for generating Fig. 1. In the motivation-driven experiments, we evaluate the distribution discrepancy between commonly adopted private training datasets and public auxiliary datasets, and investigate the impact of distribution discrepancy on attack performance of MIAs using various public auxiliary datasets and target models. ", "page_idx": 17}, {"type": "image", "img_path": "pyqPUf36D2/tmp/f21f82a87749fe9c0f3e16ab54d4e71f23190a2d3e82f94e49aa72a618a4c4e4.jpg", "img_caption": ["Figure 6: Impact of distribution discrepancies on MIAs across various settings. The attack performance of MIAs is analyzed under four distinct combinations of public auxiliary datasets $\\mathcal{D}_{\\mathrm{public}}$ and target models M, with the same private training dataset $\\ensuremath{\\mathcal{D}_{\\mathrm{private}}}=$ CelebA: (a) $\\mathcal{D}_{\\mathrm{public}}=\\mathrm{FFHQ}$ and $\\mathrm{M}=$ face.evoLVe, (b) $\\mathcal{D}_{\\mathrm{public}}=\\mathrm{FFHQ}$ and $\\mathrm{M}=\\mathrm{IR}152$ , (c) $\\mathcal{D}_{\\mathrm{public}}=$ FaceScrub and $\\mathrm{M}=$ face.evoLVe, and (d) $\\mathcal{D}_{\\mathrm{public}}=$ FaceScrub and $\\mathrm{M}=\\mathrm{IR}152$ . The attack performance consistently diminishes as the discrepancy between the $\\mathcal{D}_{\\mathrm{private}}$ (CelebA) and Dpublic increases. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "In Fig. 1(a), we employ kernel-based two-sample tests to evaluate the distributional discrepancy between public auxiliary datasets (FFHQ and FaceScrub) and the private dataset (CelebA). A $\\mathfrak{p}$ -value of 0 signifies no statistical basis to reject the null hypothesis, indicating no discernible distribution difference between the datasets. Conversely, a p-value of 1 implies definitive evidence to reject the null hypothesis. We utilize a Gaussian Kernel-based test to calculate ${\\bf p}$ -values at the feature level, extracting a 512-dimensional feature vector from each image using the face.evoLVe feature extractor. We analyze subsets of N feature vectors, with $_\\mathrm{N}$ varying from 1, 000 to $10,000$ . Each subset is sampled 20 times from both datasets, and the results represent the average of these samples. ", "page_idx": 18}, {"type": "text", "text": "In Figs. 1(b) and 1(c), we investigate the impact of distribution discrepancy on the attack performance of MIAs (LOM and KEDMI) using VGG16 as the target model and face.evoLVe as the evaluation model. The private dataset is CelebA, and the public auxiliary datasets are FFHQ and FaceScrub. We construct the proxy public auxiliary dataset by incrementally integrating the private data into the public auxiliary dataset, increasing the private data ratio by $20\\%$ at each interval, from $20\\%$ to $80\\%$ To manage computational demands, we measure MMD across batches of 250 images each, using Gaussian kernels with a bandwidth of 1024. The final MMD value is the average result across all batches. Additional results involving IR152 and face.evoLVe target models are shown in Fig. 6. ", "page_idx": 18}, {"type": "text", "text": "C.7 Experimental Details for Toy Example From Sec. 3.3 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "The target model used in the toy example is a simple 3-layer multilayer perceptron (MLP). This MLP comprises two hidden layers, with the first containing 100 neurons and the second 200 neurons. Each hidden layer is followed by a rectified linear unit (ReLU) activation function to introduce non-linearity. The model is trained on a dataset sampled from a 3-class class-conditional Gaussian distribution, employing standard cross-entropy for loss calculation. Training is performed for 6, 000 epochs using standard stochastic gradient descent (SGD) with an initial learning rate of 0.5, enhanced by a linear warm-up schedule to increase the learning rate gradually. ", "page_idx": 18}, {"type": "text", "text": "The public auxiliary dataset, distinct from the training dataset to prevent distribution overlap, is generated from a separate Gaussian distribution. The distributional prior is learned using a GAN, where both the generator and discriminator are structured as MLPs with three hidden layers of 400 neurons each. All layers in both models are followed by ReLU activations. The GAN is trained under the Wasserstein GAN with gradient penalty (WGAN-GP) [Gulrajani et al., 2017] framework to ensure a stable training process and reliable generation of new data samples. ", "page_idx": 18}, {"type": "text", "text": "We employ the learned prior to guide the attack targeting Class 1. Initially, we randomly generate 1, 000 initial points and iteratively update them to maximize the model\u2019s prediction score for Class 1 through minimizing an identity loss (i.e., cross-entropy loss), following the approach proposed by Zhang et al. [2020]. We optimize this process using SGD with a learning rate of 0.1. ", "page_idx": 18}, {"type": "text", "text": "To ensure a fair comparison between the baseline and PPDG-MI, we ensure that both attacks make the same number of queries to the model. Specifically, we query the model 1, 000 times in the baseline attack. For PPDG-MI, we first conduct one round of model inversion, making 500 queries to the target model to generate 1, 000 pseudo-private data points. Subsequently, we enhance the density of the pseudo-private data under the prior distribution by directly aligning the distribution between the ", "page_idx": 18}, {"type": "image", "img_path": "pyqPUf36D2/tmp/e9d1e570b6e5e7d41b1df0b4653af8ed51f8d02b5730f2a148425b9596ce8b0f.jpg", "img_caption": [], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "pyqPUf36D2/tmp/4e1a24f6ea6c0d5eceb9ea01c2ebca45cab2d9a2ec381a1ee2d2ee4c9a080f5c.jpg", "img_caption": ["(d) (Round 2) Model Inversion with Adjusted Prior "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Figure 7: Illustration of the rationale behind PPDG-MI using a simple 2D example (larger version). Training samples from Class 0-2 are represented by purple circles, blue triangles, and green squares, respectively, while public auxiliary data are depicted as yellow diamonds. MIAs aim to recover training samples from Class 1. Reconstructed samples by MIAs are shown as red circles. (a) Attack results of the baseline attack with a fixed prior. (b) Pseudo-private data generation. (c) Enhancing the density of pseudo-private data under prior distribution. (d) The final attack results of PPDG-MI with the tuned prior, where all the recovered points converge to the centroid of the class distribution, indicating the most representative features are revealed. ", "page_idx": 19}, {"type": "text", "text": "prior distribution and the empirical pseudo-private data distribution by minimizing the distribution discrepancy between them. Then, we apply the fine-tuned prior to guide the second round of model inversion, making 500 queries to the model. ", "page_idx": 19}, {"type": "text", "text": "The baseline attack results are shown in Fig. 7(a) and the final attack results of PPDG-MI are shown in Fig. 7(d). It is evident that, in comparison to the baseline where only a small fraction of the reconstructed samples fall within the high-density regions of the training data distribution, all reconstructed samples from PPDG-MI are located in these high-density regions. Quantitatively, the attack performance is evaluated by measuring two metrics: the average distance between the reconstructed samples and the mean of the target class distribution, and the proportion of reconstructed samples that lie in three standard deviations $(3\\sigma)$ of the mean. In comparison, the baseline achieves an average distance of 0.34 and attack accuracy of $22.60\\%$ , and PPDG-MI achieves an average distance of 0.04 and attack accuracy of $100.00\\%$ . ", "page_idx": 19}, {"type": "text", "text": "C.8 Investigate Distribution Alignment on High-dimensional Image Data ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In this experiment, we aim to extend the density enhancement strategy from the toy experiment to high-dimensional image data. We use the pre-trained StyleGAN on FFHQ as the distributional prior, with ResNet-18 trained on the CelebA dataset as the target model. We generate the pseudo-private dataset Dprivate by ", "page_idx": 19}, {"type": "table", "img_path": "pyqPUf36D2/tmp/f6b511107aba86f001435fa305e4b78c65913f1e7890183c5f1b57cb60666966.jpg", "table_caption": ["Table 5: Enhance density of pseudo-private data under the prior distribution by distribution alignment. "], "table_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "pyqPUf36D2/tmp/341d25efa4366bc8fdd1d484bf7fc13f09abca985b312b6d43b557bfc7301355.jpg", "table_caption": ["Table 6: Comparison of MI performance with representative white-box MIAs in the low-resolution setting. The target model M is face.evoLVe trained on $\\ensuremath{\\mathcal{D}_{\\mathrm{private}}}=$ CelebA. GANs are trained on $\\mathcal{D}_{\\mathrm{public}}$ $=$ CelebA or FFHQ. The symbol $\\downarrow$ (or $\\uparrow$ ) indicates smaller (or larger) values are preferred, and the green numbers represent the attack performance improvement. The running time ratio (Ratio) between prior fine-tuning and MI reflects the relative overhead of fine-tuning. "], "table_footnote": [], "page_idx": 20}, {"type": "table", "img_path": "pyqPUf36D2/tmp/35213f64d1fadc7df8ea1ca0b6dd6b68d2dbbeeb4453f72c167d3e5c1d42b373.jpg", "table_caption": ["Table 7: Comparison of MI performance with PLG-MI in the low-resolution setting. Target model M $=\\mathrm{VGG}16$ or face.evoLVe trained on $\\mathcal{D}_{\\mathrm{private}}=\\mathrm{CelebA}$ . GANs are trained on $\\mathcal{D}_{\\mathrm{public}}=$ FaceScrub. "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "conducting MIA on target identities 1-100, recovering 100 samples per identity, resulting in a total of 10,000 samples in Dsprivate. We then increase the density around Dspri vate under the prior distribution $\\mathrm{P}({\\mathcal X}_{\\mathrm{prior}})$ by fine-tuning $\\mathrm{G}(\\cdot;\\theta)$ to align with the empirical distribution of Dsprivate, using the CT measure. The results are shown in Tab. 5, where we observe a dramatic decrease in MI performance after fine-tuning the generator. This indicates that direct distribution alignment is less effective for higher-dimensional image data, as it disrupts the generator\u2019s manifold. Therefore, we need to employ a nuanced tuning strategy with smaller perturbations to the image manifold. ", "page_idx": 20}, {"type": "text", "text": "D Additional Experimental Results ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "D.1 Additional Main Results ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Comparison with white-box MIAs in the low-resolution setting. In this experiment, we utilize the face.evoLVe as the target model. The results are presented in Tab. 6, where PPDG-vanilla consistently outperforms various baseline white-box attacks, offering notable improvements in both attack accuracy and KNN distance metrics across two public auxiliary datasets, CelebA and FFHQ. For instance, in the LOM (GMI) setup, adding PPDG-vanilla results in an increase in top-1 attack accuracy from $68.09\\%$ to $71.39\\%$ for CelebA, and top-5 attack accuracy from $87.31\\%$ to $88.12\\%$ . Additionally, there is a reduction in KNN distance from 1417.23 to 1385.10 for CelebA. Similarly, significant improvements are observed for FFHQ, where attack accuracy increases and KNN distances decrease, indicating enhanced data density around pseudo-private samples. However, there is a failure case involving the setup where the target model is face.evoLVe trained on CelebA, with the public dataset as FFHQ. We attribute this failure to multiple factors, which are analyzed in Appx. E. Generally, these results suggest that even with substantial distribution shifts between the private dataset CelebA and the public dataset FFHQ, our principled vanilla fine-tuning strategy, which retains the original GAN training objectives, can effectively improve MIA performance. ", "page_idx": 20}, {"type": "text", "text": "Comparison with PLG-MI in the low-resolution setting. In this setting, we use the state-of-the-art white-box attack PLG-MI [Yuan et al., 2023] as the baseline for comparison. PLG-MI leverages the target model to generate pseudo-labels for public auxiliary datasets, enabling the training of a cGAN (conditional GAN) on this labeled data. This further decouples the search space across different classes. However, the poor visual quality of samples generated by PLG-MI results in failure cases (refer to Appx. E), limiting the effectiveness of PPDG-MI. Therefore, we manually select 50 identities ", "page_idx": 20}, {"type": "text", "text": "Table 8: Comparison of MI performance with RLB-MI and BREP-MI in the low-resolution setting. The target model $\\mathrm{M}$ is VGG-16 trained on $\\ensuremath{\\mathcal{D}_{\\mathrm{private}}}=$ CelebA, GANs are trained on $\\mathcal{D}_{\\mathrm{public}}=\\mathrm{CelebA}$ . The symbol $\\downarrow(\\mathrm{or}\\uparrow)$ indicates smaller (or larger) values are preferred, and the green numbers represent the attack performance improvement. The running time ratio (Ratio) between prior fine-tuning and MI reflects the relative overhead of fine-tuning. ", "page_idx": 21}, {"type": "table", "img_path": "pyqPUf36D2/tmp/f2cf8fbbb9b38c57450e3d430dd4c9534b3fddbf948d8e03704d4eaffc57fa80.jpg", "table_caption": [], "table_footnote": [], "page_idx": 21}, {"type": "table", "img_path": "pyqPUf36D2/tmp/6b0a825328a530da9a0d041ad999be9e5442a5adb73e2d5a5b71dda0033af0bd.jpg", "table_caption": ["Table 9: Comparison of MI performance against state-of-the-art defense methods in the low-resolution setting. The target model M is VGG16 trained on $\\mathcal{D}_{\\mathrm{private}}=\\mathrm{CelebA}$ , GANs are trained on $\\mathcal{D}_{\\mathrm{public}}=$ CelebA. Bold numbers indicate superior results. "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "with high-quality samples from 100 identities and present the quantitative results in Tab. 7. Integrating PPDG-vanilla significantly improves MI performance. This improvement can be attributed to the robust selection strategy, which identifies high-quality samples from all pseudo-private data, along with fine-tuning using the original GAN training objectives to enhance density around high-quality pseudo-private samples. These results indicate that even with a different GAN prior (i.e., cGAN), PPDG-MI shows effectiveness with strong compatibility and outperforms SOTA white-box attack. ", "page_idx": 21}, {"type": "text", "text": "Comparison with Black-box MIAs in the low-resolution setting. In this setting, we use the SOTA RLB-MI [Han et al., 2023] as the baseline for comparison, the results are shown in the upper part of Tab. 8. Han et al. [2023] formulate the latent space search problem as a Markov decision process and solve it using reinforcement learning, which requires tens of thousands of iterations for optimization, leading to inefficiency and a low probability of sampling representative samples. Integrating PPDGvanilla significantly improves MI performance. This improvement can be attributed to the fine-tuning of the GAN, which effectively enhances the density around the pseudo-private samples, thereby increasing the probability of sampling representative samples. Our experiments demonstrate that PPDG-vanilla boosts the top-1 attack accuracy from $38.50\\%$ to $61.60\\%$ and the top-5 attack accuracy from $65.10\\%$ to $85.30\\%$ in the black-box setting, with a substantial reduction in KNN distance from 1431.93 to 1337.09. Additionally, the running time ratio indicates a minimal overhead of just 0.02. These results highlight the effectiveness of our method in improving MI performance in the black-box setting with minimal additional computational cost. ", "page_idx": 21}, {"type": "text", "text": "Comparison with Label-only MIAs in the low-resolution setting. In this setting, we use the state-of-the-art BREP-MI [Kahla et al., 2022] as the baseline for comparison. The quantitative results are presented in the upper part of Tab. 8. Kahla et al. [2022] introduces a boundary-repelling algorithm to search for representative samples. This algorithm estimates the direction towards the target class\u2019s centroid using the predicted labels of the target model over a sphere. Typically, under a radius threshold where the gradient estimator still works reliably, a larger sphere radius indicates a higher-likelihood region around the sphere\u2019s centroid. A qualitative illustration of the progression of the reconstructed images towards the actual training images is depicted in Fig. 8. The upper part of Fig. 8 shows the results of BREP-MI, while the lower part shows the results after integrating BREPMI with PPDG-vanilla. It is evident that with PPDG-vanilla, BREP-MI can find more representative samples at a smaller radius. This demonstrates that our method effectively increases the density of these regions, leading to faster convergence of the search process. ", "page_idx": 21}, {"type": "image", "img_path": "pyqPUf36D2/tmp/a8220b893b1deeda1a98bd5c3e1df7415ffaead665db57c9a02734841544c154.jpg", "img_caption": ["Figure 8: A comparison of the progression of BREP-MI and BREP-MI integrated with PPDG-vanilla from the initial random point to the algorithm\u2019s termination, indicating that the latter achieves faster convergence in the search process. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "Attacks against SOTA model inversion defense methods. In the low-resolution setting, we evaluate SOTA white-box attacks LOM (GMI), KEDMI, and LOM (KEDMI) against SOTA model inversion defense methods, such as BiDO-HSIC [Peng et al., 2022] and NegLS [Struppek et al., 2024]. The results, summarized in Tab. 9 demonstrate that our PPDG-MI consistently outperforms baseline against BiDO-HSIC and NegLS. For instance, when using NegLS as the defense, PPDG-vanilla significantly enhances both the top-1 attack accuracy and the KNN distance metrics. For the LOM (GMI) attack, PPDG-vanilla improves the $\\operatorname{Acc}\\@1$ from $25.40\\%$ to $45.44\\%$ , and for KEDMI, it increases it from $27.15\\%$ to $30.82\\%$ . Additionally, PPDG-vanilla shows improvements compared with LOM (KEDMI) by enhancing $\\operatorname{Acc}\\@1$ from $64.73\\%$ to $68.94\\%$ , and decreasing KNN Distance from 1320.38 to 1308.09. The enhanced MI performance on the target model trained with SOTA defense methods further underscores the effectiveness of PPDG-MI. ", "page_idx": 22}, {"type": "text", "text": "Table 10: Ablation study on the number $K$ of high-quality samples selected for fine-tuning. \"Time\" (seconds per identity) denotes the time required for fine-tuning a single identity . ", "page_idx": 23}, {"type": "table", "img_path": "pyqPUf36D2/tmp/d6e04118a3b62ef85b4f2ab150c019f71265cebe7ad9d619ee1285aa5bc09343.jpg", "table_caption": [], "table_footnote": [], "page_idx": 23}, {"type": "table", "img_path": "pyqPUf36D2/tmp/c69e791b6a9717207cb8b553b133ab97dc43cbd6cc9f5b0f67d2a93cb5f7f888.jpg", "table_caption": ["Table 11: Ablation study on fine-tuning different layers of the StyleGAN synthesis network. \"Time\" (seconds per identity) denotes the time required for fine-tuning a single identity. "], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "D.2 Additional Ablation Study ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "D.2.1 MIAs in the High-resolution setting ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Number $K$ of high-quality pseudo-private samples. The choice of the number $K$ of high-quality pseudo-private samples affects the tuning intensity of the generator G, the informativeness of the empirical local distribution, and the computational cost. In Tab. 10, we illustrate the MI performance for different choices of $K$ . As $K$ increases, MI performance initially improves, reaching an optimal point before it starts to decline. This trend can be attributed to two main factors. First, A higher number of samples provides more detailed information about the local data distribution, allowing the generator $\\mathrm{G}$ to better capture the underlying characteristics of the data. This leads to an initial improvement in MI performance. Second, As $K$ becomes larger, the generator requires more extensive tuning to accommodate the additional samples. This can result in significant changes to the manifold, potentially disrupting the learned data structure and decreasing MI performance. Additionally, larger $K$ values substantially increase computational cost. Thus, considering all these factors, we highlight the $K$ values chosen for various fine-tuning methods in Tab. 10. ", "page_idx": 23}, {"type": "text", "text": "Fine-tune different layers. Given that the synthesis network in the StyleGAN generator consists of 18 layers\u2014two for each resolution $4^{2}-10\\dot{2}4^{2}$ \u2014with earlier layers controlling higher-level features (e.g., general hairstyle, face shape), and later layers controlling more fine-grained features (e.g., finer hairstyle details), we investigate the effect of tuning subsets of these layers. In Tab. 11, we present the results of incrementally adding layers for fine-tuning. Our study reveals that tuning layers with spatial resolutions from $4^{2}-128^{2}$ achieves comparable results to tuning all layers, i.e., spatial resolutions from $4^{2}-1024^{2}$ . This finding suggests that successful MIAs rely more on inferences about high-level features (e.g., face shape) rather than fine-grained details, aligning with the main goals of MIAs. ", "page_idx": 23}, {"type": "text", "text": "D.2.2 MIAs in the Low-resolution Setting ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In this section, we present all ablation studies on MIA in the low-resolution setting to further explore PPDG-MI. The target model, VGG16, and the evaluation model, face.evoLVe, are both trained on CelebA private dataset. GANs are also trained from scratch on CelebA public dataset. Unless otherwise specified, we use KEDMI as the attack method and perform one round of GAN fine-tuning. The size of the pseudo-private dataset is $1,000$ , and we fine-tune the GAN for 10 epochs. ", "page_idx": 23}, {"type": "table", "img_path": "pyqPUf36D2/tmp/a1225ddf1fccc7161e9463445a16401e3e587240ef34340f05ec3790dfef13e7.jpg", "table_caption": ["Table 12: Ablation study on the number of rounds of fine-tuning. "], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "Iterative fine-tuning. Fine-tuning the GAN with generated pseudo-private data increases the probability of sampling data with characteristics closer to the actual private training data. We examine the impact of iterative fine-tuning on MI performance through experiments with different fine-tuning rounds. We use LOM (GMI) as the attack method and present the results in Tab. 12. The results show that MI performance improves with more fine-tuning rounds, suggesting that pseudo-private data increasingly approximates private training data. However, after the third round, further fine-tuning does not further improve MI performance as observed in earlier rounds. This decline could be due to excessive fine-tuning, which may distort the image manifold and degrade MI performance. ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "Impact of the size of the pseudo-private dataset in GAN fine-tuning. In iterative GAN fine-tuning, the pseudo-private data generated in each round can potentially increase the probability of sampling data with characteristics similar to the real private data in subsequent rounds. This process highlights the importance of determining an appropriate size for the pseudo-private dataset to achieve improved MI performance with an acceptable level of fine-tuning overhead. We investigate how MI performance is affected by varying the size of the pseudo-private dataset, with the results presented in Tab. 13. The results indicate a trend where the MI performance initially improves and ", "page_idx": 24}, {"type": "table", "img_path": "pyqPUf36D2/tmp/c9d44e47dcdc4c3297e3fbb3a21832755ab8ffcfc27ee6a1d556c09388c6d321.jpg", "table_caption": ["Table 13: Ablation study on the number of pseudo-private data used in GAN fine-tuning, where $|\\mathcal{D}_{\\mathrm{private}}^{\\mathrm{s}}|$ represents the size of the pseudoprivate dataset. "], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "then degrades. This suggests that increasing the size of the pseudo-private dataset up to a certain point (e.g., 3, 000) can enhance MI performance. The observed decline in performance could be attributed to the large amount of pseudo-data adopted, which increases the intensity level of fine-tuning and thus disrupts the image manifold. ", "page_idx": 24}, {"type": "text", "text": "Impact of the number of epochs in GAN finetuning. During iterative GAN fine-tuning, effectively utilizing pseudo-private data while reducing computational overhead is essential. Therefore, the number of tuning epochs is a crucial hyperparameter. This experiment examines the impact of fine-tuning epochs on MI performance. Tab. 14 shows that MI performance consistently improves as the number of GAN fine-tuning epochs increases, indicating that the GAN effectively learns and utilizes pseudo-private data. However, while MI performance improves, the computational overhead (i.e., fine-tuning time for each identity) increases linearly with the number of epochs. ", "page_idx": 24}, {"type": "table", "img_path": "pyqPUf36D2/tmp/c6d29d2aeb494aea87b3ac7cfd6ceef43e0ded6752831de9f6da631314ce9c7e.jpg", "table_caption": ["Table 14: Ablation study on the number of epochs in GAN fine-tuning. \"Time\" (seconds per identity) denotes the time required for finetuning a single identity. "], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "Comparison of identity-wise fine-tuning vs. multi-identity fine-tuning. Compared to finetuning the GAN using a single identity, which is specific to one identity and increases overhead, fine-tuning with multiple identities as a whole can significantly reduce computational costs. Thus, we aim to investigate the MI performance of finetuning the GAN based on both single-identity and multi-identity approaches, respectively. The re", "page_idx": 24}, {"type": "table", "img_path": "pyqPUf36D2/tmp/1e1dd811844e00372c8c6b7c69cda6c8bff55970fab6359b9203b61a5cce13e3.jpg", "table_caption": ["Table 15: Ablation study on identity-wise finetuning vs. multi-identity fine-tuning. "], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "sults, presented in Tab. 15, indicate that the MI performance of multi-identity fine-tuning decreases by $4\\%$ compared to single-identity fine-tuning. Despite the reduction in computational overhead, fine-tuning the GAN using multiple identities results in poorer MI performance. This is because fine-tuning with multiple identities induces more intensive changes to the generator. ", "page_idx": 24}, {"type": "text", "text": "D.3 Visualization of Reconstructed Images ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "In this section, we provide qualitative evidence to demonstrate the effectiveness of our proposed PPDG-MI. Results for high-resolution settings are illustrated in Figs. 9 and 10. Specifically, Fig. 9 presents a visual comparison of reconstructed samples for the first ten identities from three target models\u2014ResNet-18, DenseNet-121, and ResNeSt-50\u2014trained on the CelebA, using GANs pretrained on FFHQ. Fig. 10 shows a similar comparison for the same target models trained on the FaceScrub, also using GANs pre-trained on FFHQ. For low-resolution settings, the results are shown in Figs. 11 and 12. Fig. 11 illustrates reconstructed samples for the first ten identities from VGG16 trained on the CelebA private dataset, using GANs trained from scratch on the CelebA public dataset. Fig. 12 depicts reconstructed samples for the first ten identities from VGG16 trained on the CelebA private dataset, using GANs trained from scratch on the FFHQ public dataset. ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "E Discussion ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Scope and Applicability of Model Inversion Attacks. Model inversion attacks (MIAs) have become a critical area of research in assessing privacy risks, especially for discriminative models that handle sensitive data. While MIAs can be effective in certain data types, tasks, and specific contexts, they also face significant limitations, particularly in cases where the data lacks clear identity markers. ", "page_idx": 25}, {"type": "text", "text": "A notable limitation arises in their applicability to biomedical imaging, where MIAs face distinct challenges. For instance, unlike identity-rich data like facial images, chest X-rays generally lack identifiable features that can be easily linked to individuals. This absence of clear personal markers complicates privacy risk evaluation, making it harder to assess the impact of MIAs. ", "page_idx": 25}, {"type": "text", "text": "Moreover, even in experimental settings using datasets like ChestXray8 [Wang et al., 2017], where the primary goal is classification tasks such as diagnosing medical conditions, challenges persist. One key concern is whether a reconstructed chest X-ray would represent a generic \"average\" image or contain identifiable features unique to specific training samples. This issue arises from the inherent complexity of chest X-rays, which often require specialized medical expertise for accurate interpretation. ", "page_idx": 25}, {"type": "text", "text": "In summary, MIAs are more effective in domains where data includes explicit identity markers, leading to higher privacy risks as reconstructed images can closely resemble individuals from the training data. However, in fields like biomedical imaging, where data lacks evident identity characteristics, privacy risks are harder to quantify but remain an important area of concern. ", "page_idx": 25}, {"type": "text", "text": "Broader Impacts. In this paper, we propose a novel model inversion pipeline to enhance the performance of generative model inversion attacks (MIAs), potentially providing new insights and paving the way for future research. From a social perspective, our research on MIAs reveals significant privacy vulnerabilities in machine learning models that, if misused, could compromise sensitive training data. By revealing these risks, we aim to raise awareness and drive the development of robust defense mechanisms and privacy-preserving algorithms that are crucial for enhancing the security of machine learning systems. Overall, although our findings could be misused, the benefits of raising awareness and improving security practices in machine learning systems far outweigh these concerns. ", "page_idx": 25}, {"type": "text", "text": "Failure Case Analysis. To better assess the feasibility of our proposed method, we closely examined the reconstructed samples. In high-resolution scenarios, we observed that with a single fine-tuning round, PPDG-MI exhibited minimal failures. However, with continued fine-tuning, the visual quality of certain identities significantly deteriorated, indicating substantial alterations in the generator\u2019s manifold. Notably, these reconstructed samples still performed well on standard metrics despite poor visual quality, suggesting potential overftiting to these metrics. This finding underscores a limitation in current metrics, such as attack accuracy, which may lack robustness in such cases. Developing more effective and resilient evaluation metrics is an important direction for future research. ", "page_idx": 25}, {"type": "text", "text": "In low-resolution scenarios, we observed a decline in visual quality and performance on standard metrics for certain identities after a single round of attack, particularly in the case of LOM (KEDMI) and PLG-MI. We hypothesized that this failure stems from the baseline attack that generates samples with poor visual quality and insufficiently representative features of the target identities. These lowquality samples negatively impact the quality of the samples produced by the fine-tuned generator. With each subsequent fine-tuning round, these negative effects accumulated, progressively degrading the generator\u2019s manifold and leading to poorer visualization and MI performance. These findings indicate that the generator\u2019s capabilities may be insufficient to produce high-quality samples that positively influence subsequent rounds of GAN fine-tuning, thereby impacting the effectiveness of PPDG-MI. Therefore, utilizing more advanced generators represents a potential direction for better demonstrating the advancements of PPDG-MI. ", "page_idx": 25}, {"type": "text", "text": "Limitations of PPDG-MI. Conducting model inversion attacks is time-consuming, and the iterative fine-tuning in our proposed PPDG-MI adds further overhead, as shown by the running time ratios between the fine-tuning phase and model inversion phase in our experiments. However, our proposed prior distribution tuning methodology provides a promising solution to mitigate the fundamental distribution discrepancies between private and prior distributions. We hope our work will inspire future research to develop more efficient and effective tuning methods. ", "page_idx": 26}, {"type": "image", "img_path": "pyqPUf36D2/tmp/cc667e5dbb2b8dbc7e20ef3b141772c73ca44b296dd2bad7e857a315b9d06b8f.jpg", "img_caption": ["Figure 9: Visual comparison in high-resolution settings. We illustrate reconstructed samples for the first ten identities in $\\ensuremath{\\mathcal{D}_{\\mathrm{private}}}=$ CelebA using GANs pre-trained on $\\mathcal{D}_{\\mathrm{public}}=\\mathrm{FFHQ}$ . "], "img_footnote": [], "page_idx": 27}, {"type": "image", "img_path": "pyqPUf36D2/tmp/62a1271856715fb1191ce4c76aadcc9f40c2c1f7e8d291e42846fb343136118c.jpg", "img_caption": ["Figure 10: Visual comparison in high-resolution settings. We illustrate reconstructed samples for the first ten identities in $\\ensuremath{\\mathcal{D}_{\\mathrm{private}}}=$ FaceScrub using GANs pre-trained on $\\mathcal{D}_{\\mathrm{public}}=\\mathrm{FFHQ}$ . "], "img_footnote": [], "page_idx": 28}, {"type": "image", "img_path": "pyqPUf36D2/tmp/2a0ae000a365ad3740eceeea728ffa4f6b587a2776e38d0b2b644e0b06584c0d.jpg", "img_caption": ["Figure 11: Visual comparison in low-resolutions settings. We illustrate reconstructed samples for the first ten identities in $\\ensuremath{\\mathcal{D}_{\\mathrm{private}}}=$ CelebA using GANs trained from scratch on $\\mathcal{D}_{\\mathrm{public}}=$ CelebA. "], "img_footnote": [], "page_idx": 29}, {"type": "image", "img_path": "pyqPUf36D2/tmp/f05b9c24c76d4f7bc371460d79dc0654bc61cac60cfc9871b617aa5e033a6c19.jpg", "img_caption": [], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "Figure 12: Visual comparison in low-resolutions settings. We illustrate reconstructed samples for the first ten identities in $\\ensuremath{\\mathcal{D}_{\\mathrm{private}}}=$ CelebA using GANs trained from scratch on $\\mathcal{D}_{\\mathrm{public}}=\\mathrm{FFHQ}$ . ", "page_idx": 30}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We reveal limitations of previous work in Section 3.1 through experimental verifications. We propose a novel algorithm with several concrete implementations in Section 3. The effectiveness of our approach is validated by extensive experiments presented in Section 4 and further supported by additional results in Appendix D. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 31}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We primarily discuss the limitations of the proposed method in Section E. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 31}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: [NA] ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 32}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: The experimental setups are briefly introduced at the beginning of Section 4, and detailed in Appendix C. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 32}, {"type": "text", "text": "5. Open access to data and code ", "page_idx": 32}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: Our well-organized source code, complete with a detailed README, is available at: https://github.com/tmlr-group/PPDG-MI. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 33}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: We present the detailed experimental setups in Appendix C. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 33}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 33}, {"type": "text", "text": "Answer: [No] ", "page_idx": 33}, {"type": "text", "text": "Justification: Due to the significant time required for MIAs, we conduct a single attack against each target model. To reduce randomness, we generate at least 100 samples for each target class across various setups. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 33}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 34}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: We provide information about the hardware and software configurations in Appendix C.1. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 34}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: We only utilize publicly available datasets to develop machine learning algorithms aimed at promoting community development. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 34}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: We primarily discuss the broader impacts of this work in Section E. Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. ", "page_idx": 34}, {"type": "text", "text": "\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 35}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: [NA] ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 35}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: We have cited the original papers that produced the code packages or datasets. Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 35}, {"type": "text", "text": "", "page_idx": 36}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: Our well-organized source code, complete with a detailed README, is available at: https://github.com/tmlr-group/PPDG-MI. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 36}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] Justification: [NA] ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 36}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 36}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: [NA] ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 36}, {"type": "text", "text": "", "page_idx": 37}]