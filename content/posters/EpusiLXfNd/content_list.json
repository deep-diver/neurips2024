[{"type": "text", "text": "3D Structure Prediction of Atomic Systems with Flow-Based Direct Preference Optimization ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Rui Jiao1,2 Xiangzhe $\\mathbf{Kong^{1,2}}$ Wenbing Huang3,4\u2217 Yang Liu1,2\u2217 ", "page_idx": 0}, {"type": "text", "text": "1Dept. of Comp. Sci. & Tech., Institute for AI, Tsinghua University 2Institute for AIR, Tsinghua University 3Gaoling School of Artificial Intelligence, Renmin University of China 4 Beijing Key Laboratory of Big Data Management and Analysis Methods, Beijing, China ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Predicting high-fidelity 3D structures of atomic systems is a fundamental yet challenging problem in scientific domains. While recent work demonstrates the advantage of generative models in this realm, the exploration of different probability paths are still insufficient, and hallucinations during sampling are persistently occurring. To address these pitfalls, we introduce FlowDPO, a novel framework that explores various probability paths with flow matching models and further suppresses hallucinations using Direct Preference Optimization (DPO) for structure generation. Our approach begins with a pre-trained flow matching model to generate multiple candidate structures for each training sample. These structures are then evaluated and ranked based on their distance to the ground truth, resulting in an automatic preference dataset. Using this dataset, we apply DPO to optimize the original model, improving its performance in generating structures closely aligned with the desired reference distribution. As confirmed by our theoretical analysis, such paradigm and objective function are compatible with arbitrary Gaussian paths, exhibiting favorable universality. Extensive experimental results on antibodies and crystals demonstrate substantial beneftis of our FlowDPO, highlighting its potential to advance the field of 3D structure prediction with generative models. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Predicting 3D structures of atomic systems is indispensable in various scientific domains, ranging from pharmaceutical drug design [1, 17] to materials science [7]. Accurate 3D modeling is not only crucial for understanding the physical and chemical properties of substances at the atomic level [2, 18] but also for simulating and predicting their behavior in various environments [3, 26]. Nevertheless, it remains challenging due to the intricate nature of atomic interactions, the vastness of the conformational space, as well as limited resources of structure data. ", "page_idx": 0}, {"type": "text", "text": "Conventional methods typically employ physics-based algorithms to derive structures at local energy optimum [24, 31, 32]. Recent advancements leverage deep generative model to learn the distribution of stable structures from available data, showcasing remarkable success across various domains. For example, DiffAb [20] designs a diffusion-based method for antigen-specific antibody design, which is further available for antibody structure prediction, and DiffCSP [14] proposes a joint diffusion framework for crystal structure prediction. Despite these advancements, generative models for structure prediction are confronted with two primary challenges. ", "page_idx": 0}, {"type": "text", "text": "First, existing structure prediction methods predominantly utilize diffusion-based generative models. While effective, this focus narrows the scope of exploration into other probability paths that could ", "page_idx": 0}, {"type": "image", "img_path": "EpusiLXfNd/tmp/4b6aaf2b06e07b63435464244764eed8da50b6cb03bd8cb70b34d2f7617a5a10.jpg", "img_caption": ["(a) Training Flow Models (b) Preference Dataset Construction (c) Direct Preference Optimization "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Figure 1: Overview of the proposed FlowDPO pipeline. As described in Section 3.1, the process begins by training a flow matching model, denoted as $\\theta_{\\mathrm{ref}}$ , using an arbitrary pre-defined Gaussian path. Next, as outlined in Section 3.2, we construct a preference dataset, $\\mathcal{D}_{\\mathrm{pair}}$ , by evaluating the distances between generated samples $\\hat{x}_{i j}$ and the ground structure $x_{i}$ under a given context condition $c_{i}$ \u2014such as an antibody sequence or crystal composition. These samples are derived from the reference training set $\\mathcal{D}_{\\mathrm{ref}}$ . This dataset is then used to fine-tune the model $\\theta_{\\mathrm{opt}}$ through the DPO training objective $\\mathcal{L}_{\\mathrm{DPO}}$ , detailed in Section 3.3. ", "page_idx": 1}, {"type": "text", "text": "potentially offer substantial benefits. A notable example is the Optimal Transport (OT) path, which has recently been demonstrated to be particularly effective in the field of molecular generation [28]. Second, current training paradigm frequently leads to hallucinated distribution peaks [1]. Most generative models are trained through maximizing the likelihood or its lower bound on the groundtruth structures, which are easily haunted by hallucinations due to the lack of negative samples during training. In the field of natural language processing or computer vision, Direct Preference Optimization (DPO) [25, 30] is proposed to align the model with human preferences, which effectively reduces hallucinations. For 3D structure prediction, such preferences can be naturally extended to similarity with the reference structure (e.g. RMSD). However, it remains unclear whether the DPO method is compatible with arbitrary probability paths. ", "page_idx": 1}, {"type": "text", "text": "To address the above pitfalls, we introduce FlowDPO, a novel framework that explores flexible selection of Gaussian paths and enhances the quality of generated structures by alignment with the reference distribution. Specifically, we approach the structure prediction task via flow matching models regarding various paths. Given a pre-trained flow matching model, we sample multiple structures for each entry in the training set, evaluate these candidates against known ground truths to compute similarity, and construct an automatic preference dataset. Notably, we theoretically derive the unified objective of DPO for arbitrary Gaussian paths, and leverage the preference to enhance the performance of the original generative model. Intuitively, such a paradigm not only augment data with self-distilled samples, but also endow the model with the ability to distinguish between high-fidelity and hallucinated samples. ", "page_idx": 1}, {"type": "text", "text": "In summary, our contributions are threefold: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We explore multiple accessible probability paths for the 3D structure prediction task, and to the best of our knowledge, we are the first to theoretically prove the compatibility of DPO with arbitrary Gaussian paths by deriving a unified objective.   \n\u2022 Based on the theoretical results, we develop a novel framework to encourage better alignment of flow matching models with desired reference distribution in 3D structure prediction, which effectively suppresses the probability of hallucinations.   \n\u2022 Our approach yields promising results on antibody and crystal structure prediction tasks, showcasing the versatility and efficacy of our FlowDPO. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Structure Prediction for Atomic Systems. 3D Structure prediction, including predicting conformations from molecular topological graphs [34], determining unit cell structures from crystal compositions [14], or inferring structures based on protein sequences [1], is crucial in computational chemistry and material science. Traditionally, these predictions have relied on physics-inspired scoring functions [21, 11] or density functional theory (DFT)-based energy calculations [10] to define the search space, with subsequent application of search algorithms to identify optimal structures. Recently, deep generative methods, particularly diffusion models [12, 27], have proven to be highly effective in this field. These models have been successfully applied across multiple specific domains, including small molecules [34], crystals [14], antibodies [20], complexes [6], and general biomolecules [1]. The emergence of flow matching models [19], which generalize diffusion paths to more flexible probability flows, has further enhanced the generative capabilities for geometric graphs [28]. The goal of our work is to explore structure prediction from the perspective of flow matching, and align these models towards more accurate predictions. ", "page_idx": 2}, {"type": "text", "text": "Aligning Generative Models. In the domain of generative model alignment, recent work has focused on refining models to better meet human preferences. Direct Preference Optimization (DPO), introduced by [25], offers a significant advancement over traditional Reinforcement Learning from Human Feedback (RLHF, [23]) methods by directly optimizing a policy based on human preference data. This approach has proven effective in aligning large language models (LLMs) with user expectations. Extending this concept, [30] propose Diffusion-DPO, a novel method that adapts DPO for text-to-image diffusion models. By reformulating the preference optimization for diffusion model likelihoods, Diffusion-DPO achieves state-of-the-art performance in generating images that are not only visually appealing but also closely aligned with textual prompts. Recently, [36] introduces ABDPO, a DPO-based method tailored for antibody design. Unlike ABDPO, which concentrates on guiding diffusion models to generate antibody candidates with lower energy, our approach emphasizes aligning flow models for precise structure predictions. ", "page_idx": 2}, {"type": "text", "text": "3 FlowDPO ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Flow Matching for Geometric Graphs ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Flow Matching (FM, [19]) is a general paradigm for generative tasks by learning a vector field to connect the pre-defined prior distribution with the targeted data distribution. Let $q$ denote the data distribution, $x_{0}$ is a data point acquired from $p_{0}\\,=\\,q$ , and $x_{1}$ is a random sample from the prior distribution $p_{1}$ . A time-dependent flow $\\psi_{t}$ is then defined to shift samples from the prior distribution to the time-dependent distribution $p_{t}$ via the vector field $v_{t}$ , that is ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\psi_{1}(x)=x_{1},{\\frac{d(\\psi_{t}(x))}{d t}}=v_{t}(\\psi_{t}(x)).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The vector field can be further parameterized by a time-dependent model $\\boldsymbol{v}_{\\theta}(\\boldsymbol{x}_{t},t)$ , leading to the continuous normalizing flows (CNFs, [5]). To avoid numerical ODE simulations to train $v_{\\theta}$ , FM simplifies the training target by aligning the model with a pre-defined vector field $u_{t}$ to yield $p_{t}$ , i.e., ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{FM}}=\\mathbb{E}_{t,x_{t}\\sim p_{t}(x_{t})}[\\|v_{\\theta}(x_{t},t)-u_{t}(x_{t})\\|_{2}^{2}].\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "However, as $p_{t}$ is still unknown, we are still unable to sample $x_{t}$ and apply the above objective. To address this gap, [19] leverages the more accessible conditional vector field $u_{t}(x_{t}|x_{0})$ and its corresponding probability path $p_{t}(x_{t}|x_{0})$ , resulting in the following Conditional Flow Matching (CFM) objective, which is equivalent to ${\\mathcal{L}}_{\\mathrm{FM}}$ in terms of gradients and accessible for sampling: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{CFM}}=\\mathbb{E}_{t,x_{t}\\sim p_{t}(x_{t})}[\\|v_{\\theta}(x_{t},t)-u_{t}(x_{t}|x_{0})\\|_{2}^{2}].\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Different vector fields lead to different probability paths. For the commonly-used Gaussian distribution defined as ", "page_idx": 2}, {"type": "equation", "text": "$$\np_{t}(x_{t}|x_{0})=\\mathcal{N}(x_{t};\\mu_{t}(x_{0}),\\sigma_{t}^{2}(x_{0})),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "the corresponding vector field [19] is calculated as ", "page_idx": 3}, {"type": "equation", "text": "$$\nu_{t}(x_{t}|x_{0})=\\mu_{t}^{\\prime}(x_{0})+\\frac{\\sigma_{t}^{\\prime}(x_{0})}{\\sigma_{t}(x_{0})}(x-\\mu_{t}(x_{0})),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mu_{t}^{\\prime},\\sigma_{t}^{\\prime}$ are derivatives of $\\mu_{t},\\sigma_{t}~w.r.t.~t$ . We consider three lines of Gaussian paths in this paper, namely the Variance Exploding (VE), Variance Preserving (VP) and Optimal Transport (OT) paths, which are listed in Table 1. ", "page_idx": 3}, {"type": "table", "img_path": "EpusiLXfNd/tmp/7e72a0a6762c961e67ebfd698642c2e3129e22eef3600c0d89652e6ffbc18839.jpg", "table_caption": ["Table 1: Parameters of different Gaussian paths. VE, VP and OT represent Variable Exploding, Variable Preserving and Optimal Transport, respectively. "], "table_footnote": [], "page_idx": 3}, {"type": "text", "text": "Based on these paths, we are capable of designing proper flow models to maintain symmetries for specific structure prediction tasks. In this paper, we mainly focus on the two typical tasks on atomic systems: antibody structure prediction and crystal structure prediction. Note that symmtries are crucial in 3D atomic systems, and we provide more discussions in Appendix B. ", "page_idx": 3}, {"type": "text", "text": "Example 1: Antibody Structure Prediction. Antibodies are Y-shaped proteins generated by the immune system to identify and bind to specific antigens, with the structure depicted in Figure 2. Researchers mainly center on the variable domains of antibodies, which comprise a heavy chain and a light chain. Each chain includes three Complementarity-Determining Regions $(C D R s)$ and four framework regions in an alternating sequence. The six CDRs are volatile and crucial in defining the binding specificity and affinity, while the framework regions remain conserved. Among them, CDR-H3, which is the third CDR on the heavy chain, is the most diverse region and the primary focus of antibody design. Therefore, it is a fundamental yet challenging problem to accurately predict the structure of the CDRs upon binding. ", "page_idx": 3}, {"type": "image", "img_path": "EpusiLXfNd/tmp/072bfce76e6b014fc92c412dc1f4240f488b6d40006696aa52a01e05ffa9be70.jpg", "img_caption": ["Figure 2: Graphical depiction of antibody variable domains, which consist of a heavy chain and a light chain. Each chain is equipped with 4 Framework Regions (FRs) and 3 Complementarity-Determining Regions (CDRs). The CDRs, especially CDR-H3, are volatile and thus are the key focus. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Task Definition: Let ${\\cal A}=\\{a_{1},a_{2},\\cdots,a_{N}\\}$ denote the sequence of the targeted CDR region with the length of $N$ , where $\\pmb{a}_{i}\\in\\{0,1\\}^{20}$ is the one-hot type of the amino acid, and $\\vec{X}=\\{\\vec{\\pmb{x}}_{1},\\vec{\\pmb{x}}_{2},\\cdots,\\vec{\\pmb{x}}_{N}\\}$ is the corresponding 3D structures with $\\mathbf{\\boldsymbol{x}}_{i}\\in\\mathbb{R}^{3\\times4}$ as the backbone coordinates including $N,C_{\\alpha},C$ , and $O$ . Similarly, the sequence and structure of the context (i.e. framework regions and the antigen) are defined as $\\bar{A}^{C},\\vec{X}^{C}$ . The goal is to predict the structure of the CDR region given the context: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\vec{X}\\sim p_{0}(\\vec{X}|A,\\vec{X}^{C},A^{C}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Probability Paths and Training Objectives: DiffAb [20] has designed the VP path for the coordinates of the CDR region as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\vec{u}_{t,\\mathrm{VP}}(\\vec{X}_{t}|\\vec{X}_{0},A,\\vec{X}^{C},A^{C})=\\frac{\\alpha_{t}^{\\prime}}{1-\\alpha_{t}^{2}}\\big(\\alpha_{t}\\vec{X}_{t}-\\vec{X}_{0}\\big),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\alpha_{t}$ is scheduled as $\\alpha_{t}\\;=\\;e^{-\\frac{1}{2}\\int_{0}^{t}\\beta(s)d s}$ . After sampling $\\vec{X}_{0}\\;=\\;\\vec{\\epsilon}\\;\\sim\\;\\mathcal{N}(0,I)$ , we have $\\vec{X_{t}}=\\alpha_{t}\\vec{X_{0}}+\\sqrt{1-\\alpha_{t}^{2}}\\vec{\\epsilon}$ . With proper reparameterization, the training objective is defined as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{VP}}=\\mathbb{E}_{t,\\vec{\\epsilon}}\\big[\\|\\vec{\\epsilon}_{\\theta}(\\vec{X}_{t},A,\\vec{X}^{C},A^{C})-\\vec{\\epsilon}\\|_{2}^{2}\\big],\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "which only requires a model $\\theta$ to predict the denoising term given the current state. ", "page_idx": 4}, {"type": "text", "text": "Moreover, it is also practicable to linearly connect the data point $\\vec{X_{0}}$ and the noisy prior $\\vec{\\epsilon}$ via the OT path as $\\vec{X}_{t}=(1-t)\\vec{X}_{0}+t\\vec{\\epsilon}$ . The vector field is then defined as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\vec{u}_{t,\\mathrm{OT}}(\\vec{X}_{t}|\\vec{X}_{0},A,\\vec{X}^{C},A^{C})=\\frac{1}{t}(\\vec{X_{t}}-\\vec{X_{0}})=\\vec{\\epsilon}-\\vec{X_{0}}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The training objective directly align the model with the simple vector field: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{OT}}=\\mathbb{E}_{t,\\vec{\\epsilon}}\\big[\\|\\vec{v}_{\\theta}(\\vec{X}_{t},A,\\vec{X}^{C},A^{C})-(\\vec{\\epsilon}-\\vec{X_{0}})\\|_{2}^{2}\\big].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Example 2: Crystal Structure Prediction. Crystal Structure Prediction (CSP), a fundamental aspect of material science, requires to predict the stable 3D structure of a compound solely from its composition. Unlike molecules or proteins, which have a finite number of atoms, the uniqueness of crystals lies in their periodic repetition in infinite 3D space. The infinite crystal structure is typically simplified by its repeating unit, which is called a unit cell. The key point of CSP is the representation and generation of the unit cell. ", "page_idx": 4}, {"type": "text", "text": "Task Definition: A unit cell is usually characterized by a triplet ${\\mathcal{M}}\\,=\\,(A,L,F)$ , where $\\textbf{A}=$ $[\\pmb{a}_{1},\\pmb{a}_{2},...,\\pmb{a}_{N}]\\;\\in\\;\\mathbb{R}^{h\\times N}$ represents the one-hot encoded atom types, ${\\cal L}\\;=\\;[l_{1},l_{2},l_{3}]\\;\\in\\;\\mathbb{R}^{3\\times3}$ denotes the lattice matrix with three basis vectors describing the crystal\u2019s periodicity, and ${\\pmb F}=$ [x1, x2, ..., xN] \u2208R[30\u00d7,1N) contains the fractional coordinates of the atoms, specifying their positions relative to the lattice matrix. The goal of CSP is to predict the lattice matrix and the atomic coordinates based on the given crystal composition as ", "page_idx": 4}, {"type": "equation", "text": "$$\n(L,F)\\sim p_{0}(L,F|A).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Probability Paths and Training Objectives: As the lattice matrix $\\textbf{\\emph{L}}$ also lies in the Euclidean space, we can design similar VP and OT paths as Eq. (7-10). Given $\\epsilon_{L}\\mathrm{~\\,~\\sim~}$ $\\mathcal{N}(0,I)$ , with ${\\cal L}_{t}=\\alpha_{t}{\\cal L}_{0}\\!+\\!\\sqrt{1-\\alpha_{t}^{2}}\\epsilon_{L}$ , the loss function of the VP path is defined as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{L,\\mathrm{VP}}=\\mathbb{E}_{t,\\epsilon_{L}}\\left[||\\epsilon_{L,\\theta}(L_{t},F_{t},A)-\\epsilon_{L}||_{2}^{2}\\right].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Besides, with $L_{t}=(1\\!-\\!t)L_{0}\\!+\\!t\\epsilon_{L}$ , the training objective of the OT path is similarly defined as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{L,\\mathrm{OT}}=\\mathbb{E}_{t,\\epsilon_{L}}\\left[\\|v_{L,\\theta}(L_{t},F_{t},A)-(\\epsilon_{L}-L_{0})\\|_{2}^{2}\\right].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "image", "img_path": "EpusiLXfNd/tmp/b0cab8a0fbb5e8d17b5ddabe1dd830db28b44eb2b206a58aad2d4a16adb5945e.jpg", "img_caption": ["Figure 3: A crystal is the infinite periodic arrangement of atoms, and the repeating unit is named as a unit cell. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "The fractional coordinates lie in the torus space of R[30\u00d7,1N) to inherently reflect the periodicity of the crystal. Previous works [15, 14] project the VE path to this manifold, and the Gaussian distribution is changed into the Wrapped Normal (WN) distribution as $p_{t}(F_{t}|F_{0})=\\mathcal{N}_{w}(F_{t};F_{0},\\sigma_{t}^{2}I)$ , ", "page_idx": 4}, {"type": "equation", "text": "$$\np_{t}(F_{t}|F_{0})=\\mathcal{N}_{w}(F_{t};F_{0},\\sigma_{t}^{2}I),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "swchoerree, $\\begin{array}{r}{\\mathcal{N}_{w}(x;\\cdot,\\cdot)=\\sum_{i=-\\infty}^{\\infty}\\mathcal{N}_{w}(x+i;\\cdot,\\cdot)}\\end{array}$ .o f ,  aacncde tshsieb lloe ssw fauyn tcoti loena risn  dtheifsin peadt ah sis to match the $p_{t}$ ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{F,\\mathrm{VE}}=\\mathbb{E}_{t,F_{t}}\\big[\\lambda_{t}\\|\\epsilon_{F,\\theta}(L_{t},F_{t},A)-\\nabla_{F_{t}}\\log p_{t}(F_{t}|F_{0})\\|_{2}^{2}\\big],\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\lambda_{t}=\\mathbb{E}^{-1}\\left[\\|\\nabla\\log\\mathcal{N}_{w}(0,\\sigma_{t}^{2})\\|_{2}^{2}\\right]$ is the pre-computed weight. If $\\sigma_{1}$ in Eq. (14) is sufficiently large, $p_{1}$ would finally approach the uniform distribution, which can be selected as the prior distribution. Apart from the VE path, it is also applicable to directly connect the data point and the prior sample via the shortest path on the manifold. Specfically, given $F_{0}\\,\\sim\\,p_{0},F_{1}\\,\\sim\\,p_{1}$ , where $p_{1}$ is defined as the uniform distribution, the shortest path $s(F_{0},F_{1})$ can be determined by the logarithmic map from $\\scriptstyle F_{0}$ to $F_{1}$ as $s(F_{0},F_{1})=\\log_{F_{0}}F_{1}=w(F_{1}-F_{0}+0.5)-0.5$ . Alternatively, $F_{1}$ can also be considered as the destination of $s(F_{0},F_{1})$ via the exponential map from $F_{0}$ as $\\exp_{F_{0}}s(F_{0},F_{1})=w(F_{0}+s(F_{0},F_{1}))$ . To eliminate the effect of the overall translation introduced by the prior, we further normalize $F_{1}$ as $\\hat{F}_{1}=\\exp_{F_{0}}\\hat{s}(F_{0},F_{1})=\\exp_{F_{0}}\\left(s(F_{0},F_{1})-\\bar{s}(F_{0},F_{1})\\right)$ , where $\\bar{s}$ averages the paths of all atoms. With the path of $\\textstyle F_{t}$ defined as $F_{t}=\\exp_{F_{0}}\\left(t\\hat{s}(F_{0},F_{1})\\right)$ , the training objective for the OT path is ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{F,\\mathrm{OT}}=\\mathbb{E}_{t,F_{1}}\\Big[\\|v_{F,\\theta}(L_{t},F_{t},A)-\\hat{s}(F_{0},F_{1})\\|_{2}^{2}\\Big].\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Generalized Notations. Overall, the structure prediction tasks aims at generating the targeted structure $x$ given some condition $c$ , i.e. to learn $p_{0}(x|c)$ . And the flow matching objective minimizes the Mean Square Error (MSE) of the predicted and pre-defined vector fields with proper reparameterization or simplification, which can be generalized as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}=\\mathbb{E}_{t,x_{0}\\sim p_{0},x_{1}\\sim p_{1}}[\\mathbf{MSE}_{t}(x_{0},x_{1};\\theta)].\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Hereinafter, we use these generalized notations for simplicity. ", "page_idx": 5}, {"type": "text", "text": "3.2 Preference Dataset Construction ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Building on the flow paths introduced in $\\S\\ 3.1$ , we now delve into the details of constructing a preference dataset, which is pivotal for the application of DPO, as detailed in $\\S\\ 3.3$ . ", "page_idx": 5}, {"type": "table", "img_path": "EpusiLXfNd/tmp/070d19f121f0a7f8a0c47c24ded0ad86e86460d8e8a014411b202c74027ef39b.jpg", "table_caption": [], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "Candidate Generation As shown in Algorithm 1, the construction of the preference dataset begins with the generation of multiple candidate structures for each sample in our reference dataset, $\\boldsymbol{\\mathcal{D}}_{r e f}$ . Leveraging the pre-trained flow-based generative model $\\theta_{\\mathrm{ref}}$ , we generate $M$ candidate structures $\\{\\hat{x}_{i j}\\}_{j=1}^{M}$ for each sample $(x_{i},c_{i})$ via $p(x|c_{i};\\theta_{\\mathrm{ref}})$ , ensuring that each generated structure is contextually relevant and adheres to the geometric constraints discussed previously. ", "page_idx": 5}, {"type": "text", "text": "As each candidate is generated, we compute the distance between $\\hat{x}_{i j}$ and the original structure $x_{i}$ using a predefined metric $d(\\cdot,\\cdot)$ . If this distance is less than or equal to a threshold $\\delta$ , the candidate is considered a close match and is added to $\\mathcal{D}_{p o s}$ , a subset of promising candidates. This step is crucial for efficiently filtering the generated data to retain only the most relevant candidates for DPO. ", "page_idx": 5}, {"type": "text", "text": "Preference Pairs Construction Subsequently, we construct $K$ preference pairs $(x_{i k}^{w},x_{i k}^{l})$ for each sample $i$ by Algorithm 2, where $x_{i k}^{w}$ is preferred over $x_{i k}^{l}$ . This preference is determined based on their proximity to the original structure $x_{i}$ . Apart from sampling pairs from generated structures, we also use a ratio $r$ to select the ground truth as the preferred sample. Moreover, if all generated structures for a sample are far from the original, the original structure $x_{i}$ is always preferred. The other pairs are formed by selecting $x_{i k}^{w}$ from the promising subset $\\mathcal{D}_{p o s}$ and $x_{i k}^{l}$ from the broader set $\\mathcal{D}_{g e n}$ . This process ensures that the pairs reflect a clear preference based on the closeness to the original structure, facilitating effective training through DPO, which is explored in the next section. ", "page_idx": 5}, {"type": "text", "text": "3.3 Direct Preference Optimization ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "To align large language models with human preference, DPO [25] is proposed to replace the RLHF [23] training objective with directly maximizing the likelihood of the preference. [30] extends DPO to text-to-image generation task, adapting the DPO target to diffusion models. Given the preference pair $(x^{w},\\bar{x}^{l})$ , DPO [25] designs the training objective as ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{DPO}}=-\\mathbb{E}_{x^{w},x^{l}}\\Big[\\log\\sigma\\big(\\beta\\log\\frac{p_{\\mathrm{opt}}(x^{w})}{p_{\\mathrm{ref}}(x^{w})}-\\beta\\log\\frac{p_{\\mathrm{opt}}(x^{l})}{p_{\\mathrm{ref}}(x^{l})}\\big)\\Big],\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $p_{\\mathrm{opt}},p_{\\mathrm{ref}}$ are probabilities yielded by the fine-tuned model $\\theta_{\\mathrm{opt}}$ and the pre-trained flow model, and $\\beta$ is a hyperparameter to control the KL divergence of these two distributions. ", "page_idx": 6}, {"type": "text", "text": "It is nontrivial to efficiently acquire $p(x)$ via iterative generative models. Inspired by [30], we uniformly discretize the time interval into $T$ steps, where step $i$ is located at $t=i/T$ . By formulating the probability from the path $x_{0:T}$ , Eq. (18) can be rewritten as ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{\\mathrm{DPO}}=-\\mathbb{E}_{x^{w},x^{l}}\\log\\sigma\\Big(\\beta\\mathbb{E}_{x_{1:T}^{w},x_{1:T}^{l}}\\big[\\log\\frac{p_{\\mathrm{opt}}\\left(x_{0:T}^{w}\\right)}{p_{\\mathrm{ref}}\\left(x_{0:T}^{w}\\right)}-\\log\\frac{p_{\\mathrm{opt}}\\left(x_{0:T}^{l}\\right)}{p_{\\mathrm{ref}}\\left(x_{0:T}^{l}\\right)}\\big]\\Big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "To avoid costly sampling through the entire path, Jensen\u2019s inequality [30] is applied to bound Eq. (19) as ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{\\mathrm{DPO}}\\leq-\\mathbb{E}_{x^{w},x^{l},i}\\log\\sigma\\Big(B\\big[\\log\\frac{p_{\\mathrm{opt}}(x_{i-1}^{w}|x_{i}^{w})}{p_{\\mathrm{ref}}(x_{i-1}^{w}|x_{i}^{w})}-\\log\\frac{p_{\\mathrm{opt}}(x_{i-1}^{l}|x_{i}^{l})}{p_{\\mathrm{ref}}(x_{i-1}^{l}|x_{i}^{l})}\\big]\\Big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $B\\:=\\:\\beta T$ servers as a hyperparameter. As directly sampling $x_{i-1},x_{i}$ from an arbitrary intermediate step $i$ is still unfeasible, we can estimate them via the accessible Gaussian paths $p$ in Table 1 as ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{\\mathrm{DPO}}=-\\mathbb{E}_{x^{w},x^{l},i}\\log\\sigma\\Big(B\\mathbb{E}_{p(x_{i-1}^{w}|x_{i,0}^{w}),p(x_{i-1}^{w}|x_{i,0}^{l})}\\big[\\log\\frac{p_{\\mathrm{opt}}(x_{i-1}^{w}|x_{i}^{w})}{p_{\\mathrm{ref}}(x_{i-1}^{w}|x_{i}^{w})}-\\log\\frac{p_{\\mathrm{opt}}(x_{i-1}^{l}|x_{i}^{l})}{p_{\\mathrm{ref}}(x_{i-1}^{l}|x_{i}^{l})}\\big]\\Big)}\\\\ {(\\mathrm{\\normalfont~--~}\\mathbb{E}_{x^{w},x^{l},i}\\log\\sigma\\Big(B\\big[\\mathcal{T}(x_{i}^{w};p,p_{\\mathrm{ref}})-\\mathcal{T}(x_{i}^{w};p,p_{\\mathrm{opt}})-\\mathcal{T}(x_{i}^{l};p,p_{\\mathrm{ref}})+\\mathcal{T}(x_{i}^{l};p,p_{\\mathrm{opt}})\\big]\\Big)\\Big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\mathcal{I}(x_{i}^{w};p,p_{\\theta})$ denotes $D_{\\mathrm{KL}}\\big(p(x_{i-1}^{w}|x_{i,0}^{w})\\|p_{\\theta}(x_{i-1}^{w}|x_{i}^{w})\\big)$ and the same for $\\mathcal{I}(x_{i}^{l};p,p_{\\theta})$ . As $p$ and $p_{\\theta}$ are Gaussian distributions with the same noise scheduler, the KL divergence can be formulated as ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{I}(x_{i};p,p_{\\theta})=\\frac{1}{2\\sigma_{i-1|i}^{2}}\\Big\\|\\mu(x_{i-1}|x_{i,0})-\\mu_{\\theta}(x_{i-1}|x_{i})\\Big\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "According to DDIM [27], if a time-dependent Gaussian path follows the form $x_{i}\\sim\\mathcal{N}(x_{i};k_{i}x_{0},\\sigma_{i}I)$ , we can further design $p(x_{i-1}|x_{i,0})=\\mathcal{N}\\big(x;\\mu(x_{i-1}|x_{i,0}),\\sigma_{i-1|i}^{2}\\big)$ . Given $\\sigma_{i-1|i}^{2}$ , the mean can be formulated as ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mu(x_{i-1}|x_{i,0})=\\frac{1}{\\sigma_{i}}\\sqrt{\\sigma_{i-1}^{2}-\\sigma_{i-1|i}^{2}}x_{i}+\\Big(k_{i-1}-\\frac{k_{i}}{\\sigma_{i}}\\sqrt{\\sigma_{i-1}^{2}-\\sigma_{i-1|i}^{2}}\\Big)x_{0}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Fortunately, all paths defined in Table 1 follows this form. And $\\mu_{\\theta}(x_{i-1}|x_{i})$ can be parameterized similarly as Eq. (24), with estimating $x_{0}$ via predicted vector field or denoising terms. Hence, we can approximate $\\mathcal{I}(x_{i};p,p_{\\theta})$ by $\\mathrm{MSE}_{i}(x_{0},x_{1};\\theta)$ . With sufficiently large $T$ , Eq. (??) can be changed into an applicable form as follows, which is our final training objective. ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{\\mathrm{DPO}}=-\\mathbb{E}_{x_{0,1}^{w},x_{0,1}^{l},t}\\log\\sigma\\Big(B\\big[\\mathrm{MSE}_{t}(x_{0}^{w},x_{1}^{w};\\theta_{\\mathrm{ref}})-\\mathrm{MSE}_{t}(x_{0}^{w},x_{1}^{w};\\theta_{\\mathrm{opt}})}\\\\ {-\\,\\mathrm{MSE}_{t}(x_{0}^{l},x_{1}^{l};\\theta_{\\mathrm{ref}})+\\mathrm{MSE}_{t}(x_{0}^{l},x_{1}^{l};\\theta_{\\mathrm{opt}})\\big]\\Big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "table", "img_path": "EpusiLXfNd/tmp/0c85985305523a77e991462b57b5a43588797069b2a3b30b8885678576ebd27e.jpg", "table_caption": ["Table 2: $C_{\\alpha}$ and bb indicates RMSD calculated on $C_{\\alpha}$ atoms and backbone atoms, repectively. $\\textstyle\\mathbf{C}_{\\alpha}$ -w and bb-w averages the RMSDs of the worst generated conformations of each complex. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We validate our method on two distinct domains: antibody structure prediction $(\\S\\,4.1)$ and crystal structure prediction $(\\S\\,4.2)$ . ", "page_idx": 7}, {"type": "text", "text": "4.1 Antibody Structure Prediction ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Dataset Following previous literature [20], we extract antibody structures from the SAbDab database [8] for training and utilize the manually curated test set from DiffAb [20], which contains 19 antibody-antigen complexes. We first derive all structures deposited before April 11th, 2024, and remove those with resolution above $4.0\\mathring\\mathrm{A}$ or non-protein targets, resulting in 12,428 antibodies. Subsequently, we use mmseqs2 [29] to cluster the antibodies based on $50\\%$ sequence identity for each CDR, and exclude those in the same clusters as the test set antibodies. The dataset is then split into training and validation sets at a 9:1 ratio based on the clusters. ", "page_idx": 7}, {"type": "text", "text": "Metrics We employ the following metrics for evaluation. ${\\mathbf{RMSD}}_{C_{\\alpha}}$ measures the Root Mean Square Deviation of the generated alpha carbon coordinates with respect to the reference. $\\mathbf{RMSD}_{b b}$ is the RMSD calculated on the four backbone atoms including $C,C_{\\alpha},N,O$ . To better proflie the generated distribution, for each antibody, we generate 20 structures and use two strategies to aggregate the results across different antibodies. Strategy worst select the worst generated structure per antibody according to RMSD and then averagse across different antibodies, while strategy mean averages the RMSD of 20 candidates first, and then across antibodies. Strategy worst measures the furthest deviation of the generated distribution compared to the reference, while strategy mean is commonly adopted in previous works [20, 16]. Results aggregated with worst are denoted as $\\mathbf{C}_{\\alpha}$ -w and bb-w, while those with mean are denoted as $\\mathbf{C}_{\\alpha}$ and bb. ", "page_idx": 7}, {"type": "text", "text": "Results We evaluate VP path (DiffAb) [20] and OT path [19] with the proposed FlowDPO on CDR structure prediction. Results in Table 2 illustrate that either using VP path or OT path, further training with DPO consistently enhances performance across different CDRs. Notably, on the most challenging part (i.e. CDR-H3), the DPO phase yields the most significant improvement. Metrics aggregated with strategy worst demonstrate noticeable gains, indicating effective supperssion of low-quality samples by the DPO phase, which we attribute to the objective of DPO in distinguishing the prefered samples. Such characteristics are favorable in practical applications where it requires blind selection of generated structures without prior knowledge of which structures might be more correct. We also depict the distributions of RMSD and examples of generated CDR-H3 structures in Figure 4. It shows that the blue curves, yielded by the original flow models, often exhibit a bimodal distribution. While the first peak at a lower RMSD indicating higher quality generations, the second peak at a higher RMSD suggests the models experience hallucination, confidently generating conformations that significantly deviate from the ground truth. DPO effectively suppresses this erroneous second peak, leading to an overall improvement in the quality of generated samples. On closer inspection, this correction also addresses physical invalidities, such as the twisted backbone seen in Figure 4. ", "page_idx": 7}, {"type": "image", "img_path": "EpusiLXfNd/tmp/bb22cb9596796bf4d28e85ef2d1be9c5de2cf402761f78b67d94bc72b43453f4.jpg", "img_caption": ["Figure 4: Examples of generated CDR-H3 structures and the distribution of $\\mathrm{RMSD}_{C_{\\alpha}}$ for different antigen-antibody complexes and different probability paths. The visualized samples are the ones with the lowest RMSD of all the generated counterparts for the corresponding complexes. In addition to driving the distribution towards lower RMSD, it is also observed that the DPO phase tends to rectify the physical invalidity (e.g. twisted backbone in the above examples) in the generated samples. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "4.2 Crystal Structure Prediction ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Dataset We conduct the crystal structure prediction task on three datasets in line with previous works [33, 14]. Perov-5 [4] includes 18,928 perovskite crystals, each characterized by similar structures but varying compositions, and exactly 5 atoms per unit cell. MP-20 [13] comprises 45,231 materials from the Materials Project, featuring a wide range of compositions and structures, with each material containing no more than 20 atoms per unit cell. These materials predominantly represent crystals that have been synthesized experimentally. MPTS-52 is an advanced version of MP-20, containing 40,476 structures with unit cells that include up to 52 atoms, presenting a more complex challenge. For Perov-5 and MP-20, we maintain the conventional 60-20-20 split for training, validation, and testing. For the MPTS-52 dataset, we use a chronological split, assigning 27,380 crystals for training, 5,000 for validation, and 8,096 for testing. ", "page_idx": 8}, {"type": "text", "text": "Metrics For inference, we generate one structure given each composition. The predicted sample is then matched with the ground truth via the StructureMatcher class in pymatgen [22] with thresholds sto $_{1=0.5}$ , angle_to $_{\\mathrm{=}10}$ , lto $_{=0.3}$ as applied in previous works [33, 14]. We use Match Rate (MR) as the proportion of matched structures among the testing set, and the RMSD is averaged over the matched pairs, and normalized by $\\sqrt[3]{V/N}$ where $V$ is the volume of the unit cell. ", "page_idx": 8}, {"type": "text", "text": "Results We compare the results with two generative baselines P-cG-SchNet [9] and CDVAE [33]. The results are shown in Table 3, where we explore three combinations of paths for jointly generating the lattice and the fractional coordinates: $\\mathrm{VP+VE}$ , $\\mathrm{OT+OT}$ , and $\\mathrm{OT+VE}$ . Notably, the $\\mathrm{VP+VE}$ path is previously developed by DiffCSP [14]. We find that the OT path is more effective for lattice generation, while the VE path provides more accurate predictions of atomic coordinates within the cell. Overall, the $\\mathrm{OT+VE}$ combination generally delivers the best performance. Furthermore, DPO consistently enhances the performance of the model trained on each combination, demonstrating its capability to refine the predictions to a more precise alignment with experimental structures. We additionally visualize the RMSD distribution of predicted structures from different Gaussian paths. Results in Figure 5 reveal a similar pattern to Figure 4, demonstrating that DPO reduces the probability of low-quality generations. ", "page_idx": 8}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this work, we propose FlowDPO, a novel framework for 3D structure prediction that integrates flowbased generative models with Direct Preference Optimization. We achieve 3D structure prediction via flow matching models with various probability paths, and generalize the DPO training objective to arbitrary Gaussian paths. To refine the model via DPO, we generate multiple candidate structures and construct the preference dataset by aligning with ground truth. The results demonstrate substantial improvements in prediction accuracy for both antibody and crystal structures, highlighting the effectiveness and versatility of FlowDPO in the field of 3D structure prediction. ", "page_idx": 8}, {"type": "table", "img_path": "EpusiLXfNd/tmp/087a46fcf5f612ab422e9c8ba7eea6384ab0328dce9be102485a32d91bd745d8.jpg", "table_caption": ["Table 3: Results on crystal structure prediction task. MR stands for Match Rate. "], "table_footnote": [], "page_idx": 9}, {"type": "image", "img_path": "EpusiLXfNd/tmp/034e3bcfa12434b3315cadee511c1d90cbcbc1d63cb94d7562f63fc259b3ea4a.jpg", "img_caption": ["Figure 5: Visualizations on crystal structure prediction results. The left column depicts the RMSD distribution of the models before (blue) and after (red) DPO. The middle column shows the ground truth structures, and the right column shows typical high RMSD generations to be suppressed. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work is jointly supported by the National Science and Technology Major Project under Grant 2020AAA0107300, the National Natural Science Foundation of China (No. 61925601, No. 62376276, No. 62236011), and Beijing Nova Program (20230484278). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] J. Abramson, J. Adler, J. Dunger, R. Evans, T. Green, A. Pritzel, O. Ronneberger, L. Willmore, A. J. Ballard, J. Bambrick, et al. Accurate structure prediction of biomolecular interactions with alphafold 3. Nature, pages 1\u20133, 2024. ", "page_idx": 9}, {"type": "text", "text": "[2] I. Batatia, D. P. Kovacs, G. Simm, C. Ortner, and G. Cs\u00e1nyi. Mace: Higher order equivariant message passing neural networks for fast and accurate force fields. Advances in Neural ", "page_idx": 9}, {"type": "text", "text": "Information Processing Systems, 35:11423\u201311436, 2022. ", "page_idx": 10}, {"type": "text", "text": "[3] S. Batzner, A. Musaelian, L. Sun, M. Geiger, J. P. Mailoa, M. Kornbluth, N. Molinari, T. E. Smidt, and B. Kozinsky. E (3)-equivariant graph neural networks for data-efficient and accurate interatomic potentials. Nature communications, 13(1):2453, 2022.   \n[4] I. E. Castelli, D. D. Landis, K. S. Thygesen, S. Dahl, I. Chorkendorff, T. F. Jaramillo, and K. W. Jacobsen. New cubic perovskites for one-and two-photon water splitting using the computational materials repository. Energy & Environmental Science, 5(10):9034\u20139043, 2012.   \n[5] R. T. Chen, Y. Rubanova, J. Bettencourt, and D. K. Duvenaud. Neural ordinary differential equations. Advances in neural information processing systems, 31, 2018.   \n[6] G. Corso, B. Jing, R. Barzilay, T. Jaakkola, et al. Diffdock: Diffusion steps, twists, and turns for molecular docking. In International Conference on Learning Representations (ICLR 2023), 2023.   \n[7] G. R. Desiraju. Cryptic crystallography. Nature materials, 1(2):77\u201379, 2002.   \n[8] J. Dunbar, K. Krawczyk, J. Leem, T. Baker, A. Fuchs, G. Georges, J. Shi, and C. M. Deane. Sabdab: the structural antibody database. Nucleic acids research, 42(D1):D1140\u2013D1146, 2014.   \n[9] N. W. Gebauer, M. Gastegger, S. S. Hessmann, K.-R. M\u00fcller, and K. T. Sch\u00fctt. Inverse design of 3d molecular structures with conditional generative neural networks. Nature communications, 13(1):1\u201311, 2022.   \n[10] J. Hafner. Ab-initio simulations of materials using vasp: Density-functional theory and beyond. Journal of computational chemistry, 29(13):2044\u20132078, 2008.   \n[11] T. A. Halgren, R. B. Murphy, R. A. Friesner, H. S. Beard, L. L. Frye, W. T. Pollard, and J. L. Banks. Glide: a new approach for rapid, accurate docking and scoring. 2. enrichment factors in database screening. Journal of medicinal chemistry, 47(7):1750\u20131759, 2004.   \n[12] J. Ho, A. Jain, and P. Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:6840\u20136851, 2020.   \n[13] A. Jain, S. P. Ong, G. Hautier, W. Chen, W. D. Richards, S. Dacek, S. Cholia, D. Gunter, D. Skinner, G. Ceder, et al. Commentary: The materials project: A materials genome approach to accelerating materials innovation. APL materials, 1(1):011002, 2013.   \n[14] R. Jiao, W. Huang, P. Lin, J. Han, P. Chen, Y. Lu, and Y. Liu. Crystal structure prediction by joint equivariant diffusion. Advances in Neural Information Processing Systems, 36, 2024.   \n[15] B. Jing, G. Corso, J. Chang, R. Barzilay, and T. Jaakkola. Torsional diffusion for molecular conformer generation. Advances in Neural Information Processing Systems, 35:24240\u201324253, 2022.   \n[16] X. Kong, W. Huang, and Y. Liu. Conditional antibody design as 3d equivariant graph translation. In The Eleventh International Conference on Learning Representations, 2022.   \n[17] J. Kreitz, M. J. Friedrich, A. Guru, B. Lash, M. Saito, R. K. Macrae, and F. Zhang. Programmable protein delivery with a bacterial contractile injection system. Nature, 616(7956):357\u2013364, 2023.   \n[18] Y.-L. Liao and T. Smidt. Equiformer: Equivariant graph attention transformer for 3d atomistic graphs. In The Eleventh International Conference on Learning Representations, 2022.   \n[19] Y. Lipman, R. T. Chen, H. Ben-Hamu, M. Nickel, and M. Le. Flow matching for generative modeling. In The Eleventh International Conference on Learning Representations, 2022.   \n[20] S. Luo, Y. Su, X. Peng, S. Wang, J. Peng, and J. Ma. Antigen-specific antibody design and optimization with diffusion-based generative models for protein structures. Advances in Neural Information Processing Systems, 35:9754\u20139767, 2022.   \n[21] G. M. Morris, R. Huey, W. Lindstrom, M. F. Sanner, R. K. Belew, D. S. Goodsell, and A. J. Olson. Autodock4 and autodocktools4: Automated docking with selective receptor flexibility. Journal of computational chemistry, 30(16):2785\u20132791, 2009.   \n[22] S. P. Ong, W. D. Richards, A. Jain, G. Hautier, M. Kocher, S. Cholia, D. Gunter, V. L. Chevrier, K. A. Persson, and G. Ceder. Python materials genomics (pymatgen): A robust, open-source python library for materials analysis. Computational Materials Science, 68:314\u2013319, 2013.   \n[23] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:27730\u201327744, 2022.   \n[24] C. J. Pickard and R. Needs. Ab initio random structure searching. Journal of Physics: Condensed Matter, 23(5):053201, 2011.   \n[25] R. Rafailov, A. Sharma, E. Mitchell, C. D. Manning, S. Ermon, and C. Finn. Direct preference optimization: Your language model is secretly a reward model. Advances in Neural Information Processing Systems, 36, 2024.   \n[26] M. Schreiner, O. Winther, and S. Olsson. Implicit transfer operator learning: Multiple timeresolution surrogates for molecular dynamics. arXiv preprint arXiv:2305.18046, 2023.   \n[27] J. Song, C. Meng, and S. Ermon. Denoising diffusion implicit models. In International Conference on Learning Representations, 2020.   \n[28] Y. Song, J. Gong, M. Xu, Z. Cao, Y. Lan, S. Ermon, H. Zhou, and W.-Y. Ma. Equivariant flow matching with hybrid probability transport for 3d molecule generation. Advances in Neural Information Processing Systems, 36, 2024.   \n[29] M. Steinegger and J. S\u00f6ding. Mmseqs2 enables sensitive protein sequence searching for the analysis of massive data sets. Nature biotechnology, 35(11):1026\u20131028, 2017.   \n[30] B. Wallace, M. Dang, R. Rafailov, L. Zhou, A. Lou, S. Purushwalkam, S. Ermon, C. Xiong, S. Joty, and N. Naik. Diffusion model alignment using direct preference optimization. arXiv preprint arXiv:2311.12908, 2023.   \n[31] Y. Wang, J. Lv, L. Zhu, and Y. Ma. Crystal structure prediction via particle-swarm optimization. Physical Review B, 82(9):094116, 2010.   \n[32] B. D. Weitzner, J. R. Jeliazkov, S. Lyskov, N. Marze, D. Kuroda, R. Frick, J. Adolf-Bryfogle, N. Biswas, R. L. Dunbrack, and J. J. Gray. Modeling and docking of antibody structures with rosetta. Nature protocols, 12(2):401\u2013416, 2017.   \n[33] T. Xie, X. Fu, O.-E. Ganea, R. Barzilay, and T. S. Jaakkola. Crystal diffusion variational autoencoder for periodic material generation. In International Conference on Learning Representations, 2021.   \n[34] M. Xu, L. Yu, Y. Song, C. Shi, S. Ermon, and J. Tang. Geodiff: A geometric diffusion model for molecular conformation generation. In International Conference on Learning Representations, 2021.   \n[35] C. Zeni, R. Pinsler, D. Z\u00fcgner, A. Fowler, M. Horton, X. Fu, S. Shysheya, J. Crabb\u00e9, L. Sun, J. Smith, et al. Mattergen: a generative model for inorganic materials design. arXiv preprint arXiv:2312.03687, 2023.   \n[36] X. Zhou, D. Xue, R. Chen, Z. Zheng, L. Wang, and Q. Gu. Antigen-specific antibody design via direct energy-based preference optimization. arXiv preprint arXiv:2403.16576, 2024. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A From RLHF to DPO ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Given the preference pair $(x^{w},x^{l})$ with condition $c$ , the Bradley-Terry (BT) model considers a latent reward model $r(x|c)$ behind them and formulates the preference as ", "page_idx": 12}, {"type": "equation", "text": "$$\np(x^{w}\\succ x^{l}|c)=\\frac{\\exp(r(x^{w}|c))}{\\exp(r(x^{w}|c))+\\exp(r(x^{l}|c))}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "RLHF [23] optimizes the generative model by explicitly training a reward model $r_{\\phi}$ , and maximizing the reward with a KL regularization term to control the model by the original reference $p_{\\mathrm{ref}}$ as ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{p_{\\mathrm{opt}}}\\mathbb{E}_{x\\sim p_{\\mathrm{opt}}(x)}[r_{\\phi}(x)]-\\beta D_{\\mathrm{KL}}[p_{\\mathrm{opt}}(x)||p_{\\mathrm{ref}}(x)].\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "We omit the condition $c$ for simplicity. As Eq. (27) exists a close-form solution $p_{\\theta}^{*}(x)~=$ $p_{\\mathrm{ref}}(x)e^{r^{\\ast}(x)/\\beta}/Z$ , where $Z$ is the normalization term, we can rewrite the optimal reward model as ", "page_idx": 12}, {"type": "equation", "text": "$$\nr^{*}(x)=\\beta\\log\\frac{p_{\\mathrm{opt}}(x)}{p_{\\mathrm{ref}}(x)}+\\beta Z.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "After introducing Eq. (28) into Eq. (26) and directly maximizing the log likelihood, DPO [25] simplifies the training objective as ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{DPO}}=-\\mathbb{E}_{x^{w},x^{l}}\\Big[\\log\\sigma\\big(\\beta\\log\\frac{p_{\\mathrm{opt}}(x^{w})}{p_{\\mathrm{ref}}(x^{w})}-\\beta\\log\\frac{p_{\\mathrm{opt}}(x^{l})}{p_{\\mathrm{ref}}(x^{l})}\\big)\\Big].\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "B Required Symmetries of Atomic Systems ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "The design of flow paths is constrained by the symmetry requirements of specific atomic systems, which are detailed as follows. ", "page_idx": 12}, {"type": "text", "text": "Antibody Structure Prediction The designed vector field should maintain equivariance to any rotation $Q\\in S O(3)$ and be invariant to any translation $\\vec{t}\\in\\mathbb R^{3}$ : ", "page_idx": 12}, {"type": "equation", "text": "$$\n{\\vec{u}}_{t}(Q{\\vec{X}}_{t}+{\\vec{t}}|Q{\\vec{X}}_{0}+{\\vec{t}},A,Q{\\vec{X}}^{C}+{\\vec{t}},A^{C})=Q{\\vec{u}}_{t}({\\vec{X}}_{t}|{\\vec{X}}_{0},A,{\\vec{X}}^{C},A^{C}).\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Crystal Structure Prediction Previous works [14, 35] consider the task defined in Eq. (11) as a joint generation task on $\\textbf{\\emph{L}}$ and $\\pmb{F}$ . For the generative process, the vector field of the lattice should be equivariant to an arbitrary rotation $Q\\in\\mathrm{SO(3)}$ , and that of the coordinates is required to ensure the periodic translation invariance for any translation vector $\\pmb{t}$ . Specfically, we have ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r}{u_{L,t}(Q L_{t}|Q L_{0},F_{0},A)=Q u(L_{t}|L_{0},F_{0},A),}\\\\ {u_{F,t}(w(F_{t}+t)|L_{0},w(F_{0}+t),A)=u(L_{t}|L_{0},F_{0},A),\\quad\\quad}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where the operation $w(\\pmb{F})=\\pmb{F}-\\lfloor\\pmb{F}\\rfloor\\in[0,1)^{3\\times N}$ returns the coordinates back to the unit cell. ", "page_idx": 12}, {"type": "text", "text": "After maintaining the required symmetries, the proposed flow model is capable of generating equivalent structures under different E(3) transformations. An example of the OT-OT path for the crystal is shown in Figure 6. ", "page_idx": 12}, {"type": "text", "text": "C Implementation Details ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "C.1 Antibody Structure Prediction ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "We use the framework of DiffAb [20] to train the flow models. The original denoising network in DiffAb requires orientation matrices as input, yet the OT path of the SO(3) matrices is not naive to derive, which is out of our main scope. Therefore, we replace the denoising network with the multi-channel EGNN proposed in MEAN [16] to avoid this problem. All experiments can be run on one GeForce RTX 3090 GPU. Detailed hyperparameters for our FlowDPO are presented in Table 4. ", "page_idx": 12}, {"type": "text", "text": "Figure 6: Visualizations of multiple generated crystals via OT-OT path on MP-20. As the designed path maintain the symmetries, the model is able to generate structures equivalent to the ground truth after proper rotations and (periodic) translations. ", "page_idx": 13}, {"type": "table", "img_path": "EpusiLXfNd/tmp/1e1478b9509e905778c6c93bc9fa42a30344d9591ccfb57b9cfa0b4d12bd7648.jpg", "table_caption": ["Table 4: Hyperparameters for the antibody structure prediction task. "], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "C.2 Crystal Structure Prediction ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We use the denoising network designed in DiffCSP [14] as the backbone model to train the flow models. To predict the vector field via the denoising output, for the OT path designed on lattice, we apply the reparameterization as ", "page_idx": 13}, {"type": "equation", "text": "$$\nv_{L,\\theta}(L_{t},F_{t},A)=\\left\\{\\frac{0,t=1,}{\\underline{{\\epsilon_{L,\\theta}(L_{t},F_{t},A)-L_{t}}},0\\leq t<1.}\\right.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "And for the OT path on the fractional coordinates, we directly use $\\begin{array}{r l}{\\pmb{v}_{F,\\theta}(\\pmb{L}_{t},\\pmb{F}_{t},\\pmb{A})}&{{}=}\\end{array}$ $\\epsilon_{F,\\theta}(L_{t},F_{t},A)$ . We select RMSD defined by StructureMatcher class in pymatgen [22] with thresholds stol=0.5, angle_tol ${\\it=}10\\,$ , $1{\\mathrm{tol}}{=}0.3$ as the distance metric to construct the preference dataset. Specially, the RMSD of the unmatched structure is set as $+\\infty$ , and such candidates will never be selected as the preferred sample. ", "page_idx": 13}, {"type": "text", "text": "The detailed hyperparameters for the FlowDPO pipeline on each crystal dataset are provided in Table 5. Each experiment is run on one GeForce RTX 3090 GPU. ", "page_idx": 13}, {"type": "text", "text": "D Comparison with Regressive Methods ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "To further investigate the advantages of the generative paradigm, we employ the same backbone model (MEAN) for a direct regression task as an additional baseline. The results are presented in Table 6. Our findings indicate that generative models outperform the regression model in 4 of the 6 CDRs, particularly in the highly variable and functionally critical regions, CDR-H3 and CDR-L3. ", "page_idx": 13}, {"type": "table", "img_path": "EpusiLXfNd/tmp/f05b2b2d5d1d9ba7dd1c967c49f7ea3ce396f137784ecb861582eb47379585bc.jpg", "table_caption": ["Table 5: Hyperparameters for the crystal structure prediction task. "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "Additionally, we report both the mean and minimum RMSD values across 20 generations for each generative model. The significantly lower minimum RMSD values demonstrate that generative models not only yield predictions closer to the observed reference structures but also possess the capability to generate multiple feasible conformations around the stable state. ", "page_idx": 14}, {"type": "table", "img_path": "EpusiLXfNd/tmp/a7b927e4a1cd628ded25c9c756b7499bbf72b7818cc3672b0b81b8dde47a2da2.jpg", "table_caption": ["Table 6: Results compare to regressive baselines on antibody structure prediction tasks. "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "E Limitations ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "As detailed in $\\S\\ 3.3$ , the derivation of the rationality of DPO for flow models primarily focuses on Gaussian paths. However, flow models have the potential to learn mappings from an arbitrary prior to the data distribution if the probability path is appropriately defined. Therefore, a more general derivation that does not rely on Gaussian assumptions could be explored in future research. Additionally, our current evaluation is based predominantly on computational metrics. Conducting wet-lab experiments would provide a more robust validation of the model\u2019s effectiveness in practical applications. ", "page_idx": 14}, {"type": "text", "text": "F Broader Impacts ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The introduction of FlowDPO marks a pivotal advancement in scientific domains such as drug development, materials research, and molecular informatics. Recent developments, such as AlphaFold3, have demonstrated remarkable accuracy in predicting structures across various domains [1]. However, issues such as hallucinations, like erroneous structural order in inherently disordered regions, remain a challenge. It is intriguing to explore whether alignment strategies based on DPO can mitigate these hallucinations and enhance overall prediction accuracy. ", "page_idx": 14}, {"type": "text", "text": "G Codes ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Our codes are available at https://github.com/jiaor17/FlowDPO. ", "page_idx": 14}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 15}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Justification: In abstract and introduction, we summarize our contribution as enabling DPO for flow-based structure prediction models, constructing preference dataset to align model predictions with reference, and achieving promising results for antibody and crystal prediction tasks. These claims are detailed and verified by the method (\u00a73) and experiment (\u00a74) sections. ", "page_idx": 15}, {"type": "text", "text": "Guidelines: ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 15}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 15}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Justification: The limitations of this paper are discussed in Appendix E. ", "page_idx": 15}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 15}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Justification: The derivation are provided in $\\S3$ . ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 16}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: The used datasets and evaluation setups are provided in $\\S4$ , and we provide more details in Appendix C. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 16}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: Our codes are provided in Appendix G. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 17}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: The hyperparameters are provided in Appendix C. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 17}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 17}, {"type": "text", "text": "Answer: [No] ", "page_idx": 17}, {"type": "text", "text": "Justification: The dataset used in the experiments are large and the results are relatively stable. Rerunning the pipeline for multiple times is costly. Instead, we further compare the prediction results for different models from the perspective of distribution in Figure 4. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 17}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 18}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: The compute resources are provided in Appendix C. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 18}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Justification: This paper definitely follows the NeurIPS Code of Ethics. Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 18}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: The broader impacts of this paper are discussed in Appendix ??. Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 18}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 19}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 19}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 19}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 19}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Justification: All datasets used in this paper have been properly cited. Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: This paper does not release new assets. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 20}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 20}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 20}]