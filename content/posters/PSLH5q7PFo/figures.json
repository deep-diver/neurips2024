[{"figure_path": "PSLH5q7PFo/figures/figures_7_1.jpg", "caption": "Figure 1: X-RayAge. Performance of active sampling strategies when comparisons are simulated using a logistic model according to (2). In-sample Kendall\u2019s Tau distance R1D on 200 images (left) and generalization error R1E - R1D for models trained on 150 images and evaluated on 150 images from a different distribution (right). All results are averaged over 100 different random seeds.", "description": "The figure shows the performance of different active sampling strategies on an X-ray image ordering task where comparisons are simulated using a logistic model.  The left panel displays the in-sample Kendall\u2019s Tau distance (a measure of ordering error) for models trained on 200 images, as a function of the number of comparisons made. The right panel shows the generalization error (the difference in error between a test set and training set) for models trained on 150 images and tested on another 150 images from a different distribution, again as a function of the number of comparisons made. Error bars representing one standard deviation are shown for all methods.", "section": "6.1 Ordering X-ray images under the logistic model"}, {"figure_path": "PSLH5q7PFo/figures/figures_7_2.jpg", "caption": "Figure 1: X-RayAge. Performance of active sampling strategies when comparisons are simulated using a logistic model according to (2). In-sample Kendall\u2019s Tau distance R<sub>ID</sub> on 200 images (left) and generalization error R<sub>IE</sub> - R<sub>ID</sub> for models trained on 150 images and evaluated on 150 images from a different distribution (right). All results are averaged over 100 different random seeds.", "description": "This figure shows the performance comparison of different active sampling strategies on the X-RayAge dataset. The left panel shows the in-sample Kendall\u2019s Tau distance (a measure of ordering error) for models trained on 200 images, while the right panel presents the generalization error (difference between out-of-sample and in-sample error) for models trained on 150 images and tested on another 150 images from a different distribution.  The results are averaged over 100 independent runs, showing the mean and standard deviation (error bars) for each method. The figure illustrates the sample efficiency and generalization ability of various active learning approaches for ordering problems.", "section": "6.1 Ordering X-ray images under the logistic model"}, {"figure_path": "PSLH5q7PFo/figures/figures_8_1.jpg", "caption": "Figure 2: The empirical error Rp(h) on a holdout comparison set D' when comparisons are made by human annotators. The plots are averaged over 100 (a,b) or 10 (c,d) seeds, and the shaded area represents one standard deviation above and below the mean. For every seed, 10% of comparisons were used for the holdout set. In (d) we initially order a list ID of 3 000 images. After 10 000 comparisons the remaining 3 072 images, I \\ ID, are added.", "description": "This figure compares the performance of different algorithms (Uniform, GURO, BayesGURO, BALD, COLSTIM, and TrueSkill) on three real-world datasets (ImageClarity, WiscAds, and IMDB-WIKI-SbS) and a synthetic dataset with human preference feedback.  The y-axis represents the error rate on a held-out set of comparisons, showing the generalization performance of each algorithm. The x-axis represents the number of comparisons made.  Shaded areas indicate standard deviation.  The IMDB-WIKI-SbS plots also show continual learning (adding new items after initial training). The results demonstrate the superior performance of GURO and GURO Hybrid compared to other algorithms.", "section": "6 Experiments"}, {"figure_path": "PSLH5q7PFo/figures/figures_27_1.jpg", "caption": "Figure 3: Additional figures from the X-RayAge experiment.", "description": "This figure shows two subfigures. Subfigure (a) shows the result of adding the NormMin algorithm to the experiment shown in Figure 1a, demonstrating that not only does NormMin perform worse than GURO, but is also outperformed by uniform sampling. Subfigure (b) shows the in-sample error (RID) during the generalization experiment performed in Figure 1b.", "section": "6.1 Ordering X-ray images under the logistic model"}, {"figure_path": "PSLH5q7PFo/figures/figures_27_2.jpg", "caption": "Figure 1: X-RayAge. Performance of active sampling strategies when comparisons are simulated using a logistic model according to (2). In-sample Kendall\u2019s Tau distance R<sub>ID</sub> on 200 images (left) and generalization error R<sub>IE</sub> - R<sub>ID</sub> for models trained on 150 images and evaluated on 150 images from a different distribution (right). All results are averaged over 100 different random seeds.", "description": "This figure shows the performance of different active learning algorithms for ordering X-ray images based on perceived age.  The left panel displays the in-sample Kendall\u2019s Tau distance (a measure of ordering error) for models trained on 200 images. The right panel shows the generalization error, calculated as the difference between out-of-sample and in-sample Kendall\u2019s Tau distance,  for models trained on 150 images and tested on a separate set of 150 images from a different distribution.  The results are averaged across 100 different random seeds.  The figure highlights the sample efficiency and generalization capabilities of different methods.", "section": "6.1 Ordering X-ray images under the logistic model"}, {"figure_path": "PSLH5q7PFo/figures/figures_27_3.jpg", "caption": "Figure 1: X-RayAge. Performance of active sampling strategies when comparisons are simulated using a logistic model according to (2). In-sample Kendall\u2019s Tau distance RID on 200 images (left) and generalization error RIE - RID for models trained on 150 images and evaluated on 150 images from a different distribution (right). All results are averaged over 100 different random seeds.", "description": "This figure presents the results of an experiment comparing different active sampling strategies for learning an item ordering from pairwise comparisons simulated using a logistic model.  The left panel shows the in-sample performance (Kendall\u2019s Tau distance) of the algorithms on 200 images as a function of the number of comparisons. The right panel shows their out-of-sample generalization performance, which measures the difference between in-sample and out-of-sample ordering errors when trained on 150 images and tested on a separate set of 150 images drawn from a different distribution.  Error bars represent 1 standard deviation, and the results are averaged over 100 random trials.", "section": "6.1 Ordering X-ray images under the logistic model"}, {"figure_path": "PSLH5q7PFo/figures/figures_27_4.jpg", "caption": "Figure 1: X-RayAge. Performance of active sampling strategies when comparisons are simulated using a logistic model according to (2). In-sample Kendall\u2019s Tau distance RID on 200 images (left) and generalization error RIE - RID for models trained on 150 images and evaluated on 150 images from a different distribution (right). All results are averaged over 100 different random seeds.", "description": "This figure shows the performance of different active learning algorithms on a synthetic X-ray image ordering task.  The left panel displays the in-sample Kendall\u2019s Tau distance (a measure of ordering error) for models trained on 200 images. The right panel shows the generalization error (the difference in ordering error between a test set and training set of 150 images each) for models trained on 150 images and tested on 150 unseen images from a different distribution. The results show the average performance and a 1-sigma error region over 100 independent runs, allowing a visualization of the variability.", "section": "6.1 Ordering X-ray images under the logistic model"}, {"figure_path": "PSLH5q7PFo/figures/figures_28_1.jpg", "caption": "Figure 2: The empirical error Rp(h) on a holdout comparison set D\u2032 when comparisons are made by human annotators. The plots are averaged over 100 (a,b) or 10 (c,d) seeds, and the shaded area represents one standard deviation above and below the mean. For every seed, 10% of comparisons were used for the holdout set. In (d) we initially order a list ID of 3 000 images. After 10 000 comparisons the remaining 3 072 images, I \\ ID, are added.", "description": "This figure shows the performance of different algorithms in four image ordering tasks using real-world feedback from human annotators.  The plot shows the empirical error (Rp(h)) on a holdout comparison set. The results are averaged over multiple random seeds, with shaded areas representing standard deviations.  A key aspect highlighted is the continual learning ability, demonstrated in subplot (d), where new items are added after an initial training period.", "section": "6 Experiments"}, {"figure_path": "PSLH5q7PFo/figures/figures_28_2.jpg", "caption": "Figure 2: The empirical error Rp(h) on a holdout comparison set D' when comparisons are made by human annotators. The plots are averaged over 100 (a,b) or 10 (c,d) seeds, and the shaded area represents one standard deviation above and below the mean. For every seed, 10% of comparisons were used for the holdout set. In (d) we initially order a list ID of 3 000 images. After 10 000 comparisons the remaining 3 072 images, I \\ ID, are added.", "description": "This figure presents the results of experiments evaluating the performance of different active learning algorithms on three real-world datasets with human-provided preference feedback.  Each plot shows the ordering error on a held-out set of comparisons as a function of the number of comparisons collected.  The shaded area represents the standard deviation across multiple runs. The IMDB-Wiki-SbS dataset has two subplots showing the error before and after the addition of new items to test generalization.  The algorithms evaluated were Uniform sampling, GURO, BayesGURO, BALD, CoLSTIM, and TrueSkill.", "section": "6 Experiments"}, {"figure_path": "PSLH5q7PFo/figures/figures_29_1.jpg", "caption": "Figure 2: The empirical error R\u0189, (h) on a holdout comparison set D' when comparisons are made by human annotators. The plots are averaged over 100 (a,b) or 10 (c,d) seeds, and the shaded area represents one standard deviation above and below the mean. For every seed, 10% of comparisons were used for the holdout set. In (d) we initially order a list ID of 3 000 images. After 10 000 comparisons the remaining 3 072 images, I \\ ID, are added.", "description": "This figure shows the performance of different algorithms on three real-world datasets with human feedback, evaluating the ordering error on held-out comparisons.  It highlights the difference in performance between fully contextual, hybrid, and non-contextual methods, demonstrating the superior generalization capabilities of hybrid models, particularly when dealing with unseen items. The error bars represent one standard deviation, and the shaded region reflects the uncertainty.", "section": "6 Experiments"}, {"figure_path": "PSLH5q7PFo/figures/figures_29_2.jpg", "caption": "Figure 3: Additional figures from the X-RayAge experiment.", "description": "This figure includes two subfigures. Figure 3a shows the performance of NormMin algorithm in the X-RayAge experiment, demonstrating that it performs worse than GURO and is seemingly outperformed by uniform sampling.  Figure 3b shows the in-sample error (RID) during the generalization experiment presented in Figure 1b, showing similar trends as in Figure 1a.", "section": "6.1 Ordering X-ray images under the logistic model"}]