[{"figure_path": "9hKN99RNdR/figures/figures_1_1.jpg", "caption": "Figure 1: Model-based GCRL Framework", "description": "This figure illustrates the framework of Model-based Goal-Conditioned Reinforcement Learning (GCRL).  It shows how a world model learns the environment's dynamics from real-world interactions. The learned world model is then used to train both a goal-conditioned policy (\u03c0<sup>G</sup>(a|s,g)) for reaching specific goals and an exploration policy (\u03c0<sup>E</sup>(a|s<sub>t</sub>)) for discovering new areas of the state space. The trajectories collected are stored in a replay buffer and used to further improve the world model and policies.  This iterative process allows the agent to effectively explore the environment and learn to achieve diverse tasks.", "section": "Model-based GCRL"}, {"figure_path": "9hKN99RNdR/figures/figures_5_1.jpg", "caption": "Figure 2: We conduct experiments on 6 environments: Point Maze, Ant Maze, Walker, 3-Block Stacking, Block Rotation, Pen Rotation.", "description": "This figure shows six different simulated robotic environments used to evaluate the performance of the proposed Cluster Edge Exploration (CE2) algorithm.  These diverse environments test the algorithm's ability to explore effectively in various scenarios, ranging from simple navigation (Point Maze) to complex manipulation tasks (3-Block Stacking). Each environment presents unique challenges in terms of state space complexity, action space dimensionality, and reward sparsity, allowing for a comprehensive assessment of the algorithm's generalization capabilities.", "section": "Experiments"}, {"figure_path": "9hKN99RNdR/figures/figures_6_1.jpg", "caption": "Figure 3: Experiment results comparing CE2 with the baselines over 5 random seeds.", "description": "This figure displays the mean learning performance of CE2 and three baseline methods (PEG, MEGA, and a variant of CE2) across six different goal-conditioned reinforcement learning tasks.  The x-axis represents the number of steps, and the y-axis represents the success rate (averaged over 5 random seeds). The shaded regions indicate standard deviations.  The figure demonstrates CE2's superior performance in most tasks compared to the baselines, particularly in terms of speed and success rate.", "section": "4.3 Results"}, {"figure_path": "9hKN99RNdR/figures/figures_6_2.jpg", "caption": "Figure 4: Comparison of exploration goals (represented as red points) generated by CE\u00b2, MEGA, and PEG in the Ant Maze environment.", "description": "This figure compares the exploration strategies of three different algorithms: CE\u00b2, MEGA, and PEG, within the Ant Maze environment.  Each algorithm selects goals to guide the agent's exploration, with the goals represented by red points on the maze map. The color gradient indicates the order in which states are visited during exploration, going from green (visited earlier) to yellow (visited later). The figure visually demonstrates how each algorithm approaches exploration differently, showing the distribution and selection of exploration goals.", "section": "4.3 Results"}, {"figure_path": "9hKN99RNdR/figures/figures_7_1.jpg", "caption": "Figure 3: Experiment results comparing CE2 with the baselines over 5 random seeds.", "description": "This figure presents the learning performance of CE2 and three baseline methods (PEG, MEGA, and MEGA-wPEG) across six different goal-conditioned reinforcement learning tasks.  The y-axis represents the success rate (probability of the agent successfully reaching a goal), and the x-axis represents the number of steps taken. The shaded regions around the lines indicate the standard deviation across five runs with different random seeds. This visualization shows the comparative performance of the proposed CE2 algorithm and its baseline counterparts. Note that all six environments are for unsupervised GCRL settings.", "section": "4.3 Results"}, {"figure_path": "9hKN99RNdR/figures/figures_7_2.jpg", "caption": "Figure 6: Cluster evolution in CE2 as the training progresses. The red points means the goals picked by CE2 to explore and other points in different colors represent the clusters CE2 learned.", "description": "This figure visualizes how the state clusters learned by the CE2 algorithm evolve during the training process. Each color represents a different cluster, grouping states easily reachable from one another by the current policy. Red points represent the goals (states) selected by CE2 for exploration.  The figure shows that the exploration goals selected by CE2 are located at the boundaries of state clusters. The visualization helps to understand how the method effectively explores previously unseen state regions in the environment by moving to the boundaries of already-explored regions and building up the clusters over time.", "section": "4.3 Results"}, {"figure_path": "9hKN99RNdR/figures/figures_8_1.jpg", "caption": "Figure 3: Experiment results comparing CE2 with the baselines over 5 random seeds.", "description": "This figure displays the mean learning performance of different unsupervised GCRL methods (CE2, PEG, and MEGA) across six challenging robotics tasks.  The y-axis represents the success rate in achieving goals, averaged over five different random seeds. The x-axis shows the number of steps in the training process.  The figure showcases CE2's superior performance compared to the baselines in most tasks, indicating its effectiveness in improving goal-reaching efficiency by prioritizing accessible goals for exploration.", "section": "4.3 Results"}, {"figure_path": "9hKN99RNdR/figures/figures_15_1.jpg", "caption": "Figure 9: Illustration of differences between our mothod CE2-G and other exploration methods.", "description": "This figure illustrates the key difference between the proposed CE2-G method and other conventional exploration methods. In CE2-G, the agent explores by targeting goal states located at the boundaries of latent state clusters. The clusters represent regions where the agent's current policy is familiar. In contrast, other exploration methods may select goal states randomly in less-explored areas of the state space, even if those states are not easily reachable by the current agent's policy.  The figure shows how CE2-G strategically selects goals along the boundaries of known regions, leading to more efficient and targeted exploration.", "section": "3 State Cluster Edge Exploration"}, {"figure_path": "9hKN99RNdR/figures/figures_22_1.jpg", "caption": "Figure 10: Space explored by CE2 and PEG in the 3-Block Stacking environment at 1M steps. X-axis: the sum of the three sides of the triangle projected on the x-y plane by the three block-connected triangles. Y-axis: sum of heights (z-coordinates) of the three blocks. Red points: evaluation goals. Other points: observations of trajectories sampled in real environment. Color from green to yellow means to be sampled more recent.", "description": "This figure compares the exploration performance of CE2 and PEG in the 3-Block Stacking environment after 1 million steps.  The x-axis represents the sum of the lengths of the sides of a triangle formed by connecting the centers of the three blocks, projected onto the x-y plane. The y-axis represents the sum of the z-coordinates (heights) of the three blocks.  The plot shows the distribution of visited states, with the color gradient indicating the recency of visits (green for older, yellow for more recent). Red points represent the evaluation goals.  The figure highlights that CE2 explores more effectively around the target goal region, while PEG's exploration is more dispersed.", "section": "H.1 Space explored image for 3-Block Stacking"}, {"figure_path": "9hKN99RNdR/figures/figures_23_1.jpg", "caption": "Figure 11: Comparison of exploration goals generated by CE2, MEGA and PEG", "description": "This figure compares the exploration goals generated by three different algorithms: CE2, MEGA, and PEG, across various stages (10%, 20%, 30%, 40%, 60%) of the training process in the Ant Maze environment.  Each subplot shows the maze layout and the locations of the exploration goals (red points) generated by each algorithm at a specific training percentage.  The figure visually demonstrates the differences in exploration strategies, highlighting CE2's tendency to select goals near the frontiers of explored areas compared to MEGA and PEG, which often select goals further away from the currently known areas.", "section": "4.3 Results"}, {"figure_path": "9hKN99RNdR/figures/figures_23_2.jpg", "caption": "Figure 3: Experiment results comparing CE2 with the baselines over 5 random seeds.", "description": "This figure presents the learning performance comparison of CE2 against other unsupervised GCRL methods (PEG and MEGA) across six different challenging robotics environments.  The success rate (y-axis) is plotted against the number of steps (x-axis).  The plots show the mean success rate with shaded regions representing the standard deviation across five random seeds for each method.  The results highlight CE2's superior performance in most environments, demonstrating its improved exploration and goal-reaching capabilities.", "section": "4.3 Results"}, {"figure_path": "9hKN99RNdR/figures/figures_24_1.jpg", "caption": "Figure 3: Experiment results comparing CE2 with the baselines over 5 random seeds.", "description": "This figure presents the experimental results comparing the performance of CE2 with several baseline methods across six different robotic tasks. Each subplot represents a specific task (Point Maze, Ant Maze, Walker, 3-Block Stacking, Block Rotation, Pen Rotation). The x-axis represents the number of steps, and the y-axis shows the success rate. The lines represent the average success rate over five different random seeds for each algorithm, while the shaded area depicts the standard deviation, showcasing the robustness of each method's performance. The figure visually demonstrates that CE2 generally outperforms the baseline methods (PEG and MEGA) in terms of both speed and success rate, particularly in more complex tasks such as 3-Block Stacking, Block Rotation and Pen Rotation.", "section": "4.3 Results"}, {"figure_path": "9hKN99RNdR/figures/figures_25_1.jpg", "caption": "Figure 14: Ablation Study with Different Cluster Number.", "description": "This figure presents the results of ablation studies conducted to assess the impact of varying the number of clusters (10, 30, 50) used in the CE2 algorithm on the success rate of goal-reaching tasks across different environments. Each subfigure corresponds to a specific environment: (a) 3-Block Stacking, (b) Ant Maze, (c) Point Maze, (d) Block Rotation, (e) Pen Rotation, and (f) Walker. The x-axis represents the number of steps taken, while the y-axis shows the success rate.  The results indicate that CE2's performance is relatively insensitive to the choice of cluster number within the tested range, suggesting robustness of the approach.", "section": "H.5 More Ablation Experiments"}]