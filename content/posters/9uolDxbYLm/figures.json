[{"figure_path": "9uolDxbYLm/figures/figures_1_1.jpg", "caption": "Figure 1: Decision boundary shift when counterfactuals are treated as ordinary labeled points.", "description": "This figure illustrates the problem of decision boundary shift in model reconstruction using counterfactuals.  The black curve represents the decision boundary of the original target model. When counterfactuals (represented by black triangles) are treated as ordinary labeled points during surrogate model training, the surrogate model's decision boundary (red dashed line) can shift significantly from the target model's boundary. This shift is particularly problematic when counterfactuals are one-sided (i.e., only available for instances with unfavorable predictions).  The figure shows the shift causing misclassifications near the decision boundary.", "section": "1 Introduction"}, {"figure_path": "9uolDxbYLm/figures/figures_3_1.jpg", "caption": "Figure 2: Problem setting", "description": "The figure illustrates the model reconstruction problem setting. A target machine learning model is hosted on a Machine Learning as a Service (MLaaS) platform, accessible through an API. An adversary (user) can query the model with a set of input instances (D). The model returns the predictions for these instances and, for the instances with unfavorable predictions, provides corresponding counterfactuals. The adversary aims to build a surrogate model that closely mimics the target model based on these queries and counterfactuals. This setup highlights the interplay between model explainability and privacy.", "section": "2 Preliminaries"}, {"figure_path": "9uolDxbYLm/figures/figures_4_1.jpg", "caption": "Figure 3: Polytope approximation of a convex decision boundary using the closest counterfactuals.", "description": "This figure illustrates how a convex decision boundary can be approximated using a polytope constructed from supporting hyperplanes obtained through counterfactual queries.  The target model's decision boundary (a curve) is approximated by a polygon formed by the intersections of tangent hyperplanes at each of the closest counterfactuals. The figure highlights the queries, the resulting counterfactuals (one-sided), and the resulting polytope approximation.", "section": "3.1 Convex decision boundaries and closest counterfactuals"}, {"figure_path": "9uolDxbYLm/figures/figures_4_2.jpg", "caption": "Figure 4: Approximating a concave region needs denser queries w.r.t. a convex region.", "description": "This figure illustrates the difference in query density required for approximating convex versus concave decision boundaries.  In a convex region, the tangent hyperplanes obtained from closest counterfactuals are relatively well-spaced. However, in a concave region, due to length contraction, a much denser set of query points is needed to obtain equally spaced tangent hyperplanes that provide an accurate approximation of the decision boundary.", "section": "3.1 Convex decision boundaries and closest counterfactuals"}, {"figure_path": "9uolDxbYLm/figures/figures_5_1.jpg", "caption": "Figure 5: Ne grid and inverse counterfactual regions. Thick solid lines indicate the decision boundary pieces (Hi's). White color depicts the accepted region. Pale-colored are the inverse counterfactual regions of the Hi's with the matching color. In this case k(e) = 7 and v*(e) is the area of lower amber region.", "description": "This figure illustrates the concept of inverse counterfactual regions within a uniform grid. The decision boundary is broken into linear pieces within each cell. The inverse counterfactual region for a piece of the decision boundary (Hi) is the set of points whose closest counterfactuals fall within Hi.  The figure visually demonstrates how the volume of these inverse counterfactual regions (vi(e)) influences the probability of successful model reconstruction. The area of the lower amber region represents the minimum volume (v*(e)) across all regions.", "section": "3.2 ReLU networks and closest counterfactuals"}, {"figure_path": "9uolDxbYLm/figures/figures_6_1.jpg", "caption": "Figure 6: Rationale for Counterfactual Clamping Strategy.", "description": "This figure illustrates the core idea behind the Counterfactual Clamping Attack (CCA). The target model's decision boundary (red curve) and surrogate model's decision boundary (blue curve) are shown.  The green circles represent the counterfactuals, which lie near the decision boundary. The goal is to force the surrogate model to make similar predictions to the target model around these counterfactuals by using a unique loss function. The dotted lines connect the counterfactuals to their corresponding instances. The loss function penalizes the surrogate model only if its prediction is lower than a threshold for the counterfactuals, ensuring that the surrogate model's decision boundary remains close to that of the target model for counterfactuals.", "section": "3.3 Beyond closest counterfactuals"}, {"figure_path": "9uolDxbYLm/figures/figures_8_1.jpg", "caption": "Figure 7: A 2-D demonstration of the proposed strategy. Orange and blue shades denote the favorable and unfavorable decision regions of each model. Circles denote the target model's training data.", "description": "This figure provides a visual comparison of three different models on a 2D synthetic dataset: the target model, the baseline model (from A\u00efvodji et al., 2020), and the CCA model (the authors' proposed method).  It illustrates how the baseline model suffers from decision boundary shift, while the CCA model effectively mitigates this issue, resulting in a more accurate approximation of the target model's decision boundary. The colors represent the predicted class for each region (orange for one class, blue for the other), and the different symbols show the types of data points used: training data, counterfactuals, and queries.  The figure demonstrates the efficacy of the CCA model in reconstructing the target model's decision boundary by using a modified loss function that addresses the issue of decision boundary shifts. ", "section": "4 Experiments"}, {"figure_path": "9uolDxbYLm/figures/figures_13_1.jpg", "caption": "Figure 9: Line joining the query and its closest counterfactual is perpendicular to the decision boundary at the counterfactual. See Lemma 3.1 for details.", "description": "The figure shows a geometrical illustration of Lemma 3.1.  A query point is shown with its closest counterfactual on a decision boundary. A line connecting the query and counterfactual is shown perpendicular to the tangent of the decision boundary at the point of the counterfactual. This illustrates that the line connecting a query point and its closest counterfactual is perpendicular to the decision boundary at the counterfactual point.", "section": "3 Main Results"}, {"figure_path": "9uolDxbYLm/figures/figures_18_1.jpg", "caption": "Figure 7: A 2-D demonstration of the proposed strategy. Orange and blue shades denote the favorable and unfavorable decision regions of each model. Circles denote the target model\u2019s training data.", "description": "This figure compares the decision boundaries of three models: the target model, a baseline model, and the CCA model.  The orange and blue regions represent the positive and negative classes, respectively, for each model. The target model\u2019s training data points are shown as circles. The plot shows how the baseline model suffers from decision boundary shift, while CCA model produces a decision boundary that is more aligned with the target model, indicating improved model reconstruction.", "section": "4 Experiments"}, {"figure_path": "9uolDxbYLm/figures/figures_19_1.jpg", "caption": "Figure 7: A 2-D demonstration of the proposed strategy. Orange and blue shades denote the favorable and unfavorable decision regions of each model. Circles denote the target model\u2019s training data.", "description": "This figure compares the decision boundaries of three models in a two-dimensional space: the target model, a baseline model trained with a standard loss function, and a CCA (Counterfactual Clamping Attack) model trained with the proposed loss function.  The different colored regions represent the favorable and unfavorable regions of each model. The plot illustrates how the CCA model better approximates the decision boundary of the target model compared to the baseline model, which demonstrates the effectiveness of CCA in mitigating decision boundary shift.", "section": "4 Experiments"}, {"figure_path": "9uolDxbYLm/figures/figures_19_2.jpg", "caption": "Figure 7: A 2-D demonstration of the proposed strategy. Orange and blue shades denote the favorable and unfavorable decision regions of each model. Circles denote the target model\u2019s training data.", "description": "This figure shows a 2D visualization comparing the target model\u2019s decision boundary with those of the baseline model and the CCA model.  The orange and blue shades represent the favorable and unfavorable regions for each model.  Circles indicate the training data points from the original target model.  The figure visually demonstrates how the CCA model\u2019s decision boundary is much closer to the target model\u2019s boundary, in contrast to the baseline model, which exhibits a larger decision boundary shift.", "section": "4 Experiments"}, {"figure_path": "9uolDxbYLm/figures/figures_20_1.jpg", "caption": "Figure 7: A 2-D demonstration of the proposed strategy. Orange and blue shades denote the favorable and unfavorable decision regions of each model. Circles denote the target model's training data.", "description": "This figure visualizes the performance of the proposed Counterfactual Clamping Attack (CCA) method compared to a baseline method on a synthetic 2D dataset.  The orange and blue regions represent the favorable and unfavorable prediction regions, respectively, for each model.  The plot shows that the baseline model's decision boundary shifts away from the true boundary, indicating a problem with existing methods. In contrast, CCA's decision boundary better approximates the true boundary, illustrating its effectiveness.", "section": "4 Experiments"}, {"figure_path": "9uolDxbYLm/figures/figures_20_2.jpg", "caption": "Figure 14: A comparison of the query complexity derived in Theorem 3.2 with the empirical query complexities obtained on the Adult Income and HELOC datasets. The graphs are on a log-log scale. We observe that the analytical query complexity is an upper bound for the empirical query complexities. All the graphs are recentered with an additive constant for presentational convenience. However, this does not affect the slope of the graph, which corresponds to the complexity.", "description": "This figure compares the theoretical query complexity from Theorem 3.2 with empirical results from Adult Income and HELOC datasets.  The log-log scale graphs show that the theoretical complexity provides an upper bound for the empirical complexities found in the experiments. Note that a constant was added to the graphs for presentation purposes, which does not change the slope of the lines representing the complexities.", "section": "B.2.3 Empirical and theoretical rates of convergence"}, {"figure_path": "9uolDxbYLm/figures/figures_21_1.jpg", "caption": "Figure 7: A 2-D demonstration of the proposed strategy. Orange and blue shades denote the favorable and unfavorable decision regions of each model. Circles denote the target model's training data.", "description": "This figure visually demonstrates the effectiveness of the proposed counterfactual clamping (CCA) strategy compared to a baseline method.  A synthetic 2D dataset is used to illustrate how CCA mitigates decision boundary shift. The orange and blue regions represent the favorable and unfavorable regions predicted by each model (target, baseline, and CCA). The circles represent the training data points used to train the target model.  The figure clearly shows that the CCA model's decision boundary (blue/orange separation) is a much closer approximation to the target model's decision boundary than the baseline model's decision boundary.  This highlights the CCA strategy's improved fidelity in reconstructing the target model's behavior.", "section": "4 Experiments"}, {"figure_path": "9uolDxbYLm/figures/figures_23_1.jpg", "caption": "Figure 7: A 2-D demonstration of the proposed strategy. Orange and blue shades denote the favorable and unfavorable decision regions of each model. Circles denote the target model's training data.", "description": "This figure shows a 2D visualization comparing the target model, the baseline model (without counterfactual clamping), and the CCA model (with counterfactual clamping).  The orange and blue regions represent the favorable and unfavorable prediction regions of each model. The plots illustrate how the baseline model suffers from decision boundary shift while CCA improves model reconstruction fidelity by better aligning the surrogate model's decision boundary with that of the target model.", "section": "4 Experiments"}, {"figure_path": "9uolDxbYLm/figures/figures_24_1.jpg", "caption": "Figure 7: A 2-D demonstration of the proposed strategy. Orange and blue shades denote the favorable and unfavorable decision regions of each model. Circles denote the target model's training data.", "description": "This figure visualizes the performance of the proposed Counterfactual Clamping Attack (CCA) strategy compared to a baseline method on a 2D synthetic dataset. The orange and blue regions represent the favorable and unfavorable prediction areas, respectively. The circles indicate the target model's training data. The figure demonstrates that CCA effectively mitigates decision boundary shift, a common problem in model reconstruction approaches that use counterfactuals as ordinary data points. In contrast to the baseline method, CCA produces a surrogate model with a decision boundary that closely aligns with the target model.", "section": "4 Experiments"}, {"figure_path": "9uolDxbYLm/figures/figures_25_1.jpg", "caption": "Figure 13: Histograms of probabilities predicted by \u201cBaseline\u201d and \u201cCCA\u201d models under the \u201cUnknown Architecture\u201d scenario (model 1) for the HELOC dataset. Note how the \u201cBaseline\u201d model provides predictions higher than 0.5 for a comparatively larger number of instances with [m(x)] = 0 due to the boundary shift issue. The clamping effect of the novel loss function is evident in the \u201cCCA\u201d model\u2019s histogram, where the decision boundary being held closer to the counterfactuals is causing the two prominent modes in the favorable region. The mode closer to 0.5 is due to counterfactuals and the mode closer to 1.0 is due to instances with [m(x)] = 1.", "description": "This figure compares the probability distributions of predictions made by the baseline model and the CCA model for the HELOC dataset. It highlights how CCA mitigates the decision boundary shift, resulting in a distribution where counterfactuals are clustered around 0.5, and other instances are around 1.0.", "section": "B.2.3 Empirical and theoretical rates of convergence"}, {"figure_path": "9uolDxbYLm/figures/figures_25_2.jpg", "caption": "Figure 3: Polytope approximation of a convex decision boundary using the closest counterfactuals.", "description": "This figure illustrates how a convex decision boundary can be approximated using a polytope.  The target model's decision boundary (a curve) is shown in dark blue.  Red dots represent one-sided counterfactual queries (points for which the model prediction was unfavorable). Blue dots represent the corresponding closest counterfactuals (points that yield a favorable outcome with minimum perturbation).  The dashed black line shows the polytope approximation of the decision boundary which is created by the intersection of the tangent hyperplanes at each counterfactual point. The shaded green region highlights the area where the approximation differs from the target model's decision boundary.", "section": "Background on Geometry of Decision Boundaries"}, {"figure_path": "9uolDxbYLm/figures/figures_26_1.jpg", "caption": "Figure 20: Verifying Theorem 3.2: Dotted and solid lines indicate the theoretical and empirical rates of convergence.", "description": "This figure validates Theorem 3.2 by comparing the theoretical and empirical rates of convergence of the approximation error (epsilon) with the number of queries (n) for different dimensionality values (d). The dotted lines represent the theoretical rates, and the solid lines represent the empirical results obtained from experiments.  The plot uses logarithmic scales for both epsilon and n, allowing for better visualization of the convergence behavior. The results show that the empirical convergence rates generally follow the predicted theoretical rates, with some deviation observed at higher dimensions.", "section": "B.3 Empirical and theoretical rates of convergence"}]