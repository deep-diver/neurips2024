[{"figure_path": "rog0J435OO/tables/tables_13_1.jpg", "caption": "Table 1: Training Hyperparameters for Various Scales of LLaMA2 Models.", "description": "This table lists the hyperparameters used for training different sizes of the LLaMA2 model.  The hyperparameters are consistent across model sizes to ensure fair comparison of FlashMask's performance.  The parameters listed include batch size, gradient accumulation steps, and various sharding degrees for different parallelism strategies used for training.", "section": "4.1 Setup"}]