[{"figure_path": "zZVqZRXSao/tables/tables_7_1.jpg", "caption": "Table 1: Performance comparison (mAP@All) between ours and other baseline methods on Office-31 and DomainNet in Close-set Unsupervised Cross-Domain Retrieval. We blue and bold the best performance, and bold the second best, same for all tables.", "description": "This table presents a comparison of the mean Average Precision (mAP) at all retrieved results (mAP@All) for different cross-domain retrieval methods on the Office-31 and DomainNet datasets.  The methods are compared in a close-set unsupervised setting, meaning that the category spaces of the query and retrieval domains are identical. The table shows the performance of various methods across different domain pairs (e.g., A\u2192D, A\u2192W, etc.) and highlights the best-performing methods.", "section": "4 Experiments"}, {"figure_path": "zZVqZRXSao/tables/tables_8_1.jpg", "caption": "Table 2: Performance comparison (mAP@All) between ours and other baseline methods on Office-Home in Partial Unsupervised Cross-Domain Retrieval.", "description": "This table presents a comparison of the mean Average Precision (mAP@All) achieved by the proposed UEM method and several other baseline methods on the Office-Home dataset.  The experiment setting is Partial Unsupervised Cross-Domain Retrieval, meaning that the query domain contains only half of the label space of the retrieval domain, and the query label space is randomly selected. The table shows the performance of each method across multiple domain pairs, highlighting the superior performance of the proposed UEM method.", "section": "4.1 Effectiveness of UEM When Solving U2CDR"}, {"figure_path": "zZVqZRXSao/tables/tables_8_2.jpg", "caption": "Table 3: Performance comparison (mAP@All for shared-label set, detection accuracy for open-label set) between ours and other baseline methods on DomainNet in Open-set Unsupervised Cross-Domain Retrieval.", "description": "This table presents a performance comparison of different methods on the DomainNet dataset for open-set unsupervised cross-domain retrieval.  Two metrics are used: mean Average Precision (mAP@All) for the shared labels and detection accuracy for open (private) labels.  It compares the proposed method (Ours) against several state-of-the-art baselines (CDS, PCS, DARL, DN2A, UCDIR, CODA, DGDIR) across multiple domain pairs.", "section": "4 Experiments"}, {"figure_path": "zZVqZRXSao/tables/tables_8_3.jpg", "caption": "Table 4: Ablation studies of UEM on Office-31, Office-Home, and DomainNet in Open-set Unsupervised Cross-Domain Retrieval. The average values of Shared-set mAP@All and Open-set Acc for all domain pairs of a single dataset are reported here.", "description": "This table presents the ablation study results of the proposed UEM model on three datasets (Office-31, Office-Home, DomainNet) in the open-set unsupervised cross-domain retrieval setting.  It shows the impact of removing key components of the UEM framework (IPM, SEL, SPDA, SN2M) on the overall performance, measured by shared-set mAP@All and open-set accuracy.", "section": "4.2 Ablation Study"}, {"figure_path": "zZVqZRXSao/tables/tables_15_1.jpg", "caption": "Table 1: Performance comparison (mAP@All) between ours and other baseline methods on Office-31 and DomainNet in Close-set Unsupervised Cross-Domain Retrieval. We blue and bold the best performance, and bold the second best, same for all tables.", "description": "This table presents a comparison of the mean Average Precision (mAP@All) achieved by the proposed UEM method and several other state-of-the-art baseline methods on two benchmark datasets, Office-31 and DomainNet.  The experiments were conducted using a close-set unsupervised cross-domain retrieval setting, meaning the category spaces of the query and retrieval domains were identical. The best and second-best performing methods for each task are highlighted in blue and bold, respectively.", "section": "4 Experiments"}, {"figure_path": "zZVqZRXSao/tables/tables_16_1.jpg", "caption": "Table 6: Performance comparison (mAP@All) between ours and other baseline methods on Office-31 and DomainNet in Partial Unsupervised Cross-Domain Retrieval.", "description": "This table compares the performance (measured by mean Average Precision at All ranks, or mAP@All) of the proposed UEM method against other state-of-the-art methods on two benchmark datasets (Office-31 and DomainNet) in the context of Partial Unsupervised Cross-Domain Retrieval.  Partial retrieval means that the query domain only contains half of the labels present in the retrieval domain. The table shows the results for different cross-domain retrieval tasks (e.g., A\u2192D represents retrieving from domain D using queries from domain A).  The results help demonstrate the effectiveness of UEM in scenarios where there's an incomplete overlap in the category spaces of the two domains.", "section": "4.1 Effectiveness of UEM When Solving U2CDR"}, {"figure_path": "zZVqZRXSao/tables/tables_16_2.jpg", "caption": "Table 1: Performance comparison (mAP@All) between ours and other baseline methods on Office-31 and DomainNet in Close-set Unsupervised Cross-Domain Retrieval. We blue and bold the best performance, and bold the second best, same for all tables.", "description": "This table compares the performance (measured by mean Average Precision at All ranks, or mAP@All) of the proposed UEM method against several state-of-the-art baseline methods on two benchmark datasets, Office-31 and DomainNet. The experiments are conducted using a close-set unsupervised cross-domain retrieval setting, where the category spaces of the query and retrieval domains are identical.  The best and second-best performing methods are highlighted in blue and bold, respectively. This demonstrates UEM's effectiveness in this specific scenario.", "section": "4 Experiments"}, {"figure_path": "zZVqZRXSao/tables/tables_17_1.jpg", "caption": "Table 1: Performance comparison (mAP@All) between ours and other baseline methods on Office-31 and DomainNet in Close-set Unsupervised Cross-Domain Retrieval. We blue and bold the best performance, and bold the second best, same for all tables.", "description": "This table presents a comparison of the mean Average Precision at all ranks (mAP@All) achieved by the proposed method and several other state-of-the-art methods for close-set unsupervised cross-domain retrieval (UCDR).  The comparison is performed on two datasets: Office-31 and DomainNet.  Close-set UCDR implies that the category spaces of the query and retrieval domains are identical. The best performing method for each setting is highlighted in blue and bold, with the second-best highlighted in bold.", "section": "4 Experiments"}]