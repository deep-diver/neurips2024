[{"type": "text", "text": "Semantic Feature Learning for Universal Unsupervised Cross-Domain Retrieval ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Lixu Wang Xinyu Du Northwestern University, IL, USA General Motors Global R&D, MI,USA ixuwang2025@u.northwestern.edu xinyu.du@gm.com ", "page_idx": 0}, {"type": "text", "text": "Qi Zhu Northwestern University, IL, USA qzhu@northwestern.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Cross-domain retrieval (CDR) is finding increasingly broad applications across various domains. However, existing efforts have several major limitations, with the most critical being their reliance on accurate supervision. Recent studies thus focus on achieving unsupervised CDR, but they typically assume that the category spaces across domains are identical, an assumption that is often unrealistic in real-world scenarios. This is because only through dedicated and comprehensive analysis can the category composition of a data domain be obtained, which contradicts the premise of unsupervised scenarios. Therefore, in this work, we introduce the problem of Universal Unsupervised Cross-Domain Retrieval $({\\tt U}^{2}{\\tt C D R})$ for the first time and design a two-stage semantic feature learning framework to address it. In the first stage, a cross-domain unified prototypical structure is established under the guidance of an instance-prototype-mixed contrastive loss and a semantic-enhanced loss, to counteract category space differences. In the second stage, through a modified adversarial training mechanism, we ensure minimal changes for the established prototypical structure during domain alignment, enabling more accurate nearest-neighbor searching. Extensive experiments across multiple datasets and scenarios, including close-set, partial, and open-set CDR, demonstrate that our approach significantly outperforms existing state-of-the-art CDR methods and other related methods in solving ${\\tt U}^{2}{\\tt C D R}$ challenges. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In real-world applications, cross-domain retrieval (CDR) finds extensive utility across diverse domains, such as image search [1], product recommendations [2], and artistic creation [3, 4]. However, the efficacy of current CDR methods relies heavily on accurate and sufficient supervision [5, 6] to provide categorical or cross-domain pairing labels. The acquisition of such information demands costly efforts and resources. Hence, there is an urgent need to develop unsupervised CDR techniques. ", "page_idx": 0}, {"type": "text", "text": "For the regular Unsupervised CDR (UCDR) problem [7, 8], there are two data domains with semantic similarity but distinct characteristics: the query domain and the retrieval domain. Despite the absence of category labels, regular UCDR typically assumes that the label spaces of both domains are identical. However, in real-world applications [5], the categorical composition of an unlabeled data domain is usually uncertain, which is hard to acquire without detailed analysis and dedicated expertise. In this work, we focus on extending UCDR to more universal scenarios, which allow for the possibility of disparate category spaces across domains. The objective of this Universal UCDR $(\\mathtt{U}^{2}\\mathtt{C D R})$ problem is to retrieve samples from the retrieval domain that share the same category label with a query sample from the query domain. Naturally, in the case of private categories exclusive to the query domain, the retrieval result should be null. ", "page_idx": 0}, {"type": "image", "img_path": "zZVqZRXSao/tmp/e9c6cc10e772a8c9ba9290aa6e640aefb7f45451798180b5783663b2ccb1259f.jpg", "img_caption": ["Figure 1: Overview of our proposed UEM semantic feature learning framework for ${\\tt U}^{2}{\\tt C D R}$ . In the first stage of Intra-Domain Semantic-Enhanced Learning, UEM establishes a unified prototypical structure across domains, which is driven and enhanced by an instance-prototype-mixed contrastive loss and a semantic-enhanced loss. In the second stage of Cross-Domain Semantic-Matched Learning, SemanticPreserving Domain Alignment aligns domains while preserving the built prototypical structure, and Switchable Nearest Neighboring Match achieves more accurate cross-domain categorical pairing. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Two issues must be addressed in solving traditional UCDR: 1) effectively distinguishing data samples in each domain, and 2) achieving alignment across domains for samples of the same category. For the first issue, self-supervised learning [9, 10] (SSL) is employed independently within each domain. For the second, many nearest-neighbor searching algorithms may apply. However, for ${\\tt U}^{2}{\\tt C D R}$ , applying these existing methods introduces new challenges. First, the prevailing SSL methods, particularly contrastive learning [9, 11, 10], are highly influenced by the category label space [12], which means different label spaces lead to distinct semantic structures. Second, existing nearest-neighbor searching algorithms [13, 14, 7] overlook the presence of domain gaps. We found that only by first addressing the domain gap can the nearest neighbor searching become reliable and accurate. ", "page_idx": 1}, {"type": "text", "text": "Thus, to effectively address the above challenges in solving ${\\tt U}^{2}{\\tt C D R}$ , we propose a two-stage Unified, Enhanced, and Matched (UEM) semantic feature learning framework, as in Figure 1. In the first stage, we establish a cross-domain unified prototypical structure with an instance-prototype-mixed (IPM) contrastive loss, accompanied by a semantic-enhanced loss (SEL). In the second stage, before conducting cross-domain category alignment, we incorporate Semantic-Preserving Domain Alignment (SPDA) to diminish the domain gap while ensuring minimal changes for the established prototypical structure. As the domain gap diminishes, we propose Switchable Nearest Neighboring Match $(\\mathtt{S N}^{2}\\mathtt{M})$ , to select more reliable cross-domain neighbors based on the relationship between instances and prototypes. Extensive experiments and ablation studies on popular benchmark datasets demonstrate that our method can substantially outperform state-of-the-art methods from UCDR and other related problems. In addition, we also theoretically analyze the principles behind the major challenges of $\\mathsf{\\dot{U}}^{2}\\mathsf{C D R}$ and the design intuition of UEM. In summary, this work made the following contributions: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We are the first to identify and solve an important problem when employing UCDR in practice \u2013 Universal UCDR $({\\tt U}^{2}{\\tt C D R})$ , where the category spaces of different domains are distinct. \u2022 We propose a two-stage Unified, Enhanced, and Matched (UEM) semantic feature learning framework to solve $\\mathrm{U^{2}C D R}$ . In the first stage, UEM establishes a unified prototypical structure across domains, to ensure consistent semantic learning under category space differences. In the second stage, UEM achieves more effective domain alignment and cross-domain pairing. \u2022 We conduct extensive experiments on multiple benchmark datasets, with settings including Closeset, Partial, and Open-set UCDR. The results demonstrate that UEM can substantially outperform state-of-the-art methods of UCDR and other potential solutions in all settings. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Cross-Domain Retrieval (CDR) is not very difficult to achieve if there are categorical labels [15, 16]. However, in real-world applications, such categorical labeling information is hard to acquire, thus more recent works [7, 8, 17] focus on achieving unsupervised CDR (UCDR). CDS [14] proposes a contrastive learning-based cross-domain pre-training to align different domains. PCS [13] incorporates prototype contrastive learning [11] into the cross-domain pre-training. Recent studies also search ways like clustering [7], pseudo-labeling [18], classifier mixup [17], and data augmentation [8] to achieve more advanced CDR. However, all these UCDR works assume that the query and retrieval domains share the same category space. Although there is a study [19] that can achieve CDR with distinct categories, its effectiveness relies on accurate and sufficient data labels. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Universal Cross-Domain Learning. Cross-domain learning consists of domain adaptation (DA) and domain generalization (DG) [20]. Regular DA and DG also only consider the scenario where the label space of the target domain is the same as the source label space, which is termed Closeset DA/DG. Recently, more studies have realized that the target label space may be a subset of the source one (i.e., Partial DA/DG) [21, 22] or contain some private labels that other domains do not have (i.e., Open-set DA/DG) [23, 24]. To deal with more universal setups, UniDA [25] unifies entropy and domain similarity to quantify sample transferability across domains. CMU [26] extends transferability quantification into entropy, consistency, and confidence. More recent works search ways like clustering [27, 28] and nearest neighbor matching [29, 30] to achieve universal DA/DG. In addition, some studies appear to achieve unsupervised DG where the source domain is also unlabeled [31, 32], which is similar to the setup of UCDR. However, these studies consider the classification task and cannot effectively work in image retrieval, especially in completely unsupervised cases. ", "page_idx": 2}, {"type": "text", "text": "3 Methodology ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Problem Formulation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "idnostmaanicness ,a rwe hpircohv iadree dd easn outnelda base In the problem of $\\mathrm{U^{2}C D R}$ , we assume there are two domains characterized by $\\mathcal{D}^{\\mathrm{A}}\\,{=}\\,\\{\\pmb{x}_{i}^{\\mathrm{A}}\\}_{i=1}^{N^{\\mathrm{A}}}$ acantde $\\mathcal{D}^{\\mathrm{B}}\\,{=}\\,\\{\\mathbf{x}_{i}^{\\mathrm{B}}\\}_{i=1}^{N^{\\mathrm{B}}}$ ,a rsessupmeec ttihveeilry .l aAbletlh sopuagche st $N^{\\mathrm{A}}$ and $N^{\\mathrm{B}}$ unlabeled $\\mathcal{V}^{\\mathrm{A}},\\mathcal{V}^{\\mathrm{B}}$ consist of $C^{\\mathrm{\\hat{A}}}$ and $C^{\\mathrm{B}}$ different categories, and there is a relationship that $C^{\\mathrm{A}}\\neq C^{\\mathrm{B}},\\mathcal{Y}^{\\mathrm{A}}\\cap\\mathcal{Y}^{\\mathrm{B}}\\neq$ $\\mathcal{V}^{\\mathrm{A}}\\cup\\mathcal{V}^{\\mathrm{B}}$ . Without losing generality, if we regard domain A as the query domain, while domain B is the retrieval domain, the objective of ${\\tt U}^{2}{\\tt C D R}$ is to retrieve correct data from domain B that belongs to the same categories as the query data provided by domain A. To achieve this objective, it is required to train a valid feature extractor $f_{\\theta}:\\mathcal{X}\\rightarrow\\mathcal{R}$ that can map both these two domains from the input space $\\mathcal{X}$ to a feature space $\\mathcal{R}$ . Then the retrieval process $\\operatorname{\\prime}_{R(f_{\\theta},\\,x_{i}^{\\mathrm{A}})}$ is shaped like for a particular query instance $\\pmb{x}_{i}^{\\mathrm{A}}$ with the label $y_{i}^{\\mathrm{A}}$ from domain A, the representation distance between all instances in domain B and $\\pmb{x}_{i}^{\\mathrm{A}}$ needs to be calculated to form a set, i.e., $\\begin{array}{r}{S=\\{\\mathrm{d}(f(\\pmb{x}_{j}^{\\mathrm{B}}),f(\\pmb{x}_{i}^{\\mathrm{A}}))\\}_{j=1}^{N^{\\mathrm{B}}}}\\end{array}$ where $\\mathrm{d}(\\cdot)$ is a particular distance metric (e.g., Euclidean Distance), and we have ", "page_idx": 2}, {"type": "equation", "text": "$$\nR(f_{\\theta},\\mathbf{{x}}_{i}^{\\mathrm{{A}}})=\\left\\{\\begin{array}{l l}{\\mathrm{null},\\,\\mathrm{if}\\,y_{i}^{\\mathrm{A}}\\in\\mathcal{y}^{\\mathrm{A}}\\setminus\\mathcal{y}^{\\mathrm{B}}}\\\\ {\\mathrm{sort}_{\\uparrow}(\\mathcal{S})[1:k],\\,\\mathrm{otherwise},}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\mathrm{sort}_{\\uparrow}(\\cdot)$ means ascending order sorting, and $[1:k]$ denotes the first $k$ elements of a set. ", "page_idx": 2}, {"type": "text", "text": "Method Overview. To solve $\\mathtt{U}^{2}\\mathtt{C D R}$ , we propose a Unified, Enhanced, and Matched (UEM) semantic feature learning framework that consists of two stages \u2013 Intra-Domain Semantic-Enhanced (IDSE, Section 3.2) Learning and Cross-Domain Semantic-Matched (CDSM, Section 3.3) Learning, which is shown in Figure 1. IDSE can help the feature extractor $f_{\\theta}$ to extract categorical semantics and ensure a unified semantic structure across domains at the same time, which is achieved by instanceprototype-mixed (IPM, Section 3.2.1) contrastive learning and a novel Semantic-Enhanced Loss (SEL, Section 3.2.2). After IDSE, CDSM conducts Semantic-Preserving Domain Alignment (SPDA, Section 3.3.1) to minimize the domain gap while preserving the semantic structure learned by IDSE. With the minimization of the domain gap, more accurate nearest-neighbor searching can be achieved by our Switchable Nearest Neighboring Match $\\mathrm{\\DeltaSN^{2}M}$ , Section 3.3.2). ", "page_idx": 2}, {"type": "text", "text": "3.2 Intra-Domain Semantic-Enhanced Learning ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "To achieve effective cross-domain retrieval, feature extractor $f_{\\theta}$ needs to learn consistent crossdomain features to differentiate data categories. Instance Discrimination [9] is usually employed to ", "page_idx": 2}, {"type": "text", "text": "achieve discriminative feature learning, but directly applying it in ${\\tt U}^{2}{\\tt C D R}$ has four fundamental issues that hinder the possibility of accurate cross-domain categorical matching later: ", "page_idx": 3}, {"type": "text", "text": "1) Instance discrimination tends to extract semantics that separate domains rather than categories, ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{d}(f(\\pmb{x}_{i}^{\\mathrm{A}}),f(\\pmb{x}_{j}^{\\mathrm{A}}))<\\mathrm{d}(f(\\pmb{x}_{i}^{\\mathrm{A}}),f(\\pmb{x}_{j}^{\\mathrm{B}})),\\,y_{i}^{\\mathrm{A}}=y_{j}^{\\mathrm{B}}\\neq y_{j}^{\\mathrm{A}}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "2) Instance discrimination cannot characterize categorical semantics in the feature space, ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}(\\pmb{x}_{i}^{\\mathrm{A}},\\pmb{x}_{j_{1}}^{\\mathrm{A}})}{\\mathrm{d}(\\pmb{x}_{i}^{\\mathrm{A}},\\pmb{x}_{j_{2}}^{\\mathrm{A}})}<\\frac{\\mathrm{d}(f(\\pmb{x}_{i}^{\\mathrm{A}}),f(\\pmb{x}_{j_{1}}^{\\mathrm{A}}))}{\\mathrm{d}(f(\\pmb{x}_{i}^{\\mathrm{A}}),f(\\pmb{x}_{j_{2}}^{\\mathrm{A}}))},\\,y_{i}^{\\mathrm{A}}=y_{j_{1}}^{\\mathrm{A}}\\neq y_{j_{2}}^{\\mathrm{A}}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "3) The randomness introduced by stochastic data augmentations results in evident changes in learned categorical semantic structures during training, i.e., ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{d}(G(\\mathcal{P}_{t}^{\\mathrm{A}}),G(\\mathcal{P}_{t+1}^{\\mathrm{A}}))\\gg\\operatorname*{min}_{\\mathcal{P}_{i}^{\\mathrm{A}},\\mathcal{P}_{j}^{\\mathrm{A}}\\sim\\mathcal{H}}\\mathrm{d}(G(\\mathcal{P}_{i}^{\\mathrm{A}}),G(\\mathcal{P}_{j}^{\\mathrm{A}})),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $G(\\cdot)$ corresponds to a graph constructed by the input vectors, and $\\mathcal{P}$ denotes the set of categorical prototypes for a domain. The subscript $t$ denotes different training iterations, while $\\mathcal{H}$ represents the hypothesis space of possible categorical prototype sets for a particular domain. $\\mathrm{d}(\\cdot)$ here is a measurement for graph difference, e.g., graph edit distance [33]. ", "page_idx": 3}, {"type": "text", "text": "4) Distinct label spaces make instance discrimination learn different categorical semantic structures: ", "page_idx": 3}, {"type": "text", "text": "Theorem 3.1 (Geometry Distinctness). Suppose data distributions of two domains (A and $B$ ) have mutually disjoint supports, and they are uniform over these supports. Assuming the support sets of domains A and B are not identical, the optimal feature extractors $f^{*}$ that minimize the instance discrimination loss of different domains present distinct geometric feature spaces. ", "page_idx": 3}, {"type": "text", "text": "3.2.1 Instance-Prototype-Mixed Contrastive Learning. ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "To fix the above issues, we adopt a slowly momentum-updated contrastive learning algorithm \u2013 MoCo [10], to handle the third issue reflected by Eq. (4). Moreover, the MoCo-based instance discrimination is conducted separately for each domain, which encourages $f_{\\theta}$ to focus less on learning domain semantics (for the first issue, Eq. (2)). Besides, we accompany MoCo with a prototypical contrastive loss, to enhance the mapping of categorical semantics from the input space to the feature space (for addressing the second issue, Eq. (3)). With a well-crafted prototype update mechanism, this prototypical contrastive loss can also help build a unified semantic structure across domains (for the last issue, Theorem 3.1). Then let us introduce IPM contrastive learning in detail. First of all, two memory banks $\\mathcal{M}^{\\mathrm{A}}$ and $\\mathcal{M}^{\\mathrm{B}}$ are maintained for domains A and $\\mathbf{B}$ , which store historical features $\\mathbf{\\nabla}m$ of data samples $\\textbf{\\em x}$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{M}^{\\mathrm{A}}=\\left[m_{1}^{\\mathrm{A}},...,m_{N^{\\mathrm{A}}}^{\\mathrm{A}}\\right],\\mathcal{M}^{\\mathrm{B}}=\\left[m_{1}^{\\mathrm{B}},...,\\mathbf{m}_{N^{\\mathrm{B}}}^{\\mathrm{B}}\\right],\\mathrm{where}\\;m_{i}\\gets\\beta m_{i}+(1-\\beta)f_{\\theta}(\\pmb{x}_{i}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Here $\\pmb{m}_{i}$ is initialized by the feature of $\\pmb{x}_{i}$ extracted by the initial $f_{\\theta}$ and updated in momentum, where $\\beta$ controls the momentum speed, and we set it as a popular value 0.99. With these two memory banks, MoCo builds the positive pairs as the pair of each instance and its historical feature, while the negative ones are pairs of each instance and the historical features of all other instances: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{INCE}}=\\sum_{i=1}^{B}-\\log\\frac{\\exp\\left(f_{\\theta}(\\mathbf{x}_{i})\\cdot\\mathbf{m}_{i}/\\tau\\right)}{\\sum_{j=1}^{B}\\exp\\left(f_{\\theta}(\\mathbf{x}_{i})\\cdot\\mathbf{m}_{j}/\\tau\\right)},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $B$ is the batch size and $\\tau$ is a temperature factor that is set as 0.07. ", "page_idx": 3}, {"type": "text", "text": "As for the design of our prototypical contrastive loss, K-Means is applied on $\\mathcal{M}^{\\mathrm{A}}$ and $\\mathcal{M}^{\\mathrm{B}}$ to construct prototypes as cluster centers $\\mathcal{P}\\,=\\,\\{p_{c}\\}_{c=1}^{\\widehat{C}}$ . In our problem, the cluster number $C$ is unknown, thus we apply the Elbow approach [34] to estimate it as $\\widehat{C}$ . Then, for each instance $\\pmb{x}_{i}$ , if it belongs to the $c_{i}$ -th cluster, the prototypical contrastive loss $\\mathcal{L}_{\\mathrm{PNCE}}$ shapes like, ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{PNCE}}=\\sum_{i=1}^{B}-\\log\\frac{\\exp\\left(f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i})\\cdot\\boldsymbol{p}_{c_{i}}/\\tau\\right)}{\\sum_{c=1}^{\\widehat{C}}\\exp\\left(f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i})\\cdot\\boldsymbol{p}_{c}/\\tau\\right)}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Until here, the first three issues can be fixed by mixing $\\mathcal{L}_{\\mathrm{INCE}}$ and $\\mathcal{L}_{\\mathrm{PNCE}}$ , but the last issue, Theorem 3.1, is the bottleneck of $\\mathtt{U}^{2}\\mathtt{C D R}$ . Next, let us introduce how we build a unified prototypical ", "page_idx": 3}, {"type": "text", "text": "structure for $\\mathcal{L}_{\\mathrm{PNCE}}$ to address the last issue. Specifically, after obtaining the prototype sets $\\mathcal{P}^{\\mathrm{A}},\\mathcal{P}^{\\mathrm{B}}$ of domain A and $\\mathbf{B}$ , if we take domain A as an example to illustrate the prototypical structure building process, the prototype set $\\mathcal{P}^{\\mathrm{B}}$ of domain B will be translated to domain A as, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{P}^{\\mathrm{B\\rightarrowA}}=\\{p_{c}^{\\mathrm{B\\rightarrowA}}=\\overrightarrow{p_{c}^{\\mathrm{B}}}+\\overrightarrow{\\overline{{\\mathcal{M}^{\\mathrm{B}}}}\\,\\overrightarrow{\\mathcal{M}^{\\mathrm{A}}}}\\}_{c=1}^{\\widehat{C}^{\\mathrm{B}}},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\overline{{\\mathcal{M}}}$ denotes the average vector of all vectors in $\\mathcal{M}$ . Next, each prototype $p_{c}^{\\mathrm{A}}\\in\\mathcal{P}^{\\mathrm{A}}$ searches its closest $p_{c^{\\prime}}^{\\mathrm{B\\toA}}\\in\\mathcal{P}^{\\mathrm{B\\toA}}$ (we use Hungarian algorithm to search the closest cross-domain prototypes) for the opportunity of merging, which needs to satisfy the condition, ", "page_idx": 4}, {"type": "equation", "text": "$$\nd(p_{c^{\\prime}}^{\\mathrm{B\\toA}},p_{c}^{\\mathrm{A}})<\\operatorname*{min}\\left[\\operatorname*{min}_{p_{i},p_{j}\\in\\mathcal{P}^{\\mathrm{A}}}d(p_{i},p_{j}),\\operatorname*{min}_{p_{i},p_{j}\\in\\mathcal{P}^{\\mathrm{B}}}d(p_{i},p_{j})\\right],\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $d(\\cdot,\\cdot)$ computes the Euclidean distance. If we use the symbol $\\oplus$ to identify prototypes that satisfy this merging condition, the final prototypical structure for domain A is $\\mathcal{P}^{\\mathrm{A}^{\\prime}}=\\left(\\mathcal{P}^{\\mathrm{A}}\\setminus\\mathcal{P}^{\\mathrm{A},\\oplus}\\right)\\cup\\left(\\mathcal{P}^{\\mathrm{B\\toA}}\\setminus\\mathcal{P}^{\\mathrm{B\\toA},\\oplus}\\right)\\cup\\left(\\mathcal{P}^{\\mathrm{A},\\oplus}\\oplus\\mathcal{P}^{\\mathrm{B\\toA},\\oplus}\\right)$ where $\\left(\\mathcal{P}^{\\mathrm{A,\\oplus}}\\oplus\\mathcal{P}^{\\mathrm{B\\rightarrowA,\\oplus}}\\right)=$ $\\left\\{\\left(p_{c^{\\prime}}^{\\mathrm{B}\\rightarrow\\mathrm{A},\\oplus}+p_{c}^{\\mathrm{A},\\oplus}\\right)/2\\right\\}_{c=1}^{C^{\\oplus}}$ . Then the computation of LPANCE \u2013 Eq. (7) for domain A is conducted on the newly-built $\\mathcal{P}^{\\mathrm{A^{\\prime}}}$ , and all these operations are same for domain B. ", "page_idx": 4}, {"type": "text", "text": "However, as establishing the semantic cluster-like structure requires time, it is unreasonable to conduct prototype contrastive learning from the beginning of training. Therefore, we conduct instance discrimination at the beginning and progressively incorporate $\\mathcal{L}_{\\mathrm{PNCE}}$ . In this case, not only are the constructed cluster centers more reliable, but the Elbow approach also provides more accurate cluster number estimations. Specifically, we use a coefficient $\\alpha$ that is scheduled by a Sigmoid function to control the incorporation weight of $\\mathcal{L}_{\\mathrm{PNCE}}$ , i.e., ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{IPM}}=\\mathcal{L}_{\\mathrm{INCE}}+\\alpha\\mathcal{L}_{\\mathrm{PNCE}},\\;\\mathrm{where}\\,\\alpha=\\frac{1}{1+\\exp\\left(0.5E-e\\right)},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "$E$ and $e$ here are the overall training epochs and the current epoch for IDSE. ", "page_idx": 4}, {"type": "text", "text": "3.2.2 Semantic-Enhanced Loss. ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "For the IPM contrastive learning, it is arbitrary to allocate a data instance $\\pmb{x}_{i}$ to a single cluster when the preferred semantic prototypical structure cannot be learned in advance. As a result, to speed up the structure-building process, we propose a novel Semantic-Enhanced Loss (SEL) to align data instances with the prototypes better. Specifically, instead of assigning data instances with a single cluster, SEL considers potential semantic relationships between instances with all clusters, which are measured by the Softmax probability. Moreover, as both $\\mathcal{P}^{\\mathrm{A}}$ and $\\mathcal{P}^{\\mathrm{B}}$ are obtained by the Euclidean distance-based K-Means, we directly minimize the Euclidean distance between samples and prototypes: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{SEL}}=\\frac{1}{B}\\sum_{i=1}^{B}\\sum_{c=1}^{\\tilde{C}}\\frac{\\exp(f_{\\theta}(\\boldsymbol{x}_{i})\\cdot\\boldsymbol{p}_{c}/\\tau)}{\\sum_{c=1}^{\\tilde{C}}\\exp(f_{\\theta}(\\boldsymbol{x}_{i})\\cdot\\boldsymbol{p}_{c}/\\tau)}d(f_{\\theta}(\\boldsymbol{x}_{i}),\\boldsymbol{p}_{c}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\widetilde{C}$ denotes the number of prototypes after merging, e.g., $\\widetilde{C}^{\\mathrm{A}}$ is the number of elements in $\\mathcal{P}^{\\mathrm{A^{\\prime}}}$ . B y taking all potential semantic correlations into account, S EL can alleviate the impact of the noise within the K-Means clustering results and further guide the model to learn more distinguishable semantic information in terms of Euclidean distance. Certainly, such SEL benefits also rely on high-quality semantic prototypical structures. As a result, we also apply the progressive coefficient $\\alpha$ to SEL, then the final optimization objective for IDSE is ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{IDSE}}=(\\mathcal{L}_{\\mathrm{IPM}}^{\\mathrm{A}}+\\mathcal{L}_{\\mathrm{IPM}}^{\\mathrm{B}})+\\alpha(\\mathcal{L}_{\\mathrm{SEL}}^{\\mathrm{A}}+\\mathcal{L}_{\\mathrm{SEL}}^{\\mathrm{B}}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "3.3 Cross-Domain Semantic-Matched Learning ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "3.3.1 Semantic-Preserving Domain Alignment. ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Domain invariance is another requirement for the extracted features in UEM. However, it is difficult to effectively align feature clusters across domains when no category label nor correspondence annotation can be utilized in $\\mathtt{U}^{2}\\mathtt{C D R}$ . Instance matching [7, 32, 13] is proposed to match an instance $\\pmb{x}_{i}^{\\mathrm{A}}$ to another instance $\\pmb{x}_{j}^{\\mathrm{B}}$ in the other domain with the most similar features. However, due to the domain gap, instances can be easily mapped mistakenly. For example, if there is an instance in one domain that is extremely close to the other domain, it will be determined as the nearest neighbor for all instances in the other domain [13], shown in Figure 2. As a result, before conducting instance matching, the domain gap needs to be diminished. Existing works usually leverage discrepancy minimization [35] or adversarial learning [36] to achieve domain alignment. However, these methods provide inferior performance due to semantic categorical structure changes, i.e., the semantic correlations among instances within domains change a great deal during domain alignment, shown in Figure 3. ", "page_idx": 4}, {"type": "image", "img_path": "zZVqZRXSao/tmp/2c141007379d962f528c37f316db5bc4475a290071d0860345f4f63aad12b4d3.jpg", "img_caption": ["Figure 2: Comparison of nearest neighbor searching before or after domain alignment. ", "Figure 3: Comparison between standard adversarial learning and our semantic-preserving domain alignment in terms of semantic structure changes. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "To achieve more effective domain alignment, especially with semantic preservation, we propose Semantic-Preserving Domain Alignment (SPDA). Similar to the standard domain adversarial learning, SPDA happens on two parties, one is the feature extractor $f_{\\theta}$ , and the other is a domain classifier $g_{\\omega}$ parameterized on $\\omega$ . The domain classifier tries to distinguish the representations of two domains, while the feature extractor tries to fool the domain classifier. Thus, the training is shaped like a bi-level optimization in terms of the domain classification task on $\\theta$ and $\\omega$ , shown as follows, ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{DAL}}=\\sum_{i=1}^{2B}-y_{i}\\cdot\\log g_{\\omega}(f_{\\theta}(x_{i}))-(1-y_{i})\\cdot\\log(1-g_{\\omega}(f_{\\theta}(x_{i}))),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $y$ here denotes the domain label, e.g., if we regard $y=1$ for domain A, the domain label of domain $\\mathbf{B}$ is $y=0$ . After sufficient adversarial training, the feature extractor captures nearly domain-invariant features, thus achieving domain alignment. ", "page_idx": 5}, {"type": "text", "text": "To prevent the semantic structure from being changed, we first make a copy for the model trained by IDSE and denote it as $f_{\\theta}^{\\prime}$ . Then for a mini-batch of a particular domain, we feed all instances to $f_{\\theta}^{\\prime}$ and calculate pair-wise cosine similarity and Euclidean distance. In this way, all these instances constrain and influence each other, which means that when the correlation of a particular instance pair changes, it will affect the correlations of other related pairs. Then, we apply a semantic-preserving regulation into domain adversarial learning to make the pair-wise correlations unchanged, ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{\\mathrm{SPR}}=\\displaystyle\\frac{1}{B^{2}}\\sum_{i=1}^{B}\\sum_{j=1}^{B}\\Bigl\\{\\left[\\frac{f_{\\theta}(\\pmb{x}_{i})\\cdot f_{\\theta}(\\pmb{x}_{j})}{|f_{\\theta}(\\pmb{x}_{i})||f_{\\theta}(\\pmb{x}_{j})|}-\\frac{f_{\\theta^{\\prime}}(\\pmb{x}_{i})\\cdot f_{\\theta^{\\prime}}(\\pmb{x}_{j})}{|f_{\\theta^{\\prime}}(\\pmb{x}_{i})||f_{\\theta^{\\prime}}(\\pmb{x}_{j})|}\\right]^{2}}\\\\ {+\\left[d(f_{\\theta}(\\pmb{x}_{i}),f_{\\theta}(\\pmb{x}_{j}))-d(f_{\\theta^{\\prime}}(\\pmb{x}_{i}),f_{\\theta^{\\prime}}(\\pmb{x}_{j}))\\right]^{2}\\Bigr\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Recall IPM Contrastive Learning. Actually, aligning two domains together without any semantic structure change is impossible. As a result, there is a need for a dedicated design to alleviate the impact of such unavoidable changes. Our solution is to strengthen the instances\u2019 semantic correlations by enhancing the cluster\u2019s inner density and inter-separability. For the final convergence of IPM contrastive learning, each instance is optimized to get as close to its corresponding cluster prototype as possible, and as far to other cluster prototypes as possible: ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.2 (Convergence of IPM). Suppose the data distribution of a domain has mutually disjoint supports, and it is uniform over these supports. Simplex Equiangular Tight Frame (ETF) representations [37] minimize the Instance-Prototype-Mixed Loss of this domain. ", "page_idx": 5}, {"type": "text", "text": "3.3.2 Switchable Nearest Neighboring Match. ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "With SPDA, the domain gap could be effectively minimized to enable more accurate cross-domain instance matching. However, existing instance matching approaches [7, 13, 32] lack the capability of measuring the matching reliability between an instance and its nearest neighbor, which allows us to conduct cross-domain matching with different weights. For example, if an instance is located at the joint boundary of multiple categories, which indicates that the current feature extractor cannot extract sufficiently distinguishable semantic features for this instance, we are supposed to lay less emphasis on this case. To fix such issues, we propose the Switchable Nearest Neighboring Match $(\\mathbb{S}\\mathbb{N}^{2}\\Bar{\\mathbb{M}})$ . ", "page_idx": 6}, {"type": "text", "text": "The principle behind $\\mathsf{S N^{2}M}$ is that prototypes are more convincing and reliable. Specifically, for a particular sample $\\pmb{x}_{i}^{\\mathrm{A}}$ in domain A, we first determine its inner nearest cluster prototype $p_{c_{i}}^{\\mathrm{A}}$ in domain A. We can also search for the nearest instance $\\pmb{x}_{i}^{\\mathrm{A,B}}$ in domain B. Both these two searching processes are based on the product of a modified cosine similarity and Euclidean distance, ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\pmb{p}_{c_{i}}^{\\mathrm{A}}=\\arg\\underset{\\pmb{p}_{j}^{\\mathrm{A}}}{\\operatorname*{min}}\\left[\\left(1-\\frac{f_{\\theta}\\left(\\pmb{x}_{i}^{\\mathrm{A}}\\right)\\cdot\\pmb{p}_{j}^{\\mathrm{A}}}{\\lvert f_{\\theta}\\left(\\pmb{x}_{i}^{\\mathrm{A}}\\right)\\rvert\\lvert\\pmb{p}_{j}^{\\mathrm{A}}\\rvert}\\right)\\cdot d(f_{\\theta}(\\pmb{x}_{i}^{\\mathrm{A}}),\\pmb{p}_{j}^{\\mathrm{A}})\\right]}\\\\ &{\\pmb{x}_{i}^{\\mathrm{A,B}}=\\arg\\underset{\\pmb{x}_{j}^{\\mathrm{B}}}{\\operatorname*{min}}\\left[\\left(1-\\frac{f_{\\theta}\\left(\\pmb{x}_{i}^{\\mathrm{A}}\\right)\\cdot f_{\\theta}\\left(\\pmb{x}_{j}^{\\mathrm{B}}\\right)}{\\lvert f_{\\theta}\\left(\\pmb{x}_{i}^{\\mathrm{A}}\\right)\\rvert\\lvert f_{\\theta}\\left(\\pmb{x}_{j}^{\\mathrm{B}}\\right)\\rvert}\\right)\\cdot d(f_{\\theta}(\\pmb{x}_{i}^{\\mathrm{A}}),f_{\\theta}(\\pmb{x}_{j}^{\\mathrm{B}}))\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "After obtaining $\\pmb{x}_{i}^{\\mathrm{A,B}}$ , $\\mathsf{S N}^{2}\\mathbb{M}$ searches for its inner nearest prototype pA,B\u2032in PB\u2032(which has been merged with the translated prototype set $\\mathcal{P}^{\\mathrm{A\\rightarrowB}}$ from domain A) and two cases allows us to measure the reliability of $\\pmb{x}_{i}^{\\mathrm{A,B}}$ . Before introducing these two cases, the inner nearest prototype $p_{c_{i}}^{\\mathrm{A}}$ of $\\pmb{x}_{i}^{\\mathrm{A}}$ needs to be translated to domain B and checked whether should be merged to follow condition Eq. (9), and we denote the translated prototype as $\\widetilde{\\pmb{p}}_{c_{i}}^{\\mathrm{A}}$ . Then the first potential case is pcA,B\u2032is exactly identical to $\\widetilde{\\pmb{p}}_{c_{i}}^{\\mathrm{A}}$ , which means, $\\pmb{x}_{i}^{\\mathrm{A,B}}$ is convincing since it shares the same prototype correlations with $\\pmb{x}_{i}^{\\mathrm{A}}$ across two domains. Then the pair of $\\pmb{x}_{i}^{\\mathrm{A,B}}$ and $\\pmb{x}_{i}^{\\mathrm{A}}$ should be viewed as a positive pair in the contrastive loss. Otherwise, if $p_{c_{i}}^{\\mathrm{A,B^{\\prime}}}$ is different from $\\widetilde{\\pmb{p}}_{c_{i}}^{\\mathrm{A}}$ , it may be located at the intersection region of multiple clusters. In this case, the pair of $\\pmb{x}_{i}^{\\mathrm{A,B}}$ a nd $\\pmb{x}_{i}^{\\mathrm{A}}$ is not supposed to be treated as a positive pair. However, it does not mean $\\mathrm{SN^{2}\\bar{M}}$ does nothing for these unreliable cases, instead, $\\mathsf{S N^{2}M}$ leverages a modified prototype contrastive loss to match the pair of $\\pmb{x}_{i}^{\\mathrm{A}}$ and $\\widetilde{\\pmb{p}}_{c_{i}}^{\\mathrm{A}}$ . The modification is to incorporate cross-domain instance-wise negative comparison, ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{SN^{2}M}}=\\frac{1}{B}\\sum_{i=1}^{B}-\\log\\frac{\\Delta}{\\sum_{c=1}^{\\tilde{C}^{\\mathrm{B}}}\\exp(f_{\\theta}(\\boldsymbol{x}_{i}^{\\mathrm{A}})\\cdot\\boldsymbol{p}_{c}^{\\mathrm{B^{\\prime}}}/\\tau)+\\sum_{j=1}^{N^{\\mathrm{B}}}\\exp(f_{\\theta}(\\boldsymbol{x}_{i}^{\\mathrm{A}})\\cdot\\boldsymbol{f}_{\\theta}(\\boldsymbol{x}_{j}^{\\mathrm{B}})/\\tau)}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Finally, the overall optimization objective of CDSM follows ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{CDSM}}=\\mathcal{L}_{\\mathrm{DAL}}+\\mathcal{L}_{\\mathrm{SPR}}^{\\mathrm{A}}+\\mathcal{L}_{\\mathrm{SPR}}^{\\mathrm{B}}+\\mathcal{L}_{\\mathrm{SN^{2}M}}^{\\mathrm{A}}+\\mathcal{L}_{\\mathrm{SN^{2}M}}^{\\mathrm{B}}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "The datasets, experimental settings, and comparison baselines are introduced below. More implementation details, experiment results, and source codes are provided in the Supplementary Materials. ", "page_idx": 6}, {"type": "text", "text": "Datasets. Office-31 [38] includes three domains with 31 classes: Amazon (A), DSLR (D), Webcam (W). Office-Home [39] contains four different domains: Art (A), Clipart (C), Product (P), Real (R). And each domain has 67 data categories. DomainNet [40] is the most challenging cross-domain dataset to our best knowledge, which includes six domains: Quickdraw (Qu), Clipart (Cl), Painting $(\\mathrm{Pa})$ , Infograph (In), Sketch (Sk) and Real (Re). DomainNet is originally class-imbalanced, thus we follow [7] to select 7 data classes that contain more than 200 samples. ", "page_idx": 6}, {"type": "text", "text": "Experiment Settings. For fair comparison, we apply ResNet-50 [41] pre-trained with ImageNet in MoCov2 [10] as the feature extractor. The domain classifier consists of two fully-connected layers. ", "page_idx": 6}, {"type": "text", "text": "The SGD optimizer with a momentum of 0.9 is adopted with an initial learning rate of 0.0002 that is scheduled to zero by a cosine learning strategy. The batch size is 64. The training epochs of IDSE are 100 for Office-31 and Office-Home, and 200 for DomainNet. The epoch number of CDSM is 50 for all three datasets. Following [17], we adopt mean average precision on all retrieved results (mAP@All) to measure the performance. All experiments are run repeatedly 3 times with seeds 2024, 2025, and 2026, and we report the mean performance and standard deviation. ", "page_idx": 7}, {"type": "text", "text": "Comparison Baselines. Our proposed method is compared with a comprehensive set of state-of-theart works from Cross-Domain Representation Learning (CDS [14], PCS [13]), Unsupervised Domain Generalization (DARL [31], DN2A [32]), and Unsupervised Cross-Domain Retrieval (UCDIR [7], CoDA [17], DGDIR [8]). We follow their default settings and only conduct compulsory customization. ", "page_idx": 7}, {"type": "table", "img_path": "zZVqZRXSao/tmp/a02212986c07071b86e76005ffa16d3853324a3db2a77b2d23b9501a57ea8807.jpg", "table_caption": ["Table 1: Performance comparison (mAP@All) between ours and other baseline methods on Office-31 and DomainNet in Close-set Unsupervised Cross-Domain Retrieval. We blue and bold the best performance, and bold the second best, same for all tables. "], "table_footnote": ["Ours 76.2\u00b11.4 77.0\u00b12.1 75.6\u00b12.0 92.5\u00b10.7 78.9\u00b13.0 91.0\u00b10.2 81.9\u00b10.5 31.9\u00b10.9 39.4\u00b11.4 35.0\u00b10.7 29.8\u00b10.6 35.7\u00b11.8 34.4\u00b10.5 "], "page_idx": 7}, {"type": "image", "img_path": "zZVqZRXSao/tmp/7d207d138d642d4fe73cd7d3ed63041686fca75f17a973b35c605e7de9631530.jpg", "img_caption": [], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Figure 4: Retrieval results of UEM and DGDIR on Office-31 $(\\mathrm{A}{\\rightarrow}\\mathrm{D})$ , Office-Home $\\scriptstyle(\\mathbf{A}\\to\\mathbf{C})$ , and DomainNet $\\scriptstyle(\\mathbf{Cl}\\to\\mathbf{Pa})$ ) in Close-set Unsupervised Cross-Domain Retrieval. Green and red rectangle denote correct and incorrect retrieval results. Best viewed in colors. ", "page_idx": 7}, {"type": "text", "text": "4.1 Effectiveness of UEM When Solving ${\\tt U}^{2}{\\tt C D R}$ ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Close-set Unsupervised Cross-Domain Retrieval. For the close-set setting, the label space of the query domain is identical to the retrieval domain. All domain pairs of Office-31 and Office-Home are tested, and 5 pairs of DomainNet. The results for Office-31 and DomainNet are shown in Table 1 (Office-Home can be found in the Appendix). According to these results, we can observe that UEM significantly outperforms other baselines in all cases. Specifically, we can achieve mAP $@$ All improvement of $2.6\\%$ compared to the best baseline method on Office-31, and such improvement is even larger on DomainNet with an average of $3.6\\%$ . Figure 4 also shows the retrieval results of our approach on Office-31, Office-Home, and DomainNet, and we can observe that all results are correct. ", "page_idx": 7}, {"type": "text", "text": "Partial Unsupervised Cross-Domain Retrieval. To establish the partial setting, the query domain contains only half of the label space of the retrieval domain, and the query label space is randomly selected. As shown in Table 2, we can observe that UEM exceeds all other baseline methods significantly in mAP@All, which is much more substantial than close-set UCDR. For example, UEM outperforms the second-best with an average of $12.4\\%$ on Office-Home. Similar improvements on Office-31 and DomainNet can be observed (results are provided in the Appendix). Besides, we can easily observe that existing state-of-the-art studies are nearly incapable of dealing with the label space difference in partial UCDR (see Theorem 3.1), while our UEM can work effectively. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Open-set Unsupervised Cross-Domain Retrieval. As for the open-set setups, we ensure that the label space of the retrieval domain is half of the query label space. The experiment results for DomainNet (results for Office-31 and Office-Home are in the Appendix) are presented in Table 3 with two metrics $-\\ensuremath{\\mathrm{\\mAP}}(\\partial$ All for the shared label set, and detection accuracy for the private (open-set) query labels (please refer to Appendix for how to detect the private query labels). According to these results, we can easily observe that our approach substantially exceeds other baseline methods in both metrics. Similar trends can also be found for Office-31 and Office-Home. All these results strongly validate the effectiveness of UEM in open-set UCDR. ", "page_idx": 8}, {"type": "table", "img_path": "zZVqZRXSao/tmp/c1a33988411d9b6d40c51aef5d295c6679114008811df8260ad0d325f722531e.jpg", "table_caption": ["Table 2: Performance comparison $(\\mathrm{mAP}@\\$ All) between ours and other baseline methods on OfficeHome in Partial Unsupervised Cross-Domain Retrieval. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Table 3: Performance comparison (mAP $@$ All for shared-label set, detection accuracy for open-label set) between ours and other baseline methods on DomainNet in Open-set Unsupervised Cross-Domain Retrieval. ", "page_idx": 8}, {"type": "table", "img_path": "zZVqZRXSao/tmp/2a24d7b8fae784cba813ec0b111090d126e659b18cd4e4b92bd9028731dba8be.jpg", "table_caption": [], "table_footnote": ["Ours 30.3 72.9 40.2 88.1 36.0 82.5 31.1 78.2 36.6 83.0 34.8 80.9 "], "page_idx": 8}, {"type": "text", "text": "Table 4: Ablation studies of UEM on Office-31, Office-Home, and DomainNet in Open-set Unsupervised Cross-Domain Retrieval. The average values of Sharedset mAP $@$ All and Open-set Acc for all domain pairs are reported here. ", "page_idx": 8}, {"type": "table", "img_path": "zZVqZRXSao/tmp/ebd45f0cdbbd86d974f80b7a4c94f23724344ab2f209eb554cf67abb819b1c86.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "4.2 Ablation Study ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "All the ablation studies are carried out in open-set UCDR on three datasets, and the average metrics (Shared-set mAP $@$ All and Open-set Acc) for all domain pairs of a single dataset are reported here. ", "page_idx": 8}, {"type": "text", "text": "Effectiveness of Prototype Merging. When evaluating the effectiveness of a unified prototypical structure, we do not use prototype merging in IDSE. According to the results of \u2018Ours w/o P.M.\u2019 in Table 4, there is a non-negligible performance drop in both shared-set $\\ensuremath{\\mathbf{m}}\\ensuremath{\\mathbf{A}}\\ensuremath{\\mathbf{P}}(\\partial\\!\\!\\!\\!/$ All and open-set accuracy. This validates the importance of building a unified prototypical structure across domains. ", "page_idx": 8}, {"type": "text", "text": "Effectiveness of SEL. When evaluating SEL, we detach it during the model training. According to the results of \u2018Ours w/o SEL\u2019 in Table 4, there is also an evident performance drop compared to the full UEM. This validates that SEL is vital as it can help prepare a better base model for CDSM. ", "page_idx": 8}, {"type": "text", "text": "Effectiveness of SPDA. We replace our SPDA with the standard domain adversarial learning for the ablation study. As shown in Table 4, there is a significant performance difference between \u2018Ours w/o SPDA\u2019 and the full UEM, which illustrates the importance of semantic preservation during domain alignment, as well as indirectly verifying the necessity of SPDA. ", "page_idx": 8}, {"type": "text", "text": "Effectiveness of $\\mathrm{\\boldmath~SN}^{2}\\mathbb{M}$ . We also replace $\\mathsf{S N^{2}M}$ with the nearest neighboring search approach leveraged by UCDIR. By comparing the results of \u2018Ours w/o $\\mathsf{S N^{2}M^{\\prime}}$ and \u2018Ours\u2019, we can conclude that $\\mathrm{SN^{2}\\bar{M}}$ is more compatible with UEM and able to achieve more accurate cross-domain categorical matching. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we focus on two major challenges when conducting cross-domain retrieval (CDR) in real-world scenarios: one is that the category space across domains is usually distinct, and the other is that both the query and retrieval domains are unlabeled. To tackle these challenges, we propose a Unified, Enhanced, and Matched (UEM) semantic feature learning framework that can establish a unified semantic structure across domains and preserve this structure during categorical matching. Extensive experiments in cases including close-set, partial, open-set unsupervised CDR on multiple datasets demonstrate the effectiveness and universality of UEM, which are reflected in the substantial performance improvement over state-of-the-art studies from Cross-Domain Representation Learning, Unsupervised Domain Generalization, and Unsupervised CDR. ", "page_idx": 9}, {"type": "text", "text": "6 Limitations and Future Work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "To solve the real-world challenges when employing cross-domain retrieval, especially considering the category distinctness across unsupervised data domains, we propose the UEM semantic feature learning framework in this work. Although extensive empirical evaluation and theoretical analysis have validated the effectiveness of UEM, some minor limitations still need more exploration. First, the current UEM framework is composed of two stages, and we empirically determine the switching point. In the future, we need to achieve a real end-to-end UEM by changing the training stage automatically. Besides, there is a lack of theoretical analysis for the second stage (CDSM). In future efforts, we should theoretically prove the semantic preservation of SPDA and the reliability of $\\mathsf{S N}^{2}\\mathbb{M},$ . Lastly, the general applicability of UEM also needs testing. For instance, cross-person generalization in wearable devices [42] and property analysis of material [43] or molecular [44] structures require cross-domain retrieval. Therefore, we should test UEM on other modalities like time series and graph data. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We gratefully acknowledge the support of a grant from General Motors. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Benjamin Klein and Lior Wolf. End-to-end supervised product quantization for image search and retrieval. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5041\u20135050, 2019.   \n[2] Muhammad Murad Khan, Roliana Ibrahim, and Imran Ghani. Cross domain recommender systems: a systematic literature review. ACM Computing Surveys (CSUR), 50(3):1\u201334, 2017.   \n[3] Min Jin Chong, Wen-Sheng Chu, Abhishek Kumar, and David Forsyth. Retrieve in style: Unsupervised facial feature transfer and retrieval. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 3887\u20133896, 2021.   \n[4] Bojana Gajic and Ramon Baldrich. Cross-domain fashion image retrieval. In Proceedings of the IEEE conference on computer vision and pattern recognition workshops, pages 1869\u20131871, 2018.   \n[5] Xiaoping Zhou, Xiangyu Han, Haoran Li, Jia Wang, and Xun Liang. Cross-domain image retrieval: methods and applications. International Journal of Multimedia Information Retrieval, 11(3):199\u2013218, 2022.   \n[6] Junshi Huang, Rogerio S Feris, Qiang Chen, and Shuicheng Yan. Cross-domain image retrieval with a dual attribute-aware ranking network. In Proceedings of the IEEE international conference on computer vision, pages 1062\u20131070, 2015.   \n[7] Conghui Hu and Gim Hee Lee. Feature representation learning for unsupervised cross-domain image retrieval. In European Conference on Computer Vision, pages 529\u2013544. Springer, 2022.   \n[8] Conghui Hu, Can Zhang, and Gim Hee Lee. Unsupervised feature representation learning for domain-generalized cross-domain image retrieval. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 11016\u201311025, 2023.   \n[9] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In International conference on machine learning, pages 1597\u20131607. PMLR, 2020.   \n[10] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 9729\u20139738, 2020.   \n[11] Junnan Li, Pan Zhou, Caiming Xiong, and Steven Hoi. Prototypical contrastive learning of unsupervised representations. In International Conference on Learning Representations, 2020.   \n[12] Cuie Yang, Yiu-Ming Cheung, Jinliang Ding, Kay Chen Tan, Bing Xue, and Mengjie Zhang. Contrastive learning assisted-alignment for partial domain adaptation. IEEE Transactions on Neural Networks and Learning Systems, 2022.   \n[13] Xiangyu Yue, Zangwei Zheng, Shanghang Zhang, Yang Gao, Trevor Darrell, Kurt Keutzer, and Alberto Sangiovanni Vincentelli. Prototypical cross-domain self-supervised learning for few-shot unsupervised domain adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13834\u201313844, 2021.   \n[14] Donghyun Kim, Kuniaki Saito, Tae-Hyun Oh, Bryan A Plummer, Stan Sclaroff, and Kate Saenko. Cds: Cross-domain self-supervised pre-training. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9123\u20139132, 2021.   \n[15] Junshi Huang, Rogerio S Feris, Qiang Chen, and Shuicheng Yan. Cross-domain image retrieval with a dual attribute-aware ranking network. In Proceedings of the IEEE international conference on computer vision, pages 1062\u20131070, 2015.   \n[16] Xiaoping Zhou, Xiangyu Han, Haoran Li, Jia Wang, and Xun Liang. Cross-domain image retrieval: methods and applications. International Journal of Multimedia Information Retrieval, 11(3):199\u2013218, 2022.   \n[17] Xu Wang, Dezhong Peng, Ming Yan, and Peng Hu. Correspondence-free domain alignment for unsupervised cross-domain image retrieval. arXiv preprint arXiv:2302.06081, 2023.   \n[18] Mingyuan Ge, Jianan Shui, Junyu Chen, and Mingyong Li. Pseudo-label based unsupervised momentum representation learning for multi-domain image retrieval. In International Conference on Multimedia Modeling, pages 369\u2013380. Springer, 2024.   \n[19] Kaipeng Fang, Jingkuan Song, Lianli Gao, Pengpeng Zeng, Zhi-Qi Cheng, Xiyao Li, and Heng Tao Shen. Pros: Prompting-to-simulate generalized knowledge for universal crossdomain retrieval. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024.   \n[20] Jindong Wang, Cuiling Lan, Chang Liu, Yidong Ouyang, Tao Qin, Wang Lu, Yiqiang Chen, Wenjun Zeng, and Philip Yu. Generalizing to unseen domains: A survey on domain generalization. IEEE Transactions on Knowledge and Data Engineering, 2022.   \n[21] Zhangjie Cao, Lijia Ma, Mingsheng Long, and Jianmin Wang. Partial adversarial domain adaptation. In Proceedings of the European conference on computer vision (ECCV), pages 135\u2013150, 2018.   \n[22] Lingjing Kong, Shaoan Xie, Weiran Yao, Yujia Zheng, Guangyi Chen, Petar Stojanov, Victor Akinwande, and Kun Zhang. Partial disentanglement for domain adaptation. In International Conference on Machine Learning, pages 11455\u201311472. PMLR, 2022.   \n[23] Pau Panareda Busto and Juergen Gall. Open set domain adaptation. In Proceedings of the IEEE international conference on computer vision, pages 754\u2013763, 2017.   \n[24] Hong Liu, Zhangjie Cao, Mingsheng Long, Jianmin Wang, and Qiang Yang. Separate to adapt: Open set domain adaptation via progressive separation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2927\u20132936, 2019.   \n[25] Kaichao You, Mingsheng Long, Zhangjie Cao, Jianmin Wang, and Michael I Jordan. Universal domain adaptation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2720\u20132729, 2019.   \n[26] Bo Fu, Zhangjie Cao, Mingsheng Long, and Jianmin Wang. Learning to detect open classes for universal domain adaptation. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part XV 16, pages 567\u2013583. Springer, 2020.   \n[27] Guangrui Li, Guoliang Kang, Yi Zhu, Yunchao Wei, and Yi Yang. Domain consensus clustering for universal domain adaptation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 9757\u20139766, 2021.   \n[28] Kuniaki Saito and Kate Saenko. Ovanet: One-vs-all network for universal domain adaptation. In Proceedings of the ieee/cvf international conference on computer vision, pages 9000\u20139009, 2021.   \n[29] Liang Chen, Qianjin Du, Yihang Lou, Jianzhong He, Tao Bai, and Minghua Deng. Mutual nearest neighbor contrast and hybrid prototype self-training for universal domain adaptation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 6248\u20136257, 2022.   \n[30] Liang Chen, Yihang Lou, Jianzhong He, Tao Bai, and Minghua Deng. Evidential neighborhood contrastive learning for universal domain adaptation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 6258\u20136267, 2022.   \n[31] Xingxuan Zhang, Linjun Zhou, Renzhe Xu, Peng Cui, Zheyan Shen, and Haoxin Liu. Towards unsupervised domain generalization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4910\u20134920, 2022.   \n[32] Yuchen Liu, Yaoming Wang, Yabo Chen, Wenrui Dai, Chenglin Li, Junni Zou, and Hongkai Xiong. Promoting semantic connectivity: Dual nearest neighbors contrastive learning for unsupervised domain generalization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3510\u20133519, 2023.   \n[33] Xinbo Gao, Bing Xiao, Dacheng Tao, and Xuelong Li. A survey of graph edit distance. Pattern Analysis and applications, 13:113\u2013129, 2010.   \n[34] Fan Liu and Yong Deng. Determine the number of unknown targets in open world based on elbow method. IEEE Transactions on Fuzzy Systems, 29(5):986\u2013995, 2020.   \n[35] Mahsa Baktashmotlagh, Mehrtash Har, Mathieu Salzmann, et al. Distribution-matching embedding for visual domain adaptation. Journal of Machine Learning Research, 17(108):1\u201330, 2016.   \n[36] Mingsheng Long, Zhangjie Cao, Jianmin Wang, and Michael I Jordan. Conditional adversarial domain adaptation. Advances in neural information processing systems, 31, 2018.   \n[37] Vardan Papyan, XY Han, and David L Donoho. Prevalence of neural collapse during the terminal phase of deep learning training. Proceedings of the National Academy of Sciences, 117(40):24652\u201324663, 2020.   \n[38] Kate Saenko, Brian Kulis, Mario Fritz, and Trevor Darrell. Adapting visual category models to new domains. In Computer Vision\u2013ECCV 2010: 11th European Conference on Computer Vision, Heraklion, Crete, Greece, September 5-11, 2010, Proceedings, Part IV 11, pages 213\u2013226. Springer, 2010.   \n[39] Hemanth Venkateswara, Jose Eusebio, Shayok Chakraborty, and Sethuraman Panchanathan. Deep hashing network for unsupervised domain adaptation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5018\u20135027, 2017.   \n[40] Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo Wang. Moment matching for multi-source domain adaptation. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1406\u20131415, 2019.   \n[41] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.   \n[42] Chin-Chia Michael Yeh, Huiyuan Chen, Xin Dai, Yan Zheng, Junpeng Wang, Vivian Lai, Yujie Fan, Audrey Der, Zhongfang Zhuang, Liang Wang, et al. An efficient content-based time series retrieval system. In Proceedings of the 32nd ACM International Conference on Information and Knowledge Management, pages 4909\u20134915, 2023.   \n[43] Tahoura Mosavirik, Mohammad Hashemi, Mohammad Soleimani, Vahid Nayyeri, and Omar M Ramahi. Accuracy-improved and low-cost material characterization using power measurement and artificial neural network. IEEE Transactions on Instrumentation and Measurement, 70:1\u20139, 2021.   \n[44] Zichao Wang, Weili Nie, Zhuoran Qiao, Chaowei Xiao, Richard Baraniuk, and Anima Anandkumar. Retrieval-based controllable molecule generation. In The Eleventh International Conference on Learning Representations, 2022.   \n[45] Pranjal Awasthi, Nishanth Dikkala, and Pritish Kamath. Do more negative samples necessarily hurt in contrastive learning? In International conference on machine learning, pages 1101\u20131116. PMLR, 2022. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "This Appendix includes additional details for the paper\u201cSemantic Feature Learning for Universal Unsupervised Cross-Domain Retrieval\u201d, including theoretical proofs (Section A), implementation details (Section B), additional experiment results (Section C), and broader impact (Section D). ", "page_idx": 13}, {"type": "text", "text": "A Theoretical Proofs ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Theorem 3.1 (Geometry Distinctness). Suppose data distributions of two domains (A and $B$ ) have mutually disjoint supports, and they are uniform over these supports. Assuming the support sets of domains A and B are not identical, the optimal feature extractors $f^{*}$ that minimize the instance discrimination loss of different domains present distinct geometric feature spaces. ", "page_idx": 13}, {"type": "text", "text": "Proof. Suppose a data distribution is made of mutually disjoint supports and distributed uniformly over these supports, there is a theorem demonstrating the representation geometry learned by instance discrimination in a research work [45], i.e., ", "page_idx": 13}, {"type": "text", "text": "Theorem A.1. Assuming a data distribution has mutually disjoint supports and is uniform over these supports, any Simplex ETF representation extracted by $f$ minimizes $\\mathcal{L}_{\\mathrm{NCE}}(f)$ for any convex and non-increasing loss function l. Moreover, $i f l$ is strictly convex (e.g., logistic loss), then Simplex ETF representations are the only minimizes of $\\mathcal{L}_{\\mathrm{NCE}}(f)$ . ", "page_idx": 13}, {"type": "text", "text": "The Simplex ETF representations are defined as follows, ", "page_idx": 13}, {"type": "text", "text": "Definition A.2 (Simplex ETF). A simplex ETF is a collection of equal-length and maximally equiangular vectors. We call a $P\\times K$ matrix M an ETF if it satisfies ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbf{M}^{\\mathrm{T}}\\mathbf{M}=\\alpha\\left(\\frac{K}{K-1}\\mathbf{I}-\\frac{1}{K-1}\\mathbf{1}_{K}\\mathbf{1}_{K}^{\\mathrm{T}}\\right),\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where \u03b1 is a non-zero scalar, I is the identity matrix and ${\\bf1}_{K}$ is an all-ones vector. ", "page_idx": 13}, {"type": "text", "text": "In representation learning, ETF representations mean that samples from different categories $(\\overline{{\\pmb{x}_{i}}},\\overline{{y_{i}}}),(\\pmb{x}_{j},y_{j})$ satisfy ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\forall y_{i}\\neq y_{j},{\\frac{f_{\\theta}(\\pmb{x}_{i})\\cdot f_{\\theta}(\\pmb{x}_{j})}{|f_{\\theta}(\\pmb{x}_{i})||f_{\\theta}(\\pmb{x}_{j})|}}={\\frac{-1}{C-1}}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "With the above preparations, we apply proof by contradiction to prove that different domains hold distinct geometric feature spaces when their respective instance discrimination loss achieves minimization. Specifically, we assume domains A and $\\mathbf{B}$ share $C$ categories and there is only one category exclusively owned by domain B, i.e., ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathcal{Y}^{\\mathrm{A}}\\cap\\mathcal{Y}^{\\mathrm{B}}=\\{y^{i}\\}_{i=1}^{C},\\mathcal{Y}^{\\mathrm{B}}\\setminus\\mathcal{Y}^{\\mathrm{A}}=y^{C+1,\\mathrm{B}},\\,C^{\\mathrm{B}}=C^{\\mathrm{A}}+1.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "This assumption satisfies that the support sets of domains A and $\\mathbf{B}$ are not identical. Then suppose that the geometric structures of domains A and $\\mathbf{B}$ are identical when the instance discrimination loss has been minimized, according to Theorem A.1, both domains A and B present Simplex ETF representations. In this case, if we randomly select two categories $y^{p},y^{q}\\;\\in\\;\\mathcal{V}^{\\mathrm{A}}\\cap\\mathcal{V}^{\\mathrm{B}}$ from the domain-shared category set, and their samples within each domain have the following relation, ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\forall y^{p},y^{q}\\in\\mathcal{y}^{\\mathrm{A}}\\cap\\mathcal{y}^{\\mathrm{B}},\\frac{f_{\\theta}(x^{p,\\mathrm{A}})\\cdot f_{\\theta}(x^{q,\\mathrm{A}})}{|f_{\\theta}(x^{p,\\mathrm{A}})||f_{\\theta}(x^{q,\\mathrm{A}})|}=\\frac{-1}{C^{\\mathrm{A}}-1},\\frac{f_{\\theta}(x^{p,\\mathrm{B}})\\cdot f_{\\theta}(x^{q,\\mathrm{B}})}{|f_{\\theta}(x^{p,\\mathrm{B}})||f_{\\theta}(x^{q,\\mathrm{B}})|}=\\frac{-1}{C^{\\mathrm{B}}-1}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "According to Eq. (21), Eq. (22) implies that the same category pair across domains has different cosine similarities, which contradicts the assumption of identical geometry across domains. \u53e3 ", "page_idx": 13}, {"type": "text", "text": "Theorem 3.2 (Convergence of IPM). Suppose the data distribution of a domain has mutually disjoint supports, and it is uniform over these supports. Simplex Equiangular Tight Frame $(E T F)$ representations minimize the Instance-Prototype-Mixed Loss of this domain. ", "page_idx": 13}, {"type": "text", "text": "Proof. As shown in Eq. (10), the Instance-Prototype-Mixed loss is composed of instance discrimination and prototype contrastive loss. The simplex ETF representations have been proven to minimize instance discrimination in Theorem A.1. Then we only need to prove that the prototype contrastive loss satisfies the convex and non-increasing properties, ", "page_idx": 13}, {"type": "text", "text": "Property A.3. For a loss function $l$ defined on a set $\\boldsymbol{\\mathcal{V}}=\\{v_{i}\\}_{i=1}^{t}$ with the size of $t$ , it holds for all subsets of index $\\mathcal{S}\\subseteq\\{1,...,t\\}$ that ", "page_idx": 14}, {"type": "equation", "text": "$$\nl(\\mathcal{V})\\geq\\frac{1}{|\\mathcal{S}|}\\sum_{j\\in\\mathcal{S}}l(\\mathcal{V}^{\\mathcal{S}\\leftarrow j}),\\,w h e r e\\,\\mathcal{V}_{i}^{\\mathcal{S}\\leftarrow j}:=\\left\\{\\begin{array}{l l}{v_{i},\\,i f i\\notin\\mathcal{S}}\\\\ {v_{j},\\,o t h e r w i s e.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "To prove the non-increasing property of prototype contrastive loss, we can view $\\mathcal{L}_{\\mathrm{PNCE}}$ is built on the set $\\mathcal{V}=\\{v_{c}=f_{\\theta}(\\mathbf{x}_{i})\\cdot\\mathbf{\\dot{p}}_{c_{i}}/\\tau-f_{\\theta}(\\mathbf{\\dot{x}}_{i})\\cdot\\dot{\\mathbf{p}}_{c}/\\tau\\}_{c=1}^{C}$ , then $\\mathcal{L}_{\\mathrm{PNCE}}(\\pmb{x}_{i})\\,=\\,l_{\\log}(\\nu):=\\,\\log(1+$ $\\textstyle\\sum_{c}\\exp(-v_{c})\\bar{\\rangle}$ . We can leverage the concavity of the log function (Jensen\u2019s inequality) and denote $\\begin{array}{r}{\\overline{{T}}:=1+\\sum_{j\\notin\\mathcal{S}}\\exp(-v_{j})}\\end{array}$ , and we have ", "page_idx": 14}, {"type": "equation", "text": "$$\nl_{\\log}(\\mathcal{V})=\\log(T+\\sum_{c\\in\\mathcal{S}}\\exp(-v_{c}))\\geq\\frac{1}{\\left|\\mathcal{S}\\right|}\\sum_{c\\in\\mathcal{S}}\\log(T+\\left|\\mathcal{S}\\right|\\exp(-v_{c}))=\\frac{1}{\\left|\\mathcal{S}\\right|}\\sum_{c\\in\\mathcal{S}}l_{\\log}(\\mathcal{V}^{\\mathcal{S}\\leftarrow c}).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Therefore, the prototype contrastive loss also holds the non-increasing property, which proves that simplex ETF representations minimize the Instance-Prototype-Mixed Loss. \u53e3 ", "page_idx": 14}, {"type": "text", "text": "B Implementation Details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "B.1 Open-set Query Label Detection ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In open-set unsupervised cross-domain retrieval (UCDR) settings, the query domain has some private categories that are not included in the retrieval domain. In this case, the retrieval results of query samples for such private query categories should be null. As aforementioned, for a query sample $\\pmb{x}_{i}^{\\mathrm{A}}$ , the retrieval process needs to calculate the distance between all samples in the retrieval domain and $\\pmb{x}_{i}^{\\mathrm{A}}$ . Then the most similar retrieval samples are supposed to be those located as close to $\\pmb{x}_{i}^{\\mathrm{A}}$ as possible. Intuitively, if $\\pmb{x}_{i}^{\\mathrm{A}}$ belongs to the private query categories, the nearest sample in the retrieval domain should be relatively distant. Therefore, there is a need for a threshold that allows us to determine whether the closest retrieval sample of a query sample is located too far to be a similar sample. Next, let us introduce how our UEM detects and identifies whether a query sample belongs to private query categories. ", "page_idx": 14}, {"type": "text", "text": "In our UEM framework, the crucial design is to build a unified prototypical structure across domains, which also shapes the private label detection strategy. Specifically, after the training of CDSM, we apply K-Means to the query and retrieval domain datasets again to construct the prototype sets $\\mathcal{P}^{\\mathrm{A}}$ and $\\bar{\\mathcal{P}}^{\\mathrm{B}}$ . Then the prototypes of the retrieval domain are translated to the query domain for potential merging. The detailed prototype translation and merging have been introduced in Section 3.2.1. After the prototype merging, both $\\mathcal{P}^{\\mathrm{A}}$ and $\\mathcal{P}^{\\mathrm{B}}$ are divided into two groups by satisfying the merging condition (Eq. (9)) or not. For the prototype pairs that satisfy the merging condition, we record the maximum inter-sample distance among their clusters as ", "page_idx": 14}, {"type": "equation", "text": "$$\nD_{c\\oplus}=\\operatorname*{max}_{\\substack{x_{i}^{\\mathrm{A}}\\in\\mathcal{X}_{c\\oplus}^{\\mathrm{A}},\\,x_{j}^{\\mathrm{B}}\\in\\mathcal{X}_{c\\oplus}^{\\mathrm{B}}}}\\left[\\left(1-\\frac{f_{\\theta}(\\mathbf{x}_{i}^{\\mathrm{A}})\\cdot f_{\\theta}(\\mathbf{x}_{j}^{\\mathrm{B}})}{|f_{\\theta}(\\mathbf{x}_{i}^{\\mathrm{A}})||f_{\\theta}(\\mathbf{x}_{j}^{\\mathrm{B}})|}\\right)\\cdot d(f_{\\theta}(\\mathbf{x}_{i}^{\\mathrm{A}}),f_{\\theta}(\\mathbf{x}_{j}^{\\mathrm{B}}))\\right]\n$$", "text_format": "latex", "page_idx": 14}, {"type": "equation", "text": "$$\n\\gamma_{c^{\\oplus}}^{\\mathrm{A}}=\\left\\{x_{i}^{\\mathrm{A}}\\left|\\arg\\operatorname*{min}_{p^{\\mathrm{A}}}\\left[d(f_{\\theta}(x_{i}^{\\mathrm{A}}),p^{\\mathrm{A}})\\right]=p_{c^{\\oplus}}^{\\mathrm{A},\\oplus}\\right\\},\\mathcal{X}_{c^{\\oplus}}^{\\mathrm{B}}=\\left\\{x_{j}^{\\mathrm{B}}\\right|\\arg\\operatorname*{min}_{p^{\\mathrm{B}}}\\left[d(f_{\\theta}(x_{j}^{\\mathrm{B}}),p^{\\mathrm{B}})\\right]=p_{c^{\\oplus}}^{\\mathrm{B},\\oplus}\\right\\}\\left[\\left(\\left(\\left(\\mathcal{A}_{p}\\right)\\right)\\right)\\left(\\left(\\mathcal{A}_{p}\\right)\\right)\\left(\\mathcal{B}_{p}\\right)\\left(\\left(\\mathcal{A}_{p}\\right)\\right)\\right].\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Then for any query sample $\\pmb{x}_{i}^{\\mathrm{A}}$ , there are two cases for its belonging. One is $\\pmb{x}_{i}^{\\mathrm{A}}$ belongs to the clusters of prototypes unmerged, i.e., $\\mathcal{P}^{\\mathrm{A}}\\setminus\\mathcal{P}^{\\mathrm{A},\\oplus}$ . In this case, $\\pmb{x}_{i}^{\\mathrm{A}}$ is supposed to come from private query labels with high confidence. The other case is that the closest prototype of $\\pmb{x}_{i}^{\\mathrm{A}}$ satisfies the merging condition, i.e., ar $\\begin{array}{r}{\\mathrm{g}\\operatorname*{min}_{p^{\\mathrm{A}}}\\left[d(f_{\\theta}(\\pmb{x}_{i}^{\\mathrm{A}}),\\pmb{p}^{\\mathrm{A}})\\right]=\\pmb{p}_{c^{\\mathrm{\\oplus}}}^{\\mathrm{A},\\oplus}\\in\\mathcal{P}^{\\mathrm{A},\\oplus}}\\end{array}$ , in which we should compare the recorded $D_{c^{\\oplus}}$ and the minimal distance between all samples in the retrieval domain and $\\pmb{x}_{i}^{\\mathrm{A}}$ , i.e., ", "page_idx": 14}, {"type": "equation", "text": "$$\nD_{i}^{\\mathrm{A\\rightarrowB}}=\\operatorname*{min}_{x_{j}^{\\mathrm{B}}\\in\\mathcal{D}^{\\mathrm{B}}}\\left[\\left(1-\\frac{f_{\\theta}({\\boldsymbol x}_{i}^{\\mathrm{A}})\\cdot f_{\\theta}({\\boldsymbol x}_{j}^{\\mathrm{B}})}{|f_{\\theta}({\\boldsymbol x}_{i}^{\\mathrm{A}})||f_{\\theta}({\\boldsymbol x}_{j}^{\\mathrm{B}})|}\\right)\\cdot d(f_{\\theta}({\\boldsymbol x}_{i}^{\\mathrm{A}}),f_{\\theta}({\\boldsymbol x}_{j}^{\\mathrm{B}}))\\right].\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "If $D_{c^{\\oplus}}<D_{i}^{\\mathrm{A\\toB}}$ , we have faith that there is no sample similar enough in the retrieval domain, which means $\\pmb{x}_{i}^{\\mathrm{A}}$ should belong to private query categories. Otherwise, $\\pmb{x}_{i}^{\\mathrm{A}}$ comes from the shared label set, and we should conduct the normal retrieval operation. ", "page_idx": 15}, {"type": "text", "text": "B.2 Comparison Baseline Implementation ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In our evaluation process, we implement a number of state-of-the-art baseline methods to compare our proposed UEM in $\\bar{{\\tt U}}^{2}{\\tt C D R}$ . For a fair comparison, we ensure two principles for all these used baselines \u2013 one is that the training data consists of at least two domains, and the other is that the training data is unlabeled. Following these two principles, in addition to unsupervised cross-domain retrieval studies [7, 8, 17], two other problems share similar setups: cross-domain representation learning (CDRL) [14, 13] and unsupervised domain generalization (UDG) [31, 32]. For CDRL, the objective is to learn domain-generalizable representations that provide domain-transferable knowledge for any downstream task. One of the most typical downstream tasks of CDRL is cross-domain retrieval. As for UDG, in addition to achieving effective cross-domain representation learning, domain-generalizable classifiers are also needed. However, in our evaluation, the retrieval process does not require any classifier, thus we omit all techniques related to classifier training for the used UDG approaches. ", "page_idx": 15}, {"type": "text", "text": "For the specific setups of our experiments, we consider close-set, partial, and open-set UCDR. The close-set UCDR assumes that the label spaces of the query and retrieval domains are identical, which is the benchmark setup of other baseline methods. In this case, we follow the default settings of these baseline methods to evaluate their performance in close-set UCDR. As for the partial UCDR, the label space of the query domain is half of the retrieval label space, where most baseline methods can work normally without any modification. But some baseline approaches (PCS [13], UCDIR [7], DGDIR [8]), especially those based on prototype learning, require the knowledge of category numbers for domains, therefore, we suppose the category numbers are known to these approaches. The last open-set UCDR supposes the retrieval label space is half of the query label space. In this setup, the query domain has private categories, and if we conduct retrieval for samples from these categories, the retrieval results should be null. To detect private categories, we follow the strategy leveraged by UEM (Section B.1) to employ a similar one for the used baseline methods. Specifically, we divide all baseline methods into two groups by whether there is a dedicated nearest neighboring searching algorithm. For those (DARL [31]) that don\u2019t have the nearest neighboring search, we attach the searching algorithm used by UCDIR [7]. After conducting all training and operations of any baseline method, we use the nearest neighboring search to pair samples from the query and retrieval domains. Moreover, we conduct K-Means in the query domain to build the prototype set $\\mathcal{P}^{\\mathrm{A}}$ (for CoDA [17], we leverage its auxiliary classifiers to construct the prototype set). Then for each cluster in the query domain, we record the maximum inter-sample distance $\\bar{D}_{c}^{\\mathrm{A}}$ . Note that the distance measurement here is different from the product of minus cosine similarity and Euclidean distance (Eq. (25)), and different approaches use diverse measurement, e.g., UCDIR [7] uses cosine similarity while DN2A [32] leverages Euclidean distance. ", "page_idx": 15}, {"type": "table", "img_path": "zZVqZRXSao/tmp/9daf729a482229694f288a2d120db0190070856863f8b944a40a280353a1f09a.jpg", "table_caption": ["Table 5: Performance comparison (mAP@All) between ours and other baseline methods on OfficeHome in Close-set Unsupervised Cross-Domain Retrieval. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "C Additional Experiments ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Here we provide the additional experiment results including the close-set UCDR on Office-Home (Table 5), partial UCDR on Office-31 and DomainNet (Table 6), and open-set UCDR on Office-31 (Table 7) and Office-Home (Table 8). According to these results, we can obtain similar observations and conclusions to the main paper. First, our UEM can achieve the best performance in close-set ", "page_idx": 15}, {"type": "table", "img_path": "zZVqZRXSao/tmp/56981887d1e7ded7f33227031eaaf0a7971fed12306656a74b021617af53e83f.jpg", "table_caption": ["Table 6: Performance comparison (mAP $@$ All) between ours and other baseline methods on Office-31 and DomainNet in Partial Unsupervised Cross-Domain Retrieval. "], "table_footnote": [], "page_idx": 16}, {"type": "table", "img_path": "zZVqZRXSao/tmp/c0508b8a92371f8473c54efcd41c4c54b6548aca5e66821a2b688874d006558e.jpg", "table_caption": ["Table 7: Performance comparison $(\\mathrm{mAP}@\\$ All for shared-label set, detection accuracy for open-label set) between ours and other baselines on Office-31 in Open-set Unsupervised Cross-Domain Retrieval. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "UCDR, which is reflected by the average performance improvement of $3.1\\%$ on Office-Home. Moreover, UEM can exceed all other baseline methods much more substantially in both partial and open-set UCDR. Specifically, our methods outperform the best baseline with a margin of $5.0\\%$ on DomainNet and $12.0\\%$ on Office-31 in partial UCDR. As for open-set UCDR, UEM exceeds other baseline methods with a range of $6.8\\%$ and $7.9\\%$ on Office-31 for shared-set mAP $@$ All and open-set detection accuracy respectively, and such improvement is much higher on Office-Home in $15.1\\%$ for shared-set mAP $@$ All and $13.1\\%$ for open-set detection accuracy. In addition, if we compare the results of the same domain pairs between close-set and partial/open-set UCDR for baseline methods, we can observe that the performance drops a lot, which also validates the existence of geometry distinctness (Theorem 3.1). In particular, such geometry distinctness originating from the category space difference can incur a performance drop of up to $29\\%$ for DGDIR on Office-31 between close-set and partial UCDR. ", "page_idx": 16}, {"type": "text", "text": "D Broader Impact ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "The research development for solving Universal Unsupervised Cross-Domain Retrieval $({\\tt U}^{2}{\\tt C D R})$ has the potential to make a significant positive impact across various sectors. By enabling more accurate and flexible retrieval of information across different domains without the need for supervision and the concern about semantic category distinctness, our proposed UEM framework can enhance user experiences in product recommendation systems, leading to more personalized and relevant suggestions. In the realm of artistic creation, UEM can facilitate novel connections and inspirations by retrieving cross-domain artistic elements, fostering creativity and innovation. Beyond these applications, UEM can also be beneficial in healthcare, finance, and scientific research with certain adaptive modifications. In healthcare, it can improve diagnostic tools and personalized treatment plans by integrating diverse data sources. In finance, it can enhance risk assessment and fraud detection by analyzing cross-domain financial data. In scientific research, it can accelerate discoveries by connecting insights from different fields. While UEM has substantial benefits, it is crucial that its deployment adheres to existing privacy and intellectual property protection regulations and policies. Ensuring that data used in cross-domain retrieval respects user privacy and intellectual property rights is essential to prevent misuse and maintain public trust. Thus, applying this research in the real world should consider ethical issues to ensure responsible and fair use, thereby aiming for a positive societal impact without any negative social consequences. ", "page_idx": 16}, {"type": "table", "img_path": "zZVqZRXSao/tmp/50db3f0d01c14c6b4531859a52a321a8fcb2661bdabd5a57a3859ad6fa954ed5.jpg", "table_caption": ["Table 8: Performance comparison $(\\mathrm{mAP}@\\$ All for shared-label set, detection accuracy for open-label set) between ours and others on Office-Home in Open-set Unsupervised Cross-Domain Retrieval. "], "table_footnote": ["Ours 40.6 80.8 49.0 87.7 55.4 92.0 33.7 72.2 45.0 85.3 47.7 87.9 53.5 90.2 48.7 87.8 64.4 90.0 52.2 90.3 47.7 86.7 64.5 89.5 50.2 86.7 "], "page_idx": 17}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: The main claims in the abstract and introduction are addressed in Sections 3 and 4 of the main paper. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 17}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Justification: The limitations are discussed in Section 6 of the main paper. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. ", "page_idx": 17}, {"type": "text", "text": "\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. \u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 18}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: The main theoretical insights are discussed in Section 3 and detailed proofs are given in Section A of the Appendix. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 18}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: All the implementation details to reproduce the results are given in Section 4 of the main paper and Section B of the Appendix. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.   \n(b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.   \n(c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).   \n(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 19}, {"type": "text", "text": "Answer: [No] ", "page_idx": 19}, {"type": "text", "text": "Justification: We are checking the relevant regulations of our institutions to release the source code. All the datasets used for this work are publicly available. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 19}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: All the experiment details are given in Section 4 and the Appendix. Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 19}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: All the experiment results are supplemented with standard deviations as the error bar resulting from 3 independent runs. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 20}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: The Appendix discusses all relevant computing requirements in detail. Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 20}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Justification: Our work adheres to all the ethical guidelines outlined by NeurIPS. Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 20}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: The broader impacts are discussed in Section D of the Appendix. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 21}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: Our work does not pose any explicit risks as we use public datasets, and applying our proposed methods to other areas needs dedicated modifications. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 21}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: All dataset details and original authorship are cited in Section 4. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 22}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: The paper does not release any new assets. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 22}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: Our paper does not involve crowdsourcing or research with human subjects. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 22}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 22}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: Our paper does not involve crowdsourcing nor research with human subjects, hence do not require IRB approval. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 23}]