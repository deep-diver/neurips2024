{"importance": "This paper is crucial because **overconfident language models (LLMs)** hinder trust and effective human-AI collaboration.  The research directly addresses this by introducing a novel finetuning method, offering a significant advancement in LLM calibration and paving the way for more reliable and trustworthy AI systems.  Its findings are immediately relevant to the current focus on improving LLM reliability and open exciting avenues for future research on pragmatic AI.", "summary": "LACIE: Listener-aware finetuning improves LLM confidence calibration, reducing incorrect answers accepted by human listeners by 47% while maintaining correct answer acceptance.", "takeaways": ["LACIE, a novel listener-aware finetuning method, significantly improves large language model (LLM) confidence calibration.", "Human evaluation demonstrates a 47% reduction in incorrect answers accepted due to LACIE, enhancing model trustworthiness.", "LACIE's improvements generalize to other datasets, indicating broad applicability and robustness."], "tldr": "Large Language Models (LLMs) often suffer from overconfidence, providing unreliable answers and eroding user trust.  This overconfidence stems from a lack of pragmatic grounding \u2013 LLMs don't consider how their answers are perceived by a listener and aren't trained on the consequences of providing incorrect or poorly phrased information.  This lack of listener-awareness makes it challenging to effectively calibrate both implicit (tone, details) and explicit (numeric scores) confidence cues. \nTo tackle this problem, the researchers propose LACIE (Listener-Aware Calibration in LLMs).  LACIE employs a novel two-agent (speaker-listener) training approach using Direct Preference Optimization (DPO). This method directly models the listener's response to an LLM's answer, finetuning the model to align its confidence with how likely the answer is to be accepted by a human or simulated listener.  The experiments demonstrate that LACIE substantially improves LLM calibration across various models and datasets, significantly reducing the acceptance of incorrect answers by human evaluators.  This listener-aware approach offers a powerful technique for improving LLM trustworthiness and opens up new avenues for research in pragmatic AI.", "affiliation": "UNC Chapel Hill", "categories": {"main_category": "Natural Language Processing", "sub_category": "Large Language Models"}, "podcast_path": "RnvgYd9RAh/podcast.wav"}