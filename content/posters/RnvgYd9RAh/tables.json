[{"figure_path": "RnvgYd9RAh/tables/tables_6_1.jpg", "caption": "Table 1: TriviaQA performance (and standard error) with metrics computed according to a Mistral-7B listener model. We bold the best value for each model.", "description": "This table presents the performance of different LLMs (Mistral-7B, Llama-3 8B, Llama-3 70B) on TriviaQA using a Mistral-7B listener model.  The performance is measured using several metrics: AUROC (Area Under the Receiver Operating Characteristic Curve), ECE (Expected Calibration Error), Precision, Recall, and the percentage of responses where the model abstained from answering.  Multiple baselines are included for comparison: the base LLM model, instruction-tuned chat versions of the LLMs, and a model finetuned to maximize truthfulness (truthful-only baseline). The main focus is to show the improvement in calibration achieved by the LACIE method (listener-aware finetuning).", "section": "4.2 Results with Modeled Listener"}, {"figure_path": "RnvgYd9RAh/tables/tables_6_2.jpg", "caption": "Table 2: Evaluation with human listeners. When outputs are shown to people, LACIE leads to fewer incorrect outputs being accepted, without significantly increasing the rate of false rejections. Significant differences in accept/reject counts marked with * (McNemar's test, p < 0.5).", "description": "This table presents the results of a human evaluation comparing the performance of the base Mistral-7B model and the LACIE-finetuned Mistral-7B model.  Human annotators were asked to accept or reject answers generated by each model to TriviaQA questions. The table shows the number of true accepts, false accepts, and false rejects for each model, along with precision and recall scores.  The asterisk (*) indicates statistically significant differences (p<0.05 using McNemar's test) between the two models' accept/reject counts.", "section": "4.3 Human Evaluation"}, {"figure_path": "RnvgYd9RAh/tables/tables_8_1.jpg", "caption": "Table 3: TruthfulQA performance with Mistral-7B, measured by Truthfulness (Truth.) and Informativeness (Info.) metrics.", "description": "This table presents the results of evaluating the Mistral-7B model's performance on the TruthfulQA benchmark after training with different methods: a base model, a model trained only to improve truthfulness, and a model trained with the LACIE method.  The metrics used are Truthfulness and Informativeness, reflecting the balance between factual correctness and the amount of information provided in the model's responses.  The table shows that LACIE training significantly improves truthfulness compared to both baselines, albeit at a slight cost to informativeness.", "section": "Experiments and Results"}, {"figure_path": "RnvgYd9RAh/tables/tables_9_1.jpg", "caption": "Table 1: TriviaQA performance (and standard error) with metrics computed according to a Mistral-7B listener model. We bold the best value for each model.", "description": "This table presents the results of evaluating different language models on the TriviaQA dataset, using a Mistral-7B model as a listener to assess the calibration of the speaker models.  It compares baseline models (with and without truthful-only fine-tuning) to models fine-tuned with LACIE (Listener-Aware Calibration for Implicit and Explicit confidence) and a prompt-based method from prior work (Tian et al., 2023). Metrics reported include AUROC (Area Under the Receiver Operating Characteristic curve), ECE (Expected Calibration Error), Precision, Recall, and the percentage of times the model abstained from answering.  The table shows how LACIE improves calibration and reduces overconfidence compared to the baselines across various model sizes (Mistral-7B, Llama-8B, Llama-70B).", "section": "4.2 Results with Modeled Listener"}, {"figure_path": "RnvgYd9RAh/tables/tables_14_1.jpg", "caption": "Table 1: TriviaQA performance (and standard error) with metrics computed according to a Mistral-7B listener model. We bold the best value for each model.", "description": "This table presents the results of evaluating several language models on the TriviaQA dataset using a Mistral-7B listener model.  The models are compared using several metrics to assess their calibration: AUROC (Area Under the Receiver Operating Characteristic Curve), ECE (Expected Calibration Error), Precision, Recall, and the percentage of answers where the model abstained.  The models evaluated include baseline Mistral-7B and Llama3 models, versions of these models finetuned for truthfulness, versions finetuned with the LACIE method, and baselines from prior work. The table shows that the LACIE method generally leads to better calibration and precision, especially in terms of reducing the acceptance rate of incorrect answers.", "section": "4.2 Results with Modeled Listener"}, {"figure_path": "RnvgYd9RAh/tables/tables_14_2.jpg", "caption": "Table 1: TriviaQA performance (and standard error) with metrics computed according to a Mistral-7B listener model. We bold the best value for each model.", "description": "This table presents the performance of different LLMs (Mistral-7B, Llama-3 8B, Llama-3 70B) on TriviaQA, using a Mistral-7B listener model to evaluate induced listener calibration.  The models were tested in their base and instruction-tuned forms and compared against baselines (truthful-only, and Tian et al. 2023). Metrics include AUROC, ECE, precision, recall, and percentage of abstentions.", "section": "4.2 Results with Modeled Listener"}, {"figure_path": "RnvgYd9RAh/tables/tables_14_3.jpg", "caption": "Table 1: TriviaQA performance (and standard error) with metrics computed according to a Mistral-7B listener model. We bold the best value for each model.", "description": "This table presents the performance of different language models on the TriviaQA dataset, evaluated using a Mistral-7B listener model.  The models are categorized as base models, instruction-tuned (chat) models, models fine-tuned with a focus on truthfulness, and models fine-tuned with LACIE.  Metrics include AUROC (Area Under the Receiver Operating Characteristic Curve), ECE (Expected Calibration Error), Precision, Recall, and the percentage of responses where the model abstained from answering. The table highlights the improvement in listener calibration and precision achieved by the LACIE fine-tuning method.", "section": "4.2 Results with Modeled Listener"}, {"figure_path": "RnvgYd9RAh/tables/tables_15_1.jpg", "caption": "Table 8: Count of true accepts, false accepts and false rejects for Mistral-7B on TriviaQA.", "description": "This table presents the counts of true accepts, false accepts, and false rejects for the Mistral-7B model evaluated on the TriviaQA dataset. It shows the number of times human annotators correctly accepted correct answers, incorrectly accepted incorrect answers, and incorrectly rejected correct answers for different model training settings: the base model, the instruction-tuned chat model, the base model finetuned only on truthful answers, and the base model finetuned with the listener-aware method (LACIE).  The numbers reflect the performance of each model training variant in terms of human judgment agreement, highlighting the impact of different training approaches.", "section": "4.3 Human Evaluation"}]