[{"figure_path": "RxXdokK2qz/figures/figures_5_1.jpg", "caption": "Figure 1: Comparison of \u03b8n, \u03b8n and \u03b8n/2:n for various \u03b1.", "description": "This figure shows sample trajectories of three different estimators of the parameter \u03b8 in a stochastic approximation algorithm with constant step size \u03b1 and Markovian noise.  The three estimators are: the standard iterative estimator \u03b8n, the Polyak-Ruppert average \u03b8n, and a modified Polyak-Ruppert average \u03b8n/2:n which averages only the second half of the iterations. The figure illustrates the impact of the step size \u03b1 on the estimators' convergence and bias.  Smaller step sizes (\u03b1) lead to a reduced variance and bias in the Polyak-Ruppert averages.", "section": "4.1 Theoretical results"}, {"figure_path": "RxXdokK2qz/figures/figures_6_1.jpg", "caption": "Figure 1: Comparison of \u03b8n, \u03b8n and \u03b8n/2:n for various \u03b1.", "description": "The figure shows sample trajectories of three different estimators for the parameter \u03b8 in a stochastic approximation algorithm with Markovian noise and constant step size \u03b1.  The estimators are: \u03b8n (the standard iterative estimator), \u03b8n (the Polyak-Ruppert average), and \u03b8n/2:n (a modified Polyak-Ruppert average starting from the middle of the trajectory).  The plots illustrate the convergence behavior of these estimators for different values of \u03b1, demonstrating how the bias and variance of the estimates change with the step size. In particular, it shows how the Polyak-Ruppert average significantly reduces the variance and improves the estimate's closeness to the true value, even more so when starting the average from halfway through the trajectory.", "section": "4.1 Theoretical results"}, {"figure_path": "RxXdokK2qz/figures/figures_7_1.jpg", "caption": "Figure 1: Comparison of \u03b8n, \u03b8n and \u03b8n/2:n for various \u03b1.", "description": "This figure compares the performance of three different estimators for the parameter \u03b8 in a stochastic approximation algorithm with Markovian noise and constant step size \u03b1.  The three estimators are:\n\n1. **\u03b8n:** The raw, un-averaged estimate at iteration n.\n2. **\u03b8n:** The Polyak-Ruppert average of \u03b8n over all iterations up to n.\n3. **\u03b8n/2:n:** The Polyak-Ruppert average of \u03b8n from iteration n/2 to n.\n\nThe figure shows sample trajectories for various values of \u03b1 (step sizes).  It demonstrates that Polyak-Ruppert averaging significantly reduces the noise in the estimate compared to the raw estimate (especially for small \u03b1).  Averaging only over the latter half of the iterations (\u03b8n/2:n) also exhibits a better convergence rate than averaging over all iterations (\u03b8n).", "section": "4.1 Theoretical results"}, {"figure_path": "RxXdokK2qz/figures/figures_8_1.jpg", "caption": "Figure 4: Illustration of the behavior of \u03b8n, \u03c6n(\u03b80) and \u03c6n\u2212k(\u03b8k).", "description": "This figure illustrates the comparison between the stochastic recurrence and its deterministic counterpart. The left panel shows the trajectories of \u03b8n and \u03c6n(\u03b80) for the same source of randomness. The right panel shows the effect of changing the value of k in \u03c6n\u2212k(\u03b8k). For n > k, it represents the stochastic recurrence applied to the first k steps and then the deterministic recurrence for the remaining steps (n\u2212k). For n < k, it represents only the deterministic recurrence. The figure is used to illustrate the methodology of the proof in Section 5.2.", "section": "Main results and illustrations"}, {"figure_path": "RxXdokK2qz/figures/figures_12_1.jpg", "caption": "Figure 1: Comparison of \u03b8n, \u03b8n and \u03b8n/2:n for various \u03b1.", "description": "This figure compares the behavior of three different estimators of the root of the ODE (\u03b8*, which is 1 in the example) as a function of the step size \u03b1.  The first estimator (\u03b8n) is the direct output of the stochastic approximation algorithm, while the other two (\u03b8n and \u03b8n/2:n) use Polyak-Ruppert averaging to reduce the variance.  The plots show sample trajectories for various values of \u03b1, highlighting how the averaging techniques improve the accuracy of the estimation. The plot shows that the Polyak-Ruppert averaging significantly reduces the variance, bringing the estimate closer to the true value (\u03b8* = 1). This improvement is more pronounced for smaller values of \u03b1.", "section": "4.1 Theoretical results"}, {"figure_path": "RxXdokK2qz/figures/figures_13_1.jpg", "caption": "Figure 6: Example of a model with a periodic Xn illustrating the necessity of averaging the iterates.", "description": "This figure shows the necessity of averaging the iterates when the Markov chain Xn is periodic.  Two plots are presented, illustrating the behavior of the iterates \u03b8n and their averages \u03b8n for different step sizes (\u03b1 = 0.1 and \u03b1 = 0.05).  While the averaged iterates \u03b8n converge smoothly toward the equilibrium \u03b8*, the non-averaged iterates \u03b8n exhibit oscillations due to the periodic nature of Xn, highlighting the importance of averaging to obtain stable results.", "section": "A.4 Why is Theorem 1 for \u03b8n and Theorem 2 and 3 for \u03b8n?"}]