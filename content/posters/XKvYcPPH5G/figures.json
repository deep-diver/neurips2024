[{"figure_path": "XKvYcPPH5G/figures/figures_4_1.jpg", "caption": "Figure 1: SPO search: n rollouts, represented by particles x\u00b9, ..., xn, each of which represents an SMC trajectory sample, are performed in parallel according to \u03c0i (left to right). At each environment step, the weights of the particles are adjusted (indicated in the diagram by circle size). We show two resampling regions where particles are resampled, favouring those with higher weights, and their weights are reset. The target distribution is estimated from the initial actions of the surviving particles (rightmost particles). This target estimate, qi, is then used to update \u03c0 in the M-step.", "description": "This figure illustrates the SPO search process, which consists of parallel rollouts and weight updates.  Multiple trajectories are sampled according to the current policy (\u03c0i), and their importance weights are adjusted at each step, with periodic resampling to prevent weight degeneracy.  The initial actions of the surviving particles after the search provides an estimate of the target distribution (qi), which is then used in the M-step (policy optimization).", "section": "4.2.1 Estimating Target Distribution"}, {"figure_path": "XKvYcPPH5G/figures/figures_7_1.jpg", "caption": "Figure 2: Learning curves for discrete and continuous environments. The Y-axis represents the interquartile mean of min-max normalised scores, with shaded regions indicating 95% confidence intervals, across 5 random seeds.", "description": "This figure displays the learning curves for both discrete and continuous control tasks.  The y-axis shows the normalized mean episode return, representing the average performance of each algorithm across 5 different random seeds. Shaded regions show 95% confidence intervals, illustrating the variability in performance. The x-axis represents the number of timesteps during training.  The figure compares SPO's performance against several baselines (MPO, PPO, VMPO, SMC-ENT, AlphaZero) across various environments (Ant, HalfCheetah, Humanoid, Rubik's Cube 7, Boxoban Hard). The results highlight SPO's improved and more consistent performance across diverse tasks compared to other methods.", "section": "5.2 Results"}, {"figure_path": "XKvYcPPH5G/figures/figures_8_1.jpg", "caption": "Figure 3: (left) Scaling: Mean normalised performance across all continuous environments on 108 environment steps, varying particle numbers N and horizon h for SPO during training. (right) Wall Clock Time Comparison: Performance on Rubik's cube plotted against wall-clock time for AlphaZero and 3 versions of SPO (varying by SMC search depth), with total search budget labeled at each point.", "description": "This figure shows two plots. The left plot shows how the performance of SPO scales with the number of particles and the search depth (horizon) during training. The x-axis represents search depth (horizon), and the y-axis shows the normalized mean episode return. Different colored bars show results for different numbers of particles. The right plot compares the wall-clock time performance of AlphaZero and SPO on the Rubik's Cube task. The x-axis represents time per step, and the y-axis shows the solve rate. Different colored lines represent different versions of SPO with varying search depths (horizons). The total search budget for each point is also shown.", "section": "5.3 Scaling SPO during training"}, {"figure_path": "XKvYcPPH5G/figures/figures_16_1.jpg", "caption": "Figure 1: SPO search: n rollouts, represented by particles x\u00b9, \u2026, xn, each of which represents an SMC trajectory sample, are performed in parallel according to \u03c0i (left to right). At each environment step, the weights of the particles are adjusted (indicated in the diagram by circle size). We show two resampling regions where particles are resampled, favouring those with higher weights, and their weights are reset. The target distribution is estimated from the initial actions of the surviving particles (rightmost particles). This target estimate, qi, is then used to update \u03c0 in the M-step.", "description": "This figure illustrates the SPO search process. Multiple trajectory samples (particles) are generated in parallel using the current policy.  At each step, particle weights are updated based on their performance, favoring high-performing trajectories. Periodically, particles are resampled to focus computation on the most promising trajectories. The initial actions of the surviving particles provide an estimate of the target distribution, used to improve the policy in the M-step of the EM algorithm.", "section": "4.2.1 Estimating Target Distribution"}, {"figure_path": "XKvYcPPH5G/figures/figures_18_1.jpg", "caption": "Figure 5: Brax", "description": "This ablation shows that SPO with an adaptive temperature is among the top performing hyperparameter settings across all environments. However we also note that it is possible to tune a temperature that works well when considering a wide range of temperatures. This is consistent with previous results in Peng et al. [62] that also find practically for specific problems a fixed temperature can be used. Of course in practice having an algorithm that can learn this parameter itself is practically beneficial, removing the need for costly hyperparameter tuning, since the appropriate temperature is likely problem dependant. Subsequently, we evaluated whether the partial optimisation of the temperature parameter \u03b7 effectively maintained the desired KL divergence constraint and how different values of this constraint affected performance.", "section": "B Ablations"}, {"figure_path": "XKvYcPPH5G/figures/figures_18_2.jpg", "caption": "Figure 5: Brax", "description": "This figure shows the ablation study on the effect of using fixed temperature values for the KL divergence constraint in the Expectation Maximisation framework against using an adaptive temperature updated every iteration in SPO.  The x-axis represents training timesteps, while the y-axis represents the normalised mean episode return.  The plot compares SPO with an adaptive temperature to SPO with fixed temperature values of 0.1, 1.0, 5.0, and 10.0. The shaded regions represent 95% confidence intervals. The results suggest that using an adaptive temperature leads to better performance overall compared to using various fixed temperatures.", "section": "B Ablations"}, {"figure_path": "XKvYcPPH5G/figures/figures_18_3.jpg", "caption": "Figure 7: (a) The estimated KL divergence between the prior policy \u03c0 and the target policy q generated by SMC for different values of \u03b5 during training on the Brax Ant task. (b) Evaluation performance during training for different values of \u03b5.", "description": "This figure shows the effect of different KL divergence constraints (\u03b5) on both the KL divergence between the prior and target policies and the resulting performance.  Subfigure (a) plots the KL divergence over training steps for various \u03b5 values, demonstrating how the constraint is maintained. Subfigure (b) shows the corresponding performance curves, indicating that a larger KL divergence can lead to better performance.", "section": "B Ablations"}, {"figure_path": "XKvYcPPH5G/figures/figures_19_1.jpg", "caption": "Figure 8: Q-value ablation on Brax tasks", "description": "This ablation study compares the performance of SPO using advantages against using Q-values for the policy improvement step within the Expectation Maximisation framework.  The results show that using advantages consistently outperforms using Q-values across all Brax benchmark tasks.  The shaded areas represent 95% confidence intervals across five random seeds.", "section": "B Ablations"}, {"figure_path": "XKvYcPPH5G/figures/figures_19_2.jpg", "caption": "Figure 9: Comparison of the KL divergence to large Monte Carlo simulation of target policy for different planning horizons and particle counts for Sokoban", "description": "This figure shows the KL divergence between the SMC estimated target policy and a Monte Carlo oracle for Sokoban.  It demonstrates the impact of increasing the number of particles and planning depth (horizon) on the accuracy of the SMC estimation.  Higher particle counts and deeper planning horizons lead to lower KL divergence, indicating improved estimation of the target distribution. The results highlight the importance of balancing breadth (particles) and depth (planning horizon) in SMC for accurate target estimation.", "section": "B.3 SMC Target Estimation Validation"}, {"figure_path": "XKvYcPPH5G/figures/figures_20_1.jpg", "caption": "Figure 10: Aggregate point metrics for Brax suite. 95% confidence intervals generated from stratified bootstrapping across tasks and seeds are reported.", "description": "This figure presents a comparison of different reinforcement learning algorithms across various continuous control tasks from the Brax suite.  It shows the median, interquartile mean (IQM), and mean normalized episode returns achieved by each algorithm. The 95% confidence intervals reflect the uncertainty in the performance estimates due to the limited number of runs and random seeds.  The results are aggregated and visualized across multiple tasks to give a robust comparison of the algorithms.", "section": "C.1 Detailed Summary Results"}, {"figure_path": "XKvYcPPH5G/figures/figures_20_2.jpg", "caption": "Figure 10: Aggregate point metrics for Brax suite. 95% confidence intervals generated from stratified bootstrapping across tasks and seeds are reported.", "description": "This figure shows the performance comparison across different algorithms on the Brax suite of continuous control environments.  Three performance metrics are displayed: Median, Interquartile Mean (IQM), and Mean.  Each metric represents the aggregated performance across all tasks and seeds within the Brax environment. The error bars represent the 95% confidence intervals, calculated using stratified bootstrapping, providing a measure of the statistical uncertainty associated with the performance estimates.", "section": "Expanded Results"}, {"figure_path": "XKvYcPPH5G/figures/figures_21_1.jpg", "caption": "Figure 12: Performance profiles. The Y-axis represents the fraction of runs that achieved greater than a specific normalised score represented on the x-axis, with shaded regions indicating 95% confidence intervals generated from stratified bootstrapping across both tasks and random seeds.", "description": "This figure presents performance profiles which visually illustrate the distribution of scores across all tasks and seeds for each algorithm.  The Y-axis shows the fraction of runs achieving a normalized score greater than the value on the X-axis. The shaded areas represent 95% confidence intervals, obtained through stratified bootstrapping.  This visualization helps to compare the algorithms' performance across a range of score thresholds and highlights the consistency (low variance) of the algorithms. The curves' relative positions indicate the algorithms' relative performance.", "section": "Expanded Results"}, {"figure_path": "XKvYcPPH5G/figures/figures_21_2.jpg", "caption": "Figure 13: Probability of Improvement.", "description": "This figure presents the probability of improvement plots for both Brax and Sokoban/Rubik's Cube environments.  It visually shows the likelihood that SPO outperforms another algorithm (VMPO, SMC-ENT, AZ, PPO, MPO) on a randomly selected task.  The plots show that SPO has a high probability of improvement compared to all baselines, with all probabilities exceeding 0.5 and their confidence intervals (CIs) entirely above 0.5.  This indicates statistical significance according to the methodology used in the paper.", "section": "Expanded Results"}, {"figure_path": "XKvYcPPH5G/figures/figures_22_1.jpg", "caption": "Figure 14: Performance across different environments.", "description": "This figure presents the performance of different reinforcement learning algorithms across various continuous and discrete control environments. Each subplot corresponds to a specific environment (Ant, HalfCheetah, Humanoid, Sokoban, and Rubik's Cube).  The y-axis shows the mean episode return, a metric measuring the average reward accumulated during an episode. The x-axis represents the training progress, measured in timesteps. The shaded regions indicate 95% confidence intervals, highlighting the uncertainty associated with the results.  The results illustrate that SPO consistently outperforms other algorithms across multiple environments. ", "section": "C.3 Individual Environment Results"}]