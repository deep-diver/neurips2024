[{"figure_path": "k1VrxRS6WZ/tables/tables_5_1.jpg", "caption": "Table 1: Characteristics of experimental data sets.", "description": "This table presents the characteristics of eight benchmark multi-label datasets used in the experiments. For each dataset, it provides the number of instances (|S|), the number of features (dim(S)), the number of class labels (L(S)), label cardinality (LCard(S)), label density (LDen(S)), the number of distinct label sets (DL(S)), and the proportion of distinct label sets (PDL(S)).  These characteristics help to understand the nature of each dataset and its suitability for evaluating multi-label learning models.", "section": "4 Experiments"}, {"figure_path": "k1VrxRS6WZ/tables/tables_6_1.jpg", "caption": "Table 2: Experimental results of each compared approach (mean\u00b1std) with different label batch size (denoted by #label). The best and the second best performance of each data set methods are highlighted in boldface and underline respectively. In addition, \n\u26ab/o indicates whether SLAN is statistically superior/inferior to the comparing approaches on each data set with pairwise t-test (at 0.05 significance level).", "description": "This table shows the experimental results comparing SLAN with other methods using different label batch sizes.  The table presents ranking loss for multiple datasets across various numbers of labels.  Statistical significance (p<0.05) is indicated to show whether SLAN outperforms other methods.", "section": "4.2 Experimental Results"}, {"figure_path": "k1VrxRS6WZ/tables/tables_7_1.jpg", "caption": "Table 2: Experimental results of each compared approach (mean\u00b1std) with different label batch size (denoted by #label). The best and the second best performance of each data set methods are highlighted in boldface and underline respectively. In addition, \u26ab/o indicates whether SLAN is statistically superior/inferior to the comparing approaches on each data set with pairwise t-test (at 0.05 significance level).", "description": "This table presents the experimental results comparing SLAN's performance against other methods across various datasets.  Different label batch sizes were used. The table shows the mean and standard deviation of the ranking loss for each method and dataset.  Statistical significance is indicated using a pairwise t-test at the 0.05 level.", "section": "4.2 Experimental Results"}, {"figure_path": "k1VrxRS6WZ/tables/tables_12_1.jpg", "caption": "Table 2: Experimental results of each compared approach (mean\u00b1std) with different label batch size (denoted by #label). The best and the second best performance of each data set methods are highlighted in boldface and underline respectively. In addition, \u2022/o indicates whether SLAN is statistically superior/inferior to the comparing approaches on each data set with pairwise t-test (at 0.05 significance level).", "description": "This table presents the experimental results comparing the performance of SLAN against other methods on several multi-label datasets.  The results are shown in terms of ranking loss, using different batch sizes of labels. Statistical significance (p<0.05) is indicated with symbols to show whether SLAN outperforms other methods.", "section": "4.2 Experimental Results"}, {"figure_path": "k1VrxRS6WZ/tables/tables_13_1.jpg", "caption": "Table 5: Experimental results of each compared approach (mean\u00b1std) in terms of Macro-averaging AUC with different label batch size (denoted by #label). The best and the second best performance of each data set methods are highlighted in boldface and underline respectively. In addition, \u2022/o indicates whether SLAN is statistically superior/inferior to the comparing approaches on each data set with pairwise t-test (at 0.05 significance level).", "description": "This table presents the Macro-averaging AUC scores achieved by different multi-label learning approaches (LIFT, MUENLPLR, SENCE, LIMIC, and SLAN) across various datasets under different experimental conditions.  Each dataset is tested with varying numbers of labels considered as known (indicated by #label), allowing for comparison of performance in open-set scenarios.  The best and second-best results for each dataset and method are highlighted, along with statistical significance testing to show if SLAN outperforms others.", "section": "4.2 Experimental Results"}, {"figure_path": "k1VrxRS6WZ/tables/tables_13_2.jpg", "caption": "Table 6: Win/tie/loss counts (pairwise t-test at 0.05 significant level) for SLAN against other multi-label approaches.", "description": "This table presents the results of pairwise t-tests comparing the performance of the proposed SLAN approach against four other multi-label learning methods (LIFT, MUENLPLR, SENCE, LIMIC) across five different evaluation metrics (Ranking loss, One-error, Coverage, Average precision, Macro-averaging AUC).  For each metric, the table shows the number of times SLAN performed better, tied, or worse than each of the other methods.  This provides a statistical assessment of the relative performance of SLAN.", "section": "4.2 Experimental Results"}, {"figure_path": "k1VrxRS6WZ/tables/tables_14_1.jpg", "caption": "Table 7: Experimental results of each compared approach (mean\u00b1std) in terms of Coverage with different label batch size (denoted by #label). The best and the second best performance of each data set methods are highlighted in boldface and underline respectively. In addition, \u2022/o indicates whether SLAN is statistically superior/inferior to the comparing approaches on each data set with pairwise t-test (at 0.05 significance level).", "description": "This table presents the results of five multi-label classification approaches (LIFT, MUENLPLR, SENCE, LIMIC, and SLAN) evaluated using the Coverage metric.  The experiment is repeated with different batch sizes of labels. The best and second-best results for each dataset and label batch are highlighted.  Statistical significance testing (pairwise t-test) compares SLAN against the other methods.", "section": "4.2 Experimental Results"}, {"figure_path": "k1VrxRS6WZ/tables/tables_14_2.jpg", "caption": "Table 8: Win/tie/loss counts (pairwise t-test at 0.05 significant level) for SLAN against other anomaly detection approaches.", "description": "This table presents the results of pairwise t-tests comparing the performance of SLAN against three other anomaly detection methods (OC-SVM, IFOREST, and MUENLFOREST) across multiple datasets.  The numbers represent the counts of wins, ties, and losses for SLAN in terms of the F-measure metric.  A win indicates that SLAN's performance is statistically significantly better than the compared method; a tie indicates no significant difference; and a loss means the compared method performed significantly better.", "section": "4 Experiments"}, {"figure_path": "k1VrxRS6WZ/tables/tables_15_1.jpg", "caption": "Table 2: Experimental results of each compared approach (mean\u00b1std) with different label batch size (denoted by #label). The best and the second best performance of each data set methods are highlighted in boldface and underline respectively. In addition, \\textbullet{}/\\textcircled{o} indicates whether SLAN is statistically superior/inferior to the comparing approaches on each data set with pairwise t-test (at 0.05 significance level).", "description": "This table presents the experimental results comparing SLAN with four other multi-label learning approaches across multiple datasets.  The results are shown using different label batch sizes, highlighting the best and second-best performing methods. Statistical significance is also indicated via a pairwise t-test.", "section": "4.2 Experimental Results"}]