[{"type": "text", "text": "Multi-Label Open Set Recognition ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yi-Bo Wang, Jun-Yi Hang, Min-Ling Zhang ", "page_idx": 0}, {"type": "text", "text": "School of Computer Science and Engineering, Southeast University, Nanjing 210096, China   \nKey Laboratory of Computer Network and Information Integration (Southeast University), Ministry of Education, China {wang_yb, hangjy, zhangml}@seu.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In multi-label learning, each training instance is associated with multiple labels simultaneously. Traditional multi-label learning studies primarily focus on closed set scenario, i.e. the class label set of test data is identical to those used in training phase. Nevertheless, in numerous real-world scenarios, the environment is open and dynamic where unknown labels may emerge gradually during testing. In this paper, the problem of multi-label open set recognition (MLOSR) is investigated, which poses significant challenges in classifying and recognizing instances with unknown labels in multi-label setting. To enable open set multi-label prediction, a novel approach named SLAN is proposed by leveraging sub-labeling information enriched by structural information in the feature space. Accordingly, unknown labels are recognized by differentiating the sub-labeling information from holistic supervision. Experimental results on various datasets validate the effectiveness of the proposed approach in dealing with the MLOSR problem. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Multi-label Learning (MLL) deals with the problem where an instance can be associated with multiple labels simultaneously [37, 19]. As a practical machine learning paradigm, multi-label learning has been widely applied in various real-world applications, such as image annotation [30], text categorization [26], information retrieval [12]. ", "page_idx": 0}, {"type": "text", "text": "Traditional multi-label learning studies focus on closed set scenario. That is, they assume that the class label set of test data is identical to that in the training set [37, 8, 21]. However, in many real-world scenarios, this assumption rarely holds because the environment is open and dynamic [29]. In addition to the extant label knowledge at training phase, the unknown labels may emerge gradually with the data streams during the testing phase. For example, in Figure 1, the test image is annotated with several relevant labels, some of which are unseen in the training set. The classification task becomes much more challenging because the label correlation between known and unknown labels may degrade the performance of the predictive model. Furthermore, due to the presence of unknown labels in the class label set of test data, these test data are hardly employed in subsequent learning processes, such as incremental learning [15]. ", "page_idx": 0}, {"type": "text", "text": "Motivated by the potential applications, we formalize a novel framework named multi-label open set recognition (MLOSR), whose goal is to learn a multi-label model that can correctly classify known labels for the unseen instance and recognize unknown labels within its relevant label set. This can be regarded as a special weakly supervised learning. In MLOSR, the most challenging part is to recognize the unknown labels associated with instances. Since we do not have any prior knowledge of unknown labels and they almost always co-occur with some known labels, it is very difficult to separate instances with unknown labels from those with the known labels only. ", "page_idx": 0}, {"type": "image", "img_path": "k1VrxRS6WZ/tmp/e815d1712239e4d334183f17a80183ef9f4d57dd3b0feca874f51211cb4a16d6.jpg", "img_caption": ["Figure 1: An example. The test image is associated with a variety of relevant labels. Among the set of relevant labels, \"cloud\", \"car\", \"ship\", \"building\", \"tree\" and \"sky\" are known labels seen in the training set, while \"sea\", \"mountain\" and \"trestle bridge\" are unknown labels emerging in the testing phase. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Open set recognition (OSR) is a paradigm previously proposed in [24] and is formalized as a riskminimizing constrained functional optimization problem. OSR describes a scenario where new classes unseen in training occur in testing, thus classifiers must be able to properly identify seen samples while rejecting unseen ones [9]. [24] proposes a 1-vs-Set machine to minimize open set risk by sculpting a decision space from the marginal distances of binary SVM. OSR problem is further studied via algorithm adaptation [1, 16], statistical extreme value theory (EVT) [25, 32], margin distribution [23] or hierarchical Dirichlet process [10]. These existing works are based on the fact that each instance owns one ground-truth label for multi-class cases. Thus, they cannot be used to directly solve the MLOSR problem, due to the multiple ground-truth labels in MLL. ", "page_idx": 1}, {"type": "text", "text": "To address MLOSR problem, we propose a tailored algorithm named SLAN, i.e. Sub-Labeling informAtion reconstructioN for multi-label open set recognition. The basic strategy of SLAN is to enrich the sub-labeling information in the sub-label space by leveraging the structural information in the feature space and differentiating it from the labeling information from holistic supervision. Specifically, the underlying structure of feature space is characterized by the sparse reconstruction relationships among training instances. After that, the reconstruction information is utilized to guide the enrichment of sub-labeling information. Then, a unified optimization framework is presented to simultaneously facilitate open set recognizer and multi-label classifier induced with alternating optimization. Our empirical study on datasets from diverse domains demonstrates the effectiveness of the proposed approach. ", "page_idx": 1}, {"type": "text", "text": "The rest of the paper is organized as follows. We present a brief review of related works. Then we formulate the problem and propose the algorithm. Next, experimental results are reported, followed by the conclusion. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "The task of multi-label learning has been extensively studied in recent years [37, 19]. Generally, the major challenge for multi-label learning is its huge output space which is exponential to the number of class labels. Therefore, exploiting label correlations has been adopted as a common strategy to facilitate the learning process. Roughly speaking, extant approaches can be grouped into three categories based on the order of correlations, i.e. first-order approaches, second-order approaches and high-order approaches. First-order approaches tackle multi-label learning problem in a label-by-label manner [2, 33]. Second-order approaches exploit pairwise interactions among class labels [7, 3]. High-order approaches exploit relationships among a subset of or all class labels [22, 14]. ", "page_idx": 1}, {"type": "text", "text": "OSR is critical for the tasks where incomplete knowledge exists at training time, and unknown classes can be submitted to an algorithm during the testing phase, requiring the classifiers to classify the seen classes and deal with unseen ones. According to [9], traditional machine learning methods are adapted to OSR scenario. For instance, SVM-based models add extra constraints on the score space in [24, 4]; A collective decision-based model implemented by hierarchical Dirichlet process is proposed in [10]; and distance-based models [1, 16] are developed by modifying existing classifiers, such as nearest class mean classifier and nearest neighbor classifier. Some other approaches focus on the EVT [17]. [25] combines the EVT for score calibration with two separated compact abating probability SVMs, where the first SVM is used as a conditioner and the second SVM fitted yields the posterior estimate. [32] transforms the OSR problem into a set of hypothesis testing problems by modeling the tail part of reconstruction error distribution via EVT. [23] formulates the extreme value machine with distributional information, which can be interpreted by EVT. There are also some methods trying to incorporate few-shot learning into OSR [5]. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "MLOSR can be regarded as a combination of MLL and OSR. Thus, a straightforward approach is to generate a independent recognizer besides the multi-label classifier. However, as unknown labels may co-occur with known labels, it is difficult to separate instances with unknown labels from instances with known labels only, which leads to OSR approaches that could not be applied in MLOSR problems. ", "page_idx": 2}, {"type": "text", "text": "Streaming multi-label learning (SMLL) [31] is similar to our MLOSR problem but differs in the setting of unknown labels. It aims to derive a unified model by taking care of the continually emerging new unknown labels on the training data. [31] trains a linear classifier for new labels with the linear hypotheses between labels and classifiers. [28] proposes a novel DNN-based framework to model the emerging new labels depending on high-order representations. [29] presents probabilistic streaming label tree to incorporate new labels, which capture hierarchical correlations among labels. Compared to SMLL, MLOSR is much more challenging in recognizing unknown labels as the unknown labels only emerge in testing phase. In the next section, the first attempt towards MLOSR is introduced. ", "page_idx": 2}, {"type": "text", "text": "3 The SLAN Approach ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Problem Formulation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Formally, let $\\mathcal{X}=\\mathbb{R}^{d}$ denote the $d$ -dimensional input space and $\\mathcal{V}=\\{l_{1},l_{2},\\ldots,l_{q}\\}$ denote the label space including $q$ class labels. Each multi-label instance can be denoted as $(x_{i},Y_{i})$ , where $\\mathbf{\\boldsymbol{x}}_{i}\\in\\mathcal{X}$ is its feature vector and $Y_{i}\\subseteq\\mathcal{Y}$ is the set of relevant labels associated with $\\pmb{x}_{i}$ . Here, a $q$ -dimensional indicator vector $\\pmb{y}_{i}=[y_{i1},y_{i2},...y_{i q}]^{\\top}$ is utilized to denote $Y_{i}$ , where $y_{i k}=1$ indicates class label $l_{k}\\,\\in\\,Y$ and $y_{i k}\\,=\\,-1$ otherwise. By arranging feature vectors and label vectors of $m$ training instances, we obtain the feature matrix $\\mathbf{X}=[\\bar{\\pmb{x}}_{1},\\bar{\\dots},\\pmb{x}_{m}]$ and label matrix $\\mathbf{Y}=[y_{1},\\ldots,y_{m}]$ . ", "page_idx": 2}, {"type": "text", "text": "Given the multi-label training set $\\mathcal{D}=\\{(\\pmb{x}_{i},Y_{i})\\mid1\\leq i\\leq m\\}$ , the goal of MLOSR is to learn a model from $\\mathcal{D}$ that can correctly classify known labels for the unseen instance and recognize unknown labels within its relevant label set. Conceptually, given the multi-label training set $\\mathcal{D}$ , an open space risk function $R_{\\mathcal{O}}$ and an empirical risk function $R_{\\varepsilon}$ , multi-label open set recognition aims to derive a measurable recognition function $f\\in\\mathcal H$ by minimizing the following Open Set Risk: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{argmin}_{f\\in\\mathcal{H}}R_{\\mathcal{O}}(f)+\\lambda_{r}R_{\\varepsilon}(f(\\mathcal{D}))\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\lambda_{r}$ is a regularization constant. ", "page_idx": 2}, {"type": "text", "text": "3.2 Structural Information Discovery ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "To characterize the underlying manifold structure of feature space, a weighted directed graph $G=$ $(\\boldsymbol{\\gamma},\\boldsymbol{\\mathcal{E}},\\mathbf{S})$ is constructed over the set of training instances, where $\\nu=\\{\\pmb{x}_{i}\\mid0\\leq i\\leq m\\}$ corresponds to the set of vertices and $\\mathcal{E}=\\{(\\pmb{x}_{i},\\pmb{x}_{j})\\mid s_{i j}\\,\\'\\neq0,1\\leq i\\neq j\\leq m\\}$ corresponds to the set of edges from $\\pmb{x}_{i}$ to $\\pmb{x}_{j}$ with nonzero weight. ", "page_idx": 2}, {"type": "text", "text": "Furthermore, $\\mathbf{S}=[s_{i j}]_{m\\times m}$ corresponds to the weight matrix encoding the relationships among all training instances. Conceptually, the weight value $s_{i j}$ reflects relative importance of $\\pmb{x}_{i}$ in reconstructing $\\pmb{x}_{j}$ . Thus, by implementing global sparse reconstruction, the weight matrix $\\mathbf{S}$ is instantiated by solving the following optimization problem: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\underset{\\mathbf{S}}{\\operatorname*{min}}\\,||\\mathbf{X}\\mathbf{S}-\\mathbf{X}||_{\\mathrm{F}}^{2}+\\mu_{0}||\\mathbf{S}||_{1}}\\\\ {\\mathrm{s}.\\mathrm{t}.\\quad s_{i i}=0,\\forall1\\leq i\\leq m}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Here, the first term controls the linear reconstruction error via squared Frobenius norm while the second term controls the sparsity of reconstruction via $\\ell_{1}$ norm. The relative importance is balanced ", "page_idx": 2}, {"type": "text", "text": "by the trade-off parameter $\\mu_{0}$ . Then, the constrained optimization problem in Eq.(2) can be solved via a standard ADMM (Alternating Direction Method of Multiplier) procedure [11]. ", "page_idx": 3}, {"type": "text", "text": "3.3 Sub-Labeling Information Enrichment ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "According to the manifold assumption, the structural relationship specified in the feature space should also be preserved in the entire label space to enrich labeling information originally encoding in the indicator vector $\\pmb{y}_{i}$ [13, 38]. That is, $\\pmb{y}_{i}$ can be transformed into a numerical labeling vector $z_{i}=[z_{i1},z_{i2},.\\ldots,z_{i q}]^{\\top}$ under holistic supervision which encodes richer semantics for predictive model induction. ", "page_idx": 3}, {"type": "text", "text": "However, this assumption might be suboptimal in the entire label space. Considering one specific label $l_{k}\\in\\mathcal{V}$ , let $\\pmb{f}_{i}^{k}=[f_{i1}^{\\hat{k}},\\dots,f_{i,k-1}^{\\check{k}},f_{i,k+1}^{k\\;\\;\\;\\hat{1}},\\dots,f_{i q}^{k}]^{\\intercal}$ represent the enriched sub-labeling information of $\\pmb{x}_{i}$ . With the manifold assumption, the structural information would be maintained in the sub-label space $\\mathcal{V}\\backslash\\{l_{k}\\}$ . Then, instances with $l_{k}$ can be reconstructed via the instances without $l_{k}$ as well as the enriched sub-labeling information. Nevertheless, these reconstructed instances can not be assigned with $l_{k}$ since there\u2019s not enough positive labeling information w.r.t. $l_{k}$ . That is, the sub-labeling information is differentiated from labeling information with holistic supervision. Thus, in open set scenario, if $l_{k}$ is specified as an unknown class label, such difference can be employed as a criterion for one specific instance to distinguish whether it is associated with an unknown class label. ", "page_idx": 3}, {"type": "text", "text": "Let concatenate all $\\pmb{f}_{i}^{k}$ , denoted by $\\mathbf{F}_{k}=[\\pmb{f}_{1}^{k},\\dots,\\pmb{f}_{m}^{k}]\\in\\mathbb{R}^{(q-1)\\times m}$ and all $z_{i}$ , denoted by $\\mathbf{Z}=$ $[z_{1},\\dots,z_{m}]\\ \\in\\ \\mathbb{R}^{q\\times{\\dot{m}}}$ . The enriched sub-labeling information is generated via leveraging the structural information encoded in $\\mathbf{S}$ by solving the following optimization problem: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{\\mathbf{F}_{1},\\ldots,\\mathbf{F}_{q}}{\\mathrm{min}}}&{\\frac{\\gamma}{2}\\displaystyle\\sum_{i=1}^{m}||z_{i}-y_{i}||_{2}^{2}+\\frac{\\beta}{2}\\displaystyle\\sum_{k=1}^{q}\\sum_{i=1}^{m}||f_{i}^{k}-\\sum_{j=1}^{m}s_{j i}f_{j}^{k}||_{2}^{2}}\\\\ &{\\displaystyle+\\,\\frac{\\alpha}{2}\\displaystyle\\sum_{k=1}^{q}\\sum_{i=1}^{m}||\\delta_{i}^{k}(f_{i}^{k}-\\mathbf{P}_{k}z_{i})||_{2}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Here, $\\delta_{i}^{k}$ is a indicator variable, where $\\delta_{i}^{k}=1$ if $l_{k}$ is not associated with $\\pmb{x}_{i}$ ; otherwise $\\delta_{i}^{k}=0$ . $\\mathbf{P}_{k}$ is a $(q-1)\\times q$ projection matrix to align $z_{i}$ with $\\pmb{f}_{i}^{k}$ without regard to $l_{k}$ , which removes the $k$ -th row of the identity matrix. The first term supervises the labeling information from the holistic aspect. The second term conveys the manifold structure specified in the feature space to the sub-label space. The third term minimizes the labeling information difference between sub-label space and entire label space to differentiate instances with or without such unknown class label, which implicitly reduces the open space risk. ", "page_idx": 3}, {"type": "text", "text": "During the testing phase, for an unseen instance $\\pmb{x}_{*}$ , the reconstruction coefficients w.r.t. $l_{k}$ is identified by resorting to the ADMM technique over the training set. After that, the enriched sublabeling information $\\bar{\\mathbf{f}}_{*}^{k}$ of $\\pmb{x}_{*}$ is determined via the second term of Eq.(3). Thereafter, whether unknown labels are associated with $^{x\\ast}$ is determined by the following recognizer $G$ via ensemble majority voting: ", "page_idx": 3}, {"type": "equation", "text": "$$\nG(\\pmb{x}_{*})=\\left\\{\\!\\!\\begin{array}{l l}{\\mathrm{unknown},}&{i f\\ \\sum_{k=1}^{q}g_{k}(\\pmb{x}_{*})\\leq0,}\\\\ {\\mathrm{known},}&{i f\\ \\sum_{k=1}^{q}g_{k}(\\pmb{x}_{*})>0.}\\end{array}\\!\\!\\right.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $g_{k}(\\mathbf{x}_{*})=-1$ if $||\\boldsymbol{f}_{*}^{k}-\\mathbf{P}_{k}\\boldsymbol{z}_{*}||_{2}^{2}>\\rho_{k}$ ; otherwise $g_{k}(\\mathbf x_{*})=1$ . $\\rho_{k}$ is the threshold, and can be chosen so that $100\\!\\times\\!\\tau\\%$ instances with $l_{k}$ in training set satisfy $||f_{i}^{k}-\\mathbf{P}_{k}z_{i}||_{2}^{2}>\\rho_{k}.$ . The labeling information $z_{\\ast}$ under holistic supervision is generated by the following MLL classifier. ", "page_idx": 3}, {"type": "text", "text": "3.4 MLL Classifier Training ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Similar to the existing strategy in previous OSR algorithms, instead of training the classifier independently, we perform open set recognizer and multi-label classifier induced simultaneously. Then, the sub-labeling information can be optimized by considering both the manifold assumption and model outputs. Furthermore, considering the label correlation between known and unknown labels, such jointly optimization procedure with recognizer can facilitate the classifier more robust. We denote $\\mathbf{\\dot{W}}\\in\\mathbb{R}^{\\dot{q}\\times d}$ and $\\mathbf{b}\\in\\mathbb{R}^{q}$ as a multi-label classifier and adopt the squared Frobenius norm as the regularization term to control the model complexity: ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{W,b}\\sum_{i=1}^{m}\\frac{1}{2}||\\mathbf{W}x_{i}+\\mathbf{b}-z_{i}||_{2}^{2}+\\frac{\\mu_{1}}{2}||\\mathbf{W}||_{\\mathrm{F}}^{2}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Let $\\mathbf{B_{k}}=[b_{p i}^{k}](q\\!-\\!1)\\!\\times\\!m$ denote the indicator matrix with $b_{p i}^{k}=\\delta_{i}^{k}$ . Then, the objective function of the unified framework is shown as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\operatorname*{min}_{\\mathbf{W},\\mathbf{b},\\mathbf{Z},\\mathbf{\\phi}}\\sum_{i=1}^{q}(\\frac{\\beta}{2}||\\mathbf{F}_{k}\\mathbf{S}-\\mathbf{F}_{k}||_{\\mathrm{F}}^{2}+\\frac{\\alpha}{2}||\\mathbf{B}_{k}\\circ(\\mathbf{P}_{k}\\mathbf{Z}-\\mathbf{F}_{k})||_{\\mathrm{F}}^{2})}}\\\\ &{\\quad\\quad\\quad+\\frac{1}{2}||\\mathbf{Z}-(\\mathbf{W}\\mathbf{X}+\\mathbf{b}\\mathbf{1}_{n}^{\\top})||_{\\mathrm{F}}^{2}+\\frac{\\gamma}{2}||\\mathbf{Z}-\\mathbf{Y}||_{\\mathrm{F}}^{2}}\\\\ &{\\quad\\quad\\quad+\\frac{\\mu_{1}}{2}||\\mathbf{W}||_{\\mathrm{F}}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Here, The first two terms control the open space risk and remaining terms control the empirical risk. ", "page_idx": 4}, {"type": "text", "text": "3.5 Alternative Optimization ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Update $\\mathbf{Z}$ With fixed $\\mathbf{F}_{1},\\ldots,\\mathbf{F}_{q}$ , $\\mathbf{W}$ and $\\mathbf{b}$ , the optimization problem Eq.(6) can be stated as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\underset{{\\bf Z}}{\\operatorname*{min}}\\frac{1}{2}||{\\bf Z}-({\\bf W}{\\bf X}+{\\bf b}{\\bf1}_{n}^{\\top})||_{\\mathrm{F}}^{2}+\\frac{\\gamma}{2}||{\\bf Z}-{\\bf Y}||_{\\mathrm{F}}^{2}}\\\\ &{\\displaystyle~~~+\\frac{\\alpha}{2}\\sum_{k=1}^{q}||{\\bf B}_{k}\\circ({\\bf P}_{k}{\\bf Z}-{\\bf F}_{k})||_{\\mathrm{F}}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The above optimization problem can be solved by updating $\\mathbf{Z}$ with gradient descent. The gradient of the objective function w.r.t. $\\mathbf{Z}$ is ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\displaystyle{\\nabla{\\bf Z}=({\\bf Z}-({\\bf W}{\\bf X}+{\\bf b1}_{n}^{\\top}))+\\gamma({\\bf Z}-{\\bf Y})}}\\ ~}\\\\ {{\\displaystyle~~~~+\\alpha\\sum_{k=1}^{q}{\\bf P}_{k}^{\\top}({\\bf B}_{k}\\circ({\\bf P}_{k}{\\bf Z}-{\\bf F}_{k}))}}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Update $\\mathbf{F}_{1},\\ldots,\\mathbf{F}_{q}$ With $\\mathbf{Z}$ , W and $\\mathbf{b}$ fixed, the optimization problem Eq.(6) can be stated as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathbf{F}_{k}}\\frac{\\beta}{2}||\\mathbf{F}_{k}\\mathbf{S}-\\mathbf{F}_{k}||_{\\mathrm{F}}^{2}+\\frac{\\alpha}{2}||\\mathbf{B}_{k}\\circ(\\mathbf{P}_{k}\\mathbf{Z}-\\mathbf{F}_{k})||_{\\mathrm{F}}^{2}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Similarly, gradient descent is employed, and the gradient w.r.t. $\\mathbf{F}_{k}$ is: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\nabla\\mathbf{F}_{k}=\\beta\\mathbf{F}_{k}\\mathbf{T}+\\alpha\\mathbf{B}_{k}\\circ\\left(\\mathbf{F}_{k}-\\mathbf{P}_{k}\\mathbf{Z}\\right)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathbf{T}=(\\mathbf{S}-\\mathbf{I}_{m\\times m})(\\mathbf{S}-\\mathbf{I}_{m\\times m})^{\\top}$ ", "page_idx": 4}, {"type": "text", "text": "Update W and b While $\\mathbf{Z}$ and $\\mathbf{F}_{1},\\ldots,\\mathbf{F}_{q}$ are fixed, the optimization problem (4) can be stated as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\mathbf{W},\\mathbf{b}}{\\operatorname*{min}}\\,t r(\\mathbf{E}\\mathbf{E}^{\\top})+\\mu_{1}t r(\\mathbf{W}\\mathbf{W}^{\\top})}\\\\ &{\\mathrm{s.t.}\\quad\\mathbf{Z}=\\mathbf{W}\\mathbf{X}+\\mathbf{b}\\mathbf{1}_{m}^{\\top}+\\mathbf{E}}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Here, $\\mathbf{E}=[e_{1},\\dots,e_{m}]\\in\\mathbb{R}^{q\\times m}$ , where $\\boldsymbol{e}_{i}=\\boldsymbol{z}_{i}-(\\mathbf{W}\\boldsymbol{x}_{i}+\\mathbf{b})$ . To achieve better performance of the predictive model, a kernel extension is further facilitated for the general nonlinear case. Let $\\Phi=\\left[\\stackrel{\\cdot}{\\phi}(\\pmb{x}_{1}),\\dots,\\phi(\\pmb{x}_{m})\\right]\\in\\mathbb{R}^{h\\times m}$ , where $\\phi(\\bullet):\\mathbb{R}^{d}\\to\\mathbb{R}^{h}$ corresponds to the feature mapping that maps the feature space to some higher dimensional Hilbert space with $h$ dimensions. Then, the Lagrangian function of this problem can be formulated as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}(\\mathbf{W},\\mathbf{b},\\mathbf{E},\\mathbf{A})=t r(\\mathbf{E}\\mathbf{E}^{\\top})+\\mu_{1}t r(\\mathbf{W}\\mathbf{W}^{\\top})}\\\\ &{\\qquad\\qquad\\qquad-t r(\\mathbf{A}^{\\top}(\\mathbf{W}\\Phi+\\mathbf{b}\\mathbf{1}_{m}^{\\top}+\\mathbf{E}-\\mathbf{Z}))}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Algorithm 1 The pseudo-code of SLAN ", "page_idx": 5}, {"type": "text", "text": "Input: The multi-label training set $\\mathcal{D}$ , the trade-off parameters $\\alpha,\\beta,\\gamma,\\mu_{1},\\tau$ , an unseen instance $^{x_{*}}$ Output: The predicted label set $Y_{*}$ for $\\pmb{x}_{*}$ , the recognition result $G(x_{*})$ .   \nProcess:   \n1: Instantiate the weighted graph $G=(\\boldsymbol{\\nu},\\boldsymbol{\\varepsilon},\\mathbf{S})$ by solving Eq.(2) with ADMM procedure; 2: Calculate the kernel matrix $\\mathbf{K}=[\\kappa(\\pmb{x}_{i},\\pmb{x}_{j})]_{m\\times m}$ ;   \n3: Initialize $\\mathbf{Z}$ with $\\mathbf{Y}$ ;   \n4: Initialize $\\mathbf{F}_{k}$ with $\\mathbf{P}_{k}\\mathbf{Y}$ $1\\leq k\\leq q)$ ;   \n5: repeat   \n6: Update $\\mathbf{Z}$ according to Eq.(7);   \n7: Update $\\mathbf{F}_{k}$ according to Eq.(9);   \n8: Update W and b according to Eq.(11);   \n9: until convergence or maximum number of iterations being reached   \n10: return $Y_{*}$ and $G(x_{*})$ according to Eq.(14) and Eq.(4). ", "page_idx": 5}, {"type": "text", "text": "where ${\\bf A}=[a_{k i}]\\in\\mathbb{R}^{q\\times m}$ stores the Lagrange multipliers. According to the KKT conditions, we can obtain: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbf{b}=\\frac{\\mathbf{Z}\\mathbf{H}^{-1}\\mathbf{1}_{m}}{\\mathbf{1}_{m}^{\\top}\\mathbf{H}^{-1}\\mathbf{1}_{m}}}\\\\ {\\displaystyle\\mathbf{A}=(\\mathbf{Z}-\\mathbf{b}\\mathbf{1}_{m}^{\\top})\\mathbf{H}^{-1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\begin{array}{r}{\\mathbf{H}\\,=\\,\\frac{1}{\\mu_{1}}\\mathbf{K}+\\mathbf{I}_{m\\times m}}\\end{array}$ and $\\mathbf{K}\\,=\\,\\Phi^{\\top}\\Phi$ with its element $k_{i j}\\,=\\,\\kappa({\\pmb x}_{i},{\\pmb x}_{j})\\,=\\,\\phi({\\pmb x}_{i})^{\\top}\\phi({\\pmb x}_{j})$ based on the chosen kernel function $\\kappa(\\cdot,\\cdot)$ . Then, by incorporating the specified kernel function, the modeling output is denoted by $\\scriptstyle{\\frac{1}{\\mu_{1}}}\\mathbf{A}\\mathbf{K}\\,+\\,\\mathbf{b}\\mathbf{1}_{m}^{\\top}$ . Furthermore, given an unseen instance $\\mathbf{\\mathcal{x}}_{*}\\in\\mathcal{X}$ , its relevant label set is predicted as: ", "page_idx": 5}, {"type": "equation", "text": "$$\nY_{*}=\\{l_{k}\\ |\\sum_{i=1}^{m}a_{k i}\\kappa(\\mathbf{x}_{*},\\mathbf{x}_{i})\\geq0,0\\leq k\\leq q\\}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The pseudo-code of SLAN is summarized in Algorithm 1. Given the multi-label training set, a weighted graph is constructed to characterize the manifold structure of feature space (Step 1). After that, the alternative optimization strategy is adopted to optimize open set recognizer and multi-label classifier simultaneously (Step 2-9). Finally, the relevant label set of the unseen instance is predicted and the recognition result is generated based on the learned model (Step 10). ", "page_idx": 5}, {"type": "table", "img_path": "k1VrxRS6WZ/tmp/14bcfee559fff11a44429280ac75944691ba1d8142cd564c8d480a93de16f0dc.jpg", "table_caption": ["Table 1: Characteristics of experimental data sets. "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "4.1 Experimental Setup ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Table 1 summarizes the detailed characteristics of each benchmark multi-label data set $\\boldsymbol{S}$ employed in the experiments, including the number of instances $|{\\mathcal{S}}|$ , number of features $d i m(S)$ , number of class labels $L(S)$ , label cardinality $L C a r d(S)$ , label density $L D e n(S)$ , number of distinct label sets $D L(S)$ and proportion of distinct label sets $P D L(S)$ . To alleviate the influence of extreme ", "page_idx": 5}, {"type": "text", "text": "Table 2: Experimental results of each compared approach (mean $\\pm$ std) with different label batch size (denoted by #label). The best and the second best performance of each data set methods are highlighted in boldface and underline respectively. In addition, $\\bullet/\\circ$ indicates whether SLAN is statistically superior/inferior to the comparing approaches on each data set with pairwise t-test (at 0.05 significance level). ", "page_idx": 6}, {"type": "table", "img_path": "k1VrxRS6WZ/tmp/8a66bc482826f7e5e00e3036faee0219b1fdabecc978023cfd397e47ac1bce42.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "imbalance, any class label with rare appearance or with overly-high imbalance ratio is excluded from the label space following previous settings [34]. ", "page_idx": 6}, {"type": "text", "text": "For a given dataset, we randomly select $50\\%$ labels as known labels and the remaining labels as unknown labels with different label batch sizes. Then, we sample $60\\%$ instances without unknown labels to form training set while the remaining instances are treated as test data. The sampling procedure is repeated ten times, and the mean metric value as well as standard deviation for each label batch are reported. ", "page_idx": 6}, {"type": "text", "text": "To evaluate the performance of multi-label classifiers, we utilize five widely-used multi-label evaluation metrics [37], including Ranking loss, One-error, Coverage, Average precision and Macroaveraging AUC. In addition, $F$ -measure is employed to evaluate the performance of recognizers. ", "page_idx": 6}, {"type": "text", "text": "To validate the effectiveness of the proposed SLAN approach in multi-label learning, four multi-label learning approaches are used for comparative studies. ", "page_idx": 6}, {"type": "text", "text": "\u2022 LIFT [35]: A multi-label learning approach, which induces classifiers with the label-specific features generated via conducting clustering analysis for each class label. [parameter configuration: $r=0.1_{\\circ}$ ] ", "page_idx": 6}, {"type": "text", "text": "Table 3: Experimental results of each compared approach (mean $\\pm$ std) with different label batch size (denoted by #label). The best and the second best performance of each data set methods are highlighted in boldface and underline respectively. In addition, $\\bullet/\\circ$ indicates whether SLAN is statistically superior/inferior to the comparing approaches on each data set with pairwise t-test (at 0.05 significance level). ", "page_idx": 7}, {"type": "table", "img_path": "k1VrxRS6WZ/tmp/386b1c06ba8160bbee83bd9ced21d28c83c7f7c70edd699f529e7190353258ce.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "\u2022 MUENLPLR [39]: A SVM-based dynamic multi-label learning approach which trains a set of linear classifiers by minimizing misclassification loss and pairwise ranking loss. [parameter configuration: $C_{1}=1,C_{2}=1]$   \n\u2022 SENCE [27]: A multi-label learning approach based on label-specific features generated by mixture-based clustering ensemble. [parameter configuration: $r{=}\\,0.4$ .   \n\u2022 LIMIC [21]: A multi-semantics multi-label metric learning approach coupled with MLLNN [36] which learns one global and multiple label-specific local metrics simultaneously. [parameter configuration: $\\lambda_{1}=1,\\lambda_{2}=100,\\gamma=2,K=10]$ . With the first attempt towards solving the MLOSR problem, there is no method can be directly applied.   \nThus, we compare the proposed SLAN approach with existing anomaly detection approaches.   \n\u2022 OC-SVM [20]: A SVM-based approach which constructs a hyper-sphere surrounding all instances from known labels.   \n\u2022 IFOREST [18]: An unsupervised forest-based anomaly detection approach which employs average path length over all trees as the anomaly score.   \n\u2022 MUENLFOREST [39]: A forest-based dynamic multi-label learning approach which utilizes clustering process in each nodes by considering the feature space and the label patterns. [parameter configuration: $q=5,\\psi=256,g=100,e_{m}=9]$ . ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "For the proposed SLAN approach, trade-off parameters are set as $\\alpha=0.1,\\beta=0.1,\\gamma=10,\\mu_{1}=$ 0. $.1,\\tau=0.8$ . $\\mu_{0}$ is fixed to be 0.1. The sensitivity analysis of parameter configurations is conducted in Subsection 4.3. A Linux server equipped with Intel Xeon CPU (48 cores $\\textit{@2.67}\\mathrm{GHz}$ and 256GB memory is used for supporting the experiments. ", "page_idx": 7}, {"type": "text", "text": "4.2 Experimental Results ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "The detailed experimental results in terms of Ranking loss and $F$ -measure are reported in Table 2-3. Due to the page limit, the results on other metrics are shown in the Appendix. Meanwhile, pairwise t-test [6] is conducted to demonstrate whether the performance of SLAN is statistically superior/inferior to the comparing approaches on each data set. The resulting win/tie/loss counts are summarized in the supplementary material. ", "page_idx": 8}, {"type": "text", "text": "Based on the reported experimental results, the following observations can be made: ", "page_idx": 8}, {"type": "text", "text": "\u2022 Compared with the performance on close set instances $(\\#{\\mathrm{label}}=0)$ ), the performance of all comparing approaches on open set instances degrades. Nonetheless, across all multi-label evaluation metrics, SLAN achieves superior or at least comparable performance against the comparing approaches in $91.7\\%$ cases. The results clearly indicate the jointly optimization with recognizer serves a more effective way to achieve more robust multi-label classifier in the open environment.   \n\u2022 Comparing with anomaly detection approaches, SLAN achieves better performance in $83.3\\%$ cases. Possible reasons are that: (a) OC-SVM and IFOREST are previously designed for multiclass scenario which can not directly solve MLOSR problem. (b) For the construction of MUENLFOREST, instance is augmented with its predictive values derived from MUENLPLR. However, the predictive values might be suboptimal as MUENLPLR is trained without considering open space risk.   \n\u2022 In the multi-label setting, instances with unknown labels may share the same dense region of instances with known labels, which makes multi-class anomaly detection approaches tend to reject instances with unknown labels. That is why since MUENLPLR is under multi-label setting, it still inferior to IFOREST and OC-SVM. ", "page_idx": 8}, {"type": "text", "text": "4.3 Parameter Sensitivity Analysis ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, we study the sensitivity analysis of trade-off parameters $\\alpha,\\beta,\\gamma,\\mu_{1},\\tau$ shown in Algorithm 1. Figure 2 illustrates how the performance of SLAN changes with varying parameter configurations on data set enron. As shown in Figure 2, SLAN achieves relatively stable performance on multi-label metrics and somewhat sensitive on $F$ -measure. In this paper, trade-off parameters are set as $\\alpha=0.1,\\beta=0.1$ , $\\gamma=10$ , $\\mu_{1}=0.1$ , $\\tau=0.8$ , which can be employed as the default parameter setting. ", "page_idx": 8}, {"type": "image", "img_path": "k1VrxRS6WZ/tmp/2c2b4d79f5a83809c6f405b351cc753a4ad46051a33dc750abe50fe83308610a.jpg", "img_caption": ["Figure 2: Performance of SLAN with varying value of trade-off parameters on enron. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "The major contributions of our work are two-fold: 1) We formalize a novel learning framework named multi-label open set recognition (MLOSR), which aims to classify and recognize instances with unknown labels in multi-label setting, suggesting a new direction for multi-label learning. 2) We propose a novel MLOSR approach named SLAN which can facilitate open set multi-label classification by utilizing sub-labeling information and recognize the unknown labels by differentiating the sublabeling information from holistic supervision. Extensive experimental results clearly validate the effectiveness of the proposed SLAN approach. ", "page_idx": 8}, {"type": "text", "text": "However, SLAN enriches #labels $+1$ sub-labeling information, which could hardly generalize to extreme multi-label data set. Meanwhile, SLAN works in the multi-label learning schema where the feature representations of instances may less informative. In the future, it is interesting to investigate towards extreme multi-label learning to achieve tolerable scalability and design deep MLOSR approaches with discriminative feature representations. Furthermore, it is desirable to extend evaluation metrics for MLOSR. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The authors wish to thank the anonymous reviewers for their helpful comments and suggestions. This work was supported by the National Science Foundation of China (62225602) and the Big Data Computing Center of Southeast University. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Abhijit Bendale and Terrance E. Boult. Towards open world recognition. In Proceeding of 2015 IEEE Conference on Computer Vision and Pattern Recognition, pages 1893\u20131902, Boston, MA, 2015.   \n[2] Matthew R. Boutell, Jie bo Luo, Xi-Peng Shen, and Christopher M. Brown. Learning multi-label scene classification. Pattern Recognition, 37(9):1757\u20131771, 2004.   \n[3] Christian Brinker, Eneldo Loza Menc\u00eda, and Johannes F\u00fcrnkranz. Graded multilabel classification by pairwise comparisons. In Proceedings of the 14th IEEE International Conference on Data Mining, pages 731\u2013736, Shenzhen, China, 2014.   \n[4] Hakan Cevikalp. Best fitting hyperplanes for classification. IEEE Trans. Pattern Anal. Mach. Intell., 39(6):1076\u20131088, 2017.   \n[5] Yongjuan Che, Yuexuan An, and Hui Xue. Boosting few-shot open-set recognition with multirelation margin loss. In Proceedings of the 32th International Joint Conference on Artificial Intelligence, pages 3505\u20133513, Macao, SAR, China, 2023. ijcai.org.   \n[6] Janez Demsar. Statistical comparisons of classifiers over multiple data sets. J. Mach. Learn. Res., 7:1\u201330, 2006.   \n[7] Johannes F\u00fcrnkranz, Eyke H\u00fcllermeier, Eneldo Loza Menc\u00eda, and Klaus Brinker. Multilabel classification via calibrated label ranking. Machine Learning, 73(2):133\u2013153, 2008. [8] Yi Gao, Miao Xu, and Min-Ling Zhang. Unbiased risk estimator to multi-labeled complementary label learning. In Proceedings of the 32th International Joint Conference on Artificial Intelligence, pages 3732\u20133740, Macao, SAR, China, 2023. ijcai.org.   \n[9] Chuan-Xing Geng, Sheng-Jun Huang, and Song-Can Chen. Recent advances in open set recognition: A survey. IEEE Trans. Pattern Anal. Mach. Intell., 43(10):3614\u20133631, 2021.   \n[10] Chuanxing Geng and Songcan Chen. Collective decision for open set recognition. IEEE Trans. Knowl. Data Eng., 34(1):192\u2013204, 2022.   \n[11] Euhanna Ghadimi, Andr\u00e9 Teixeira, Iman Shames, and Mikael Johansson. Optimal parameter selection for the alternating direction method of multipliers (ADMM): quadratic problems. IEEE Trans. Autom. Control., 60(3):644\u2013658, 2015.   \n[12] Siddharth Gopal and Yiming Yang. Multilabel classification with meta-level features. In Proceeding of the 33rd International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 315\u2013322, Geneva, Switzerland, 2010.   \n[13] Peng Hou, Xin Geng, and Min-Ling Zhang. Multi-label manifold learning. In Proceedings of the 30th AAAI Conference on Artificial Intelligence, pages 1680\u20131686, Phoenix, AZ, 2016.   \n[14] Ming Huang, Fu-Zhen Zhuang, Xiao Zhang, Xiang Ao, Zheng-Yu Niu, Min-Ling Zhang, and Qing He. Supervised representation learning for multi-label classification. Machine Learning, 108(5):747\u2013763, 2019.   \n[15] Sheng-Jun Huang and Zhi-Hua Zhou. Active query driven by uncertainty and diversity for incremental multi-label learning. In Hui Xiong, George Karypis, Bhavani Thuraisingham, Diane J. Cook, and Xindong Wu, editors, Proceedings of the 13th IEEE International Conference on Data Mining, pages 1079\u20131084, Dallas,TX, 2013.   \n[16] Pedro Ribeiro Mendes J\u00fanior, Roberto Medeiros de Souza, Rafael de Oliveira Werneck, Bernardo V. Stein, Daniel V. Pazinato, Waldir R. de Almeida, Ot\u00e1vio A. B. Penatti, Ricardo da Silva Torres, and Anderson Rocha. Nearest neighbors distance ratio open-set classifier. Mach. Learn., 106(3):359\u2013386, 2017.   \n[17] Samuel Kotz and Saralees Nadarajah. Extreme value distributions: theory and applications. World Wcientific, 2000.   \n[18] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou. Isolation forest. In Proceedings of the 8th IEEE International Conference on Data Mining, pages 413\u2013422, Pisa, Italy, 2008.   \n[19] Wei-Wei Liu, Hao-Bo Wang, Xiao-Bo Shen, and Ivor W. Tsang. The emerging trends of multilabel learning. IEEE transactions on pattern analysis and machine intelligence, 44(11):7955\u2013 7974, 2021.   \n[20] Jun-Shui Ma and Simon Perkins. Time-series novelty detection using one-class support vector machines. In Proceedings of the 2003 International Joint Conference on Neural Networks, volume 3, pages 1741\u20131745, Portland, OR, 2003.   \n[21] Jun-Xiang Mao, Wei Wang, and Min-Ling Zhang. Label specific multi-semantics metric learning for multi-label classification: Global consideration helps. In Proceedings of the 32th International Joint Conference on Artificial Intelligence, pages 4055\u20134063, Macao, SAR, China, 2023.   \n[22] Jesse Read, Bernhard Pfahringer, Geoff Holmes, and Eibe Frank. Classifier chains for multilabel classification. Machine Learning, 85(3):333\u2013359, 2011.   \n[23] Ethan M. Rudd, Lalit P. Jain, Walter J. Scheirer, and Terrance E. Boult. The extreme value machine. IEEE Trans. Pattern Anal. Mach. Intell., 40(3):762\u2013768, 2018.   \n[24] Walter J. Scheirer, Anderson de Rezende Rocha, Archana Sapkota, and Terrance E. Boult. Toward open set recognition. IEEE Trans. Pattern Anal. Mach. Intell., 35(7):1757\u20131772, 2013.   \n[25] Walter J. Scheirer, Lalit P. Jain, and Terrance E. Boult. Probability models for open set recognition. IEEE Trans. Pattern Anal. Mach. Intell., 36(11):2317\u20132324, 2014.   \n[26] Pingjie Tang, Meng Jiang, Bryan (Ning) Xia, Jed W. Pitera, Jeffrey Welser, and Nitesh V. Chawla. Multi-label patent categorization with non-local attention-based graph convolutional network. In Proceedings of the 34th AAAI conference on artificial intelligence, pages 9024\u20139031, New York, NY, 2020.   \n[27] Yi-Bo Wang, Jun-Yi Hang, and Min-Ling Zhang. Stable label-specific features generation for multi-label learning via mixture-based clustering ensemble. IEEE CAA J. Autom. Sinica, 9(7):1248\u20131261, 2022.   \n[28] Zhen Wang, Liu Liu, and DaCheng Tao. Deep streaming label learning. In Proceedings of the 37th International Conference on Machine Learning, volume 119, pages 9963\u20139972, Virtual Event, 2020.   \n[29] Tong Wei, Jiang-Xin Shi, and Yu-Feng Li. Probabilistic label tree for streaming multi-label learning. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 1801\u20131811, Virtual Event, Singapore, 2021.   \n[30] Ren-Chun You, Zhi-Yao Guo, Lei Cui, Xiang Long, Ying-Ze Bao, and Shi-Lei Wen. Crossmodality attention with semantic graph embedding for multi-label classification. In The 34th AAAI Conference on Artificial Intelligence, pages 12709\u201312716, New York, NY, 2020.   \n[31] Shan You, Chang Xu, Yun-He Wang, Chao Xu, and Da-Cheng Tao. Streaming label learning for modeling labels on the fly. CoRR, abs/1604.05449, 2016.   \n[32] He Zhang and Vishal M. Patel. Sparse representation-based open set recognition. IEEE Trans. Pattern Anal. Mach. Intell., 39(8):1690\u20131696, 2017.   \n[33] Min-Ling Zhang, Yu-Kun Li, Xu-Ying Liu, and Xin Geng. Binary relevance for multi-label learning: An overview. Frontiers of Computer Science, 12(2):191\u2013202, 2018.   \n[34] Min-Ling Zhang, Yu-Kun Li, Hao Yang, and Xu-Ying Liu. Towards class-imbalance aware multi-label learning. IEEE Trans. Cybern., 52(6):4459\u20134471, 2022.   \n[35] Min-Ling Zhang and Lei Wu. Lift: Multi-label learning with label-specific features. IEEE Trans. Pattern Anal. Mach. Intell., 37(1):107\u2013120, 2015.   \n[36] Min-Ling Zhang and Zhi-Hua Zhou. ML-KNN: A lazy learning approach to multi-label learning. Pattern Recognit., 40(7):2038\u20132048, 2007.   \n[37] Min-Ling Zhang and Zhi-Hua Zhou. A review on multi-label learning algorithms. IEEE transactions on knowledge and data engineering, 26(8):1819\u20131837, 2013.   \n[38] Qian-Wen Zhang, Yun Zhong, and Min-Ling Zhang. Feature-induced labeling information enrichment for multi-label learning. In Proceedings of the 32th AAAI Conference on Artificial Intelligence, pages 4446\u20134453, New Orleans, LA, 2018.   \n[39] Yue Zhu, Kai Ming Ting, and Zhi-Hua Zhou. Multi-label learning with emerging new labels. IEEE Trans. Knowl. Data Eng., 30(10):1901\u20131914, 2018. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Appendix ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "A.1 Experimental Results ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Table 4, 5, 7 and 9 report detailed experimental results in terms of Average precision, Macroaveraging AUC, Coverage and One-error, which are not covered in the Experimental Results part of the main body due to page limit. Besides, the pairwise t-test is conducted to demonstrate whether the performance of SLAN is statistically superior/inferior to the comparing approaches on each data set. The resulting win/tie/loss counts in terms of multi-label evaluation metrics are summarized in Table 6 as well as $F$ -measure in Table 8. ", "page_idx": 12}, {"type": "text", "text": "Table 4: Experimental results of each compared approach (mean\u00b1std) with different label batch size (denoted by #label). The best and the second best performance of each data set methods are highlighted in boldface and underline respectively. In addition, $\\bullet/\\circ$ indicates whether SLAN is statistically superior/inferior to the comparing approaches on each data set with pairwise t-test (at 0.05 significance level). ", "page_idx": 12}, {"type": "table", "img_path": "k1VrxRS6WZ/tmp/7c3f8883c5d3389eae867e80bfb7c32e4b02e947c81d91c61785891e736707c1.jpg", "table_caption": [], "table_footnote": [], "page_idx": 12}, {"type": "table", "img_path": "k1VrxRS6WZ/tmp/bbe8f93c4ae359cc3e5590c3321b4860c1e650eca1e7df391df7247f1e313f59.jpg", "table_caption": ["Table 5: Experimental results of each compared approach (mean\u00b1std) in terms of Macro-averaging AUC with different label batch size (denoted by #label). The best and the second best performance of each data set methods are highlighted in boldface and underline respectively. In addition, $\\bullet/\\circ$ indicates whether SLAN is statistically superior/inferior to the comparing approaches on each data set with pairwise t-test (at 0.05 significance level). "], "table_footnote": [], "page_idx": 13}, {"type": "table", "img_path": "k1VrxRS6WZ/tmp/cbacbf5daf0cb76a071f2e706b5bdef823ee62c9c61849081cc892031275fb98.jpg", "table_caption": ["Table 6: Win/tie/loss counts (pairwise t-test at 0.05 significant level) for SLAN against other multi-label approaches. "], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "Table 7: Experimental results of each compared approach (mean $\\pm$ std) in terms of Coverage with different label batch size (denoted by #label). The best and the second best performance of each data set methods are highlighted in boldface and underline respectively. In addition, $\\bullet/\\circ$ indicates whether SLAN is statistically superior/inferior to the comparing approaches on each data set with pairwise t-test (at 0.05 significance level). ", "page_idx": 14}, {"type": "table", "img_path": "k1VrxRS6WZ/tmp/b1075567f39f5d8dcfb0ab033b3624266a224704061486b4d77c1ee5cec3a784.jpg", "table_caption": [], "table_footnote": [], "page_idx": 14}, {"type": "table", "img_path": "k1VrxRS6WZ/tmp/66e3c2536b24a72349b71911e4160064e8204ae5b636c41504af784488e65fdf.jpg", "table_caption": ["Table 8: Win/tie/loss counts (pairwise t-test at 0.05 significant level) for SLAN against other anomaly detection approaches. "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "Table 9: Experimental results of each compared approach (mean\u00b1std) in terms of One-error with different label batch size (denoted by #label). The best and the second best performance of each data set methods are highlighted in boldface and underline respectively. In addition, $\\bullet/\\circ$ indicates whether SLAN is statistically superior/inferior to the comparing approaches on each data set with pairwise t-test (at 0.05 significance level). ", "page_idx": 15}, {"type": "table", "img_path": "k1VrxRS6WZ/tmp/91bfedad93da8de66d6507f7232e0d43e9b4fcc9b973323f23cba6afb9d49a11.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "A.2 Data and Code Availability ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Our source code can be found at https://palm.seu.edu.cn/zhangml/. The datasets used in this paper are public, and can be found in Section 4.1. ", "page_idx": 15}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] Justification: We claim the paper\u2019s contributions and scope in the introduction (Section 1). ", "page_idx": 16}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: We discussed the limitations of this paper in the conclusion (Section 5). ", "page_idx": 16}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 16}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 16}, {"type": "text", "text": "Justification: The paper does not include theoretical results. ", "page_idx": 16}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: Please refer to Section 4.1 and A.1 of this paper. ", "page_idx": 16}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: The paper provides the data and code availability in Section A.2. ", "page_idx": 16}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: Please refer to Section 4.1 and A.1 of this paper. ", "page_idx": 16}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: In Section 4.1 and A.1 of this paper, we presented the mean and standard deviation of multiple training rounds and conduct pairwise t-test between the proposed approach and the comparing approaches. ", "page_idx": 16}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: Please refer to Section 4.1. ", "page_idx": 16}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: We have already reviewed the NeurIPS Code of Ethics. ", "page_idx": 17}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 17}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 17}, {"type": "text", "text": "Justification: Our work is foundational research and does not involve societal impact. ", "page_idx": 17}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 17}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 17}, {"type": "text", "text": "Justification: The paper does not pose such risk. ", "page_idx": 17}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 17}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 17}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 17}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 17}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 17}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 17}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 17}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 17}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 17}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 17}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 17}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 17}]