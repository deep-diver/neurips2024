[{"figure_path": "SXbyy0a3rY/tables/tables_8_1.jpg", "caption": "Table 1: Quantitative comparisons of grounding accuracy on HRS [3] and DrawBench [43] benchmarks. Bold represents the best, and underline represents the second best method.", "description": "This table presents a quantitative comparison of the grounding accuracy achieved by different methods on two benchmark datasets: HRS and DrawBench.  The grounding accuracy is evaluated using three criteria: spatial accuracy (how well the generated objects align with the given bounding boxes), size accuracy (how well the size of generated objects matches the size of the bounding boxes), and color accuracy (how well the color of generated objects matches the color of the objects in the bounding boxes). The results show that GROUNDIT outperforms all other methods across all three criteria on both datasets.", "section": "6.2 Grounding Accuracy"}, {"figure_path": "SXbyy0a3rY/tables/tables_9_1.jpg", "caption": "Table 2: Quantitative comparisons on prompt fidelity on HRS benchmark [3]. Bold represents the best method.", "description": "This table presents a quantitative comparison of prompt fidelity between the proposed method (GROUNDIT) and the baseline method (PixArt-R&B). Three metrics are used to evaluate the generated images from the HRS dataset: CLIP score, ImageReward, and PickScore.  GROUNDIT demonstrates higher CLIP and ImageReward scores, indicating improved adherence to text prompts and better overall image quality, respectively.  While PickScore shows a slight decrease, it remains comparable to the baseline.  This highlights the effectiveness of GROUNDIT in generating images that closely match the intended prompt.", "section": "6.3 Prompt Fidelity"}, {"figure_path": "SXbyy0a3rY/tables/tables_14_1.jpg", "caption": "Table 4: Quantitative comparisons of mIoU (\u2191) on a subset of MS-COCO-2014 [33], HRS-Spatial [3], and our custom dataset. Bold represents the best, and underline represents the second best method.", "description": "This table presents a quantitative comparison of the Intersection over Union (IoU) metric across three different datasets: a subset of MS-COCO-2014, HRS-Spatial, and a custom dataset.  The comparison is made between GROUNDIT and several baseline methods.  The table highlights the performance of GROUNDIT in terms of spatial grounding accuracy, demonstrating its superiority over the baselines, especially when dealing with a greater number of bounding boxes.", "section": "6.2 Grounding Accuracy"}, {"figure_path": "SXbyy0a3rY/tables/tables_15_1.jpg", "caption": "Table 4: Quantitative comparisons of mIoU (\u2191) on a subset of MS-COCO-2014 [33], HRS-Spatial [3], and our custom dataset. Bold represents the best, and underline represents the second best method.", "description": "This table presents a quantitative comparison of the mean Intersection over Union (mIoU) scores achieved by different methods on three datasets: a subset of MS-COCO-2014, HRS-Spatial, and a custom dataset.  The mIoU metric measures the overlap between the predicted bounding boxes generated by each method and the ground truth bounding boxes.  Higher mIoU indicates better spatial grounding accuracy.  The table is broken down by backbone model used (Stable Diffusion and PixArt-\u03b1), showing the performance of various training-free spatial grounding approaches including Layout-Guidance, Attention-Refocusing, BoxDiff, and R&B, in comparison to the proposed GROUNDIT method.  The results highlight GROUNDIT's superior performance across different datasets, demonstrating its effectiveness in spatial grounding.", "section": "6.2 Grounding Accuracy"}, {"figure_path": "SXbyy0a3rY/tables/tables_15_2.jpg", "caption": "Table 2: Quantitative comparisons on prompt fidelity on HRS benchmark [3]. Bold represents the best method.", "description": "This table presents a quantitative comparison of prompt fidelity (how well the generated images adhere to the text prompt) between the proposed GROUNDIT method and the PixArt-R&B baseline.  Three metrics are used for evaluation: CLIP score (measures how well the generated image matches the text prompt), ImageReward (measures human preference by considering both prompt fidelity and overall image quality), and PickScore (measures preference between pairs of images).  The table shows that GROUNDIT achieves a higher CLIP score and ImageReward compared to PixArt-R&B, indicating better prompt fidelity.  The PickScore shows a slight underperformance, but overall, GROUNDIT's prompt fidelity is comparable to the baseline.", "section": "6.3 Prompt Fidelity"}, {"figure_path": "SXbyy0a3rY/tables/tables_17_1.jpg", "caption": "Table 1: Quantitative comparisons of grounding accuracy on HRS [3] and DrawBench [43] benchmarks. Bold represents the best, and underline represents the second best method.", "description": "This table presents a quantitative comparison of the spatial grounding accuracy achieved by different methods on two benchmark datasets: HRS and DrawBench.  The methods compared include several state-of-the-art training-free approaches for spatial grounding, along with the authors' proposed method, GROUNDIT. The table shows the performance of each method across three criteria for spatial grounding: spatial accuracy (percentage of objects correctly placed within their bounding boxes), size accuracy (how well the size of the generated object matches the specified size), and color accuracy (how well the generated object's color matches the specified color).  The results demonstrate that GROUNDIT achieves superior performance compared to existing methods on both datasets, particularly in terms of spatial accuracy.", "section": "6.2 Grounding Accuracy"}, {"figure_path": "SXbyy0a3rY/tables/tables_17_2.jpg", "caption": "Table 6: Comparison of the average inference time based on the number of bounding boxes. Values in the table are given in seconds.", "description": "This table compares the average inference time in seconds for different methods (R&B [47], PixArt-R&B, and GROUNDIT) across varying numbers of bounding boxes (3, 4, 5, and 6). It showcases the impact of the number of bounding boxes on the computational cost of each method.  GROUNDIT shows a modest increase in inference time as the number of bounding boxes increases, highlighting its scalability.", "section": "6.2 Grounding Accuracy"}]