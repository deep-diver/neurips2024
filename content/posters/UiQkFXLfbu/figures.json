[{"figure_path": "UiQkFXLfbu/figures/figures_2_1.jpg", "caption": "Figure 1: Overview of the DLM language-conditioned reward design loop. We provide three context descriptions to the LLM: a language command (full list of commands in Table 3), a list of per-arm demographic features available for proposed reward functions, and syntax cues enabling LLM reward function output directly in code. From this context, the 1) LLM then proposes 2) candidate reward functions which are used to train 3) optimal policies under proposed rewards. Trained policies are simulated to generate 4) policy outcome comparisons showing state-feature distributions over key demographic groups. Finally, we query an LLM to perform 5) self-reflection [43, 21] by choosing the best candidate reward aligning with the original language command; selected candidates are used as context to guide future reward generation.", "description": "This figure illustrates the Decision-Language Model (DLM) process.  It starts with a human-provided language command and relevant arm features.  The LLM uses this information to propose candidate reward functions.  These are then tested in multi-agent simulations, and the results (policy outcomes) are fed back to the LLM for reflection and refinement.  The LLM selects the best candidate, and the process iterates until satisfactory policy is achieved. The feedback loop uses state-feature distributions to guide refinement instead of ground truth.", "section": "4 Decision-Language Model for RMABs"}, {"figure_path": "UiQkFXLfbu/figures/figures_6_1.jpg", "caption": "Figure 2: Main results. We compute normalized reward (Section 5.2) for each method over 200 seeds, and report the interquartile mean (IQM) and standard error of the IQM across all runs [47]. We compare the topline Base reward policy to the performance of DLM with No Reflection and with Reflection. We also compare to a No Action and Random policy, and a Default policy that demonstrates how the original (fixed) reward function would perform for each new task. Our method is able to achieve near-base reward performance across tasks, and consistently outperform the fixed Default reward policy in a completely automated fashion. For some tasks, DLM with Reflection is also able to significantly improve upon zero-shot proposed reward.", "description": "This figure shows the main results of the paper, comparing the performance of different methods for reward generation in a restless multi-armed bandit setting. The methods compared are: Random, No Action, Base, Default, DLM (No Reflection), and DLM (Reflection). The y-axis represents the mean normalized reward, and the x-axis represents different tasks. The figure demonstrates that DLM (Reflection), which uses the proposed method of iterative refinement based on feedback from simulations, achieves near-optimal performance across most tasks and significantly outperforms the fixed default reward policy.", "section": "5 Experimental Evaluation"}, {"figure_path": "UiQkFXLfbu/figures/figures_7_1.jpg", "caption": "Figure 1: Overview of the DLM language-conditioned reward design loop. We provide three context descriptions to the LLM: a language command (full list of commands in Table 3), a list of per-arm demographic features available for proposed reward functions, and syntax cues enabling LLM reward function output directly in code. From this context, the 1) LLM then proposes 2) candidate reward functions which are used to train 3) optimal policies under proposed rewards. Trained policies are simulated to generate 4) policy outcome comparisons showing state-feature distributions over key demographic groups. Finally, we query an LLM to perform 5) self-reflection [43, 21] by choosing the best candidate reward aligning with the original language command; selected candidates are used as context to guide future reward generation.", "description": "This figure illustrates the iterative process of the Decision-Language Model (DLM) for designing reward functions in a restless multi-armed bandit (RMAB) setting.  The DLM uses an LLM to interpret human-language commands, propose reward functions as code, and refine these functions based on feedback from RMAB simulations. The process involves three main stages:\n\n1. **LLM Reward Generation:** The LLM receives a language command, arm features, and code syntax as input, and then generates candidate reward functions. \n2. **Multi-Agent Simulation:**  The proposed reward functions are evaluated using multi-agent simulations to generate policy outcomes. \n3. **LLM Reflection:** The LLM evaluates the policy outcomes, selects the best candidate reward function, and then uses this selection as context for future reward function generation.", "section": "4 Decision-Language Model for RMABs"}]