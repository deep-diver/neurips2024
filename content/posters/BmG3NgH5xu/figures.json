[{"figure_path": "BmG3NgH5xu/figures/figures_1_1.jpg", "caption": "Figure 1: Illustration of preferences in different examples. The solid red curves represent the Pareto front, dashed lines represent preference constraints.", "description": "This figure illustrates two examples of how preferences are incorporated into multi-objective optimization problems.  The first example (a) shows a trade-off between fairness and accuracy in machine learning.  The Pareto front represents the set of optimal solutions balancing these objectives, while preference constraints (dashed lines) specify desired minimum levels of fairness (epsilon). The second example (b) illustrates drug molecule design, where a preference vector (v) guides the optimization towards solutions with specific property combinations, aligning with a particular direction in the objective space. ", "section": "1 Introduction"}, {"figure_path": "BmG3NgH5xu/figures/figures_2_1.jpg", "caption": "Figure 2: Illustration of CA-dominance. The solid red curves are the Pareto fronts, green dots are the reference points, gray shaded regions are the set of objectives dominating the reference points, under different CA in (a) and (b).", "description": "This figure illustrates the concept of CA-dominance, a generalization of Pareto dominance.  In (a), the standard Pareto dominance is shown, where a point dominates another if it is better in all objectives.  The gray shaded area represents the points dominated by the reference point.  In (b), a more general cone (CA) is used to define dominance.  This allows for more flexibility in defining preferences. The gray shaded area now encompasses a broader range of points than in (a), showing that CA-dominance can capture more diverse preference structures.", "section": "Problem Setup and A Meta Algorithm"}, {"figure_path": "BmG3NgH5xu/figures/figures_7_1.jpg", "caption": "Figure 3: Converging solutions (blue dots) and optimization trajectories (blue lines) on the objective space of different methods on synthetic objectives given in (5.1). Dashed arrows represent pre-specified preference vectors. The green dots represent initial objective values.", "description": "This figure compares the performance of different multi-objective optimization methods on a synthetic problem with two objectives. The goal is to find solutions that align with pre-specified preference vectors, represented by dashed arrows. The blue dots show the final solutions found by each method, while the blue lines trace their optimization trajectories. The green dots indicate the starting point of the optimization process. The figure demonstrates how different methods converge to different parts of the Pareto front, highlighting the unique capabilities of the FERERO framework.", "section": "5 Experiments"}, {"figure_path": "BmG3NgH5xu/figures/figures_7_2.jpg", "caption": "Figure 4: Outputs (colored markers) and optimization trajectories (colored lines) of different methods when initial objectives are near the Pareto front. Different colors represent different preferences.", "description": "This figure compares the performance of various multi-objective optimization methods when the initial objective values are close to the Pareto front. Each color represents a different preference, and the colored lines show the optimization trajectory of each method.  The figure illustrates how each method approaches and converges to the Pareto front, highlighting differences in efficiency and ability to align with specified preferences.", "section": "Experiments"}, {"figure_path": "BmG3NgH5xu/figures/figures_8_1.jpg", "caption": "Figure 5: Training losses and accuracies of various methods with different preferences across three image datasets. The horizontal and vertical axes represent results for objective 1 and objective 2, respectively. Different colored dashed arrows indicate various preference vectors. Different markers denote the solutions obtained by different methods, with marker colors matching the preferences.", "description": "This figure shows the training losses and accuracies of different multi-objective optimization methods on three image datasets (Multi-MNIST, Multi-Fashion, and Multi-F+M). Each subplot represents a dataset, with the horizontal and vertical axes showing the results for objective 1 and objective 2, respectively.  Different colored dashed arrows represent predefined preferences, and different colored markers represent the results obtained by different methods. The marker color corresponds to the preference being targeted. The results illustrate how each method performs in finding solutions that align with different preferences.", "section": "5 Experiments"}, {"figure_path": "BmG3NgH5xu/figures/figures_9_1.jpg", "caption": "Figure 5: Training losses and accuracies of various methods with different preferences across three image datasets. The horizontal and vertical axes represent results for objective 1 and objective 2, respectively. Different colored dashed arrows indicate various preference vectors. Different markers denote the solutions obtained by different methods, with marker colors matching the preferences.", "description": "This figure shows the training losses and accuracies for three multi-objective image classification datasets (Multi-MNIST, Multi-Fashion, Multi-F+M).  Each subplot represents a dataset and displays the performance of different algorithms (LS, EPO, PMTL, XWC-MGDA, and FERERO) in terms of accuracy and loss for two objectives.  Different colored markers represent different algorithms, and dashed arrows show the direction of the specified preferences.  The plot illustrates how well each algorithm can achieve a target preference.", "section": "5 Experiments"}, {"figure_path": "BmG3NgH5xu/figures/figures_40_1.jpg", "caption": "Figure 3: Converging solutions (blue dots) and optimization trajectories (blue lines) on the objective space of different methods on synthetic objectives given in (5.1). Dashed arrows represent pre-specified preference vectors. The green dots represent initial objective values.", "description": "This figure compares the performance of various multi-objective optimization methods on a synthetic dataset.  Each method starts at the same initial point (green dot) and aims to find an optimal solution guided by a specific preference vector (dashed arrows). The blue dots represent the final solutions found by each algorithm, and the blue lines show their optimization trajectories. The plot shows how different algorithms navigate the objective space, highlighting the differences in their convergence behavior and ability to reach preference-guided solutions.", "section": "5 Experiments"}, {"figure_path": "BmG3NgH5xu/figures/figures_40_2.jpg", "caption": "Figure 7: Scale invariance verification.", "description": "This figure demonstrates the scale invariance property of the proposed FERERO algorithm. The left subplot shows the optimization trajectory without scaling, while the right subplot shows the trajectory with scaling applied to one of the objectives. The results show that the algorithm's convergence and preference alignment are not affected by the scaling, which demonstrates its robustness and ability to handle various scales of objective functions.", "section": "Efficient Algorithm Development"}, {"figure_path": "BmG3NgH5xu/figures/figures_41_1.jpg", "caption": "Figure 9: Relative loss profile for all methods on Emotions and Music dataset.", "description": "This figure compares different preference-guided multi-objective optimization methods' performance on the Emotions and Music dataset. The x-axis represents the six different emotion categories (E1-E6). The y-axis displays the relative loss profile, which quantifies the alignment between the obtained solutions and the predefined preferences. The error bars represent the standard deviation over multiple runs.  The figure allows for a visual assessment of how well each method aligns its solutions with the specified preferences for each emotion category, showing which methods better satisfy user-defined trade-offs across multiple objectives.", "section": "5 Experiments"}]