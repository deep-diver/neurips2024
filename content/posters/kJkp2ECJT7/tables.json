[{"figure_path": "kJkp2ECJT7/tables/tables_1_1.jpg", "caption": "Table 1: Comparisons with previous representative methods in three aspects of model capabilities. To the best of our knowledge, our FleVRS is the first one-stage model capable of performing standard, promptable, and open-vocabulary visual relationship segmentation all at once.", "description": "This table compares FleVRS with other relevant methods across three key aspects: handling of standard visual relationship segmentation tasks (HOI, SGG), ability to handle promptable segmentation (using textual prompts), and capacity for open-vocabulary segmentation (generalizing to unseen relationships).  It highlights FleVRS's unique position as the first one-stage model to achieve all three capabilities.", "section": "1 Introduction"}, {"figure_path": "kJkp2ECJT7/tables/tables_7_1.jpg", "caption": "Table 2: Quantitative results on the HICO-DET test set. We report both box and mask mAP under the Default setting [4] containing the Full (F), Rare (R), and Non-Rare (N) sets. no_interaction class is removed in mask mAP. The best score is highlighted in bold, and the second-best score is underscored. '-' means the model did not release weights and we cannot get the mask mAP. Due to space limit, we show the complete table with more models in the appendix.", "description": "This table presents a quantitative comparison of the proposed FleVRS model with various existing methods for human-object interaction (HOI) segmentation on the HICO-DET dataset.  It evaluates performance using both box and mask mAP metrics across three sets of HOI categories: Full, Rare, and Non-Rare. The results highlight FleVRS's superior performance in both box and mask mAP compared to other state-of-the-art models.", "section": "4 Experiments"}, {"figure_path": "kJkp2ECJT7/tables/tables_7_2.jpg", "caption": "Table 3: Quantitative results on V-COCO. We report both box and mask mAP.The best score is highlighted in bold, and the second-best score is underscored. '-' means the model did not release weights and we cannot get the mask mAP. Due to space limit, we show the complete table with more models in the appendix.", "description": "This table compares the performance of various models on the V-COCO dataset for visual relationship detection.  The metrics used are box mAP and mask mAP.  The table highlights the best performing model and indicates when mask mAP data was unavailable due to the model not releasing its weights.", "section": "4 Experiments"}, {"figure_path": "kJkp2ECJT7/tables/tables_8_1.jpg", "caption": "Table 4: Quantitative results on PSG. The best score is highlighted in bold, and the second-best score is underscored.", "description": "This table presents a comparison of the proposed FleVRS model's performance on the Panoptic Scene Graph Generation (PSG) task with several existing methods.  The results are shown for Recall (R) and mean Recall (mR) at different top-K thresholds (K=20, 50, 100). The table is categorized into methods adapted from SGG methods, one-stage PSG methods, and methods that utilize additional training data.  It shows the backbone used for each method and highlights the best performing model for each metric.", "section": "4 Experiments"}, {"figure_path": "kJkp2ECJT7/tables/tables_8_2.jpg", "caption": "Table 5: Comparison of promptable VRD results with the baseline on VRD dataset [55].", "description": "This table compares the performance of the proposed FleVRS model with existing methods (VRD [55] and SSAS [38]) on the VRD dataset [55] for the promptable visual relationship segmentation task.  It specifically evaluates the ability of each method to localize the subject and object when only part of the triplet information (subject, predicate, object) is provided.  The metrics used are S-IoU (subject Intersection over Union) and O-IoU (object Intersection over Union) for different scenarios: when the subject is missing, when the object is missing, and when only the predicate is known. The results show that FleVRS significantly outperforms previous methods in all these scenarios.", "section": "4.3 Promptable VRS"}, {"figure_path": "kJkp2ECJT7/tables/tables_9_1.jpg", "caption": "Table 6: Results of open-vocabulary HOI detection on HICO-DET.", "description": "This table presents a comparison of the proposed FleVRS model's performance on open-vocabulary HOI detection against several other state-of-the-art models. The results are broken down by three different scenarios (Rare First Unseen Composition, Non-rare First Unseen Composition, Unseen Object, and Unseen Verb), each evaluating the model's ability to generalize to unseen object or predicate categories.  The metrics used are mask mAP for HICO-DET and role mAP for VCOCO and PSG.", "section": "4 Experiments"}, {"figure_path": "kJkp2ECJT7/tables/tables_9_2.jpg", "caption": "Table 7: Ablations of different loss types, backbones, design choices and training sets. We adopt the Focal-L backbone by default.", "description": "This table presents ablation studies on the FleVRS model. It examines the impact of various design choices, including different loss functions (disentangled CE loss, triplet CE loss, and a combination), visual backbones (Focal Tiny and Focal Large), design choices (using only box heads, only mask heads, or both), and training datasets (using a single dataset or multiple datasets).  The results are evaluated using mask mAP on HICO-DET, mask AP on V-COCO, and R/mR@20 on PSG, showcasing the impact of each modification on overall model performance.  The study helps determine the optimal configuration for FleVRS.", "section": "4 Experiments"}, {"figure_path": "kJkp2ECJT7/tables/tables_19_1.jpg", "caption": "Table 2: Quantitative results on the HICO-DET test set. We report both box and mask mAP under the Default setting [4] containing the Full (F), Rare (R), and Non-Rare (N) sets. no_interaction class is removed in mask mAP. The best score is highlighted in bold, and the second-best score is underscored. '-' means the model did not release weights and we cannot get the mask mAP. Due to space limit, we show the complete table with more models in the appendix.", "description": "This table presents a quantitative comparison of the proposed FleVRS model with several existing methods for Human-Object Interaction (HOI) segmentation on the HICO-DET dataset.  It shows the performance (box and mask mean Average Precision - mAP) broken down by three subsets of the dataset (Full, Rare, and Non-Rare), highlighting the model's superior performance compared to state-of-the-art models.", "section": "4 Experiments"}, {"figure_path": "kJkp2ECJT7/tables/tables_20_1.jpg", "caption": "Table 3: Quantitative results on V-COCO. We report both box and mask mAP.The best score is highlighted in bold, and the second-best score is underscored. '-' means the model did not release weights and we cannot get the mask mAP. Due to space limit, we show the complete table with more models in the appendix.", "description": "This table presents a comparison of various models' performance on the V-COCO dataset for visual relationship detection.  The metrics used are box and mask mean Average Precision (mAP).  The table highlights the best performing model and indicates models which did not release their weights.", "section": "4 Experiments"}, {"figure_path": "kJkp2ECJT7/tables/tables_20_2.jpg", "caption": "Table 2: Quantitative results on the HICO-DET test set. We report both box and mask mAP under the Default setting [4] containing the Full (F), Rare (R), and Non-Rare (N) sets. no_interaction class is removed in mask mAP. The best score is highlighted in bold, and the second-best score is underscored. '-' means the model did not release weights and we cannot get the mask mAP. Due to space limit, we show the complete table with more models in the appendix.", "description": "This table presents a comparison of the proposed FleVRS model's performance on the HICO-DET dataset against various other state-of-the-art methods for HOI segmentation.  It shows the box and mask mean average precision (mAP) for different subsets of the dataset (Full, Rare, and Non-Rare) and highlights the best and second-best performing models. The table also notes when mask mAP data was unavailable from the source publication.", "section": "4 Experiments"}]