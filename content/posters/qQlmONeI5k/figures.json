[{"figure_path": "qQlmONeI5k/figures/figures_1_1.jpg", "caption": "Figure 1: Illustration of our idea. Existing methods rely on fixed manual annotation, complex architecture and prior-knowledge-based optimization to enrich infrared modality. Leading to significant time and labor cost, additional parameters and data sensitivity. In contrast, our method employs VLM and LLM to automatically generate dynamic text, improving the robustness against text variation; fine-tunes a pre-trained VLM through aligning features across all modalities, enabling the framework to create fusion features semantically consistent with visible modality in a parameter-free manner.", "description": "This figure compares the proposed method with existing methods for Visible-Infrared Person Re-identification (VI-ReID). Existing methods use manual annotation, complex architectures, and prior knowledge, resulting in high time and labor costs, increased parameters, and data sensitivity.  The proposed method uses Vision-Language Models (VLMs) and Large Language Models (LLMs) to automatically generate dynamic text descriptions, improving robustness. It also fine-tunes a pre-trained VLM to align features across modalities, creating semantically consistent fusion features without extra parameters.  The figure visually shows this difference in approach and highlights the advantages of the proposed method.", "section": "1 Introduction"}, {"figure_path": "qQlmONeI5k/figures/figures_3_1.jpg", "caption": "Figure 2: Illustration of our TVI-LFM, including Modality-Specific Caption (MSC), Incremental Fine-tuning Strategy (IFS), and Modality Ensemble Retrieval (MER). MSC utilizes fine-tuned VLMs as modal-specific captioners and employs an LLM for augmentation. IFS fine-tunes a pre-trained VLM to create fusion features semantically consistent with visible features. MER leverages the strengths of all query modalities to form ensemble queries, thereby improving the retrieval performance.", "description": "This figure illustrates the proposed TVI-LFM framework for visible-infrared person re-identification.  It highlights three key modules: \n\n1. **Modality-Specific Caption (MSC):** Uses fine-tuned Vision-Language Models (VLMs) to generate captions for visible and infrared images, with an additional Large Language Model (LLM) for augmentation, enriching the infrared modality with textual information.\n2. **Incremental Fine-tuning Strategy (IFS):**  Fine-tunes a pre-trained VLM to align the generated textual features with the original image features. This step creates fusion features that semantically align with visible features, leading to improved representation of infrared modality. Modality Joint Learning is also used to align the features across all modalities.\n3. **Modality Ensemble Retrieval (MER):** Combines the strengths of all query modalities (infrared, visible, and text features) to create an ensemble query that leverages their complementary strengths, improving retrieval accuracy.", "section": "3 Proposed Method"}, {"figure_path": "qQlmONeI5k/figures/figures_5_1.jpg", "caption": "Figure 3: The Visualization of SFF. With the aligned features of generated texts and original images, SFF creates fusion features semantically consistent with visible modality by arithmetically adding the textual complementary information for infrared modality to the infrared features.", "description": "This figure illustrates the Semantic Filtered Fusion (SFF) module.  It shows how the module leverages the alignment between generated text features and original image features to create fusion features.  By arithmetically adding textual complementary information to the infrared features, SFF generates fusion features semantically consistent with the visible modality.  The examples highlight how color information, missing in the infrared, is effectively incorporated into the fusion features by using the text descriptions. This process ensures the semantic consistency between fusion and visible modalities.", "section": "3.2 Incremental Fine-tuning Strategy (IFS)"}, {"figure_path": "qQlmONeI5k/figures/figures_9_1.jpg", "caption": "Figure 4: First row (a-d) shows the t-SNE feature distribution of the 20 randomly selected identities, triangles means infrared features (w/wo textual enhancement), circles means visible features. Different colors indicate different identities. Figures in the second row (e-h) represent the intra-class (blue) and inter-class (green) distances of infrared features (w/wo textual fusion) and visible features.", "description": "This figure visualizes the effectiveness of the proposed TVI-LFM method by comparing feature distributions using t-SNE.  The top row shows t-SNE plots of feature distributions for different methods: initial features, features from a VI-ReID backbone, features from the baseline model, and features from TVI-LFM.  The different colors represent different identities, triangles represent infrared features, and circles represent visible features. This helps illustrate how well the features are separated.  The bottom row displays the distribution of intra-class (blue) and inter-class (green) distances for each method.  The goal is to show that TVI-LFM better separates features of different identities and reduces the intra-class distance while increasing inter-class distances, indicating improved performance.", "section": "4.4 Visualization"}, {"figure_path": "qQlmONeI5k/figures/figures_14_1.jpg", "caption": "Figure 2: Illustration of our TVI-LFM, including Modality-Specific Caption (MSC), Incremental Fine-tuning Strategy (IFS), and Modality Ensemble Retrieval (MER). MSC utilizes fine-tuned VLMs as modal-specific captioners and employs an LLM for augmentation. IFS fine-tunes a pre-trained VLM to create fusion features semantically consistent with visible features. MER leverages the strengths of all query modalities to form ensemble queries, thereby improving the retrieval performance.", "description": "This figure illustrates the architecture of the proposed Text-enhanced Visible-Infrared Person Re-identification framework driven by Large Foundation Models (TVI-LFM). It shows three main modules: Modality-Specific Caption (MSC), Incremental Fine-tuning Strategy (IFS), and Modality Ensemble Retrieval (MER). MSC automatically generates text captions for visible and infrared images using fine-tuned Vision-Language Models (VLMs) and augments them with a Large Language Model (LLM). IFS fine-tunes a pre-trained VLM to align generated texts with original images and creates fusion features. MER combines features from all modalities to form ensemble queries, improving retrieval performance. The figure highlights the interactions and data flow between different modules, demonstrating the overall workflow of the proposed method.", "section": "3 Proposed Method"}, {"figure_path": "qQlmONeI5k/figures/figures_15_1.jpg", "caption": "Figure 2: Illustration of our TVI-LFM, including Modality-Specific Caption (MSC), Incremental Fine-tuning Strategy (IFS), and Modality Ensemble Retrieval (MER). MSC utilizes fine-tuned VLMs as modal-specific captioners and employs an LLM for augmentation. IFS fine-tunes a pre-trained VLM to create fusion features semantically consistent with visible features. MER leverages the strengths of all query modalities to form ensemble queries, thereby improving the retrieval performance.", "description": "This figure illustrates the proposed TVI-LFM framework, which consists of three main modules: Modality-Specific Caption (MSC), Incremental Fine-tuning Strategy (IFS), and Modality Ensemble Retrieval (MER). MSC uses fine-tuned Vision-Language Models (VLMs) to generate captions for visible and infrared images, then leverages a Large Language Model (LLM) to augment these captions. IFS incrementally fine-tunes a pre-trained VLM to align generated textual features with original image features, creating semantically consistent fusion features.  Finally, MER combines features from all modalities to form an ensemble query, enhancing retrieval accuracy.", "section": "3 Proposed Method"}]