[{"type": "text", "text": "Stochastic Concept Bottleneck Models ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Moritz Vandenhirtz,\u2217 Sonia Laguna,\u2217 Ric\u02c7ards Marcinkevic\u02c7s, Julia E. Vogt Department of Computer Science ETH Zurich Switzerland ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Concept Bottleneck Models (CBMs) have emerged as a promising interpretable method whose final prediction is based on intermediate, human-understandable concepts rather than the raw input. Through time-consuming manual interventions, a user can correct wrongly predicted concept values to enhance the model\u2019s downstream performance. We propose Stochastic Concept Bottleneck Models (SCBMs), a novel approach that models concept dependencies. In SCBMs, a single-concept intervention affects all correlated concepts, thereby improving intervention effectiveness. Unlike previous approaches that model the concept relations via an autoregressive structure, we introduce an explicit, distributional parameterization that allows SCBMs to retain the CBMs\u2019 efficient training and inference procedure. Additionally, we leverage the parameterization to derive an effective intervention strategy based on the confidence region. We show empirically on synthetic tabular and natural image datasets that our approach improves intervention effectiveness significantly. Notably, we showcase the versatility and usability of SCBMs by examining a setting with CLIP-inferred concepts, alleviating the need for manual concept annotations. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In today\u2019s world, machine learning plays a crucial role in making important decisions, from healthcare to finance and law. However, as these algorithms become more complex, understanding how they arrive at their decisions becomes increasingly challenging. This lack of interpretability is a significant concern, especially in situations where trustworthiness, transparency, and accountability are paramount (Lipton, 2016; Doshi-Velez & Kim, 2017). Recent studies have focused on Concept Bottleneck Models (CBMs) (Koh et al., 2020; Havasi et al., 2022; Shin et al., 2023), a class of models that predict human-understandable concepts upon which the final target prediction is based. CBMs offer interpretability since a user can inspect the predicted concept values to understand how the model arrives at its final target prediction. Moreover, if they disagree with a concept prediction, they can intervene by adjusting it to the right value, which in turn affects the target prediction. ", "page_idx": 0}, {"type": "text", "text": "For example, consider the yellow warbler in Figure 1 (a), where a user might notice that the binary concept \u2018yellow primary color\u2019 is mispredicted. Upon this realization, they can intervene on the CBM by setting its value to 1, which increases the probability of the class yellow warbler. This way of interacting allows any untrained user to engage with the model to increase its predictive performance. ", "page_idx": 0}, {"type": "text", "text": "However, if the user input is that the primary color is yellow, should not the likelihood of a yellow crown increase too? This adaptation would increase the predicted likelihood of the correct class even more, as yellow warblers are characterized by their fully yellow body. Currently, vanilla CBMs do not exhibit this behavior as they do not use the intervened-on concepts to update their remaining concept predictions. This indicates that they suboptimally adapt to the additional knowledge gained. ", "page_idx": 0}, {"type": "image", "img_path": "iSjqTQ5S1f/tmp/64f97688873419a7e4973d857efc97da4d0603637ebff4fce49b82a18523ff5f.jpg", "img_caption": ["Figure 1: Overview of the proposed method for the CUB dataset. (a) A user intervenes on the concept of \u2018primary color: yellow\u2019. Unlike CBMs, our method then uses this information to adjust the predicted probability of correlated concepts, thereby affecting the target prediction. (b) Schematic overview of the intervention procedure. A user\u2019s intervention $c_{S}^{\\prime}$ is used to infer the logits $\\eta_{\\setminus{S}}$ of the remaining concepts. (c) Visualization of the learned global dependency structure as a correlation matrix for the 112 concepts of CUB (Wah et al., 2011). Characterization of concepts on the left. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "To this end, we propose to extend the concept predictions with the modeling of their dependencies, as depicted in Figure 1. ", "page_idx": 1}, {"type": "text", "text": "The proposed approach captures the concept dependencies by modeling the concept logits with a learnable non-diagonal normal distribution, which enables efficient, scalable computing of the effect of interventions on other concepts. By integrating concept correlations, we reduce the time and effort of having to laboriously intervene on many correlated variables and increase the efficacy of interventions on the downstream prediction. Thanks to the explicit distributional assumptions, the model is trained end-to-end, retaining the training and inference speed of classic CBMs as well as the beneftis of training the concept and target predictor jointly. Moreover, we show that our method excels when querying user interventions based on predicted concept uncertainty (Shin et al., 2023), further highlighting the practical utility of our approach as such policies spare users from manually sifting through the concepts to identify necessary interventions. Lastly, based on the distributional concept parameterization, we propose a novel approach for computing dependency-aware interventions through the likelihood-based confidence region. ", "page_idx": 1}, {"type": "text", "text": "Contributions This work contributes to the line of research on concept bottleneck models in several ways. (i) We propose to capture and model concept dependencies with a multivariate normal distribution. (ii) We derive a novel intervention strategy based on the confidence region of the normal distribution that incorporates concept correlations. Using the learned concept dependencies during the intervention procedure allows for stronger interventional effectiveness. (iii) We provide a thorough empirical assessment of the proposed method on synthetic tabular and natural image data. Additionally, we combine our method with concept discovery where we alleviate the need for annotations by using CLIP-inferred concepts. In particular, we show the proposed method (a) discovers meaningful, interpretable patterns in the form of concept dependencies, (b) allows for fast, scalable inference, and (c) outperforms related work with respect to intervention effectiveness thanks to the proposed concept modeling and intervention strategy. ", "page_idx": 1}, {"type": "text", "text": "2 Background & Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Concept bottleneck models (Koh et al., 2020; Lampert et al., 2009; N. Kumar et al., 2009) are typically trained on data points $(\\pmb{x},\\pmb{c},y)$ , comprising the covariates $\\pmb{x}\\in\\mathcal{X}$ , target $y\\in\\mathcal{V}$ , and $C$ annotated binary concepts $c\\in{\\mathcal{C}}$ . Consider a neural network $f_{\\theta}$ parameterized by $\\pmb{\\theta}$ and a slice $\\langle g_{\\psi},h_{\\phi}\\rangle$ (Leino et al., 2018) s.t. $\\hat{y}:=f_{\\pmb{\\theta}}\\left(\\pmb{x}\\right)=g_{\\pmb{\\psi}}\\left(h_{\\phi}\\left(\\pmb{x}\\right)\\right)$ . CBMs enforce a concept bottleneck $\\hat{\\pmb{c}}:=h_{\\phi}(\\pmb{x})$ such that the model\u2019s final output depends on the covariates $\\textbf{\\em x}$ solely through the predicted concepts $\\hat{c}$ . ", "page_idx": 1}, {"type": "text", "text": "While Koh et al. (2020) propose the soft CBM, where the concept logits parameterize the bottleneck, Havasi et al. (2022) argue that such a representation leads to leakage, where additional unwanted information in the concept representation is used to predict the target (Margeloiu et al., 2021; Mahinpei et al., 2021). Thus, they parameterize the bottleneck by binarized concept predictions and call it the hard CBM. Then, Havasi et al. (2022) equip the hard CBM with an autoregressive structure of the form $c_{i}|\\mathbf{\\boldsymbol{x}},\\mathbf{\\boldsymbol{c}}_{<i}$ , which is supposed to learn the concept dependencies. As such, the implicit autoregressive modeling of concept dependencies by Havasi et al. (2022) is the most related to the current work. Complementary to our work, Heidemann et al. (2023) analyze how a CBM\u2019s performance is affected by concept correlations. Unlike approaches that restrict the bottleneck to prevent leakage, Concept Embedding Models (CEM) (Espinosa Zarlenga et al., 2022) represent each concept with an embedding vector from which the concept probabilities can be inferred. E. Kim et al. (2023) model the embedding with a normal distribution, assuming a diagonal covariance matrix, which prevents them from capturing concept dependencies. Therefore, their intervention performance is not expected to differ from that of CEMs. Recent works explored how a CBM-like structure can be enforced even without a concept-annotated training set. Yuksekgonul et al. (2023) transform a pre-trained model into a CBM via a concept bank from concept activation vectors and multimodal models (B. Kim et al., 2018), while Oikarinen et al. (2023) query GPT-3 (Brown et al., 2020) for the concept set $\\mathcal{C}$ and assign the values of the concept activations to each datapoint $\\textbf{\\em x}$ with CLIP (Radford et al., 2021) similarities. Similarly, Panousis et al. (2023) uses CLIP to probabilistically discover a sparse set of concepts for each input, which could be used in our model for a fully probabilistic pipeline. Lastly, Marcinkevic\u02c7s et al. (2024) instead relax the need for a concept labeled training set to a smaller validation set by fine-tuning a pre-trained model. ", "page_idx": 2}, {"type": "text", "text": "Intervenability (Marcinkevi\u02c7cs et al., 2024) is a crucial element of CBMs as it allows the user to correct wrongly predicted concepts $\\hat{c}$ to $c^{\\prime}$ , which in turn affects the target prediction of the model $\\hat{y}^{\\prime}$ . If multiple concepts are intervened on sequentially, the order of interventions is important. To this end, Sheth et al. (2022) and Shin et al. (2023) explore multiple policies according to which the order of concepts is determined. Chauhan et al. (2023) propose to combine predefined policies with learnable weighting parameters, while Espinosa Zarlenga et al. (2024) learn the policy itself. Concurrently, Singhi et al. (2024) learn a realignment module to align concept predictions. Steinmann et al. (2023) argue that instance-specific interventions are costly and store previous interventions in a memory to automatically reapply them for similar data points. Lastly, Collins et al. (2023) explore the advantages of including uncertainty rather than treating humans as oracles. ", "page_idx": 2}, {"type": "text", "text": "Our work models concept dependencies by parameterizing the bottleneck with a distribution. In a similar vein, Variational Autoencoders (Kingma & Welling, 2014) parameterize the bottleneck with a normal distribution to model and generate new data. Stochastic Segmentation Networks (Monteiro et al., 2020) parameterize the logits of a segmentation map with a non-diagonal normal distribution to capture the spatial correlations of pixels and model the aleatoric uncertainty. The modeling of uncertainty with a distribution is also explored by Bayesian Neural Networks (Neal, 1995) that learn a probability distribution over the neurons of a neural network. ", "page_idx": 2}, {"type": "text", "text": "3 Methods ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We propose Stochastic Concept Bottleneck Models1 (SCBM), a novel concept-based method that relaxes the implicit CBM assumption of independent concepts. SCBM captures the concept dependencies by learning their multivariate distribution. As a result, interventions become more effective and scalable, as a single intervention can influence multiple correlated concepts. A schematic overview of the proposed method is depicted in Figure 1 (b). ", "page_idx": 2}, {"type": "text", "text": "3.1 Model Formulation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "To capture the concept dependencies, we model the concept logits $\\eta$ with a learned multivariate normal distribution. Modeling logits with a normal distribution has proven to be effective in the context of segmentation (Monteiro et al., 2020). While Monteiro et al. (2020) use it to capture the spatial dependencies of pixels, we, instead, model the relations between concepts, where the properties of the normal distribution will prove useful. A neural network is trained to predict the distribution\u2019s parameters $\\pmb{\\eta}\\mid\\pmb{x}\\sim\\mathcal{N}\\left(\\pmb{\\mu}(\\pmb{x})\\right),\\mathbf{\\bar{\\Sigma}}(\\pmb{x}))$ , where $\\pmb{\\mu}(\\pmb{x})\\in\\mathbb{R}^{C}$ , and $\\Sigma(\\pmb{x})\\in\\mathbb{R}^{C^{\\mathrm{\\star}}C}$ . Thus, the traditional assumption of independent concepts $c_{i}\\perp\\!\\!\\!\\perp c_{j}\\mid\\pmb{x},\\;\\forall i\\neq j$ is relaxed to $c_{i}\\perp\\!\\!\\!\\perp c_{j}\\mid\\pmb{\\eta},\\ \\forall i\\neq j$ , where the assumed normal distribution induces linear concept dependencies. The inductive bias of linearity is useful in practice as it is more robust to overftiting and computationally more scalable with respect to $C$ compared to its nonlinear alternative (Havasi et al., 2022), as we will show in Section 5. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "To learn the distribution, we minimize the negative log-likelihood ", "page_idx": 3}, {"type": "equation", "text": "$$\n-\\log p(\\pmb{c}\\mid\\pmb{x})=-\\log\\int p(\\pmb{c}\\mid\\pmb{\\eta})p_{\\phi}(\\pmb{\\eta}\\mid\\pmb{x})d\\pmb{\\eta},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\phi$ are the parameters of a neural network that predicts the distribution $\\textbf{\\textit{\\eta}}|\\textbf{\\textit{x}}\\sim$ $\\mathcal{N}\\left(\\pmb{\\mu}(\\pmb{x})\\right),\\pmb{\\Sigma}(\\pmb{x})\\right)$ . This integral is intractable due to the softmax operation applied in $p(c\\mid\\eta)$ . Thus, the integral is approximated by $M$ Monte Carlo samples ", "page_idx": 3}, {"type": "equation", "text": "$$\n-\\log\\int p(c\\mid\\eta)p_{\\phi}(\\eta\\mid x)d\\eta\\approx-\\log\\frac{1}{M}\\sum_{m=1}^{M}p(c\\mid\\eta^{(m)}),\\;\\;\\;\\eta^{(m)}\\mid x\\sim N\\left(\\mu(x)\\right),\\Sigma(x))\\,.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "In order to learn $\\phi$ , we make use of the parameterization as normal distribution and employ the reparameterization trick $\\eta^{(m)}\\mid x=\\mu(x)\\stackrel{\\cdot}{+}{\\bf L}(x)\\epsilon^{(m)},\\quad{\\bf L}(x){\\bf L}(x)^{T}=\\Sigma(x),\\quad\\epsilon^{(m)}\\sim$ $\\epsilon^{(m)}\\sim\\bar{\\mathcal{N}}\\left(\\mathbf{0},\\pmb{I}\\right)$ such that gradients can be computed with respect to the parameters. Lastly, we incorporate the new relaxed conditional independence assumption ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\log p({\\boldsymbol{c}}\\mid{\\boldsymbol{\\eta}})=\\log\\prod_{i=1}^{C}p(c_{i}\\mid{\\boldsymbol{\\eta}}_{i})=\\sum_{i=1}^{C}\\log p(c_{i}\\mid{\\boldsymbol{\\eta}}_{i}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $p(c_{i}\\mid\\eta_{i})$ describes a Bernoulli distribution parameterized by the sigmoid-transformed logits $\\sigma(\\eta_{i})$ . Combining the above considerations results in the following reformulation of the negative log-likelihood: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle-\\log p(\\boldsymbol{c}\\mid\\boldsymbol{x})\\approx-\\log\\frac{1}{M}\\sum_{m=1}^{M}p(\\boldsymbol{c}\\mid\\eta^{(m)})}\\\\ {\\displaystyle\\propto-\\log\\sum_{m=1}^{M}\\exp\\sum_{i=1}^{C}\\log p(c_{i}\\mid\\eta_{i}^{(m)})}\\\\ {\\displaystyle=-\\log\\sum_{m=1}^{M}\\exp\\sum_{i=1}^{C}\\left[-\\mathrm{BCE}(c_{i},\\sigma(\\eta_{i}^{(m)}))\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where BCE stands for Binary Cross Entropy, and the logsumexp trick is used for numerical stability. ", "page_idx": 3}, {"type": "text", "text": "The distribution-based modeling procedure allows for efficient sampling, thus, enabling SCBM to train concept and target predictors jointly, sequentially, or independently. In contrast, the autoregressive alternative (Havasi et al., 2022) requires independent training due to the computational complexity. We adopt a joint training scheme to obtain the benefits of end-to-end learning where concept and target predictors can adjust to each other. To prevent leakage, we follow Havasi et al. (2022) and train the model with the hard $\\{0,1\\}$ concept values as bottleneck rather than the logits used in the original CBM (Koh et al., 2020). To this end, we employ the straight-through Gumbel-Softmax trick (Jang et al., 2017; Maddison et al., 2017) that approximates Bernoulli samples while being differentiable. The target predictor $g_{\\psi}$ is then learned by minimizing the negative log-likelihood ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{-\\log p(y\\mid x)=-\\log\\displaystyle\\sum_{c\\in\\mathcal{C}}p_{\\psi}(y\\mid c)p(c\\mid x)}}\\\\ {{\\approx-\\log\\displaystyle\\frac{1}{M}\\sum_{m=1}^{M}p_{\\psi}(y\\mid c^{(m)}),\\qquad c^{(m)}\\sim p(c\\mid x).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Lastly, the learned dependencies are regularized by following Occam\u2019s razor and to prevent overftiting. We take inspiration from the Graphical Lasso (Friedman et al., 2008) and penalize the off-diagonal elements of the precision matrix $\\mathbf{\\dot{Z}}^{-1}$ . ", "page_idx": 3}, {"type": "text", "text": "By combining concept, target, and precision loss with weighting factors $\\lambda_{1}$ and $\\lambda_{2}$ , we arrive at the final loss function ", "page_idx": 3}, {"type": "equation", "text": "$$\n-\\log\\sum_{m=1}^{M}\\exp\\sum_{i=1}^{C}-\\mathrm{BCE}\\left(c_{i},\\sigma(\\eta_{i}^{(m)})\\right)+\\lambda_{1}\\mathrm{CE}\\left(y,\\frac{1}{M}\\sum_{m=1}^{M}g_{\\psi}(c^{(m)})\\right)+\\lambda_{2}\\sum_{i\\neq j}\\Sigma(\\pmb{x})_{i,j}^{-1}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "3.2 Covariance Learning ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The introduced amortized covariance matrix $\\Sigma(x)$ provides the flexibility to tailor its predicted concept dependencies to each data point, making it adaptable to many data-generating mechanisms. For example, in the commonly used CUB (Wah et al., 2011; Koh et al., 2020), it can learn the class-wise concept structure present in the dataset. The explicit dependency representation inferred by the learned covariance matrix is useful as it provides insights into the learned correlations among the concepts, which is important for understanding and interpreting the model behavior. ", "page_idx": 4}, {"type": "text", "text": "However, an amortized covariance matrix comes at the price of not being able to visualize and interpret a unified concept structure on a dataset level. Depending on the need of the application, such a global structure might be preferable. Thus, we propose a variation of SCBM, where the covariance matrix is not amortized $(\\Sigma(x))$ , but learned globally $(\\Sigma)$ . An example of the global concept structure learned on CUB is shown in Figure 1 (c). This variation has the inductive bias of assuming a constant covariance matrix, whose utility depends on the underlying data-generating mechanism. We recommend using the more flexible, amortized version by default and only utilizing a global covariance if the strong assumption of fixed dependencies is reasonable. We will explore this empirically in more detail in Section 5. ", "page_idx": 4}, {"type": "text", "text": "3.3 Interventions ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "A distinguishing property of CBM-like methods is the user\u2019s capacity to correct wrongly predicted concepts, which in turn affects the target prediction (Marcinkevi\u02c7cs et al., 2024). For a big concept set, this intervention procedure can become quite laborious as a user has to inspect and manually intervene on each concept separately. SCBMs are designed to alleviate this need by utilizing the learned concept dependencies such that a single intervention affects all related concepts as modeled by the multivariate normal distribution. ", "page_idx": 4}, {"type": "text", "text": "The parameterization as a multivariate normal distribution allows for a quick, scalable intervention procedure. Given a set $\\mathcal{S}\\subset\\{1,\\ldots,C\\}$ of concept interventions, the effect on the remaining concepts $c_{\\setminus S}$ is computed via their logits $\\eta_{\\setminus\\mathcal{S}}$ by conditioning on the intervention logits $\\eta_{S}^{\\prime}$ , utilizing the known properties of the normal distribution ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\eta_{\\backslash\\textit{S}}|\\ x,\\eta_{S}^{\\prime}\\sim{\\mathcal{N}}\\left(\\bar{\\mu}(\\pmb{x}),\\overline{{\\Sigma}}(\\pmb{x})\\right),}\\\\ &{\\qquad\\quad\\quad\\bar{\\mu}=\\mu_{\\backslash\\mathscr{S}}+\\Sigma_{\\backslash\\mathscr{S},\\mathscr{S}}\\Sigma_{\\mathscr{S},\\mathscr{S}}^{-1}(\\eta_{S}^{\\prime}-\\mu_{S}),}\\\\ &{\\qquad\\quad\\quad\\overline{{\\Sigma}}=\\Sigma_{\\backslash\\mathscr{S},\\backslash\\mathscr{S}}-\\Sigma_{\\backslash\\mathscr{S},\\mathscr{S}}\\Sigma_{\\mathscr{S},\\mathscr{S}}^{-1}\\Sigma_{\\mathscr{S},\\backslash\\mathscr{S}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "In standard CBMs, an intervention affects only the concepts on which the user intervenes. As such, Koh et al. (2020) set $\\eta_{i}^{\\prime}$ to the 5th percentile of the training distribution if $c_{i}\\,=\\,0$ and the 95th percentile if $c_{i}=1$ . While this strategy is effective for SCBMs too, see Appendix C.5, the modeling of the concept dependencies warrants a more thorough analysis of the intervention strategy. We present two desiderata, which our intervention strategy should fulfill. ", "page_idx": 4}, {"type": "text", "text": "i) $p(c_{i}\\mid\\eta_{i}^{\\prime})\\geq p(c_{i}\\mid\\mu_{i})$ The likelihood of the intervened-on concept $c_{i}$ should always increase after the intervention. If SCBMs used the same strategy as CBMs, it could happen that the initially predicted $\\mu_{i}$ was more extreme than the selected training percentile. Then, the interventional shift $\\eta_{i}^{\\prime}-\\mu_{i}$ in Eq. 7 would point in the wrong direction. This would cause $\\eta_{\\setminus{S}}$ to shift incorrectly.   \nii) $|\\eta_{i}^{\\prime}-\\mu_{i}|$ should not be \u201ctoo large\u201d. We posit that the interventional shift should stay within a reasonable range of values. Otherwise, the effect on $\\eta_{\\setminus{S}}$ would be unreasonably large such that the predicted $\\pmb{\\mu}_{\\backslash S}$ would be completely disregarded. ", "page_idx": 4}, {"type": "text", "text": "To fulfill these desiderata, we take advantage of the explicit distributional representation: the likelihood-based confidence region of $\\mu_{i}$ provides a natural way of specifying the region of possible $\\eta_{S}^{\\prime}$ that fulflil our desiderata. Informally, a confidence region captures the region of plausible values for a parameter of a distribution. Note that the confidence region takes concept dependencies into account when describing the area of possible $\\eta_{S}^{\\prime}$ . To determine the specific point within this region, we search for the values $\\eta_{S}^{\\prime}$ , which maximize the log-likelihood of the known, intervened-on concepts $\\mathbf{c}_{S}$ , implicitly focusing on concepts that the model predicts poorly: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\eta_{S}^{\\prime}=\\underset{\\eta_{S}}{\\arg\\operatorname*{max}}\\log p(c_{S}\\mid\\eta_{S})}\\\\ &{\\qquad\\qquad\\mathrm{s.t.}-2\\left(\\log p(\\eta_{S}\\mid\\mu_{S},\\Sigma_{S,S})-\\log p(\\mu_{S}\\mid\\mu_{S},\\Sigma_{S,S})\\right)\\leq\\chi_{d,1-\\alpha}^{2}}\\\\ &{\\qquad\\qquad\\qquad\\eta_{i}^{\\prime}-\\mu_{i}\\geq0\\,\\,\\mathrm{if}\\,\\,c_{i}=1,\\quad\\forall i\\in\\mathcal{S}}\\\\ &{\\qquad\\qquad\\eta_{i}^{\\prime}-\\mu_{i}\\leq0\\,\\,\\mathrm{if}\\,\\,c_{i}=0,\\quad\\forall i\\in\\mathcal{S},}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $d=|\\boldsymbol{S}|$ . The first inequality describes the confidence region. It is based on the logarithm of the likelihood ratio, which, after multiplying with $-2$ , asymptotically follows a $\\chi^{2}$ distribution (Silvey, 1975). The last two inequalities restrict the region to the desired direction. Note that $\\eta_{S}^{\\prime}$ is computed to determine the conditional effect of the interventions on $\\eta_{\\setminus{S}}$ using Equation 7. When predicting $\\hat{y}^{\\prime}$ under interventions, the logits $\\eta_{\\setminus\\mathcal{S}}$ are then used for sampling the binary concept values $c_{\\setminus S}$ while the intervened-on concepts $c_{S}^{\\prime}$ are directly set to their known, binary value. ", "page_idx": 5}, {"type": "text", "text": "4 Experimental Setup ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Datasets and Evaluation We perform experiments on a variety of datasets to showcase the validity of our method. Inspired by Marcinkevic\u02c7s et al. (2024), we introduce a synthetic tabular dataset with a data-generating mechanism that contains fixed concept dependencies we can regulate. In particular, the concept logits $\\eta$ are sampled from a randomly initialized positive definite covariance matrix and generate $\\textbf{\\em x}$ . Binary concept values $^c$ are inferred from $\\eta$ and generate the target $y$ . We refer to Appendix A.1 for a more detailed description. ", "page_idx": 5}, {"type": "text", "text": "As a natural image classification benchmark, we evaluate on the Caltech-UCSD Birds-200-2011 dataset (Wah et al., 2011), comprised of bird photographs from 200 distinct classes. It includes 112 concepts, such as wing color and beak shape, shared across the same class instances as revised in the original CBM work (Koh et al., 2020). Additionally, we explore another natural image classification task on CIFAR-10 (Krizhevsky et al., 2009) with 10 classes. To mitigate the concept annotations requirement, the concepts are synthetically acquired in a similar fashion to the concept discovery literature. We adopt the 143 concept classes generated via GPT-3 (Brown et al., 2020) in prior work (Oikarinen et al., 2023). To obtain the binary concept values, we use the CLIP model (Radford et al., 2021) to compute the similarity between each instance of an image with the text embedding of a specific concept and compare it to the similarity of its negative counterpart, i.e. not the concept. Appendix A.2 contains further details about the natural image datasets. ", "page_idx": 5}, {"type": "text", "text": "To compare methods, we evaluate the model performance based on the concept and target accuracy. We compute test performance before and after intervening on an increasing number of concepts. The order of concepts in the intervention is determined by an uncertainty-based policy (Shin et al., 2023) that selects the concept whose predicted probability is closest to 0.5. We also show results for a random policy in Appendix C.3. Additionally, we evaluate the calibration of the predicted concept uncertainties that are being used for the uncertainty-based policy, with the Brier score (Brier, 1950) and the Expected Calibration Error (Naeini et al., 2015; A. Kumar et al., 2019). ", "page_idx": 5}, {"type": "text", "text": "Baselines We evaluate the performance of our method in comparison with state-of-the-art models. Namely, we focus on the vanilla concept bottleneck model (CBM) by Koh et al. (2020) in its hard version (Havasi et al., 2022), trained jointly using the straight-through Gumbel-Softmax trick (Jang et al., 2017; Maddison et al., 2017), as a sensical baseline to our binary modeling of concepts. Additionally, we explore the concept embedding model (CEM) by Espinosa Zarlenga et al. (2022) that learns two concept embeddings, $\\hat{c}_{i}^{+}$ and $\\hat{c}_{i}^{-}$ . These representations are used to predict the final concept probability with a learnable scoring function $\\hat{p}_{i}\\,=\\,s(\\hat{\\pmb{c}}_{i}^{+},\\hat{\\pmb{c}}_{i}^{-})\\,=\\,\\sigma({\\bf W}_{s}[\\hat{\\pmb{c}}_{i}^{+},\\hat{\\pmb{c}}_{i}^{-}]^{T}+{\\bf b}_{s})$ and are then combined into a final concept embedding $\\hat{c}_{i}=(\\hat{p}_{i}\\hat{c}_{i}^{+}+(1-\\hat{p}_{i})\\hat{c}_{i}^{-})$ that is passed to the target predictor. Interventions are modeled by altering the concept probabilities $\\hat{p}_{i}$ . Note that Espinosa Zarlenga et al. (2022) optimize for intervention performance during training, which we omit, to ensure a fair comparison where no method was explicitly trained for intervention performance. Finally, we evaluate the autoregressive CBM structure proposed by Havasi et al. (2022), where concept dependencies are learned with an autoregressive structure. Here, each concept $c_{i}$ is predicted with a separate MLP that takes as input a latent representation of the input $f_{\\pmb\\theta}(\\pmb x)$ and all previous concepts $c_{1},...,c_{i-1}$ . To obtain a good initialization of the autoregressive structure, it is pretrained for 50 epochs. As the Monte Carlo sampling from the autoregressive structure is time-consuming, the target predictor $g_{\\psi}$ is trained independently using the ground-truth concepts as input. At intervention time, a normalized importance sampling algorithm is used to estimate the concept distribution. ", "page_idx": 5}, {"type": "table", "img_path": "iSjqTQ5S1f/tmp/70a5e125f7d8c8bf6eedb6dc4143e558b7cbb27ff2f61a9bbc2bac114b83e445.jpg", "table_caption": ["Table 1: Test-set concept and target accuracy $(\\%)$ prior to interventions. Results are reported as averages and standard deviations of model performance across ten seeds. For each dataset and metric, the best-performing method is bolded and the runner-up is underlined. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Implementation Details The model architectures comprise a backbone for concept prediction followed by a linear layer as head for an interpretable target prediction. More details can be found in Appendix B. To ensure the positive definiteness of the concept covariance matrix $\\Sigma$ , we parameterize it via its Cholesky decomposition $\\pmb{\\Sigma}=\\pmb{L}\\pmb{L}^{\\top}$ . Thus, we directly predict the lower triangular Cholesky matrix $\\textbf{\\emph{L}}$ . We will evaluate two options for SCBMs: using a global $(\\Sigma)$ or an amortized covariance matrix $\\left(\\Sigma(x)\\right)$ . For the amortized version, we set the weighting terms $\\lambda_{1}$ and $\\lambda_{2}$ of Equation 6 to 1. For the global version, we initialize it with the estimated empirical covariance matrix and set $\\lambda_{2}=0$ , as we did not observe big differences when varying $\\lambda_{2}$ . In Appendix C.4, we provide an ablation study, demonstrating that SCBMs are not very sensitive to the choice of $\\lambda_{2}$ . At intervention time, we solve the optimization problem based on the $99\\%$ -confidence region with the SLSQP algorithm (Kraft, 1988). In Appendix C.6, we provide an ablation with different confidence levels. ", "page_idx": 6}, {"type": "text", "text": "5 Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Test performance In Table 1, we report the results of the concept and target accuracy prior to interventions. Overall, SCBM performs on par with the baseline methods, with no clear outperforming or underperforming technique throughout the datasets. In Appendix C.7, we show that other metrics lead to the same interpretation. This shows that the additional overhead of learning the concept dependencies does not negatively affect the predictive performance. We note that the amortized covariance variant con", "page_idx": 6}, {"type": "table", "img_path": "iSjqTQ5S1f/tmp/16cc2da5d9b5a9dee0e45a626f1a5f8e66be3b40d5b54b310b7d926d230f4799.jpg", "table_caption": ["Table 2: Relative time it takes for one epoch in the CUB dataset when training on the training set, or evaluating on the test set, respectively. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "sistently surpasses the globally learned matrix due to its ability to adjust the predicted concept dependency structure and uncertainty on an instance level. On the other hand, the global variant offers a unified understanding of the concept correlations, an example of which is presented in Figure 1 (c). Notably, in CIFAR-10, even though the concept performance of CEM is the worst of all methods, it has the best target performance. This might suggest the presence of leakage in CEM\u2019s embeddings, as in CIFAR-10, the concept set alone is not sufficient to predict the target, and learning additional information might be useful. In Table 2, we show the time it takes for training and testing of the methods. It is evident that the autoregressive CBM of Havasi et al. (2022) suffers from a slow sampling process due to its autoregressive structure, while SCBMs retain the efficiency of CBMs. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "image", "img_path": "iSjqTQ5S1f/tmp/d14661323ddf83d8f0e4e01b2dec5fcd4839c1544fd8e9788c0b1087408d803a.jpg", "img_caption": ["Figure 2: Performance after intervening on concepts in the order of highest predicted uncertainty. Concept and target accuracy $(\\%)$ are shown in the first and second rows, respectively. Results are reported as averages and standard deviations of model performance across ten seeds. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Interventions In this paragraph, we analyze the intervention performance of SCBMs and their baseline models, focusing on their effectiveness in modeling concept dependencies and improving target accuracy. Figure 2 shows the intervention curves across ten seeds, where the performance is measured based on the concept and target accuracy. The order of concepts to intervene on is determined by an uncertainty-based policy that makes use of the predicted probabilities. In Appendix C.3, we present the intervention performance if concepts were selected randomly. The intervention curves in the first row show that SCBMs are superior in modeling the concept dependencies, as evidenced by their significantly steeper intervention curves compared to the baseline methods. Furthermore, the second row of Figure 2 indicates that the strong concept modeling translates to a significant improvement in downstream performance, partly thanks to the intervention strategy introduced in Section 3.3. We note that especially for the most practical scenario of only a small number of interventions, SCBMs outperform their counterparts. Comparing the SCBM variants, the natural image datasets show an overall better intervention performance with the amortized covariance matrix, following the trend of Table 1, as it can capture the instance-wise correlation structure of the data. Only in the synthetic dataset, where the data-generating covariance matrix is fixed, does the global SCBM slightly outperform the amortized one. Thus, we advocate for the usage of the global variant only if the underlying assumption of a fixed covariance is reasonable. Lastly, the success of SCBMs on CIFAR-10, with CLIP-based concepts, shows our proposed method can work without human-annotated concepts. To strengthen this point and also showcase the scalability of our method, in Appendix C.1, we provide results on CIFAR-100 with 892 concepts, where our SCBMs also strongly outperform baselines. ", "page_idx": 7}, {"type": "text", "text": "Analyzing the performance of the autoregressive CBM, which also captures concept dependencies, we observe that they expectedly have a better intervention performance than the hard vanilla CBM, which does not take correlations into account. However, it becomes evident that, compared to the concept performance of SCBMs, their autoregressive structure does not capture the dependencies to the full extent. This shows in the target accuracy, where they only match or outperform SCBMs towards the full set of intervened concepts. We attribute the better performance on the full intervention set to the independent training procedure utilized by autoregressive CBMs, which comes at the cost of lower test performance in CIFAR-10. Arguably, in a realistic use-case, such a high number of instance-level interventions is not sensible, and if it were, SCBMs could also be trained independently. Finally, the CEM shows reduced intervention performance as the expressive concept embeddings, which are prone to information leakage, seem to suboptimally adapt to the injected concept information. ", "page_idx": 7}, {"type": "text", "text": "Table 3: Test-set calibration $(\\%)$ of concept predictions. Results are reported as averages and standard deviations of model performance across ten seeds. For each dataset and metric, the best-performing method is bolded and the runner-up is underlined. Lower is better. ", "page_idx": 8}, {"type": "text", "text": "Figure 3: Intervention performance of SCBMs measured in concept and target accuracy $(\\%)$ on CUB for random and uncertainty-based policy. ", "page_idx": 8}, {"type": "table", "img_path": "iSjqTQ5S1f/tmp/aaff1ae2b72d013238a1e875a75c52d7ec04d998a837ff25ea03cdd978ed0cbe.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Modeling the concept distribution A cornerstone of SCBMs is the explicit, distributional parameterization of concepts. This helps in understanding the data correlations and allows for visualization, as the example seen in Figure 1 (c). The explicit probabilistic modeling results in improved concept uncertainty estimates compared to the baseline CBM counterparts, as shown in Table 3, where lower metrics imply better estimates. This proves useful for interventions, where the uncertainty estimates can be leveraged for the choice of concepts to intervene on, improving the target prediction more effectively and reducing the need for manual user inspection. In Figure 3, we compare the performance of randomly intervening versus intervening based on the predicted uncertainty. We observe that there is a big gap between the two policies, indicating the usefulness of the estimated probabilities. Nevertheless, note that intervening at random remains successful and supports the observations made in the previous paragraph, as shown in Appendix C.3. ", "page_idx": 8}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this paper, we introduced SCBMs, a new concept-based method that models concept dependencies with a multivariate normal distribution. We proposed a novel, effective intervention strategy that takes concept correlations into account and is based on the confidence region inferred from the distributional parameterization. We showed that our modeling approach retains CBMs\u2019 training and inference speed, thus, being able to harness the benefits of end-to-end concept and target training. Additionally, the explicit parameterization offers the user a clearer understanding of the learned concept dependencies, providing deeper insights into how predictions and interventions are made. Empirically, we demonstrated that by modeling the concept dependencies, SCBMs offer a substantial improvement in intervention effectiveness, in concept as well as target accuracy, compared to related work. We showed that our method excels when iteratively intervening on the most uncertain concept predictions, sparing users from having to manually search through the concept set to identify necessary interventions. Additionally, our results indicate that learning the concept correlations does not decrease performance prior to interventions, in many cases even improving the performance over the baselines. Finally, the versatility of SCBMs is highlighted through their superior performance on CIFAR-10 and CIFAR-100, where concept values are CLIP-based rather than human-annotated. ", "page_idx": 8}, {"type": "text", "text": "Limitations & Future Work This work opens multiple new research avenues. A natural extension is to go beyond binary concepts, such as continuous domains with their corresponding adaptations of modeling the concept distribution. Additionally, addressing the quadratic memory complexity of the covariance matrix is essential for scaling to larger concept sets. Our proposed intervention strategy accounts for model uncertainty, but further research is needed to accommodate user uncertainty, as human interventions are not always the ground truth. This work allows the editing of the learned dependency structure by adjusting the entries of the predicted covariance matrix, which could be explored. Lastly, to model additional information and reduce leakage, Koh et al. (2020); Havasi et al. (2022) propose the adoption of a side channel. The complementary effectiveness of incorporating the side channel in the covariance structure could be explored in the context of SCBMs. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We thank Alexander Marx for the insightful discussions. MV and SL are supported by the Swiss State Secretariat for Education, Research, and Innovation (SERI) under contract number MB22.00047. RM is supported by the SNSF grant #320038189096. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Ansel, J., Yang, E., He, H., Gimelshein, N., Jain, A., Voznesensky, M., . . . others (2024). Pytorch 2: Faster machine learning through dynamic python bytecode transformation and graph compilation. In Proceedings of the 29th acm international conference on architectural support for programming languages and operating systems, volume 2 (pp. 929\u2013947). [Referenced on page 14]   \nBrier, G. W. (1950). Verification of forecasts expressed in terms of probability. Monthly weather review, 78(1), 1\u20133. Retrieved from https://doi.org/10.1175/1520-0493(1950)078<0001: VOFEIT>2.0.CO;2 [Referenced on page 6]   \nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., . . . others (2020). Language models are few-shot learners. Advances in neural information processing systems, 33, 1877\u20131901. [Referenced on page 3, 6]   \nChauhan, K., Tiwari, R., Freyberg, J., Shenoy, P., & Dvijotham, K. (2023). Interactive concept bottleneck models. In Proceedings of the aaai conference on artificial intelligence (Vol. 37, pp. 5948\u20135955). [Referenced on page 3]   \nCollins, K. M., Barker, M., Zarlenga, M. E., Raman, N., Bhatt, U., Jamnik, M., ... Dvijotham, K. (2023). Human uncertainty in concept-based AI systems. In F. Rossi, S. Das, J. Davis, K. FirthButterfield, & A. John (Eds.), Proceedings of the 2023 AAAI/ACM conference on ai, ethics, and society, AIES 2023, montr\u00e9al, qc, canada, august 8-10, 2023 (pp. 869\u2013889). ACM. [Referenced on page 3]   \nDoshi-Velez, F., & Kim, B. (2017, March). Towards A Rigorous Science of Interpretable Machine Learning (No. arXiv:1702.08608). arXiv. doi: 10.48550/arXiv.1702.08608 [Referenced on page 1]   \nEspinosa Zarlenga, M., Barbiero, P., Ciravegna, G., Marra, G., Giannini, F., Diligenti, M., . . . others (2022). Concept embedding models: Beyond the accuracy-explainability trade-off. In Advances in neural information processing systems (Vol. 35, pp. 21400\u201321413). [Referenced on page 3, 6]   \nEspinosa Zarlenga, M., Collins, K., Dvijotham, K., Weller, A., Shams, Z., & Jamnik, M. (2024). Learning to receive help: Intervention-aware concept embedding models. Advances in Neural Information Processing Systems, 36. [Referenced on page 3]   \nFriedman, J., Hastie, T., & Tibshirani, R. (2008). Sparse inverse covariance estimation with the graphical lasso. Biostatistics, 9(3), 432\u2013441. [Referenced on page 4]   \nHavasi, M., Parbhoo, S., & Doshi-Velez, F. (2022). Addressing leakage in concept bottleneck models. In A. H. Oh, A. Agarwal, D. Belgrave, & K. Cho (Eds.), Advances in neural information processing systems. Retrieved from https://openreview.net/forum?id $=$ tglniD_fn9 [Referenced on page 1, 3, 4, 6, 8, 10]   \nHe, K., Zhang, X., Ren, S., & Sun, J. (2016). Deep residual learning for image recognition. In Proceedings of the ieee conference on computer vision and pattern recognition (pp. 770\u2013778). [Referenced on page 14]   \nHeidemann, L., Monnet, M., & Roscher, K. (2023). Concept correlation and its effects on conceptbased models. In Proceedings of the ieee/cvf winter conference on applications of computer vision (pp. 4780\u20134788). [Referenced on page 3]   \nJaccard, P. (1901). \u00c9tude comparative de la distribution florale dans une portion des alpes et des jura. Bull Soc Vaudoise Sci Nat, 37, 547\u2013579. [Referenced on page 17]   \nJang, E., Gu, S., & Poole, B. (2017). Categorical reparameterization with gumbel-softmax. In 5th international conference on learning representations, ICLR 2017, toulon, france, april 24-26, 2017, conference track proceedings. OpenReview.net. Retrieved from https://openreview.net/ forum?id $\\equiv$ rkE3y85ee [Referenced on page 4, 6]   \nKim, B., Wattenberg, M., Gilmer, J., Cai, C., Wexler, J., Viegas, F., & Sayres, R. (2018). Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (TCAV). In J. Dy & A. Krause (Eds.), Proceedings of the 35th international conference on machine learning (Vol. 80, pp. 2668\u20132677). PMLR. Retrieved from https://proceedings.mlr.press/v80/ kim18d.html [Referenced on page 3]   \nKim, E., Jung, D., Park, S., Kim, S., & Yoon, S. (2023). Probabilistic concept bottleneck models. In A. Krause, E. Brunskill, K. Cho, B. Engelhardt, S. Sabato, & J. Scarlett (Eds.), Proceedings of the ", "page_idx": 10}, {"type": "text", "text": "40th international conference on machine learning (Vol. 202, pp. 16521\u201316540). PMLR. Retrieved ", "page_idx": 11}, {"type": "text", "text": "from https://proceedings.mlr.press/v202/kim23g.html [Referenced on page 3]   \nKingma, D. P., & Ba, J. (2015). Adam: A method for stochastic optimization. In Y. Bengio & Y. LeCun (Eds.), 3rd international conference on learning representations, ICLR 2015, san diego, ca, usa, may 7-9, 2015, conference track proceedings. Retrieved from http://arxiv.org/abs/ 1412.6980 [Referenced on page 14]   \nKingma, D. P., & Welling, M. (2014). Auto-encoding variational bayes. In Y. Bengio & Y. LeCun (Eds.), 2nd international conference on learning representations, ICLR 2014, banff, ab, canada, april 14-16, 2014, conference track proceedings. Retrieved from http://arxiv.org/abs/ 1312.6114 [Referenced on page 3]   \nKoh, P. W., Nguyen, T., Tang, Y. S., Mussmann, S., Pierson, E., Kim, B., & Liang, P. (2020). Concept bottleneck models. In H. D. III & A. Singh (Eds.), Proceedings of the 37th international conference on machine learning (Vol. 119, pp. 5338\u20135348). Virtual: PMLR. Retrieved from https://proceedings.mlr.press/v119/koh20a.html [Referenced on page 1, 2, 3, 4, 5, 6, 10, 14, 17]   \nKraft, D. (1988). A software package for sequential quadratic programming. ForschungsberichtDeutsche Forschungs- und Versuchsanstalt fur Luft- und Raumfahrt. [Referenced on page 7]   \nKrizhevsky, A., Hinton, G., et al. (2009). Learning multiple layers of features from tiny images. [Referenced on page 6, 14]   \nKumar, A., Liang, P. S., & Ma, T. (2019). Verified uncertainty calibration. Advances in Neural Information Processing Systems, 32. [Referenced on page 6]   \nKumar, N., Berg, A. C., Belhumeur, P. N., & Nayar, S. K. (2009). Attribute and simile classifiers for face verification. In 2009 ieee 12th international conference on computer vision (pp. 365\u2013 372). Kyoto, Japan: IEEE. Retrieved from https://doi.org/10.1109/ICCV.2009.5459250 [Referenced on page 2]   \nLampert, C. H., Nickisch, H., & Harmeling, S. (2009). Learning to detect unseen object classes by between-class attribute transfer. In 2009 IEEE conference on computer vision and pattern recognition. Miami, FL, USA: IEEE. Retrieved from https://doi.org/10.1109/CVPR.2009 .5206594 [Referenced on page 2]   \nLeino, K., Sen, S., Datta, A., Fredrikson, M., & Li, L. (2018). Influence-directed explanations for deep convolutional networks. In 2018 IEEE international test conference (ITC). IEEE. Retrieved from https://doi.org/10.1109/test.2018.8624792 [Referenced on page 2]   \nLipton, Z. C. (2016, June). The Mythos of Model Interpretability. Communications of the ACM, 61(10), 35\u201343. doi: 10.48550/arxiv.1606.03490 [Referenced on page 1]   \nMaddison, C. J., Mnih, A., & Teh, Y. W. (2017). The concrete distribution: A continuous relaxation of discrete random variables. In 5th international conference on learning representations, ICLR 2017, toulon, france, april 24-26, 2017, conference track proceedings. OpenReview.net. Retrieved from https://openreview.net/forum?id $=$ S1jE5L5gl [Referenced on page 4, 6]   \nMahinpei, A., Clark, J., Lage, I., Doshi-Velez, F., & Pan, W. (2021). Promises and pitfalls of black-box concept learning models. Retrieved from https://doi.org/10.48550/arXiv.2106.13314 (arXiv:2106.13314) [Referenced on page 3]   \nMarcinkevi\u02c7cs, R., Laguna, S., Vandenhirtz, M., & Vogt, J. E. (2024). Beyond concept bottleneck models: How to make black boxes intervenable? In Advances in neural information processing systems (Vol. 37). [Referenced on page 3, 5]   \nMarcinkevi\u02c7cs, R., Reis Wolfertstetter, P., Klimiene, U., Chin-Cheong, K., Paschke, A., Zerres, J., ... Vogt, J. E. (2024). Interpretable and intervenable ultrasonography-based machine learning models for pediatric appendicitis. Medical Image Analysis, 91, 103042. Retrieved from https:// www.sciencedirect.com/science/article/pii/S136184152300302X [Referenced on page 6]   \nMargeloiu, A., Ashman, M., Bhatt, U., Chen, Y., Jamnik, M., & Weller, A. (2021). Do concept bottleneck models learn as intended? Retrieved from https://doi.org/10.48550/arXiv .2105.04289 (arXiv:2105.04289) [Referenced on page 3] Monteiro, M., Le Folgoc, L., Coelho de Castro, D., Pawlowski, N., Marques, B., Kamnitsas, K., ... Glocker, B. (2020). Stochastic segmentation networks: Modelling spatially correlated aleatoric uncertainty. In Advances in neural information processing systems (Vol. 33, pp. 12756\u201312767). [Referenced on page 3] Naeini, M. P., Cooper, G., & Hauskrecht, M. (2015). Obtaining well calibrated probabilities using bayesian binning. In Proceedings of the aaai conference on artificial intelligence (Vol. 29). [Referenced on page 6] Neal, R. M. (1995). Bayesian learning for neural networks (Doctoral dissertation, University of Toronto, Canada). Retrieved from https://librarysearch.library.utoronto.ca/ permalink/01UTORONTO_INST/14bjeso/alma991106438365706196 [Referenced on page   \n3] Oikarinen, T., Das, S., Nguyen, L. M., & Weng, T.-W. (2023). Label-free concept bottleneck models. In The 11th international conference on learning representations. Retrieved from https://openreview.net/forum?id $\\equiv$ FlCg47MNvBA [Referenced on page 3, 6, 15] Panousis, K. P., Ienco, D., & Marcos, D. (2023). Sparse linear concept discovery models. In Proceedings of the ieee/cvf international conference on computer vision (pp. 2767\u20132771). [Referenced on page 3] Panousis, K. P., Ienco, D., & Marcos, D. (2024). Coarse-to-fine concept bottleneck models. In Neurips 2024-38th annual conference on neural information processing systems. [Referenced on page 17] Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., . . . others (2021). Learning transferable visual models from natural language supervision. In International conference on machine learning (pp. 8748\u20138763). [Referenced on page 3, 6] Sheth, I., Rahman, A. A., Sevyeri, L. R., Havaei, M., & Kahou, S. E. (2022). Learning from uncertain concepts via test time interventions. In Workshop on trustworthy and socially responsible machine learning, neurips 2022. Retrieved from https://openreview.net/forum?id $\\cdot$ WVe3vok8Cc3 [Referenced on page 3] Shin, S., Jo, Y., Ahn, S., & Lee, N. (2023). A closer look at the intervention procedure of concept bottleneck models. In A. Krause, E. Brunskill, K. Cho, B. Engelhardt, S. Sabato, & J. Scarlett (Eds.), Proceedings of the 40th international conference on machine learning (Vol. 202, pp. 31504\u2013   \n31520). PMLR. Retrieved from https://proceedings.mlr.press/v202/shin23a.html [Referenced on page 1, 2, 3, 6, 16] Silvey, S. (1975). Statistical inference. Taylor & Francis. Retrieved from https://books.google .ch/books?id $\\equiv,$ qIKLejbVMf4C [Referenced on page 6] Singhi, N., Kim, J. M., Roth, K., & Akata, Z. (2024). Improving intervention efficacy via concept realignment in concept bottleneck models. arXiv preprint arXiv:2405.01531. [Referenced on page 3] Steinmann, D., Stammer, W., Friedrich, F., & Kersting, K. (2023). Learning to intervene on concept bottlenecks. Retrieved from https://doi.org/10.48550/arXiv.2308.13453 (arXiv:2308.13453) [Referenced on page 3] Wah, C., Branson, S., Welinder, P., Perona, P., & Belongie, S. (2011). The caltech-ucsd birds-200-   \n2011 dataset. [Referenced on page 2, 5, 6, 14] Yuksekgonul, M., Wang, M., & Zou, J. (2023). Post-hoc concept bottleneck models. In The 11th international conference on learning representations. Retrieved from https://openreview .net/forum?id $\\equiv$ nA5AZ8CEyow [Referenced on page 3] ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Dataset Details ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this section, we provide additional details on the datasets that are being used in the experiments. ", "page_idx": 13}, {"type": "text", "text": "A.1 Synthetic Data-Generating Mechanism ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Here, we describe the data-generating mechanism of the synthetic dataset in more detail. Let $N,p$ , and $C$ denote the number of independent data points $\\{(\\pmb{x}_{n},\\pmb{c}_{n},y_{n})\\}_{n=1}^{N}$ , covariates, and concepts, respectively. We set $N=50{,}000$ , $p=1{,}500$ , and $C=100$ , with a $60\\%-20\\%-20\\%$ train-validationtest split. The generative process is as follows: ", "page_idx": 13}, {"type": "text", "text": "1. Randomly sample $W\\in\\mathbb{R}^{C\\times10}$ s.t. $w_{i,j}\\sim\\mathcal{N}(0,1)$ for $1\\leq i\\leq C$ and $1\\leq j\\leq10$ .   \n2. Generate a positive definite matrix $\\Sigma\\in\\mathbb{R}^{C\\times C}$ s.t. $\\pmb{\\Sigma}=\\pmb{W}\\pmb{W}^{T}+\\pmb{D}$ . Let $D\\in\\mathbb{R}^{C\\times C}$ s.t.   \n${\\cal D}=\\delta{\\cal I}$ , where $\\delta_{i}\\sim\\mathcal{U}_{[0,1]}$ for $1\\leq i\\leq C$ .   \n3. Randomly sample logits $H\\in\\mathbb{R}^{N\\times C}$ s.t. $\\pmb{\\eta}_{n}\\sim\\mathcal{N}(\\mathbf{0},\\pmb{\\Sigma})$ for $1\\leq n\\leq N$ .   \n4. Let $c_{n,i}=\\mathbb{1}_{\\{\\eta_{n,i}\\geq0\\}}$ for $1\\leq n\\leq N$ and $1\\leq i\\leq C$ .   \n5. Let $h:\\,\\mathbb{R}^{C}\\to\\mathbb{R}^{p}$ be a randomly initialised multilayer perceptron with ReLU nonlinearities.   \n6. Let ${\\pmb x}_{n}=h\\left({\\pmb\\eta}_{n}\\right)+{\\pmb\\epsilon}_{n}$ s.t. $\\pmb{\\epsilon}_{n}\\sim\\mathcal{N}(\\mathbf{0},\\pmb{I})$ for $1\\leq n\\leq N$ .   \n7. Let $g:\\,\\mathbb{R}^{C}\\to\\mathbb{R}$ be a randomly initialized linear perceptron.   \n8. Let $y_{n}=\\mathbb{1}_{\\left\\{(g(\\mathbf{c}_{n})\\geq y_{m e d})\\right\\}}$ for $1\\leq n\\leq N$ , where $y_{m e d}$ denotes the median of $g\\left(c_{n}\\right)$ . ", "page_idx": 13}, {"type": "text", "text": "A.2 Natural Image Datasets ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Caltech-UCSD Birds-200-2011 We evaluate on the Caltech-UCSD Birds-200-2011 (CUB)2 dataset (Wah et al., 2011). It comprises 11,788 photographs from 200 distinct bird species annotated with 312 concepts, such as belly color and pattern. In this manuscript, we follow the original train-test split and revised the proposed dataset in the initial CBM work (Koh et al., 2020). Here, only the 112 most widespread binary attributes are included in the final dataset, and concepts are shared across samples in identical classes. The images were resized to a resolution of $224\\times224$ pixels. Finally, following the original proposed augmentations, we applied random horizontal flips, modified the brightness and saturation, and applied normalization during training. ", "page_idx": 13}, {"type": "text", "text": "CIFAR-10 CIFAR- $10^{3}$ (Krizhevsky et al., 2009) is a natural image benchmark with $60\\mathrm{,}000\\;32\\mathrm{x}32$ colour images and 10 classes. We kept the original train-test split, with 50,000 samples in the train set and a balanced total of 6,000 images per class. We generated 143 concept labels as described in Section 4 using large language and vision models. At training time, as for CUB, we applied augmentations including modifications to brightness and saturation, random horizontal flips and normalisation. Images were rescaled to a size of $224\\times224$ pixels. ", "page_idx": 13}, {"type": "text", "text": "B Implementation Details ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "This section provides further implementation details of SCBM and the evaluated baselines. All methods were implemented using PyTorch (v 2.1.1) (Ansel et al., 2024). All models are trained for 150 epochs for the synthetic and 300 epochs for the natural image datasets with the Adam optimizer (Kingma & Ba, 2015) with a learning rate of $10^{-4}$ and a batch size of 64. For the independently trained autoregressive model, we split the training epochs into $2/3$ for the concept predictor and $1/3$ for the target predictor. For the methods requiring sampling, the number of Monte Carlo samples is set to $M=100$ . We provide an ablation for $M=10$ in Appendix C.2. Note that since the predictor head is very simple, the MC sampling of SCBMs is extremely fast and does not influence computational complexity by more than $0.1\\%$ . For the synthetic tabular data, we use a fully connected neural network as backbone, with 3 non-linear layers, batch normalization, and dropout. For the CUB dataset, we use a pretrained ResNet-18 (He et al., 2016), and for the lower-resolution ", "page_idx": 13}, {"type": "text", "text": "CIFAR-10 a simple convolutional neural network with 2 convolutional layers followed by ReLU, Dropout, and a fully connected layer. For fairness in the comparisons, all baselines have the same model architecture choices and all experiments are performed over 10 random seeds. ", "page_idx": 14}, {"type": "text", "text": "Resource Usage For the experiments of the main paper, we used a cluster of mostly GeForce RTX 2080s with 2 CPU workers. Over all methods, we estimate an average runtime of 8h per experiment, each running on a single GPU. This amounts to 5 methods $\\times\\ 3$ datasets $\\times\\ 10$ seeds $\\times\\ 8$ hours $=$ 1200 hours. Adding to that, the Ablation Figures required another 40 runs, amounting to a full total of 1520 hours of compute. Please note that we only report the numbers to generate the final results but not the development time, which we roughly estimate to be around 10 times bigger. ", "page_idx": 14}, {"type": "text", "text": "C Further Experiments ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this section, we show additional experiments to provide a more in-depth understanding of SCBM\u2019s effectiveness. We ablate multiple hyperparameters to provide an understanding of how they influence the model performance, as well as show the performance of our model in other settings. ", "page_idx": 14}, {"type": "image", "img_path": "iSjqTQ5S1f/tmp/6478d2cd717d4f79f023f1842f2c764d81edcdc79cc6718658849b7516d2972c.jpg", "img_caption": ["Figure 4: Performance after intervening on concepts in the order of highest predicted uncertainty in CIFAR-100 with 892 concepts. Concept and target accuracy $(\\%)$ are shown in the first and second rows, respectively. Results are reported as averages and standard deviations of model performance across 3 seeds. "], "img_footnote": [], "page_idx": 14}, {"type": "image", "img_path": "iSjqTQ5S1f/tmp/e10975bb5929ba8dbd60473c0579fc7afed70fa955dd4b69ea99fa8bbc64a02e.jpg", "img_caption": ["Figure 5: Intervention performance in the order of highest predicted uncertainty in CUB. Concept and target accuracy $(\\%)$ are shown in the first and second rows, respectively. Results are reported as averages and standard deviations of model performance across 3 seeds. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "C.1 Intervention Performance on CIFAR-100 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We present the result on the CIFAR-100 dataset with 892 concepts obtained from Oikarinen et al. (2023) in Figure 4 to showcase the scalability of SCBMs. The results underline the efficiency of our method. Notably, the Autoregressive baseline has a negative dip, which is likely due to the independently trained target predictor not being aligned with the concept predictors in this noisy CLIP-annotated scenario. Note that they need to train independently to avoid the sequential MC sampling during training, which would otherwise increase training time significantly. Our jointly trained SCBMs do not have this issue and surpass the baselines. We use the same configuration as for CIFAR-10, with the exception that we set $M=10$ to reduce the memory requirement. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "C.2 Number of Monte Carlo Samples ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "To showcase that SCBMs do not rely on a huge number of Monte Carlo samples, we provide an ablation of $M$ in Figure 5. It shows that even for $M=10$ , SCBMs thrive. Note, however, that since $M$ is not a driving factor of SCBMs computational cost, one can leave it at a high number. ", "page_idx": 15}, {"type": "image", "img_path": "iSjqTQ5S1f/tmp/39d82d055047cfe273d8de34135894caf832785627a24af7f1b9379bcaa0a244.jpg", "img_caption": ["C.3 Random Intervention Policy ", "Figure 6: Performance after intervening on concepts in random order. Concept and target accuracy $(\\%)$ are shown in the first and second rows, respectively. Results are reported as averages and standard deviations of model performance across ten seeds. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "In Figure 6, we present the intervention performance of SCBM and baseline methods. Compared to the uncertainty-based intervention policy of Figure 2, the intervention curves of all methods are less steep, confirming the usefulness of Shin et al. (2023)\u2019s proposed policy. Following the previous statements, SCBMs still outperform baseline methods with the amortized beating the global variant for real-world datasets. We observe that in CIFAR-10 for the first interventions, an improvement in concept accuracy is not directly reflected in improved target prediction for SCBMs, which is likely due to the low signal-to-noise ratio of the CLIP-inferred concepts. ", "page_idx": 15}, {"type": "text", "text": "C.4 Regularization Strength ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In Figure 7, we analyze the impact of the strength of $\\lambda_{2}$ from Equation 6. Due to environmental considerations, we conducted experiments using only 5 seeds and limited the number of interventions to 20. Our findings indicate that SCBMs are not sensitive to the choice of $\\lambda_{2}$ , except that the unregularized amortized variant exhibits slight patterns of overfitting. ", "page_idx": 15}, {"type": "image", "img_path": "iSjqTQ5S1f/tmp/69011e3a48f113e42d77bb179ba6645f14d8fef84e69e81523cf829fe20f5259.jpg", "img_caption": ["Figure 7: Performance on CUB after intervening on concepts in the order of highest predicted uncertainty with differing regularization strengths. Concept and target accuracy $(\\%)$ are shown in the first and second columns, respectively. Results are reported as averages and standard deviations of model performance across five seeds. For each SCBM variant, we choose a darker color, the higher the regularization strength of $\\lambda_{2}$ . "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "C.5 Intervention Strategy ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In Figure 8, we analyze the effect of the intervention strategy. Our findings indicate that while SCBMs are still effective with the proposed strategy from Koh et al. (2020), that sets the logits to the 5th (if $c_{i}=0$ ) or 95th (if $c_{i}=1$ ) percentile of the training distribution, our proposed strategy based on the confidence region results in stronger intervenability. ", "page_idx": 16}, {"type": "image", "img_path": "iSjqTQ5S1f/tmp/1b57e5df11618184b248f035b7caf99aa1a204fc102e1aeaf757d66c11c71192.jpg", "img_caption": [], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "Figure 8: Performance on CUB after intervening on concepts in the order of highest predicted uncertainty, comparing the proposed intervention strategy to Koh et al. (2020)\u2019s intervention of setting the logits to the 5th or 95th empirical percentile of the training distribution. Concept and target accuracy $(\\%)$ are shown in the first and second columns, respectively. Results are reported as averages and standard deviations of model performance across five seeds. ", "page_idx": 16}, {"type": "text", "text": "C.6 Confidence Region Level ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In Figure 9, we analyze the effect of the level $1-\\alpha$ of the likelihood-based confidence region. Our findings indicate that the SCBMs are not sensitive to the choice of $1-\\alpha$ , with higher levels being slightly better in performance. ", "page_idx": 16}, {"type": "text", "text": "C.7 Jaccard Index ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Panousis et al. (2024) propose to interpret the interpretation capacity of concepts with the Jaccard Index (Jaccard, 1901). As such, in Table 4, we extend Table 1 with this metric. It is evident that the interpretation does not change, indicating that the performance is robust to the choice of evaluation metric. ", "page_idx": 16}, {"type": "image", "img_path": "iSjqTQ5S1f/tmp/fb198c2112a2a7d9455309aa59fe3036c53c3014d87cae618a1c50736eb5df71.jpg", "img_caption": ["Figure 9: Performance on CUB after intervening on concepts in the order of highest predicted uncertainty with differing levels $1-\\alpha$ of the confidence region. Concept and target accuracy $(\\%)$ are shown in the first and second columns, respectively. Results are reported as averages and standard deviations of model performance across three seeds. "], "img_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "iSjqTQ5S1f/tmp/b87d01c4112bb91e49082127f386659f2659f2e8acbf87c6006b94bd3ab9f711.jpg", "table_caption": ["Table 4: Test-set performance before interventions. Results are averaged across ten seeds. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Claims ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Justification: Claims are supported by evidence in the Results section and Appendix. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 17}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: Yes, we have a Limitations & Future Work paragraph at the end of the conclusion. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 18}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Justification: We provide derivations of the method\u2019s theoretical foundations (detailed up to an acceptable degree of expected math knowledge) in the Method section. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 18}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: We disclose hyperparameters in the main text and Appendix. We also offer the code for reproducibility in case any information is missing. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 19}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Justification: We have released an anonymized version of the repository. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https:// nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: See Question 4. Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 20}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We provide error bars in all experiments as we believe this to be of utmost importance to reproducible research. For the Appendix, we have reduced the number of seeds and/or experiment size to save computational resources for the environment\u2019s sake. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 20}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: See Appendix ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 21}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: The code of ethics was followed. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 21}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: Given the more foundational work of this paper, there is not a direct negative influence that the authors can think of that might arise from this work specifically. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 21}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: To the best of our knowledge, our work does not have high risk for misuse. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 22}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: Licenses for all used datasets were clearly stated. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 22}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: In the Appendix, the data generating mechanism is clearly stated for the introduced synthetic dataset. Additionally, the new method is described in detail. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used. ", "page_idx": 22}, {"type": "text", "text": "\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 23}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 23}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 23}]