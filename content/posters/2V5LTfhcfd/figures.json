[{"figure_path": "2V5LTfhcfd/figures/figures_1_1.jpg", "caption": "Figure 1: Illustration of the task of evaluating the generalization error of a model h. The mechanisms for C and W vary across domains.", "description": "This figure illustrates the challenge of evaluating a model's performance in a new, unseen domain (target domain).  Two source domains (M\u00b9, M\u00b2) with data distributions P\u00b9 and P\u00b2 are used to train a model h. The causal mechanisms governing the variables C, W, Y, and Z (where Y is the outcome variable and C and W are covariates) differ across domains. The goal is to infer the target risk Rp*(h) (the expected performance on the target domain M*) based only on data from the source domains and assumptions encoded in a causal diagram (not explicitly shown in this figure but described in the paper).  The core issue is the non-transportability of the target domain risk, meaning we cannot directly estimate Rp*(h) from source domain data alone.", "section": "1 Introduction"}, {"figure_path": "2V5LTfhcfd/figures/figures_4_1.jpg", "caption": "Figure 2: Selection diagram & Canonical param.", "description": "This figure illustrates the selection diagram and canonical parameterization for Example 3 (The bow model). The selection diagram (a) shows the relationships between variables X and Y in multiple SCMs, including source domains and target domains, and how discrepancies (indicated by the selection nodes S1 and S2) affect transportability. The canonical parameterization (b) represents a simplified SCM model with correlated discrete latent variables Rx and Ry used for consistently solving the partial transportability task by reducing the dimensionality and complexity of the problem.", "section": "Canonical Models for Partial Transportability"}, {"figure_path": "2V5LTfhcfd/figures/figures_5_1.jpg", "caption": "Figure 3: Selection diagram for Example 4.", "description": "This selection diagram shows the causal relationships between variables in Example 4 of the paper.  The diagram illustrates how the variables C, W, Y, Z are causally related, as well as how the selection nodes S1 and S2 impact the observation of these variables across different domains. The dashed lines represent potential discrepancies in the causal mechanisms between source and target domains, indicated by the selection node. This highlights the complexities in domain generalization, where a model trained on source domains may not generalize well to the target domain due to differences in these underlying causal mechanisms.", "section": "4 Neural Causal Models for Partial Transportability"}, {"figure_path": "2V5LTfhcfd/figures/figures_8_1.jpg", "caption": "Figure 4: (a-c): worst-case risk evaluation results as a function of Neural-TR (Alg. 1) training iterations. (d,e): worst-case risk evaluation of CRO.", "description": "This figure shows the results of worst-case risk evaluation experiments using two algorithms: Neural-TR and CRO.  Subfigures (a), (b), and (c) display the worst-case risk (y-axis) over training iterations (x-axis) for three different scenarios (Example 2, Example 3, and CMNIST dataset). The plots show how the worst-case risk converges as the algorithms train. Subfigures (d) and (e) show the worst-case risk obtained after training with CRO for Example 2 and Example 3, respectively. These plots illustrate the effectiveness of the algorithms in finding tight upper bounds on the generalization error.", "section": "5 Experiments"}, {"figure_path": "2V5LTfhcfd/figures/figures_8_2.jpg", "caption": "Figure 5: GOMNIST", "description": "This figure shows the causal graph used in the Colored MNIST experiment.  The variables represent: W (grayscale MNIST image), C (color, red or green), Y (binary label, whether the digit is 5 or greater), and Z (colored image). The dashed arrows from W indicate that the mechanism for W might differ across domains.  The nodes S1 and S2 represent selection nodes indicating that there might be discrepancies in the mechanisms of C across domains.", "section": "5.2 Colored MNIST"}, {"figure_path": "2V5LTfhcfd/figures/figures_9_1.jpg", "caption": "Figure 4: (a-c): worst-case risk evaluation results as a function of Neural-TR (Alg. 1) training iterations. (d,e): worst-case risk evaluation of CRO.", "description": "The figure shows the results of worst-case risk evaluation using Neural-TR and CRO algorithms for Examples 2 and 3, and CMNIST dataset. The plots (a-c) illustrate how the worst-case risk converges over training iterations for different classifiers (h1, h2, h3 in Example 2; h1, h2, h3, h4 in Example 3; hERM, hIRM, hDRO in CMNIST). The plots (d,e) display the worst-case risk obtained by CRO, comparing it with the performance of other classifiers.  The results demonstrate the effectiveness of Neural-TR in assessing worst-case risk and the ability of CRO to find optimal classifiers under worst-case scenarios.", "section": "5 Experiments"}, {"figure_path": "2V5LTfhcfd/figures/figures_16_1.jpg", "caption": "Figure 4: (a-c): worst-case risk evaluation results as a function of Neural-TR (Alg. 1) training iterations. (d,e): worst-case risk evaluation of CRO.", "description": "The figure shows the results of worst-case risk evaluation for different classifiers using two different methods. (a-c) show the worst-case risk evaluation using Neural-TR (Algorithm 1), plotting the worst-case risk against the number of training iterations for three different examples. (d,e) show the worst-case risk evaluation using CRO (Algorithm 2) for two of the examples.  Each subfigure shows how the worst-case risk converges over training iterations (or epochs).", "section": "5 Experiments"}, {"figure_path": "2V5LTfhcfd/figures/figures_17_1.jpg", "caption": "Figure 9: NCM experimental results on Examples 6 and 7.", "description": "This figure presents the results of applying Neural Causal Models (NCMs) to two examples (Example 6 and 7) from the paper. Each example involves a causal inference task where the goal is to estimate the risk of different classifiers in a target domain.  The figure shows the worst-case risk evaluated by the Neural-TR algorithm across training epochs. The plots compare the performance of multiple classifiers which differ in the variables they use to make predictions. This helps to understand how well the classifiers generalize from source data to a new, unseen domain (M*). The left plot shows results for Example 6 (lung cancer) while the right shows results for Example 7 (Alzheimer's disease).", "section": "5.1 Simulations"}, {"figure_path": "2V5LTfhcfd/figures/figures_18_1.jpg", "caption": "Figure 10: Violin plots that describe MCMC samples of Rp*(h) for Example 6. The upper end-point is an estimate of max Rp*(h). n stands for the number of source domain samples used as a conditioning set in the posterior evaluation.", "description": "Violin plots showing the results of a Markov Chain Monte Carlo (MCMC) sampling method used to estimate the worst-case risk (Rp*) for different classifiers (h1, h2, h3) in Example 6 of the paper. The x-axis represents the number of samples (n) from source domains used as conditioning data in the posterior estimation, and the y-axis represents the worst-case risk (Rp*). Each violin plot shows the distribution of the estimated worst-case risks, and the upper end-point of each violin represents an estimate of the maximum worst-case risk.", "section": "B Additional Experiments and Details"}, {"figure_path": "2V5LTfhcfd/figures/figures_18_2.jpg", "caption": "Figure 4: (a-c): worst-case risk evaluation results as a function of Neural-TR (Alg. 1) training iterations. (d,e): worst-case risk evaluation of CRO.", "description": "The figure shows the worst-case risk evaluation results obtained by applying Neural-TR and CRO algorithms on different examples, including simulated examples and the colored MNIST dataset. Subfigures (a), (b), and (c) illustrate the convergence of worst-case risk evaluation using Neural-TR as a function of the number of training iterations for three different examples (Examples 2, 3, and CMNIST). Subfigures (d) and (e) show the worst-case risk evaluations obtained with the CRO algorithm for Examples 2 and 3, respectively. These results demonstrate the effectiveness of the proposed algorithms in bounding the worst-case generalization error in different scenarios.", "section": "5 Experiments"}, {"figure_path": "2V5LTfhcfd/figures/figures_19_1.jpg", "caption": "Figure 12: Selection diagrams for additional experiments", "description": "This figure presents two selection diagrams used in additional experiments presented in Appendix B.  The diagrams graphically represent the relationships between variables and assumptions about which variables' mechanisms are invariant across domains. (a) shows the diagram for Example 6, a study of smoking and lung cancer.  (b) shows the diagram for Example 7, which explores the prediction of Alzheimer's disease. These diagrams are crucial for applying the Partial Transportability framework, highlighting the causal relationships and discrepancies between the source and target domains.", "section": "B Additional Experiments and Details"}, {"figure_path": "2V5LTfhcfd/figures/figures_20_1.jpg", "caption": "Figure 3: Selection diagram for Example 4.", "description": "This selection diagram shows the causal relationships between variables in Example 4 of the paper.  The variables are denoted by circles, and the arrows indicate the direction of causality.  The dashed lines represent the domain discrepancies, meaning that the mechanisms governing the relationships between the variables are not the same in all domains. This is important for the task of partial transportability because the goal is to compute bounds on statistical queries in the target domain given only data from source domains and assumptions about the causal relationships.  By encoding these assumptions in the selection diagram, the researchers can more effectively leverage source data to make inferences about the target domain.", "section": "4 Neural Causal Models for Partial Transportability"}, {"figure_path": "2V5LTfhcfd/figures/figures_21_1.jpg", "caption": "Figure 12: Selection diagrams for additional experiments", "description": "The figure shows two selection diagrams, (a) and (b), which represent the graphical assumptions for two additional experiments described in Appendix B. Selection diagrams extend causal diagrams by adding selection nodes to represent domain discrepancies. In diagram (a), the discrepancy set for Example 6 is {S}, meaning the mechanism for variable S (smoking status) may differ across domains, while the mechanisms for other variables (tar in the lungs, lung cancer, etc.) are assumed invariant. Diagram (b) shows the selection diagram for Example 7, where the discrepancy set is {W} (blood pressure). This indicates that the mechanism for blood pressure may vary across domains, while other variables related to Alzheimer's disease (treatments for hypertension and clinical depression, a symptom of Alzheimer's) are assumed invariant.", "section": "B Additional Experiments and Details"}, {"figure_path": "2V5LTfhcfd/figures/figures_23_1.jpg", "caption": "Figure 15: Conceptual illustration of CRO. The rectangle represents the space of all distributions over X, Y, and the circle inside it represents the subset of that are plausible target distributions, as characterized by the source distributions and selection diagram. Iteration 1: At first we start with some classifier that may or may not perform well for all distributions in the plausible subset; the darker spots indicate distributions that yield higher risk for the classifier at hand. Neural-TR uses gradient ascend steps to find an SCM that entails a distribution which yields the highest risk for the classifier at hand, i.e., the darkest spot within the plausible subset (likely at the boundary of it), shown by the star blue in Fig. (a). We register this distribution by taking samples from it and adding them to the collection D*. Iteration 2: We update the classifier at hand to have group robustness to the collection of distributions D*; in this case, only risk minimizer, since there is only one distribution in the collection. Now the distributions that are close to the registered distribution would entail small risk, thus, the region around the first star is now brighter. Once again, using Neural-TR we find a distribution that yields high risk for the classifier at hand. Iteration 3: We update the classifier, this time to minimize the risk on both registered distributions indicated with yellow starts using group DRO. Now the risk is smaller in most parts of the plausible set, though Neural-TR still finds another distribution at the boundary with high risk. Equilibrium: We update the classifier using group DRO over the three registered distributions. This time, the registered distributions correctly represent the plausible set, meaning that the maximum risk inside the plausible set is not significantly larger than what is achieved at the registered points through group DRO.", "description": "This figure illustrates the iterative process of Causal Robust Optimization (CRO).  It shows how Neural-TR finds increasingly difficult target distributions, while DRO adapts the classifier to minimize the maximum risk across seen distributions.  The process continues until an equilibrium is reached, where the worst-case risk is minimized.", "section": "C.2 Illustration of CRO (Algorithm 2)"}]