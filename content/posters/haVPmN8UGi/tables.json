[{"figure_path": "haVPmN8UGi/tables/tables_6_1.jpg", "caption": "Table 1: Performance of GraphVis compared with the original VLM model across benchmarks and VQA tasks. As current baselines on LLMs are not open-sourced yet, we include the results directly reported from their papers (Zhang et al., 2022; Feng et al., 2023; Tian et al., 2024). We use FT to indicate if a method involves fine-tuning. The bold numbers indicate the best results among all methods and underscored numbers represent the second best.", "description": "This table presents a comparison of the performance of GraphVis against various baseline models on commonsense reasoning QA tasks (CSQA and OBQA).  It shows the accuracy achieved by different methods, including those that use fine-tuning (FT) and those that do not.  The baseline models represent a range of approaches, from smaller language models enhanced with knowledge graphs to larger language models with and without KG integration. GraphVis consistently outperforms other methods, demonstrating a significant improvement in accuracy, especially when compared to methods which use prompting rather than fine tuning. Note that results for some baselines are from published papers, as their code was not available.", "section": "5.2 Main Results"}, {"figure_path": "haVPmN8UGi/tables/tables_7_1.jpg", "caption": "Table 2: VQA performance of GraphVis based on llava-v1.6-mistral-7b.", "description": "This table presents the performance comparison of the base Large Vision Language Model (LLaVA-v1.6-mistral-7b) and the same model after applying the proposed GraphVis method on various Visual Question Answering (VQA) tasks.  The tasks include ScienceQA (focused on scientific diagrams), MMBench (a multi-modal benchmark encompassing various task types), and POPE (assessing object hallucination). For each task, it reports the Image Accuracy (Img-Acc) and Overall accuracy/F1-score as relevant.  The numbers in parentheses indicate the performance gain achieved by incorporating GraphVis.", "section": "5 Experiments"}, {"figure_path": "haVPmN8UGi/tables/tables_8_1.jpg", "caption": "Table 1: Performance of GraphVis compared with the original VLM model across benchmarks and VQA tasks. As current baselines on LLMs are not open-sourced yet, we include the results directly reported from their papers (Zhang et al., 2022; Feng et al., 2023; Tian et al., 2024). We use FT to indicate if a method involves fine-tuning. The bold numbers indicate the best results among all methods and underscored numbers represent the second best.", "description": "This table compares the performance of GraphVis against various baselines on question answering tasks (CSQA and OBQA).  It shows the accuracy improvements achieved by GraphVis over a base Large Vision Language Model (LLVM), and also compares it to existing Knowledge Graph (KG)-enhanced LLMs (KSL, KAPING, GNP) and other LLMs (GPT-3.5, FLAN-T5). The table highlights GraphVis's significant accuracy gains, particularly when compared to methods involving fine-tuning, which are more costly and difficult to adapt to larger LLMs.", "section": "5.2 Main Results"}, {"figure_path": "haVPmN8UGi/tables/tables_9_1.jpg", "caption": "Table 4: Performance of LLaVA-v1.6 before and after fine-tuning on each graph comprehension task. N. denotes node and E. denotes Edge. For node description and triple listing, we consider the average accuracy of each test example. We use exact matching to determine the accuracy, which may be a stricter evaluation.", "description": "This table presents the performance of the LLaVA-v1.6 model on various graph comprehension tasks before and after fine-tuning.  The tasks assess the model's ability to extract information from visual graph representations.  The metrics include accuracy in describing nodes, determining node degrees, identifying the node with the highest degree, counting nodes and edges, and listing graph triples.  The table highlights the improvement in performance after fine-tuning, indicating that the model's ability to understand and interpret visual graphs significantly enhanced after the fine-tuning process.", "section": "5.2.1 Leveraging KG and Textual Data to Enhance LVLM"}, {"figure_path": "haVPmN8UGi/tables/tables_15_1.jpg", "caption": "Table 1: Performance of GraphVis compared with the original VLM model across benchmarks and VQA tasks. As current baselines on LLMs are not open-sourced yet, we include the results directly reported from their papers (Zhang et al., 2022; Feng et al., 2023; Tian et al., 2024). We use FT to indicate if a method involves fine-tuning. The bold numbers indicate the best results among all methods and underscored numbers represent the second best.", "description": "This table presents a comparison of the performance of GraphVis against various baselines on commonsense reasoning question answering benchmarks (CSQA and OBQA).  It also includes results from zero-shot visual question answering (VQA) tasks.  The table highlights GraphVis's improvements over a base model and existing knowledge graph (KG)-enhanced language models. Note that fine-tuning (FT) is indicated, and the best and second-best results are highlighted.", "section": "5.2 Main Results"}, {"figure_path": "haVPmN8UGi/tables/tables_15_2.jpg", "caption": "Table 6: Performance of LLaVA-v1.6 on ScienceQA compared with GraphVis and GraphVis (joint fine-tuning).", "description": "This table presents the performance comparison of three different models on the ScienceQA benchmark: the base LLaVA-v1.6 model, GraphVis with joint fine-tuning, and GraphVis with the proposed curriculum fine-tuning.  It shows the accuracy (in percentage) achieved by each model. The results demonstrate that the GraphVis model with curriculum learning outperforms both the base model and GraphVis with joint fine-tuning on ScienceQA.", "section": "5.2.1 Leveraging KG and Textual Data to Enhance LVLM"}]