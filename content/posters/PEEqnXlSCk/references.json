{"references": [{"fullname_first_author": "Dan Alistarh", "paper_title": "QSGD: Communication-Efficient SGD via Gradient Quantization and Encoding", "publication_date": "2017-12-01", "reason": "This paper introduces QSGD, a foundational work in communication-efficient stochastic gradient descent, which is highly relevant to the current paper's focus on reducing communication overhead in large language model training."}, {"fullname_first_author": "Ilya Loshchilov", "paper_title": "Decoupled Weight Decay Regularization", "publication_date": "2017-11-17", "reason": "This paper introduces a decoupled weight decay regularization technique, which is widely adopted in training large language models and is relevant to the current paper's experimental setup."}, {"fullname_first_author": "Mohammad Shoeybi", "paper_title": "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism", "publication_date": "2019-09-08", "reason": "This paper introduces Megatron-LM, the training framework used in the current paper's experiments, making it crucial to understanding the experimental setup and results."}, {"fullname_first_author": "Sebastian U. Stich", "paper_title": "Local SGD Converges Fast and Communicates Little", "publication_date": "2019-04-01", "reason": "This paper presents Local SGD, an algorithm focusing on reducing communication overhead in distributed training, providing valuable context to the current paper's exploration of communication-efficient training strategies."}, {"fullname_first_author": "Ilia Markov", "paper_title": "Quantized Distributed Training of Large Models with Convergence Guarantees", "publication_date": "2023-07-01", "reason": "This paper explores quantized distributed training, which is directly related to the core technique of the current paper and offers valuable comparison with prior work."}]}