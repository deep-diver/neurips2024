[{"figure_path": "PEEqnXlSCk/tables/tables_4_1.jpg", "caption": "Table 1: Final validation loss of pre-training with different quantization strategies.", "description": "This table presents the final validation loss achieved during the pre-training phase of GPT models with varying sizes (125M, 350M, and 1.3B parameters) using different quantization strategies.  The baseline represents the full-precision training without quantization.  The strategies compared include: qW (weight quantization), qWD (weight difference quantization), TLq (two-level gradient smooth quantization), and TLq-HS (two-level gradient smooth quantization with Hadamard smoother).  The table showcases the impact of different quantization techniques on the final model accuracy, demonstrating that SDP4Bit approaches the accuracy of full-precision training.", "section": "5.2 Accuracy Evaluation"}, {"figure_path": "PEEqnXlSCk/tables/tables_5_1.jpg", "caption": "Table 1: Final validation loss of pre-training with different quantization strategies.", "description": "This table presents the final validation loss achieved during the pre-training phase of various GPT models, comparing different quantization strategies.  The baseline represents full precision training, while 'qW' uses 4-bit quantization on weights, 'qWD' uses 4-bit quantization on weight differences, 'TLq' employs two-level gradient quantization, and 'TLq-HS' adds a Hadamard smoother to TLq. The table demonstrates the impact of each strategy on the model's final accuracy, highlighting the effectiveness of SDP4Bit in achieving near-baseline accuracy with significant compression.", "section": "5.2 Accuracy Evaluation"}, {"figure_path": "PEEqnXlSCk/tables/tables_7_1.jpg", "caption": "Table 1: Final validation loss of pre-training with different quantization strategies.", "description": "This table presents the final validation loss achieved during the pre-training phase of GPT models with varying sizes (125M, 350M, and 1.3B parameters).  It compares the performance of different quantization strategies, including the baseline (no quantization), weight quantization (qW and qWD), and gradient quantization (TLq and TLq-HS), against the proposed SDP4Bit method.  Lower validation loss indicates better model performance.", "section": "5.2 Accuracy Evaluation"}, {"figure_path": "PEEqnXlSCk/tables/tables_8_1.jpg", "caption": "Table 2: E2E throughput on different model sizes with std.", "description": "This table shows the end-to-end (E2E) throughput, measured in TFLOPS (trillion floating-point operations per second), for different GPT model sizes (1.3B, 2.7B, 6.7B, 13B, and 18B parameters) using two different hardware setups.  The first setup uses 16 nodes, each with 4xA100 GPUs and interconnected with a Slingshot 10 network. The second setup uses 16 nodes with 8xH800 GPUs and InfiniBand interconnect. For each model size and hardware setup, the table presents the baseline throughput (without SDP4Bit), the throughput achieved using the SDP4Bit technique, and the speedup factor (SDP4Bit throughput divided by baseline throughput). The speedup factor indicates the performance improvement obtained by using the SDP4Bit method.", "section": "5.3 Throughput Evaluation"}, {"figure_path": "PEEqnXlSCk/tables/tables_8_2.jpg", "caption": "Table 3: Final validation loss of GPT-125M with different group sizes.", "description": "This table shows the final validation loss for the GPT-125M model using different group sizes for three different quantization methods: TLq-HS, qWD, and qW.  It demonstrates the effect of varying the group size on the accuracy of each quantization technique, showing how the choice of group size impacts the final validation loss compared to the baseline.", "section": "5.2 Accuracy Evaluation"}, {"figure_path": "PEEqnXlSCk/tables/tables_8_3.jpg", "caption": "Table 4: Performance of Different Quantization Strategies on GPT-1.3B over 32 A100 with standard deviation.", "description": "This table compares the performance of different quantization strategies on a GPT-1.3B model trained on 32 A100 GPUs.  It shows the gradient communication time (in milliseconds) and the resulting throughput in TFLOPS for four strategies: Baseline (no quantization), TLq-HS (Two-Level quantization with Hadamard Smoother), ULq (Uniform Level quantization), and SDP4Bit.  The table also includes a comparison for SDP4Bit without kernel fusion of Hadamard transform for reference. This comparison demonstrates the efficiency gains achieved by SDP4Bit in reducing communication overhead while improving throughput.", "section": "5.3 Throughput Evaluation"}, {"figure_path": "PEEqnXlSCk/tables/tables_17_1.jpg", "caption": "Table 5: (De)quantization Throughput with/without Hadamard, including std. dev.", "description": "This table presents the throughput results of the quantization and dequantization processes with and without the Hadamard transform.  The measurements are given in terms of the throughput (in MB/s) for different input/output sizes (8 MB to 2048 MB).  The standard deviations are included to show the variability of the measurements. The data suggests that the Hadamard transform has minimal impact on the throughput of (de)quantization process.", "section": "5.4 Ablation Study"}, {"figure_path": "PEEqnXlSCk/tables/tables_17_2.jpg", "caption": "Table 1: Final validation loss of pre-training with different quantization strategies.", "description": "This table presents the final validation loss achieved during the pre-training phase of GPT models with varying sizes (125M, 350M, and 1.3B parameters).  It compares the performance of different quantization strategies: the baseline (no quantization),  quantization on weights (qW), quantization on weight differences (qWD), two-level gradient smooth quantization (TLq), and two-level gradient smooth quantization with Hadamard Smoother (TLq-HS). The results demonstrate the effectiveness of SDP4bit's approach in maintaining accuracy while significantly reducing communication overhead.", "section": "5.2 Accuracy Evaluation"}, {"figure_path": "PEEqnXlSCk/tables/tables_18_1.jpg", "caption": "Table 1: Final validation loss of pre-training with different quantization strategies.", "description": "This table presents the final validation loss achieved during the pre-training of GPT models with various sizes (125M, 350M, and 1.3B parameters) using different quantization strategies.  The strategies compared include the baseline (no quantization), quantization on weights (qW), quantization on weight differences (qWD), two-level gradient smooth quantization (TLq), and two-level gradient smooth quantization with Hadamard smoother (TLq-HS). This allows for a comparison of the impact of different quantization approaches on the final model accuracy, measured by validation loss.", "section": "5.2 Accuracy Evaluation"}, {"figure_path": "PEEqnXlSCk/tables/tables_18_2.jpg", "caption": "Table 8: Parallel Configuration for Throughput Test", "description": "This table shows the parallel configuration used for the throughput tests in the paper. It specifies the tensor parallel (TP), pipeline parallel (PP) size, and accumulation step for different GPT model sizes (1.3B, 2.7B, 6.7B, 13B, and 18B).  These configurations were chosen to maximize throughput on the hardware platforms used in the experiments. The variation in TP and PP values reflects different strategies for maximizing throughput with varying model sizes and hardware limitations.", "section": "5.1 Experimental Setup"}, {"figure_path": "PEEqnXlSCk/tables/tables_18_3.jpg", "caption": "Table 1: Final validation loss of pre-training with different quantization strategies.", "description": "This table shows the final validation loss achieved during the pre-training of GPT models with different sizes (125M, 350M, and 1.3B parameters) using various quantization strategies. The strategies include a baseline (no quantization), weight quantization (qW), weight difference quantization (qWD), two-level gradient quantization (TLq), and two-level gradient quantization with Hadamard smoother (TLq-HS).  The table highlights the impact of different quantization methods on the final model accuracy, comparing them to the baseline (full precision training).", "section": "5.2 Accuracy Evaluation"}]