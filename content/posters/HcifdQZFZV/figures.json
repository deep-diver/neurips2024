[{"figure_path": "HcifdQZFZV/figures/figures_1_1.jpg", "caption": "Figure 1: Overview of Safe LoRA. We first obtain an alignment matrix V = Waligned-Wunaligned from a pair of unaligned and aligned LLMs, denoted as Wunaligned and Waligned, respectively. Note that Wunaligned/Waligned can be the base/chat checkpoints of pre-trained (open-weight) models. For example, Wunaligned can be the Llama-2-7b-base model, while Waligned can be the Llama-2-7b-chat model. Next, for each layer in the LLM undergoing LoRA updates \u25b3W = ABT, we use the projection operator C = VVT/||V||F to calculate the similarity score between the projected LoRA weights CABT and the original LoRA weights ABT. If the similarity score is below a certain threshold 7, we use the projected LoRA weights as the final updates to Waligned.", "description": "This figure illustrates the Safe LoRA method. It starts by obtaining an alignment matrix from a pair of unaligned and aligned LLMs.  For each layer in the LLM being fine-tuned, Safe LoRA calculates a similarity score between the original LoRA weight updates and a projected version of these updates onto the alignment subspace. If the similarity score is below a threshold, the projected weights are used as updates, thus preventing the LLM's safety guardrails from being weakened during fine-tuning.", "section": "1 Introduction"}, {"figure_path": "HcifdQZFZV/figures/figures_4_1.jpg", "caption": "Figure 2: Comparison of Safe LoRA results using alignment matrices derived from the base model versus those obtained by fine-tuning with a few harmful samples. Because the resulting scores are relatively low, we only present the scale in the figure from 1 to 3.", "description": "This figure compares the safety scores of a base language model and a model fine-tuned with harmful data, both evaluated using SafeLoRA.  The radar charts show the harmfulness scores for 11 different categories of potentially harmful content. The base model shows relatively low scores across all categories indicating good safety. In contrast, the fine-tuned model shows significantly higher scores in several categories, demonstrating a loss of safety after fine-tuning with harmful data. SafeLoRA's ability to maintain similar safety performance to the base model even after fine-tuning on malicious data is also highlighted in this figure.", "section": "3 Methodology"}, {"figure_path": "HcifdQZFZV/figures/figures_8_1.jpg", "caption": "Figure 3: Comparison of harmfulness score versus utility on the Llama-2-Chat model trained on the Dialog Summary dataset.", "description": "This figure shows the trade-off between the harmfulness score and the utility (F1-score) of the Llama-2-Chat model when fine-tuned on the Dialog Summary dataset using SafeLoRA with varying similarity score thresholds (represented by \u03c4).  Each point represents a different threshold, and the corresponding percentage of projected layers is shown in parentheses.  As the threshold (\u03c4) increases, the percentage of projected layers increases, leading to a decrease in the harmfulness score but also a decrease in utility (F1-score). The optimal point seems to be around 11% of projected layers, where there is a balance between improved safety and acceptable performance.", "section": "4.1 Performance Evaluation"}, {"figure_path": "HcifdQZFZV/figures/figures_8_2.jpg", "caption": "Figure 3: Comparison of harmfulness score versus utility on the Llama-2-Chat model trained on the Dialog Summary dataset.", "description": "This figure shows the trade-off between utility (Rouge F1 score) and safety (harmfulness score) on the Llama-2-Chat model when different numbers of layers are projected using SafeLoRA on the Dialog Summary dataset. The x-axis represents the percentage of layers projected, while the y-axis shows both the harmfulness score and Rouge F1 score.  A threshold (\u03c4) is shown as a horizontal red line; layers with similarity scores below this threshold are projected. The plot helps determine the optimal number of layers to project to achieve a balance between maintaining utility and improving safety.", "section": "4.1 Performance Evaluation"}, {"figure_path": "HcifdQZFZV/figures/figures_13_1.jpg", "caption": "Figure 1: Overview of Safe LoRA. We first obtain an alignment matrix V = Waligned-Wunaligned from a pair of unaligned and aligned LLMs, denoted as Wunaligned and Waligned, respectively. Note that Wunaligned/Waligned can be the base/chat checkpoints of pre-trained (open-weight) models. For example, Wunaligned can be the Llama-2-7b-base model, while Waligned can be the Llama-2-7b-chat model. Next, for each layer in the LLM undergoing LoRA updates \u25b3W = ABT, we use the projection operator C = VVT/||V||F to calculate the similarity score between the projected LoRA weights CABT and the original LoRA weights ABT. If the similarity score is below a certain threshold \u03c4, we use the projected LoRA weights as the final updates to Waligned.", "description": "This figure illustrates the Safe LoRA method.  It begins by calculating an alignment matrix from the weights of an unaligned and an aligned LLM.  Then, for each layer of an LLM being fine-tuned using LoRA, it calculates a similarity score between the original LoRA updates and the projection of those updates onto the alignment matrix subspace.  If the similarity score is below a threshold, the projected LoRA weights (more aligned with the original models) are used as updates; otherwise, the original LoRA updates are used.", "section": "3 Methodology"}]