{"references": [{"fullname_first_author": "Mengdi Wang", "paper_title": "Stochastic compositional gradient descent: algorithms for minimizing compositions of expected-value functions", "publication_date": "2017-00-00", "reason": "This paper is foundational in the field of stochastic compositional optimization, introducing a key algorithm and analysis that many subsequent works build upon."}, {"fullname_first_author": "Saeed Ghadimi", "paper_title": "A single timescale stochastic approximation method for nested stochastic optimization", "publication_date": "2020-00-00", "reason": "This work provides a significant improvement over previous methods by using a single timescale for updates, simplifying the algorithm and improving its convergence."}, {"fullname_first_author": "Andrzej Ruszczynski", "paper_title": "A stochastic subgradient method for nonsmooth nonconvex multilevel composition optimization", "publication_date": "2021-00-00", "reason": "This paper extends the analysis of SCO to the nonsmooth nonconvex case, directly addressing the challenge that the current paper tackles."}, {"fullname_first_author": "Jingzhao Zhang", "paper_title": "Complexity of finding stationary points of nonsmooth nonconvex functions", "publication_date": "2020-00-00", "reason": "This work introduces the crucial concept of (\u03b4, \u03b5)-Goldstein stationary points, providing a refined measure of stationarity suitable for non-smooth, non-convex optimization problems."}, {"fullname_first_author": "Yurii Nesterov", "paper_title": "Random gradient-free minimization of convex functions", "publication_date": "2017-00-00", "reason": "This paper is highly influential in the development of gradient-free optimization methods, providing theoretical foundations for zeroth-order techniques used in the current paper."}]}