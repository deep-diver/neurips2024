[{"type": "text", "text": "Reciprocal Learning ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Julian Rodemann Christoph Jansen Georg Schollmeyer Department of Statistics Computing & Communications Department of Statistics LMU Munich Lancaster University Leipzig LMU Munich j.rodemann@lmu.de c.jansen@lancaster.ac.uk g.schollmeyer@lmu.de ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We demonstrate that numerous machine learning algorithms are specific instances of one single paradigm: reciprocal learning. These instances range from active learning over multi-armed bandits to self-training. We show that all these algorithms not only learn parameters from data but also vice versa: They iteratively alter training data in a way that depends on the current model fit. We introduce reciprocal learning as a generalization of these algorithms using the language of decision theory. This allows us to study under what conditions they converge. The key is to guarantee that reciprocal learning contracts such that the Banach fixed-point theorem applies. In this way, we find that reciprocal learning converges at linear rates to an approximately optimal model under some assumptions on the loss function, if their predictions are probabilistic and the sample adaption is both non-greedy and either randomized or regularized. We interpret these findings and provide corollaries that relate them to active learning, self-training, and bandits. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The era of data abundance is drawing to a close. While GPT-3 [9] still had to make do with 300 billion tokens, Llama 3 [102] was trained on 15 trillion. With the stock of high-quality data growing at a much smaller rate [67], adequate training data might run out within this decade [58, 107]. Generally and beyond language models, machine learning is threatened by degrading data quality and quantity [60]. Apparently, learning ever more parameters from ever more data is not the exclusive route to success. Models also have to learn from which data to learn. This has sparked a lot of interest in sample efficiency [70, 92, 111, 7, 46, 27, 105], subsampling [47, 101, 71], coresets [62, 76, 86], data subset selection [55, 113, 13, 82], and data pruning [32, 118, 57, 5] in recent years. ", "page_idx": 0}, {"type": "text", "text": "Instead of proposing yet another method along these lines, we demonstrate that a broad spectrum of well-established machine learning algorithms already exhibits a reciprocal relationship between data and parameters. That is, parameters are not only learned from data, but data is also iteratively chosen based on currently optimal parameters with the aim of increasing sample efficiency. For instance, consider self-training algorithms in semi-supervised learning [106, 12, 103, 87], see section 2.1. They iteratively add pseudo-labeled variants of unlabeled data to the labeled training data. The pseudo-labels are predicted by the current model, and thus depend on the parameters learned by the model from the labeled data in the first place. Other examples comprise active learning [93], Bayesian optimization [63, 64, 98], superset learning [35, 34, 85, 36], and multi-armed bandits [3, 81], see Appendix A for details. ", "page_idx": 0}, {"type": "text", "text": "In this paper, we develop a unifying framework, called reciprocal learning, that allows for a principled analysis of all these methods. After an initial model fit to the training data, reciprocal learning algorithms alter the latter in a way that depends on the fit. This dependence can have various facets, ranging from predicting labels (self-training) over taking actions (bandits) to querying an oracle (active learning), all based on the current model fit. It can entail both adding and removing data. Figure 7 illustrates this oscillating procedure and compares it to a well-known illustration of classical machine learning. A pressing question naturally arises: Given the additional degrees of freedom these algorithms enjoy through data selection, can they at all reach a stable point; that is, can they converge? Convergence is well-understood for classical empirical risk minimization (ERM), where it refers to the sequence of model parameters. In reciprocal learning, however, we need to extend the notion of convergence to the sequence of both parameters and data. It is self-evident that convergence is a desirable property of any learning algorithm. Only if an algorithm converges to a unique solution, we can identify a unique model and use it for deployment or assessment on test data. Practically speaking, convergence of a training procedure means that subsequent iterations will no longer change the model, giving rise to stopping criteria. Generally speaking, convergence is a prerequisite for any further theoretical or empirical assessment of such methods. In the literature on reciprocal learning algorithms like active learning, self-training, or multi-armed bandits, there is no consensus on when to stop them. And while a myriad of stopping criteria exist [109, 121, 48, 110, 28, 22, 11, 79, 96], only some come with generalization error bounds [41, 40, 88], but none \u2013 to the best of our knowledge \u2013 comes with rigorous guarantees of convergence. We address this research gap by proving convergence of all these methods under a set of sufficient conditions. ", "page_idx": 0}, {"type": "image", "img_path": "mhhlZeAr67/tmp/22013eb11d947a92a5da8ed94e65554227e9c55117fbe757d49e5dc001ccc180.jpg", "img_caption": ["(a) Classical machine learning "], "img_footnote": [], "page_idx": 1}, {"type": "image", "img_path": "mhhlZeAr67/tmp/e0cacdafdfa5d388f24a430469c82821c8f33a21c229d7eeb85f2d62a1801b84.jpg", "img_caption": ["Figure 1: (A) Classical machine learning ftis a model from the model space (restricted by red curve) to a realized sample from the sample space (blue-grey); figure replicated from \"The Elements of Statistical Learning\" [31, Figure 7.2]. (B) In reciprocal learning, the realized sample is no longer static, but changes in response to the model fit. Grey ellipse indicates restriction of sample space in $t=2$ through realization in $t=1$ . Sample in $t$ thus depends on model in $t-1$ and sample in $t-1$ . ", "(b) Reciprocal learning "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Our strategy will be to take a decision-theoretic perspective on both the parameter and the data selection problem. While the former is well studied, in particular its ubiquitous solution strategy ERM, little attention is commonly paid to a formal treatment of the other side of the coin: data selection. We particularly study the hidden interaction between parameter and data selection. We identify a sample adaption function $f$ which maps from the sample and the empirical risk minimizer in iteration $t$ to the sample in $t+1$ . Bounding the change of the sample in $t+1$ by the change in the sample and the change in the model in $t$ (i.e., $f$ being Lipschitz-continuous with a sufficiently small constant) will turn out to guarantee convergence of reciprocal learning algorithms. In response to this key finding, we study which algorithms fulfill this restriction on $f$ . We prove that the sample adaption is sufficiently Lipschitz for reciprocal learning to converge, if (1) it is non-greedy, i.e., it adds and removes data, (2) predictions are probabilistic and (3) the selection of data is either randomized or regularized. Conclusively, we transfer these results to common practices in active learning, self-training, and bandits, showing which algorithms converge and which do not. ", "page_idx": 1}, {"type": "text", "text": "2 Reciprocal learning ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Machine learning deals with two pivotal objects: data and parameters. Typically, parameters are learned from data through ERM. In various branches of machine learning, however, the relationship between data and parameters is in fact reciprocal, as argued above. In what follows, we show that this reciprocity corresponds to two interdependent decision problems and explicitly study how learned parameters affect the subsequent training data. We emphasize that our analysis focuses on reciprocity between parameters and training data only. The population and test data thereof are assumed to be fixed, i.e., our inference goal is static. Specifically, we call a machine learning algorithm reciprocal if it performs iterative ERM on training data that depends on the previous ERM, see definition 1. This dependence can be induced by any kind of data collection, removal, or generation that is affected by the model fti. In particular, it can be stochastic (think of Thompson-sampling in multi-armed bandits) as well as deterministic in nature (think of maximizing a confidence score in self-training). ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Definition 1 (Reciprocal Learning, informal). An algorithm that iteratively outputs $\\theta_{t}\\quad=$ arg $\\begin{array}{r}{\\operatorname*{min}_{\\theta}\\mathbb{E}_{(Y,X)\\sim\\mathbb{P}_{t}}\\,\\ell(Y,X,\\theta)}\\end{array}$ shall be called reciprocal learning algorithm if ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbb{P}_{t}=f(\\theta_{t-1},\\mathbb{P}_{t-1},n_{t-1}),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\mathbb{P}_{t}\\,\\in\\,\\mathcal{P}$ are empirical distributions \u2013 from a space of probability distributions $\\mathcal{P}-o f Y,X$ of size $n_{t}$ in iteration $t\\,\\in\\,\\{1,\\ldots,T\\}$ . Let $\\ell(Y,X,\\theta)\\,=\\,\\ell(Y,p(X,\\theta))$ denote a loss function with $p(X,\\theta)$ a prediction function that maps to the image of \ud835\udc4c. Further denote by $Y,X$ random variables describing the training data, and $\\theta_{t}\\in\\Theta$ a parameter vector of the model in $t$ . ", "page_idx": 2}, {"type": "text", "text": "In principle, the above definition needs no restriction on the nestedness between data in $t$ and $t-1$ . In practice, however, most algorithms iteratively either only add training data or both add and remove instances, see extensive list of examples in appendix A. That is, data in $t$ is either a superset of data in $t-1$ or a distinct set. We will address these two cases in the remainder of the paper, referring to the former as greedy (only adding data) and to the latter as non-greedy (adding and removing data). For classification problems, i.e., discrete image of $Y$ , we typically have $p(X,\\theta)=\\sigma(g(X,\\theta))$ with $\\sigma:\\mathbb{R}\\rightarrow[0,1]$ a sigmoid function and $g:\\mathcal{X}\\times\\Theta\\rightarrow\\mathbb{R}$ . For regression problems, we simply have $p:\\mathcal{X}\\times\\Theta\\to\\mathbb{R}$ . The notation $\\mathbb{P}_{t}=f(\\theta_{t-1},\\mathbb{P}_{t-1},n_{t-1})$ shall be understood as a mere indication of the distribution\u2019s dependence on ERM in the previous iteration. We will be more specific soon. ", "page_idx": 2}, {"type": "text", "text": "2.1 An illustrating running example: self-training ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In appendix A, we demonstrate at length that several well-established machine learning procedures turn out to be special cases of reciprocal learning as specified in definition 1 and more formally in definitions 6 and 7 below. Here, we seek to illustrate the principles of reciprocal learning by the simple running example of self-training in a semi-supervised learning (SSL) setup. The aim of SSL is to learn a predictive classification function $\\hat{\\mathrm{y}}(x,\\theta)$ parameterized by $\\theta$ utilizing both labeled and unlabeled data. Self-training is a popular algorithm class within SSL. Algorithms of that class start by ftiting a model on labeled data by ERM and then exploit this model to predict labels for the unlabeled data. In a second step, some instances of the unlabeled data are selected (according to a \u201cconfidence score\u201d, a measure of predictive uncertainty, see [2, 49, 80, 83, 53, 84, 18] for examples) to be added to the training data together with the predicted labels. In other words, self-training algorithms label unlabeled data themselves and ultimately learn from these \u201cpseudo-labels\u201d by iteratively adding pseudo-labeled variants of unlabeled data to the labeled training data. The pseudo-labels are predicted by the current model, and thus depend on the parameters learned by the model from the labeled data in the first place. This latter dependence constitutes the sample adaption function in definition 1. The sample of labeled and pseudo-labeled data in $t$ depends on the sample and the model (through its predicted pseudo-labels) in $t-1$ . For a more comprehensive and formal introduction of self-training, we refer the curious reader to appendix A.1. ", "page_idx": 2}, {"type": "text", "text": "2.2 A decision-theoretic perspective ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "On a high level, reciprocal learning can be viewed as sequential decision-making. First, a parameter $\\theta_{t}$ is fitted through ERM, which corresponds to solving a decision problem characterized by the triple $(\\Theta,\\mathbb{A},{\\mathcal{L}})$ with $\\Theta$ the unknown set of states of nature, the action space $\\mathbb{A}=\\Theta$ of potential parameter fits (estimates), and a loss function $\\mathcal{L}:\\mathbb{A}\\times\\Theta\\,\\rightarrow\\,\\mathbb{R}$ , analogous to classical statistical decision theory [6]. Secondly, features $x_{t}\\in\\mathcal X$ are chosen and data points $(x_{t},y_{t})$ are added to or removed from the training data inducing a new empirical distribution $\\mathbb{P}_{t+1}$ , where $y_{t}$ is predicted (self-training), queried (active learning) or observed (bandits). These features $x_{t}$ are found by solving another decision problem $(\\Theta,\\mathbb{A},\\mathcal{L}_{\\theta_{t}})$ , where \u2013 crucially \u2013 the loss function $\\mathcal{L}_{\\theta_{t}}$ depends on the previous decision problem\u2019s solution $\\theta_{t}$ . This time, the action space corresponds to the feature space $\\mathbb{A}=\\mathcal{X}$ . Illustration 1. Think of reciprocal learning as a sequential decision-making problem: ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "$t=1$ : $\\theta_{1}$ solves decision problem $(\\Theta,\\Theta,{\\mathcal{L}})$ $t=2$ : $\\theta_{2}$ solves decision problem $(\\Theta,\\Theta,\\mathcal{L}_{a_{1}(\\theta_{1})})$ $a_{1}$ solves decision problem $(\\Theta,\\mathbb{A},\\mathcal{L}_{\\theta_{1}})$ $a_{2}$ solves decision problem $(\\Theta,\\mathbb{A},\\mathcal{L}_{\\theta_{2}})$ ", "page_idx": 2}, {"type": "text", "text": "Loosely speaking, the data is judged in light of the parameters here. Excitingly, such an approach is symmetrical to any type of classical machine learning, where parameters are judged in light of the data. This twist in perspective will later pave the way for another type of regularization \u2013 that of data, not of parameters. As the decision problem $(\\Theta,\\Theta,{\\mathcal{L}})$ is well-known through its solution strategy ERM, see definition 1, we want to be more specific about $(\\Theta,\\mathbb{A},\\mathcal{L}_{\\theta_{t}})$ with $\\mathbb{A}=\\mathcal{X}$ . In particular, we need a solution strategy for all of the loss functions in the family $\\dot{\\mathcal{L}}_{\\theta}:\\mathcal{X}\\times\\Theta\\rightarrow\\mathbb{R}$ ; $(x,\\theta)\\mapsto\\mathcal{L}_{\\theta_{t}}(\\theta,x)$ in iteration $t$ . Definition 2 does the job. The family ${\\mathcal{L}}_{\\theta}$ describes all potential loss functions in the data selection problem arising from respective solutions of the parameter selection problem.1 Redefining this family of functions as a single one $\\tilde{\\mathcal{L}}:\\mathcal{X}\\times\\Theta\\times\\Theta\\stackrel{}{\\rightarrow}\\mathbb{R}$ makes it clear that we can retrieve the decision criterion $c:\\mathcal{X}\\times\\Theta\\rightarrow\\mathbb{R}$ from it, which is a generalization of classical decision criteria $c:\\mathcal{X}\\to\\mathbb{R}$ retrieved from classical losses $\\mathcal{L}:\\mathcal{X}\\times\\Theta\\rightarrow\\mathbb{R}$ , see [6] for instance. ", "page_idx": 3}, {"type": "text", "text": "Definition 2 (Data Selection). Let $c:\\mathcal{X}\\!\\times\\!\\Theta\\rightarrow\\mathbb{R}$ be a criterion for the decision problem $(\\Theta,\\mathbb{A},\\mathcal{L}_{\\theta_{t}})$ with bounded $\\mathbb{A}=\\mathcal{X}$ of selecting features to be added to the sample in iteration \ud835\udc61. Define $\\tilde{c}:\\mathcal{X}\\!\\times\\!\\Theta\\xrightarrow{}$ [0, 1]; (\ud835\udc65, \ud835\udf03\ud835\udc61) \u21a6\u2192 \u222b\ud835\udc65\u2032 exp(\ud835\udc50(\ud835\udc65\u2032,\ud835\udf03\ud835\udc61)\ud835\udc61)\ud835\udc51\ud835\udf07(\ud835\udc65) as standardized version thereof with $\\mu$ the Lebesgue measure on $\\mathcal{X}$ . For a model $\\theta_{t}$ in iteration $t$ , it assigns to each feature vector \ud835\udc65a value between 0 and 1 that can be used as drawing probabilities. Drawing $x\\in\\mathcal{X}$ according to $\\tilde{c}(\\boldsymbol{x},\\theta_{t})$ shall be called stochastic data selection $x_{s}(\\theta_{t})$ .2 The function $x_{d}:\\Theta\\to\\mathcal{X};\\theta_{t}\\ \\vdash$ \u2192arg $\\operatorname*{max}_{x\\in\\mathcal{X}}\\tilde{c}(x,\\theta_{t})$ shall be called deterministic data selection function. ", "page_idx": 3}, {"type": "text", "text": "The data selection function can be understood as the workhorse of reciprocal learning: It describes the non-trivial part of the sample adaption function $f$ , see definition 1. For any model $\\theta_{t}$ in $t$ , a data selection function chooses a feature vector to be added to the training data in $t+1$ , based on a criterion $c$ . This happens either stochastically through $x_{s}$ by drawing from $\\mathcal{X}$ according to $\\tilde{c}$ or deterministically through $x_{d}$ . Examples for $c$ comprise confidence measures in selftraining, acquisition functions in active learning, or policies in multi-armed bandits. For an example of stochastic data ", "page_idx": 3}, {"type": "image", "img_path": "mhhlZeAr67/tmp/afe309c5a338f96ececfa687abc1f31335d23597171fa4c605ab69353e7e926d.jpg", "img_caption": ["Figure 2: Data regularization is symmetrical to classical regularization, see illustration in \"The Elements of Statistical Learning\" [31, Figure 7.2]. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "selection, consider $c$ to be the classical Bayes criterion [6] in $(\\Theta,\\mathbb{A},\\mathcal{L}_{\\theta_{t}})$ . In this case, drawing from $\\mathcal{X}$ as prescribed by $x_{s}$ corresponds to well-known Thompson sampling [12, 90]. As already hinted at, we will need some regularization (definition 3) of the data selection. Intuitively, the regularization term smoothes out the criterion $c(\\cdot,\\theta)$ . In other words, the higher the constant $\\frac{1}{L_{s}}$ , the less the selection of data is affected by small changes of $\\theta$ for given $\\mathcal{R}(\\cdot)$ . This is completely symmetrical to classical statistical regularization in ERM, where the regularization terms smoothes out the effect of the data on parameter selection, see also figure 2. ", "page_idx": 3}, {"type": "text", "text": "Definition 3 (Data Regularization). Consider $c:\\mathcal{X}\\times\\Theta\\rightarrow\\mathbb{R}$ a criterion for the decision problem $(\\Theta,\\mathbb{A},\\mathcal{L}_{\\theta_{t}})$ with $\\tilde{c}$ as in definition 2. Define the following regularized (deterministic) data selection function: ", "page_idx": 3}, {"type": "equation", "text": "$$\nx_{d,\\mathcal{R}}:\\Theta\\rightarrow\\mathcal{X};\\;\\theta\\mapsto\\underset{x\\in\\mathcal{X}}{\\mathrm{argmax}}\\left\\lbrace c(\\pmb{x},\\theta)+\\frac{1}{L_{s}}\\mathcal{R}(\\pmb{x})\\right\\rbrace,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathcal{R}(\\cdot)$ is a $\\kappa$ -strongly convex regularizer. In complete analogy to definition 2, we can define a stochastic regularized data selection function as $x_{s,\\mathcal{R}}(\\theta)$ by drawing $x\\in\\mathcal{X}$ according to a normalized version of $\\begin{array}{r}{c(\\pmb{x},\\theta)+\\frac{1}{L_{s}}\\mathcal{R}(\\pmb{x})}\\end{array}$ . ", "page_idx": 3}, {"type": "text", "text": "We will denote a generic data selection function as $\\pmb{\\mathfrak{V}}\\in\\{x_{d},x_{s},x_{d,\\mathcal{R}},x_{s,\\mathcal{R}}\\}$ in what follows. For the non-greedy variant of reciprocal learning, where data is both added and removed, we need to define data removal as well. A straightforward strategy is to randomly remove data points with uniform removal probabilities. The following function $\\pmb{v}^{-}$ describes the effect of this procedure in expectation. ", "page_idx": 4}, {"type": "text", "text": "Definition 4 (Data Removal Function). Given an empirical distribution $\\mathbb{P}(Y,X)$ of a sample, the function $\\pmb{\\mathfrak{D}}^{-}:\\mathcal{P}\\rightarrow\\mathcal{X}$ ; $\\mathbb{P}(Y,X)\\mapsto\\int\\dot{X}d\\mathbb{P}(X)$ shall be called data removal function. ", "page_idx": 4}, {"type": "text", "text": "2.3 Formal definition and desirable properties ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In order to study reciprocal learning in a meaningful way, we need to be a bit more specific about how $\\mathbb{P}_{t}$ depends on empirical risk minimization in $t-1$ , and specifically on $\\theta_{t-1}$ . The following definition 5 of the sample adaption function allows for this. It will be the pivotal object in this work. The function describes in a general way and for any $t$ how empirical distributions of training data in $t$ are affected by the model, the empirical distribution of training data, and its size in $t-1$ , respectively. ", "page_idx": 4}, {"type": "text", "text": "Definition 5 (Sample Adaption). Denote by $\\Theta$ a parameter space, by $\\mathcal{P}$ a space of probability distributions of $X$ and $Y$ , and $\\mathbb{N}$ the natural numbers. The function $f:\\Theta\\times\\mathcal{P}\\times\\mathbb{N}\\rightarrow\\mathcal{P}$ shall be called the greedy and the function $f_{n}:\\Theta\\times\\mathcal{P}\\xrightarrow{}\\mathcal{P}$ the non-greedy sample adaption function. ", "page_idx": 4}, {"type": "text", "text": "A greedy sample adaption function outputs a distribution $\\mathbb{P}^{\\prime}(Y,X)\\in\\mathcal{P}$ in the iteration after $\\theta\\in\\Theta$ solved ERM on a sample of size $n\\in\\mathbb{N}$ described by $\\mathbb{P}(Y,X)\\in\\mathcal{P}$ , which led to an enhancement of the training data that changed $\\mathbb{P}(Y,X)$ to $\\mathbb{P}^{\\prime}(Y,X)$ . It will come in different flavors for different types of algorithms, see examples in section A. Generally, we have $f(\\theta,\\mathbb{P}(Y,X),n)=\\mathbb{P}^{\\prime}(Y,X)$ , with $\\mathbb{P}^{\\prime}(Y,X)$ being induced by ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbb{P}^{\\prime}(Y=1,X=x)=\\int\\int\\frac{1(x=\\pmb{\\nabla}(\\pmb{\\theta}))\\cdot\\pmb{\\mathfrak{S}}(\\pmb{\\nabla}(\\pmb{\\theta}),\\pmb{\\theta})\\,+\\,n\\,\\mathbb{P}(Y=1,\\,X=x)}{n+1}\\;\\tilde{P}_{\\Im}\\,d y\\;\\tilde{P}_{\\triangledown}\\,d x,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "in case of $\\mathcal{Y}=\\{0,1\\}$ , where $\\mathfrak{s}:\\mathcal{X}\\times\\Theta\\rightarrow\\{0,1\\}$ is any function that assigns a label $y$ , potentially based on the model $\\theta$ , to selected $x$ , and $\\pmb{v}$ any function that selects features $x$ given a model $\\theta$ , for example, $x_{d},x_{s},x_{d,\\mathcal{R}}$ , or $x_{s,\\mathcal{R}}$ as defined above. They give rise to $\\tilde{P}_{\\smash{\\mathbf{y}}}$ and $\\tilde{P}_{\\perp}$ ,respectively. We can be so specific about the sample adaption function due to $P(Y=1,X=x)=P(X=x)-P(Y=0,X=x)$ in binary classification problems. We can analogously define the non-greedy variant $f_{n}(\\theta,\\mathbb{P}(Y,X))$ where one instance is removed by $\\mathfrak{v}^{-}$ and one instance is added by $\\pmb{v}$ per iteration. To this end, define $\\mathbb{P}^{\\prime}(Y=1,X=x)$ by replacing the integrand in equation (1) by ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\frac{1(x=\\mathfrak{v}(\\theta))\\cdot\\mathfrak{I}(\\mathfrak{v}(\\theta),\\theta)\\,+\\,n_{0}\\,\\mathbb{P}(Y=1,\\,X=x)-1(x=\\mathfrak{v}^{-}(\\mathbb{P}(Y,X)))\\cdot\\mathfrak{I}(\\mathfrak{v}^{-}(\\mathbb{P}(Y,X)),\\theta)}{n_{0}},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $n_{0}$ is the size of the initial training data set. Notably, we observe that both sample adaption functions entail a reflexive effect of the model on subsequent data akin to performative prediction [29], see section 5 for a discussion. ", "page_idx": 4}, {"type": "text", "text": "We can now define reciprocal learning (definition 1) more formally given the sample adaption function as follows, both in greedy and non-greedy flavors. ", "page_idx": 4}, {"type": "text", "text": "Definition 6 (Greedy Reciprocal Learning). With $\\ni,\\,\\mathcal{P},\\,X,\\,Y_{i}$ , and $\\mathbb{N}$ as above, we define ", "page_idx": 4}, {"type": "equation", "text": "$$\nR:\\left\\{\\begin{array}{l l}{\\Theta\\times\\mathcal{P}\\times\\mathbb{N}}&{\\to\\Theta\\times\\mathcal{P}\\times\\mathbb{N};}\\\\ {(\\theta,\\mathbb{P}(Y,X),n)}&{\\mapsto(\\theta^{\\prime},\\mathbb{P}^{\\prime}(Y,X),n^{\\prime})}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "as reciprocal learning, where $\\begin{array}{r l r}{\\theta^{\\prime}}&{{}=}&{\\arg\\operatorname*{min}_{\\theta}\\mathbb{E}_{(Y,X)\\sim\\mathbb{P}^{\\prime}(Y,X)}\\ell(Y,X,\\theta)}\\end{array}$ and $\\begin{array}{r l}{\\mathbb{P}^{\\prime}(Y,X)}&{{}=}\\end{array}$ $f(\\theta,\\mathbb{P}(Y,X),n)$ as well as $n^{\\prime}=n+1$ , with $f$ a sample adaption function, see definition 5. Note the equivalence to the informal recursive definition $^{\\,l}$ with $f(\\theta_{t-1},\\mathbb{P}(Y,X)_{t-1},n_{t-1})=\\mathbb{P}(Y,X)_{t}$ . ", "page_idx": 4}, {"type": "text", "text": "Definition 7 (Non-Greedy Reciprocal Learning). With \u0398, $\\mathcal{P}$ , $X$ , and $Y$ as above, we define ", "page_idx": 4}, {"type": "equation", "text": "$$\nR_{n}:{\\left\\{\\Theta\\times\\mathcal{P}\\begin{array}{l l}{}&{\\to\\Theta\\times\\mathcal{P};}\\\\ {(\\theta,\\mathbb{P}(Y,X))}&{\\mapsto(\\theta^{\\prime},\\mathbb{P}^{\\prime}(Y,X))}\\end{array}\\right.}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "as reciprocal learning, where $\\mathbb{P}^{\\prime}(Y,X)=f_{n}(\\theta,\\mathbb{P}(Y,X))$ and $\\begin{array}{r}{\\theta^{\\prime}=\\arg\\operatorname*{min}_{\\theta}\\mathbb{E}_{(Y,X)\\sim\\mathbb{P}^{\\prime}(Y,X)}~\\ell(Y,X,\\theta)}\\end{array}$ with $f_{n}$ a non-greedy sample adaption function, see definition 5. ", "page_idx": 4}, {"type": "text", "text": "We introduce two desirable properties of reciprocal learning. First, we define convergence as a state in which the model stops changing in response to newly added data. This kind of stability allows to stop the process in good faith: Hypothetical subsequent iterations would not have changed the model. Definition 8 offers a straightforward way of formalizing this, implying standard Cauchy convergence. ", "page_idx": 5}, {"type": "text", "text": "Definition 8 (Convergence of Reciprocal Learning). Let $g:\\mathbb{N}\\to\\mathbb{R}$ be a strictly monotone decreasing function and $R$ $\\left(R_{n}\\right)$ any (non-greedy) reciprocal learning algorithm (definitions 6 and 7) outputting $R_{t}\\ (R_{n,t})$ in iteration $t$ . Then $\\varrho\\,\\in\\,\\{R,R_{n}\\}$ is said to converge if $\\|\\varrho_{k},\\varrho_{j}||\\leq g(t)$ for all $k,j\\geq t$ , and $\\begin{array}{r}{\\operatorname*{lim}_{t\\to\\infty}g(t)=0}\\end{array}$ , where $||\\cdot||$ is a norm on the codomains of $R$ and $R_{n}$ , respectively. In this case, define $\\varrho_{c}\\in\\{R_{c},R_{n,c}\\}$ as the limit of this convergent sequence $\\varrho$ . ", "page_idx": 5}, {"type": "text", "text": "Contrary to classical ERM, convergence of reciprocal learning implies stability of both data and parameters. Technically, it refers to all components of the functions $R$ and $R_{n}$ , respectively, see definition 8. It guarantees that $\\theta_{t-1}$ solves ERM on the sample induced by it in $t$ . However, this does not say much about its optimality in general. What if the algorithm had outputted a different $\\theta_{t-1}$ in the first place? The empirical risk could have been lower on the sample in $t$ induced by it. The following definition describes such a look-ahead optimality. It can be interpreted as the optimal data-parameter combination. ", "page_idx": 5}, {"type": "text", "text": "Definition 9 (Optimal Data-Parameter Combination). Consider (non-greedy) reciprocal learning $R\\ (R_{n}).$ , see definitions $^{\\sc6}$ and 7. Define $R^{*}$ and $R_{n}^{*}$ as optimal data-parameter combination in reciprocal learning if $R_{n}^{*}=\\left(\\theta_{n}^{*},\\mathbb{P}_{n}^{*}\\right)=\\arg\\operatorname*{min}_{\\theta,\\mathbb{P}}\\mathbb{E}_{(Y,X)\\sim f_{n}(\\theta,\\mathbb{P})}~\\ell(Y,X,\\theta)$ , and $R^{*}=(\\theta^{*},\\mathbb{P}^{*},n^{*})=$ arg $\\begin{array}{r}{\\operatorname*{min}_{\\theta,\\mathbb{P},n}\\mathbb{E}_{(Y,X)\\sim f(\\theta,\\mathbb{P},n)}\\ \\ell(Y,X,\\theta)}\\end{array}$ , respectively. ", "page_idx": 5}, {"type": "text", "text": "An optimal $\\theta^{*}$ (or $\\theta_{n}^{*}$ , analogously) not only solves ERM on the sample it induces, but is also the best ERM-solution among all possible $\\theta$ $\\left({\\boldsymbol{\\theta}}_{n}\\right)$ that could have led to optimality on the respectively induced sample. In other words, $\\theta^{*}\\left(\\theta_{n}^{*}\\right)$ is found by minimizing the empirical risk with respect to whole $R$ $\\left(R_{n}\\right)$ . That is, it is found by minimizing the empirical risk with respect to $\\theta$ given a sample (characterized by $\\mathbb{P}$ and $n$ ) and steering this very sample through $\\theta$ simultaneously given only the initial sample. Technically, optimality (definition 9) is a bivariate arg min-condition on $\\mathbb{E}_{(Y,X)\\sim f_{n}(\\theta,\\mathbb{P})}\\,\\,\\ell(Y,X,\\theta)$ and $\\mathbb{E}_{(Y,X)\\sim f(\\theta,\\mathbb{P},n)}~\\ell(Y,X,\\theta)$ , respectively. In contrast, convergence (definition 8) translates to a fixed-point condition on the arg min views as a function $\\Theta\\times\\mathcal{P}\\times\\mathbb{N}\\rightarrow\\Theta\\times\\mathcal{P}\\times\\mathbb{N}$ in case of $R$ and as $\\Theta\\times\\mathcal{P}\\rightarrow\\Theta\\times\\mathcal{P}$ in case of $R_{n}$ , see section 3. ", "page_idx": 5}, {"type": "text", "text": "2.4 Self-training is an instance of reciprocal learning ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Let us get back to our running example of self-training. It is easy to see that self-training is a special case of reciprocal learning with the sample adaption function $f_{S S L}:\\Theta\\times\\mathcal{P}\\times\\mathbb{N}\\rightarrow$ $\\mathcal{P}$ ; $(\\theta,\\mathbb{P}(Y,X),n)\\mapsto\\mathbb{P}^{\\prime}(Y,X)$ defined through $\\mathbb{P}^{\\prime}(Y,X)$ being induced by ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{P}^{\\prime}(Y=1,X=x)=\\int\\int{\\frac{1(x=x(\\theta))\\cdot{\\hat{y}}(x_{d}(\\theta),\\theta)\\,+\\,n\\,\\mathbb{P}(Y=1,X=x)}{n+1}}\\ \\tilde{P}_{Y|X}\\,d y\\ \\tilde{P}_{X}\\,d x\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $x_{d}(\\theta)$ (definition 2) selects data with highest \u201cconfidence score\u201d [2, 49, 80, 83, 53, 84, 18], see section 2.1, according to the model $\\theta$ , and gives rise to $\\tilde{P}_{X}$ . The prediction function ${\\hat{y}}:{\\mathcal{X}}\\times\\Theta\\rightarrow\\{0,1\\}$ returns the predicted \u201cpseudo-label\u201d of the selected $x_{d}(\\theta)$ based on the learned model $\\theta$ and gives rise to ${\\tilde{P}}_{Y\\mid X}$ . Moreover, we still assume binary target variables, i.e., the image of $Y$ is $\\{0,1\\}$ , real-valued features $X$ , and only consider cases where the sample changes through the addition of one instance per iteration.3 The averaging with respect to $\\tilde{P}_{X}$ and $\\tilde{P_{Y|X}}$ accounts for the fact that we allow stochastic inclusion of $X$ in the sample through randomized actions and for probabilistic predictions of $Y\\mid X$ , respectively. For now, however, it suffices to think of the special case of degenerate distributions $\\tilde{P_{X}}$ and $\\tilde{P_{Y|X}}$ putting point mass 1 on data with hard labels in the sample and 0 elsewhere.4 Through averaging with respect to $\\tilde{P}_{Y\\mid X}$ we can describe the joint distribution of hard labels $(y_{1},x_{1}),\\ldots,(y_{n},x_{n})$ and predicted soft labels $\\tilde{y}=\\hat{p}(Y=1\\mid x,\\theta)\\in[0,1]$ of $(\\tilde{y}_{n+1},x_{n+1}),\\ldots,(\\tilde{y}_{n+t},x_{n+t})$ . Summing up, both deterministic data selection and non-probabilistic (i.e., hard labels) predictions are well-defined special cases of the above with $\\tilde{P}_{Y\\mid X}$ and $\\tilde{P_{X}}$ collapsing to trivial Dirac measures, respectively. ", "page_idx": 5}, {"type": "text", "text": "3 Convergence of reciprocal learning: Lipschitz is all you need ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "After having generalized several widely adopted machine learning algorithms to reciprocal learning, we will study their convergence (definition 8) and optimality (definition 9). Our general aim is to identify sufficient conditions for any reciprocal learning algorithm to converge and then show that such a convergent solution is sufficiently close to the optimal one. This will not only allow to assess convergence and optimality of examples 1 through 3 (self-training, active learning, multi-armed bandits, see appendix A) but of any other reciprocal learning algorithm. Besides further existing examples not detailed in this paper like superset learning [35] or Bayesian optimization [63], we are especially aiming at potential future \u2013 yet to be proposed \u2013 algorithms. On this background, our conditions for convergence and optimality can be understood as design principles. Before turning to these concrete conditions on reciprocal learning algorithms, we need some general assumptions on the loss function for the remainder of the paper. Assumptions 1 and 2 can be considered quite mild and are fulfilled by a broad class of loss functions, see [95, Chapter 12] or [14]. For instance, the L2-regularized (ridge) logistic loss has Lipschitz-continuous gradients both with respect to features and parameters. For a discussion of assumption 3, we refer to appendix E.2. ", "page_idx": 6}, {"type": "text", "text": "Assumption 1 (Continuous Differentiability in Features). $A$ loss function $\\ell(Y,X,\\theta)$ is said to be continuously differentiable with respect to features if the gradient $\\nabla_{X}\\ell(Y,X,\\theta)$ exists and is $\\alpha$ -Lipschitz continuous in \ud835\udf03, $x$ , and $y$ with respect to the $L2$ -norm on domain and codomain. ", "page_idx": 6}, {"type": "text", "text": "Assumption 2 (Continuous Differentiability in Parameters). $A$ loss function $\\ell(Y,X,\\theta)$ is continuously differentiable with respect to parameters if the gradient $\\nabla_{\\theta}\\ell(Y,X,\\theta)$ exists and is $\\beta$ -Lipschitz continuous in $\\theta,x_{;}$ , and $y$ with respect to the $L2$ -norm on domain and codomain. ", "page_idx": 6}, {"type": "text", "text": "Assumption 3 (Strong Convexity). Loss $\\ell(Y,X,\\theta)$ is said to be $\\gamma$ -strongly convex if $\\ell(y,x,\\theta)\\geq$ $\\ \\ell\\left(y,x,\\theta^{\\prime}\\right)+\\nabla_{\\theta}\\ell\\left(y,x,\\theta^{\\prime}\\right)^{\\top}\\left(\\theta-\\theta^{\\prime}\\right)+{\\textstyle\\frac{\\gamma}{2}}\\left\\|\\theta-\\theta^{\\prime}\\right\\|_{2}^{2}$ , for all $\\theta,\\theta^{\\prime},y,x.$ . Observe convexity for $\\gamma=0$ . ", "page_idx": 6}, {"type": "text", "text": "Let us now turn to specific and more constructive conditions on reciprocal learning\u2019s workhorse, the data selection problem $(\\Theta,\\mathcal{X},\\mathcal{L}_{\\theta})$ . At the heart of these conditions lies a common goal: We want to establish some continuity in how the data changes from $t-1$ to $t$ in response to $\\theta_{t-1}$ and $\\mathbb{P}_{t-1}$ . It is self-evident that without any such continuity, convergence seems out of reach. As it will turn out, bounding the change of the data in $t$ by the change of what happens in $t-1$ will be sufficient for convergence, see figure 3. We thus need the sample adaption function (definition 5) to be Lipschitz-continuous. Theorem 1 will deliver this for subsets of conditions 1 through 5 in case of binary classification problems. The reason for the latter restriction is that we need an explicit definition of $f$ to constructively prove its Lipschitz-continuity. ", "page_idx": 6}, {"type": "text", "text": "Condition 1 (Data Regularization). Data selection is regularized as per definition 3. ", "page_idx": 6}, {"type": "text", "text": "Condition 2 (Soft Labels Prediction). The prediction function ${\\hat{y}}:{\\mathcal{X}}\\times\\Theta\\rightarrow\\{0,1\\}$ on bounded $\\mathcal{X}$ gives rise to a non-degenerate distribution of $Y\\mid X$ for any $\\theta$ such that we can consider soft label predictions $p:\\mathcal{X}\\times\\Theta\\rightarrow[0,1]$ with $p(x,\\theta)=\\sigma(g(X,\\theta))$ with $\\sigma:\\mathbb{R}\\rightarrow[0,1]$ a sigmoid function. Further, assume that the loss is jointly smooth in these predictions. That is, $\\nabla_{p}\\ell(y,p(x,\\theta))$ exists and is Lipschitz-continuous in $x$ and $\\theta$ . ", "page_idx": 6}, {"type": "text", "text": "Condition 3 (Stochastic Data Selection). Data is selected stochastically according to $x_{s}$ by drawing from a normalized criterion \u222b\ud835\udc65\u2032 exp(\ud835\udc50(\ud835\udc65\u2032,\ud835\udf03\ud835\udc61))\ud835\udc51\ud835\udf07(\ud835\udc65) , see definition 2. ", "page_idx": 6}, {"type": "text", "text": "Condition 4 (Continuous Selection Criterion). It holds for the decision criterion $c:\\mathcal{X}\\times\\Theta\\rightarrow\\mathbb{R}$ in the decision problem $(\\Theta,\\mathbb{A},\\mathcal{L}_{\\theta_{t}})$ of selecting features to be added to the sample that $\\nabla_{x}c(x,\\theta)$ and $\\nabla_{\\theta}c(x,\\theta)$ are bounded from above. ", "page_idx": 6}, {"type": "text", "text": "Condition 5 (Linear Selection Criterion). The decision criterion $c:\\mathcal{X}\\times\\Theta\\rightarrow\\mathbb{R}$ in $(\\Theta,\\mathbb{A},\\mathcal{L}_{\\theta_{t}})$ is linear in $x$ and Lipschitz-continuous in $\\theta$ with a Lipschitz constant $L_{c}$ that is independent from $x$ . ", "page_idx": 6}, {"type": "text", "text": "We can interpret $p$ as $P_{\\theta}(Y\\mid X=x)$ in condition 2, see also definition 1. In other words, soft labels in the form of probability distributions are available. Adding observations with soft labels to the data can be implemented either through randomization, i.e., by adding $x$ with label 1 with probability $p$ and vice versa, or through weighted retraining. Note that condition 4 implies condition 5 through characterization of Lipschitz-continuity by bounded gradients. We need two implications of these conditions to establish Lipschitz-continuity of the sample adaption in reciprocal learning. First, it can be shown that regularized data selection (condition 1) is Lipschitz-continuous in the model, see lemma 1. Second, the soft label prediction function (condition 2) is Lipschitz in both data and model, if the data selection, in turn, is Lipschitz-continuous in the model, see lemma 2. ", "page_idx": 6}, {"type": "text", "text": "Lemma 1 (Regularized Data Selection is Lipschitz). Regularized Data Selection ", "page_idx": 7}, {"type": "equation", "text": "$$\nx_{d,\\mathcal{R}}:\\Theta\\rightarrow\\mathcal{X};\\;\\theta\\mapsto\\underset{\\pmb{x}\\in\\mathcal{X}}{\\mathrm{argmax}}\\left\\lbrace\\pmb{c}(\\pmb{x},\\theta)+\\frac{1}{L_{s}}\\mathcal{R}(\\pmb{x})\\right\\rbrace\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "with $\\cal{K}$ -strongly convex regularizer, see definition 3 and condition $^{\\,l}$ , is $\\frac{L_{s}\\!\\cdot\\!L_{c}}{\\kappa}$ -Lipschitz continuous, if $c$ is linear in $x$ (condition 5) and Lipschitz-continuous in $\\theta$ with a Lipschitz constant $L_{c}$ that is independent of $\\overrightharpoon{x}$ . ", "page_idx": 7}, {"type": "text", "text": "Lemma 2 (Soft Label Prediction is Lipschitz). The soft label prediction function (condition 2) ", "page_idx": 7}, {"type": "equation", "text": "$$\np:\\mathcal{X}\\times\\Theta\\rightarrow[0,1];p(\\pmb{\\mathscr{v}}(\\theta),\\theta)=\\int\\int\\hat{y}(\\pmb{\\mathscr{v}}(\\theta),\\theta)\\tilde{p}\\,d y\\,\\tilde{P}_{\\perp}d\\pmb{\\mathscr{v}}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "is Lipschitz-continuous in both $x\\in\\mathcal{X}$ and $\\theta\\in\\Theta$ and $(x,\\theta)\\in\\mathcal{X}\\times\\Theta$ if $\\int\\pmb{\\mathfrak{v}}(\\theta)\\tilde{P}_{\\pmb{\\mathfrak{v}}}d\\pmb{\\mathfrak{v}}$ is Lipschitzcontinuous. ", "page_idx": 7}, {"type": "text", "text": "Proofs of all results in this paper can be found in appendix F. With the help of lemma 1 and 2, we are now able to state two key results. They tell us under which conditions the sample adaption functions in both greedy and non-greedy reciprocal learning are Lipschitz-continuous, which will turn out to be sufficient for convergence. ", "page_idx": 7}, {"type": "text", "text": "Theorem 1 (Regularization Makes Sample Adaption Lipschitz-Continuous). If predictions are soft (condition 2) and the data selection is regularized (conditions 1 and 5), both greedy and non-greedy sample adaption functions $f$ and $f_{n}$ (see definition 5) in reciprocal learning with $\\mathcal{Y}=\\{0,1\\}$ are Lipschitz-continuous with respect to the $L2$ -norm on \u0398 and N, and the Wasserstein- $^{\\,l}$ -distance on $\\mathcal{P}$ . ", "page_idx": 7}, {"type": "text", "text": "Theorem 2 (Randomization Makes Sample Adaption Lipschitz-Continuous). If predictions are soft (condition 2) and the data selection is randomized (conditions 3 and 4), greedy and non-greedy sample adaption functions are Lipschitz-continuous in the sense of theorem 1. ", "page_idx": 7}, {"type": "text", "text": "The general idea for both proofs is to show Lipschitz-continuity component-wise and then infer that $f$ and $f_{n}$ are Lipschitz with the supremum of all component-wise Lipschitz-constants. We can now leverage these theorems to state our main result. It tells us (via theorems 1 and 2 and conditions $1\\cdot5$ ) which types of reciprocal learning algorithms converge. Recall that convergence (definition 8) in reciprocal learning implies a convergent model and a convergent data set. ", "page_idx": 7}, {"type": "text", "text": "Theorem 3 (Convergence of Non-Greedy Reciprocal Learning). If the non-greedy sample adaption $f_{n}$ is Lipschitz-continuous with $\\begin{array}{r}{L\\leq(1+\\frac{\\beta}{\\gamma})^{-1}}\\end{array}$ , the iterates $R_{n,t}=(\\theta_{t},\\mathbb{P}_{t})$ of non-greedy reciprocal learning $R_{n}$ (definition 7) converge to $R_{n,c}=(\\theta_{c},\\mathbb{P}_{c})$ at a linear rate. ", "page_idx": 7}, {"type": "text", "text": "The proof idea is as follows. We relate the Lipschitz-continuity of $f_{n}$ to the Lipschitzcontinuity of $R_{n}$ via the dual characterization of the Wasserstein metric [43]. If $f_{n}$ is Lipschitz with $\\begin{array}{r}{L\\ \\leq\\ (1+\\frac{\\beta}{\\gamma})^{-1}}\\end{array}$ , we further show that $R_{n}$ is a bivariate contraction. The Banach fixed-point theorem [4, 72, 17] then directly delivers uniqueness and existence of $(\\theta_{c},\\mathbb{P}_{c})$ as convergent fixed point, which means that it holds $\\begin{array}{r}{\\theta_{c}=\\arg\\operatorname*{min}_{\\theta}\\mathbb{E}_{(Y,X)\\sim f(\\theta_{c},\\mathbb{P}_{c})}\\,\\,\\ell(Y,X,\\theta)}\\end{array}$ . A complete proof can be found in appendix F.5. Building on earlier work on performatively optimal predictions [73], we can further relate this convergent training solution to the global solution of reciprocal learning, i.e., the optimal data-parameter fit, see definition 9. The following theorem 4 states that our convergent solution is close to the optimal one. It tells us that we did not enforce a trivial or even degenerate form of ", "page_idx": 7}, {"type": "image", "img_path": "mhhlZeAr67/tmp/9efbc5332cd10730f8d2a750696bbcffe57f5f58609fa961ab07f0627dcf76c4.jpg", "img_caption": ["Figure 3: Reciprocal learning converges if the change in sample (purple) is bounded by the change in model (yellow) and previous sample. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "convergence (e.g., constant $\\theta_{t}$ ) by regularization and randomization. Theorem 4 only refers to the convergent parameter solution, not to the data. Note that the parameter solution is the crucial part of reciprocal learning for later deployment and assessment on test data. ", "page_idx": 7}, {"type": "text", "text": "Theorem 4 (Optimality of Convergent Solution). If non-greedy reciprocal learning converges in the sense of theorem 3, it holds $\\begin{array}{r}{||\\theta_{c}-\\theta^{*}||_{2}\\leq\\frac{2L_{\\ell}L}{\\gamma}}\\end{array}$ for $\\theta_{c}$ and $\\theta^{*}$ from the convergent d a-parameter tuple $R_{n,c}=(\\theta_{c},\\mathbb{P}_{c})$ and the optimal one $R_{n}^{*}=(\\theta^{*},\\mathbb{P}^{*})$ if the loss is -Lipschitz in $X$ and \ud835\udc4c. ", "page_idx": 8}, {"type": "text", "text": "While theorem 1 and 2 guarantee that both greedy $f$ and non-greedy $f_{n}$ are Lipschitz, theorem 3 and thus also theorem 4 only hold for non-greedy reciprocal learning. The question immediately comes to mind whether we can say anything about the asymptotic behavior of the greedy variant, too. The following theorem 5 gives an affirmative answer. Intuitively, there is no fixed point in $\\Theta\\times\\mathcal{P}\\times\\mathbb{N}$ if data is constantly being added and not removed such that $n\\to\\infty$ . ", "page_idx": 8}, {"type": "text", "text": "Theorem 5. Greedy Reciprocal Learning does not converge in the sense of definition 8. ", "page_idx": 8}, {"type": "text", "text": "We conclude this section with another negative result. It states that theorem 3 is tight in theorem 1 and 2. Summing things up, Lipschitz-continuity is all you need for non-greedy reciprocal learning to converge. ", "page_idx": 8}, {"type": "text", "text": "Theorem 6. If the sample adaption $f_{n}$ is not Lipschitz, non-greedy reciprocal learning can diverge. ", "page_idx": 8}, {"type": "text", "text": "4 Which reciprocal learning algorithms converge? ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We briefly relate the above results to specific algorithms in active learning, bandits, and our running example of self-training. Assume a binary target variable and $\\begin{array}{r}{L\\,\\leq\\,(1+\\frac{\\beta}{\\gamma})^{-1}}\\end{array}$ with $\\gamma$ and $\\beta$ in the sense of assumption 1-3 throughout. First observe that any greedy (definition 6) algorithm that only adds data without removal does not converge in the sense of defintion 8 with respect to $\\theta,\\mathbb{P}.$ , and $n$ , see theorem 5. This provides a strong case for non-greedy self-training algorithms, often referred to as amending strategies [103] or self-training with editing [52] and noise filters [104], that add and remove data, as opposed to greedy ones like incremental or batch-wise self-training [103], see example 1, that only add pseudo-labeled data without removing any data. For detailed explanation and comparison of the two, please refer to Appendix A.1.1. ", "page_idx": 8}, {"type": "text", "text": "Corollary 1 (Self-Training). Amending self-training algorithms converge in the sense of definition 8, if predicted pseudo-labels are soft (condition 2) and data selection is regularized (condition 1) or randomized (condition 3). ", "page_idx": 8}, {"type": "text", "text": "Furthermore, we shed some light on the debate [97, 120] in the literature on multi-armed bandits about whether to use deterministic strategies like upper confidence bound [10, 100, 42] or stochastic ones like epsilon-greedy [45, 54] search or Thompson sampling [89, 90], see example 3. Note, however, that this insight relates to in-sample convergence only, see definition 8. ", "page_idx": 8}, {"type": "text", "text": "Corollary 2 (Bandits). Non-greedy multi-armed bandits with Thompson sampling and epsilon-greedy strategies converge in the sense of definition 8 under additional condition 2, while bandits with upper confidence bound (UCB) are not guaranteed to converge. ", "page_idx": 8}, {"type": "text", "text": "What is more, condition 2 allows us to distinguish between active learning from weak and strong oracles, see [59, 93, 78] for literature surveys. The former posits the availability of probabilistic or noisy oracle feedback through soft labels [59, 114, 117]; the latter assumes the oracle to have access to an undeniable ground truth via hard labels [26, 19, 20]. ", "page_idx": 8}, {"type": "text", "text": "Corollary 3 (Active Learning). Active Learning from a strong oracle (i.e., providing hard labels) is not guaranteed to converge, while active learning from a weak oracle (soft labels) converges in the sense of definition 8 under additional condition 1 or 3. ", "page_idx": 8}, {"type": "text", "text": "5 Related work ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Convergence of active learning, self-training, and other special cases of reciprocal learning has been touched upon in the context of stopping criteria [109, 121, 48, 110, 28, 22, 83, 11, 79, 96]. We refer to section 1 for a discussion and relate reciprocal learning to other fields in what follows. ", "page_idx": 8}, {"type": "text", "text": "Continual Learning: While reciprocity through, e.g., gradient-based data selection is a known phenomenon in continual learning [1, 112, 15], the inference goal is not static as in reciprocal learning. Continual learning rather aims at excelling at new tasks (that is, new populations), while reciprocal learning can simply be seen as a greedy approximation of extended ERM, see section 2. ", "page_idx": 8}, {"type": "text", "text": "Online Learning: In online learning and online convex optimization, the task is to predict $y$ by $\\hat{y}$ from iteratively receiving $x$ . After each prediction, the true \ud835\udc66and corresponding loss $\\ell(y,\\hat{y})$ is observed, see [94] for an introduction and appendix B.2 for an illustration. Reciprocal learning can thus be considered a special online learning framework. Typically, online learning assumes incoming data to be randomly drawn or even selected by an adversarial player, while being selected by the algorithm itself in reciprocal learning. The majority of the online learning literature is concerned with how to update a model in light of new data, while we focus on how data is selected based on the current model fit. Loosely speaking, online learning deals with only one side of the coin explicitly, while we take a reciprocal point of view: We study both how to learn parameters from data and how to select data in light of fitted parameters. ", "page_idx": 9}, {"type": "text", "text": "Coresets: The aim of coreset construction is to find subsamples that lead to parameter fits close to the originally learned parameters [62, 76, 69, 24, 68, 33]. It can be seen as a post hoc approach, while reciprocal learning algorithms directly learn a \u201cparameter-efficient\u201d sample on the go. ", "page_idx": 9}, {"type": "text", "text": "Performative Prediction: The sample adaption functions in reciprocal learning are reminiscent of performative prediction, where today\u2019s predictions change tomorrow\u2019s population [74, 29, 61], and, more generally, of the \u201creflexivity problem\u201d in social sciences [99, 66]. We identify analogous reflexive effects on the sample level in reciprocal learning via the sample adaption function $f$ (or $f_{n})$ ), see section 2. Contrary to performative prediction, however, $f\\ (f_{n})$ describes an in-sample (performative prediction: population) reflexive effect of both data and parameters (performative prediction: only parameters). Moreover, reciprocal learning describes specific and implementable algorithms, which allows for an explicit study of these reflexive effects. While we rely on similar techniques as in [74, 61], namely Lipschitz-continuity and Wasserstein-distance, our work is thus conceptually different. For an illustration of these differences, see appendix B.1. ", "page_idx": 9}, {"type": "text", "text": "Safe Active Learning: Safe active learning explores the feature space by optimizing an acquisition criterion under a safety constraint [122, 51, 91]. While this can be viewed as regularization akin to the one we propose in definition 3, both aim and structure are different: We want to enforce Lipschitz-continuity explicitly via a penalty term in data selection; safe active learning optimizes a selection criterion without penalty terms under constraints that are motivated by domain knowledge. ", "page_idx": 9}, {"type": "text", "text": "6 Discussion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Summary: We have embedded a wide range of established machine learning algorithms into a unifying framework, called reciprocal learning. This gave rise to a rigorous analysis of (1) under which conditions and (2) how fast these algorithms converge to an approximately optimal model. We further applied these results to common practices in self-training, active learning, and bandits. ", "page_idx": 9}, {"type": "text", "text": "Limitations: While our results guarantee the convergence of reciprocal learning algorithms, the opposite does generally not hold. That is, if our conditions are violated, we cannot rule out the possibility of (potentially weaker notions) of convergence. Furthermore, our analysis requires assumptions on the loss functions, as detailed in section 3 and appendix E. In particular, it needs to be $\\gamma$ -strongly convex and have $\\beta$ -Lipschitz gradients, such that $\\begin{array}{r}{L\\leq(1+\\frac{\\beta}{\\gamma})^{-1}}\\end{array}$ with $L$ the Lipschitzconstant of the sample adaption. This limits our results\u2019 applicability. From another perspective, however, this is a feature rather than a bug, since the described restrictions can serve as design principles for self-training, active learning, or bandit algorithms that shall converge, see below. ", "page_idx": 9}, {"type": "text", "text": "Future Work: This article identifies sufficient conditions for convergence of reciprocal learning. These restrictions pave the way for a theory-informed design of novel algorithms. In particular, our results emphasize the importance of regularization of both parameters and data for convergence. While the former is needed to control $\\gamma$ and $_\\beta$ , see appendix E.2 for the example of Tikhonovregularization, the latter guarantees Lipschitz-continuity of the sample adaption through theorem 1. Parameter regularization is well-studied and has been heavily applied. We conjecture that the concept of data regularization might bear similar practical potential. Another line of future research would be to address the question whether reciprocal learning algorithms are stable with respect to slight changes in the initial training data. In this sense, [8, 30] might serve as a bridge to future research. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "We sincerely thank Thomas Augustin, James Bailie, and Lea H\u00f6hler for helpful comments on earlier versions of this manuscript. We also thank all four anonymous reviewers for their assessment of our paper. Moreover, we are indebted to several participants of the 2024 Workshop on Machine Learning under Weakly Structured Information in Munich for critically assessing preliminary ideas and conjectures regarding reciprocal learning presented at the workshop. ", "page_idx": 10}, {"type": "text", "text": "Julian Rodemann acknowledges support by the Federal Statistical Office of Germany within the co-operation project \u201cMachine Learning in Official Statistics\u201d, the Bavarian Academy of Sciences (BAS) through the Bavarian Institute for Digital Transformation (bidt), and the LMU mentoring program of the Faculty of Mathematics, Informatics, and Statistics. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Rahaf Aljundi et al. \u201cGradient based sample selection for online continual learning\u201d. In: Advances in Neural Information Processing Systems. Ed. by H. Wallach et al. Vol. 32. Curran Associates, Inc., 2019.   \n[2] Eric Arazo et al. \u201cPseudo-labeling and confirmation bias in deep semi-supervised learning\u201d. In: 2020 International Joint Conference on Neural Networks. IEEE. 2020, pp. 1\u20138.   \n[3] Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. \u201cFinite-time analysis of the multiarmed bandit problem\u201d. In: Machine learning 47 (2002), pp. 235\u2013256. [4] Stefan Banach. \u201cSur les op\u00e9rations dans les ensembles abstraits et leur application aux \u00e9quations int\u00e9grales\u201d. In: Fundamenta mathematicae 3.1 (1922), pp. 133\u2013181. [5] Emanuel Ben-Baruch et al. \u201cDistilling the Knowledge in Data Pruning\u201d. In: arXiv preprint arXiv:2403.07854 (2024).   \n[6] James O. Berger. Statistical decision theory and Bayesian analysis. 2nd. Springer, 1985.   \n[7] Surojit Biswas et al. \u201cLow-N protein engineering with data-efficient deep learning\u201d. In: Nature methods 18.4 (2021), pp. 389\u2013396. [8] Olivier Bousquet and Andr\u00e9 Elisseeff. \u201cStability and generalization\u201d. In: The Journal of Machine Learning Research 2 (2002), pp. 499\u2013526.   \n[9] Tom Brown et al. \u201cLanguage models are few-shot learners\u201d. In: Advances in neural information processing systems 33 (2020), pp. 1877\u20131901.   \n[10] Alexandra Carpentier et al. \u201cUpper-confidence-bound algorithms for active learning in multiarmed bandits\u201d. In: International Conference on Algorithmic Learning Theory. Springer. 2011, pp. 189\u2013203.   \n[11] Deepayan Chakrabarti et al. \u201cMortal multi-armed bandits\u201d. In: Advances in neural information processing systems 21 (2008).   \n[12] Olivier Chapelle, Bernhard Sch\u00f6lkopf, and Alexander Zien. Semi-supervised learning. Adaptive computation and machine learning series. MIT Press, 2006.   \n[13] Anshuman Chhabra et al. \u201cWhat data beneftis my classifier? Enhancing model performance and interpretability through influence-based data selection\u201d. In: International Conference on Learning Representations. 2024.   \n[14] Geoffrey Chinot, Guillaume Lecu\u00e9, and Matthieu Lerasle. \u201cRobust statistical learning with Lipschitz and convex loss functions\u201d. In: Probability Theory and related fields 176.3 (2020), pp. 897\u2013940.   \n[15] Aristotelis Chrysakis and Marie-Francine Moens. \u201cOnline Continual Learning from Imbalanced Data\u201d. In: Proceedings of the 37th International Conference on Machine Learning. Ed. by Hal Daum\u00e9 III and Aarti Singh. Vol. 119. Proceedings of Machine Learning Research. PMLR, 13\u201318 Jul 2020, pp. 1952\u20131961.   \n[16] David A Cohn, Les Atlas, and Richard Ladner. \u201cImproving generalization with active learning\u201d. In: Machine Learning 15.2 (1994), pp. 201\u2013221.   \n[17] Patrick L Combettes and Jean-Christophe Pesquet. \u201cFixed point strategies in data science\u201d. In: IEEE Transactions on Signal Processing 69 (2021), pp. 3878\u20133905.   \n[18] Stefan Dietrich, Julian Rodemann, and Christoph Jansen. \u201cSemi-Supervised Learning guided by the Generalized Bayes Rule under Soft Revision\u201d. In: arXiv preprint arXiv:2405.15294 (2024).   \n[19] Shi Dong. \u201cMulti class SVM algorithm with active learning for network traffic classification\u201d. In: Expert Systems with Applications 176 (2021), p. 114885.   \n[20] Liat Ein Dor et al. \u201cActive learning for BERT: an empirical study\u201d. In: Proceedings of the 2020 conference on empirical methods in natural language processing (EMNLP). 2020, pp. 7949\u20137962.   \n[21] Dheeru Dua and Casey Graff. UCI Machine Learning Repository. http://archive.ics. uci.edu/ml. 2017.   \n[22] Eyal Even-Dar et al. \u201cAction elimination and stopping conditions for the multi-armed bandit and reinforcement learning problems.\u201d In: Journal of machine learning research 7.6 (2006).   \n[23] Gabriele Farina et al. \u201cStable-predictive optimistic counterfactual regret minimization\u201d. In: International conference on machine learning. PMLR. 2019, pp. 1853\u20131862.   \n[24] Susanne Frick, Amer Krivosija, and Alexander Munteanu. \u201cScalable Learning of Item Response Theory Models\u201d. In: Proceedings of The 27th International Conference on Artificial Intelligence and Statistics. Ed. by Sanjoy Dasgupta, Stephan Mandt, and Yingzhen Li. Vol. 238. Proceedings of Machine Learning Research. PMLR, Feb. 2024, pp. 1234\u20131242.   \n[25] Yarin Gal, Riashat Islam, and Zoubin Ghahramani. \u201cDeep bayesian active learning with image data\u201d. In: International conference on machine learning. PMLR. 2017, pp. 1183\u20131192.   \n[26] Mingfei Gao et al. \u201cConsistency-based semi-supervised active learning: Towards minimizing labeling cost\u201d. In: Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part X 16. Springer. 2020, pp. 510\u2013526.   \n[27] Laura Graves, Vineel Nagisetty, and Vijay Ganesh. \u201cAmnesiac machine learning\u201d. In: Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 35. 13. 2021, pp. 11516\u2013 11524.   \n[28] Edita Grolman et al. \u201cHow and when to stop the co-training process\u201d. In: Expert Systems with Applications 187 (2022), p. 115841.   \n[29] Moritz Hardt and Celestine Mendler-D\u00fcnner. \u201cPerformative Prediction: Past and Future\u201d. In: arXiv preprint arXiv:2310.16608 (2023).   \n[30] Moritz Hardt, Ben Recht, and Yoram Singer. \u201cTrain faster, generalize better: Stability of stochastic gradient descent\u201d. In: Proceedings of The 33rd International Conference on Machine Learning. Ed. by Maria Florina Balcan and Kilian Q. Weinberger. Vol. 48. Proceedings of Machine Learning Research. New York, New York, USA: PMLR, 20\u201322 Jun 2016, pp. 1225\u20131234.   \n[31] Trevor Hastie et al. The elements of statistical learning: data mining, inference, and prediction. Vol. 2. Springer, 2009.   \n[32] Shuo He et al. \u201cCandidate Label Set Pruning: A Data-centric Perspective for Deep Partiallabel Learning\u201d. In: The Twelfth International Conference on Learning Representations. 2023.   \n[33] Jonathan Huggins, Trevor Campbell, and Tamara Broderick. \u201cCoresets for scalable Bayesian logistic regression\u201d. In: Advances in neural information processing systems 29 (2016).   \n[34] Eyke H\u00fcllermeier. \u201cLearning from imprecise and fuzzy observations: Data disambiguation through generalized loss minimization\u201d. In: International Journal of Approximate Reasoning 55 (2014), pp. 1519\u20131534.   \n[35] Eyke H\u00fcllermeier and Weiwei Cheng. \u201cSuperset learning based on generalized loss minimization\u201d. In: Joint European Conference on Machine Learning and Knowledge Discovery in Databases. Springer. 2015, pp. 260\u2013275.   \n[36] Eyke H\u00fcllermeier, S\u00e9bastien Destercke, and Ines Couso. \u201cLearning from imprecise data: adjustments of optimistic and pessimistic variants\u201d. In: International Conference on Scalable Uncertainty Management. Springer, 2019, pp. 266\u2013279.   \n[37] Nathan Huntley and Matthias Troffaes. \u201cSubtree perfectness, backward induction, and normalextensive form equivalence for single agent sequential decision making under arbitrary choice functions\u201d. In: arXiv preprint arXiv:1109.3607 (2011).   \n[38] Nathan Huntley and Matthias CM Troffaes. \u201cNormal form backward induction for decision trees with coherent lower previsions\u201d. In: Annals of Operations Research 195.1 (2012), pp. 111\u2013134.   \n[39] Guido W Imbens and Donald B Rubin. Causal inference in statistics, social, and biomedical sciences. Cambridge university press, 2015.   \n[40] Hideaki Ishibashi and Hideitsu Hino. \u201cStopping criterion for active learning based on deterministic generalization bounds\u201d. In: International Conference on Artificial Intelligence and Statistics. PMLR. 2020, pp. 386\u2013397.   \n[41] Hideaki Ishibashi and Hideitsu Hino. \u201cStopping criterion for active learning based on error stability\u201d. In: arXiv preprint arXiv:2104.01836 (2021).   \n[42] Anand Kalvit and Assaf Zeevi. \u201cA closer look at the worst-case behavior of multi-armed bandit algorithms\u201d. In: Advances in Neural Information Processing Systems 34 (2021), pp. 8807\u20138819.   \n[43] Leonid Vasilevich Kantorovich and SG Rubinshtein. \u201cOn a space of totally additive functions\u201d. In: Vestnik of the St. Petersburg University: Mathematics 13.7 (1958), pp. 52\u201359.   \n[44] William Karush. \u201cMinima of functions of several variables with inequalities as side constraints\u201d. In: M. Sc. Dissertation. Dept. of Mathematics, Univ. of Chicago (1939).   \n[45] Volodymyr Kuleshov and Doina Precup. \u201cAlgorithms for multi-armed bandit problems\u201d. In: arXiv preprint arXiv:1402.6028 (2014).   \n[46] Yongchan Kwon et al. \u201cDatainf: Efficiently estimating data influence in lora-tuned llms and diffusion models\u201d. In: arXiv preprint arXiv:2310.00902 (2023).   \n[47] Hunter Lang, Aravindan Vijayaraghavan, and David Sontag. \u201cTraining subset selection for weak supervision\u201d. In: Advances in Neural Information Processing Systems 35 (2022), pp. 16023\u201316036.   \n[48] Florian Laws and Hinrich Sch\u00fctze. \u201cStopping criteria for active learning of named entity recognition\u201d. In: Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008). 2008, pp. 465\u2013472.   \n[49] Dong-Hyun Lee et al. \u201cPseudo-label: The simple and efficient semi-supervised learning method for deep neural networks\u201d. In: Workshop on challenges in representation learning, International Conference on Machine Learning. Vol. 3. 2013, p. 896.   \n[50] David D Lewis and William A Gale. \u201cA sequential algorithm for training text classifiers\u201d. In: SIGIR \u201994: Proceedings of the 17th annual international ACM SIGIR conference on Research and development in information retrieval. Springer. 1994, pp. 3\u201312.   \n[51] Cen-You Li, Barbara Rakitsch, and Christoph Zimmer. \u201cSafe active learning for multi-output gaussian processes\u201d. In: International Conference on Artificial Intelligence and Statistics. PMLR. 2022, pp. 4512\u20134551.   \n[52] Ming Li and Zhi-Hua Zhou. \u201cSETRED: Self-training with editing\u201d. In: Pacific-Asia Conference on Knowledge Discovery and Data Mining. Springer. 2005, pp. 611\u2013621.   \n[53] Shuangshuang Li et al. \u201cPseudo-label selection for deep semi-supervised learning\u201d. In: 2020 IEEE International Conference on Progress in Informatics and Computing (PIC). IEEE. 2020, pp. 1\u20135.   \n[54] Tian Lin, Jian Li, and Wei Chen. \u201cStochastic online greedy learning with semi-bandit feedbacks\u201d. In: Advances in Neural Information Processing Systems 28 (2015).   \n[55] Wei Liu et al. \u201cWhat makes good data for alignment? a comprehensive study of automatic data selection in instruction tuning\u201d. In: arXiv preprint arXiv:2312.15685 (2023).   \n[56] Debmalya Mandal, Stelios Triantafyllou, and Goran Radanovic. \u201cPerformative reinforcement learning\u201d. In: International Conference on Machine Learning. PMLR. 2023, pp. 23642\u2013 23680.   \n[57] Max Marion et al. \u201cWhen less is more: Investigating data pruning for pretraining llms at scale\u201d. In: arXiv preprint arXiv:2309.04564 (2023).   \n[58] Nestor Maslej et al. \u201cArtificial intelligence index report 2023\u201d. In: arXiv preprint arXiv:2310.03715 (2023).   \n[59] Bj\u00f6rn Mattsson. \u201cActive learning of neural network from weak and strong oracles\u201d. In: (2017).   \n[60] Lara Mauri and Ernesto Damiani. \u201cEstimating degradation of machine learning data assets\u201d. In: ACM Journal of Data and Information Quality (JDIQ) 14.2 (2021), pp. 1\u201315.   \n[61] John P Miller, Juan C Perdomo, and Tijana Zrnic. \u201cOutside the echo chamber: Optimizing the performative risk\u201d. In: International Conference on Machine Learning. PMLR. 2021, pp. 7710\u20137720.   \n[62] Baharan Mirzasoleiman, Jeff Bilmes, and Jure Leskovec. \u201cCoresets for data-efficient training of machine learning models\u201d. In: International Conference on Machine Learning. PMLR. 2020, pp. 6950\u20136960.   \n[63] Jonas Mo\u02c7ckus. \u201cOn Bayesian methods for seeking the extremum\u201d. In: Optimization Techniques IFIP Technical Conference: Novosibirsk, July 1\u20137, 1974. Springer. 1975, pp. 400\u2013 404.   \n[64] Jonas Moc\u02c7kus, Vytautas Tiesis, and Antanas Zilinskas. \u201cThe application of Bayesian methods for seeking the extremum\u201d. In: Towards global optimization 2.117-129 (1978), p. 2.   \n[65] Robert Munro Monarch. Human-in-the-Loop Machine Learning: Active learning and annotation for human-centered AI. Simon and Schuster, 2021.   \n[66] O Morgenstern. \u201cWirtschaftsprognose: Eine Untersuchung ihrer Voraussetzungen und M\u00f6glichkeiten, Wien 1928, cited after: G. Betz (2004), Empirische und aprioristische Grenzen von Wirtschaftsprognosen: Oskar Morgenstern nach 70 Jahren\u201d. In: Wissenschaftstheorie in \u00d6konomie und Wirtschaftsinformatik, Deutscher Universit\u00e4ts-Verlag, Wiesbaden (1928), pp. 171\u2013190.   \n[67] Niklas Muennighoff et al. \u201cScaling data-constrained language models\u201d. In: Advances in Neural Information Processing Systems 36 (2024).   \n[68] Alexander Munteanu and Chris Schwiegelshohn. \u201cCoresets-Methods and History: A Theoreticians Design Pattern for Approximation and Streaming Algorithms\u201d. In: KI - K\u00fcnstliche Intelligenz 32.1 (Feb. 2018), pp. 37\u201353. ISSN: 1610-1987.   \n[69] Alexander Munteanu et al. \u201cOn Coresets for Logistic Regression\u201d. In: Advances in Neural Information Processing Systems. Ed. by S. Bengio et al. Vol. 31. Curran Associates, Inc., 2018.   \n[70] Ofir Nachum et al. \u201cData-efficient hierarchical reinforcement learning\u201d. In: Advances in neural information processing systems 31 (2018).   \n[71] Malte Nalenz, Julian Rodemann, and Thomas Augustin. \u201cLearning de-biased regression trees and forests from complex samples\u201d. In: Machine Learning (2024), pp. 1\u201320.   \n[72] Vittorino Pata et al. Fixed point theorems and applications. Vol. 116. Springer, 2019.   \n[73] Juan Perdomo. \u201cPerformative Prediction: Theory and Practice\u201d. PhD thesis. UC Berkeley, 2023.   \n[74] Juan Perdomo et al. \u201cPerformative prediction\u201d. In: International Conference on Machine Learning. PMLR. 2020, pp. 7599\u20137609.   \n[75] Nitin Namdeo Pise and Parag Kulkarni. \u201cA survey of semi-supervised learning methods\u201d. In: 2008 International conference on computational intelligence and security. Vol. 2. IEEE. 2008, pp. 30\u201334.   \n[76] Omead Pooladzandi, David Davini, and Baharan Mirzasoleiman. \u201cAdaptive second order coresets for data-efficient machine learning\u201d. In: International Conference on Machine Learning. PMLR. 2022, pp. 17848\u201317869.   \n[77] Daniel Reker. \u201cPractical considerations for active machine learning in drug discovery\u201d. In: Drug Discovery Today: Technologies 32 (2019), pp. 73\u201379.   \n[78] Pengzhen Ren et al. \u201cA survey of deep active learning\u201d. In: ACM computing surveys (CSUR) 54.9 (2021), pp. 1\u201340.   \n[79] Paul Reverdy, Vaibhav Srivastava, and Naomi Ehrich Leonard. \u201cSatisficing in multi-armed bandit problems\u201d. In: IEEE Transactions on Automatic Control 62.8 (2016), pp. 3788\u20133803.   \n[80] Mamshad Nayeem Rizve et al. \u201cIn Defense of Pseudo-Labeling: An Uncertainty-Aware Pseudo-label Selection Framework for Semi-Supervised Learning\u201d. In: International Conference on Learning Representations, 2020. 2020.   \n[81] Herbert Robbins. \u201cSome aspects of the sequential design of experiments\u201d. In: Bulletin of the American Mathematical Society 58.5 (1952), pp. 527\u2013535.   \n[82] Julian Rodemann. \u201cBayesian Data Selection\u201d. In: arXiv preprint arXiv:2406.12560 (2024). 5th Workshop on Data-Centric Machine Learning Research (DMLR) at ICML 2024.   \n[83] Julian Rodemann et al. \u201cApproximately Bayes-optimal pseudo-label selection\u201d. In: Proceedings of the Thirty-Ninth Conference on Uncertainty in Artificial Intelligence (UAI). Vol. 216. Proceedings of Machine Learning Research. PMLR, 2023, pp. 1762\u20131773. [84] Julian Rodemann et al. \u201cIn all likelihoods: Robust selection of pseudo-labeled data\u201d. In: International Symposium on Imprecise Probability: Theories and Applications. PMLR. 2023, pp. 412\u2013425.   \n[85] Julian Rodemann et al. \u201cLevelwise Data Disambiguation by Cautious Superset Classification\u201d. In: International Conference on Scalable Uncertainty Management. Springer. 2022, pp. 263\u2013 276. [86] Julian Rodemann et al. \u201cNot All Data Are Created Equal: Lessons From Sampling Theory For Adaptive Machine Learning\u201d. In: International Conference on Statistics and Data Science (ICSDS) by the Institute of Mathematical Statistics (IMS). 2022. [87] Chuck Rosenberg, Martial Hebert, and Henry Schneiderman. \u201cSemi-Supervised Self-Training of Object Detection Models\u201d. In: 2005 Seventh IEEE Workshops on Applications of Computer Vision (WACV/MOTION\u201905)-Volume 1. Vol. 1. IEEE. 2005, pp. 29\u201336. [88] Jean-Francis Roy, Mario Marchand, and Fran\u00e7ois Laviolette. \u201cA column generation bound minimization approach with PAC-Bayesian generalization guarantees\u201d. In: Artificial Intelligence and Statistics. PMLR. 2016, pp. 1241\u20131249.   \n[89] Daniel J Russo and Benjamin Van Roy. \u201cAn information-theoretic analysis of thompson sampling\u201d. In: Journal of Machine Learning Research 17.68 (2016), pp. 1\u201330.   \n[90] Daniel J Russo et al. \u201cA tutorial on thompson sampling\u201d. In: Foundations and Trends\u00ae in Machine Learning 11.1 (2018), pp. 1\u201396.   \n[91] Jens Schreiter et al. \u201cSafe exploration for active learning with Gaussian processes\u201d. In: Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2015, Porto, Portugal, September 7-11, 2015, Proceedings, Part III 15. Springer. 2015, pp. 133\u2013149.   \n[92] Max Schwarzer et al. \u201cPretraining representations for data-efficient reinforcement learning\u201d. In: Advances in Neural Information Processing Systems 34 (2021), pp. 12686\u201312699.   \n[93] Burr Settles. Active Learning Literature Survey. Tech. rep. Computer Sciences Technical Report 1648. University of Wisconsin\u2013Madison, 2010. [94] Shai Shalev-Shwartz et al. \u201cOnline learning and online convex optimization\u201d. In: Foundations and Trends\u00ae in Machine Learning 4.2 (2012), pp. 107\u2013194. [95] Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to algorithms. Cambridge university press, 2014. [96] Jaehyeok Shin, Aaditya Ramdas, and Alessandro Rinaldo. \u201cOn Conditional Versus Marginal Bias in Multi-Armed Bandits\u201d. In: Proceedings of the 37th International Conference on Machine Learning. Ed. by Hal Daum\u00e9 III and Aarti Singh. Vol. 119. Proceedings of Machine Learning Research. PMLR, 13\u201318 Jul 2020, pp. 8852\u20138861. [97] Aleksandrs Slivkins et al. \u201cIntroduction to multi-armed bandits\u201d. In: Foundations and Trends\u00ae in Machine Learning 12.1-2 (2019), pp. 1\u2013286.   \n[98] Jasper Snoek, Hugo Larochelle, and Ryan P Adams. \u201cPractical bayesian optimization of machine learning algorithms\u201d. In: Advances in neural information processing systems 25 (2012).   \n[99] George Soros. The alchemy of finance. John Wiley & Sons, 2015.   \n[100] Niranjan Srinivas et al. \u201cInformation-theoretic regret bounds for gaussian process optimization in the bandit setting\u201d. In: IEEE transactions on information theory 58.5 (2012), pp. 3250\u2013 3265.   \n[101] Bernadette J Stolz. \u201cOutlier-robust subsampling techniques for persistent homology\u201d. In: Journal of Machine Learning Research 24.90 (2023), pp. 1\u201335.   \n[102] Hugo Touvron et al. \u201cLlama: Open and efficient foundation language models\u201d. In: arXiv preprint arXiv:2302.13971 (2023).   \n[103] Isaac Triguero, Salvador Garc\u00eda, and Francisco Herrera. \u201cSelf-labeled techniques for semisupervised learning: taxonomy, software and empirical study\u201d. In: Knowledge and Information systems 42.2 (2015), pp. 245\u2013284.   \n[104] Isaac Triguero et al. \u201cOn the characterization of noise fliters for self-training semi-supervised in nearest neighbor classification\u201d. In: Neurocomputing 132 (2014), pp. 30\u201341.   \n[105] Vishaal Udandarao et al. \u201cNo\" zero-shot\" without exponential data: Pretraining concept frequency determines multimodal model performance\u201d. In: arXiv preprint arXiv:2404.04125 (2024).   \n[106] Jesper E Van Engelen and Holger H Hoos. \u201cA survey on semi-supervised learning\u201d. In: Machine Learning 109.2 (2020), pp. 373\u2013440.   \n[107] Pablo Villalobos et al. \u201cWill we run out of data? an analysis of the limits of scaling datasets in machine learning\u201d. In: arXiv preprint arXiv:2211.04325 (2022).   \n[108] C\u00e9dric Villani et al. Optimal transport: old and new. Vol. 338. Springer, 2009.   \n[109] Andreas Vlachos. \u201cA stopping criterion for active learning\u201d. In: Computer Speech & Language 22.3 (2008), pp. 295\u2013312.   \n[110] Wenquan Wang, Wenbin Cai, and Ya Zhang. \u201cStability-based stopping criterion for active learning\u201d. In: 2014 IEEE International Conference on Data Mining. IEEE. 2014, pp. 1019\u2013 1024.   \n[111] Ximei Wang et al. \u201cSelf-tuning for data-efficient deep learning\u201d. In: International Conference on Machine Learning. PMLR. 2021, pp. 10738\u201310748.   \n[112] Felix Wiewel and Bin Yang. \u201cEntropy-based Sample Selection for Online Continual Learning\u201d. In: 2020 28th European Signal Processing Conference (EUSIPCO). 2021, pp. 1477\u2013 1481.   \n[113] Zimo Yin et al. \u201cEmbrace sustainable AI: Dynamic data subset selection for image classification\u201d. In: Pattern Recognition (2024), p. 110392.   \n[114] Taraneh Younesian et al. \u201cQactor: Active learning on noisy labels\u201d. In: Asian Conference on Machine Learning. PMLR. 2021, pp. 548\u2013563.   \n[115] Kelly Zhang, Lucas Janson, and Susan Murphy. \u201cStatistical inference with m-estimators on adaptively collected data\u201d. In: Advances in neural information processing systems 34 (2021), pp. 7460\u20137471.   \n[116] Lijun Zhang, Tie-Yan Liu, and Zhi-Hua Zhou. \u201cAdaptive regret of convex and smooth functions\u201d. In: International Conference on Machine Learning. PMLR. 2019, pp. 7414\u20137423.   \n[117] Wentao Zhang et al. \u201cInformation gain propagation: a new way to graph active learning with soft labels\u201d. In: arXiv preprint arXiv:2203.01093 (2022).   \n[118] Bo Zhao, Konda Reddy Mopuri, and Hakan Bilen. \u201cDataset Condensation with Gradient Matching\u201d. In: International Conference on Learning Representations. 2020.   \n[119] Peng Zhao et al. \u201cDynamic regret of convex and smooth functions\u201d. In: Advances in Neural Information Processing Systems 33 (2020), pp. 12510\u201312520.   \n[120] Li Zhou. \u201cA survey on contextual multi-armed bandits\u201d. In: arXiv preprint arXiv:1508.03326 (2015).   \n[121] Jingbo Zhu et al. \u201cConfidence-based stopping criteria for active learning for data annotation\u201d. In: ACM Transactions on Speech and Language Processing (TSLP) 6.3 (2010), pp. 1\u201324.   \n[122] Christoph Zimmer, Mona Meister, and Duy Nguyen-Tuong. \u201cSafe active learning for timeseries modeling with Gaussian processes\u201d. In: Advances in Neural Information Processing Systems 31 (2018). ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "A Familiar examples of reciprocal learning ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We will demonstrate that well-established machine learning procedures are special cases of reciprocal learning. We start by illustrating reciprocal learning by self-training in semi-supervised learning (SSL), see section 2.1, and then turn to active learning and multi-armed bandits. ", "page_idx": 16}, {"type": "text", "text": "A.1 Self-Training ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "For ease of exposition, we will start by focusing on binary target variables, i.e., the image of $Y$ is $\\{0,1\\}$ , with real-valued features $X$ . Moreover, we will only consider cases where the sample changes through the addition of one instance per iteration.5 Leaning on [106, 12, 103], we describe SSL as follows. Consider labeled data ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\boldsymbol{\\mathcal{D}}=\\left\\{(x_{i},y_{i})\\right\\}_{i=1}^{n}\\in(\\boldsymbol{\\mathcal{X}}\\times\\boldsymbol{\\mathcal{Y}})^{n}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and unlabeled data $x\\in\\mathcal{X}$ . The aim of SSL is to learn a predictive classification function $\\hat{\\mathrm{y}}(x,\\theta)$ parameterized by $\\theta$ utilizing both labeled and unlabeled data. According to [75] and [106], SSL can be broadly categorized into self-training and co-training. We will focus on the former. Self-training involves fitting a model on $\\mathcal{D}$ by ERM and then exploiting this model to predict labels for $\\mathcal{X}$ . In a second step, some instances $\\{x_{i}\\}_{i=n+1}^{m}\\in\\mathcal{X}$ are selected to be added to the training data together with the predicted label, typically the ones with the highest confidence according to some criterion, see [2, 49, 80, 83, 53, 84, 18] for examples. ", "page_idx": 16}, {"type": "text", "text": "Example 1 (Self-Training). Self-training is an instance of reciprocal learning with the sample adaption function (see definition $^{5}$ ) $f_{S S L}:\\Theta\\times\\mathcal{P}\\times\\mathbb{N}\\to\\mathcal{P}$ ; $(\\theta,\\mathbb{P}(Y,X),n)\\,\\mapsto\\,\\mathbb{P}^{\\prime}(Y,X)$ with $\\mathbb{P}^{\\prime}(Y,X)$ induced by ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{P}^{\\prime}(Y=1,X=x)=\\int\\int{\\frac{1(x=x(\\theta))\\cdot{\\hat{y}}(x_{d}(\\theta),\\theta)\\,+\\,n\\,\\mathbb{P}(Y=1,X=x)}{n+1}}\\ \\tilde{P}_{Y|X}\\,d y\\ \\tilde{P}_{X}\\,d x\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $x_{d}(\\theta)$ (definition 2) selects data with highest confidence score, see [2, 80, 53], according to the model $\\theta$ , and gives rise to $\\tilde{P}_{X}$ . The prediction function $\\hat{y}:\\mathcal{X}\\times\\Theta\\rightarrow\\{0,1\\}$ returns the predicted label of the selected $x_{d}(\\theta)$ based on the learned model $\\theta$ and gives rise to ${\\tilde{P}}_{Y\\mid X}$ . ", "page_idx": 16}, {"type": "text", "text": "The averaging with respect to $\\tilde{P}_{X}$ and ${\\tilde{P}}_{Y\\mid X}$ accounts for the fact that we allow stochastic inclusion of $X$ in the sample through randomized actions and for probabilistic predictions of $Y\\mid X$ , respectively. For now, however, it suffices to think of the special case of degenerate distributions $\\tilde{P}_{X}$ and $\\tilde{P}_{Y|\\dot{X}}$ putting point mass 1 on data with hard labels in the sample and 0 elsewhere.6 Through averaging with respect to ${\\tilde{P}}_{Y\\mid X}$ we can describe the joint distribution of hard labels $(y_{1},x_{1}),\\ldots,(y_{n},x_{n})$ and predicted soft labels $\\tilde{y}\\,=\\,\\hat{p}(Y\\,=\\,1\\,\\mid\\,x,\\theta)\\,\\in\\,[0,1]$ of $(\\tilde{y}_{n+1},x_{n+1}),\\ldots,(\\tilde{y}_{n+t},x_{n+t})$ . Summing up, both deterministic data selection and non-probabilistic (i.e., hard labels) predictions are well-defined special cases of the above with ${\\tilde{P}}_{Y\\mid X}$ and $\\tilde{P}_{X}$ collapsing to trivial Dirac measures, respectively. ", "page_idx": 16}, {"type": "text", "text": "A.1.1 Implications of convergence results ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Our corollaries in section 4 shed some light on the convergence of self-training methods in a semisupervised learning regime. Recall from above that the aim of these methods is to learn a predictive classification function $\\hat{\\mathrm{y}}(x,\\theta)$ parameterized by $\\theta$ utilizing both labeled data $\\mathcal{D}\\,=\\,\\{(x_{i},\\bar{y}_{i})\\}_{i=1}^{n}\\,\\in$ $(\\mathcal{X}\\times\\mathcal{Y})^{n}$ and unlabeled data $\\mathcal{U}=\\{(x_{i},\\mathcal{Y})\\}_{i=n+1}^{m}\\in\\left(\\mathcal{X}\\times2^{\\mathcal{Y}}\\right)^{m-n}$ from the same data generation process. Self-training involves fitting a model identified with parameters $\\theta$ on $\\mathcal{D}$ by ERM and then exploiting this model to predict labels for $\\boldsymbol{u}$ . In incremental self-training, some instances from $\\boldsymbol{u}$ are selected to be added to the training data (together with the predicted label) according to some regularized data selection criterion $\\begin{array}{r}{c_{r}(x,\\theta)\\,=\\,c(x,\\theta)+\\frac{1}{L_{s}}\\mathcal{R}(x)}\\end{array}$ , see example 1 and pseudo code below. Amending self-training does the same, but additionally removes instances from $\\boldsymbol{u}$ , see below. ", "page_idx": 16}, {"type": "text", "text": "The key insight from our analysis is that the sequence of $\\theta$ converges at a linear rate in case of amending self-training and regularized data selection. ", "page_idx": 17}, {"type": "image", "img_path": "mhhlZeAr67/tmp/3e07920d261d5242162bfdeb27cdf826e9a16234eff6ecdb43270c398bd4335d.jpg", "img_caption": [], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "A.2 Active learning ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Active learning is a machine learning paradigm where the learning algorithm iteratively asks an oracle to provide true labels for training data [50, 16, 93]. The goal is to improve the sample efficiency of the learning process by asking queries that are expected to provide the most information. Let $\\mathcal{X}$ be the input space and $y$ the set of possible labels. Consider training data ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\boldsymbol{\\mathcal{D}}=\\left\\{(x_{i},y_{i})\\right\\}_{i=1}^{n}\\in(\\boldsymbol{\\mathcal{X}}\\times\\boldsymbol{\\mathcal{Y}})^{n}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "as above. The active learning cycle is as follows. First, train a model on the currently labeled dataset $\\mathcal{D}$ . Next, select the most informative sample $x^{*}\\in\\mathcal{X}$ based on an acquisition function (criterion) such as uncertainty, representativeness, or expected model change and obtain the label $y^{*}$ for the selected instance $x^{*}$ from an oracle (e.g., human expert). Finally, update the training data $\\mathcal{D}\\leftarrow\\mathcal{D}\\cup\\{(x^{*},y^{*})\\}$ and refti the model. This cycle is repeated until a stopping criterion is met (e.g., performance threshold). ", "page_idx": 17}, {"type": "text", "text": "Example 2 (Active Learning). Active Learning is an instance of reciprocal learning with the following sample adaption function (see definition 5) $f_{A L}:\\Theta\\times\\mathcal{P}\\times\\mathbb{N}\\to\\mathcal{P}$ ; $(\\theta,\\mathbb{P}(Y,X),n)\\mapsto\\mathbb{P}^{\\prime}(Y,X)$ with $\\mathbb{P}^{\\prime}(Y,X)$ induced by ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{P}^{\\prime}(Y=1,X=x)=\\int\\int{\\frac{1(x=x(\\theta))\\cdot q_{y}(x_{s}(\\theta))\\,+\\,n\\,\\mathbb{P}(Y=1,\\,X=x)}{n+1}}\\ \\tilde{P}_{Y\\mid X}\\,d y\\ \\tilde{P}_{X}\\,d x\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $x_{s}(\\theta)$ is a data selection function, see definition 2. Its induced distribution on $\\mathcal{X}$ is $\\tilde{P}_{X}$ . The query function $q_{\\mathrm{y}}:\\mathcal{X}\\to[0,1]$ returns the true class (probability) for the selected $x_{s}(\\theta)$ and gives rise to ${\\tilde{P}}_{Y\\mid X}$ . In contrast to self-training (example $^{\\,l}$ ), the queried labels $q_{\\mathrm{y}}(x_{s}(\\theta))$ do not directly depend on the model $\\theta$ , only indirectly through $x_{s}(\\theta)$ . ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "Again, both deterministic data selection through $x_{d}(s)$ and non-probabilistic (i.e., hard labels) queries through $q_{y}:\\mathcal{X}\\!\\times\\!\\Theta\\rightarrow\\{0,1\\}$ are well defined special cases of the above with $\\tilde{P}_{Y\\mid X}$ and $\\tilde{P}_{X}$ collapsing to trivial Dirac measures, respectively. As far as we can oversee the active learning literature, hard label queries [77, 25, 65] are more common than probabilistic or soft queries [114]. ", "page_idx": 18}, {"type": "text", "text": "A.3 Multi-armed bandits ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "The multi-armed bandit problem is one of the most general setups for evaluating decision-making strategies when facing uncertain outcomes. It is named after the analogy of a gambler at a row of slot machines, where each machine provides a different, unknown reward distribution. The gambler must develop a strategy to maximize their rewards over a series of spins, balancing the exploration of machines to learn more about their rewards versus exploiting known information to maximize returns. Typically, a contextual bandit algorithm is comprised of contexts $\\{X_{t}\\}_{t=1}^{T}$ , actions $\\{A_{t}\\}_{t=1}^{T}$ , and primary outcomes $\\left\\{Y_{t}\\right\\}_{t=1}^{T}$ , again with binary image of $Y_{t}$ for simplicity, denoted by $\\mathcal{Y}=\\{0,1\\}$ . We assume that rewards are a deterministic function of the primary outcomes, i.e., $R_{t}=f\\left(Y_{t}\\right)$ for some known function $f$ . Following [115], we use potential outcome notation [39] and let $\\{Y(a):a\\in\\mathcal{A}\\}$ denote the potential outcomes of the primary outcome and let $Y_{t}:=Y\\left(A_{t}\\right)$ be the observed outcome. Define these quantities analogously for $X(a)$ and call ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathcal{H}_{t}:=\\{X_{t^{\\prime}},A_{t^{\\prime}},Y_{t^{\\prime}}\\}_{t^{\\prime}=1}^{t}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "the history for $t\\,\\geq\\,1$ and $\\mathcal{H}_{0}\\,:=\\,\\emptyset$ as in [115]. The fixed and (time-independent) potential joint distributions of $X_{t}$ and $Y_{t}$ shall be denoted by ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\{X_{t},Y_{t}(a):a\\in{\\mathcal{A}}\\}\\,\\sim\\,\\mathbb{P}(X,Y)\\,\\in{\\mathcal{P}}{\\mathrm{~for~}}t\\in\\{1,\\dots,T\\}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Further assume that we learn a model of $\\mathbb{P}(X,Y,A)$ or ${\\mathbb{P}}(Y\\mid X,A)$ , which can be parameterized by $\\theta_{t}\\in\\Theta$ . Note that for a specific reward function and $\\mathcal{A}=\\mathcal{X}$ , active learning could be formulated as a multi-armed bandit problem. The following embedding into reciprocal learning, however, is much more general. It only requires that the probability of playing an action $\\mathbb{P}\\left(A_{t}\\mid\\mathcal{H}_{t-1}\\right)$ is informed by our model $\\theta$ , i.e., ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(A_{t}\\ |\\ \\mathcal{H}_{t-1}\\right)=\\mathbb{P}\\left(A_{t}\\ |\\ \\theta(\\mathcal{H}_{t-1})\\right),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "which is a very mild assumption given that the latter is the whole point of $\\theta$ in multi-armed contextual bandit problems. ", "page_idx": 18}, {"type": "text", "text": "Example 3 (Multi-Armed Bandits). Multi-Armed Bandits are instances of reciprocal learning with the following sample adaption function (see definition 5) $f_{M A B}:\\Theta\\times\\mathcal{P}\\times\\mathbb{N}\\to\\mathcal{P}$ ; $(\\theta,\\mathbb{P}(Y,X),n)\\mapsto$ $\\mathbb{P}^{\\prime}(Y,X)$ with $\\mathbb{P}^{\\prime}(Y,X)$ induced by ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{P}^{\\prime}(Y=1,X=x)=\\int\\frac{1(x=x(a(\\theta)))\\cdot Y(a(\\theta))\\,+\\,n\\,\\mathbb{P}(Y=1,\\,X=x)}{n+1}\\,\\mathbb{P}\\left(A_{t}\\mid\\theta(\\mathcal{H}_{t-1})\\right)\\,d a\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $a(\\theta):\\Theta\\rightarrow\\mathcal{A}$ is an action selection function7, also referred to as policy function in the bandit literature that induces the well-known action selection probabilities $\\mathbb{P}\\left(A\\mid\\theta(\\mathcal{H}_{t-1})\\right)$ , often called policies and denoted by $\\pi:=\\{\\pi_{t}\\}_{t\\ge1}$ . Further note that the indicator function takes an argument that depends on $\\theta$ only through $a$ , contrary to active and semi-supervised learning. ", "page_idx": 18}, {"type": "text", "text": "Several strategies exist to solve multi-armed bandit problems, including upper confidence bound (deterministic), epsilon-greedy (stochastic) and already mentioned Thompson sampling (stochastic). Deterministic strategies like upper confidence bound can be embedded into the above general stochastic formulation through degenerate policies $\\mathbb{P}\\left(A\\mid\\theta({\\mathcal{H}}_{t-1})\\right)$ ) putting point mass 1 on the deterministically optimal action. ", "page_idx": 18}, {"type": "text", "text": "B Additional Illustrations ", "text_level": 1, "page_idx": 19}, {"type": "image", "img_path": "mhhlZeAr67/tmp/fa6fc683197f92ae23f98fde722118394fbd1f23ecef5ba2cf27be6f002a47de.jpg", "img_caption": ["B.1 Difference between reciprocal learning and performative prediction "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Figure 4: (A) Reciprocal learning ftis a model from the model space (restricted by red curve) to a realized sample from the sample space (blue-grey) that depends on the previous model fit, see Figure 1b. (B) In performative prediction, the population, not the sample, changes in response to the model fit. In other words, reciprocal learning algorithms have a static inference goal, while performative prediction is concerned with moving targets. ", "page_idx": 19}, {"type": "text", "text": "B.2 Reciprocal learning compared to general online learning ", "text_level": 1, "page_idx": 19}, {"type": "image", "img_path": "mhhlZeAr67/tmp/9a20b81a67b520b936323854a3b0290ddaca5e93727a9dc8561f1a873861b0e1.jpg", "img_caption": [], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Figure 5: (A) Classical one-shot machine learning fits a model from the model space (restricted by red curve) to a realized sample from the sample space (blue-grey), see [31, Figure 7.2]. (B) Reciprocal learning fits a model from the model space (restricted by red curve) to a realized osanmlpilne efr olme tahre nsianmgpl.ep snpgace (blue-grey) that depends on the previous model fti, see Figure 1b. (C) In the general online learning setup, there is no interaction between sample in $t$ and model in $t-1$ . ", "page_idx": 19}, {"type": "text", "text": "C Illustrative experiments on data regularization ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We run two simple experiments to illustrate the effect of data regularization (definition 3) on stability of parameters $\\theta_{t}$ in reciprocal learning by the example of self-training in semi-supervised learning, see sections 2.1, A.1 and example 1. Code to reproduce findings can be found in ", "page_idx": 19}, {"type": "text", "text": "https://github.com/rodemann/simulations-self-training-reciprocal-learning. ", "page_idx": 20}, {"type": "text", "text": "Specifically, we deploy incremental self-training with soft labels on a real world datasets (banknote data) with $90\\%$ , $80\\%$ , and $70\\%$ unlabeled data, see figures 7a, 7b and 7c. The task is to predict the authenticity of a banknote based on labeled and unlabeled data. We use a generalized additive model and multiple selection criteria from the literature ([83, 35, 80, 103]), one of which is regularized according to section 3. We want to compare the stability of the parameter vector $\\theta_{t}$ of self-training with regularized data selection to self-training with unregularized data selection criteria. Specifically, we are interested in comparing the regularized Bayesian selection criterion (gold) to its unregularized counterpart (red). The goal is not to study convergence under all conditions 1 to 5, but merely to illustrate the stabilizing effect ot the novel concept of data regularization (condition 1) on the sequence of learned parameters. To do so, we compute the L2-norm of the parameter vector $\\theta_{t}$ at each iteration $t$ , see figures 7a, 7b and 7c. It becomes evident that self-training with regularized data selection is more stable than with unregularized data selection. Note that this setup analyzes variation (or rather, the absence thereof) within an experiment. We also assess the variations of $\\theta_{t}$ between experiments. In order to do so, we restart the experiment 40 times and average the L2-norm of $\\theta_{t}$ over these 40 restarts of the experiment and compute $95\\%$ -confidence intervals to assess the variation between experiments. We observe that the regularized selection criterion has much higher variation than its unregularized counterpart. Interestingly, the lower in-experiment variation due to data regularization seems to come at the cost of higher between-experiment variation. ", "page_idx": 20}, {"type": "text", "text": "D Alternative stochastic data selection ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Definition 10 (Data Selection alternative). Let $c:\\mathcal{X}\\!\\times\\!\\Theta\\rightarrow\\mathbb{R}$ be a criterion for the decision problem $(\\Theta,\\mathbb{A},\\mathcal{L}_{\\theta_{t}})$ of selecting features to be added to the sample in iteration \ud835\udc61. Let ${\\mathcal{D}}({\\mathcal{X}})$ denote a suitable set of probability measures on the measurable space $(\\mathcal{X},\\sigma(\\mathcal{X}))$ . Define ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\tilde{c}:\\left\\{\\mathcal{D}(\\mathcal{X})\\times\\Theta\\right.\\ \\rightarrow\\mathbb{R}}\\\\ {(\\lambda,\\theta_{t})\\qquad\\qquad\\left.\\mapsto\\mathbb{E}_{\\lambda}(c(\\cdot,\\theta_{t}))\\right.}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Each $\\lambda\\in\\mathcal{D}(\\mathcal{X})$ is interpreted as a randomized feature selection strategy. The function $\\tilde{c}$ evaluates randomized feature selection strategies based on the expectation of the criterion $c$ under the randomization weights. ", "page_idx": 20}, {"type": "text", "text": "E Discussion of assumptions on loss ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "E.1 General discussion of assumption 1, 2, and 3 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Assumptions 2 and 3 address the loss function $\\ell$ in standard ERM, i.e., the first decision problem, as detailed in section 2. These are general assumptions often needed in a wide array of repeated (empirical) risk minimization setups, see [56, 116, 119] and particularly [74] as well as [95] for an overview. Assumption 1 will be needed in both decision problems of data selection and parameter selection, but is still fairly general and mild. ", "page_idx": 20}, {"type": "text", "text": "E.2 Discussion of assumption 3: Strong convexity of loss function ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Assumption 3 is typically required for fast convergence of repeated ERM solution strategies. Here, it is needed for reciprocal learning to converge at all. Thus, it can be considered a stronger assumption than assumptions 1 and 2. It is easy to see that common loss functions like the linear loss $\\ell(y,x,\\theta)=\\theta x y$ are convex, but not strongly convex. The same even holds for the logistic loss $\\ell(y,x,\\theta)=\\log(1+$ $\\exp(\\theta x y))$ . To see this, consider its second partial derivative $\\nabla_{\\theta}^{2}\\ell\\left(y,\\bar{x_{,}}\\,\\theta\\right)$ . It is ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\nabla_{\\theta}^{2}\\ell\\left(y,x,\\theta\\right)=\\frac{y^{2}x^{2}\\exp(\\theta y x)}{(1+\\exp(\\theta y x))^{2}}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "It becomes evident that $\\begin{array}{r}{\\operatorname*{lim}_{x\\rightarrow\\infty}\\operatorname*{lim}_{y\\rightarrow\\infty}\\nabla_{\\theta}^{2}\\ell\\left(y,x,\\theta^{\\prime}\\right)\\;=\\;0}\\end{array}$ Hence, there is no $K\\ >\\ 0$ that can bound $\\nabla_{\\theta}^{2}\\ell\\left(y,x,\\theta\\right)$ from below. However, a Tikhonov-regularized version thereof $\\ell_{r}(y,x,\\theta)\\,=$ $\\begin{array}{r}{\\log(1+\\exp(\\theta x y))+\\frac{\\gamma}{2}||\\theta||_{2}}\\end{array}$ is $\\gamma$ -strongly convex, which follows from analogous reasoning. This sheds some light on the nature of our sufficient conditions for convergence, see also section 6. In ", "page_idx": 20}, {"type": "image", "img_path": "mhhlZeAr67/tmp/1cd4d35a389fb4bdd3a34a08fbf84be3238bae309dbed59b41458abbd6b018d2.jpg", "img_caption": ["(a) Self-training on banknote data with $90\\%$ unlabeled data. "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "mhhlZeAr67/tmp/9165873668b31de2bc152db7827f4093a55678652fb10bf1a2abce863771a2b7.jpg", "img_caption": ["(b) Self-training on banknote data with $80\\%$ unlabeled data. "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "mhhlZeAr67/tmp/1208ff3f7deaf8538cfe5406612722bf9d50ec286734f79ae418228c64b4ae0f.jpg", "img_caption": ["(c) Self-training on banknote data with $70\\%$ unlabeled data. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "Figure 6: Self-training with soft labels and varying selection criteria $c(\\boldsymbol{x},\\theta)$ , one of which (Bayes-crit-reg) is regularized, on banknote data [21] with $70\\%$ (a) and $80\\%$ (b) unlabeled data; y-axis shows L2-Norm of $\\theta_{t}$ at iteration \ud835\udc61. Iterations vary between (a), (b), and (c) due to varying size of unlabeled data. Model: Generalized additive regression. Data source: Public UCI Machine Learning Repository [21]. References for other selection criteria: Bayes-crit: Rodemann, J., et al. \"Approximately Bayes-optimal pseudo-label selection.\" [83]. Likelihood: H\u00fcllermeier, E., Cheng, W. \"Superset learning based on generalized loss minimization.\" [35] Predictive Var: Rizve, M, N., et al. \"In Defense of Pseudo-Labeling: An Uncertainty-Aware Pseudo-label Selection Framework for Semi-Supervised Learning.\" [80]. Probability Score: Triguero, I., Garc\u00eda, S., Herrera, F. (2015). \"Self-labeled techniques for semi-supervised learning: taxonomy, software and empirical study.\" [103]. For details, see https://github.com/rodemann/simulations-self-training-reciprocal-learning. ", "page_idx": 21}, {"type": "image", "img_path": "mhhlZeAr67/tmp/06929c7ffc89aef398fbe37d39554ee1a87cd53077c69166c8b4186667edc200.jpg", "img_caption": ["(a) Self-training on banknote data with $90\\%$ unlabeled data.; distribution of L2-norms over 40 restarts. "], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "mhhlZeAr67/tmp/eab024384de07bca06c1a0e5727208f577aba23b180189c9ec50a9f7483e346e.jpg", "img_caption": ["(b) Self-training on banknote data with $80\\%$ unlabeled data; distribution of L2-norms over 40 restarts. "], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "mhhlZeAr67/tmp/58fdea89794349abe56c54a3fbf7458d89d4744dde814cb48d8fd0e9e2d85334.jpg", "img_caption": ["(c) Self-training on banknote data with $70\\%$ unlabeled data; distribution of L2-norms over 40 restarts. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "Figure 7: Self-training with soft labels and varying selection criteria $c(\\boldsymbol{x},\\theta)$ , one of which (Bayes-crit-reg) is regularized, on banknote data [21] with $70\\%$ (a) and $80\\%$ (b) unlabeled data; y-axis shows L2-Norm of $\\theta_{t}$ averaged over 40 restarts. Shaded area indicates $95\\%$ -confidence region. ", "page_idx": 22}, {"type": "text", "text": "fact, we require regularization of parameters to obtain strong convexity of the loss function and of data to obtain Lipschitz-continuity of the sample adaptions, see theorem 1. This paves the way for theory-informed design of regularized reciprocal learning algorithms that are guaranteed to converge. ", "page_idx": 23}, {"type": "text", "text": "F Proofs ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "F.1 Proof of Lemma 1 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Proof. Recall definition 2 of $c:\\mathcal{X}\\times\\Theta\\rightarrow\\mathbb{R}$ as a criterion for the decision problem $(\\Theta,\\mathbb{A},\\mathcal{L}_{\\theta_{t}})$ . We will prove the Lipschitz-continuity of regularized data selection: ", "page_idx": 23}, {"type": "equation", "text": "$$\nx_{d,\\mathcal{R}}:\\Theta\\rightarrow\\mathcal{X};\\;\\theta\\mapsto\\underset{x\\in\\mathcal{X}}{\\mathrm{argmax}}\\left\\lbrace c(\\pmb{x},\\theta)+\\frac{1}{L_{s}}\\mathcal{R}(\\pmb{x})\\right\\rbrace,\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $\\mathcal{R}(\\cdot)$ is a $\\cal{K}$ -strongly convex regularizer. The variational inequality for the optimality of argmax $\\begin{array}{r}{\\left\\{c(\\pmb{x},\\theta)+\\frac{1}{L_{s}}\\mathcal{R}(\\pmb{x})\\right\\}=:s(\\theta)}\\end{array}$ implies ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\left(\\nabla_{x}c(s(\\theta),\\theta)+\\frac{1}{L_{s}}\\nabla_{x}\\mathcal{R}(s(\\theta))\\right)\\left(s(\\theta^{\\prime})-s(\\theta)\\right)\\geq0.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Symmetrically for $s\\left(\\theta^{\\prime}\\right)$ ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\left(\\nabla_{x}c(s(\\theta^{\\prime}),\\theta^{\\prime})+\\frac{1}{L_{s}}\\nabla_{x}\\mathcal{R}(s(\\theta^{\\prime}))\\right)\\left(s(\\theta)-s(\\theta^{\\prime})\\right)\\geq0.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Summing the above two inequalities yields ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\nabla_{x}c(s(\\theta),\\theta)+\\frac{1}{L_{s}}\\nabla_{x}\\mathcal{R}(s(\\theta))\\bigg)\\left(s(\\theta^{\\prime})-s(\\theta)\\right)+\\bigg(\\nabla_{x}c(s(\\theta^{\\prime}),\\theta^{\\prime})+\\frac{1}{L_{s}}\\nabla_{x}\\mathcal{R}(s(\\theta^{\\prime}))\\bigg)\\left(s(\\theta)-s(\\theta^{\\prime})\\right)\\geq0\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Rearranging terms, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\frac{1}{L_{s}}\\left(\\nabla_{x}\\mathcal{R}(s(\\theta))-\\nabla_{x}R\\left(s\\left(\\theta^{\\prime}\\right)\\right)\\right)\\left(s(\\theta)-s\\left(\\theta^{\\prime}\\right)\\right)\\leq\\left(\\nabla_{x}c(s(\\theta^{\\prime}),\\theta^{\\prime})-\\nabla_{x}c(s(\\theta),\\theta)\\right)\\left(s(\\theta)-s(\\theta^{\\prime})\\right)\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "It is a known fact that for any $\\cal{K}$ -strongly convex function $\\mathcal{R}$ it holds that $(\\nabla{\\mathcal{R}}(x)-\\nabla{\\mathcal{R}}(y))^{T}(x-y)\\geq$ $\\kappa\\|x-y\\|^{2},\\;\\forall x,y$ . This allows lower-bounding the left-hand side by $\\begin{array}{r}{\\frac{\\kappa}{L_{s}}\\left\\|s(\\theta)-s\\left(\\theta^{\\prime}\\right)\\right\\|^{2}}\\end{array}$ . ", "page_idx": 23}, {"type": "text", "text": "If $c$ is linear in $x$ we have $c(x,\\theta)=x\\cdot g(\\theta)$ for some appropriate function $g$ . Thus, $\\nabla_{x}c(x,\\theta)=g(\\theta)$ and $||\\nabla_{x}c(s(\\theta^{\\prime}),\\theta^{\\prime})-\\nabla_{x}c(s(\\theta),\\theta)||\\leq L_{c}||\\theta^{\\prime}-\\theta||$ and we can also upper bound the right-hand side using the generalized Cauchy-Schwarz inequality, see also [23, Appendix A.1]. ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\frac{\\kappa}{L_{s}}\\left\\|s(\\theta)-s\\left(\\theta^{\\prime}\\right)\\right\\|^{2}\\leq L_{c}\\cdot\\left\\|s(\\theta)-s\\left(\\theta^{\\prime}\\right)\\right\\|\\left\\|\\theta-\\theta^{\\prime}\\right\\|.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Equivalently, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\left\\|s(\\theta)-s\\left(\\theta^{\\prime}\\right)\\right\\|\\leq\\frac{L_{s}\\cdot L_{c}}{\\kappa}\\left\\|\\theta-\\theta^{\\prime}\\right\\|,\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "which was to be shown. ", "page_idx": 23}, {"type": "text", "text": "F.2 Proof of Lemma 2 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Proof. By Fubini, $\\begin{array}{r}{\\int\\int\\hat{y}(\\pmb{\\mathscr{v}}(\\theta),\\theta)\\tilde{p}\\,d y d\\,\\tilde{P}_{\\pmb{\\mathscr{v}}}\\;=\\;\\int\\int\\hat{y}(\\pmb{\\mathscr{v}}(\\theta),\\theta)\\tilde{p}\\,d\\tilde{P}_{\\pmb{\\mathscr{v}}}\\,d y}\\end{array}$ . For brevity, set $\\tilde{\\pmb{\\Omega}}(\\theta)\\;=\\;$ $\\int\\pmb{\\mathfrak{v}}(\\theta)\\tilde{P}_{\\pmb{\\mathfrak{v}}}d\\pmb{\\mathfrak{v}}$ .To prove that $p(\\tilde{\\pmb{\\mathscr{D}}}(\\theta),\\theta)$ is Lipschitz-continuos, we will proceed as follows. First, we will show that $p(\\tilde{\\pmb{\\mathscr{D}}}(\\theta),\\cdot)$ is Lipschitz-continuous. Second, we will show that $p(\\cdot,\\theta)$ is Lipschitzcontinuous. Third, we will show that the Lipschitz-continuity of $p(\\tilde{\\pmb{\\mathscr{D}}}(\\theta),\\theta)$ follows from the first and second result. ", "page_idx": 23}, {"type": "text", "text": ". To show that $p(\\tilde{\\pmb{\\mathscr{D}}}(\\theta),\\cdot)$ is Lipschitz-continuous with Lipschitz-constant $L_{\\tilde{\\mathcal{D}}}L_{p}$ , first observe that this holds if $\\tilde{\\pmb{v}}(\\theta)$ and $p(\\tilde{\\pmb{v}},\\cdot)$ are both Lipschitz-continuous with Lipschitz-constants $L_{\\tilde{v}}$ and $L_{p}$ , respectively, since ", "page_idx": 24}, {"type": "equation", "text": "$$\n||p(\\tilde{\\boldsymbol{\\vartheta}}(\\boldsymbol{\\theta}),\\cdot)-p(\\tilde{\\boldsymbol{\\vartheta}}(\\boldsymbol{\\theta}^{\\prime}),\\cdot)||_{2}\\leq L_{p}||\\tilde{\\boldsymbol{\\vartheta}}(\\boldsymbol{\\theta})-\\tilde{\\boldsymbol{\\vartheta}}(\\boldsymbol{\\theta}^{\\prime})||_{2}\\leq L_{p}L_{\\tilde{\\boldsymbol{\\vartheta}}}||\\boldsymbol{\\theta}-\\boldsymbol{\\theta}^{\\prime}||_{2}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "The first premise of the above statement holds per assumption. Let us now show that the second premise for the above statement holds. ", "page_idx": 24}, {"type": "text", "text": "To show that $p(\\tilde{\\pmb{v}},\\cdot)$ is Lipschitz-continuous, first recall condition 2, by which we have that $p(\\tilde{\\pmb{\\mathscr{V}}},\\cdot)\\,=\\,p(x,\\theta)\\,=\\,\\sigma\\bigl(g(x,\\theta)\\bigr)$ with $\\sigma:\\mathbb{R}\\rightarrow[0,1]$ a sigmoid function. Further recall that the prediction function of a classifier $p(x,\\theta)$ is implicitly given by a loss function $\\ell(y,p(x,\\theta))$ as per definition 1. By assumption 1 we inter alia have that $\\nabla_{x}\\ell(y,p(x,\\theta))$ is Lipschitz-continuous in $x$ for all $y$ and $\\theta$ . By chain rule, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\nabla_{x}\\ell(y,p(x,\\theta))=\\nabla_{p}\\ell(y,p(x,\\theta))\\nabla_{x}p(x,\\theta).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Now note that we also have by condition 2 that $\\nabla_{p}\\ell(y,p(x,\\theta))$ is Lipschitz-continuous in $x$ . The Lipschitz-continuity of $\\nabla_{x}\\ell(y,p(x,\\theta))$ in $x$ per assumption 1 thus implies the Lipschitz-continuity of $\\nabla_{x}p(x,\\theta)$ in $x$ , because the first is a product of the second and another function that is Lipschitz-continuous in x. Recall that $\\mathcal{X}$ is bounded (Definition 2 and Condition 2). It is a known fact that any Lipschitz-continuous function is bounded on a bounded domain. We thus concluded that $\\nabla_{x}p(x,\\theta)$ is bounded on the whole domain $\\mathcal{X}$ . Any differentiable function is Lipschitz-continuous if and only if its gradient is bounded. See [95], for instance. We can thus conclude that $p(x,\\theta)$ is Lipschitz-continuous in $x$ for all $y\\in\\mathcal{Y}$ and $\\theta\\in\\Theta$ . ", "page_idx": 24}, {"type": "text", "text": "2. Let us now show that $p(\\cdot,\\theta)$ is Lipschitz-continuous, too. By assumption 2, we have that $\\nabla_{\\theta}\\ell(Y,p(x,\\theta))$ is Lipschitz-continuous in $\\theta$ . With reasoning analogous to 1 (b), it follows that $p(x,\\theta)$ is Lipschitz-continuous in $\\theta$ for all $y\\in\\mathcal{Y}$ and all $x\\in\\mathcal{X}$ . ", "page_idx": 24}, {"type": "text", "text": "3. It remains to be proven that the Lipschitz-continuity of $p(\\tilde{\\mathfrak{U}}(\\theta),\\theta):\\mathcal{X}\\times\\Theta\\to[0,1]$ follows from those of $p(x,\\cdot):\\mathcal{X}\\rightarrow[0,1]$ and $p(\\cdot,\\theta):\\Theta\\to[0,1]$ . To do so, denote by $L_{x}$ the Lipschitz-constant of $p(x,\\cdot)$ and by $L_{\\theta}$ the Lipschitz-constant of $p(\\cdot,\\theta)$ . ", "page_idx": 24}, {"type": "text", "text": "First note $\\forall\\theta,\\tilde{\\theta}\\in\\Theta;\\,\\forall x,\\tilde{x}\\in\\mathcal{X}$ : ", "page_idx": 24}, {"type": "equation", "text": "$$\n||p(\\theta,x)-p(\\tilde{\\theta},\\tilde{x})||_{2}\\leq||p(\\theta,x)-p(\\theta,\\tilde{x})+p(\\theta,\\tilde{x})-p(\\tilde{\\theta},\\tilde{x})||_{2}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "By triangle inequality, we get ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{||p(\\theta,x)-p(\\tilde{\\theta},\\tilde{x})||_{2}\\leq||p(\\theta,x)-p(\\theta,\\tilde{x})||_{2}+||p(\\theta,\\tilde{x})-p(\\tilde{\\theta},\\tilde{x})||_{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Exploiting the Lipschitz-continuity of $p(x,\\cdot)$ and $p(\\cdot,\\theta)$ allows us to upper bound this expression by ", "page_idx": 24}, {"type": "equation", "text": "$$\nL_{\\theta}||x-\\tilde{x}||_{2}+L_{x}||\\theta-\\tilde{\\theta}||_{2},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "which eventually delivers ", "page_idx": 24}, {"type": "equation", "text": "$$\n||p(\\theta,x)-p(\\widetilde{\\theta},\\widetilde{x})||_{2}\\leq\\operatorname*{sup}\\{L_{\\theta},L_{x}\\}(||x-\\widetilde{x}||_{2}+||\\theta-\\widetilde{\\theta}||_{2}).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "We conclude that $p(x,\\theta)$ is Lipschitz-continuous with Lipschitz-constant $\\operatorname*{sup}\\{L_{\\theta},L_{x}\\}$ if $p(x,\\cdot)$ and $p(\\cdot,\\theta)$ are Lipschitz-continuous with Lipschitz constants $L_{\\theta}$ and $L_{x}$ , respectively. ", "page_idx": 24}, {"type": "text", "text": "F.3 Proof of Theorem 1 ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Proof. We will first prove the Lipschitz-continuity of the greedy sample adaption $f:\\Theta\\!\\times\\!\\mathcal{P}\\!\\times\\!\\mathbb{N}\\to\\mathcal{P}$ The strategy of the proof is as follows. We first show that 1. $f(\\theta,\\cdot,\\cdot)$ , 2. $f(\\cdot,\\mathbb{P}(Y,X),\\cdot)$ , and 3. $f(\\cdot,\\cdot,n)$ are Lipschitz-continuos with Lipschitz constants $L_{\\theta},\\,L_{\\mathbb{P}}$ , and $L_{n}$ , respectively. We then show in 4. that the Lipschitz-continuity of $f(\\theta,\\mathbb{P}(Y,X),n)$ follows with Lipschitz constant $L=\\operatorname*{max}\\{L_{\\theta},L_{\\mathbb{P}},L_{n}\\}$ . ", "page_idx": 25}, {"type": "text", "text": "We prove the Lipschitz-continuity of $f(\\theta,\\cdot,\\cdot)$ given conditions 1, 2, and 5. To show that $f(\\theta,\\mathbb{P}(Y,X),n)$ is Lipschitz-continuous in $\\theta$ , it is sufficient to show that ", "page_idx": 25}, {"type": "equation", "text": "$$\nf({\\boldsymbol{\\mathfrak{U}}}(\\theta),\\theta)=\\int\\int\\,1(x={\\boldsymbol{\\mathfrak{U}}}(\\theta))\\cdot\\mathbf{\\mathfrak{L}}({\\boldsymbol{\\mathfrak{U}}}(\\theta),\\theta)\\,{\\boldsymbol{\\tilde{p}}}\\,d y\\ {\\tilde{P}}_{\\boldsymbol{\\mathfrak{U}}}\\,d x\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "is Lipschitz-continuous in $\\theta$ . First note that by Conditions 1 and 4 we can directly infer that $\\pmb{\\nabla}(\\theta)$ is Lipschitz-continuous through lemma 1. In the remainder of the proof, the strategy is as follows. We first show that $f(\\pmb{\\ v},\\cdot)$ is Lipschitz-continuos and then demonstrate the Lipschitz-continuity of $f(\\pmb{\\ v}(\\theta),\\theta)$ in $\\theta$ . With reasoning analogous to argument (3.) in the proof of lemma 2, it then follows that $f(\\pmb{\\ v}(\\theta),\\theta)$ is Lipschitz-conitnuous on $\\Theta\\times\\Theta$ . ", "page_idx": 25}, {"type": "text", "text": "(a) We start by showing that the function ", "page_idx": 25}, {"type": "equation", "text": "$$\nf(\\mathfrak{V},\\cdot)=\\int\\int\\,1(x=\\mathfrak{V})\\cdot\\mathfrak{V}(\\mathfrak{V},\\theta)\\,\\tilde{p}\\,d y\\ \\tilde{P}_{\\overline{{\\mathfrak{V}}}}\\,d x\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "is Lipschitz-continuous in $\\pmb{v}$ .Apply Fubini to get ", "page_idx": 25}, {"type": "equation", "text": "$$\nf(\\mathfrak{V},\\cdot)=\\int\\int\\,1(x=\\mathfrak{V})\\cdot\\mathfrak{L}(\\mathfrak{V},\\theta)\\,\\tilde{P}_{\\perp}\\,d x\\ \\tilde{p}\\,d y\\ .\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Condition 3 implies that features are drawn according to ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\tilde{c}:\\left\\{\\mathcal{X}\\times\\Theta\\right.\\ \\rightarrow[0,1]}\\\\ {(x,\\theta)\\qquad\\longmapsto\\frac{\\exp(c(x,\\theta))}{\\int_{x^{\\prime}}\\exp(c(x^{\\prime},\\theta))d\\mu(x)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "see definition 2. That is, $\\begin{array}{r}{\\int1(x\\,=\\,\\mathfrak{V})\\,\\cdot\\,\\mathfrak{L}(\\mathfrak{V},\\theta)\\,\\tilde{P}_{\\perp}\\,d x\\,=\\,\\tilde{c}(\\mathfrak{V},\\theta)\\,\\cdot\\,\\mathfrak{L}(\\tilde{c}(\\mathfrak{V},\\theta),\\theta)}\\end{array}$ . Per condition 5, $c(\\boldsymbol{x},\\theta)$ is linear in $x$ and thus Lipschitz-continuous in $x$ . Further note that the mapping $c\\to\\tilde{c}$ is a softmax function, which is continuosly differentiable and thus Lipschitz-continuous. We thus conclude with the argument (1.) in the proof of lemma 2, see equation 16, that $\\tilde{c}(\\boldsymbol{x},\\theta)$ is Lipschitz-continuous in $x$ , since it is a composition of two Lipschitz-continuous functions. ", "page_idx": 25}, {"type": "text", "text": "Now recall that by Conditions 1 and 5 we can infer that $\\pmb{\\nabla}(\\theta)$ is Lipschitz-continuous through lemma 1. This and Condition 2 imply that we can apply lemma 2, which delivers that ", "page_idx": 25}, {"type": "equation", "text": "$$\nf(\\pmb{\\mathscr{v}})=\\int\\tilde{c}(\\pmb{\\mathscr{v}},\\theta)\\cdot\\pmb{\\mathfrak{s}}(\\tilde{c}(\\pmb{\\mathscr{v}},\\theta),\\theta)\\tilde{p}\\,d y\\,=\\tilde{c}(\\pmb{\\mathscr{v}},\\theta)\\int\\hat{y}(\\pmb{\\mathscr{v}}(\\theta),\\theta)\\tilde{p}\\,d y=\\tilde{c}(\\pmb{\\mathscr{v}},\\theta)p(\\pmb{\\mathscr{v}},\\theta).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "and that $p(\\pmb{\\mathfrak{v}},\\pmb{\\theta})$ is Lipschitz-continuous in both arguments. Now note that both $\\tilde{c}:$ $\\mathcal{X}\\times\\Theta\\rightarrow[0,1]$ and $p:\\mathcal{X}\\times\\Theta\\to[0,1]$ are both bounded from above by 1. We thus conclude by triangle inquality that $f(\\pmb{\\ v})$ is Lipschitz-continuous in $\\pmb{v}$ .Explicitly, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{||f(\\boldsymbol{\\Psi})-f(\\boldsymbol{\\Psi^{\\prime}})||_{2}}\\\\ &{=||\\tilde{c}(\\boldsymbol{\\Psi},\\boldsymbol{\\theta})p(\\boldsymbol{\\Psi},\\boldsymbol{\\theta})-\\tilde{c}(\\boldsymbol{\\Psi^{\\prime}},\\boldsymbol{\\theta})p(\\boldsymbol{\\Psi^{\\prime}},\\boldsymbol{\\theta})||_{2}}\\\\ &{\\leq||\\tilde{c}(\\boldsymbol{\\Psi},\\boldsymbol{\\theta})p(\\boldsymbol{\\Psi},\\boldsymbol{\\theta})-\\tilde{c}(\\boldsymbol{\\Psi},\\boldsymbol{\\theta})p(\\boldsymbol{\\Psi^{\\prime}},\\boldsymbol{\\theta})||_{2}+||\\tilde{c}(\\boldsymbol{\\Psi},\\boldsymbol{\\theta})p(\\boldsymbol{\\Psi^{\\prime}},\\boldsymbol{\\theta})-\\tilde{c}(\\boldsymbol{\\Psi^{\\prime}},\\boldsymbol{\\theta})p(\\boldsymbol{\\Psi^{\\prime}},\\boldsymbol{\\theta})||_{2}}\\\\ &{=||\\tilde{c}(\\boldsymbol{\\Psi},\\boldsymbol{\\theta})\\left[p(\\boldsymbol{\\Psi},\\boldsymbol{\\theta})-p(\\boldsymbol{\\Psi^{\\prime}},\\boldsymbol{\\theta})\\right]||_{2}+||p(\\boldsymbol{\\Psi^{\\prime}},\\boldsymbol{\\theta})\\left[\\tilde{c}(\\boldsymbol{\\Psi},\\boldsymbol{\\theta})-p(\\boldsymbol{\\Psi^{\\prime}},\\boldsymbol{\\theta})\\right]||_{2}}\\\\ &{\\leq||p(\\boldsymbol{\\Psi},\\boldsymbol{\\theta})-p(\\boldsymbol{\\Psi^{\\prime}},\\boldsymbol{\\theta})||_{2}+||\\tilde{c}(\\boldsymbol{\\Psi},\\boldsymbol{\\theta})-p(\\boldsymbol{\\Psi^{\\prime}},\\boldsymbol{\\theta})||_{2}}\\\\ &{\\leq L_{p}||\\boldsymbol{\\Psi}-\\boldsymbol{\\Psi^{\\prime}}||_{2}+L_{\\tilde{c}}||\\boldsymbol{\\Psi}-\\boldsymbol{\\Psi^{\\prime}}||_{2}}\\\\ &{\\leq(L_{p}+L_{\\tilde{c}})||\\boldsymbol{\\Psi}-\\boldsymbol{\\Psi^{\\prime}}||_{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $L_{p}$ and $L_{\\tilde{c}}$ denote the Lipschitz constants of $\\tilde{c}$ and $p$ , respectively. ", "page_idx": 25}, {"type": "text", "text": "(b) To show the Lipschitz-continuity of $f(\\pmb{\\ v}(\\theta),\\theta)$ in $\\theta$ , first note the Lipschitz-continuity $f(\\pmb{\\ v}(\\theta),\\cdot)$ in $\\theta$ directly follows from the facts that 1) $\\pmb{\\nabla}(\\theta)$ is Lipschitz-continuous, 2) $f(\\pmb{\\ v},\\cdot)$ is Lipschitz-continuous in $\\pmb{v}$ ,and 3) any composition of Lipschitz-continuous functions is Lipschitz-continuous. We have proven 1) and 2) right above. For a proof of 3), see equation 16 in the proof of lemma 2. ", "page_idx": 26}, {"type": "text", "text": "What remains to be shown is the Lipschitz-continuity of $f(\\cdot,\\theta)$ , which translates to the Lipschitz-continuity of ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\int\\,1(x=\\mathfrak{U})\\int\\,\\mathfrak{L}(\\mathfrak{V},\\theta)\\,\\tilde{p}\\,d y\\ \\tilde{P}_{\\perp}\\,d x.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "in $\\theta$ , which in turn translates to the Lipschitz-continuity of the inner integral. Condition 2 and lemma 2 deliver ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\int\\,\\mathfrak{s}(\\mathfrak{v},\\theta)\\,\\tilde{p}\\,d y=\\int\\,\\hat{y}(\\mathfrak{v}(\\theta),\\theta)\\tilde{p}\\,d y=p(\\mathfrak{v},\\theta)\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "with $p(\\pmb{\\ v},\\pmb{\\theta})$ being Lipschitz-continuous in $\\theta$ . ", "page_idx": 26}, {"type": "text", "text": "(c) It remains to be shown that $f(\\pmb{\\ v}(\\theta),\\theta)$ is Lipschitz-continuous on $\\Theta\\times\\Theta$ . This follows from reasoning analogous to the proof of lemma 2, resulting in ", "page_idx": 26}, {"type": "equation", "text": "$$\n||f(\\boldsymbol{\\mathfrak{U}}(\\boldsymbol{\\theta}),\\boldsymbol{\\theta})-f(\\widetilde{\\boldsymbol{\\mathfrak{U}}}(\\boldsymbol{\\theta}),\\widetilde{\\boldsymbol{\\theta}})||_{2}\\le\\operatorname*{sup}\\{L_{\\boldsymbol{\\theta}},L_{\\boldsymbol{\\mathfrak{U}}(\\boldsymbol{\\theta})}\\}(||\\boldsymbol{\\theta}-\\widetilde{\\boldsymbol{\\theta}}||_{2}+||\\boldsymbol{\\mathfrak{U}}(\\boldsymbol{\\theta})-\\widetilde{\\boldsymbol{\\vartheta}}(\\boldsymbol{\\theta})||_{2}),\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "with $L_{\\theta}$ and $L_{\\pmb{\\mathscr{v}}(\\theta)}$ being the Lipschitz-constants of $\\theta$ and $\\pmb{\\nabla}(\\theta)$ , respectively.   \nThis concludes the proof that $f(\\theta,\\mathbb{P}(Y,X),n)$ is Lipschitz-continuous in $\\theta$ . ", "page_idx": 26}, {"type": "text", "text": "2. To see that $f(\\cdot,\\mathbb{P}(Y,X),\\cdot)$ is Lipschitz-continuous with respect to Wasserstein-1-distance on both domain $\\mathcal{P}$ and codomain $\\mathcal{P}$ , recall the definition of Wasserstein-p-distancees [108]. Let $(\\mathcal{X},d)$ be a metric space, and let $p\\in[1,\\infty)$ . For any two probability measures $\\mu,\\nu$ on $\\mathcal{X}$ , the Wasserstein distance of order $p$ between $\\mu$ and $\\nu$ is defined by ", "page_idx": 26}, {"type": "equation", "text": "$$\nW_{p}(\\mu,\\nu)=\\left(\\operatorname*{inf}_{\\pi\\in\\Pi(\\mu,\\nu)}\\int_{\\mathcal{X}}d(x,y)^{p}d\\pi(x,y)\\right)^{1/p}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "$\\Pi(\\mu,\\nu)$ denotes the set of all joint measures on $\\mathcal{X}\\times\\mathcal{X}$ with marginals $\\mu$ and $\\nu$ . For $p=1$ and empirical distributions $\\mathbb{P}(Z)$ and $\\mathbb{P}^{\\prime}(Z^{\\prime})$ this translates to ", "page_idx": 26}, {"type": "equation", "text": "$$\nW_{1}\\big(\\mathbb{P}(Z),\\mathbb{P}^{\\prime}(Z^{\\prime})\\big)=\\operatorname*{min}\\left\\{\\sum_{i,j}\\pi_{i,j}\\,d(z_{i},z_{j}^{\\prime})\\,:\\,\\pi_{i,j}\\geq0,\\,\\sum_{i}\\pi_{i,j}=\\beta_{j},\\,\\sum_{j}\\pi_{i,j}=\\alpha_{i}\\right\\}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "with $\\pi_{i,j}$ a joint measure on $Z$ and $Z^{\\prime}$ and $\\alpha_{i}$ and $\\beta_{j}$ corresponding to marginal measures of $Z$ and ${\\dot{Z}}^{\\prime}$ , respectively. ", "page_idx": 26}, {"type": "text", "text": "It becomes evident that any marginal change in $f(\\theta,\\mathbb{P}(Y,X),n)$ caused by a change $\\mathbb{P}(Y,X)$ is essentially \ud835\udc5b+1. ", "page_idx": 26}, {"type": "text", "text": "That is, ", "page_idx": 26}, {"type": "equation", "text": "$$\n{\\frac{\\delta f(\\theta,\\mathbb{P}(Y,X),n)}{\\delta\\mathbb{P}(Y,X)}}={\\frac{n}{n+1}},\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "which analytically follows from ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f(\\theta,\\mathbb{P}(Y,X),n)}\\\\ &{=\\int\\int\\frac{1(x=\\ v(\\theta))\\cdot\\mathfrak{s}(\\equiv(\\theta),\\theta)\\,+n\\,\\mathbb{P}(Y=1,X=x)}{n+1}\\;\\tilde{p}\\,d y\\;\\tilde{P}_{\\perp}\\,d x}\\\\ &{=\\int\\int\\frac{1(x=\\ v(\\theta))\\cdot\\mathfrak{s}(\\equiv(\\theta),\\theta)}{n+1}\\;\\tilde{p}\\,d y\\;\\tilde{P}_{\\perp}\\,d x+\\frac{n\\,\\mathbb{P}(Y=1,X=x)}{n+1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "The partial derivative in equation 32 is trivially upper-bounded by 1. It is a known fact that differentiable functions are Lipschitz-continuous if and only if the gradient is upper bounded, see [95, page 161], for instance. ", "page_idx": 26}, {"type": "text", "text": "3. Choose $n,n^{\\prime}\\in\\mathbb{N}$ such that $n\\ne n^{\\prime}$ arbitrarily. It is self-evident that for fixed $x$ ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\frac{1(x=\\nabla(\\theta))\\cdot\\mathbf{5}(\\nabla(\\theta),\\theta)\\,+\\,n\\,\\mathbb{P}(Y=1,\\,X=x)}{n+1}-\\frac{1(x=\\nabla(\\theta))\\cdot\\mathbf{5}(\\nabla(\\theta),\\theta)\\,+\\,n\\,\\mathbb{P}(Y=1,\\,X=x)}{n+1}\\le1.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "And thus also for any $x$ ", "page_idx": 27}, {"type": "equation", "text": "$$\nf(\\theta,\\mathbb{P}(Y,X),n)-f(\\theta,\\mathbb{P}(Y,X),n^{\\prime})\\leq1.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Since $\\operatorname{supp}(Z)=\\operatorname{supp}(Z^{\\prime})$ with $Z\\sim f(\\theta,\\mathbb{P}(Y,X),n)$ and $Z^{\\prime}\\sim f(\\theta,\\mathbb{P}(Y,X),n^{\\prime})$ , we have ", "page_idx": 27}, {"type": "equation", "text": "$$\nW_{1}(f(\\theta,\\mathbb{P}(Y,X),n),f(\\theta,\\mathbb{P}(Y,X),n^{\\prime}))\\leq1\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "as well as $|n\\!-\\!n^{\\prime}|\\geq1$ , from which the assertion that $f(\\theta,\\mathbb{P}(Y,X),n)$ is Lipschitz-continuous in $n$ directly follows. ", "page_idx": 27}, {"type": "text", "text": "4. With reasoning analogous to (3.) in the proof of lemma 2, we have that the Lipschitzcontinuity of $f(\\theta,\\mathbb{P}(Y,X),n)$ follows from the Lipschitz-continuity of 1. $f(\\theta,\\cdot,\\cdot)$ , 2. $f(\\cdot,\\mathbb{P}(Y,X),\\cdot)$ , and 3. $f(\\cdot,\\cdot,n)$ . That is, ", "page_idx": 27}, {"type": "equation", "text": "$$\nW_{1}(f(\\theta,\\mathbb{P},n),f(\\theta^{\\prime},\\mathbb{P}^{\\prime},n^{\\prime}))\\leq\\operatorname*{max}\\{L_{\\theta},L_{\\mathbb{P}},L_{n}\\}\\cdot(||\\theta-\\theta^{\\prime}||_{2}+W_{1}(\\mathbb{P},\\mathbb{P}^{\\prime})+||n-n^{\\prime}||_{2}),\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "from which the assertion follows with $p=1$ for the $p$ -norm. ", "page_idx": 27}, {"type": "text", "text": "What remains to be proven is the Lipschitz-continuity of the non-greedy sample adaption function $f_{n}:\\Theta\\times\\mathcal{P}\\xrightarrow{}\\mathcal{P}$ with $f_{n}(\\theta,\\mathbb{P}(Y,X))=\\mathbb{P}^{\\prime}(Y,X)$ induced by ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{l}{^{\\prime}(Y=1,X=x)}\\\\ {=\\displaystyle\\int\\int\\frac{1(x=\\nabla(\\theta))\\cdot\\mathbf{x}(\\nabla(\\theta),\\theta)\\,+\\,n_{0}\\,\\mathbb{P}(Y=1,X=x)-1(x=\\nabla^{-}(\\mathbb{P}(Y,X)))\\cdot\\mathbf{x}(\\nabla^{-}(\\mathbb{P}(Y,X)),\\theta)}{n_{0}}\\,\\,\\,\\tilde{P}_{\\Im}\\,d y\\,\\,\\tilde{P}_{\\nabla}\\,d x}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "The reasoning is completely analogous to the greedy sample adaption function $f$ , see 4. above. In particular, we can directly transfer the proof of 1. $f(\\theta,\\cdot,\\cdot)$ being Lipschitz-continuous. What remains to be shown is that the Lipschitz-continuity in $\\mathbb{P}\\in\\mathcal{P}$ also holds for $f_{n}$ . This translates to showing that $\\pmb{v}^{-}$ is Lipschitz-continuous in $\\mathbb{P}$ , since we have the Lipschitz-continuity of $n_{0}\\cdot\\mathbb{P}(X,Y)$ with analogous reasoning as in 2. above. However, note that the Lipschitz-constant is not necessarily the same, since the partial derivative of $f_{n}$ also includes the indirect effect through $\\pmb{\\nabla}^{-}$ and $\\mathfrak{s}$ . ", "page_idx": 27}, {"type": "text", "text": "To see that $\\mathfrak{v}^{-}$ is Lipschitz-continuous, note that for two arbitrary $\\mathbb{P},\\mathbb{P}^{\\prime}\\in\\mathcal{P}$ we have that ", "page_idx": 27}, {"type": "equation", "text": "$$\n||\\Psi^{-}(\\mathbb{P})-\\vartheta^{-}(\\mathbb{P}^{\\prime})||_{2}=||\\int X d\\mathbb{P}-\\int X^{\\prime}d\\mathbb{P}^{\\prime}||_{2}=\\int(x-x^{\\prime})d\\rho(x,x^{\\prime})\\leq\\int|x-x^{\\prime}|d\\rho(x,x^{\\prime}),\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where $\\rho(x,x^{\\prime})$ is any joint probability measure on $\\mathcal{X}\\times\\mathcal{X}$ . Now recall that the Wasserstein-1-distance is defined as the infimum of $\\int|x-x^{\\prime}|d\\rho(x,x^{\\prime})$ with respect to $\\rho(x,x^{\\prime})$ . We conclude that ", "page_idx": 27}, {"type": "equation", "text": "$$\n||\\mathfrak{V}^{-}(\\mathbb{P})-\\mathfrak{V}^{-}(\\mathbb{P}^{\\prime})||_{2}\\leq L_{\\mathfrak{V}^{-}}\\cdot W_{1}(\\mathbb{P},\\mathbb{P}^{\\prime})\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "with $L_{\\pmb{\\mathscr{D}}^{-}}$ a constant. ", "page_idx": 27}, {"type": "text", "text": "The Lipschitz-continuity of $f_{n}$ then follows from the Lipschitz-continuity of $f_{n}$ in $\\theta$ and the Lipschitzcontinuity in $\\mathbb{P}$ with the argument in 4. ", "page_idx": 27}, {"type": "text", "text": "F.4 Proof of Theorem 2 ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Proof. The structure of the proof is analogous to the proof of theorem 1. We show that 1. $f(\\theta,\\cdot,\\cdot)$ , 2. $f(\\cdot,\\mathbb{P}(Y,X),\\cdot)$ , and 3. $f(\\cdot,\\cdot,n)$ are Lipschitz-continuos with Lipschitz constants $L_{\\theta},\\,L_{\\mathbb{P}}$ , and ", "page_idx": 27}, {"type": "text", "text": "$L_{n}$ , respectively. We then show in 4. that the Lipschitz-continuity of $f(\\theta,\\mathbb{P}(Y,X),n)$ follows with Lipschitz constant $L=\\operatorname*{max}\\{L_{\\theta},L_{\\mathbb{P}},L_{n}\\}$ . Since none of conditions 1 through 5 were required to show 1., 2., and 4., in the proof of theorem 1, we only need to show that 1. also holds under conditions 2, 3, and 4. ", "page_idx": 28}, {"type": "text", "text": "To show that $f(\\theta,\\mathbb{P}(Y,X),n)$ is Lipschitz-continuous in $\\theta$ , it is sufficient to show that ", "page_idx": 28}, {"type": "equation", "text": "$$\nf({\\boldsymbol{\\mathfrak{U}}}(\\theta),\\theta)=\\int\\int\\,1(x={\\boldsymbol{\\mathfrak{U}}}(\\theta))\\cdot\\,\\exists({\\boldsymbol{\\mathfrak{U}}}(\\theta),\\theta)\\,{\\boldsymbol{\\tilde{p}}}\\,d y\\ {\\tilde{P}}_{\\boldsymbol{\\mathfrak{U}}}\\,d x\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "is Lipschitz-continuous in $\\theta$ . ", "page_idx": 28}, {"type": "text", "text": "In the remainder of the proof, the strategy is as follows. We first show that $f(\\pmb{\\ v},\\cdot)$ is Lipschitzcontinuos and then demonstrate the Lipschitz-continuity of $f(\\pmb{\\ v}(\\theta),\\theta)$ in $\\theta$ . With reasoning analogous to argument (3.) in the proof of lemma 2, it then follows that $f(\\pmb{\\ v}(\\theta),\\theta)$ is Lipschitz-conitnuous on $\\Theta\\times\\Theta$ . ", "page_idx": 28}, {"type": "text", "text": "We start by showing that the function ", "page_idx": 28}, {"type": "equation", "text": "$$\nf(\\mathfrak{V},\\cdot)=\\int\\int\\,1(x=\\mathfrak{V})\\cdot\\mathfrak{V}(\\mathfrak{V},\\theta)\\,\\tilde{p}\\,d y\\ \\tilde{P}_{\\perp}\\,d x\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "is Lipschitz-continuous in $\\pmb{v}$ .Apply Fubini to get ", "page_idx": 28}, {"type": "equation", "text": "$$\nf(\\mathfrak{V},\\cdot)=\\int\\int\\,1(x=\\mathfrak{V})\\cdot\\mathfrak{L}(\\mathfrak{V},\\theta)\\,\\tilde{P}_{\\perp}\\,d x\\ \\tilde{p}\\,d y\\ .\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Condition 3 implies that features are drawn according to ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\tilde{c}:\\left\\{\\mathcal{X}\\times\\Theta\\right.\\,\\,\\to\\,[0,1]\\qquad\\qquad\\qquad\\qquad\\qquad\\,\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "see definition 2. That is, $\\begin{array}{r}{\\int1(x=\\mathfrak{v})\\cdot\\mathfrak{s}(\\mathfrak{v},\\theta)\\,\\tilde{P}_{\\mathfrak{v}}\\,d x=\\tilde{c}(\\mathfrak{L},\\theta)\\cdot\\mathfrak{s}(\\tilde{c}(\\mathfrak{L},\\theta),\\theta)}\\end{array}$ . Per condition 4, $c(\\boldsymbol{x},\\theta)$ has bounded gradients with respect to $x$ , which implies that $c(\\boldsymbol{x},\\theta)$ is Lipschitz-continuous in $x$ Further note that the mapping $c\\to\\tilde{c}$ is a softmax function, which is continuosly differentiable and thus Lipschitz-continuous. We thus conclude with the argument (1.) in the proof of lemma 2, see equation 16, that $\\tilde{c}(\\boldsymbol{x},\\theta)$ is Lipschitz-continuous in $x$ , since it is a composition of two Lipschitzcontinuous functions. ", "page_idx": 28}, {"type": "text", "text": "We now need to verify that $\\int\\pmb{\\mathfrak{v}}(\\theta)\\tilde{P}_{\\pmb{\\mathfrak{v}}}d\\pmb{\\mathfrak{v}}$ is Lipschitz such that we can apply lemma 2. By condition 3 we have that $\\begin{array}{r}{\\int\\pmb{\\mathfrak{O}}(\\theta)\\tilde{P}_{\\pmb{\\mathfrak{v}}}d\\pmb{\\mathfrak{v}}=\\tilde{c}(x,\\theta)}\\end{array}$ , which is Lipschitz-continuous in $\\theta$ per condition 4. ", "page_idx": 28}, {"type": "text", "text": "Condition 2 and lemma 2 then directly deliver that ", "page_idx": 28}, {"type": "equation", "text": "$$\nf(\\pmb{\\mathscr{v}})=\\int\\tilde{c}(\\pmb{\\mathscr{v}},\\pmb{\\theta})\\cdot\\pmb{\\mathfrak{\\mathfrak{z}}}(\\tilde{c}(\\pmb{\\mathscr{v}},\\pmb{\\theta}),\\pmb{\\theta})\\tilde{p}\\,d y\\,=\\tilde{c}(\\pmb{\\mathscr{v}},\\pmb{\\theta})\\int\\hat{y}(\\pmb{\\mathscr{v}}(\\pmb{\\theta}),\\pmb{\\theta})\\tilde{p}\\,d y=\\tilde{c}(\\pmb{\\mathscr{v}},\\pmb{\\theta})p(\\pmb{\\nabla}\\pmb{\\theta},\\pmb{\\theta})\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "with $p(\\pmb{\\mathfrak{v}},\\pmb{\\theta})$ Lipschitz continuous in both arguments. Now note that both $\\widetilde{c}:\\mathcal{X}\\times\\Theta\\rightarrow[0,1]$ and $p:\\mathcal{X}\\times\\Theta\\to[0,1]$ are both bounded from above by 1. We thus conclude by triangle inquality that $f(\\pmb{\\ v})$ is Lipschitz-continuous in $\\pmb{v}$ ,analogous to the proof of theorem 1. Explicitly, ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{||f(\\pmb{\\mathscr{u}})-f(\\pmb{\\mathscr{v}}^{\\prime})||_{2}}\\\\ &{=||\\tilde{c}(\\pmb{\\mathscr{v}},\\theta)p(\\pmb{\\mathscr{v}},\\theta)-\\tilde{c}(\\pmb{\\mathscr{v}}^{\\prime},\\theta)p(\\pmb{\\mathscr{v}}^{\\prime},\\theta)||_{2}}\\\\ &{\\leq||\\tilde{c}(\\pmb{\\mathscr{v}},\\theta)p(\\pmb{\\mathscr{v}},\\theta)-\\tilde{c}(\\pmb{\\mathscr{v}},\\theta)p(\\pmb{\\mathscr{v}}^{\\prime},\\theta)||_{2}+||\\tilde{c}(\\pmb{\\mathscr{v}},\\theta)p(\\pmb{\\mathscr{v}}^{\\prime},\\theta)-\\tilde{c}(\\pmb{\\mathscr{v}}^{\\prime},\\theta)p(\\pmb{\\mathscr{v}}^{\\prime},\\theta)||_{2}}\\\\ &{=||\\tilde{c}(\\pmb{\\mathscr{v}},\\theta)\\left[p(\\pmb{\\mathscr{v}},\\theta)-p(\\pmb{\\mathscr{v}}^{\\prime},\\theta)\\right]||_{2}+||p(\\pmb{\\mathscr{v}}^{\\prime},\\theta)\\left[\\tilde{c}(\\pmb{\\mathscr{v}},\\theta)-p(\\pmb{\\mathscr{v}}^{\\prime},\\theta)\\right]||_{2}}\\\\ &{\\leq||p(\\pmb{\\mathscr{v}},\\theta)-p(\\pmb{\\mathscr{v}}^{\\prime},\\theta)||_{2}+||\\tilde{c}(\\pmb{\\mathscr{v}},\\theta)-p(\\pmb{\\mathscr{v}}^{\\prime},\\theta)||_{2}}\\\\ &{\\leq L_{p}||\\pmb{\\mathscr{v}}-\\pmb{\\mathscr{v}}^{\\prime}||_{2}+L_{\\tilde{c}}||\\pmb{\\mathscr{v}}-\\pmb{\\mathscr{v}}^{\\prime}||_{2}}\\\\ &{\\leq(L_{p}+L_{\\tilde{c}})||\\pmb{\\mathscr{v}}-\\pmb{\\mathscr{v}}^{\\prime}||_{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $L_{p}$ and $L_{\\tilde{c}}$ denote the Lipschitz constants of $\\tilde{c}$ and $p$ , respectively. ", "page_idx": 28}, {"type": "text", "text": "To show the Lipschitz-continuity of $f(\\pmb{\\ v}(\\theta),\\theta)$ in $\\theta$ , first note the Lipschitz-continuity $f(\\pmb{\\mathscr{v}}(\\theta),\\cdot)$ in $\\theta$ directly follows from the facts that 1) $\\pmb{\\nabla}(\\theta)$ is Lipschitz-continuous, 2) $f(\\pmb{\\ v},\\cdot)$ is Lipschitz-continuous ", "page_idx": 28}, {"type": "text", "text": "in $\\pmb{v}$ ,and 3) any composition of Lipschitz-continuous functions is Lipschitz-continuous. We have proven 1) and 2) right above. For a proof of 3), see equation 16 in the proof of lemma 2. ", "page_idx": 29}, {"type": "text", "text": "What remains to be shown is the Lipschitz-continuity of $f(\\cdot,\\theta)$ , which translates to the Lipschitzcontinuity of ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\int\\,1(x=\\mathfrak{V})\\int\\,\\mathfrak{L}(\\mathfrak{V},\\theta)\\,\\tilde{p}\\,d y\\ \\tilde{P}_{\\perp}\\,d x.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "in $\\theta$ , which in turn translates to the Lipschitz-continuity of the inner integral. Condition 2 and lemma 2 (which we can apply, since $\\int\\pmb{\\mathfrak{v}}(\\theta)\\tilde{P}_{\\pmb{\\mathfrak{v}}}d\\pmb{\\mathfrak{v}}$ is Lipschitz, see above) deliver ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\int\\,\\mathfrak{s}(\\mathfrak{L},\\theta)\\,\\tilde{p}\\,d y=\\int\\,\\hat{y}(\\mathfrak{L}(\\theta(\\theta),\\theta)\\tilde{p}\\,d y=p(\\mathfrak{L},\\theta)\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "with $p(\\pmb{\\ v},\\pmb{\\theta})$ being Lipschitz-continuous in $\\theta$ . ", "page_idx": 29}, {"type": "text", "text": "It remains to be shown that $f(\\pmb{\\ v}(\\theta),\\theta)$ is Lipschitz-continuous on $\\Theta\\times\\Theta$ . This follows from reasoning analogous to the proof of lemma 2, resulting in ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r}{||f(\\boldsymbol{\\mathfrak{U}}(\\boldsymbol{\\theta}),\\boldsymbol{\\theta})-f(\\widetilde{\\boldsymbol{\\mathfrak{U}}}(\\boldsymbol{\\theta}),\\widetilde{\\boldsymbol{\\theta}})||_{2}\\leq\\operatorname*{sup}\\{L_{\\boldsymbol{\\theta}},L_{\\boldsymbol{\\mathfrak{U}}(\\boldsymbol{\\theta})}\\}(||\\boldsymbol{\\theta}-\\widetilde{\\boldsymbol{\\theta}}||_{2}+||\\boldsymbol{\\mathfrak{U}}(\\boldsymbol{\\theta})-\\widetilde{\\boldsymbol{\\vartheta}}(\\boldsymbol{\\theta})||_{2}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "with $L_{\\theta}$ and $L_{\\pmb{\\mathscr{v}}(\\theta)}$ being the Lipschitz-constants of $\\theta$ and $\\pmb{\\nabla}(\\theta)$ , respectively. ", "page_idx": 29}, {"type": "text", "text": "This concludes the proof that $f(\\theta,\\mathbb{P}(Y,X),n)$ is Lipschitz-continuous in $\\theta$ . The Lipschitz-continuity of $f_{n}(\\theta,\\mathbb{P}(Y,X))$ directly follows, since the Lipschitz-continuity of $v^{-}(\\mathbb{P})$ has been shown in the proof of theorem 1. ", "page_idx": 29}, {"type": "text", "text": "F.5 Proof of Theorem 3 ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Proof. Choose $(\\theta,\\mathbb{P}),(\\theta^{\\prime},\\mathbb{P}^{\\prime})\\ \\in\\ \\Theta\\ \\times\\ \\mathcal{P}$ arbitrarily. Set $F(\\eta)\\;:=\\;\\mathbb{E}_{(Y,X)\\sim f_{n}(\\theta,\\mathbb{P})}\\,\\,\\,\\ell(Y,X,\\eta)$ and $F^{\\prime}(\\eta):=\\mathbb{E}_{(Y,X)\\sim f_{n}(\\theta^{\\prime},\\mathbb{P}^{\\prime})}\\ \\ell(Y,X,\\eta)$ . As integrals over $\\gamma$ -strongly convex functions, both $F$ and $F^{\\prime}$ are $\\gamma$ -strongly convex themselves. ", "page_idx": 29}, {"type": "text", "text": "Let $R_{1}:=R_{1}(\\theta,\\mathbb{P})$ and $R_{1}^{\\prime}:=R_{1}(\\theta^{\\prime},\\mathbb{P}^{\\prime})$ be first components of $R_{n}(\\theta,\\mathbb{P})$ and $R_{n}(\\theta^{\\prime},\\mathbb{P}^{\\prime})$ , respectively. Since, by construction, we know that $R_{1}$ is the unique minimizer of $F$ and that $R_{1}^{\\prime}$ is the unique minimizer of $F^{\\prime}$ , we can conclude that: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{F(R_{1})-F(R_{1}^{\\prime})\\geq(R_{1}-R_{1}^{\\prime})^{T}\\nabla F(R_{1}^{\\prime})+\\frac{\\gamma}{2}\\left\\|R_{1}-R_{1}^{\\prime}\\right\\|_{2}^{2}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r}{F(R_{1}^{\\prime})-F(R_{1})\\ge\\frac{\\gamma}{2}\\left\\|R_{1}-R_{1}^{\\prime}\\right\\|_{2}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Adding the above inequalities yields ", "page_idx": 29}, {"type": "equation", "text": "$$\n-\\gamma\\left\\|R_{1}-R_{1}^{\\prime}\\right\\|_{2}^{2}\\geq(R_{1}-R_{1}^{\\prime})^{T}\\nabla F(R_{1}^{\\prime})\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Now, consider the function $T(x,y):=(R_{1}-R_{1}^{\\prime})^{T}\\nabla\\ell(y,x,R_{1}^{\\prime})$ . Due to Cauchy-Schwarz inequality, we have that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\|T(x,y)-T(x^{\\prime},y^{\\prime})\\|_{2}\\leq\\left\\|R_{1}-R_{1}^{\\prime}\\right\\|_{2}\\left\\|\\nabla\\ell(y,x,R_{1}^{\\prime})-\\ell(y^{\\prime},x^{\\prime},R_{1}^{\\prime})\\right\\|_{2}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "As $\\ell$ is $_\\beta$ -jointly smooth, we have that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\left\\|\\nabla\\ell(y,x,R_{1}^{\\prime})-\\ell(y^{\\prime},x^{\\prime},R_{1}^{\\prime})\\right\\|_{2}\\leq\\beta\\left\\|(x,y)-(x^{\\prime},y^{\\prime})\\right\\|_{2}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Thus, together, the latter two inequalities imply ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\|T(x,y)-T(x^{\\prime},y^{\\prime})\\|_{2}\\leq\\left\\|R_{1}-R_{1}^{\\prime}\\right\\|_{2}\\beta\\left\\|(x,y)-(x^{\\prime},y^{\\prime})\\right\\|_{2}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "showing that $T$ is $\\left\\|\\boldsymbol{R}_{1}-\\boldsymbol{R}_{1}^{\\prime}\\right\\|_{2}\\beta$ -Lipschitz. This implies that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\tilde{T}:=(\\left|\\left|R_{1}-R_{1}^{\\prime}\\right|\\right|_{2}\\beta)^{-1}T\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "is 1-Lipschitz. As, due to theorem 1, the non-greedy sample adaption function $f_{n}$ is Lipschitz with respect to $W_{1}$ and $\\|\\cdot\\|_{p}$ for some constant $L$ , we can use the dual characterization of the Wasserstein metric, i.e. the Kantorovich-Rubinstein lemma [43], to obtain ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\left|\\mathbb{E}_{(Y,X)\\sim f_{n}(\\theta,\\mathbb{P})}(\\tilde{T}(Y,X))-\\mathbb{E}_{(Y,X)\\sim f_{n}(\\theta^{\\prime},\\mathbb{P}^{\\prime})}(\\tilde{T}(Y,X))\\right|\\le L(||\\theta-\\theta^{\\prime}||_{2}+W_{1}(\\mathbb{P},\\mathbb{P}^{\\prime}))\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "We compute: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathbb{E}_{(Y,X)\\sim f_{n}(\\theta,\\mathbb{P})}(\\tilde{T}(Y,X))=\\frac{\\mathbb{E}_{(Y,X)\\sim f_{n}(\\theta,\\mathbb{P})}(T(Y,X))}{\\left\\|R_{1}-R_{1}^{\\prime}\\right\\|_{2}\\beta}=\\frac{(R_{1}-R_{1}^{\\prime})^{T}}{\\left\\|R_{1}-R_{1}^{\\prime}\\right\\|_{2}\\beta}\\nabla F(R_{1}^{\\prime})\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Here, we used that $(R_{1}-R_{1}^{\\prime})^{T}$ is a constant with respect to the measure $f_{n}(\\theta,\\mathbb{P})$ and that the order of integration and differentiation can be exchanged according to Lebesgue\u2019s dominated convergence theorem. Analogously, we obtain ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathbb{E}_{(Y,X)\\sim f_{n}(\\theta^{\\prime},\\mathbb{P}^{\\prime})}(\\tilde{T}(Y,X))=\\frac{\\mathbb{E}_{(Y,X)\\sim f_{n}(\\theta^{\\prime},\\mathbb{P}^{\\prime})}(T(Y,X))}{\\left\\|R_{1}-R_{1}^{\\prime}\\right\\|_{2}\\beta}=\\frac{(R_{1}-R_{1}^{\\prime})^{T}}{\\left\\|R_{1}-R_{1}^{\\prime}\\right\\|_{2}\\beta}\\nabla F^{\\prime}(R_{1}^{\\prime})\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Together, this yields ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(R_{1}-R_{1}^{\\prime})^{T}\\nabla F(R_{1}^{\\prime})-(R_{1}-R_{1}^{\\prime})^{T}\\nabla F^{\\prime}(R_{1}^{\\prime})\\geq-L\\beta\\left\\|R_{1}-R_{1}^{\\prime}\\right\\|_{2}(||\\theta-\\theta^{\\prime}||_{2}+W_{1}(\\mathbb{P},\\mathbb{P}^{\\prime}))}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "As $R_{1}^{\\prime}$ is the unique minimizer of $F^{\\prime}$ , we conclude that the second product on the left-hand side of this inequality is larger or equal to 0. Thus, the inequality reduces to ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(R_{1}-R_{1}^{\\prime})^{T}\\nabla F(R_{1}^{\\prime})\\ge-L\\beta\\left\\|R_{1}-R_{1}^{\\prime}\\right\\|_{2}(||\\theta-\\theta^{\\prime}||_{2}+W_{1}(\\mathbb{P},\\mathbb{P}^{\\prime}))}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "which, together with Equation (51), yields ", "page_idx": 30}, {"type": "equation", "text": "$$\n-\\gamma\\left\\|\\boldsymbol{R}_{1}-\\boldsymbol{R}_{1}^{\\prime}\\right\\|_{2}^{2}\\geq-L\\beta\\left\\|\\boldsymbol{R}_{1}-\\boldsymbol{R}_{1}^{\\prime}\\right\\|_{2}(||\\boldsymbol{\\theta}-\\boldsymbol{\\theta}^{\\prime}||_{2}+W_{1}(\\mathbb{P},\\mathbb{P}^{\\prime}))\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "which proves \u2013 after some rearranging \u2013 that $R_{1}\\,:\\,\\Theta\\times\\mathcal{P}\\,\\rightarrow\\,\\Theta$ is Lipschitz-continuous with Lipschitz-constant $L\\frac{\\beta}{\\gamma}$ . Precisely, we get ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\left\\|\\boldsymbol{R}_{1}-\\boldsymbol{R}_{1}^{\\prime}\\right\\|_{2}\\leq L\\frac{\\beta}{\\gamma}(||\\boldsymbol{\\theta}-\\boldsymbol{\\theta}^{\\prime}||_{2}+W_{1}(\\mathbb{P},\\mathbb{P}^{\\prime}).\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "We further have per theorems 1 and 2 that $f_{n}:\\Theta\\times\\mathcal{P}\\xrightarrow{}\\mathcal{P}$ is Lipschitz-continuous with constant $L$ , as used above. It can easily be verified (see definitions 7 and 5) that the second component $R_{2}:=R_{2}(\\theta,\\mathbb{P})$ of $R(\\theta,\\mathbb{P})$ equates $f_{n}$ . Thus, ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\|R_{2}-R_{2}^{\\prime}\\|\\leq L(||\\theta-\\theta^{\\prime}||_{2}+W_{1}(\\mathbb{P},\\mathbb{P}^{\\prime})\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "with $R_{2}^{\\prime}:=R_{2}(\\theta^{\\prime},\\mathbb{P}^{\\prime})$ and arbitrary $\\theta,\\theta^{\\prime}\\in\\Theta$ and arbitrary $\\mathbb{P},\\mathbb{P}^{\\prime}\\in\\mathcal{P}$ . We can conclude that $R_{n}$ is Lipschitz-continuous with Lipschitz-constant $\\begin{array}{r}{\\leq L(1+\\frac{\\beta}{\\gamma})}\\end{array}$ and the sum-metric on $\\Theta\\times{\\mathcal{P}}$ by adding the two Lipschitz-inequalities, yielding ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\bigl\\|R_{1}-R_{1}^{\\prime}\\bigr\\|_{2}+\\bigl\\|R_{2}-R_{2}^{\\prime}\\bigr\\|_{2}\\leq L\\frac{\\beta}{\\gamma}(\\|\\theta-\\theta^{\\prime}\\|_{2}+W_{1}(\\mathbb{P},\\mathbb{P}^{\\prime})+L(\\|\\theta-\\theta^{\\prime}\\|_{2}+W_{1}(\\mathbb{P},\\mathbb{P}^{\\prime}).\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "That is, ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\left\\|R_{n}-R_{n}^{\\prime}\\right\\|\\leq L(1+\\frac{\\beta}{\\gamma})(||\\theta-\\theta^{\\prime}||_{2}+W_{1}(\\mathbb{P},\\mathbb{P}^{\\prime})).\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Remains to be shown that, assuming $\\begin{array}{r}{L<(1+\\frac{\\beta}{\\gamma})^{-1}}\\end{array}$ , the sequence $(R_{n}(\\theta_{t},\\mathbb{P}_{t}))_{t\\in\\mathbb{N}}$ converges to a fix point at a linear rate. The existence and uniqueness of a fix point follows from Banach\u2019s fix point theorem [4], since equation 65 guarantees that the map $R_{n}$ is a contraction for $\\begin{array}{r}{L<(1+\\frac{\\beta}{\\gamma})^{-1}}\\end{array}$ on a complete metric space. So, let $(\\theta_{c},\\mathbb{P}_{c})$ denote such a fix point. Observe that it holds per equation 65 for all $t\\in\\mathbb{N}$ ", "page_idx": 31}, {"type": "equation", "text": "$$\n||(\\theta_{t},\\mathbb{P}_{t})-(\\theta_{c},\\mathbb{P}_{c})||\\leq L(1+\\frac{\\beta}{\\gamma})||(\\theta_{t},\\mathbb{P}_{t})-(\\theta_{c},\\mathbb{P}_{c})||\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Repeatedly applying this yields ", "page_idx": 31}, {"type": "equation", "text": "$$\n||(\\theta_{t},\\mathbb{P}_{t})-(\\theta_{c},\\mathbb{P}_{c})||\\leq L^{t}(1+\\frac{\\beta}{\\gamma})^{t}||(\\theta_{0},\\mathbb{P}_{0})-(\\theta_{c},\\mathbb{P}_{c})||\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Setting the expression on the right-hand side to be at most $\\Delta$ gives ", "page_idx": 31}, {"type": "equation", "text": "$$\n||(\\theta_{t},\\mathbb{P}_{t})-(\\theta_{c},\\mathbb{P}_{c})||\\leq L^{t}(1+\\frac{\\beta}{\\gamma})^{t}||(\\theta_{0},\\mathbb{P}_{0})-(\\theta_{c},\\mathbb{P}_{c})||\\leq\\Delta\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Rearranging and setting the expression on the right-hand side to be at most $\\Delta$ gives ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\log||(\\theta_{t},\\mathbb{P}_{t})-(\\theta_{c},\\mathbb{P}_{c})||\\leq t\\log\\left\\{L(1+\\frac{\\beta}{\\gamma})\\right\\}\\leq\\log\\frac{\\Delta}{||(\\theta_{t},\\mathbb{P}_{t})-(\\theta_{c},\\mathbb{P}_{c})\\}||\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Rearranging for $t$ and exploiting that $\\begin{array}{r}{L(1+\\frac{\\beta}{\\gamma})<1}\\end{array}$ yields ", "page_idx": 31}, {"type": "equation", "text": "$$\nt\\geq\\frac{\\log\\frac{\\vert\\vert(\\theta_{0},\\mathbb{P}_{0})-(\\theta_{c},\\mathbb{P}_{c})\\vert\\vert}{\\Delta}}{\\log L(1+\\frac{\\beta}{\\gamma})}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "since $||(\\theta_{0},\\mathbb{P}_{0})-(\\theta_{c},\\mathbb{P}_{c})||$ and $\\begin{array}{r}{L(1+\\frac{\\beta}{\\gamma})}\\end{array}$ are fixed quantities, we have that ", "page_idx": 31}, {"type": "equation", "text": "$$\n||(\\theta_{t},\\mathbb{P}_{t})-(\\theta_{c},\\mathbb{P}_{c})||\\leq\\Delta\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Note that the proof was mainly an application of Banach fixed-point theorem [4] and as such similar to the proof of [74, theorem 3.5]. The key differences to [74, theorem 3.5] are: (1) We need to prove the Lipschitz-continuity of the sample adaption function $f$ first, see theorem 1, which is non-trivial for several instances of reciprocal learning. (2) [74, theorem 3.5] considers simple repeated risk minimization, i.e., a mapping $\\Theta\\to\\Theta$ , while reciprocal learning is $R:\\Theta\\times\\mathcal{P}\\times\\mathbb{N}\\to\\Theta\\times\\mathcal{P}\\times\\mathbb{N}$ or $R_{n}:\\Theta\\times\\mathcal{P}\\rightarrow\\Theta\\times\\mathcal{P}$ (definitions 6 and 7). ", "page_idx": 31}, {"type": "text", "text": "F.6 Proof of Theorem 4 ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Proof. Recall that $R_{n}^{*}\\,=\\,\\bigl(\\theta^{*},\\mathbb{P}^{*}\\bigr)\\,=\\,\\arg\\operatorname*{min}_{\\theta,\\mathbb{P}}\\mathbb{E}_{(Y,X)\\sim f_{n}(\\theta,\\mathbb{P})}~\\ell\\bigl(Y,X,\\theta\\bigr)$ per definition 9 and $\\theta_{c}=$ arg $\\begin{array}{r}{\\operatorname*{min}_{\\theta}\\mathbb{E}_{(Y,X)\\sim f_{n}(\\theta_{c},\\mathbb{P}_{c})}\\ \\ell(Y,X,\\theta)}\\end{array}$ per definition 8. First assume that $\\theta_{c}\\neq\\theta^{*}$ , since otherwise the the statement would be trivial due to $L>0,L\\ell>0,\\gamma>0$ per assumptions. ", "page_idx": 31}, {"type": "text", "text": "We will now prove the statement $\\begin{array}{r}{\\lvert|\\theta_{c}-\\theta^{*}\\rvert|\\leq\\frac{2L_{\\ell}L}{\\gamma}}\\end{array}$ by contradiction. Thus, assume that $||\\theta_{c}-\\theta^{*}||>$ $\\frac{2L_{\\ell}L}{\\gamma}$ ", "page_idx": 31}, {"type": "text", "text": "First observe that $\\mathbb{E}_{(Y,X)\\sim f_{n}(\\,\\theta_{c},\\mathbb{P}_{c}\\,)}\\ \\ell(Y,X,\\theta)$ is $(L L_{\\ell})$ -Lipschitz in $\\theta$ for fixed $\\mathbb{P}_{c}$ and $\\theta_{c}$ , since the loss is $L_{\\ell}$ -Lipschitz and $f_{n}$ is $L$ -Lipschitz in $\\theta$ , since it is $L_{\\theta}$ -Lipschitz in $\\theta$ (see 1. in proof of theorem 1) and $L=\\operatorname*{max}\\{L_{\\theta},L_{\\mathbb{P}},L_{n}\\}$ . That is, ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathbb{\\mathbb{B}}_{(Y,X)\\sim f_{n}(\\theta_{c},\\mathbb{p}_{c})}~\\ell(Y,X,\\theta_{c})\\,-\\,\\mathbb{\\mathbb{B}}_{(Y,X)\\sim f_{n}(\\theta_{c},\\mathbb{p}_{c})}~\\ell(Y,X,\\theta^{*})\\le L_{\\ell}L||\\theta^{*}-\\theta_{c}||.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Further note that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathbb{E}_{(Y,X)\\sim f_{n}(\\theta_{c},\\mathbb{P}_{c})}~\\ell(Y,X,\\theta^{*})\\,-\\,\\mathbb{E}_{(Y,X)\\sim f_{n}(\\theta_{c},\\mathbb{P}_{c})}~\\ell(Y,X,\\theta_{c})\\ge\\frac{\\gamma}{2}||\\theta^{*}-\\theta_{c}||^{2},\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "which holds because we can state due to assumption 3 (strong convexity) that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r}{^{!}(Y,X)\\sim f_{n}(\\theta_{c},\\mathbb{P}_{c})\\ \\left[\\ell(Y,X,\\theta^{*})-\\ell(Y,X,\\theta_{c})\\right]\\geq\\mathbb{B}_{(Y,X)\\sim f_{n}}(\\theta_{c},\\mathbb{P}_{c})\\ \\left[\\nabla_{\\theta}\\ell(Y,X,\\theta_{c})^{T}(\\theta^{*}-\\theta_{c})\\right]+\\frac{\\gamma}{2}||\\theta^{*}-\\theta_{c}||^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "by taking expectations on the inequality stated in assumption 3 (strong convexity). By classical first-order optimality conditions [44] we know that the first term on the right-hand side is larger or equal to 0, from which equation 73 directly follows, see also [74, Theorem 4.3]. ", "page_idx": 32}, {"type": "text", "text": "If now $\\begin{array}{r}{\\lvert|\\theta_{c}-\\theta^{*}\\rvert|>\\frac{2L_{\\ell}L}{\\gamma}}\\end{array}$ , or equivalently $\\begin{array}{r}{\\frac{\\gamma}{2}||\\theta_{c}-\\theta^{*}||^{2}>L_{\\ell}L||\\theta_{c}-\\theta^{*}||}\\end{array}$ as we assumed, we have per equations 72 and 73 that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{B}_{(Y,X)\\sim f_{n}(\\theta_{c},\\mathcal{F}_{c})}\\ \\ell(Y,X,\\theta_{c})\\ -\\ \\mathbb{B}_{(Y,X)\\sim f_{n}(\\theta_{c},\\mathcal{F}_{c})}\\ \\ell(Y,X,\\theta^{*})}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad<\\mathbb{B}_{(Y,X)\\sim f_{n}(\\theta_{c},\\mathcal{F}_{c})}\\ \\ell(Y,X,\\theta^{*})\\ -\\ \\mathbb{B}_{(Y,X)\\sim f_{n}(\\theta_{c},\\mathcal{F}_{c})}\\ \\ell(Y,X,\\theta_{c}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "which would imply ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{(Y,X)\\sim f_{n}(\\theta_{c},\\mathbb{P}_{c})}~\\ell(Y,X,\\theta^{*})>~\\mathbb{E}_{(Y,X)\\sim f_{n}(\\theta_{c},\\mathbb{P}_{c})}~\\ell(Y,X,\\theta_{c}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "which contradicts definition 9 and the non-negativity of the loss function. ", "page_idx": 32}, {"type": "text", "text": "F.7 Proof of Theorem 5 ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Proof. By Cauchy criterion for series $R_{t}$ , $t\\in\\mathbb{N}$ with respect to sum or product norm on $\\Theta\\times\\mathcal{P}\\times\\mathbb{N}$ . According to the Cauchy criterion the series $R_{t}$ diverges, if there is an $\\epsilon$ such that $\\exists t\\in\\mathbb{N}:\\,\\forall m,n\\geq$ $t:d_{p}(R_{m}-R_{n})=d_{p}((\\dot{\\theta}_{m},\\mathbb{P}_{m},n_{m})-(\\theta_{n},\\mathbb{P}_{n},n_{n})))\\overset{.}{>}\\epsilon$ with $d_{p}$ the sum norm and $m\\neq n$ ; $m,n\\in\\mathbb{N}$ . This holds for $\\epsilon\\in(0,1)$ , since $||n_{m}-n_{n}||\\geq1$ . \u25a1 ", "page_idx": 32}, {"type": "text", "text": "F.8 Proof of Theorem 6 ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Proof. By counterexample. Assume $f_{n}:\\Theta\\times\\mathcal{P}\\to\\mathcal{P}$ is not Lipschitz-continuous, i.e., no $L<\\infty$ exists such that ", "page_idx": 32}, {"type": "equation", "text": "$$\nW_{1}(f(\\theta,\\mathbb{P}),f(\\theta^{\\prime},\\mathbb{P}^{\\prime}))\\leq L\\cdot||(||\\theta-\\theta^{\\prime}||_{2},W_{1}(\\mathbb{P},\\mathbb{P}^{\\prime}))||_{p}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Let again $R_{n,1}:=R_{n,1}(\\theta,\\mathbb{P})$ and $R_{n,1}^{\\prime}:=R_{n,1}(\\theta^{\\prime},\\mathbb{P}^{\\prime})$ be first components of $R(\\theta,\\mathbb{P})$ and $R(\\theta^{\\prime},\\mathbb{P}^{\\prime})$ , respectively. Further assume that ${R}_{n,1}=C+\\theta^{\\prime}L.$ , $C\\in\\mathbb{R}$ . It becomes evident that for any fixed point $\\theta_{c}$ it must hold: $\\begin{array}{r}{\\theta_{c}=\\frac{C}{1-L}}\\end{array}$ . If $L\\to\\infty$ , this fixed point does not exist: $\\begin{array}{r}{\\operatorname*{lim}_{L\\to\\infty}(\\theta_{c})=\\pm\\infty}\\end{array}$ . \u25a1 ", "page_idx": 32}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: Abstract and section 1 (introduction) of the paper accurately reflect the paper\u2019s contribution and scope, covering all results presented in the paper. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 33}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Justification: The limitations of the work are explicitely discussed in sections 1 and 6 (Subsection \"Limitations) of the main paper as well as mentioned when stating the main results in sections 3 and 4. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 33}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: The theoretical results of the paper are stated in Theorems 1, 2, 3, 4, 5, and 6 as well as in Lemmata 1 and 2. For each of these theorems and Lemmata, full sets of assumptions and conditions are provided. Moreover, complete proofs for all these statements are provided in the paper\u2019s appendix. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 34}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: The paper does include two experiments in section C. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 34}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: See section C. Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 35}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: See section C. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 35}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: See section C. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 35}, {"type": "text", "text": "", "page_idx": 36}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 36}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: The research conducted in the paper is, in every aspect, conform with the NeurIPS Code of Ethics. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 36}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: We do not foresee direct negative societal impact from the current work. Positive societal impact in form of making decisions based on reciprocal learning algorithms such as active learning or self-training more reliable and trustworthy are discussed in section 1, 2, and 6 of the paper. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 36}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 37}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 37}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 37}, {"type": "text", "text": "", "page_idx": 38}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 38}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 38}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 38}]