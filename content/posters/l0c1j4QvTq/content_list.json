[{"type": "text", "text": "Diffusion Actor-Critic with Entropy Regulator ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yinuo Wang1 Likun Wang1 Yuxuan Jiang1 Wenjun Zou1 Tong Liu1 Xujie Song1 Wenxuan Wang1 Liming Xiao2 Jiang Wu2 Jingliang Duan1,2\u2217 Shengbo Eben Li1\u2217 ", "page_idx": 0}, {"type": "text", "text": "1School of Vehicle and Mobility, Tsinghua University 2School of Mechanical Engineering, University of Science and Technology Beijing ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Reinforcement learning (RL) has proven highly effective in addressing complex decision-making and control tasks. However, in most traditional RL algorithms, the policy is typically parameterized as a diagonal Gaussian distribution with learned mean and variance, which constrains their capability to acquire complex policies. In response to this problem, we propose an online RL algorithm termed diffusion actor-critic with entropy regulator (DACER). This algorithm conceptualizes the reverse process of the diffusion model as a novel policy function and leverages the capability of the diffusion model to fti multimodal distributions, thereby enhancing the representational capacity of the policy. Since the distribution of the diffusion policy lacks an analytical expression, its entropy cannot be determined analytically. To mitigate this, we propose a method to estimate the entropy of the diffusion policy utilizing Gaussian mixture model. Building on the estimated entropy, we can learn a parameter $\\alpha$ that modulates the degree of exploration and exploitation. Parameter $\\alpha$ will be employed to adaptively regulate the variance of the added noise, which is applied to the action output by the diffusion model. Experimental trials on MuJoCo benchmarks and a multimodal task demonstrate that the DACER algorithm achieves state-of-the-art (SOTA) performance in most MuJoCo control tasks while exhibiting a stronger representational capacity of the diffusion policy. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Recently, deep reinforcement learning (RL) has emerged as an effective method for solving optimal control problems in the physical world [14, 29, 22, 25]. In most existing RL algorithms, the policy is parameterized as a deterministic function or a diagonal Gaussian distribution with the learned mean and variance [31, 32, 16, 10]. However, the theoretically optimal policy may exhibit strong multimodality, which cannot be well modeled by deterministic or diagonal Gaussian policies [43, 21, 45]. Restricted policy representation capabilities can make algorithms prone to local optimal solutions, damaging policy performance. For instance, in situations where two distinct actions in the same state yield approximately the same Q-value, the Gaussian policy approximates the bimodal action by maximizing the Q-value. This results in the policy displaying mode-covering behavior, concentrating high density in the intermediate region between the two patterns, which is inherently a low-density region with a lower Q-value. Consequently, modeling the policy with a unimodal Gaussian distribution is likely to significantly impair policy learning. ", "page_idx": 0}, {"type": "text", "text": "Lately, the diffusion model has become widely known as a generative model for its powerful ability to fit multimodal distributions [18, 35, 8]. It learns the original data distribution through the idea of stepwise addition and removal of noise and has excellent performance in the fields of image [46, 28] and video generation [9, 3]. The policy network in RL can be seen as a state-conditional generative model. Given the ability of diffusion models to fti complex distributions, there is increasing work on combining RL with diffusion models. Online RL learns policies by interacting with the environment [16, 32]. Offline RL, also known as batch RL, aims to effectively learn policies from previously collected data without interacting with the environment [1, 4]. In practical applications, many control problems have excellent simulators. At this time, using offline RL is not appropriate, as online RL with interaction capabilities performs better. Therefore, this paper focuses on how the diffusion model can be combined with online RL. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In this work, we propose diffusion actor-critic with entropy regulator (DACER), a generalized new approach to combine diffusion policy with online RL. Specifically, we base DACER on the denoising diffusion probabilistic model (DDPM) [18]. A recent work by He et al. [44] points out that the representational power of diffusion models stems mainly from the reverse diffusion processes, not from the forward diffusion processes. Inspired by this work, we reconceptualize the reverse process of the diffusion model as a novel policy approximator, leveraging its powerful representation capabilities to enhance the performance of RL algorithms. The optimization objective of this novel policy function is to maximize the expected Q-value. Maximizing entropy is important for policy exploration in RL, but the entropy of the diffusion policy is difficult to determine. Therefore, we choose to sample actions at fixed intervals and use a Gaussian mixture model (GMM) to fit the action distributions. Subsequently, We can calculate the approximate entropy of the policy in each state. The average of these entropies is then used as an approximation of the current diffusion policy entropy. Then, we use the estimated entropy to regulate the degree of exploration and exploitation of diffusion policy. ", "page_idx": 1}, {"type": "text", "text": "In summary, the key contributions of this paper are the following: 1) We propose to consider the reverse process of the diffusion model as a novel policy function. The objective function of the diffusion policy is to maximize the expected Q-value and thus achieve policy improvement. 2) We propose a method for estimating the entropy of diffusion policy. The estimated value is utilized to achieve an adaptive adjustment of the exploration level of the diffusion policy, thus improving the policy performance. 3) We evaluate the efficiency and generality of our method on the popular MuJoCo benchmarking. Compared with DDPG [33], TD3 [12], PPO [32], SAC [16], DSAC [11, 10], and TRPO [31], our approach achieves the SOTA performance. In addition, we demonstrate the superior representational capacity of our algorithm through a specific multi-goal task. 4) We provide the DACER code written in PyTorch and JAX to facilitate future researchers to follow our work. ", "page_idx": 1}, {"type": "text", "text": "Section 2 introduces and summarizes existing approaches to diffusion policy in offline RL and online RL, pointing out some of their problems. Section 3 provides an introduction to online RL and diffusion models. Our approach to combining diffusion policy with the mainstream actor-critic framework, as well as methods to enhance the performance of diffusion policy will be presented in section 4. The results of the experiments in the MuJoCo environment, the ablation experiments as well as the multimodality task will be presented in section 5. Section 6 provides the conclusions of this paper. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Diffusion Policy in Offilne RL Offilne RL leverages pre-collected datasets for policy development, circumventing direct environmental interaction. Current offline RL research utilizing diffusion models as policy networks primarily adhere to the behavioral cloning framework [7, 26]. Within this framework, two main objectives emerge: performance enhancement and training efficiency improvement. For the former, Cheng et al. [6] proposed a Diffusion Policy, casting the policy as a conditional denoising diffusion process within the action space to accommodate complex multimodal action distributions. Wang et al. [43] introduced Diffusion-QL, which integrates behavior cloning via diffusion model loss with Q-learning for policy improvement. Ajay et al. [2] created Decision Diffusion, incorporating classifier-free guidance into the diffusion model to integrate trajectory information, such as rewards and constraints. Addressing the latter, Kang et al. [20] developed efficient diffusion policies (EDP), an evolution of Diffusion-QL. EDP accelerates training by utilizing initial actions from state-action pairs in the buffer and applying a one-step sample for final action derivation. Chen et al. [5] proposed a consistency policy that enhances diffusion algorithm efficiency through one-step action generation from noise during training and inference. Although Diffusion Policy\u2019s powerful ability to fit multimodal policy distributions can achieve good performance in offilne RL tasks, this method of policy improvement based on behavioral cloning cannot be directly transferred to online RL. In addition, the biggest challenge facing offline RL, the distribution shift problem, has not been completely solved. This paper focuses on online RL, moving away from the framework of behavioral cloning. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Diffusion Policy in Online RL Online RL, characterized by real-time environment interaction, contrasts with offline RL\u2019s dependence on pre-existing datasets. To date, only two studies have delved into integrating online RL with diffusion models. Yang et al. [45] pioneered this approach by proposing action gradient method. This approach achieves policy improvement by updating the action in the replay buffer through the $\\nabla_{a}Q$ , followed by mimicry learning of the action post-update using a diffusion model. However, action gradient increased the additional training time. Furthermore, it is difficult to fully learn both the action gradient and imitation learning steps simultaneously, which also resulted in suboptimal performance of this method in MuJoCo tasks. Psenka et al. [30] proposed Q-score matching (QSM), a new methodology for off-policy reinforcement learning that leverages the score-based structure of diffusion model policies to align with the $\\nabla_{a}Q$ . This approach aims to overcome the limitations of simple behavior cloning in actor-critic settings by integrating the policy\u2019s score with the Q-function\u2019s action gradient. However, QSM needs to accurately learn $\\nabla_{a}Q$ in most of the action space to achieve optimal guidance. This is difficult to accomplish, resulting in suboptimal performance of QSM. ", "page_idx": 2}, {"type": "text", "text": "Our method is motivated to propose a diffusion policy that can be combined with most existing actor-critic frameworks. We first consider the reverse diffusion process of the diffusion model as a policy function with strong representational power. Then, we use the entropy estimation method to balance the exploration and utilization of diffusion policy and improve the performance of the policy. ", "page_idx": 2}, {"type": "text", "text": "Comparison with Diffusion-QL Diffusion-QL [43] made a successful attempt by replacing the diagonal Gaussian policy with a diffusion model. It also guides the updating of the policy by adding the normalized Q-value in the policy loss term. The main differences between our work and DiffusionQL are as follows: 1) Diffusion-QL is still essentially an architecture for imitation learning, and policy updates are mainly motivated by the imitation learning loss term. 2) Our work adaptively regulates the standard deviation of random noise in the sampling process $\\pmb{a}=\\pmb{a}+\\lambda\\alpha\\cdot\\mathcal{N}(0,\\pmb{I})$ , where $\\alpha$ is a learned parameter, $\\lambda$ is a hyperparameter. This method effectively balances exploration and exploitation and subsequently enhances the performance of the diffusion policy. Ablation experiments provide evidence supporting these findings. ", "page_idx": 2}, {"type": "text", "text": "3 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Online Reinforcement Learning ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In the conventional framework of RL, interactions between the agent and its environment occur in sequential discrete time steps. Typically, the environment is modeled as a Markov decision process (MDP) with continuous states and actions [38]. The environment provides feedback through a bounded reward function denoted by $r(s_{t},a_{t})$ . The likelihood of transitioning to a new state based on the agent\u2019s action is expressed by the probability $p(s_{t+1}|s_{t},a_{t})$ . State-action pairs for the current and next steps are indicated as $(s,a)$ and $(s^{\\prime},a^{\\prime})$ . The decision-making of an agent at any state $s_{t}$ is guided by a stochastic policy $\\pi(a_{t}|s_{t})$ , which determines the probability distribution over feasible actions at that state. ", "page_idx": 2}, {"type": "text", "text": "In the realm of online RL, agents engage in real-time learning and decision-making through direct interactions with their environments. Such interactions are captured within a tuple $(s_{t},a_{t},r_{t},s_{t+1})$ , representing the transition during each interaction. It is common practice to store these transitions in an experience replay buffer, symbolized as $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ . Throughout the training phase, random samples drawn from $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ produce batches of data that contribute to a more consistent training process. The fundamental aim of traditional online RL strategies is to craft a policy that optimizes the expected total reward: ", "page_idx": 2}, {"type": "equation", "text": "$$\nJ_{\\pi}=\\mathbb{E}_{(s_{i\\geq t},a_{i\\geq t})\\sim\\pi}\\Big[\\sum_{i=t}^{\\infty}\\gamma^{i-t}r(s_{i},a_{i})\\Big],\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\gamma\\in(0,1)$ represents the discount factor. The Q-value for a state-action pair $(s,a)$ is given by ", "page_idx": 2}, {"type": "equation", "text": "$$\nQ(s,a)=\\mathbb{E}_{\\pi}\\Big[\\sum_{i=0}^{\\infty}\\gamma^{i}r(s_{i},a_{i})|s_{0}=s,a_{0}=a\\Big].\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "RL typically employs an actor-critic framework [25, 24], which includes both a policy function, symbolized by $\\pi$ , and a corresponding Q-value function, noted as $Q^{\\pi}$ . The process of policy iteration is often used to achieve the optimal policy $\\pi^{*}$ , cycling through phases of policy evaluation and enhancement. In the policy evaluation phase, the $\\mathrm{{Q}}.$ -value $Q^{\\pi}$ is recalibrated according to the self-consistency requirements dictated by the Bellman equation: ", "page_idx": 3}, {"type": "equation", "text": "$$\nQ^{\\pi}(s,a)=r(s,a)+\\gamma\\mathbb{E}_{s^{\\prime}\\sim p,a^{\\prime}\\sim\\pi}[Q^{\\pi}(s^{\\prime},a^{\\prime})].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "In the policy improvement phase, an enhanced policy $\\pi_{\\mathrm{new}}$ is sought by optimizing current Q-value $Q^{\\pi_{\\mathrm{old}}}$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\pi_{\\mathrm{new}}=\\arg\\operatorname*{max}_{\\pi}\\mathbb{E}_{s\\sim d_{\\pi},a\\sim\\pi}[Q^{\\pi_{\\mathrm{old}}}(s,a)].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "In practical applications, neural networks are often used to parameterize both the policy and value functions, represented by $\\pi_{\\theta}$ and $Q_{\\phi}$ , respectively. These functions are refined through the application of gradient descent methods aimed at reducing the loss functions for both the critic, $\\overline{{\\mathcal{L}}}_{q}(\\theta)\\ =\\ \\overline{{\\mathbb{E}}}_{(s,a,s^{\\prime})\\sim\\mathcal{B}}\\left[\\Big(r(s,a)+\\gamma\\mathbb{E}_{s^{\\prime}\\sim p,a^{\\prime}\\sim\\pi}[Q^{\\pi}(s^{\\prime},a^{\\prime})]-Q_{\\phi}^{\\pi}(s,a)\\Big)^{2}\\right]$ , and the actor, $\\mathcal{L}_{\\pi}(\\phi)\\,=\\,-\\mathbb{E}_{s\\sim d_{\\pi},a\\sim\\pi}[Q^{\\pi_{\\mathrm{old}}}(\\bar{s},a)]$ . These loss functions are structured based on the principles outlined in (3) and (4). ", "page_idx": 3}, {"type": "text", "text": "3.2 Diffusion Models ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Diffusion models [34, 18, 36, 37] are highly effective generative tools. They convert data from its original distribution to a Gaussian noise distribution by gradually adding noise and then reconstruct the data by gradually removing this noise through a reverse process. This process is typically described as a continuous Markov chain: the forward process incrementally increases the noise level, while the reverse process involves a conditional generative model trained to predict the optimal reverse transitions at each denoising step. Consequently, the model reverses the diffusion sequence to generate data samples starting from pure noise. ", "page_idx": 3}, {"type": "text", "text": "Let us define $\\begin{array}{r}{p_{\\theta}(\\mathbf{x}_{0})\\,:=\\,\\int p_{\\theta}(\\mathbf{x}_{0:T})\\mathrm{d}\\mathbf{x}_{1:T}.}\\end{array}$ , where $\\mathbf{\\Delta}x_{1},\\dots,x_{T}$ denote latent variables sharing the same dimensionality as the data variable $x_{0}\\sim q(x_{0})$ , where $q(x_{0})$ means original data distribution. In a forward diffusion chain, the noise is incrementally introduced to the data $x_{0}\\sim q(x_{0})$ across $T$ steps, adhering to a predetermined variance sequence denoted by $\\beta_{t}$ , described as ", "page_idx": 3}, {"type": "equation", "text": "$$\nq(\\pmb{x}_{1:T}|\\pmb{x}_{0})=\\prod_{t=1}^{T}q(\\pmb{x}_{t}|\\pmb{x}_{t-1}),\\quad q(\\pmb{x}_{t}|\\pmb{x}_{t-1})=\\mathcal{N}(\\pmb{x}_{t};\\sqrt{1-\\beta_{t}}\\pmb{x}_{t-1},\\beta_{t}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "When $T\\rightarrow\\infty$ , $x_{T}$ distributes as an isotropic Gaussian distribution [21]. The reverse diffusion process of the diffusion model can be represented as ", "page_idx": 3}, {"type": "equation", "text": "$$\np_{\\theta}({\\pmb x}_{0:T})=p({\\pmb x}_{T})\\prod_{t=1}^{T}p_{\\theta}({\\pmb x}_{t-1}|{\\pmb x}_{t}),\\quad p_{\\theta}({\\pmb x}_{t-1}|{\\pmb x}_{t})=\\mathcal N({\\pmb x}_{t-1};{\\pmb\\mu}_{\\theta}({\\pmb x}_{t},t),\\pmb\\Sigma_{\\theta}({\\pmb x}_{t},t)),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $p(\\mathbf{\\boldsymbol{x}}_{T})=\\mathcal{N}(\\mathbf{\\boldsymbol{x}}_{T};\\mathbf{0},I)$ under the condition that $\\textstyle\\prod_{t=1}^{T}(1-\\beta_{t})\\approx0$ . ", "page_idx": 3}, {"type": "text", "text": "4 Method ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we detail the design of our diffusion actor-critic with entropy regulator (DACER). First, we consider the reverse diffusion process of the diffusion model as a new policy approximator, serving as the policy function in RL. Second, We directly optimize the diffusion policy using gradient descent, whose objective function is to maximize expected Q-values. This feature allows it to be integrated with mainstream RL algorithms that do not require entropy. However, the diffusion policy learned this way produces overly deterministic actions with poor performance. When attempting to integrate the maximization entropy RL framework, we find the entropy of the diffusion policy is difficult to analytically determine. Therefore, we use GMM to approximate the entropy of the diffusion policy, and then learn a parameter $\\alpha$ based on it to adjust the exploration level of diffusion policy. ", "page_idx": 3}, {"type": "text", "text": "4.1 Diffusion Policy Representation ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We use the reverse process of a conditional diffusion model as a parametric policy: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\pi_{\\theta}(\\pmb{a}|\\pmb{s})=p_{\\theta}(\\pmb{a}_{0:T}|\\pmb{s})=p(\\pmb{a}_{T})\\prod_{t=1}^{T}p_{\\theta}(\\pmb{a}_{t-1}|\\pmb{a}_{t},\\pmb{s}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathbf{\\Omega}_{p}(\\mathbf{a}_{T})\\mathbf{\\Omega}=\\mathbf{\\Omega}\\cdot\\mathcal{N}(0,\\pmb{I})$ , the end sample of the reverse chain, $\\pmb{a}_{0}$ , is the action used for RL evaluation. Generally, $p_{\\theta}(a_{t-1}|\\bar{a_{t}},\\bar{s})$ could be modeled as a Gaussian distribution $\\mathcal{N}(\\mathbf{\\boldsymbol{a}}_{t-1};\\mathbf{\\boldsymbol{\\mu}}_{\\boldsymbol{\\theta}}(\\mathbf{\\boldsymbol{a}}_{t},\\mathbf{\\boldsymbol{s}},t),\\Sigma_{\\boldsymbol{\\theta}}(\\mathbf{\\boldsymbol{a}}_{t},\\bar{\\mathbf{\\boldsymbol{s}}_{\\boldsymbol{\\ell}}}))$ . We choose to parameterize $\\pi_{\\boldsymbol{\\theta}}(\\boldsymbol{a}|\\boldsymbol{s})$ like DDPM [18], which sets $\\Sigma_{\\theta}(a_{t},s,t)=\\beta_{t}I$ to fixed time-dependent constants, and constructs the mean $\\pmb{\\mu}_{\\theta}$ from a noise prediction model as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mu_{\\theta}(a_{t},s,t)=\\frac{1}{\\sqrt{\\alpha_{t}}}\\left(a_{t}-\\frac{\\beta_{t}}{\\sqrt{1-\\bar{\\alpha}_{t}}}\\epsilon_{\\theta}(a_{t},s,t)\\right),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\begin{array}{r}{\\alpha_{t}=1-\\beta_{t},\\bar{\\alpha}_{t}=\\prod_{k=1}^{t}\\alpha_{k}}\\end{array}$ , and $\\epsilon_{\\theta}$ is a parametric model. ", "page_idx": 4}, {"type": "text", "text": "To obtain an action from DDPM, we need to draw samples from $T$ different Gaussian distributions sequentially. The sampling process can be reformulated as ", "page_idx": 4}, {"type": "equation", "text": "$$\na_{t-1}=\\frac{1}{\\sqrt{\\alpha_{t}}}\\left(a_{t}-\\frac{\\beta_{t}}{\\sqrt{1-\\bar{\\alpha}_{t}}}\\epsilon_{\\theta}(a_{t},s,t)\\right)+\\sqrt{\\beta_{t}}\\epsilon,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "with the reparametrization trick, where $\\pmb{\\epsilon}\\sim\\mathcal{N}(0,\\pmb{I})$ , $t$ is the reverse timestep from $T$ to $0$ , $a_{T}\\sim$ $\\mathcal{N}(0,I)$ . ", "page_idx": 4}, {"type": "text", "text": "4.2 Diffusion Policy Learning ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In integrating diffusion policy with offilne RL, policy improvement relies on minimizing the behaviorcloning term. However, in online RL, without a dataset to imitate, we discarded the behavior-cloning term and the imitation learning framework. In this study, the policy-learning objective is to maximize the expected Q-values of the actions generated by the diffusion network given the state: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\theta}\\mathbb{E}_{s\\sim\\mathcal{B},\\mathbf{a}_{0}\\sim\\pi_{\\theta}(\\cdot|s)}\\left[Q_{\\phi}(s,\\mathbf{a}_{0})\\right].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Unlike the traditional reverse diffusion process, our study requires recording the gradient of the whole process. The gradient of the Q-value function with respect to the action is backpropagated through the entire diffusion chain. ", "page_idx": 4}, {"type": "text", "text": "Policy improvement is introduced above; next, we introduce policy evaluation. The Q-value function is learned through a conventional approach, which involves minimizing the Bellman operator [13, 25, 38] with the double Q-learning trick [40]. We built two Q-networks $\\bar{Q_{\\phi_{1}}}(s,a),Q_{\\phi_{2}}(\\bar{s},a)$ , and target network $Q_{\\phi_{1}^{\\prime}}(s,a),Q_{\\phi_{2}^{\\prime}}(s,a)$ . Then we give the objective function of policy evaluation, which is shown as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\phi_{i}}\\mathbb{E}_{(s,a,s^{\\prime})\\sim\\mathcal{B}}\\left[\\left(\\left(r(s,\\pmb{a})+\\gamma\\operatorname*{min}_{i=1,2}Q_{\\phi_{i}^{\\prime}}(s^{\\prime},\\pmb{a}^{\\prime})\\right)-Q_{\\phi_{i}}(\\pmb{s},\\pmb{a})\\right)^{2}\\right],\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\textbf{\\em a}^{'}$ is obtained by inputting the $s^{'}$ into the diffusion policy, $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ means replay buffer. Building on this, we employ the tricks in DSAC [11, 10] to mitigate the problem of Q-value overestimation. ", "page_idx": 4}, {"type": "text", "text": "The diffusion policy we construct can be directly combined with mainstream RL algorithms that do not require policy entropy. However, training with the above diffusion policy learning method suffers from overly deterministic policy actions, resulting in poor performance of the final diffusion policy. In the next section, we will propose entropy estimation to solve this problem and obtain diffusion policy with SOTA performance. ", "page_idx": 4}, {"type": "text", "text": "4.3 Diffusion Policy with Entropy ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The diffusion policy\u2019s distribution lacks an analytic expression, so we cannot directly determine its entropy. However, in the same state, we can use multiple samples to obtain a series of actions. By fitting these action points, we can estimate the action distribution corresponding to the state. ", "page_idx": 4}, {"type": "text", "text": "In this paper, we use Gaussian mixture model (GMM) to fti the policy distribution. The GMM forms a complex probability density function by combining multiple Gaussian distributions, which can be represented as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\hat{f}(\\pmb{a})=\\sum_{k=1}^{K}w_{k}\\cdot\\mathcal{N}(\\pmb{a}|\\pmb{\\mu}_{k},\\pmb{\\Sigma}_{k}),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $K$ is the number of Gaussian distributions, and $w_{k}$ is the mixing weight of the $k$ -th component, satisfying $\\begin{array}{r}{\\sum_{k=1}^{K}w_{k}=1,w_{k}\\geq0.\\;\\pmb{\\mu}_{k},}\\end{array}$ $\\Sigma_{k}$ are the mean and covariance matrices of the $k$ -th Gaussian distribution, respectively. ", "page_idx": 5}, {"type": "text", "text": "For each state, we use a diffusion policy to sample $N$ actions, $\\pmb{a}^{1},\\pmb{a}^{2},\\dots,\\pmb{a}^{N}\\in\\mathcal{A}$ . The ExpectationMaximization algorithm is then used to estimate the parameters of the GMM. In the expectation step, the posterior probability that each data point $\\pmb{a}^{i}$ belongs to each component $k$ is computed, denoted as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\gamma(z_{k}^{i})=\\frac{w_{k}\\cdot\\mathcal{N}(\\pmb{a}^{i}|\\pmb{\\mu}_{k},\\pmb{\\Sigma}_{k})}{\\sum_{j=1}^{K}w_{j}\\cdot\\mathcal{N}(\\pmb{a}^{i}|\\pmb{\\mu}_{j},\\pmb{\\Sigma}_{j})},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\gamma(z_{k}^{i})$ denotes that under the current parameter estimates, the observed data $\\pmb{a}^{i}$ come from the $k$ -th component of the probability. In the maximization step, the results of the Eq. (13) calculations are used to update the parameters and mixing weights for each component: ", "page_idx": 5}, {"type": "equation", "text": "$$\nw_{k}=\\frac{1}{N}\\sum_{i=1}^{N}\\gamma(z_{k}^{i}),\\mu_{k}=\\frac{\\sum_{i=1}^{N}\\gamma(z_{k}^{i})\\cdot a^{i}}{\\sum_{i=1}^{N}\\gamma(z_{k}^{i})},\\Sigma_{k}=\\frac{\\sum_{i=1}^{N}\\gamma(z_{k}^{i})(a^{i}-\\mu_{k})(a^{i}-\\mu_{k})^{\\mathrm{T}}}{\\sum_{i=1}^{N}\\gamma(z_{k}^{i})}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Iterative optimization continues until parameter convergence. Based on our experimental experience in the MuJoCo environments, a general setting of $K=3$ provides a better fti to the action distribution. According to Eq. (12), we can estimate the entropy of the action distribution corresponding to the state by [19] ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{H}_{s}\\approx-\\sum_{k=1}^{K}w_{k}\\log w_{k}+\\sum_{k=1}^{K}w_{k}\\cdot\\frac{1}{2}\\log\\left((2\\pi e)^{d}|\\Sigma_{k}|\\right),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $d$ is the dimension of action. Then, the mean of the entropy of the actions associated with the chosen batch of states is used as the estimated entropy $\\hat{\\mathcal{H}}$ of the diffusion policy. ", "page_idx": 5}, {"type": "text", "text": "Similar to maximizing entropy RL, we learn a parameter $\\alpha$ based on the estimated entropy. We update this parameter using ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\alpha\\leftarrow\\alpha-\\beta_{\\alpha}[\\hat{\\mathcal{H}}-\\overline{{\\mathcal{H}}}],\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\overline{{\\mathcal{H}}}$ is target entropy. Finally, we use $\\pmb{a}=\\pmb{a}+\\lambda\\alpha\\cdot\\mathcal{N}(0,\\pmb{I})$ to adjust the diffusion policy entropy during training, where $\\lambda$ is a hyperparameter and $\\textbf{\\em a}$ is the output of diffusion policy. Additionally, no noise is added during the evaluation phase. We summarize our implementation in Algorithm 1. ", "page_idx": 5}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We evaluate the performance of our method in some control tasks of RL within MuJoCo [39]. The benchmark tasks utilized in this study are depicted in Fig. 5, including Humanoid-v3, Ant-v3, HalfCheetah-v3, Walker2d-v3, InvertedDoublePendulum-v3, Hopper-v3, Pusher-v2, and Swimmerv3. Moreover, we conducted experiments in a multi-goal task to demonstrate the excellent representational and exploratory capabilities of our diffusion policy. We also provide ablation studies on the critical components for better understanding. All baseline algorithms are available in GOPS [42], an open-source RL solver developed with PyTorch. ", "page_idx": 5}, {"type": "text", "text": "Baselines. Our algorithm is compared and evaluated against the six well-known model-free algorithms. These include DDPG [33], TD3 [12], PPO [32], SAC [16], DSAC [11, 10], and TRPO [31]. These baselines have been extensively tested and applied in a series of demanding domains. ", "page_idx": 5}, {"type": "text", "text": "Input: \u03bb, \u03b8, $\\phi_{1}$ , $\\phi_{2}$ , $\\phi_{1}^{'}$ , $\\phi_{2}^{'}$ , \u03b1, \u03b2q, \u03b2\u03b1, $\\beta_{\\pi}$ , and $\\rho$   \nfor each iteration do for each sampling step do Sample $a\\sim\\pi_{\\theta}(\\cdot|s)$ by Eq. (7) Add noise $\\pmb{a}=\\pmb{a}+\\lambda\\alpha\\cdot\\mathcal{N}(0,\\pmb{I})$ Get reward $\\pmb{r}$ and new state $s^{\\prime}$ Store a batch of samples $(s,a,r,s^{\\prime})$ in replay buffer $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ end for for each update step do Sample data from $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ Update critic networks using $\\phi_{i}\\leftarrow\\phi_{i}-\\beta_{q}\\nabla_{\\phi_{i}}\\mathcal{L}_{q}(\\phi_{i})$ for $i=\\{1,2\\}$ Update diffusion policy network using $\\theta\\leftarrow\\theta-\\beta_{\\pi}\\nabla_{\\theta}\\mathcal{L}_{\\pi}(\\theta)$ if step mod $10000==0$ then Estimate the entropy of diffusion policy $\\mathcal{\\hat{H}}=\\mathbb{E}_{s\\sim\\mathcal{B}}\\left[\\mathcal{H}_{s}\\right]$ Update $\\alpha$ using Eq. (16) Update target networks using $\\phi_{i}^{\\prime}=\\rho\\phi_{i}^{\\prime}+(1-\\rho)\\phi_{i}$ for $i=\\{1,2\\}$ end for   \nend for ", "page_idx": 6}, {"type": "text", "text": "Experimental details. To ensure a fair comparison, we incorporated the diffusion policy as a policy approximation function within GOPS and implemented DACER with JAX, which improves training speed by 4-5 times compared to PyTorch while maintaining consistent performance. All algorithms and tasks use the same three-layer MLP neural network with GeLU [17] or Mish [27] activation functions, the latter used only for the noise prediction network in the diffusion policy. Initially, we encode timestep $t$ into 16 dimensions using sinusoidal embedding [41], then merge this encoded result with the state $\\pmb{s}$ and action $\\pmb{a}_{t}$ during the current denoising step, and input it into the prediction noise network to generate the output. The impact of the reverse diffusion step size, $T$ , on the experimental results will be examined in the ablation experiments. $T$ is set to 20 for all experiments eventually. The Adam [23] optimization method is employed for all parameter updates. In this paper, the total training step size for all experiments is set at 1.5 million, with the results of all experiments averaged over five random seeds. The CPU used for the experiment is the AMD Ryzen Threadripper 3960X 24-Core Processor, and the GPU is NVIDIA GeForce RTX 3090Ti. Taking Humanoid-v3 as an example, the time taken to train 1.5 million in the JAX framework is 6 hours. More detailed hyperparameters are provided in Appendix A.2 due to space limits. ", "page_idx": 6}, {"type": "text", "text": "Evaluation Protocol. In this paper, we use the same assessment metrics as DSAC. For each seed, the metric is derived by averaging the highest return values observed during the final $10\\%$ of iteration steps in each run, with evaluations conducted every 15,000 iterations. Each assessment result is the average of ten episodes. The results from the five seeds are then aggregated to calculate the mean and standard deviation. Additionally, the training curves in Fig. 1 provide insights into the stability of the training process. ", "page_idx": 6}, {"type": "text", "text": "5.1 Comparative Evaluation ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Each algorithm was subjected to five distinct tests, utilizing a variety of consistent random seeds to ensure robustness in the results. Fig. 1 and Table 1 display the learning curves and performance strategies, respectively. Our comprehensive findings reveal that across all evaluated tasks, the DACER algorithm consistently matched or surpassed the performance of all competing benchmark algorithms. Specifically, in the Humanoid-v3 scenario, our algorithm demonstrated enhancements of $124.7\\%$ , $111.1\\%$ , $73.1\\%$ , $27.3\\%$ , $9.8\\%$ , and $1131.9\\%$ over DDPG, TD3, PPO, SAC, DSAC, and TRPO, respectively. ", "page_idx": 6}, {"type": "text", "text": "5.2 Policy Representation Experiment ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we conduct an experiment to confirm the representation capability of the diffusion policy. We use an environment called \"Multi-goal\" [15], as shown in Fig. 2, where the $x$ -axis and $y$ -axis represent 2D states. In this setup, the agent is represented as a 2D point mass situated on a $7*7$ plane. The objective for the agent is to navigate towards one of four symmetrically positioned points: $(0,5)$ , $(0,-5)$ , $(5,0)$ , and $\\bar{(-5,0)}$ . Since the goal positions are symmetrically distributed at the four points, a policy with strong representational capacity should enable the Q-function to learn the four symmetric peaks across the entire state space. This result reflects the policy\u2019s capacity for exploration in understanding the environment. ", "page_idx": 6}, {"type": "image", "img_path": "l0c1j4QvTq/tmp/760384ad384e1306b657bf3747b21639f35775bc8ad290c5e5a4099d7080fe06.jpg", "img_caption": ["Figure 1: Training curves on benchmarks. The solid lines represent the mean, while the shaded regions indicate the $95\\%$ confidence interval over five runs. The iteration of PPO and TRPO is measured by the number of network updates. "], "img_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "l0c1j4QvTq/tmp/ce3254651236ee9191de01a0b5f6796a0584d349af47cb5d70595aa1360d8db4.jpg", "table_caption": ["Table 1 Average final return. Computed as the mean of the highest return values observed in the final $10\\%$ of iteration steps per run, with an evaluation interval of 15,000 iterations. The maximum value for each task is bolded. $\\pm$ corresponds to standard deviation over five runs. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "We compare the performance of DACER with DSAC, TD3, and PPO, as shown in Fig. 2. The results show that DACER\u2019s actions are likely to point to the nearest peak in different states. DACER\u2019s value function curve shows four symmetrical peaks, aligning with the previous analysis. Compared to DSAC, our method learns a better policy representation, mainly due to using a diffusion policy instead of an MLP. In contrast, TD3 and PPO generate more random actions with poorer policy representation, lacking the symmetrical peaks in their value function curves. Overall, our method demonstrates superior representational capability. ", "page_idx": 7}, {"type": "text", "text": "To demonstrate the powerful multimodality of DACER, we select five points requiring multimodal policies: (0.5, 0.5), (0.5, -0.5), (-0.5, -0.5), (-0.5, 0.5), and $(0,0)$ . For each point, we sampled 100 trajectories. The trajectories are plotted in Fig. 3. The results show that compared with DSAC, DACER exhibits strong multimodality. This also explains why only the Q-function of DACER can learn the nearly perfectly symmetrical four peaks. ", "page_idx": 7}, {"type": "text", "text": "5.3 Ablation Study ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we analyze why DACER outperforms all other baseline algorithms on MuJoCo tasks. We conduct ablation experiments to investigate the impact of the following three aspects on the performance of the diffusion policy: 1) whether adding Gaussian noise to the final output action of the diffusion policy; 2) whether the standard deviation of the added Gaussian noise can be adaptively adjusted by estimated entropy; 3) different reverse diffusion step size $T$ . ", "page_idx": 7}, {"type": "image", "img_path": "l0c1j4QvTq/tmp/834a800aeb927e14ff68b56d510f0efbe8527a2d58ba4fb028f86273d2db3e18.jpg", "img_caption": ["Figure 2: Policy representation comparison of different policies on a multimodal environment. The first row exhibits the policy distribution. The length of the red arrowheads denotes the size of the action vector, and the direction of the red arrowheads denotes the direction of actions. The second row shows the value function of each state point. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "l0c1j4QvTq/tmp/2f6225912c8153d02b6df777901d05cb5481ea9893aa2d0f3c96b18ed5ea65ae.jpg", "img_caption": ["Figure 3: Multi-goal multimodal experiments. We selected 5 points that require multimodal policies: $(0,0)$ , (-0.5, 0.5), (0.5, 0.5), (0.5, -0.5), (-0.5, -0.5), and sampled 100 trajectories for each point. The top row shows the experimental results of DACER, another shows the experimental results of DSAC. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Only Q-learning. In section 4.2, we propose a method using the reverse diffusion process as a policy approximator, which can be combined with the non-maximizing entropy RL algorithm. However, the diffusion policy trained without entropy exhibits poor exploratory properties, leading to suboptimal performance. Using Walker2d-v3 as an example, we compared the training curves of this method with the DACER algorithm, as shown in Fig. 4(a). ", "page_idx": 8}, {"type": "text", "text": "Fixed and linear decay noise factor. In order to verify that using the estimated entropy to adaptively adjust the noise factor plays an important role in the final performance, we conducted the following two experiments in the Walker2d-v3 task: 1) Fixed noise factor to 0.1; 2) The noise factor starts from 0.27 and linearly decreases to 0.1 during the training process. These two values were chosen because the starting and ending noise factor for adaptive tuning in this setting is about in this range. As shown in Fig. 4(b), our method of adaptively adjusting the noise factor based on the estimated entropy achieves the best performance. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Diffusion steps. We further examined the performance of the diffusion policy as the number of diffusion timesteps $T$ varied. We used the Walker2d-v3 task to plot training curves for $T=10,20$ , and 30, as shown in Fig. 4(c). Experimental results indicate that a larger number of diffusion steps does not necessarily lead to better performance. Excessive diffusion steps can cause gradient explosion, significantly reducing the performance of diffusion policy. After balancing performance and computational efficiency, we selected 20 diffusion steps for all experiments. ", "page_idx": 9}, {"type": "image", "img_path": "l0c1j4QvTq/tmp/8e00470fe198d6130d55a9be0f0f964e33642988c2de9a75c337d0322a90ddb1.jpg", "img_caption": [], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Figure 4: Ablation experiment curves. (a) DAC stands for not using the entropy regulator. DACER\u2019s performance on Walker2d-v3 is far better than DAC. (b) Adaptive tuning of the noise factor based on the estimated entropy achieved the best performance compared to fixing the noise factor or using the adaptive tuning method with initial, end values followed by a linear decay method. (c) The best performance was achieved with diffusion steps equal to 20, in addition to the instability of the training process when equal to 30. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this study, we propose the diffusion actor-critic with entropy regulator (DACER) algorithm, a novel RL method designed to overcome the limitations of traditional RL methods that use diagonal Gaussian distributions for policy parameterization. By utilizing the inverse process of the diffusion model, DACER effectively handles multimodal distributions, enabling the creation of more complex policies and improving policy performance. A significant challenge arises from the lack of analytical expressions to determine the entropy of a diffusion strategy. To address this, we employ GMM to estimate entropy, thereby facilitating the learning of a key parameter, $\\alpha$ , which adjusts the explorationexploitation balance by regulating the noise variance in the action output. Empirical tests on the MuJoCo benchmark and a multimodal task show the superior performance of DACER. ", "page_idx": 9}, {"type": "text", "text": "7 Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This study is supported by National Key R&D Program of China with 2022YFB2502901, and Tsinghua University Initiative Scientific Research Program. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Rishabh Agarwal, Dale Schuurmans, and Mohammad Norouzi. An optimistic perspective on offline reinforcement learning. In International Conference on Machine Learning, pages 104\u2013114. PMLR, 2020.   \n[2] Anurag Ajay, Yilun Du, Abhi Gupta, Joshua Tenenbaum, Tommi Jaakkola, and Pulkit Agrawal. Is conditional generative modeling all you need for decision-making? The Eleventh International Conference on Learning Representations, 2023.   \n[3] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023.   \n[4] David Brandfonbrener, Will Whitney, Rajesh Ranganath, and Joan Bruna. Offline rl without off-policy evaluation. Advances in Neural Information Processing Systems, 34:4933\u20134946, 2021.   \n[5] Yuhui Chen, Haoran Li, and Dongbin Zhao. Boosting continuous control with consistency policy. arXiv preprint arXiv:2310.06343, 2023.   \n[6] Cheng Chi, Siyuan Feng, Yilun Du, Zhenjia Xu, Eric Cousineau, Benjamin Burchfiel, and Shuran Song. Diffusion policy: Visuomotor policy learning via action diffusion. arXiv preprint arXiv:2303.04137, 2023.   \n[7] Felipe Codevilla, Eder Santana, Antonio M L\u00f3pez, and Adrien Gaidon. Exploring the limitations of behavior cloning for autonomous driving. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9329\u20139338, 2019.   \n[8] Florinel-Alin Croitoru, Vlad Hondru, Radu Tudor Ionescu, and Mubarak Shah. Diffusion models in vision: A survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023.   \n[9] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in Neural Information Processing Systems, 34:8780\u20138794, 2021.   \n[10] Jingliang Duan, Yang Guan, Shengbo Eben Li, Yangang Ren, Qi Sun, and Bo Cheng. Distributional soft actor-critic: Off-policy reinforcement learning for addressing value estimation errors. IEEE Transactions on Neural Networks and Learning Systems, 33(11):6584\u20136598, 2021.   \n[11] Jingliang Duan, Wenxuan Wang, Liming Xiao, Jiaxin Gao, and Shengbo Eben Li. Dsac-t: Distributional soft actor-critic with three refinements. arXiv preprint arXiv:2310.05858, 2023.   \n[12] Scott Fujimoto, Herke Hoof, and David Meger. Addressing function approximation error in actor-critic methods. In International Conference on Machine Learning, pages 1587\u20131596. PMLR, 2018.   \n[13] Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without exploration. In International Conference on Machine Learning, pages 2052\u20132062. PMLR, 2019.   \n[14] Yang Guan, Yangang Ren, Qi Sun, Shengbo Eben Li, Haitong Ma, Jingliang Duan, Yifan Dai, and Bo Cheng. Integrated decision and control: Toward interpretable and computationally efficient driving intelligence. IEEE Transactions on Cybernetics, 53(2):859\u2013873, 2022.   \n[15] Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement learning with deep energy-based policies. In International Conference on Machine Learning, pages 1352\u20131361. PMLR, 2017.   \n[16] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Offpolicy maximum entropy deep reinforcement learning with a stochastic actor. In International Conference on Machine Learning, pages 1861\u20131870. PMLR, 2018.   \n[17] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415, 2016.   \n[18] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems, 33:6840\u20136851, 2020.   \n[19] Marco F Huber, Tim Bailey, Hugh Durrant-Whyte, and Uwe D Hanebeck. On entropy approximation for gaussian mixture random vectors. In 2008 IEEE International Conference on Multisensor Fusion and Integration for Intelligent Systems, pages 181\u2013188. IEEE, 2008.   \n[20] Bingyi Kang, Xiao Ma, Chao Du, Tianyu Pang, and Shuicheng Yan. Efficient diffusion policies for offilne reinforcement learning. Conference and Workshop on Neural Information Processing Systems, 2023.   \n[21] Bingyi Kang, Xiao Ma, Chao Du, Tianyu Pang, and Shuicheng Yan. Efficient diffusion policies for offline reinforcement learning. Advances in Neural Information Processing Systems, 36, 2024.   \n[22] Elia Kaufmann, Leonard Bauersfeld, Antonio Loquercio, Matthias M\u00fcller, Vladlen Koltun, and Davide Scaramuzza. Champion-level drone racing using deep reinforcement learning. Nature, 620(7976):982\u2013987, 2023.   \n[23] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.   \n[24] Vijay Konda and John Tsitsiklis. Actor-critic algorithms. Advances in Neural Information Processing Systems, 12, 1999.   \n[25] S Eben Li. Reinforcement Learning for Sequential Decision and Optimal Control. Springer Verlag, Singapore, 2023.   \n[26] Abdoulaye O Ly and Moulay Akhlouf.i Learning to drive by imitation: An overview of deep behavior cloning methods. IEEE Transactions on Intelligent Vehicles, 6(2):195\u2013209, 2020.   \n[27] Diganta Misra. Mish: A self regularized non-monotonic activation function. arXiv preprint arXiv:1908.08681, 2019.   \n[28] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 4195\u20134205, 2023.   \n[29] Baiyu Peng, Qi Sun, Shengbo Eben Li, Dongsuk Kum, Yuming Yin, Junqing Wei, and Tianyu Gu. End-to-end autonomous driving through dueling double deep q-network. Automotive Innovation, 4:328\u2013337, 2021.   \n[30] Michael Psenka, Alejandro Escontrela, Pieter Abbeel, and Yi Ma. Learning a diffusion model policy from rewards via q-score matching. arXiv preprint arXiv:2312.11752, 2023.   \n[31] John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In International Conference on Machine Learning, pages 1889\u20131897. PMLR, 2015.   \n[32] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.   \n[33] David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller. Deterministic policy gradient algorithms. In International Conference on Machine Learning, pages 387\u2013395. PMLR, 2014.   \n[34] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International Conference on Machine Learning, pages 2256\u20132265. PMLR, 2015.   \n[35] Yang Song, Conor Durkan, Iain Murray, and Stefano Ermon. Maximum likelihood training of score-based diffusion models. Advances in Neural Information Processing Systems, 34:1415\u2013 1428, 2021.   \n[36] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. Advances in Neural Information Processing Systems, 32, 2019.   \n[37] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020.   \n[38] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.   \n[39] Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In Intelligent Robots and Systems, 2012.   \n[40] Hado Van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double q-learning. In Proceedings of the AAAI conference on artificial intelligence, volume 30, 2016.   \n[41] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in Neural Information Processing Systems, 30, 2017.   \n[42] Wenxuan Wang, Yuhang Zhang, Jiaxin Gao, Yuxuan Jiang, Yujie Yang, Zhilong Zheng, Wenjun Zou, Jie Li, Congsheng Zhang, Wenhan Cao, et al. Gops: A general optimal control problem solver for autonomous driving and industrial control applications. Communications in Transportation Research, 3:100096, 2023.   \n[43] Zhendong Wang, Jonathan J Hunt, and Mingyuan Zhou. Diffusion policies as an expressive policy class for offline reinforcement learning. The Eleventh International Conference on Learning Representations, 2023.   \n[44] Saining Xie Xinlei Chen, Zhuang Liu and Kaiming He. Deconstructing denoising diffusion models for self-supervised learning. arXiv preprint arXiv:2401.14404, 2024.   \n[45] Long Yang, Zhixiong Huang, Fenghao Lei, Yucun Zhong, Yiming Yang, Cong Fang, Shiting Wen, Binbin Zhou, and Zhouchen Lin. Policy representation via diffusion probability model for reinforcement learning. arXiv preprint arXiv:2305.13122, 2023.   \n[46] Ruihan Yang, Prakhar Srivastava, and Stephan Mandt. Diffusion probabilistic modeling for video generation. Entropy, 25(10):1469, 2023. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Environmental Details ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A.1 Experimental Environment Introduction ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "The benchmark tasks utilized in this study are depicted in Fig. 5, including Humanoid-v3, Ant-v3, HalfCheetah-v3, Walker2d-v3, InvertedDoublePendulum-v3, Hopper-v3, Pusher-v2, and Swimmerv3. ", "page_idx": 13}, {"type": "image", "img_path": "l0c1j4QvTq/tmp/02cb9457a6c1ffd1e73af7e92c383ab255f3b683603ae8a8a5dd2e755fe6a3fd.jpg", "img_caption": ["Figure 5: Simulation tasks. (a) Humanoid-v3: $(s\\,\\times\\,a)\\,\\in\\,\\mathbb{R}^{376}\\,\\times\\,\\mathbb{R}^{17}$ . (b) Ant-v3: $(s\\,\\times\\,a)\\;\\in$ $\\mathbb{R}^{\\tilde{11}1}\\times\\mathbb{R}^{8}$ . (c) HalfCheetah- $\\cdot\\mathbf{v}3:(s\\times a)\\in\\mathbb{R}^{17}\\times\\mathbb{R}^{6}$ . (d) Walker2d-v3: $(\\boldsymbol{s}\\times\\boldsymbol{a})\\in\\mathbb{R}^{17}\\times\\mathbb{R}^{6}$ . (e) InvertedDoublePendulum-v3: $(s\\times a)\\in\\mathbb{R}^{6}\\times\\mathbb{R}^{1}$ . (f) Hopper-v3: $(s\\times a)\\in\\dot{\\mathbb{R}}^{11}\\times\\dot{\\mathbb{R}}^{3}$ . (g) Pusher-v2: $(s\\times a)\\in\\mathbb{R}^{23}\\times\\mathbb{R}^{7}$ . (h) Swimmer-v3: $(s\\times a)\\in\\mathbb{R}^{8}\\times\\mathbf{\\bar{R}}^{2}$ . "], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "A.2 Training Details on MuJoCo tasks ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Mujoco [39] is a simulation engine primarily designed for research in RL and robotics. It provides a versatile, physics-based platform for developing and testing various RL algorithms. Core features of Mujoco include a highly efficient physics engine, realistic modeling of dynamic systems, and support for complex articulated robots. Currently, it is one of the most recognized benchmark environments for RL and continuous control. ", "page_idx": 13}, {"type": "text", "text": "The hyperparameters of all baseline algorithms are shown in Table 2. Moreover, the hyperparameters of the DACER in the MuJoCo task are shown in Table 3. ", "page_idx": 13}, {"type": "text", "text": "B Limitation and Future Work ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this study, we propose using GMM to estimate the entropy of the diffusion policy and, based on this estimate, learn a parameter $\\alpha$ to balance exploration and exploitation. However, the process of estimating entropy requires a large number of samples and takes a long time (about $40\\,\\mathrm{ms}$ ). Therefore, we estimate the entropy of diffusion policy every 10,000 iterations to reduce the impact on training time. But, this approach prevents perfect integration with maximizing entropy RL. In future work, we will avoid using batch-size data to estimate entropy and find a balance between estimation accuracy and computational efficiency so as to better combine our method with maximizing entropy RL. ", "page_idx": 13}, {"type": "text", "text": "C Positive and Negative Social Impact ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this paper, we propose DACER, an online RL algorithm that uses the reverse diffusion process as a policy approximator. Diffusion policy has powerful multimodal representation capabilities, making it widely applicable in complex environments such as automated manufacturing, autonomous ", "page_idx": 13}, {"type": "table", "img_path": "l0c1j4QvTq/tmp/e4febb2fa3b108ddf4bfedfe72b48e72d05d0341e12c75c3ffa24d866464396c.jpg", "table_caption": ["TABLE 2 DETAILED HYPERPARAMETERS. "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "driving, and industrial control. However, DACER could also enhance the exploratory capabilities and operational efficiency of military AI, potentially posing threats to citizen privacy and security. ", "page_idx": 14}, {"type": "table", "img_path": "l0c1j4QvTq/tmp/6463f47c4aa7b5dab58583acd1fa9c51b4fc667e07e0fb994c223e98306bc91d.jpg", "table_caption": ["TABLE 3 ALGORITHM HYPERPARAMETER "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Claims ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 15}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 15}, {"type": "text", "text": "Justification: In both the abstract and the introduction we give a formulation of the contribution points. These include considering the reverse diffusion process as a novel policy approximator, proposing a Gaussian mixture model method to estimate the entropy of the diffusion policy and learning a parameter $\\alpha$ for the regulation of the policy exploration level, and open-sourcing the code. Experimental results in eight MuJoCo environments as well as one multimodal environment demonstrate the good performance of our method and the strong characterization capability of the diffusion policy. ", "page_idx": 15}, {"type": "text", "text": "Guidelines: ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper. \u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. \u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. ", "page_idx": 15}, {"type": "text", "text": "\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 16}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: In Section B of the appendix, we acknowledge the limitations of our methodology, estimating entropy requires a considerable amount of time, making it difficult to perfectly integrate this method with maximum entropy RL. However, in future work, we will improve the estimation efficiency by reducing the number of states used for estimation. Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 16}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 16}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 16}, {"type": "text", "text": "Justification: ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 16}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Justification: First, in Section 4.3, we provide pseudo-code for the algorithm 1 of diffusion actor-critic with entropy regulator (DACER). Second, in Section 5, we provide details of the experimental environment setup. Finally, among the two tables in the Appendix A.2, we give details of the hyperparameter configuration. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 17}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: All the reproductions of the experimental results of the baseline algorithms can be obtained by running the general optimal control problem solver (GOPS). And GOPS can be searched on github. We provide the PyTorch code for DACER as a function approximator in GOPS. Besides, we have implemented the DACER algorithm in JAX. ", "page_idx": 17}, {"type": "text", "text": "Regarding the DACER algorithm, we give an implementation of all the core parts, but the complete training code we will open source after the review. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code. \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details. ", "page_idx": 17}, {"type": "text", "text": "\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 18}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: In Appendix A.2, we give the design of all hyperparameters, including optimizer selection, learning rate, neural network configuration, and so on. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 18}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Justification: All results of this experiment came from in 5 random seeds. In Section 5, we mention the specific way of evaluation and the fact that all results are presented as mean $\\pm$ standard deviation. Regarding the training curves, the solid lines represent the mean, while the shaded regions indicate the $95\\%$ confidence interval over five runs. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified. ", "page_idx": 18}, {"type": "text", "text": "\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). \u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 19}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: In Section 5, we provide the CPU and GPU models used for training, which are Xeon(R) Platinum 8352V and NVIDIA GeForce RTX 4090, respectively. We take the example of training 1.5 million Humanoid-v3, and the time required to train it in the JAX framework is about 7 hours. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 19}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: We made sure the code was anonymous. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 19}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Justification: In Appendix C, we discuss the potential positive and negative social impacts of our work. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate ", "page_idx": 19}, {"type": "text", "text": "to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. ", "page_idx": 20}, {"type": "text", "text": "\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 20}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 20}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We used the GOPS solver for training and cited the corresponding paper. This open-source code library is licensed under the Apache-2.0 license. Copyright $\\copyright$ 2022 Intelligent Driving Laboratory (iDLab). All rights reserved. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 20}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 21}, {"type": "text", "text": "\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 21}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 21}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 21}]