[{"figure_path": "l0c1j4QvTq/figures/figures_7_1.jpg", "caption": "Figure 1: Training curves on benchmarks. The solid lines represent the mean, while the shaded regions indicate the 95% confidence interval over five runs. The iteration of PPO and TRPO is measured by the number of network updates.", "description": "This figure displays the training curves for eight different MuJoCo control tasks across eight reinforcement learning algorithms: DACER, DDPG, TD3, PPO, SAC, DSAC, and TRPO.  The x-axis represents the number of iterations (or network updates for PPO and TRPO), and the y-axis represents the average return achieved by each algorithm.  Shaded regions around the mean curves show the 95% confidence intervals based on five separate training runs, illustrating the variability in performance.  The plot allows for a direct comparison of the learning progress and overall performance of DACER against other state-of-the-art algorithms.", "section": "5 Experiments"}, {"figure_path": "l0c1j4QvTq/figures/figures_8_1.jpg", "caption": "Figure 2: Policy representation comparison of different policies on a multimodal environment. The first row exhibits the policy distribution. The length of the red arrowheads denotes the size of the action vector, and the direction of the red arrowheads denotes the direction of actions. The second row shows the value function of each state point.", "description": "This figure compares the policy representation of four different reinforcement learning algorithms (DACER, DSAC, TD3, and PPO) in a multimodal environment.  The top row displays a visualization of the learned policy distributions for each algorithm. The length and direction of the red arrows represent the magnitude and direction of the actions predicted by the policy for each state. The bottom row shows 3D surface plots representing the Q-value function (value function) for each algorithm. These plots visualize how the algorithms learn to value different states and actions. The multimodal nature of the environment is evident in the multiple peaks in the Q-value functions of successful algorithms (DACER and DSAC). Algorithms like TD3 and PPO show flatter Q-value functions indicating less ability to discriminate between states and actions in the multimodal environment.", "section": "5.2 Policy Representation Experiment"}, {"figure_path": "l0c1j4QvTq/figures/figures_8_2.jpg", "caption": "Figure 3: Multi-goal multimodal experiments. We selected 5 points that require multimodal policies: (0, 0), (\u22120.5, 0.5), (0.5, 0.5), (0.5, \u22120.5), (\u22120.5, \u22120.5), and sampled 100 trajectories for each point. The top row shows the experimental results of DACER, another shows the experimental results of DSAC.", "description": "This figure compares the experimental results of DACER and DSAC on a multi-goal environment. Five points were selected that demand multimodal policies. For each point, 100 trajectories were sampled and plotted to show the trajectory distribution. The figure aims to visually demonstrate the superior multimodal capabilities of the DACER algorithm compared to DSAC by showing distinct trajectory clusters corresponding to different goal locations.", "section": "5.2 Policy Representation Experiment"}, {"figure_path": "l0c1j4QvTq/figures/figures_9_1.jpg", "caption": "Figure 1: Training curves on benchmarks. The solid lines represent the mean, while the shaded regions indicate the 95% confidence interval over five runs. The iteration of PPO and TRPO is measured by the number of network updates.", "description": "This figure displays the training curves for various reinforcement learning algorithms across multiple MuJoCo benchmark tasks.  The curves show the average total reward obtained over multiple runs. Shaded areas represent the 95% confidence interval, illustrating the variability in performance.  The x-axis represents the number of training iterations, while the y-axis represents the total average return.  Different colors represent different algorithms being compared.", "section": "5 Experiments"}, {"figure_path": "l0c1j4QvTq/figures/figures_13_1.jpg", "caption": "Figure 5: Simulation tasks. (a) Humanoid-v3: (s \u00d7 a) \u2208 R376 \u00d7 R17. (b) Ant-v3: (s \u00d7 a) \u2208 R111 \u00d7 R8. (c) HalfCheetah-v3 : (s \u00d7 a) \u2208 R17 \u00d7 R6. (d) Walker2d-v3: (s \u00d7 a) \u2208 R17 \u00d7 R6. (e) InvertedDoublePendulum-v3: (s \u00d7 a) \u2208 R6 \u00d7 R\u00b9. (f) Hopper-v3: (s \u00d7 a) \u2208 R11 \u00d7 R\u00b3. (g) Pusher-v2: (s \u00d7 a) \u2208 R23 \u00d7 R7. (h) Swimmer-v3: (s \u00d7 a) \u2208 R8 \u00d7 R2.", "description": "This figure shows eight different simulated environments used in the MuJoCo benchmark for evaluating reinforcement learning algorithms. Each subfigure displays a visual representation of one task and specifies the dimensionality of its state space (s) and action space (a), indicating the complexity of each environment.", "section": "A Environmental Details"}]