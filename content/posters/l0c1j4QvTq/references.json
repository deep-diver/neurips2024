{"references": [{"fullname_first_author": "Jonathan Ho", "paper_title": "Denoising diffusion probabilistic models", "publication_date": "2020-12-01", "reason": "This paper introduces the denoising diffusion probabilistic model (DDPM), a foundational model for diffusion models that is the basis of the novel policy function in the current paper."}, {"fullname_first_author": "Richard S Sutton", "paper_title": "Reinforcement learning: An introduction", "publication_date": "2018-01-01", "reason": "This is a foundational textbook in reinforcement learning, providing the background and context for the actor-critic framework used in the current paper."}, {"fullname_first_author": "Tuomas Haarnoja", "paper_title": "Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor", "publication_date": "2018-01-01", "reason": "This paper introduces the Soft Actor-Critic (SAC) algorithm, a key algorithm in the online reinforcement learning field that is used as a comparison in this paper."}, {"fullname_first_author": "John Schulman", "paper_title": "Trust region policy optimization", "publication_date": "2015-01-01", "reason": "This paper introduces the Trust Region Policy Optimization (TRPO) algorithm, another key algorithm used for comparison in the current paper."}, {"fullname_first_author": "Scott Fujimoto", "paper_title": "Addressing function approximation error in actor-critic methods", "publication_date": "2018-01-01", "reason": "This paper addresses the function approximation error problem in actor-critic methods, which is a relevant issue for the current paper's approach."}]}