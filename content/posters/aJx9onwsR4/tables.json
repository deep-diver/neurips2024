[{"figure_path": "aJx9onwsR4/tables/tables_5_1.jpg", "caption": "Table 1: We evaluate models on the following distribution shift benchmarks.", "description": "This table lists the in-distribution (ID) and out-of-distribution (OOD) datasets used in the paper's experiments to evaluate the performance of models under distribution shift.  The datasets cover various domains and types of shifts, including image classification (CIFAR, ImageNet, OfficeHome, WILDS), and natural language processing (MNLI, SNLI, SQUAD).  The ID datasets represent the data used for fine-tuning the models, while the OOD datasets represent data with distribution shifts compared to the training data.", "section": "3.1 VLM-based Image Classification"}, {"figure_path": "aJx9onwsR4/tables/tables_6_1.jpg", "caption": "Table 2: ALine-D MAPE (%) of different sources of diversity for CLIP linear probing and GPT2-Medium/OPT-125M full finetuning. We average the score over all corruptions for CIFAR10C.", "description": "This table presents the Mean Absolute Percentage Error (MAPE) of the ALine-D algorithm for predicting out-of-distribution (OOD) performance.  The MAPE is calculated for three different sources of diversity in model ensembles: random linear heads, data ordering, and data subsetting. The results are shown for four different datasets: CIFAR10C, SQUAD-Shifts Amazon, SQUAD-Shifts Reddit, and SNLI. The table shows that random linear heads yield the lowest MAPE across all datasets, indicating that this diversity strategy is most effective for accurate OOD performance prediction.", "section": "3.3 Summary and Implications"}, {"figure_path": "aJx9onwsR4/tables/tables_8_1.jpg", "caption": "Table 3: The MAPE (%) of predicting OOD performance using AGL-based ALine and other baseline methods. We collect a diverse ensemble by randomizing the linear initialization and including multiple base models. *We filter out shifts with low correlation in agreement.", "description": "This table presents the Mean Absolute Percentage Error (MAPE) in estimating out-of-distribution (OOD) performance using two AGL-based methods (ALine-D and ALine-S), and several other baseline methods.  The diverse ensemble was constructed by using multiple base models and randomizing the linear initialization.  The asterisk (*) indicates that datasets with low correlation in agreement were excluded.", "section": "5 Estimating OOD Accuracy using AGL in Diverse Ensembles"}, {"figure_path": "aJx9onwsR4/tables/tables_16_1.jpg", "caption": "Table 4: Correlation coefficient between OOD Accuracy and Prediction for ALine-D and ProjNorm", "description": "This table compares the correlation coefficient between the predicted OOD accuracy from ALine-D and ProjNorm methods against the actual OOD accuracy for two different datasets: SQUAD-Shifts Amazon and SQUAD-Shifts Reddit.  It demonstrates the stronger correlation of ALine-D's predictions with the ground truth OOD accuracy compared to ProjNorm.", "section": "2.3 Accuracy and agreement on the line"}, {"figure_path": "aJx9onwsR4/tables/tables_17_1.jpg", "caption": "Table 3: The MAPE (%) of predicting OOD performance using AGL-based ALine and other baseline methods. We collect a diverse ensemble by randomizing the linear initialization and including multiple base models. *We filter out shifts with low correlation in agreement.", "description": "This table presents the Mean Absolute Percentage Error (MAPE) for various OOD performance estimation methods.  It compares AGL-based methods (ALine-D, ALine-S) against other baselines (Naive Agr, ATC, AC, DOC-Feat). The diverse ensemble was created by using different random linear initializations and multiple base models.  The asterisk (*) indicates that datasets with low correlation in agreement were excluded.", "section": "5 Estimating OOD Accuracy using AGL in Diverse Ensembles"}, {"figure_path": "aJx9onwsR4/tables/tables_18_1.jpg", "caption": "Table 6: CLIP Linear Probing Hyperparameters", "description": "This table shows the hyperparameters used for CLIP linear probing experiments on various datasets.  These hyperparameters include the learning rate and batch size.  Different hyperparameters were used for different datasets to achieve an even distribution of in-distribution accuracies.", "section": "A.2.1 Linear Probing over CLIP for Vision Tasks"}, {"figure_path": "aJx9onwsR4/tables/tables_18_2.jpg", "caption": "Table 13: The average MAPE (%) of ALine estimates of OOD performance of full finetuned CLIP models across all 19 CIFAR10C shifts. As can be seen from Figure 4, only ensembles with diverse random initialization consistently results in smallest MAPE values.", "description": "This table shows the mean absolute percentage error (MAPE) of the ALine algorithm in estimating out-of-distribution (OOD) performance for fully fine-tuned CLIP models across 19 different CIFAR10-C corruption types.  The results are broken down by the source of diversity in the model ensemble (random linear heads, data ordering, data subsetting), highlighting that using random linear heads leads to the lowest MAPE, indicating better OOD performance prediction accuracy.", "section": "3.1 VLM-based Image Classification"}, {"figure_path": "aJx9onwsR4/tables/tables_19_1.jpg", "caption": "Table 3: The MAPE (%) of predicting OOD performance using AGL-based ALine and other baseline methods. We collect a diverse ensemble by randomizing the linear initialization and including multiple base models. *We filter out shifts with low correlation in agreement.", "description": "This table presents the Mean Absolute Percentage Error (MAPE) of different methods in predicting out-of-distribution (OOD) performance.  It compares AGL-based methods (ALine-D, ALine-S) to other baselines (ATC, AC, DOC-Feat, Naive Agr). The diverse ensemble is created by combining models with randomized linear initializations and multiple base models. The table highlights which OOD datasets have been filtered due to a low correlation in agreement.", "section": "5 Estimating OOD Accuracy using AGL in Diverse Ensembles"}, {"figure_path": "aJx9onwsR4/tables/tables_19_2.jpg", "caption": "Table 13: The average MAPE (%) of ALine estimates of OOD performance of full finetuned CLIP models across all 19 CIFAR10C shifts. As can be seen from Figure 4, only ensembles with diverse random initialization consistently results in smallest MAPE values.", "description": "This table shows the average Mean Absolute Percentage Error (MAPE) of the ALine method for estimating out-of-distribution (OOD) performance of CLIP models finetuned on CIFAR10 and evaluated on CIFAR10C.  It compares the performance using different sources of diversity: random linear heads, data ordering, and data subsetting.  The results demonstrate that only the models with diverse random linear head initialization consistently achieve the smallest MAPE, highlighting the importance of this diversity source for accurate OOD performance prediction.", "section": "3.1 VLM-based Image Classification"}, {"figure_path": "aJx9onwsR4/tables/tables_19_3.jpg", "caption": "Table 13: The average MAPE (%) of ALine estimates of OOD performance of full finetuned CLIP models across all 19 CIFAR10C shifts. As can be seen from Figure 4, only ensembles with diverse random initialization consistently results in smallest MAPE values.", "description": "This table shows the mean average percentage error (MAPE) of the ALine algorithm for estimating out-of-distribution (OOD) performance for fully fine-tuned CLIP models on 19 CIFAR10-C corruption datasets.  It compares the performance using three different sources of ensemble diversity: random linear heads, data ordering, and data subsetting.  The results demonstrate that ensembles created using random linear head initialization are significantly more accurate at predicting OOD performance than ensembles created using the other two methods.", "section": "3.1 VLM-based Image Classification"}, {"figure_path": "aJx9onwsR4/tables/tables_19_4.jpg", "caption": "Table 11: Full Finetuning OPT-125M on MNLI", "description": "This table shows the hyperparameters used for full finetuning the OPT-125M language model on the MNLI dataset.  It breaks down the hyperparameters used for two different diversity strategies:  (1) Initialization + Ordering, where the model's initialization and data ordering were varied; and (2) Subsetting (10% of data), where the model used a subset of 10% of the data, with the learning rate as the varied hyperparameter. For each strategy, the table lists the learning rate, weight decay, batch size, and maximum number of epochs.", "section": "3.2 LLM-based Question-Answering and Text Classification"}, {"figure_path": "aJx9onwsR4/tables/tables_20_1.jpg", "caption": "Table 12: The ALine-S MAPE(%) for ensembles trained on each domain of OfficeHome.", "description": "This table presents the Mean Absolute Percentage Error (MAPE) of the ALine-S algorithm for estimating out-of-distribution (OOD) performance.  The results are broken down by the OfficeHome domain used for training (Art, ClipArt, Product, Real) and the source of diversity in the ensemble (Random Linear Heads, Data Ordering, Data Subsetting).  Lower MAPE values indicate better OOD performance estimation.", "section": "A.3.1 OfficeHome Linear Probing Diversity Experiments"}, {"figure_path": "aJx9onwsR4/tables/tables_25_1.jpg", "caption": "Table 13: The average MAPE(%) of ALine estimates of OOD performance of full finetuned CLIP models across all 19 CIFAR10C shifts. As can be seen from Figure 4, only ensembles with diverse random initialization consistently results in smallest MAPE values.", "description": "This table shows the mean absolute percentage error (MAPE) of the ALine algorithm for estimating out-of-distribution (OOD) performance of CLIP models.  The models are fully finetuned on the CIFAR10 dataset and tested on the CIFAR10-C dataset (19 different corruption types).  The table compares the performance of three different methods for creating model ensembles: random linear heads, data ordering, and data subsetting.  Results show that the ensemble with randomly initialized linear heads performs significantly better (lower MAPE) in estimating OOD performance.", "section": "3.1 VLM-based Image Classification"}, {"figure_path": "aJx9onwsR4/tables/tables_29_1.jpg", "caption": "Table 14: The average MAPE (%) of ALine-D OOD performance estimates of OPT-125M models full finetuned on SQUAD.", "description": "This table presents the mean average percentage error (MAPE) of the ALine-D algorithm for estimating out-of-distribution (OOD) performance. The ALine-D algorithm leverages the \"agreement-on-the-line\" phenomenon for OOD estimation. The table shows the MAPE values for three different sources of ensemble diversity: random linear heads, data ordering, and data subsetting, on two different SQUAD-Shifts datasets: Amazon and Reddit.", "section": "A.4 More Experiments on the Effect of Diversity Source for Extractive Question Answering"}, {"figure_path": "aJx9onwsR4/tables/tables_29_2.jpg", "caption": "Table 15: The average MAPE (%) of ALine-D OOD performance estimates of BERT models full finetuned on SQUAD.", "description": "This table shows the Mean Absolute Percentage Error (MAPE) of the ALine-D algorithm in estimating out-of-distribution (OOD) performance for BERT models that were fully fine-tuned on the SQUAD dataset.  The MAPE is calculated for different methods of introducing diversity during training: random linear heads, data ordering, and data subsetting. The results are shown for two OOD datasets: SQUAD-Shifts Amazon and SQUAD-Shifts Reddit. Lower MAPE values indicate better performance of the ALine-D algorithm in predicting OOD performance.", "section": "A.4 More Experiments on the Effect of Diversity Source for Extractive Question Answering"}, {"figure_path": "aJx9onwsR4/tables/tables_41_1.jpg", "caption": "Table 16: The average MAPE (%) of ALine-D performance estimates of accuracy on SNLI.", "description": "This table presents the mean absolute percentage error (MAPE) of the ALine-D method for estimating out-of-distribution (OOD) accuracy on the Stanford Natural Language Inference (SNLI) dataset.  The MAPE is calculated for three different methods of introducing diversity into the model ensembles: Random Linear Heads, Data Ordering, and Data Subsetting.  Lower MAPE values indicate better accuracy of the OOD performance estimation.", "section": "A.5 More Experiments on the Effect of Diversity Source for Text Classification"}, {"figure_path": "aJx9onwsR4/tables/tables_42_1.jpg", "caption": "Table 17: ALine-S MAE (%) of different sources of diversity for generative question-answering on GPT2", "description": "This table presents the Mean Absolute Error (MAE) of the ALine-S algorithm for estimating out-of-distribution (OOD) performance in generative question-answering tasks.  It compares the performance of three different methods for introducing diversity into the model ensemble: random linear heads, data ordering, and data subsetting.  The MAE values are shown for four different SQuAD-Shifts datasets (Amazon, Reddit, New Wiki, and NYT). Lower MAE values indicate better OOD performance prediction.", "section": "A.6.1 Experiments on the effect of Diversity Source"}, {"figure_path": "aJx9onwsR4/tables/tables_42_2.jpg", "caption": "Table 17: ALine-S MAE (%) of different sources of diversity for generative question-answering on GPT2", "description": "This table presents the Mean Absolute Error (MAE) values obtained using the ALine-S method for estimating out-of-distribution (OOD) performance in generative question-answering tasks.  Different sources of diversity in model training (random linear heads, data ordering, and data subsetting) are compared across four different SQUAD-Shifts datasets (Amazon, Reddit, New Wiki, and NYT).  The MAE values reflect the average error in the OOD performance prediction for each dataset and diversity method.", "section": "A.6.1 Experiments on the effect of Diversity Source"}]