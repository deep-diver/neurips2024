{"references": [{"fullname_first_author": "C. Baek", "paper_title": "Agreement-on-the-line: Predicting the performance of neural networks under distribution shift", "publication_date": "2022-12-01", "reason": "This paper introduces the core methodology of agreement-on-the-line, which is the central focus and contribution of the current paper."}, {"fullname_first_author": "J. P. Miller", "paper_title": "Accuracy on the line: on the strong correlation between out-of-distribution and in-distribution generalization", "publication_date": "2021-07-01", "reason": "This paper establishes the crucial accuracy-on-the-line phenomenon, a foundational observation leveraged by the current work."}, {"fullname_first_author": "A. Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-07-01", "reason": "CLIP, introduced in this paper, is a major foundation model used extensively in the experiments of the current work, significantly impacting its empirical findings."}, {"fullname_first_author": "T. Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-12-01", "reason": "This paper's introduction of foundational models (FMs) as powerful few-shot learners directly motivates the focus of the current paper on evaluating FMs."}, {"fullname_first_author": "A. Radford", "paper_title": "Language models are unsupervised multitask learners", "publication_date": "2019-01-01", "reason": "This paper lays the groundwork for understanding the capabilities and applications of large language models (LLMs), a key element in the research context of this work."}]}