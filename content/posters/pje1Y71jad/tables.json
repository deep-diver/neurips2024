[{"figure_path": "pje1Y71jad/tables/tables_6_1.jpg", "caption": "Table 1: Performance comparison among state-of-the-art baselines and Coke on three benchmark datasets in terms of both inferential accuracy and cost saving ($ API fees).", "description": "This table presents a comparison of the performance of the proposed 'Coke' method against several state-of-the-art baselines on three benchmark datasets (CommonsenseQA, OpenBookQA, and MedQA).  The comparison considers two key metrics: inferential accuracy (percentage of correctly answered questions) and cost savings (reduction in API fees for LLMs).  It breaks down the results for various model categories: fine-tuned Language Models (LMs), Knowledge Graph Based Small Models (KGMs), and Large Language Models (LLMs) with both local and API series.  The table shows the dev and test accuracy for each model and dataset, and also calculates percentage improvements in accuracy and cost savings compared to the best performing baseline models (Llama3 and GPT-4).", "section": "4.3 Main Results"}, {"figure_path": "pje1Y71jad/tables/tables_8_1.jpg", "caption": "Table 1: Performance comparison among state-of-the-art baselines and Coke on three benchmark datasets in terms of both inferential accuracy and cost saving ($ API fees).", "description": "This table presents a comparison of the performance of the proposed method (Coke) and various state-of-the-art baselines on three benchmark datasets (CommonsenseQA, OpenBookQA, and MedQA).  The comparison is based on two key metrics: inferential accuracy and cost savings (measured in API fees).  The table shows the accuracy and cost savings for different categories of models: fine-tuned language models, knowledge graph-based small models, and large language models (both local series and API series). It helps in illustrating the effectiveness of Coke in achieving a better balance between accuracy and cost compared to existing methods.", "section": "4.3 Main Results"}]