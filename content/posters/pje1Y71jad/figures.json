[{"figure_path": "pje1Y71jad/figures/figures_1_1.jpg", "caption": "Figure 1: A sketched overview of LLMs and small KGMs in (a) We visualize the Acc./Param size of both pipelines of models in (b) The overlaps among different model predictions are shown in (c).", "description": "This figure provides a comparison of Large Language Models (LLMs) and smaller Knowledge Graph based Models (KGMs) for Knowledge-Based Question Answering (KBQA).  Panel (a) shows a schematic overview of the two approaches. Panel (b) is a scatter plot showing the relationship between model accuracy and the number of parameters for various models. This highlights the trade-off between accuracy and computational cost. Panel (c) uses Venn diagrams to illustrate the overlap in predictions made by different models on the OpenBookQA benchmark dataset. This demonstrates that different models excel at different types of questions.", "section": "1 Introduction"}, {"figure_path": "pje1Y71jad/figures/figures_7_1.jpg", "caption": "Figure 2: A visualization of Pareto frontier of both inferential accuracy and cost saving as budget B increases on three datasets.", "description": "This figure shows the Pareto frontier for both inferential accuracy (represented as error rate) and cost saving as the budget (B) varies.  Points on the lower-left are better, indicating higher accuracy and lower cost.  The plot shows how the proposed Coke method compares to GPT-4 on three benchmark datasets (CommonsenseQA, OpenBookQA, and MedQA) across different budget ratios.  The curves illustrate the trade-off between accuracy and cost, demonstrating how Coke achieves a balance.", "section": "4.4 Hyperparameter Analysis"}, {"figure_path": "pje1Y71jad/figures/figures_7_2.jpg", "caption": "Figure 3: Performance changes based on the search of \u03bb.", "description": "This figure shows how the performance of the model (accuracy) changes as the hyperparameter \u03bb is varied.  The x-axis represents the value of \u03bb, while the y-axis shows the accuracy achieved on three different datasets: CSQA, OBQA, and MedQA.  The plot illustrates the impact of \u03bb on the balance between exploration and exploitation in the model's decision-making.  A suitable value of \u03bb needs to be selected to achieve a balance between accuracy and cost efficiency.", "section": "4.5 Observations on search of \u03bb"}, {"figure_path": "pje1Y71jad/figures/figures_8_1.jpg", "caption": "Figure 4: A 3D toy visualization of the selection regret on three datasets as iteration k goes.", "description": "This figure provides a 3D visualization of the selection regret across three datasets (CommonsenseQA, OpenBookQA, and MedQA) as the number of iterations (k) increases.  The selection regret represents the difference between the expected reward of the best arm (model) and the selected arm in each iteration.  The visualization helps illustrate how the regret evolves and converges over time, offering insights into the model's performance and the effectiveness of the selection strategy. The x-axis represents the number of iterations, the y-axis represents the number of arms (models), and the z-axis shows the selection regret. The color gradient shows the magnitude of the regret.", "section": "4.7 Selection Regret Analysis"}, {"figure_path": "pje1Y71jad/figures/figures_9_1.jpg", "caption": "Figure 5: A case study of the model selection on three domain-specific datasets as k goes. The color changes from deep to shallow indicates an exploration process, while an exploitation reversely.", "description": "This figure visualizes the model selection process of the proposed Coke framework on three datasets: CommonsenseQA, OpenBookQA, and MedQA.  Each subplot represents a dataset and shows a heatmap illustrating the number of times each model (HamQA, ChatGPT, GPT-4) was selected within specific ranges of iterations (k).  The color intensity indicates the frequency of selection, with darker colors representing more frequent selections.  This visualization showcases how Coke balances exploration (trying different models) and exploitation (using the best-performing models) over time. The transitions from lighter to darker shades within each subplot show the progression of the model selection process, indicating how Coke learns and adapts its strategy as more questions are answered.", "section": "4.8 Case Studies"}]