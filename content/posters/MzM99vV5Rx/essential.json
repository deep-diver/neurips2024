{"importance": "This paper is crucial for researchers working on interactive question answering (IQA) and large language models (LLMs). It introduces a novel automatic evaluation framework, addressing the limitations of existing methods and offering a scalable solution for assessing LLM performance in interactive contexts.  The findings could significantly influence the development and evaluation of future LLMs designed for interactive applications, potentially leading to more helpful and engaging human-AI interactions.  Moreover, it directly addresses the high cost associated with human evaluations of IQA systems.", "summary": "IQA-EVAL: An automatic evaluation framework uses LLMs to simulate human-AI interactions and evaluate interactive question answering, achieving high correlation with human judgments.", "takeaways": ["IQA-EVAL, a new automatic evaluation framework for interactive question answering, uses LLMs to simulate human-model interactions.", "IQA-EVAL shows high correlation with human evaluations, offering a scalable and cost-effective alternative to manual assessment.", "Assigning personas to LLMs within IQA-EVAL improves correlations with human evaluations, enabling more nuanced and realistic assessments of interactive AI systems."], "tldr": "Traditional methods for evaluating LLMs in question answering focus on single-turn responses, failing to capture the dynamic nature of human-AI interaction.  Human evaluation, while more accurate, is costly and time-consuming. This creates a need for automatic, scalable evaluation frameworks.  \nIQA-EVAL addresses this by employing an LLM-based evaluation agent to simulate human interactions with an IQA model and automatically assess them. This method uses GPT-4 (or Claude) and shows strong correlation with human evaluations, particularly when personas are assigned to the agent to reflect diverse human interaction styles. The framework was successfully used to evaluate five recent LLMs across complex and ambiguous IQA tasks, significantly reducing the cost associated with human evaluations.", "affiliation": "University of Texas at Dallas", "categories": {"main_category": "Natural Language Processing", "sub_category": "Question Answering"}, "podcast_path": "MzM99vV5Rx/podcast.wav"}