[{"figure_path": "MzM99vV5Rx/tables/tables_5_1.jpg", "caption": "Table 1: IQA-EVAL evaluation results of IQA models (TDA: TextDavinci; TB: TextBabbage; DA: Davinci). Bold numbers indicate they are the most close to human results. The empty set symbol (\u00d8) indicates the number cannot be calculated due to the model's inability to follow instructions and produce a gradable answer.", "description": "This table presents the results of evaluating three different IQA models (TextDavinci, TextBabbage, and Davinci) using the IQA-EVAL framework.  The evaluation metrics include Helpfulness, Fluency, the Number of Queries, and Accuracy.  Human evaluations are included as a baseline for comparison, and bold numbers highlight the IQA-EVAL results that are closest to the human evaluations.  Empty sets indicate that the model couldn't follow instructions well enough to produce a gradable answer.", "section": "4.1 Experiment Settings"}, {"figure_path": "MzM99vV5Rx/tables/tables_5_2.jpg", "caption": "Table 2: Pearson Correlation (\u03c1) between IQA-EVAL evaluations and human judgments.", "description": "This table presents the Pearson correlation coefficients between the automatic IQA-EVAL evaluations and human judgments for three different Large Language Models (LLMs) used as Evaluation Agents (LEAs): GPT-4, Claude, and GPT-3.5.  The correlations are shown for the Helpfulness and Fluency metrics, as well as an overall correlation score.  Higher correlation coefficients indicate a stronger agreement between the automatic and human evaluations, suggesting that the IQA-EVAL framework accurately captures human judgment.", "section": "4.2 Experiment Results"}, {"figure_path": "MzM99vV5Rx/tables/tables_7_1.jpg", "caption": "Table 3: IQA-EVAL evaluation results of IQA models (TDA: TextDavinci; TB: TextBabbage; DA: Davinci). LEAs, based on GPT3.5, are assigned specific personas when representing specific groups of workers.", "description": "This table presents the results of evaluating three different IQA models (TextDavinci, TextBabbage, and Davinci) using the IQA-EVAL framework.  The key aspect is that different personas (Expert, Critical-Thinker, Adaptability-Seeker, Clarity-Seeker) were assigned to the LLM-based Evaluation Agents (LEAs) to simulate diverse user groups. The table shows the performance of each IQA model across different personas, indicating how well each model adapts to various user interaction styles.  Metrics include Helpfulness, Fluency, Number of Queries, and Accuracy.", "section": "5 Effect of Assigning Persona to LLM Evaluation Agent (LEA)"}, {"figure_path": "MzM99vV5Rx/tables/tables_7_2.jpg", "caption": "Table 1: IQA-EVAL evaluation results of IQA models (TDA: TextDavinci; TB: TextBabbage; DA: Davinci). Bold numbers indicate they are the most close to human results. The empty set symbol (\u00d8) indicates the number cannot be calculated due to the model's inability to follow instructions and produce a gradable answer.", "description": "This table presents the results of evaluating three different IQA models (TextDavinci, TextBabbage, and Davinci) using the IQA-EVAL framework.  The evaluation is done by comparing the models' performance against human evaluations across four metrics: Helpfulness, Fluency, Number of Queries, and Accuracy.  The bold numbers highlight the model scores that are closest to the human evaluation for each metric.  The empty set symbol indicates cases where the model failed to follow instructions and produce a meaningful answer, resulting in an inability to calculate certain scores.", "section": "4.1 Experiment Settings"}, {"figure_path": "MzM99vV5Rx/tables/tables_7_3.jpg", "caption": "Table 5: IQA-EVAL benchmarking results on HotpotQA and AmbigQA datasets.", "description": "This table presents the results of benchmarking various LLMs (Large Language Models) using the IQA-EVAL framework on two different question answering datasets: HotpotQA and AmbigQA.  For each LLM and dataset, the table shows the scores for four metrics: Helpfulness, Fluency, Number of Queries (# Queries), and Accuracy. Higher scores for Helpfulness and Fluency indicate better performance, while a lower number of queries and a higher accuracy score reflect improved efficiency and correctness. This provides a comparison of the different LLMs' ability to engage in interactive question-answering tasks and demonstrates the effectiveness of IQA-EVAL in evaluating different models' performance on these complex QA tasks.", "section": "6 Benchmarking LLMs with IQA-EVAL on more Types of Questions"}, {"figure_path": "MzM99vV5Rx/tables/tables_8_1.jpg", "caption": "Table 1: IQA-EVAL evaluation results of IQA models (TDA: TextDavinci; TB: TextBabbage; DA: Davinci). Bold numbers indicate they are the most close to human results. The empty set symbol (\u00d8) indicates the number cannot be calculated due to the model's inability to follow instructions and produce a gradable answer.", "description": "This table presents the results of evaluating three different IQA models (TextDavinci, TextBabbage, and Davinci) using the IQA-EVAL framework.  The evaluation metrics include Helpfulness, Fluency, Number of Queries, and Accuracy.  Human evaluation scores are provided as a baseline for comparison. Bold numbers highlight the IQA-EVAL results closest to the human ratings for each metric and model.  Empty sets indicate cases where the model failed to follow instructions and produce a comparable result.", "section": "4.1 Experiment Settings"}, {"figure_path": "MzM99vV5Rx/tables/tables_8_2.jpg", "caption": "Table 7: Comparison between new prompts and our prompts used in Table 5 on benchmarking LLMs with IQA-EVAL.", "description": "This table compares the results of benchmarking several LLMs using the IQA-EVAL framework with two different sets of prompts. The \"Our Prompts\" column represents the original prompts used in Table 5, while the \"New Prompts\" column shows the results obtained using modified prompts that were designed to be more complex and to include effective debiasing instructions. The table presents the results of the Helpfulness, Fluency, Number of Queries, and Accuracy metrics for each set of prompts and each LLM evaluated.", "section": "6 Benchmarking LLMs with IQA-EVAL on more Types of Questions"}, {"figure_path": "MzM99vV5Rx/tables/tables_9_1.jpg", "caption": "Table 8: IQA-EVAL-Multi-Perspective Results of IQA Models. MP indicates \u201cMulti-Perspective\u201d. Bold numbers indicate they are the closest to human results.", "description": "This table presents the results of the IQA-EVAL framework using a multi-perspective approach, where multiple LLMs (GPT4, Claude, and GPT3.5) evaluate the IQA models.  The table shows the helpfulness, fluency, number of queries, and accuracy scores for three IQA models (TextDavinci, TextBabbage, and Davinci) as evaluated by each of the three LLMs. Bold numbers highlight the scores that are closest to human evaluations, demonstrating the effectiveness of the multi-perspective evaluation method.", "section": "4.2 Experiment Results"}, {"figure_path": "MzM99vV5Rx/tables/tables_9_2.jpg", "caption": "Table 9: Pearson Correlation (p) between IQA-EVAL-Multi-Persepctive evaluations and human judgments.", "description": "This table presents the Pearson correlation coefficients between human evaluations and the IQA-EVAL-Multi-Perspective evaluations (using multiple LEA models) for the metrics of Helpfulness, Fluency, and an Overall score.  It demonstrates the degree of agreement between automatic evaluations and human judgments across different LEA models.", "section": "4.2 Experiment Results"}, {"figure_path": "MzM99vV5Rx/tables/tables_9_3.jpg", "caption": "Table 10: Accuracy of IQA Models (recent LLMs) on two datasets (Non-interactive setting).", "description": "This table presents the accuracy scores achieved by various large language models (LLMs) on two different question-answering datasets: HotpotQA and AmbigQA.  The results are obtained in a non-interactive setting, meaning the LLMs provide direct answers to the questions without engaging in a multi-turn dialogue.  The table allows for comparison of the models' performance across different types of questions and complexity levels.", "section": "6 Benchmarking LLMs with IQA-EVAL on more Types of Questions"}, {"figure_path": "MzM99vV5Rx/tables/tables_18_1.jpg", "caption": "Table 1: IQA-EVAL evaluation results of IQA models (TDA: TextDavinci; TB: TextBabbage; DA: Davinci). Bold numbers indicate they are the most close to human results. The empty set symbol (\u00d8) indicates the number cannot be calculated due to the model's inability to follow instructions and produce a gradable answer.", "description": "This table presents the results of evaluating three different IQA models (TextDavinci, TextBabbage, and Davinci) using the IQA-EVAL framework.  The evaluation metrics include Helpfulness, Fluency, Number of Queries, and Accuracy, each scored on a Likert scale or as a percentage.  Human evaluations are included for comparison. Bold numbers highlight the scores closest to the human evaluations for each model and metric.  The empty set symbol indicates instances where a model could not follow instructions and produce a score.", "section": "4.1 Experiment Settings"}, {"figure_path": "MzM99vV5Rx/tables/tables_19_1.jpg", "caption": "Table 12: IQA-Eval benchmarking results on the Natural Questions by Claude-3", "description": "This table presents the benchmarking results of IQA-Eval on the Natural Questions dataset using Claude-3 as the evaluation agent. It shows the Helpfulness, Fluency, Number of Queries, and Accuracy scores for four different IQA models: GPT3.5, Claude, Llama2, and Zephyr.  Each score is presented on a scale of 5. The results provide insights into the performance of these models in interactive question answering tasks, considering factors beyond simple accuracy.", "section": "6.1 Datasets"}, {"figure_path": "MzM99vV5Rx/tables/tables_19_2.jpg", "caption": "Table 13: IQA-Eval benchmarking results on the Natural Question by GPT-4", "description": "This table presents the results of benchmarking several IQA models (GPT3.5, Claude, Llama2, and Zephyr) on the Natural Questions dataset using the IQA-EVAL framework with GPT-4 as the evaluation agent.  The metrics evaluated include Helpfulness, Fluency, the Number of Queries, and Accuracy.  The values represent average scores across multiple interactions.", "section": "6.1 Datasets"}, {"figure_path": "MzM99vV5Rx/tables/tables_19_3.jpg", "caption": "Table 14: Average number of sentences and accuracy scores of IQA Models (non-interactive setting)", "description": "This table presents the average number of sentences and the accuracy scores achieved by four different IQA models (GPT3.5, Claude, Llama2, and Zephyr) in a non-interactive question-answering setting.  These metrics provide a baseline measure of the models' performance before evaluating their performance within the interactive IQA-EVAL framework described in the paper.", "section": "6.1 Datasets"}, {"figure_path": "MzM99vV5Rx/tables/tables_19_4.jpg", "caption": "Table 15: IQA-EVAL results under different persona distribution on the expert persona.", "description": "This table presents the results of IQA-EVAL under different persona distributions focusing on the \"Expert\" persona. It shows the Helpfulness and Fluency scores for different IQA models (TDA, TB, DA) evaluated by IQA-EVAL with varying proportions of the Expert persona (0%, 20%, 40%, 60%, 80%, and 100%).  The Pearson correlation (\u03c1) between IQA-EVAL scores and human evaluations is also provided for each persona distribution. The last two rows show the human evaluation results and IQA-EVAL results when only pure experts are considered.", "section": "5.2 Experimental Results"}, {"figure_path": "MzM99vV5Rx/tables/tables_20_1.jpg", "caption": "Table 17: Evaluation results of non-interactions (direct answers) between LEA and IQA models.", "description": "This table presents the evaluation results obtained when using Language Model-based Evaluation Agents (LEAs) to assess the direct answers provided by various Interactive Question Answering (IQA) models, without any conversational interaction.  The evaluation metrics include Helpfulness, Fluency, and Accuracy, each scored on a scale seemingly from 0 to 5,  for different LEA models (GPT 3.5, Claude-instant, Llama2-8b, and Zephyr-Alpha) and IQA models (GPT4 and Claude). The results show the performance of IQA models in terms of the quality of their direct responses, as evaluated by the LEAs.", "section": "6 Benchmarking LLMs with IQA-EVAL on more Types of Questions"}, {"figure_path": "MzM99vV5Rx/tables/tables_20_2.jpg", "caption": "Table 18: Evaluating persona biases on offensiveness and harmful metrics. A high score indicates better results.", "description": "This table presents the results of an experiment designed to evaluate the potential biases introduced by different personas in the IQA-EVAL framework.  It assesses the \"offensiveness\" and \"harmfulness\" of responses generated by the model under various personas (Expert, Critical-Thinker, Adaptability-Seeker, Clarity-Seeker) using the RealToxicityPrompts dataset and two evaluation models (IQA-EVAL-GPT3.5 and IQA-EVAL-Claude). A high score indicates better results (less offensive or harmful).  A baseline \"None\" persona is included for comparison.", "section": "G Bias Evaluation"}]