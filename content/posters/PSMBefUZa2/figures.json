[{"figure_path": "PSMBefUZa2/figures/figures_3_1.jpg", "caption": "Figure 1: Overview of the RLGSSL Framework. The prediction networks (student \u03b8s, teacher \u03b8\u0442) serve as the policy functions, and the soft pseudo-labeling (P\u03b8\u0442 (Xu)) acts as the actions. The model has three loss terms in total: RL loss (Lrl), supervised loss (Lsup), and consistency loss (Lcons). The teacher policy function is used to execute the actions and compute the consistency loss, while the student policy function is used for all other aspects.", "description": "This figure illustrates the RLGSSL framework, which formulates semi-supervised learning as a one-armed bandit problem.  The framework consists of a teacher and student network, where the teacher network generates pseudo-labels for unlabeled data. These pseudo-labels, along with the labeled data, are used to create mixup data. A reward function evaluates the performance of the model on this mixup data, guiding the learning process through an RL loss. The framework also incorporates a supervised loss on labeled data and a consistency loss to improve learning stability and enhance generalization.  The teacher network provides stability by using an exponential moving average of the student network's weights.", "section": "3 The Proposed Method"}, {"figure_path": "PSMBefUZa2/figures/figures_9_1.jpg", "caption": "Figure 2: Sensitivity analysis for hyper-parameters \u03bb1 and \u03bb2 on CIFAR-100 using 10000 labeled samples.", "description": "This figure shows the sensitivity analysis for the two hyperparameters (\u03bb1 and \u03bb2) used in the RLGSSL model.  The x-axis represents the values of the hyperparameters, while the y-axis represents the test error. The left subplot shows that \u03bb1 = 0.1 is the optimal value, balancing the supervised loss and preventing overfitting.  The right subplot displays the optimal value of \u03bb2 to be around 0.1, balancing the consistency regularization and preventing over-regularization. The analysis suggests finding an optimal balance between these two losses is important for model performance.", "section": "4.4 Hyper-parameter Analysis"}]