{"references": [{"fullname_first_author": "Francis Bach", "paper_title": "Breaking the curse of dimensionality with convex neural networks", "publication_date": "2017-00-00", "reason": "This paper is foundational for the theoretical analysis of the curse of dimensionality in neural networks and introduces the F1-norm as a key complexity measure."}, {"fullname_first_author": "Mikhail Belkin", "paper_title": "Reconciling modern machine-learning practice and the classical bias-variance trade-off", "publication_date": "2019-00-00", "reason": "This paper provides insights into the implicit bias of DNNs, which is crucial for understanding their generalization capabilities."}, {"fullname_first_author": "Arthur Jacot", "paper_title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks", "publication_date": "2018-00-00", "reason": "This paper introduces the Neural Tangent Kernel (NTK) framework, a key tool for analyzing the behavior of infinitely wide neural networks."}, {"fullname_first_author": "Zeyuan Allen-Zhu", "paper_title": "A convergence theory for deep learning via over-parameterization", "publication_date": "2019-00-00", "reason": "This paper provides a theoretical analysis of the convergence properties of over-parameterized neural networks, supporting the empirical success of DNNs."}, {"fullname_first_author": "Johannes Schmidt-Hieber", "paper_title": "Nonparametric regression using deep neural networks with ReLU activation function", "publication_date": "2020-00-00", "reason": "This paper establishes approximation rates for deep ReLU networks, showing their ability to approximate complex functions in high-dimensional spaces."}]}