{"references": [{"fullname_first_author": "S. Biderman", "paper_title": "Pythia: A suite for analyzing large language models across training and scaling", "publication_date": "2023", "reason": "This paper introduces the Pythia framework, used extensively in the current research for evaluating different tokenization methods."}, {"fullname_first_author": "T. Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020", "reason": "This foundational work establishes LLMs as few-shot learners and highlights their versatility, which is central to the current study's context."}, {"fullname_first_author": "J. Devlin", "paper_title": "Bert: Pre-training of deep bidirectional transformers for language understanding", "publication_date": "2018", "reason": "This paper introduces BERT, a significant LLM architecture, and its tokenization methods, which serves as a baseline for the current research."}, {"fullname_first_author": "R. Sennrich", "paper_title": "Neural machine translation of rare words with subword units", "publication_date": "2015", "reason": "This work introduces Byte Pair Encoding (BPE), a prominent subword tokenization technique, that is compared with the proposed method in this study."}, {"fullname_first_author": "T. Kudo", "paper_title": "Subword regularization: Improving neural network translation models with multiple subword candidates", "publication_date": "2018", "reason": "This paper introduces Unigram Language Model, another important subword tokenization method, that is used as a baseline in this research."}]}