{"references": [{"fullname_first_author": "Arora, S.", "paper_title": "Stronger generalization bounds for deep nets via a compression approach.", "publication_date": "2018-MM-DD", "reason": "This paper provides theoretical foundations for understanding generalization in deep learning through the lens of compression, a concept that is relevant to the current paper's exploration of model complexity and generalization."}, {"fullname_first_author": "Foret, P.", "paper_title": "Sharpness-aware minimization for efficiently improving generalization.", "publication_date": "2021-MM-DD", "reason": "This paper introduces the Sharpness-Aware Minimization (SAM) algorithm, a key method compared against in the current work, offering a state-of-the-art approach to improving generalization performance in deep learning."}, {"fullname_first_author": "Ishida, T.", "paper_title": "Do we need zero training loss after achieving zero training error?.", "publication_date": "2020-MM-DD", "reason": "This paper introduces the flooding method, a heuristic for improving test accuracy in classification which serves as the primary motivation and baseline for the current paper's SoftAD method."}, {"fullname_first_author": "Zhao, Y.", "paper_title": "Penalizing gradient norm for efficiently improving generalization in deep learning.", "publication_date": "2022-MM-DD", "reason": "This paper proposes Gradient Norm Penalization (GNP), a regularization technique that is closely related to SAM and relevant to the current paper's investigation of techniques for improving generalization and controlling model norms."}, {"fullname_first_author": "Zhang, C.", "paper_title": "Understanding deep learning requires rethinking generalization.", "publication_date": "2017-MM-DD", "reason": "This paper highlights the problem of generalization in deep learning, providing crucial background and context to the central theme of the current work's focus on improving generalization performance."}]}