[{"type": "text", "text": "An Empirical Analysis of Compute-Optimal Inference for Problem-Solving with Language Models ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 The optimal training configurations of large language models (LLMs) with respect   \n2 to model sizes and compute budgets have been extensively studied. But how to   \n3 optimally configure LLMs during inference has not been explored in sufficient   \n4 depth. We study compute-optimal inference: designing models and inference   \n5 strategies that optimally trade off additional inference-time compute for improved   \n6 performance. As a first step towards understanding and designing compute-optimal   \n7 inference methods, we assessed the effectiveness and computational efficiency   \n8 of multiple inference strategies such as Greedy Search, Majority Voting, Best-of  \n9 N, Weighted Voting, and their variants on two different Tree Search algorithms,   \n10 involving different model sizes (e.g., 7B and 34B) and computational budgets. We   \n11 found that a smaller language model with a novel tree search algorithm typically   \n12 achieves a Pareto-optimal trade-off. These results highlight the potential beneftis of   \n13 deploying smaller models equipped with more sophisticated decoding algorithms   \n14 in end-devices to enhance problem-solving accuracy. For instance, we show that   \n15 the Llemma-7B model can achieve competitive accuracy to a Llemma-34B model   \n16 on MATH500 while using $2\\times$ less FLOPs. Our findings could potentially apply to   \n17 any generation task with a well-defined measure of success. ", "page_idx": 0}, {"type": "text", "text": "18 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "19 Scaling laws of neural networks [Hestness et al., 2017, Rosenfeld et al., 2019] have been established   \n20 across a range of domains, including language modeling [Kaplan et al., 2020, Hoffmann et al., 2022,   \n21 OpenAI, 2023], image modeling [Henighan et al., 2020, Yu et al., 2022, Peebles and Xie, 2023],   \n22 video modeling [Brooks et al., 2024], reward modeling [Gao et al., 2023], and board games [Jones,   \n23 2021]. These studies have demonstrated how model performance is influenced by both the size of the   \n24 model and the amount of training computation. However, there is limited knowledge on how varying   \n25 the compute during inference affects model performance after the model has been trained.   \n26 To improve the task performance of large language models (LLMs), inference techniques typically   \n27 involve additional computation in a performance maximization step at inference time [Nye et al.,   \n28 2021, Wei et al., 2022, Wang et al., 2022b, Yao et al., 2023, Chen et al., 2024b]. This cost must be   \n29 taken into account for compute-optimal inference. For example, a Monte Carlo Tree Search (MCTS)   \n30 method [Jones, 2021] may improve task performance, but potentially cost much more than simply   \n31 sampling solutions multiple times. Generally speaking, we need a comprehensive understanding of   \n32 how various inference-time methods (e.g., Best-of-N, majority voting) trade off between performance   \n33 and cost. To improve our understanding, this paper presents a thorough empirical evaluation with   \n34 careful analysis over various configurations of representative LLMs and inference algorithms.   \n35 Specifically, we explore how to select an optimal model size (e.g., 7B or 34B) for the policy model   \n36 and an effective inference strategy (e.g., Greedy Search, Majority Voting, Best-of-N, Weighted Voting,   \n37 and their Tree Search variants) to maximize performance (i.e., accuracy) within a given compute   \n38 budget. We manipulate the inference computation (FLOPs) of a fixed model by generating additional   \n39 tokens through the policy model, sampling further candidate solutions, and ranking them with a   \n40 reward model. We analyze the performance of a family of math-specialized LLMs (i.e., Llemma-7B   \n41 and Llemma-34B [Azerbayev et al., 2023]) fine-tuned on the MetaMath dataset [Yu et al., 2023] and   \n42 measure the error rate on the GSM8K test set [Cobbe et al., 2021a] and MATH500 test set [Hendrycks   \n43 et al., 2021b, Lightman et al., 2023b].   \n44 Our analysis shows that voting-based methods generally outperform the strategy which selects the   \n45 best solution (i.e., Best-of-N), and weighted voting has the most favorable results (Section 4.3,   \n46 Figure 5 & 6). However, neither method shows a desirable behavior at high levels of compute. For   \n47 instance, weighted voting saturates when sampling more than 128 solutions (Figure 1). We have also   \n48 found that the commonly used MCTS method does not perform well with weighted voting, as it often   \n49 yields many unfinished solutions, hence having less votes. To address this issue, we propose a novel   \n50 tree search algorithm, REward BAlanced SEarch (REBASE), which pairs well with weighted voting   \n51 and improves the Pareto-optimal trade-off between accuracy and inference compute. The key idea of   \n52 REBASE is to use a node-quality based reward to control the exploitation and pruning properties of   \n53 tree search, while ensuring enough candidate solutions for voting or selection.   \n54 In our experiments, REBASE consistently outperforms sampling and MCTS methods across all   \n55 settings, models, and tasks. Importantly, we find that REBASE with a smaller language model   \n56 typically achieves a Pareto-optimal trade-off. For instance, we show that the Llemma-7B model can   \n57 achieve competitive accuracy to a Llemma-34B model while using $2\\times$ less FLOPs when evaluating   \n58 on MATH500 (Figure 1) or GSM8K (Figure 4). These findings underscore the advantages of using   \n59 smaller models with advanced inference-time algorithms on end-devices to improve problem-solving. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "image", "img_path": "vYmvgxpgwH/tmp/53c8ab0a3459ada9555588cabec3bddabb5933be1800626e5892ac66e6ae0899.jpg", "img_caption": ["Figure 1: The inference computation scaling laws exhibited in error rate on the MATH500 test set based on weighted majority voting, where the left figure shows sampling vs. MCTS, and the right figure shows our proposed REBASE. Clearly, the error rate decreases steadily when the computation increases, and REBASE exhibits a Pareto-optimal tradeoff during inference. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "60 2 Related Works ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "61 Mathematical Reasoning with LLMs. Large language models have made significant progress   \n62 in recent years, and have exhibited strong reasoning abilities [Brown et al., 2020, Hoffmann et al.,   \n63 2022, Chowdhery et al., 2022, Lewkowycz et al., 2022]. Mathematical problem solving is a key task   \n64 for measuring LLM reasoning abilities [Cobbe et al., 2021a, Hendrycks et al., 2021b]. [Ling et al.,   \n65 2017] first developed the method of producing step by step solutions that lead to the final answer.   \n66 Later, [Cobbe et al., 2021b] extended the work by finetuning the pre-trained language model on a   \n67 large dataset to solve math word problems, a verifier is trained for evaluating solutions and ranking   \n68 solutions. Nye et al. [2021] train models to use a scratchpad and improve their performance on   \n69 algorithmic tasks. Wei et al. [2022] demonstrate that the reasoning ability of a language model can   \n70 be elicited through the prompting. Subsequent research [Kojima et al., 2022, Lewkowycz et al., 2022,   \n71 Zhou et al., 2022] in reasoning tasks has also highlighted the efficacy of rationale augmentation. We   \n72 choose problem solving in mathematics as the task to study the compute-optimal strategy since it   \n73 allows us to accurately evaluate the problem solving ability of LLMs.   \n74 Inference Strategies of LLM Problem Solving. A variety of inference (also called decoding)   \n75 strategies have been developed to generate sequences with a trained model. Deterministic methods   \n76 such as greedy decoding and beam search [Teller, 2000, Graves, 2012] find highly probable sequences,   \n77 often yielding high quality results but without diversity. Sampling algorithms (e.g., temperature   \n78 sampling [Ackley et al., 1985]) can produce a diverse set of results which are then aggregated to   \n79 achieve higher accuracy (e.g., via majority voting [Wang et al., 2022a]). Recent methods combine   \n80 search algorithms with modern LLMs, including breadth-first or depth-first search [Yao et al., 2023],   \n81 Monte-Carlo Tree Search (MCTS) [Zhang et al., 2023, Zhou et al., 2023, Liu et al., 2024, Choi et al.,   \n82 2023], and Self-evaluation Guided Beam Search [Xie et al., 2023]. All of these methods show that   \n83 using search at inference time can lead to performance gains in various tasks. However, the trade-off   \n84 for the improved performance is the use of compute to perform the search. Analyzing the trade-off   \n85 between compute budget and LLM inference performance remains understudied. In this paper, we   \n86 systematically analyze the trade-off between compute budget and problem-solving performance, and   \n87 propose a tree search method that is empirically Pareto-optimal.   \n88 Process Reward Models. Process reward models (PRMs) have emerged as a technique to improve   \n89 the reasoning and problem-solving capabilities of LLMs. These models assign rewards to the   \n90 intermediate steps of the LLM generated sequences. PRMs have been shown effective in selecting   \n91 reasoning traces with a low error rate, and for providing rewards in reinforcement learning-style   \n92 algorithms [Uesato et al., 2022, Polu and Sutskever, 2020, Gudibande et al., 2023]. Ma et al. [2023]   \n93 applies the PRM to give rewards on the intermediate steps and guide the multi-step reasoning process.   \n94 The PRM can be either trained on human labeled data [Lightman et al., 2023a] or model-labeled   \n95 synthetic data [Wang et al., 2023]. In our work, we use the PRM as the reward model for selecting   \n96 generated solutions, and for selecting which partial solutions to explore in tree search. ", "page_idx": 1}, {"type": "image", "img_path": "vYmvgxpgwH/tmp/a2d730e691ddd3ce8cc474185e9b5f4ff42976bf97c6b4ae4f1f03883c289b03.jpg", "img_caption": ["Figure 2: Illustration of compute-optimal scaling laws in training and inference. The Chinchilla scaling law shows how to choose a model size and number of training tokens under a trainingcompute budget, while ours shows how to choose a model size and an inference strategy under a inference-compute budget. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "97 3 An Empirical Analysis of Compute-Optimal Inference for Problem-Solving ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "98 We explore the following question: Given a fixed FLOPs budget, how should one select an optimal   \n99 model size for the policy model, and an effective inference strategy to maximize performance (i.e.,   \n100 accuracy)? To address this, we represent the problem-solving error rate $E(N,T)$ as a function of the   \n101 number of model parameters $N$ and the number of generated tokens $T$ . The computational budget $C$   \n102 is a deterministic function $\\mathrm{FLOPs}(N,T)$ , based on $N$ and $T$ . Our goal is to minimize $E$ under the   \n103 test-time compute constraint ${\\mathrm{FLOPs}}(N,T)=C$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\nN_{o p t}(C),T_{o p t}(C)=\\underset{N,T\\mathrm{~s.t.~}\\mathrm{FLOPs}(N,T)=C}{\\arg\\operatorname*{min}}E(N,T)\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "104 where $N_{o p t}(C)$ and $T_{o p t}(C)$ denote the optimal allocation of a computational budget $C$ . ", "page_idx": 2}, {"type": "text", "text": "105 Here, the inference computation (FLOPs) for a fixed model can be modulated by generating more   \n106 tokens with the policy model, e.g., by sampling additional candidate solutions and subsequently   \n107 ranking them using a reward model. We primarily consider sampling and tree-search approaches   \n108 with reranking or majority voting as the means to consume more tokens, including Greedy Search,   \n109 Majority Voting, Best-of-N, Weighted Voting, and their variants on tree search methods. ", "page_idx": 3}, {"type": "text", "text": "110 3.1 Inference Strategies ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "111 3.1.1 Sampling ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "112 Greedy Search. This strategy generates tokens one at a time by selecting the highest probability token   \n113 at each step, without considering future steps. It is computationally efficient but often suboptimal in   \n114 terms of diversity.   \n115 Best-of-n. This strategy, also known as rejection sampling, samples multiple solutions and chooses   \n116 the one with the highest score given by the reward model.   \n117 Majority Voting. In this strategy, multiple model outputs are generated, and the final answer to the   \n118 problem is determined by the most frequently occurring answer in all the outputs.   \n119 Weighted Majority Voting. This strategy is a variant of majority voting in which the votes are   \n120 weighted based on the score given by the reward model. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "121 3.1.2 Monte Carlo Tree Search (MCTS) ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "122 Monte Carlo Tree Search (MCTS) has proven effective in domains such as board games where   \n123 strategic decision-making is required [Silver et al., 2016, 2017, Jones, 2021]. Recent work has shown   \n124 that adapting MCTS to the context of LLMs can enhance the text generation process [Zhang et al.,   \n125 2023, Zhou et al., 2023, Liu et al., 2024, Choi et al., 2023, Chen et al., 2024a, Tian et al., 2024,   \n126 Chen et al., 2024a]. In this context, MCTS is often paired with a value model to score and guide the   \n127 exploration steps. For additional background, we provide a review of MCTS in Appendix B.   \n128 Recent work in MCTS or its variants (e.g., Tree of Thoughts [Yao et al., 2023]) mainly focus on   \n129 improving the performance (e.g., accuracy) on the studied tasks. However, generic comparisons of   \n130 MCTS with conventional methods like Best-of-n and Majority Voting in terms of computational   \n131 budget, measured in generated tokens or processing time, are either scarce or indicating inference  \n132 time issues. For example, MCTS consumes substantially more resources, often requiring dozens of   \n133 times more generated tokens than simpler methods. Specifically, a significant portion of the paths   \n134 in the search tree are used to estimate and select nodes, and these paths do not necessarily become   \n135 a part of the final candidate solution, although MCTS ensures that the sampled solutions comprise   \n136 high-quality intermediate steps. In contrast, sampling methods generate multiple solutions in parallel   \n137 and independently, and all the generated sequences are included in the candidate solutions. However,   \n138 the intermediate steps in these sequences are not guaranteed to be of high quality, as there is no   \n139 mechanism for pruning poor steps or exploiting promising ones.   \n140 This highlights the need for developing a new tree search method that can achieve a comparable (or   \n141 better) performance as MCTS, and that is computationally less costly, just like weighted majority   \n142 voting and best-of-n. This need leads to the development of our new method named Reward Balanced   \n143 SEarch (REBASE), as introduced next. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "144 3.1.3 Reward Balanced Search (REBASE) ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "145 The REBASE tree search method inherits the exploitation and pruning properties of tree search,   \n146 while using the reward model alone to estimate the nodes\u2019 qualities without additional computation   \n147 for estimating values by sampling children. The efficiency is achieved by constraining the total   \n148 expansion width of the tree at a certain depth. REBASE balances the expansion width among the   \n149 nodes at the same depth based on the rewards given by the Process Reward Model (PRM). The details   \n150 are provided below:   \n151 Notations. We consider the fine-tuned LLM as a policy $\\pi_{\\theta}$ . Given a question $q$ and the first $k$ steps   \n152 of a solution $x_{1},\\cdot\\cdot\\cdot\\,,x_{k}$ , the $(k+1)$ -th step is produced by $\\pi_{\\theta}(x_{k+1}|q,x_{1}\\cdot\\cdot\\cdot x_{k}).$ . When generating   \n153 solutions using tree search, the root of the tree corresponds to the question $q$ . The node corresponding   \n154 to $x_{k+1}$ is the child of the node corresponding to $x_{k}$ if it is sampled from $\\pi_{\\theta}(\\cdot|q,x_{1}\\cdot\\cdot\\cdot\\,,x_{k})$ . The   \n155 reward of a node $n(x_{k})$ is determined by the PRM as $R(n(x_{k}))\\stackrel{*}{=}R(q,x_{1},\\cdots\\,,x_{k})$ .   \n156 Initialization. Given the question $q$ , balance temperature $T_{b}$ , and sampling number of solutions N,   \n157 we sample $_\\mathrm{N}$ instances of the first step for the question, yielding all the nodes of depth 1 in the search   \n158 tree. We set the sampling budget of depth 0 $B_{0}=N$ as initialization.   \n159 Reward modeling and update. In the $i$ -th iteration, the PRM assigns the rewards to all the nodes   \n160 at depth $i$ . After that, the algorithm examines whether the solutions up to depth $i$ are complete.   \n161 Supposing there are $C_{i}$ completed solutions, we update the sampling budget using $B_{i}\\leftarrow B_{i-1}-C_{i}$ .   \n162 If $B_{i}=0$ , the process ends, and we obtain $N$ solutions.   \n163 Exploration balancing and expansion. For all of the nodes $n_{j}$ with reward $R(n_{j})$ in the depth $i$ of   \n164 the tree, we calculate the expansion width of the $n_{j}$ as: ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "image", "img_path": "vYmvgxpgwH/tmp/38fa206fe0ca1706ea67c2a24822c04118a742e611c18765f4947095eb2ab78f.jpg", "img_caption": ["Figure 3: Illustration of one iteration of REward BAlanced SEarch (REBASE). "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\nW_{j}=\\mathrm{Round}\\left(B_{i}\\frac{\\exp\\left(R(n_{j})/T_{b}\\right)}{\\sum_{k}\\exp\\left(R(n_{k})/T_{b}\\right)}\\right).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "165 Then we sample $W_{j}$ children for $n_{j}$ for all the nodes in depth $i$ , and start the next iteration. ", "page_idx": 4}, {"type": "text", "text": "166 3.1.4 Theoretical Analysis ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "167 Before empirically studying the scaling effects of increasing the inference-time compute budget,   \n168 we present two theorems which will help us understand the experimental results later. These two   \n169 theorems give an upper bound on the performance of sampling when fixing the LLM generator.   \n170 We assume the vocabulary is limited and the sequence length is constrained, thus the number of   \n171 possible solutions and answers are finite. The proofs are provided in the Appendix A.   \n172 Theorem 1. Given a test dataset $\\mathcal{D}$ and a LLM $\\pi$ . $|{\\mathcal{A}}|$ is the finite set of all possible answers given   \n173 by LLM, the ground truth function $g$ maps test data $d$ to the true answer. Denote the accuracy of the   \n174 LLM on this dataset with majority over $N$ samples as $A C C_{M V}(\\pi,D,N)$ . The accuracy of majority   \n175 voting on the LLM will eventually saturate, i.e. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{N\\to\\infty}A C C_{M V}(\\pi,\\mathcal{D},N)=\\frac{\\sum_{d\\in\\mathcal{D}}\\mathbb{I}\\left(\\left(g(d)=\\arg\\operatorname*{max}_{a\\in\\mathcal{A}}\\pi(a|d)\\right)\\right.}{|\\mathcal{D}|}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "image", "img_path": "vYmvgxpgwH/tmp/3df599fa64061a8681ed3fe7366e9b3cd43a4def9857901c28436b91fa6e01d3.jpg", "img_caption": ["Figure 4: The inference computation scaling comparisons across different model sizes. The left/right panel shows the GSM8K problem-solving error rate on GSM8K based on Weighted Mjority/Best-of-N. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "176 where $\\pi(x|d)$ denotes the probability that the LLM answers x given input d and I is the indicator   \n177 function.   \n178 Theorem 2. Assume the reward model assigns an expected reward of $R(a)$ to $a\\in A$ among the   \n179 different solutions generated by LLM that yields a. Given a test dataset $\\mathcal{D}$ and a LLM $\\pi$ . $|{\\mathcal{A}}|$ is the   \n180 finite set of all possible answers given by LLM, the ground truth function $g$ maps test data $d$ to the   \n181 true answer. Denote the accuracy of the LLM on this dataset with weighted majority over $N$ samples   \n182 as $A C C_{W V}(\\pi,{\\cal D},N,R)$ . The accuracy of weighted majority voting on the LLM will eventually   \n183 saturate, i.e. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{N\\to\\infty}A C C_{W V}(\\pi,\\mathcal{D},N,R)=\\frac{\\sum_{d\\in\\mathcal{D}}\\mathbb{I}\\left((g(d)=\\arg\\operatorname*{max}_{a\\in\\mathcal{A}}R(a)\\pi(a|d)\\right)}{|\\mathcal{D}|}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "184 where $\\pi(x|d)$ denotes the probability that the LLM answers $x$ given input $d$ and $\\mathbb{I}$ denotes the   \n185 indicator function.   \n186 Theorem 2 shows that as long as the reward model assigns higher rewards than the policy for correct an  \n187 swers versus other answers in expectation, the upper bound of Weighted Majority Voting will be higher   \n188 than Majority Voting since I $\\begin{array}{r}{((g\\bar{(}d)=\\arg\\operatorname*{max}_{a\\in\\mathcal{A}}R(a)\\pi(a|d))\\stackrel{*}{>}\\mathbb{I}\\left((g(\\Breve{d})=\\arg\\operatorname*{max}_{a\\in\\mathcal{A}}\\pi(a\\Breve{|}d)\\right)}\\end{array}$ .   \n189 We put the figures comparing BoN and Weighted Majority Voting in the main paper and leave the   \n190 Majority Voting figures in Appendix $\\mathrm{D}$ since Majority Voting is dominated by Weighted Majority   \n191 Voting. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "192 4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "193 4.1 Setup", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "194 Datasets. We conduct experiments on two mathematical problem-solving datasets to investigate   \n195 the scaling effects of computation and our REBASE method for both challenging and simpler   \n196 problems. Specifically, MATH [Hendrycks et al., 2021a] and GSM8K[Cobbe et al., 2021b] are   \n197 datasets containing high school mathematics competition-level problems and grade-school level   \n198 mathematical reasoning problems, respectively. Following [Lightman et al., 2023b, Wang et al., 2024,   \n199 Sun et al., 2024], we use the MATH500 subset as our test set.   \n200 Generators. We use Llemma-7B and Llemma-34B [Azerbayev et al., 2024] as our base models and   \n201 finetune them on the MetaMath dataset [Yu et al., 2024] using full parameter supervised fine-tuning   \n202 (Full-SFT), The detailed finetuning configuration is given in the Appendix. Additionally, we test the   \n203 Mistral-7B model to expand our findings across different models.   \n204 Reward Model. All of the experiments use the same Llemma-34B reward model, which we   \n205 finetuned on the synthetic process reward modeling dataset, Math-Shepherd [Wang et al., 2024]. We   \n206 added a reward head to make the model, enabling it to output a scalar reward at the end of each step.   \n207 Inference Configuration. For the MATH dataset, we sample 1, 2, 4, 8, 16, 32, 64, 128, and 256   \n208 solutions for the 7B models, and 1 to 64 solutions for the 34B Llemma model. For the GSM8K dataset,   \n209 we sample 1 to 32 solutions, as it is relatively easier. We use sampling and REBASE to generate   \n210 these samples and select the answer through Best-of-N, Majority Voting, and Weighted Voting.   \n211 Each configuration is run multiple times to calculate the mean and variance, thereby mitigating the   \n212 randomness and ensuring the reliability of our conclusions. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "image", "img_path": "vYmvgxpgwH/tmp/2945d6c3f7ea954bdf3a3ca5a6a1f76b229c54031ea3027c3fb5c1b98cbd27eb.jpg", "img_caption": ["Figure 5: The inference computation scaling laws of different models for the problem-solving error rate on MATH500 test set. The tested models are Llemma-7B (left), Llemma-34B (middle), & Mistral-7B (right). In the legend, W.M. and BoN refer to Weighted Majority and Best-of-N, respectively. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "213 4.2 Main Results of Compute-Optimal Inference ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "214 In order to compare the compute budgets of 7B and 34B models, we plot the figures with the number   \n215 of FLOPs used per question during inference. We compute the inference FLOPs based on the standard   \n216 formula from [Kaplan et al., 2020].   \n217 Llemma-7B model achieves competitive accuracy to Llemma-34B model with lower compute   \n218 budget. Figures 1 and 4 show the curves of error rates versus total number of inference FLOPs per   \n219 question. Inference methods with different model sizes are plotted in the same diagram. We found   \n220 that Llemma-7B costs approximately $2\\mathbf{x}$ less total FLOPs than Llemma-34B under the same method   \n221 (Sampling, MCTS, REBASE) and task (MATH, GSM8K) while achieving competitive accuracy.   \n222 This result suggests that, with the same training dataset and model family, training and inference with   \n223 a smaller model could be more favorable in terms of compute budget if multiple sampling or search   \n224 methods are employed.   \n225 All inference configurations will saturate eventually. This is expected as Theorem 1 and Theorem   \n226 2 show. Also illustrated in Figures 5 and 6, the slope of the erro rate curves start large, then decreases   \n227 and the curves finally become nearly flat as the number of samples scales, showing the effect of   \n228 saturation.   \n229 Scaling law of compute-optimal inference. The findings in our experiments are consistent with   \n230 the Theorem 1 and 2, After a threshold the accruacy of sampling more solutions saturate, we should   \n231 scale the model size. We interpolate the smoothed test error rate curve in Figure 1 and Figure 4,   \n232 and fit power laws to estimate the optimal model size $N$ and number of generated tokens $T$ for any   \n233 given amount of compute. We obtained a relationship $N_{o p t}\\propto C^{a}$ and $\\bar{T}_{o p t}\\propto C^{b}$ , where $a=1.0$   \n234 and $b=0.0$ for both sampling-based weighted voting and our tree-search method REBASE. Our   \n235 fitted curves indicate that the optimal inference strategy is invariant to the amount of compute (e.g.,   \n236 re-ranking with 32 sampled solutions or REBASE tree search with a compute budget of 64 for   \n237 MATH), and the optimal model size grows linearly with the increased compute budget. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "238 4.3 Comparing REBASE to Other Baselines ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "239 REBASE is Pareto-optimal. While MCTS undeperforms Sampling (Fig. 1), from Fig. 1, 4, 5,   \n240 and 6, we found that REBASE consistently outperforms the Sampling method in all settings, when   \n241 fixing the model and the evaluation task. Table 1 shows that REBASE can achieve competitive   \n242 accuracy with even a lower compute budget than the sampling method. This finding is novel, and   \n243 differs from previous tree search works which typically improve the performance at the cost of higher   \n244 computational expense compared to sampling [Chen et al., 2024a, Xie et al., 2023]. Table 2 shows   \n245 that given the same compute budget (sampling 32 solutions for the 7B model and 8 solutions for 34B   \n246 model), using REBASE yields higher accuray than sampling.   \n247 Weaker models gain more from Tree Search. From Fig. 2, we saw that compared with sampling,   \n248 Mistral-7B, Llemma-7B, Llemma-34B increase $5.3\\%$ , $3.3\\%$ , $2.6\\%$ in MATH and $0.7\\%$ , $1.9\\%$ , $0.9\\%$   \n249 in GSM8K. The order of accuracy increase is inversely related to the model\u2019s corresponding greedy   \n250 search on those datasets. This suggests that weaker models, as indicated by their lower greedy search   \n251 accuracy, benefit more from tree search methods like REBASE.   \n252 REBASE saturates later than sampling with higher accuray. From Figure 5 and Figure 6, we   \n253 observe that both sampling and REBASE saturate early in GSM8K and relatively late in MATH,   \n254 which we attribute to the difference of the difficulty level. This can be explained through the LLM   \n255 may assign high probability to the true answer in easy problems than those of harder problem, as   \n256 suggested by Theorem 1 and 2 with their proofs A. On MATH (Figure 5), we see that REBASE   \n257 finally saturates with a higher accuracy than sampling. We hypothesize the reason is that REBASE   \n258 samples the truth answer with higher probability than sampling. And as demonstrated by Theorem 1   \n259 and 2, the upper bound becomes higher. ", "page_idx": 6}, {"type": "image", "img_path": "vYmvgxpgwH/tmp/9b6bc7420a66247196159845ad1f5dec472da2ff54e3fc62e22fd58ed2497d30.jpg", "img_caption": ["Figure 6: The inference computation scaling laws of different models for the problem-solving error rate on GSM8K test set. The tested models are Llemma-7B (left), Llemma-34B (middle), & Mistral-7B (right). In the legend, W.M. and BoN refer to Weighted Majority and Best-of-N, respectively. "], "img_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "vYmvgxpgwH/tmp/a3c656ba738cd7342c3e013936539045abb7014bf862d08020bb9684571cf0c4.jpg", "table_caption": ["Table 1: REBASE with lower compute budget has competitive accuracy against Sampling with higher compute budget. We use weighted voting to aggreagte the candidate solutions in both Sampling and REBASE. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "table", "img_path": "vYmvgxpgwH/tmp/f80a53eeba54a21293792db64aeed635d74a16ed97cf559c9811168cb14b72f9.jpg", "table_caption": ["Table 2: Accuracy of diffrent inference configurations under a specific compute budget. MV, BoN and WV denote Majority Voting, Best-of-N and Weighted Voting, respectively. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "260 5 Conclusion & Limitations ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "261 In this work, we have conducted a comprehensive empirical analysis of compute-optimal inference   \n262 for problem-solving with language models. Our study has revealed several key findings. First, with   \n263 an optimal inference configuration, a small language model can achieve competitive accuracy to a   \n264 $4\\times$ larger model while using approximately $2\\times$ less total FLOPs under the same inference method   \n265 (Sampling, MCTS, REBASE) and task (MATH, GSM8K), suggesting that training and inference   \n266 with smaller models could be more favorable in terms of compute budget when combined with   \n267 multiple sampling or search strategies. Second, our new REBASE tree-search method consistently   \n268 outperforms sampling (and MCTS) across all settings, models, and tasks, achieving competitive   \n269 accuracy with even lower compute budget compared to sampling. Our findings highlight the potential   \n270 of deploying smaller models equipped with more sophisticated inference strategies like REBASE to   \n271 enhance problem-solving accuracy while maintaining computational efficiency.   \n272 Limitations First, our experiments focused specifically on mathematical problem-solving tasks,   \n273 and the generalizability of our findings to other domains remains to be explored. Second, we only   \n274 investigated a limited range of model scales, primarily focusing on 7B and 34B models. Future   \n275 research could extend our analysis to a wider range of model sizes to gain a more comprehensive   \n276 understanding of the scaling laws for compute-optimal inference. Finally, our experiments mainly   \n277 utilized the MetaMath dataset for training the models. It would be valuable to explore the impact of   \n278 different training datasets on the performance and efficiency of compute-optimal inference strategies   \n279 for mathematical problem-solving. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "280 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "281 David H Ackley, Geoffrey E Hinton, and Terrence J Sejnowski. A learning algorithm for boltzmann   \n282 machines. Cognitive science, 9(1):147\u2013169, 1985.   \n283 Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen McAleer, Albert Q   \n284 Jiang, Jia Deng, Stella Biderman, and Sean Welleck. Llemma: An open language model for   \n285 mathematics. arXiv preprint arXiv:2310.10631, 2023.   \n286 Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen McAleer, Albert Q.   \n287 Jiang, Jia Deng, Stella Biderman, and Sean Welleck. Llemma: An open language model for   \n288 mathematics, 2024.   \n289 Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr,   \n290 Joe Taylor, Troy Luhman, Eric Luhman, Clarence $\\mathrm{Ng}$ , Ricky Wang, and Aditya Ramesh.   \n291 Video generation models as world simulators. 2024. URL https://openai.com/research/   \n292 video-generation-models-as-world-simulators.   \n293 Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D. Kaplan, Prafulla Dhariwal,   \n294 Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are   \n295 few-shot learners. Advances in Neural Information Processing Systems, 33:1877\u20131901, 2020.   \n296 Guoxin Chen, Minpeng Liao, Chengxi Li, and Kai Fan. Alphamath almost zero: process supervision   \n297 without process, 2024a.   \n298 Ziru Chen, Michael White, Raymond Mooney, Ali Payani, Yu Su, and Huan Sun. When is tree search   \n299 useful for llm planning? it depends on the discriminator. arXiv preprint arXiv:2402.10890, 2024b.   \n300 Sehyun Choi, Tianqing Fang, Zhaowei Wang, and Yangqiu Song. Kcts: Knowledge-constrained tree   \n301 search decoding with token-level hallucination detection, 2023.   \n302 Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam   \n303 Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. PaLM:   \n304 Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.   \n305 Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,   \n306 Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John   \n307 Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168,   \n308 2021a.   \n309 Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,   \n310 Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve   \n311 math word problems. arXiv preprint arXiv:2110.14168, 2021b.   \n312 Leo Gao, John Schulman, and Jacob Hilton. Scaling laws for reward model overoptimization. In   \n313 International Conference on Machine Learning, pages 10835\u201310866. PMLR, 2023.   \n314 Alex Graves. Sequence transduction with recurrent neural networks, 2012.   \n315 Arnav Gudibande, Eric Wallace, Charlie Snell, Xinyang Geng, Hao Liu, Pieter Abbeel, Sergey Levine,   \n316 and Dawn Song. The false promise of imitating proprietary llms. arXiv preprint arXiv:2305.15717,   \n317 2023.   \n318 Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin   \n319 Burns, Samir Puranik, Horace He, Dawn Song, et al. Measuring coding challenge competence   \n320 with apps. arXiv preprint arXiv:2105.09938, 2021a.   \n321 Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn   \n322 Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. In   \n323 Thirty-ffith Conference on Neural Information Processing Systems Datasets and Benchmarks Track   \n324 (Round 2), 2021b.   \n325 Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson, Heewoo Jun,   \n326 Tom B. Brown, Prafulla Dhariwal, Scott Gray, et al. Scaling laws for autoregressive generative   \n327 modeling. arXiv preprint arXiv:2010.14701, 2020.   \n328 Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan Kianinejad,   \n329 Md Patwary, Mostofa Ali, Yang Yang, and Yanqi Zhou. Deep learning scaling is predictable,   \n330 empirically. arXiv preprint arXiv:1712.00409, 2017.   \n331 Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza   \n332 Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al.   \n333 Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022.   \n334 Andy L Jones. Scaling scaling laws with board games. arXiv preprint arXiv:2104.03113, 2021.   \n335 Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child,   \n336 Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models.   \n337 arXiv preprint arXiv:2001.08361, 2020.   \n338 Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large   \n339 language models are zero-shot reasoners. arXiv preprint arXiv:2205.11916, 2022.   \n340 Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ra  \n341 masesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. Solving quantitative   \n342 reasoning problems with language models. arXiv preprint arXiv:2206.14858, 2022.   \n343 Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan   \n344 Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let\u2019s verify step by step. arXiv preprint   \n345 arXiv:2305.20050, 2023a.   \n346 Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike,   \n347 John Schulman, Ilya Sutskever, and Karl Cobbe. Let\u2019s verify step by step, 2023b.   \n348 Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. Program induction by rationale gen  \n349 eration: Learning to solve and explain algebraic word problems. In Proceedings of the 55th   \n350 Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages   \n351 158\u2013167, 2017.   \n352 Jiacheng Liu, Andrew Cohen, Ramakanth Pasunuru, Yejin Choi, Hannaneh Hajishirzi, and Asli   \n353 Celikyilmaz. Don\u2019t throw away your value model! generating more preferable text with value  \n354 guided monte-carlo tree search decoding, 2024.   \n355 Qianli Ma, Haotian Zhou, Tingkai Liu, Jianbo Yuan, Pengfei Liu, Yang You, and Hongxia Yang.   \n356 Let\u2019s reward step by step: Step-level reward model as the navigators for reasoning, 2023.   \n357 Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David   \n358 Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. Show your work:   \n359 Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114,   \n360 2021.   \n361 OpenAI. Gpt-4 technical report, 2023.   \n362 William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of   \n363 the IEEE/CVF International Conference on Computer Vision, pages 4195\u20134205, 2023.   \n364 Stanislas Polu and Ilya Sutskever. Generative language modeling for automated theorem proving.   \n365 arXiv preprint arXiv:2009.03393, 2020.   \n366 Jonathan S Rosenfeld, Amir Rosenfeld, Yonatan Belinkov, and Nir Shavit. A constructive prediction   \n367 of the generalization error across scales. arXiv preprint arXiv:1909.12673, 2019.   \n368 David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,   \n369 Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering   \n370 the game of Go with deep neural networks and tree search. Nature, 529(7587):484\u2013489, 2016.   \n371 David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,   \n372 Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go without   \n373 human knowledge. nature, 550(7676):354\u2013359, 2017.   \n374 Zhiqing Sun, Longhui Yu, Yikang Shen, Weiyang Liu, Yiming Yang, Sean Welleck, and Chuang   \n375 Gan. Easy-to-hard generalization: Scalable alignment beyond human supervision. arXiv preprint   \n376 arXiv:2403.09472, 2024.   \n377 Virginia Teller. Speech and language processing: an introduction to natural language processing,   \n378 computational linguistics, and speech recognition, 2000.   \n379 Ye Tian, Baolin Peng, Linfeng Song, Lifeng Jin, Dian Yu, Haitao Mi, and Dong Yu. Toward self  \n380 improvement of llms via imagination, searching, and criticizing. arXiv preprint arXiv:2404.12253,   \n381 2024.   \n382 Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia   \n383 Creswell, Geoffrey Irving, and Irina Higgins. Solving math word problems with process- and   \n384 outcome-based feedback. arXiv preprint arXiv:2211.14275, 2022.   \n385 Peiyi Wang, Lei Li, Zhihong Shao, RX Xu, Damai Dai, Yifei Li, Deli Chen, Y Wu, and Zhifang   \n386 Sui. Math-shepherd: Verify and reinforce llms step-by-step without human annotations. CoRR,   \n387 abs/2312.08935, 2023.   \n388 Peiyi Wang, Lei Li, Zhihong Shao, R. X. Xu, Damai Dai, Yifei Li, Deli Chen, Y. Wu, and Zhifang   \n389 Sui. Math-shepherd: Verify and reinforce llms step-by-step without human annotations, 2024.   \n390 Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdh  \n391 ery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models.   \n392 International Conference on Learning Representations (ICLR 2023), 2022a.   \n393 Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and   \n394 Hannaneh Hajishirzi. Self-instruct: Aligning language model with self generated instructions.   \n395 arXiv preprint arXiv:2212.10560, 2022b.   \n396 Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou.   \n397 Chain-of-thought prompting elicits reasoning in large language models. NeurIPS, 2022.   \n398 Yuxi Xie, Kenji Kawaguchi, Yiran Zhao, Xu Zhao, Min-Yen Kan, Junxian He, and Qizhe Xie.   \n399 Self-evaluation guided beam search for reasoning, 2023.   \n400 Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, and Karthik   \n401 Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. arXiv   \n402 preprint arXiv:2305.10601, 2023.   \n403 Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan,   \n404 Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for content  \n405 rich text-to-image generation. arXiv preprint arXiv:2206.10789, 2(3):5, 2022.   \n406 Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T Kwok, Zhenguo   \n407 Li, Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical questions for   \n408 large language models. arXiv preprint arXiv:2309.12284, 2023.   \n409 Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T. Kwok,   \n410 Zhenguo Li, Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical   \n411 questions for large language models, 2024.   \n412 Shun Zhang, Zhenfang Chen, Yikang Shen, Mingyu Ding, Joshua B. Tenenbaum, and Chuang Gan.   \n413 Planning with large language models for code generation, 2023.   \n414 Andy Zhou, Kai Yan, Michal Shlapentokh-Rothman, Haohan Wang, and Yu-Xiong Wang. Language   \n415 agent tree search unifies reasoning acting and planning in language models, 2023.   \n416 Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan,   \n417 and Jimmy Ba. Large language models are human-level prompt engineers. arXiv preprint   \n418 arXiv:2211.01910, 2022. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "419 A Proofs of Theorem 1 and 2 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "420 A.1 Proof of Theorem 1 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "421 Proof. Suppose the possible answers of the LLM are $x_{1},x_{2},x_{3},\\cdot\\cdot\\cdot\\cdot,x_{|A|}$ , with $\\pi(x_{1}|d)\\ >$   \n422 $\\pi(x_{2}|d)>\\cdot\\cdot\\cdot>\\pi(x_{|A|}|d)$ . After sampling $N$ solutions from the LLM, we denote the occurence of   \n423 $x_{i}$ as $f(x_{i})$ , the probability that $x_{1}$ is not the most frequent output is ", "page_idx": 12}, {"type": "equation", "text": "$$\nP(f(x_{1})\\neq\\arg\\operatorname*{max}_{x}f(x))\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "424 With Union bound, we get ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad P(x_{1}\\neq\\arg\\operatorname*{max}f(x))}\\\\ &{\\leq\\frac{|A|}{\\sum_{i=2}^{M}P(f(x_{1})\\leq f(x_{i}))}}\\\\ &{\\leq\\!|A|P(f(x_{1})\\leq f(x_{2}))}\\\\ &{=\\!|A|\\left(1-P\\left(f(x_{1})\\geq f(x_{2})\\right)\\right)}\\\\ &{\\leq\\!|A|\\left(1-P\\left(f(x_{1})\\geq\\frac{\\pi(x_{1}\\mid d)+\\pi(x_{2}\\mid d)}{2}N\\right)P\\left(f(x_{2})\\leq\\frac{\\pi(x_{1}\\mid d)+\\pi(x_{2}\\mid d)}{2}N\\right)\\right)}\\\\ &{\\leq\\!|A|\\left(1-\\left(1-e^{-\\frac{d^{2}}{2}\\pi(x_{1}\\mid d)N}\\right)\\left(1-e^{-\\frac{d^{2}}{2+\\delta_{2}}\\pi(x_{2}\\mid d)N}\\right)\\right)}\\\\ &{\\leq\\!|A|\\!\\left(1-\\left.\\!\\left(1\\!-\\!e^{-\\frac{d^{2}}{2}\\pi(x_{1}\\mid d)N}\\right)\\!\\left(1-e^{-\\frac{\\pi(x_{2}\\mid d)}{2}(x_{2}\\mid d)N}\\right)\\!\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "425 Where (11) is by Chernoff Bound, $\\begin{array}{r}{\\delta_{1}=\\frac{\\pi(x_{1}|d)-\\pi(x_{2}|d)}{2\\pi(x_{1}|d)}}\\end{array}$ and $\\begin{array}{r}{\\delta_{2}=\\frac{\\pi(x_{1}|d)-\\pi(x_{2}|d)}{2\\pi(x_{2}|d)}}\\end{array}$ . As $N\\rightarrow\\infty$ , we   \n426 have ", "page_idx": 12}, {"type": "equation", "text": "$$\nf(x)={\\left\\{\\begin{array}{l l}{M(x|N)=1}&{{\\mathrm{if~}}x=\\arg\\operatorname*{max}_{a\\in A}\\pi(a|d)}\\\\ {M(x|N)=0}&{{\\mathrm{otherwise~}}.}\\end{array}\\right.}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "427 Where $M(x|N)$ denotes the probability that majority voting over $N$ sampled solutions gives $x$ . The   \n428 proof of original theorem is automatically completed by (13). \u53e3 ", "page_idx": 12}, {"type": "text", "text": "429 A.2 Proof of Theorem 2 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "430 Proof. The proof is similar to the Theorem 1, We rank $x_{1},x_{2},\\cdot\\cdot\\cdot,x_{|A|}$ with $R(x_{1})f(x_{1})>\\cdot\\cdot\\cdot>$   \n431 $R(x_{|A|}f(x_{|A|})$ . Denotes $w(x_{i})$ as the the total weights of answer $x_{i}$ after sampling $\\mathbf{N}$ solutions. As   \n432 $N\\rightarrow\\infty$ , $w({\\dot{x}}_{i})\\to R(x_{i})f(x_{i})$ . Same as proof in theorem 1, we have ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad P(x_{1}\\neq\\arg\\operatorname*{max}{f(x)})}\\\\ &{\\leq\\lvert A\\rvert P(w(x_{1})\\leq w(x_{2}))}\\\\ &{=\\lvert A\\rvert\\left(1-P\\left(w(x_{1})\\geq w(x_{2})\\right)\\right)}\\\\ &{\\leq\\lvert A\\rvert\\left(1-P\\left(w(x_{1})\\geq\\frac{v\\left(x_{1}\\right)+v\\left(x_{2}\\right)}{2}N\\right)P\\left(w(x_{2})\\leq\\frac{v\\left(x_{1}\\right)+v\\left(x_{2}\\right)}{2}N\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "433 Where $v(x)=R(x)\\pi(x|d)$ , the remaining proof completely follows Theorem 1. ", "page_idx": 12}, {"type": "text", "text": "434 B MCTS Details ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "435 The MCTS process can be represented as the following steps: ", "page_idx": 12}, {"type": "text", "text": "436 Selection The process begins at the root node. Here, the algorithm recursively selects the child   \n437 node that offers the highest Upper Confidence Bound applied to Trees (UCT) value, continuing until   \n438 a node is reached that has not been expanded. The UCT is calculated using the formula ", "page_idx": 12}, {"type": "equation", "text": "$$\nU C T(s)=Q(s)+C\\sqrt{\\frac{\\ln\\left(N(P a r e n t(s))\\right)}{N(s)}}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Table 3: Fine-tuning Hyper-parameters: LR refers to the learning rate, BS refers to the batch size. Llemma-7B and LLemma-34B are the generators we use in our experiments, RM is short for Reward Model. ", "page_idx": 13}, {"type": "table", "img_path": "vYmvgxpgwH/tmp/730e0058e0cfcb16e92524cf03572e4c74eb178a4f99972fc9bbadb49498dd3b.jpg", "table_caption": [], "table_footnote": [], "page_idx": 13}, {"type": "image", "img_path": "vYmvgxpgwH/tmp/6b3a58922216cf9012f7987d189b281a9c8a393b18c5e25ab62a3d03ef5bf10c.jpg", "img_caption": ["Figure 7: The inference computation scaling laws of different models for the problem-solving error rate on MATH test set. The tested models are Llemma-7B (left), Llemma-34B (middle), & Mistral-7B (right). In the legend, M.V. refer to Majority Voting. "], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "439 Where $Q(s)$ represents the quality score of node $s$ , $N(s)$ is the number of visits to node $s$ , and $C$ is a   \n440 constant determining the level of exploration.   \n441 Expansion and evaluation Upon reaching a non-terminal node $s$ , the node is expanded by gener  \n442 ating multiple child nodes. Each child node $c$ is then evaluated using a value function $V(c)$ , which   \n443 predicts the potential quality of continuing the sequence from node $c$ .   \n444 Backpropagation After evaluation, the algorithm updates the UCT values and the visit counts for   \n445 all nodes along the path from the selected node back to the root. For any node $n$ in this path, the   \n446 updates are made as follows: ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{N(n)\\leftarrow N(n)+1,}}\\\\ {{Q(n)\\leftarrow\\frac{(N(n)-1)\\,Q(n)+V(s)}{N(n)}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "447 C Hyper-parameters ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "448 Finetuning We put all the hyperparameters of fine-tuned models in the table 3. We preprocess the   \n449 MetaMath Dataset to make the solutions in a stepwise format.   \n450 Inference For all the inference strategies, the temperature of the LLM is set to 1.0. Max tokens   \n451 for the output is 1024 and max tokens for one step is 256. For REBASE, we chose the balance   \n452 temperature (the softmax temperature in the REBASE algorithm) as $T_{b}=0.1$ . For MCTS, we set   \n453 $C$ in the UCT value as 1 and we expand 4, 8, 16 children for the root, 2 children for other selected   \n454 nodes with total 32, 64, 128 expansions respectively. ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "455 D Supplementary Figures ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "456 In the main part of paper, there isn\u2019t enough space for showing the scaling effects of Majority Voting,   \n457 we append the figures about Majority Voting and Majority Voting v.s. Weighted Majority Voting   \n458 (Fig. 7, 8 ,9, 10) in this appendix. The experiments show that although the gap between Majority   \n459 Voting and Weighted Majority Voting on sampling is huge. This gap becomes much smaller if we   \n460 apply REBASE. This phenomenon can be caused by the selection ability of tree search like REBASE.   \n461 Once REBASE already samples solutions with high rewards, conducing weighted majority voting   \n462 gains less since the sampled solutions may all have relatively high and stable rewards compared with   \n463 those of sampling. ", "page_idx": 13}, {"type": "image", "img_path": "vYmvgxpgwH/tmp/faad843a0543a059b48b498fbdb4846a6e436f56c9dd045e6c08f52400e7fd41.jpg", "img_caption": ["Figure 8: The inference computation scaling laws of different models for the problem-solving error rate on GSM8K test set. The tested models are Llemma-7B (left), Llemma-34B (middle), & Mistral-7B (right). In the legend, M.V. refer to Majority Voting. "], "img_footnote": [], "page_idx": 14}, {"type": "image", "img_path": "vYmvgxpgwH/tmp/9aff7eac5bb2d16fcfdf03468a5d9fbe7b93f2dfba8d8f5bd5bfa6a6d307d7c7.jpg", "img_caption": ["Figure 9: The inference computation scaling laws of different models for the problem-solving error rate on MATH test set. The tested models are Llemma-7B (left), Llemma-34B (middle), & Mistral-7B (right). In the legend, M.V. and W.M. refer to Majority Voting and Weighted Majority, respectively. "], "img_footnote": [], "page_idx": 14}, {"type": "image", "img_path": "vYmvgxpgwH/tmp/05a3670e8f467d4d56c79adc40522207f0759d3169cfb99e9acbdeb30d3ab1db.jpg", "img_caption": ["Figure 10: The inference computation scaling laws of different models for the problem-solving error rate on GSM8K test set. The tested models are Llemma-7B (left), Llemma-34B (middle), & Mistral-7B (right). In the legend, M.V. and W.M. refer to Majority Voting and Weighted Majority, respectively. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "464 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "465 1. Claims   \n466 Question: Do the main claims made in the abstract and introduction accurately reflect the   \n467 paper\u2019s contributions and scope?   \n468 Answer: [Yes]   \n469 Justification: In Abstract and Introduction, we claim that we investigate the compute-optimal   \n470 inference: designing models and inference strategies that optimally trade off additional   \n471 inference-time compute for improved performance.   \n472 Guidelines:   \n473 \u2022 The answer NA means that the abstract and introduction do not include the claims   \n474 made in the paper.   \n475 \u2022 The abstract and/or introduction should clearly state the claims made, including the   \n476 contributions made in the paper and important assumptions and limitations. A No or   \n477 NA answer to this question will not be perceived well by the reviewers.   \n478 \u2022 The claims made should match theoretical and experimental results, and reflect how   \n479 much the results can be expected to generalize to other settings.   \n480 \u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals   \n481 are not attained by the paper.   \n482 2. Limitations   \n483 Question: Does the paper discuss the limitations of the work performed by the authors?   \n484 Answer: [Yes]   \n485 Justification: The discussion is in the last section of the main paper.   \n486 Guidelines:   \n487 \u2022 The answer NA means that the paper has no limitation while the answer No means that   \n488 the paper has limitations, but those are not discussed in the paper.   \n489 \u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n490 \u2022 The paper should point out any strong assumptions and how robust the results are to   \n491 violations of these assumptions (e.g., independence assumptions, noiseless settings,   \n492 model well-specification, asymptotic approximations only holding locally). The authors   \n493 should reflect on how these assumptions might be violated in practice and what the   \n494 implications would be.   \n495 \u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was   \n496 only tested on a few datasets or with a few runs. In general, empirical results often   \n497 depend on implicit assumptions, which should be articulated.   \n498 \u2022 The authors should reflect on the factors that influence the performance of the approach.   \n499 For example, a facial recognition algorithm may perform poorly when image resolution   \n500 is low or images are taken in low lighting. Or a speech-to-text system might not be   \n501 used reliably to provide closed captions for online lectures because it fails to handle   \n502 technical jargon.   \n503 \u2022 The authors should discuss the computational efficiency of the proposed algorithms   \n504 and how they scale with dataset size.   \n505 \u2022 If applicable, the authors should discuss possible limitations of their approach to   \n506 address problems of privacy and fairness.   \n507 \u2022 While the authors might fear that complete honesty about limitations might be used by   \n508 reviewers as grounds for rejection, a worse outcome might be that reviewers discover   \n509 limitations that aren\u2019t acknowledged in the paper. The authors should use their best   \n510 judgment and recognize that individual actions in favor of transparency play an impor  \n511 tant role in developing norms that preserve the integrity of the community. Reviewers   \n512 will be specifically instructed to not penalize honesty concerning limitations.   \n513 3. Theory Assumptions and Proofs   \n514 Question: For each theoretical result, does the paper provide the full set of assumptions and   \n515 a complete (and correct) proof?   \n19 \u2022 The answer NA means that the paper does not include theoretical results.   \n20 \u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross  \n21 referenced.   \n22 \u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n23 \u2022 The proofs can either appear in the main paper or the supplemental material, but if   \n24 they appear in the supplemental material, the authors are encouraged to provide a short   \n25 proof sketch to provide intuition.   \n26 \u2022 Inversely, any informal proof provided in the core of the paper should be complemented   \n27 by formal proofs provided in appendix or supplemental material.   \n28 \u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "529 4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "530 Question: Does the paper fully disclose all the information needed to reproduce the main ex  \n531 perimental results of the paper to the extent that it affects the main claims and/or conclusions   \n532 of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: We introduce our method in Section 3 and the hyperparameters are introduced in the Appendix. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 16}, {"type": "text", "text": "568 5. Open access to data and code ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "569 Question: Does the paper provide open access to the data and code, with sufficient instruc  \n570 tions to faithfully reproduce the main experimental results, as described in supplemental   \n571 material?   \n572 Answer: [Yes]   \n573 Justification: We only used open-source models in this work. The code will be released.   \n574 Guidelines:   \n575 \u2022 The answer NA means that paper does not include experiments requiring code.   \n576 \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/   \n577 public/guides/CodeSubmissionPolicy) for more details.   \n578 \u2022 While we encourage the release of code and data, we understand that this might not be   \n579 possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not   \n580 including code, unless this is central to the contribution (e.g., for a new open-source   \n581 benchmark).   \n582 \u2022 The instructions should contain the exact command and environment needed to run to   \n583 reproduce the results. See the NeurIPS code and data submission guidelines (https:   \n584 //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n585 \u2022 The authors should provide instructions on data access and preparation, including how   \n586 to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n587 \u2022 The authors should provide scripts to reproduce all experimental results for the new   \n588 proposed method and baselines. If only a subset of experiments are reproducible, they   \n589 should state which ones are omitted from the script and why.   \n590 \u2022 At submission time, to preserve anonymity, the authors should release anonymized   \n591 versions (if applicable).   \n592 \u2022 Providing as much information as possible in supplemental material (appended to the   \n593 paper) is recommended, but including URLs to data and code is permitted.   \n594 6. Experimental Setting/Details   \n595 Question: Does the paper specify all the training and test details (e.g., data splits, hyper  \n596 parameters, how they were chosen, type of optimizer, etc.) necessary to understand the   \n597 results?   \n598 Answer: [Yes]   \n599 Justification: We used the standard training and test splits or MATH and GSM8K and report   \n600 the hyperparameters in the appendix.   \n601 Guidelines:   \n602 \u2022 The answer NA means that the paper does not include experiments.   \n603 \u2022 The experimental setting should be presented in the core of the paper to a level of detail   \n604 that is necessary to appreciate the results and make sense of them.   \n605 \u2022 The full details can be provided either with the code, in appendix, or as supplemental   \n606 material.   \n607 7. Experiment Statistical Significance   \n608 Question: Does the paper report error bars suitably and correctly defined or other appropriate   \n609 information about the statistical significance of the experiments?   \n610 Answer: [Yes]   \n611 Justification: The error bars are included in our figures.   \n612 Guidelines:   \n613 \u2022 The answer NA means that the paper does not include experiments.   \n614 \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confi  \n615 dence intervals, or statistical significance tests, at least for the experiments that support   \n616 the main claims of the paper.   \n617 \u2022 The factors of variability that the error bars are capturing should be clearly stated (for   \n618 example, train/test split, initialization, random drawing of some parameter, or overall   \n619 run with given experimental conditions).   \n620 \u2022 The method for calculating the error bars should be explained (closed form formula,   \n621 call to a library function, bootstrap, etc.)   \n622 \u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n623 \u2022 It should be clear whether the error bar is the standard deviation or the standard error   \n624 of the mean.   \n625 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should   \n626 preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis   \n627 of Normality of errors is not verified.   \n628 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or   \n629 figures symmetric error bars that would yield results that are out of range (e.g. negative   \n630 error rates).   \n631 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how   \n632 they were calculated and reference the corresponding figures or tables in the text.   \n633 8. Experiments Compute Resources   \n634 Question: For each experiment, does the paper provide sufficient information on the com  \n635 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n636 the experiments?   \n637 Answer: [Yes]   \n638 Justification: All the experiments are conducted on $8\\times{\\mathrm{Hl}}00$ GPUs.   \n639 Guidelines:   \n640 \u2022 The answer NA means that the paper does not include experiments.   \n641 \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,   \n642 or cloud provider, including relevant memory and storage.   \n643 \u2022 The paper should provide the amount of compute required for each of the individual   \n644 experimental runs as well as estimate the total compute.   \n645 \u2022 The paper should disclose whether the full research project required more compute   \n646 than the experiments reported in the paper (e.g., preliminary or failed experiments that   \n647 didn\u2019t make it into the paper).   \n648 9. Code Of Ethics   \n649 Question: Does the research conducted in the paper conform, in every respect, with the   \n650 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n651 Answer: [Yes]   \n652 Justification: Yes, we conform with the NeurIPS Code of Ethics.   \n653 Guidelines:   \n654 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n655 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n656 deviation from the Code of Ethics.   \n657 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid  \n658 eration due to laws or regulations in their jurisdiction).   \n659 10. Broader Impacts   \n660 Question: Does the paper discuss both potential positive societal impacts and negative   \n661 societal impacts of the work performed?   \n662 Answer: [NA]   \n663 Justification: We do not find significant positive societal impacts and negative societal   \n664 impacts of our work.   \n665 Guidelines:   \n666 \u2022 The answer NA means that there is no societal impact of the work performed.   \n667 \u2022 If the authors answer NA or No, they should explain why their work has no societal   \n668 impact or why the paper does not address societal impact.   \n669 \u2022 Examples of negative societal impacts include potential malicious or unintended uses   \n670 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n671 (e.g., deployment of technologies that could make decisions that unfairly impact specific   \n672 groups), privacy considerations, and security considerations.   \n673 \u2022 The conference expects that many papers will be foundational research and not tied   \n674 to particular applications, let alone deployments. However, if there is a direct path to   \n675 any negative applications, the authors should point it out. For example, it is legitimate   \n676 to point out that an improvement in the quality of generative models could be used to   \n677 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n678 that a generic algorithm for optimizing neural networks could enable people to train   \n679 models that generate Deepfakes faster.   \n680 \u2022 The authors should consider possible harms that could arise when the technology is   \n681 being used as intended and functioning correctly, harms that could arise when the   \n682 technology is being used as intended but gives incorrect results, and harms following   \n683 from (intentional or unintentional) misuse of the technology.   \n684 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n685 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n686 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n687 feedback over time, improving the efficiency and accessibility of ML).   \n688 11. Safeguards   \n689 Question: Does the paper describe safeguards that have been put in place for responsible   \n690 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n691 image generators, or scraped datasets)?   \n692 Answer: [NA]   \n693 Justification:   \n694 Guidelines:   \n695 \u2022 The answer NA means that the paper poses no such risks.   \n696 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n697 necessary safeguards to allow for controlled use of the model, for example by requiring   \n698 that users adhere to usage guidelines or restrictions to access the model or implementing   \n699 safety filters.   \n700 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n701 should describe how they avoided releasing unsafe images.   \n702 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n703 not require this, but we encourage authors to take this into account and make a best   \n704 faith effort.   \n705 12. Licenses for existing assets   \n706 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n707 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n708 properly respected?   \n709 Answer: [Yes]   \n710 Justification: We use the proper citations.   \n711 Guidelines:   \n712 \u2022 The answer NA means that the paper does not use existing assets.   \n713 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n714 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n715 URL.   \n716 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n717 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n718 service of that source should be provided.   \n719 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n720 package should be provided. For popular datasets, paperswithcode.com/datasets   \n721 has curated licenses for some datasets. Their licensing guide can help determine the   \n722 license of a dataset. ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 20}, {"type": "text", "text": "725 \u2022 If this information is not available online, the authors are encouraged to reach out to   \n726 the asset\u2019s creators.   \n727 13. New Assets   \n728 Question: Are new assets introduced in the paper well documented and is the documentation   \n729 provided alongside the assets?   \n730 Answer: [Yes]   \n731 Justification: We use the proper citations.   \n732 Guidelines:   \n733 \u2022 The answer NA means that the paper does not release new assets.   \n734 \u2022 Researchers should communicate the details of the dataset/code/model as part of their   \n735 submissions via structured templates. This includes details about training, license,   \n736 limitations, etc.   \n737 \u2022 The paper should discuss whether and how consent was obtained from people whose   \n738 asset is used.   \n739 \u2022 At submission time, remember to anonymize your assets (if applicable). You can either   \n740 create an anonymized URL or include an anonymized zip file.   \n741 14. Crowdsourcing and Research with Human Subjects   \n742 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n743 include the full text of instructions given to participants and screenshots, if applicable, as   \n744 well as details about compensation (if any)?   \n745 Answer: [NA]   \n746 Justification: No crowdsourcing experiments are used.   \n747 Guidelines:   \n748 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n749 human subjects.   \n750 \u2022 Including this information in the supplemental material is fine, but if the main contribu  \n751 tion of the paper involves human subjects, then as much detail as possible should be   \n752 included in the main paper.   \n753 \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation,   \n754 or other labor should be paid at least the minimum wage in the country of the data   \n755 collector.   \n756 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human   \n757 Subjects   \n758 Question: Does the paper describe potential risks incurred by study participants, whether   \n759 such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)   \n760 approvals (or an equivalent approval/review based on the requirements of your country or   \n761 institution) were obtained?   \n762 Answer: [NA]   \n763 Justification: No human subjects are involved.   \n764 Guidelines:   \n765 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n766 human subjects.   \n767 \u2022 Depending on the country in which research is conducted, IRB approval (or equivalent)   \n768 may be required for any human subjects research. If you obtained IRB approval, you   \n769 should clearly state this in the paper.   \n770 \u2022 We recognize that the procedures for this may vary significantly between institutions   \n771 and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the ", "page_idx": 20}, {"type": "text", "text": "guidelines for their institution. \u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 20}]