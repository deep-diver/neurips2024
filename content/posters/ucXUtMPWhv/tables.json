[{"figure_path": "ucXUtMPWhv/tables/tables_6_1.jpg", "caption": "Table 1: Results (mean\u00b1std) on long-term forecasting scenarios with the best in bold and the second underlined. Each result contains three independent runs with different seeds. During the training phase, ElasTST utilizes a loss reweighting strategy where a single trained model is applied across all inference horizons, where the Hmax is set to 720. Other baseline models undergo horizon-specific training and tuning. Additional baseline results are detailed in Appendix D.1.", "description": "This table presents the main results of the paper, comparing the performance of ElasTST with other state-of-the-art time series forecasting models across various datasets and forecasting horizons.  ElasTST uses a reweighting scheme to train a single model effective for all forecast horizons.  Other models are trained and tuned for specific horizons.  Results are reported as mean \u00b1 standard deviation, across three independent runs.", "section": "4.1 Main Results"}, {"figure_path": "ucXUtMPWhv/tables/tables_14_1.jpg", "caption": "Table 2: Summary of Time Series Foundation Models.", "description": "This table summarizes existing time series foundation models, excluding those based on LLMs.  It compares the models based on their backbone architecture (Encoder-Decoder or Decoder-only Transformer), decoding scheme (Autoregressive or Non-Autoregressive), positional encoding methods, and tokenization techniques.  The table provides context for understanding how ElasTST, the model presented in the paper, relates to and improves upon existing approaches.", "section": "Related Work"}, {"figure_path": "ucXUtMPWhv/tables/tables_14_2.jpg", "caption": "Table 1: Results (mean\u00b1std) on long-term forecasting scenarios with the best in bold and the second underlined. Each result contains three independent runs with different seeds. During the training phase, ElasTST utilizes a loss reweighting strategy where a single trained model is applied across all inference horizons, where the Hmax is set to 720. Other baseline models undergo horizon-specific training and tuning. Additional baseline results are detailed in Appendix D.1.", "description": "This table presents the results of long-term forecasting experiments comparing ElasTST with other state-of-the-art models.  ElasTST uses a reweighting scheme during training, allowing a single model to be used for all forecasting horizons, while other models are trained and tuned specifically for each horizon. The table shows the Normalized Mean Absolute Error (NMAE) and Normalized Root Mean Squared Error (NRMSE) for various prediction lengths (96, 192, 336, and 720) across multiple datasets.", "section": "4.1 Main Results"}, {"figure_path": "ucXUtMPWhv/tables/tables_15_1.jpg", "caption": "Table 1: Results (mean\u00b1std) on long-term forecasting scenarios with the best in bold and the second underlined. Each result contains three independent runs with different seeds. During the training phase, ElasTST utilizes a loss reweighting strategy where a single trained model is applied across all inference horizons, where the Hmax is set to 720. Other baseline models undergo horizon-specific training and tuning. Additional baseline results are detailed in Appendix D.1.", "description": "This table presents the results of long-term forecasting experiments comparing ElasTST with other state-of-the-art models.  ElasTST uses a loss reweighting strategy during training to simulate varied forecasting horizons, whereas other models are trained and tuned specifically for each horizon.  The table shows NMAE and NRMSE scores for various prediction lengths across multiple datasets.", "section": "4.1 Main Results"}, {"figure_path": "ucXUtMPWhv/tables/tables_16_1.jpg", "caption": "Table 1: Results (mean\u00b1std) on long-term forecasting scenarios with the best in bold and the second underlined. Each result contains three independent runs with different seeds. During the training phase, ElasTST utilizes a loss reweighting strategy where a single trained model is applied across all inference horizons, where the Hmax is set to 720. Other baseline models undergo horizon-specific training and tuning. Additional baseline results are detailed in Appendix D.1.", "description": "This table presents the results of long-term forecasting experiments comparing ElasTST with other state-of-the-art models.  ElasTST uses a reweighting strategy during training, while other models are trained and tuned specifically for each horizon. The results show NMAE and NRMSE across various forecasting horizons (96, 192, 336, 720) and datasets (ETTm1, ETTm2, ETTh1, ETTh2, Electricity, Traffic, Weather, Exchange).  Bold values indicate the best performance for each horizon/dataset combination.", "section": "4.1 Main Results"}, {"figure_path": "ucXUtMPWhv/tables/tables_20_1.jpg", "caption": "Table 1: Results (mean\u00b1std) on long-term forecasting scenarios with the best in bold and the second underlined. Each result contains three independent runs with different seeds. During the training phase, ElasTST utilizes a loss reweighting strategy where a single trained model is applied across all inference horizons, where the Hmax is set to 720. Other baseline models undergo horizon-specific training and tuning. Additional baseline results are detailed in Appendix D.1.", "description": "This table presents the results of long-term forecasting experiments comparing ElasTST with other state-of-the-art models.  It highlights ElasTST's performance across various forecasting horizons (96, 192, 336, and 720) and datasets (ETTm1, ETTm2, ETTh1, ETTh2, Electricity, Traffic, Weather, and Exchange).  The key takeaway is that ElasTST, trained with a reweighting scheme on a single horizon, achieves competitive or superior performance compared to models that were specifically trained and tuned for each horizon.", "section": "4.1 Main Results"}, {"figure_path": "ucXUtMPWhv/tables/tables_21_1.jpg", "caption": "Table 7: Memory consumption of ElasTST using different position encoding approaches. The batch size is 1 and the forecasting horizon is 1024.", "description": "This table compares the maximum GPU memory usage and the number of parameters (NPARAMS) for ElasTST using three different position encoding methods: Absolute Position Embedding (Abs PE), Rotary Position Embedding (RoPE) without tunable parameters, and Rotary Position Embedding with tunable parameters (Tunable RoPE).  It demonstrates the minimal memory overhead introduced by using RoPE.", "section": "F.2 Memory Usage Introduced by TROPE"}, {"figure_path": "ucXUtMPWhv/tables/tables_21_2.jpg", "caption": "Table 8: Memory consumption under different patch size settings. The batch size is 1 and the forecasting horizon is 1024.", "description": "This table shows the maximum GPU memory usage and the number of parameters (NPARAMS) for different patch size configurations in the ElasTST model.  The configurations include using a single patch size (p=1, p=8, p=16, p=32) and multiple patch sizes (p={1,8,16,32}, p={8,16,32}). The results demonstrate the impact of patch size on memory consumption and model parameters.", "section": "F.3 Memory Usage Introduced by Multi-scale Patch Assembly"}]