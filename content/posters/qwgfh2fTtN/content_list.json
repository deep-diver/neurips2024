[{"type": "text", "text": "Easy-to-Hard Generalization: Scalable Alignment Beyond Human Supervision ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Zhiqing $\\mathbf{Sun}^{1}\\cdot$ \u2217, Longhui $\\mathbf{Y}\\mathbf{u}^{2*},$ Yikang Shen3, Weiyang Liu4,5, Yiming Yang1\u2020, Sean Welleck1\u2020, Chuang Gan3,6\u2020 ", "page_idx": 0}, {"type": "text", "text": "1Carnegie Mellon University, 2Peking University, 3MIT-IBM Watson AI Lab 4University of Cambridge, $^{5}\\mathrm{Max}$ Planck Institute for Intelligent Systems, 6UMass Amherst ", "page_idx": 0}, {"type": "text", "text": "Code: Edward-Sun/easy-to-hard ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Current AI alignment methodologies rely on human-provided demonstrations or judgments, and the learned capabilities of AI systems would be upper-bounded by human capabilities as a result. This raises a challenging research question: How can we keep improving the systems when their capabilities have surpassed the levels of humans? This paper answers this question in the context of tackling hard reasoning tasks (e.g., level 4-5 MATH problems) via learning from human annotations on easier tasks (e.g., level 1-3 MATH problems), which we term as easy-to-hard generalization. Our key insight is that an evaluator (reward model) trained on supervisions for easier tasks can be effectively used for scoring candidate solutions of harder tasks and hence facilitating easy-to-hard generalization over different levels of tasks. Based on this insight, we propose a novel approach to scalable alignment, which firstly trains the (process-supervised) reward models on easy problems (e.g., level 1-3), and then uses them to evaluate the performance of policy models on hard problems. We show that such easy-to-hard generalization from evaluators can enable easy-to-hard generalizations in generators either through re-ranking or reinforcement learning (RL). Notably, our process-supervised 7b RL model and 34b model (reranking $@1024)$ achieves an accuracy of $34.0\\%$ and $52.5\\%$ on MATH500, respectively, despite only using human supervision on easy problems. Our approach suggests a promising path toward AI systems that advance beyond the frontier of human supervision. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Rapid advancements in large language models (LLMs) indicate that in the near future, highly sophisticated AI systems could surpass human capabilities in certain areas, significantly enhancing our capabilities in solving harder problems beyond the levels we can currently solve [47, 49]. Since the current AI alignment methods mostly rely on either supervised fine-tuning (SFT) with humanprovided demonstrations [59, 78, 14] or reinforcement learning from human feedback (RLHF) [97, 68, 50], their capabilities would be inherently limited as humans cannot always provide helpful demonstrations or supervision on the hard tasks beyond their expertise [64]. ", "page_idx": 0}, {"type": "text", "text": "In order to build future AI systems for tackling complex challenges, such as advancing scientific knowledge, it is crucial to develop new approaches for scalable oversight challenge, i.e., to supervise the AI systems that can potentially outperform humans in most skills [9]. The key question is: ", "page_idx": 0}, {"type": "image", "img_path": "qwgfh2fTtN/tmp/4b774194aed0c7eca3376a866d7cd75e7b5093af5461aefcebf221d3b78079b9.jpg", "img_caption": [], "img_footnote": [], "page_idx": 1}, {"type": "image", "img_path": "qwgfh2fTtN/tmp/d80df2db3bf34db8e0deedfe133ece5e40df36bf4a975fadce98b64210f84756.jpg", "img_caption": [], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "humans supervise strong models on hard tasks ", "page_idx": 1}, {"type": "image", "img_path": "qwgfh2fTtN/tmp/e2e6d4d611f0eeacb468e8c26ce4008a4245e1692ee3955d2cdee588468c0670.jpg", "img_caption": ["weak models unreliably supervise strong models on hard tasks that humans can evaluate "], "img_footnote": [], "page_idx": 1}, {"type": "image", "img_path": "qwgfh2fTtN/tmp/9cd76d441b5339a2ab3cd0091b7042d7677c0bd2a2e49d388da4783d0240da11.jpg", "img_caption": ["humans reliably supervise strong models on easy tasks and evaluate them on hard tasks "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Figure 1: Illustration of different alignment scenarios: traditional alignment relies on human demonstrations or judgements [50]; scalable alignment [9] assumes that humans cannot reliably supervise smarter-than-human models; weak-to-strong generalization [11] focuses on using weak models with unreliable labels to supervise strong models; Our proposed easier-to-general generalization focuses on the transfer of rewarding policies from weak models to harder tasks. ", "page_idx": 1}, {"type": "text", "text": "We refer to this scenario as Easy-to-Hard Generalization [63, 95, 11, 29]. This setting requires no human supervision on the harder tasks, which differs from existing work that either enhances humans\u2019 ability to verify the outputs of AI systems [81, 60, 9, 57] or enables weak-to-strong generalization via a teacher that only offers unreliable or noisy supervision [11]. ", "page_idx": 1}, {"type": "text", "text": "The most basic form of easy-to-hard generalization can be achieved by training the policy models (i.e., generator) using supervised fine-tuning (SFT) or in-context learning (ICL) on easy tasks [55, 10], and expect this will unlock the ability to perform well on hard tasks. However, it has been observed that SFT or ICL training of generators on easy tasks often fails to generalize to hard tasks [71, 24, 95]. We hypothesize and show that methods beyond these can enable stronger degrees of easy-to-hard generalization. Our intuition is guided by the observation that evaluation is easier than generation [34, 46], so an evaluator may offer a degree of easy-to-hard generalization that is useful for improving a generator. If that is true, we can first train a verifier on easy tasks, then make use of its generalization ability to supervise the generator on hard tasks. ", "page_idx": 1}, {"type": "text", "text": "Complex tasks can often be broken down into smaller steps [95] and verified by validating the individual steps \u2013 a strategy that is commonly employed in solving mathematical problems [74, 40, 73]. Inspired by this, we train outcome-supervised and process-supervised reward models [74, 85, 75, 40] as our easy-to-hard evaluators. The training dataset is often comprised of a set of labeled easy tasks, each with a question and a high-quality solution1, paired with a set of unlabeled hard tasks that are represented only by their questions. This simulates the practical setting of having numerous problems with known solutions, as well as significant unresolved challenges, such as the Millennium Prize Problems [12], which present challenging open problems. The pivotal aspect of easy-to-hard generalization thus lies in how we effectively leverage the capabilities of easier-level models in solving harder problems. ", "page_idx": 1}, {"type": "text", "text": "Our investigation includes to training policy and reward models on the easy (i.e., level 1-3) portion of the PRM800K [40] dataset, and comparing the performance of majority voting with the policy model only and weighted majority voting with the policy model and PRMs (Process-supervised Reward Models). We also introduce the Outcome & Process Reward Model (OPRM), which harnesses the complementary strengths of outcome reward models (ORMs) and process reward models (PRMs): judging if each step in reasoning is correct (like PRMs do) and deciding if the final answer is right (like ORMs do). Our findings reveal a marked performance improvement with the inclusion of reward models, especially on the hard (i.e., level 4-5) portion of the MATH500 test set. This improvement indicates that easier-level evaluators can maintain their effectiveness on harder tasks. We have similar observations in our experiments on the MetaMath dataset [86] and the Math-Shepherd dataset [75]. ", "page_idx": 1}, {"type": "image", "img_path": "qwgfh2fTtN/tmp/5f43849bc55671eab2d0015c2ebd27ff82e70b92368d654b0180557a2e325126.jpg", "img_caption": ["Figure 2: We first train the evaluator with process supervision or outcome supervision (which simulates the process supervision) to enable easy-to-hard evaluation, and then use it to facilitate easy-to-hard generation via re-ranking or RL. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "We further investigate the use of the easy-to-hard evaluator as a reward model in reinforcement learning, where the evaluator provides targeted, step-by-step guidance in solving hard problems. We have an intriguing finding that training with human supervision only on the easy tasks (i.e., training with Level 1-3 problems and answers) can outperform both SFT and Final-Answer RL training on the full dataset (Level 1-5). This finding underscores the potential of using easy-to-hard evaluation to improve easy-to-hard generators, particularly when dealing with varied levels of task complexity. ", "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 Scalable Oversight ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "While present-day models operate within the scope of human assessment, future, more advanced models may engage in tasks that are beyond human evaluation capabilities. This raises a concern that such models might prioritize objectives other than maintaining accuracy (Andreas 3, Perez et al. 53, Sharma et al. 64, Wei et al. 80). To address this, a branch of research develops techniques to enhance the human capacity to supervise such models, such as via using AI to evaluate the work of other AIs [1, 38, 60, 9]. Our setting differs from enhancing human oversight; instead, we focus on enabling models to excel in hard tasks where human supervision may not be available. This also differs from weak-to-strong generalization [11], where human supervision may be available, but not reliable, on hard tasks. However, our framework aligns with the \u201csandwiching\u201d concept proposed for measuring progress in scalable oversight, which involves domain experts evaluating the outputs of AI-assisted non-experts [18, 9, 57]. ", "page_idx": 2}, {"type": "image", "img_path": "qwgfh2fTtN/tmp/17fd8659e302f5955bf21b665159eabf998356631f144e17c07b4d4f83b1224d.jpg", "img_caption": ["Figure 3: The overview diagram of our methods: the different components of modeling and training and how they are interconnected. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "2.2 Compositional Generalization ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Compositional generalization is a fundamental aspect of how language works [13]. It refers to the ability to understand and utilize novel combinations based on the understanding of basic concepts and a limited number of their combinations [23]. Recently, least-to-most prompting [95, 20] teaches language models how to solve a complex problem by reducing it to a series of easier sub-problems, achieving easy-to-hard generalization on semantic parsing tasks like SCAN [37] and CFQ [35] with perfect generalization accuracy. In addition, least-to-most prompting has also been successful in mathematical reasoning tasks, specifically in datasets like GSM8K [16] and DROP [21], by teaching language models to solve problems more difficult than those seen in the prompts. This success not only underscores the capacity of language models to effectively break down complex tasks into simpler sub-tasks Perez et al. [51], but also demonstrates their generalization capability in solving these sub-problems. ", "page_idx": 3}, {"type": "text", "text": "2.3 Easy-to-Hard Generalization ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Past work has evaluated easy-to-hard generalization by training easy-to-hard generators on easy tasks using supervised finetune-tuning (SFT) or in-context learning (ICL) [55, 10]. Nevertheless, Swayamdipta et al. [71] showed that the BERT model performs poorly on common-sense reasoning when only trained on easy data. Fu et al. [24] showed similar results for ICL on reasoning tasks like GSM8K [17]. In concurrent work, Hase et al. [29] evaluate the performance of easy-to-hard generators on more datasets and models, and find that ICL or SFT on easy tasks is a strong baseline for multiple-choice tasks like ARC [15] and MMLU [30]. In contrast, we evaluate the easy-to-hard generation performance on the more challenging MATH dataset [32], and show that easy-to-hard evaluation can improve a generator\u2019s easy-to-hard generalization beyond ICL and SFT. Iterative machine teaching [43] gives theoretical justification to show that training classifiers from easy to hard examples yield better generalization. ", "page_idx": 3}, {"type": "text", "text": "3 Methodology ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We study the easy-to-hard generalization problem: how can we enable capabilities beyond human supervision? Specifically, we explore the efficacy and scalability of various easy-to-hard methodologies on competition-level mathematical problem-solving problems (MATH; Hendrycks et al. 32). This dataset is suitable for our study since it explicitly categorizes problems across five difficulty levels. We consider levels 1-3 as \u201ceasy\u201d tasks, encompassing both the problems and their respective solution demonstrations, along with the correct answers. Conversely, levels 4-5, characterized by their more complex nature, are treated as \u201chard\u201d tasks and are represented solely by their questions. The MATH dataset\u2019s difficulty distribution roughly follows a $1:2:2:3:3$ ratio across levels 1 to 5. So our division maintains a balanced number of easy and hard tasks. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "The remainder of the paper aims to answer following research questions: ", "page_idx": 4}, {"type": "text", "text": "RQ1: How do generators generalize from easy to hard? ", "page_idx": 4}, {"type": "text", "text": "RQ2: How do evaluators generalize from easy to hard? ", "page_idx": 4}, {"type": "text", "text": "RQ3: If evaluators generalize better than generators, how can we take advantage of this to enable stronger easy-to-hard generalization in generators? ", "page_idx": 4}, {"type": "text", "text": "3.1 Setup", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Dataset MATH [32] is a dataset of 12,500 challenging competition mathematics problems, where 7,500 of them are training problems and 5,000 are originally used for testing. Following Lightman et al. [40], Wang et al. [75], we use the identical subset of 500 representative problems (i.e., MATH500) as our test set, uniformly sample another 500 problems for validation, across all five difficulty levels, and leave the rest 4,000 MATH test split problems combined with the original 7,500 MATH training split problems as our training set. ", "page_idx": 4}, {"type": "text", "text": "Simulated Human Demonstrations While the original MATH dataset provides full step-by-step solutions, these solutions typically skip many chain-of-thought steps [79], which can be hard for language models to directly imitate2. Instead, we consider filtered PRM800K [40] and MetaMATH [86] as our SFT training data: the former is generated by a Minerva-style base GPT-4 model using few-shot prompting after filtering the correct answers [39, 48], while the latter is generated by ChatGPT [47]. We keep all the GSM8K data in the MetaMATH dataset since they are typically easier than the problems in MATH. PRM800K comes with human annotated process labels, while for MetaMath, we use Math-Shepherd as the corresponding process labels [75]. ", "page_idx": 4}, {"type": "text", "text": "3.2 Generators ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "For a given dataset (e.g., a variant of MATH), we consider the following generator models: ", "page_idx": 4}, {"type": "text", "text": "Full & Hard ICL Full in-context learning (ICL) is a base model prompted with exemplars sampled from all difficulty levels, or only from the level 5 [24]. ", "page_idx": 4}, {"type": "text", "text": "Easy-to-Hard ICL This model is prompted with exemplars from easy problems. This baseline evaluates the degree to which a model can solve problems more difficult than those seen in the prompts [95]. ", "page_idx": 4}, {"type": "text", "text": "Full SFT As prior work suggests that finetuning should outperform prompting alone [68, 52, 50], the full supervised fine-tuning (SFT) model is typically considered as a ceiling that a model can achieve on a type of task [11, 29]. ", "page_idx": 4}, {"type": "text", "text": "Easy-to-Hard SFT This generator model is trained only on the easy tasks. Prior work suggests that it can generalize to hard tasks but with some degeneration in performance [71]. ", "page_idx": 4}, {"type": "text", "text": "The generator models are evaluated in greedy decoding and self-consistency (also known as majority voting) settings [76]. ", "page_idx": 4}, {"type": "text", "text": "3.3 Evaluators ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Similarly, we consider the following evaluator models that can be trained either on the easy tasks only, or on the full dataset. Notably, unlike final-answer rewards, reward models trained on easy tasks can be applied to evaluate solutions to hard problems. ", "page_idx": 4}, {"type": "text", "text": "Final-Answer Reward is a symbolic reward that provides a binary reward based on the accuracy of the model\u2019s final answer. The matching is performed after normalization3. ", "page_idx": 4}, {"type": "text", "text": "Table 1: Easy-to-hard generalization of generators. We compare generator performance under various decoding settings. PRM800K and METAMATH indicate the SFT training data and ICL exemplars. Evaluations are performed on the same MATH500 test set. ", "page_idx": 5}, {"type": "table", "img_path": "qwgfh2fTtN/tmp/57100500d52543aa08a0ddc393d3d6697c009806daaa9c3692added3ab663c81.jpg", "table_caption": [], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "Outcome Reward Model (ORM) is trained on the Final-Answer rewards. Following Cobbe et al. [16], Uesato et al. [74], Lightman et al. [40], we train the reward head to predict on every token whether the solution is correct, in a similar sense to a value model [85]. At inference time, we use the ORM\u2019s prediction at the final token as the reward of the solution. ", "page_idx": 5}, {"type": "text", "text": "Process Reward Model (PRM) is trained to predict whether each step (delimited by newlines) in the chain-of-thought reasoning path is correct. The labels are usually labeled by humans [74, 40] or estimated with rollouts [65, 75]. ", "page_idx": 5}, {"type": "text", "text": "Outcome & Process Reward Model (OPRM) Building on the distinct advantages of ORMs and PRMs, we introduce the Outcome & Process Reward Model (OPRM), which harnesses the complementary strengths of both. OPRM is trained on the mixed data of ORMs and PRMs. Specifically, it evaluates the correctness of each intermediate reasoning step, akin to PRMs, while also assesses the overall solution\u2019s accuracy at the final answer stage, mirroring the functionality of ORMs. ", "page_idx": 5}, {"type": "text", "text": "3.4 Optimizing Generators Against Evaluators ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Finally, given a generator model (i.e., policy model) and a evaluator model (i.e., reward model; RM), we optimize the generator against the evaluator using either re-ranking or reinforcement learning. ", "page_idx": 5}, {"type": "text", "text": "Best-of- ${\\mathbf{}}_{n}$ (BoN), also known as rejection sampling, is a reranking approach that sample multiple solutions from the generator and selects one with the highest RM score. ", "page_idx": 5}, {"type": "text", "text": "Weighted Voting is similar to majority voting or self-consistency [76], but weights each solution according to its RM score [74]. ", "page_idx": 5}, {"type": "text", "text": "Reinforcement Learning (RL) We consider three online/offline RL variants, Reinforced SelfTraining (ReST) [28, 67], Direct Policy Optimization (DPO) [56], and Proximal Policy Optimization (PPO) [62]. Due to the space limit, please find their detailed description in Appendix B. ", "page_idx": 5}, {"type": "text", "text": "3.5 Evaluation Metrics ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this study, we have chosen not to establish terms analogous to the weak-to-strong performance gap recovery (PGR) as discussed in Burns et al. [11] or the easy-to-hard supervision gap recovery (SGR) highlighted by Hase et al. [29]. This decision is based on our observations that sometimes, models trained exclusively on simpler tasks\u2014particularly when employing RL training\u2014can outperform those trained across the entire spectrum of problem difficulties. Therefore, we mainly focus on the absolute and relative performance of generators (optionally optimized by the evaluator) on the MATH500 test set [40]. ", "page_idx": 5}, {"type": "text", "text": "3.6 Implementation Details ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Base Language Model Llemma is a large language model for mathematics [6], which is continue pretrained from Code Llama [58] / LlaMA-2 [72]. We use both 7b and 34b variants in our experiments. ", "page_idx": 5}, {"type": "image", "img_path": "qwgfh2fTtN/tmp/9e017f8cc1d76683c9f0b59d0fd03ad01e8342d03739317d6a70a46897d81d7c.jpg", "img_caption": ["Figure 4: Easy-to-hard generalization of 7b (upper) and 34b (lower) evaluators. Both SFTs and RMs are trained on the easy data. We found that PRMs trained on easy tasks can significantly improve the re-ranking (i.e., weighted voting) performance on hard tasks. The shaded margin of the curve plot in this paper represents the performance variance. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "SFT / RL / Reward Model We fine-tune all models in full fine-tuning with frozen input-output embedding layers and normalization layers. RMs are initialized from the base model, and have an added scalar head to output the reward. In PPO training, we initialize the value model from the reward model. ", "page_idx": 6}, {"type": "text", "text": "Hyper-parameters Due to the space limit, our training hyper-parameters can be found in Appendix. C. ", "page_idx": 6}, {"type": "text", "text": "4 Main Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "4.1 Easy-to-Hard Generalization of Generators ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In Table 1, we compare the easy-to-hard generalization performance of the generators under various decoding settings: ", "page_idx": 6}, {"type": "text", "text": "Supervised Fine-Tuning (SFT) outperforms In-Context Learning (ICL): This is consistent with prior work [68, 50, 74]. We also find that the performance of ICL has larger variance than SFT with respect to data ordering (or random seeds) [19, 93]. ", "page_idx": 6}, {"type": "text", "text": "SFT data quality impacts easy-to-hard generalization: PRM800K data is generated by a base (unaligned) GPT-4 model through few-shot prompting and is thus of lower quality than well-aligned ChatGPT-generated MetaMATH data. We find that only MetaMath-trained models have certain easy-to-hard gaps (e.g., 16.6 v.s. 14.2 in MetaMath-7b-ICL), while such gaps in PRM800K-trained models are very small (less than $1\\%$ ), or even inverted in the ICL setting. We hypothesize that low-quality SFT data may only teach the model the format of the task [59, 78, 76], while high-quality (imitation) SFT data can teach the model the principles of solving the task [70, 27]. Nevertheless, the strongest performance is achieved by full SFT on the high-quality MetaMath data (35.4), showing an unignorable difference, with a gap of up to 3.2, compared to its easy-to-hard SFT counterpart (32.2). ", "page_idx": 6}, {"type": "text", "text": "4.2 Easy-to-Hard Generalization of Evaluators ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "The primary metric we use to assess the effectiveness of our process reward model is not the average accuracy of verifying each step in a solution but rather the overall performance achieved through re-ranking methods (See discussion in Sec. 3.5). We first use re-ranking to evaluate the easy-to-hard generalization performance of evaluators. ", "page_idx": 6}, {"type": "image", "img_path": "qwgfh2fTtN/tmp/8bada9ffb4f16c2c319eba721c60835d68f3218c67610bca8e8318c047c89a2b.jpg", "img_caption": ["Figure 5: Easy-to-hard generalization of evaluators applied to generators of different sizes. We evaluated 7b generator $+\\,34\\mathfrak{b}$ evaluator (upper) and 34b generator $\\mathbf{+\\,7b}$ evaluator (lower). Both SFTs and RMs are trained on the easy data. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "4.2.1 Re-ranking ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We consider two re-ranking strategies: Best-of- $^{n}$ (or rejection sampling) and Weighted Voting. In our easy-to-hard generalization setting, both SFT models and Reward Models (RMs) are trained on easier tasks (levels 1-3), but evaluated on all difficulty levels (1-5). We compare the performance between majority voting (SFT only) and re-ranking $\\mathrm{(SFT+OPRM)}$ on the PRM800K dataset in Figure 4-5, and the performance of different reward models (PRMs, ORMs, & OPRMs) on the PRM800K dataset in Figure 8-9. Specifically, we use min as the reward aggregation function for best-of- $n$ and prod for weighted voting4. The figures illustrate the performance of different decoding strategies or reward models under the same number of sampled solutions per problem. We have the following findings: ", "page_idx": 7}, {"type": "text", "text": "OPRMs outperforms ORMs and PRMs This confirms our hypothesis that Process Reward Models (PRMs) and Outcome Reward Models (ORMs) capture different aspects of task-solving processes. By integrating the strengths of both PRMs and ORMs, Outcome & Process Reward Models (OPRMs) demonstrate superior performance. However, follow-up experiments conducted on the MetaMath/Math-Shepherd datasets do not demonstrate significant improvements from incorporating additional ORM training examples. This lack of enhancement may be attributed to the fact that Math-Shepherd is already generated from final-answer rewards. This suggests that there remains a substantial difference between process rewards labeled by humans (e.g., PRM800K) and those generated automatically (e.g., Math-Shepherd). ", "page_idx": 7}, {"type": "text", "text": "Weighted voting outshines Best-of- $n$ This finding diverges from past research where minimal performance differences were observed between weighted voting and Best-of- ${\\mathbf{\\nabla}}n$ [40, 74]. Our hypothesis is that this discrepancy arises from our specific experiment, which involves training a less powerful base model (Llemma; Azerbayev et al. 6) on more difficult tasks (MATH; Hendrycks et al. 32). This setup might diminish the effectiveness of the reward model, potentially leading to an over-optimization of rewards [25]. Given these insights, weighted voting is preferred as the primary re-ranking method for further discussions. Nevertheless, Best-of- ${\\cdot n}$ still achieves competitive performance to majority voting when producing only one full solution. In Figure 5, we also find that the 34b evaluator can significantly improve the 7b generator, while the 7b evaluator can still improve the performance of the 34b generator. ", "page_idx": 7}, {"type": "text", "text": "Greater effectiveness of re-ranking on harder tasks: Weighted voting not only consistently surpasses majority voting but also shows a more pronounced advantage on harder tasks. This observation leads to the conclusion that evaluators demonstrate better easy-to-hard generalization capabilities in comparison to generators. This motivates us to explore RL approaches that optimize the generator against the evaluator to further improve the performance of easy-to-hard generation. ", "page_idx": 7}, {"type": "table", "img_path": "qwgfh2fTtN/tmp/1dab95d5a8293f18e1a4961027f2b061efcede60898f82c865211d493bf31a0c.jpg", "table_caption": ["Table 2: Comparing reinforcement learning (RL) approaches for easy-to-hard generalization. All methods are of 7b size and evaluated with greedy decoding. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "4.2.2 Reinforcement Learning (RL) ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Given the conclusion above, an important question arises: how can evaluators once again assist generators in achieving enhanced easy-to-hard generalization capabilities? We further investigate the enhancement of policy models through RL, utilizing easy-to-hard evaluators as reward models. Similar to re-ranking, SFT and PRM are only trained on easy data. For a fair comparison between PRM800K and MetaMath, we only use vanilla PRMs in the RL training. All the RL methods use the validation accuracy for selecting the best checkpoint5. Our comparison spans offilne (ReST & DPO) and online (PPO) RL algorithms under two training conditions: ", "page_idx": 8}, {"type": "text", "text": "Easy Questions & Easy Final Answers. The SFT model samples from easy questions and receives the corresponding Final-Answer and optional PRM rewards. ", "page_idx": 8}, {"type": "text", "text": "All Questions & Easy Final Answers. This assumes access to a range of easy and hard problems for RL training, with rewards for hard tasks solely provided by the easy-to-hard evaluator. ", "page_idx": 8}, {"type": "text", "text": "Based on the results reported in Table 2, we have the following findings: ", "page_idx": 8}, {"type": "text", "text": "DPO and PPO excel over ReST. Among the RL algorithms trained on the PRM800K dataset, PPO emerges as the most effective, significantly surpassing both ReST and DPO. On the MetaMATH dataset, PPO and DPO achieve top performance, while ReST shows only marginal improvements over the SFT baseline. The comparative analysis between DPO and PPO across the PRM800K and MetaMATH datasets indicates that while DPO\u2019s efficacy is on par with PPO given a high-quality SFT model as initialization, PPO\u2019s effectiveness is less contingent on the quality of the underlying SFT model [50, 56]. ", "page_idx": 8}, {"type": "text", "text": "PRM rewards are more beneficial than Final-Answer rewards for hard tasks. Notably, models trained with PRM rewards with human supervision on the easy tasks (achieving a top performance of 34.0) outperform the previous state-of-the-art model trained across all task levels (33.0). This highlights the effectiveness of leveraging easy-to-hard evaluations to improve generator performance across varying task difficulties. ", "page_idx": 8}, {"type": "table", "img_path": "qwgfh2fTtN/tmp/9885fded0ab95797b7cc52a364b94209ee84a7b30edac61766b91ef2162cf32f.jpg", "table_caption": ["Table 3: Easy-to-hard generalization of evaluators on coding problems (APPS). Both SFTs and RMs are trained on the easy (Introductory) data. We found that ORMs trained on easy tasks can improve the re-ranking (Best-of-N) performance on hard (Interview & Competition) coding problems. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "4.3 Easy-to-Hard Generalization on the Coding Domain ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We conduct further experiments in the coding domain with the APPS dataset [31]. Similarly to Lightman et al. [40], we sub-sampled 500 questions from the original test set of APPS as our test set. Specifically, we sub-sampled 100 Introductory questions, 300 Interview questions, and 100 Competition questions, following the original distribution in the test set. ", "page_idx": 9}, {"type": "text", "text": "In Table 3, we compare the performance of SFT-trained Code Llama [58] (7b & 34b) with greedy decoding and best-of-N approach. In the latter, an Outcome Reward Model (ORM) of the same model size is trained to select the best coding one from N sampled solutions. ", "page_idx": 9}, {"type": "text", "text": "We found that while the reward model is only trained on the outcome supervision of easy (Introductory) data, it significantly improves the model performance on hard (Interview & Competition) data. These findings extend the premise of easy-to-hard generalization beyond the confines of mathematical reasoning, suggesting its applicability across diverse domains. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Our study advances the field of AI alignment by demonstrating the potential of easy-to-hard generalization, where models trained on simpler tasks can be guided to solve more complex problems without direct human supervision on these harder tasks. Through the use of (process-supervised) reward models for evaluating and enhancing policy models, we show that evaluators can facilitate this form of generalization, outperforming traditional training methods. Our findings highlight the effectiveness of re-ranking strategies and reinforcement learning (RL) in leveraging evaluators for performance gains on difficult tasks. This approach presents a promising direction for developing AI systems capable of surpassing human problem-solving capabilities, suggesting a scalable alignment method that could enable AI to independently advance knowledge in complex domains. ", "page_idx": 9}, {"type": "text", "text": "While our study provides valuable insights into easy-to-hard generalization and the potential of process-supervised reward models, there are limitations to consider. These include the focus on specific model sizes and datasets, the domain specificity of reasoning tasks, and the need for further research on the long-term implications and robustness of the method. ", "page_idx": 9}, {"type": "text", "text": "6 Acknowledgement ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work is supported by OpenAI Superalignment Fast Grants and Microsoft Accelerate Foundation Models Research (AFMR) Initiative. Additionally, ZS thanks Google PhD Fellowship; SW thanks NSF SCALE (NSF DMS 2134012) and Convergent Research. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Man\u00e9. Concrete problems in ai safety. arXiv preprint arXiv:1606.06565, 2016. 3 ", "page_idx": 9}, {"type": "text", "text": "[2] Shengnan An, Zexiong Ma, Zeqi Lin, Nanning Zheng, Jian-Guang Lou, and Weizhu Chen. Learning from mistakes makes llm better reasoner. arXiv preprint arXiv:2310.20689, 2023. 18 [3] Jacob Andreas. Language models as agent models. In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 5769\u20135779, 2022. 3 [4] Thomas Anthony, Zheng Tian, and David Barber. Thinking fast and slow with deep learning and tree search. Advances in neural information processing systems, 30, 2017. 18 [5] Mohammad Gheshlaghi Azar, Mark Rowland, Bilal Piot, Daniel Guo, Daniele Calandriello, Michal Valko, and R\u00e9mi Munos. A general theoretical paradigm to understand learning from human preferences. arXiv preprint arXiv:2310.12036, 2023. 18 [6] Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen McAleer, Albert Q Jiang, Jia Deng, Stella Biderman, and Sean Welleck. Llemma: An open language model for mathematics. arXiv preprint arXiv:2310.10631, 2023. 6, 8, 18, 24 [7] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862,   \n2022. 18 [8] Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran-Johnson, Ethan Perez, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamile Lukosuite, Liane Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schiefer, Noemi Mercado, Nova DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston, Shauna Kravec, Sheer El Showk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly, Tom Henighan, Tristan Hume, Samuel R. Bowman, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph, Sam McCandlish, Tom Brown, and Jared Kaplan. Constitutional ai: Harmlessness from ai feedback, 2022. 18 [9] Samuel R Bowman, Jeeyoon Hyun, Ethan Perez, Edwin Chen, Craig Pettit, Scott Heiner, Kamile Lukosuite, Amanda Askell, Andy Jones, Anna Chen, et al. Measuring progress on scalable oversight for large language models. arXiv preprint arXiv:2211.03540, 2022. 1, 2, 3 [10] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D. Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in Neural Information Processing Systems, 33:1877\u20131901, 2020.   \n2, 4 [11] Collin Burns, Pavel Izmailov, Jan Hendrik Kirchner, Bowen Baker, Leo Gao, Leopold Aschenbrenner, Yining Chen, Adrien Ecoffet, Manas Joglekar, Jan Leike, et al. Weak-to-strong generalization: Eliciting strong capabilities with weak supervision. arXiv preprint arXiv:2312.09390,   \n2023. 2, 3, 5, 6 [12] James A Carlson, Arthur Jaffe, and Andrew Wiles. The millennium prize problems. American Mathematical Soc., 2006. 2 [13] Noam Chomsky. On the representation of form and function. 1981. 4 [14] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416, 2022. 1 [15] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. 4   \n[16] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. 4, 6, 18   \n[17] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. 4   \n[18] Ajeya Cotra. The case for aligning narrowly superhuman models. In AI Alignment Forum, 2021. 3   \n[19] Jesse Dodge, Gabriel Ilharco, Roy Schwartz, Ali Farhadi, Hannaneh Hajishirzi, and Noah Smith. Fine-tuning pretrained language models: Weight initializations, data orders, and early stopping. arXiv preprint arXiv:2002.06305, 2020. 7   \n[20] Andrew Drozdov, Nathanael Sch\u00e4rli, Ekin Aky\u00fcrek, Nathan Scales, Xinying Song, Xinyun Chen, Olivier Bousquet, and Denny Zhou. Compositional semantic parsing with large language models. In The Eleventh International Conference on Learning Representations, 2022. 4   \n[21] Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2368\u20132378, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1246. URL https://aclanthology. org/N19-1246. 4   \n[22] Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, and Tatsunori B Hashimoto. Alpacafarm: A simulation framework for methods that learn from human feedback. arXiv preprint arXiv:2305.14387, 2023. 20   \n[23] Jerry A Fodor and Ernest Lepore. The compositionality papers. Oxford University Press, 2002. 4   \n[24] Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, and Tushar Khot. Complexity-based prompting for multi-step reasoning. In The Eleventh International Conference on Learning Representations, 2022. 2, 4, 5, 18   \n[25] Leo Gao, John Schulman, and Jacob Hilton. Scaling laws for reward model overoptimization. In International Conference on Machine Learning, pages 10835\u201310866. PMLR, 2023. 8   \n[26] Zhibin Gou, Zhihong Shao, Yeyun Gong, Yujiu Yang, Minlie Huang, Nan Duan, Weizhu Chen, et al. Tora: A tool-integrated reasoning agent for mathematical problem solving. arXiv preprint arXiv:2309.17452, 2023. 18   \n[27] Arnav Gudibande, Eric Wallace, Charlie Snell, Xinyang Geng, Hao Liu, Pieter Abbeel, Sergey Levine, and Dawn Song. The false promise of imitating proprietary llms. arXiv preprint arXiv:2305.15717, 2023. 7   \n[28] Caglar Gulcehre, Tom Le Paine, Srivatsan Srinivasan, Ksenia Konyushkova, Lotte Weerts, Abhishek Sharma, Aditya Siddhant, Alex Ahern, Miaosen Wang, Chenjie Gu, et al. Reinforced self-training (rest) for language modeling. arXiv preprint arXiv:2308.08998, 2023. 6, 18   \n[29] Peter Hase, Mohit Bansal, Peter Clark, and Sarah Wiegreffe. The unreasonable effectiveness of easy training data for hard tasks. arXiv preprint arXiv:2401.06751, 2024. 2, 4, 5, 6   \n[30] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In International Conference on Learning Representations, 2020. 4   \n[31] Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song, et al. Measuring coding challenge competence with apps. arXiv preprint arXiv:2105.09938, 2021. 10 [32] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. 4, 5, 8, 18, 24 [33] Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. Swe-bench: Can language models resolve real-world github issues? arXiv preprint arXiv:2310.06770, 2023. 18 [34] Richard M Karp. On the computational complexity of combinatorial problems. Networks, 5(1):   \n45\u201368, 1975. 2 [35] Daniel Keysers, Nathanael Sch\u00e4rli, Nathan Scales, Hylke Buisman, Daniel Furrer, Sergii Kashubin, Nikola Momchev, Danila Sinopalnikov, Lukasz Stafiniak, Tibor Tihon, et al. Measuring compositional generalization: A comprehensive method on realistic data. In International Conference on Learning Representations, 2019. 4 [36] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. arXiv preprint arXiv:2205.11916, 2022. 18 [37] Brenden Lake and Marco Baroni. Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks. In International conference on machine learning, pages 2873\u20132882. PMLR, 2018. 4 [38] Jan Leike, David Krueger, Tom Everitt, Miljan Martic, Vishal Maini, and Shane Legg. Scalable agent alignment via reward modeling: a research direction. arXiv preprint arXiv:1811.07871,   \n2018. 3 [39] Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. Solving quantitative reasoning problems with language models. arXiv preprint arXiv:2206.14858, 2022.   \n5, 18 [40] Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let\u2019s verify step by step. arXiv preprint arXiv:2305.20050, 2023. 2, 5, 6, 8, 10, 18, 19, 30 [41] Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. Program induction by rationale generation: Learning to solve and explain algebraic word problems. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 158\u2013167, 2017. 18 [42] Bingbin Liu, Sebastien Bubeck, Ronen Eldan, Janardhan Kulkarni, Yuanzhi Li, Anh Nguyen, Rachel Ward, and Yi Zhang. Tinygsm: achieving> $80\\%$ on gsm8k with small language models. arXiv preprint arXiv:2312.09241, 2023. 18 [43] Weiyang Liu, Bo Dai, Ahmad Humayun, Charlene Tay, Chen Yu, Linda B Smith, James M Rehg, and Le Song. Iterative machine teaching. In International Conference on Machine Learning, pages 2149\u20132158. PMLR, 2017. 4 [44] Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, and Dongmei Zhang. Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. arXiv preprint arXiv:2308.09583,   \n2023. 18 [45] R\u00e9mi Munos, Michal Valko, Daniele Calandriello, Mohammad Gheshlaghi Azar, Mark Rowland, Zhaohan Daniel Guo, Yunhao Tang, Matthieu Geist, Thomas Mesnard, Andrea Michi, et al. Nash learning from human feedback. arXiv preprint arXiv:2312.00886, 2023. 18 [46] Moni Naor. Evaluation may be easier than generation. In Proceedings of the twenty-eighth annual ACM symposium on Theory of computing, pages 74\u201383, 1996. 2 [47] OpenAI. OpenAI: Introducing ChatGPT, 2022. URL https://openai.com/blog/chatgpt.   \n1, 5 ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "[49] OpenAI. OpenAI: GPT-4, 2023. URL https://openai.com/research/gpt-4. 1 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "[50] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155, 2022. 1, 2, 5, 7, 9,   \n18, 20 [51] Ethan Perez, Patrick Lewis, Wen-tau Yih, Kyunghyun Cho, and Douwe Kiela. Unsupervised question decomposition for question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 8864\u20138880, 2020. 4 [52] Ethan Perez, Douwe Kiela, and Kyunghyun Cho. True few-shot learning with language models. Advances in neural information processing systems, 34:11054\u201311070, 2021. 5 [53] Ethan Perez, Sam Ringer, Kamile\u02d9 Luko\u0161iu\u00afte\u02d9, Karina Nguyen, Edwin Chen, Scott Heiner, Craig Pettit, Catherine Olsson, Sandipan Kundu, Saurav Kadavath, et al. Discovering language model behaviors with model-written evaluations. arXiv preprint arXiv:2212.09251, 2022. 3 [54] Stanislas Polu and Ilya Sutskever. Generative language modeling for automated theorem proving. arXiv preprint arXiv:2009.03393, 2020. 18 [55] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by generative pre-training. 2018. 2, 4 [56] Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D Manning, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. arXiv preprint arXiv:2305.18290, 2023. 6, 9, 18, 20 [57] David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R Bowman. Gpqa: A graduate-level google-proof q&a benchmark. arXiv preprint arXiv:2311.12022, 2023. 2, 3 [58] Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, J\u00e9r\u00e9my Rapin, et al. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950, 2023. 6, 10 [59] Victor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, et al. Multitask prompted training enables zero-shot task generalization. In International Conference on Learning Representations,   \n2021. 1, 7 [60] William Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Long Ouyang, Jonathan Ward, and Jan Leike. Self-critiquing models for assisting human evaluators. arXiv preprint arXiv:2206.05802,   \n2022. 2, 3 [61] John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. Highdimensional continuous control using generalized advantage estimation. arXiv preprint arXiv:1506.02438, 2015. 20 [62] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. 6, 18 [63] Avi Schwarzschild, Eitan Borgnia, Arjun Gupta, Furong Huang, Uzi Vishkin, Micah Goldblum, and Tom Goldstein. Can you learn an algorithm? generalizing from easy to hard problems with recurrent networks. Advances in Neural Information Processing Systems, 34:6695\u20136706, 2021.   \n2 [64] Mrinank Sharma, Meg Tong, Tomasz Korbak, David Duvenaud, Amanda Askell, Samuel R Bowman, Newton Cheng, Esin Durmus, Zac Hatfield-Dodds, Scott R Johnston, et al. Towards understanding sycophancy in language models. arXiv preprint arXiv:2310.13548, 2023. 1, 3   \n[65] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of Go with deep neural networks and tree search. Nature, 529(7587):484\u2013489, 2016. 6   \n[66] David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go without human knowledge. nature, 550(7676):354\u2013359, 2017. 18   \n[67] Avi Singh, John D Co-Reyes, Rishabh Agarwal, Ankesh Anand, Piyush Patil, Peter J Liu, James Harrison, Jaehoon Lee, Kelvin Xu, Aaron Parisi, et al. Beyond human data: Scaling self-training for problem-solving with language models. arXiv preprint arXiv:2312.06585, 2023. 6, 18, 19, 20   \n[68] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F Christiano. Learning to summarize with human feedback. Advances in Neural Information Processing Systems, 33:3008\u20133021, 2020. 1, 5, 7, 18   \n[69] Zhiqing Sun, Yikang Shen, Hongxin Zhang, Qinhong Zhou, Zhenfang Chen, David Cox, Yiming Yang, and Chuang Gan. Salmon: Self-alignment with principle-following reward models. arXiv preprint arXiv:2310.05910, 2023. 18   \n[70] Zhiqing Sun, Yikang Shen, Qinhong Zhou, Hongxin Zhang, Zhenfang Chen, David Cox, Yiming Yang, and Chuang Gan. Principle-driven self-alignment of language models from scratch with minimal human supervision. arXiv preprint arXiv:2305.03047, 2023. 7   \n[71] Swabha Swayamdipta, Roy Schwartz, Nicholas Lourie, Yizhong Wang, Hannaneh Hajishirzi, Noah A Smith, and Yejin Choi. Dataset cartography: Mapping and diagnosing datasets with training dynamics. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 9275\u20139293, 2020. 2, 4, 5   \n[72] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. 6, 18   \n[73] Trieu H Trinh, Yuhuai Wu, Quoc V Le, He He, and Thang Luong. Solving olympiad geometry without human demonstrations. Nature, 625(7995):476\u2013482, 2024. 2   \n[74] Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, and Irina Higgins. Solving math word problems with process- and outcome-based feedback. arXiv preprint arXiv:2211.14275, 2022. 2, 6, 7, 8, 18, 29   \n[75] Peiyi Wang, Lei Li, Zhihong Shao, RX Xu, Damai Dai, Yifei Li, Deli Chen, Y Wu, and Zhifang Sui. Math-shepherd: Verify and reinforce llms step-by-step without human annotations. CoRR, abs/2312.08935, 2023. 2, 3, 5, 6, 9, 19, 20, 26   \n[76] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language model with self generated instructions. arXiv preprint arXiv:2212.10560, 2022. 5, 6, 7   \n[77] Zihan Wang, Yunxuan Li, Yuexin Wu, Liangchen Luo, Le Hou, Hongkun Yu, and Jingbo Shang. Multi-step problem solving through a verifier: An empirical analysis on model-induced process supervision. arXiv preprint arXiv:2402.02658, 2024. 30   \n[78] Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. In International Conference on Learning Representations, 2021. 1, 7   \n[79] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. NeurIPS, 2022. 5, 18   \n[80] Jerry Wei, Da Huang, Yifeng Lu, Denny Zhou, and Quoc V Le. Simple synthetic data reduces sycophancy in large language models. arXiv preprint arXiv:2308.03958, 2023. 3   \n[81] Jeff Wu, Long Ouyang, Daniel M Ziegler, Nisan Stiennon, Ryan Lowe, Jan Leike, and Paul Christiano. Recursively summarizing books with human feedback. arXiv preprint arXiv:2109.10862, 2021. 2   \n[82] Jing Xu, Andrew Lee, Sainbayar Sukhbaatar, and Jason Weston. Some things are more cringe than others: Preference optimization with the pairwise cringe loss. arXiv preprint arXiv:2312.16682, 2023. 18   \n[83] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629, 2022. 18   \n[84] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Grifftihs, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. arXiv preprint arXiv:2305.10601, 2023. 18   \n[85] Fei Yu, Anningzhe Gao, and Benyou Wang. Outcome-supervised verifiers for planning in mathematical reasoning. arXiv preprint arXiv:2311.09724, 2023. 2, 6   \n[86] Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical questions for large language models. arXiv preprint arXiv:2309.12284, 2023. 3, 5, 18, 19, 20, 26   \n[87] Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Chuanqi Tan, and Chang Zhou. Scaling relationship on learning mathematical reasoning with large language models. arXiv preprint arXiv:2308.01825, 2023. 18   \n[88] Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mammoth: Building math generalist models through hybrid instruction tuning. arXiv preprint arXiv:2309.05653, 2023. 18   \n[89] Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. Star: Bootstrapping reasoning with reasoning. Advances in Neural Information Processing Systems, 35:15476\u201315488, 2022. 18, 19   \n[90] Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. Automatic chain of thought prompting in large language models. arXiv preprint arXiv:2210.03493, 2022. 18   \n[91] Yao Zhao, Mikhail Khalman, Rishabh Joshi, Shashi Narayan, Mohammad Saleh, and Peter J Liu. Calibrating sequence likelihood improves conditional language generation. In The Eleventh International Conference on Learning Representations, 2022. 18   \n[92] Yao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman, Mohammad Saleh, and Peter J Liu. Slichf: Sequence likelihood calibration with human feedback. arXiv preprint arXiv:2305.10425, 2023. 18   \n[93] Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. Calibrate before use: Improving few-shot performance of language models. In International Conference on Machine Learning, pages 12697\u201312706. PMLR, 2021. 7   \n[94] Chuanyang Zheng, Zhengying Liu, Enze Xie, Zhenguo Li, and Yu Li. Progressive-hint prompting improves reasoning in large language models. arXiv preprint arXiv:2304.09797, 2023. 18   \n[95] Denny Zhou, Nathanael Sch\u00e4rli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc V Le, et al. Least-to-most prompting enables complex reasoning in large language models. In The Eleventh International Conference on Learning Representations, 2022. 2, 4, 5, 18   \n[96] Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy Ba. Large language models are human-level prompt engineers. arXiv preprint arXiv:2211.01910, 2022. 18 ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "[97] Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019. 1 ", "page_idx": 16}, {"type": "text", "text": "A Additional Related Work ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "A.1 Rationale-Augmented (Mathematical) Reasoning ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Ling et al. [41] pioneer the work of solving math word problems by generating step-by-step solutions before the final answer. Cobbe et al. [16] extend this work by constructing a much larger dataset to finetune a pre-trained large language model to solve math word problems, and a outcome-supervised verifier to rank candidate solutions. Wei et al. [79] demonstrate that the reasoning ability of a language model can be elicited through the use of prefixed rationales. Subsequent research [36, 83, 39, 96, 84] in tasks requiring human-level reasoning skills has also highlighted the efficacy of rationale augmentation. ", "page_idx": 17}, {"type": "text", "text": "Among all the reasoning tasks, we select mathematical reasoning to evaluate easy-to-hard generalization ability, given that mathematical reasoning serves as a valuable assessment for complex reasoning abilities and features a clear delineation of difficulty levels. Recent research efforts focus on prompt design [79, 95, 24, 90, 94] to elicit the intrinsic reasoning capabilities of models, or data engineering for fine-tuning [44, 87, 88, 86, 26, 42, 2, 6], which draws on experts to provide high-quality training datasets. Our work is categorized as fine-tuning based work. However, unlike previous work, our focus lies in exploring how to generalize to more challenging mathematical problems when only provided with easy mathematical data. ", "page_idx": 17}, {"type": "text", "text": "A.2 Outcome Reward Models & Process Reward Models ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "For some multi-step complex reasoning tasks, such as generating highly complex code, it may be challenging for humans to fully grasp the outputs produced by an advanced AI system. In such scenarios, process-supervised reward models (PRMs) present a promising solution [74, 40]. These models operate by supervising each step in the reasoning or generation process, rather than focusing solely on the end result. They are particularly effective in tasks where the reasoning process itself is as important as the final outcome [32, 33]. ", "page_idx": 17}, {"type": "text", "text": "Uesato et al. [74] find that process-supervised reward models (PRMs) achieve better performance than outcome-supervised reward models (ORMs) when re-ranking sampled solutions from the policy model, but their performance is similar during reinforcement learning (RL) via expert iteration [66, 4, 54, 89, 28, 67]. Lightman et al. [40] compare ORMs and PRMs with a more capable base model [48] and significantly more human-labeled process feedback on the more challenging MATH dataset, and also find that PRMs significantly outperform ORMs in the reranking setting. In contrast to these works, which only study the effectivenss of PRM in an independent and identically distributed (IID) domain, we study the utilization of PRMs in the easy-to-hard generalization scenario, and show that easy-to-hard evaluators instantiated by PRMs can enable easy-to-hard generation of policy models. ", "page_idx": 17}, {"type": "text", "text": "B Reinforcement Learning Algorithms ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Reinforced Self-Training (ReST) is an offline RL algorithm, which alternates between generating samples from the policy, which are then used to improve the LLM policy with RM-weighted SFT [28, 67]. Its variants include expert iteration [4] and rejection sampling fine-tuning [72, 87]. ", "page_idx": 17}, {"type": "text", "text": "Direct Policy Optimization (DPO) is a class of offilne RL algorithms [56] that consider both positive and negative gradient updates. It fine-tunes the policy model on a preference dataset consisting of paired positive and negative samples. The variants include NLHF [45], IPO [5], and SLiC [91, 92]. Recent work shows that iteratively applying DPO leads to improved performance [82]. ", "page_idx": 17}, {"type": "text", "text": "Proximal Policy Optimization (PPO) is an online RL algorithm which samples from the policy during fine-tuning [62]. It is widely used in RLHF [68, 7, 50] and RLAIF [8, 69]. ", "page_idx": 17}, {"type": "text", "text": "C Hyper-parameters ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "C.1 Supervised Fine-Tuning & Reward Modeling ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "For the PRM800K dataset [40], the SFT model is trained using steps that are labeled as correct. For the MetaMath dataset [86], given that the original dataset can contain upwards of ten solutions for the same question, potentially leading to over-ftiting, we implement a flitering process. This process ensures that, during any given epoch, no more than three solutions per question are retained, thereby mitigating the risk of over-fitting. ", "page_idx": 18}, {"type": "text", "text": "The PRMs are trained on the corresponding released dataset [40, 75]. For generating solutions to train ORMs, we sample 32 solutions for each question from the language model using top-K sampling with $K{=}20$ and temperature of 0.7. We also ensure that the ratio between positive and negative samples for each question is between 1:3 to 3:1. ", "page_idx": 18}, {"type": "text", "text": "See Table 4 for a list of training hyper-parameters used in the training jobs. We use full fine-tuning for all SFT/RM training. ", "page_idx": 18}, {"type": "table", "img_path": "qwgfh2fTtN/tmp/69517fec38fdea85d98af033eb2010db3878f35806a63043f29c50347885e784.jpg", "table_caption": ["Table 4: Hyper-parameters in our SFT/RM training jobs "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "C.2 Re-Ranking ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "For majority voting, weighted voting, and best-of- ${\\cdot n}$ , we sample from the language model using top-K sampling with $K{=}20$ and temperature of 0.7. At test time, we use the ORM\u2019s prediction at the final token as the overall score for the solution, and use the PRM\u2019s prediction at each intermediate step (denoted by the new line symbol) and the final token as the process reward scores. ", "page_idx": 18}, {"type": "text", "text": "C.3 Reinforcement Learning ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We use full fine-tuning during the RL stage. ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "ReST-EM Following Singh et al. [67], we sample 32 solutions for each question from the language model using top-K sampling with $\\mathrm{K}{=}40$ . We also used a cut-off threshold of 10 for the maximum number of solutions per problem [89, 67]. We performed iterative ReST training for two epochs, and observed performance degeneration starting from the third epoch. For PRM800K, we used a temperature of 1.0, while for MetaMath, we used a temperature of 1.2. The rest training hyperparameters are the same as in SFT training. ", "page_idx": 18}, {"type": "text", "text": "Iterative DPO We sample 8 solutions for each question from the language model using top-K sampling with $K{=}20$ and temperature of 1.0. We use the process reward model to assign a score between 0 and 1 to each solution, and use final-answer reward to assign an additional 0/1 score to each solution. A preference training pair is constructed only when the score difference between positive and negative solutions is greater than 1.0. We used a cut-off threshold of 3 for the maximum number of preference pairs per problem. ", "page_idx": 18}, {"type": "table", "img_path": "qwgfh2fTtN/tmp/68567a4f21bd6899bc9a8b8fed05aedbc87d85218b32dc5bec922050be2e608c.jpg", "table_caption": ["Table 5: Full results of comparing reinforcement learning (RL) approaches for easy-to-hard generalization. All methods are of 7b size and evaluated with greedy decoding. $^{\\dagger}$ indicates the model is trained with additional final-answer labels on hard tasks (similar to Singh et al. [67]), which is not strictly a easy-to-hard generalization setup. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "For all DPO training [56], we used a learning rate of $2\\times10^{-6}$ , a batch size of 64, and a DPO training epoch of 1. We set $\\beta=0.1$ for all DPO experiments, and performed at most 5 DPO iterations (i.e., sampling new solutions and performing one DPO epoch). ", "page_idx": 19}, {"type": "text", "text": "PPO We follow Dubois et al. [22] on the implementation of the PPO algorithm, which is a variant of $[50]^{6}$ . Specifically, we normalize the advantage across the entire batch of rollouts obtained for each PPO step and initialize the value model from the reward model. ", "page_idx": 19}, {"type": "text", "text": "We clipped the gradient by its Euclidean norm at a limit of 1. Our training spanned 500 PPO steps on the RL data (MATH questions except MATH500 and our 500 validation questions). For generalized advantage estimation (GAE; Schulman et al. [61]), both $\\lambda$ and $\\gamma$ were set at 1. ", "page_idx": 19}, {"type": "text", "text": "For PRM800K, we used a batch size of 512 for each PPO step. This comprised 8 epochs of gradient steps, each having 64 rollouts. We applied a peak learning rate of $2\\times10^{-5}$ with cosine decay. We opted for a constant KL regularizer coefficient of 0.01, and a sampling temperature of 0.7. ", "page_idx": 19}, {"type": "text", "text": "For MetaMath/Math-Shepherd, we used a batch size of 512 for each PPO step. This comprised 2 epochs of gradient steps, each having 256 rollouts. We applied a peak learning rate of $5^{'}\\times10^{-6}$ with cosine decay. We opted for a constant KL regularizer coefficient of 0.002, and a sampling temperature of 1.2. ", "page_idx": 19}, {"type": "text", "text": "D Re-ranking Results on MetaMath ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Similar to Sec. 4.2.1, we assess the effectiveness of process reward models on the MetaMath/MathShepherd dataset [86, 75]. From Figure 6, we can see that PRMs are also more effective on harder tasks when trained on MetaMath/Math-Shepherd. ", "page_idx": 19}, {"type": "table", "img_path": "qwgfh2fTtN/tmp/9b8db961a561b7b5d3fe4b23caa2eb132ac429af42ed3b111ef7cf1d6b22e266.jpg", "table_caption": ["Table 6: Pass $\\boldsymbol{\\@}\\mathbf{N}$ scores (upper bound of Best-of-N) on coding problems (APPS). "], "table_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "qwgfh2fTtN/tmp/adf3a2d27e222dc9150df501c4cd04d72f95dd5b02dc33591da5ea8394c4481b.jpg", "img_caption": ["Figure 6: Easy-to-hard generalization of evaluators on MetaMath. Both SFTs and RMs are trained on the easy data. "], "img_footnote": [], "page_idx": 20}, {"type": "table", "img_path": "qwgfh2fTtN/tmp/2e436015732ea841570854922baa8dd0c62f9a07671a56e5f8c07585fa6da5d4.jpg", "table_caption": [], "table_footnote": ["Table 7: Qualitative comparison between MetaMath-7b-SFT and MetaMath- $\\cdot7\\mathfrak{b}$ -PPO, as well as the first error detected in MetaMath-SFT greedy decoding by Math-Shepherd. Examples are cherrypicked for better illustration. "], "page_idx": 21}, {"type": "text", "text": "F Performance of DPO Iterations ", "text_level": 1, "page_idx": 21}, {"type": "image", "img_path": "qwgfh2fTtN/tmp/7325a4248a9cf960ef4985591241e710c28f7eccd7591a3b134c1af6cfc2f23f.jpg", "img_caption": [], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "", "img_caption": ["Figure 7: Test performance as a function of DPO iterations. "], "img_footnote": [], "page_idx": 21}, {"type": "table", "img_path": "qwgfh2fTtN/tmp/43045055571af9fd834d93e480feec73a1c11f07b0382f9009b9ef7d3ebe629e.jpg", "table_caption": [], "table_footnote": ["Table 8: The distribution shift before and after PPO training of a Llemma-7B model trained on PRM800K. We believe such a big shift could explain why does PPO work exceptionally well on PRM800K, where the policy is initialized from a weak SFT model. "], "page_idx": 22}, {"type": "text", "text": "G Re-ranking Performance Analysis of PRMs, ORMs & OPRMs ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We compare the re-ranking performance of Process Reward Models (PRMs), Outcome Reward Models (ORMs), and our proposed Outcome & Process Reward Models (OPRMs). Figure 8 shows the results on 7b models and Figure 9 is on 34b models. We find that in our setting of Llemma $\\left[6\\right]+$ MATH [32], PRMs and ORMs perform similarly, with PRMs slightly outperforming ORMs on hard tasks. But the OPRMs that trained on the mixed data of PRMs and ORMs significantly outperforms both of them. ", "page_idx": 23}, {"type": "image", "img_path": "qwgfh2fTtN/tmp/8df344a673b87da54aaaa3cac24d819c0802dc252e20019a5c57008c7465da94.jpg", "img_caption": ["Figure 8: Comparing process reward models (PRMs, left), outcome reward models (ORMs, middle), and outcome & process reward models (OPRMs, right) on 7b models trained on the PRM800K dataset. Both SFTs and RMs are trained on the easy data. "], "img_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "qwgfh2fTtN/tmp/d1dd7b2d3a3cf06cd85c3036c8b576544869a8c3251b7af562e7a16618898bc5.jpg", "img_caption": ["Figure 9: Comparing process reward models (PRMs, left), outcome reward models (ORMs, middle), and outcome $\\&$ process reward models (OPRMs, right) on 34b models trained on the PRM800K dataset. Both SFTs and RMs are trained on the easy data. "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "H Re-ranking Results on MetaMath ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Similar to Sec. 4.2.1, we assess the effectiveness of process reward models on the MetaMath/MathShepherd dataset [86, 75]. From Figure 10, we can see that PRMs are also more effective on harder tasks when trained on MetaMath/Math-Shepherd. ", "page_idx": 25}, {"type": "image", "img_path": "qwgfh2fTtN/tmp/ea257b363618a6aade387dc2b6bd0f7496528e8c11097a73f604abbf1b161a02.jpg", "img_caption": ["Figure 10: Comparing process reward models (PRMs, left, trained on Meth-Shepherd), outcome reward models (ORMs, middle), and outcome & process reward models (OPRMs, right) on 7b models trained on the MetaMath dataset. Both SFTs and RMs are trained on the easy data. "], "img_footnote": [], "page_idx": 25}, {"type": "image", "img_path": "qwgfh2fTtN/tmp/f6903041a7bdd87c55c051efdbdbaa74564e898231201538f9c9082b21102d33.jpg", "img_caption": ["I More Comparisons ", "Figure 11: Comparing different reward models with Weighted Voting (upper) and Best-of-N (lower) on 7b models trained on the PRM800K dataset. Both SFTs and RMs are trained on the easy data. "], "img_footnote": [], "page_idx": 26}, {"type": "image", "img_path": "qwgfh2fTtN/tmp/fb821a2f68c7219e77efeae83c5960558a018925b6130da9fdf6a6858da49048.jpg", "img_caption": ["Figure 12: Comparing different reward models with Weighted Voting (upper) and Best-of-N (lower) on 34b models trained on the PRM800K dataset. Both SFTs and RMs are trained on the easy data. "], "img_footnote": [], "page_idx": 26}, {"type": "image", "img_path": "qwgfh2fTtN/tmp/546c37cad88069bdbcfcdfb2aaee8d478a92752f0a7068998ad178f8118f3052.jpg", "img_caption": ["Figure 13: Comparing different reward models with Weighted Voting (upper) and Best-of-N (lower) on 34b SFT model and 7b reward model trained on the PRM800K dataset. Both SFTs and RMs are trained on the easy data. "], "img_footnote": [], "page_idx": 27}, {"type": "image", "img_path": "qwgfh2fTtN/tmp/1a74c6825ac30acfeed5f9f7eee0294d48fed9bc9190b9ef03008de4a0dc71b6.jpg", "img_caption": ["Figure 14: Comparing different reward models with Weighted Voting (upper) and Best-of-N (lower) on 7b models trained on the MetaMath dataset. Both SFTs and RMs are trained on the easy data. "], "img_footnote": [], "page_idx": 27}, {"type": "table", "img_path": "qwgfh2fTtN/tmp/8d639f71e1415e6a78cb9270a26ffa6e8ad6d7bca136201c3066d20480ac48d1.jpg", "table_caption": ["Table 9: Results of Full, Easy-to-Hard, & Hard-to-Easy SFT training of the Llemma-7b model "], "table_footnote": [], "page_idx": 28}, {"type": "image", "img_path": "qwgfh2fTtN/tmp/9d0553feba71238d1170930f02b2b7c8b379ce3e6ad32ed345b6b09dc9812afc.jpg", "img_caption": ["Figure 15: The agreement between the prediction from the Llemma-7b-based reward model when trained on ORM and PRM data, and their agreement to ground-truth final-answer labels. "], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "J Hard-to-Easy Generalization ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "From Table 5, it is evident that reinforcement learning training on hard tasks alone significantly underperforms compared to training the model on easy tasks or on all tasks. This difference is especially pronounced for PPO on the PRM800K dataset. This raises a crucial question: does training on hard tasks only generalize to easy tasks? ", "page_idx": 28}, {"type": "text", "text": "To address this, we fine-tuned the Llemma-7b model using all data (easy and hard), only easy data, and only hard data. As shown in Table 9, we found that training on all data consistently yields the best performance. Conversely, the generator\u2019s performance deteriorates when transitioning from easyto-hard and hard-to-easy tasks. This suggests that language models face difficulties in generalizing in both directions. ", "page_idx": 28}, {"type": "text", "text": "It is also worth noting that while Full SFT underperforms Easy-to-Hard SFT and Hard-to-Easy SFT on hard test questions, it eventually outperforms Easy-to-Hard SFT and Hard-to-Easy SFT when evaluated on all test questions. We believe that this is because by exposing the model to a wider variety of unique questions and difficulties, it gains a better understanding of the problem space in general, as measured by the accuracy on the full distribution. ", "page_idx": 28}, {"type": "text", "text": "K On ORM\u2019s Approximation of PRM Labels ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "From Sec. G, we observe that in PRM800K, PRMs and ORMs exhibit similar performance levels, with OPRMs outperforming both. This raises the question of why ORMs also demonstrate strong easy-to-hard generalization ability. A straightforward explanation is that ORMs are trained to approximate PRM labels [74]. Specifically, ORMs are trained to predict the correctness of the entire solution through value estimation. As Uesato et al. [74] state, \u201cit is simpler for the ORM to learn to recognize when steps are correct than it is to check the answer by internally computing the final answer itself.\u201d ", "page_idx": 28}, {"type": "text", "text": "Nevertheless, people may argue that the conclusion from Uesato et al. [74] is based on GSM8K\u2019s experimental results, so the conclusion may not transfer to the more challenging Hendrick\u2019s MATH dataset. To show the universal existence of \u201cORM\u2019s approximation of PRM labels\u201d, we further conduct evaluation of agreement between different rewards on two variants of the MATH dataset: PRM800K and MetaMath. ", "page_idx": 28}, {"type": "text", "text": "The results are shown in Figure 15. Similarly to the findings from Uesato et al. [74], we see that the ORM has higher agreement with the PRM, despite being trained to predict the Final-Answer rewards. ", "page_idx": 28}, {"type": "text", "text": "Thus, \u201cthis result indicates that the ORM tends more towards predicting whether the full trace is correct, and not just whether the final answer is correct.\u201d ", "page_idx": 29}, {"type": "text", "text": "Overall, this shows easy-to-hard generalization is not exclusively linked to reward models trained on explicit step-wise annotations. It also applies to ORMs that are trained to perform value estimation and practically evaluates each solution step. ", "page_idx": 29}, {"type": "text", "text": "We also perform DPO training on a MetaMath-initialized Llemma-7b model. We find that in this RL setting, re-ranking the output pairs with ORM also gives similar performance to re-ranking with PRM (29.2 v.s. 30.4 & 31.2 v.s. 34.0). ", "page_idx": 29}, {"type": "text", "text": "L Analysis of Aggregation Functions in PRMs & OPRMs ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "We explored different methods to consolidate step-wise prediction scores into a single score value, a process we describe as employing an aggregation function, during the use of the evaluator. Lightman et al. [40] report comparable performance when using min (minimum) and prod (product) as the aggregation function to reduce multiple scores into a single solution-level score. Note that when training PRMs on PRM800K [40], we have already considered neutral steps to be positive as training labels. ", "page_idx": 29}, {"type": "text", "text": "Following Wang et al. [77], given $\\{p_{1},p_{2},\\dots,p_{n}\\}$ as a list of predicted correctness probability of each step (including the final answer), we considered the following aggregation functions: ", "page_idx": 29}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}&{\\operatorname{ain}=\\operatorname*{min}\\{p_{1},p_{2},\\dots,p_{n}\\}}\\\\ &{{\\mathrm{~as=~}}\\operatorname*{max}\\{p_{1},p_{2},\\dots,p_{n}\\}}\\\\ &{\\qquad\\qquad\\operatorname{prod}=\\prod p_{i}}\\\\ &{\\qquad\\qquad\\operatorname{man}={\\frac{\\sum_{i}p_{i}}{n}}}\\\\ &{\\qquad{\\mathrm{~as=~}}\\operatorname*{logit}=\\sigma\\left({\\frac{\\sum_{i}\\log\\frac{p_{i}}{1-p_{i}}}{n}}\\right)}\\\\ &{\\qquad{\\mathrm{~nean\\_odd=~}}\\operatorname{ReLU}\\left(\\sum_{i}{\\frac{p_{i}}{1-p_{i}}}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad}\\\\ &{\\qquad\\qquad\\qquad\\mathrm{~last=}p_{n}}\\end{array}}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "In Figure 16-18, we perform analysis of aggregation functions on PRM800K and Math-Shepherd (from MetaMath) datasets with weighted voting and best-of- ${\\mathbf{\\nabla}}n$ decoding and PRMs or OPRMs. In general, we find prod works universally well in weighted voting and min works well in best-of- ${\\cdot n}$ . So we adopt these two strategies in our main experiments. ", "page_idx": 29}, {"type": "text", "text": "One interesting finding is that for reward models trained on the human annotated process reward (e.g., PRM800K), the last strategy does not perform very well, but last works much better on OPRMs and pseudo PRMs (e.g., Math-Shepherd). This could partially explain why OPRMs does not further improve the performance on the Math-Shepherd dataset. ", "page_idx": 29}, {"type": "image", "img_path": "qwgfh2fTtN/tmp/185ec191c53c950d621c1784aa32bdbf8638dee8d7c1764a8f22eaea36c937a9.jpg", "img_caption": ["Figure 16: Analysis of aggregation functions in process reward models (PRMs) on the PRM800K dataset with Weighted Voting and Best-of-N. Both SFTs and RMs are trained on the easy data. "], "img_footnote": [], "page_idx": 30}, {"type": "image", "img_path": "qwgfh2fTtN/tmp/eedcb8253f32b0e080bf4c00ef9bccc1cb658a240621411f327a2c84bff3633d.jpg", "img_caption": ["Figure 17: Analysis of aggregation functions in outcome & process reward models (OPRMs) on the PRM800K dataset with Weighted Voting and Best-of-N. Both SFTs and RMs are trained on the easy data. "], "img_footnote": [], "page_idx": 31}, {"type": "image", "img_path": "qwgfh2fTtN/tmp/79165363cd5107379a3a0d3abb186d5ec98e6d3030f748e6c3dac25cd1067090.jpg", "img_caption": ["MetaMath- $.7\\mathbf{b}+$ Math-Shepherd- $.7\\mathbf{b}+$ Weighted Voting MetaMath- $.7\\mathbf{b}+$ Math-Shepherd $.7\\mathbf{b}+$ Weighted Voting MetaMath $.7\\mathbf{b}+$ Math-Shepherd $.7\\mathfrak{b}+$ Weighted Voting Accuracy on Easy (Level 1-3) Problems Accuracy on Hard (Level 4-5) Problems Accuracy on All (Level 1-5) Problems ", "Figure 18: Analysis of aggregation functions in psuedo process reward models (PRMs) on the Math-Shepherd (from MetaMath) dataset with Weighted Voting and Best-of-N. Both SFTs and RMs are trained on the easy data. "], "img_footnote": [], "page_idx": 32}, {"type": "text", "text": "M Societal Impact ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Our work on easy-to-hard generalization has the potential for both positive and negative societal impacts. On the positive side, this approach could enable AI systems to tackle increasingly complex problems in domains such as scientific discovery, healthcare, and education, potentially leading to groundbreaking advancements that benefit society. However, the development of AI systems that can operate beyond human supervision also raises concerns about the transparency, accountability, and potential misuse of such systems. It is crucial to carefully consider the ethical implications and establish robust safeguards to mitigate the risks of unintended consequences or malicious applications. Ongoing research and public discourse on the responsible development and deployment of these technologies will be essential to ensure that their societal beneftis outweigh the potential drawbacks. ", "page_idx": 32}, {"type": "image", "img_path": "qwgfh2fTtN/tmp/c3c07d830a9e1143e5e266a350f47967eaa89f7a463d905b20893a2a944ecfed.jpg", "img_caption": ["N Fine-Grained Analysis of OPRMs\u2019 Re-ranking strategies ", "Figure 19: Easy-to-hard generalization for different difficulty levels\u2019 data. Both SFTs and OPRMs are trained on the level1-3 data. Each row compares the performance of different OPRMs\u2019 reranking strategies across different levels\u2019 data. "], "img_footnote": [], "page_idx": 33}, {"type": "text", "text": "As shown in Figure 19, both the Best-of-N and Weighted Voting strategies demonstrate strong performance across all levels, which leverage the advantages of OPRM methods and thereby underscoring OPRM\u2019s effectiveness. Furthermore, despite the SFT models and OPRM models being trained on level 1-3 data, re-ranking strategies enhanced by OPRMs continue to perform well on the unseen and more challenging level 4-5 data. This indicates the feasibility of generalizing from easier to harder tasks using OPRMs. ", "page_idx": 33}, {"type": "image", "img_path": "qwgfh2fTtN/tmp/9295151a0bcf0ca042729e7bb7d163e47a29bfd2979f2c284cea59879efb0e7f.jpg", "img_caption": ["Figure 20: Easy-to-hard generalization for different type\u2019s data. Both SFTs and OPRMs are trained on the easy data. Each row compares the performance of different OPRMs\u2019 reranking strategies across different types\u2019 data. "], "img_footnote": [], "page_idx": 34}, {"type": "text", "text": "To determine which types of data benefti more from OPRM\u2019s easy-to-hard generalization and which types still struggle with this challenging generalization, we compare OPRM\u2019s generalization abilities on different problem types in Figure 20. Among Algebra, Counting & Probability, Geometry, Intermediate Algebra, Number Theory, Prealgebra, and Precalculus problems, OPRMs generalize best on Algebra, Intermediate Algebra, and Precalculus problems. Conversely, OPRMs generalize worst on Counting & Probability problems. These findings are highly valuable for practical system design, allowing us to decide when to use OPRMs to enhance performance based on the downstream data type. ", "page_idx": 34}, {"type": "image", "img_path": "qwgfh2fTtN/tmp/dfcdc5fdfcd021231e2eb222c5d6fe8ed5369aa942a355385355da0b513a66ac.jpg", "img_caption": ["O Pass@N Analysis for Different Math Questions ", "Figure 21: The increase of $\\mathrm{Pass}@\\mathrm{N}$ metrics with the number of samples, divided by level and category. As the number of samples increases, the Pass $\\mathbb{\\mathrm{(}}\\mathbb{\\alpha}\\mathrm{\\mathbf{N}}$ rate tends to improve. However, not all problems will eventually yield a correct solution, meaning that even as $_\\mathrm{N}$ approaches infinity, the Pass $\\mathbb{\\@N}$ metric may not reach $100\\%$ . "], "img_footnote": [], "page_idx": 35}, {"type": "text", "text": "Figure 21 shows the changes in Pass $\\mathbb{\\@N}$ as the number of samples increases. As shown in the figure, not all problems reach a $100\\%$ Pass $\\boldsymbol{\\@}\\mathbf{N}$ rate, even with a larger N. Specifically, three key observations can be made: First, Pass $\\mathbb{\\mathrm{(}}\\mathbb{\\alpha}\\mathrm{\\mathbf{N}}$ is strongly correlated with problem difficulty. As depicted in Figure 21, for Level 1 problems, the Pass $\\mathbb{\\textregistered}$ rate nearly saturates with more samples, while for Level 5 problems, many instances still do not achieve a correct solution. Second, larger models (e.g., 34b) perform significantly better on $\\operatorname{Pass}(\\varpi\\,1$ compared to smaller models (e.g., 7b). However, as N increases, the advantage of the larger model diminishes on metrics like Pass $@1024$ . Third, the variations in Pass $\\mathbb{\\mathrm{(}}\\mathbb{\\alpha}\\mathrm{\\mathbf{N}}$ across different categories of math questions are different, which is expected, as the difficulty of different types of math questions varies for large language models. ", "page_idx": 35}, {"type": "text", "text": "P Case Studies for OPRM ", "text_level": 1, "page_idx": 36}, {"type": "image", "img_path": "qwgfh2fTtN/tmp/7988e21de15f30c7fa1a9f6b5e951cac60695d812bb441f0c9f2b3d2e2e68325.jpg", "img_caption": [], "img_footnote": [], "page_idx": 36}, {"type": "text", "text": "Figure 22: Case Study 1. The evaluator can help identify and reduce the confidence of hallucinations caused by misleading information. ", "page_idx": 36}, {"type": "image", "img_path": "qwgfh2fTtN/tmp/d35f868005fc4b140b4d28984da7aadb6b5b589d7a911eb89db46349fd59ad44.jpg", "img_caption": [], "img_footnote": [], "page_idx": 36}, {"type": "text", "text": "Figure 23: Case Study 2. The evaluator can assist in reducing the confidence of solutions that misuse mathematical theorems or formulas. ", "page_idx": 36}, {"type": "text", "text": "We have included more case studies in Figures 22 and 23. We find that evaluator can help generalize to harder ones in the following ways: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The evaluator can help identify and reduce the confidence of hallucinations caused by misleading information in problems. As demonstrated in Case Study 1, the solution selected by majority voting with an answer of 36 is misled by the different units of measurement in the problem (2.5 hours and 90 seconds), resulting in an incorrect solution. Then, the ORPM model successfully gives this solution a low score.   \n\u2022 The evaluator can assist in reducing the confidence of solutions that misuse mathematical theorems. In Case Study 2, the majority solution incorrectly applies the theorem \"the sum of the exterior angles of a polygon is $360^{\\circ}$ , leading to erroneous reasoning, and low confidence by the ORPM model. ", "page_idx": 36}, {"type": "text", "text": "Q Step and Outcome ROC Curve ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "To assess the accuracy of reasoning step judgments of different reward models, we conducted additional experiments using the PRM800K-test data, which includes correctness annotations for each step, to test our model\u2019s ability to distinguish correct reasoning steps. We randomly selected a portion of PRM800K-test data to balance positive and negative samples. The accuracy of the reasoning steps for the three models is shown in Table 10. This ", "page_idx": 37}, {"type": "table", "img_path": "qwgfh2fTtN/tmp/9c11a62ffe7648bbe0ae460fc4653c59b72368feb97ad2ff8c2b89d750d9a6b7.jpg", "table_caption": [], "table_footnote": [], "page_idx": 37}, {"type": "text", "text": "Table 10: The accuracy of the reasoning steps for different models. ", "page_idx": 37}, {"type": "text", "text": "table demonstrates the effectiveness of our trained PRM, showing that PRM has a significantly greater ability to distinguish steps compared to ORM. Additionally, in Figure 24, we present the Step ROC curves of three models, where PRM and OPRM exhibit better step discrimination abilities compared to ORM. However, it is important to note that a stronger ability to distinguish steps does not necessarily indicate that the evaluator is more helpful for generation. We then also present the Outcome ROC curves of three models on discriminating the final outcome. We collect data generated on MATH500 test set from our 7B policy model. According to the final outcome and groundtruth, we label each data and select a positive-negative balanced set to plot the Outcome ROC curves, where OPRM exhibits better outcome discrimination abilities compared to ORM and PRM. The above table also shows the effectiveness of OPRM on Outcome discrimination ability. ", "page_idx": 37}, {"type": "image", "img_path": "qwgfh2fTtN/tmp/a455dcf8574503efe3d797cefea98bf9ef39694233d9dbc5c85a08a2a51a382e.jpg", "img_caption": ["Figure 24: The Step and Outcome ROC Curve show the ability of discriminate the reasoning step and the whole reasoning solution, illustrating OPRM\u2019s effectiveness on identifying both steps and outcomes. "], "img_footnote": [], "page_idx": 37}, {"type": "text", "text": "R Few-Shot Prompt in In-Context Learning ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "We sample with temperature $\\mathrm{T}=0.9$ for ICL-related experiments. We list our few-shot sample for the In-context learning experiments in Table 1. ", "page_idx": 38}, {"type": "text", "text": "4-shot example for PRM800K Full ICL: ", "text_level": 1, "page_idx": 38}, {"type": "table", "img_path": "qwgfh2fTtN/tmp/8c0c684324c9062d76b56f5e37b4b89f8093da1dba0dd821a8a642cd991cfc6b.jpg", "table_caption": [], "table_footnote": [], "page_idx": 38}, {"type": "text", "text": "4-shot example for PRM800K Hard ICL: ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Problem:   \nIn right triangle $\\Phi P{\\sf I R}\\Phi$ , we have \\$\\\\angle Q $=$ \\\\angle $\\mathbb{R}\\Phi$ and $\\Phi\\mathbb{P}\\mathbb{R}~=~6\\backslash\\backslash\\mathtt{s q r t}\\{2\\}\\Phi\\,.$ . What is the area of $\\mathfrak{F}$ \\\\triangle $\\mathtt{P Q R}\\Phi?$ ", "page_idx": 39}, {"type": "text", "text": "Solution: ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "I see that this is an isosceles right triangle, since \\$\\\\angle Q = \\\\angle $\\texttt{R}=$ 45^\\\\circ\\$.\\n\\nThat means that $\\Phi P Q=\\Phi R=\\mathsf{\\Phi}6\\backslash\\mathsf{V s q r t}\\{2\\}\\Phi\\cdot\\backslash\\mathfrak{n}\\backslash\\mathfrak{n}\\mathbb{T}\\circ$ find the area of a triangle, I need to multiply the base and the height and divide by $2.\\backslash\\mathtt{n}\\backslash\\mathtt{n}\\mathtt{I n}$ this case, the base and the height are both $\\mathfrak{F6}\\backslash\\backslash\\mathtt{s q r t}\\{2\\}\\mathfrak{F}$ , so the area is $\\Phi(6\\backslash\\backslash$ sqrt{2})(6\\\\sqrt{2}) $/2\\Phi$ . $\\setminus\\tt n V n I$ can simplify this expression by using the property that $\\phi\\backslash\\backslash\\mathtt{s q r t}\\{\\mathtt{a}\\}\\backslash\\backslash\\mathtt{s q r t}\\{\\mathtt{b}\\}\\ =\\ \\backslash\\backslash\\mathtt{s q r t}\\{\\mathtt{a b}\\}\\Phi\\ .\\ \\backslash\\mathtt{n}\\backslash\\mathtt{n S o}\\,,$ the area is $\\Phi(\\setminus\\setminus\\tt s q r t)$ {2}\\\\sqrt{2}) $(6`\\backslash\\mathsf{c d o t\\ 6})/2\\ =\\ 2\\backslash\\setminus\\mathsf{c d o t\\ 36}/2\\ =\\ 36\\Updownarrow.$ ", "page_idx": 39}, {"type": "text", "text": "Final Answer: The final answer is $\\mathbb{5368}$ . I hope it is correct. ", "page_idx": 39}, {"type": "text", "text": "Problem: ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "The length of the longer side of rectangle $\\Phi\\tt R\\Phi$ is $\\Phi10\\Phi$ percent more than the length of a side of square $\\Phi^{\\mathrm{S}}\\cdot\\Phi$ The length of the shorter side of rectangle $\\Phi\\tt R\\Phi$ is $\\Phi10\\Phi$ percent less than the length of a side of square $\\mathbb{S}\\mathbf{S}\\cdot\\mathbb{S}$ What is the ratio of the area of rectangle $\\Phi\\mathbb{R}\\Phi$ to the area of square $\\Phi\\mathbf{S}\\mathbf{?}\\Phi$ Express your answer as a common fraction. ", "page_idx": 39}, {"type": "text", "text": "Let the side of the square be \\$s\\$.\\n\\nThen the longer side of the rectangle is $\\Phi1.1$ $\\mathtt{s}\\Phi.\\backslash\\mathtt{n}\\backslash\\mathtt{n}\\mathtt{A n d}$ the shorter side of the rectangle is $\\Phi0\\,.98\\Phi\\,.\\,\\backslash\\,\\mathtt{n}\\backslash\\mathtt{n}\\mathtt{T h}\\cdot$ e area of the square is $\\mathbb{5}\\mathbf{s}^{-}2\\mathbb{5}$ .\\n\\nThe area of the rectangle is $\\oint\\!\\!1\\,.\\,1{\\bf s}\\ \\ast\\ 0\\,.\\,9{\\bf s}\\ =\\ 0\\,.\\,99{\\bf s}\\!\\sim\\!2\\Phi\\,.\\,\\backslash\\!{\\bf n}\\backslash{\\bf n}{\\bf S}\\circ$ the ratio of the area of rectangle $\\Phi\\mathbb{R}\\Phi$ to the area of square $\\Phi{\\tt S}\\Phi$ is \\$\\\\frac $\\{0\\,.99\\mathbf{s}{\\sim}2\\}\\{\\mathbf{s}{\\sim}2\\}\\ =\\ 0\\,.99\\Phi$ .\\n\\nTherefore, the answer is $\\mathfrak{F}$ \\\\boxed{\\\\frac{99}{100}}\\$.   \nFinal Answer: The final answer is . I hope it is correct. ", "page_idx": 39}, {"type": "text", "text": "Problem: ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Suppose that $\\mathfrak{s y}^{-}3\\mathfrak{s}$ varies inversely with \\$\\\\sqrt[3] $\\{\\mathbf{z}\\}\\Phi$ . If $\\mathfrak{g}_{\\mathtt{y}=2\\mathfrak{b}}$ when $\\mathbb{9}\\mathbf{z}{=}1\\mathbb{9}$ , find the value of $\\Phi_{}{\\bf{z}}\\Phi$ when $\\Phi_{\\mathrm{{J}}}{=}4\\Phi$ . Express your answer in simplest fractional form. ", "page_idx": 39}, {"type": "text", "text": "Solution: ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "I know that inverse variation means that the product of the two quantities is constant, so I can write an equation of the form \\$y^3\\\\cdot\\\\sqrt[3] $\\{{z}\\}{=}\\mathbf{k}\\Phi$ , where $\\mathbb{S}\\mathbb{k}\\Phi$ is some constant. $\\scriptstyle\\mathtt{\\backslash n\\backslash n T o}$ find $\\mathbb{S}\\mathbf{k}\\Phi$ , I can plug in the given values of $\\mathfrak{F}\\mathfrak{F}\\mathfrak{G}$ and $\\Phi_{7}\\Phi$ : $\\mathfrak{S}2^{-}3^{\\backslash}$ \\\\cdot\\\\sqrt[3] $\\{1\\}{=}\\mathbf{k}\\Phi$ , which simplifies to $\\Phi8=1\\times8$ . $\\ln^{'}$ \\nNow I can use this equation to find $\\Phi_{}{\\bf{z}}\\Phi$ when $\\Phi_{\\mathrm{{J}}}{=}4\\Phi$ : $\\mathbb{\\Phi}4^{-3}\\mathrm{`}$ \\cdot\\\\sqrt[3] $\\{{z}\\}{=}8\\Phi$ , which implies that $\\mathfrak{F}$ \\\\sqrt[3] $|\\{z\\}=\\backslash\\backslash\\mathtt{f r a c}\\{8\\}\\{64\\}=\\backslash\\backslash\\mathtt{f r a c}\\{1\\}\\{8\\}\\Phi.\\backslash\\mathtt{n}\\backslash\\mathtt{n}\\mathtt{T o}$ get rid of the cube root, I can cube both sides: $\\Phi\\mathbf{z}{=}$ \\\\left(\\\\frac{1}{8}\\\\right) $-3=$ \\\\frac $\\{1\\}\\{512\\}\\Phi$ . ", "page_idx": 39}, {"type": "text", "text": "Final Answer: The final answer is $\\mathfrak{F}\\backslash\\{\\mathtt{f r a c}\\{1\\}\\{512\\}\\mathfrak{F}$ . I hope it is correct. ", "page_idx": 39}, {"type": "text", "text": "Problem: ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Let $\\Phi\\mathrm{d}\\Phi$ be a positive number such that when $\\Phi109\\Phi$ is divided by $\\Phi\\mathrm{d}\\Phi$ , the remainder is $\\mathbb{54}.\\mathbb{5}$ Compute the sum of all possible two-digit values of $\\Phi\\mathrm{d}\\Phi$ . ", "page_idx": 39}, {"type": "text", "text": "Solution:   \nThis problem involves finding the divisors of a given number, as well as using the concept of remainders. $\\scriptstyle\\mathtt{\\backslash n\\backslash n0n e}$ way to approach this is to write the division as a quotient and a remainder, like this: $\\Phi109~=~{\\tt q d}~+~4\\Phi$ , where $\\mathfrak{s q\\Phi}$ is the quotient. $\\scriptstyle\\mathtt{\\backslash n\\backslash n T h e n}$ , I can rearrange this equation to get $\\Phi105\\ =\\ {\\mathfrak{q}}{\\mathfrak{d}}\\Phi$ , which means that $\\Phi\\mathrm{d}\\Phi$ is a divisor of $\\Phi105\\Phi$ .\\n\\nNow, I need to find all the two-digit divisors of $\\mathbb{\\Phi}105\\mathbb{\\Phi}.\\backslash{\\mathtt{n}}\\backslash{\\mathtt{n}}{\\mathtt{I}}$ can use prime factorization to help me do this. $\\setminus\\tt n V n I$ notice that $\\Phi105~=~3\\$ \\\\times 5 \\\\times $^{7\\Phi}$ , which are all prime numbers. $\\left.\\ln\\right\\backslash$ nTherefore, any divisor of $\\Phi105\\Phi$ must be a product of some combination of these three factors. $\\scriptstyle\\mathtt{\\backslash n\\backslash n T h e}$ possible products are $\\Phi1$ , 3, 5, 7, 15, 21, 35, $105\\Phi.\\backslash\\mathtt{n}\\backslash$ nHowever, not all of these are two-digit numbers. Only $\\mathbb{9}15$ , 21, $35\\Phi$ are. $\\left.\\ln\\right\\backslash$ nThese are the only possible values of $\\Phi\\mathrm{d}\\Phi$ that satisfy the given condition. $\\left.\\ln\\right\\backslash$ nTo find their sum, I just add them up: $\\mathbb{8}15~+~21~+~35~=~71\\mathbb{\\Phi}$ . ", "page_idx": 39}, {"type": "text", "text": "Final Answer: The final answer is $^{\\Phi71\\Phi}$ . I hope it is correct. ", "page_idx": 39}, {"type": "text", "text": "4-shot example for PRM800K Easy-to-Hard ICL: ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Problem:   \nHow many 4-letter words with at least one consonant can be constructed from the letters $\\Phi\\mathbb{A}\\Phi$ , $\\Phi{\\tt B}\\Phi$ , $\\Phi{\\mathrm{C}}\\Phi$ , $\\Phi{\\tt D}\\Phi$ , and $\\Phi\\mathbb{E}\\Phi?$ (Note that $\\Phi\\tt B\\Phi$ , $\\Phi{\\mathrm{C}}\\Phi$ , and $\\Phi\\tt D\\Phi$ are consonants, any word is valid, not just English language words, and letters may be used more than once.)   \nSolution:   \nTo count the number of 4-letter words with at least one consonant, I can use the complement principle and subtract the number of 4-letter words with no consonants from the total number of 4-letter words. $\\scriptstyle\\mathtt{\\backslash n\\backslash n T h e}$ total number of 4- letter words is simply $\\Phi5^{-}4\\Phi$ , since each letter has 5 choices and the order matters. $\\scriptstyle\\mathtt{\\backslash n\\backslash n T h e}$ number of 4-letter words with no consonants is just the number of ways to choose 4 letters from the 2 vowels $\\Phi\\mathbb{A}\\Phi$ and $\\Phi\\mathbb{E}\\Phi$ , which is $\\mathfrak{P}2^{\\sim}4\\mathfrak{P}.\\backslash\\mathfrak{n}\\backslash$ nTherefore, the number of 4-letter words with at least one consonant is $\\mathbb{5}5{\\sim}4$ - $2^{\\sim}4\\Phi$ . 625 - $16~=~609$   \nFinal Answer: The final answer is $\\Phi609\\Phi$ . I hope it is correct.   \nProblem:   \nCompute the integer $\\Phi\\mathbb{k}~>~2\\Phi$ for which\\n\\\\[\\\\log_{10} (k - 2)! + \\\\log_{10} (k - 1)! $+\\;\\;2\\;=\\;2\\;\\;\\backslash\\backslash20{\\bf g}_{-}\\{10\\}\\;\\;{\\bf k}!\\cdot\\backslash\\backslash]$   \nSolution:   \nI recognize that this equation involves logarithms of factorials, which are products of consecutive integers. $\\setminus\\setminus\\Upsilon\\subset$ also know that logarithms have some useful properties, such as $\\mathfrak{F}\\backslash\\backslash\\mathtt{l o g}_{-}\\{10\\}$ $a+\\sqrt{10g_{-}\\{10\\}}\\ b\\ =\\ \\sqrt{120g_{-}\\{10\\}}\\ \\ (\\mathrm{ab})\\oint$ and $\\Phi\\subset$ \\\\log_{10} d $=$ \\\\log_{10} $\\mathtt{d}^{\\sim}\\mathtt{c}\\mathfrak{b}$ .\\n\\nUsing these properties, I can simplify the equation as follows: \\\\[\\\\log_{ $.0\\}\\ \\ (\\mathtt{k}\\ -\\ 2)\\!\\ !:\\ +\\ \\backslash\\ \\!\\backslash\\mathtt{l}2\\mathtt{g}_{-}\\{10\\}\\ \\ (\\mathtt{k}\\ -\\ 1)\\!\\ !\\!:\\ +\\ 2\\ =\\ 2$ \\\\log_{10} k! \\\\implies \\\\log_{10} \\\\left[ (k - 2)! (k - 1)! 100 \\\\right] = \\\\ log_{10} $(\\mathbf{k}!)\\!\\uparrow\\!2.\\backslash\\!\\backslash\\!\\Vert\\setminus\\!\\mathbf{n}\\backslash\\!\\mathbf{n}\\mathbf{S}\\mathbf{i}$ nce the bases of the logarithms are equal, I can conclude that the arguments must also be equal, i.e., $\\backslash\\backslash\\left[({\\bf{k}}~-~2)!~\\left({\\bf{k}}~-~1\\right)!~100\\right.$ $=(\\mathbf{k}!)\\!\\setminus\\!\\!2.\\!\\setminus\\!\\!\\!\\bigcup\\setminus\\!\\!\\!\\setminus\\!\\!\\!\\bigcup\\!\\!\\!\\setminus\\!\\!\\!\\bigcup\\!\\!\\!\\setminus\\!\\!\\!\\!\\bigcup\\!\\!\\!\\!\\setminus\\!\\!\\!\\!\\bigcup\\!\\!\\!\\!\\setminus\\!\\!\\!\\!\\bigstar$ have a simpler equation to solve for $\\mathbb{\\Phi}\\mathtt{k}\\mathbb{\\Phi}.\\backslash\\mathtt{n}\\backslash\\mathtt{n}\\mathtt{I}$ notice that the left-hand side has a factor of $\\Phi(\\textbf{k}-\\textbf{2}):\\Phi$ , which is also a factor of $\\mathfrak{F}$ $(\\texttt{k-1}):\\Phi$ and $\\Phi(\\mathbf{k}!)\\!\\sim\\!2\\Phi.\\setminus\\!\\mathbf{n}^{\\prime}$ \\nTherefore, I can divide both sides by $\\Phi(\\textbf{k}-\\textbf{2}):\\Phi$ to get \\\\[(k - 1)! $100\\ =\\ \\mathbf{k}!$ $(\\mathtt{k}\\ -\\ 1):\\backslash\\backslash\\bigsqcup\\backslash\\mathtt{n}\\backslash\\mathtt{n}\\mathtt{S}:$ implifying further, I get \\\\ $[100~=~\\mathbf{k}$ !.\\\\]\\n\\nThis means that $\\mathbb{S}\\mathbf{k}\\Phi$ is the smallest integer such that its factorial is greater than or equal to $100\\,.\\,\\backslash\\,\\mathtt{n}\\backslash\\mathtt{n}\\mathtt{I}$ can try some values of $\\mathbb{S}\\mathbb{k}\\Phi$ to find the right one. $\\scriptstyle\\mathtt{\\setminus n V I f}$ $\\Phi\\mathbf{\\lambda}=\\mathbf{\\lambda}3\\Phi$ , then $\\oint\\bf{k}\\,!\\quad=\\;3\\,!\\quad=\\;\\overline{{{6}}}\\,\\mathrm{~<~}\\,100\\oint\\cdot\\backslash\\ n\\backslash\\ n I f\\,\\hat{\\mathrm{~\\bf~\\Phi~}}\\oint\\bf{k}\\;\\;=\\;4\\oint$ , then $\\Phi\\mathbf{{k}}!=$ $4!\\ =\\ 24<\\ 100\\Phi\\cdot\\backslash\\ n\\backslash\\ n\\mathbb{I}\\ f$ $\\Phi\\mathbf{\\lambda}=\\mathbf{\\lambda}\\mathbf{\\cdot}\\mathbf{\\nabla}5\\Phi$ , then $\\Phi\\bf{k}\\,!\\;\\;=\\;\\;5\\,!\\;\\;=\\;\\;120\\;>\\;100\\Phi\\,.$ \\n\\nTherefore, $\\Phi\\mathtt{k}{\\ }=\\ 5$ $\\mathfrak{F}$ is the smallest integer that satisfies the equation.   \nFinal Answer: The final answer is $\\Phi5\\Phi$ . I hope it is correct.   \nProblem:   \nWhat is the largest perfect square factor of 1512?   \nSolution:   \nI know that 1512 is divisible by 2, so I can try to factor out powers of 2 until I get an odd number.\\n\\nDividing by 2 once, I get 756.\\n\\nDividing by 2 again, I get 378.\\n\\nDividing by 2 one more time, I get 189.\\n\\nNow 189 is odd, so I can \u2019t divide by 2 anymore. $\\setminus\\setminus\\Upsilon\\subset$ notice that 189 is divisible by 3, so I can try to factor out powers of 3.\\n\\nDividing by 3 once, I get 63.\\n\\nDividing by 3 again, I get 21.\\n\\nDividing by 3 one more time, I get is prime, so I can\u2019t factor it anymore. $\\setminus\\tt n V n I$ have factored 1512 as $2^{\\sim}3^{\\ \\star}\\ 3^{\\sim}3^{\\ \\star}\\ 7\\,.\\,\\backslash\\mathtt{n}\\backslash\\mathtt{n}\\mathtt{T o}$ find the largest perfect square factor, I need to use the highest even powers ", "page_idx": 40}, {"type": "text", "text": "", "page_idx": 40}, {"type": "text", "text": "of each prime factor. $\\scriptstyle\\mathtt{\\backslash n\\backslash n T h e}$ highest even power of 2 that I have is $2{\\hat{\\mathbf{\\Gamma}}}2.\\backslash\\mathtt{n}\\backslash\\mathtt{n}\\mathtt{T h e}$ highest even power of 3 that I have is $_{3\\sim2}$ .\\n\\nThe highest even power of 7 that I have is $7^{\\sim}0$ , which is just $1.\\backslash\\mathtt{n}\\backslash\\mathtt{n}\\mathtt{S}\\circ$ the largest perfect square factor is $2{\\hat{-}}2*3{\\hat{-}}2*{\\hat{-}}0$ , which is 36. ", "page_idx": 40}, {"type": "text", "text": "Final Answer: The final answer is $\\mathbb{5368}$ . I hope it is correct. ", "page_idx": 40}, {"type": "text", "text": "Problem: If $83p+49=88$ and $84{\\bf p}\\!+\\!3{\\bf q}\\!=\\!13\\Phi$ , what is $\\mathfrak{s q\\Phi}$ equal to? ", "page_idx": 40}, {"type": "text", "text": "Solution: ", "page_idx": 40}, {"type": "table", "img_path": "qwgfh2fTtN/tmp/bc919c144a3be612c47f55906907a6d00a8220f3c75e1d12fbaad16bf1696b06.jpg", "table_caption": [], "table_footnote": [], "page_idx": 41}, {"type": "text", "text": "4-shot example for MetaMath Full ICL: ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Problem:   \nIn right triangle $\\Phi P{\\sf I R}\\Phi$ , we have $\\mathfrak{F}$ \\\\angle Q $=$ \\\\angle $\\mathbb{R}\\Phi$ and $\\Phi\\mathbb{P}\\mathbb{R}~=~6\\backslash\\backslash\\mathtt{s q r t}\\{2\\}\\Phi\\,.$ . What is the area of $\\mathfrak{F}$ \\\\triangle PQR\\$? ", "page_idx": 42}, {"type": "text", "text": "Solution: ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Since \\$\\\\angle $\\textsf{Q}=$ \\\\angle $\\mathbb{R}\\Phi$ , we have \\$\\\\triangle PQR\\$ is an isosceles right triangle.\\nTherefore, $\\Phi P\\mathbb{Q}~=~\\mathsf{P R}~=~6\\backslash\\backslash\\mathsf{s q r t}\\{2\\}\\Phi$ .\\nThe area of \\$\\\\triangle PQR\\$ is \\$\\\\frac{1}{2} \\\\cdot PQ \\\\cdot PR $=$ \\\\frac{1}{2} \\\\cdot 6\\\\sqrt{2} \\\\cdot 6\\\\ $\\mathtt{s q r t}\\{2\\}\\ =\\ \\backslash\\backslash\\mathtt{b o x e d}\\{36\\}\\Phi$ . ", "page_idx": 42}, {"type": "text", "text": "Final Answer: The final answer is $\\mathbb{5368}$ . I hope it is correct. ", "page_idx": 42}, {"type": "text", "text": "Problem: ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "The length of the longer side of rectangle $\\Phi\\tt R\\Phi$ is $\\Phi10\\Phi$ percent more than the length of a side of square $\\Phi^{\\mathrm{S}}\\cdot\\Phi$ The length of the shorter side of rectangle $\\Phi\\tt R\\Phi$ is $\\Phi10\\Phi$ percent less than the length of a side of square $\\mathbb{S}\\mathbf{S}\\cdot\\mathbb{S}$ What is the ratio of the area of rectangle $\\Phi\\mathbb{R}\\Phi$ to the area of square $\\Phi\\mathbf{S}\\mathbf{?}\\Phi$ Express your answer as a common fraction. ", "page_idx": 42}, {"type": "text", "text": "Solution: ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Let the side length of square $\\Phi{\\tt S}{\\tt S}$ be $\\Phi\\mathbf{x}\\Phi$ .\\nThen the longer side of rectangle $\\Phi\\mathbb{R}\\Phi$ is $\\Phi1.1\\mathbf{x}\\Phi$ and the shorter side is $\\Phi0.9\\mathbf{x}\\Phi$ .\\nThe area of square $\\Phi{\\tt S}\\Phi$ is $\\mathbb{S}\\mathbf{x}^{-}2\\mathbb{S}$ and the area of rectangle $\\Phi\\mathbb{R}\\Phi$ is $\\oint(1\\,.\\,1\\mathbf{x})\\,(0\\,.\\,9\\mathbf{x})\\ =\\ 0\\,.\\,99\\mathbf{x}{\\sim}2\\Phi$ .\\nTherefore, the ratio of the area of rectangle $\\Phi\\mathbb{R}\\Phi$ to the area of square $\\Phi{\\tt S}\\Phi$ is $\\mathfrak{F}\\backslash\\{\\mathtt{r a c}\\{0.99\\mathtt{x}^{-}2\\}\\{\\mathtt{x}^{-}2\\}\\ =$ \\\\boxed{\\\\frac{99}{100}}\\$. ", "page_idx": 42}, {"type": "text", "text": "Final Answer: The final answer is $\\mathfrak{F}\\backslash\\{\\mathtt{f r a c}\\{99\\}\\{100\\}\\mathfrak{F}$ . I hope it is correct. ", "page_idx": 42}, {"type": "text", "text": "Problem:   \nCompute the integer $\\Phi\\mathbb{k}~>~2\\Phi$ for which\\n\\\\[\\\\log_{10} (k - 2)! + \\\\log_{10} (k - 1)! $+\\;\\;2\\;=\\;2\\;\\;\\backslash\\backslash20{\\bf g}_{-}\\{10\\}\\;\\;{\\bf k}!\\cdot\\backslash\\backslash]$ ", "page_idx": 42}, {"type": "text", "text": "Solution: ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Using the properties of logarithms, we can rewrite the equation as\\n\\\\[\\\\log_{10} ((k-2)! \\\\cdot (k-1)!) $+~~2~=$ \\\\log_{10} (k!)^2.\\\\]\\nThen, using the fact that $\\mathfrak{F}$ \\\\log_{a} $b+\\setminus\\setminus\\bigcup0g_{-}\\{a\\}\\;\\mathrm{~c~}=\\;\\setminus\\bigcup0g_{-}\\{a\\}\\;\\;(\\mathfrak{b}c)\\Phi\\,,$ we have\\n\\\\[\\\\log_{10} ((k-2)! \\\\ cdot (k-1)! \\\\cdot 100) $=$ \\\\log_{10} (k!)^2.\\\\]\\nFinally, using the fact that $\\mathfrak{F}$ \\\\log_{a} b = \\\\log_{a} ${\\mathsf{c}}\\Phi$ if and only if $\\Phi\\mathfrak{b}~=~\\mathsf{c}\\Phi$ , we have\\n\\\\[(k-2)! \\\\cdot (k -1)! \\\\cdot $100~=~(\\bf{k}!)\\tilde{\\Omega}2.\\setminus\\backslash\\backslash\\nabla\\backslash\\nabla\\bf{n S i m p l i f y i}$ ng, we have\\n\\\\[(k-2)! \\\\cdot (k-1)! \\\\ cdot $100~=$ (k \\\\cdot (k-1) \\\\cdot (k-2))!.\\\\]\\nCancelling out the common ", "page_idx": 42}, {"type": "text", "text": "factors, we have\\n\\\\ $[100~=~\\mathbf{k}$ \\\\cdot (k-1) \\\\cdot (k-2).\\\\]\\nFactoring, we find that $\\Phi100~=~5\\$ \\\\cdot 5 \\\\cdot $4~=~2{\\hat{\\mathbf{\\Gamma}}}2$ \\\\cdot $5^{\\sim}2\\Phi$ .\\nSince $\\Phi\\mathbf{k}~>~2\\Phi$ , we see that $\\Phi\\mathbf{k}~=~\\setminus\\{\\mathbf{boxed}\\{5\\}\\Phi$ . ", "page_idx": 42}, {"type": "text", "text": "Final Answer: The final answer is $\\Phi5\\Phi$ . I hope it is correct. ", "page_idx": 42}, {"type": "text", "text": "Problem: If $83p+49=88$ and $84{\\bf p}\\!+\\!3{\\bf q}\\!=\\!13\\Phi$ , what is $\\mathfrak{s q\\Phi}$ equal to? ", "page_idx": 42}, {"type": "text", "text": "Solution: ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "We can solve this system of equations using the method of elimination.\\nMultiplying the first equation by 3 and the second equation by 4 gives us: $\\backslash\\mathtt{n}\\Phi\\mathtt{S p}+12\\mathtt{q}{=}24\\Phi\\backslash$ $\\mathtt{n}\\mathfrak{F}\\mathtt{1}6\\mathtt{p}{+}12\\mathtt{q}{=}52\\mathfrak{F}\\backslash\\mathtt{n}\\mathtt{W e}$ can subtract the first equation from the second equation to eliminate the q term: $:\\backslash\\mathbf{n\\Phi}\\mathbb{1}6\\mathbf{p}+\\mathbb{1}2\\mathbf{q}-(9\\mathbf{p}+\\mathbb{1}2\\mathbf{q})=52-24\\mathbb{1}\\leqslant\\backslash\\mathbf{n\\Phi}\\mathbb{F}7\\mathbf{p}{=}28\\mathbb{1}\\sin$ Dividing both sides by 7 gives us: $\\therefore\\Delta B P=48$ \\nSubstituting this value of p into either of the original equations gives us: $\\operatorname{\\setminus}\\!\\operatorname{a}\\!\\Phi\\!4\\!\\left(4\\right)\\!+\\!3\\mathbf{q}\\!=\\!13\\Phi\\!\\left\\{\\!\\mathrm{n}\\!\\Phi\\!1\\!6\\!+\\!3\\mathbf{q}\\!=\\!13\\Phi\\!\\left\\backslash\\!\\mathrm{n}\\!\\Phi\\!3\\mathbf{q}\\!=\\!-3\\Phi\\!\\left\\backslash\\!\\mathrm{n}\\!\\Phi\\!{\\sf q}\\!=\\!\\setminus\\!\\mathrm{boxed}\\!\\left\\{-1\\right\\}\\Phi\\!\\left(\\!\\mathrm{n}\\!\\Phi\\!1\\!6\\!+\\!3\\mathbf{q}\\!=\\!1\\!5\\Phi\\!\\left(\\!\\mathrm{n}\\!\\Phi\\!\\!1\\!6\\!+\\!3\\mathbf{q}\\!=\\!1\\!5\\Phi\\!\\left(\\!\\mathrm{n}\\!\\!+\\!3\\mathbf{q}\\!=\\!1\\!5\\Phi\\!\\left(\\!\\mathrm{n}\\!+\\!3\\mathbf{q}\\!=\\!1\\!5\\Phi\\!\\left(\\!\\mathrm{n}\\!+\\!3\\mathbf{q}\\!\\right)\\!\\right)\\!\\right)\\!\\right)\\!\\right\\}.$   \nFinal Answer: The final answer is $\\mathfrak{P}{-}1\\mathfrak{P}$ . I hope it is correct. ", "page_idx": 42}, {"type": "text", "text": "4-shot example for MetaMath Hard ICL: ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Problem:   \nIn right triangle $\\Phi P{\\sf I R}\\Phi$ , we have \\$\\\\angle Q $=$ \\\\angle $\\mathbb{R}\\Phi$ and $\\Phi\\mathbb{P}\\mathbb{R}~=~6\\backslash\\backslash\\mathtt{s q r t}\\{2\\}\\Phi\\,.$ . What is the area of $\\mathfrak{F}$ \\\\triangle PQR\\$? ", "page_idx": 43}, {"type": "text", "text": "Solution: ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Since \\$\\\\angle $\\textsf{Q}=$ \\\\angle $\\mathbb{R}\\Phi$ , we have \\$\\\\triangle PQR\\$ is an isosceles right triangle.\\nTherefore, $\\Phi P\\mathbb{Q}~=~\\mathsf{P R}~=~6\\backslash\\backslash\\mathsf{s q r t}\\{2\\}\\Phi$ .\\nThe area of \\$\\\\triangle PQR\\$ is \\$\\\\frac{1}{2} \\\\cdot PQ \\\\cdot PR $=$ \\\\frac{1}{2} \\\\cdot 6\\\\sqrt{2} \\\\cdot 6\\\\ $\\mathtt{s q r t}\\{2\\}\\ =\\ \\backslash\\backslash\\mathtt{b o x e d}\\{36\\}\\Phi$ . ", "page_idx": 43}, {"type": "text", "text": "Final Answer: The final answer is $\\mathbb{5368}$ . I hope it is correct. ", "page_idx": 43}, {"type": "text", "text": "Problem: ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "The length of the longer side of rectangle $\\Phi\\tt R\\Phi$ is $\\Phi10\\Phi$ percent more than the length of a side of square $\\Phi^{\\mathrm{S}}\\cdot\\Phi$ The length of the shorter side of rectangle $\\Phi\\tt R\\Phi$ is $\\Phi10\\Phi$ percent less than the length of a side of square $\\mathbb{S}\\mathbf{S}\\cdot\\mathbb{S}$ What is the ratio of the area of rectangle $\\Phi\\mathbb{R}\\Phi$ to the area of square $\\Phi\\mathbf{S}\\mathbf{?}\\Phi$ Express your answer as a common fraction. ", "page_idx": 43}, {"type": "text", "text": "Solution: ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Let the side length of square $\\Phi{\\tt S}{\\tt S}$ be $\\Phi\\mathbf{x}\\Phi$ .\\nThen the longer side of rectangle $\\Phi\\mathbb{R}\\Phi$ is $\\Phi1.1\\mathbf{x}\\Phi$ and the shorter side is $\\Phi0.9\\mathbf{x}\\Phi$ .\\nThe area of square $\\Phi{\\tt S}\\Phi$ is $\\mathbb{S}\\mathbf{x}^{-}2\\mathbb{S}$ and the area of rectangle $\\Phi\\mathbb{R}\\Phi$ is $\\oint(1\\,.\\,1\\mathbf{x})\\,(0\\,.\\,9\\mathbf{x})\\ =\\ 0\\,.\\,99\\mathbf{x}{\\sim}2\\Phi$ .\\nTherefore, the ratio of the area of rectangle $\\Phi\\mathbb{R}\\Phi$ to the area of square $\\Phi{\\tt S}\\Phi$ is $\\mathfrak{F}\\backslash\\{\\mathtt{r a c}\\{0.99\\mathtt{x}^{-}2\\}\\{\\mathtt{x}^{-}2\\}\\ =$ \\\\boxed{\\\\frac{99}{100}}\\$. ", "page_idx": 43}, {"type": "text", "text": "Final Answer: The final answer is $\\mathfrak{F}\\backslash\\{\\mathtt{f r a c}\\{99\\}\\{100\\}\\mathfrak{F}$ . I hope it is correct. ", "page_idx": 43}, {"type": "text", "text": "Problem: Suppose that $\\mathfrak{s y}^{-}3\\mathfrak{s}$ varies inversely with $\\mathfrak{F}\\backslash\\backslash\\mathtt{s q r t}\\left[3\\right]\\backslash\\mathtt{z}\\right\\}\\mathfrak{F}$ . If $\\mathfrak{g}_{\\mathtt{y}=2\\mathfrak{b}}$ when $\\mathbb{9}\\mathbf{z}{=}1\\mathbb{9}$ , find the value of $\\Phi_{}{\\bf{z}}\\Phi$ when $\\Phi_{\\mathrm{{J}}}{=}4\\Phi$ . Express your answer in simplest fractional form. ", "page_idx": 43}, {"type": "text", "text": "Solution: ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Since $\\mathfrak{F}\\mathfrak{y}^{-3\\mathfrak{b}}$ varies inversely with \\$\\\\sqrt[3] $\\{\\mathbf{z}\\}\\mathfrak{E}$ , we can write the equation as $\\mathfrak{s y}$ ^3\\\\sqrt[3] $\\{{z}\\}{=}\\mathbf{k}\\Phi$ , where $\\mathbb{S}\\mathbf{k}\\Phi$ is a constant.\\nWe are given that $\\mathfrak{g}_{\\mathtt{y}=2\\mathfrak{s}}$ when $\\Phi z{=}1\\Phi$ , so we can substitute these values into the equation to solve for $\\Phi\\mathbb{k}\\Phi\\colon\\backslash\\mathtt{n}\\Phi2^{\\sim}3\\backslash\\backslash$ sqrt[3] $\\{1\\}{=}\\mathtt{k}\\Phi\\backslash\\mathtt{n}\\Phi8\\backslash$ \\cdot $1{=}1\\!\\!\\times\\!\\!\\Phi\\backslash\\!\\!\\mathtt{n}\\!\\!\\Phi\\mathtt{k}{=}8\\!\\!\\Phi$ \\nNow we can use this value of $\\mathbb{S}\\mathbf{k}\\Phi$ to find $\\Phi_{}{\\bf{z}}\\Phi$ when $\\Phi y{=}4\\Phi:\\backslash\\mathtt{n}\\Phi4{\\sim}3\\backslash$ \\sqrt[3] $\\{z\\}{=}8\\Phi\\backslash\\mathtt{n}\\Phi64^{\\backslash}$ \\\\sqrt[3] $\\{z\\}{=}8\\Phi\\backslash\\mathtt{n}\\Phi^{\\prime}$ \\\\sqrt[3] $\\{{z}\\}{=}$ \\\\frac $\\{8\\}\\{64\\}=\\backslash\\backslash\\mathtt{f r a c}\\{1\\}\\{8\\}\\Phi\\backslash\\mathtt{n}\\Phi z=\\backslash\\backslash\\mathtt{l e m s}$ ft(\\\\frac{1}{8}\\\\right) $-3=$ \\\\frac{1}{512}\\$\\nSo when $\\Phi_{\\mathrm{{J}}}{=}4\\Phi$ , $\\Phi\\mathbf{z}{=}^{\\prime}$ \\\\boxed{\\\\frac{1}{512}}\\$. ", "page_idx": 43}, {"type": "text", "text": "Problem: ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Let $\\Phi\\mathrm{d}\\Phi$ be a positive number such that when $\\Phi109\\Phi$ is divided by $\\Phi\\mathrm{d}\\Phi$ , the remainder is $\\mathbb{54}.\\mathbb{5}$ Compute the sum of all possible two-digit values of $\\Phi\\mathrm{d}\\Phi$ . ", "page_idx": 43}, {"type": "text", "text": "Solution: ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "If $\\Phi109\\Phi$ is divided by $\\Phi\\mathrm{d}\\Phi$ and the remainder is $\\Phi4\\Phi$ , then $\\Phi\\mathrm{d}\\Phi$ divides $\\mathbb{9}109{-}4{=}105\\Phi\\,.\\,\\backslash$ nThe prime factorization of $\\mathbb{5105\\Phi}$ is \\$3\\\\cdot5\\\\cdot $^{7\\Phi}$ .\\nSince $\\Phi\\mathrm{d}\\Phi$ must be a factor of $\\Phi105\\Phi$ , the possible values of $\\Phi\\mathrm{d}\\Phi$ are $\\Phi1$ , 3, 5, 7, 15, 21, 35, $105\\Phi.\\setminus$ nOut of these, only the two-digit values are $\\Phi15$ , 21, $35\\Phi$ .\\nThe sum of these values is $\\mathfrak{F15+21+35=\\backslash\\backslash b o x e d\\{71\\}\\mathfrak{F}}$ . ", "page_idx": 43}, {"type": "text", "text": "4-shot example for MetaMath Easy-to-Hard ICL: ", "text_level": 1, "page_idx": 44}, {"type": "table", "img_path": "qwgfh2fTtN/tmp/f7685b1ae9782ec885f715f50fadc11e426b1d14dfca08dc666bd908b5f3206c.jpg", "table_caption": [], "table_footnote": [], "page_idx": 44}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 45}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 45}, {"type": "text", "text": "Justification: They are supported by the experiments. ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 45}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 45}, {"type": "text", "text": "Answer:[Yes] ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Justification: We discuss this in the section of \u201cConclusion & Limitations\u201d ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 45}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 45}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 45}, {"type": "text", "text": "Justification: We have no theoretical results. ", "page_idx": 46}, {"type": "text", "text": "Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 46}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 46}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 46}, {"type": "text", "text": "Justification: We have open-sourced the code and released the model. ", "page_idx": 46}, {"type": "text", "text": "Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 46}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 46}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 47}, {"type": "text", "text": "Justification: See our supplementary materials. Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 47}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 47}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 47}, {"type": "text", "text": "Justification: We do that in our experiments section. Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 47}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 47}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Justification: We have error bars in our curves. ", "page_idx": 47}, {"type": "text", "text": "Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 47}, {"type": "text", "text": "", "page_idx": 48}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 48}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 48}, {"type": "text", "text": "Justification: We open-sourced the code. ", "page_idx": 48}, {"type": "text", "text": "Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 48}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 48}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 48}, {"type": "text", "text": "Justification: Yes. ", "page_idx": 48}, {"type": "text", "text": "Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 48}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 48}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 48}, {"type": "text", "text": "Justification: Yes, We discuss it in the appendix. ", "page_idx": 48}, {"type": "text", "text": "Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to ", "page_idx": 48}, {"type": "text", "text": "generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. ", "page_idx": 49}, {"type": "text", "text": "\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 49}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 49}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 49}, {"type": "text", "text": "Justification: We don\u2019t pre-train language models. ", "page_idx": 49}, {"type": "text", "text": "Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 49}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 49}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 49}, {"type": "text", "text": "Justification: Yes, we do the proper citations. ", "page_idx": 49}, {"type": "text", "text": "Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 49}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 49}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 50}, {"type": "text", "text": "Justification: Yes, we open-sourced the code. ", "page_idx": 50}, {"type": "text", "text": "Guidelines: ", "page_idx": 50}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 50}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 50}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 50}, {"type": "text", "text": "Justification: No crowdsourcing is used. ", "page_idx": 50}, {"type": "text", "text": "Guidelines: ", "page_idx": 50}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 50}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 50}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 50}, {"type": "text", "text": "Justification: No human subjects are involved. ", "page_idx": 50}, {"type": "text", "text": "Guidelines: ", "page_idx": 50}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 50}]