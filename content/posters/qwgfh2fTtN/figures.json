[{"figure_path": "qwgfh2fTtN/figures/figures_1_1.jpg", "caption": "Figure 1: Illustration of different alignment scenarios: traditional alignment relies on human demonstrations or judgements [50]; scalable alignment [9] assumes that humans cannot reliably supervise smarter-than-human models; weak-to-strong generalization [11] focuses on using weak models with unreliable labels to supervise strong models; Our proposed easier-to-general generalization focuses on the transfer of rewarding policies from weak models to harder tasks.", "description": "This figure illustrates different AI alignment approaches. Traditional alignment uses human supervision for both easy and hard tasks. Scalable alignment acknowledges the limitations of human supervision when dealing with superhuman models. Weak-to-strong generalization uses weaker models to supervise stronger ones.  This paper proposes easy-to-hard generalization, which leverages human supervision on easy tasks to improve performance on harder tasks.", "section": "1 Introduction"}, {"figure_path": "qwgfh2fTtN/figures/figures_1_2.jpg", "caption": "Figure 1: Illustration of different alignment scenarios: traditional alignment relies on human demonstrations or judgements [50]; scalable alignment [9] assumes that humans cannot reliably supervise smarter-than-human models; weak-to-strong generalization [11] focuses on using weak models with unreliable labels to supervise strong models; Our proposed easier-to-general generalization focuses on the transfer of rewarding policies from weak models to harder tasks.", "description": "This figure illustrates four different alignment scenarios. Traditional alignment uses human demonstrations or judgments for training models on hard tasks.  Scalable alignment, or superalignment, acknowledges the limitations of human supervision for superhuman models, which means human cannot reliably supervise superhuman models on hardest tasks. Weak-to-strong generalization utilizes weak models with unreliable labels to supervise strong models, and this paper proposes easier-to-hard generalization which focuses on transferring rewarding policies from easy tasks to harder tasks.", "section": "1 Introduction"}, {"figure_path": "qwgfh2fTtN/figures/figures_1_3.jpg", "caption": "Figure 1: Illustration of different alignment scenarios: traditional alignment relies on human demonstrations or judgements [50]; scalable alignment [9] assumes that humans cannot reliably supervise smarter-than-human models; weak-to-strong generalization [11] focuses on using weak models with unreliable labels to supervise strong models; Our proposed easier-to-general generalization focuses on the transfer of rewarding policies from weak models to harder tasks.", "description": "This figure illustrates four different AI alignment scenarios. Traditional alignment uses human demonstrations or judgements to supervise models, limiting capabilities to human levels. Scalable alignment acknowledges that humans can't supervise superhuman models. Weak-to-strong generalization utilizes weaker models with unreliable labels to supervise stronger ones. The paper introduces easy-to-hard generalization, transferring rewarding policies from easier tasks to improve performance on harder tasks, overcoming the limitations of other methods.", "section": "1 Introduction"}, {"figure_path": "qwgfh2fTtN/figures/figures_1_4.jpg", "caption": "Figure 1: Illustration of different alignment scenarios: traditional alignment relies on human demonstrations or judgements [50]; scalable alignment [9] assumes that humans cannot reliably supervise smarter-than-human models; weak-to-strong generalization [11] focuses on using weak models with unreliable labels to supervise strong models; Our proposed easier-to-general generalization focuses on the transfer of rewarding policies from weak models to harder tasks.", "description": "This figure illustrates four different AI alignment approaches.  Traditional alignment uses human supervision for both easy and hard tasks.  Scalable alignment acknowledges the limitations of human supervision for superhuman models. Weak-to-strong generalization explores the use of weaker models for supervision, accepting lower reliability. The authors' proposed Easy-to-Hard generalization focuses on transferring reward policies learned from easier tasks to harder tasks, minimizing reliance on human supervision for challenging problems.", "section": "1 Introduction"}, {"figure_path": "qwgfh2fTtN/figures/figures_2_1.jpg", "caption": "Figure 2: We first train the evaluator with process supervision or outcome supervision (which simulates the process supervision) to enable easy-to-hard evaluation, and then use it to facilitate easy-to-hard generation via re-ranking or RL.", "description": "This figure illustrates the two-stage training process proposed in the paper.  First, an evaluator model is trained using process supervision on easier tasks. This trained evaluator is then used to assess the output of a generator model on harder tasks. The generator model can be optimized through techniques like re-ranking or reinforcement learning based on the evaluator's feedback, enabling generalization from easy to hard tasks without relying on human supervision for the harder tasks.", "section": "3 Methodology"}, {"figure_path": "qwgfh2fTtN/figures/figures_3_1.jpg", "caption": "Figure 3: The overview diagram of our methods: the different components of modeling and training and how they are interconnected.", "description": "This figure shows a flowchart of the training process for the proposed easy-to-hard generalization method.  It details the different components used:  a base language model, easy-to-hard supervised fine-tuning (SFT) and in-context learning (ICL) models (generators), and an easy-to-hard reward model (evaluator). The generators produce solution samples, which are then evaluated by majority voting, reranking methods (weighted voting, best-of-N), or reinforcement learning (REST-EM, DPO, PPO).  The training data consists of easy and hard mathematical problems. Easy problems and their solutions (including process labels for the reward model) are used to train the models. The performance of the trained models is then evaluated on hard problems.", "section": "3 Methodology"}, {"figure_path": "qwgfh2fTtN/figures/figures_6_1.jpg", "caption": "Figure 2: We first train the evaluator with process supervision or outcome supervision (which simulates the process supervision) to enable easy-to-hard evaluation, and then use it to facilitate easy-to-hard generation via re-ranking or RL.", "description": "This figure illustrates the two-stage process of the proposed easy-to-hard generalization approach.  First, an evaluator model (reward model) is trained using process supervision (or outcome supervision as a proxy) on easier tasks. This trained evaluator is then used to assess candidate solutions generated for harder tasks, guiding the generation process through either re-ranking or reinforcement learning (RL). This shows how leveraging evaluation on easy tasks can enable generation on hard tasks, thereby scaling alignment beyond human supervision.", "section": "3 Methodology"}, {"figure_path": "qwgfh2fTtN/figures/figures_7_1.jpg", "caption": "Figure 2: We first train the evaluator with process supervision or outcome supervision (which simulates the process supervision) to enable easy-to-hard evaluation, and then use it to facilitate easy-to-hard generation via re-ranking or RL.", "description": "This figure illustrates the two-stage process of the proposed easy-to-hard generalization approach.  First, an evaluator (reward model) is trained using process supervision (or outcome supervision as a proxy) on easy tasks. This trained evaluator is then used to facilitate generation on harder tasks by either re-ranking candidate solutions generated by a separate generator model or through reinforcement learning (RL) where the evaluator provides feedback to guide the generator's improvement on harder problems.", "section": "3 Methodology"}, {"figure_path": "qwgfh2fTtN/figures/figures_20_1.jpg", "caption": "Figure 4: Easy-to-hard generalization of 7b (upper) and 34b (lower) evaluators. Both SFTs and RMs are trained on the easy data. We found that PRMs trained on easy tasks can significantly improve the re-ranking (i.e., weighted voting) performance on hard tasks. The shaded margin of the curve plot in this paper represents the performance variance.", "description": "This figure shows the results of experiments evaluating the easy-to-hard generalization capabilities of 7B and 34B evaluators.  Both the Supervised Fine-Tuning (SFT) models and Reward Models (RMs) were trained only on easy problems. The figure demonstrates that Process Reward Models (PRMs), when used for re-ranking (weighted voting), significantly improve performance on hard tasks compared to using the SFT models alone. The shaded regions represent the performance variance across multiple runs.", "section": "4 Main Results"}, {"figure_path": "qwgfh2fTtN/figures/figures_21_1.jpg", "caption": "Figure 4: Easy-to-hard generalization of 7b (upper) and 34b (lower) evaluators. Both SFTs and RMs are trained on the easy data. We found that PRMs trained on easy tasks can significantly improve the re-ranking (i.e., weighted voting) performance on hard tasks. The shaded margin of the curve plot in this paper represents the performance variance.", "description": "This figure shows the performance of 7B and 34B evaluators on easy and hard tasks using different re-ranking methods (Majority Voting, Weighted Voting with Reward Model, and Best-of-N with Reward Model).  The results demonstrate that reward models (RMs), particularly those trained on easier problems (PRM), significantly enhance the performance of re-ranking methods, especially on harder tasks.", "section": "4.2 Easy-to-Hard Generalization of Evaluators"}, {"figure_path": "qwgfh2fTtN/figures/figures_23_1.jpg", "caption": "Figure 2: We first train the evaluator with process supervision or outcome supervision (which simulates the process supervision) to enable easy-to-hard evaluation, and then use it to facilitate easy-to-hard generation via re-ranking or RL.", "description": "This figure illustrates the overall process of the proposed easy-to-hard generalization approach. It consists of two main stages: training an evaluator (reward model) and using it to optimize a generator (policy model). The evaluator is trained on easy problems with either process supervision (step-by-step guidance) or outcome supervision (only final answer is supervised), to allow easy-to-hard generalization. Then, this trained evaluator is used to assess the performance of the generator in hard tasks, to improve its performance through re-ranking (selecting best solutions) or reinforcement learning (RL).", "section": "3 Methodology"}, {"figure_path": "qwgfh2fTtN/figures/figures_24_1.jpg", "caption": "Figure 2: We first train the evaluator with process supervision or outcome supervision (which simulates the process supervision) to enable easy-to-hard evaluation, and then use it to facilitate easy-to-hard generation via re-ranking or RL.", "description": "This figure illustrates the process of easy-to-hard generalization.  It shows that an evaluator model, trained on process supervision from easier tasks, can be used to effectively evaluate candidate solutions for harder tasks. This evaluation is then used to improve the generator model's performance on these harder tasks through techniques like re-ranking or reinforcement learning. The diagram visually represents the two stages: training the evaluator and then using it to improve the generator.", "section": "3 Methodology"}, {"figure_path": "qwgfh2fTtN/figures/figures_25_1.jpg", "caption": "Figure 2: We first train the evaluator with process supervision or outcome supervision (which simulates the process supervision) to enable easy-to-hard evaluation, and then use it to facilitate easy-to-hard generation via re-ranking or RL.", "description": "This figure illustrates the process of easy-to-hard generalization.  It starts by training evaluators (reward models) on easy tasks using either process supervision or outcome supervision (simulated process supervision). These trained evaluators are then used to score candidate solutions for harder tasks, thus facilitating easy-to-hard generalization in generators (policy models).  This generalization is achieved through either re-ranking of generated solutions based on the evaluator scores or via reinforcement learning (RL), where the evaluator's scores act as rewards to guide the learning process of the generator.", "section": "3 Methodology"}, {"figure_path": "qwgfh2fTtN/figures/figures_26_1.jpg", "caption": "Figure 2: We first train the evaluator with process supervision or outcome supervision (which simulates the process supervision) to enable easy-to-hard evaluation, and then use it to facilitate easy-to-hard generation via re-ranking or RL.", "description": "This figure illustrates the pipeline of the proposed easy-to-hard generalization approach.  It starts by training an evaluator model (reward model) on easy tasks using process supervision or outcome supervision. This trained evaluator is then used to assess candidate solutions generated for harder tasks. This assessment is used to improve the solution generation process through re-ranking or reinforcement learning (RL), enabling the model to generalize from easy to hard tasks without direct human supervision on the hard tasks.", "section": "3 Methodology"}, {"figure_path": "qwgfh2fTtN/figures/figures_26_2.jpg", "caption": "Figure 4: Easy-to-hard generalization of 7b (upper) and 34b (lower) evaluators. Both SFTs and RMs are trained on the easy data. We found that PRMs trained on easy tasks can significantly improve the re-ranking (i.e., weighted voting) performance on hard tasks. The shaded margin of the curve plot in this paper represents the performance variance.", "description": "This figure displays the performance of 7B and 34B evaluators (reward models) on easy and hard tasks from the MATH dataset.  Both the SFT (supervised fine-tuning) models and Reward Models (RMs) are initially trained only on the easier tasks (levels 1-3).  The figure demonstrates that using process-supervised reward models (PRMs) trained on the easier tasks leads to significantly improved performance in re-ranking (specifically weighted voting) on harder tasks (levels 4-5). The shaded regions around the lines show the performance variance.", "section": "4 Main Results"}, {"figure_path": "qwgfh2fTtN/figures/figures_27_1.jpg", "caption": "Figure 2: We first train the evaluator with process supervision or outcome supervision (which simulates the process supervision) to enable easy-to-hard evaluation, and then use it to facilitate easy-to-hard generation via re-ranking or RL.", "description": "This figure illustrates the two-stage training process of the proposed method. First, evaluators are trained using process supervision or outcome supervision (simulating process supervision) on easy problems to facilitate easy-to-hard evaluation. Subsequently, these trained evaluators are used to improve the generation process on hard problems. This improvement is done via re-ranking techniques or Reinforcement Learning (RL).", "section": "3 Methodology"}, {"figure_path": "qwgfh2fTtN/figures/figures_27_2.jpg", "caption": "Figure 2: We first train the evaluator with process supervision or outcome supervision (which simulates the process supervision) to enable easy-to-hard evaluation, and then use it to facilitate easy-to-hard generation via re-ranking or RL.", "description": "This figure illustrates the proposed easy-to-hard generalization approach.  It shows a two-stage process: first, training an evaluator (reward model) using process or outcome supervision on easy tasks; and second, using this trained evaluator to improve the performance of generators (policy models) on hard tasks via re-ranking or reinforcement learning (RL). The figure visually depicts the flow of information and the distinct roles of evaluators and generators in achieving scalable alignment beyond human supervision.", "section": "3 Methodology"}, {"figure_path": "qwgfh2fTtN/figures/figures_28_1.jpg", "caption": "Figure 2: We first train the evaluator with process supervision or outcome supervision (which simulates the process supervision) to enable easy-to-hard evaluation, and then use it to facilitate easy-to-hard generation via re-ranking or RL.", "description": "This figure illustrates the easy-to-hard generalization framework.  It shows how evaluators, trained on easier tasks with process supervision (or outcome supervision simulating this), are used to assess solutions to harder tasks. This assessment then facilitates the generation of solutions to harder tasks via techniques like re-ranking (selecting the best solutions based on evaluator scores) or reinforcement learning (RL, using evaluator scores as rewards to guide model training). The figure highlights the successful transfer of evaluation capabilities from easy to hard tasks, thereby enabling scalable alignment beyond the limits of human supervision.", "section": "3 Methodology"}, {"figure_path": "qwgfh2fTtN/figures/figures_30_1.jpg", "caption": "Figure 4: Easy-to-hard generalization of 7b (upper) and 34b (lower) evaluators. Both SFTs and RMs are trained on the easy data. We found that PRMs trained on easy tasks can significantly improve the re-ranking (i.e., weighted voting) performance on hard tasks. The shaded margin of the curve plot in this paper represents the performance variance.", "description": "This figure showcases the results of experiments evaluating the easy-to-hard generalization capabilities of 7B and 34B evaluators (reward models).  Both the supervised fine-tuning (SFT) models and reward models (RMs) were trained only on easy-level problems. The key finding is that process reward models (PRMs) trained on easy problems significantly enhance the performance of re-ranking (weighted voting) methods on difficult problems.  The shaded areas represent the variance in performance across multiple runs.", "section": "4 Main Results"}, {"figure_path": "qwgfh2fTtN/figures/figures_31_1.jpg", "caption": "Figure 2: We first train the evaluator with process supervision or outcome supervision (which simulates the process supervision) to enable easy-to-hard evaluation, and then use it to facilitate easy-to-hard generation via re-ranking or RL.", "description": "This figure illustrates the methodology of the proposed approach. It shows that the evaluators are trained using process supervision or outcome supervision on easy tasks to enable easy-to-hard evaluation. Then, the trained evaluators are used to improve the generators by facilitating easy-to-hard generation via re-ranking or reinforcement learning (RL).", "section": "3 Methodology"}, {"figure_path": "qwgfh2fTtN/figures/figures_32_1.jpg", "caption": "Figure 4: Easy-to-hard generalization of 7b (upper) and 34b (lower) evaluators. Both SFTs and RMS are trained on the easy data. We found that PRMs trained on easy tasks can significantly improve the re-ranking (i.e., weighted voting) performance on hard tasks. The shaded margin of the curve plot in this paper represents the performance variance.", "description": "This figure shows the results of experiments evaluating the easy-to-hard generalization performance of 7B and 34B evaluators.  The evaluators (Reward Models) were trained only on easy tasks (levels 1-3), then tested on easy, hard (levels 4-5), and all tasks. The results demonstrate that process-supervised reward models (PRMs) trained on easy tasks significantly improve the accuracy of re-ranking (weighted voting and best-of-N) methods on harder tasks.  The shaded areas represent performance variance.", "section": "4 Main Results"}, {"figure_path": "qwgfh2fTtN/figures/figures_33_1.jpg", "caption": "Figure 2: We first train the evaluator with process supervision or outcome supervision (which simulates the process supervision) to enable easy-to-hard evaluation, and then use it to facilitate easy-to-hard generation via re-ranking or RL.", "description": "This figure illustrates the two-stage training process of the proposed easy-to-hard generalization method. In the first stage, an evaluator model (reward model) is trained on easier tasks using process or outcome supervision. In the second stage, this trained evaluator is then used to improve the performance of a generator model (policy model) on harder tasks through re-ranking or reinforcement learning. This demonstrates the effectiveness of using easy-to-hard evaluation to facilitate easy-to-hard generalization in the generator.", "section": "3 Methodology"}, {"figure_path": "qwgfh2fTtN/figures/figures_34_1.jpg", "caption": "Figure 20: Easy-to-hard generalization for different type's data. Both SFTs and OPRMs are trained on the easy data. Each row compares the performance of different OPRMs' reranking strategies across different types' data.", "description": "This figure shows the performance of different types of reward models (process reward models, outcome reward models, and outcome & process reward models) on various types of math problems (algebra, counting & probability, geometry, intermediate algebra, number theory, prealgebra, and precalculus).  The results demonstrate the ability of the models to generalize from easier tasks to harder tasks, especially for certain problem types. It also compares the effectiveness of different re-ranking strategies (majority voting, weighted voting with reward model, and best-of-N with reward model).", "section": "4.2 Easy-to-Hard Generalization of Evaluators"}, {"figure_path": "qwgfh2fTtN/figures/figures_35_1.jpg", "caption": "Figure 4: Easy-to-hard generalization of 7b (upper) and 34b (lower) evaluators. Both SFTs and RMs are trained on the easy data. We found that PRMs trained on easy tasks can significantly improve the re-ranking (i.e., weighted voting) performance on hard tasks. The shaded margin of the curve plot in this paper represents the performance variance.", "description": "This figure shows the results of experiments evaluating the easy-to-hard generalization ability of 7B and 34B evaluators (reward models).  Both the SFTs and Reward Models (RMs) were trained only on easy tasks.  The plots show the accuracy on easy (level 1-3), hard (level 4-5), and all (level 1-5) tasks using three different re-ranking strategies: Majority Voting, Weighted Voting with Reward Models, and Best-of-N with Reward Models.  The shaded area represents the performance variance.", "section": "4 Main Results"}, {"figure_path": "qwgfh2fTtN/figures/figures_36_1.jpg", "caption": "Figure 2: We first train the evaluator with process supervision or outcome supervision (which simulates the process supervision) to enable easy-to-hard evaluation, and then use it to facilitate easy-to-hard generation via re-ranking or RL.", "description": "This figure illustrates the two-stage training process of the proposed method. First, an evaluator model is trained using process supervision or outcome supervision on easy tasks. This evaluator is then used to facilitate easy-to-hard generation in two ways: re-ranking or reinforcement learning (RL).  The figure visually represents the flow of data and model training.", "section": "3 Methodology"}, {"figure_path": "qwgfh2fTtN/figures/figures_36_2.jpg", "caption": "Figure 2: We first train the evaluator with process supervision or outcome supervision (which simulates the process supervision) to enable easy-to-hard evaluation, and then use it to facilitate easy-to-hard generation via re-ranking or RL.", "description": "This figure illustrates the two-stage process of the proposed easy-to-hard generalization approach.  First, an evaluator model (reward model) is trained using process supervision or outcome supervision on easier tasks. This trained evaluator is then used to score candidate solutions generated by a policy model (generator) on harder tasks, guiding the generator towards better performance through either reranking or reinforcement learning. The figure visually represents the training and evaluation stages for both the evaluator and the generator models.", "section": "3 Methodology"}, {"figure_path": "qwgfh2fTtN/figures/figures_37_1.jpg", "caption": "Figure 4: Easy-to-hard generalization of 7b (upper) and 34b (lower) evaluators. Both SFTs and RMS are trained on the easy data. We found that PRMs trained on easy tasks can significantly improve the re-ranking (i.e., weighted voting) performance on hard tasks. The shaded margin of the curve plot in this paper represents the performance variance.", "description": "This figure shows the performance of 7B and 34B evaluators on easy and hard tasks using different re-ranking methods.  The evaluators (reward models) are trained only on easy tasks (levels 1-3). The results demonstrate that PRMs (Process-supervised Reward Models) trained on easy tasks can significantly improve the performance of re-ranking methods, such as weighted voting and best-of-N on hard tasks (levels 4-5). The shaded areas indicate the performance variance.", "section": "4 Main Results"}]