{"references": [{"fullname_first_author": "Jared Kaplan", "paper_title": "Scaling laws for neural language models", "publication_date": "2020-00-00", "reason": "This paper introduced scaling laws for LLMs, providing a foundational understanding of how model size and data affect performance, which is directly relevant to the current study's findings on scaling laws for many-shot jailbreaking."}, {"fullname_first_author": "Ethan Perez", "paper_title": "Training language models to follow instructions with human feedback", "publication_date": "2022-00-00", "reason": "This paper details the RLHF methodology used to align LLMs, a technique central to safety and mitigation strategies, and is therefore crucial context for the paper's evaluation of mitigation against many-shot jailbreaking."}, {"fullname_first_author": "Anish A. Vashistha", "paper_title": "Tricking LLMs into disobedience: Understanding, analyzing and preventing jailbreaks", "publication_date": "2023-00-00", "reason": "This paper directly investigates few-shot jailbreaking, the foundation upon which the current study extends the jailbreaking methodology to the many-shot regime."}, {"fullname_first_author": "A. Wei", "paper_title": "Jailbroken: How does LLM safety training fail?", "publication_date": "2023-00-00", "reason": "This paper investigates the failure modes of current alignment techniques in the context of safety, directly informing the current paper's analysis of mitigation strategies and their limitations."}, {"fullname_first_author": "Zou", "paper_title": "Universal and transferable adversarial attacks on aligned language models", "publication_date": "2023-00-00", "reason": "This paper introduces a gradient-based white-box attack method, relevant as the current paper leverages a similar approach for generating attack strings and evaluating attack success in the context of MSJ."}]}