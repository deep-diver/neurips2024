[{"figure_path": "JpqEzPTuv6/tables/tables_2_1.jpg", "caption": "Table 1: Comparison of techniques used in eight prominent PLL methods. \u221a/\u00d7 indicates whether a technique is used, and an underline denotes the key components of the respective methods.", "description": "This table compares eight prominent Partial Label Learning (PLL) methods, showing which techniques they utilize.  The techniques include mini-batch purification, Mixup, data augmentation, exponential moving average, data augmentation match (DA), and dual-model match (DM).  The table also lists the main assumption each method makes.", "section": "2 Preliminaries"}, {"figure_path": "JpqEzPTuv6/tables/tables_5_1.jpg", "caption": "Table 1: Comparison of techniques used in eight prominent PLL methods. \u221a/\u00d7 indicates whether a technique is used, and an underline denotes the key components of the respective methods.", "description": "This table compares eight prominent partial-label learning (PLL) methods, highlighting the techniques used in each.  It shows whether each method utilizes mini-batch purification, Mixup, data augmentation, exponential moving average, Match (DA or DM), and the main assumption behind the method's design. The table helps to understand the diversity of approaches and the key components driving their success.", "section": "2 Preliminaries"}, {"figure_path": "JpqEzPTuv6/tables/tables_9_1.jpg", "caption": "Table 3: Classification accuracy (%) improvement of PLL methods with StreamPurify.", "description": "This table shows the improvement in classification accuracy achieved by adding the StreamPurify method to various Partial Label Learning (PLL) algorithms.  The results are presented for different datasets (FMNIST, CIFAR-10, CIFAR-100, mini-ImageNet) and varying noise levels (represented by the flipping probability).  The improvement is calculated as the difference in accuracy between the original PLL method and the same method augmented with StreamPurify.", "section": "3.5 Probing the Implementation of Mini-Batch PL Purification"}, {"figure_path": "JpqEzPTuv6/tables/tables_14_1.jpg", "caption": "Table 1: Comparison of techniques used in eight prominent PLL methods. \u221a/\u00d7 indicates whether a technique is used, and an underline denotes the key components of the respective methods.", "description": "This table compares eight prominent Partial Label Learning (PLL) methods, highlighting the techniques used in each.  It shows whether each method utilizes mini-batch purification, mixup, data augmentation, exponential moving average, match, and other techniques. It also briefly describes the main assumption behind each method. This helps in understanding the different approaches to PLL and their key components.", "section": "2 Preliminaries"}, {"figure_path": "JpqEzPTuv6/tables/tables_14_2.jpg", "caption": "Table 5: Average training / test accuracy (%) of learning with three loss functions.", "description": "This table presents the average training and testing accuracy results obtained using three different loss functions: lneg, lAPL, and lmaxi.  The results are shown for various datasets (FMNIST, CIFAR-100, mini-ImageNet) and different noise levels (0.3, 0.7 for FMNIST; 0.05, 0.1 for CIFAR-100; instance-dependent for mini-ImageNet). It demonstrates the impact of different loss functions on the accuracy of partial label learning models.", "section": "3.4 PLL through Pseudo-Label Manipulation vs.Loss Minimization"}]