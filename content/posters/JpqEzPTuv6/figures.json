[{"figure_path": "JpqEzPTuv6/figures/figures_1_1.jpg", "caption": "Figure 1: Training accuracy and confidence margin of predicted pseudo-labels for traditional IBS and ABS methods on FMNIST with PLs: (a-b) show the granularity of EM execution in classical IBS methods is refined from a single step to an entire epoch, facilitating a smoother transition from uniform to one-hot pseudo-labels; (c-d) demonstrate that when ABS methods are optimized using SGD, the optimization targets for candidate labels can gradually become distinct.", "description": "This figure shows the training accuracy and confidence margin of predicted pseudo-labels for both traditional identification-based strategy (IBS) and average-based strategy (ABS) methods on the Fashion-MNIST dataset with partial labels (PLs). The subfigures (a) and (b) illustrate how the granularity of the Expectation-Maximization (EM) execution in IBS methods changes during training.  Initially, the EM process is performed in a single step, but this is gradually refined to encompass an entire epoch. This refinement causes a smoother transition from uniform to one-hot pseudo-labels. Subfigures (c) and (d) show that when using stochastic gradient descent (SGD) optimization, the optimization targets for candidate labels in ABS methods gradually become more distinct over time. This observation highlights that ABS methods might behave in a way similar to IBS methods under appropriate conditions.", "section": "The widely accepted PLL taxonomy [47, 24] divides methods based on the treatment of PLs into identification-based strategy (IBS) and average-based strategy (ABS):"}, {"figure_path": "JpqEzPTuv6/figures/figures_6_1.jpg", "caption": "Figure 2: Comparative results of different training setups of DASM on FMNIST with PLs. (a) shows the pseudo-label accuracy, while (b) presents the test accuracy.", "description": "This figure compares the performance of different training setups for the DASM (Dual Augmentation Single Model) algorithm on the FMNIST dataset with partial labels.  The left panel (a) displays the accuracy of the pseudo-labels generated during training, illustrating how well the model is able to identify the true labels within the sets of candidate labels. The right panel (b) shows the test accuracy, which represents the generalization performance of the model on unseen data. Different colors show different training setup configurations.  The results indicate that a specific training setup (SADM) outperforms other strategies, highlighting the importance of the choice of training setup for effective PLL (Partial Label Learning).", "section": "3.3 Does Data Augmentation Help Identification Better?"}, {"figure_path": "JpqEzPTuv6/figures/figures_6_2.jpg", "caption": "Figure 3: Warm up different iterations on FM-NIST with PLs. Red lines mean terminating the warm-up phase at a local maximum in validation accuracy.", "description": "This figure shows the training and validation accuracy curves for three different warm-up iteration numbers (1, 5, and 500) on the Fashion-MNIST dataset with partial labels.  The x-axis represents the training epoch, and the y-axis represents the accuracy.  The red lines indicate where the validation accuracy reached a local maximum, suggesting that prematurely stopping the warm-up phase before overfitting may improve performance. The optimal warm-up strategy appears to be neither too short nor too long.", "section": "3.3 Does Data Augmentation Help Identification Better?"}, {"figure_path": "JpqEzPTuv6/figures/figures_14_1.jpg", "caption": "Figure 4: Training accuracy and confidence margin of predicted pseudo-labels for SASM with different learning rate or optimizer on FMNIST with PLs.", "description": "This figure visualizes the training accuracy and confidence margin of predicted pseudo-labels for the SASM (Single Augmentation Single Model) method on the FMNIST dataset with partial labels.  It explores the impact of different learning rates (1e-1 and 1e-4) and optimizers (SGD with momentum 0.9, SGD without momentum, and Adam) on the model's performance. The plots show how these hyperparameters affect the convergence of the model and the clarity of the pseudo-labels during training.", "section": "3 Understanding Minimal Algorithm Design Principles"}, {"figure_path": "JpqEzPTuv6/figures/figures_14_2.jpg", "caption": "Figure 5: Illustrations of four effective units of SOTA PLL methods. We omit a symmetric loss from the other path, except PRODEN+ which can only compute one-path losses. \"//\" means stop gradient.", "description": "This figure illustrates the core components of four state-of-the-art partial label learning (PLL) methods: DASM, SADM, DADM, and PRODEN+.  Each method's architecture is shown, highlighting the key components: dual augmentations, dual models, and the use of mini-batch PL purification.  The figure simplifies the architecture of each method to illustrate the commonalities and differences in their approaches to addressing the challenges of partial labeling.", "section": "3 Understanding Minimal Algorithm Design Principles"}]