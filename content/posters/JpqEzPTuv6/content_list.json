[{"type": "text", "text": "What Makes Partial-Label Learning Algorithms Effective? ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Jiaqi Lv1,5, Yangfan $\\mathbf{Liu^{1}}$ , Shiyu Xia1, Ning $\\mathbf{X}\\mathbf{u}^{1,5}$ , Miao $\\mathbf{X}\\mathbf{u}^{2}$ , Gang Niu3,1, Min-Ling Zhang1,5, Masashi Sugiyama3,4, Xin Geng1,5\u2217 1Southeast University 2The University of Queensland ", "page_idx": 0}, {"type": "text", "text": "3RIKEN Center for Advanced Intelligence Project 4The University of Tokyo 5Key Laboratory of New Generation Artificial Intelligence Technology and Its Interdisciplinary Applications (Southeast University), Ministry of Education, China {is.jiaqi.lv, gang.niu.ml}@gmail.com, miao.xu@uq.edu.au, sugi@k.u-tokyo.ac.jp, {liuyangfan, shiyu_xia, xning, zhangml, xgeng}@seu.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "A partial label (PL) specifies a set of candidate labels for an instance and partiallabel learning (PLL) trains multi-class classifiers with PLs. Recently, many methods that incorporate techniques from other domains have shown strong potential. The expectation that stronger techniques would enhance performance has resulted in prominent PLL methods becoming not only highly complicated but also quite different from one another, making it challenging to choose the best direction for future algorithm design. While it is exciting to see higher performance, this leaves open a fundamental question: what makes a PLL method effective? We present a comprehensive empirical analysis of this question and summarize the success of PLL so far into some minimal algorithm design principles. Our findings reveal that high accuracy on benchmark-simulated datasets with PLs can misleadingly amplify the perceived effectiveness of some general techniques, which may improve representation learning but have limited impact on addressing the inherent challenges of PLs. We further identify the common behavior among successful PLL methods as a progressive transition from uniform to one-hot pseudo-labels, highlighting the critical role of mini-batch PL purification in achieving top performance. Based on our findings, we introduce a minimal working algorithm that is surprisingly simple yet effective, and propose an improved strategy to implement the design principles, suggesting a promising direction for improvements in PLL. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Partial-label learning (PLL) [14, 12, 42] has been an established discipline in the weakly supervised learning field [50, 51] for decades. It aims to train multi-class classifiers from instances with partiallabels (PLs)\u2014a PL for an instance is a set of candidate labels, where a fixed but unknown candidate is the true label. ", "page_idx": 0}, {"type": "text", "text": "As benchmarking on simulated PLL versions of vision datasets becomes standard practice for evaluating PLL methods, new PLL approaches are emerging that integrate diverse advanced techniques to enhance the performance. While many methods show great promise and some headway has been made in understanding the methodology against ambiguous label assignments [24], we find that state-of-the-art (SOTA) approaches look quite complicated and differ significantly from each other (Table 1), making it challenging to choose the best direction for better algorithm design. ", "page_idx": 0}, {"type": "image", "img_path": "JpqEzPTuv6/tmp/4ec1a490f4a5699a0d75e571f626ecb527bccce32dadbb04fec3d9ca21aac7bc.jpg", "img_caption": ["Figure 1: Training accuracy and confidence margin of predicted pseudo-labels for traditional IBS and ABS methods on FMNIST with PLs: (a-b) show the granularity of EM execution in classical IBS methods is refined from a single step to an entire epoch, facilitating a smoother transition from uniform to one-hot pseudo-labels; (c-d) demonstrate that when ABS methods are optimized using SGD, the optimization targets for candidate labels can gradually become distinct. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "In this paper, we aim to summarize the success of PLL so far, and understand the indispensable elements in a well-performing PLL method, which can be condensed into minimal algorithm design principles. For this purpose, we think of the taxonomy of PLL approaches first, comb the evolution and trends in this field, thereby uncovering key factors that drive their effectiveness and motivating future research. ", "page_idx": 1}, {"type": "text", "text": "The widely accepted PLL taxonomy [47, 24] divides methods based on the treatment of PLs into identification-based strategy (IBS) and average-based strategy (ABS): ", "page_idx": 1}, {"type": "text", "text": "\u2022 IBS disambiguates each PL to select the most likely true label for training;   \n\u2022 ABS treats all candidate labels equally for training. ", "page_idx": 1}, {"type": "text", "text": "However, the boundary between these two categories is vague, resulting in a lack of consensus on the category of many recent approaches which purifies each PL on the fly during model training [25, 37, 39]. They initially look like ABS, since uniform targets are always used to prepare for true-label selection, and as training progresses, the optimization targets for candidate labels become distinct gradually, such that they exhibit IBS-like characteristics. The tricky fact is that such approaches are classified as ABS or IBS hinging on which definition of category researchers are willing to relax (e.g., [45] versus [35])! In light of the evolving approaches within PLL, is there a need to establish a third category within the taxonomy to capture the \"hybrid strategy\", as attempted by recent work like [43, 5]? ", "page_idx": 1}, {"type": "text", "text": "We suggest that the answer is in the negative, since our analysis confirms the fluidity of method categorization within PLL, emphasizing that IBS, ABS, and so-called hybrid strategy often overlap due to the dynamic nature of their implementation: ", "page_idx": 1}, {"type": "text", "text": "\u2022 Training manner of IBS. Typical IBS approaches [48, 13] perform one-step EM to identify the true label in each PL (E step) and then train a supervised classifier (M step). To mitigate overfitting to wrongly identify labels, multi-step EM IBS [11, 10] are proposed. As iterations are executed more frequently, the identifocation of true labels becomes increasingly smooth (Figure 1). \u2022 Implicit differentiation in ABS. Typical ABS approaches [19, 6] design a loss function that does not differentiate candidate labels, e.g., enforcing the softmax outputs of them to sum to 1 [19]. However, stochastic optimization algorithms can implicitly lead to distinct outputs for each candidate label and show a progressive purification characteristic (Figure 1 and Lemma 3.1). \u2022 Execution matters. Whether a method truly exhibits progressive characteristics and the extent of progression also depend on the optimizers used and hyperparameters like the learning rate and batch size (Figure 4). ", "page_idx": 1}, {"type": "text", "text": "Therefore, predefining a method\u2019s category and then asserting its utility can limit our understanding of its true nature and effectiveness. Further, our empirical incestigations reveal a key insight: all successful PLL algorithms exhibit a common behavior characterized by a progressive transition from uniform to one-hot pseudo-labels, facilitated by the combination of PL purification and model updates in a mini-batch-wise manner. Building on this core strategy, modern methods also integrate various techniques from other domains, exemplified by Match from semi-supervised learning [3, 29], aiming to further enhance model performance. While intuitively, better performance could be achieved if stronger techniques are employed, our findings indicate that these enhancements often yield marginal gains when compared to the primary beneftis derived from mini-batch PL purification. Notably, even when these additional techniques result in significant improvements, they tend to boost the model\u2019s ability to learn representations rather than resolving the inherent ambiguities of PLs. ", "page_idx": 1}, {"type": "table", "img_path": "JpqEzPTuv6/tmp/cf3ab7029630f48d9d0caadb1813bfbf55300ec2198dada7baaf46a68ce4e018.jpg", "table_caption": ["Table 1: Comparison of techniques used in eight prominent PLL methods. $\\surd\\times$ indicates whether a technique is used, and an underline denotes the key components of the respective methods. "], "table_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "The main contributions of our paper can be summarized as follows: (i) We advance the understanding of PLL taxonomy and establish minimal algorithm design principles. At the core of these principles is mini-batch PL purification, a fundamental aspect that goes beyond using supervised information. These principles not only enhance the efficiency of algorithm development but also act as a conclusive work to prevent redundant efforts in future research. (ii) We analyze the design philosophies and component frameworks of SOTA PLL methods, conducting comprehensive studies on benchmarksimulated datasets with PLs. Building on this, we highlight a minimal working algorithm that adheres to our design principles, and propose an enhancement strategy to mini-batch PL purification that have the potential to elevate performance across all existing SOTA methods. ", "page_idx": 2}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 Notation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Consider a $k$ -class classification problem. Let $\\textbf{\\em x}\\in\\textbf{\\em X}$ be features and $y\\,\\in\\,\\mathcal{Y}\\,\\doteq\\,\\bigl\\{1,2,\\ldots,k\\bigr\\}$ be labels. Then one has $(\\pmb{x},y)$ sampled from the ground-truth joint density $p(\\pmb{x},y)$ over $\\mathcal X\\times\\mathcal Y$ in supervised learning. PLL deals with PLL data $({\\pmb x},S)$ , which is independently drawn from a corrupted distribution $p({\\pmb x},S)$ of $p(\\pmb{x},y)$ with $p(x)$ unchanged. $S\\,\\in\\,\\{2^{[k]}\\backslash\\emptyset\\backslash[k]\\}$ denotes a $\\mathrm{PL}$ , and $\\bar{\\boldsymbol{D}}=\\left(\\mathbf{\\boldsymbol{x}}_{i},S_{i}\\right)_{i=1}^{n}$ is a PLL dataset. The key assumption of PLL is that the latent true label of an instance is always included in its $\\mathrm{PL}$ , i.e., $p(y\\,\\in\\,S|x,S)=1$ . Let $\\Delta^{k-1}\\subset[0,1]^{k}$ denote the $k$ -dimensional simplex. Let $f:\\mathcal{X}\\to\\Delta^{k-1}$ be a multi-label classifier to be trained, specifically, a composite of a backbone (e.g., ResNet [18]) and an inverse link function $\\psi^{-1}$ [27] (e.g., softmax), so that $f(x)$ can be interpreted as probabilities. Let $\\ell:\\Delta^{k-1}\\times\\mathcal{V}\\rightarrow\\mathbb{R}_{+}$ be a surrogate loss function, e.g., cross-entropy loss. The classification risk of $f$ is defined as $\\mathcal{R}(f)=\\mathbb{E}_{p(\\mathbf{x},y)}[\\ell\\bar{(f(\\mathbf{x}),y)}]$ , which is the performance measure we would like to optimize. ", "page_idx": 2}, {"type": "text", "text": "2.2 Backgrounds ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we review recent prominent PLL works. In PLL, a common practice is to adopt a weighted objective of the form ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{R}(f)=\\mathbb{E}_{p(\\pmb{x},S)}\\big[\\sum_{z\\in S}w(\\pmb{x},z)\\ell(f(\\pmb{x}),z)\\big],\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "with the optimal weights $w(\\pmb{x},z)~=~p(z|\\pmb{x})$ . If $\\ell$ is the cross-entropy loss, the weight can be integrated directly into the loss function, acting as optimization target (pseudo-label) for $\\textbf{\\em x}$ directly. In classical taxonomy for PLL [47], IBS trains predictive models based on fixed weights assigned to the training instances, i.e., $\\hat{w}(\\pmb{x},z)=\\{0,1\\}^{k}$ , while ABS sets uniform weights during training, i.e., $\\hat{w}(\\pmb{x},z)=\\^{\\top}\\!1/|S|$ . ", "page_idx": 2}, {"type": "text", "text": "Definition 2.1 (Mini-batch PL purification). Mini-batch PL purification is a process where for each mini-batch $B\\subset\\mathcal{D}$ selected at iteration $t$ , the weights are updated such that the distinction among candidate labels\u2019 contributions increases over iterations: ", "page_idx": 3}, {"type": "equation", "text": "$$\nw_{t+1}(\\pmb{x};f,S)=g(\\mathfrak{x}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "with $g$ being a strictly increasing function that increases the weight for more likely candidate labels according to the model\u2019s confidence. The model\u2019s parameters $\\theta_{t}$ are updated by optimizing a weighted loss over $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\theta_{t+1}=\\theta_{t}-\\eta_{t}\\nabla_{\\theta}\\sum_{(\\boldsymbol{x},\\boldsymbol{S})\\in\\mathcal{B}}\\ell(f(\\boldsymbol{x};\\theta_{t}),\\boldsymbol{S};\\boldsymbol{w}_{t+1}(\\boldsymbol{x})).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The standard practice is initializing the weights uniformly $w_{i}^{0}=1/\\vert S\\vert$ if $i\\in S$ and $w_{i}^{0}=0$ otherwise, and let $f^{0}$ be initialized randomly. The model $f^{0}$ is then updated for at least one epoch to perform a preliminary training phase. Then in each mini-batch of $t$ -epoch, $w(x)$ is computed where $f$ is fixed, and then $f$ is updated by the weighted objective where $w$ keeps fixed in backpropagation. An instantiation of mini-batch PL purification was first introduced by [25]. They use a delayed mechanism, i.e., $w(x)$ is computed by the output of historical model $f^{t-1}$ on $\\textbf{\\em x}$ , implicitly assuming that DNNs learn pattern first [2], and the delayed mechanism mitigates the accumulation of errors. Then, some methods replace the delayed mechanism with Match techniques to estimate $w$ , which may rely on either Siamese networks [4] applied to two or more inputs (dual-augmentation match, i.e., DA), or co-teaching networks [16] where one network\u2019s outputs serve as targets for the other (dual-model match, i.e., DM). Furthermore, some methods advocate $\\mathrm{DA}{+}\\mathrm{DM}$ framework; for example, PiCO [34] involves mutual guidance of two heterogeneous classifiers, with one built on top of a supervised contrastive learning architecture [20]. In addition techniques borrowed from various communities have propelled PLL methods to top performance, such as mixup [46] and data augmentation like cropping and flipping [28], which have become mainstream. ", "page_idx": 3}, {"type": "text", "text": "As shown in Table 1, prominent methods in recent years rely on the mini-batch PL purification strategy and specific enhanced tools. However, our research reveals a significant disparity in the impact of these components, at least on current benchmarks. Mini-batch PL purification is sufficient to provide a reliable guarantee of performance, while the additional tools contribute relatively little. Although techniques like data augmentation can enhance the performance by improving the model\u2019s robustness to input variations, they primarily boost representation learning, but not benefits the disambiguition for PLL. ", "page_idx": 3}, {"type": "text", "text": "3 Understanding Minimal Algorithm Design Principles ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we inverstigate four SOTA PLL methods that have consistently demonstrated top accuracy across various benchmark tasks. By methodically dissecting these methods and analyzing the components credited for their robustness against PLs, we distill the essential elements contributing to their success. We defer the experiments details to Appendix. ", "page_idx": 3}, {"type": "text", "text": "3.1 PLL with DA Match ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Algorithm details. The key contribution of DPLL [38] lies in incorporating neighborhood consistency, a technique adapted from semi-supervised learning, into PLL. This technique maximizes the similarity among several perturbed views of the same instance, thereby inducing smoothness in the structure of learned representations, referred to as dual-augmentation match (DA Match). ", "page_idx": 3}, {"type": "text", "text": "Specifically, DA Match instantiates Eq. 1 by specifying the loss function as the KL divergence and estimating the weights by a weighted sum of outputs of all augmentations in $\\mathcal{A}(\\pmb{x})$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{R}(\\pmb{x},f)=-\\sum_{\\mathcal{A}(\\pmb{x})}\\hat{w}(\\pmb{x})^{\\top}\\log f(\\mathcal{A}(\\pmb{x})),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{w}(\\pmb{x},z)=\\left\\{\\frac{\\left(\\prod_{\\alpha\\in\\mathcal{A}(x)}f_{z}(\\alpha)\\right)^{1/|\\mathcal{A}(x)|}}{\\sum_{j\\in\\mathcal{S}}\\left(\\prod_{\\alpha\\in\\mathcal{A}(x)}f_{j}(\\alpha)\\right)^{1/|\\mathcal{A}(x)|}}~~~z\\in S,}\\\\ {0~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\mathrm{otherw}}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "$\\hat{w}$ is updated in a mini-batch-wise manner along with the model parameters and is initialized uniformly (mini-batch PL purification). In addition, the learning objective of DPLL includes another loss function as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\ell(\\pmb{x},f)=-\\sum_{i\\notin S}\\log(1-f_{i}(\\pmb{A}(\\pmb{x}))),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where the vector subscript indicates the element is that position. It encourages the output of each non-candidate label to be zero. ", "page_idx": 4}, {"type": "text", "text": "Evaluation 1. Specific implementations do not matter. First, we replace the two terms in learning objective with alternative approaches. Eq. 6 is conceptually equivalent to encouraging the sum of the outputs for the candidate labels to be close to one, i.e, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\ell(\\pmb{x},f)=-\\log\\sum_{i\\in S}f_{i}(\\pmb{A}(\\pmb{x})).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Instead of using a shared target for all views, we modifies the optimization target for each view separately based on the output of the other view: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{R}(\\pmb{x},f)=-\\big(\\hat{w}_{2}(\\pmb{x})^{\\top}\\log f(\\pmb{x}_{1})+\\hat{w}_{1}(\\pmb{x})^{\\top}\\log f(\\pmb{x}_{2})\\big),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where ${\\pmb x}_{1},{\\pmb x}_{2}\\in\\mathcal{A}({\\pmb x})$ , and ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{w}_{2(1)}(\\pmb{x},z)=\\left\\{\\begin{array}{l l}{f_{z}(\\pmb{x}_{1(2)})/\\sum_{j\\in S}f_{j}(\\pmb{x}_{1(2)})}&{z\\in S,}\\\\ {0}&{\\mathrm{otherwise}}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We combine two DA Match terms and two loss functions in pairs, respectively. As can be seen from Table 2, there was no significant difference between the results of these four combinations. This indicates that the specific implementation methods of DA Match, including the exact form of the loss function, is not critical. ", "page_idx": 4}, {"type": "text", "text": "Evaluation 2. Additional losses do not matter. We then split the combined learning objectives into two separate objectives to examine the difference in the contribution of these two components to learning. We found that Eq. 6 and Eq. 8 were comparable under the relatively simple settings, but Eq. 8 outperformed Eq. 4 in challenging scenarios (CIFAR-100 and mini-ImageNet). Note that Eq.6 is a traditionally considered ABS loss, and Eq. 8 implements mini-batch PL purification. It is commonly believed that ABS does not require identifying the true labels during training, leading to over-parameterized DNNs memorizing all candidate labels [44], which results in poor performance. However, as discussed in Section 1, ABS can still achieve acceptable performance because the optimization process may induce differentiated outputs. We will elaborate on this in more detail in Section 3.4. Another noteworthy observation is that Eq. 8 not only did not lead to a decrease in accuracy compared to the original DPLL, but even showed some improvement, which is likely because Eq. 8 can model neighborhood consistency better than Eq. 4. ", "page_idx": 4}, {"type": "text", "text": "So far, we have extracted a core unit from DPLL, which takes the form of Siamese networks [4]: a weight-sharing network applied on two (or more) inputs for comparing, and PLs prevent the model from collapsing, i.e., outputting a constant for all inputs. We call Eq. 8 dual augmentations single model (DASM), as depicted in Figure 5. However, it remains to be seen whether the effectiveness of DASM is due to mini-batch PL purification or neighborhood consistency. ", "page_idx": 4}, {"type": "text", "text": "Evaluation 3. Mini-batch PL purification does matter. We modify the learning strategy of DASM by either altering the mini-batch PL purification strategy or removing the consistency component. We compare them in Table 2, where the $\"{\\mathrm{-H\"}}$ , \"-S\" or \"-E\" suffix means using the hard pseudo-labels, one-step iteration or epoch-wise iteration. Specifically, DASM-H uses hard labels as optimization targets for Eq. 8: ", "page_idx": 4}, {"type": "equation", "text": "$$\n{\\hat{w}}_{2}(\\pmb{x})=e^{i},{\\hat{w}}_{1}(\\pmb{x})=e^{j},\\;\\;{\\mathrm{where~}}i=\\arg\\operatorname*{max}f(\\pmb{x}_{1}){\\mathrm{~and~}}j=\\arg\\operatorname*{max}f(\\pmb{x}_{2}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $e^{i}$ denotes the $i$ -th standard canonical vector, i.e., $e^{i}\\in\\{0,1\\}^{k}$ , ${\\bf1}^{\\top}e^{i}=1$ . DASM-S simulates one-step EM methods by training the model with uniform targets for the first 50 epochs, and then transforming PLL into supervised learning by using one-hot pseudo-labels (i.e., the argmax of the model\u2019s output for each instance at the 50th epoch) for the next 450 epochs. The results indicated that the performance of DASM-H and DASM-S was inferior to DASM. This can be attributed to the models\u2019 inability to adjust learning targets at the appropriate time based on the underlying learned patterns: Since DNNs tend to fti easy patterns first and gradually memorize harder ones, a phenomenon known as memorization effects [2], DASM-H may remember unreliable information due to random initialization, and DASM-S may lead remember too much undesired memorization [15]. DASM-E shifts from a mini-batch-wise manner to an epoch-wise manner, performing pseudo-label estimation and model updates at the epoch level, which resulted in decreased accuracy. Compared to DASM-E, mini-batch manner benefits from using the up-to-date model for generating optimization objectives and also improves computational efficiency. In addition, we change the dual feedforwarding to single. The optimization target of an input is modified in place according to its own output, which we term single augmentations single model (SASM). It was somewhat surprising that this method does not require additional components, performed remarkably well, implying DA may not be necessary. ", "page_idx": 4}, {"type": "table", "img_path": "JpqEzPTuv6/tmp/343af73c930db33c1703aee598204838a796b0c0241329b5aebea571c321bded.jpg", "table_caption": ["Table 2: Conceptual and empirical comparisons $(\\%)$ of various simplifications of DPLL. $\\surd\\times$ indicates whether a technique is used. "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Evaluation 4. Does data augmentation matter? Additionally, removing data augmentation from DASM, as in DASM w/o.aug, causes a decrease in accuracy as expected. We will discuss its impact further in Section 3.3. ", "page_idx": 5}, {"type": "text", "text": "3.2 PLL with DA $^{+}$ DM Match ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Algorithm details. PiCO [34] enhances representation learning by incorporating supervised contrastive learning into PLL. It utilizes two heterogeneous classifiers sharing a backbone (one linear and one contrastive-based), guiding each other to instantiate Eq. 1. PaPi [40] investigates PiCO and identifies limitations in the contrastive learning module. Thus PaPi adopts a more efficient approach inspired by the delayed mechanism in PRODEN, instantiating Eq. 1 without the need to maintain two separate models. Besides, PaPi also uses the zero-and-normalized outputs of historical models to guide a prototypical classifier that shares the same backbone with the linear classifier. Both methods feed forward different views of the same input. We refer to such franework as dual-augmentation and dual-model match $\\scriptstyle\\mathrm{DA+DM}$ Match). CroSel [30] is the latest PLL method that, in addition to using the DM framework to generate optimization targets for each other, also selects samples with more accurate pseudo-labels for the other model to compute supervised loss. ", "page_idx": 5}, {"type": "text", "text": "Evaluation. At a high level, PiCO draws inspiration from co-teaching [16]: instead of training a single classifier, it trains two classifiers simultaneously and lets them teach each other in every minibatch. We simplify this idea by removing the contrastive-based classifier and using two networks with the same architecture but different initialization, which is also CroSel without its sample selection module. We call such method dual augmentations dual models (DADM). Taking it a step further, single augmentation dual models (SADM) removes one data augmentation, feeding both networks the same view of an instance within the same epoch. Conversely, if we cancel DADM from CroSel, the remaining implementation is co-teaching adapted for PLL (Coteaching in Tabel 4). For PaPi, we strip away the prototypical classifier, resulting in a streamlined version akin to PRODEN with added data augmentation $(\\mathrm{PRODEN+})$ ), to explore whether the specific instantiation of DM makes a difference. Figure 5 illustrates their basic workflow. ", "page_idx": 5}, {"type": "text", "text": "Our results are shown in Table 4. Both DADM and SADM outperformed PiCO and were comparable with CroSel, and $\\mathrm{PRODEN+}$ generally matches the performance of PaPi. Deleting the implementation of mini-batch PL purification, whether by altering the iteration frequency or replacing soft pseudolabels with hard pseudo-labels, consistently leads to a decline in performance. The observations reaffirm that mini-batch PL purification is essential for achieving top performance. By comparing the performance of the DM framework with the SM framework, we discover that the DM framework\u2019s edge often comes from the diverse capabilities of the two networks, which help handle the noise introduced by PLs. On the other hand, the DA methods without any special design (DASM, DADM, PRODEN+) showed suboptimal performance than the SA methods, perhaps hinting at the importance of exploring different augmentation methods [31] to discover appropriate choices for class-invariant patterns, remaining for future research. ", "page_idx": 5}, {"type": "image", "img_path": "JpqEzPTuv6/tmp/e017b396e6d4cca28bd59645326d594b5da8a66b45fb8fc4a198ff7114719901.jpg", "img_caption": ["Figure 2: Comparative results of different training setups of DASM on FMNIST with PLs. (a) shows the pseudo-label accuracy, while (b) presents the test accuracy. "], "img_footnote": [], "page_idx": 6}, {"type": "image", "img_path": "JpqEzPTuv6/tmp/4fb988fb889e155f183769260b09263059a72f5c144693733be40ef6fc8ac5a8.jpg", "img_caption": ["Figure 3: Warm up different iterations on FMNIST with PLs. Red lines mean terminating the warm-up phase at a local maximum in validation accuracy. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "3.3 Does Data Augmentation Help Identification Better? ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Our findings indicate that mini-batch PL purification and data augmentation are pivotal for PLL, achieving competitive performance when both techniques are implemented. Data augmentation is a well-established regularization tool, enhancing model robustness to input variations. In the following, we explore whether its role in PLL extends beyond this conventional purpose, specifically whether it facilitates the crucial task of identifying the true label among candidate labels. If data augmentation aids true-label identification, we would expect that augmented examples generate more accurate pseudo-labels, then enhancing the performance. Specifically, we experiment with DASM by setting one view as augmented and the other as non-augmented. We also use different training setups: (i) using zero-and-normalized pseudo-labels generated from original $\\textbf{\\em x}$ to supervise augmented $\\textbf{\\em x}$ , and (ii) the reverse, using augmented $\\textbf{\\em x}$ to supervise original $\\textbf{\\em x}$ . The results are presented in Figure 2. ", "page_idx": 6}, {"type": "text", "text": "Switching from dual paths to a single path while retaining supervised learning on augmented instances with pseudo-labels from the original instances showed little change in accuracy. However, when using pseudo-labels from augmented instance to supervise original one, performance drops significantly. This suggests that data augmentation alone is insufficient to preserve the mutual information between examples and their true labels, as augmented views may discard task-relevant information, thereby degrading performance. Following this reasoning, our results suggest that data augmentation indirectly benefits the classifiers built upon representation learning rather than aiding in label disambiguation. Therefore, it should NOT be considered a design principle for PLL. ", "page_idx": 6}, {"type": "text", "text": "3.4 PLL through Pseudo-Label Manipulation vs.Loss Minimization ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Mainstream methods mentioned above conduct mini-batch PL purification primarily by directly manipulating the optimization targets of candidate labels. However, as analyzed in Section 1 and observed in Eq. 6 of Table 2, using stochastic algorithms to minimize loss functions, even ABS loss funtions that are traditionally considered to \"treat all candidate labels equally\", can also implicitly result in a progressive effect of pseudo-labels. To our knowledge, no existing research has explored the relationship between pseudo-label manipulation and loss minimization. Our findings raise the question: how do these two methods compare in terms of their effectiveness and mechanisms in the context of PL identification? ", "page_idx": 6}, {"type": "text", "text": "We investigate several representative PLL losses, which are traditionally thought as ABS: \u2022 Modified negative log likelihood loss (Eq. 6) [38]: $\\begin{array}{r}{\\ell_{\\mathrm{neg}}(\\pmb{x},f)=-\\sum_{i\\notin S}\\log(1-f_{i}(\\pmb{x}))}\\end{array}$ ; \u2022 APL loss [24]: $\\begin{array}{r}{\\ell_{\\mathrm{APL}}(\\pmb{x},f)=\\frac{1}{|S|}\\sum_{i\\in S}\\tilde{\\ell}(f(\\pmb{x}),i)}\\end{array}$ , with GCE loss [49] as the component $\\tilde{\\ell}$ ; ", "page_idx": 6}, {"type": "text", "text": "\u2022 Maximum likelihood loss [19]: $\\begin{array}{r}{\\ell_{\\mathrm{maxi}}(\\pmb{x},f)=-\\log\\sum_{i\\in S}f_{i}(\\pmb{x})}\\end{array}$ . ", "page_idx": 7}, {"type": "text", "text": "Denote the scores of $\\textbf{\\em x}$ outputted by the last layer before softmax as $_{\\textit{z}}$ , i.e., $\\psi^{-1}(z)=f(\\pmb{x})$ where $f=(\\psi^{-1}\\circ f^{(n)}\\circ\\cdots\\circ f^{(1)})$ . Let us look at the gradients of $\\ell_{\\mathrm{neg}}$ and $\\ell_{\\mathrm{APL}}$ : ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\frac{\\partial\\ell_{\\mathrm{neg}}}{\\partial z_{k}}=\\left\\{\\!\\!\\begin{array}{l l}{-f_{k}\\sum_{i\\notin S}\\frac{e^{z_{i}}}{\\sum_{j\\in\\mathcal{N}}e^{z_{j}}-e^{z_{i}}}}&{k\\in S,}\\\\ {f_{k}\\big(1-\\sum_{i\\notin S,i\\neq k}\\frac{e^{z_{i}}}{\\sum_{j\\in\\mathcal{N}}e^{z_{j}}-e^{z_{i}}}\\big)}&{\\mathrm{otherwise,}}\\end{array}\\!\\!\\right.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "equation", "text": "$$\n\\frac{\\partial\\ell_{\\mathrm{APL}}}{\\partial z_{k}}=\\left\\{\\frac{-\\frac{1}{|S|}(f_{k}\\sum_{i\\in S,i\\neq k}f_{i}^{q}-f_{k}^{q}(1-f_{k}))}{\\frac{1}{|S|}f_{k}\\sum_{i\\in S}f_{i}^{q}}\\right.\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\left.\\rule{0.3cm}{0cm}\\right.\\rule{0.3cm}{0ex}\\mathrm{~for~}\\mathrm{AP}\\mathrm{{erwise}},\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $q\\in(0,1]$ is a tunable parameter of GCE. From a gradient perspective, we observe that while the implicit optimization targets for all candidate labels are the same for each loss function, their optimization speeds differ. Specifically, the gradients of the candidate labels are consistently negative until one candidate label accumulates all the probabilities, which implies that both loss functions promote the output of each candidate label to converge to 1. As a result, the candidate labels compete for dominance. A label with a higher output probability experiences a larger gradient and thus becomes the winner. According to the memorization effects of DNNs, such a label is more likely to be the true label receiving a larger gradient at the beginning, which explains why the labels become distinguishable with these two losses. However, since the optimization targets for all candidate labels remain the same, minimizing these two losses does not align with our definition of mini-batch PL purification where the optimization targets are expected to diverge progressively. ", "page_idx": 7}, {"type": "text", "text": "Then we focus on the third loss $\\ell_{\\mathrm{maxi}}$ . We examine its gradients as ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\frac{\\partial\\ell_{\\mathrm{maxi}}}{\\partial z_{k}}=\\left\\{\\!\\!\\begin{array}{l l}{f_{k}-\\frac{e^{z_{k}}}{\\sum_{i\\in S}e^{z_{i}}}}&{k\\in S,}\\\\ {f_{k}}&{\\mathrm{otherwise.}}\\end{array}\\!\\!\\right.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "It is crucial to recognize two key points: (i) The philosophy of $\\ell_{\\mathrm{maxi}}$ is to ensure that the output of all candidate labels sums to 1, which is exactly the same as that of $\\ell_{\\mathrm{neg}}$ , which requires the output of all non-candidates to sum to 0. However, their gradients are completely different, indicating that they lead to different optimal empirical solutions even with identical initialization and stochastic optimizer. (ii) When learning with $\\ell_{\\mathrm{maxi}}$ , the implicit optimization target of an instance is the zero-and-normalized outputs of current model on this instance itself, exhibiting consistent behavior with pseudo-label manipulation in SASM. ", "page_idx": 7}, {"type": "text", "text": "Lemma 3.1. Suppose using the same stochastic optimizer, then performing SASM is mathematically equivalent to minimizing $\\ell_{\\mathrm{maxi}}$ . ", "page_idx": 7}, {"type": "text", "text": "Empirical results in Table 5 also verified the theoretical findings. However, pseudo-label manipulation offers greater flexibility as it allows for more arbitrary modifications to the optimization targets, as DASM, SADM, etc. have done, and additional steps over generating soft pseudo-labels, such as sharpening [29], but the targets of loss functions are fixed. ", "page_idx": 7}, {"type": "text", "text": "Now, our empirical investigations have firmly established the necessity of mini-batch PL purification along with using supervision. Supervision can be used in two ways: directly within the loss function as it in supervised learning, or to manipulate pseudo-labels on-the-fly. We identify SASM is the minimal working algorithm. It achieved superior or at least comparable performance compared with SOTA PLL methods in most cases, while not requiring multiple forward propagations or additional components. ", "page_idx": 7}, {"type": "text", "text": "3.5 Probing the Implementation of Mini-Batch PL Purification ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Here, we examine the implementation of mini-batch PL purification, raising at least two questions: Q1. Does different initialization methods impact the performance? Q2. Why does model confidence in PLs is effective to disambiguation? ", "page_idx": 7}, {"type": "text", "text": "In early training stages, our primary concern is if initialization methods might cause certain candidate labels to receive significantly higher confidence. Consider a neural network where weights are independently drawn from a standard normal distribution with zero mean and variance $\\bar{\\sigma^{2}}$ (as in normal initialization, Xavier initialization [1], He initialization [17]). For an instance $\\textbf{\\em x}$ , the output $z$ of a neuron after a ReLU activation is given by $z=\\operatorname*{max}(0,w x+b)$ , where $\\mathbf{\\nabla}w$ represents the weight vector and $b$ is a small constant bias. By the Central Limit Theorem, the mean of the weights $\\begin{array}{r}{\\bar{w}\\stackrel{{}}{=}1/n\\sum_{i=1}^{n}{\\pmb w}_{i}}\\end{array}$ for a sufficiently large number of neurons $n$ approaches a normal distribution with mean zero and variance $\\delta^{2}/n$ . Applying Chebyshev\u2019s inequality, we have $P(|\\bar{w}|\\geq a)\\leq\\delta^{2}/(n a^{2})$ . As $n\\to\\infty$ , $\\bar{w}$ is close to zero with high probability. Then the output for each neuron $z$ will be close to $b$ . By a similar reasoning, this result generalizes to deeper layers, suggesting that the initial outputs across classes approximate a uniform distribution regardless of the number of layers. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Lemma 3.2. For a neural network initialized with weights w drawn from a normal distribution ${\\mathcal{N}}(0,\\delta^{2})$ , the initial outputs across classes approximate a uniform distribution as the number of neurons $n\\to\\infty$ . ", "page_idx": 8}, {"type": "text", "text": "Then we explain why using model\u2019s output as a proxy for the probability that a candidate label is the true label works. This practice of using the high-confidence label as the true label, introduced to PLL by [25], has long been foundational in noisy-label learning (e.g., [16, 23]). Its success in PLL hinges on a key assumption: the true label has a higher probability of appearing among the candidate labels than any incorrect label. In fact, the experimental setups employed across PLL methods impose a further restriction on this assumption, that is, $p(y\\in S|x,S)\\stackrel{.}{=}\\dot{1},p(i\\in S|x,S)>1,\\forall i\\not=y$ . This setup implies that, within any sufficiently small neighborhood in the data space, the true label will dominate. Consequently, when stochastic gradient-based optimizers are used, the true label tends to contribute more frequently to the objective function, making it more likely to be learned first. ", "page_idx": 8}, {"type": "text", "text": "4 Improving Mini-Batch PL Purification ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "4.1 Motivation ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "The standard practice involves initiating the process with uniform pseudo-labels for one epoch to bootstrap the classifier\u2019s basic capabilities. Our first observation is that the effect of warming up for one epoch is nearly indistinguishable from not warming up at all, prompting further investigation into the actual efficacy of the warm-up phase. Suppose a neural network is initialized with weights drawn from a Gaussian distribution with mean zero and a sufficiently small variance. If the inputs are normalized, the pre-activation values across a neural network tend to be small and centered around zero. When these values are input to a softmax function, the resulting distribution across classes tends to uniformity. ", "page_idx": 8}, {"type": "text", "text": "Our second observation is that a prolonged warm-up using uniform pseudo-labels often leads to the network overfitting to candidate labels, as illustrated in Figure 3, evident around 50 epochs with a decline in validation accuracy. While overfitting to the training data becomes apparent about 120 epochs, marked by a drop in training accuracy after previously reaching a peak. ", "page_idx": 8}, {"type": "text", "text": "We simply terminate the warm-up phase at a local maximum in validation accuracy, about 5 epoch, preventing excessive memorization while preserving more information beneficial for generalization. Then the results showed a little improvement. Several approaches [34, 24] have used multiple epochs for warm-up and treat the number of warm-up epochs as a hyperparameter. However, due to the varying difficulty levels across samples, using a uniform duration for the warm-up phase could result in performance disparities among different subgroups within the dataset. This suggests the need for an adaptive warm-up strategy. ", "page_idx": 8}, {"type": "text", "text": "4.2 StreamPurify: An Instance-Dependent Warm-Up Strategy ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Building upon the analysis and our established design principles, we propose StreamPurify, a novel instance-dependent warm-up strategy, which fine-tunes the entry into the mini-batch PL purification phase based on each instance\u2019s readiness. During the initial training phase, it selectively channels instances that have higher confidence in the accuracy of their pseudo-labels into the PL purification phase, while others continue training with uniform targets until they meet the readiness criteria. This filtered progression helps prevent DNNs from harmfully memorizing incorrect pseudo-labels and mitigate the accumulation of errors from inadequately learned samples. Differing from sample selection techniques used in noisy-label learning [36] that reevaluated samples in every iteration, our filter approach is conducted without replacement. Once the training samples are transitioned to the purification phase, they do not revert to uniform targets, ensuring that the strategy remains in line with mini-batch PL purification. ", "page_idx": 8}, {"type": "table", "img_path": "JpqEzPTuv6/tmp/e217ab4eab18ac125024acbeb3086214c3aa72da29444fc8453989044f1484d1.jpg", "table_caption": ["Table 3: Classification accuracy $(\\%)$ improvement of PLL methods with StreamPurify. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "StreamPurify is compatible to existing PLL methods and can absorb various sample selection criteria, such as small-loss trick. We adopt the sample selection method from CroSel [30] that chooses the samples with stable and high confidence as a filter criterion within StreamPurify, and combine it with the mini-batch PL purification approaches discussed in our paper. Empirical evaluations indicate that methods augmented with StreamPurify generally exceed the performance of their conventional counterparts, validating the effectiveness of StreamPurify, especially in complicated learning scenarios. This substantiates the robustness and adaptability of instance-dependent warm-up, suggesting it as a promising direction for future improvements in PLL. ", "page_idx": 9}, {"type": "text", "text": "5 Discussion and Future Work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We have systematically delineated the core components underlying successful PLL methods, centering our insights around the pivotal effect of mini-batch PL purification. We have proposed StreamPurify, an enhanced mini-batch PL purification strategy that tailors the learning path for each sample based on its state of readiness. We reaffirm that strictly categorizing PLL methods as IBS or ABS may oversimplify the dynamics of how these methods operate in practice. In the future, we wish to explore advanced PLL approaches guided by the minimal algorithm design principles. We also wish to extend the applicability and understanding of PLL methods to domains beyond vision tasks. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This research was supported by the National Science Foundation of China (62406066, 62206050, 62125602, 62176055, 62076063), Jiangsu Province Science Foundation for Youths (BK20241297, BK20210220), China Postdoctoral Science Foundation (2021M700023), Young Elite Scientists Sponsorship Program of Jiangsu Association for Science and Technology (TJ-2022-078), the Australian Research Council Discovery Early Career Research Award (DE230101116), the Fundamental Research Funds for the Central Universities (2242024k30035), the Big Data Computing Center of Southeast University, JST CREST Grant Number JPMJCR18A2 and a grant from Apple, Inc. Any views, opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and should not be interpreted as reflecting the views, policies or position, either expressed or implied, of Apple Inc. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] X. G. andnY. Bengio. Understanding the difficulty of training deep feedforward neural networks. In Proceedings of 13th International Conference on Artificial Intelligence and Statistics (AISTATS\u201910), pages 249\u2013256, 2010. ", "page_idx": 9}, {"type": "text", "text": "[2] D. Arpit, S. Jastrzebski, N. Ballas, D. Krueger, E. Bengio, M. S. Kanwal, T. Maharaj, A. Fischer, A. C. Courville, Y. Bengio, and S. Lacoste-Julien. A closer look at memorization in deep networks. In Proceedings of 34th International Conference on Machine Learning (ICML\u201917), volume 70, pages 233\u2013242, 2017. ", "page_idx": 9}, {"type": "text", "text": "[3] D. Berthelot, N. Carlini, I. Goodfellow, N. Papernot, A. Oliver, and C. A. Raffel. Mixmatch: A holistic approach to semi-supervised learning. In Advances in Neural Information Processing Systems 32 (NeurIPS\u201919), pages 5050\u20135060, 2019. [4] J. Bromley, I. Guyon, Y. LeCun, E. Sackinger, and R. Shah. Signature verification using a siamese time delay neural network. In Advances in neural information processing systems 7 (NeurIPS\u201994), pages 737\u2013744, 1994. [5] X. Cheng, D. Wang, L. Feng, M. Zhang, and B. An. Partial-label regression. In Proceedings of 37th AAAI Conference on Artificial Intelligence (AAAI\u201923), pages 7140\u20137147, 2023.   \n[6] T. Cour, B. Sapp, and B. Taskar. Learning from partial labels. Journal of Machine Learning Research, 12(5):1501\u20131536, 2011.   \n[7] E. D. Cubuk, B. Zoph, D. Man\u00e9, V. Vasudevan, and Q. V. Le. Autoaugment: Learning augmentation strategies from data. In Proceedings of 32nd IEEE Conference on Computer Vision and Pattern Recognition (CVPR\u201918), pages 113\u2013123, 2019. [8] T. Devries and G. W. Taylor. Improved regularization of convolutional neural networks with cutout. arXiv preprint arXiv:1708.04552, 2017.   \n[9] J. C. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12:2121\u20132159, 2011.   \n[10] L. Feng and B. An. Partial label learning by semantic difference maximization. In Proceedings of 28th International Joint Conference on Artificial Intelligence (IJCAI\u201919), pages 2294\u20132300, 2019.   \n[11] L. Feng and B. An. Partial label learning with self-guided retraining. In Proceedings of 33rd AAAI Conference on Artificial Intelligence (AAAI\u201919), pages 3542\u20133549, 2019.   \n[12] L. Feng, J. Lv, B. Han, M. Xu, G. Niu, X. Geng, B. An, and M. Sugiyama. Provably consistent partial-label learning. In Advances in Neural Information Processing Systems 33 (NeurIPS\u201920), pages 10948\u201310960, 2020.   \n[13] C. Gong, T. Liu, Y. Tang, J. Yang, J. Yang, and D. Tao. A regularization approach for instancebased superset label learning. IEEE Transactions on Cybernetics, 48(3):967\u2013978, 2018.   \n[14] Y. Grandvalet and Y. Bengio. Learning from partial labels with minimum entropy. 2004.   \n[15] B. Han, G. Niu, X. Yu, Q. Yao, M. Xu, I. Tsang, and M. Sugiyama. Sigua: Forgetting may make learning with noisy labels more robust. In Proceedings of 37th International Conference on Machine Learning (ICML\u201920), pages 4006\u20134016, 2020.   \n[16] B. Han, Q. Yao, X. Yu, G. Niu, M. Xu, W. Hu, I. Tsang, and M. Sugiyama. Co-teaching: Robust training of deep neural networks with extremely noisy labels. In Advances in Neural Information Processing Systems 31 (NeurIPS\u201918), pages 8527\u20138537, 2018.   \n[17] K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In Proceedings of 15th IEEE International Conference on Computer Vision (ICCV\u201915), 2015.   \n[18] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings of 29th IEEE conference on Computer Vision and Pattern Recognition (CVPR\u201916), pages 770\u2013778, 2016.   \n[19] R. Jin and Z. Ghahramani. Learning with multiple labels. In Advances in Neural Information Processing Systems 16 (NeurIPS\u201903), pages 921\u2013928, 2003.   \n[20] P. Khosla, P. Teterwak, C. Wang, A. Sarna, Y. Tian, P. Isola, A. Maschinot, C. Liu, and D. Krishnan. Supervised contrastive learning. In Advances in Neural Information Processing Systems 33 (NeurIPS\u201920), 2020.   \n[21] A. Krizhevsky and G. Hinton. Learning multiple layers of features from tiny images. Citeseer, 2009.   \n[22] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278\u20132324, 1998.   \n[23] J. Li, R. Socher, and S. C. H. Hoi. Dividemix:learning with noisy labels as semi-supervised learning. In Proceedings of 8th International Conference on Learning Representations (ICLR\u201920), 2020.   \n[24] J. Lv, B. Liu, L. Feng, N. Xu, M. Xu, B. An, G. Niu, X. Geng, and M. Sugiyama. On the robustness of average losses for partial-label learning. IEEE Transactions on Pattern Analysis and Machine Intelligence, 46(5):2569\u20132583, 2024.   \n[25] J. Lv, M. Xu, L. Feng, G. Niu, X. Geng, and M. Sugiyama. Progressive identification of true labels for partial-label learning. In Proceedings of 37th International Conference on Machine Learning (ICML\u201920), pages 6500\u20136510, 2020.   \n[26] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems 32 (NeurIPS\u201919), pages 8024\u20138035, 2019.   \n[27] M. D. Reidand and R. C. Williamson. Composite binary losses. The Journal of Machine Learning Research, 11:2387\u20132422, 2010.   \n[28] C. Shorten and T. M. Khoshgoftaar. A survey on imagedata augmentation for deep learning. Journal of Big Data, 6:1146\u20131151, 2019.   \n[29] K. Sohn, D. Berthelot, N. Carlini, Z. Zhang, H. Zhang, C. Raffel, E. D. Cubuk, A. Kurakin, and C. Li. Fixmatch: Simplifying semi-supervised learning with consistency and confidence. In Advances in Neural Information Processing Systems 33 (NeurIPS\u201920), 2020.   \n[30] S. Tian, H. Wei, Y. Wang, and L. Feng. Crosel: Cross selection of confident pseudo labels for partial-label learning. In Proceedings of 37th IEEE Conference on Computer Vision and Pattern Recognition (CVPR\u201924), 2024.   \n[31] Y. Tian, C. Sun, B. Poole, D. Krishnan, C. Schmid, and P. Isola. What makes for good views for contrastive learning? In Advances in Neural Information Processing Systems 33 (NeurIPS\u201920), 2020.   \n[32] O. Vinyals, C. Blundell, T. Lillicrap, K. Kavukcuoglu, and D. Wierstra. Matching networks for one shot learning. In Advances in neural information processing systems 29 (NeurIPS\u201916), pages 3630\u20133638, 2016.   \n[33] H. Wang, M. Xia, Y. Li, Y. Mao, L. Feng, G. Chen, and J. Zhao. Solar: Sinkhorn label refinery for imbalanced partial-label learning. In Advances in Neural Information Processing Systems 35 (NeurIPS\u201922), 2022.   \n[34] H. Wang, R. Xiao, Y. Li, L. Feng, G. Niu, G. Chen, and J. Zhao. Pico: Contrastive label disambiguation for partial label learning. In Proceedings of 10th International Conference on Learning Representations (ICLR\u201922), 2022.   \n[35] W. Wang and M. Zhang. Partial label learning with discrimination augmentation. In Proceedings of 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining $'K D D^{\\prime}O\\&,$ ), pages 1920\u20131928, 2022.   \n[36] Z. Wang, J. Jiang, B. Han, L. Feng, B. An, G. Niu, and G. Long. Seminll: A framework of noisy-label learning by semi-supervised learning. Transactions on Machine Learning Research, 2022, 2022.   \n[37] H. Wen, J. Cui, H. Hang, J. Liu, Y. Wang, and Z. Lin. Leveraged weighted loss for partial label learning. In Proceedings of 36th International Conference on Machine Learning (ICML\u201921), pages 11091\u201311100, 2021.   \n[38] D. Wu, D. Wang, and M. Zhang. Revisiting consistency regularization for deep partial label learning. In Proceedings of 37th International Conference on Machine Learning (ICML\u201922), volume 162, pages 24212\u201324225, 2022.   \n[39] Z. Wu, J. Lv, and M. Sugiyama. Learning with proper partial labels. Neural Computation, 35(1):58\u201381, 2022.   \n[40] S. Xia, J. Lv, N. Xu, G. Niu, and X. Geng. Towards effective visual representations for partiallabel learning. In Proceedings of 36th IEEE Conference on Computer Vision and Pattern Recognition (CVPR\u201923), pages 15589\u201315598, 2023.   \n[41] H. Xiao, K. Rasul, and R. Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. arXiv preprint arXiv:1708.07747, 2017.   \n[42] N. Xu, C. Qiao, X. Geng, and M. Zhang. Instance-dependent partial label learning. In Advances in Neural Information Processing Systems 34 (NeurIPS\u201921), pages 3615\u20133621, 2021.   \n[43] Y. Yao, C. Gong, J. Deng, and J. Yang. Network cooperation with progressive disambiguation for partial label learning. In The European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML-PKDD), pages 471\u2013488, 2020.   \n[44] C. Zhang, S. Bengio, M. Hardt, B. Recht, and O. Vinyals. Understanding deep learning requires rethinking generalization. In Proceedings of 5th International Conference on Learning Representations (ICLR\u201917), 2017.   \n[45] F. Zhang, L. Feng, B. Han, T. Liu, G. Niu, T. Qin, and M. Sugiyama. Exploiting class activation value for partial-label learning. In Proceedings of 10th International Conference on Learning Representations (ICLR\u201922), 2022.   \n[46] H. Zhang, M. Cisse, Y. N. Dauphin, and D. Lopez-Paz. mixup: Beyond empirical risk minimization. In Proceedings of 6th International Conference on Learning Representations (ICLR\u201918), 2018.   \n[47] M. Zhang. Disambiguation-free partial label learning. In Proceedings of the 14th SIAM International Conference on Data Mining, pages 37\u201345, 2014.   \n[48] M. Zhang, B. Zhou, and X. Liu. Partial label learning via feature-aware disambiguation. In Proceedings of 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD\u201916), pages 1335\u20131344, 2016.   \n[49] Z. Zhang and M. Sabuncu. Generalized cross entropy loss for training deep neural networks with noisy labels. In Advances in Neural Information Processing Systems 31 (NeurIPS\u201918), pages 8778\u20138788, 2018.   \n[50] Z. Zhou. A brief introduction to weakly supervised learning. National Science Review, 5(1):44\u2013 53, 2017.   \n[51] Z. Zhou. Open-environment machine learning. National Science Review, 2022. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A Experiments Details ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "As benchmarking on partially labeled vision datasets has become standard practice in evaluating deep PLL methods, we conduct experiments on FMNIST [41], CIFAR-100 [21] and mini-ImageNet [32]. We generated PLs by the Flipping strategy [24] for FMNIST, CIFAR-10 and CIFAR-100. Each label $i$ is added into PL with a flipping probability $\\eta_{i}^{y}=p(i\\in S|y)$ independently and features are untouched: $\\begin{array}{r}{p(s|x,y)\\,=\\,M\\prod_{i\\in S}\\eta_{i}^{y}\\prod_{i\\notin S}(1-\\eta_{i}^{y})}\\end{array}$ where $\\begin{array}{r}{M=1/\\bigl(1-\\prod_{i\\neq y}\\eta_{i}^{y}\\bigr)}\\end{array}$ . We assumed $\\eta_{i}^{y}=\\eta,\\forall i\\neq y$ and $\\eta_{y}^{y}=1$ , and set $\\eta=\\{0.3,0.7\\}$ on FMNIST and CIFAR-10, $\\eta=\\{0.05,0.1\\}$ on CIFAR-100. For mini-ImageNet and ablation experiments (Figure 1, 2, 3, 4), we simulated the real scenario by setting the filpping probability for each incorrect label individually for each instance. We first trained a classifier with clean labels, and then for each instance, set the confidence prediction of the classifier as the flipping probability [42]. ", "page_idx": 13}, {"type": "text", "text": "Our explorations used three backbones: 5-layer LeNet [22] on FMNIST, 18-layer ResNet [18] on CIFAR-10 and CIFAR-100, and 34-layer ResNet [18] on mini-ImageNet. All the methods were trained for 500 epochs with a standard SGD optimizer [9] with a momentum of 0.9 and the batch size was 256 (128 for mini-ImageNet). We left out $10\\%$ of the corrupted training samples as a validation set, and searched the initial learning rate from $\\left\\lbrace0.1,0.07,0.05,0.03\\right\\rbrace$ with cosine learning rate scheduling. We conducted 3 trials for each experiment, and recorded the mean test accuracy in percentage. There were two kinds of random augmentations involved. \u201cWeak\u201d augmentation was a random horizontal flips and crops [3]. For \u201cstrong\u201d augmentation on FMNIST and CIFAR-10, we added Cutout [8] to the weak augmentation, and on CIFAR-100 and mini-ImageNet, we additional leveraged AutoAugment [7]. We denote the augmentation by $A(\\cdot)$ , with method clear from context. The implementation was based on PyTorch [26] and experiments were carried out with GeForce RTX $4090\\,\\mathrm{{D}}$ . ", "page_idx": 13}, {"type": "text", "text": "Notably, we focused on the core components of the SOTA PLL methods mentioned in this paper, rather than strictly adhering to the settings detailed in their original implementations. This approach was taken to ensure fair and meaningful comparisons. For instance, we did not include techniques such as mixup in PaPi or the triple augmentation used in DPLL. As a result, the reported performance metrics in our paper might be slightly lower than those presented in the original publications. ", "page_idx": 13}, {"type": "text", "text": "B Experimental Results ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In Figure 1 (c-d), CLPL is a traditionally considered ABS loss from [6]. Figure 4 illustrates the dynamic changes in the same method under different hyperparameters and optimization methods. One can expect that with more extreme hyperparameters or optimization methods, the approach may degrade to one-step EM or persist with nearly uniform optimization targets. This indicates that method categorization must be assessed on a case-by-case basis post hoc. Figure 5 is the workflow of several key methods proposed in the paper. Table 4 provides conceptual and empirical comparisons of various simplifications of PiCO, PaPi and CroSel. Consistent with Table 2, the results reiterate that mini-batch PL purification is pivotal to achieve top performance. Table 5 presents the results of minimizing three loss functions. It is evident that $\\ell_{\\mathrm{maxi}}$ yield similar results with SASM, and owing to the mini-batch PL purification, and due to the implementation of mini-batch PL purification, the accuracy is higher than the other two losses. ", "page_idx": 13}, {"type": "table", "img_path": "JpqEzPTuv6/tmp/cc58b5d8d1a35ac887f5fd07f8baae39e328b231b122264e31fc440a61541ae1.jpg", "table_caption": ["Table 4: Conceptual and empirical comparisons $(\\%)$ of various simplifications of PiCO, PaPi and CroSel. "], "table_footnote": [], "page_idx": 14}, {"type": "table", "img_path": "JpqEzPTuv6/tmp/949093ab4f037a9f9ef6bff45f56e6c094dea9f0f783b980486fa5a2e3dc5960.jpg", "table_caption": ["Table 5: Average training / test accuracy $(\\%)$ of learning with three loss functions. "], "table_footnote": [], "page_idx": 14}, {"type": "image", "img_path": "JpqEzPTuv6/tmp/8890b57b906dcc372b3c882436d868e2cf5fbc8c3071ba1d2ecad0cc9f657d57.jpg", "img_caption": [], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "Figure 4: Training accuracy and confidence margin of predicted pseudo-labels for SASM with different learning rate or optimizer on FMNIST with PLs. ", "page_idx": 14}, {"type": "image", "img_path": "JpqEzPTuv6/tmp/73d0f080e6b23e5e26b2eab50a53c7db42c70c5cfbd607e604c9ba6b7752f5f0.jpg", "img_caption": ["Figure 5: Illustrations of four effective units of SOTA PLL methods. We omit a symmetric loss from the other path, except PRODEN+ which can only compute one-path losses. \u201d//\u201d means stop gradient. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The papers not including the checklist will be desk rejected. The checklist should follow the references and follow the (optional) supplemental material. The checklist does NOT count towards the page limit. ", "page_idx": 15}, {"type": "text", "text": "Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist: ", "page_idx": 15}, {"type": "text", "text": "\u2022 You should answer [Yes] , [No] , or [NA] .   \n\u2022 [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.   \n\u2022 Please provide a short (1\u20132 sentence) justification right after your answer (even for NA). ", "page_idx": 15}, {"type": "text", "text": "The checklist answers are an integral part of your paper submission. They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper. ", "page_idx": 15}, {"type": "text", "text": "The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a proper justification is given (e.g., \"error bars are not reported because it would be too computationally expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found. ", "page_idx": 15}, {"type": "text", "text": "IMPORTANT, please: ", "page_idx": 15}, {"type": "text", "text": "\u2022 Delete this instruction block, but keep the section heading \u201cNeurIPS paper checklist\", \u2022 Keep the checklist subsection headings, questions/answers and guidelines below. \u2022 Do not modify the questions and only use the provided macros for your answers. ", "page_idx": 15}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 15}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Justification: The main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope. ", "page_idx": 15}, {"type": "text", "text": "Guidelines: ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 15}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 15}, {"type": "text", "text": "Justification: The paper discuss the limitations of the work performed by the authors. Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 16}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Justification: The paper provide the full set of assumptions and a complete proof. Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 16}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: The paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper.   \nGuidelines: ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 17}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 17}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Justification: The paper focuses on understand existing algorithms rather than to fundamentally improve them. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. ", "page_idx": 17}, {"type": "text", "text": "\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). \u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 18}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: The paper specify all the training and test details. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 18}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: The paper report appropriate information about the statistical significance of the experiments. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 18}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 18}, {"type": "text", "text": "Answer: [No] ", "page_idx": 18}, {"type": "text", "text": "Justification: The paper does not involve large-scale model training or extensive computational experiments. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 19}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: The research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 19}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 19}, {"type": "text", "text": "Answer: [No] ", "page_idx": 19}, {"type": "text", "text": "Justification: The paper does not delve into the broader societal impacts. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 19}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 19}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 20}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: The paper has cited them appropriately. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 20}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: The paper has cited them appropriately. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 20}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "page_idx": 20}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 21}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 21}]