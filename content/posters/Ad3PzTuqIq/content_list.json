[{"type": "text", "text": "SpelsNet: Surface Primitive Elements Segmentation by B-Rep Graph Structure Supervision ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Kseniya Cherenkova Elona Dupont SnT, University of Luxembourg, Artec3D SnT, University of Luxembourg kseniya.cherenkova@uni.lu elona.dupont@uni.lu ", "page_idx": 0}, {"type": "text", "text": "Anis Kacem Gleb Gusev Djamila Aouada SnT, University of Luxembourg Artec3D SnT, University of Luxembourg anis.kacem@uni.lu gleb@artec3d.com djamila.auoada@uni.lu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Within the realm of Computer-Aided Design (CAD), Boundary-Representation (B-Rep) is the standard option for modeling shapes. We present SpelsNet, a neural architecture for the segmentation of 3D point clouds into surface primitive elements under topological supervision of its B-Rep graph structure. We also propose a pointto-BRep adjacency representation that allows for adapting conventional Linear Algebraic Representation of B-Rep graph structure to the point cloud domain. Thanks to this representation, SpelsNet learns from both spatial and topological domains to enable accurate and topologically consistent surface primitive element segmentation. In particular, SpelsNet is composed of two main components; (1) a supervised 3D spatial segmentation head that outputs B-Rep element types and memberships; (2) a graph-based head that leverages the proposed topological supervision. To enable the learning of SpelsNet with the proposed point-to-BRep adjacency supervision, we extend two existing CAD datasets with the required annotations, and conduct a thorough experimental validation on them. The obtained results showcase the efficacy of SpelsNet and its topological supervision compared to a set of baselines and state-of-the-art approaches. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Creating a structured and editable Computer-Aidded Design (CAD) representation Mortenson [2006], Shah and M\u00e4ntyl\u00e4 [1995] from an unstructured 3D scan (e.g. point cloud) is a core challenge, often referred to as reverse engineering. This field has a long history of extensive research due to its numerous commercial applications Abella et al. [1994], Varady et al. [1997], B\u00e9ni\u00e8re et al. [2013], Liu et al. [2023]. Modern CAD workflows commonly use Boundary Representation as the primary format to model complex shapes Lambourne et al. [2021], Guo et al. [2022]. The wide adoption of B-Rep in most CAD software and recent advances in neural point cloud representations challenges the reverse engineering research towards the problem of learnable B-Rep inference from point clouds Liu et al. [2023], Guo et al. [2022], Yan et al. [2021], Huang et al. [2021]. ", "page_idx": 0}, {"type": "text", "text": "Boundary Representation (B-Rep) is a collection of connected surface elements with their geometric definitions in a form of parametric surfaces, curves and points Shah and M\u00e4ntyl\u00e4 [1995]. The topology of these elements is also described by the connection of face, edge and vertex components. Thus, face is a bounded surface, edge is a bounded curve, and vertex is a realisation of a 3D point DiCarlo et al. [2014]. B-Rep is a compact representation and it retains more structural information about an object than a point cloud. Most popular approaches to reverse engineer B-Reps from point clouds follow a segmentation-ftiting paradigm, i.e. the point cloud is firstly segmented into surface patches, and then parameterized by fitting a specific surface type Sharma et al. [2020], Li et al. [2019], Yan et al. [2021], Huang et al. [2021]. However, existing segmentation-based approaches mostly deal with either surface patches or boundary curves, ignoring the full B-Rep structure. This often leads to inaccurate and disjoint reconstruction of its elements Sharma et al. [2020]. Aware of this limitation, ComplexGen Guo et al. [2022] modeled the B-Rep as a chain complex Hatcher [2002] and formulated the prediction of validness and primitive types as classification tasks to recover corners, curves, and patches together with their mutual topological features. Assembled in a probabilistic graph constraints, this topological information is further used in a time consuming post-processing topological and geometrical optimization to recover plausible geometry. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In this work, we propose to exploit the topological information from B-Rep as a direct neural supervision within a Graph Neural Network (GNN) paradigm. To incorporate this supervision together with geometric data into a single learnable pipeline, we consider the Linear Algebraic Representation (LAR) of B-Rep chain complex DiCarlo et al. [2014]. LAR is defined on B-Reps and fully encodes their chain complex in sparse and compact matrices offering desirable learning properties. To allow for direct supervision on point clouds using LAR, we adapt it to the point cloud domain and propose a novel point-to-BRep adjacency representation. Furthermore, we design a novel end-to-end trainable network architecture, named SpelsNet, for the inference of B-Rep elements from point clouds. SpelsNet is composed of a spatial and a topological component leveraging both the classical segmentation and the proposed point-to-BRep adjacency supervision signals, respectively. The contributions of this work can be summarized to: ", "page_idx": 1}, {"type": "text", "text": "\u2022 A novel LAR-based representation of B-Rep chain complex adapted to point clouds, called point-to-BRep adjacency, that allows for direct neural supervision of B-Rep topological information on point clouds. To the best of our knowledge, we are the first to propose a direct B-Rep chain complex supervision on point clouds;   \n\u2022 SpelsNet, an end-to-end trainable architecture for B-Rep element segmentation from point clouds. SpelsNet unifies 3D spatial and graph neural networks in a single design and exploits the proposed LAR-based point-to-BRep adjacency supervision in addition to the classical B-Rep element segmentation supervision;   \n\u2022 Extended versions of two existing CAD datasets ABCParts Li et al. [2019] and CC3D Cherenkova et al. [2020]. The new versions, called ABC-VEF and CC3D-VEF, include the proposed LAR-based point-to-BRep adjacency representation on the point clouds and will be made publicly available to enable further research;   \n\u2022 A thorough experimental validation showcasing the superiority of the proposed method over multiple baselines and state-of-the-art approaches. ", "page_idx": 1}, {"type": "text", "text": "The rest of the paper is organized as follows: Section 2 discusses the related works. In Section 3, we provide background on B-Rep chain complex and present the proposed point-to-BRep adjacency representation. Section 4 offers a detailed description of the proposed SpelsNet network. The experiments are reported and discussed in Section 5. Finally, Section 6 concludes the paper and provides perspectives for future works. ", "page_idx": 1}, {"type": "text", "text": "2 Related Works ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Existing research on Scan-to-Brep often focuses on specific aspects of the problem, such as enhancing segmentation, improving surface fitting, or refining topology. We categorize these approaches accordingly in the following discussion. ", "page_idx": 1}, {"type": "text", "text": "Starting with Efficient Ransac Schnabel et al. [2007], which progressively estimates primitive parameters within point cloud in a sample consensus paradigm, continuing with data-driven learning methods Li et al. [2019], Sharma et al. [2020], that train point-based neural networks to assign patch primitive types and parameters to each input point, it became common to solve Scan-to-Brep problem in two-phase manner, namely, decomposition (segmentation) and fitting. PrimitiveNet Huang et al. [2021] proposes to treat primitive types as semantic classes and use adversarial learning to guide feature enrichment for better surface property representation. HPNet Yan et al. [2021] runs a mean-shift clustering over the hybrid representations that are combined by learnt weights. ", "page_idx": 1}, {"type": "text", "text": "ParSeNet Sharma et al. [2020] in comparison to SPFN Li et al. [2019] constructs an additional SplineNet component to extend the set of supported surface primitives with bspline surfaces. BP", "page_idx": 1}, {"type": "text", "text": "Net Fu et al. [2023] discards the primitive types and approximates all surface patches with bspline surfaces. QuadricsNet Wu et al. [2023] defines a fitting process in a form of quadrics. Several approaches focus solely on edge reconstruction to generate wireframes. For instance, NerVE Zhu et al. [2023] utilizes a neural volumetric edge representation for piecewise linear curves extraction, while DEF Matveev et al. [2022] regresses a continuous distance field to the closest edge supplemented by spline-based curve extraction. SepicNet Cherenkova et al. [2023] builds an end-to-end trainable network, where the curve ftiting is formulated in a primitive-differentiable manner. Mentioned aboveFu et al. [2023], Wu et al. [2023], Zhu et al. [2023] can be considered as alternative representations, though interesting, but fall off the traditional B-Rep structure. ", "page_idx": 2}, {"type": "text", "text": "A major challenge in previous work has been the discontinuity of predicted surface elements, often requiring extra post-processing. While ParSeNet Sharma et al. [2020] offers an optional refinement module, Li et al. [2023] tackle this issue directly by simultaneously detecting surfaces and edges using a two-branch network. AutoGPart Liu et al. [2022] presents a generalizable approach for 3D part segmentation with geometric priors, potentially improving continuity. ", "page_idx": 2}, {"type": "text", "text": "We propose to leverage B-Rep inherent graph structure, with nodes representing elements like vertices, edges, and faces, directly within a Graph Neural Network by developing a unified representation for both spatial and graph domains. This approach uniquely utilizes the B-Rep topological relationships, incorporating node features like element type, and potentially even additional classifications like sharp vs. smooth edges. ", "page_idx": 2}, {"type": "text", "text": "Various graph-based learning techniques, particularly Graph Convolutional Networks (GCNs) and Message Passing Neural Networks (MPNNs), have been applied to diverse data types such as part assemblies, social networks, etc Zhou et al. [2020]. Notably, GCNs have been successfully employed in tasks like automatic mating prediction in CAD assemblies Jones et al. [2021] while MPNNs have been adapted for specific B-rep tasks, such as face segmentation in BrepNet Lambourne et al. [2021]. While Spectral Convolutional Networks offer potential e.g., Smirnov and Solomon [2021], their computational cost can be prohibitive for large graphs. ComplexGen Guo et al. [2022] predicts validity and primitive types while recovering topological features, but relies on time-consuming post-processing. ", "page_idx": 2}, {"type": "text", "text": "Our approach leverages a unified segmentation framework with trainable LAR characteristic matrices to directly learn the B-Rep structure. ", "page_idx": 2}, {"type": "text", "text": "3 Point-to-BRep Adjacency Formulation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Given an input 3D point cloud, our method aims to identify and extract individual elements of the corresponding CAD model\u2019s Boundary-Representation (B-Rep), i.e. vertices, edges, and faces. In addition, the method determines the primitive type of these elements and their topological connectivity information. We present the essential background on B-Reps and their topological structure, then detail our approach for adapting them to point cloud data. ", "page_idx": 2}, {"type": "text", "text": "3.1 Background on Boundary-Representation (B-Rep) Chain Complex ", "text_level": 1, "page_idx": 2}, {"type": "image", "img_path": "Ad3PzTuqIq/tmp/16e539bad68f4ffe284a6793479c506cbc1282394ef57a28bfc2720439ee9f2c.jpg", "img_caption": [], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "Figure 1: Left panel: The B-Rep elements and their topological connectivity in the form of $\\boldsymbol{\\mathrm{LAR}}^{b r e p}$ with edge-vertex characteristic matrix ${{\\bf{M}}_{1}}$ and face-vertex characteristic matrix ${\\bf M}_{2}$ . Right panel: The proposed point-to-BRep adjacency representation $L\\mathbf{A}\\mathbf{R}^{p c d}$ and its characteristic matrices $\\mathbf{M}_{1}^{p}$ and $\\mathbf{\\bar{M}}_{2}^{\\bar{p}}$ for points on edges (in red) and faces (in blue). ", "page_idx": 2}, {"type": "text", "text": "A B-Rep $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ is composed of three elements, namely faces, edges and vertices. $V=\\{v_{i}\\}$ is as the set of all vertices where $N_{v}$ is the number of vertices. Similarly, $E=\\{e_{i}\\}$ is the set of $N_{e}$ edges, and $\\begin{array}{r}{F=\\{f_{i}\\}}\\end{array}$ the set of $N_{f}$ faces. Each edge $e_{i}$ is defined by a curve of a specific type (e.g. line, arc, etc). Each face $f_{i}$ is also defined by a surface (e.g. planar, spherical, etc) and its corresponding parametric description. Crucially, a B-Rep $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ not only stores information about these individual elements but also encapsulates the connectivity information between vertices, edges, and faces. This connectivity information is essential for defining the overall topology of the model. As shown in an example in the left panel of Figure 1, a pyramid\u2019s B-Rep data structure stores information about its vertices, edges, and faces. The topological relationships between these elements are represented in a Vertex-Edge-Face graph, defining the pyramid\u2019s overall topology. ", "page_idx": 3}, {"type": "text", "text": "Formally, the B-Rep can be described as a chain complex $\\mathcal{C}=(V,E,F,\\delta,\\Pi)$ of order $d=3$ , where boundary operator $\\delta$ connects the elements of different orders, and $\\Pi$ is a set of possible attributes (refer to Hatcher [2002] for more details). For instance, $\\delta_{2}f_{i}\\in E$ gives the edges which define the boundary of a face $f_{i}$ and $\\delta_{1}e_{i}\\in V$ the end vertices of the edge $e_{i}$ . In other words, each element set $V,E,F$ induces a corresponding vector space $\\mathbb{V},\\mathbb{E},\\mathbb{F}$ and its boundary transition $\\mathbb{F}\\xrightarrow{\\delta_{2}}\\mathbb{E}\\xrightarrow{\\delta_{1}}\\mathbb{V}$ . Further, we describe the use of the Linear Algebraic Representation (LAR) described DiCarlo et al. [2014], a convenient and efficient representation that supports topological constructions that typically arise in a cellular decomposition of B-Rep space. Formally, LAR encodes a chain complex $\\mathcal{C}$ of order $d$ by a set of binary characteristic matrices $\\mathbf{M}_{u}$ , with $1\\,\\leq\\,u\\,<\\,d$ , encoding the incidence of B-Rep elements. These matrices provide a convenient and sparse-compact form for describing topological relations of B-Rep elements. For a B-Rep chain complex (i.e. of order 3) there exist two characteristic matrices, $\\mathbf{M}_{1}\\,\\stackrel{\\_}{=}\\,\\Delta(E,V)\\,\\,\\in\\,\\{\\mathbf{0},\\mathbf{1}\\}^{\\bar{N}_{e}\\,\\times\\,N_{v}}$ and $\\mathbf{M}_{2}\\,=\\,\\Delta(F,V)\\,\\,\\,\\in\\,\\{{\\bf0},{\\bf1}\\}^{N_{f}\\,\\times\\,N_{v}}$ . Here, $\\pmb{\\Delta}(E,V)$ assigns 1 to $\\mathbf{M}_{1}[i,j]$ if an edge $e_{i}\\mathrm{~\\ensuremath~{~\\in~}~}E$ is bounded by vertex $\\boldsymbol{v}_{j}\\mathrm{~\\in~}\\ \\boldsymbol{V}$ and 0, otherwise. Similarly, $\\Delta(F,V)$ operates on faces and vertices to construct $\\mathbf{M}_{2}$ . An example of a characteristic matrix ${{\\bf{M}}_{1}}$ can be found in the left panel of Figure 1. As mentioned in DiCarlo et al. [2014], ${{\\bf{M}}_{1}}$ and $\\mathbf{M}_{2}$ can fully characterize the B-Rep chain complex and can be used to obtain the following incidence and adjacency matrices, ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\boldsymbol{\\mathbf{A}}_{f f}=\\mathbf{M}_{2}\\boldsymbol{\\mathbf{M}}_{2}^{T};\\quad\\boldsymbol{\\mathbf{A}}_{e e}=\\mathbf{M}_{1}\\mathbf{M}_{1}^{T};\\quad\\boldsymbol{\\mathbf{A}}_{v v}=\\mathbf{M}_{1}^{T}\\boldsymbol{\\mathbf{M}}_{1}\\;.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Here, $\\mathbf{A}_{f f}$ represents the adjacency of faces in a B-Rep, that is the faces that are bounded by a common edge. Similarly, $\\mathbf{A}_{e e}$ provides the edges that are bounded by a common vertex and $\\mathbf{A}_{v v}$ provides the vertices that bound a common edge. Note that as explained in DiCarlo et al. [2014], the characteristic matrices are typically sparse for actual B-Rep chain complexes, so they can be stored and operated in memory-efficient Compressed Sparse Row (CSR) format. The product and transposition of such CSR matrices, needed to compute the boundary, adjacency and incidence operators between such linear spaces, are intrinsically efficient, since the sparse matrix-vector (SpMV) multiplication is linear in the size of the output. ", "page_idx": 3}, {"type": "text", "text": "3.2 Proposed Point-to-BRep Adjacency Representation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The LAR representation is the core concept for our proposed topological supervision. As LAR are defined on B-Rep, we formulate a mechanism to transfer LAR elements from B-Rep to point cloud domain such that the learning of the B-Rep characteristics from a point cloud can be facilitated. We use the terminology $L\\mathbf{A}\\mathbf{R}^{b r e p}$ and $L\\mathbf{A}\\mathbf{R}^{p c d}$ to distinguish between the LAR of B-Rep and its point cloud reformulation. ", "page_idx": 3}, {"type": "text", "text": "The right panel of Figure 1 depicts an example of the topological transfer to a point cloud. Let $\\mathbf{P}=\\{\\bar{p}_{i}\\in\\mathbb{R}^{d_{p}}|i=\\bar{1_{\\cdot}}N_{p}\\}$ be a point cloud composed of $N_{p}$ points, where $d_{p}$ denotes the dimension of point features. We define the characteristic matrix M1p = \u2206p(E, P) \u2208 {0, 1}Ne\u00d7Np in LARpcd as a binary matrix with rows representing the edges of the B-Rep and columns the points of $\\mathbf{P}$ . Here, $\\Delta_{p}(E,{\\bf P})$ assigns the value of $\\mathbf{M}_{1}^{p}[i,k]$ to 1 if a given point $p_{i}~\\in{\\bf~P}$ belongs to an edge $e_{k}~\\in~E$ . This function also sets the value $\\bar{\\mathbf{M}}_{1}^{p}[i,l]$ to 1 if the edge $e_{l}~\\in~E$ is adjacent to $e_{k}$ . Otherwise, the value is set 0. Similarly, the characteristic matrix $\\mathbf{M}_{2}^{p}=\\Delta_{p}(F,\\mathbf{P})\\ \\in\\ \\{\\mathbf{0},\\mathbf{1}\\}^{N_{f}\\times N_{p}}$ in $L\\mathbf{A}\\mathbf{R}^{p c d}$ is also a binary matrix. The rows correspond to the faces of the B-Rep and the columns to the points of the point cloud. Here also, $\\Delta_{p}(F,{\\bf P})$ operates in the same way as $\\Delta_{p}(E,{\\bf P})$ but on faces instead of edges. Note that the characteristic matrices in $\\boldsymbol{\\mathrm{LAR}^{p c d}}$ encode the per-point B-Rep edge and face memberships along with their connectivity that are essential elements of the B-Rep structure. As in $\\boldsymbol{\\mathrm{LAR}}^{b r e p}$ , the adjacency of edges can be computed as $\\mathbf{A}_{e e}=\\mathbf{M}_{1}^{p}\\mathbf{M}_{1}^{p T}$ and the adjacency of faces as Aff = M2pM2pT . ", "page_idx": 3}, {"type": "image", "img_path": "Ad3PzTuqIq/tmp/a95b796d570122f8b57b40517ae135e9aaad69b26a486a7725f02c8df93ae5a6.jpg", "img_caption": ["Figure 2: SpelsNet architecture overview. The SparseCNN encoder outputs the point-wise spatial embeddings $\\mathbf{F}_{e}$ . Primitive types and membership segmentation learning is done in spatial domain in the SpelsNetspmodule together with topological supervision by B-Rep-level elements and structure prediction in Graph Neural Network in the SpelsNet ${\\mathit{v e f}}$ . "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Given a point cloud $\\mathbf{P}$ , the goal of SpelsNet is to predict the corresponding B-Rep structure including the connectivity and adjacency between the primitives (i.e. edges and faces) and their types. In addition to per-point face and edge memberships, SpelsNet leverages the proposed formulation of $\\mathrm{LAR}^{p c d}$ to guide the adjacency learning via topological supervision. The proposed SpelsNet is described in the next Section. ", "page_idx": 4}, {"type": "text", "text": "4 Proposed Network ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We design SpelsNet, a network architecture to segment an input point cloud $\\mathbf{P}$ into B-Rep elements. The overall structure of SpelsNet is depicted in Figure 2. SpelsNet operates on point clouds with a SparseCNN encoder Choy et al. [2019] and it is composed of two main components: (1) SpelsNetsp operates in the spatial domain and consists of a type classification head and a membership head; (2) SpelsNet $v e f$ leverages the point-to-BRep adjacency supervision to learn the B-Rep topology. In the following, the individual components of SpelsNet are described. ", "page_idx": 4}, {"type": "text", "text": "4.1 Point Cloud Encoding ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The point cloud encoder $\\Phi_{p}$ is composed of sparse 3D convolutions Choy et al. [2019] in geometric space. In practice, we use a SparseCNN encoder module with a ResUnet backbone, implemented as in Choy et al. [2019]. The input point cloud $\\mathbf{P}$ is discretized on a voxel grid with a chosen resolution $\\rho$ , the input features of dimension $d_{p}$ are the 3D coordinates of each point and optionally its point normal. As a result, the point cloud is encoded into per-point features $\\mathbf{F}_{e}$ of dimension $d_{e}=92$ . ", "page_idx": 4}, {"type": "text", "text": "4.2 Spatial Domain Classification and Segmentation, SpelsNetsp ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Type Classification: The first component of the SpelsNet $s p$ classifies each point as belonging to an edge or a face as well as the type of the primitive $\\mathbf{T}_{p}$ . This is achieved by decoding the point embedding $\\mathbf{F}_{e}$ with an MLP $\\Phi_{c}$ and using the soft logits to classify each point into one of $n_{T}=11$ types. The $n_{T}$ types are composed of 4 curve types, 6 surface types and a class for all possible unknown types. As a result, it is possible to deduce whether a point is an edge or a face point from the predicted type $\\tilde{\\mathbf{T}}_{p}$ . Point-wise primitive element types are learnt with multi-class cross-entropy loss, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{p c l s}=\\frac{1}{N_{p}}\\sum_{i=0}^{N_{p}}C E(\\tilde{\\mathbf{T}}_{p}[i],\\mathbf{T}_{p}[i])\\mathrm{~,~}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathbf{T}_{p},\\tilde{\\mathbf{T}}_{p}$ stands for ground-truth and predicted primitive types. ", "page_idx": 5}, {"type": "text", "text": "Membership Segmentation: In order to segment points as belonging to the same curve or surface patch a metric learning approach is followed. As depicted in Figure 2, the point-wise embeddings $\\mathbf{F}_{e}$ are encoded using an MLP $\\Phi_{s}$ into features $\\mathbf{F}_{s}\\in\\mathbb{R}^{N_{p}\\times d_{s}}$ with $d_{s}=128$ . The learning of $\\mathbf{F}_{s}$ is conducted using a triplet loss $\\mathcal{L}_{s e g}$ . For a triplet of point-embeddings $\\mathbf{f}_{s}^{+},\\mathbf{f}_{s}^{-},\\mathbf{f}_{s}^{a}\\in\\mathbf{F}_{s}$ of positive, negative and anchor input, respectively, the triplet loss is given by ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{s e g}=m a x(\\lvert\\lvert\\mathbf{f}_{s}^{a}-\\mathbf{f}_{s}^{+}\\rvert\\rvert_{2}-\\lvert\\lvert\\mathbf{f}_{s}^{a}-\\mathbf{f}_{s}^{-}\\rvert\\rvert_{2}+m,0)\\,.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The default margin value $m$ is set to 0.05 and for each sample the number of points is restricted to 8000 for efficiency reasons. At inference time, the clustering step is done using HDBScan McInnes et al. [2017] to segment the points into edge membership $\\tilde{\\mathbf{W}_{e}}$ and face membership $\\tilde{\\mathbf{W}}_{f}$ that approximate the ground truth memberships ${\\mathbf W}_{e}$ and $\\mathbf{W}_{f}$ , respectively. ", "page_idx": 5}, {"type": "text", "text": "4.3 B-Rep Topological Supervision, SpelsNetvef ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The topological supervision includes two main modules, a Graph Structure Learning (GSL) layer and a Graph Convolutional Network (GCN). The GSL aims to learn the point-to-BRep adjacency $\\mathrm{LAR}^{p c d}$ , whereas the GCN leverages the B-Rep element types. ", "page_idx": 5}, {"type": "text", "text": "Graph Structure Learning (GSL): Inspired by the idea of dynamically learning to construct a graph from a point cloud Wang et al. [2019], we develop a method to connect the spatial 3D shape features, $\\mathbf{F}_{e}$ , with the learning of a graph structure that reflects the B-Rep topology. In particular, the goal of the GSL layer is to learn the characteristic matrices of $\\mathrm{LAR}^{p c d}$ , i.e. $\\mathbf{M}_{1}^{p}$ and $\\mathbf{M}_{2}^{p}$ . In order to facilitate the learning, the matrices $\\mathbf{M}_{1}^{p}$ and $\\mathbf{M}_{2}^{p}$ are concatenated in a row-wise manner to form a single matrix $\\mathbf{M}^{p}\\ \\in\\ \\{\\mathbf{0},\\mathbf{1}\\}^{(N_{e}+N_{f})\\,\\times\\,N_{p}}$ . Given the matrix of per-point point cloud embeddings $\\mathbf{F}_{e}=N_{p}\\times d_{e}$ where $N_{p}$ is the number of points in the point cloud and $d_{e}$ is the dimension of each point embedding, the GSL layer $\\Phi_{G S L}$ predicts the following weighted characteristic matrix, ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\tilde{\\mathbf{M}}^{p}=\\Phi_{G S L}(\\mathbf{F}_{e})=\\mathrm{LeakyReLU}(\\mathrm{Tanh}\\big(\\mathtt{M L P}(\\mathbf{F}_{e})\\big))\\mathrm{~.~}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "In the initial experiments, the ReLU activation, was utilized to directly enforce sparsity on the output. Due to stability issues discovered during training, this was changed in further experiments to LeakyReLU, for which outputs are further clamped to 0 as minimum value. The use of Tanh is advocated by the finding that empirically $\\mathrm{LAR}^{p c d}$ with both positive and negative weights gives better results than other options. We employ direct supervision induced by $\\boldsymbol{\\mathrm{LAR}^{p c d}}$ with an $l1$ -loss defined by, ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{l a r}=||\\tilde{\\mathbf{M}}^{p}-\\mathbf{M}^{p}||_{1}\\;.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Graph Convolutional Network (GCN): Once the characteristic matrix $\\tilde{\\mathbf{M}}_{p}$ has been obtained from the GSL layer, it is possible to leverage these topological features to build an adjacency graph in order to predict the B-Rep elements such as the edge types (e.g. lines, spline) and face types (e.g. plane, cylinder). As mentioned in Section 3.2, the edge and face adjacency matrices, $\\mathbf{A}_{e e}$ and $\\mathbf{A}_{f f}$ , can be obtained from $\\mathbf{M}_{1}^{p}$ and $\\mathbf{M}_{2}^{p}$ , respectively. These adjacency matrices are combined into one matrix given by $\\tilde{\\mathbf{A}}=\\tilde{\\mathbf{M}}_{p}\\tilde{\\mathbf{M}}_{p}^{T}$ of dimension $\\mathrm{dim}(\\tilde{\\mathbf{A}})=(N_{e}+N_{f})\\times(N_{e}+N_{f})$ . A two-layer GCN $\\Phi_{G C N}$ is introduced to exploit the graph structure inferred by GSL and defined by $(\\tilde{\\mathbf{M}}_{p},\\tilde{\\mathbf{A}})$ . The main idea is to further supervise this graph with an additional head via B-Rep element types. The initial node features of the graph are obtained by a row-wise mean pooling ${\\sf P o o l}(.)$ of $\\tilde{\\mathbf{M}}^{p}$ . The graph embedding $\\mathbf{Z}$ are learnt according to ", "page_idx": 5}, {"type": "equation", "text": "$$\n{\\bf Z}=\\Phi_{G C N}(\\tilde{\\bf M}_{p},\\tilde{\\bf A})=\\tilde{\\bf A}\\mathrm{ReLU}(\\tilde{\\bf A}\\mathrm{Poo1}(\\tilde{\\bf M}_{p}){\\bf\\Theta}^{0}){\\bf\\Theta}^{1}\\;,\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\Theta^{0}$ and $\\Theta^{1}$ are learnable parameters. The embedded $\\mathbf{Z}$ is finally passed to a Softmax layer with a number of nodes equal to that of primitive types $n_{T}$ . Finally, a primitive type classification cross entropy loss $\\mathcal{L}_{g c l s}$ is introduced on the output of Softmax similarly to $\\mathcal{L}_{p c l s}$ in Eq. (2). ", "page_idx": 5}, {"type": "text", "text": "Total Loss: The overall network SpelsNet is trained in an end-to-end manner and the loss is given by, ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{t o t a l}=\\alpha_{1}\\mathcal{L}_{p c l s}+\\alpha_{2}\\mathcal{L}_{s e g}+\\alpha_{3}\\mathcal{L}_{g c l s}+\\alpha_{4}\\mathcal{L}_{l a r}\\;.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "with $\\alpha_{1},\\,\\alpha_{4}$ set to 1 and $\\alpha_{2}{=}\\alpha_{3}=2$ . ", "page_idx": 5}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "5.1 Experimental Setup ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "ABCParts-VEF Dataset: SpelsNet is trained and evaluated on the ABCParts dataset Li et al. [2019] using the same train $(22k)$ , test $(3.5k)$ and validation $(3.5k)$ splits. We prepare the updated version of this dataset, the ABCParts-VEF dataset, by extending it with B-Rep structural information in the form of characteristic matrices $\\mathbf{M}_{1}^{p}$ and $\\mathbf{M}_{2}^{p}$ . Refer to the supplementary materials for further details. ", "page_idx": 6}, {"type": "text", "text": "CC3D-VEF Real Scan Dataset: To evaluate the ability of SpelsNet to generalize to real-world data, a cross-dataset experiment on the proposed CC3D-VEF dataset is conducted. The CC3D Cherenkova et al. [2020] dataset contains 3D scans along with corresponding B-Rep. As for ABCParts-VEF dataset, we extend the CC3D dataset with the B-Rep topological information. This proposed version of the dataset is referred to as CC3D-VEF. Testing the model using 3D scans offers an opportunity not only to evaluate how the model generalizes to out-of-distribution data, but also how the presence of realistic artifacts such as missing parts, smooth edges, and noise affects the performance. More details on the proposed CC3D-VEF dataset are provided in supplementary materials. ", "page_idx": 6}, {"type": "text", "text": "Training and Inference: The input point cloud is normalized to unit sphere, randomly rotated and discretized on a voxel grid with a chosen resolution $\\rho=0.01$ . SpelsNet is trained with AdamW solver with a cosine annealing learning rate schedule starting at $\\bar{10}^{-3}$ and weight decay $10^{-2}$ for 250 epochs to convergence. The training takes approximately 10 days on a node with 4 Nvidia A100(40Gb) GPUs. In order to facilitate the learning, we set the number of edges to $N_{e}=128$ and faces to $N_{f}=128$ . The average inference time per model is $0.5\\,\\mathrm{s}$ per model. ", "page_idx": 6}, {"type": "text", "text": "5.2 Classification and Segmentation Evaluation ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we evaluate the results of SpelsNet on the per-point classification and segmentation tasks against state-of-the-art methods. In this context, only the face type output $\\mathbf{T}_{p}$ and face segment output $\\mathbf{W}_{f}$ from the SpelsNetsp module are considered. ", "page_idx": 6}, {"type": "text", "text": "Baselines: The results are compared to state-of-the-art methods, namely, ParSeNet Sharma et al. [2020], HPNet Yan et al. [2021] on patches and PrimitiveNet Huang et al. [2021] on surface patches and boundary. For the first two methods we use the checkpoints and datasets, provided by the authors. PrimitiveNet does not provide full training and testing data, thus it was retrained on ABCParts-VEF, The results are obtained on the same test set containing input point clouds with normal vectors. ", "page_idx": 6}, {"type": "text", "text": "Test-time Augmentation: When evaluating Scan-to-Brep in the context of reverse engineering, it is crucial to consider that 3D scans or 3D reconstructions from methods like Multi-View Stereo Seitz et al. [2006] or Nerfs Mildenhall et al. [2021] often lack the alignment to standard axes found in CAD designs. Therefore, in addition to the usual assessment using aligned point clouds $(w/o\\,a u g)$ , evaluating performance under random input rotations $\\left(w/a u g\\right)$ is a key indicator of how well the method generalizes to real-world, unaligned data. Typical artifacts in 3D scans (e.g. noise, missing parts and details smoothing) are well represented in CC3D dataset Cherenkova et al. [2020]. ", "page_idx": 6}, {"type": "text", "text": "Metrics: We evaluate the per-point classification and segmentation using the same metrics as in Huang et al. [2021], Yan et al. [2021], Sharma et al. [2020]. These include mean type IoU denoted as tIoU and mean segmentation IoU denoted as $s I o U$ . More details are in the supplementary. ", "page_idx": 6}, {"type": "text", "text": "Results: Table 1 summarizes the quantitative evaluation results on the ABCParts-VEF and CC3DVEF test sets. Clearly, the results demonstrate that all methods have learnt the dataset bias to a different extent. Such, in the presence of unconventional alignment of input point cloud the performance of ParSeNet Sharma et al. [2020] and HPNet Yan et al. [2021] drops significantly. Contrary, PrimitiveNet Huang et al. [2021] and SpelsNet demonstrate more stable results under augmentation by rotation. Our method performs superior in terms of segmentation metrics, and more evidently, in primitive types prediction. Visual results in Figure 3 depict the curves, patches segments along with their type for GT data, and the predictions of our SpelsNet and PrimitiveNet. More qualitative results on the CC3D-VEF dataset can be found in the supplementary materials. ", "page_idx": 6}, {"type": "text", "text": "5.3 Topology Evaluation ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we evaluate the topology predictions of SpelsNetand provide a comparison with ComplexGen Guo et al. [2022] method. Both SpelsNet and ComplexGen leverage topology for ", "page_idx": 6}, {"type": "image", "img_path": "Ad3PzTuqIq/tmp/4cd9e2a93ca1337cd958567a712ae726045c6aecc892e13adeb0c95b69204318.jpg", "img_caption": ["Figure 3: Visual results of comparisons on ABCParts-VEF for PrimitiveNet and our SpelsNet. From-left-to-right: input point cloud, face types $(\\mathbf{T}_{f})$ and segmentation $(\\mathbf{W}_{f})$ , edge types $(\\mathbf{T}_{e})$ and segmentation $(\\mathbf{W}_{e})$ . "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "B-Rep reconstruction, but with distinct methodologies. At a high level of description, ComplexGen generates parametric curves and surfaces that correspond to B-Rep elements, along with their topological relationships (vertices, edges, face connectivity) represented by adjacency matrices. Topology prediction is compared against ground truth for matched elements, and further topological optimization ensures a valid B-Rep structure. SpelsNet decomposes the input point cloud based on per-point labels obtained from nearest B-Rep elements as the ground truth. It uses B-Rep element connectivity for segmentation supervision at point-level, constructing point-to-B-Rep adjacency. The core idea of SpelsNet is to exploit GCNs to capture relationships between B-Rep elements based on this B-Rep adjacency reformulation directly within a point cloud data. The topological module, SpelsNetvef provides additional supervision that leads to an improvement in the results. A visual comparison of predictions for both methods is illustrated in Figure 4. ", "page_idx": 7}, {"type": "text", "text": "Baselines: To the best of our knowledge, SpelsNet is the first end-to-end trainable network that predicts the topological connectivity of a B-Rep given an input point-cloud. Nevertheless, ComplexGen Guo et al. [2022] is another model that offers similar outputs and can be used as a baseline for the curve and patch type and B-Rep topology predictions. ", "page_idx": 7}, {"type": "text", "text": "Metrics: The evaluation of the topological predictions is done using similar metrics to the ones described in Guo et al. [2022]. As in Guo et al. [2022], we compute the type accuracy for both edges and faces using the predictions $\\tilde{\\mathbf{T}}_{e}$ and $\\tilde{\\mathbf{T}}_{f}$ of the GCN. To evaluate the prediction of the topology, we consider the face-edge connectivity in $L\\mathbf{A}\\mathbf{R}^{b r e p}$ using the characteristic matrix $\\mathbf{M}^{\\prime}\\in\\{\\mathbf{0},\\mathbf{1}\\}^{N_{f}\\times N_{e}}$ The value of $\\mathbf{M}^{\\prime}[i,k]$ is 1 if the face $f_{i}$ is bounded by the edge $e_{k}$ , and 0 otherwise. This characteristic ", "page_idx": 7}, {"type": "table", "img_path": "Ad3PzTuqIq/tmp/a237976f9d70aeb8f9f5a248cfd6a0b8cb6512f3a05cf541cbeb37791b4958fa.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Table 1: Evaluation results on face type and segmentation for the ABCParts-VEF and CC3D-VEF datasets. The metrics are averaged over 5 runs with different random seeds. ", "page_idx": 7}, {"type": "image", "img_path": "Ad3PzTuqIq/tmp/aecedfecae107cbb40ed054b1cf9021cd0530e8a2b35241876cc0a53d8e13ac8.jpg", "img_caption": ["Figure 4: Qualitative results for face and edge Table 2: Evaluation of topology results on segmentation for ComplexGen and our method. ABCParts-VEF dataset. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "Ad3PzTuqIq/tmp/6bcffd91ba3c071b4cf3ade1b7b7d22df440b8e167f7fe1769991adf0aecb38c.jpg", "img_caption": [], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "matrix can be easily computed from the predictions of SpelsNet as $\\mathbf{M}^{\\prime}=\\mathbf{M}_{2}^{p}\\mathbf{M_{1}^{p}}^{T}$ . Note that unlike ComplexGen, we do not need to compute the matching pairs within each group of primitive elements as it is inherently defined by our representation. We define the error $t_{f e}$ of the predicted topological structure $\\tilde{{\\bf M}^{\\prime}}$ with respect to the ground truth matrix $\\mathbf{M}^{\\prime}$ as ", "page_idx": 8}, {"type": "equation", "text": "$$\nt_{f e}=\\frac{1}{N_{f}N_{e}}\\sum_{i\\in N_{f},j\\in N_{e}}\\left|\\mathbf{M}^{\\prime}(i,j)-\\tilde{\\mathbf{M}}^{\\prime}(i,j)\\right|.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "It is important to acknowledge the implementation differences between ComplexGen and SpelsNet, which prevent a direct comparison using the same sIoU and tIoU metrics. For a fair assessment, we obtained per-point segmentation and type labels from ComplexGen predictions, transferring them to the original point cloud using a nearest neighbor approach. This allows us to compute the same metrics on the same data, with results presented in Table 1. These metrics were not presented in the original ComplexGen work due to the aforementioned implementation differences. Metrics presented in Table 2 are aligned with our SpelsNet approach. ", "page_idx": 8}, {"type": "text", "text": "Results: The results are reported in Table 2. SpelsNet achieves superior performance in its topology reconstruction as well as type prediction. The examples with segmentation results on faces in Figure 4 were obtained from unaugmented results publicly shared by the authors of ComplexGen. The discontinuity artifacts in the B-Rep reflect the higher topological error compared to our method. ", "page_idx": 8}, {"type": "text", "text": "5.4 Ablation Study ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In order to demonstrate the advantage of the joint learning of the SpelsNetsp and SpelsNetvef module, we conduct the following experiment. Three different SpelsNet models are trained: 1) $\\mathrm{SpelsNet}_{p}^{s p}$ : SpelsNet without the SpelsNetvef module and only point coordinates as input features $[d_{p}=3]$ :l salNl et $\\mathfrak{t}_{p n}^{s p}$ c: $\\mathrm{SpelsNet}_{p}^{s p}$ ofw iStphe lasdNdeetd  wpiotihn tp onionrt mcoalosr tdio ntahtee si napnudt  nfoeratmuarless  a $[d_{p}=6]$ )e aatnudr e3s). $\\mathrm{SpelsNet}_{p n}^{s p+v e f}$ Moreover, the test data is augmented with random rotation $(w/a u g)$ . The results are shown in Table 3. Adding the point normal slightly increases the segmentation results for both edges and types. These segmentation results are further increased by the joint supervision of the two modules of the network. ", "page_idx": 8}, {"type": "text", "text": "Voxel resolution sensitivity: The input point cloud, $\\mathbf{P}$ , is discretized into a voxel grid with a quantization size $\\rho$ . A default value of $\\rho=0.01$ was chosen, balancing model training time and geometric detail resolution on the ABCParts dataset. To investigate resolution sensitivity, the model was evaluated on test data quantized at levels $2\\rho$ and ${\\frac{1}{2}}\\rho$ , with all other settings held unchanged. The results, summarized in Table 4 and Figure 5, indicate the model\u2019s sensitivity to the input resolution. Furthermore, we show that the robustness could be enhanced by using a dynamic resolution with respect to adequate selection of voxel density, $\\psi$ (the average number of points per occupied voxel), during testing. For our backbone, an optimal voxel resolution corresponds to a voxel density $\\psi$ of $4-6$ points per voxel. This improves testing metrics compared to a fixed resolution, without retraining the model. Future work could explore dynamic resolution selection strategies to further enhance the model\u2019s adaptability to varying input data during training. ", "page_idx": 8}, {"type": "image", "img_path": "Ad3PzTuqIq/tmp/7ead9c4f1412f10dfaaa782646789e2891935a3c1a69b855cad2557e3136b970.jpg", "img_caption": ["Figure 5: SpelsNet results on real scanned data with respect to various voxel quantization size ${\\rho}{=}0.01$ (default), 0.02, 0.005. "], "img_footnote": [], "page_idx": 9}, {"type": "image", "img_path": "Ad3PzTuqIq/tmp/3c42373d8dade6e481418bf1d9a536993cf68ab5862b801566350b7a14484aa6.jpg", "img_caption": ["Table 4: SpelsNet $s p{+}v e f$ ablation studies results on the ABCParts dataset with respect to voxel quantization size $\\rho$ and voxel density $\\psi$ . "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "5.5 Discussions and Limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The degradation of performance under rotation can be evaluated as a negative outcome. We argue, that uncanonical alignment, specific to real scanned data, offers a way to effectively enlarge the training data and to generalize to unseen data. The CC3D dataset was chosen to demonstrate the model\u2019s generalization and robustness to realistic data as it holds a large-scale collection of 3D CAD models and their corresponding 3D scans, exhibiting realistic artifacts like missing parts, surface noise, and smoothed details. The sparse spatial representation allows us to support the input data of dynamic resolutions. While the spatial and topological components of SpelsNet ultimately produce equivalent B-Rep predictions, the topological module was initially introduced for supervising B-Rep elements segmentation. Various other attributes available from B-Rep can be helpful for topological supervision, including sharpness of edges, connectivity degrees, surface area, convexity/concavity of faces etc. In our experiments, the spatial module\u2019s predictions outperform the topological module. This could be attributed to insufficient capacity of the GCN network. One of the major limiting factors of our method in terms of learning a highly varied graph-structure is the choice of characteristic matrix of a fixed size, implying that this size should be adjusted according to the data distribution for each new dataset. As future directions, we acknowledge several experiments that could be a part of further SpelsNet performance improvement: (1) Training on CC3D data and validating on other datasets to estimate the effect of different data augmentations and artifacts; (2) The thorough investigation of a spatial backbone choice; (3) The GNN powered by Transformers Kim et al. [2022] is a promising direction to enhance the topological supervision part. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We present a novel learning approach to surface primitive elements segmentation along with their types prediction from a point cloud data. Our design allows to incorporate the features of traditional spatial learning in 3D into direct topology supervision by its underlying graph structure via Graph Neural Network. We extend the point cloud data with its corresponding B-Rep structure by an efficient reformulation employing Linear Algebraic Representations. This design allows to compose spatial convolutional and graph convolutional networks in one end-to-end trainable neural architecture. Through extensive experiments on large datasets, we uncover the effectiveness of such neural composition in terms of superior segmentation performance and primitive type prediction. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgement ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "The present project is supported by Artec3D, and the National Research Fund, Luxembourg under the BRIDGES2021/IS/16849599/FREE-3D, IF/17052459/CASCADES. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "R. J. Abella, J. M. Daschbach, and R. J. McNichols. Reverse engineering industrial applications. Computers & industrial engineering, 26(2):381\u2013385, 1994.   \nR. B\u00e9ni\u00e8re, G. Subsol, G. Gesqui\u00e8re, F. Le Breton, and W. Puech. A comprehensive process of reverse engineering from 3d meshes to cad models. Computer-Aided Design, 45(11):1382\u20131393, 2013.   \nK. Cherenkova, D. Aouada, and G. Gusev. Pvdeconv: Point-voxel deconvolution for autoencoding cad construction in 3d. In 2020 IEEE International Conference on Image Processing (ICIP), pages 2741\u20132745. IEEE, 2020.   \nK. Cherenkova, E. Dupont, A. Kacem, I. Arzhannikov, G. Gusev, and D. Aouada. Sepicnet: Sharp edges recovery by parametric inference of curves in 3d shapes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2726\u20132734, 2023.   \nC. Choy, J. Gwak, and S. Savarese. 4d spatio-temporal convnets: Minkowski convolutional neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3075\u20133084, 2019.   \nA. DiCarlo, A. Paoluzzi, and V. Shapiro. Linear algebraic representation for topological structures. Computer-Aided Design, 46:269\u2013274, 2014.   \nR. Fu, C. Wen, Q. Li, X. Xiao, and P. Alliez. Bpnet: B\u00e9zier primitive segmentation on 3d point clouds. In Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence. International Joint Conferences on Artificial Intelligence Organization, Aug. 2023. doi: 10.24963/ ijcai.2023/84. URL http://dx.doi.org/10.24963/ijcai.2023/84.   \nH. Guo, S. Liu, H. Pan, Y. Liu, X. Tong, and B. Guo. Complexgen: Cad reconstruction by b-rep chain complex generation. ACM Transactions on Graphics (TOG), 41(4):1\u201318, 2022.   \nA. Hatcher. Algebraic topology. Cambridge University Press, Cambridge, 2002. ISBN 0-521-79160- X; 0-521-79540-0.   \nJ. Huang, Y. Zhang, and M. Sun. Primitivenet: Primitive instance segmentation with local primitive embedding under adversarial metric. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 15343\u201315353, 2021.   \nB. Jones, D. Hildreth, D. Chen, I. Baran, V. G. Kim, and A. Schulz. Automate: A dataset and learning approach for automatic mating of cad assemblies. 40(6), dec 2021. ISSN 0730-0301. doi: 10.1145/3478513.3480562. URL https://doi.org/10.1145/3478513.3480562.   \nJ. Kim, T. D. Nguyen, S. Min, S. Cho, M. Lee, H. Lee, and S. Hong. Pure transformers are powerful graph learners. arXiv, abs/2207.02505, 2022. URL https://arxiv.org/abs/2207.02505.   \nJ. G. Lambourne, K. D. Willis, P. Jayaraman, A. Sanghi, P. Meltzer, and H. Shayani. Brepnet: A topological message passing system for solid models. In 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 12768\u201312777, Los Alamitos, CA, USA, jun 2021. IEEE Computer Society. doi: 10.1109/CVPR46437.2021.01258. URL https: //doi.ieeecomputersociety.org/10.1109/CVPR46437.2021.01258.   \nL. Li, M. Sung, A. Dubrovina, L. Yi, and L. J. Guibas. Supervised ftiting of geometric primitives to 3d point clouds. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2652\u20132660, 2019.   \nY. Li, S. Liu, X. Yang, J. Guo, J. Guo, and Y. Guo. Surface and edge detection for primitive ftiting of point clouds. In ACM SIGGRAPH 2023 conference proceedings, pages 1\u201310, 2023.   \nX. Liu, X. Xu, A. Rao, C. Gan, and L. Yi. Autogpart: Intermediate supervision search for generalizable 3d part segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 11624\u201311634, June 2022.   \nY. Liu, A. Obukhov, J. D. Wegner, and K. Schindler. Point2cad: Reverse engineering cad models from 3d point clouds. arXiv preprint arXiv:2312.04962, 2023.   \nA. Matveev, R. Rakhimov, A. Artemov, G. Bobrovskikh, V. Egiazarian, E. Bogomolov, D. Panozzo, D. Zorin, and E. Burnaev. Def: Deep estimation of sharp geometric features in 3d shapes. ACM Transactions on Graphics, 41(4), 2022.   \nL. McInnes, J. Healy, and S. Astels. hdbscan: Hierarchical density based clustering. Journal of Open Source Software, 2(11):205, 2017.   \nB. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoorthi, and R. Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65(1): 99\u2013106, 2021.   \nM. Mortenson. Geometric Modeling. G - Reference,Information and Interdisciplinary Subjects Series. Industrial Press, 2006. ISBN 9780831132989. URL https://books.google.lu/books?id= jPc_AQAAIAAJ.   \nR. Schnabel, R. Wahl, and R. Klein. Efficient ransac for point-cloud shape detection. In Computer graphics forum, volume 26, pages 214\u2013226. Wiley Online Library, 2007.   \nS. M. Seitz, B. Curless, J. Diebel, D. Scharstein, and R. Szeliski. A comparison and evaluation of multi-view stereo reconstruction algorithms. In 2006 IEEE computer society conference on computer vision and pattern recognition (CVPR\u201906), volume 1, pages 519\u2013528. IEEE, 2006.   \nJ. Shah and M. M\u00e4ntyl\u00e4. Parametric and Feature-Based CAD/CAM: Concepts, Techniques, and Applications. A Wiley-Interscience publication. Wiley, 1995. ISBN 9780471002147. URL https://books.google.lu/books?id $=$ 8W0E9eK2raMC.   \nG. Sharma, D. Liu, S. Maji, E. Kalogerakis, S. Chaudhuri, and R. M\u02c7ech. Parsenet: A parametric surface fitting network for 3d point clouds. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part VII 16, pages 261\u2013276. Springer, 2020.   \nD. Smirnov and J. Solomon. HodgeNet: Learning spectral geometry on triangle meshes. SIGGRAPH, 2021.   \nT. Varady, R. R. Martin, and J. Cox. Reverse engineering of geometric models\u2014an introduction. Computer-aided design, 29(4):255\u2013268, 1997.   \nY. Wang, Y. Sun, Z. Liu, S. E. Sarma, M. M. Bronstein, and J. M. Solomon. Dynamic graph cnn for learning on point clouds. ACM Trans. Graph., 38(5), oct 2019. ISSN 0730-0301. doi: 10.1145/3326362. URL https://doi.org/10.1145/3326362.   \nJ. Wu, H. Yu, W. Yang, and G.-S. Xia. Quadricsnet: Learning concise representation for geometric primitives in point clouds. arXiv preprint arXiv:2309.14211, 2023.   \nS. Yan, Z. Yang, C. Ma, H. Huang, E. Vouga, and Q. Huang. Hpnet: Deep primitive segmentation using hybrid representations. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2753\u20132762, 2021.   \nJ. Zhou, G. Cui, S. Hu, Z. Zhang, C. Yang, Z. Liu, L. Wang, C. Li, and M. Sun. Graph neural networks: A review of methods and applications. AI Open, 1:57\u201381, 2020. ISSN 2666-6510. doi: https://doi.org/10.1016/j.aiopen.2021.01.001. URL https://www.sciencedirect.com/ science/article/pii/S2666651021000012.   \nX. Zhu, D. Du, W. Chen, Z. Zhao, Y. Nie, and X. Han. Nerve: Neural volumetric edges for parametric curve extraction from point cloud. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13601\u201313610, 2023. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 12}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 12}, {"type": "text", "text": "Justification: The main claims in the abstract and introduction accurately reflect the paper\u2019s contributions and scope. This is an important aspect of evaluating the quality and reliability of any research paper. ", "page_idx": 12}, {"type": "text", "text": "Guidelines: ", "page_idx": 12}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 12}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 12}, {"type": "text", "text": "Justification: The dedicated section 5.5 provides this discussion. In addition, we mention future research directions there. ", "page_idx": 12}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 12}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 12}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 13}, {"type": "text", "text": "Justification: There are no theoretical claims made in the paper. Guidelines: ", "page_idx": 13}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 13}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 13}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 13}, {"type": "text", "text": "Justification: The paper provides a detailed description of the experimental setup, including hardware, software, datasets used and optimization parameters and configurations in 5. The metrics used to evaluate the results clearly defined in their mathematical form and explained in 5. The reproducibility of the results is supported by numerous experiments reported in 5 and the ablation study 5. ", "page_idx": 13}, {"type": "text", "text": "Guidelines: ", "page_idx": 13}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 13}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 14}, {"type": "text", "text": "Answer: [No] ", "page_idx": 14}, {"type": "text", "text": "Justification: The paper uses the data which is publicly available. The scripts to preprocess the data to obtain additional information used in our work will be made publicly available. The code with the network architecture and training can not be currently released under an open-source license that allows others to use, modify, and distribute it due to specifics of the industrial collaboration in the scope of which the work has been done. The authors described all the details required to reproduce the results within the paper itself and in accompanying supplementary material. ", "page_idx": 14}, {"type": "text", "text": "Guidelines: ", "page_idx": 14}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 14}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 14}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Justification: The paper provides sufficient details about the training and testing procedures, metrics, hyperparameters of the network, optimizer choice for another researcher to potentially reproduce the results. ", "page_idx": 14}, {"type": "text", "text": "Guidelines: ", "page_idx": 14}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 14}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 14}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 14}, {"type": "text", "text": "Justification: The only error bar in form of standard deviation of the mean is reported in Table 1 of supplementary to reflect the elements statistics in two datasets. ", "page_idx": 14}, {"type": "text", "text": "Guidelines: ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 15}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 15}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 15}, {"type": "text", "text": "Justification: The paper clearly states the computer resources used for the experiment and the time of training the network in 5.1. ", "page_idx": 15}, {"type": "text", "text": "Guidelines: ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 15}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 15}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Justification: The authors are fully aware of the NeurIPS Code of Ethics and follow it responsibly. ", "page_idx": 15}, {"type": "text", "text": "Guidelines: ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 15}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 15}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: The paper tries to adequately addresses the potential positive societal impact in the introduction to the problem and its motivation. The authors do not foresee any negative implications of their work. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 16}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 16}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Justification: The authors do not foresee any potential risks associated with the release of their method, data or model. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 16}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: All assets including data and codes used in the work are clearly cited in the paper. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 17}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: The extended versions of two public datasets are clearly described in the paper. The scripts to generate these updated versions from publicly available assets will be released with sufficient documentation. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 17}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 17}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 17}, {"type": "text", "text": "Justification: No human subjects and crouwdsourcing were involved in the work. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 17}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 17}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 18}, {"type": "text", "text": "Justification: No human subjects and crouwdsourcing were involved in the work. Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 18}]