[{"figure_path": "FqWyzyErVT/tables/tables_8_1.jpg", "caption": "Table 1: Root Mean Squared Error (RMSE) on real-world two-party fuzzy VFL datasets", "description": "This table presents the results of the two-party fuzzy VFL experiment without any privacy mechanisms. It compares the performance of FeT against three baselines (Solo, Top1Sim, FedSim) across three real-world datasets (house, bike, hdb) in terms of Root Mean Squared Error (RMSE).  The results show that FeT consistently outperforms other methods.", "section": "7.2 Performance"}, {"figure_path": "FqWyzyErVT/tables/tables_15_1.jpg", "caption": "Table 2: Basic information of real-world VFL datasets", "description": "This table presents the key characteristics of three real-world datasets used in the paper's experiments to evaluate the performance of the Federated Transformer (FeT) in the context of Vertical Federated Learning (VFL).  For each dataset, the table provides the number of samples and features in both the primary party's dataset (which includes labels) and the secondary party's dataset. It also indicates the relevant references for each dataset, the number of dimensions for the identifiers used to link the datasets across parties, and the type of task (regression in this case).", "section": "7.1 Experimental Settings"}, {"figure_path": "FqWyzyErVT/tables/tables_16_1.jpg", "caption": "Table 3: Effects of Dynamic Masking (DM) and Positional Encoding (PE) on FeT Performance", "description": "This table presents the results of an ablation study evaluating the impact of dynamic masking (DM) and positional encoding (PE) on the performance of the Federated Transformer (FeT) model.  It shows the RMSE (Root Mean Squared Error) for regression tasks on three datasets (house, bike, hdb) and accuracy for classification tasks on two datasets (MNIST, gisette). By comparing the performance of FeT with and without DM and PE, the table demonstrates the individual contributions of these components to the overall model performance.", "section": "C Ablation Study"}, {"figure_path": "FqWyzyErVT/tables/tables_16_2.jpg", "caption": "Table 4: Effect of Party Dropout Rate on FeT Performance", "description": "This table presents the results of an ablation study evaluating the impact of the party dropout rate on the performance of the Federated Transformer (FeT) model.  The study uses two datasets, gisette and MNIST, and varies the party dropout rate from 0 to 1.0.  The results show the accuracy achieved on each dataset at each dropout rate, demonstrating the effect of this technique on model generalization and performance.", "section": "C Ablation Study"}, {"figure_path": "FqWyzyErVT/tables/tables_17_1.jpg", "caption": "Table 5: Ablation study for accuracy with different PE Average Frequency", "description": "This table presents the results of an ablation study on the effect of positional encoding (PE) averaging frequency on the model's accuracy.  The study is conducted on two datasets (gisette and MNIST) with varying numbers of parties (2, 5, 20, 50). The table shows the accuracy achieved with different PE averaging frequencies (0, 1, 2, 3, 5, 10), allowing for the analysis of how this parameter affects performance under different conditions.", "section": "C Ablation Study"}, {"figure_path": "FqWyzyErVT/tables/tables_18_1.jpg", "caption": "Table 6: Performance of FeT under Exact Linkage", "description": "This table presents the performance comparison of FeT against two baselines (Solo and Top1Sim) under the setting of exact linkage, where the keys are precisely matched.  It shows the accuracy achieved by each algorithm on two datasets (gisette and MNIST) with different numbers of parties (5 and 10).  The results highlight the performance trade-offs between different methods under perfect data alignment, which is a scenario not typically encountered in real-world fuzzy VFL applications. ", "section": "7.2 Performance"}, {"figure_path": "FqWyzyErVT/tables/tables_18_2.jpg", "caption": "Table 7: Running time of summation with and without MPC in seconds", "description": "This table presents the computational efficiency comparison between standard addition and multi-party computation (MPC) addition for aggregating high-dimensional vectors.  The results show the running time (in seconds) for different numbers of parties (2, 5, 10, 20, 50, 100), using both standard summation and MPC-based summation. The 'Overhead' column indicates the additional time cost introduced by using MPC for the aggregation task.", "section": "E Efficiency"}, {"figure_path": "FqWyzyErVT/tables/tables_19_1.jpg", "caption": "Table 8: Training efficiency of FeT on RTX3090. PE: positional encoding; DM: dynamic masking.", "description": "This table shows a comparison of training efficiency and GPU memory usage for different model configurations.  It compares the baseline FedSim model with several versions of the Federated Transformer (FeT) model, each with different components enabled or disabled (positional encoding and dynamic masking). The table details the number of parameters, training time per epoch, and peak GPU memory consumption for each model and dataset (house, bike, hdb).", "section": "7.1 Experimental Settings"}, {"figure_path": "FqWyzyErVT/tables/tables_21_1.jpg", "caption": "Table 2: Basic information of real-world VFL datasets", "description": "This table presents the details of the real-world datasets used in the paper's experiments for two-party fuzzy VFL.  It lists the dataset name, the number of samples and features for both the primary (labeled) and secondary (unlabeled) parties, the reference(s) where more information can be found about each dataset, the number of dimensions of the identifiers used to link the datasets, the type of identifier (float), and finally, the type of task (regression).", "section": "7.1 Experimental Settings"}]