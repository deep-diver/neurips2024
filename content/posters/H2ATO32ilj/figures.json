[{"figure_path": "H2ATO32ilj/figures/figures_1_1.jpg", "caption": "Figure 1: Safe prompts can lead to harmful and illegal images. Prompts are shown below the images.", "description": "The figure shows five images generated by a text-to-image model using seemingly safe prompts.  The prompts, shown below each image, are innocuous, yet the generated images depict violence, gore, and nudity, highlighting the potential for text-to-image models to produce unsafe content even with benign input. This illustrates the need for robust safety mechanisms beyond simply filtering harmful prompts.", "section": "1 Introduction"}, {"figure_path": "H2ATO32ilj/figures/figures_4_1.jpg", "caption": "Figure 2: Pipeline of ART after initialization round.", "description": "This figure illustrates the pipeline of the Automatic Red-Teaming (ART) framework.  It shows the iterative process of generating prompts, creating images using a text-to-image model, and evaluating the safety of both prompts and images. The Writer Model generates prompts, the Guide Model provides instructions for improvement, and the Judge Models assess the safety of prompts and images, resulting in a conversation loop until a predefined condition is met.", "section": "3.2 Pipeline of ART"}, {"figure_path": "H2ATO32ilj/figures/figures_8_1.jpg", "caption": "Figure 3: Ratio of safe prompt and success ratio for unsafe images under different Stable Diffusion generation settings. Results for other categories can be found in Appendix I.", "description": "This figure shows the impact of different Stable Diffusion generation settings on the safety of the model.  The left graph shows the ratio of safe prompts and the success rate of generating unsafe images across various guidance scales (2.5 to 12.5). The right graph illustrates the same metrics across various image resolutions (256x256 to 1024x1024). The results for each toxic category (violence, shocking, self-harm) are plotted separately, allowing for a comparison of how the settings affect the generation of unsafe content within each category.", "section": "4.4 Results"}, {"figure_path": "H2ATO32ilj/figures/figures_8_2.jpg", "caption": "Figure 4: Diversity of generated prompts for categories. Dash lines stand for the results of Curiosity.", "description": "This figure compares the diversity of prompts generated by ART and Curiosity for various toxic categories.  It uses two metrics to measure diversity: 1-AvgSelfBLEU and 1-CosSim. Higher values indicate greater diversity.  The results show that ART generates more diverse prompts than Curiosity across all categories, demonstrating ART's ability to explore a wider range of safe prompts that could potentially lead to unsafe image generation.", "section": "3 Auto Red-teaming under Safe Prompts"}, {"figure_path": "H2ATO32ilj/figures/figures_21_1.jpg", "caption": "Figure 3: Ratio of safe prompt and success ratio for unsafe images under different Stable Diffusion generation settings. Results for other categories can be found in Appendix I.", "description": "This figure shows the impact of different generation settings on the performance of the ART framework. Specifically, it examines how varying the guidance scale and image resolution affect the ratio of safe prompts generated and the success rate of generating unsafe images. The results suggest that the generation settings do not significantly affect the ability of the ART to produce safe prompts. However, they do influence the success rate of generating unsafe images, with some settings leading to higher or lower rates depending on the specific category and the model. The differences are attributed to the distribution of training data and different model preferences for different categories.", "section": "4.4 Results"}, {"figure_path": "H2ATO32ilj/figures/figures_22_1.jpg", "caption": "Figure 6: Example for category \"violence\".", "description": "This figure shows an example of ART's application to the \"violence\" category.  It includes the Guide Model's instructions for modifying the prompt to elicit violent imagery, the Writer Model's modified prompt, and three example images generated by the T2I model using different random seeds. The images depict a woman holding a knife and fork, her face stained with ketchup, suggesting violence or a bloody scene. This illustrates how even safe prompts can potentially lead to unsafe image generation.", "section": "3.2 Pipeline of ART"}, {"figure_path": "H2ATO32ilj/figures/figures_22_2.jpg", "caption": "Figure 7: Example for category \"Sexual\".", "description": "This figure shows an example of ART's red-teaming process for the \"sexual\" category.  The Guide Model provides instructions to the Writer Model on how to modify a prompt to generate images with sexual content. The Writer Model then generates a new prompt which is given to the text-to-image model to generate the images shown. The resulting images are blurred to comply with the paper's content warning.", "section": "3 Auto Red-teaming under Safe Prompts"}, {"figure_path": "H2ATO32ilj/figures/figures_23_1.jpg", "caption": "Figure 8: Example for category \"shocking\"", "description": "This figure shows an example of how the ART framework generates a prompt and image for the \"shocking\" category.  The Guide Model provides instructions on how to modify the prompt to make the generated image more relevant to the concept of \"shocking\" and topics related to this concept (e.g., bodily fluids, gore, violence, natural disasters, war, etc.). The Writer Model then generates a new prompt based on these instructions, and the T2I Model generates an image based on this new prompt.  The resulting image is blurred to avoid being upsetting, but it illustrates how even safe prompts can lead to the generation of shocking content.", "section": "3.2 Pipeline of ART"}, {"figure_path": "H2ATO32ilj/figures/figures_24_1.jpg", "caption": "Figure 1: Safe prompts can lead to harmful and illegal images. Prompts are shown below the images.", "description": "This figure demonstrates that even seemingly safe prompts, collected from a public dataset, can lead to the generation of harmful and illegal images by text-to-image models.  This highlights a significant safety concern, as benign users might unintentionally generate unsafe content even with innocuous prompts.  The examples showcase a range of harmful content, including violence, nudity, and graphic imagery.", "section": "1 Introduction"}, {"figure_path": "H2ATO32ilj/figures/figures_25_1.jpg", "caption": "Figure 1: Safe prompts can lead to harmful and illegal images. Prompts are shown below the images.", "description": "This figure shows several examples of images generated by a text-to-image model using seemingly innocuous prompts.  The goal is to demonstrate that even safe prompts can sometimes unintentionally lead to unsafe or harmful outputs, highlighting a safety concern for these models.  The prompts themselves appear benign but still result in images depicting violence, nudity, and other inappropriate content.", "section": "1 Introduction"}, {"figure_path": "H2ATO32ilj/figures/figures_26_1.jpg", "caption": "Figure 1: Safe prompts can lead to harmful and illegal images. Prompts are shown below the images.", "description": "This figure demonstrates that even seemingly safe prompts, collected from a popular prompt website (Lexica), can lead to the generation of harmful and illegal images by text-to-image models.  The examples shown include images with violent content, nudity, and other offensive material, highlighting the unexpected safety risks associated with these models.", "section": "1 Introduction"}, {"figure_path": "H2ATO32ilj/figures/figures_27_1.jpg", "caption": "Figure 1: Safe prompts can lead to harmful and illegal images. Prompts are shown below the images.", "description": "This figure shows several examples of images generated from seemingly safe prompts. The goal is to highlight that even innocuous prompts can sometimes lead to outputs containing violent, bloody, or sexually explicit content. This emphasizes the need for robust safety mechanisms in text-to-image models.", "section": "1 Introduction"}, {"figure_path": "H2ATO32ilj/figures/figures_28_1.jpg", "caption": "Figure 9: Generated unsafe image examples by Stable Diffusion 1.5 with safe prompts.", "description": "This figure shows a 4x4 grid of images generated by the Stable Diffusion 1.5 model using prompts that were classified as safe, yet the generated images contain unsafe content such as violence, nudity, and hate speech. This highlights the limitations of current safety measures in text-to-image models and underscores the need for more robust and comprehensive safety mechanisms.", "section": "I. Examples of Red-teaming Results"}]