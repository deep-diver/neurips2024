[{"type": "text", "text": "Are Multiple Instance Learning Algorithms Learnable for Instances? ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Jaeseok Jang and Hyuk-Yoon Kwon ", "page_idx": 0}, {"type": "text", "text": "Graduate School of Data Science, Seoul National University of Science and Technology {jangjs1027, hyukyoon.kwon}@seoultech.ac.kr ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Multiple Instance Learning (MIL) has been increasingly adopted to mitigate the high costs and complexity associated with labeling individual instances, learning instead from bags of instances labeled at the bag level and enabling instance-level labeling. While existing research has primarily focused on the learnability of MIL at the bag level, there is an absence of theoretical exploration to check if a given MIL algorithm is learnable at the instance level. This paper proposes a theoretical framework based on probably approximately correct (PAC) learning theory to assess the instance-level learnability of deep multiple instance learning (Deep MIL) algorithms. Our analysis exposes significant gaps between current Deep MIL algorithms, highlighting the theoretical conditions that must be satisfied by MIL algorithms to ensure instance-level learnability. With these conditions, we interpret the learnability of the representative Deep MIL algorithms and validate them through empirical studies. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The performance of supervised learning models is greatly influenced by the amount of labeled data [1]. While various models utilizing large-scale datasets have achieved excellent performance, the cost and time of labeling have emerged as issues, especially in domains requiring expert knowledge. For example, in the case of pathology images, detailed labeling requires a significant amount of expert time [2]. ", "page_idx": 0}, {"type": "text", "text": "To address these issues, multiple instance learning (MIL) techniques have been introduced. MIL encompasses all methodologies that learn to predict the labels of instances by learning from the labels of bags composed of instances [3, 4, 5, 2, 6, 7, 8, 9, 10]. This approach allows for detecting disease areas in pathology images through whole-image labeling without requiring the labeling of regions within the image. This can significantly reduce overall labeling costs and maximize the efficiency of the method [2, 9]. As illustrated in Figure 1, if at least one instance in a bag is positive, the bag is positive, and if all instances are negative, the bag is negative in MIL. To achieve instance-level learning, it was necessary first to confirm that learning at the bag level could be performed at a high level. Therefore, traditional MIL research focused primarily on the feasibility of learning at the bag level rather than the instance level [11, 12, 13, 14], and some studies validated instance-level learning only for specific algorithms [15]. ", "page_idx": 0}, {"type": "image", "img_path": "cUcvlgkQxP/tmp/579de5a30566ac5939b0315bad0309a976d59a8b27b8cad7985b476bad95932d.jpg", "img_caption": ["Figure 1: The data structure consisting of multi-instances (Blue: Negative, Red: Positive) [16]. "], "img_footnote": [], "page_idx": 0}, {"type": "text", "text": "Recently, with the advancement of deep learning technologies, traditional MIL has evolved into Deep MIL, enabling more effective extraction of features from individual instances and consideration of interactions between instances, leading to significant improvements in prediction performance. Despite these advancements, MIL research still predominantly focuses on learning at the bag level, with a lack of exploration into the feasibility of instance-level learning [10, 17, 18]. ", "page_idx": 0}, {"type": "text", "text": "In this study, we propose a new framework to theoretically validate that Deep MIL can learn at the instance level, overcoming the aforementioned issues. The contributions of this study are summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "1. The proposed theoretical framework derives conclusions about the instance-level learnability of Deep MIL algorithms, assuming that MIL algorithms are learnable at the bag level (Assumption 1).   \n2. Utilizing the probably approximately correct (PAC) learning theory, we divide the hypothesis space of the dataset into two cases: 1) each instance being statistically independent and 2) the general case without any constraints on instance distributions, including statistically dependent instances. We theoretically derive the necessary and sufficient conditions for learnability in each hypothesis space (i.e., Condition 4 and Condition 7).   \n3. By applying the derived conditions to the existing representative types of Deep MIL, we verify their instance-level learnability.   \n4. Through Theorem 10 and Theorem 11, we show that additional information (e.g., positional information of each time point in time series, medical records provided alongside pathology images for better disease diagnosis) beyond the features directly extracted from the original data must act as weights in the independent hypothesis space of each instance. ", "page_idx": 1}, {"type": "text", "text": "This paper is organized as follows. Section 2 defines the problem. Section 3 outlines the conditions for Deep MIL to be instance learnable and provides theoretical proof. Section 4 evaluates existing Deep MIL algorithms and validates the results through experiments. Finally, Section 5 concludes the paper. ", "page_idx": 1}, {"type": "text", "text": "2 Problem Definition ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Notation 1 (MIL domain spaces) Given the feature space for an instance $\\mathcal{X}_{i n s t_{i}}\\subset\\mathbb{R}^{d}$ and the feature space for $N$ instances $\\mathcal{X}:=\\{\\mathcal{X}_{i n s t_{l}},\\mathcal{X}_{i n s t_{2}},...,\\mathcal{X}_{i n s t_{N}}\\}$ , along with the label space $y:=$ $\\{1,...,k\\}$ , we can define the joint distribution $D_{X Y}$ on $\\mathcal X\\times\\mathcal Y$ . Here, $X_{i n s t_{i}}~\\in~\\mathcal{X}_{i n s t_{i}}$ , $X\\ :=$ $(X_{i n s t_{l}},\\bar{X}_{i n s t_{2}},...,X_{i n s t_{N}})\\;\\in\\;\\chi$ , and $Y\\,\\in\\,\\mathcal{Y}$ are random variables. The MIL instance domain $D_{X_{i n s t_{i}}Y}$ represents the joint distribution of individual instances and their respective labels. The MIL bag domain $D_{X Y}$ is the joint probability distribution composed of multiple instance domains $D_{X_{i n s t_{i}}Y}$ . Here, the term \u201cdomain\u201d refers to these joint distributions. ", "page_idx": 1}, {"type": "text", "text": "Based on Notation 1, the MIL Problem can be defined as in Definition 1. ", "page_idx": 1}, {"type": "text", "text": "Definition 1 (MIL problem) Given a training dataset $S:=\\{((x_{1}^{1},...,x_{n}^{1}),y^{1}),...,((x_{1}^{m},...,x_{n}^{m})$ $,y^{m})\\}$ drawn IID from the joint distribution $D_{X Y}$ , the goal of MIL is to learn a classifier $f_{b a g}$ for the data and $(f_{i n s t_{1}},...,f_{i n s t_{n}})$ such that for any arbitrary bag random variable $x:=(x_{1},...,x_{n})$ drawn from the marginal distribution $D_{X}{\\mathrm{:~}}I$ ) $I t$ should be able to classify the class corresponding to the bag x. 2) It should be able to classify the class corresponding to the $i^{t h}$ instance $x_{i}$ of the bag. Here, a random sample $(x,y):=((x_{1},...,x_{n}),y)$ of the training data represents a bag consisting of $n$ instances drawn from $D_{X Y}$ , with a total of m bags. The training data for the $i^{t h}$ instance is given by $S_{i n s t_{i}}:=\\{(x_{i}^{1},\\stackrel{\\cdot}{y}^{1}),\\dots(x_{i}^{m},y^{m})\\}$ . \u25a1 ", "page_idx": 1}, {"type": "text", "text": "The MIL problem defined in Definition 1 is performed within the following MIL hypothesis spaces. ", "page_idx": 1}, {"type": "text", "text": "Notation 2 (MIL hypothesis spaces) $^{\\,l}$ ) The bag hypothesis space $\\mathcal{H}_{b a g}\\subset\\{h_{b a g}:X\\rightarrow Y\\}$ is a set of hypothesis functions $h_{b a g}$ that classify bags with the correct labels. Here, $h_{b a g}=f(X)$ , where $f\\in\\mathcal{F}_{b a g}$ . 2) The $i^{t h}$ instance hypothesis space $\\mathcal{H}_{i n s t_{i}}\\subset\\{h_{i n s t_{i}}:X_{i n s t_{i}}\\rightarrow Y\\}$ is a set of hypothesis functions $h_{i n s t_{i}}$ that classify the $i^{t h}$ instance with the correct labels using the feature space of the $i^{t h}$ instance. Here, $h_{i n s t_{i}}=f(X_{i n s t_{i}})$ , where $f\\in\\mathcal{F}_{i n s t_{i}}$ . Here, ${\\mathcal{F}}_{b a g}$ and $\\mathcal{F}_{i n s t_{i}}$ are sets of functions that take a bag or the $i^{t h}$ instance as input and generate the corresponding labels. ", "page_idx": 1}, {"type": "text", "text": "According to Notations 1, 2, and Definition 1, the risk for a bag in MIL is defined as in Definitions 2 and 3. ", "page_idx": 1}, {"type": "text", "text": "Definition 2 (Bag Risk) If the loss for a bag in the MIL algorithm is given by $\\ell_{b a g}(h_{b a g}(x),y),$ , the risk for a bag is defined as follows: ", "page_idx": 1}, {"type": "equation", "text": "$$\nR_{b a g}=\\mathbb{E}_{(x,y)\\sim D_{X Y}}\\ell_{b a g}(h_{b a g}(x),y)\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Definition 3 (Instance Risk) If the loss for the $i^{t h}$ instance in the MIL algorithm is given by $\\ell_{i n s t_{i}}(h_{i n s t_{i}}(x_{i}),y)$ , the risk for the $i^{t h}$ instance is defined as follows: ", "page_idx": 2}, {"type": "equation", "text": "$$\nR_{i n s t_{i}}=\\mathbb{E}_{(x_{i},y)\\sim D_{X_{i n s t_{i}}Y}}\\ell_{i n s t_{i}}(h_{i n s t_{i}}(x_{j}),y)\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Based on the definitions of bag risk and instance risk in Definition 2 and 3, the learnability for bags and instances can be defined as in Definition 4 and Definition 5. ", "page_idx": 2}, {"type": "text", "text": "Definition 4 (PAC Learnability of Bag) Given the domain space $\\mathcal{D}_{X Y}$ and the bag hypothesis space $\\mathcal{H}_{b a g}\\subset\\{h_{b a g}:\\mathcal{X}\\to\\mathcal{Y}\\},$ , the MIL algorithm $A$ is said to be learnable on $\\mathcal{H}_{b a g}$ with respect to $\\mathcal{D}_{X Y}$ $i f,$ for all domains $D_{X Y}\\in{\\mathcal{D}}_{X Y}$ , the following condition is satisfied: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbb{P}_{S\\sim D_{X Y}^{m}}[|R_{b a g}(A(S))-\\operatorname*{inf}_{h\\in\\mathcal{H}_{b a g}}R_{b a g}(h)|\\leq\\epsilon]\\geq1-\\delta\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Here, \u03f5 represents the acceptable error between the learning algorithm and the actual optimal hypothesis. In contrast, $\\delta$ represents the confidence level that the learning algorithm will return accurate results within a certain error range. Both $\\epsilon$ and $\\delta$ have a range of $0<\\epsilon,\\delta<1$ . ", "page_idx": 2}, {"type": "text", "text": "Definition 5 (PAC Learnability of Instance) When given the $i^{t h}$ instance domain space $\\mathcal{D}_{X_{i n s t_{i}}\\;Y}$ and instance hypothesis space $\\mathcal{H}_{i n s t_{i}}\\subset\\{h_{i n s t_{i}}:X\\rightarrow Y\\}$ , the algorithm $A$ is said to be learnable over $\\mathcal{H}_{i n s t_{i}}$ from $\\mathcal{D}_{X_{i n s t_{i}}}\\,Y$ if it satisfies the following for all domains $D_{X_{i n s t_{i}}Y}\\in\\mathcal{D}_{X_{i n s t_{i}}}{}_{Y}$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbb{P}_{S_{i n s t_{i}}\\sim D_{{X_{i n s t_{i}}}}^{m}}[\\left|R_{i n s t_{i}}(A(S_{i n s t_{i}}))-\\operatorname*{inf}_{h\\in\\mathcal{H}_{i n s t_{i}}}R_{i n s t_{i}}(h)\\right|\\leq\\epsilon]\\geq1-\\delta\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Based on the relationship between Definitions 4 and 5, according to Theorem 1, if a MIL algorithm is not learnable with respect to bags, it is not learnable with respect to instances. ", "page_idx": 2}, {"type": "text", "text": "Theorem 1 If MIL algorithm $A$ satisfies Condition $^{\\,l}$ , then this algorithm is not PAC learnable for any instance domain space $\\mathcal{D}_{X_{i n s t_{i}}\\;Y}$ and instance hypothesis space $\\mathcal{H}_{i n s t_{i}}\\subset\\{h_{i n s t_{i}}:X\\rightarrow Y\\}$ . ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[\\bigcup_{i=1}^{n}\\vert R_{i n s t_{i}}(A(S_{i n s t_{i}}))-\\operatorname*{inf}_{h\\in\\mathcal{H}_{i n s t_{i}}}R_{i n s t_{i}}(h)\\vert>\\epsilon\\right]>\\delta\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Condition 1 The MIL algorithm $A$ is not $P A C$ learnable for the given domain space $\\mathcal{D}_{X Y}$ and bag hypothesis space $\\mathcal{H}_{b a g}\\subset\\{h_{b a g}:X\\to Y\\}$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[|R_{b a g}(A(S))-\\operatorname*{inf}_{h\\in\\mathcal{H}_{b a g}}R_{b a g}(h)|>\\epsilon\\right]>\\delta\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Proof: The proof of Theorem 1 is conducted in Appendix C.1. ", "page_idx": 2}, {"type": "text", "text": "According to Theorem 1, MIL algorithms that are not learnable for bags do not guarantee learnability for instances. Therefore, in this study, we discuss the learnability of instances under Assumption 1. ", "page_idx": 2}, {"type": "text", "text": "Assumption 1 The MIL algorithm is PAC learnable for bags. ", "page_idx": 2}, {"type": "text", "text": "Based on the definitions, theorem, and assumption above, we can formulate the definition of when the proposed Deep MIL is learnable for instances as follows: ", "page_idx": 2}, {"type": "text", "text": "Definition 6 If the MIL algorithm satisfies Condition 2, it is learnable for instances. ", "page_idx": 2}, {"type": "text", "text": "Condition 2 The Deep MIL algorithm $A$ must exhibit equivalent PAC learnability for bags and instances: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{h\\in\\mathcal{H}(A(S))}-\\operatorname*{inf}_{h\\in\\mathcal{H}_{t_{b a g}}}R_{b a g}(h)|\\leq\\epsilon\\wedge\\bigcap_{i=1}^{n}|R_{i n s t i}(A(S_{i n s t_{i}}))-\\operatorname*{inf}_{h\\in\\mathcal{H}_{i n s t_{i}}}R_{i n s t_{i}}(h)|\\leq\\epsilon\\right]\\geq1-\\delta\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Definition 6 under Assumption 1 ensures that if Condition 2 is satisfied, the algorithm is guaranteed to be learnable for instances. On the other hand, MIL algorithms that do not satisfy Condition 2 cannot guarantee learnability for instances, even if they successfully learn for bags. Therefore, Condition 2 becomes a necessary and sufficient condition for MIL algorithms to be learnable for instances. ", "page_idx": 3}, {"type": "text", "text": "3 Proposed Theoretical Framework ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this study, we propose a theoretical framework to verify whether a given MIL algorithm satisfies Condition 2 according to Definition 6. For some Deep MIL algorithms [5, 6], instances are assumed to belong to independent bag domain spaces. Therefore, we address the problem by distinguishing between the independent bag domain space $\\mathcal{D}_{X Y}^{I n d}$ and the general bag domain space $\\mathcal{D}_{X Y}^{G e n}$ . 1) $\\mathcal{D}_{X Y}^{I n\\breve{d}}$ refers to bag domain spaces where all instances within a bag are statistically independent. 2) $\\mathcal{D}_{X Y}^{G e n}$ raemfeornsg t ion sat abnacge sd owimtahiinn  as pbaacge,  tahnadt $\\mathcal{D}_{X Y}^{I n d}$ .d eTsh abt oitsh, $\\mathcal{D}_{X Y}^{\\bar{I}n\\bar{d}}\\cup\\mathcal{D}_{X Y}^{D e p}=\\mathcal{D}_{X Y}^{G e n}$ DXDeYp , where interactions or dependencies exist ", "page_idx": 3}, {"type": "text", "text": "3.1 Overview ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Figure 2 shows the final summary of the definitions, relationships, and results of the theorems that comprise the theoretical framework proposed in this study. ", "page_idx": 3}, {"type": "image", "img_path": "cUcvlgkQxP/tmp/109d277e347fb4f821badde22b3b48072b110ca56fd3966e9fa143da5040a412.jpg", "img_caption": [], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Figure 2: Relationships between theorems: Blue arrows indicate that the pooling methods are learnable when our proposed conditions are satisfied; Red arrows indicate that they are not learnable when the conditions are not satisfied. ", "page_idx": 3}, {"type": "text", "text": "3.2 PAC Learnability for Independent Bag Domain Spaces ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The definition of $\\mathcal{D}_{X Y}^{I n d}$ is provided in Definition 7. ", "page_idx": 3}, {"type": "text", "text": "Definition 7 The independent Bag Domain Space $\\mathcal{D}_{X Y}^{I n d}$ is defined as a domain Space that encompasses all instance spaces while satisfying Condition 3: ", "page_idx": 3}, {"type": "text", "text": "Condition 3 For each Instance Space $D_{X_{i n s t_{i}}Y}$ within $\\mathcal{D}_{X Y}^{I n d}$ , there must exist a corresponding bag domain space $D_{X Y}\\,\\in\\,{\\cal D}_{X Y}^{I n d}$ DIXnYd , which should be determined as the union of each instance domain space, as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\nD_{X Y}^{I n d}:=\\bigcup_{i=1}^{N}D_{X_{i n s t_{i}}Y}\\in{\\mathcal{D}}_{X Y}^{I n d}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Since DIXnYd satisfies Condition 3, implying independence among instances, the hypothesis space for each instance is unaffected by other instances. In this case, for MIL algorithms to be learnable for instances, they must satisfy Condition 4 according to Theorem 2. ", "page_idx": 4}, {"type": "text", "text": "Theorem 2 If a MIL algorithm satisfies Condition $^{4}$ in $\\mathcal{D}_{X Y}^{I n d}$ , it is learnable for instances. ", "page_idx": 4}, {"type": "text", "text": "Condition 4 The risk of the optimal hypothesis for $D_{X Y}^{I n d}$ must ensure that it equals the sum of the risks of the optimal hypotheses for individual instance spaces within $D_{X Y}^{I n d}$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{h\\in\\mathcal{H}}R_{\\mathcal{D}_{X Y}^{I n d}}=\\sum_{i=1}^{N}\\operatorname*{inf}R_{i n s t_{i}}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Consequence: If Condition $^{4}$ is satisfied, then Condition 2 is also satisfied. Hence, Condition 4 becomes a necessary and sufficient condition for learnability for instances in $D_{X Y}^{I n d}$ . ", "page_idx": 4}, {"type": "text", "text": "Proof: The proof of Theorem 2 is provided in Appendix C.2. ", "page_idx": 4}, {"type": "text", "text": "3.3 PAC Learnability for General Bag Domain Spaces ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The definition of $\\mathcal{D}_{X Y}^{G e n}$ is provided in Definition 8. ", "page_idx": 4}, {"type": "text", "text": "Definition 8 The general bag domain space $\\mathcal{D}_{X Y}^{G e n}$ is defined as a domain space that encompasses all instance spaces while satisfying Condition $^{5}$ and 6: ", "page_idx": 4}, {"type": "text", "text": "Condition 5 For every instance space $\\mathcal{D}_{X_{i n s t_{i}}Y}$ within $\\mathcal{D}_{X Y}^{G e n}$ , there exists a corresponding bag domain space $D_{X Y}\\in{\\mathcal{D}}_{X Y}^{G e n}$ , determined as the sum of each instance domain space. ", "page_idx": 4}, {"type": "text", "text": "Condition 6 $\\mathcal{D}_{X Y}^{G e n}$ is formed using weights $\\alpha_{i}\\,\\in\\,(0,1)$ to reflect the importance of relationships among instances. Each instance domain space $D_{X_{i n s t_{i}}Y}$ should be defined along with its weight $\\alpha_{i}$ as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\nD_{X Y}^{G e n}=\\sum_{i=1}^{N}\\alpha_{i}D_{X_{i n s t_{i}}Y}\\in\\mathcal{D}_{X Y}^{G e n}\\quad s u c h\\,t h a t\\quad\\sum_{i=1}^{N}\\alpha_{i}=1,\\quad0\\leq\\alpha_{i}\\leq1\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Since $\\mathcal{D}_{X Y}^{G e n}$ satisfies Condition 5 and Condition 6, implying the existence of relationships among instances, the hypothesis space for each instance is influenced by other instances. In this case, for MIL algorithms to be learnable for instances, they must satisfy Condition 7 according to Theorem 3. ", "page_idx": 4}, {"type": "text", "text": "Theorem 3 If a MIL algorithm satisfies Condition $7$ in $\\mathcal{D}_{X Y}^{G e n}$ , it is learnable for instances. ", "page_idx": 4}, {"type": "text", "text": "Condition 7 The risk of the optimal hypothesis for $D_{X Y}^{G e n}$ must ensure that it equals the weighted sum of the risks of the optimal hypotheses for individual instance spaces within $D_{X Y}^{G e n}$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{h\\in{\\mathcal H}}R_{{D}_{X Y}^{G e n}}=\\sum_{i=1}^{N}\\alpha_{i}\\operatorname*{inf}R_{i n s t_{i}}\\quad s u c h\\,t h a t\\quad\\sum_{i=1}^{N}\\alpha_{i}=1,\\quad0\\leq\\alpha_{i}\\leq1\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Consequence: If a MIL algorithm satisfies Condition 7, it also satisfies Condition 2. Hence, Condition 7 becomes a necessary and sufficient condition for MIL algorithms to be learnable for instances in DGen. ", "page_idx": 4}, {"type": "text", "text": "Proof: The proof of Theorem 3 is provided in Appendix C.3. ", "page_idx": 4}, {"type": "text", "text": "4 Theoretical Verification of Existing Deep MILs ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "4.1 Classifications of Existing Deep MIL Methodologies ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The Deep MIL algorithms proposed so far are primarily categorized based on 1) whether they perform Aggregation at the Embedding-level [5, 6, 2, 19, 20] or at the Instance-level [5, 21, 22, 23, 9, 8, 10]. Additionally, they can be further classified into 5 types of pooling techniques according to 2) whether they do not use an attention mechanism [5, 21, 22, 6], perform Aggregation by multiplying attention weights at the embedding-level [2, 19, 20], or perform aggregation by multiplying attention weights at the instance-level [23, 9, 10], as shown in Table 1. ", "page_idx": 5}, {"type": "table", "img_path": "cUcvlgkQxP/tmp/beb664155935ccb9016adc2e0082d9664bd2686ebbf73b24ecb74ef36a4943c7.jpg", "table_caption": ["Table 1: Classification of existing Deep MIL methodologies. "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "A detailed explanation of the pooling techniques is provided in Appendix A.2. ", "page_idx": 5}, {"type": "text", "text": "4.2 Theoretical Verification ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "4.2.1 Relationship between Attention Mechanism and Learnability for Bag ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The application of an attention mechanism to MIL algorithms depends on the range of the domain space that the MIL algorithm can learn from. This implies that the feasibility of PAC learning for bags may vary depending on whether attention is applied or not. ", "page_idx": 5}, {"type": "text", "text": "Theorem 4 In $\\mathcal{D}_{X Y}^{I n d}$ , the MIL algorithm is PAC learnable for bags. ", "page_idx": 5}, {"type": "text", "text": "Proof: The proof of Theorem 4 is conducted in Appendix C.4. ", "page_idx": 5}, {"type": "text", "text": "Theorem 5 In $\\mathcal{D}_{X Y}^{G e n}$ , MIL algorithms that aggregate independent hypothesis spaces for each instance must utilize attention satisfying Condition 8 to be PAC learnable for bags. ", "page_idx": 5}, {"type": "text", "text": "Condition 8 The hypothesis space $\\mathcal{H}_{b a g_{a t t}}$ of Attention MIL should be equal to the sum of independent hypothesis spaces $h_{i n s t_{i}}$ multiplied by attention weights $A t t_{i}$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{H}_{b a g_{a t t}}=\\left\\{h_{b a g_{a t t}}\\ |\\ h_{b a g_{a t t}}=\\sum_{i=1}^{n}A t t_{i}\\cdot h_{i n s t_{i}},\\ w h e r e\\ 0<A t t_{i}<1,\\ \\sum_{i=1}^{n}A t t_{i}=1\\right\\}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Proof: The proof of Theorem 5 is conducted in Appendix C.5. ", "page_idx": 5}, {"type": "text", "text": "Validation: Experimental validation is performed in Section 4.4.1. ", "page_idx": 5}, {"type": "text", "text": "Consequence: Theorems 4 and 5 are not direct theorems about the learnability of MIL for instances. iHnostwaenvceers,.  tAhcec loeradrinnga btiol iTtyh efoorre bma 4g,s  iins  PaA pCr leeraerqnuaisbiltee $\\bar{\\mathcal{D}}_{X Y}^{I n d}$ M, IaLll  aMlgIoL raitlhgomrsi tthom bs es aatbislfey  tAo slseuamrpnt iforon $^{\\,l}$ regardless of the presence of attention mechanisms. On the other hand, in XGeYn , according to Theorem 5, the application of attention mechanisms satisfying Condition 8 becomes a necessary condition for MIL algorithms to be learnable for instances. ", "page_idx": 5}, {"type": "text", "text": "4.2.2 Verification Learnability for Instances by MIL Pooling Method ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The type of pooling technique used in Deep MIL algorithms becomes a determining factor in whether they are learnable for instances when they are learnable for bags. In this case, to verify whether a Deep MIL algorithm is learnable for instances, a definition of Lemma 1 should be established in advance, extending Condition 4 to ensure that no additional hypothesis space is included for instances. ", "page_idx": 5}, {"type": "text", "text": "Lemma 1 Condition 9 serves as a necessary condition for the learnability of instances, when the hypothesis space for the $i^{t h}$ instance of a MIL algorithm is $\\mathcal{H}_{i n s t_{i}}\\cup\\mathcal{H}_{a d d_{i}}$ . Here, $\\mathcal{H}_{i n s t_{i}}$ represents the hypothesis space for the $i^{t h}$ instance, and $\\mathcal{H}_{a d d_{i}}$ denotes the hypothesis space for the $i^{t h}$ instance generated through elements outside the $i^{t h}$ instance. ", "page_idx": 5}, {"type": "text", "text": "Condition 9 $\\mathcal{H}_{a d d_{i}}$ must be a subset of $\\mathcal{H}_{i n s t_{i}}$ : ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{H}_{i n s t_{i}}\\supset\\mathcal{H}_{a d d_{i}}:=\\{h_{a d d_{i}}:\\mathcal{X}_{a d d_{i}}\\to\\mathcal{Y}\\}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Proof: The proof of Lemma $^{\\,l}$ is conducted in Appendix C.6. ", "page_idx": 6}, {"type": "text", "text": "In the case of instance-pooling, as attention mechanisms are not utilized, Condition 8 is not satisfied. Therefore, in $\\mathcal{D}_{X Y}^{G e n}$ , the algorithm is not learnable for bags, and consequently, not learnable for instances either. However, according to Theorem 6, the algorithm is learnable for instances in $\\mathcal{D}_{X Y}^{I n d}$ . ", "page_idx": 6}, {"type": "text", "text": "Theorem 6 In $\\mathcal{D}_{X Y}^{I n d}$ , MIL algorithms that perform instance-pooling are PAC learnable for instances. ", "page_idx": 6}, {"type": "text", "text": "Proof: The proof of Theorem 6 is conducted in Appendix C.7. ", "page_idx": 6}, {"type": "text", "text": "Unlike Instance-Pooling, where the hypothesis for bags is combined from instance-level hypotheses, Embedding-Pooling does not combine bag hypotheses from instance-level hypotheses. As a result, it does not satisfy Condition 9, leading to a scenario similar to Theorem 7. ", "page_idx": 6}, {"type": "text", "text": "Theorem 7 MIL algorithms that perform Embedding-Pooling are not learnable for instances. ", "page_idx": 6}, {"type": "text", "text": "Proof: The proof of Theorem 7 is conducted in Appendix C.8. ", "page_idx": 6}, {"type": "text", "text": "Consequence: In $\\mathcal{D}_{X Y}^{I n d}$ , Deep MIL algorithms exhibit reproducibility when they avoid overfitting to easy information at the bag-level and effectively learn positive instances at the instance-level. Reproducibility in this context refers to the learnability of positive instances. They demonstrated experimentally that Deep MIL algorithms using Instance-Pooling, such as mi-Net [5] and Causal MIL [21, 22], exhibit reproducibility, while those using Embedding Pooling, such as Mi-Net [5], do not. However, Raff et al. [18] failed to provide a theoretical explanation for these results. In contrast, this study theoretically demonstrated that Instance-Pooling is learnable for instances while Embedding-Pooling is not, through Theorems 6 and 7. This provides theoretical support for the experimental findings of Raff et al. [18]. ", "page_idx": 6}, {"type": "text", "text": "To compute the attention applied to each instance\u2019s features in Attention Pooling and Additive Pooling, the bag\u2019s features $X$ are used as input. Multiplying attention weights to the features at the feature level results in additional hypothesis space $h_{a d d_{i}}$ formed by the attention operations on each instance. As a consequence, since Condition 9 is not satisfied, according to Theorem 8, the algorithm becomes not PAC Learnable for instances. ", "page_idx": 6}, {"type": "text", "text": "Theorem 8 If the MIL algorithm does not adhere to Condition 10, it is not learnable for instances. ", "page_idx": 6}, {"type": "text", "text": "Condition $\\mathbfit{10}$ The risk $R_{i n s t_{i}}$ for the $i^{t h}$ instance should be as follows: ", "page_idx": 6}, {"type": "equation", "text": "$$\nR_{i n s t_{i}}=\\mathbb{E}_{(x_{i n s t_{i}},y)\\sim D_{X_{i n s t_{i}}Y}}\\ell_{i n s t_{i}}(h,y)\\quad,\\,w h e r e\\quad h\\in\\mathcal{H}_{i n s t_{i}}\\cup\\mathcal{H}_{b a g-l e v e l_{i}}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Here, $\\mathcal{H}_{b a g-l e v e l_{i}}$ denotes the hypothesis space for the $i^{t h}$ instance generated through bag-level features, and $\\mathcal{H}_{b a g-l e v e l_{i}}:=\\{h_{b a g-l e v e l}:\\mathcal{X}\\to\\mathcal{Y}\\}$ . ", "page_idx": 6}, {"type": "text", "text": "Consequence: Attention-Pooling and Additive-Pooling multiply attention weights to each instance\u2019s feature-level. As a result, the hypothesis space for the $i^{t h}$ instance includes, in addition to InstancePooling, $h_{b a g-l e v e l}=\\{h\\mid h=f(X_{b a g}),\\overbar{f}\\in\\mathcal{F}_{b a g}\\}$ . In other words, it incorporates the hypothesis space for bag-level features into the prediction for the $i^{t h}$ instance. This, according to Theorem 8, renders it not learnable for instances. ", "page_idx": 6}, {"type": "text", "text": "Proof: The proof of Theorem 8 is conducted in Appendix C.9. ", "page_idx": 6}, {"type": "text", "text": "Validation: Experimental verification of Theorem 8 is demonstrated in Section 4.4.2. ", "page_idx": 6}, {"type": "text", "text": "On the other hand, Conjunctive-Pooling does not multiply attention at the feature level of instances, but rather at the prediction level of instances. Therefore, predictions are made based on individual features of instances, resulting in $\\mathcal{H}_{b a g-l e v e l_{i}}=\\emptyset$ , satisfying Condition 10. Additionally, Theorem 9 demonstrates that Conjunctive-Pooling operates in a manner that is learnable for instances. ", "page_idx": 6}, {"type": "text", "text": "Theorem 9 When MIL algorithms use Conjunctive-Pooling for aggregation in $\\mathcal{D}_{X Y}^{G e n}$ , they are learnable for instances. ", "page_idx": 6}, {"type": "text", "text": "Consequence: According to Theorem 9, Conjunctive-Pooling becomes the unique pooling technique learnable for instances in $\\mathcal{D}_{X Y}^{G e n}$ . Specifically, since $\\mathcal{D}_{X Y}^{G e n}$ includes $\\mathcal{D}_{X Y}^{I n d}$ by definition, ConjunctivePooling becomes a methodology that satisfies all cases. ", "page_idx": 7}, {"type": "text", "text": "Proof: The proof of Theorem 9 is conducted in Appendix C.10. ", "page_idx": 7}, {"type": "text", "text": "Validation: Experimental validation for Theorem 9 is presented in Section 4.4.2. ", "page_idx": 7}, {"type": "text", "text": "Javed et al.[9] demonstrated that the contribution of instances in MIL algorithms performing AdditivePooling is proportional to the shapley value[24]. However, their proof contained an error where the feature multiplied by attention was mistakenly assumed to be the feature of the $i^{t h}$ instance. Our theoretical framework identifies that when attention is applied to features, it fails to satisfy Condition 10, leading to the algorithm being not learnable. ", "page_idx": 7}, {"type": "text", "text": "In this study, we confirmed that the error causing failure to satisfy Condition 10 also appears in studies proposing MIL based on Conjunctive-Pooling [23, 10]. Details on this are explained in Section 4.3.1. ", "page_idx": 7}, {"type": "text", "text": "4.3 Additional Considerations ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "4.3.1 Rethinking Position Dependencies of Instances on Deep MILs ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Text data and time series data exhibit temporal dependencies, while image data often has spatial dependencies. Therefore, research in relevant fields has utilized neural networks capable of capturing dependencies, such as RNNs and CNNs [25, 26, 27, 28, 29, 30], or additional positional encoding on extracted features to enhance performance [31, 32, 33, 34, 35, 36]. Following this trend, Deep MIL studies have also employed RNN-based neural networks or positional encoding during the feature extraction process to capture temporal dependencies of instances for performance enhancement [23, 19, 10]. However, as per Theorem 10 and Theorem 11, these approaches render the models unable to learn from instances. ", "page_idx": 7}, {"type": "text", "text": "Theorem 10 If the MIL algorithm extracts features of instances through RNN-based neural networks for aggregation, it is unable to learn from instances. ", "page_idx": 7}, {"type": "text", "text": "Proof: The proof for Theorem 10 is conducted in Appendix C.11. ", "page_idx": 7}, {"type": "text", "text": "Validation: The experimental validation for Theorem $_{l l}$ is presented in Section 4.4.3. ", "page_idx": 7}, {"type": "text", "text": "Theorem 11 If the hypothesis space $\\mathcal{H}_{P o s-E n c o d e_{i}}$ generated through positional encoding values for the $i$ -th position of the MIL algorithm is not a subset of $\\mathcal{H}_{i n s t_{i}}$ , then the algorithm is not PAC learnable for instances. ", "page_idx": 7}, {"type": "text", "text": "Proof: If $\\mathcal{H}_{P o s-E n c o d e_{i}}\\ \\mathcal{C}\\ \\mathcal{H}_{i n s t_{i}}$ , the algorithm fails to satisfy Condition 9, rendering it not learnable for instances. ", "page_idx": 7}, {"type": "text", "text": "Validation: Experimental validation for Theorem 10 is shown in Section 4.4.3. ", "page_idx": 7}, {"type": "text", "text": "Consequence: According to Theorem $_{l l}$ , using values outside of an instance\u2019s features in the prediction process makes it unlearnable. Therefore, positional encoding should not be used in the process of predicting instances. ", "page_idx": 7}, {"type": "text", "text": "According to the investigation conducted in this study, it was observed that existing Deep MIL research using Conjunctive-Pooling, such as MILNET [23] and MILLET [10], all utilized RNN-based neural networks for feature extraction or performed positional encoding on instance features. Therefore, they failed to satisfy Condition 9, making them unable to learn about instances. Furthermore, these theoretical findings provide a basis for the ablation study conducted by Early et al.[10] on time series data classification problems, where Conjunctive-Pooling and positional encoding were applied to predict the class at each time step. In essence, Early et al.[10] demonstrated that while positional encoding contributed to improving prediction performance at the bag level, it acted as a detrimental factor in predicting instances. When utilizing external information such as positional dependencies through supplementary weighting factors like attention operations alone, Condition 9 may be satisfied, thereby enabling learnability concerning instances. ", "page_idx": 7}, {"type": "text", "text": "4.3.2 Learnability for Instances in Each Dimension for Multidimensional Deep MILs ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In the real world, data is often composed of bags consisting of multi-dimensional instances rather than simple instances of uniform dimensions. For instance, in video data, each frame(images) ", "page_idx": 7}, {"type": "text", "text": "is composed of patches in multi-dimensional structures for each frame dimension. Similarly, in multivariate time series data, each timestamp is composed of multivariate data points. In the case of multi-dimensional instances, Multi-dimensional Deep MIL (MD-MIL) methodologies have emerged to perform predictions on lower-level instances based solely on labels at the top level of the bag. These methodologies apply aggregation recursively, performing aggregation on data in the first dimension and sequentially on subsequent dimensions. Existing MD-MIL approaches have employed Embedding-Pooling or Attention-Pooling for each dimension\u2019s instances. However, these pooling methods have been shown by Theorems 7 and 8 to be incapable of learning about instances. Therefore, existing MD-MIL methodologies are not suitable for learning from multi-dimensional instances. ", "page_idx": 8}, {"type": "text", "text": "Therefore, to design methodologies capable of learning from multi-dimensional data, it is essential to use aggregation techniques that are capable of learning about instances, based on Theorem 6 or 9. Additionally, to ensure learnability from the top-level bag and bags in each dimension, the attention operation\u2019s results should be set to be within the instance\u2019s hypothesis space based on whether the relationships among instances in each dimension are independent or dependent. Through the experiments in Appendix E.2, we confirmed that Conjunctive-Pooling can capture the relationships between instances across different bags in an MD-MIL architecture, leading to performance improvements. These findings confirm that our proposed theoretical framework serves as a valuable guide in designing learnable models. ", "page_idx": 8}, {"type": "text", "text": "4.4 Experimental Validation ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, we conduct the following experimental validations to demonstrate whether existing Deep MIL approaches are learnable for instances based on the theorems: 1) (Theorem 5): Demonstrating the learnability of the attention mechanism for bags in $\\mathcal{D}_{X Y}^{G e n}$ . 2) (Theorem 8, 9): Showing that multiplying attention at the feature level is not learnable for instances. 3) (Theorem 10, 11): Demonstrating that inputting position-related values into instance positions is not learnable for instances. As this study assumes an environment where bags are PAC Learnable, we preprocess the MNIST dataset to match the difficulty level of each experiment. For the validation of Theorem 10 and 11, we use the WebTraffic dataset from Early et al. [10], which is a synthetic time-series classification dataset. Detailed experimental settings can be found in Appendix D. ", "page_idx": 8}, {"type": "text", "text": "4.4.1 Experimental Validation of Theorem 5 ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "To validate Theorem 5, we conducted experiments on MIL algorithms [5, 21, 37, 2, 38, 39, 19, 9, 10] representing each pooling technique. Table 2 compares prediction performance on the synthetic datasets, detailed in Appendix D.1. This reveals that Instance-Pooling based MIL algorithms [5, 21, 37], which do not apply weights to the hypothesis space, degraded learning performance for bags in $\\bar{\\mathcal{D}}_{X Y}^{G e n}$ . In contrast, the other algorithms [2, 38, 39, 19, 9, 10] that apply weights to the hypothesis space through the attention mechanism demonstrate superior performance, which is even comparable to a none-pooling-based method that, by using fully connected layers, preserves all instance-level information without any loss during prediction. This experimentally validates Theorem 5. ", "page_idx": 8}, {"type": "table", "img_path": "cUcvlgkQxP/tmp/df9676ce88657c46e5d28904e801b02368bc4af9fd8ca2df10dac317a85a39bc.jpg", "table_caption": ["Table 2: Prediction performance of Deep MIL on Bags in $\\underline{{D_{X Y}^{G e n}}}$ "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "4.4.2 Experimental Validation of Theorems 8 and 9 ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "To assess whether the algorithm [2, 38, 39, 19, 9, 10] is learnable for instances when weights from an attention mechanism that satisfies Condition 8 are multiplied at the feature level, we compared the predictive performance on instances by adjusting the variance of attention weights. ", "page_idx": 9}, {"type": "text", "text": "Table 3 shows the results on the synthetic dataset, which details in Appendix D.2. Attention-Pooling based [2, 38, 39, 19] and Additive-Pooling based [9] MIL algorithms, where attention weights are multiplied at the feature level and aggregation is performed, showed significantly lower predictive performance for instances compared to bags. In particular, SA-AbMILP [39] and TransMIL [19], which perform iterative attention operations, show a significant performance gap between bag-level predictions and instance-level predictions. This demonstrates that the Attention-Pooling process does not guarantee the learnability of MIL. In contrast, Conjunctive-Pooling based MIL algorithms [10] exhibited a much smaller difference in predictive performance between bags and instances than other algorithms. This validates, in accordance with Theorem 9, that Conjunctive-Pooling is learnable for instances. Additionally, experiments adjusting the variance of attention to determine its impact on the discrepancy between bag and instance performance are detailed in Appendix E.1. ", "page_idx": 9}, {"type": "table", "img_path": "cUcvlgkQxP/tmp/4d4eb207f9a291183ea2e9975a32e4f4101facee71016ce67d6aaa6c3b522e44.jpg", "table_caption": ["Table 3: Prediction performance comparison of MIL algorithms on bags and instances. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "4.4.3 Experimental Validation of Theorems 10 and 11 ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Table 4 compares the performance of Conjunctive MIL under various conditions, reflecting information on positional dependency: 1) Applying attention and prediction on features extracted via Positional Encoding or a GRU-layer (All), 2) Using positional information only for attention operations (Att), 3) Using positional information only for prediction (Predict), and 4) The baseline model without any positional adaptations (Default). ", "page_idx": 9}, {"type": "text", "text": "Comparative results on the WebTraffic dataset\u2019s instance prediction performance reveal that configurations All and Predict, which provide additional positional information to features as in Theorem 10 and 11, showed poorer prediction performance than Default. Particularly, RNN, which reflects more additional information than positional encoding, was found to significantly degrade performance. However, the Att configuration, which utilized positional information solely for attention operations, achieved better performance than Default. This demonstrates that incorporating helpful information such as positional data, into MIL should be selectively applied to attention computations only. ", "page_idx": 9}, {"type": "table", "img_path": "cUcvlgkQxP/tmp/87e9fee3a4df08218ecd014653e10f9ec8ae23e57b91e532a8353f9cb2a74500.jpg", "table_caption": ["Table 4: Test positional dependencies for WebTraffic datasets [10]. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "5 Conclusions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this study, we proposed a novel framework to theoretically validate that deep MIL can learn at the instance level, overcoming the aforementioned issues. The proposed theoretical framework derived conclusions about the instance-level learnability of deep MIL algorithms, assuming that MIL algorithms are learnable at the bag level. Utilizing the PAC learning theory, we theoretically derived the necessary and sufficient conditions for learnability in each hypothesis space. This provides theoretical guidance for building learnable MIL models in various domains. The practical application of our proposed framework to real-world MIL scenarios is presented in Appendix F. Limitations and future work are explained in Appendix G. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgement ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This work was supported by the National Research Foundation of Korea(NRF) grant funded by the Korea government(MSIT) (No. 2022R1F1A1067008), and by the Basic Science Research Program through the National Research Foundation of Korea(NRF) funded by the Ministry of Education (No. 2019R1A6A1A03032119). ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of machine learning. MIT press, 2018.   \n[2] Maximilian Ilse, Jakub Tomczak, and Max Welling. Attention-based deep multiple instance learning. In International conference on machine learning, pages 2127\u20132136. PMLR, 2018.   \n[3] Oded Maron and Tom\u00e1s Lozano-P\u00e9rez. A framework for multiple-instance learning. Advances in neural information processing systems, 10, 1997.   \n[4] Thomas G Dietterich, Richard H Lathrop, and Tom\u00e1s Lozano-P\u00e9rez. Solving the multiple instance problem with axis-parallel rectangles. Artificial intelligence, 89(1-2):31\u201371, 1997.   \n[5] Xinggang Wang, Yongluan Yan, Peng Tang, Xiang Bai, and Wenyu Liu. Revisiting multiple instance neural networks. Pattern Recognition, 74:15\u201324, 2018.   \n[6] Alessandro Tibo, Manfred Jaeger, and Paolo Frasconi. Learning and interpreting multi-multiinstance learning networks. Journal of Machine Learning Research, 21(193):1\u201360, 2020.   \n[7] Yu Tian, Guansong Pang, Yuanhong Chen, Rajvinder Singh, Johan W Verjans, and Gustavo Carneiro. Weakly-supervised video anomaly detection with robust temporal feature magnitude learning. In Proceedings of the IEEE/CVF international conference on computer vision, pages 4975\u20134986, 2021.   \n[8] Hui Lv, Zhongqi Yue, Qianru Sun, Bin Luo, Zhen Cui, and Hanwang Zhang. Unbiased multiple instance learning for weakly supervised video anomaly detection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 8022\u20138031, 2023.   \n[9] Syed Ashar Javed, Dinkar Juyal, Harshith Padigela, Amaro Taylor-Weiner, Limin Yu, and Aaditya Prakash. Additive mil: Intrinsically interpretable multiple instance learning for pathology. Advances in Neural Information Processing Systems, 35:20689\u201320702, 2022.   \n[10] Joseph Early, Gavin Cheung, Kurt Cutajar, Hanting Xie, Jas Kandola, and Niall Twomey. Inherently interpretable time series classification via multiple instance learning. In The Twelfth International Conference on Learning Representations, 2024.   \n[11] Peter Auer, Philip M Long, and Aravind Srinivasan. Approximating hyper-rectangles: Learning and pseudo-random sets. In Proceedings of the twenty-ninth annual ACM symposium on Theory of computing, pages 314\u2013323, 1997.   \n[12] Philip M Long and Lei Tan. Pac learning axis-aligned rectangles with respect to product distributions from multiple-instance examples. In Proceedings of the ninth annual conference on Computational learning theory, pages 228\u2013234, 1996.   \n[13] Avrim Blum and Adam Kalai. A note on learning from multiple-instance examples. Machine learning, 30:23\u201329, 1998.   \n[14] Boris Babenko. Multiple instance learning: algorithms and applications. View Article PubMed/NCBI Google Scholar, 19, 2008.   \n[15] Gary Doran and Soumya Ray. Multiple-instance learning from distributions. Journal of Machine Learning Research, 17(128):1\u201350, 2016.   \n[16] Weiming Hu, Chen Li, Xiaoyan Li, Md Mamunur Rahaman, Jiquan Ma, Yong Zhang, Haoyuan Chen, Wanli Liu, Changhao Sun, Yudong Yao, et al. Gashissdb: A new gastric histopathology image dataset for computer aided diagnosis of gastric cancer. Computers in biology and medicine, 142:105207, 2022.   \n[17] Sivan Sabato and Naftali Tishby. Multi-instance learning with any hypothesis class. The Journal of Machine Learning Research, 13(1):2999\u20133039, 2012.   \n[18] Edward Raff and James Holt. Reproducibility in multiple instance learning: A case for algorithmic unit tests. Advances in Neural Information Processing Systems, 36, 2024.   \n[19] Zhuchen Shao, Hao Bian, Yang Chen, Yifeng Wang, Jian Zhang, Xiangyang Ji, et al. Transmil: Transformer based correlated multiple instance learning for whole slide image classification. Advances in neural information processing systems, 34:2136\u20132147, 2021.   \n[20] Saul Fuster, Trygve Eftest\u00f8l, and Kjersti Engan. Nested multiple instance learning with attention mechanisms. In 2022 21st IEEE International Conference on Machine Learning and Applications (ICMLA), pages 220\u2013225. IEEE, 2022.   \n[21] Weijia Zhang, Xuanhui Zhang, Min-Ling Zhang, et al. Multi-instance causal representation learning for instance label prediction and out-of-distribution generalization. Advances in Neural Information Processing Systems, 35:34940\u201334953, 2022.   \n[22] Weijia Zhang, Lin Liu, and Jiuyong Li. Robust multi-instance learning with stable instances. In ECAI 2020, pages 1682\u20131689. IOS Press, 2020.   \n[23] Stefanos Angelidis and Mirella Lapata. Multiple instance learning networks for fine-grained sentiment analysis. Transactions of the Association for Computational Linguistics, 6:17\u201331, 2018.   \n[24] Scott M Lundberg and Su-In Lee. A unified approach to interpreting model predictions. Advances in neural information processing systems, 30, 2017.   \n[25] Jun Zhang and Kim-Fung Man. Time series prediction using rnn in multi-dimension embedding phase space. In SMC\u201998 Conference Proceedings. 1998 IEEE International Conference on Systems, Man, and Cybernetics (Cat. No. 98CH36218), volume 2, pages 1868\u20131873. IEEE, 1998.   \n[26] Jun Zhang and Kim-Fung Man. Time series prediction using rnn in multi-dimension embedding phase space. In SMC\u201998 Conference Proceedings. 1998 IEEE International Conference on Systems, Man, and Cybernetics (Cat. No. 98CH36218), volume 2, pages 1868\u20131873. IEEE, 1998.   \n[27] Gang Liu and Jiabao Guo. Bidirectional lstm with attention mechanism and convolutional layer for text classification. Neurocomputing, 337:325\u2013338, 2019.   \n[28] Rahul Chauhan, Kamal Kumar Ghanshala, and RC Joshi. Convolutional neural network (cnn) for image detection and recognition. In 2018 first international conference on secure cyber computing and communication (ICSCCC), pages 278\u2013282. IEEE, 2018.   \n[29] Kai Zhang, Wangmeng Zuo, Shuhang Gu, and Lei Zhang. Learning deep cnn denoiser prior for image restoration. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3929\u20133938, 2017.   \n[30] Yanan Sun, Bing Xue, Mengjie Zhang, Gary G Yen, and Jiancheng Lv. Automatically designing cnn architectures using the genetic algorithm for image classification. IEEE transactions on cybernetics, 50(9):3840\u20133854, 2020.   \n[31] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.   \n[32] Minghao Chen, Houwen Peng, Jianlong Fu, and Haibin Ling. Autoformer: Searching transformers for visual recognition. In Proceedings of the IEEE/CVF international conference on computer vision, pages 12270\u201312280, 2021.   \n[33] Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang. Informer: Beyond efficient transformer for long sequence time-series forecasting. In Proceedings of the AAAI conference on artificial intelligence, volume 35, pages 11106\u201311115, 2021.   \n[34] Alexey Dosovitskiy. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.   \n[35] Rui Xu, Xintao Wang, Kai Chen, Bolei Zhou, and Chen Change Loy. Positional encoding as spatial inductive bias in gans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13569\u201313578, 2021.   \n[36] Kan Wu, Houwen Peng, Minghao Chen, Jianlong Fu, and Hongyang Chao. Rethinking and improving relative position encoding for vision transformer. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 10033\u201310041, 2021.   \n[37] Pei Liu and Luping Ji. Weakly-supervised residual evidential learning for multi-instance uncertainty estimation. In Forty-first International Conference on Machine Learning, 2024.   \n[38] Xiaoshuang Shi, Fuyong Xing, Yuanpu Xie, Zizhao Zhang, Lei Cui, and Lin Yang. Lossbased attention for deep multiple instance learning. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 5742\u20135749, 2020.   \n[39] Dawid Rymarczyk, Adriana Borowa, Jacek Tabor, and Bartosz Zielinski. Kernel self-attention for weakly-supervised image classification using deep multiple instance learning. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 1721\u20131730, 2021.   \n[40] Yan Xu, Tao Mo, Qiwei Feng, Peilin Zhong, Maode Lai, I Eric, and Chao Chang. Deep learning of feature representation with multiple instance learning for medical image analysis. In 2014 IEEE international conference on acoustics, speech and signal processing (ICASSP), pages 1626\u20131630. IEEE, 2014.   \n[41] Dimitrios Kotzias, Misha Denil, Nando De Freitas, and Padhraic Smyth. From group to individual labels using deep features. In Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining, pages 597\u2013606, 2015.   \n[42] Yunjie Ji, Hao Liu, Bolei He, Xinyan Xiao, Hua Wu, and Yanhua Yu. Diversified multiple instance learning for document-level multi-aspect sentiment classification. In Proceedings of the 2020 conference on empirical methods in natural language processing (EMNLP), pages 7012\u20137023, 2020.   \n[43] Jun Wang and Jean-Daniel Zucker. Solving multiple-instance problem: A lazy learning approach. 2000.   \n[44] Waqas Sultani, Chen Chen, and Mubarak Shah. Real-world anomaly detection in surveillance videos. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 6479\u20136488, 2018.   \n[45] Oren Z Kraus, Jimmy Lei Ba, and Brendan J Frey. Classifying and segmenting microscopy images with deep multiple instance learning. Bioinformatics, 32(12):i52\u2013i59, 2016.   \n[46] Lei Zhou, Yu Zhao, Jie Yang, Qi Yu, and Xun Xu. Deep multiple instance learning for automatic detection of diabetic retinopathy in retinal images. IET Image Processing, 12(4):563\u2013571, 2018.   \n[47] Yongluan Yan, Xinggang Wang, Xiaojie Guo, Jiemin Fang, Wenyu Liu, and Junzhou Huang. Deep multi-instance learning with dynamic pooling. In Asian Conference on Machine Learning, pages 662\u2013677. PMLR, 2018.   \n[48] Seung-Kyu Hong, Jae-Seok Jang, and Hyuk-Yoon Kwon. Enhancing performance of transformer-based models in natural language understanding through word importance embedding. Knowledge-Based Systems, 304:112404, 2024.   \n[49] Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang. Informer: Beyond efficient transformer for long sequence time-series forecasting. In Proceedings of the AAAI conference on artificial intelligence, volume 35, pages 11106\u201311115, 2021.   \n[50] Jong Seong Park, Jeong-Ha Park, Jihyeok Choi, and Hyuk-Yoon Kwon. Learning with correlation-guided attention for multienergy consumption forecasting. IEEE Transactions on Industrial Informatics, 2024.   \n[51] Joseph Early, Christine Evers, and SArvapali Ramchurn. Model agnostic interpretability for multiple instance learning. In International Conference on Learning Representations, 2022.   \n[52] Hang Zhao, Yujing Wang, Juanyong Duan, Congrui Huang, Defu Cao, Yunhai Tong, Bixiong Xu, Jing Bai, Jie Tong, and Qi Zhang. Multivariate time-series anomaly detection via graph attention network. In 2020 IEEE international conference on data mining (ICDM), pages 841\u2013850. IEEE, 2020.   \n[53] Taehee Kim, Jae-Seok Jang, and Hyuk-Yoon Kwon. Correlation-driven multi-level learning for anomaly detection on multiple energy sources. Applied Soft Computing, 159:111636, 2024.   \n[54] Yingying Zhang, Desen Zhou, Siqin Chen, Shenghua Gao, and Yi Ma. Single-image crowd counting via multi-column convolutional neural network. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 589\u2013597, 2016.   \n[55] Wen Liu, Weixin Luo, Dongze Lian, and Shenghua Gao. Future frame prediction for anomaly detection\u2013a new baseline. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 6536\u20136545, 2018.   \n[56] Marc-Andr\u00e9 Carbonneau, Veronika Cheplygina, Eric Granger, and Ghyslain Gagnon. Multiple instance learning: A survey of problem characteristics and applications. Pattern Recognition, 77:329\u2013353, 2018.   \n[57] Zhi-Hua Zhou and Jun-Ming Xu. On the relation between multi-instance learning and semisupervised learning. In Proceedings of the 24th international conference on Machine learning, pages 1167\u20131174, 2007.   \n[58] Ragav Venkatesan, Parag Chandakkar, and Baoxin Li. Simpler non-parametric methods provide as good or better results to multiple-instance learning. In Proceedings of the IEEE International Conference on Computer Vision, pages 2605\u20132613, 2015.   \n[59] Wu-Jun Li et al. Mild: Multiple-instance learning via disambiguation. Ieee transactions on knowledge and data engineering, 22(1):76\u201389, 2009.   \n[60] Jaume Amores. Vocabulary-based approaches for multiple-instance data: A comparative study. In 2010 20th International Conference on Pattern Recognition, pages 4246\u20134250. IEEE, 2010.   \n[61] Yixin Chen, Jinbo Bi, and James Ze Wang. Miles: Multiple-instance learning via embedded instance selection. IEEE transactions on pattern analysis and machine intelligence, 28(12):1931\u2013 1947, 2006.   \n[62] Zhouyu Fu, Antonio Robles-Kelly, and Jun Zhou. Milis: Multiple instance learning with instance selection. IEEE Transactions on Pattern Analysis and Machine Intelligence, 33(5):958\u2013977, 2010.   \n[63] Shengye Yan, Xiaodong Zhu, Guoqing Liu, and Jianxin Wu. Sparse multiple instance learning as document classification. Multimedia tools and applications, 76:4553\u20134570, 2017.   \n[64] Dan Zhang, Yan Liu, Luo Si, Jian Zhang, and Richard Lawrence. Multiple instance learning on structured data. Advances in Neural Information Processing Systems, 24, 2011. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Related Works ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.1 Multiple Instance Learning ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Multiple Instance Learning (MIL), a type of weakly supervised learning, is a methodology where labels are assigned to a bag of instances rather than individual instances, allowing for predictions about individual instances based on the bag\u2019s label. MIL was first introduced in the field of chemistry to identify active molecules and has since been applied to various domains, including medical image analysis, text data processing, time-series data analysis, and video anomaly detection [3, 4, 40, 2, 9, 41, 23, 42, 43, 10, 44, 7, 8]. ", "page_idx": 14}, {"type": "text", "text": "In medical image analysis, accurate labeling of cancer cell locations requires significant time and effort. However, it is possible to use MIL to predict cancer cell locations with labels for the entire image. In text data, documents or reviews are treated as a single bag, and MIL allows for more detailed analysis by predicting the attributes of each instance within a document or review. MIL is also effectively used in time-series data and video anomaly detection for cause analysis and anomaly detection. ", "page_idx": 14}, {"type": "text", "text": "A.2 Type of Pooling in Multiple Instance Learning ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Deep MIL methodologies can be broadly classified based on the pooling mechanism into the following categories: 1) Instance-Pooling, 2) Embedding-Pooling, 3) Attention-Pooling, 4) Additive-Pooling, and 5) Conjunctive-Pooling. ", "page_idx": 14}, {"type": "text", "text": "The functional architecture of our model incorporates several key components designed for processing instances within a multivariate time series dataset. The feature extraction function for the $i^{\\bar{t}h}$ instance is denoted by $f_{i}(X_{i})$ . Furthermore, we utilize classifier functions to make predictions based on these features: $p_{i}(X_{i})$ targets the features of the $i^{t h}$ instance, while $p(X)$ handles features aggregated at the bag level. The attention weight assigned to the $i^{t h}$ instance is expressed as $A_{i}$ . Additionally, the aggregation function, pivotal for our pooling techniques, is represented by $g(X)$ . The various pooling methods utilized in our framework are described as follows: ", "page_idx": 14}, {"type": "text", "text": "(Instance-Pooling) ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Instance-pooling performs predictions for each instance individually and then uses the max or mean operator to aggregate the results: ", "page_idx": 14}, {"type": "equation", "text": "$$\ng_{m a x-p o o l i n g}(X)=\\operatorname*{max}_{i=1}^{N}(p_{i}\\circ f_{i})(X_{i})\n$$", "text_format": "latex", "page_idx": 14}, {"type": "equation", "text": "$$\ng_{m e a n-p o o l i n g}(X)=\\frac{1}{N}\\sum_{i=1}^{N}(p_{i}\\circ f_{i})(X_{i})\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "(Embedding-Pooling) ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Embedding-pooling obtains features for each instance and then uses the max or mean operator to aggregate these features to obtain a feature representation for the bag, on which predictions for the bag are made: ", "page_idx": 14}, {"type": "equation", "text": "$$\ng_{m a x-p o o l i n g}(X)=p(\\operatorname*{max}_{i=1}^{N}f_{i}(X_{i}))\n$$", "text_format": "latex", "page_idx": 14}, {"type": "equation", "text": "$$\ng_{m e a n-p o o l i n g}(X)=p(\\frac{1}{N}\\sum_{i=1}^{N}f_{i}(X_{i}))\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "(Attention-Pooling) ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Attention-pooling multiplies the features of each instance by attention weights and then performs embedding-pooling using the summation operator on the weighted features: ", "page_idx": 14}, {"type": "equation", "text": "$$\ng(X)=p(\\sum_{i=1}^{N}A_{i}f_{i}(X_{i}))\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "(Additive-Pooling) ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Additive-pooling multiplies the features of each instance by attention weights, obtains individual predictions for each instance, and then performs a summation operation to make predictions for the bag: ", "page_idx": 15}, {"type": "equation", "text": "$$\ng(X)=\\sum_{i=1}^{N}(p_{i}\\circ A_{i}f_{i})(X_{i})\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "(Conjunctive-Pooling) ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Conjunctive-pooling multiplies the individual predictions of instances by attention weights and performs aggregation through a weighted sum: ", "page_idx": 15}, {"type": "equation", "text": "$$\ng(X)=\\sum_{i=1}^{N}(A_{i}p_{i}\\circ f_{i})(X_{i})\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "A.2.1 Attention Mechanism ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In early deep MIL, the role of the deep network was to better extract the features of instances and to accurately predict instances through pooling operations in an end-to-end manner [5, 21, 22, 6, 45, 46, 47]. ", "page_idx": 15}, {"type": "text", "text": "Early deep MIL models had the limitation that the bag-level prediction does not accelerate the instance-level prediction because predictions for instances were performed separately from predictions for the bag. To overcome this limitation, attention mechanisms have been used in various MIL methodologies [2, 23, 19, 9, 10, 20]. In other deep learning research [31, 34, 48, 49, 50], attention mechanisms have shown significant performance improvements by capturing relationships within the data. Due to these characteristics, attention weights, based on the features of all instances, assign higher weights to important instances for bag predictions, making them useful for instance-level predictions as well [2, 19, 20]. These weights have been shown to improve the prediction performance of instances [23, 9, 10]. ", "page_idx": 15}, {"type": "text", "text": "A.2.2 Level of Aggregation ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Deep MIL requires aggregating the values of instances to perform predictions for the bag. Early deep MIL models performed individual predictions for instances and then aggregated these predicted values using differentiable pooling operators [5]. However, the approach of making individual predictions for instances was not effective in improving the prediction performance for the bag. To enhance bag prediction performance, new methods were proposed that perform pooling operations on features extracted from each instance to obtain a feature representation for the bag and then learn the bag. These methods apply the classifier used for the bag to instances [5] or cluster the features of instances to find similar instances [6]. ", "page_idx": 15}, {"type": "text", "text": "In the case of deep MIL utilizing the attention mechanism, early methods enhanced performance by multiplying the features of each instance by attention weights, performing mean pooling to extract a feature representation for the bag, and using that feature to train the bag [2, 19, 20]. This approach suggested that instances with higher attention weights contributed more to the bag\u2019s prediction. However, while feature-level aggregation allowed for effective bag predictions, it did not facilitate detailed predictions for individual instances. Conversely, applying attention and aggregating the prediction results for instances achieved high performance [9, 10]. Due to this trend, recent research prefers aggregating the prediction results for instances. ", "page_idx": 15}, {"type": "text", "text": "A.3 Theoretical Study of Multiple Instance Learning ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "According to Babenko et al. [14], early theoretical research on MIL focused on simple algorithms such as axis-aligned rectangles, which facilitated the derivation of theoretical results [11, 12]. Blum et al. [13] proposed an algorithm that illustrated the relationship between supervised learning and MIL, showing that if a supervised learning algorithm can learn a concept space, MIL can also learn within that space. Sabato et al. [17] addressed the limitations of previous MIL research, which often derived theoretical results for specific domains or simple hypothesis classes, by presenting a generalized theoretical framework applicable to various hypothesis classes and complex data structures. This framework allowed the evaluation of learnability for bags without being limited to specific algorithms. Raff et al. [18] argued that deep MIL algorithms do not adhere to standard MIL assumptions and are unsuitable for real-world environments. They proposed an algorithm unit test to verify adherence to these assumptions, ensuring that models can accurately predict bags in real-world scenarios. ", "page_idx": 16}, {"type": "text", "text": "However, all theoretical research proposed on MIL thus far has focused solely on whether MIL can learn bags as effectively as supervised learning without addressing the primary purpose of MILpredicting individual instances. Therefore, to address this issue, this study proposes a theoretical framework that can universally evaluate whether an algorithm is learnable at the instance level. ", "page_idx": 16}, {"type": "text", "text": "B Notations ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "The key notations used in Definitions, Theorems, Conditions, and Proofs in this study are summarized in Table 5. ", "page_idx": 16}, {"type": "table", "img_path": "cUcvlgkQxP/tmp/bfbf2fdab01540c3739684299622f15dfbee2c60fff14695e3a555557dac242b.jpg", "table_caption": ["Table 5: Notations. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "C Theoretical Proofs ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "C.1 Proof of Theorem 1 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Theorem 1 If MIL algorithm $A$ satisfies Condition 1, then this algorithm is not PAC learnable for any instance domain space $\\mathcal{D}_{X_{i n s t_{i}}\\;Y}$ and instance hypothesis space $\\mathcal{H}_{i n s t_{i}}\\subset\\{h_{i n s t_{i}}:X\\rightarrow Y\\}$ . ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[\\bigcup_{i=1}^{n}\\vert R_{i n s t_{i}}(A(S_{i n s t_{i}}))-\\operatorname*{inf}_{h\\in\\mathcal{H}_{i n s t_{i}}}R_{i n s t_{i}}(h)\\vert>\\epsilon\\right]>\\delta\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Condition 1 The MIL algorithm $A$ is not PAC learnable for the given domain space $\\mathcal{D}_{X Y}$ and bag hypothesis space $\\mathcal{H}_{b a g}\\subset\\{h_{b a g}:X\\rightarrow Y\\}$ : ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[|R_{b a g}(A(S))-\\operatorname*{inf}_{h\\in\\mathcal{H}_{b a g}}R_{b a g}(h)|>\\epsilon\\right]>\\delta\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof The performance of MIL algorithms on bags directly depends on the performance of the instances composing those bags. Therefore, the probability that a bag\u2019s performance does not reach the optimal hypothesis is greater than or equal to the probability that each individual instance does not reach the optimal hypothesis: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\left\\{\\bigcup_{i=1}^{n}|R_{i n s t_{i}}(A(S_{i n s t_{i}}))-\\operatorname*{inf}_{h\\in\\mathcal{H}_{i n s t_{i}}}R_{i n s t_{i}}(h)|>\\epsilon\\right\\}\\geq\\mathbb{P}\\left[|R_{b a g}(A(S))-\\operatorname*{inf}_{h\\in\\mathcal{H}_{b a g}}R_{b a g}(h)|>\\epsilon\\right]\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "If Condition 1 is satisfied, the probability that the bag\u2019s performance does not reach the optimal hypothesis can be expressed as follows: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[|R_{b a g}(A(S))-\\operatorname*{inf}_{h\\in\\mathcal{H}_{b a g}}R_{b a g}(h)|>\\epsilon\\right]>\\delta\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Ultimately, if the bag is not learnable, then one or more instances become unlearnable. ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[\\bigcup_{i=1}^{n}\\vert R_{i n s t_{i}}(A(S_{i n s t_{i}}))-\\operatorname*{inf}_{h\\in\\mathcal{H}_{i n s t_{i}}}R_{i n s t_{i}}(h)\\vert>\\epsilon\\right]>\\delta\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "C.2 Proof of Theorem 2 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Theorem 2 If a MIL algorithm satisfies Condition 4 in $\\mathcal{D}_{X Y}^{I n d}$ , it is learnable for instances. ", "page_idx": 17}, {"type": "text", "text": "Condition 4 The optimal hypothesis for $\\mathcal{D}_{X Y}^{I n d}$ must be equal to the sum of optimal hypotheses for each individual instance space within DIXnYd : ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{h\\in\\mathcal{H}}R_{\\mathcal{D}_{X Y}^{I n d}}=\\sum_{i=1}^{N}\\operatorname*{inf}R_{i n s t_{i}}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof First, let\u2019s assume that the MIL algorithm satisfies Condition 4 in $\\mathcal{D}_{X Y}^{I n d}$ . Based on this assumption, we apply Definition 4 to verify the learnability of MIL in the independent bag hypothesis space. ", "page_idx": 17}, {"type": "text", "text": "The learnability in the independent bag hypothesis space means that the algorithm is likely to return predictions within an acceptable error margin from the true optimal hypothesis: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{P}_{S\\sim D_{X Y}^{m}}\\left[|R_{D}(A(S))-\\operatorname*{inf}_{h\\in\\mathcal{H}_{b a g}}R_{D}(h)|\\leq\\epsilon\\right]\\geq1-\\delta\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Here, $S$ represents the training data, and $A(S)$ denotes the predictions returned by the MIL algorithm. $R_{D}(h)$ represents the actual risk of hypothesis $h$ . Now, this probability is expanded as follows: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{P}_{S\\sim D_{X Y}^{m}}\\left[|R_{D}(A(S))-\\operatorname*{inf}_{h\\in\\mathcal{H}_{b a g}}R_{D}(h)|\\le\\epsilon\\right]=\\mathbb{P}_{S\\sim D_{X Y}^{m}}\\left[R_{D}(A(S))\\le\\operatorname*{inf}_{h\\in\\mathcal{H}_{b a g}}R_{D}(h)+\\epsilon\\right]\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Now, this probability can be expressed as follows: ", "page_idx": 18}, {"type": "equation", "text": "$$\n^{3}s\\sim D_{X^{n}Y}^{m}\\left[R_{D}(A(S))\\leq\\operatorname*{inf}_{h\\in\\mathcal{H}_{h o p}}R_{D}(h)+\\epsilon\\right]=\\sum_{i=1}^{N}\\mathbb{P}_{S\\sim D_{X_{i n s t_{i}}}^{m}}\\left[R_{D}(A(S))\\leq\\operatorname*{inf}_{h\\in\\mathcal{H}_{i n s t_{i}}}R_{D}(h)+\\epsilon\\right]\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "In the above equation, $N$ represents the number of instances. ", "page_idx": 18}, {"type": "text", "text": "This equation implies that the learnability of bags and instances becomes equivalent. Since we assumed that bags are learnable according to Assumption 1, Condition 4 becomes a necessary and sufficient condition for the learnability of instances. ", "page_idx": 18}, {"type": "text", "text": "C.3 Proof of Theorem 3 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Theorem 3 If a MIL algorithm satisfies Condition 7 in $\\mathcal{D}_{X Y}^{G e n}$ , it is learnable for instances. ", "page_idx": 18}, {"type": "text", "text": "Condition 7 The optimal hypothesis for $\\mathcal{D}_{X Y}^{G e n}$ must guarantee that it is equal to the weighted sum of optimal hypotheses for each individual instance space within $\\mathcal{D}_{X Y}^{G e n}$ : ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{h\\in{\\mathcal H}}R_{{\\mathcal D}_{X Y}^{G e n}}=\\sum_{i=1}^{N}\\alpha_{i}\\operatorname*{inf}R_{i n s t_{i}}\\quad\\mathrm{s.t.}\\quad\\sum_{i=1}^{N}\\alpha_{i}=1,\\quad0\\leq\\alpha_{i}\\leq1\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof To demonstrate that the learnability of bags is equivalent to the learnability of instances, we need to show that the following conditions are satisfied: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[\\left|\\operatorname*{inf}{R_{D_{X Y}^{G e n}}}-\\sum_{i=1}^{N}{\\alpha_{i}\\operatorname*{inf}{R_{i n s t_{i}}}}\\right|>\\epsilon\\right]\\leq\\delta\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "This equation represents that the weighted sum of risks for bags and instances exceeds the permissible error $\\epsilon$ with probability $\\delta$ or less. Therefore, if this equation is satisfied, the learnability of bags and instances becomes equivalent. ", "page_idx": 18}, {"type": "text", "text": "In other words, it indicates that the probability that the difference between the risk for each instance and the weighted sum of the entire bag risks is within the permissible error $\\frac{\\epsilon}{N}$ is at least $1-\\delta$ . ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[\\forall i,\\left|\\operatorname*{inf}R_{D_{X Y}^{G e n}}-\\alpha_{i}\\operatorname*{inf}R_{i n s t_{i}}\\right|\\leq\\frac{\\epsilon}{N}\\right]\\geq1-\\delta\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Through the inequality for the risk of each instance, we can obtain the following: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\left|\\alpha_{i}\\operatorname{inf}R_{i n s t_{i}}-\\operatorname{inf}R_{D_{X Y}^{G e n}}\\right|\\leq{\\frac{\\epsilon}{N}},\\quad\\forall i\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "To prove this, let\u2019s define the random variable $X_{i n s t_{i}}$ as the difference in risk for each instance: ", "page_idx": 18}, {"type": "equation", "text": "$$\nX_{i n s t_{i}}=\\operatorname*{inf}{R_{D_{X Y}^{G e n}}}-\\alpha_{i}\\operatorname*{inf}{R_{i n s t_{i}}}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Let\u2019s set the range of each $X_{i n s t_{i}}$ to $[-M,M]$ , and define $c_{i}$ as the mean of $X_{i n s t_{i}}$ . $c_{i}$ becomes 0 due to cancellation between $\\mathbb{E}[\\operatorname*{inf}{R_{D_{X Y}^{G e n}}}]$ and $\\mathbb{E}[\\alpha_{i}\\operatorname*{inf}R_{i n s t_{i}}]$ . ", "page_idx": 18}, {"type": "equation", "text": "$$\nc_{i}=\\mathbb{E}[X_{i n s t_{i}}]=0\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Let\u2019s define the function $f(X_{i n s t_{1}},X_{i n s t_{2}},\\ldots,X_{i n s t_{N}})$ as the optimal risk over the general bag domain DXGeYn : ", "page_idx": 19}, {"type": "equation", "text": "$$\nf(X_{i n s t_{1}},X_{i n s t_{2}},\\ldots,X_{i n s t_{N}})=\\operatorname*{inf}R_{D_{X Y}^{G e n}}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "In this setup, the instances within the bag are not independent; instead, they interact and contribute collectively to the overall bag risk. To capture the effect of each instance on the bag\u2019s optimal risk, we introduce weights $\\alpha_{i}$ that represent the contribution of each instance\u2019s risk to the total risk. This allows us to define the expected value of $f$ as the weighted sum of the optimal risks of each instance domain: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{E}[f(X_{i n s t_{1}},\\ldots,X_{i n s t_{N}})]=\\sum_{i=1}^{N}\\alpha_{i}\\operatorname*{inf}R_{i n s t_{i}}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "This expression reflects the dependencies and interactions between instances, allowing us to represent the general bag risk as a function of each instance\u2019s weighted risk. ", "page_idx": 19}, {"type": "text", "text": "In the general bag domain space, where one or more instances may have dependent relationships, we apply the Azuma-Hoeffding inequality. ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[\\left|f(X_{i n s t_{1}},\\ldots,X_{i n s t_{N}})-\\mathbb{E}[f(X_{i n s t_{1}},\\ldots,X_{i n s t_{N}})]\\right|>t\\right]\\le2\\exp\\left(\\frac{-2t^{2}}{\\sum_{i=1}^{N}c_{i}^{2}}\\right)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Since $f(X_{i n s t_{1}},X_{i n s t_{2}},\\ldots,X_{i n s t_{N}})$ represents the risk of the bag as stated earlier, the equation can be written as follows: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[\\left|\\operatorname*{inf}R_{D_{X Y}^{G e n}}-\\mathbb{E}[\\operatorname*{inf}R_{D_{X Y}^{G e n}}]\\right|>t\\right]\\leq2\\exp\\left(\\frac{-2t^{2}}{\\sum_{i=1}^{N}c_{i}^{2}}\\right)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The $t$ term in $|\\operatorname*{inf}_{\\mathbf{\\mu}}R_{D_{X Y_{.}}^{G e n}}-\\mathbb{E}[\\operatorname*{inf}_{\\mathbf{\\mu}}R_{D_{X Y}^{G e n}}]|$ represents the deviation from the expected value, so it can be replaced by the permissible error $\\epsilon$ for learnability, yielding the following: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[|\\operatorname*{inf}R_{D_{X Y}^{G e n}}-\\mathbb{E}[\\operatorname*{inf}R_{D_{X Y}^{G e n}}]|>\\epsilon\\right]\\leq2\\exp\\left(\\frac{-2\\epsilon^{2}}{\\sum_{i=1}^{N}c_{i}^{2}}\\right)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Finally, it can be expressed as follows: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[\\left|\\operatorname*{inf}R_{D_{X Y}^{G e n}}-\\sum_{i=1}^{N}\\alpha_{i}\\operatorname*{inf}R_{i n s t_{i}}\\right|>\\epsilon\\right]\\leq2\\exp\\left(\\frac{-2\\epsilon^{2}}{\\sum_{i=1}^{N}c_{i}^{2}}\\right)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "$c_{i}$ represents the mean of errors. Since Deep MIL algorithms are trained to minimize the mean error, $\\textstyle\\sum_{i=1}^{N^{-}}c_{i}\\simeq0$ . ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[\\left|\\operatorname*{inf}{R_{b a g}}-\\sum_{i=1}^{N}{\\alpha_{i}\\operatorname*{inf}{R_{i n s t_{i}}}}\\right|>\\epsilon\\right]\\leq\\delta\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "This ensures that the sum of $c_{i}$ values remains within a certain range, guaranteeing that it is smaller than the probability of being trained within the permissible error range $\\epsilon$ , denoted as $\\delta$ . ", "page_idx": 19}, {"type": "equation", "text": "$$\n2\\exp\\left(\\frac{-2\\epsilon^{2}}{\\sum_{i=1}^{N}c_{i}^{2}}\\right)\\le\\delta\\Leftrightarrow\\frac{1}{\\sum_{i=1}^{N}c_{i}^{2}}\\geq\\frac{1}{2\\epsilon^{2}}\\log\\frac{2}{\\delta}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Therefore, under the assumption of successful learning, Condition 2 is satisfied. Thus, a well-trained MIL model becomes learnable for instances. ", "page_idx": 20}, {"type": "text", "text": "C.4 Proof of Theorem 4 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Theorem 4 In $\\mathcal{D}_{X Y}^{I n d}$ , the MIL algorithm is PAC learnable for bags. ", "page_idx": 20}, {"type": "text", "text": "Proof The PAC learnability of $\\mathcal{D}_{X Y}^{I n d}$ implies that every concept space $\\mathcal{C}_{\\mathcal{D}_{x y}^{I n d}}$ can be approximated by hypothesis spaces $\\mathcal{H}_{D_{X Y}^{I n d}}$ . ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathcal{C}_{\\mathcal{D}_{\\mathcal{X}\\mathcal{Y}}^{I n d}}\\subseteq\\mathcal{H}_{D_{X\\Upsilon}^{I n d}}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Since DIXnYd assumes that predictions for all instances are independent, predictions for bags become the union of predictions for instances. ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathcal{H}_{D_{X Y}^{I n d}}=\\bigcup_{i=1}^{n}\\mathcal{H}_{i n s t_{i}}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "The hypothesis space Hbagnon\u2212att for Non-Attention MIL algorithms, which do not use Attention, is also the union of predictions of instances: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathcal{H}_{b a g_{n o n-a t t}}=\\bigcup_{i=1}^{n}\\mathcal{H}_{i n s t_{i}}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Thus, Hbagnon\u2212att becomes equivalent to the hypothesis space $\\mathcal{H}_{D_{X Y}^{I n d}}$ of PAC Learnable $\\mathcal{D}_{X Y}^{I n d}$ : ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathcal{H}_{D_{X Y}^{I n d}}=\\mathcal{H}_{b a g_{n o n-a t t}}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Therefore, in the case of PAC Learnable scenarios, Non-Attention MIL is learnable. ", "page_idx": 20}, {"type": "text", "text": "The hypothesis space $\\mathcal{H}_{b a g_{a t t}}$ of Attention MIL encompasses the hypothesis space $\\mathcal{H}_{b a g_{n o n-a t t}}$ of Non-Attention MIL: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathcal{H}_{b a g_{n o n-a t t}}\\subseteq\\mathcal{H}_{b a g_{a t t}}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Thus, if $\\mathcal{D}_{X Y}^{I n d}$ is PAC Learnable, Attention MIL is also learnable. ", "page_idx": 20}, {"type": "text", "text": "Ultimately, if $\\mathcal{D}_{X Y}^{I n d}$ is PAC Learnable, then all MIL algorithms can learn from bags. ", "page_idx": 20}, {"type": "text", "text": "C.5 Proof of Theorem 5 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Theorem 5 In $\\mathcal{D}_{X Y}^{G e n}$ , MIL algorithms that aggregate independent hypothesis spaces for each instance must utilize attention satisfying Condition 8 to be PAC learnable for bags. ", "page_idx": 20}, {"type": "text", "text": "Condition 8 The hypothesis space $\\mathcal{H}_{b a g_{a t t}}$ of Attention MIL should be equal to the sum of independent hypothesis spaces $h_{i n s t_{i}}$ multiplied by attention weights $A t t_{i}$ : ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathcal{H}_{b a g_{a t t}}=\\left\\{h_{b a g_{a t t}}\\ |\\ h_{b a g_{a t t}}=\\sum_{i=1}^{n}A t t_{i}\\cdot h_{i n s t_{i}},\\ \\mathrm{where}\\ 0<A t t_{i}<1,\\,\\sum_{i=1}^{n}A t t_{i}=1\\right\\}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof The PAC Learnability of $\\mathcal{D}_{X Y}^{G e n}$ implies that all concept spaces $\\mathcal{C}_{\\mathcal{D}_{x y}^{G e n}}$ of $\\mathcal{D}_{X Y}^{G e n}$ can be approximated by the hypothesis space $\\mathcal{H}_{D_{X Y}^{G e n}}$ of $\\mathcal{D}_{X Y}^{G e n}$ : ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\forall\\mathcal{H}_{D_{X Y}^{G e n}}\\in\\mathcal{C}_{\\mathcal{D}_{x y}^{G e n}}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "$\\mathcal{H}_{D_{X Y}^{G e n}}$ represents the union space of hypotheses $\\mathcal{H}_{i n s t_{i}}^{G e n}$ with dependencies within the same bag. The optimal hypothesis within each instance space can be expressed as: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname*{inf}R_{i n s t_{i}}^{G e n}=\\operatorname*{inf}_{h_{i n s t_{i}}^{G e n}\\in\\mathcal{H}_{i n s t_{i}}^{G e n}}R_{i n s t_{i}}^{G e n}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Combining these optimal hypotheses yields: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\bigcup_{i=1}^{n}\\operatorname*{inf}{R_{i n s t_{i}}^{G e n}}=\\sum_{i=1}^{n}\\operatorname*{inf}{R_{i n s t_{i}}^{G e n}}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "From previous proofs, it\u2019s shown that the Azuma-Hoeffding inequality satisfies: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[\\left|\\operatorname*{inf}{R_{D_{X Y}^{G e n}}}-\\sum_{i=1}^{N}{\\alpha_{i}\\operatorname*{inf}{R_{i n s t_{i}}^{G e n}}}\\right|>\\epsilon\\right]\\leq\\delta\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Thus, the hypothesis space $\\mathcal{H}_{b a g}^{G e n}$ for $\\mathcal{D}_{X Y}^{G e n}$ can be expressed as: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathcal{H}_{D_{X Y}^{G e n}}=\\left\\{h_{D_{X Y}^{G e n}}\\mid h_{D_{X Y}^{G e n}}=\\sum_{i=1}^{n}\\alpha_{i}\\cdot h_{i n s t_{i}},\\mathrm{~where~}0<\\alpha_{i}<1,\\,\\sum_{i=1}^{n}\\alpha_{i}=1\\right\\}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "In proving the learnability of bags for $\\mathcal{D}_{X Y}^{G e n}$ , it\u2019s shown that the hypothesis space for Non-Attention MIL algorithms is: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathcal{H}_{b a g_{n o n-a t t}}=\\bigcup_{i=1}^{n}\\mathcal{H}_{i n s t_{i}}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "The hypothesis space $\\mathcal{H}_{D_{X Y}^{G e n}}$ extends beyond the range of hypotheses that can independently predict all instances, incorporating dependencies with weighted hypotheses $\\alpha_{i}$ : ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathcal{H}_{b a g_{n o n-a t t}}\\subseteq\\mathcal{H}_{D_{X Y}^{G e n}}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathcal{H}_{D_{X Y}^{G e n}}\\subsetneq\\mathcal{H}_{b a g_{n o n-a t t}}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Thus, Non-Attention MIL Algorithms are not learnable from $\\mathcal{D}_{X Y}^{G e n}$ . ", "page_idx": 21}, {"type": "text", "text": "However, according to Condition 8, the hypothesis space of Attention MIL Algorithms is: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathcal{H}_{b a g_{a t t}}=\\left\\{h_{b a g_{a t t}}\\ |\\ h_{b a g_{a t t}}=\\sum_{i=1}^{n}A t t_{i}\\cdot h_{i n s t_{i}},\\ \\mathrm{where}\\ 0<A t t_{i}<1,\\,\\sum_{i=1}^{n}A t t_{i}=1\\right\\}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Here, replacing $A t t_{i}$ with $\\alpha_{i}$ makes $\\mathcal{H}_{b a g_{a t t}}$ identical to $\\mathcal{H}_{D_{X Y}^{G e n}}$ : ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathcal{H}_{b a g_{a t t}}=\\mathcal{H}_{D_{X Y}^{G e n}}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Thus, Attention MIL is learnable from $\\mathcal{D}_{X Y}^{G e n}$ in terms of bags. ", "page_idx": 21}, {"type": "text", "text": "C.6 Proof of Lemma 1 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Lemma 1 Condition 9 serves as a necessary condition for the learnability of instances, when the hypothesis space for the $i^{t h}$ instance of a MIL algorithm is $\\mathcal{H}_{i n s t_{i}}\\cup\\mathcal{H}_{a d d_{i}}$ . Here, $\\mathcal{H}_{i n s t_{i}}$ represents the hypothesis space for the $i^{t h}$ instance, and $\\mathcal{H}_{a d d_{i}}$ denotes the hypothesis space for the $i^{t h}$ instance generated through elements outside the $i^{t h}$ instance. ", "page_idx": 22}, {"type": "text", "text": "Condition 9 $\\mathcal{H}_{a d d_{i}}$ must be a subset of $\\mathcal{H}_{i n s t_{i}}$ : ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathcal{H}_{i n s t_{i}}\\supset\\mathcal{H}_{a d d_{i}}:=\\{h_{a d d_{i}}:\\mathcal{X}_{a d d_{i}}\\to\\mathcal{Y}\\}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof ", "page_idx": 22}, {"type": "text", "text": "When $\\mathcal{H}_{i n s t_{i}}\\nsubseteq\\mathcal{H}_{a d d_{i}}:=\\{h_{a d d_{i}}:\\mathcal{X}_{a d d_{i}}\\to\\mathcal{Y}\\}$ , the hypothesis space $\\mathcal{H}_{b a g}$ for bags is as follows: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathcal{H}_{b a g}=\\bigcup_{i=1}^{N}\\mathcal{H}_{i n s t_{i}}\\cup\\mathcal{H}_{a d d_{i}}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "In this case, using the Azuma-Hoeffding Inequality to compare the deviation between $R_{b a g}$ and each $R_{i n s t_{i}}$ , we get: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\left|\\frac{1}{N}\\sum_{i=1}^{N}R_{i n s t_{i}}-R_{b a g}\\right|\\geq\\epsilon\\right)\\leq\\frac{\\sum_{i=1}^{N}\\mathrm{Var}[R_{i n s t_{i}}]}{N^{2}\\epsilon^{2}}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "If $h_{a d d_{i}}$ is not a subset of $h_{i n s t_{i}}$ , additional hypothesis space exists, resulting in a deviation between the risk of the Bag-level hypothesis and the risks of each Instance. ", "page_idx": 22}, {"type": "text", "text": "Hence, the deviation between $R_{b a g}$ and $R_{i n s t_{i}}$ is greater than or equal to 0: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{h\\in\\mathcal{H}}R_{b a g}\\leq\\sum_{i=1}^{N}\\operatorname*{inf}R_{i n s t_{i}}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Therefore, it fails to satisfy the necessary and sufficient condition for the learnability of instances in $D_{X Y}^{I n d}$ . The independent bag domain space implies that all weights in the general bag domain space are equal, thus the relation is as follows: ", "page_idx": 22}, {"type": "equation", "text": "$$\nD_{X Y}^{I n d}\\subset D_{X Y}^{G e n}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Hence, if it\u2019s not learnable in the independent bag domain space, it means it\u2019s not learnable in the general bag domain space. Therefore, through the proposed theoretical framework, Condition 7 becomes a necessary condition for MIL. ", "page_idx": 22}, {"type": "text", "text": "Thus, Condition 9 becomes a necessary condition for learning instances in MIL ", "page_idx": 22}, {"type": "text", "text": "C.7 Proof of Theorem 6 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Theorem 6 In $\\mathcal{D}_{X Y}^{I n d}$ , MIL algorithms that perform instance-pooling are PAC learnable for instances. Proof ", "page_idx": 22}, {"type": "text", "text": "First, the learnability of MIL algorithms on instances in $\\mathcal{D}_{X Y}^{I n d}$ means the algorithm satisfies Condition 4: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{h\\in\\mathcal{H}}R_{\\mathcal{D}_{X Y}^{I n d}}=\\sum_{i=1}^{N}\\operatorname*{inf}R_{i n s t_{i}}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Performing Instance-Pooling means forming bag-level hypotheses by independently combining hypotheses at each instance level. The error at the bag level derived from instance-level pooling can be defined as follows: ", "page_idx": 23}, {"type": "equation", "text": "$$\nR_{\\mathcal{D}_{X Y}^{I n d}}(h_{b a g})=\\mathbb{E}_{S\\sim\\mathcal{D}_{X Y}^{I n d}}[\\ell(h_{b a g}(X),Y)]\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "We can express the error at each instance level as: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\operatorname*{inf}R_{i n s t_{i}}=\\operatorname*{inf}_{h_{i}\\in\\mathcal{H}_{i n s t_{i}}}R_{\\mathcal{D}_{X_{i n s t_{i}}}^{I n d}Y}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Now, comparing the two errors, we have: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{t\\in\\mathbb{R}}{\\operatorname*{inf}}\\ \\bar{R}_{D_{K}^{\\prime\\prime\\prime}}(h)=\\underset{h\\in\\mathbb{R}}{\\operatorname*{inf}}\\ \\mathbb{E}_{S\\sim\\mathcal{D}_{K}^{\\prime\\prime}}{\\operatorname*{inf}}\\left[\\left<h(X),Y\\right>\\right]}\\\\ &{=\\mathbb{E}_{(X)\\sim\\mathcal{D}_{K}^{\\prime}}\\left[\\underset{h\\in\\mathbb{R}}{\\operatorname*{inf}}\\ \\left<h(X),Y\\right>\\right]}\\\\ &{=\\mathbb{E}_{(X)\\sim\\mathcal{D}_{K}^{\\prime}}\\left[\\underset{i=1}{\\overset{N}{\\sum}}\\underset{h\\in\\mathbb{R}}{\\operatorname*{inf}}\\ \\left<h(X_{i n u_{i},t_{i}}),Y_{i n u_{i},t_{i}}\\right>\\right]}\\\\ &{=\\underset{t=1}{\\overset{N}{\\sum}}\\mathbb{E}_{S\\sim\\mathcal{D}_{K}^{\\prime}}\\frac{L_{1}^{\\prime\\prime}}{\\sum_{s=1}^{N}w_{s,t_{i}}^{2}}\\left[\\underset{h\\in\\mathbb{R}}{\\operatorname*{inf}}\\ \\left<h(X_{i n u_{i},t_{i}}),Y_{i n u_{i},t_{i}}\\right>\\right]}\\\\ &{=\\underset{t=1}{\\overset{N}{\\sum}}\\underset{h\\in\\mathbb{R}}{\\operatorname*{inf}}\\ \\mathbb{E}_{(X_{i n u_{i},t_{i}})\\sim\\mathcal{D}_{K}^{\\prime\\prime}}{\\operatorname*{inf}}\\ \\left[\\left<h_{i}(X_{i n u_{i},t_{i}}),Y_{i n u_{i},t_{i}}\\right>\\right]}\\\\ &{=\\underset{t=1}{\\overset{N}{\\sum}}\\underset{h\\in\\mathbb{R}}{\\operatorname*{inf}}\\ \\underset{R\\in\\mathbb{R}}{\\operatorname*{inf}}\\ \\underset{r\\in\\mathbb{R}}{\\operatorname*{inf}}\\ \\sim\\mathcal{D}_{K_{i n u_{i},t_{i}}^{\\prime\\prime}}^{T_{i n u_{i}}}\\left[\\left<h_{i}(X_{i n u_{i},t_{i}}),Y_{i n u_{i},t_{i}}\\right>\\right]}\\\\ &{=\\underset{t=1}{\\overset{N}{\\sum}}\\underset{h\\in\\mathbb{R}}{\\operatorname*{inf}}\\ \\underset{R\\in\\mathbb{R}}{\\operatorname*{inf}}\\ }\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Thus, performing pooling on instance predictions satisfies Condition 4. ", "page_idx": 23}, {"type": "text", "text": "This proves that MIL algorithms can learn from instances in $\\mathcal{D}_{X Y}^{I n d}$ . ", "page_idx": 23}, {"type": "text", "text": "C.8 Proof of Theorem 7 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Theorem 7 MIL algorithms that perform Embedding-Pooling are not learnable for instances. ", "page_idx": 23}, {"type": "text", "text": "Proof ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In Embedding-pooling, features of each instance are combined using an aggregation function $g$ : ", "page_idx": 23}, {"type": "equation", "text": "$$\nF(X)=g(f_{1}(X_{1}),f_{2}(X_{2}),\\dots,f_{n}(X_{n}))\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Here, $f_{i}(X_{i})$ represents the feature of each $i^{t h}$ instance, and $g$ is a function that integrates features to generate a single vector. Consequently, $\\mathcal{H}_{a d d_{i}}$ includes hypotheses based on $F(X)$ , making $\\mathcal{H}_{a d d_{i}}$ dependent on the features of all instances, $f_{1}(X_{1}),f_{2}(X_{2}),\\ldots,f_{n}(X_{n})$ : ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathcal{H}_{a d d_{i}}=\\{h:F(X)\\to Y\\}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "On the other hand, $\\mathcal{H}_{i n s t_{i}}$ produces results dependent solely on the $i^{t h}$ instance feature: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathcal{H}_{i n s t_{i}}=\\{h_{i}:h_{i}(X_{i})\\rightarrow Y_{i}\\}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "A function $h\\in\\mathcal{H}_{a d d_{i}}$ using $F(X)$ cannot belong to $\\mathcal{H}_{i n s t_{i}}$ . This is because functions in $\\mathcal{H}_{i n s t_{i}}$ only use $X_{i}$ as input, while $h$ is based on $F(X)$ , i.e., the combined result of all $X_{i}$ features. Since ", "page_idx": 23}, {"type": "text", "text": "functions in $\\mathcal{H}_{a d d_{i}}$ have more complex dependencies beyond the scope of $\\mathcal{H}_{i n s t_{i}}$ , the following inequality holds: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathcal{H}_{i n s t_{i}}\\;\\mathcal{D}\\;\\mathcal{H}_{a d d_{i}}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Therefore, MIL algorithms using Embedding-pooling fail to satisfy Condition 9, rendering them incapable of learning from instances. ", "page_idx": 24}, {"type": "text", "text": "C.9 Proof of Theorem 8 ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Theorem 8 If the MIL algorithm does not adhere to Condition 10, it is not learnable for instances. ", "page_idx": 24}, {"type": "text", "text": "Condition 10 The risk $R_{i n s t_{i}}$ for the $i^{t h}$ instance should be as follows: ", "page_idx": 24}, {"type": "equation", "text": "$$\nR_{i n s t_{i}}=\\mathbb{E}_{(x_{i n s t_{i}},y)\\sim D_{X_{i n s t_{i}}}Y}\\ell_{i n s t_{i}}(h,y)\\quad,\\mathrm{where}\\quad h\\in\\mathcal{H}_{i n s t_{i}}\\cup\\mathcal{H}_{b a g-l e v e l i n}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proof ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "If $h_{b a g-l e v e l}$ is not empty, then the following holds: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\forall\\delta>0,\\exists h\\in\\mathcal{H}_{b a g}:R_{b a g}(h)>\\sum_{i=1}^{N}R_{i n s t_{i}}(h_{i n s t_{i}})+\\delta\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "For every $\\delta\\;>\\;0$ , there exists at least one predictor belonging to $h_{b a g-l e v e l}$ that can make the lower bound of $R_{b a g}$ greater than the sum of the lower bounds of $\\textstyle\\sum_{i=1}^{N}R_{i n s t_{i}}$ . This implies that if $h_{b a g-l e v e l}$ is not empty, it fails to satisfy Condition 9. Thus, $h_{b a g-l e v e l}$ must be empty for Condition 4 to hold. ", "page_idx": 24}, {"type": "text", "text": "As a result, Condition 10 also becomes a necessary condition for MIL algorithms to be learnable from instances. ", "page_idx": 24}, {"type": "text", "text": "C.10 Proof of Theorem 9 ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Theorem 9 When MIL algorithms use conjunctive pooling for aggregation in $\\mathcal{D}_{X Y}^{G e n}$ , they are learnable for instances. ", "page_idx": 24}, {"type": "text", "text": "Proof ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Condition 7 being satisfied implies learnability from instances: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{h\\in\\mathcal{H}}R_{\\mathcal{D}_{X Y}^{G e n}}=\\sum_{i=1}^{N}\\alpha_{i}\\operatorname{inf}R_{\\mathrm{inst}_{i}}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Here, $\\alpha_{i}$ represents the weight for the $i^{t h}$ instance, satisfying the following conditions: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\alpha_{i}\\in(0,1),\\sum_{i=1}^{N}\\alpha_{i}=1\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Given Assumption 1 where the MIL algorithm is assumed to be PAC learnable for bags, $R_{\\mathrm{bag}}$ becomes the lower bound of prediction error for bags, and $R_{\\mathrm{inst}_{i}}$ becomes the lower bound of prediction error for each instance. Thus, Condition 7 can be expressed as: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{h\\in\\mathcal{H}}R_{\\mathrm{bag}}=\\sum_{i=1}^{N}\\alpha_{i}\\operatorname*{inf}R_{\\mathrm{inst}_{i}}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "For Conjunctive Pooling, the optimal hypothesis $h_{\\mathrm{bag}}$ for bags is: ", "page_idx": 24}, {"type": "equation", "text": "$$\nh_{\\mathrm{bag}}=\\sum_{i=1}^{N}\\alpha_{i}h_{\\mathrm{inst}_{i}}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "The lower bound for the optimal hypothesis $h_{\\mathrm{inst}_{i}}$ for the $i^{t h}$ instance is: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\operatorname*{inf}R_{\\mathrm{inst}_{i}}=R(h_{\\mathrm{inst}_{i}})\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Therefore, the lower bound of prediction error for the optimal hypothesis $h_{\\mathrm{bag}}$ for bags is: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{inf}_{h\\in\\mathcal{H}}R_{\\mathrm{tsq}}=R(h_{\\mathrm{bag}})}\\\\ &{\\qquad\\qquad=R\\left(\\displaystyle\\sum_{i=1}^{N}\\alpha_{i}h_{\\mathrm{inst}_{i}}\\right)}\\\\ &{\\qquad=\\operatorname*{inf}_{h\\in\\mathcal{H}}\\mathbb{E}_{(x,y)\\sim\\mathcal{D}_{x Y}^{G e n}}\\left[\\ell\\left(\\displaystyle\\sum_{i=1}^{N}\\alpha_{i}h_{\\mathrm{inst}_{i}}(x),y\\right)\\right]}\\\\ &{\\qquad=\\displaystyle\\sum_{i=1}^{N}\\alpha_{i}\\operatorname*{inf}R_{\\mathrm{inst}_{i}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Ultimately, MIL algorithms using Conjunctive Pooling satisfy Condition 7 and are learnable from instances. ", "page_idx": 25}, {"type": "text", "text": "C.11 Proof of Theorem 10 ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Theorem 10 If the MIL algorithm extracts features of instances through RNN-based neural networks for aggregation, it is unable to learn from instances. ", "page_idx": 25}, {"type": "text", "text": "Proof When extracting features through an RNN, the hypothesis space of the $i^{t h}$ instance\u2019s features includes not only the hypothesis spaces based on the $i^{t\\tilde{h}}$ instance\u2019s information but also those based on the information of preceding instances up to the $i^{t h}$ one: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathcal{H}_{i n s t_{i}-r n n}=\\bigcup_{j=1}^{i}\\mathcal{H}_{i n s t_{j}}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Therefore, when extracting features of instances through neural networks like RNNs, if $\\mathcal{H}_{i n s t_{i}}:=$ $\\{h_{i n s t_{i}}:X_{i n s t_{i}}\\to Y\\}$ , then $\\mathcal{H}_{b a g-l e v e l}(x):=\\{h_{b a g-l e v e l}:\\mathcal{X}\\rightarrow\\mathcal{Y}\\}\\neq\\emptyset$ : ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\begin{array}{r l}{R_{i n s t_{i}}=\\mathbb{E}_{(x_{i n s t_{i}},y)\\sim D_{X_{i n s t_{i}}}Y}\\ell_{i n s t_{i}}(h,y)}&{\\mathrm{where}\\quad h\\in\\mathcal{H}_{i n s t_{i}}\\cup\\mathcal{H}_{b a g-l e v e l}}\\\\ {=\\mathbb{E}_{(x_{i n s t_{i}},y)\\sim D_{X_{i n s t_{i}}}Y}\\ell_{i n s t_{i}}(h,y)}&{\\mathrm{where}\\quad h\\in\\mathcal{H}_{i n s t_{i}}\\cup\\mathcal{H}_{i n s t_{i}-r n n},\\mathcal{H}_{i n s t_{i}-r n n}\\neq\\emptyset}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "This means that when performing MIL using features extracted through RNNs, it fails to satisfy Condition 10, a necessary condition for learning from instances. Hence, it becomes incapable of learning from instances. ", "page_idx": 25}, {"type": "text", "text": "Therefore, it fails to satisfy Condition 10. ", "page_idx": 25}, {"type": "text", "text": "As a side note, in the case of extracting features through bidirectional RNNs, the size of the hypothesis space would be: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathcal{H}_{i n s t_{i}-b i r n n}=\\bigcup_{j=1}^{N}\\mathcal{H}_{i n s t_{j}}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "So, the same conclusion applies. ", "page_idx": 25}, {"type": "text", "text": "D Experimental Setting ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "D.1 Experimental Validation of Theorem 5 ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Theorem 5 explains that Instance pooling MIL, which aggregates instance hypothesis spaces independently, does not satisfy Condition 8 and thus is not learnable with respect to $D_{X Y}^{G e n}$ . In contrast, the Conjunctive Pooling model, which multiplies weights by the hypothesis space of instances to reflect the relationships among them, is learnable. ", "page_idx": 26}, {"type": "text", "text": "To validate this, we conduct comparative experiments on the following synthetic datasets, assuming $D_{X Y}^{G e n}$ outside of $D_{X Y}^{I n d}$ . Each bag in the dataset is labeled based on the relationships between the ten instances constituting the bag, and one of four labels is assigned accordingly. ", "page_idx": 26}, {"type": "text", "text": "\u2022 Label 1: If the bag contains both 3 and 5, the bag\u2019s label is 1.   \n\u2022 Label 2: If the bag contains 1 but not 7, the bag\u2019s label is 2.   \n\u2022 Label 3: If the bag contains 1 and also contains 7, the bag\u2019s label is 3.   \n\u2022 Label 0: Any bag that does not meet the criteria for other labels is assigned the negative label 0. ", "page_idx": 26}, {"type": "text", "text": "Under these labeling assumptions, the MNIST dataset\u2019s training data is split into training and testing datasets in an 8:2 ratio. Then, ten images are randomly selected to form a bag, and labeling is performed according to the assumptions. ", "page_idx": 26}, {"type": "image", "img_path": "cUcvlgkQxP/tmp/b76599105c8892bdf0fe7a5195be09742478eea84e90d03961297f7f66e5de1f.jpg", "img_caption": ["Figure 3: Examples of synthetic dataset to verify Theorem 5. "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "To extract features of the synthetic dataset, which assumes each MNIST image as an instance, all models use the feature extractor structure described in Table 6. First, mi-Net [5], Attention MIL [2], Additive MIL [9], and Conjunctive MIL [10] performed Aggregation through basic Instance, Attention, Additive, and Conjunctive-pooling operations, respectively. We used the same classifier structure used to perform the pooling operations described in Appendix A.2. The employed attention mechanism is the gated attention mechanism, which is commonly used in MIL research [2]. Second, Causal MIL [21], MIREL [37], Loss-Attention [38], SA-AbMILP [39], and TransMIL [19] performed additional tuning on pooling for performance improvement. We implemented them using the official code provided by each respective paper23456. Third, for MIREL [37], although the prediction modules for the bag and the instances operate separately, the validation is adjusted to fit the nature of MIL. Specifically, the bag-level prediction module is not used, and instead, the pooled instance predictions are used for bag-level evaluation. Fourth, for Conjunctive MIL [10] and TransMIL [19], while each original paper utilizes positional information that affects the predictions, this factor is excluded from our experiments, as it does not influence the outcomes in our setup. ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "The operational processes and theoretical validation for Causal MIL [21], MIREL [37], LossAttention [38], SA-AbMILP [39], and TransMIL [19] are performed in Appendix F. ", "page_idx": 27}, {"type": "table", "img_path": "cUcvlgkQxP/tmp/44a6996c65ca87c8e3d605dd48df3a166d869b2973f95947649b03dd491ce0f8.jpg", "table_caption": ["Table 6: The architecture of the instance feature extractor. "], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "The hyperparameter settings for the models used in the experiments are shown in Table D. ", "page_idx": 27}, {"type": "table", "img_path": "cUcvlgkQxP/tmp/df29a943c6bd044a21421c9ad64d4c8a6c20ec8c04fb25dbc4cd2b5c93b81725.jpg", "table_caption": ["Table 7: Hyper parameter setting. "], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "Since the labels for the bags are multi-class, we used macro-F1 score (Macro-F1), micro-F1 score (Micro-F1), and weighted-F1 score to measure multi-class classification performance. To ensure generalized results, each algorithm was trained ten times with different initializations, and the average performance was measured. All experiments conducted in this study were performed on an Intel(R) Xeon(R) Silver 4210R 40 Core CPU $\\textcircled{a}\\ 2.40\\,\\mathrm{GHz}$ , 32GB RAM, and NVIDIA RTX A5000. ", "page_idx": 27}, {"type": "text", "text": "D.2 Experimental Validation of Theorems 8 and 9 ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Theorem 8 and Theorem 9 demonstrate that Attention MIL [2] and Additive MIL [9], which perform attention multiplication on instance features, are not capable of learning at the instance level, whereas Conjunctive MIL is capable of instance-level learning. ", "page_idx": 27}, {"type": "text", "text": "To empirically validate this, it is necessary to compare the prediction performance of a Deep MIL model on bags with its performance on instances. Therefore, experiments should be conducted on datasets that have labels for both bags and instances. In this study, we use the labeling criteria for the synthetic dataset explained in Appendix D.1. ", "page_idx": 27}, {"type": "text", "text": "The feature extractor and hyper-parameter settings followed the model structure used in Appendix D.1. The performance measurement method also adhered to the approach specified in Appendix D.1. However, due to the nature of MIL, which involves learning whether a specific instance is positive for each class, we also calculated the average AUROC performance for each class and measured the overall AUROC. ", "page_idx": 27}, {"type": "text", "text": "To analyze the impact of the attention mechanism, we adjusted the temperature parameter $(\\tau)$ used in the attention operations to three values: 0.5, 1, and 2. This process was repeated ten times to calculate the average performance. The average performance of each algorithm for the bag and the average performance difference between the bag and instances are discussed in Section 4.4.3. The performance analysis based on the adjustment of $\\tau$ is detailed in Appendix E.1. ", "page_idx": 27}, {"type": "image", "img_path": "cUcvlgkQxP/tmp/569753f4c29206f6018228a78181f3afe61b8cf52e7f15f8cc056ac8f6d4042b.jpg", "img_caption": ["Figure 4: Example synthetic dataset to verify Theorem 8, 9. "], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "D.3 Experimental Validation of Theorems 10 and 11 ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Theorem 10 and Theorem 11 demonstrate that using an RNN layer to capture positional dependencies or performing positional encoding can act as factors that hinder the learnability of instances. ", "page_idx": 28}, {"type": "text", "text": "To experimentally validate this, we use the WebTraffic dataset [10], which provides label information for each time point in a time-series sliding window data. Both the model structure and hyperparameters are set identical to those in Early et al. [10], and models are configured based on the extent to which features with positional encoding and RNN are used, as following four models7: ", "page_idx": 28}, {"type": "text", "text": "1. A model that does not use features with positional encoding and RNN for instance prediction and attention operations (Default)   \n2. A model that uses features with positional encoding and RNN for both instance prediction and attention operations (All)   \n3. A model that uses features with positional encoding and RNN only for attention operations (Att)   \n4. A model that uses features with positional encoding and RNN only for instance prediction operations (Predict) ", "page_idx": 28}, {"type": "text", "text": "All experiments conducted in this study were performed on an Intel(R) Xeon(R) Silver 4210R 40 Core CPU $\\textcircled{a}\\ 2.40\\,\\mathrm{GHz}$ , 32GB RAM, and NVIDIA RTX A5000. ", "page_idx": 28}, {"type": "text", "text": "To evaluate the instance-level prediction performance of deep MIL models, we used the normalized discounted cumulative gain $(\\mathbf{NDCG}@\\mathbf{n})$ and the area under the perturbation curve with random orderings (AOPCR) metrics, as employed by Early et al. [51, 10]. To reliably measure the performance of each model, we repeated the evaluation 10 times and calculated the average performance. AOPCR assesses the interpretability of time-series data by measuring the decrease in model predictions based on the importance of time points. Starting from the most important time point, the time series is rearranged in order of importance. AOPC is then calculated by removing instances one by one from the most important time points and evaluating how the predictive performance changes. ", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 29}, {"type": "equation", "text": "$$\nA O P C(\\mathbf{X_{i}},\\mathbf{O_{i,c}})=\\frac{1}{t-1}\\sum_{j=1}^{t-1}F_{c}(\\mathbf{X_{i}})-F_{c}(M o R F(\\mathbf{X_{i}},\\mathbf{O_{i,c}},j)),\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Here, the $M o R F$ function removes time points in the given order of importance. Instead of removing each time point individually, time points are grouped into blocks corresponding to $5\\%$ of the total time series, and perturbations are applied until $50\\%$ of the time series is removed, limiting the number of model calls to a maximum of 10. Finally, AOPCR is calculated by comparing the average AOPC value for the given order of importance to the average AOPC value for random orderings to achieve normalization: ", "page_idx": 29}, {"type": "equation", "text": "$$\nA O P C R(\\mathbf{X_{i}},\\mathbf{O_{i,c}})=\\frac{1}{T}\\sum_{r=1}^{T}\\Big(A O P C\\left(\\mathbf{X_{i}},\\mathbf{O_{i,c}}\\right)-A O P C(\\mathbf{X_{i}},\\mathbf{R_{i}^{(r)}})\\Big).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Here, $T$ represents the number of repetitions of random orderings. In this study, $T$ is set to 3 following the settings of Early et al. [10]. ", "page_idx": 29}, {"type": "text", "text": "$\\mathbf{NDCG}@\\mathbf{n}$ is a metric used to evaluate instance predictions by comparing them to the ground truth ordering for a specific class. Instances within a bag are classified as supporting instances, neutral instances, or refuting instances based on their labels. The ideal prediction order for instances should be supporting instances, neutral instances, and then refuting instances for the given class. $\\operatorname{NDCG}@\\mathtt{n}$ is calculated by ranking the instance predictions $\\{\\phi_{1},\\ldots,\\phi_{k}\\}$ for the given class from highest to lowest importance and comparing this order to the ground truth order as follows: ", "page_idx": 29}, {"type": "equation", "text": "$$\nN D C G@n={\\frac{1}{I D C G}}\\sum_{i=1}^{n}{\\frac{\\mathrm{rel}(i)}{\\log_{2}(i+1)}}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Here, IDCG is the ideal discounted cumulative gain, which normalizes the score based on the value of $n$ , where $n$ is the number of instances with a positive label. ", "page_idx": 29}, {"type": "text", "text": "D.4 Experimental Validation of MD-MIL ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "In this section, we conduct additional experiments in an extended scenario of Video Anomaly Detection (VAD) to demonstrate that the framework proposed in this study assists in designing optimal algorithms for realistic MD-MIL scenarios. The traditional VAD problem has been addressed as a type of 1D-MIL problem, as shown in Figure 5 (a), where the objective is to detect anomalous snippets based on labels for the entire video. In this context, each snippet should reflect temporal dependencies. ", "page_idx": 29}, {"type": "text", "text": "Moreover, the conventional VAD problem can be extended as an MD-MIL problem, as depicted in Figure 5 (b). The problem aims to detect both anomalous snippets and anomalous patches within each snippet based on labels for the entire video. In this case, the patches that constitute a snippet must not only reflect relationships with other patches within the same snippet but, much like multivariate time series data [52, 53], also account for temporal dependencies with corresponding patches in other snippets. Therefore, the VAD problem in an MD-MIL context requires consideration of relationships not only among instances within the same bag but also with instances at the same position across different bags. ", "page_idx": 29}, {"type": "text", "text": "To design a model that is capable of learning about instances while reflecting relationships under this complex problem setting, the following requirements must be satisfied: ", "page_idx": 29}, {"type": "text", "text": "1. The VAD problem in MD-MIL should reflect relationships among complex multi-dimensional instances. Therefore, this should be addressed by a learnable MIL method (i.e., Conjunctive-pooling) on DGen. ", "page_idx": 29}, {"type": "image", "img_path": "cUcvlgkQxP/tmp/95a2b443d75a974bf3bcf3dc8e027343f4ab14ab71eba919f3548b2fdcab20e5.jpg", "img_caption": ["(a) The video data structure consisting of 1-(b) The video data structure consisting of dimensional-instances. multi-dimensional-instances. "], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "Figure 5: The video data structure consisting of 1-dimensional and multi-dimensional-instances (Blue: Negative, Red: Positive) [54]. ", "page_idx": 30}, {"type": "text", "text": "2. Additional information about relationships between instances, such as position encoding, should be used in the attention computation process. This ensures learnability about instances and can improve performance, as demonstrated in Section 4.4.3. Based on this, attention operations should incorporate this information to reflect temporal relationships with patches at the same position in other snippets for each patch detection. ", "page_idx": 30}, {"type": "text", "text": "To validate these requirements, we measure and compare the performance between the following three types of MD-MIL models: ", "page_idx": 30}, {"type": "text", "text": "1. None-Attention: This model predicts for higher-dimensional bags through Instance-pooling in each dimension. It independently models the relationships between snippets composing a video and patches composing the same snippet. ", "page_idx": 30}, {"type": "text", "text": "2. Attention: This model predicts for higher-dimensional Bags by performing Conjunctive-Pooling based on attention weights calculated within each dimension (same bag). Through this structure, it can make predictions reflecting the relationships between snippets composing a video and patches composing the same snippet. ", "page_idx": 30}, {"type": "text", "text": "3. Cross-Attention: To reflect the temporal dependencies of patches at the same position in different snippet bags, this model computes attention weights based on features of each patch computed through a bidirectional GRU-layer and performs Conjunctive-pooling. Through this structure, it can make predictions reflecting the relationships between snippets composing a video, patches composing the same snippet, and patches at the same position in different snippets. ", "page_idx": 30}, {"type": "text", "text": "All models are trained using anomaly labels at the video level. The encoder structure for computing instances is identical across models, as shown in the Table 8, with differences only in pooling and attention operations. ", "page_idx": 30}, {"type": "text", "text": "The hyperparameter settings for the models used in the experiments are presented in the following Table 9. ", "page_idx": 30}, {"type": "text", "text": "For this study, we utilized the ShanghaiTech dataset [54, 55], a well-established benchmark dataset for Video Anomaly Detection (VAD). As illustrated in Figure 5 (b), each snippet was pre-divided into four patches. To extract features from these patches, we employed a pre-trained I3D-ResNet model 8. This model was used to extract snippet features for each patch, which were then utilized in our experiments. ", "page_idx": 30}, {"type": "text", "text": "Table 8: The architecture of the instance feature extractor in the MD-MIL model. ", "page_idx": 31}, {"type": "table", "img_path": "cUcvlgkQxP/tmp/b8019dad7f6d1d4bd973a8ecfa0026e95490267381f24f184f7573785afdf47c.jpg", "table_caption": [], "table_footnote": [], "page_idx": 31}, {"type": "table", "img_path": "cUcvlgkQxP/tmp/8db79fb7389beae310739023d49a3d97583dcef678e3da1af6e447fbce84553a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 31}, {"type": "text", "text": "E Additional Experimental Results ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "E.1 Experimental Validation of Theorems 8 and 9 ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "The results of adjusting the variance of attention by applying $\\tau$ in the softmax operation to determine how much attention affects the error between bag prediction performance and instance performance in deep MIL are presented in Table 10. While all Attention-pooling MIL models show high performance for bags, they exhibit low learning performance for instances. Conjunctive-pooling MIL, satisfying Theorem 9, shows very little error between bag performance and instance performance, with instances sometimes even outperforming bags. However, in the case of Additive-pooling MIL, as $\\tau$ increases and the variance among instances decreases, AUROC between bags and instances decreases. This result suggests that for Additive-pooling MIL to be learnable for instances, the strength of the attention must be weak. In other words, the closer the structure is to instance predict-level pooling, the more likely it is to be learnable for instances. ", "page_idx": 31}, {"type": "text", "text": "E.2 Experimental Validation of MD-MIL ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "The experimental results indicate that the Cross-Attention method achieved the best performance, followed by the Attention-driven model, while the None-Attention-driven model showed the least favorable performance. These results demonstrate that 1) attention weights are necessary to capture the relationships between video snippets and patches within the same snippet, and 2) when learning relationships between patches in different snippets (i.e., bags) are required, Cross-Attention-driven model, which utilizes such information only during attention computation, is desired. ", "page_idx": 31}, {"type": "text", "text": "F Interpretation of MIL Algorithms through Our Theoretical Framework ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "In this section, we demonstrate how the proposed theoretical framework can interpret actual existing MIL algorithms. To further validate our framework\u2019s capability to assess instance-level learning in MIL algorithms, we extended our analysis beyond the basic pooling techniques discussed earlier. We additionally selected five MIL algorithms that apply additional tuning to the existing pooling methods for performance improvement. These algorithms were chosen to demonstrate how our framework can generalize to MIL approaches that enhance standard pooling operations through additional optimization techniques. ", "page_idx": 31}, {"type": "text", "text": "1. Causal MIL [21]: This algorithm utilizes the bag-level features as external variables to identify independent latent variables directly related to the prediction of each instance\u2019s feature. It then performs instance-pooling based on these latent variables. Since Causal MIL derives predictions using individual instance features and performs instance-pooling based on these values, it can be learnable on instances under the space $\\bar{D}_{X Y}^{I n d}$ . ", "page_idx": 31}, {"type": "table", "img_path": "cUcvlgkQxP/tmp/6dd63c34ad8e9b64b672545a1d691f392ac58583c31164d6d493baab187e63ed.jpg", "table_caption": ["Table 10: Performance differences for bags vs. instances in the MIL model. "], "table_footnote": [], "page_idx": 32}, {"type": "table", "img_path": "cUcvlgkQxP/tmp/6d88a70cb6fc3f94a6467e7eeea058e101f2664ab44ad8f529f0fad7536b72ad.jpg", "table_caption": ["Table 11: Predicted performance for Snippets (i.e., bags) and patches (i.e., instances) of MD-MIL. "], "table_footnote": [], "page_idx": 32}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "2. Multi-Instance Residual Evidential Learning (MIREL) [37]: This algorithm is composed of two primary modules: 1) a module that predicts the bag label using the average feature of the instances that constitute the bag and 2) a module that performs predictions on individual instances based on their features. MIREL calculates the residual between the bag prediction and the instance predictions, which is indirectly used to perform instance-pooling. The instance prediction module of MIREL operates based on the instance features themselves without applying weighted aggregation through additional information. Thus, it can be learnable on bag predictions under the space $\\bar{D}_{X Y}^{I n d}$ : ", "page_idx": 32}, {"type": "text", "text": "", "page_idx": 33}, {"type": "text", "text": "(a) The loss function of MIREL can be expressed as the sum of two loss terms generated by the bag and instance prediction modules: ", "page_idx": 33}, {"type": "text", "text": "\u2022 $L_{b a g}=\\ell_{b a g}(\\hat{y}_{b a g},y_{b a g})$ , $\\begin{array}{r}{L_{i n s t}=\\sum_{i=1}^{N}\\ell_{i n s t}(\\hat{y}_{i},y_{i})}\\end{array}$   \n\u2022 ${\\cal L}_{t o t a l}={\\cal L}_{b a g}+\\lambda{\\cal L}_{i n s t}$ , where $\\lambda$ is a weighting factor.   \n\u2022 $\\hat{y}_{b a g}$ is the prediction made by the bag prediction module; $y_{b a g}$ is the true label for the bag; $\\hat{y}_{i}$ is the label predicted by the instance prediction module; $y_{i}$ is the label inferred for each instance based on $\\hat{y}_{b a g}$ and residual evidence. ", "page_idx": 33}, {"type": "text", "text": "(b) The risks for the bag, instance, and total can be defined as follows: ", "page_idx": 33}, {"type": "text", "text": "\u2022 $R_{b a g}=E_{(X,Y)\\sim D_{X Y}^{I n d}}[\\ell_{b a g}(\\hat{y}_{b a g},y_{b a g})]$ \u2022 $R_{i n s t_{i}}=E_{(X,Y)\\sim D_{X_{i n s t_{i}}Y}^{I n d}}\\big[\\ell_{i n s t}(\\hat{y}_{i},y_{i})\\big]$ \u2022 $\\begin{array}{r}{R_{t o t a l}=R_{b a g}+\\lambda\\sum_{i=1}^{N}R_{i n s t_{i}}}\\end{array}$ ", "page_idx": 33}, {"type": "text", "text": "(c) Since our theoretical framework evaluates instance learnability under Assumption 1, we can express this as follows: ", "page_idx": 33}, {"type": "text", "text": "\u2022 inf $\\begin{array}{r}{R_{t o t a l}=\\operatorname*{inf}R_{b a g}+\\lambda\\operatorname*{inf}\\sum_{i=1}^{N}R_{i n s t_{i}}}\\end{array}$   \n\u2022 $R_{b a g}$ is learnable under $D_{X Y}^{I n d}$ . Therefore, $\\operatorname*{inf}_{h\\in H}R_{D_{X Y}^{I n d}}=\\operatorname*{inf}R_{b a g}$ holds.   \n\u2022 inf $\\textstyle\\sum_{i=1}^{N}R_{i n s t_{i}}$ is learnable with respect to the instance labels generated by the bag prediction module. Therefore, $\\begin{array}{r}{\\operatorname*{inf}_{h\\in H}R_{D_{X Y}^{I n d}}=\\lambda\\operatorname*{inf}\\sum_{i=1}^{N}R_{i n s t_{i}}}\\end{array}$ .   \n\u2022 $\\lambda$ is a constant, $\\begin{array}{r}{\\operatorname*{inf}_{h\\in H}R_{D_{X Y}^{I n d}}=\\operatorname*{inf}\\sum_{i=1}^{N}R_{i n s t_{i}}}\\end{array}$ . Therefore, Condition 4 holds. ", "page_idx": 33}, {"type": "text", "text": "Therefore, the algorithm is learnable for instances. ", "page_idx": 33}, {"type": "text", "text": "3. Loss-based Attention for Deep Multiple Instance Learning (Loss-Attention) [38]: This algorithm directly connects the attention mechanism with the loss function to simultaneously learn instance weights, instance predictions, and bag predictions. Specifically, Instance weights are calculated based on the loss function. The parameters of the fully connected layer for bag prediction are shared with the instance weight calculation. A regularization term consisting of instance weights and cross-entropy functions is introduced to improve instance recall. A consistent cost is added to smooth the learning process. Through this approach, Loss-Attention can more effectively learn the importance of instances and improve bag classification performance compared to existing attention-based MIL methods. Loss-Attention [38] aims to simultaneously optimize the hypothesis spaces for both instances and bags by sharing parameters between them during the learning process. However, in $D_{X Y}^{G e n}$ , according to Condition 8, while the Loss-Attention algorithm is learnable for bags, it is not learnable for instances. ", "page_idx": 33}, {"type": "text", "text": "4. Self-Attention Attention-based MIL Pooling (SA-AbMILP) [39]: This algorithm enhances the reflection of relationships between instances compared to traditional Attention MIL [2] by employing a self-attention mechanism to compute instance features, which are then used for attention-pooling. Since SA-AbMILP uses transformed instance features that reflect relationships with other instances via self-attention for prediction and attention-pooling, it does not satisfy Condition 10 and is not learnable for instances. ", "page_idx": 33}, {"type": "text", "text": "5. Transformer-based MIL (TransMIL) [19]: This algorithm employs a Transformer module to better capture the relationships between instances. The Transformer module extracts a cls token feature based on the information of instances that reflect their relationships with other instances. TransMIL performs prediction on the bag using the cls token feature, a form of attention-pooling. Since TransMIL uses positional encoding features, its learnability for instances cannot be guaranteed by Theorem 11. Moreover, its attention-pooling does not satisfy Condition 10 and is not learnable for instances. ", "page_idx": 33}, {"type": "text", "text": "G Limitations and Future works ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "According to the MIL survey by Carbonneau et al. [56], recent major issues in MIL have been defined as label ambiguity [57], label noise [58, 59], complex bag composition [60, 61, 62], and handling non-i.i.d. data [63, 64]. This study has only addressed the learnability of instances and did not cover the applicability of these issues. In future work, we plan to utilize the theoretical framework developed in this study to devise countermeasures for issues such as label ambiguity and label noise, aiming to enhance the performance of MIL algorithms through theoretical research. Furthermore, we will seek solutions for problems like complex bag composition and handling non-i.i.d. data based on the theoretical framework of MIL algorithms. Additionally, noting that existing MD-MIL algorithms [6, 20] have extended non-learnable MIL algorithms to multi-dimensional cases, we will devise new MD-MIL methodologies based on the learnable MIL algorithms by our proposed theoretical framework. ", "page_idx": 33}, {"type": "text", "text": "", "page_idx": 34}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: The main contributions of the paper are presented in the abstract and introduction. ", "page_idx": 35}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: We have shown the Limitation and Future work in Appendix G. ", "page_idx": 35}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: The Theorems and Lemmas proposed in the paper are derived from the Definitions and Assumptions defined through Notation. Detailed proofs for each Theorem are provided in the Appendix. ", "page_idx": 35}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: In this study, to validate several final Theorems derived from the initial Theorems, we created appropriate experimental environments for each Theorem and conducted experiments. The results from these experiments provided empirical validation for the Theorems. ", "page_idx": 35}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: In this study, experiments were conducted using the open datasets MNIST and the WebTraffic dataset [10]. For MNIST, since it was used to create synthetic datasets tailored to the experimental environment, we submit the code for data preprocessing and model training. For the WebTraffic dataset [10], as experiments were conducted by swapping models in the code published by Early et al. [10], we submit the code related to the model structures. ", "page_idx": 35}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: We have specified the experimental setting and test details in Appendix D. ", "page_idx": 35}, {"type": "text", "text": "7. Experiment Statistical Significance ", "page_idx": 35}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 35}, {"type": "text", "text": "Answer: [No] ", "page_idx": 35}, {"type": "text", "text": "Justification: Although we did not include error bars, we ensured the reliability of the experiments by repeating each experiment 10 times and taking the average of the performance results. This approach was taken to enhance the statistical reliability of our results. ", "page_idx": 36}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: We specified the sentence \u2019All experiments conducted in this study were performed on an Intel(R) Xeon(R) Silver 4210R 40 Core CPU $\\textcircled{a}\\ 2.40\\,\\mathrm{GHz}$ , 32GB RAM, and NVIDIA RTX A5000.\u2019 in the section describing the experimental settings. ", "page_idx": 36}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: Our research proposes conditions that must be theoretically guaranteed for the MIL model to be learnable on instances. This study poses no ethical issues; rather, it is expected to ensure the reliability of MIL algorithms. ", "page_idx": 36}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: Our research proposes conditions that must be theoretically guaranteed for the MIL model to be learnable on instances. Since MIL research is utilized in fields requiring reliable predictions, such as pathological image diagnosis and video anomaly detection, satisfying these conditions is essential. In this regard, our research has a positive societal impact. ", "page_idx": 36}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: Our research proposes a theoretical framework to evaluate whether MIL algorithms are learnable on instances, which is not associated with such risks. ", "page_idx": 36}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: For experiments extending existing research, we have cited the respective studies and provided the GitHub link in the section describing the experimental settings. ", "page_idx": 36}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: We provide the code for dataset preprocessing, model training, and the model itself as additional submissions. ", "page_idx": 36}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: This study did not involve crowdsourcing. ", "page_idx": 37}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] Justification: This study did not involve crowdsourcing or research with human subjects. ", "page_idx": 37}]