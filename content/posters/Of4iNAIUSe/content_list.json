[{"type": "text", "text": "Resource-Aware Federated Self-Supervised Learning with Global Class Representations ", "text_level": 1, "page_idx": 0}, {"type": "image", "img_path": "Of4iNAIUSe/tmp/05e2f80d15fa3d834df19865c19c916946ab07d880b492d1327c3466ef8de845.jpg", "img_caption": [], "img_footnote": [], "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Due to the heterogeneous architectures and class skew, the global representation models training in resource-adaptive federated self-supervised learning face with tricky challenges: deviated representation abilities and inconsistent representation spaces. In this work, we are the first to propose a multi-teacher knowledge distillation framework, namely FedMKD, to learn global representations with whole class knowledge from heterogeneous clients even under extreme class skew. Firstly, the adaptive knowledge integration mechanism is designed to learn better representations from all heterogeneous models with deviated representation abilities. Then the weighted combination of the self-supervised loss and the distillation loss can support the global model to encode all classes from clients into a unified space. Besides, the global knowledge anchored alignment module can make the local representation spaces close to the global spaces, which further improves the representation abilities of local ones. Finally, extensive experiments conducted on two datasets demonstrate the effectiveness of FedMKD which outperforms state-of-the-art baselines $4.78\\%$ under linear evaluation on average. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The federated self-supervised learning (Fed-SSL) has emerged as a highly promising paradigm due to the extremely limited labeled data in real-world scenarios [4, 25, 30]. The Fed-SSL mechanism can learn common representations collaboratively across all the clients without labeled data [7, 8], which could enable the aggregation of knowledge from diverse unlabeled data sources and overcome the limitations caused by the high cost and scarcity of labeled data [24, 28, 31]. ", "page_idx": 0}, {"type": "text", "text": "Traditional Fed-SSL methods usually assume that each client should train the identical architecture model, such as FedU [33], FedEMA [34], FedCA [30]. But it would not be easy in resource-limited scenarios, especially for the existing arsing large-scale models [1, 26]. As shown in Fig.1, client A, client $B$ and client $C$ might train heterogeneous representation models due to the varying system resources. In addition, real-world data often exhibits skewed class distributions across clients. ", "page_idx": 0}, {"type": "text", "text": "Therefore, how to learn global class representations under the heterogeneous architectures and class skew in resource-aware Fed-SSL paradigm is challenging, particularly comparing with existing $F e d U^{2}$ [17], FedX [5] with identical architectures. ", "page_idx": 1}, {"type": "text", "text": "Although both Hetero-SSFL [20] and FedFoA [19] consider the heterogeneous client models, they can not learn a global representation model. In order to aggregate the knowledge from the clients to form global class representations, some tricky challenges arise. (1) Deviated representation abilities. Even for the same data samples, the different models might encode them into different latent spaces with deviated representation abilities. For example, client $A$ and client $B$ all have images with dog, cat, tiger, but client model A can encode cat, tiger well into different clusters, client model $B$ can only learn better representations of dog. So how could global representation models take advantage of the best of both client models? (2) Inconsistent representation spaces. The skewed class distributions across clients lead to inconsistent representation spaces. For example in Fig. 1, comparing with client $A$ , client $C$ has different kinds of images with dog, cat, airplane. Thus how to make global representation models encode the whole classes from all the clients well in a unified space? Therefore, different from the existing works, our goal is to break the gaps caused by the hybrid heterogeneity, which can learn the high-quality global representation model in federated self-supervised learning. ", "page_idx": 1}, {"type": "text", "text": "Along this line, we propose FedMKD, a multiteacher knowledge distillation based resourceadaptive Fed-SSL framework, which can learn global representations over all classes from heterogeneous clients. First, an adaptive knowledge integration module is introduced to learn high-quality representations from all the heterogeneous models with deviated representation abilities. Then in order to encode all classes from clients in a unified space, the global model uses the weighted combination of selfsupervised loss and distillation loss to update. Besides, the global knowledge anchored alignment module is applied within the server to eliminate the inconsistency in representation spaces and reduce the burden on the clients. It uses global knowledge to additionally update the local models, which can not only make the local representation spaces close to the global space but also improve the representation capability of both local models and the global ones. Code is available at https://github. com/limee-sdu/FedMKD. The main contributions of this paper can be summarized as follows ", "page_idx": 1}, {"type": "image", "img_path": "Of4iNAIUSe/tmp/53503c83b8937420ccfaa771a1439d70af579a4f91d1f022f268abc23ae7e81c.jpg", "img_caption": ["Figure 1: Illustrations of main challenges in resource-aware Fed-SSL. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "\u2022 In resource-aware Fed-SSL, we are the first to delve into global class representation learning through revealing the deviated representation abilities and inconsistent representation spaces caused by the heterogeneous architectures and class skew.   \n\u2022 We design a multi-teacher knowledge distillation framework, namely FedMKD, to adaptively aggregate positive knowledge from heterogeneous models with deviated representation abilities. Through combining the self-supervised loss and the distillation loss, FedMKD can encode skewed classes into a unified space.   \n\u2022 Extensive experiments conducted on CIFAR-10 and CIFAR-100 show the representation abilities over all classes of the FedMKD perform better than state-of-the-art baselines. Our algorithm can improve $4.22\\%$ and $5.31\\%$ separately on the two chosen datasets. ", "page_idx": 1}, {"type": "text", "text": "2 Related work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "The federated self-supervised learning aims to learn high-quality representations from clients without large labeled datasets [11]. From the beginning, several works [12, 23] simply combine federated learning with self-supervised methods. Besides, FedU [33] designs a communication-efficient mechanism by only aggregating the online encoders under non-IID data. FedUTN [16] is proposed to use the aggregated online networks for the target network updating in the self-supervised framework. L-DAWA [21] proposes the layer-wise divergence aware weight aggregation to mitigate the influence of client bias. FedEMA [34] considers the divergence-aware moving average updating in clients, measuring the divergence between local models and global model. FedX [5] proposes a unsupervised federated learning framework to learn representations through a two-sided distillation method. However, all the above works intuitively gain the global model through parameters average due to the identical client models, which can not be applied in the heterogeneous clients setting directly. In addition, although FedCA [30] address the misaligned and inconsistent representation challenges by gathering features from clients, inducing potential privacy problems. FLPD [27, 29] introduces distillation method based similarity between prototypes from a labeled public dataset to update the local model. $F e d U^{2}$ [17] focuses on mitigating representation collapse entanglement and obtaining unified representation spaces. ", "page_idx": 1}, {"type": "table", "img_path": "Of4iNAIUSe/tmp/b3d112b27dfdd594e78b057f8f5a77dbcb5534fd47ecee6f144cda8f4e99c4cf.jpg", "table_caption": ["Table 1: Comparison of federated self-supervised learning methods. "], "table_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Considering heterogeneous client models in federated self-supervised learning, Hetero-SSFL [6] introduces linear-CKA to align lower-dimensional representations between the local model and global model without architectural constraints. FedFoA [19] designs a factorization-based method to extract the cross-feature relation matrix from the local representations for aggregation. However, both Hetero-SSFL and FedFoA can not learn a global representation model considering the hybrid heterogeneity, which is the main focus of our work. The comparison details are shown in Table 1. ", "page_idx": 2}, {"type": "text", "text": "3 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The goal of federated unsupervised learning is to learn the generalized representation for some downstream tasks from several distributed unlabeled data sources. A federated learning setting consists of a central server and $N$ clients. Each client $k$ contains a local unlabeled dataset $\\mathcal{D}_{k}$ , and the server contains a public unlabeled dataset $\\mathcal{D}_{p u b}$ . The local objective at $k$ -th client is ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\theta_{k}}F(\\theta_{k})=\\mathbb{E}_{\\xi_{k}\\sim\\mathcal{D}_{k}}[\\mathcal{L}_{k}(\\theta_{k},\\xi_{k})],\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "to minimize the expected local loss of client $k$ on local dataset $\\mathcal{D}_{k}$ and $\\xi_{k}$ is the unlabeled data. In traditional FL settings, it\u2019s assumed that $\\{\\theta_{k}\\}$ are identical and gain the global model using $\\begin{array}{r}{\\theta=\\sum_{k=1}^{N}p_{k}\\theta_{k}}\\end{array}$ ,e  aw huenrieq $p_{k}$ ims othdee l waenidg thht eo fa $k$ -htiht eccliteunrte.  oBf utth ien  mreoadl-elw omrilgd hctr bose sd-idfefveirceen ts, cewnhairciho s,m eeaacnhs that traditional aggregation methods are not available. We assume that $\\theta_{k}$ is not similar to others, and use $\\{\\theta_{k}\\}$ to collaborate in training the larger global model $\\theta$ in server. Here we define the global update function is $\\theta_{t}=\\mathcal{G}(\\theta_{t-1};\\theta_{1}^{\\bar{\\ i}},\\cdot\\cdot\\cdot,\\theta_{N}^{\\bar{\\ i}})$ . Our final aim is to optimize the global goal ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\theta}F_{g l o b a l}(\\theta)=\\mathbb{E}_{\\xi\\sim\\mathcal{D}_{g l o b a l}}[\\mathcal{L}(\\theta,\\xi)],\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\mathcal{L}(\\boldsymbol{\\theta},\\boldsymbol{\\xi})$ is global loss function in server and $\\xi$ is the unlabeled data sampled from global dataset. ", "page_idx": 2}, {"type": "text", "text": "4 Designed FedMKD Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We propose a multi-teacher knowledge distillation based federated self-supervised learning framework FedMKD, which is shown in Fig. 2. In FedMKD, besides the local self-supervised learning (Sec. 4.1), we design a multi-teacher adaptive knowledge integration distillation module to adaptive determine the weight of the representations from heterogeneous local models with deviated representation abilities. The distilled loss combined with the global self-supervised loss, we can gain the weighted combined loss to update the global model, so that the global model can encode all classes from clients in a unified space (Sec. 4.2). And the global knowledge anchored alignment could improve the representation capability of clients and further benefit the global model training (Sec. 4.3). In addition, we provide the theoretical analysis of our algorithm in Appendix B. ", "page_idx": 2}, {"type": "image", "img_path": "Of4iNAIUSe/tmp/c1025a8f1fdf6bff0d979f01b6eb871f7b6daa912d6a751480f0ff09a3e20b69.jpg", "img_caption": ["Figure 2: The overall framework of FedMKD. Clients initialize the model architecture based on the local resource, then self-supervised train the local model using unlabeled local data. The server uses the multi-teacher adaptive knowledge integration distillation to aggregate positive local knowledge to train the global model and then updates local models again according to the alignment module. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "4.1 Self-supervised model training ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Each client performs self-supervised contrastive learning using an asymmetric Siamese network, inspired by BYOL [3]. The model $\\mathcal{M}$ comprises an online encoder $\\theta$ and a target encoder $\\phi$ , both sharing the same architecture, with the online network incorporating an additional predictor $p$ . That is $\\bar{\\mathcal{M}}\\bar{=}\\,\\{p(\\phi(\\cdot)),\\theta(\\cdot)\\}$ . Given an unlabeled image $x$ , we can obtain two augmented views, $v$ and $v^{\\prime}$ , serving as inputs to online and target networks, respectively. The loss function is defined as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{L}_{s e l f}=\\left\\|\\frac{p(r)}{||p(r)||}-\\frac{r^{\\prime}}{||r^{\\prime}||}\\right\\|^{2},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $r=\\theta(v)$ and $r^{\\prime}=\\phi(v^{\\prime})$ . This loss encourages the online network to produce representation $p(r)$ that is similar to the positive sample generated by the target network $r^{\\prime}$ . We then exchange the views, feeding $v^{\\prime}$ to the online network and $v$ to the target network, to compute $\\mathcal{L}_{s e l f}^{\\prime}$ . At each training step, we use stochastic gradient descent to minimize $\\tilde{\\mathcal{L}}_{s e l f}=\\mathcal{L}_{s e l f}+\\mathcal{L}_{s e l f}^{\\prime}$ to update the online network $\\phi$ alone, ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\theta\\leftarrow\\theta-\\eta\\nabla_{\\theta}\\tilde{\\mathcal{L}}_{s e l f}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The target network helps to provide regression targets to train the online network. Choosing $\\alpha\\in[0,1]$ as the target decay rate, we employ the exponential moving average (EMA) of the online network to update $\\phi$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\phi\\leftarrow\\alpha\\phi+(1-\\alpha)\\theta.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Using this self-supervised training method, the model learns intricate representations from unlabeled data, capturing high-level features and patterns inherent in the dataset. ", "page_idx": 3}, {"type": "text", "text": "4.2 Multi-teacher adaptive knowledge integration distillation. ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In contrast to homogeneous federated learning, the presence of model heterogeneity poses a challenge: direct aggregation of local models into a global model is not feasible. To overcome this, we design a multi-teacher knowledge distillation mechanism to transfer local knowledge to the server. ", "page_idx": 3}, {"type": "text", "text": "Given a batch of data $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ , the representation from the teacher model is denoted as $r_{t}$ and that from the student model as $r_{s}$ , the knowledge distillation loss is defined as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\nL_{d i s t i l l}=-l o g\\frac{e x p(s i m(r_{s,i},r_{t,i})/\\tau)}{e x p(s i m(r_{s,i},r_{t,i})/\\tau)+\\sum_{k\\in\\{{\\cal B}-i\\}}e x p(s i m(r_{s,i},r_{s,k})/\\tau)},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\tau$ is the temperature parameter controlling entropy and $s i m(\\cdot)$ is the similarity function between two representations. ", "page_idx": 4}, {"type": "text", "text": "Then we extend this knowledge distillation learning method to multi-teacher. Although the data is heterogeneous, the knowledge of each local model is valuable, each local model captures the unique characteristics of local data. Our goal is to integrate the positive knowledge of all clients to guide the global model in learning a general representation of unlabeled data. We design a multi-teacher adaptive knowledge integration distillation that can adaptively weigh the representations from clients. ", "page_idx": 4}, {"type": "text", "text": "Given a sample $x_{i}$ , representation from the $n$ -th local model is $R_{n,i}\\in\\mathbb{R}^{d}$ where $d$ is the dimension of the representation. Following [3], a fully connected layer is employed to project the representation into a lower-dimensional space, enhancing the discriminate power of the learned representations. So, we map the representation from the global model $R_{s,i}$ into the same lower latent space, obtaining ", "page_idx": 4}, {"type": "equation", "text": "$$\nr_{s,i}=g(R_{s,i}),r_{n,i}=g(R_{n,i}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $r_{n,i},r_{s,i}\\in\\mathbb{R}^{k}$ , $k$ is the dimension of the new latent space and $g(\\cdot)$ is the projector. ", "page_idx": 4}, {"type": "text", "text": "In addition, we introduce an adapter module to learn instance-level teacher importance weights for knowledge integration. After getting $r_{n,i}$ , an attention block is used to generate the weighted sum of them. In this context, representation from global model $\\boldsymbol{r}_{s,i}$ is treated as the query, while those from local models ${\\tilde{R}}=[r_{1,i},r_{2,i},\\ldots,r_{N,i}]^{T}$ is treated as the $k e y$ and value. Treating representations from the global model as query ensures consistency in knowledge transfer. The attention mechanism computes attention scores to understand the relevance of each local model\u2019s representation to the global model\u2019s query. The aggregated representation is: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\bar{r}_{i}=A t t n(r_{s,i},\\tilde{R})=s o f t m a x(\\frac{r_{s,i}\\cdot\\tilde{R}}{\\sqrt{k}})\\tilde{R},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\bar{r}_{i}$ means the aggregated representation, and $A t t n(\\cdot)$ denotes the attention block. ", "page_idx": 4}, {"type": "text", "text": "Returning to the knowledge distillation for unlabeled data proposed earlier, we treat the aggregate representation as the positive sample, and the remaining samples in the batch as the negative sample. The adaptive weight multi-teacher knowledge distillation loss function is expressed as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\nL_{d i s t i l l}=-l o g\\frac{e x p(s i m(r_{s,i},\\bar{r}_{i})/\\tau)}{e x p(s i m(r_{s,i},\\bar{r}_{i})/\\tau)+\\sum_{j\\neq i}e x p(s i m(r_{s,i},r_{s,j})/\\tau)}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Above all, the weighted combined loss for the global model is presented as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n{\\cal L}_{s e r v e r}={\\cal L}_{s e l f}+\\gamma{\\cal L}_{d i s t i l l},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\gamma$ is a hyper-parameter controlling the weight of the distillation process. ", "page_idx": 4}, {"type": "text", "text": "4.3 Global knowledge anchored alignment ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "As we mentioned, the representations from different models are inconsistent and the representation abilities of models are also deviated. So we introduce the global knowledge anchored alignment mechanism that each local model uses the global model as an anchor. It ensures that the local representation spaces are closer to the global ones. ", "page_idx": 4}, {"type": "text", "text": "Unlike methods such as FedX [5] and MOON [15], which align local models to the global model locally, our approach aims to train a better global encoder tailored for resource-constrained federated learning scenarios. Those methods are not available when clients cannot afford to store or infer the global model locally. So we transfer this alignment process to the server. ", "page_idx": 4}, {"type": "text", "text": "After finishing the global model training, we construct local twin models in the server to realize the alignment under global view. Here, we use the global online network and local online network to ", "page_idx": 4}, {"type": "image", "img_path": "Of4iNAIUSe/tmp/859d329848f625740dfd10d8a35f50ad28f5b29213acaf734b17012638233aad.jpg", "img_caption": ["(a) Standalone training(b) MOON on Partial pub-(c) FedMKD on IID public(d) FedMKD on Partial ResNet18 on Partial publiclic datset. dataset public dataset dataset. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Figure 3: T-SNE visualizations of hidden vectors from different models on CIFAR-10, the data distribution of clients is IID. ", "page_idx": 5}, {"type": "text", "text": "construct a new asymmetric siamese network called the twin of the original local model. The local online network $\\theta_{n}$ is the online model, and the global online network $\\theta_{s}$ is the target model, that is, ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\{\\theta_{n}^{\\prime},\\phi_{n}^{\\prime}\\}\\leftarrow\\{\\theta_{n},\\theta_{s}\\}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "and $\\tilde{M}=\\{\\theta_{n}^{\\prime},\\phi_{n}^{\\prime}\\}$ . The training loss is updated to ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{a l i g n}=-l o g\\frac{e x p(s i m(\\theta_{n}^{\\prime}(i),\\phi_{n}^{\\prime}(i))/\\tau)}{\\sum_{i\\in B}e x p(s i m(\\theta_{n}^{\\prime}(i),\\phi_{n}^{\\prime}(i))/\\tau)}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "According to the idea of contrastive learning, the representations learned by the online network become more consistent with the knowledge captured by the target network. This global knowledge anchored contrastive learning suggests that the global model\u2019s knowledge is used as a positive example for the local model to train itself, thus making it more consistent with the target global network. Aligning local models with the global view representation helps create a comprehensive understanding of the overall data distribution. The process refines the knowledge acquired locally, ensuring that it contributes meaningfully to the overall federated learning process. Then, we use stochastic gradient descent to minimize ${\\mathcal{L}}_{a l i g n}$ to update the $\\theta_{n}^{\\prime}$ , ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\theta_{n}^{\\prime}\\leftarrow\\theta_{n}^{\\prime}-\\eta\\nabla\\mathcal{L}_{a l i g n}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Once the global model anchored alignment is finished, the server will send the online network of the local twin network $\\theta^{\\prime}$ to the corresponding client to update the local model. The local online network $\\theta_{n}$ is replaced by the server-updated network $\\theta_{n}^{\\prime}$ . The target network is not replaced to retain more local knowledge and stabilize model training: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\{\\theta_{q,n},\\phi_{q,n}\\}\\leftarrow\\{\\theta_{q-1,n}^{\\prime},\\phi_{q-1,n}\\}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "so that, the local model can benefti from the alignment process and align to the representation under global view, which can further use the local data to train the model. ", "page_idx": 5}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we evaluate the representations learned from our proposed global model FedMKD on CIFAR-10 and CIFAR-100. We first describe the experimental setup and baselines, and then analyze the performance in comparison to other methods. Due to the space limitation, further hyperparameter analysis and communication cost analysis are represented in Appendix D. ", "page_idx": 5}, {"type": "text", "text": "5.1 Experimental setup ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We use CIFAR-10 and CIFAR-100 [13] datasets to train all the models. Both of them contain 50,000 training images and 10,000 testing images. To construct the public dataset, we sample 4000 data samples from the training set, then divide the remaining data into $N$ partitions to simulate $N$ clients. ", "page_idx": 5}, {"type": "table", "img_path": "Of4iNAIUSe/tmp/d8855df5f33c652a3d11604199f1560e655f7ec8eb116d170a1fe1251af4eace.jpg", "table_caption": ["Table 2: Top-1 accuracy comparison under linear probing on CIFAR datasets with best model performance in bold and second-best results with underlines. \u2019-\u2019 means this method is not suitable for the experiment setting. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "To assess the validity of the public dataset, we use two sampling methods to construct it. First, we use a random sampling method over all classes to generate public dataset \u2019IID\u2019. And for the public dataset \u2019Partial\u2019, data is selected randomly from $40\\%$ classes in two datasets. ", "page_idx": 6}, {"type": "text", "text": "We utilize three settings to simulate heterogeneous data distributions among all the clients. For the IID setting, each client contains the same number of samples from all classes. For the class setting, each client only has $10/N$ and $100/N$ classes on two datasets and the classes between clients have no overlap. For the non-IID setting, data heterogeneity levels are described by the Dirichlet distribution $\\operatorname{Dir}(\\beta)$ [10], where smaller $\\beta$ represents stronger heterogeneity levels, here we choose $\\beta=0.5$ . ", "page_idx": 6}, {"type": "text", "text": "Regarding the self-supervised learning framework design within each client, we use ResNet18 [9] and VGG9 [22] as the encoder network and Multi-Layer Perception (MLP) as the predictor. In order to construct the model heterogeneous setting, 2 clients train the Resnet18 encoder while 3 clients use the VGG9. And for the global representation model, Resnet18 is selected as the encoder in server. ", "page_idx": 6}, {"type": "text", "text": "5.2 Baselines and evaluation methods ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Firstly, we select several federated knowledge distillation frameworks FedMD [14], FedDF [18], FedET [2], MOON [15], MOON-KL that use unlabeled public dataset for distillation. We then replaced the local model with a self-supervised model to evaluate the process of knowledge distillation in our method. And several federated self-supervised learning frameworks FedU [33], FedEMA [34], HeteroSSFL [20] are also chosen as baselines. To verify the client\u2019s knowledge can improve the global model, we also train the global model separately on the public dataset, denoted as Std. ResNet18. Following FedEMA [30], we evaluate the performance of learned representations using linear and semi-supervised evaluation. Due to limited space, please refer to Appendix C for more details. ", "page_idx": 6}, {"type": "text", "text": "5.3 Performance Evaluation ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Table 2 and 3 shows the linear evaluation results and semi-supervised evaluation results of FedMKD compared with all the baselines on CIFAR-10 and CIFAR-100. We can gain the following observation. ", "page_idx": 6}, {"type": "text", "text": "On the whole, our FedMKD outperforms all baselines under different public dataset settings and different data heterogeneity level settings on both two datasets. Compared to the second-best results, FedMKD achieves significant improvement. On average, our model improves CIFAR-10 by $4.22\\%$ and CIFAR-100 by $5.31\\%$ under linear evaluation and gain $3.66\\%$ and $2.07\\%$ improvement on two dataset under semi-supervised evaluation. ", "page_idx": 6}, {"type": "text", "text": "The effectiveness of our multi-teacher adaptive knowledge integration distillation can be approved when compared with FedMD, FedDF, MOON-KL and MOON. Although these methods all designed new federated knowledge distillation frameworks based on unlabeled public dataset, since the original local model is supervised, they prefer using the class information from logits to distill. When observing the result of FedET, we find that although it also designs a larger global model in server which improves the model a lot, the final result is not satisfied. This is also because it designs a distillation method based on knowledge of probability distribution over classes. Next, compared with the federated self-supervised method, our FedMKD also achieves better performance. FedU and FedEMA are not applicable in model heterogeneity setting, so we cannot evaluate their effectiveness. Hetero-SSFL gain the best performance among all baselines but is worse than ours. That\u2019s because it aims to train personalised client models. Only the alignment module cannot hold the inconsistent representation space perfectly. But our global model can directly generate representation from the global model, it avoids using the representation from inconsistent clients. ", "page_idx": 6}, {"type": "table", "img_path": "Of4iNAIUSe/tmp/26818af1005adafa9a1ebbbe03ac89cf281e8e13dca2a70b8d0a73df525215ca.jpg", "table_caption": ["Table 3: Top-1 accuracy comparison on $1\\%$ of labeled data for semi-supervised learning on CIFAR datasets with best model performance in bold and second-best results with underlines. \u2019-\u2019 means this method doesn\u2019t apply for the experiment setting. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Apart from the client data heterogeneity, we consider the influence of the public dataset distribution. Here, we construct two public datasets, one is \u2019IID\u2019 to the whole data distribution and the other only has partial classes. In both two settings, our FedMKD also gets the best performance. The overall performance of \u2019IID\u2019 public dataset is better than \u2019Partial\u2019 setting. That\u2019s because the global model adapts the self-supervised learning on public dataset, and the diversity of the sample is important, which can help the model explore a wide range of features and patterns present in the data. And it\u2019s observed that when the public dataset is \u2019IID\u2019, the performance increases with the decrease of the data heterogeneity level for CIFAR-10, but it doesn\u2019t apply to the CIFAR-100. It\u2019s because there are too many classes in CIFAR-100 and the number of samples in each class is not efficient. ", "page_idx": 7}, {"type": "text", "text": "In order to evaluate the effectiveness of FedMKD, we use the dimensionality reduction algorithm t-sne to visualize the representation on the test dataset of CIFAR-10 from different encoders. As shown in Fig. 3 (b)(c)(d), the global models in FedMKD trained on both \u2019IID\u2019 and \u2019Partial\u2019 public datasets both achieve better clustering results than Standalone training and MOON. These results further verify our model can gain better generalized representations although the representation spaces of clients are inconsistent. There\u2019s also an averaged global model in MOON, but it cannot tackle the problem of inconsistent spaces well using the average method, so it only gains a poor clustering performance. Additionally, the performance on \u2019IID\u2019 public dataset is better than \u2019paritial\u2019 ones from the observation of cluster performance. This suggests that the number of classes seen by the global model also affects how well the global model can encode all classes in a unified space. Comparing the class distributions in Fig. 3 (c) and (d), we can find that although these two global models are trained on different public datasets, the final cluster layout is similar, which can further prove that our global model can encode all classes from clients even if it never sees some classes during the training. ", "page_idx": 7}, {"type": "text", "text": "Table 4: Experimental results on ablation studies of FedMKD with best model performance in bold. ", "page_idx": 8}, {"type": "table", "img_path": "Of4iNAIUSe/tmp/b92db0f65b9455c84643e1db094c9ed225b7f3344527550540975cc56b9b9c1d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "5.4 Improvement of clients ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In FedMKD, the global knowledge anchored alignment module is used to align the client model in the server which can further transfer the global knowledge to the client and incentivize the clients to participate in the federated learning. To evaluate the improvement of the clients, the local models which are standalone training locally are compared with our local models. As shown in Fig. 4, the client performance in our FedMKD framework is better than local standalone training, regardless of the archi", "page_idx": 8}, {"type": "image", "img_path": "Of4iNAIUSe/tmp/b2fa5318d43b2e21376a17bb1130c8c0042ee7ddf37d28175643d8afba5b9c14.jpg", "img_caption": ["Figure 4: Improvement of clients after involving our proposed FedMKD. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "tecture of the local model. Especially for clients with ResNet18, it improves $6.73\\%$ on average. It\u2019s concluded that clients benefit from federated training by contributing to global training. ", "page_idx": 8}, {"type": "text", "text": "5.5 Validation of inconsistent representation spaces ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "As we mentioned, the representation spaces between different clients are inconsistent because of data heterogeneity. To validate this opinion, we use Linear Discriminant Analysis (LDA) to reduce dimensionality in order to visualize the distribution of the representations. Here the data distribution of clients is Class. In Fig. 5 left, \u2019\u25e6\u2019 and $\\mathbf{\\Psi}^{,}\\times\\mathbf{\\Psi}^{,}$ denote representations of Client A and Client B, respectively. And different colors denote different classes. We can observe that in Fig. 5 left the classes \u2019cat\u2019 and \u2019dog\u2019 almost overlap while they are from different clients, which verifies that the inconsistent representation spaces did exactly exist. And Fig. 5 right shows the visualization result of the represen", "page_idx": 8}, {"type": "image", "img_path": "Of4iNAIUSe/tmp/c8d8afd55e7347cb356fe7f4901f6b41041e997cff4123f7dd30a3fef8822bda.jpg", "img_caption": ["Figure 5: LDA visualizations of hidden vectors from different models on CIFAR-10. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "tation from the global model. It\u2019s clear that the classes \u2019cat\u2019 and \u2019dog\u2019 are embedded in different positions in global space, which proves that although the local representations are inconsistent, our global model can learn a good representation. ", "page_idx": 8}, {"type": "text", "text": "5.6 Ablation experiment ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In order to investigate the effectiveness of different parts of FedMKD, we design these comparison experiments: ", "page_idx": 8}, {"type": "text", "text": "\u2022 Standalone training: The global model is trained alone using the public dataset, without the knowledge from client models. \u2022 $F e d M K D_{K L}$ : The global distillation function is replaced by the KL-divergence function to measure the similarity of the aggregated representation $\\bar{r}$ and global representation $r_{s}$ . \u2022 $F e d M K D_{w/c}$ adaptive: The adaptive knowledge integration module is removed, each local representation has the same weight to generate the aggregated representation. ", "page_idx": 8}, {"type": "table", "img_path": "Of4iNAIUSe/tmp/31257c6244be879e45a2ecb02da87c49f5cd5b98aa0987f5487e25e83da0975f.jpg", "table_caption": ["Table 5: Experimental results on scalability studies of FedMKD. ", "\u2022 $F e d M K D_{w/c}$ alignment: The global knowledge anchored alignment module is removed from FedMKD. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "Above experiment is conducted on the both CIFAR-10 and CIFAR-100 dataset, using the IID public dataset. The results of the ablation study experiment are shown in Table 4. When comparing FedMKD with $F e d M K D_{K L}$ , significant performance drop can be observed when the knowledge distillation function is replaced by the KL-divergence function. Thus we can conclude that for self-supervised learning, we prefer performing knowledge distillation based on representation, but the KL-divergence cannot capture the distribution characteristics from them. Therefore, the appropriate distillation method is critically important in self-supervised learning due to the lack of labels. And a worse performance on both two datasets can be observed when we use the equal weight instead of the adaptive weight. Because the client models are heterogeneous, their representation capabilities are different and the representation spaces are also inconsistent, so intuitively average representations may reduce the information contained in the representation. Finally, when we remove the alignment module, the performance under each setting all decreases, which demonstrates that the alignment is not only beneficial to the local models, but also improves the whole training process. ", "page_idx": 9}, {"type": "text", "text": "5.7 Scalability of algorithm ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In order to explore the scalability of our proposed algorithm FedMKD, we add the experiment that the number of clients is 5, 10, 30 on CIFAR-10, and $40\\%$ clients use the VGG model and $60\\%$ use ResNet18. And we repartition the data for each client under $\\operatorname{Dir}(\\beta=0.5)$ and set the public dataset distribution as \u2019IID\u2019. The results are shown in Table 5. We can find that as the number of clients increasing, the performance decreases. The reason is that the total number of data is fixed, if the number of clients increases, the number of data in each client will decrease, which further affect the performance of the local model. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we focused on how to solve the deviated representation abilities and inconsistent representation spaces caused by the heterogeneous architectures and class skew in federated selfsupervised learning. We proposed a multi-teacher knowledge based federated self-supervised learning framework FedMKD to learn a global model. Firstly, the adaptive knowledge integration module could learn high-quality representation knowledge from heterogeneous models. And the combination of the self-supervised loss and the distillation loss enabled the global model to encode all classes from clients in a unified space. Then a global knowledge anchored alignment module improved the local representation models in server and fed it back to corresponding clients. The experiments conducted on two datasets demonstrated that our proposed FedMKD was state-of-the-art and outperformed existing methods. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgement ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was supported in part by the National Natural Science Foundation of China under Grant 62202273, 62176014, 92370204, in part by National Science Fund for Excellent Young Scholars of China under Grant 62122042, in part by Shandong Provincial Natural Science Foundation of China under Grant ZR2021QF044, in part by the Fundamental Research Funds for the Central Universities, in part by the Major Basic Research Program of Shandong Provincial Natural Science Foundation under Grant ZR2022ZD02, in part by the Joint Key Funds of National Natural Science Foundation of China under Grant U23A20302, in part by Guangzhou-HKUST(GZ) Joint Funding Program under Grant 2023A03J0008, in part by the Education Bureau of Guangzhou Municipality. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.   \n[2] Yae Jee Cho, Andre Manoel, Gauri Joshi, Robert Sim, and Dimitrios Dimitriadis. Heterogeneous ensemble knowledge transfer for training large models in federated learning. arXiv preprint arXiv:2204.12703, 2022.   \n[3] Jean-Bastien Grill, Florian Strub, Florent Altch\u00e9, Corentin Tallec, Pierre H. Richemond, Elena Buchatskaya, Carl Doersch, Bernardo \u00c1vila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, Bilal Piot, Koray Kavukcuoglu, R\u00e9mi Munos, and Michal Valko. Bootstrap your own latent - A new approach to selfsupervised learning. In Hugo Larochelle, Marc\u2019Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020.   \n[4] Wei Guo, Fuzhen Zhuang, Xiao Zhang, Yiqi Tong, and Jin Dong. A comprehensive survey of federated transfer learning: challenges, methods and applications. Frontiers Comput. Sci., 18(6):186356, 2024.   \n[5] Sungwon Han, Sungwon Park, Fangzhao Wu, Sundong Kim, Chuhan Wu, Xing Xie, and Meeyoung Cha. Fedx: Unsupervised federated learning with cross knowledge distillation. In European Conference on Computer Vision, pages 691\u2013707. Springer, 2022.   \n[6] Chaoyang He, Zhengyu Yang, Erum Mushtaq, Sunwoo Lee, Mahdi Soltanolkotabi, and Salman Avestimehr. Ssfl: Tackling label deficiency in federated learning via personalized self-supervision. arXiv preprint arXiv:2110.02470, 2021.   \n[7] Jingxuan He, Lechao Cheng, Chaowei Fang, Zunlei Feng, Tingting Mu, and Mingli Song. Progressive feature self-reinforcement for weakly supervised semantic segmentation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 2085\u20132093, 2024.   \n[8] Jingxuan He, Lechao Cheng, Chaowei Fang, Dingwen Zhang, Zhangye Wang, and Wei Chen. Mitigating undisciplined over-smoothing in transformer for weakly supervised semantic segmentation. arXiv preprint arXiv:2305.03112, 2023.   \n[9] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.   \n[10] Tzu-Ming Harry Hsu, Hang Qi, and Matthew Brown. Measuring the effects of non-identical data distribution for federated visual classification. CoRR, abs/1909.06335, 2019.   \n[11] Yilun Jin, Yang Liu, Kai Chen, and Qiang Yang. Federated learning without full labels: A survey. arXiv preprint arXiv:2303.14453, 2023.   \n[12] Yilun Jin, Xiguang Wei, Yang Liu, and Qiang Yang. Towards utilizing unlabeled data in federated learning: A survey and prospective. arXiv preprint arXiv:2002.11545, 2020.   \n[13] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.   \n[14] Daliang Li and Junpu Wang. Fedmd: Heterogenous federated learning via model distillation. arXiv preprint arXiv:1910.03581, 2019.   \n[15] Qinbin Li, Bingsheng He, and Dawn Song. Model-contrastive federated learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10713\u201310722, 2021.   \n[16] Simou Li, Yuxing Mao, Jian Li, Yihang Xu, Jinsen Li, Xueshuo Chen, Siyang Liu, and Xianping Zhao. Fedutn: federated self-supervised learning with updating target network. Applied Intelligence, 53(9):10879\u2013 10892, 2023.   \n[17] Xinting Liao, Weiming Liu, Chaochao Chen, Pengyang Zhou, Fengyuan Yu, Huabin Zhu, Binhui Yao, Tao Wang, Xiaolin Zheng, and Yanchao Tan. Rethinking the representation in federated unsupervised learning with non-iid data. ArXiv, abs/2403.16398, 2024.   \n[18] Tao Lin, Lingjing Kong, Sebastian U Stich, and Martin Jaggi. Ensemble distillation for robust model fusion in federated learning. Advances in Neural Information Processing Systems, 33:2351\u20132363, 2020.   \n[19] Yi Liu, Song Guo, Jie Zhang, Qihua Zhou, Yingchun Wang, and Xiaohan Zhao. Feature correlation-guided knowledge transfer for federated self-supervised learning. arXiv preprint arXiv:2211.07364, 2022.   \n[20] Disha Makhija, Nhat Ho, and Joydeep Ghosh. Federated self-supervised learning for heterogeneous clients. arXiv preprint arXiv:2205.12493, 2022.   \n[21] Yasar Abbas Ur Rehman, Yan Gao, Pedro Porto Buarque de Gusmao, Mina Alibeigi, Jiajun Shen, and Nicholas D Lane. L-dawa: Layer-wise divergence aware weight aggregation in federated self-supervised visual representation learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 16464\u201316473, 2023.   \n[22] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.   \n[23] Bram van Berlo, Aaqib Saeed, and Tanir Ozcelebi. Towards federated unsupervised representation learning. In Proceedings of the third ACM international workshop on edge systems, analytics and networking, pages 31\u201336, 2020.   \n[24] Kun Wang, Hao Liu, Lirong Jie, Zixu Li, Yupeng Hu, and Liqiang Nie. Explicit granularity and implicit scale correspondence learning for point-supervised video moment localization. In Proceedings of the 32nd ACM International Conference on Multimedia, MM \u201924, page 9214\u20139223, New York, NY, USA, 2024. Association for Computing Machinery.   \n[25] Lirui Wang, Kaiqing Zhang, Yunzhu Li, Yonglong Tian, and Russ Tedrake. Does learning from decentralized non-iid unlabeled data benefit from self supervision? In The Eleventh International Conference on Learning Representations, 2022.   \n[26] Yangyang Wang, Xiao Zhang, Mingyi Li, Tian Lan, Huashan Chen, Hui Xiong, Xiuzhen Cheng, and Dongxiao Yu. Theoretical convergence guaranteed resource-adaptive federated learning with mixed heterogeneity. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD \u201923, page 2444\u20132455, New York, NY, USA, 2023. Association for Computing Machinery.   \n[27] Yuzhu Wang, Lechao Cheng, Manni Duan, Yongheng Wang, Zunlei Feng, and Shu Kong. Improving knowledge distillation via regularizing feature norm and direction. arXiv preprint arXiv:2305.17007, 2023.   \n[28] Ziming Ye, Xiao Zhang, Xu Chen, Hui Xiong, and Dongxiao Yu. Adaptive clustering based personalized federated learning framework for next poi recommendation with location noise. IEEE Transactions on Knowledge and Data Engineering, 36(5):1843\u20131856, 2024.   \n[29] Chen Zhang, Yu Xie, Tingbin Chen, Wenjie Mao, and Bin Yu. Prototype similarity distillation for communication-efficient federated unsupervised representation learning. IEEE Transactions on Knowledge and Data Engineering, pages 1\u201313, 2024.   \n[30] Fengda Zhang, Kun Kuang, Long Chen, Zhaoyang You, Tao Shen, Jun Xiao, Yin Zhang, Chao Wu, Fei Wu, Yueting Zhuang, et al. Federated unsupervised representation learning. Frontiers of Information Technology & Electronic Engineering, 24(8):1181\u20131193, 2023.   \n[31] Xiao Zhang, Ziming Ye, Jianfeng Lu, Fuzhen Zhuang, Yanwei Zheng, and Dongxiao Yu. Fine-grained preference-aware personalized federated poi recommendation with data sparsity. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR \u201923, page 413\u2013422, New York, NY, USA, 2023. Association for Computing Machinery.   \n[32] Weiming Zhuang, Xin Gan, Yonggang Wen, and Shuai Zhang. Easyfl: A low-code federated learning platform for dummies. IEEE Internet of Things Journal, 2022.   \n[33] Weiming Zhuang, Xin Gan, Yonggang Wen, Shuai Zhang, and Shuai Yi. Collaborative unsupervised visual representation learning from decentralized data. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 4912\u20134921, 2021.   \n[34] Weiming Zhuang, Yonggang Wen, and Shuai Zhang. Divergence-aware federated self-supervised learning. In International Conference on Learning Representations, 2022. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "We provide more details about our work and experiments in the appendices: ", "page_idx": 12}, {"type": "text", "text": "\u2022 Appendix A: the additional details of the proposed algorithm FedMKD. \u2022 Appendix B: the detailed proof of the convergence analysis of our proposed algorithm FedMKD. \u2022 Appendix C: the details of experimental settings including datasets and federated simulations, reproduction details, baselines, evaluation methods and ablation experiment setting. \u2022 Appendix D: additional experimental results including Appendix D.1, the overall performance under two evaluation methods; Appendix D.2, the communication and storage efficiency analysis; Appendix D.3 discusses the impact of hyperparameters. \u2022 Appendix E: the details of limitations and broader impacts of this work. ", "page_idx": 12}, {"type": "text", "text": "A Algorithm description ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "We outline the FedMKD algorithm in Algorithm 1. In round $q$ , the clients and server perform the following updates: ", "page_idx": 12}, {"type": "text", "text": "\u2022 Starting from the resource-adaptive model $\\theta_{q,n,0}$ , we update the local parameters for $t\\in[T]$ ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\qquad\\theta_{q,n,t}\\leftarrow\\theta_{q,n,t-1}-\\eta\\nabla l_{q,n,t}.}\\\\ &{\\qquad\\phi_{q,n,t}\\leftarrow\\alpha\\phi_{q,n,t-1}+(1-\\alpha)\\theta_{q,n,t}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "\u2022 After $T$ times local updates, upload the local parameter $\\theta_{q,n,T}$ . ", "page_idx": 12}, {"type": "text", "text": "\u2022 Server uses all local model to process multi-teacer adaptive knowledge integration distillation, update the global model for $t\\in[T^{\\prime}]$ , ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\theta_{q,s}\\gets\\theta_{q,s}-\\eta\\nabla l_{q,s}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "equation", "text": "$$\n\\phi_{q,s}\\leftarrow\\alpha\\phi_{q-1,s}+(1-\\alpha)\\theta_{q,s}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "\u2022 Server aligns all client models to the global model, for $n\\in N$ , ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\theta_{q,n}^{\\prime}\\leftarrow\\theta_{q,n}^{\\prime}-\\eta\\nabla l_{q,n}^{\\prime}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "B Convergence Analysis ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "In this section, we show the convergence analysis of our FedMKD. Firstly, we give some commonly used assumptions in federated learning: ", "page_idx": 12}, {"type": "text", "text": "Assumption 1 (Lipschitz Condition). Every function $F(\\cdot)$ is with $L$ -Lipschitz gradient: $\\forall n\\ \\in$ $[N],\\theta,\\varphi\\in R^{d}$ ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\lVert\\nabla F({\\boldsymbol{\\theta}})-\\nabla F({\\boldsymbol{\\varphi}})\\rVert\\leq L\\lVert{\\boldsymbol{\\theta}}-{\\boldsymbol{\\varphi}}\\rVert\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Assumption 2 (Bounded variance). The stochastic gradients $\\nabla F_{n}\\big(\\theta_{q,n,t};\\xi_{n,t}\\big)$ is an unbiased estimator of the gradient, with the variance bounded by $\\sigma>0$ : ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\xi_{n,t}\\sim D_{n}}\\|\\nabla F_{n}(\\theta_{q,n,t};\\xi_{n,t})-\\nabla F_{n}(\\theta_{q,n,t})\\|^{2}\\le\\sigma^{2},\\quad\\forall q,n,t}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "In order to analyze the convergence rate of our proposed FedMKD, we firstly state some preliminary lemmas as follows: ", "page_idx": 12}, {"type": "text", "text": "Lemma 1 (Jensen\u2019s inequality). For any convex function $h$ and any variable $x_{1},\\ldots,x_{n}$ we have ", "page_idx": 12}, {"type": "equation", "text": "$$\nh(\\frac{1}{n}\\sum_{i=1}^{n}x_{i})\\leq\\frac{1}{n}\\sum_{i=1}^{n}h(x_{i}).\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Especially, when $h(x)=\\|x\\|^{2}$ , we can get ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\|\\frac{1}{n}\\sum_{i=1}^{n}x_{i}\\|^{2}\\leq\\frac{1}{n}\\sum_{i=1}^{n}\\|x_{i}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "1 Initialize: total number of clients $N$ ; Number of rounds $T$ ; Local data in clients $\\{D_{1},...,D_{N}\\}$ ; Public data in server $D_{p u b}$ ; Local online network $\\{\\theta_{1},\\hdots,\\theta_{N}\\}$ , target network $\\{\\phi_{1},\\ldots,\\phi_{N}\\}$ ; Server online network $\\theta_{s}$ , target network $\\phi_{s}$ ; Learning rate $\\eta$ ; Adapter network $A t t n(\\cdot)$ . for $q=1$ to $Q$ do   \n2 LocalUpdate: for $n=1$ to $N$ (all clients in parallel) do   \n3 if $\\overline{{q=1}}$ then   \n4 Random initialize \u03b8q,n,0.   \n5 else   \n6 $\\begin{array}{r l}{|}&{{}\\{\\theta_{q,n,0},\\phi_{q,n,0}\\}\\leftarrow\\{\\theta_{q-1,n}^{\\prime},\\phi_{q-1,n,T}\\}.}\\end{array}$   \n7 end   \n8 for epoch $t=1$ to $T$ do   \n9 $l_{q,n,t}=\\mathcal{L}_{s e l f}(\\theta_{q,n,t-1};\\phi_{q,n,t-1};D_{n}).$   \n10 $\\theta_{q,n,t}\\gets\\theta_{q,n,t-1}-\\eta\\nabla l_{q,n,t}.$   \n11 $\\phi_{q,n,t}\\leftarrow\\alpha\\phi_{q,n,t-1}+(1-\\alpha)\\theta_{q,n,t}$ .   \n12 end   \n13 Upload $\\theta_{q,n,t}$ . end ServerExecution: # Multi-teacher adaptive knowledge integration distillation. for epoch $t=1$ to $T^{\\prime}$ do   \n18 for batch $b\\in D_{p u b}$ do   \n19 for $n=1$ to $N$ do   \n20 $r_{q,n}=g(\\theta_{q,n,t}(b)).$   \n21 end   \n22 $\\begin{array}{r l}&{\\tilde{R}=[r_{q,1},\\cdots,r_{q,N}]}\\\\ &{r_{s}=g(\\theta_{q,s}(b)).}\\\\ &{\\bar{r}=A t t n(r_{s},\\tilde{R})}\\\\ &{l_{q,s}=\\mathcal{L}_{s e l f}(\\theta_{q,s};\\phi_{q,s};b)+\\gamma\\mathcal{L}_{d i s t i l l}(\\theta_{q,s};r_{s};\\bar{r}).}\\\\ &{\\theta_{q,s}\\leftarrow\\theta_{q,s}-\\eta\\nabla l_{q,s}.}\\\\ &{\\phi_{q,s}\\leftarrow\\alpha\\phi_{q-1,s}+(1-\\alpha)\\theta_{q,s}.}\\end{array}$   \n23   \n24   \n25   \n26   \n27   \n28 end end # Alignment client models in server. for $n=1$ to $N$ do   \n32 $\\{\\theta_{q,n}^{\\prime},\\phi_{q,n}^{\\prime}\\}\\leftarrow\\{\\theta_{q,n,T},\\theta_{q,s}\\}$   \n33 $l_{q,n}^{\\prime}=\\mathcal{L}_{a l i g n}(\\theta_{q,n}^{\\prime};\\phi_{q,n}^{\\prime};D_{p u b}).$   \n34 $\\theta_{q,n}^{\\prime}\\gets\\theta_{q,n}^{\\prime}-\\eta\\nabla l_{q,n}^{\\prime}$ . end Send $\\theta_{q,n}^{\\prime}$ to client $n$ .   \n37 end ", "page_idx": 13}, {"type": "text", "text": "14   \n15   \n16   \n17   \n29   \n30   \n31   \n35   \n36 ", "page_idx": 13}, {"type": "text", "text": "Lemma 2 For random variable $x_{1},\\ldots,x_{n}$ we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\|x_{1}+\\cdot\\cdot\\cdot+x_{n}\\|^{2}]\\leq n\\mathbb{E}[\\|x_{1}\\|^{2}+\\cdot\\cdot\\cdot+\\|x_{n}\\|^{2}].\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Lemma 3 For independent random variables $x_{1},\\ldots,x_{n}$ whose mean is $\\begin{array}{l}{\\displaystyle{O,}}\\end{array}$ , we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\|x_{1}+\\cdot\\cdot\\cdot+x_{n}\\|^{2}]=\\mathbb{E}[\\|x_{1}\\|^{2}+\\cdot\\cdot\\cdot+\\|x_{n}\\|^{2}].\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Based on the above assumptions, we present the theoretical results for the non-convex problem. ", "page_idx": 13}, {"type": "text", "text": "Lemma 4 (Deviation bound of the optimization function) In each communication round, the function value in server reduce after $T^{\\prime}$ epochs and is bounded as: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbb{E}[F(\\theta_{q,T^{\\prime}})]\\leq\\mathbb{E}[F(\\theta_{q,0})]-(\\eta-\\frac{L\\eta^{2}}{2})\\sum_{t=1}^{T^{\\prime}}\\|\\nabla F(\\theta_{q,t})\\|^{2}+\\frac{L T^{\\prime}\\eta^{2}}{2}\\sigma^{2}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Lemma 4 indicates the deviation bound of the optimization function of the server in each global round. ", "page_idx": 14}, {"type": "text", "text": "Theorem 1 (Non-convex divergence for FedMKD) Let Assumption $^{\\,l}$ to 3 hold and $\\Delta=F(\\theta_{0})-$ $F(\\theta_{T^{\\prime}})$ , given any $\\delta>0$ , suppose that the learning rates satisfy $0\\le\\eta\\le2/L$ , after ", "page_idx": 14}, {"type": "equation", "text": "$$\nQ=\\frac{2\\Delta}{T^{\\prime}\\delta(2\\eta-L\\eta^{2})-T^{\\prime}L\\eta^{2}\\sigma^{2}}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "communication round, we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\frac{1}{Q T^{\\prime}}\\sum_{q=0}^{Q-1}\\sum_{t=0}^{T^{\\prime}-1}\\mathbb{E}[\\nabla\\mathcal{L}_{q,t})]\\leq\\delta\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "the convergence can be guaranteed. ", "page_idx": 14}, {"type": "text", "text": "Proof. Let\u2019s start the proof from $L$ -Lipschitz condition: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{F(\\theta_{q,t+1})\\overset{(a)}{\\leq}F(\\theta_{q,t})+\\langle\\nabla F(\\theta_{q,t}),\\theta_{q,t+1}-\\theta_{q,t}\\rangle+\\frac{L}{2}\\|\\theta_{q,t+1}-\\theta_{q,t}\\|^{2}}\\\\ &{\\quad\\quad\\quad=F(\\theta_{q,t})+\\langle\\nabla F(\\theta_{q,t}),-\\eta\\nabla F(\\theta_{q,t},\\xi_{q,t})\\rangle+\\frac{L}{2}\\|-\\eta\\nabla F(\\theta_{q,t},\\xi_{q,t})\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where (a) is from Assumption 1. Taking expectation of both sides, we obtain ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[F(\\theta_{q,t+1})]\\leq\\mathbb{E}[F(\\theta_{q,t})]-\\eta\\mathbb{E}[\\|\\nabla F(\\theta_{q,t})\\|^{2}]+\\displaystyle\\frac{L\\eta^{2}}{2}\\mathbb{E}[\\|F(\\theta_{q,t},\\xi_{q,t})\\|^{2}]}\\\\ &{\\quad\\quad\\quad\\quad=\\mathbb{E}[F(\\theta_{q,t})]-\\eta\\mathbb{E}[\\|\\nabla F(\\theta_{q,t})\\|^{2}]+\\displaystyle\\frac{L\\eta^{2}}{2}\\mathbb{E}[\\|F(\\theta_{q,t}\\|^{2}+\\sigma^{2}]}\\\\ &{\\displaystyle\\stackrel{(b)}{\\leq}\\mathbb{E}[F(\\theta_{q,t})]-(\\eta-\\frac{L\\eta^{2}}{2})\\mathbb{E}[\\|\\nabla F(\\theta_{q,t})\\|^{2}]+\\displaystyle\\frac{L\\eta^{2}}{2}\\sigma^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where (b) follows from Assumption 2. Let\u2019s set the learning step at the start of training to $T^{\\prime}$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{E}[F(\\theta_{q,T^{\\prime}})]\\leq\\mathbb{E}[F(\\theta_{q,0})]-(\\eta-\\frac{L\\eta^{2}}{2})\\sum_{t=1}^{T^{\\prime}}\\|\\nabla F(\\theta_{q,t})\\|^{2}+\\frac{L T^{\\prime}\\eta^{2}}{2}\\sigma^{2}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Proof of Theorem $^{\\,l}$ . According to Lemma 4, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{1}{Q T^{\\prime}}(\\eta-\\frac{\\eta^{2}L}{2})\\sum_{q=0}^{Q-1}\\sum_{t=0}^{T^{\\prime}-1}\\mathbb{E}[\\nabla\\mathcal{L}_{q,t})]\\leq\\frac{1}{Q T^{\\prime}}\\sum_{q=0}^{Q-1}\\mathbb{E}[F(\\theta_{q,T^{\\prime}})]-\\frac{1}{Q T^{\\prime}}\\sum_{q=0}^{Q-1}\\mathbb{E}[F(\\theta_{q,0})]+\\frac{L T^{\\prime}\\eta^{2}}{2}\\sigma^{2}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\leq\\delta(\\eta-\\frac{\\eta^{2}L}{2})}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Therefore, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\frac{\\Delta}{Q}\\leq\\delta(\\eta-\\frac{\\eta^{2}L}{2})-\\frac{L T^{\\prime}\\eta^{2}}{2}\\sigma^{2},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "which is equal to ", "page_idx": 14}, {"type": "equation", "text": "$$\nQ=\\frac{2\\Delta}{T^{\\prime}\\delta(2\\eta-L\\eta^{2})-T^{\\prime}(L\\eta^{2}\\sigma^{2})}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "C Experiment supplements ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "C.1 Datasets and federated simulations ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We use CIFAR-10 and CIFAR-100 [13] datasets to train all the models. Both of them contain 50,000 training images and 10,000 testing images. To construct the public dataset, we sample 4000 data samples from the training set, then divide the remaining data into $N$ partitions to simulate $N$ clients. To assess the validity of the public dataset, we use two sampling methods to construct it. First, we use a random sampling method over all classes to generate public dataset \u2019IID\u2019. And for the public dataset \u2019Partial\u2019, data is selected randomly from $40\\%$ classes in two datasets. ", "page_idx": 14}, {"type": "image", "img_path": "Of4iNAIUSe/tmp/a3e990d5e32c799421ed723c1f8546633310627c2aa9467f113d12e1cacd07e5.jpg", "img_caption": ["Figure 6: Illustrations of # of samples per class allocated to each client, for different distributions. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "We utilise three settings to simulate heterogeneous data distributions among all the clients. For the IID setting, each client contains the same number of samples from all classes. For the class setting, each client only has $10/N$ and $100/N$ classes on two datasets and the classes between clients have no overlap. For the Non-IID setting, data heterogeneity levels are described by the Dirichlet distribution $\\operatorname{Dir}(\\beta)$ [10], where smaller $\\beta$ represents stronger heterogeneity levels. A value of $\\beta=0.1$ is chosen to simulate a high degree of heterogeneity, and $\\beta\\,=\\,0.5$ for a lower level. Fig. 6 shows the data distribution among clients on CIFAR-10 dataset. The $\\mathbf{X}$ -axis represents 10 classes and the y-axis is the total of 5 clients. The size of each circle denotes the number of samples for the specific class in the respective client. ", "page_idx": 15}, {"type": "text", "text": "C.2 Reproduction details ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Regarding the self-supervised learning framework design within each client, we use ResNet18 [9] and VGG9 [22] as the encoder network and Multi-Layer Perception (MLP) as the predictor. To compare with other baselines, we set the number of clients $N=5$ , and conduct the experiments for $R=100$ rounds. In order to construct the model heterogeneous setting, 2 clients train the Resnet18 encoder while 3 clients use the VGG9. And for the global representation model, Resnet18 is selected as the encoder in server. The hyper-parameter $\\gamma$ in the loss of global model is set to 0.9. During the training process, each client trains locally for $T=5$ epochs while the server also distills for $T^{\\prime}=5$ epochs. Finally, we set the target decay rate $\\alpha=0.99$ , with a batch size of $B=128$ , and utilize SGD for optimization with a learning rate of $\\eta=0.032$ . We implement all the methods in Python using EasyFL[32] based on PyTorch. ", "page_idx": 15}, {"type": "text", "text": "C.3 Baselines ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Firstly, we select several federated knowledge distillation frameworks that use unlabeled public dataset for distillation. We then replaced the local model with a self-supervised model to evaluate the process of knowledge distillation in our method. ", "page_idx": 15}, {"type": "text", "text": "\u2022 FedMD[14]: Each client trains convergence on the public dataset and then on local data. In each round, clients upload the embedding of the public dataset to the server. The server averages the embeddings and sends averaged embedding to clients. Clients first use average embedding to update local models on public dataset, then train on local data for a few epochs. There\u2019s no global model, the local models are evaluated to make comparison. \u2022 FedDF[18]: Clients in FedDF train locally and upload the model parameter to server. The server uses all client models to compute the embedding of the public dataset and uses the average embedding to train the global model. But it has no global model, so we only test the performance of local models. \u2022 FedET[2]: There is a global large model and several alternative small models on the server. Each client selects the appropriate small model for training locally and then uploads the parameter to the server. The server uses all the small models to compute representations of ", "page_idx": 15}, {"type": "text", "text": "the public dataset and updates the global model with the averaged representation. Then the server utilizes the representation from the global model to update all the small models. \u2022 MOON[15]: Moon introduces a model-contrastive learning approach, treating the representation from the global model as positive knowledge to update the local models. \u2022 MOON-KL: Instead of using the NT-Xent loss in MOON, MOON-KL utilises the KLdivergence function to measure the similarity between global representation and local representation. To make comparisons, both MOON and MOON-KL distribute the global model to each client, and then the client uses the local model to update the global model. The server aggregates the updated global models as the final global model. ", "page_idx": 16}, {"type": "text", "text": "And several federated self-supervised learning frameworks are also chosen as baselines. ", "page_idx": 16}, {"type": "text", "text": "\u2022 FedU[33]: Clients in FedU upload the local online encoder and predictor for server aggregation. Then they update the local online encoder using the aggregate ones, and dynamically update the predictor based on the divergence. But it cannot applied to the model heterogeneity setting.   \n\u2022 FedEMA[34]: FedEMA uses the aggregated local online network as a global model and measures the divergence between global and local networks. The divergence is then applied to the local exponential moving average update. It\u2019s also not applicable when models are heterogeneous.   \n\u2022 Hetero-SSFL[20]: Each client trains locally and uploads the kernel matrix over the public dataset. The server aggregates them and sends to clients. Then client uses linear-CKA normalization term to align to the averaged kernel matrix. There\u2019s no global model, so we just test local models\u2019 performance. ", "page_idx": 16}, {"type": "text", "text": "To verify that the client\u2019s knowledge can improve the global model, we also train the global model separately on the public dataset, denoted as Std. ResNet18. ", "page_idx": 16}, {"type": "text", "text": "C.4 Evaluation methods ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Following [33] and [30], we evaluate the performance of learned representations using linear and semi-supervised evaluation. For linear evaluation, all the models are trained without any supervised labels. Subsequently, the model encoder is then frozen and the representations are utilized for training a new classifier over 200 epochs. For semi-supervised evaluation, we consider the scenario that only a small subset of the data has a label, here only $1\\%$ data are labeled. Then, different from freezing the encoder, we fine-tune the whole model with a new classifier using the labeled data for 100 epochs. For FedMD, FedDF and Hetero-SSFL, only local models can be evaluated. So we use the averaged representation from local models to do the linear evaluation. But in semi-supervised evaluation, we need to fine-tune the whole model, so we evaluate each client\u2019s performance alone and use the average result as the final result. ", "page_idx": 16}, {"type": "text", "text": "D Additional Experiments ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "D.1 Numerical Results ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Table 6, Table 7 and Table 8, Table 9 shows the linear evaluation results and semi-supervised evaluation results of FedMKD compared with all the baselines on CIFAR-10 and CIFAR-100. Comparing to the result in main text, we extra add the result on $\\operatorname{Dir}(\\beta=0.1)$ ) to prove the efficiency of our algorithm totally. ", "page_idx": 16}, {"type": "text", "text": "D.2 Communication and storage efficiency ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Considering the experimental setting, the communication and storage capabilities are limited, so we analyze the communication and computation efficiency of FedMKD compared to Hetero-SSFL and all the experiments maintain the original settings. Firstly, we calculate that the memory of ResNet18 and VGG9 is 42.63MB and 16.38MB separately. We use $S(\\theta)$ to denote the storage cost of model $\\theta$ . ", "page_idx": 16}, {"type": "text", "text": "About the communication cost, the client in FedMKD only needs to upload the online encoder to the server, and download the updated encoder, so the cost per communication is equal to the ResNet18 or ", "page_idx": 16}, {"type": "table", "img_path": "Of4iNAIUSe/tmp/30e0ed8547a3f0e6b26778964f7051c144af95c086071e99ea52669ab4f83b4e.jpg", "table_caption": ["Table 6: Top-1 accuracy comparison under linear probing on CIFAR-10 datasets with best model performance in bold and second-best results with underlines. \u2019-\u2019 means this method is not suitable for the experiment setting. "], "table_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "Of4iNAIUSe/tmp/1dc68b3589108cc1900ab4a9ec93c3f9ff6f62c77772635ea8703af61811872c.jpg", "table_caption": ["Table 7: Top-1 accuracy comparison under linear probing on CIFAR-100 dataset with best model performance in bold and second-best results with underlines. \u2019-\u2019 means this method is not suitable for the experiment setting. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "VGG9. But for Hetero-SSFL, each client needs to upload the kernel metric $\\boldsymbol{K}\\in\\mathbb{R}^{L\\times L}$ , $L=|D_{p u b}|$ is the size of the public dataset. In order to make comparison, we fix the communication rounds. Compared to the Hetero-SSFL, when $S(\\theta_{n})*2\\leq|D_{p u b}|^{2}$ , our FedMKD is better in communication cost. And compared to the MOON, for larger server model, $S(\\theta_{n})\\ast2\\leq S(\\theta_{s})\\ast2.$ , MOON will cost more when the server model is larger. ", "page_idx": 17}, {"type": "text", "text": "About local storage for each client, we compute the summary of the model and data. For FedMKD and Hetero-SSFL, each client runs the siamese network, so the model storage is double of the model memory, that is $2*S(\\theta_{n})$ . But for the MOON, each client processes the local contrastive learning and the model contrastive learning, so they must store two local models and one global model, that is $2*S(\\theta_{n})+S(\\theta_{s})$ . Here, we neglect the storage of the predictor which is usually a 2-layer MLP. We use $S(|D|)$ to denote the storage of the dataset. And each client in FedMKD only needs to store the ", "page_idx": 17}, {"type": "table", "img_path": "Of4iNAIUSe/tmp/2ea32505a332d53290f7428fc564d7d20f7643e0092fbe9519efcf98fce9fa6d.jpg", "table_caption": ["Table 8: Top-1 accuracy comparison on $1\\%$ of labeled data for semi-supervised learning on CIFAR-10 dataset with best model performance in bold and second-best results with underlines. \u2019-\u2019 means this method doesn\u2019t apply for the experiment setting. "], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "Of4iNAIUSe/tmp/2f2d4ae139bf530c73935f0383cc5dd9701f328218274881224c53b9ab6ea461.jpg", "table_caption": ["Table 9: Top-1 accuracy comparison on $1\\%$ of labeled data for semi-supervised learning on CIFAR100 dataset with best model performance in bold and second-best results with underlines. \u2019-\u2019 means this method doesn\u2019t apply for the experiment setting. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "Table 10: Communication and storage cost comparison of FedMKD and several baselines. ", "page_idx": 19}, {"type": "table", "img_path": "Of4iNAIUSe/tmp/2013a4e4f2b2e756c9bdafbc71be5de8e251ffb46b912e9bf5e57d2d39ca882e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "Of4iNAIUSe/tmp/6c8b15b341832a30bcea13f7ae13d8c4513399eab36f7f2c989021f0e89cd738.jpg", "img_caption": ["(a) The performance of FedMKD(b) The performance of FedMKD(c) The performance of FedMKD with different public dataset sizewith different global rounds onwith different server epochs on on Cifar-10. Cifar-10. Cifar-10. "], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "Of4iNAIUSe/tmp/892b28732c98eb82fd7832785be7ed3df87f9afe61684bf2dd61c821a91af7ea.jpg", "img_caption": ["(d) The performance of FedMKD(e) The performance of FedMKD(f) The performance of FedMKD with different public dataset sizewith different global rounds onwith different server epochs on on Cifar-100. Cifar-100. Cifar-100. ", "Figure 7: The top-1 test accuracy of different hyperparameter settings on CIFAR-10 and CIFAR100 "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "local training data without the public data, that is $S(|D_{n}|)$ and for Hetero-SSFL client needs to store both two $S(|D_{n}|)+S(|D_{p u b}|)$ . MOON doesn\u2019t use the public dataset, so the client only needs to store the local data $S(|D_{n}|)$ also. Totally, we can get ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{2*S(\\theta_{n})+S(|D|)\\leq2*S(\\theta_{n})+S(|D_{n}|)+S(|D_{p u b}|),}\\\\ &{2*S(\\theta_{n})+S(|D|)\\leq2*S(\\theta_{n})+S(\\theta_{s})+S(|D|).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "D.3 Hyperparameter analysis ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "To explore the influence of different hyperparameters, we conduct experiments on several key parameters of FedMKD. ", "page_idx": 19}, {"type": "text", "text": "Impact of public dataset size. As mentioned in [20], the size and construct method of the public dataset are both critically important, especially since our global model needs training based on it. About the latter one we\u2019ve given an analysis in the last section, so we explore the influence of the size on the model performance. Considering the construct method of the public dataset, we choose part of the dataset as a public dataset and set it aside to be used. The performance change with respect to its size is shown in Fig. 7 (a)(d). We can observe that in both two datasets, when the public dataset is small, the performance of FedMKD is quite worse, and as the size of the public dataset increases, the performance gets better. But when the size is larger than 4000, the gain of performance is small, and because of the design of the model, the time cost of the server computation is larger, so we choose 4000 as our experiment setting. ", "page_idx": 19}, {"type": "text", "text": "Impact of the number of global rounds. In order to investigate the impact of the training rounds to FedMKD, we fix the other hyperparameters and train the model for 200 rounds. The results showed in Fig. 7 (b)(e) demonstrate that while the total training rounds increase, the performance of the FedMKD gets better. For both two kinds clients data distribution in CIFAR-10, the global model converges at nearly 150 rounds. For CIFAR-100, the global model converges at 200 rounds. There\u2019s no denying that the increase of the global rounds will result in better performance, especially when the global round is small, the improvement of the performance is significant. ", "page_idx": 19}, {"type": "text", "text": "Impact of the number of server epochs. Since our global model is trained on the public dataset, through standalone training and knowledge distillation. The lower server epoch means the global model may under-ftiting not only the distribution of the public dataset but also the client knowledge. On the other hand, large server epoch will lead large computation cost, and over-fit the distribution of the public dataset, which is harmful to the generalization of the model, so the epoch in server is important. We choose 5 different settings $T^{\\prime}\\,{\\bar{=}}\\,\\{1,3,5,7,10\\}$ to validate the influence of the server epochs. Based on the experiment results shown in Fig. 7(c)(f), we can see that the performance of the global model improves when the server epoch increases with the distribution of the public is IID. But the change of improvement gets smaller when the server epoch is larger than 5. ", "page_idx": 20}, {"type": "text", "text": "E Additional discussion ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In this section, we discuss the limitations and broader impacts of the work. ", "page_idx": 20}, {"type": "text", "text": "Limitations. Although we provide detailed explanations of the proposed algorithm and extensive experiments analysis, the theoretical proof of FedMKD is not rigorous enough. The characteristic of multi-teacher distillation of the global model has not been sufficiently theoretically justified and we only combined it with the self-training loss as one whole loss to analyze. ", "page_idx": 20}, {"type": "text", "text": "Broader Impacts. FedMKD offers significant societal and technological benefits, which is crucial in kinds of domains like healthcare and finance. It promotes inclusivity by leveraging diverse data from various sources, thereby reducing biases and improving model generalization. Technologically, FedMKD lowers the dependency on labeled data, making federated self-supervised learning more efficient and scalable, and drives innovation in heterogeneous resource-limited devices. By carefully navigating these challenges: deviated representation abilities and inconsistent representation spaces, FedMKD can lead to responsible and equitable advancements in distributed AI technology. ", "page_idx": 20}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: The abstract and introduction sections of our paper clearly articulate the main claims and contributions of our research. These include explicit statements of the proposed framework innovations, theoretical advancements, and empirical findings is summarized in lines 62-73. Furthermore, the theoretical arguments in Sec.B and the experimental evidence in Sec.5 of the main body of the paper fully support the claims made in this paper. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 21}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: In this work, we are the first to propose a multi-teacher knowledge distillation framework, namely FedMKD, to learn global representations with whole class knowledge from heterogeneous clients even under extreme class skew. However, the paper still exists some limitations about the privacy preservation. We discuss the limitations of this work in Appendix E. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best ", "page_idx": 21}, {"type": "text", "text": "judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 22}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: In this work, we provide the full set of theoretical assumptions and the complete process process of the proposed multi-teacher knowledge distillation framework FedMKD in the Appendix.B. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 22}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We propose a multi-teacher knowledge distillation framework FedMKD and describe this framework in detail in Sec.4. Furthermore, this paper disclose all the information needed to reproduce the main experimental results of it in Sec.5 and Appendix. C. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.   \n(c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).   \n(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: In this work, we use the public dataset CIFAR-10 and CIFAR-100 as the datasets. And the code of our work is available at https://github.com/limee-sdu/ FedMKD, as stated in Sec.5. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 23}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: We provide the training and test details in Sec.5 and Appendix. C. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 23}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We provide the error bars of the experiments in Sec. 5 and Appendix. D. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 24}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: In this work, we use the NVIDIA GeForce RTX 3090 cards with 24GB memory as the server and the clients. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 24}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We have reviewed the NeurIPS Code of Ethics. And we ensure that our work does not involve any inference of personal information and that it will only be used for academic purposes. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 24}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: We discuss the broader impacts of this work in the last paragraph of Appendix.E. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 25}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 25}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: In this work, we introduce the public datasets CIFAR-10 and CIFAR-100 and the ResNet18 [9] and VGG9 [22] as the backbones in the Sec.5.1. And we properly cite the related work in the main body and the appendix of this paper. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should citep the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 26}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 26}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 26}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 27}]