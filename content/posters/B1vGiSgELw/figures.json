[{"figure_path": "B1vGiSgELw/figures/figures_1_1.jpg", "caption": "Figure 1: Our model, MQT-LLAVA, matches LLaVA-1.5 performance on 11 benchmarks using only 256 visual tokens instead of 576. We achieve a 2x speed-up with 256 tokens and 8X speed-up in TFLOPs using 16 tokens with only a 2.4 performance drop compared to LLaVA-1.5 on MMBench.", "description": "This figure demonstrates the performance and efficiency of the proposed model, MQT-LLAVA, compared to the baseline LLaVA-1.5 model.  The left panel shows that MQT-LLAVA achieves similar performance to LLaVA-1.5 across 11 benchmarks while using significantly fewer visual tokens (256 vs 576). The right panel illustrates the computational efficiency gains, showing a 2x speedup with 256 tokens and an 8x speedup with only 16 tokens, at the cost of a small performance drop (2.4 points on MMBench).", "section": "3 Experiments"}, {"figure_path": "B1vGiSgELw/figures/figures_2_1.jpg", "caption": "Figure 2: Our model employs a query transformer to encode images as visual tokens. We randomly select the first m tokens during training, and enable flexible choice of any m number under M during inference, where M is the maximum number of initialized tokens.", "description": "This figure illustrates the architecture of the Matryoshka Query Transformer (MQT). An image is first processed by a vision encoder which extracts grid features.  These features are then fed into a query transformer, which generates a variable number of elastic visual tokens (m tokens, where m <= M, and M is the maximum number). The number of visual tokens selected (m) is randomly determined during training but can be flexibly chosen at inference time. The elastic visual tokens are then passed to a pre-trained large language model (LLM) for further processing.", "section": "2 Matryoshka Query Transformer"}, {"figure_path": "B1vGiSgELw/figures/figures_4_1.jpg", "caption": "Figure 3: With only 2 visual tokens, MQT-LLAVA outperforms InstructBLIP (which uses 32 visual tokens) on all 8 benchmarks it is evaluated on.", "description": "The figure shows a graph comparing the average performance of MQT-LLAVA and InstructBLIP-7B across 8 benchmarks with varying numbers of visual tokens. MQT-LLAVA consistently outperforms InstructBLIP-7B, especially when using only 2 visual tokens.", "section": "3.3 Computational Efficiency"}, {"figure_path": "B1vGiSgELw/figures/figures_5_1.jpg", "caption": "Figure 4: Grad-CAM visualization of 1 randomly picked token from using 8, 16, 64, 256 visual tokens, respectively, to encode an image. The model effectively concentrates on high-level concepts using fewer tokens and delves into low-level details with more tokens. The complete input to the third image is \u201cList all the objects on the desk. The objects on the desk include a computer monitor, a keyboard, a mouse, a cell phone, and a pair of headphones\u201d.", "description": "This figure visualizes how the model's focus changes with the number of visual tokens used.  Grad-CAM is used to highlight the areas of the image that are most important to the model's prediction for different numbers of tokens (8, 16, 64, 256). The results show that with fewer tokens, the model focuses on high-level concepts, while with more tokens, it attends to low-level details.", "section": "4.1 How does the focus of the model change with varying numbers of visual tokens?"}, {"figure_path": "B1vGiSgELw/figures/figures_6_1.jpg", "caption": "Figure 1: Our model, MQT-LLAVA, matches LLaVA-1.5 performance on 11 benchmarks using only 256 visual tokens instead of 576. We achieve a 2x speed-up with 256 tokens and 8X speed-up in TFLOPs using 16 tokens with only a 2.4 performance drop compared to LLaVA-1.5 on MMBench.", "description": "This figure shows a comparison of the performance and computational efficiency of the proposed model, MQT-LLAVA, against the baseline model, LLaVA-1.5, across 11 benchmark datasets. The left panel shows the average score across all benchmarks for different numbers of visual tokens. The right panel shows the relationship between the number of visual tokens and the computational cost (measured in TFLOPs).  The results demonstrate that MQT-LLAVA achieves comparable performance to LLaVA-1.5 while significantly reducing the number of visual tokens and the computational cost. Using 256 visual tokens, MQT-LLAVA achieves similar performance as LLaVA-1.5 with a 2x speed-up, while using only 16 tokens results in an 8x speed-up with only a minor performance drop on MMBench.", "section": "3 Experiments"}, {"figure_path": "B1vGiSgELw/figures/figures_6_2.jpg", "caption": "Figure 6: Examples from MME Cognition. Grad-CAM results are from using 16 tokens which answered all the questions correctly.", "description": "This figure shows three examples from the MME Cognition benchmark dataset. Each example includes an image and a question. For each example, the Grad-CAM visualization using 16 visual tokens is shown, highlighting the regions of the image that the model focused on when answering the question. The results demonstrate the model's ability to correctly answer questions by focusing on relevant image regions, even with a relatively small number of visual tokens.", "section": "4 Analyses"}, {"figure_path": "B1vGiSgELw/figures/figures_7_1.jpg", "caption": "Figure 7: Comparison of correct and failure cases in 16 vs 144 visual tokens on Science-QA (test-set).", "description": "This figure shows Grad-CAM visualizations for two Science-QA questions, comparing model performance using 16 and 144 visual tokens.  In the first example (identifying a common property of a tortoise shell, crown, and basketball), the model with 16 tokens correctly focuses on the overall objects and their shared characteristic (opaque), while the model with 144 tokens fails by focusing on individual parts of each object. In the second example (identifying the highlighted state on a US map), the model with 144 tokens correctly identifies Virginia, but the model with 16 tokens makes a wrong prediction by focusing on a different area of the map. This illustrates how the optimal number of visual tokens varies depending on task complexity and the level of detail needed for accurate reasoning.", "section": "4 Analyses"}, {"figure_path": "B1vGiSgELw/figures/figures_13_1.jpg", "caption": "Figure 1: Our model, MQT-LLAVA, matches LLaVA-1.5 performance on 11 benchmarks using only 256 visual tokens instead of 576. We achieve a 2x speed-up with 256 tokens and 8X speed-up in TFLOPs using 16 tokens with only a 2.4 performance drop compared to LLaVA-1.5 on MMBench.", "description": "This figure displays the performance comparison between MQT-LLAVA and LLaVA-1.5 across 11 benchmarks. It highlights MQT-LLAVA's ability to match LLaVA-1.5's performance while using significantly fewer visual tokens (256 instead of 576). The chart also demonstrates the computational efficiency gains of MQT-LLAVA, achieving a 2x speed-up with 256 tokens and an 8x speed-up with only 16 tokens, resulting in a minimal performance drop of 2.4 points on MMBench.", "section": "3 Experiments"}, {"figure_path": "B1vGiSgELw/figures/figures_14_1.jpg", "caption": "Figure 1: Our model, MQT-LLAVA, matches LLaVA-1.5 performance on 11 benchmarks using only 256 visual tokens instead of 576. We achieve a 2x speed-up with 256 tokens and 8X speed-up in TFLOPs using 16 tokens with only a 2.4 performance drop compared to LLaVA-1.5 on MMBench.", "description": "This figure showcases the performance and computational efficiency of MQT-LLAVA compared to LLaVA-1.5 across 11 benchmarks.  The left panel demonstrates that MQT-LLAVA achieves comparable performance to LLaVA-1.5 while using significantly fewer visual tokens (256 instead of 576). The right panel highlights the substantial speed-up gained by MQT-LLAVA, especially when using only 16 visual tokens, with a minimal performance decrease compared to LLaVA-1.5 on the MMBench benchmark.", "section": "3 Experiments"}, {"figure_path": "B1vGiSgELw/figures/figures_14_2.jpg", "caption": "Figure 4: Grad-CAM visualization of 1 randomly picked token from using 8, 16, 64, 256 visual tokens, respectively, to encode an image. The model effectively concentrates on high-level concepts using fewer tokens and delves into low-level details with more tokens. The complete input to the third image is \u201cList all the objects on the desk. The objects on the desk include a computer monitor, a keyboard, a mouse, a cell phone, and a pair of headphones\u201d.", "description": "This figure shows Grad-CAM visualizations for different numbers of visual tokens (8, 16, 64, 256). Grad-CAM highlights the image regions that are most important for a particular token's activation.  The visualization demonstrates that with fewer tokens, the model focuses on high-level concepts, while more tokens allow the model to attend to finer details within the image.", "section": "4.1 How does the focus of the model change with varying numbers of visual tokens?"}]