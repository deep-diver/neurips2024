[{"figure_path": "pf4OuJyn4Q/figures/figures_3_1.jpg", "caption": "Figure 1: Results on over-optimization in Direct Alignment Algorithms for DPO, IPO and SLiC. Results show model win-rates over the dataset summary on an evaluation set of prompts as judged by GPT-4. The top row shows the final performance after 1 epoch of training, while the second row also includes 4 intermediate checkpoints as well. The fitted dotted curves utilize scaling laws from [21] applied to direct alignment, with GPT4 winrates taking the place of the gold reward model score.", "description": "This figure presents the results of an experiment evaluating over-optimization in three different Direct Alignment Algorithms (DAAs): DPO, IPO, and SLiC.  The experiment measures model performance (GPT-4 win rate) against the square root of KL divergence for different model sizes (1B, 2.8B, 6.9B).  The top row shows the final performance after one epoch, while the bottom row includes intermediate checkpoints to show the training dynamics.  The dotted lines represent fitted scaling law curves based on previous work.", "section": "3.1 Evaluating Model-Overoptimization"}, {"figure_path": "pf4OuJyn4Q/figures/figures_4_1.jpg", "caption": "Figure 1: Results on over-optimization in Direct Alignment Algorithms for DPO, IPO and SLiC. Results show model win-rates over the dataset summary on an evaluation set of prompts as judged by GPT-4. The top row shows the final performance after 1 epoch of training, while the second row also includes 4 intermediate checkpoints as well. The fitted dotted curves utilize scaling laws from [21] applied to direct alignment, with GPT4 winrates taking the place of the gold reward model score.", "description": "This figure shows the results of an experiment on over-optimization in three different direct alignment algorithms (DPO, IPO, and SLiC).  The x-axis represents the square root of KL divergence, a measure of how much the model's behavior changes from its initial state during training. The y-axis shows the model's win rate against GPT-4 judgments on a held-out set of prompts. The top row shows the final performance of each algorithm after one epoch of training, while the bottom row shows performance at multiple checkpoints during training.  The dotted lines show fitted scaling law curves, adapted from previous research to show how model size impacts the over-optimization effect. The results demonstrate that the three algorithms exhibit similar over-optimization patterns, performing worse at higher KL budgets (more changes from the initial model).", "section": "3.1 Evaluating Model-Overoptimization"}, {"figure_path": "pf4OuJyn4Q/figures/figures_5_1.jpg", "caption": "Figure 3: Left: KL budget versus win-rates (over dataset human answer) with and without length-regularization [44]. While including a length correction in the optimization objective changes the KL-win-rate Pareto Frontier, it does not alleviate reward over-optimization and might even exacerbate it. Right: Scaling behavior for length extrapolation - smaller capacity models (either by size or KL budget) extrapolate more strongly on simpler features such as length.", "description": "The left plot shows the relationship between KL divergence budget and win-rate on the Reddit TL;DR summarization dataset for the 2.8B Pythia model with and without length regularization.  Adding length regularization changes the Pareto frontier, but does not solve the reward over-optimization problem.  The right plot displays the correlation between the R^2 of a linear regression (between implicit reward and length of response) and KL divergence across three model sizes (1B, 2.8B, 6.9B). Smaller models or smaller KL budgets exhibit stronger correlation, indicating increased reliance on length as a feature.", "section": "3 Empirical Analysis of Overoptimization in DAAs"}, {"figure_path": "pf4OuJyn4Q/figures/figures_6_1.jpg", "caption": "Figure 1: Results on over-optimization in Direct Alignment Algorithms for DPO, IPO and SLiC. Results show model win-rates over the dataset summary on an evaluation set of prompts as judged by GPT-4. The top row shows the final performance after 1 epoch of training, while the second row also includes 4 intermediate checkpoints as well. The fitted dotted curves utilize scaling laws from [21] applied to direct alignment, with GPT4 winrates taking the place of the gold reward model score.", "description": "This figure displays the results of an experiment evaluating three different direct alignment algorithms (DPO, IPO, and SLiC) on their tendency towards over-optimization.  The x-axis represents the square root of the KL divergence, a measure of how much the model's behavior changes from its initial state during training. The y-axis represents the model's win rate against GPT-4, an indicator of actual performance. Separate plots are given for different model sizes (1B, 2.8B, and 6.9B parameters).  The top row shows final performance after one training epoch, while the bottom row shows performance at four intermediate checkpoints during that same epoch.  Dotted lines represent scaling law curve fits to the data, helping illustrate the trend.", "section": "3.1 Evaluating Model-Overoptimization"}, {"figure_path": "pf4OuJyn4Q/figures/figures_7_1.jpg", "caption": "Figure 1: Results on over-optimization in Direct Alignment Algorithms for DPO, IPO and SLiC. Results show model win-rates over the dataset summary on an evaluation set of prompts as judged by GPT-4. The top row shows the final performance after 1 epoch of training, while the second row also includes 4 intermediate checkpoints as well. The fitted dotted curves utilize scaling laws from [21] applied to direct alignment, with GPT4 winrates taking the place of the gold reward model score.", "description": "This figure presents the results of an experiment evaluating over-optimization in three different direct alignment algorithms (DPO, IPO, and SLiC) applied to various sized language models. The x-axis represents the square root of the KL divergence, a measure of how much the model's distribution changes during training. The y-axis represents the model's win rate against GPT-4 judgements on a held-out evaluation set. The top row shows the final performance after one epoch of training, while the bottom row shows the performance at four intermediate checkpoints.  The dotted lines are fitted curves based on scaling laws from prior work, adapted for direct alignment.", "section": "3.1 Evaluating Model-Overoptimization"}, {"figure_path": "pf4OuJyn4Q/figures/figures_8_1.jpg", "caption": "Figure 7: (Top row) Probability of OOD trajectories. DAA algorithms end up placing a substantial probability mass of some of the OOD trajectories during training. (Bottom row) Probability of in-distribution (preference-pair) trajectories decreases during training.", "description": "This figure shows the probability of out-of-distribution (OOD) and in-distribution trajectories during the training process of three different Direct Alignment Algorithms (DAAs): DPO, IPO, and SLiC. The top row illustrates that DAAs allocate a significant probability mass to OOD trajectories, while the bottom row shows that the probability mass of in-distribution trajectories decreases during training. This phenomenon highlights a potential issue with DAAs, where they tend to overfit to the training data and extrapolate poorly to unseen data.", "section": "Understanding Behavior of DAAs on OOD sequences"}, {"figure_path": "pf4OuJyn4Q/figures/figures_9_1.jpg", "caption": "Figure 6: An illustration of the Tree MDP. At each state, we can choose one of 3 actions (a0, a1, a2), which deterministically maps to the next state. Furthermore, all the leaf nodes in this tree MDP, transition to the terminal absorbing state s\u221e, irrespective of the chosen action", "description": "This figure illustrates a simple Tree Markov Decision Process (MDP) used in the paper to model the token-level process of Large Language Models (LLMs).  The MDP starts at a single state (S1) and branches out deterministically into a tree structure based on the actions taken (a0, a1, a2). Each path through the tree eventually leads to the terminal state (s\u221e). This simplified MDP is used to analyze the behavior of Direct Alignment Algorithms (DAAs) when extrapolating to out-of-distribution (OOD) sequences during training.", "section": "E.1 Designing a toy LLM MDP"}, {"figure_path": "pf4OuJyn4Q/figures/figures_16_1.jpg", "caption": "Figure 1: Results on over-optimization in Direct Alignment Algorithms for DPO, IPO and SLiC. Results show model win-rates over the dataset summary on an evaluation set of prompts as judged by GPT-4. The top row shows the final performance after 1 epoch of training, while the second row also includes 4 intermediate checkpoints as well. The fitted dotted curves utilize scaling laws from [21] applied to direct alignment, with GPT4 winrates taking the place of the gold reward model score.", "description": "This figure shows the results of an experiment evaluating over-optimization in three different direct alignment algorithms (DPO, IPO, and SLiC) using various model sizes (1B, 2.8B, 6.9B) and KL divergence budgets.  The top row displays the final performance after one epoch of training, while the bottom row presents the performance at four intermediate checkpoints throughout the training process. Dotted curves are fitted to the data points to show how the performance changes with the square root of the KL divergence, illustrating the over-optimization trend.", "section": "3.1 Evaluating Model-Overoptimization"}, {"figure_path": "pf4OuJyn4Q/figures/figures_20_1.jpg", "caption": "Figure 1: Results on over-optimization in Direct Alignment Algorithms for DPO, IPO and SLiC. Results show model win-rates over the dataset summary on an evaluation set of prompts as judged by GPT-4. The top row shows the final performance after 1 epoch of training, while the second row also includes 4 intermediate checkpoints as well. The fitted dotted curves utilize scaling laws from [21] applied to direct alignment, with GPT4 winrates taking the place of the gold reward model score.", "description": "This figure displays the results of an experiment evaluating over-optimization in three different direct alignment algorithms (DPO, IPO, and SLiC).  The experiment measured model performance (win rate against GPT-4 judgments) across different KL divergence budgets and model sizes (1B, 2.8B, 6.9B parameters). The top row shows the final win rates after one epoch of training, while the bottom row provides a more detailed view, including four intermediate checkpoints.  Dotted lines represent fitted scaling law curves adapted from prior research.  The figure highlights how the performance of all three algorithms degrades at higher KL budgets, indicating a similar over-optimization trend to classical RLHF methods even without explicit reward modeling.", "section": "3.1 Evaluating Model-Overoptimization"}, {"figure_path": "pf4OuJyn4Q/figures/figures_21_1.jpg", "caption": "Figure 1: Results on over-optimization in Direct Alignment Algorithms for DPO, IPO and SLiC. Results show model win-rates over the dataset summary on an evaluation set of prompts as judged by GPT-4. The top row shows the final performance after 1 epoch of training, while the second row also includes 4 intermediate checkpoints as well. The fitted dotted curves utilize scaling laws from [21] applied to direct alignment, with GPT4 winrates taking the place of the gold reward model score.", "description": "This figure displays the results of an experiment evaluating over-optimization in three different Direct Alignment Algorithms (DAAs): Direct Preference Optimization (DPO), Inverse Preference Optimization (IPO), and Supervised Learning with Implicit Preference Calibration (SLIC).  The experiment uses three different model sizes (1B, 2.8B, and 6.9B parameters) and various KL divergence budgets. The plots show the models' win rates (performance) as judged by GPT-4 against the square root of the KL divergence.  The top row shows the final win rate after one epoch of training. The bottom row includes four intermediate checkpoints to show the training dynamics. Dotted lines represent scaling law curve fits, providing a comparative analysis of over-optimization across different DAAs, model sizes, and KL budgets.", "section": "3.1 Evaluating Model-Overoptimization"}, {"figure_path": "pf4OuJyn4Q/figures/figures_22_1.jpg", "caption": "Figure 1: Results on over-optimization in Direct Alignment Algorithms for DPO, IPO and SLiC. Results show model win-rates over the dataset summary on an evaluation set of prompts as judged by GPT-4. The top row shows the final performance after 1 epoch of training, while the second row also includes 4 intermediate checkpoints as well. The fitted dotted curves utilize scaling laws from [21] applied to direct alignment, with GPT4 winrates taking the place of the gold reward model score.", "description": "This figure displays the results of an experiment evaluating three different direct alignment algorithms (DPO, IPO, and SLiC) for reward model over-optimization.  The experiment used prompts judged by GPT-4 to assess model performance across various model sizes and KL divergence budgets.  The top row shows the winrates after one epoch of training, highlighting how performance peaks and then declines with increasing KL budgets. The bottom row shows the same winrates but also includes intermediate checkpoints, providing a more detailed view of the training dynamics.", "section": "3.1 Evaluating Model-Overoptimization"}, {"figure_path": "pf4OuJyn4Q/figures/figures_23_1.jpg", "caption": "Figure 1: Results on over-optimization in Direct Alignment Algorithms for DPO, IPO and SLiC. Results show model win-rates over the dataset summary on an evaluation set of prompts as judged by GPT-4. The top row shows the final performance after 1 epoch of training, while the second row also includes 4 intermediate checkpoints as well. The fitted dotted curves utilize scaling laws from [21] applied to direct alignment, with GPT4 winrates taking the place of the gold reward model score.", "description": "This figure displays the results of an experiment evaluating over-optimization in three different direct alignment algorithms (DPO, IPO, and SLiC) for aligning large language models.  The performance is measured as the win rate against human-generated answers, as judged by GPT-4, across various KL divergence budgets. The top row shows the performance at the end of one epoch of training, while the bottom row shows intermediate checkpoints to illustrate performance changes throughout training.  Dotted lines represent scaling law curve fits, adapted from prior research, to help visualize the performance trends. The results demonstrate that over-optimization occurs across all three algorithms and a range of KL budgets.", "section": "3.1 Evaluating Model-Overoptimization"}, {"figure_path": "pf4OuJyn4Q/figures/figures_24_1.jpg", "caption": "Figure 1: Results on over-optimization in Direct Alignment Algorithms for DPO, IPO and SLiC. Results show model win-rates over the dataset summary on an evaluation set of prompts as judged by GPT-4. The top row shows the final performance after 1 epoch of training, while the second row also includes 4 intermediate checkpoints as well. The fitted dotted curves utilize scaling laws from [21] applied to direct alignment, with GPT4 winrates taking the place of the gold reward model score.", "description": "This figure displays the results of an experiment on over-optimization in three different direct alignment algorithms (DPO, IPO, and SLiC).  The experiment evaluated model performance (win rate against GPT-4 judgments) across various KL divergence budgets and model sizes (1B, 2.8B, and 6.9B parameters). The top row shows the final win rates after one training epoch, while the bottom row shows win rates at four intermediate checkpoints during training. Dotted lines represent scaling law curve fits, adapting previous research to the context of direct alignment algorithms. The results highlight similar over-optimization trends across different algorithms and model scales.", "section": "3.1 Evaluating Model-Overoptimization"}, {"figure_path": "pf4OuJyn4Q/figures/figures_25_1.jpg", "caption": "Figure 1: Results on over-optimization in Direct Alignment Algorithms for DPO, IPO and SLiC. Results show model win-rates over the dataset summary on an evaluation set of prompts as judged by GPT-4. The top row shows the final performance after 1 epoch of training, while the second row also includes 4 intermediate checkpoints as well. The fitted dotted curves utilize scaling laws from [21] applied to direct alignment, with GPT4 winrates taking the place of the gold reward model score.", "description": "This figure displays the results of an experiment evaluating over-optimization in three different direct alignment algorithms (DPO, IPO, and SLiC).  The experiment used three different model sizes (1B, 2.8B, and 6.9B parameters) and varied the KL divergence budget. The win rate, determined by GPT-4 judgments on a held-out set of prompts, is plotted against the square root of the KL divergence. The top row shows final performance after one epoch of training, while the bottom row includes four intermediate checkpoints. Dotted lines represent scaling law curve fits based on previous research.", "section": "3.1 Evaluating Model-Overoptimization"}, {"figure_path": "pf4OuJyn4Q/figures/figures_26_1.jpg", "caption": "Figure 1: Results on over-optimization in Direct Alignment Algorithms for DPO, IPO and SLiC. Results show model win-rates over the dataset summary on an evaluation set of prompts as judged by GPT-4. The top row shows the final performance after 1 epoch of training, while the second row also includes 4 intermediate checkpoints as well. The fitted dotted curves utilize scaling laws from [21] applied to direct alignment, with GPT4 winrates taking the place of the gold reward model score.", "description": "This figure displays the results of an experiment evaluating three different Direct Alignment Algorithms (DAAs): Direct Preference Optimization (DPO), Inverse Preference Optimization (IPO), and Sequence Likelihood Calibration (SLiC).  The experiment measured model performance (win rate against GPT-4 judgments) across various KL divergence budgets and model sizes. The top row shows the final win rates after one epoch of training, while the bottom row includes intermediate checkpoints to illustrate performance changes over time. Dotted lines represent fitted curves based on scaling laws from prior work, providing a comparative context.", "section": "3.1 Evaluating Model-Overoptimization"}, {"figure_path": "pf4OuJyn4Q/figures/figures_27_1.jpg", "caption": "Figure 1: Results on over-optimization in Direct Alignment Algorithms for DPO, IPO and SLiC. Results show model win-rates over the dataset summary on an evaluation set of prompts as judged by GPT-4. The top row shows the final performance after 1 epoch of training, while the second row also includes 4 intermediate checkpoints as well. The fitted dotted curves utilize scaling laws from [21] applied to direct alignment, with GPT4 winrates taking the place of the gold reward model score.", "description": "This figure shows the results of an experiment evaluating three different direct alignment algorithms (DPO, IPO, and SLiC) for reward model overoptimization.  The experiment was conducted using three different model sizes (1B, 2.8B, and 6.9B parameters).  The y-axis represents the model's win rate (performance) as judged by GPT-4 on an evaluation set of prompts. The x-axis represents the square root of the KL divergence, a measure of how much the model's distribution changes during training. The top row shows the final win rates after one epoch of training, while the bottom row shows the win rates at four intermediate checkpoints during the training process. Dotted curves represent scaling law fits to the data, which helps to better understand and characterize the over-optimization behavior. The results show that all algorithms exhibit similar over-optimization trends; performance initially improves with increasing KL divergence but then decreases as KL divergence continues to increase.", "section": "3.1 Evaluating Model-Overoptimization"}, {"figure_path": "pf4OuJyn4Q/figures/figures_28_1.jpg", "caption": "Figure 1: Results on over-optimization in Direct Alignment Algorithms for DPO, IPO and SLiC. Results show model win-rates over the dataset summary on an evaluation set of prompts as judged by GPT-4. The top row shows the final performance after 1 epoch of training, while the second row also includes 4 intermediate checkpoints as well. The fitted dotted curves utilize scaling laws from [21] applied to direct alignment, with GPT4 winrates taking the place of the gold reward model score.", "description": "This figure displays the results of an experiment on over-optimization in three different direct alignment algorithms (DPO, IPO, and SLiC).  The x-axis represents the square root of the KL divergence, a measure of how much the model's distribution changes during training. The y-axis shows the model's win rate against GPT-4 on an evaluation set.  The top row presents the final results after one epoch of training, whereas the bottom row shows the results at four intermediate checkpoints during training. The dotted lines represent curve fits based on scaling laws described in a previous work [21], highlighting how model performance deteriorates at higher KL budgets.", "section": "3.1 Evaluating Model-Overoptimization"}, {"figure_path": "pf4OuJyn4Q/figures/figures_29_1.jpg", "caption": "Figure 1: Results on over-optimization in Direct Alignment Algorithms for DPO, IPO and SLiC. Results show model win-rates over the dataset summary on an evaluation set of prompts as judged by GPT-4. The top row shows the final performance after 1 epoch of training, while the second row also includes 4 intermediate checkpoints as well. The fitted dotted curves utilize scaling laws from [21] applied to direct alignment, with GPT4 winrates taking the place of the gold reward model score.", "description": "This figure displays the results of an experiment evaluating over-optimization in three different direct alignment algorithms (DPO, IPO, and SLiC) using Pythia language models of varying sizes (1B, 2.8B, and 6.9B parameters).  The plots show the GPT-4 win rate (a measure of model performance) against the square root of the KL divergence (a measure of how much the model's output distribution differs from the initial distribution) at various KL budgets. The top row shows performance after one epoch of training, while the bottom row shows performance at four intermediate checkpoints during training. Dotted lines represent scaling law curves fitted to the data, illustrating how the model's performance degrades at higher KL budgets.", "section": "3.1 Evaluating Model-Overoptimization"}, {"figure_path": "pf4OuJyn4Q/figures/figures_29_2.jpg", "caption": "Figure 1: Results on over-optimization in Direct Alignment Algorithms for DPO, IPO and SLiC. Results show model win-rates over the dataset summary on an evaluation set of prompts as judged by GPT-4. The top row shows the final performance after 1 epoch of training, while the second row also includes 4 intermediate checkpoints as well. The fitted dotted curves utilize scaling laws from [21] applied to direct alignment, with GPT4 winrates taking the place of the gold reward model score.", "description": "This figure displays the results of an experiment evaluating three different direct alignment algorithms (DPO, IPO, and SLiC) for various model sizes and KL divergence budgets. The results are presented as GPT-4 win rates over a set of prompts. The top row shows the performance at the end of one training epoch, while the bottom row shows the performance at four intermediate checkpoints during training. Dotted curves show fitted scaling laws.", "section": "3.1 Evaluating Model-Overoptimization"}]