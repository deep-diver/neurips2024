{"references": [{"fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-12-01", "reason": "This paper introduced the Transformer architecture, a crucial component in many modern vision and language models, including the OneRef model itself."}, {"fullname_first_author": "Jacob Devlin", "paper_title": "Bert: Pre-training of deep bidirectional transformers for language understanding", "publication_date": "2018-10-11", "reason": "BERT introduced a powerful language representation model that has significantly influenced the field of natural language processing, providing a solid base for many visual grounding models."}, {"fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-07-01", "reason": "CLIP provided a foundational multimodal model demonstrating the effectiveness of learning from image-text pairs, greatly impacting visual grounding and many downstream applications."}, {"fullname_first_author": "Hangbo Bao", "paper_title": "Beit: Bert pre-training of image transformers", "publication_date": "2021-01-01", "reason": "BEiT introduced a masked image modeling approach for pre-training image transformers, which OneRef builds on to create its modality-shared architecture."}, {"fullname_first_author": "Aishwarya Kamath", "paper_title": "Mdetr-modulated detection for end-to-end multi-modal understanding", "publication_date": "2021-10-01", "reason": "MDETR showed the feasibility of end-to-end multi-modal understanding for referring expression tasks, serving as a major precursor for OneRef's unified framework."}]}