[{"figure_path": "siPdcro6uD/tables/tables_6_1.jpg", "caption": "Table 1: Comparison with latest SoTA methods on the five datasets for REC/PG tasks with single-dataset fine-tuning setting. We highlight best result of base model in red and bold for large model.", "description": "This table compares the performance of the proposed OneRef model against other state-of-the-art (SoTA) methods on five benchmark datasets for referring expression comprehension (REC) and phrase grounding (PG) tasks.  The comparison is done under a single-dataset fine-tuning setting, meaning each model is trained and evaluated only on a single dataset.  The table shows the performance metrics (presumably accuracy) achieved by each method on different datasets (RefCOCO, RefCOCO+, RefCOCOg, ReferIt, Flickr30k).  The best results for the base model are highlighted in red and bold, while results for larger models are also presented.", "section": "4 Experiments"}, {"figure_path": "siPdcro6uD/tables/tables_6_2.jpg", "caption": "Table 1: Comparison with latest SoTA methods on the five datasets for REC/PG tasks with single-dataset fine-tuning setting. We highlight best result of base model in red and bold for large model.", "description": "This table compares the OneRef model's performance with other state-of-the-art (SOTA) methods on five benchmark datasets for referring expression comprehension (REC) and phrase grounding (PG) tasks.  The comparison is done using a single-dataset fine-tuning setting, meaning that the models are only trained on one dataset at a time.  The table shows various performance metrics for each dataset and model. The best result of the base model is highlighted in red, while the best result of the large model is shown in bold.", "section": "4.2 Comparison with state-of-the-art methods"}, {"figure_path": "siPdcro6uD/tables/tables_7_1.jpg", "caption": "Table 3: Comparison with latest SoTA methods (mIoU metric) on the three datasets for RES task with both single-dataset fine-tuning setting and dataset-mixed intermediate pre-training setting.", "description": "This table compares the performance of the proposed OneRef model with state-of-the-art (SoTA) methods on three datasets for the Referring Expression Segmentation (RES) task.  It shows the mean Intersection over Union (mIoU) scores achieved by different methods using two different training settings: single-dataset fine-tuning and dataset-mixed intermediate pre-training.  The table helps to evaluate the effectiveness of OneRef in comparison to other methods and the impact of different training approaches on the model's performance.", "section": "4.2 Comparison with state-of-the-art methods"}, {"figure_path": "siPdcro6uD/tables/tables_7_2.jpg", "caption": "Table 4: Ablation of MRefM on mixup pre-training setting.", "description": "This table presents the ablation study results of the Mask Referring Modeling (MRefM) paradigm on the mixup pre-training setting. It shows the performance of different combinations of MIM (Mask Image Modeling), MLM (Mask Language Modeling), and image masking strategies (vanilla, referring-aware, random) on the RefCOCO+ and RefCOCOg datasets. The results are measured in terms of validation accuracy and test accuracy (testA and testB) on the two datasets.  The table helps to understand the contribution of each component of the MRefM to the overall performance.", "section": "4.3 Ablation study"}, {"figure_path": "siPdcro6uD/tables/tables_8_1.jpg", "caption": "Table 6: Generality study of MRefM on RefCOCOg.", "description": "This table shows the results of a generality study conducted on the RefCOCOg dataset to evaluate the effectiveness of the proposed Mask Referring Modeling (MRefM) method.  It compares the performance of several visual grounding models, including the original models and the same models enhanced by the MRefM technique, under two different training settings: single-dataset fine-tuning and mixup pre-training. The results demonstrate the improvements achieved by integrating MRefM across various models and training scenarios on the RefCOCOg dataset.", "section": "4.4 Generality study"}, {"figure_path": "siPdcro6uD/tables/tables_8_2.jpg", "caption": "Table 6: Generality study of MRefM on RefCOCOg.", "description": "This table shows the results of the Generality study of the Mask Referring Modeling (MRefM) on the RefCOCOg dataset. It compares the performance of different models, including TransVG, TransVG++ (reproduced by the authors), CLIP-VG, and LAVT, with and without the MRefM paradigm. The results are presented in terms of validation and test accuracy for the RefCOCOg dataset.", "section": "4.4 Generality study"}, {"figure_path": "siPdcro6uD/tables/tables_17_1.jpg", "caption": "Table 8: The detailed statistics of RefCOCO [101], RefCOCO+ [101], RefCOCOg [62], ReferItGame [34] and Flickr30K Entities [68] datasets. We represent test split and testA split in the same column.", "description": "This table presents a detailed breakdown of the statistics for five different datasets used in the paper's experiments on referring expression comprehension and segmentation tasks.  For each dataset, it shows the number of images, instances, total queries, and the number of queries in the train, validation, testA, and testB splits.  The table provides essential information about the size and composition of the datasets used in the evaluation of the OneRef model.", "section": "Appendix B: Introduction of the datasets"}, {"figure_path": "siPdcro6uD/tables/tables_17_2.jpg", "caption": "Table 9: Comparison of datasets used in the pre-trained models of the comparable methods.", "description": "This table compares the size and composition of datasets used to pre-train the various vision-language models discussed in the paper.  It highlights the differences in the scale of the datasets used (in terms of image-text pairs, images, text corpora) and dataset types (e.g., general web-crawled data, curated datasets for specific vision tasks). These differences can significantly influence the performance and capabilities of the downstream models.", "section": "4 Experiments"}, {"figure_path": "siPdcro6uD/tables/tables_18_1.jpg", "caption": "Table 10: Network structure of our proposed OneRef framework.", "description": "This table details the architecture of the OneRef model, specifying the backbone used (BEIT-B/16 or BEIT-L/16), input resolution, number of layers and dimensions in the one-tower transformer, number of heads, and the total number of parameters (including all MoE heads) for both the referring expression comprehension (REC) and referring expression segmentation (RES) tasks.", "section": "3.1 Preliminaries"}, {"figure_path": "siPdcro6uD/tables/tables_18_2.jpg", "caption": "Table 11: Hyperparameters of our framework during training.  Ir. denotes the learning rate.", "description": "This table lists the hyperparameters used during the training process of the OneRef model. It includes settings for both the base and large model versions, covering aspects such as the optimizer, number of epochs, learning rates, weight decay, patch size, masking ratios (for image and language modeling), and batch sizes.  The hyperparameters were tuned for optimal performance on the referring tasks.", "section": "3. Methodology"}, {"figure_path": "siPdcro6uD/tables/tables_20_1.jpg", "caption": "Table 12: Comparison with latest SoTA methods for PG task with dataset-mixed intermediate pre-training setting. \u2018RefC\u2019 represents the mixup of RefCOCO/+/g training data. \u2020 indicates RefC has been used during pre-training.", "description": "This table compares the performance of the proposed OneRef model with other state-of-the-art (SOTA) methods on the Phrase Grounding (PG) task.  The comparison considers a setting where models are pre-trained on a mixture of datasets, including RefCOCO, RefCOCO+, and RefCOCOg.  The table shows the performance on the ReferIt and Flickr30K Entities datasets, using metrics relevant to the task, and indicates model size (parameters).", "section": "4.2 Comparison with state-of-the-art methods"}, {"figure_path": "siPdcro6uD/tables/tables_20_2.jpg", "caption": "Table 3: Comparison with latest SoTA methods (mIoU metric) on the three datasets for RES task with both single-dataset fine-tuning setting and dataset-mixed intermediate pre-training setting.", "description": "This table compares the performance of OneRef with other state-of-the-art (SoTA) methods on three referring expression segmentation (RES) datasets: RefCOCO, RefCOCO+, and RefCOCOg.  It shows the mean Intersection over Union (mIoU) scores achieved by different methods under two different training settings: single-dataset fine-tuning and dataset-mixed intermediate pre-training.  The table allows for a comparison of OneRef's performance against SoTA methods across various backbone models and training strategies.", "section": "4.2 Comparison with state-of-the-art methods"}, {"figure_path": "siPdcro6uD/tables/tables_21_1.jpg", "caption": "Table 14: Comparison of the computational cost in REC task. The results are obtained on RefCOCO dataset. The testing environment is 1 NVIDIA A100 GPU. \u2020 indicates that the model's code is not publicly available, and the replicated estimation results are shown. The backbone parameters of our UniRef model only include the actual calculated parameters, specifically those of the V-L expert head in MoE, while excluding the parameters of unused visual and language expert heads and their uni-modal branches. We highlight the best result in bold. (FPS: images / (GPU \u00b7 second))", "description": "This table compares the computational cost of different models on the REC task using the RefCOCO dataset.  It shows the number of parameters for the backbone, fusion head, total model, FLOPs, fine-tuning FPS, test FPS, test time, and test accuracy. The table highlights the efficiency of the proposed UniRef model compared to other state-of-the-art models.", "section": "4.2 Comparison with state-of-the-art methods"}, {"figure_path": "siPdcro6uD/tables/tables_21_2.jpg", "caption": "Table 15: Complete ablation study of MRefM using our OneRef-base model in REC task on both single-dataset fine-tuning setting and mixup intermediate pre-training setting.(Acc@0.5(%))", "description": "This table presents the ablation study results of the Mask Referring Modeling (MRefM) paradigm proposed in the paper. It shows the performance of the OneRef-base model on the REC task under two different settings: single-dataset fine-tuning and dataset-mixed intermediate pre-training. The table includes different variations of MRefM, such as using vanilla MIM and MLM, different image masking strategies (random, block-wise, and referring-aware), and different combinations of MIM and MLM modules. The results are presented in terms of accuracy at a threshold of 0.5 (Acc@0.5) for different evaluation metrics on the RefCOCO, RefCOCO+, and RefCOCOg datasets.", "section": "4.3 Ablation study"}, {"figure_path": "siPdcro6uD/tables/tables_22_1.jpg", "caption": "Table 16: Ablation study of the mask ratio in referring-aware dynamic masking strategy on Ref-COCOg(val) dataset.", "description": "This table shows the results of an ablation study on the mask ratio used in the referring-aware dynamic image masking strategy. The study was conducted on the RefCOCOg validation dataset. The table shows the impact of varying the mask ratio (\u03b2 and \u03b3) on the accuracy of the model (Acc@0.5%). The best performance is achieved with \u03b2 = 0.35 and \u03b3 = 0.75.", "section": "E.5 Ablation study of the mask ratio in referring-aware dynamic masking strategy"}]