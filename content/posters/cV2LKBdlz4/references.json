{"references": [{"fullname_first_author": "Josh Alman", "paper_title": "Fast attention requires bounded entries", "publication_date": "2023", "reason": "This paper provides foundational results on the computational complexity of attention mechanisms, which are crucial for understanding the efficiency limits of DiTs."}, {"fullname_first_author": "Minshuo Chen", "paper_title": "Score approximation, estimation and distribution recovery of diffusion models on low-dimensional data", "publication_date": "2023", "reason": "This paper lays the groundwork for the statistical analysis of diffusion models in low-dimensional spaces, providing essential theoretical underpinnings for the DiT analysis."}, {"fullname_first_author": "Benjamin L Edelman", "paper_title": "Inductive biases and variable creation in self-attention mechanisms", "publication_date": "2022", "reason": "This paper provides insights into the inductive biases inherent in transformer architectures, which influence the DiT's capacity for score approximation."}, {"fullname_first_author": "Patrick Esser", "paper_title": "Scaling rectified flow transformers for high-resolution image synthesis", "publication_date": "2024", "reason": "This paper showcases the empirical success of DiTs in high-resolution image generation, demonstrating their practical relevance and providing a benchmark for evaluating theoretical findings."}, {"fullname_first_author": "Jerry Yao-Chieh Hu", "paper_title": "On sparse modern hopfield model", "publication_date": "2023", "reason": "This paper offers a theoretical analysis of hopfield networks which is relevant for establishing the approximation capabilities and memory capacity of transformer-based score networks in DiTs."}]}