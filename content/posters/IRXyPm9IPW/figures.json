[{"figure_path": "IRXyPm9IPW/figures/figures_0_1.jpg", "caption": "Figure 1: Fine-grained feedback from multimodal large language model help to yield more human-preferred images. Left: Output generated by the baseline text-to-image generative model. Right: Output generated by the baseline model optimized with fine-grained feedback from multimodal large language model. We illustrate improvements in generation quality across four aspects: Prompt-Following, Aesthetic, Fidelity and Harmlessness. See in Appendix for more visualization examples.", "description": "This figure shows the improvements in image generation quality after optimizing a baseline text-to-image model using fine-grained feedback from a multimodal large language model.  The left column displays images generated by the baseline model, while the right column presents the improved images generated after optimization. Four aspects are highlighted and compared: Prompt Following (how well the image matches the prompt), Aesthetics (visual appeal), Fidelity (accuracy of object representation), and Harmlessness (absence of inappropriate content). The improvement is visually evident across all four aspects.", "section": "Abstract"}, {"figure_path": "IRXyPm9IPW/figures/figures_3_1.jpg", "caption": "Figure 2: VisionPrefer construction pipeline. We sample textual prompts and text-to-image generative models from pools to generate diverse comparison data, then query GPT-4 V with detailed illustrations for fine-grained and high-quality annotations in both textual and numerical formats.", "description": "The figure illustrates the process of constructing the VisionPrefer dataset. It starts with selecting prompts from the DiffusionDB and then polishing them using GPT-4 to remove biases and inconsistencies.  These polished prompts are used to generate images using diverse text-to-image generative models (GANs and diffusion models). Finally, GPT-4 V provides fine-grained preference annotations for these generated images considering four aspects: prompt-following, fidelity, aesthetic, and harmlessness.  The annotations are given in both numerical scores and textual explanations.", "section": "3 VisionPrefer"}, {"figure_path": "IRXyPm9IPW/figures/figures_4_1.jpg", "caption": "Figure 3: Evaluation results of the text-to-image model's generation quality across multiple reward models when maximizing scores from VP-Score during the PPO training process. All scores are normalized for a better visualization.", "description": "This figure shows the evaluation results of text-to-image model generation quality using multiple reward models while maximizing VP-Score during the PPO training process. The y-axis represents the normalized reward score, and the x-axis represents the training steps. The lines represent different reward models, including ImageReward, Aesthetic, PickScore, HPS v2, and the proposed VP-Score. The figure illustrates how VP-Score improves the generation quality over training steps compared to other reward models.", "section": "4.2 Fine-tuning Text-to-Image Generative Models"}, {"figure_path": "IRXyPm9IPW/figures/figures_4_2.jpg", "caption": "Figure 4: Win rates of generative models optimized with VP-Score compared to other reward models on three test benchmarks for PPO experiments. 'Tie' indicates instances where annotators think two images are of comparable quality.", "description": "This figure presents a comparison of the performance of generative models fine-tuned using different reward models. The models were evaluated on three benchmark datasets (DiffusionDB, ImageRewardDB, and HPD v2) using the Proximal Policy Optimization (PPO) method.  The VP-Score reward model, trained using the VisionPrefer dataset, is compared against other reward models (CLIP, Aesthetic, ImageReward, PickScore, and HPS v2). The win rate, tie rate, and loss rate are shown for each model on each dataset, providing a comprehensive view of the comparative performance of VP-Score and existing reward models in improving text-to-image generation.", "section": "4.2 Fine-tuning Text-to-Image Generative Models"}, {"figure_path": "IRXyPm9IPW/figures/figures_5_1.jpg", "caption": "Figure 5: Qualitative results for PPO experiments. SD 1.5 denotes the Stable Diffusion v1.5 model without any fine-tune. See Appendix for more samples.", "description": "This figure shows the qualitative results of using Proximal Policy Optimization (PPO) to fine-tune the Stable Diffusion v1.5 model. Two prompts are used to generate images with different reward models, including the baseline Stable Diffusion v1.5 model and those fine-tuned with various reward models, such as CLIP, Aesthetic, ImageReward, PickScore, HPS v2, and VP-Score (the proposed model). The figure showcases improvements in the quality of generated images with the VP-Score model in terms of both prompt adherence and aesthetic appeal.", "section": "4.2 Fine-tuning Text-to-Image Generative Models"}, {"figure_path": "IRXyPm9IPW/figures/figures_5_2.jpg", "caption": "Figure 4: Win rates of generative models optimized with VP-Score compared to other reward models on three test benchmarks for PPO experiments. 'Tie' indicates instances where annotators think two images are of comparable quality.", "description": "This figure shows the results of a human evaluation study comparing the performance of generative models fine-tuned using VP-Score against other reward models.  The evaluation is performed on three different test benchmarks (DiffusionDB, ImageRewardDB, and HPD v2). The win rate represents the percentage of times the model using VP-Score produced a better image than the other models.  A 'Tie' indicates cases where human evaluators deemed the images to be of comparable quality.  The figure visually represents the comparative performance of VP-Score against other existing reward models in improving the alignment of generated images with human preferences.", "section": "4.2 Fine-tuning Text-to-Image Generative Models"}, {"figure_path": "IRXyPm9IPW/figures/figures_6_1.jpg", "caption": "Figure 5: Qualitative results for PPO experiments. SD 1.5 denotes the Stable Diffusion v1.5 model without any fine-tune. See Appendix for more samples.", "description": "This figure shows the qualitative results of applying Proximal Policy Optimization (PPO) to fine-tune the Stable Diffusion v1.5 model using different reward models.  The left side displays images generated by the baseline model (SD 1.5), while the right side showcases results after optimization using various methods. Each row represents a different prompt, and the columns represent different reward models: CLIP, Aesthetic, ImageReward, PickScore, HPS v2, and the authors' VP-Score.  The improved quality of images generated with VP-Score, particularly in terms of aesthetics and alignment with the prompt, is highlighted.", "section": "4.2 Fine-tuning Text-to-Image Generative Models"}, {"figure_path": "IRXyPm9IPW/figures/figures_6_2.jpg", "caption": "Figure 2: VisionPrefer construction pipeline. We sample textual prompts and text-to-image generative models from pools to generate diverse comparison data, then query GPT-4 V with detailed illustrations for fine-grained and high-quality annotations in both textual and numerical formats.", "description": "The figure illustrates the pipeline used to construct the VisionPrefer dataset.  It begins with sampling textual prompts and selecting text-to-image generative models. These are used to generate pairs of images, which are then passed to GPT-4 V along with a detailed guideline.  GPT-4 V provides fine-grained preference annotations in textual and numerical formats, covering Prompt-Following, Fidelity, Aesthetic, and Harmlessness. This process generates a large-scale, high-quality, and fine-grained preference dataset for text-to-image generative models.", "section": "3 VisionPrefer"}, {"figure_path": "IRXyPm9IPW/figures/figures_7_1.jpg", "caption": "Figure 1: Fine-grained feedback from multimodal large language model help to yield more human-preferred images. Left: Output generated by the baseline text-to-image generative model. Right: Output generated by the baseline model optimized with fine-grained feedback from multimodal large language model. We illustrate improvements in generation quality across four aspects: Prompt-Following, Aesthetic, Fidelity and Harmlessness. See in Appendix for more visualization examples.", "description": "This figure shows the improvements in image generation quality when using a multimodal large language model to provide fine-grained feedback. The left side displays images generated by a baseline text-to-image model, while the right side shows images generated by the same model but optimized with feedback from the multimodal model.  The improvement is illustrated across four key aspects: how well the image follows the prompt, its aesthetic quality, its fidelity to the prompt's description, and whether it is harmless (i.e., free from inappropriate content).", "section": "Abstract"}, {"figure_path": "IRXyPm9IPW/figures/figures_8_1.jpg", "caption": "Figure 11: Fine-grained feedback enables our model (denoted as VP-Score) to generate results that better align with the input prompt. See Appendix for more samples.", "description": "This figure shows a comparison of image generation results from different reward models, including VP-Score, when given the prompt: \"A hyper-realistic portrait of a woman holding flowers, featuring a cottagecore and grunge aesthetic.\"  VP-Score's results show a better alignment with the prompt's specific aesthetic requests (cottagecore and grunge) compared to other reward models.  The Appendix contains more examples.", "section": "4.2 Fine-tuning Text-to-Image Generative Models"}, {"figure_path": "IRXyPm9IPW/figures/figures_8_2.jpg", "caption": "Figure 1: Fine-grained feedback from multimodal large language model help to yield more human-preferred images. Left: Output generated by the baseline text-to-image generative model. Right: Output generated by the baseline model optimized with fine-grained feedback from multimodal large language model. We illustrate improvements in generation quality across four aspects: Prompt-Following, Aesthetic, Fidelity and Harmlessness. See in Appendix for more visualization examples.", "description": "This figure showcases the impact of incorporating fine-grained feedback from a multimodal large language model (MLLM) on the quality of images generated by a text-to-image model. The left side displays images generated by the baseline model, while the right side shows images generated after optimization using MLLM feedback.  The improvements are highlighted across four key aspects: how well the image follows the prompt, its aesthetic appeal, image fidelity (accuracy in representing the prompt), and harmlessness (absence of NSFW or offensive content).", "section": "Abstract"}, {"figure_path": "IRXyPm9IPW/figures/figures_13_1.jpg", "caption": "Figure 5: Qualitative results for PPO experiments. SD 1.5 denotes the Stable Diffusion v1.5 model without any fine-tune. See Appendix for more samples.", "description": "This figure presents a qualitative comparison of images generated using different reward models in conjunction with the Stable Diffusion v1.5 model. The comparison highlights the impact of fine-tuning using the VP-Score reward model compared to other reward models such as CLIP, Aesthetic, ImageReward, PickScore, and HPS v2. The images shown represent examples of model outputs across different prompts.  The appendix contains additional examples.", "section": "4.2 Fine-tuning Text-to-Image Generative Models"}, {"figure_path": "IRXyPm9IPW/figures/figures_13_2.jpg", "caption": "Figure 5: Qualitative results for PPO experiments. SD 1.5 denotes the Stable Diffusion v1.5 model without any fine-tune. See Appendix for more samples.", "description": "This figure shows the qualitative results obtained from experiments using Proximal Policy Optimization (PPO).  It compares image generation results from the Stable Diffusion v1.5 model (without fine-tuning) against those fine-tuned using different reward models (CLIP, Aesthetic, ImageReward, PickScore, HPS v2, and VP-Score).  The comparison highlights the improvements in image quality achieved by using the VP-Score reward model, showcasing better alignment with the provided prompts.  Additional examples are provided in the Appendix.", "section": "4.2 Fine-tuning Text-to-Image Generative Models"}, {"figure_path": "IRXyPm9IPW/figures/figures_13_3.jpg", "caption": "Figure 5: Qualitative results for PPO experiments. SD 1.5 denotes the Stable Diffusion v1.5 model without any fine-tune. See Appendix for more samples.", "description": "This figure shows qualitative results obtained after fine-tuning the Stable Diffusion v1.5 model using Proximal Policy Optimization (PPO).  It compares image generation results using different reward models: CLIP, Aesthetic, ImageReward, PickScore, HPS v2, and VP-Score (the model trained with the VisionPrefer dataset).  The goal is to demonstrate how the VisionPrefer dataset and the VP-Score reward model improve the alignment between the generated images and the text prompts.  The \"SD 1.5\" column displays results from the Stable Diffusion v1.5 model without any fine-tuning, serving as a baseline.  The additional samples in the Appendix offer a more comprehensive comparison.", "section": "4.2 Fine-tuning Text-to-Image Generative Models"}, {"figure_path": "IRXyPm9IPW/figures/figures_13_4.jpg", "caption": "Figure 13: Qualitative comparison between text-to-image generative model optimized with the guidance of VP-Score and other reward models. SD 1.5 denotes the Stable Diffusion v1.5 model without any fine-tune.", "description": "This figure shows a qualitative comparison of images generated by Stable Diffusion v1.5 model fine-tuned with different reward models, including VP-Score and other reward models like CLIP, PickScore, Aesthetic, and HPS v2.  Each row presents a different prompt, and each column shows the output generated by a specific reward model.  The purpose is to visually demonstrate the improved quality and alignment with the prompt achieved by using VP-Score, compared to other methods.  The images demonstrate that VP-Score leads to images better aligned with the prompt and higher quality.", "section": "4.2 Fine-tuning Text-to-Image Generative Models"}, {"figure_path": "IRXyPm9IPW/figures/figures_14_1.jpg", "caption": "Figure 14: Qualitative comparison between generative model trained on VisionPrefer and other human-annotated preference datasets. SD 1.5 denotes the Stable Diffusion v1.5 model without any fine-tune.", "description": "This figure showcases a qualitative comparison of image generation results using different preference datasets to fine-tune the Stable Diffusion v1.5 model.  The top row shows images generated with prompts related to Batman, and the bottom row shows images generated with prompts related to cars. The different columns represent different preference datasets used for fine-tuning, including the proposed VisionPrefer and other existing datasets such as HPD, ImageRewardDB, and Pick-a-Pic.  The goal is to visually demonstrate how using the VisionPrefer dataset results in images that more closely align with user preferences compared to models trained on the other datasets.", "section": "4.2 Fine-tuning Text-to-Image Generative Models"}, {"figure_path": "IRXyPm9IPW/figures/figures_14_2.jpg", "caption": "Figure 14: Qualitative comparison between generative model trained on VisionPrefer and other human-annotated preference datasets. SD 1.5 denotes the Stable Diffusion v1.5 model without any fine-tune.", "description": "This figure shows a qualitative comparison of images generated by Stable Diffusion v1.5 model fine-tuned using different preference datasets.  The top row shows images generated with the prompt \"batman monster digital art, fantasy, magic, trending on artstation, ultra detailed, professional illustration by Basil Gogos\". The bottom row shows images generated with the prompt \"car in center JZX100 twin turbo drift on a road, surrounded by trees and buildings in Tokyo prefecture, rooftops are Japanese architecture, city at sunset heavy mist over streetlights, cinematic lighting, photorealistic, detailed wheels, highly detailed\". Each column represents a different preference dataset used for fine-tuning: SD1.5 (no fine-tuning), HPD, ImageRewardDB, Pick-a-Pic, and VisionPrefer (multiple columns for VisionPrefer to show variations). The figure demonstrates the impact of the different preference datasets on image quality and alignment with the text prompts.  VisionPrefer shows improvements in detail, style, and adherence to the prompt.", "section": "4.2 Fine-tuning Text-to-Image Generative Models"}, {"figure_path": "IRXyPm9IPW/figures/figures_15_1.jpg", "caption": "Figure 15: Ablation study for the size of training data used in optimizing VP-Score.", "description": "The figure shows the ablation study for different sizes of training datasets used in optimizing VP-Score. The x-axis represents the training data size in millions, and the y-axis represents the accuracy. Three lines represent the accuracy on three different datasets: ImageRewardDB, HPD v2, and Pick-a-Pic. The results show that increasing training data enhances VP-Score's prediction accuracy. This indicates that models trained on VisionPrefer exhibit strong performance scalability, implying that more training data leads to further performance improvements.", "section": "A Ablation Study"}, {"figure_path": "IRXyPm9IPW/figures/figures_17_1.jpg", "caption": "Figure 1: Fine-grained feedback from multimodal large language model help to yield more human-preferred images. Left: Output generated by the baseline text-to-image generative model. Right: Output generated by the baseline model optimized with fine-grained feedback from multimodal large language model. We illustrate improvements in generation quality across four aspects: Prompt-Following, Aesthetic, Fidelity and Harmlessness. See in Appendix for more visualization examples.", "description": "This figure shows a comparison between images generated by a baseline text-to-image model and those generated by the same model after being fine-tuned using feedback from a multimodal large language model (MLLM).  The left side displays images from the baseline model, while the right side shows images generated after the fine-tuning. The improvement is highlighted across four key aspects: how well the image follows the prompt, its aesthetic appeal, image fidelity (accuracy in portraying specified details), and harmlessness (absence of inappropriate content).", "section": "Abstract"}, {"figure_path": "IRXyPm9IPW/figures/figures_18_1.jpg", "caption": "Figure 1: Fine-grained feedback from multimodal large language model help to yield more human-preferred images. Left: Output generated by the baseline text-to-image generative model. Right: Output generated by the baseline model optimized with fine-grained feedback from multimodal large language model. We illustrate improvements in generation quality across four aspects: Prompt-Following, Aesthetic, Fidelity and Harmlessness. See in Appendix for more visualization examples.", "description": "This figure shows the improvements in image generation quality achieved by using fine-grained feedback from a multimodal large language model. The left side displays images generated by a baseline text-to-image model, while the right side shows images generated by the same model after optimization with the multimodal feedback.  The improvements are illustrated across four key aspects: how well the image follows the prompt, its aesthetic appeal, its fidelity to the prompt's description, and its harmlessness.", "section": "Abstract"}, {"figure_path": "IRXyPm9IPW/figures/figures_18_2.jpg", "caption": "Figure 1: Fine-grained feedback from multimodal large language model help to yield more human-preferred images. Left: Output generated by the baseline text-to-image generative model. Right: Output generated by the baseline model optimized with fine-grained feedback from multimodal large language model. We illustrate improvements in generation quality across four aspects: Prompt-Following, Aesthetic, Fidelity and Harmlessness. See in Appendix for more visualization examples.", "description": "This figure shows the impact of multimodal large language models (MLLMs) on improving the alignment of text-to-image generative models. The left side displays images generated by a baseline model, while the right shows images generated by the same model but optimized using feedback from the MLLM. The improvements are illustrated in four aspects: how well the image follows the text prompt, its aesthetic quality, its fidelity to the prompt's description, and its harmlessness (absence of inappropriate content).", "section": "Abstract"}, {"figure_path": "IRXyPm9IPW/figures/figures_19_1.jpg", "caption": "Figure 5: Qualitative results for PPO experiments. SD 1.5 denotes the Stable Diffusion v1.5 model without any fine-tune. See Appendix for more samples.", "description": "This figure shows the qualitative results of using Proximal Policy Optimization (PPO) for fine-tuning text-to-image generative models.  It compares images generated by the Stable Diffusion v1.5 model without any fine-tuning (SD 1.5) against images generated after fine-tuning with various reward models (CLIP, Aesthetic, ImageReward, PickScore, HPS v2, and VP-Score). The goal is to visually demonstrate the improvement in image quality and alignment with text prompts achieved by using the VP-Score reward model, which leverages the VisionPrefer dataset.", "section": "4.2 Fine-tuning Text-to-Image Generative Models"}, {"figure_path": "IRXyPm9IPW/figures/figures_19_2.jpg", "caption": "Figure 5: Qualitative results for PPO experiments. SD 1.5 denotes the Stable Diffusion v1.5 model without any fine-tune. See Appendix for more samples.", "description": "This figure displays the qualitative results of the Proximal Policy Optimization (PPO) experiments conducted in the paper.  It showcases images generated by Stable Diffusion v1.5 (SD 1.5) model without any fine-tuning (baseline) and the same model after fine-tuning with various reward models, including the model's own VP-Score.  The comparison highlights the improvement in image generation quality when using the VP-Score for fine-tuning, specifically illustrating differences in prompt-following, aesthetic quality, fidelity, and harmlessness across various prompts. The appendix contains additional samples.", "section": "4.2 Fine-tuning Text-to-Image Generative Models"}, {"figure_path": "IRXyPm9IPW/figures/figures_19_3.jpg", "caption": "Figure 1: Fine-grained feedback from multimodal large language model help to yield more human-preferred images. Left: Output generated by the baseline text-to-image generative model. Right: Output generated by the baseline model optimized with fine-grained feedback from multimodal large language model. We illustrate improvements in generation quality across four aspects: Prompt-Following, Aesthetic, Fidelity and Harmlessness. See in Appendix for more visualization examples.", "description": "This figure shows the impact of using multimodal large language models to improve text-to-image generation. The left side displays images generated by a standard model, while the right shows improved images produced after incorporating feedback from the multimodal model.  The improvements are categorized into four key areas: how well the image follows the text prompt, the aesthetic quality of the image, the accuracy of the details in the image, and the harmlessness (absence of NSFW content).", "section": "Abstract"}, {"figure_path": "IRXyPm9IPW/figures/figures_19_4.jpg", "caption": "Figure 19: Fine-grained feedback reduce the image distortion. SD 1.5 denotes the Stable Diffusion v1.5 model without any fine-tune.", "description": "The figure shows a comparison of images generated by different reward models. The red boxes highlight the distortion in the generated images. VP-Score shows less distortion, indicating that fine-grained feedback helps to reduce image distortion.", "section": "B.3 Reduce Image Distortion"}, {"figure_path": "IRXyPm9IPW/figures/figures_21_1.jpg", "caption": "Figure 1: Fine-grained feedback from multimodal large language model help to yield more human-preferred images. Left: Output generated by the baseline text-to-image generative model. Right: Output generated by the baseline model optimized with fine-grained feedback from multimodal large language model. We illustrate improvements in generation quality across four aspects: Prompt-Following, Aesthetic, Fidelity and Harmlessness. See in Appendix for more visualization examples.", "description": "This figure shows the impact of multimodal large language models on improving the alignment of text-to-image generative models with human preferences. The left side displays images generated by a baseline model, while the right side shows images from the same model, but optimized with feedback from a multimodal large language model. The improvements are categorized into four aspects: Prompt-Following, Aesthetics, Fidelity, and Harmlessness, showcasing the model's enhanced ability to generate more human-preferred outputs.", "section": "Abstract"}, {"figure_path": "IRXyPm9IPW/figures/figures_21_2.jpg", "caption": "Figure 1: Fine-grained feedback from multimodal large language model help to yield more human-preferred images. Left: Output generated by the baseline text-to-image generative model. Right: Output generated by the baseline model optimized with fine-grained feedback from multimodal large language model. We illustrate improvements in generation quality across four aspects: Prompt-Following, Aesthetic, Fidelity and Harmlessness. See in Appendix for more visualization examples.", "description": "This figure shows a comparison of images generated by a baseline text-to-image model and the same model optimized using feedback from a multimodal large language model. The improvement is demonstrated across four aspects: how well the image follows the prompt, its aesthetic quality, its fidelity (accuracy in representing details of the prompt), and its harmlessness.  The left side displays images from the baseline model, while the right shows the improved results.", "section": "Abstract"}, {"figure_path": "IRXyPm9IPW/figures/figures_21_3.jpg", "caption": "Figure 23: (a) Win rate versus classifier-free guidance value for Stable Diffusion XL. (b) Preference distribution when comparing Stable Diffusion 2.1 with Dreamlike Photoreal 2.05.", "description": "This figure presents the results of two different analyses related to the impact of classifier-free guidance values and model selection on the preferences of GPT-4V.  (a) shows the win, tie, and loss ratios for Stable Diffusion XL across different guidance values, demonstrating that higher values generally lead to higher win rates. (b) shows a comparison of preferences between Stable Diffusion 2.1 and Dreamlike Photoreal 2.05, indicating that GPT-4V tends to prefer images generated by Dreamlike Photoreal 2.05.", "section": "C Statistics of VisionPrefer"}, {"figure_path": "IRXyPm9IPW/figures/figures_23_1.jpg", "caption": "Figure 1: Fine-grained feedback from multimodal large language model help to yield more human-preferred images. Left: Output generated by the baseline text-to-image generative model. Right: Output generated by the baseline model optimized with fine-grained feedback from multimodal large language model. We illustrate improvements in generation quality across four aspects: Prompt-Following, Aesthetic, Fidelity and Harmlessness. See in Appendix for more visualization examples.", "description": "This figure shows a comparison of images generated by a baseline text-to-image model and the same model optimized using fine-grained feedback from a multimodal large language model.  The left side displays images from the baseline model, while the right side shows images produced after optimization. Four aspects are highlighted to illustrate the improvement:  how well the image follows the prompt, its aesthetic appeal, its fidelity (accuracy in representing details), and whether the image is harmless (free from offensive or inappropriate content).  The figure demonstrates the effectiveness of incorporating multimodal large language model feedback to enhance image generation.", "section": "Abstract"}, {"figure_path": "IRXyPm9IPW/figures/figures_26_1.jpg", "caption": "Figure 2: VisionPrefer construction pipeline. We sample textual prompts and text-to-image generative models from pools to generate diverse comparison data, then query GPT-4 V with detailed illustrations for fine-grained and high-quality annotations in both textual and numerical formats.", "description": "The pipeline consists of three main steps: prompt generation, image generation, and preference annotation.  First, prompts are generated and polished (to remove biases and NSFW content). Then, text-to-image models generate four images for each prompt, aiming for diversity. Finally, GPT-4 provides fine-grained feedback, including scalar scores, ranking, and textual explanations for each image across four aspects: Prompt-Following, Fidelity, Aesthetic, and Harmlessness.", "section": "3 VisionPrefer"}]