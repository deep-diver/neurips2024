{"references": [{"fullname_first_author": "Y. Bai", "paper_title": "Training a helpful and harmless assistant with reinforcement learning from human feedback", "publication_date": "2022-04-05", "reason": "This paper is foundational for RLHF methods and directly relevant to the paper's approach of leveraging human feedback to improve text-to-image alignment."}, {"fullname_first_author": "K. Black", "paper_title": "Training diffusion models with reinforcement learning", "publication_date": "2023-05-13", "reason": "This paper explores the application of reinforcement learning to fine-tune diffusion models, a technique directly relevant to the methodology employed in this work."}, {"fullname_first_author": "K. Clark", "paper_title": "Directly fine-tuning diffusion models on differentiable rewards", "publication_date": "2023-09-17", "reason": "This paper presents a direct fine-tuning method for diffusion models using differentiable rewards, which is directly compared to and contrasted with in this work."}, {"fullname_first_author": "J. Li", "paper_title": "Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation", "publication_date": "2022-00-00", "reason": "BLIP is used as the backbone for the reward model in this work, making it a crucial component of the proposed methodology."}, {"fullname_first_author": "J. Xu", "paper_title": "Imagereward: Learning and evaluating human preferences for text-to-image generation", "publication_date": "2023-04-05", "reason": "This paper introduces a reward model based on human preferences for text-to-image generation, which serves as a key baseline for evaluating the performance of the proposed reward model."}]}