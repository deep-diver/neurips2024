[{"type": "text", "text": "MoTE: Reconciling Generalization with Specialization for Visual-Language to Video Knowledge Transfer ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Minghao Zhu Zhengpu Wang Mengxian Hu Ronghao Dang Xiao Lin Xun Zhou Chengju Liu\u2217 Qijun Chen ", "page_idx": 0}, {"type": "text", "text": "Tongji University, Shanghai, China {zmhh_h, wangzhengpu, humengxian, dangronghao, linx_xx, zhouxun, liuchengju, qjchen}@tongji.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Transferring visual-language knowledge from large-scale foundation models for video recognition has proved to be effective. To bridge the domain gap, additional parametric modules are added to capture the temporal information. However, zero-shot generalization diminishes with the increase in the number of specialized parameters, making existing works a trade-off between zero-shot and close-set performance. In this paper, we present MoTE, a novel framework that enables generalization and specialization to be balanced in one unified model. Our approach tunes a mixture of temporal experts to learn multiple task views with various degrees of data fitting. To maximally preserve the knowledge of each expert, we propose Weight Merging Regularization, which regularizes the merging process of experts in weight space. Additionally with temporal feature modulation to regularize the contribution of temporal feature during test. We achieve a sound balance between zero-shot and close-set video recognition tasks and obtain stateof-the-art or competitive results on various datasets, including Kinetics-400 & 600, UCF, and HMDB. Code is available at https://github.com/ZMHH-H/MoTE. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "With the advent of large-scale Vision-Language Models (VLMs), adapting such foundation models (e.g., CLIP [33], ALIGN [13], Florence [54]) for downstream tasks has become an emerging paradigm of intense scrutiny. Their semantic visual concepts empowered by aligning large-scale image-text pairs can be transferred to a wide range of downstream tasks, such as open-vocabulary classification [55, 56], detection [11, 51], and segmentation [21, 52]. ", "page_idx": 0}, {"type": "text", "text": "The crux of an adaptation procedure for foundation models lies in injecting specialized knowledge of the domain of interest. For video recognition tasks, this necessity is reflected in the fact that the dynamic nature of video data requires effective comprehension of context and temporal correlation by the model. Therefore, to condition the adapted model on the video-specialized knowledge, one general and effective way is to incorporate additional parameters in the form of well-designed prompts [43, 45], adapters [30, 53], and temporal modules [28, 49, 50]. However, we observe that while the increased model capacity brought by more additional parameters enables the better fitting of videospecific inductive bias, it comes at the cost of catastrophically forgetting the generalization knowledge of the original VLM. To better illustrate this phenomenon, we present an overview of existing methods in Figure 1. A clear trade-off problem between zero-shot and close-set performance emerges in existing works, which correlates to the scale of additional parameters. The former relies more on the generalization capability inherent in VLMs while the latter requires intensive video-specialized knowledge, but no method achieves the best of both worlds. On one hand, introducing new knowledge while preserving existing knowledge continually is desirable and significant for the broader adaptation of the foundation model. The rapidly evolving real-world applications also require both specialization and generalization capabilities. However, how to manage the generalization/specialization trade-off of additional parameters in transfer learning remains under-explored. ", "page_idx": 0}, {"type": "image", "img_path": "vpEq2bzsS0/tmp/87ae1a9d999820ceb6a905aefb68da6bc48aee54e687013dc74e738b7019f531.jpg", "img_caption": ["Figure 1: Overview of existing VLM knowledge transfer methods. (a) Trade-off plots between zero-shot (Harmonic mean of UCF, HMDB, and K600) and close-set (K400) performance of recent CLIP-based methods (ViT-B/16). (b) As the number of temporal layers increases, the generalization of the standard Transformer layer severely degrades while our proposed MoTE consistently improves the zero-shot and close-set performance. (c) Our proposed MoTE seeks to construct a reconciled feature space between the optimal generalized and specialized manifolds. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "This paper addresses the aforementioned challenge by delving into two inquiries: (i) How can the generalization capability of additional parameters be enhanced? Given the substantially smaller scale of the fine-tuning dataset compared to the pre-training dataset of VLMs, the newly added parameters risk overfitting the fine-tuning data bias, thereby constraining the model generalization. Addressing this question enables steering parameters towards high generalization. Nevertheless, generalization and specialization have proven to be somewhat confilcting [18, 29, 36] in model training, prompting us to further explore (ii) How can generalization and specialization coexist in one unified model? This question is crucial for a balanced model to preserve both new and existing knowledge, yet has received limited exploration. We investigate these two questions with a widely employed temporal module [14, 49, 50] (i.e. N-layer transformer), which already features a high specialization degree but is less performant on zero-shot tasks. ", "page_idx": 1}, {"type": "text", "text": "With this in mind, we present MoTE, a mixture-of-temporal-experts approach with well-balanced generalization and specialization. We offer a novel perspective in addressing the first question: constructing a more generalized model using multiple data bias views. In contrast to conventional temporal modules that encode patterns with a single feedforward network (FFN) in each Transformer layer, MoTE uses multiple FFN experts to capture various data bias views. During fine-tuning, incoming frame token sequences are routed to one of the temporal experts, keeping the computational cost the same as using a single FFN. To enlarge the discrepancy in knowledge learned by each expert, we devise a routing algorithm based on the multinomial distribution. During inference, we apply weights merging to collapse multiple experts into one module, enabling the patched model to aggregate the generalized knowledge of each expert. For the second question, we propose Weight Merging Regularization which regularizes the merging process of experts in the weight space. The proposed regularizer drives a range of the merged parameters optimal with respect to the task-specific objective, allowing a more effective aggregation of generalized and specialized knowledge in the patched model. To further alleviate the overftiting at test time, we devise a plug-and-play module that modulates the contribution of temporal features by measuring the semantic association between the proxy text features from the fine-tuning and the test datasets. ", "page_idx": 1}, {"type": "text", "text": "Our main contributions can be summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We propose MoTE, a knowledge transfer framework from visual-language to video domain. MoTE tackles the ftiting trade-off challenge posed by additional parameters, an aspect largely overlooked by previous works. \u2022 We provide new insights for enhancing parameter generalization from the perspective of data bias fitting, all while keeping the computation cost and final structure constant $(\\S\\ 3.1,\\S\\ 3.2)$ . \u2022 We propose Weight Merging Regularization, a novel regularizer for more effective knowledge aggregation, realizing the coexistence of generalization and specialization in weight space $(\\S\\ 3.3)$ . Together with Temporal Feature Modulation to further improve the generalization of MoTE (\u00a7 3.4). ", "page_idx": 1}, {"type": "text", "text": "\u2022 Extensive experiments demonstrate MoTE achieves an optimal trade-off between zero-shot and close-set performance with one unified model. Thorough ablation studies show the scalability and effectiveness of our proposed method $(\\S\\,4)$ . ", "page_idx": 2}, {"type": "text", "text": "2 Preliminary: Transferring CLIP for Video Recognition ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Recent works have adapted CLIP [33] to video datasets and obtained superior results [28, 41]. We briefly describe the typical cross-modal video recognition pipeline. Consider a video $V$ with $T$ frames and the corresponding text label $C$ described in a set of textual prompts. Each frame is encoded independently by the CLIP visual encoder $f(\\cdot|\\theta_{v})$ with parameter $\\theta_{v}$ and produces frame-level embeddings $\\mathbf{\\bar{\\{e}}}_{i}\\in\\mathbb{R}^{D}\\}_{i=1}^{T}$ . The text embedding $\\mathbf{y}\\in\\mathbb{R}^{\\bar{D}}$ is generated by the CLIP text encoder $g(\\cdot|\\theta_{c})$ with parameter $\\theta_{c}$ , where $D$ is the embedding dimension, ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbf{e}_{1},\\cdots,\\mathbf{e}_{T}=f(V|\\theta_{v}),\\;\\;\\mathbf{y}=g(C|\\theta_{c}).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The CLIP visual encoder captures rich spatial information. To bridge the domain gap between image and video, we apply a commonly used temporal module $h(\\cdot|\\theta_{t e m})$ for cross-frame communication [14, 49, 50], which is parameterized by several Transformer layers. The final video embedding $\\mathbf{z}\\in\\mathbb{R}^{D}$ consists of spatial and temporal embeddings connected in a residual form: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{z}=\\mathrm{AvgPool}([\\mathbf{e}_{1},\\cdots,\\mathbf{e}_{T}]+h(\\mathbf{e}_{1},\\cdots,\\mathbf{e}_{T}|\\theta_{t e m})).}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "During optimization, the text encoder is typically frozen. We tune the visual encoder and the temporal module to maximize the similarity $\\begin{array}{r}{\\mathrm{sim}(\\mathbf{z},\\mathbf{y})\\,=\\,\\frac{\\langle\\mathbf{z},\\mathbf{y}\\rangle}{\\|\\mathbf{z}\\|\\|\\mathbf{y}\\|}}\\end{array}$ between the video embedding ${\\bf z}$ and the text embedding $\\mathbf{y}$ if $V$ and $C$ are matched, otherwise minimize it. The training objective can be formulated as: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}(\\theta_{v},\\theta_{t e m};\\mathcal{D})=\\mathbb{E}_{(V,C)\\sim\\mathcal{D}}[\\mathcal{Z}(\\sin(\\mathbf{z},\\mathbf{y}),\\mathrm{onehot}(C))],}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\mathcal{D}$ is the fine-tuning dataset, $\\mathcal{T}$ is the cross-entropy function with softmax operation. ", "page_idx": 2}, {"type": "text", "text": "3 Methodology ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "An overview of our proposed method is presented in Figure 2. This section first analyzes the parameter generalization from the perspective of data bias fitting $(\\S\\ 3.1)$ . Then we detail the structure, routing policy, and inference of MoTE (\u00a7 3.2). Finally, we present Weight Merging Regularization $(\\S\\,3.3)$ and Temporal Feature Modulation $(\\S\\ 3.4)$ towards the coexistence of generalization and specialization. ", "page_idx": 2}, {"type": "text", "text": "3.1 Intuition and Motivation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Typically, more additional parameters allow for better ftiting of the training data distribution, leading to better close-set performance. However, steering the model specialized on the target distribution potentially makes it sensitive to out-of-distribution shifts, resulting in the generalization drop on downstream data distributions with unseen video categories. Although one could enlarge the training data distribution to cover as many potential unseen categories as possible, the costly computation and collection of video data make this infeasible, indicating the need for a more generalized architecture. ", "page_idx": 2}, {"type": "text", "text": "Neural network optimization has many solutions in different loss basins due to the non-convexity of the loss landscape. Models optimized with different configurations (e.g. initialization, optimizer, and data) have different optimization trajectories and may converge to separate local minima, thereby capturing various feature patterns. This inspires our method to construct a more generalized model using multiple data bias views. Instead of improving generalization by searching for a flatter minimum in the loss landscape [22, 48], we expect the aggregation of diverse knowledge from multiple minima can provide a more comprehensive representation. Intuitively, diverse temporal patterns can better facilitate recognizing an unseen video category. For example, aggregating information on \u2019player movement\u2019 and \u2019racket-ball interaction\u2019 can help better recognize \u2019playing tennis\u2019. Our architecture design takes inspiration from Mixture-of-Experts [37] to learn multiple data bias views with a set of experts. While MoE was originally proposed for building large pre-trained models, we extend it to the context of transfer learning and demonstrate its effectiveness in improving parameter generalization. ", "page_idx": 2}, {"type": "image", "img_path": "vpEq2bzsS0/tmp/5095fb94eac3b3d3f1ce9ea90c4f689f0c1e436c1357c634eb6be02e90e5bc1d.jpg", "img_caption": ["Figure 2: An overview of the MoTE framework. (Left): We independently extract the feature of each frame with the CLIP visual encoder. Then, the frame token sequences from a given batch are routed to an activated expert for temporal pattern encoding. To regularize the merging process, we sample the temperature $\\tau$ from a discrete set and use it to collapse multi-experts into one merged FFN. (Right): Temporal feature modulation. We modulate the contribution of the temporal feature with the semantic association, which is measured by the similarity between the proxy text features retrieved from the fine-tuning and the test categories. The modulated embedding is used for inference. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "3.2 Mixture-of-Temporal-Experts ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Architecture Consider the temporal module $h(\\cdot|\\theta_{t e m})$ as $L$ repeated Transformer layers which consist of a self-attention layer and a fully-connected feed-forward network (FFN). For clarity, the temporal module\u2019s parameter $\\theta_{t e m}$ is factorized into parameter $\\theta_{a t t}$ for attention layers and parameter $\\theta_{f}$ for FFNs, where $\\theta_{t e m}\\,=\\,\\theta_{a t t}\\cup\\theta_{f}$ . Recent studies [5, 9] suggest that factual knowledge is mainly stored in the FFN, which consists of two learnable projection matrices and an activation saa smete  sotfr uecxtpuerret sa $\\{\\{E_{i}^{j}\\}_{i=1}^{N}\\}_{j=1}^{L}$ ,  ewxpheerrte $E_{i}^{j}$ t sr etrpariensienngt sf rtohme $i^{t h}$ eerxepnet $N$ antd tohme $j^{t h}$ i allaiyzeatri oann dt o heanss tuhree different optimization trajectories, which we experimentally show as critical for learning distinct knowledge (i.e. data bias views). Denote the parameter of the expert $E_{i}^{j}$ as $\\theta_{i}^{j}$ and the activated expert\u2019s index at the $j^{t h}$ layer as $\\mathcal{E}(j)$ . The training objective becomes: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{TE}}=\\mathcal{L}(\\theta_{v},\\theta_{a t t},\\boxed{\\{\\theta_{\\mathcal{E}(j)}^{j}\\}_{j=1}^{L}};\\mathcal{D}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Routing Policy The routing algorithm of MoE determines which experts process inputs. Classic routing mechanisms (i.e. top- $k$ gating [37]) make decisions using a learnable gate network and require auxiliary losses to handle load balancing. Instead, we adopt the stochastic routing algorithm [59] considering the architecture brevity, as it avoids the need for extra network, computation, and loss. ", "page_idx": 3}, {"type": "text", "text": "Recall that we expect each expert to learn distinct feature patterns. To further enlarge the discrepancy in knowledge learned by each expert, we present a stochastic routing algorithm based on the multinomial distribution. By assigning different activation probabilities to experts, we can control the training data volume of each expert, empowering their knowledge with different degrees of generalization and specialization. Formally, set $\\dot{\\mathbf{A}}=[\\alpha_{i}^{l}]_{i=1}^{\\overline{{N}}}$ to be a random vector in the $l^{t\\widetilde{h}}$ layer, where $\\alpha_{i}^{l}\\in\\{0,1\\}$ indicates whether the $i^{t h}$ expert is activated and $\\textstyle\\sum_{i=1}^{N}\\alpha_{i}^{l}=1$ . We specify the variable A to follow the multinomial distribution where the activation probability of each expert is given as ", "page_idx": 3}, {"type": "equation", "text": "$$\nP(\\alpha_{i}^{l}=1)=\\frac{\\exp(i)}{\\sum_{i=1}^{N}\\exp(i)},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\exp(\\cdot)$ is the natural exponential function. This allows experts with a greater index to be activated more likely and therefore receive a larger volume of data during training. Once the expert is selected, all inputs in a given batch are processed by the same expert. ", "page_idx": 3}, {"type": "text", "text": "Knowledge Aggregation in Inference During inference, knowledge across experts can be aggregated by either merging weights [44, 47] or ensembling logits [16]. We adopt merging weights with the following benefits: (i) More effective knowledge aggregation as evidenced in Table 2. (ii) Constant computation cost, while the latter increases linearly with the expert number. (iii) Consistent parameter structure with its initial state. Specifically, for the experts $\\{E_{i}^{l}\\}_{i=1}^{N}$ in the $l^{t h}$ layer, we average the weights of all the experts to obtain the final parameters \u03b8\u02dcl for inference, where ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\tilde{\\theta^{l}}\\gets\\frac{1}{N}\\sum_{i=1}^{N}\\theta_{i}^{l}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "3.3 Weight Merging Regularization ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "While the introduction of MoTE notably improves model generalization, it still incurs a decrease in close-set performance as presented in Table 1, which again proves the conflicting nature of generalization and specialization in weight space. We attribute this phenomenon to the difficulty of simultaneously preserving the generalized and specialized knowledge during expert merging process. ", "page_idx": 4}, {"type": "text", "text": "In this regard, we propose Weight Merging Regularization to reconcile and promote the aggregation efficacy of expert knowledge. To maximally aggregate the specialized knowledge of experts, the final merged model necessitates explicit optimization with respect to the task-specific objective. As for the preservation of the generalization knowledge, previous works [17, 22, 27] observe that the flatness (i.e. sharpness) of local minima in the loss landscape is strongly correlated with the model generalization. They suggest that models with flatter local minima in the loss landscape generalize better on unseen data, where the flatness refers to the robustness of the loss value to perturbations in the parameters. Within a flat basin of the loss landscape, moderate variations of parameters do not lead to significant increases in loss. Inspired by this observation, we propose to facilitate preserving the generalized knowledge of experts by explicitly constructing a flat region around the loss landscape of the final merged model. We optimize the constructed region with the task-specific loss objective and demonstrate that region construction beneftis the coexistence of specialization and generalization in one unified model. Formally, for the experts $\\{E_{i}^{l}\\}_{i=1}^{N}$ in the $l^{t h}$ layer, we first merge the weights as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\phi^{l}\\leftarrow\\sum_{i=1}^{N}\\frac{\\exp(i/\\tau)}{\\sum_{i^{\\prime}=1}^{N}\\exp(i^{\\prime}/\\tau)}\\cdot\\theta_{i}^{l},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\tau$ is a temperature parameter used to control the softness of the distribution. By varying the value of $\\tau$ , we can build a region of the merged parameters in the weight space. Considering that sampling $\\tau$ in a continuous space may lead to difficulties in model convergence, we sample $\\tau$ from a discrete set given as $\\{\\pm2^{n}\\cdot{\\dot{\\beta}}\\}_{n=0}^{4}\\cup\\{\\infty\\}$ , where $\\beta$ is a hyper-parameter of the candidate set. With parameters $\\bar{\\{\\phi^{j}\\}}_{j=1}^{L}$ merged using different $\\tau$ in each layer, the regularizer is defined as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{WMR}}=\\mathcal{L}(\\theta_{v},\\theta_{a t t},\\boxed{\\{\\phi^{j}\\}_{j=1}^{L}};\\mathcal{D}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Denote the generated video feature when computing ${\\mathcal{L}}_{\\mathrm{WMR}}$ as ${\\bf z}_{r}$ . We further slightly regularize the consistency between ${\\bf z}_{r}$ and the pooled spatial feature $\\mathbf{e}$ from the CLIP visual encoder: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{MSE}}=\\mathbb{E}_{(V,C)\\sim\\mathcal{D}}[\\left\\lvert\\mathbf{z}_{r}-\\mathbf{e}\\right\\rvert\\right\\rvert_{2}^{2}],\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\lVert\\cdot\\rVert_{2}$ is the L2 norm. This regularizer encourages MoTE to model the temporal dynamics with a light temporal feature, which potentially reduces the model complexity and improves generalization. The overall learning objective can be written as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{\\mathrm{ALL}}=\\mathcal{L}_{\\mathrm{TE}}+\\lambda\\mathcal{L}_{\\mathrm{WMR}}+\\eta\\mathcal{L}_{\\mathrm{MSE}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where we set $\\lambda=0.5$ and $\\eta=0.1$ by default. ", "page_idx": 4}, {"type": "text", "text": "3.4 Temporal Feature Modulation ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "While applying MoTE to downstream recognition tasks, a potential risk is that its semantic space is limited to the category names of the fine-tuning dataset. To demonstrate this, we collect the category names of Kinetics-400, UCF-101, and Kinetics-600 datasets and manually remove duplicate ones. We conduct tests with the mixed category names and observe $2.9\\%$ closed-set performance drops, $29.8\\%$ and $31.1\\%$ zero-shot performance drops on Kinetics-400, UCF-101, and Kinetics-600, respectively. That is, the model tends to classify all videos into fine-tuning categories. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "In light of this, we propose a test-time adaptation strategy for networks where the temporal module is separated from the CLIP encoder, to measure the confidence of temporal features by means of CLIP\u2019s text space and modulates the contribution of temporal features to the prediction. Given the pooled feature $\\mathbf{e}$ from the CLIP visual encoder, we generate fine-tuning and test proxy features $\\mathbf{\\dot{y}}_{f},\\mathbf{y}_{t}\\,\\in\\,\\mathbb{R}^{K\\times D}$ by retrieving the $K$ nearest text features in the fine-tuning and the test dataset categories. We estimate the semantic association $\\rho$ between the two proxy features as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\rho=\\exp(-(1-\\mathcal{M}(\\mathbf{y}_{t}\\mathbf{y}_{f}^{T}))/\\gamma),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\gamma$ stands for a scale hyper-parameter and $\\mathcal{M}(\\cdot)$ is a sequential maximum and average pooling operation. Denote the pooled temporal feature from MoTE as $\\mathbf{t}$ , the final video embedding $\\mathbf{z}$ used for inference becomes $\\mathbf{z}=\\mathbf{e}+\\boldsymbol{\\rho}\\cdot\\mathbf{t}$ . ", "page_idx": 5}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "4.1 Experimental Setup ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Architecture. We employ the CLIP [33] pre-trained ViT-B/16 and ViT-L/14 in our experiments. On top of the visual encoder, we add 6 layers MoTE for ViT-L/14 and 4 layers MoTE for ViT-B/16, with 4 temporal experts per layer by default. ", "page_idx": 5}, {"type": "text", "text": "Implementation Details. We fine-tune our model using the Kinetics-400 [15] dataset as in previous works [28]. During fine-tuning, we sparsely sample $T$ (e.g. 8 or 16) frames as the video input. Each input example is randomly cropped and resized to the size of $224\\times224$ and then undergoes random horizontal filp and random grayscale. We adopt AdamW [25] as the optimizer with a weight decay of 0.2, following a half-period cosine learning rate decay. The initial learning rate is set to $5\\times10^{-5}$ with a total batch size of 144. Furthermore, we set the candidate set $\\beta$ for ${\\mathcal{L}}_{\\mathrm{WMR}}$ to 0.6 and the scale parameter $\\gamma$ for temporal feature modulation to 0.05, and $K$ to 5. We apply temporal feature modulation only in evaluation. Please see supplementary for more details. ", "page_idx": 5}, {"type": "text", "text": "Evaluation Protocols. We thoroughly evaluate our method with close-set, zero-shot, and few-shot video recognition. Close-set: We evaluate the close-set performance on Kinetics-400 [15], using one single clip with a center crop (i.e. $1\\times1$ views) or 4 clips with 3 crops (i.e. $4\\times3$ views) per video [28]. Each view contains 8 or 16 sparsely sampled frames. Zero-shot: Following previous works [28, 34], we evaluate zero-shot performance on UCF-101 [38], HMDB-51 [19], and Kinetics-600 [3]. For K600, we adopt the three splits provided by [4]. Each split contains 160 categories out of 220 new categories. In zero-shot setting, we test using $3\\times1$ views with 8 frames per view. Few-shot: We consider standard K-shot setting and evaluate on UCF-101, HMDB-51, and Something-Something v2 [10]. We adopt a single view for evaluation. ", "page_idx": 5}, {"type": "text", "text": "4.2 Ablation Studies ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Component-wise analysis of MoTE. In Table 1, we perform in-depth ablations of the proposed components with the ViT-L/14 network. We adopt Text4Vis [49] as our baseline, which serves as a prevalent CLIP adaptation framework in the video domain. Text4Vis uses a 6-layer Transformer for temporal modeling, featuring a high degree of specialization but low generalization capability. We observe that adopting the temporal experts boosts zero-shot performance significantly, validating our idea of improving generalization with multiple data bias views. We then introduce ${\\mathcal{L}}_{\\mathrm{WMR}}$ for achieving the coexistence of generalization and specialization, which facilitates a more efficient aggregation of generalized knowledge while achieving the same level of specialization as Text4Vis. Adding $\\mathcal{L}_{\\mathrm{MSE}}$ further improves the zero-shot performance. Moreover, we find that zero-shot performance beneftis from modulating the temporal features for unseen categories during evaluation, especially for HMDB51. In summary, MoTE achieves a sound balance between generalization and specialization. ", "page_idx": 5}, {"type": "text", "text": "Expert-wise performance of MoTE. To better understand the reconciliation effect of MoTE, we show the performance of each expert and the final merged model in Figure 3. In particular, ", "page_idx": 5}, {"type": "table", "img_path": "vpEq2bzsS0/tmp/99c5b5eeff5c5a9a359469fd2e0036e49581f797ad82342eb6d5cf9275ae9cab.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Table 2: Ablation studies on key details. We report close-set accuracy on K400 and zero-shot accuracy on UCF-101 and K600 split1, using the ViT-L/14 network. Default settings are colored in gray. ", "page_idx": 6}, {"type": "table", "img_path": "", "table_caption": ["(a) Effect of initialization and train- (b) Varying numbers of temporal ex- (c) Various types of knowledge aging data across experts. perts in each MoTE layer. gregation. "], "table_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "vpEq2bzsS0/tmp/203b43041700da73de88039e15305b6c1e5832e4dbc9da0edb9393619671a629.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "CLIP-Mean denotes a fine-tuned CLIP model with mean pooling for temporal modeling. We do not apply temporal feature modulation in this study. As shown in the figure, each expert exhibits distinct zero-shot and close-set performances, suggesting that diverse knowledge is learned across experts, as expected in Section 3.2. We find that the zero-shot performance of each dataset presents different sweet spots in terms of the generalization degree. For example, Expert_1 achieves the best results on UCF-101, while Expert_0 performs best on HMDB-51. After weight merging using Equation 6, the merged model yields better zero-shot results than any expert and CLIP-Mean, and the close-set performance slightly outperforms Text4Vis. It demonstrates that our method effectively preserves the generalized and specialized knowledge of every expert, which can be attributed to ${\\mathcal{L}}_{\\mathrm{WMR}}$ . ", "page_idx": 6}, {"type": "text", "text": "Effect of distinct optimization trajectories. In Table 2a, we show that learning diverse knowledge from distinct optimization trajectories is critical for improving generalization. \"Data\" indicates whether the input data is sent to all experts or routed to one expert for encoding. In the first row, the optimization trajectories across experts are identical since all experts share the same initialization and input data. We find that applying different initialization and training data across experts can significantly improve generalization, respectively. When comparing row 3 with row 4, using the routing strategy achieves similar generalization performance while keeping the computation cost low. ", "page_idx": 6}, {"type": "text", "text": "Varying numbers of experts and temporal layers We ablate the number of experts in Table 2b. We observe that the zero-shot performance gains stabilize after a proper number of experts, and 4 temporal experts per layer are sufficient to capture various data bias views. As for the number of temporal layers, we obtain consistent gains with different numbers of layers as shown in Figure 1 (b), demonstrating the scalability of our method. ", "page_idx": 6}, {"type": "text", "text": "Various types of knowledge aggregation. As shown in Table 2c, the inferior performance of random routing indicates the necessity of leveraging diverse knowledge across experts. Compared with ensembling logits, merging weights aggregate knowledge more efficiently with fewer parameters. Meanwhile, the computation cost of ensembling logits is $4\\times$ merging weights. ", "page_idx": 6}, {"type": "text", "text": "Effect of different routing policies. Table 2d ablates different routing policies. The fixed routing policy sends all inputs to a single expert. Our multinomial routing policy achieves the best performance since each expert learns more discrepant knowledge than random routing. ", "page_idx": 6}, {"type": "text", "text": "Weight Merging Regularization. We study different types of the merging regularizer ${\\mathcal{L}}_{\\mathrm{WMR}}$ as well as candidate set parameters $\\beta$ in Table 2e. \"Point\" indicates the regularizer with the average merged parameters (i.e. without region construction). Due to the conflicting nature of generalization and specialization in optimization, the regularizer without region construction cannot adequately aggregate expert knowledge. Instead, constructing the region in the loss landscape enables generalization and specialization to coexist in one model. The parameter $\\beta$ is used to construct the candidate set of the weights merging temperature. As shown in the table, the performance gain is robust to this parameter. ", "page_idx": 6}, {"type": "table", "img_path": "vpEq2bzsS0/tmp/c2b8e4a289bc7a8f4f7d05c846526c68e4289f09b8dc89f876ec3e6c238cec17.jpg", "table_caption": ["Table 3: Zero-shot video recognition performance compared with the state-of-the-art methods on UCF-101, HMDB-51, and Kinetics-600. $\\dagger$ denotes reproduced results with our implementation. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Varying numbers of $\\gamma$ for Temporal Feature Modulation. The parameter $\\gamma$ is used to scale the semantic association to an appropriate interval. In general, $\\gamma=0.05$ yields better results. We find that different datasets exhibit varying sensitivities to this parameter and a relatively small contribution of temporal features generalizes to the unseen categories. Please see supplementary for more ablations. ", "page_idx": 7}, {"type": "text", "text": "4.3 Main Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Zero-shot video recognition. In Table 3, we compare our method with the state-of-the-art results under the zero-shot setting. Our method achieves new state-of-the-art results on UCF-101 and HMDB51, with only 8 frames as input. The remarkable performance can be scaled up as the network size and the input frame number increase, demonstrating the great scalability of our method. For K600, although significant improvements are achieved over the baseline, our method still underperforms some methods that do not apply any additional parameters such as Open-VCLIP [46] and MAXI [23]. Since the categories of Kinetics-600 are much more complex, we argue that it is still challenging for randomly initialized parameters to generalize well on such complex unseen categories. Note that our method still outperforms any method that employs additional parameters on K600. Overall, our method presents superior generalization performance. ", "page_idx": 7}, {"type": "text", "text": "Close-set and zero-shot performance trade-off. Table 4 presents comparisons with the state-ofthe-art methods under the close-set setting. We also list the harmonic mean of the zero-shot results on UCF, HMDB, and K600 as an indicator of the generalization capability, denoted as $\\mathrm{HM}_{\\mathrm{ZS}}$ . To evaluate the holistic performance in close-set and zero-shot settings, we define the \"Trade-off\" score as the harmonic mean of Top- $1_{\\mathrm{K400}}$ and $\\mathrm{HM_{ZS}}$ , as we consider the specialization and generalization capabilities to be equally important for a balanced model. ", "page_idx": 7}, {"type": "text", "text": "As shown in the table, our method presents strong close-set results competitive with SOTAs and state-of-the-art zero-shot performance. More importantly, while existing methods can perform well on only one task setting, we achieve superior performance on both settings simultaneously with one unified model. Our method consistently exhibits significant trade-off performance advantages across different networks, evidencing the effectiveness and scalability of MoTE in reconciling generalization and specialization capabilities. Even against ViFi-CLIP [34] which uses different training hyperparameters for close-set and zero-shot settings and takes more frames as the input, our method still shows better balanced performance $74.7\\%$ vs. $72.9\\%$ ). Noteworthy that FROSTER [12] is a strong zero-shot action recognition model but performs less well in the close-set setting, which can be attributed to the the application of the weights ensemble and the distillation supervision from the Frozen CLIP limits the model\u2019s ability to learn video-specialized knowledge. ", "page_idx": 7}, {"type": "table", "img_path": "vpEq2bzsS0/tmp/ac8543174ebf8b661f9faf023b064d590e1418e6edba4b9d003a5adf0400544f.jpg", "table_caption": ["Table 4: Close-set and zero-shot performance trade-off compared with the state-of-the-art methods. We report close-set results on K400. $\"\\mathrm{HM}_{\\mathrm{ZS}}'$ \" indicates the harmonic mean of zero-shot results on UCF, HMDB, and K600. \"Trade-off\" is defined as the harmonic mean of Top- $1_{\\mathrm{K400}}$ and $\\mathrm{HM_{ZS}}$ . \"Unified model\" indicates whether the method is evaluated using the same model in both settings. "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "vpEq2bzsS0/tmp/4f962ac1a1713eff79c7b65a0fdfae1896bc0f5f536711efb58f52b17c803f16.jpg", "table_caption": ["Table 5: Few-shot results compared with the state-of-the-art methods on HMDB, UCF, and SSv2. All methods directly fine-tune on CLIP, except MAXI (Tuning after pre-training on K400.). "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Few-shot video recognition. We perform all-way few-shot video recognition in Table 5, which requires both specialization and generalization to rapidly adapt to a novel set of categories with limited samples. Our method presents consistent improvements across sample shots in all datasets, demonstrating the strong learning capacity and transferability. Our method even outperforms MAXI [23] which uses K400 fine-tuning weights as initialization and more diverse textual knowledge. ", "page_idx": 8}, {"type": "text", "text": "4.4 Category-wise performance visualization. ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "To better understand the specific video categories that each expert excels at recognizing, we visualize the Top-1 classification accuracy for categories sampled from the UCF-101 dataset in Figure 4. The experiment is conducted in the zero-shot setting. We observe that different experts show distinct performance across video categories, likely due to the diverse generalization knowledge learned by each expert. The merged expert benefits from the aggregation of expert knowledge, particularly for categories where temporal information is crucial, such as \u2019Handstand Pushups\u2019 and \u2019Wall Pushups\u2019. This suggests that each expert can capture various temporal patterns within the same category. ", "page_idx": 8}, {"type": "image", "img_path": "vpEq2bzsS0/tmp/90f5f54c33ea54fd0d8b6a5c612e029a899a86775f6db23091486891cb7596e1.jpg", "img_caption": ["Figure 4: Visualization of the Top-1 accuracy for each video category sampled from UCF-101 with respect to the merged expert and each individual expert. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "5 Related Work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Video recognition. In the era of deep learning, early works explored diverse variants of convolutional networks for joint spatiotemporal modeling, such as 3D convolution [2, 39] and factorized spatial and temporal convolution [40]. The advent of Transformer then attracted the attention of researchers and has been widely applied for video recognition [1, 6]. In addition, self-supervised learning methods learn transferable representations from large-scale unlabeled data and achieve promising performances [7, 31, 57]. ", "page_idx": 9}, {"type": "text", "text": "Transferring VLMs for videos. Transferring VLMs for video recognition task has been proven to be effective. ViFi-CLIP [34] suggests that a direct fine-tuning process generalizes well on various settings. Open-VCLIP [46] constructs an open-vocabulary video model by interpolating the model weights and its optimization trajectory. Vita-CLIP [45] extracts discriminative information using multi-level prompts. X-CLIP [28] proposes cross-frame attention and multi-frame integration modules for temporal modeling. Text4Vis [49] and BIKE [50] explore the way for more effective knowledge transfer and use multi-transformer layers to capture temporal cues. FROSTER [12] mitigates catastrophic forgetting by ensuring the learned features do not diverge too far from the frozen ones through distillation. This divergence comes from the variations of both CLIP and additional trainable parameters. Differently, we believe the main cause of the forgetting problem is the overfitting of additional parameters regardless of whether the CLIP parameters are tuned. Note that FROSTER can also potentially improve the generalization of additional parameters through knowledge distillation, but our method presents a more explicit way to achieve this goal. Existing methods face a trade-off of introducing more specialized knowledge or preserving more generalized knowledge. Our work addresses this challenge and presents superior results on both close-set and zero-shot settings. ", "page_idx": 9}, {"type": "text", "text": "Mixture-of-Experts. The sparsely activated MoE structure [37] enables the model capacity (i.e. number of parameters) to be vastly scaled up while keeping the computation cost per sample basically unchanged. This technique has been widely investigated in building large-scale pre-trained models in various fields [20, 26, 35]. Several works employ MoE on large-scale language models for parameterefficient tuning to improve their specialized performance [8, 44]. In this paper, we demonstrate its effectiveness in balancing generalization and specialization in VLM knowledge transfer. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we present MoTE, an effective Visual-Language to video knowledge transfer framework that enjoys both superior generalization and specialization. MoTE leverages a mixture of temporal experts to enhance performance in both close-set and zero-shot video recognition, all while maintaining the conventional temporal module\u2019s structure and computational efficiency. With the proposed weight merging regularization and temporal feature modulation, we achieve the coexistence of generalization and specialization in one unified model. Extensive experiments validate MoTE\u2019s ability to strike an optimal trade-off between close-set and zero-shot performance. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This paper is supported by the National Natural Science Foundation of China under Grants (62073245, 62173248, 62233013), Shanghai Municipal Science and Technology Major Project (2021SHZDZX0100) and Innovation Action Plan (22511104900), the Fundamental Research Funds for the Central Universities. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Luc\u02c7ic\u00b4, and Cordelia Schmid. Vivit: A video vision transformer. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 6836\u20136846, 2021.   \n[2] Joao Carreira and Andrew Zisserman. Quo vadis, action recognition? a new model and the kinetics dataset. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 6299\u20136308, 2017.   \n[3] Joao Carreira, Eric Noland, Andras Banki-Horvath, Chloe Hillier, and Andrew Zisserman. A short note about kinetics-600. arXiv preprint arXiv:1808.01340, 2018.   \n[4] Shizhe Chen and Dong Huang. Elaborative rehearsal for zero-shot action recognition. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2021.   \n[5] Nicola De Cao, Wilker Aziz, and Ivan Titov. Editing factual knowledge in language models. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6491\u20136506, 2021.   \n[6] Haoqi Fan, Bo Xiong, Karttikeya Mangalam, Yanghao Li, Zhicheng Yan, Jitendra Malik, and Christoph Feichtenhofer. Multiscale vision transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 6824\u20136835, 2021.   \n[7] Christoph Feichtenhofer, Haoqi Fan, Bo Xiong, Ross Girshick, and Kaiming He. A large-scale study on unsupervised spatiotemporal representation learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 3299\u20133309, 2021.   \n[8] Ze-Feng Gao, Peiyu Liu, Wayne Xin Zhao, Zhong-Yi Lu, and Ji-Rong Wen. Parameter-efficient mixture-ofexperts architecture for pre-trained language models. In Proceedings of the 29th International Conference on Computational Linguistics, pages 3263\u20133273, 2022.   \n[9] Mor Geva, Roei Schuster, Jonathan Berant, and Omer Levy. Transformer feed-forward layers are key-value memories. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), 2021.   \n[10] Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski, Joanna Materzynska, Susanne Westphal, Heuna Kim, Valentin Haenel, Ingo Fruend, Peter Yianilos, Moritz Mueller-Freitag, et al. The\" something something\" video database for learning and evaluating visual common sense. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2017.   \n[11] Xiuye Gu, Tsung-Yi Lin, Weicheng Kuo, and Yin Cui. Open-vocabulary object detection via vision and language knowledge distillation. In International Conference on Learning Representations (ICLR), 2022.   \n[12] Xiaohu Huang, Hao Zhou, Kun Yao, and Kai Han. Froster: Frozen clip is a strong teacher for openvocabulary action recognition. In International Conference on Learning Representations (ICLR), 2024.   \n[13] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In International Conference on Machine Learning (ICML), pages 4904\u20134916, 2021.   \n[14] Chen Ju, Tengda Han, Kunhao Zheng, Ya Zhang, and Weidi Xie. Prompting visual-language models for efficient video understanding. In Proceedings of the European Conference on Computer Vision (ECCV), pages 105\u2013124, 2022.   \n[15] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, et al. The kinetics human action video dataset. arXiv preprint arXiv:1705.06950, 2017.   \n[16] Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, and Tie-Yan Liu. Lightgbm: A highly efficient gradient boosting decision tree. In Advances in Neural Information Processing Systems (NeurIPS), 2017.   \n[17] Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang. On large-batch training for deep learning: Generalization gap and sharp minima. In International Conference on Learning Representations (ICLR), 2016.   \n[18] Simon Kornblith, Ting Chen, Honglak Lee, and Mohammad Norouzi. Why do better loss functions lead to less transferable features? In Advances in Neural Information Processing Systems (NeurIPS), 2021.   \n[19] H. Kuehne, H. Jhuang, E. Garrote, T. Poggio, and T. Serre. Hmdb: A large video database for human motion recognition. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 2556\u20132563, 2011.   \n[20] Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with conditional computation and automatic sharding. arXiv preprint arXiv:2006.16668, 2020.   \n[21] Boyi Li, Kilian Q Weinberger, Serge Belongie, Vladlen Koltun, and Rene Ranftl. Language-driven semantic segmentation. In International Conference on Learning Representations (ICLR), 2021.   \n[22] Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein. Visualizing the loss landscape of neural nets. In Advances in Neural Information Processing Systems (NeurIPS), volume 31, 2018.   \n[23] Wei Lin, Leonid Karlinsky, Nina Shvetsova, Horst Possegger, Mateusz Kozinski, Rameswar Panda, Rogerio Feris, Hilde Kuehne, and Horst Bischof. Match, expand and improve: Unsupervised finetuning for zero-shot action recognition with language knowledge. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2023.   \n[24] Xiao Lin, Minghao Zhu, Ronghao Dang, Guangliang Zhou, Shaolong Shu, Feng Lin, Chengju Liu, and Qijun Chen. Clipose: Category-level object pose estimation with pre-trained vision-language knowledge. IEEE Transactions on Circuits and Systems for Video Technology, 2024.   \n[25] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017.   \n[26] Basil Mustafa, Carlos Riquelme, Joan Puigcerver, Rodolphe Jenatton, and Neil Houlsby. Multimodal contrastive learning with limoe: the language-image mixture of experts. In Advances in Neural Information Processing Systems (NeurIPS), pages 9564\u20139576, 2022.   \n[27] Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nati Srebro. Exploring generalization in deep learning. In Advances in Neural Information Processing Systems (NeurIPS), 2017.   \n[28] Bolin Ni, Houwen Peng, Minghao Chen, Songyang Zhang, Gaofeng Meng, Jianlong Fu, Shiming Xiang, and Haibin Ling. Expanding language-image pretrained models for general video recognition. In Proceedings of the European Conference on Computer Vision (ECCV), pages 1\u201318, 2022.   \n[29] Changdae Oh, Mijoo Kim, Hyesu Lim, Junhyeok Park, Euiseog Jeong, Zhi-Qi Cheng, and Kyungwoo Song. Towards calibrated robust fine-tuning of vision-language models. arXiv preprint arXiv:2311.01723, 2023.   \n[30] Junting Pan, Ziyi Lin, Xiatian Zhu, Jing Shao, and Hongsheng Li. St-adapter: Parameter-efficient imageto-video transfer learning. In Advances in Neural Information Processing Systems (NeurIPS), pages 26462\u201326477, 2022.   \n[31] Rui Qian, Tianjian Meng, Boqing Gong, Ming-Hsuan Yang, Huisheng Wang, Serge Belongie, and Yin Cui. Spatiotemporal contrastive video representation learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021.   \n[32] Zhiwu Qing, Shiwei Zhang, Ziyuan Huang, Yingya Zhang, Changxin Gao, Deli Zhao, and Nong Sang. Disentangling spatial and temporal learning for efficient image-to-video transfer learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2023.   \n[33] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning (ICML), pages 8748\u20138763, 2021.   \n[34] Hanoona Rasheed, Muhammad Uzair Khattak, Muhammad Maaz, Salman Khan, and Fahad Shahbaz Khan. Fine-tuned clip models are efficient video learners. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 6545\u20136554, 2023.   \n[35] Carlos Riquelme, Joan Puigcerver, Basil Mustafa, Maxim Neumann, Rodolphe Jenatton, Andr\u00e9 Susano Pinto, Daniel Keysers, and Neil Houlsby. Scaling vision with sparse mixture of experts. In Advances in Neural Information Processing Systems (NeurIPS), pages 8583\u20138595, 2021.   \n[36] Mert Bulent Sariyildiz, Yannis Kalantidis, Karteek Alahari, and Diane Larlus. No reason for no supervision: Improved generalization in supervised models. In International Conference on Learning Representations (ICLR), 2022.   \n[37] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017.   \n[38] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. Ucf101: A dataset of 101 human actions classes from videos in the wild. arXiv preprint arXiv:1212.0402, 2012.   \n[39] Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani, and Manohar Paluri. Learning spatiotemporal features with 3d convolutional networks. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 4489\u20134497, 2015.   \n[40] Du Tran, Heng Wang, Lorenzo Torresani, Jamie Ray, Yann LeCun, and Manohar Paluri. A closer look at spatiotemporal convolutions for action recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 6450\u20136459, 2018.   \n[41] Shuyuan Tu, Qi Dai, Zuxuan Wu, Zhi-Qi Cheng, Han Hu, and Yu-Gang Jiang. Implicit temporal modeling with learnable alignment for video recognition. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2023.   \n[42] Mengmeng Wang, Jiazheng Xing, and Yong Liu. Actionclip: A new paradigm for video action recognition. arXiv preprint arXiv:2109.08472, 2021.   \n[43] Qiang Wang, Junlong Du, Ke Yan, and Shouhong Ding. Seeing in flowing: Adapting clip for action recognition with motion prompts learning. In Proceedings of the 31st ACM International Conference on Multimedia, pages 5339\u20135347, 2023.   \n[44] Yaqing Wang, Sahaj Agarwal, Subhabrata Mukherjee, Xiaodong Liu, Jing Gao, Ahmed Hassan, and Jianfeng Gao. Adamix: Mixture-of-adaptations for parameter-efficient model tuning. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 5744\u20135760, 2022.   \n[45] Syed Talal Wasim, Muzammal Naseer, Salman Khan, Fahad Shahbaz Khan, and Mubarak Shah. Vita-clip: Video and text adaptive clip via multimodal prompting. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 23034\u201323044, 2023.   \n[46] Zejia Weng, Xitong Yang, Ang Li, Zuxuan Wu, and Yu-Gang Jiang. Open-vclip: Transforming clip to an open-vocabulary video model via interpolated weight optimization. In International Conference on Machine Learning (ICML), pages 36978\u201336989, 2023.   \n[47] Mitchell Wortsman, Gabriel Ilharco, Samir Ya Gadre, Rebecca Roelofs, Raphael Gontijo-Lopes, Ari S Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, et al. Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time. In International Conference on Machine Learning (ICML), pages 23965\u201323998, 2022.   \n[48] Lei Wu, Zhanxing Zhu, et al. Towards understanding generalization of deep learning: Perspective of loss landscapes. arXiv preprint arXiv:1706.10239, 2017.   \n[49] Wenhao Wu, Zhun Sun, and Wanli Ouyang. Revisiting classifier: Transferring vision-language models for video recognition. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 2847\u20132855, 2023.   \n[50] Wenhao Wu, Xiaohan Wang, Haipeng Luo, Jingdong Wang, Yi Yang, and Wanli Ouyang. Bidirectional cross-modal knowledge exploration for video recognition with pre-trained vision-language models. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 6620\u2013 6630, 2023.   \n[51] Xiaoshi Wu, Feng Zhu, Rui Zhao, and Hongsheng Li. Cora: Adapting clip for open-vocabulary detection with region prompting and anchor pre-matching. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 7031\u20137040, 2023.   \n[52] Jiarui Xu, Shalini De Mello, Sifei Liu, Wonmin Byeon, Thomas Breuel, Jan Kautz, and Xiaolong Wang. Groupvit: Semantic segmentation emerges from text supervision. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 18134\u201318144, 2022.   \n[53] Taojiannan Yang, Yi Zhu, Yusheng Xie, Aston Zhang, Chen Chen, and Mu Li. Aim: Adapting image models for efficient video action recognition. In International Conference on Learning Representations (ICLR), 2022.   \n[54] Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella, Xiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong Huang, Boxin Li, Chunyuan Li, et al. Florence: A new foundation model for computer vision. arXiv preprint arXiv:2111.11432, 2021.   \n[55] Renrui Zhang, Rongyao Fang, Wei Zhang, Peng Gao, Kunchang Li, Jifeng Dai, Yu Qiao, and Hongsheng Li. Tip-adapter: Training-free clip-adapter for better vision-language modeling. In Proceedings of the European Conference on Computer Vision (ECCV), 2022.   \n[56] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Learning to prompt for vision-language models. International Journal of Computer Vision (IJCV), 130(9):2337\u20132348, 2022.   \n[57] Minghao Zhu, Xiao Lin, Ronghao Dang, Chengju Liu, and Qijun Chen. Fine-grained spatiotemporal motion alignment for contrastive video representation learning. In Proceedings of the 31st ACM International Conference on Multimedia, pages 4725\u20134736, 2023.   \n[58] Yan Zhu, Junbao Zhuo, Bin Ma, Jiajia Geng, Xiaoming Wei, Xiaolin Wei, and Shuhui Wang. Orthogonal temporal interpolation for zero-shot video recognition. In Proceedings of the 31st ACM International Conference on Multimedia, pages 7491\u20137501, 2023.   \n[59] Simiao Zuo, Xiaodong Liu, Jian Jiao, Young Jin Kim, Hany Hassan, Ruofei Zhang, Jianfeng Gao, and Tuo Zhao. Taming sparsely activated transformer with stochastic experts. In International Conference on Learning Representations (ICLR), 2021. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Supplemental material ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A Limitation and Broader Impact ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Limitation and future work While our method yields superior results in various settings of video recognition, there are still several aspects for improvement. First, the text space of MoTE is limited to video category names, which do not provide discriminative and semantically rich textual representations. Thus, exploring how to extend the semantic space with large-scale generative models may further enhance the performance of MoTE. Besides, additional parameters take various forms when adapting VLM to downstream tasks. In future work, we will further extend our approach to other forms of parameters to explore the generality and versatility of our method. ", "page_idx": 14}, {"type": "text", "text": "Broader Impact Adapting foundation models to downstream tasks has become a dominant paradigm in machine learning [24, 30], we believe that it is worthwhile and important to explore how to preserve the existing knowledge of the foundation model when introducing new knowledge. We hope this work brings new insights for the broader and long-term adaptation of foundation models. Besides, our work focuses on video recognition tasks, which have a wide range of real-world application scenarios, such as surveillance. However, this work requires careful elimination of privacy and rights concerns before real-world deployments. ", "page_idx": 14}, {"type": "table", "img_path": "vpEq2bzsS0/tmp/29086ba0aac7385f1dc7c0e924dcde578da0f829c9febe4dc80994a1e7308845.jpg", "table_caption": ["Table 6: Hyper-parameter details during fine-tuning. "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "B More Implementation Details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Close-set and zero-shot video recognition. In Table 6, we present the hyper-parameters set for optimization. Note that we train one model for both close-set and zero-shot tasks, rather than training for each task separately. The parameters of all projection matrices in MoTE are initialized with values drawn from the normal distribution (mean $\\mathbf{=}0$ , std $1{=}0.02$ ). All bias terms are initialized as zero. We conduct experiments with 3 NVIDIA GeForce RTX 4090. ", "page_idx": 14}, {"type": "text", "text": "For zero-shot evaluation, the methods are evaluated on three official splits of UCF-101 and HMDB-51. For Kinetics-600, we adopt the three splits provided by [4]. Each split contains 160 categories out of 220 new categories that do not exist in K400. We report the average Top-1 accuracy and the standard deviation on three splits. ", "page_idx": 14}, {"type": "text", "text": "Few-shot video recognition. We consider the standard K-shot setting [28, 34], where 2, 4, 8, and 16 video data are randomly sampled for each category for constructing the training dataset. Following previous work [49], we repeat the constructed training dataset to maintain the same size as the original full dataset. In few-shot settings, we fine-tune models for 2 epochs on SSv2 and 10 epochs on UCF-101 and HMDB-51. We adopt a single view for evaluation. ", "page_idx": 14}, {"type": "image", "img_path": "vpEq2bzsS0/tmp/03d2b3626e7d466201b59f693d1185dda54fcc30f67b564d92961a06cd843ab5.jpg", "img_caption": ["Figure 5: Illustration of optional architecture designs for the temporal expert. We omit the activation function between the projection matrices for brevity. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "Table 7: Additional ablation studies. Default settings are colored in gray. ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "(a) Optional designs for the tempo- (b) Varying neighbor numbers $K$ for (c) Different types of Weight Mergral expert. the temporal feature modulation. ing Regularization. ", "page_idx": 15}, {"type": "table", "img_path": "vpEq2bzsS0/tmp/c2d34e7a81246cf77904d8450864ffa50eee3f2798a9959cb6610836d0d837c7.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "(d) Ablation study on the training (e) Results of fine-tuning on UCF (f) Effect of temperature selection costs of MoTE. and zero-shot evaluating on K400. schemes. ", "page_idx": 15}, {"type": "text", "text": "Method GPU-days GFLOPs Method UCF K400 Type K400 K600   \nBaseline 4.35 649 CLIP 80.9 59.2 Discrete set 86.8 78.9   \n$+$ Temporal Experts 4.35 $+0.34$ Baseline 95.6 56.6 Continuous normal dist. 86.5 77.4   \n+ LWMR 4.50 +0.34 MoTE 96.1 66.3 Continuous uniform dist. 86.0 77.9   \n+ LMSE 4.51 ", "page_idx": 15}, {"type": "text", "text": "C Additional Ablations ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Optional architecture of temporal expert. By default, we replace the whole FFN with experts of the same structure as the FFN. Optionally, we can replace one or both of the projection matrices in the FFN with experts of the same structure, as illustrated in Figure 5. We study the optional architecture design of the temporal expert in Table 7a. As shown in the table, we observe degraded performance with projection-level expert designs. We argue that the shared projection matrix or two-stage routing design of the projection-level expert allows knowledge communication between experts, such implicit communication may lead to mutual ftiting between the experts. Therefore, preventing the knowledge exchange across experts is critical for learning diverse generalized knowledge. ", "page_idx": 15}, {"type": "text", "text": "Varying neighbor numbers $K$ for Temporal Feature Modulation. In Section 3.4 of the main manuscript, we generate fine-tuning proxy features $\\mathbf{y}_{f}$ and test proxy features $\\mathbf{y}_{t}$ by retrieving the $K$ nearest text features in the fine-tuning and the test dataset categories. We study the influence of various $K$ in Table 7b. We find that a large $K$ leads to a slight performance degradation as the retrieved text features may not represent the semantic information of the video properly. Overall, our performance gains are stable for this parameter. ", "page_idx": 15}, {"type": "text", "text": "Various Loss Types of Weight Merging Regularization. In Section 3.3 of the main manuscript, we design the weight merging regularization ${\\mathcal{L}}_{\\mathrm{WMR}}$ based on the cross-entropy loss function. In this study, we use other loss function types to redesign the weight merging regularization ${\\mathcal{L}}_{\\mathrm{WMR}}$ . Note that our training procedure requires two forward passes of the temporal module, one when optimizing the activated expert with $\\mathcal{L}_{\\mathrm{TE}}$ and the second in calculating weight merging regularization ${\\mathcal{L}}_{\\mathrm{WMR}}$ . When we optimize ${\\mathcal{L}}_{\\mathrm{WMR}}$ using cross entropy, we use the ground truth label of the video as the supervised signal. We try to redesign ${\\mathcal{L}}_{\\mathrm{WMR}}$ with other supervisions. (i) KL Divergence: We adopt the logits generated during the computation of $\\mathcal{L}_{\\mathrm{TE}}$ as the supervision and optimize with KL divergence loss, denoted as $\\mathcal{L}_{\\mathrm{WMR\\_KL}}$ . (ii) Mean Square Error: We use the output feature when computing $\\mathcal{L}_{\\mathrm{TE}}$ as the supervision and optimizing with MSE loss, denoted as $\\mathcal{L}_{\\mathrm{WMR\\_MSE}}$ . The results are presented in Table 7c. Interestingly, we find that the MSE loss achieves notable results in a weakly supervised manner, which indicates the potential scalability of our proposed approach. ", "page_idx": 15}, {"type": "image", "img_path": "vpEq2bzsS0/tmp/4bd3d6d205aa70f1435182744df2950b631339645799a7172e65a10669a6c31e.jpg", "img_caption": ["Figure 6: Visualization of attention maps. We show the RGB image and the attention maps of the merged expert, expert_0, and expert_4. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "Training cost analysis of MoTE. We report the actual training time of our method with respect to the baseline in Table 7d. The wall-clock time of training is benchmarked on 3 4090 GPUs with a batch size of 144. GPU days are calculated by the number of GPUs multiplied by the training time in days. As shown in the table, applying the mixture of temporal experts does not introduce additional training overhead over baseline, which can be attributed to the use of the routing strategy. Adding ${\\mathcal{L}}_{\\mathrm{WMR}}$ and $\\mathcal{L}_{\\mathrm{MSE}}$ brings a $+0.16$ days training time increase since it requires an additional forward pass of the temporal module. ", "page_idx": 16}, {"type": "text", "text": "Transferring CLIP with small-scale fine-tuning data. In the main text, our experimental setting follows previous works in fine-tuning on large-scale K400 and then evaluating zero-shot performance on relatively small downstream datasets. To further investigate MoTE\u2019s capabilities when the finetuning data are limited, we train the ViT-L/14 network on UCF-101 and evaluate it on K400, results are shown in Table 7e. MoTE yields a 0.5 improvement over the Baseline on UCF-101, while zeroshot performance on K400 significantly outperforms both raw CLIP and Baseline. This demonstrates the applicability of MoTE to small-scale datasets and its ability to learn generalized knowledge from limited data. ", "page_idx": 16}, {"type": "text", "text": "Effect of temperature selection schemes. In this study, we compare our discrete temperature sampling strategy with the continuous space selection schemes. We implement two continuous space selection schemes. (1) Sampling from a continuous standard normal distribution (mean $\\mathbf{=}0$ , variance $^{-1}$ ). (2) Sampling from a continuous uniform distribution. As shown in Table 7f, the continuous space sampling strategy results in a notable performance degradation. ", "page_idx": 16}, {"type": "text", "text": "D Textual Prompt Templates ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In our work, we adopt a set of hand-craft textual prompt templates to generate text embeddings. Following CLIP [33], we perform prompt ensembling over the 28 templates in order to provide comprehensive semantics. The templates are listed in Table 8. ", "page_idx": 16}, {"type": "table", "img_path": "vpEq2bzsS0/tmp/a06818a375cce4a30ec2e7fbf50b625b6b446132e9ebdc4fd2bc33aaf3875b00.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "E Qualitative Analysis ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "E.1 Model Attention ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "To better understand what knowledge the experts capture, we present the visualization of the model attention in Figure 6. We show the attention map of the merged expert, expert_0, and expert_4 to explore whether diverse knowledge is learned across experts. Since MoTE only requires the input of frame-level tokens from the CLIP visual encoder, we calculate the attention map between the temporal video features output by MoTE and the image patch tokens from the CLIP encoder. As shown in the figure, experts_0 and experts_4 always focus on different regions, indicating that they capture various temporal patterns. Besides, we observe that the merged expert is able to focus on more precise and broader foreground areas. This phenomenon suggests that the merged expert sufficiently aggregates and leverages the knowledge from different experts. ", "page_idx": 17}, {"type": "text", "text": "E.2 Representation similarity across experts. ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We visualize the representation similarities across each expert and the final merged model in Figure 7. The representation similarities are averaged on 100 randomly sampled data of unseen categories from the K600 dataset. As show in the affinity map, the different similarities demonstrate that each expert learns distinctive knowledge from different optimization trajectories. Besides, we observe that the similarities between the merged model and each expert are relatively stable, indicating that the merged expert efficiently leverages the knowledge contained in each expert. ", "page_idx": 17}, {"type": "image", "img_path": "vpEq2bzsS0/tmp/0900ef31d65368bf91cfb99851483725c6810a60b2275b0a5672d7781fa4631b.jpg", "img_caption": ["Figure 7: Visualization of representation similarities across each expert and the final merged model. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "F Dataset Details ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Kinetics-400 [15] is a large-scale dataset in the video domain. The dataset contains ${\\sim}240\\mathrm{k}$ training videos and ${\\sim}20\\mathbf{k}$ validation videos in 400 human action categories, with an average length of 10 seconds. The high quality of the dataset makes it the most popular benchmark for video recognition ", "page_idx": 18}, {"type": "text", "text": "Kinetics-600 [3] is an extension of Kinetics-400, consisting of ${\\sim}392\\mathrm{k}$ training videos, ${\\sim}30\\mathrm{k}$ validation videos, and ${\\sim}60\\mathrm{k}$ test videos in 600 human action categories. The dataset contains an additional 220 new action categories over Kinetics-400. We evaluate the zero-shot performance on 220 new categories and adopt three splits provided by the previous work [4]. We use its test set for evaluation and report the average performance on three splits. ", "page_idx": 18}, {"type": "text", "text": "UCF-101 [38] is an action recognition dataset that contains 13,320 videos in 101 action categories, collected from YouTube. There are three official splits of training data and validation data. ", "page_idx": 18}, {"type": "text", "text": "HMDB-51 [19] contains $7\\mathbf{k}$ videos in 51 action categories, collected from movie clips and web videos. There are three official splits of the dataset, each with 3,570 training data and 1,530 validation data. is a collection of realistic videos from various sources, including movies and web videos. The dataset comprises 7,000 video clips from 51 action categories. ", "page_idx": 18}, {"type": "text", "text": "Somethin-Something V2 [10] is a temporal-heavy dataset that requires the fine-grained temporal understanding capability of the model. It contains 220,000 videos in 174 action categories. ", "page_idx": 18}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: Our main contributions are detailed in lines 6-14 of the abstract and lines 67-77 of the introduction. ", "page_idx": 19}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: We analyze the limitations of this paper in Section A of the Appendix ", "page_idx": 19}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 19}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 19}, {"type": "text", "text": "Justification: As an applied paper, we aim to provide a more generalized and effective VLMs knowledge transfer framework for video recognition tasks. We do not include theoretical results in this paper. Note that all assumptions and conclusions mentioned in this paper are precisely cited. ", "page_idx": 19}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: We detail the implementation details of the paper in Section 4.1 of the main manuscript and Section B of the Appendix. Besides, we provide our code in the supplementary zip file. The code will also be released when the paper is accepted. ", "page_idx": 19}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: We provide our code in the supplementary zip file with a detailed instruction to reproduce our method. The code will also be released when the paper is accepted. ", "page_idx": 19}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: We specify all training and evaluation details of our method in Section 4.1 of the main manuscript and Section B of the Appendix. We also provide statistics details for the dataset used in this paper in Section $\\mathrm{F}$ of the Appendix. ", "page_idx": 19}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] , ", "page_idx": 19}, {"type": "text", "text": "Justification: For zero-shot experiments, following the previous works, we evaluate our method on three splits of UCF-101, HMDB-51, and K600. We report the mean accuracy and the standard deviation of performance of the three splits on each dataset, respectively. As for close-set and few-shot experiments, considering the computational effort, we do not include error bars following previous works. ", "page_idx": 20}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We report computational resources and training costs in Section B and Section $\\mathrm{C}$ of the Appendix. ", "page_idx": 20}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: : All authors have read the NeurIPS Code of Ethics. ", "page_idx": 20}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We discuss the potential broader impacts in Section A of the Appendix. ", "page_idx": 20}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: This paper poses no such risks. ", "page_idx": 20}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: All references and datasets in this paper are precisely cited and licensed with corresponding licenses. ", "page_idx": 20}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We provide detailed instructions on submitted assets. The details of the dataset and model are reported in our paper. ", "page_idx": 20}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] Justification: This paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 20}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 20}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] Justification: This paper poses no such risks. ", "page_idx": 21}]