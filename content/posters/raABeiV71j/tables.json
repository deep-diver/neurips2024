[{"figure_path": "raABeiV71j/tables/tables_6_1.jpg", "caption": "Table 1: Explanation of key-budget and dimensionality (Dim.) for different approaches, along with the expected speedup and memory savings.", "description": "This table compares three different sparse attention methods: Exact Top-K, H2O, and Loki.  For each method, it specifies the budget (fraction of keys or keys & values used), the dimensionality of the keys used for attention score computation, a description of the method, the expected speedup compared to full attention, and any memory savings.  Exact Top-K computes exact attention scores for a subset of keys; H2O uses a heuristic to select keys & values, and Loki uses low-dimensional projections of the keys to select keys & values for efficient attention.", "section": "6.1 Comparison with Full Attention"}, {"figure_path": "raABeiV71j/tables/tables_7_1.jpg", "caption": "Table 2: Perplexity evaluation of Loki and other approaches for different models (lower is better).", "description": "This table compares the perplexity scores achieved by four different methods: Full Attention (baseline), Exact-TopK (a sparse attention method), H2O (another sparse attention method), and Loki (the proposed method).  The perplexity is a measure of how well the model predicts a sequence of tokens. Lower perplexity indicates better performance. The table shows the perplexity for four different large language models (LLMs): Llama2-7B, Llama2-13B, Llama3-8B, and Mistral-7B.  The 'kf' column indicates the fraction of keys kept in the key-value cache for the sparse attention methods. The 'df' column shows the fraction of the full dimensionality used for the keys in the Loki method. The 'Speedup' column indicates whether the method uses speedup techniques.  The results demonstrate the relative performance of Loki compared to other sparse attention techniques, showing that Loki achieves comparable performance to the baseline method while incorporating speedup techniques.", "section": "6.2 Comparison with Other Sparse Attention Methods"}, {"figure_path": "raABeiV71j/tables/tables_18_1.jpg", "caption": "Table 3: Performance of different models compared to hugging face baseline with different configurations of k and d using pre-rotary PCA transformation.", "description": "This table presents the performance comparison of various LLMs (Llama2-7B, Llama2-13B, Llama2-70B, Llama3-8B, Llama3-70B, TinyLlama-1.1B, Mistral-7B, Mixtral-8x7B) using different configurations of the Loki model against the Hugging Face baseline.  The metrics include perplexity (PPL), Hellaswag, TQA, Winogrande, ARC, GSM8K, and MMLU.  Different values of k (fraction of keys selected) and d (reduced dimensionality) are explored to analyze their impact on performance.", "section": "B.1 Performance of Loki on Perplexity and Short-Context Downstream Tasks"}, {"figure_path": "raABeiV71j/tables/tables_19_1.jpg", "caption": "Table 3: Performance of different models compared to hugging face baseline with different configurations of k and d using pre-rotary PCA transformation.", "description": "This table compares the performance of several large language models (LLMs) using the Loki method with different settings of hyperparameters (k and d) against the Hugging Face baseline.  The models are evaluated on a range of downstream tasks, including perplexity, Hellaswag, TQA, Winogrande, ARC, GSM8K, and MMLU.  The table shows the performance differences for various combinations of the hyperparameters, allowing for an analysis of how these parameter choices affect model performance across multiple evaluation metrics.", "section": "B.1 Performance of Loki on Perplexity and Short-Context Downstream Tasks"}, {"figure_path": "raABeiV71j/tables/tables_23_1.jpg", "caption": "Table 5: Performance of PCAAttn with various cache configurations.", "description": "This table presents the performance of the PCAAttn method (a variant of Loki that directly uses reduced-dimensional attention scores) compared to full attention, Exact TopK, and H2O on two different models (Llama2-7B and Mistral-7B) using different cache configurations (kf=0.5, kf=0.25).  The metrics used are Perplexity and several downstream task accuracies (Hellaswag, Winogrande, MathQA, OpenbookQA, RTE, COPA). The results showcase the poor performance of PCAAttn compared to the baselines, suggesting that the dimensionality reduction approach used in this method is not effective in the presence of rotary embeddings.", "section": "C Comparison of our kernels with SparQ"}]