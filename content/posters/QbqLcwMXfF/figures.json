[{"figure_path": "QbqLcwMXfF/figures/figures_4_1.jpg", "caption": "Figure 2: The operator norm of W with and without Query-temperature scaling, scaled by \u00d7103. The figure depicts the distribution across 1000 tokens. The dashed line is the average norm. Notably, the norm of the vanilla attention layer is approximately three times larger than that of SSA(dashed red line compare to green line). Furthermore, the vanilla attention layer exhibits a lower spikiness score (0.39) compared to SSA (0.26), where a lower value indicates higher spikiness.", "description": "This figure shows the distribution of the operator norm of the combined query-key weight matrix (W) with and without query temperature scaling.  The x-axis represents ranges of the norm values, and the y-axis represents the probability of a token having a norm within that range. The dashed lines indicate the average norm for both vanilla attention and SSA. The results show that SSA has a significantly smaller norm than vanilla attention, with a much lower average norm and a higher spikiness (sparsity).  The lower average norm and higher spikiness suggest that SSA is more efficient in controlling contextual sparsity.", "section": "4.1 The benefits of incorporating query embedding"}, {"figure_path": "QbqLcwMXfF/figures/figures_5_1.jpg", "caption": "Figure 3: We compare 1-layer SSA and 1-layer attention when solving next-token prediction on a small vocabulary of size 8. (a) is the graph associated to the token transition dynamics. (b) is the the pairwise token transition matrix of this vocabulary. Each row of P represents an attention map where a particular token is the query and all tokens in the vocabulary serve as keys (see Sec 4.1 for details). The transition matrix P estimated by SSA in (c) is sharper and more closely resembles the optimal P. SSA achieves a smaller cross-entropy loss compared to vanilla attention, 0.009 vs 0.0126. The l\u2081 approximation error of the attention map of SSA is also smaller than that of vanilla attention, 0.358 vs 0.543.", "description": "This figure compares the performance of Selective Self-Attention (SSA) and standard self-attention in a next-token prediction task using a small vocabulary.  Subfigure (a) shows the graph representing the token transition dynamics. Subfigure (b) displays the ground-truth token transition matrix (P*). Subfigures (c) and (d) show the learned transition matrices using SSA and standard self-attention, respectively.  The results demonstrate that SSA learns a transition matrix that is sharper, closer to the ground truth, and has lower cross-entropy loss and L1 approximation error than standard self-attention.", "section": "4.1 The benefits of incorporating query embedding"}, {"figure_path": "QbqLcwMXfF/figures/figures_9_1.jpg", "caption": "Figure 4: Comparison of training curves. SSA provides reasonable benefits in terms of training speedup.", "description": "This figure shows a comparison of the training curves for vanilla attention and SSA (Selective Self-Attention) when fine-tuning the Llama model on the Wikitext dataset.  The x-axis represents the number of tokens processed (in millions), and the y-axis represents the perplexity (ppl), a measure of the model's performance.  The graph shows that SSA achieves comparable perplexity scores to vanilla attention but in fewer training steps, demonstrating the training speedup offered by SSA. The red arrow emphasizes the speedup by highlighting the reduced number of tokens required by SSA to reach a similar perplexity level to the vanilla model.", "section": "5.2 Passkey Retrieval"}]