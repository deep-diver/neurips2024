[{"figure_path": "B1FOes6cyq/tables/tables_6_1.jpg", "caption": "Table 1: The test perplexity of the teacher model in LOT and the baseline on PTB and WikiText-103. Results are averaged over three runs. LOT achieves consistent perplexity reduction over different choices of architectures and benchmarks.", "description": "This table presents the test perplexity results for different language models (LSTM, AWD-LSTM, Transformer-XL) trained on two benchmark datasets (PTB and WikiText-103) using both the proposed LOT method and a standard teacher-only training approach.  The results are averaged over three runs to demonstrate the consistency of the method. The table highlights the consistent improvement in test perplexity achieved by LOT across various model architectures and datasets.", "section": "3.3 Language Modeling"}, {"figure_path": "B1FOes6cyq/tables/tables_6_2.jpg", "caption": "Table 2: The accuracy of the teacher model in LOT and the baseline on GSM8K and MATH. Results are averaged over three runs.", "description": "This table presents the accuracy results of the teacher model trained with LOT and compared to the baselines (In-context learning and supervised fine-tuning) on two mathematical reasoning benchmarks: GSM8K and MATH.  The results are averaged across three runs, highlighting the performance improvement achieved by LOT.", "section": "3.3.2 Supervised Fine-tuning"}, {"figure_path": "B1FOes6cyq/tables/tables_7_1.jpg", "caption": "Table 3: The test accuracy of the teacher model for various teacher-student model combinations in LOT and the baseline. Results are averaged over three runs. LOT consistently enhances test performance in all model choices and datasets.", "description": "This table presents the test accuracy results for image classification experiments using various teacher-student model combinations.  The models were pre-trained on ImageNet-1K or ImageNet-21K and then fine-tuned on CIFAR-100 and ImageNet-1K.  Both the \"Teacher-only\" (no LOT regularization) and \"LOT\" (with Learning from Teaching regularization) approaches are compared for each combination.  The results demonstrate that LOT consistently improves test accuracy across all model combinations.", "section": "3.4 Image Classification"}, {"figure_path": "B1FOes6cyq/tables/tables_8_1.jpg", "caption": "Table 4: Performance comparison of Teacher-only, BAN and LOT on CIFAR-100. LOT achieves superior performance to Teacher-only and BAN.", "description": "This table compares the performance of three different methods on the CIFAR-100 dataset: Teacher-only, BAN (Born Again Networks), and LOT (Learning from Teaching).  For each method, the table shows the teacher and student model architectures used, and the resulting accuracy. The results demonstrate that LOT consistently outperforms both Teacher-only and BAN, highlighting the effectiveness of LOT in enhancing the generalization capabilities of deep neural networks.", "section": "3. Experiments"}, {"figure_path": "B1FOes6cyq/tables/tables_18_1.jpg", "caption": "Table 5: Hyperparameters for Language Modeling.", "description": "This table lists the hyperparameters used for the language modeling experiments.  It specifies the model (LSTM, AWD-LSTM, Transformer-XL-B, Transformer-XL-L, LLaMA-1 7B, LLaMA-2 7B), the dataset (PTB, WikiText-103, GSM8K, MATH), the regularization coefficient (\u03b1), the student steps ratio (N), the optimizer (SGD, ASGD, ADAM, ADAMW), the learning rate, the number of training epochs or steps, and the temperature used for the KL divergence calculation.", "section": "3.3 Language Modeling"}, {"figure_path": "B1FOes6cyq/tables/tables_19_1.jpg", "caption": "Table 6: Hyperparameters for Image Classification.", "description": "This table lists the hyperparameters used in the image classification experiments.  It shows the model, dataset, alpha (\u03b1) value, number of students (N), optimizer, learning rate, training epochs/steps, and temperature used in the LOT experiments for various image classification models and datasets.", "section": "3.4 Image Classification"}, {"figure_path": "B1FOes6cyq/tables/tables_20_1.jpg", "caption": "Table 7: Performance of the teacher model in LOT and Teacher-only on image classification. The hyperparameters are the same as the corresponding experiments in the paper.", "description": "This table compares the performance of the teacher model trained with the LOT regularizer and the Teacher-only baseline on image classification tasks using different training steps.  It shows that LOT consistently improves the performance of the teacher model, even when using the same number of total training steps.", "section": "3.5 Analysis of Computational Cost and Efficiency"}, {"figure_path": "B1FOes6cyq/tables/tables_21_1.jpg", "caption": "Table 8: Performance of LOT and Teacher-only on ImageNet-R and ImageNet-Sketch.", "description": "This table presents the performance comparison between the proposed LOT method and the baseline Teacher-only method on two image datasets: ImageNet-R and ImageNet-Sketch.  The results are shown for different combinations of teacher and student model architectures (ViT-B/16 and ViT-L/16), demonstrating the impact of LOT on out-of-distribution generalization.", "section": "3.4 Image Classification"}, {"figure_path": "B1FOes6cyq/tables/tables_21_2.jpg", "caption": "Table 1: The test perplexity of the teacher model in LOT and the baseline on PTB and WikiText-103. Results are averaged over three runs. LOT achieves consistent perplexity reduction over different choices of architectures and benchmarks.", "description": "This table shows the test perplexity results for different language models (LSTM, AWD-LSTM, Transformer-XL) trained on two datasets (PTB and WikiText-103) using two methods: LOT (Learning from Teaching) and a baseline (Teacher-only).  The results, averaged over three runs, demonstrate that LOT consistently achieves lower perplexity (better performance) across various model architectures and datasets.", "section": "3.3 Language Modeling"}, {"figure_path": "B1FOes6cyq/tables/tables_21_3.jpg", "caption": "Table 11: The test perplexity of the teacher model in LOT and the baseline on PTB and WikiText-103. Results are averaged over three runs. LOT achieves consistent perplexity reduction over different choices of architectures and benchmarks.", "description": "This table presents the test perplexity results for language modeling experiments on the Penn Treebank (PTB) and WikiText-103 datasets.  The results compare the performance of the teacher model trained with the proposed Learning from Teaching (LOT) regularization against a baseline (Teacher-only) model,  for different model architectures (LSTM, AWD-LSTM, Transformer-XL).  The perplexity, a measure of how well a language model predicts a sample of text, is lower for the LOT models across all architectures and datasets, demonstrating the effectiveness of the LOT regularization technique in improving generalization.", "section": "3.3 Language Modeling"}, {"figure_path": "B1FOes6cyq/tables/tables_22_1.jpg", "caption": "Table 12: Computational resources, memory usage, and training time of LOT and Teacher-only.", "description": "This table details the computational resources, memory usage, and training time required for both LOT and Teacher-only methods across various tasks (Atari game, language modeling, image classification). It provides a comprehensive comparison of resource utilization and training efficiency for different models and datasets, highlighting the computational overhead introduced by the student models in LOT while showing that the total training time is often comparable or even lower than that of Teacher-only, demonstrating the efficiency of LOT.", "section": "3.5 Analysis of Computational Cost and Efficiency"}, {"figure_path": "B1FOes6cyq/tables/tables_22_2.jpg", "caption": "Table 13: Performance of using L2 loss for the LOT regularizer on CIFAR100.", "description": "This table presents the results of experiments conducted to evaluate the performance of using L2 loss as the imitability metric in the LOT regularizer, compared to using KL-divergence.  The experiment used the CIFAR-100 dataset, with different combinations of teacher and student ViT models (ViT-B/16 and ViT-L/16).  The table shows the test accuracy achieved by the teacher model under three different scenarios: the Teacher-only baseline (no LOT regularization), LOT using KL-divergence as the imitability metric, and LOT using L2 loss. The results demonstrate that using L2 loss for the LOT regularizer also improves the generalization performance of the teacher model.", "section": "Ablation of Metrics in LOT Regularizer"}]