[{"figure_path": "B1FOes6cyq/figures/figures_4_1.jpg", "caption": "Figure 1: Training and test KL-divergence losses of student models in LOT using ViT-B/16 and ViT-L/16 on CIFAR-100 with different teacher models. The sophisticated students achieve lower losses than the deceptive students given the same computational budget.", "description": "This figure displays the training and testing KL divergence losses for student models trained using the LOT method.  Two types of teacher models are used: sophisticated (capturing generalizable correlations) and deceptive (capturing spurious correlations). The results show that student models imitating sophisticated teachers consistently exhibit lower losses during both training and testing compared to students imitating deceptive teachers, even with the same computational resources. This finding supports the hypothesis that generalizable correlations are easier to learn than spurious correlations.", "section": "3.1 Generalizable Correlations are Easier to Imitate than Spurious Correlations"}, {"figure_path": "B1FOes6cyq/figures/figures_5_1.jpg", "caption": "Figure 2: The episodic return of the teacher agent in LOT and the Teacher-only on four Atari games (averaged over ten runs). LOT demonstrates return gains over Teacher-only on all games.", "description": "This figure displays the episodic return of both the LOT (Learning from Teaching) and Teacher-only agents across four different Atari games: BeamRider, Gravitar, UpNDown, and Breakout.  The results are averaged over ten independent runs.  The shaded area represents the standard deviation across these runs. The graph shows that the LOT agent consistently outperforms the Teacher-only agent in terms of episodic return across all four games. The improvement becomes more significant as the number of training steps (in millions) increases.", "section": "3.2 Atari Games"}, {"figure_path": "B1FOes6cyq/figures/figures_7_1.jpg", "caption": "Figure 1: Training and test KL-divergence losses of student models in LOT using ViT-B/16 and ViT-L/16 on CIFAR-100 with different teacher models. The sophisticated students achieve lower losses than the deceptive students given the same computational budget.", "description": "This figure shows the training and test KL-divergence losses for four different student models trained using the LOT method. Two models were trained using a teacher model that learned generalizable correlations and two other models were trained using a teacher model that learned spurious correlations. The results demonstrate that the sophisticated students (those that learn generalizable correlations) achieve lower training and test losses than the deceptive students (those that learn spurious correlations), even with the same computational budget.", "section": "3.1 Generalizable Correlations are Easier to Imitate than Spurious Correlations"}, {"figure_path": "B1FOes6cyq/figures/figures_8_1.jpg", "caption": "Figure 4: Effects of regularization coefficient \u03b1 (left) and student steps ratio N (right). \u03b1 = 1 is the best \u03b1 value to achieve the lowest test perplexity of the teacher model, and moderate student steps ratio N such as 4 and 5 benefit the teacher model the most.", "description": "This figure shows the impact of hyperparameters \u03b1 and N on the performance of the LOT method.  The left panel shows that a regularization coefficient \u03b1 of 1 yields the lowest test perplexity (a measure of model performance), suggesting a balance between the main task loss and the LOT regularizer is crucial for optimal results. The right panel shows that a moderate student steps ratio N (around 4 or 5) leads to the best teacher model performance, indicating that too little or too much interaction with the student models hinders overall generalization.", "section": "3.5 Analysis of Computational Cost and Efficiency"}, {"figure_path": "B1FOes6cyq/figures/figures_20_1.jpg", "caption": "Figure 5: Training and test KL-divergence losses of student models in LOT using ResNet-18 and ResNet-50 on CIFAR-100 with different teacher models.", "description": "This figure shows the training and test KL-divergence losses for student models trained using the Learning from Teaching (LOT) regularization method.  Two types of teacher models are used: sophisticated teachers, which effectively capture generalizable correlations, and deceptive teachers, which primarily learn spurious correlations.  For each teacher type, two student models are trained: one to imitate the sophisticated teacher and one to imitate the deceptive teacher. The results show that the student models imitating the sophisticated teacher (i.e., those learning generalizable correlations) achieve significantly lower KL-divergence losses during both training and testing, and converge faster, compared to the student models imitating the deceptive teacher. This supports the hypothesis that generalizable correlations are easier to imitate.", "section": "3.1 Generalizable Correlations are Easier to Imitate than Spurious Correlations"}]