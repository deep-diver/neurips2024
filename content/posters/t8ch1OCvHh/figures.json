[{"figure_path": "t8ch1OCvHh/figures/figures_2_1.jpg", "caption": "Figure 1: Calibration results on GLU-implemented and non GLU-implemented LLMs. We present the maximum magnitudes of input activations for each linear modules and layer-wise hidden states. For more results on different LLMs, see Appendix A.2, A.3.", "description": "This figure shows the calibration results for both GLU-implemented and non-GLU-implemented LLMs.  The x-axis represents the layers in the model, and the y-axis shows the maximum magnitude of input activations for each linear module (query/key/value, self-attention output, feed-forward up/gate, feed-forward down) and the hidden states. The different colors represent the different components of the model.  The GLU-implemented LLMs show significantly higher activation magnitudes in specific layers compared to the non-GLU models, highlighting the concentration of activation spikes in those models. The appendix contains additional calibration results for other LLMs.", "section": "3 Activation Spikes: Excessive Magnitude of GLU Activations"}, {"figure_path": "t8ch1OCvHh/figures/figures_3_1.jpg", "caption": "Figure 2: Token-level scales in a specific layer with an activation spike. When quantizing the input activations using a per-tensor scale, the scale of the activation spike dominates the scales of the other tokens. For more examples, see Appendix D.2.", "description": "This figure shows a comparison of per-token and per-tensor scaling for input activations in a layer with an activation spike. The left panel shows LLaMA-2-7B Layer 2 feed-forward.down, and the right panel shows LLaMA-2-70B Layer 9 feed-forward.down.  In both cases, a single token (an apostrophe in this example) has a significantly larger scale than the others when using per-tensor scaling. This highlights how a single, excessively large activation can skew the scaling for the whole tensor, leading to quantization errors.", "section": "3.2 Token-level Scale Analysis within Activation Spikes"}, {"figure_path": "t8ch1OCvHh/figures/figures_4_1.jpg", "caption": "Figure 3: Overview of QFeM and QFeP. (Left): QFeM excludes the modules whose r(m) is larger than the hyperparameter \u03b1 from quantization. (Right): QFeP computes in advance the prefix of activation spikes and utilizes solely their KV cache during the quantization phase, effectively preventing further activation spikes in subsequent sequences.", "description": "This figure illustrates the two proposed methods to mitigate the impact of activation spikes on quantization: Quantization-free Module (QFeM) and Quantization-free Prefix (QFeP). QFeM selectively excludes specific modules from quantization based on a calculated ratio (r(m)), while QFeP precomputes a prefix of activation spikes and uses only the key-value (KV) cache for these prefixes during quantization. Both methods aim to improve the quantization performance by isolating or preventing the activation spikes.", "section": "4 Mitigating Quantization Quality Degradation Based on the Observation"}, {"figure_path": "t8ch1OCvHh/figures/figures_4_2.jpg", "caption": "Figure 4: Trade-off between perplexity (stands for performance) and |Mung| (stands for latency) according to the threshold \u03b1 for LLaMA-2-13B model.", "description": "This figure shows the trade-off between model performance (measured by perplexity) and latency (represented by the number of unquantized modules |Mung|) when varying the threshold \u03b1 in the QFeM method.  The x-axis represents different values of the threshold \u03b1, while the y-axis on the left shows the number of unquantized modules, and the y-axis on the right shows the perplexity.  As \u03b1 increases, fewer modules are excluded from quantization (|Mung| decreases), leading to higher latency but potentially better performance.  The optimal threshold \u03b1 balances these competing factors.", "section": "4.1 Quantization-free Module (QFeM)"}, {"figure_path": "t8ch1OCvHh/figures/figures_7_1.jpg", "caption": "Figure 6: Prefix ablation. Y-axis represents averaged accuracy of four zero-shot tasks.", "description": "This ablation study investigates the impact of different prefix lengths on the QFeP method's performance.  The prefixes tested include a random sequence, the BOS token only, the identified activation spike prefix without context, and the identified activation spike prefix with context. The results show that using a context token in the prefix consistently improves the average zero-shot accuracy across multiple models, indicating that context is crucial for effective activation spike mitigation in this approach.", "section": "5.4 Ablation Study"}, {"figure_path": "t8ch1OCvHh/figures/figures_8_1.jpg", "caption": "Figure 7: Accuracy-latency comparison of different activation quantization schemes: dynamic per-token (AQ1), dynamic per-tensor (AQ2), and static per-tensor (AQ3).", "description": "This figure compares the accuracy and latency of three different activation quantization methods across two LLMs (LLaMA-2-13B and LLaMA-2-70B).  The methods are: dynamic per-token quantization (AQ1), dynamic per-tensor quantization (AQ2), and static per-tensor quantization (AQ3).  The x-axis represents inference latency in milliseconds, and the y-axis shows the accuracy of the zero-shot tasks. The figure shows that while the dynamic per-token method (AQ1) offers the best accuracy, it's also the slowest. The dynamic per-tensor (AQ2) and static per-tensor (AQ3) methods provide faster inference, but with some accuracy loss, demonstrating a trade-off between speed and performance. The addition of QFeM and/or QFeP improves latency significantly while maintaining comparable accuracy to AQ1, especially with the AQ2 scheme.", "section": "5.5 Computational Cost Analysis"}, {"figure_path": "t8ch1OCvHh/figures/figures_13_1.jpg", "caption": "Figure 8: Calibration results on GLU-implemented LLMs (Mixtral-8x7B).", "description": "Calibration results for Mixtral-8x7B model showing activation magnitudes across different layers and modules (q_proj, o_proj, experts.0.w2, experts.0.w3,... experts.7.w3, hidden_states).  The graph illustrates the scale of activations, highlighting the presence of activation spikes in specific layers and modules.", "section": "A.2 Other Calibration Results on GLU-implementation"}, {"figure_path": "t8ch1OCvHh/figures/figures_14_1.jpg", "caption": "Figure 9: Calibration results on GLU-implemented LLMs.", "description": "The figure displays calibration results for several large language models (LLMs) that utilize the GLU (Gated Linear Unit) architecture.  Calibration is a process used to estimate the quantization parameters for efficient model compression. The plots show the maximum magnitudes of activation values across different layers of the LLMs. The layers include self-attention queries, keys, and values (self-attn.q/k/v), self-attention output (self-attn.out), feed-forward up/gate, feed-forward down, and hidden states.  Each plot shows the activation magnitudes for a specific LLM, highlighting the layers where activation spikes (excessively large values) occur. The models shown are LLaMA-2-13B, LLaMA-2-70B, LLaMA-3-8B, LLaMA-3-70B, SOLAR-10.7B, StableLM-2-12B, and Gemma-7B.", "section": "3.1 Existence of Activation Spikes in GLU Variants"}, {"figure_path": "t8ch1OCvHh/figures/figures_14_2.jpg", "caption": "Figure 1: Calibration results on GLU-implemented and non GLU-implemented LLMs. We present the maximum magnitudes of input activations for each linear modules and layer-wise hidden states. For more results on different LLMs, see Appendix A.2, A.3.", "description": "This figure displays the maximum magnitudes of input activations for each linear module and layer-wise hidden states in both GLU-implemented and non-GLU-implemented LLMs.  It shows how the scale of input activations varies across different layers and modules within the model architecture. The difference in activation magnitudes between GLU and non-GLU models highlights a potential cause of quantization errors that are addressed in the paper. Appendix A.2 and A.3 provide additional results on different LLMs.", "section": "3 Activation Spikes: Excessive Magnitude of GLU Activations"}, {"figure_path": "t8ch1OCvHh/figures/figures_16_1.jpg", "caption": "Figure 1: Calibration results on GLU-implemented and non GLU-implemented LLMs. We present the maximum magnitudes of input activations for each linear modules and layer-wise hidden states. For more results on different LLMs, see Appendix A.2, A.3.", "description": "This figure shows the maximum magnitudes of input activations for each linear module and layer-wise hidden states in both GLU-implemented and non GLU-implemented LLMs.  The GLU-implemented LLMs show a pattern of activation spikes in the early and late layers of the feed-forward network, specifically in the down module.  In contrast, non GLU-implemented LLMs show a more modest distribution of activation magnitudes. The figure helps to visualize the difference in activation magnitude between different LLMs and layer types, highlighting the specific characteristics of GLU-based architectures.", "section": "3 Activation Spikes: Excessive Magnitude of GLU Activations"}, {"figure_path": "t8ch1OCvHh/figures/figures_16_2.jpg", "caption": "Figure 2: Token-level Scale Analysis within Activation Spikes. In the previous section, we observed the excessive scale of the input activations derived from GLU activation. When quantizing the input activations, the variance of input activation scales for each token affects the quantization performance [55]. To delve into the disparity between token-wise scales in the activation spikes, we unroll them through the sequence of tokens. Figure 2 illustrates the individual input activation scales where the activation spike appears. Given a token sequence, the large magnitudes of input activations are observed in a couple of tokens, such as the BOS token, newline (\\n), and apostrophe ('). These specific tokens coincide with the observations of [42], which suggests that such tokens exhibit massive values in the hidden states. Thus, the activation spike is associated with the process of assigning a special role to these tokens in later transformer layers. However, the excessive scale of specific token hinders the estimation of scale factor for the other tokens, such as in per-tensor quantization. Additionally, the largest scale is dedicated to the first instance of the specified token, while the following usage exhibits a modest scale. This phenomenon makes the quantization more complicated, as the activation spikes dynamically occur depending on the current input sequence.", "description": "This figure shows a comparison of per-token and per-tensor scales for input activations in a specific layer exhibiting an activation spike.  The per-tensor scale is dominated by the large values associated with specific tokens like BOS and newline, highlighting the challenge of using a single scale for quantization when activation spikes are present.", "section": "3.2 Token-level Scale Analysis within Activation Spikes"}, {"figure_path": "t8ch1OCvHh/figures/figures_16_3.jpg", "caption": "Figure 2: Token-level Scale Analysis within Activation Spikes. In the previous section, we observed the excessive scale of the input activations derived from GLU activation. When quantizing the input activations, the variance of input activation scales for each token affects the quantization performance [55]. To delve into the disparity between token-wise scales in the activation spikes, we unroll them through the sequence of tokens. Figure 2 illustrates the individual input activation scales where the activation spike appears. Given a token sequence, the large magnitudes of input activations are observed in a couple of tokens, such as the BOS token, newline (\\n), and apostrophe ('). These specific tokens coincide with the observations of [42], which suggests that such tokens exhibit massive values in the hidden states. Thus, the activation spike is associated with the process of assigning a special role to these tokens in later transformer layers. However, the excessive scale of specific token hinders the estimation of scale factor for the other tokens, such as in per-tensor quantization. Additionally, the largest scale is dedicated to the first instance of the specified token, while the following usage exhibits a modest scale. This phenomenon makes the quantization more complicated, as the activation spikes dynamically occur depending on the current input sequence.", "description": "This figure shows the token-wise activation scales for a specific layer that contains activation spikes.  The x-axis represents the tokens in a sequence and the y-axis represents the activation magnitude. It illustrates how the activation spike, a significant outlier, strongly affects the overall scale and demonstrates the disparity between the scale of the spike tokens and the other tokens in the sequence. This disparity makes accurate quantization challenging, as the large scale of the spike dominates the quantization process, resulting in poor quantization of the other, smaller scale tokens.", "section": "3.2 Token-level Scale Analysis within Activation Spikes"}]