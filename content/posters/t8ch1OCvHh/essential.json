{"importance": "This paper is crucial for researchers working on **large language model (LLM) optimization** and **quantization**. It directly addresses the challenge of activation spikes in GLU-based LLMs, a critical issue impacting inference efficiency. The proposed methods offer practical solutions for improving LLM quantization, opening avenues for more efficient and cost-effective LLM deployment.  The insights on activation spike patterns are valuable for designing future LLM architectures and quantization techniques.", "summary": "New methods, QFeM and QFeP, mitigate quantization errors in GLU-based LLMs caused by activation spikes, significantly improving performance, especially with coarse-grained quantization.", "takeaways": ["Activation spikes in GLU-based LLMs severely hinder activation quantization.", "QFeM and QFeP effectively isolate and mitigate the effects of activation spikes during quantization.", "The proposed methods significantly improve LLM quantization performance, especially under coarse-grained settings."], "tldr": "Large language models (LLMs) are computationally expensive, hence post-training quantization (PTQ) is used to reduce inference costs. However, activation quantization in LLMs with gated linear units (GLUs) presents challenges due to activation spikes, significantly degrading performance. These spikes are excessively large activation values generated in specific linear layers of the feed-forward network (FFN), particularly in early and late layers, affecting only a few tokens. \nThis paper introduces two novel methods: Quantization-free Module (QFeM) and Quantization-free Prefix (QFeP). QFeM identifies and excludes problematic modules during quantization, while QFeP isolates activation spikes by preserving the context using a KV cache.  Extensive experiments across various state-of-the-art LLMs demonstrate that these methods significantly enhance quantization performance, especially when using coarse-grained quantization, outperforming existing outlier mitigation techniques.  This work offers valuable insights into the characteristics of activation spikes and provides practical solutions for more efficient LLM deployment.", "affiliation": "string", "categories": {"main_category": "Natural Language Processing", "sub_category": "Large Language Models"}, "podcast_path": "t8ch1OCvHh/podcast.wav"}