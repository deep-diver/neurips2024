[{"figure_path": "t8ch1OCvHh/tables/tables_3_1.jpg", "caption": "Table 1: Perplexity and MSE of partial activation quantization of LLMs", "description": "This table presents the results of an experiment on partial activation quantization of several Large Language Models (LLMs).  It compares the perplexity and Mean Squared Error (MSE)  achieved when only certain portions of the activation layers are quantized, categorized as \"Top 4\", \"Middle 4\", and \"Bottom 4\", based on a max-median ratio. The FP16 column shows results for full precision (no quantization) to serve as a baseline for comparison.  The table highlights the impact of activation spikes, showcasing significant performance degradation when quantizing the most sensitive layers (\"Top 4\").", "section": "3.3 Effect of Quantization on Activation Spikes"}, {"figure_path": "t8ch1OCvHh/tables/tables_5_1.jpg", "caption": "Table 2: Specifications for QFeM and QFeP used in experiments. |M| denotes the total number of linear layers in the LLM, and |Mung| represents the number of unquantized layers for QFeM.", "description": "This table shows the hyperparameters used for the Quantization-free Module (QFeM) and Quantization-free Prefix (QFeP) methods in the experiments conducted in the paper.  Specifically, it lists the prefix tokens identified for each language model, the threshold value (\u03b1) used to determine which layers to exclude from quantization in QFeM, and the ratio of unquantized modules to total modules for each model in the QFeM method.", "section": "4 Mitigating Quantization Quality Degradation Based on the Observation"}, {"figure_path": "t8ch1OCvHh/tables/tables_6_1.jpg", "caption": "Table 3: Perplexity and MSE of partial activation quantization of LLMs", "description": "This table presents the results of experiments evaluating the impact of partial activation quantization on the performance of several LLMs.  It compares the perplexity and mean squared error (MSE) for three different quantization schemes: quantizing the top 4, middle 4, and bottom 4 modules based on a max-median ratio. The modules are selected based on their sensitivity to quantization errors caused by activation spikes. The table shows that quantizing only the most sensitive modules (top 4) significantly degrades performance, highlighting the detrimental effect of activation spikes on accuracy.", "section": "5.2 Main Results"}, {"figure_path": "t8ch1OCvHh/tables/tables_7_1.jpg", "caption": "Table 4: Evaluation of outlier alleviation methods with QFeM and QFeP. We report perplexity on WikiText-2 and averaged accuracy of four zero-shot tasks. The same quantization scheme for used on both SQ and OSP. Per-tensor weight quantization results are provided in Appendix C.1.", "description": "This table presents the results of experiments combining the proposed Quantization-free Module (QFeM) and Quantization-free Prefix (QFeP) methods with existing outlier alleviation techniques, SmoothQuant (SQ) and OutlierSuppressionPlus (OSP).  The table shows perplexity scores on the WikiText-2 benchmark and average accuracy across four zero-shot tasks (PIQA, LAMBADA, HellaSwag, WinoGrande) for various LLMs (LLaMA-2-7B, LLaMA-2-13B, LLaMA-2-70B). It demonstrates the performance improvements achieved by integrating QFeM and QFeP with SQ and OSP, highlighting the effectiveness of the proposed methods in mitigating quantization errors caused by activation spikes.", "section": "5.3 Combining Outlier Alleviation Methods"}, {"figure_path": "t8ch1OCvHh/tables/tables_13_1.jpg", "caption": "Table 3: Perplexity and MSE of partial activation quantization of LLMs", "description": "This table presents the results of applying partial activation quantization to several LLMs.  It compares the perplexity (a measure of how well a language model predicts a sequence) and the mean-squared error (MSE, a measure of the difference between the model's predictions and the actual values) for different quantization schemes. The table shows the baseline performance (FP16), the performance when the top 4, middle 4, and bottom 4 layers are quantized, based on a max-median ratio metric that identifies layers most sensitive to quantization errors. The goal is to show the detrimental effect of activation spikes concentrated in certain layers and how partial quantization impacts the different LLMs.", "section": "3.3 Effect of Quantization on Activation Spikes"}, {"figure_path": "t8ch1OCvHh/tables/tables_15_1.jpg", "caption": "Table 7: BMM quantization results.", "description": "This table presents the results of experiments evaluating the impact of Batch Matrix-Multiplication (BMM) quantization on the overall performance. Two quantization methods are compared: W8A8 (quantization of both weights and activations to 8 bits) and W8A8 + QFeP (W8A8 combined with the Quantization-free Prefix method). The table shows the average accuracy for three different model sizes (7B, 13B, and 70B parameters) under two conditions: with and without BMM quantization.  The results demonstrate the effectiveness of QFeP in improving performance regardless of whether BMM quantization is used or not.", "section": "B BMM Quantization"}, {"figure_path": "t8ch1OCvHh/tables/tables_15_2.jpg", "caption": "Table 3: Perplexity and zero-shot evaluation for the quantization on LLaMA-2 models. FP16 denotes the original model precision, and W8A8 denotes the model quantized to INT8 for both weights and activations.", "description": "This table presents the results of perplexity and zero-shot evaluation for different quantization methods on three LLaMA-2 models (7B, 13B, and 70B parameters).  The baseline is FP16 (full precision), while W8A8 represents quantization of both weights and activations to 8 bits. The table also shows the results after applying the proposed quantization-free module (QFeM) and quantization-free prefix (QFeP) methods individually and in combination. Lower perplexity scores and higher accuracy scores indicate better performance.  The results demonstrate the impact of activation spikes on quantization and the effectiveness of QFeM and QFeP in mitigating this effect.", "section": "5.2 Main Results"}]