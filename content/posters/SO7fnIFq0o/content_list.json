[{"type": "text", "text": "Ensemble sampling for linear bandits: small ensembles suffice ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "David Janz Alexander E. Litvak Csaba Szepesv\u00e1ri University of Oxford\u2217 University of Alberta University of Alberta david.janz@stats.ox.ac.uk alitvak@ualberta.ca szepesva@ualberta.ca ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We provide the first useful and rigorous analysis of ensemble sampling for the stochastic linear bandit setting. In particular, we show that, under standard assumptions, for a $d$ -dimensional stochastic linear bandit with an interaction horizon $T$ , ensemble sampling with a\u221an ensemble of size of order $d\\log T$ incurs regret at most of the order $(\\stackrel{\\bullet}{d}\\log T)^{5/2}\\sqrt{T}$ . Ours is the first result in any structured setting not to require the size of the ensemble to scale linearly wit\u221ah $T$ \u2014which defeats the purpose of ensemble sampling\u2014while obtaining near $\\sqrt{T}$ order regret. Ours is also the first result that allows infinite action sets. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Ensemble sampling (Lu and Van Roy, 2017) is a family of randomised algorithms for balancing exploration and exploitation within sequential decision-making tasks. These algorithms maintain an ensemble of perturbed models of the value of the available actions and, in each step, select an action that has the highest value according to a model chosen uniformly at random from the ensemble. The ensemble is then incrementally updated the new observations. ", "page_idx": 0}, {"type": "text", "text": "Ensemble sampling was introduced as an alternative to Thompson sampling (Thompson, 1933) that is tractable whenever incremental model updates are cheap (Osband et al., 2016; Lu and Van Roy, 2017). It is thus particularly popular in deep reinforcement learning, where the models (neural networks) are trained via gradient descent in an incremental fashion. Therein, ensemble sampling features directly in algorithms such as bootstrapped DQN and ensemble $^+$ (Osband et al., 2016, 2018), as part of other methods (Dimakopoulou and Van Roy, 2018; Curi et al., 2020), and as the motivation for methods, including random network distillation (Burda et al., 2018) and hypermodels for exploration (Dwaracherla et al., 2020). Ensemble sampling has also been applied to online recommendation systems (Lu et al., 2018; Hao et al., 2020; Zhu and Van Roy, 2021), the behavioural sciences (Eckles and Kaptein, 2019) and to marketing (Yang et al., 2020). ", "page_idx": 0}, {"type": "text", "text": "Despite its practicality and simple nature, ensembl\u221ae sampling has thus far resisted analysis. Indeed, Qin et al. (2022), who showed a bound of order $\\sqrt{T}$ on its Bayesian regret under the impractical condition that the size of the ensemble scales at least linearly with the horizon $T$ , summarise the state-of-the-art in this regard as follows: ", "page_idx": 0}, {"type": "text", "text": "\u2018A lot of work has attempted to analyze ensemble sampling, but none of them has been successful.\u2019 ", "page_idx": 0}, {"type": "text", "text": "Readers familiar with the literature know that the analysis of randomised exploration methods, such as Thompson sampling, or perturbed history exploration (Kveton et al., 2020; Janz et al., 2024), relies on showing that at each step there is a constant probability of choosing a model that overestimates the true value (Agrawal and Goyal, 2013; Abeille and Lazaric, 2017). For these methods, the analysis is relatively simple, because the algorithms can be described as first fitting a model to the past data, and then using a fresh source of randomness to perturb the model, which is then used to derive the action to be used. Ensemble sampling does not fti this pattern: here, the distribution of models given the past is controlled only implicitly, making the analysis challenging. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Our contribution is a guarantee that (a symmetrised version of) ensemble sampling, given an ensemble size logarith\u221amic in $T$ and linear in the number of features $d$ , incurs regret no worse than order $(d\\log{\\bar{T}})^{5/2}{\\sqrt{T}}$ . This is the first successful analysis of ensemble sampling in the stochastic linear bandit setting, or indeed, any structured setting (see Remarks 5 and 6 for a discussion of this claim). Our result is based on (a slight extension of) the now-standard framework of Abeille and Lazaric (2017), and as such it ought to be possible to extend it to the usual settings: generalised linear bandits (Filippi et al., 2010), kernelised bandits (Srinivas et al., 2010), deep learning (via the neural tangent kernel, per Jacot et al., 2018) and reinforcement learning (following the work of Zanette et al., 2020). ", "page_idx": 1}, {"type": "text", "text": "2 Linear ensemble sampling ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We now outline our problem setting, a version of the linear ensemble sampling algorithm and our main result, and discuss these in the context of prior literature. We encourage readers less familiar with the motivation around ensemble sampling to consult Lu and Van Roy (2017) or Osband et al. (2019); our focus will be on analysis. We will use the following notation: ", "page_idx": 1}, {"type": "text", "text": "Sets and real numbers We write $\\mathbb{N}^{+}=\\{1,2,\\dots\\}$ , $\\mathbb{N}=\\mathbb{N}^{+}\\cup\\{0\\}$ and $[n]=\\{1,2,\\ldots,n\\}$ . We use the shorthand $(a_{t})_{t}$ for a sequence indexed by $t$ in $\\mathbb{N}$ or $\\mathbb{N}^{+}$ when this index set can be deduced from the context. For $a,b\\in\\mathbb{R}$ , that is, for real numbers $a$ and $b_{\\cdot}$ , $a\\wedge b$ denotes their minimum and $a\\vee b$ their maximum. ", "page_idx": 1}, {"type": "text", "text": "Vectors and matrices We identify vectors and linear operators with matrices. We write $\\|\\cdot\\|_{2}$ for the 2-norm of a vector, and $\\|\\cdot\\|$ for the operator norm of a matrix corresponding to the largest singular value, which we denote by $s_{1}(\\cdot)$ ; we write $s_{2}(\\cdot),s_{3}(\\cdot),\\ldots$ for the remaining singular values, ordered descendingly. For vectors $u,v$ of matching dimension, $\\langle u,v\\rangle=u^{\\mathsf{T}}v$ denotes their usual inner product. We write $\\mathbb{B}_{2}^{d}$ for the 2-ball in $\\mathbb{R}^{d}$ , $\\mathbb{S}_{2}^{d-1}=\\partial\\mathbb{B}_{2}^{d}$ for its surface, the $(d-1)$ -sphere, and $H_{u}$ for the closed half-space $\\{v\\in\\mathbb{R}^{d}\\colon\\langle u,v\\rangle\\geq1\\}$ . ", "page_idx": 1}, {"type": "text", "text": "Probability We work on a probability space with probability measure $\\mathbb{P}$ , and denote the corresponding expectation operator by $\\mathbb{E}$ . We write $\\sigma(\\cdot)$ for the $\\sigma$ -algebra generated by the argument. For an event $A$ , we write ${\\bf1}[A]$ for the indicator function of $A$ . ", "page_idx": 1}, {"type": "text", "text": "Uniform distribution We write $\\mathcal{U}(B)$ for the uniform distribution over a set $B$ (when well-defined), and $\\mathcal{U}(B)^{\\otimes m}$ for its $m$ -fold outer product; that is, $\\left(U_{1},\\ldots,U_{m}\\right)\\sim\\mathcal{U}(B)^{\\otimes m}$ are $m$ i.i.d. random variables with common law $\\mathcal{U}(B)$ . ", "page_idx": 1}, {"type": "text", "text": "2.1 Problem setting: stochastic linear bandits ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We consider the standard stochastic linear bandit setting. At each step $t\\in\\mathbb{N}^{+}$ , a learner selects an action $X_{t}$ from an action set $\\mathcal{X}$ , a compact subset of $\\mathbb{B}_{2}^{d}$ , and receives a random reward $Y_{t}\\in\\mathbb{R}$ that obeys the following assumption: ", "page_idx": 1}, {"type": "text", "text": "Assumption 1. At each time step $t\\in\\mathbb{N}^{+}$ , the agent receives a reward of the form $Y_{t}=\\left<X_{t},\\theta^{\\star}\\right>+Z_{t}$ with $\\theta^{\\star}\\in\\ensuremath{\\mathbb{B}}_{2}^{d}\\,a$ fixed instance parameter and $Z_{t}$ a random variable satisfying ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\exp\\{s Z_{t}\\}\\mid\\sigma(\\mathcal{F}_{t-1}\\cup\\mathcal{A}_{t}\\cup\\sigma(X_{t}))]\\le\\exp\\{s^{2}/2\\},\\quad\\forall s\\in\\mathbb{R},\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "almost surely, where $\\mathcal{F}_{t-1}=\\sigma(X_{1},Y_{1},\\ldots,X_{t-1},Y_{t-1})$ and $\\boldsymbol{A}_{t}$ is the $\\sigma$ -algebra generated by the random variables used by the learner to select its actions up to and including time $t$ . ", "page_idx": 1}, {"type": "text", "text": "The learner is given a horizon $T\\in\\mathbb{N}^{+}$ and its performance is evaluated by the (pseudo-)regret, $R(T)$ , incurred for the first $T$ steps, defined as ", "page_idx": 1}, {"type": "equation", "text": "$$\nR(T)=\\operatorname*{max}_{x\\in\\mathcal{X}}\\sum_{t=1}^{T}\\langle x-X_{t},\\theta^{\\star}\\rangle\\,,\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "The smaller the regret, the better the lea\u221arner. Under our assumptions, up to logarithmic factors, the regret of the best learners is of order $d\\sqrt{T}$ (Chapters 19\u201324, Lattimore and Szepesv\u00e1ri, 2020). Our ", "page_idx": 1}, {"type": "text", "text": "main result will show that if a learner follows the upcoming ensemble sampling algorithm to select the actions $X_{1},\\ldots,X_{T}$ , the regret it incurs will satisfy a similar high probability bound, with a slightly worse dependence on the dimension $d$ . ", "page_idx": 2}, {"type": "text", "text": "2.2 Algorithm: linear ensemble sampling ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Linear ensemble sampling, listed as Algorithm 1, proceeds as follows. At the outset, we fix an ensemble size $m\\,\\in\\,\\bar{\\mathbb{N}}^{+}$ , a regularisation parameter $\\lambda\\,>\\,0$ and a sequence of perturbation scale parameters $r_{0},r_{1},r_{2},\\ldots>0$ . Then, before the start of each round $t\\in\\bar{\\mathbb{N}}^{+}$ , linear ensemble sampling computes $m+1\\,d$ -dimensional vectors. The first of these is the usual ridge regression estimate of $\\theta^{\\star}$ , ", "page_idx": 2}, {"type": "equation", "text": "$$\n{\\hat{\\theta}}_{t-1}=V_{t-1}^{-1}\\sum_{i=1}^{t-1}X_{i}Y_{i}\\quad{\\mathrm{where}}\\quad V_{t-1}=V_{0}+\\sum_{i=1}^{t-1}X_{i}X_{i}^{\\top}\\quad{\\mathrm{and}}\\quad V_{0}=\\lambda I\\,.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The remaining $m$ parameter vectors, which shall serve as perturbations, are of the form ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\tilde{\\theta}_{t-1}^{j}=V_{t-1}^{-1}\\biggl(S_{0}^{j}+\\sum_{i=1}^{t-1}X_{i}U_{i}^{j}\\biggr)\\quad\\mathrm{for}\\;\\mathrm{each}\\quad j\\in[m]\\,,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $S_{0}^{j}\\in\\mathbb{R}^{d}$ is a random initialisation, and is taken to be uniform on $\\lambda\\sqrt{d}\\mathbb{S}_{2}^{d-1}$ , and $U_{1}^{j},U_{2}^{j},\\dots$ are random targets, uniform on the interval $[-1,1]$ . All of these random variables are independent of the past at the time they are sampled and across the $m$ -many replications. The algorithm then selects a random index $J_{t}\\in[m]$ and sign $\\xi_{t}\\in\\{\\pm1\\}$ , both uniformly distributed over their respective ranges, computes the perturbed parameter ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\theta_{t}=\\hat{\\theta}_{t-1}+r_{t-1}\\xi_{t}\\tilde{\\theta}_{t-1}^{J_{t}}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "and selects an action ", "page_idx": 2}, {"type": "equation", "text": "$$\nX_{t}\\in\\underset{x\\in\\mathcal{X}}{\\arg\\operatorname*{max}}\\langle x,\\theta_{t}\\rangle\\,,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "with ties dealt with in a measurable way. All in all, the vectors ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\hat{\\theta}_{t-1}\\pm r_{t-1}\\tilde{\\theta}_{t-1}^{1},\\dots,\\hat{\\theta}_{t-1}\\pm r_{t-1}\\tilde{\\theta}_{t-1}^{m}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "serve as an ensemble of $2m$ -many perturbed estimates of $\\theta^{\\star}$ , and in each round the algorithm acts optimally with respect to one of these, selected at uniformly at random. ", "page_idx": 2}, {"type": "text", "text": "Our linear ensemble sampling algorithm deviates in two ways from that of Lu and Van Roy (2017): ", "page_idx": 2}, {"type": "text", "text": "1. The random sequence of targets used to fit the ensembles in our algorithm consists of uniform random variables, rather than Gaussians, as used in previous literature. This choice simplifies the proofs, but our results hold (up to constant factors) for any suitable subgaussian targets, including Gaussian. We return to this in Remark 12.   \n2. We symmetrise our perturbations using Rademacher random variables, which does not feature in previous formulations of the ensemble sampling algorithm. This helps to create a more uniform distribution of the perturbations $(\\tilde{\\theta}_{t}^{j})_{t}$ around zero with minimal computational overhead, and is important for our proof technique. ", "page_idx": 2}, {"type": "text", "text": "In Appendix A, we provide reformulation of our linear ensemble sampling algorithm that is more in line with the style of presentation of the algorithm given by Lu and Van Roy (2017). ", "page_idx": 2}, {"type": "text", "text": "2.3 Regret bound for linear ensemble sampling ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Our advertised result is captured in the following theorem. To state the theorem we need the sequence ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\beta_{t}^{\\delta}=\\sqrt{\\lambda}+\\sqrt{2\\log(1/\\delta)+\\log(\\operatorname*{det}(V_{t})/\\lambda^{d})}\\,,\\quad t\\in\\mathbb{N}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Here and in related quantities, the superscripted $\\delta$ expresses the dependence on a $\\delta\\in(0,1]$ . ", "page_idx": 2}, {"type": "text", "text": "Algorithm 1 Linear ensemble sampling ", "page_idx": 3}, {"type": "text", "text": "Input regularisation param\u221aeter $\\lambda>0$ , ensemble size $m\\in\\mathbb{N}^{+}$ , perturbation scales $r_{0},r_{1},\\dots>0$   \nSample $(S_{0}^{j})_{j\\in[m]}\\sim\\mathcal{U}(\\lambda\\sqrt{d}\\mathbb{S}_{2}^{d-1})^{\\otimes m}$   \nLet $V_{0}=\\lambda I$ , $\\hat{\\theta}_{0}=0$ , and let $\\tilde{\\theta}_{0}^{j}=V_{0}^{-1}S_{0}^{j}$ for each $j\\in[m]$ ", "page_idx": 3}, {"type": "text", "text": "for $t\\in\\mathbb{N}^{+}$ do ", "page_idx": 3}, {"type": "text", "text": "Sample $(\\xi_{t},J_{t})\\sim\\mathcal{U}(\\{\\pm1\\}\\times[m])$ and let $\\theta_{t}=\\hat{\\theta}_{t-1}+r_{t-1}\\xi_{t}\\tilde{\\theta}_{t-1}^{J_{t}}$ Compute an $X_{t}\\in\\arg\\operatorname*{max}_{x\\in\\mathcal{X}}\\langle x,\\theta_{t}\\rangle$ , play action $X_{t}$ and receive reward $Y_{t}$ Sample $(U_{t}^{j})_{j\\in[m]}\\sim\\mathcal{U}([-1,1])^{\\otimes m}$ and let $S_{t}^{j}=S_{t-1}^{j}+U_{t}^{j}X_{t}$ for each $j\\in[m]$ Let $V_{t}=V_{t-1}+X_{t}X_{t}^{\\mathsf{T}}$ , $\\begin{array}{r}{\\hat{\\theta}_{t}=V_{t}^{-1}\\sum_{i=1}^{t}Y_{i}X_{i}}\\end{array}$ , and let $\\tilde{\\theta}_{t}^{j}=V_{t}^{-1}S_{t}^{j}$ for each $j\\in[m]$ ", "page_idx": 3}, {"type": "text", "text": "Theorem 1. Fix $\\delta\\in(0,1]$ and take $r_{t}=7\\beta_{t}^{\\delta}$ for all $t\\in\\mathbb{N},$ , $\\lambda\\geq5$ and $m\\geq400\\log(N T/\\delta)$ for $N=(134\\sqrt{1+T/\\lambda})^{d}$ . Then there exist a universal constant $C>0$ such that, with probability at least $1-\\delta$ , the regret incurred by a learner following linear ensemble sampling with these parameters in our stochastic linear bandit setting (formalised in Assumption $^{\\,l}$ ) is bounded as ", "page_idx": 3}, {"type": "equation", "text": "$$\nR(\\tau)\\leq C m^{3/2}\\beta_{\\tau-1}^{\\delta}\\left(\\sqrt{d\\tau\\log(1+\\tau/(\\lambda d))}+\\sqrt{(\\tau/\\lambda)\\log(\\tau/(\\lambda\\delta))}\\right)\\quad f o r\\,a l l\\quad\\tau\\in[T]\\,.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Remark 1. If $\\delta\\geq1/T^{\\alpha}$ for some $\\alpha>0$ and we take $\\lambda=5$ and m to be as small as possible given the constraint of Theorem $^{\\,I}$ , then ", "page_idx": 3}, {"type": "equation", "text": "$$\nm\\leq C_{\\alpha}d\\log T\\quad a n d\\quad R(T)\\leq C_{\\alpha}^{\\prime}(d\\log T)^{5/2}{\\sqrt{T}}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "for some constants $C_{\\alpha},C_{\\alpha}^{\\prime}>0$ that depend on $\\alpha$ only and where the bound on the regret holds with probability $1-\\delta$ . That is, for polynomially sized conf\u221aidence parameters, the ensemble size scales linearly with $d\\log(T)$ , while the regret scales with $d^{5/2}\\sqrt{T}$ up to logarithmic-in- $\\cdot\\!T$ factors. The latter scaling is slightly wors\u221ae than that obtained for Thompson sampling (cf. Theorem 17), where the regret scales with $d^{3/2}{\\sqrt{T}}$ , again, up to logarithmic-in- $\\cdot T$ factors. ", "page_idx": 3}, {"type": "text", "text": "Remark 2. Theorem 1 does not recover the expected behaviour for large ensemble sizes; that is, as $m\\rightarrow\\infty$ . On one hand, this is not an issue: we are interested in the practical regime where the ensemble size is small. On the other, it suggests that better analysis might be possible. In Remark 10, we discuss a potential looseness in our analysis, which, if addressed, would result in the removal of a factor of m from the bound, thus bringing the regret in line with that of Thompson sampl\u221aing. The technique that would be required to do so could then also be used to change the remaining $\\sqrt{m}$ dependence to an order $\\sqrt{d\\log T/\\delta}$ dependence, thus recovering a bound with the right dependence on m. However, as discussed in Remark $I O$ , such improvements, if possible, may be hard to attain. ", "page_idx": 3}, {"type": "text", "text": "Remark 3. Our ensemble sampling algorithm requires the ensemble size m to be fixed in advance. As m depends on the horizon $T$ , the method only provides guarantees for a fixed, finite horizon $T$ , and our regret bound has a direct dependence on $T$ . The doubling trick would be a simple, but somewhat unsatisfactory way of removing this dependence. An alternative is to grow the ensemble online. However, a naive implementation of growing the ensemble size would require storing all past observations and computation per time step also growing with time, which is counter to the idea of a fast incremental method. ", "page_idx": 3}, {"type": "text", "text": "Remark 4. In light of Theorem 1, linear ensemble sampling might be seen as a less effective and less computationally efficient version of linear Thompson sampling. This is correct: in the linear setting, where Thompson sampling may be implemented in closed form, ensemble sampling has no clear advantages. With that said, variants of ensemble sampling and related bootstrapping methods are popular in more complex settings where closed forms are not available (say, deep reinforcement learning, per Osband et al., 2016, 2018), and the linear setting provides an important testing ground for the soundness of these algorithms. ", "page_idx": 3}, {"type": "text", "text": "2.4 Comparison to related results ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We now relate our result to those of Lu and Van Roy (2017) and Qin et al. (2022): ", "page_idx": 3}, {"type": "text", "text": "Remark 5. The work of Lu and Van Roy (2017) makes strong claims on the frequentist regret of linear ensemble sampling. However, as pointed out by Qin et al. (2022), and confirmed by Lu and Van Roy (2017) in their updated arxiv manuscript, the analysis of Lu and Van Roy (2017) is flawed. ", "page_idx": 3}, {"type": "text", "text": "Remark 6. The only correct result on the regret of linear ensemble sampling is by Qin et al. (2022), which gives that for a $d$ -dimensional linear bandit with an action set $\\mathcal{X}$ of cardinality $K$ , the Bayesian regret incurred\u2014that is, average regret for $\\theta^{\\star}\\sim\\mathcal{N}(0,I_{d})$ \u2014is bounded as ", "page_idx": 4}, {"type": "equation", "text": "$$\nB R(T)\\leq C\\sqrt{d T\\log K}+C T\\sqrt{\\frac{K\\log(m T)}{m}}(d\\wedge\\log K)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "for some universal constant $C>0$ . Observe that this bound needs an ensemble size linear in $T$ for Bayesian regret that scales as $\\sqrt{T}$ (up to constant and polylogarithmic factors), which largely defeats the purpose of ensemble sampling. Furthermore, the ensemble size m needs to scale linearly with $K$ to get a $\\log K$ overall dependence on $K$ . Therefore, to tackle a bandit with $\\boldsymbol{\\mathcal{X}}=\\mathbb{B}_{2}^{d}$ using the bound of Qin et al. (2022) and discretisation, since order $2^{d-1}$ -many actions would be needed to guarantee a small discretisation error, the ensemble size m would need to scale exponentially in $d$ . ", "page_idx": 4}, {"type": "text", "text": "The results of Lu and Van Roy (2017) and Qin et al. (2022) form the entirety of the previous analysis of ensemble sampling in any structured setting. In light of that, we believe that our result is the first to begin justifying the practical effectiveness of ensemble sampling. ", "page_idx": 4}, {"type": "text", "text": "3 Analysis of linear ensemble sampling ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Our analysis of ensemble sampling will be based on a master theorem, presented in Section 3.1 and proven in Appendix B. The master theorem provides a method for obtaining regret bound for Thompson-sampling-like randomised algorithms. Thereafter, in Section 3.2, we apply the said master theorem to linear ensemble sampling, using some intermediate results proven in Sections 3.3 and 3.4, and some routine calculations that have been deferred to the appendix. Additionally, in Appendix D, we validate our master theorem by demonstrating that it recovers a previous result for a more classical Thompson-sampling-type algorithm. ", "page_idx": 4}, {"type": "text", "text": "3.1 Master Theorem: a regret bound for optimistic randomised algorithms ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Our analysis of ensemble sampling will rely on the revered principle of optimism. To make this precise, consider a fixed instance parameter $\\bar{\\theta}^{\\star}\\in\\mathbb{R}^{d}$ . Writing $J(\\theta)=\\operatorname*{max}_{x\\in\\mathcal{X}}\\langle x,\\theta\\rangle$ , we call ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\Theta^{\\mathrm{OPT}}=\\{\\theta\\in\\mathbb{R}^{d}\\colon J(\\theta)\\geq J(\\theta^{\\star})\\}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "the set of parameters optimistic for $\\theta^{\\star}$ . We now present a \u2018master theorem\u2019 that bounds the regret of any algorithm that chooses actions based on a randomly chosen parameter in terms of the probabilities that, given the past, the algorithm samples a parameter that falls in the intersection of ellipsoidal confidence sets and the optimistic region $\\Theta^{\\mathrm{OPT}}$ . This result generalises a similar theorem stated for Thompson sampling by Abeille and Lazaric (2017), extending it to allow for finitely supported perturbations (critical for the proof of Theorem 1) and for a finer control over the way dependencies between time-steps are handled. ", "page_idx": 4}, {"type": "text", "text": "Theorem 2 (Master regret bound). Fix $T\\in\\mathbb{N}^{+}\\cup\\{+\\infty\\},$ , $\\delta\\in(0,1]$ , $\\lambda\\geq1$ , and let $(V_{t},\\hat{\\theta}_{t})_{t}$ be defined per equation (1). Let Assumption $^{\\,l}$ hold, recalling the filtrations $(\\mathcal{F}_{t})_{t}$ and $(\\mathbf{\\mathcal{A}}_{t})_{t}$ defined therein, and let $(\\mathbf{\\mathcal{A}}_{t}^{\\prime})_{t}$ be any filtration satisfying ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\boldsymbol{A}_{t-1}\\subset\\mathcal{A}_{t-1}^{\\prime}\\subset\\mathcal{A}_{t}\\,,\\quad\\forall t\\in\\mathbb{N}^{+}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Let $(b_{t})_{t\\in\\mathbb{N}}$ be a sequence of $\\sigma(\\mathcal{F}_{t}\\cup\\mathcal{A}_{t}^{\\prime})_{t}$ -adapted nonnegative random variables, and define the ellipsoids ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Theta_{t}=\\hat{\\theta}_{t}+b_{t}V_{t}^{-1/2}\\mathbb{B}_{2}^{d}\\,,\\quad t\\in\\mathbb{N}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Let $(\\theta_{t})_{t}$ be $\\iota\\,(\\sigma(\\mathcal{F}_{t}\\cup\\mathcal{A}_{t}))_{t}$ -adapted $\\mathbb{R}^{d}$ -valued sequence and suppose that ", "page_idx": 4}, {"type": "equation", "text": "$$\nX_{t}\\in\\arg\\operatorname*{max}_{x\\in\\mathcal{X}}\\langle x,\\theta_{t}\\rangle\\,,\\quad\\forall t\\in[T]\\,.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Suppose further that there exist events $\\mathcal{E}_{T}$ and $\\mathcal{E}_{T}^{\\star}$ satisfying ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{E}_{T}\\subset\\cap_{t=1}^{T}\\{\\theta_{t}\\in\\Theta_{t-1}\\}\\,,\\quad\\mathcal{E}_{T}^{\\star}\\subset\\cap_{t=1}^{T}\\{\\theta^{\\star}\\in\\Theta_{t-1}\\}\\quad a n d\\quad\\mathbb{P}(\\mathcal{E}_{T})\\wedge\\mathbb{P}(\\mathcal{E}_{T}^{\\star})\\geq1-\\delta\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Then, writing ", "page_idx": 4}, {"type": "equation", "text": "$$\np_{t-1}=\\mathbb{P}(\\theta_{t}\\in\\Theta^{\\mathrm{OPT}}\\cap\\Theta_{t-1}\\mid\\sigma(\\mathcal{F}_{t-1}\\cup\\mathcal{A}_{t-1}^{\\prime}))\\,,\\quad t\\in\\mathbb{N}^{+}\\,,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "we have that on a subset of $\\mathcal{E}_{T}\\cap\\mathcal{E}_{T}^{\\star}$ of probability at least 1 \u22123\u03b4, for all $\\tau\\in[T]$ , ", "page_idx": 5}, {"type": "equation", "text": "$$\nR(\\tau)\\le2\\operatorname*{max}_{i\\in[\\tau]}\\frac{b_{i-1}}{p_{i-1}}\\left(2\\sqrt{2d\\tau\\log\\left(1+\\frac{\\tau}{d\\lambda}\\right)}+\\sqrt{2(4\\tau/\\lambda+1)\\log\\left(\\frac{\\sqrt{4\\tau/\\lambda+1}}{\\delta}\\right)}\\right)\\,.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "We defer the proof of Theorem 2 to Appendix B. ", "page_idx": 5}, {"type": "text", "text": "Remark 7. To apply the theorem, we need to show that our algorithm generates $(\\theta_{t})_{t}$ such that the probability of each $\\theta_{t}$ landing in $\\Theta^{\\mathrm{OPT}}\\cap\\Theta_{t-1}$ , conditional on $\\sigma(\\mathcal{F}_{t-1}\\cup\\mathcal{A}_{t-1}^{\\prime})$ , is bounded away from zero for all $t\\in[T]$ . We have two degrees of freedom in our analysis: ", "page_idx": 5}, {"type": "text", "text": "1. We can choose $(b_{t})_{t}$ , the widths of the ellipsoids $(\\Theta_{t})_{t}$ . Larger widths may make it easier to bound $(p_{t})_{t}$ away from zero, but at a linear cost in the regret bound. 2. We can choose $\\mathcal{A}_{t-1}^{\\prime}$ , the \u2018point\u2019 between observing $Y_{t-1}$ and selecting $\\theta_{t}$ with respect to which we consider the aforementioned conditional probabilities defined. ", "page_idx": 5}, {"type": "text", "text": "Remark 8. The introduction of $\\mathcal{A}_{t-1}^{\\prime}$ serves to model the case where $\\theta_{t}$ is sampled from a distribution $P_{t}$ that itself depends on $X_{1},Y_{1},\\ldots,X_{t-1},Y_{t-1}$ in a random manner. The $\\sigma$ -algebra $\\mathcal{A}_{t-1}^{\\prime}$ is then such that $P_{t}$ is $\\sigma(\\mathcal{F}_{t-1}\\cup\\mathcal{A}_{t-1}^{\\prime})$ -measurable. ", "page_idx": 5}, {"type": "text", "text": "The upcoming two lemmas will be helpful our applications of the Master Theorem; both are stated in terms of the random functions $\\psi_{t}\\colon\\ensuremath{\\mathbb{R}}^{d}\\to\\ensuremath{\\mathbb{R}}^{d}$ given by ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\psi_{t}^{\\delta}(u)=\\hat{\\theta}_{t}+\\beta_{t}^{\\delta}V_{t}^{-1/2}u,\\quad t\\in\\mathbb{N}\\,.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The first is the classic concentration result of Abbasi-Yadkori et al. (2011), and the second, Proposition 5 of Abeille and Lazaric (2017). ", "page_idx": 5}, {"type": "text", "text": "Lemma 3. Under Assumption $^{\\,I}$ , for any $\\begin{array}{r}{\\delta\\in(0,1],\\mathbb{P}(\\forall t\\in\\mathbb{N},\\,\\theta^{\\star}\\in\\psi_{t}^{\\delta}(\\mathbb{B}_{2}^{d}))\\geq1-\\delta.}\\end{array}$ ", "page_idx": 5}, {"type": "text", "text": "Lemma 4. If Assumption $^{\\,I}$ holds and ${\\theta}^{\\star}\\in\\psi_{t}^{\\delta}(\\mathbb{B}_{2}^{d})$ , then for any measure $Q$ over $\\mathbb{R}^{d}$ and $a>0$ , ", "page_idx": 5}, {"type": "equation", "text": "$$\nQ(\\Theta^{\\mathrm{OPT}}\\cap\\psi_{t}^{\\delta}(a\\mathbb{B}_{2}^{d}))\\geq\\operatorname*{inf}_{u\\in\\mathbb{S}_{2}^{d-1}}Q(\\psi_{t}^{\\delta}(H_{u}\\cap a\\mathbb{B}_{2}^{d}))\\,,\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where we recall that $H_{u}$ denotes the closed halfspace $\\{v\\in\\mathbb{R}^{d}\\colon\\langle v,u\\rangle\\geq1\\}$ . ", "page_idx": 5}, {"type": "text", "text": "We provide a short, clean proof of Lemma 4 in Appendix C. ", "page_idx": 5}, {"type": "text", "text": "3.2 Proof of Theorem 1: regret bound for linear ensemble sampling ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Let $\\Gamma_{0},\\Gamma_{1},\\ldots$ be the sequence of random matrices in $\\mathbb{R}^{d\\times m}$ with the $j$ th column given by ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\Gamma_{t}^{j}=V_{t}^{-1/2}S_{t}^{j}\\,,\\quad\\mathrm{such\\,that}\\quad\\tilde{\\theta}_{t-1}^{j}=V_{t-1}^{-1/2}\\Gamma_{t-1}^{j}\\,.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Recall that $\\begin{array}{r}{S_{t}^{j}\\,=\\,S_{0}^{j}+\\sum_{i=1}^{t}U_{i}^{j}X_{i}}\\end{array}$ and observe that each $(S^{j})_{t}$ is a vector-valued random walk with increments given b y $({\\bar{U}}_{t}{\\bar{X}}_{t})_{t}$ , where $(X_{t})_{t}$ is a serially correlated vector-valued sequence and $(U_{t}^{j})_{t}$ is a sequence of independent $\\mathcal{U}([-1,1])$ variables, and $(\\Gamma_{t}^{j})_{t}$ is a normalised version of $(S_{t}^{j})_{t}$ . Consider the extremal singular values of $\\Gamma_{t}$ , ", "page_idx": 5}, {"type": "equation", "text": "$$\ns_{d}(\\Gamma_{t})=\\operatorname*{min}_{u\\in\\mathbb S_{2}^{d-1}}\\|\\Gamma_{t}^{\\mathsf T}u\\|_{2}=\\operatorname*{min}_{u\\in\\mathbb S_{2}^{d-1}}\\bigg(\\sum_{j=1}^{m}\\langle\\Gamma_{t}^{j},u\\rangle^{2}\\bigg)^{\\frac12}\\quad\\mathrm{and}\\quad s_{1}(\\Gamma_{t})=\\|\\Gamma_{t}^{\\mathsf T}\\|=\\operatorname*{max}_{u\\in\\mathbb S_{2}^{d-1}}\\|\\Gamma_{t}^{\\mathsf T}u\\|_{2}\\,.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The lower of these may be interpreted as capturing how well the columns $\\Gamma_{t}^{1},\\dots\\Gamma_{t}^{m}$ cover all directions $u\\in\\mathbb{S}_{2}^{d-1}$ , and the upper, their maximal deviations from zero. The following theorem, proven in Section 3.3, shows that, for a suffic\u221aiently large $m$ , with high probability, for all $t\\in[T]$ , the singular values of $\\Gamma_{t-1}$ are on the order of $\\sqrt{m}$ . ", "page_idx": 5}, {"type": "text", "text": "Theorem 5. For $\\lambda\\geq5$ and $m\\geq400\\log(3+T)\\vee10d,$ the event ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{E}_{T}=\\{\\forall t\\in[T],\\ \\sqrt{m}/7\\leq s_{d}(\\Gamma_{t-1})\\leq s_{1}(\\Gamma_{t-1})\\leq10\\sqrt{m}/7\\}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "satisfies $\\mathbb{P}(\\mathcal{E}_{T})\\geq1-N T e^{-\\frac{m}{400}}$ , where $N=(134\\sqrt{1+T/\\lambda})^{d}$ . ", "page_idx": 5}, {"type": "text", "text": "Proof of Theorem $^{\\,l}$ . We will use the master theorem, Theorem 2. We let the filtrations $(\\mathbf{\\mathcal{A}}_{t})_{t}$ and $(\\mathbf{\\mathcal{A}}_{t}^{\\prime})_{t}$ needed in this theorem be defined recursively via ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{A}_{0}^{\\prime}=\\sigma(S_{0}^{1},\\ldots,S_{0}^{m}),\\,\\,\\,\\mathcal{A}_{t}=\\sigma(\\mathcal{A}_{t-1}^{\\prime}\\cup\\sigma(J_{t},\\xi_{t}))\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "With this, the conditions concerning these filtrations hold. ", "page_idx": 6}, {"type": "text", "text": "We let $b_{t}=a\\beta_{t}^{\\delta}$ for $a=10\\sqrt{m}$ , for each $t\\in\\mathbb{N}$ , which is $\\sigma(\\mathcal{F}_{t}\\cup A_{t}^{\\prime})_{t}$ -adapted, as required. For the two required events, we take the event $\\mathcal{E}_{T}$ from Theorem 5 and the event $\\mathcal{E}_{T}^{\\star}=\\cap_{t\\in\\mathbb{N}}\\{\\theta_{\\star}\\in\\psi_{t}(\\mathbb{B}_{2}^{d})\\}$ . To see that these satisfy the requirements given in equation (2), observe the following: ", "page_idx": 6}, {"type": "text", "text": "Conditions on $\\mathcal{E}_{T}$ By our assumptions on $m$ and $\\lambda$ , the conditions for Theorem 5 are satisfied. Hence, $\\mathbb{P}(\\mathcal{E}_{T})\\ge1-\\delta$ . We now show that on $\\mathcal{E}_{T}$ , for all $t\\in[T],\\theta_{t}\\in\\Theta_{t-1}$ . Note this is equivalent to the statement that on $\\mathcal{E}_{T}$ , for all $t\\in[T]$ $\\cap_{\\cdot}||\\xi_{t}\\Gamma_{t-1}^{J_{t}}||_{2}\\leq b_{t-1}/r_{t-1}=10\\sqrt{m}/7$ . And indeed, on $\\mathcal{E}_{T}$ , ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\xi_{t}\\Gamma_{t-1}^{J_{t}}\\|_{2}\\leq\\operatorname*{max}_{j}\\|\\Gamma_{t-1}^{j}\\|_{2}=s_{1}\\big(\\Gamma_{t-1}\\big)\\leq10\\sqrt{m}/7\\,,\\quad\\forall t\\in[T]\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Conditions on $\\mathcal{E}_{T}^{\\star}$ Since $a\\geq1$ , $\\psi_{t}({\\mathbb{B}}_{2}^{d})\\subset\\psi_{t}(a{\\mathbb{B}}_{2}^{d})=\\Theta_{t}$ , and thus, by Lemma 3, $\\theta_{\\star}\\in\\Theta_{t}$ for all $t\\in\\mathbb{N}$ jointly with probability at least $1-\\delta$ . ", "page_idx": 6}, {"type": "text", "text": "It remains to lower-bound the sequence $(p_{t})_{t}$ . For this, define $\\mathbb{P}_{t-1}^{\\prime}(\\cdot)=\\mathbb{P}(\\cdot\\mid\\sigma(\\mathcal{F}_{t-1}\\cup\\mathcal{A}_{t-1}^{\\prime}))$ ). Fixing $t\\in\\mathbb{N}^{+}$ , writing $Q(A)=\\mathbb{P}_{t-1}^{\\prime}(\\theta_{t}\\in A)$ , we have that on $\\mathcal{E}_{T}^{\\star}$ , ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{p_{t-1}=Q(\\Theta^{\\mathrm{OPT}}\\cap\\Theta_{t-1})}\\\\ &{\\qquad\\underset{u\\in\\mathbb{S}_{2}^{d-1}}{\\operatorname*{inf}}Q(\\psi_{t-1}^{\\delta}(H_{u}\\cap a\\mathbb{B}_{2}^{d}))}\\\\ &{\\qquad=\\underset{u\\in\\mathbb{S}_{2}^{d-1}}{\\operatorname*{inf}}\\mathbb{P}_{t-1}^{\\prime}(\\psi_{t-1}^{\\delta}(7\\xi_{t}\\Gamma_{t-1}^{J_{t}})\\in\\psi_{t-1}^{\\delta}(H_{u}\\cap a\\mathbb{B}_{2}^{d}))}\\\\ &{\\qquad=\\underset{u\\in\\mathbb{S}_{2}^{d-1}}{\\operatorname*{inf}}\\mathbb{P}_{t-1}^{\\prime}(7\\xi_{t}\\Gamma_{t-1}^{J_{t}}\\in H_{u}\\cap a\\mathbb{B}_{2}^{d})}\\\\ &{\\qquad=\\underset{u\\in\\mathbb{S}_{2}^{d-1}}{\\operatorname*{inf}}\\frac{1}{2m}\\underset{(s,j)\\in\\{\\pm1\\}\\times[m]}{\\sum}\\mathbf{1}[7s\\Gamma_{t-1}^{j}\\in H_{u}\\cap a\\mathbb{B}_{2}^{d}]\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where the last equality used the definitions of $\\mathbb{P}_{t-1}^{\\prime},\\mathcal{F}_{t-1}$ and $\\mathcal{A}_{t-1}^{\\prime}$ . ", "page_idx": 6}, {"type": "text", "text": "We now show that on $\\mathcal{E}_{T}$ , regardless of the choice of $u$ , for at least one $(s,j)\\;\\in\\;\\{\\pm1\\}\\,\\times\\,[m]$ , $\\mathsf{7}s\\Gamma_{t-1}^{j}\\in H_{u}\\cap a\\mathbb{B}_{2}^{d}$ . Assume thus that $\\mathcal{E}_{T}$ holds. First observe that for any $(s,j)$ , by equation (3), $7s\\Gamma_{t-1}^{j}\\in a\\mathbb{B}_{2}^{d}$ . Hence, it remains to show that for any $u$ , for some $(s,j)$ , $7s\\Gamma_{t-1}^{j}\\in H_{u}$ ; this holds because for any $u$ , ", "page_idx": 6}, {"type": "equation", "text": "$$\n1\\leq\\frac{s_{d}^{2}(7\\Gamma_{t-1})}{m}\\leq\\frac{1}{m}\\sum_{j=1}^{m}\\langle7\\Gamma_{t-1}^{j},u\\rangle^{2}\\leq\\operatorname*{max}_{j}\\langle7\\Gamma_{t-1}^{j},u\\rangle^{2}\\,.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Hence, on $\\mathcal{E}_{T}\\cap\\mathcal{E}_{T}^{\\star}$ , $p_{t-1}\\geq1/(2m)$ and thus $b_{t-1}/p_{t-1}\\leq20m^{3/2}\\beta_{t-1}^{\\delta}$ . Inserting this bound into the regret bound of Theorem 2 establishes the result. \u53e3 ", "page_idx": 6}, {"type": "text", "text": "Remark 9 (On symmetrisation). If the algorithm were run without symmetrisation, following the steps of the proof we see that we need to show that on $\\mathcal{E}_{T}$ , regardless the choice of $u$ , for at least one of $j\\in[m],\\,7\\Gamma_{t-1}^{j}\\in H_{u}$ . In the presence of symmetrisation, the equivalent statement is that regardless of the choice of $\\dot{\\boldsymbol{u}}_{i}$ , for at least one \u2018particle\u2019 $j$ , we have either $7\\Gamma_{t-1}^{\\bar{\\jmath}}\\in H_{u}$ or $7\\Gamma_{t-1}^{j}\\in H_{-u}$ . Through equation (4), the latter reduces to studying quadratic forms, which lead to convenient algebra. ", "page_idx": 6}, {"type": "text", "text": "maxj\u27e87\u0393tj\u22121, u\u27e92 by the averagem1 jm=1\u27e87\u0393tj\u22121, u\u27e92, whicSh2 we show exceeds 1, thus establish- Remark 10 (Can we improve our bound?). For any ing the existence of at least one element $\\pm7\\Gamma_{t-1}^{j}$ in $H_{u}$ $u\\,\\in\\,\\mathbb{S}_{2}^{d-1}$ (out of , we lower bound the maximum ). This gives a $1/(2m)$ lower bound for $p_{t-1}$ . However, lower bounding a maximum by an average might be wasteful. If, instead, we managed to lower bound the median (or any other fixed-proportion order statistic) of $\\langle7\\Gamma_{t-1}^{1},u\\rangle^{2},\\dots,\\langle7\\Gamma_{t-1}^{m},u\\rangle^{2}$ , we would be able to conclude that $p_{t}$ is lower bounded by a constant (since a constant proportion of \u2018particles\u2019 are then contained in $H_{\\pm u.}$ ), and thus conclude that linear ensemble sampling performs similarly to Thompson sampling (see Appendix $D$ for an analysis of the latter). However, order statistics are considerably more technical to work with than averages, and we do not currently know if such a result is attainable. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "3.3 Setting up to prove Theorem 5: bound on singular values ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Theorem 5 for $t=0$ follows by standard methods (see Appendix E for proof): Lemma 6. Whenever $m\\geq10d$ , ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left({\\sqrt{m/2}}\\leq s_{d}(\\Gamma_{0})\\leq s_{1}(\\Gamma_{0})\\leq{\\sqrt{3m/2}}\\right)\\geq1-e^{-m/24}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "To extend the result to $t\\,>\\,0$ , we will consider the processes $(R_{t}^{j}(u))_{t}$ and $(R_{t}(u))_{t}$ defined for $u\\in\\mathbb{R}^{d}$ , $u\\ne0$ by ", "page_idx": 7}, {"type": "equation", "text": "$$\nR_{t}^{j}(u)=\\frac{\\langle u,S_{t}^{j}\\rangle^{2}}{\\|u\\|_{V_{t}}^{2}}\\quad\\mathrm{and}\\quad R_{t}(u)=\\frac{1}{m}\\sum_{j}R_{t}^{j}(u).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Note that for $v\\,=\\,V_{t}^{1/2}u\\,\\ne0$ one has $R_{t}^{j}(u)=\\langle v,\\Gamma_{t}^{j}\\rangle^{2}/\\|v\\|^{2}$ . Since $V_{t}$ is positive-definite (and hence a bijection when viewed as a linear map) we observe the following relations: ", "page_idx": 7}, {"type": "text", "text": "Claim 7. For all $t\\geq0,\\,j\\in[m],$ , ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{u\\neq0}R_{t}^{j}(u)=\\operatorname*{sup}_{v\\neq0}\\frac{\\langle v,\\Gamma_{t}^{j}\\rangle^{2}}{\\|v\\|_{2}^{2}}=\\|\\Gamma_{t}^{j}\\|_{2}^{2}\\quad a n d\\quad\\operatorname*{sup}_{u\\neq0}R_{t}(u)=\\operatorname*{sup}_{v\\neq0}\\frac{\\|\\Gamma_{t}^{\\top}v\\|_{2}^{2}}{m\\|v\\|_{2}^{2}}=\\frac{s_{1}^{2}(\\Gamma_{t})}{m}\\,,\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "and likewise $\\begin{array}{r}{\\operatorname*{inf}_{u\\neq0}R_{t}(u)=s_{d}^{2}(\\Gamma_{t})/m}\\end{array}$ ", "page_idx": 7}, {"type": "text", "text": "In the upcoming section (Section 3.4) we establish the following bound on $R_{t}(u)$ for a fixed $u\\in\\mathbb{S}_{2}^{d-1}$ ", "page_idx": 7}, {"type": "text", "text": "Lemma 8. Fix $u\\in\\mathbb{S}_{2}^{d-1}$ and $\\lambda\\geq5$ . Suppose that $m\\geq400\\log(3+2T)$ . Then, there exists an event $\\mathcal{E}_{T}^{\\prime}$ with $\\mathbb{P}(\\mathcal{E}_{T}^{\\prime})\\geq1-T e^{-\\frac{m}{400}}$ such that on $\\begin{array}{r}{\\mathcal{E}_{T}^{\\prime}\\cap\\{\\frac{1}{2}\\leq R_{0}(u)\\leq\\frac{3}{2}\\}}\\end{array}$ , ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\frac{9}{100}\\leq R_{t}(u)\\leq\\frac{5}{3}\\,,\\quad\\forall t\\in[T]\\,.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Our proof of Theorem 5 employs the above pointwise bound together with a covering argument over $\\mathbb{S}_{2}^{d-\\bar{1}}$ .  sFtaonr dtharatd,  bwoeu nnede od nt hthe ef oslilzoe woifn -Lnieptss cohfi s (rLeesumlt,m pa r4o.v1e0n  iinn  PAispipeer,n d1i9x9 F9 )(.by simple algebra), $\\epsilon$ $\\mathbb{S}_{2}^{d-1}$ ", "page_idx": 7}, {"type": "text", "text": "Lemma 9. On $\\mathbb{S}_{2}^{d-1}$ , $R_{t}$ is $L$ -Lipschitz with $L\\leq4\\|\\Gamma_{t}\\|^{2}\\|V_{t}^{1/2}\\|/(m\\sqrt{\\lambda}).$ ", "page_idx": 7}, {"type": "text", "text": "Lemma 10. For all $\\epsilon$ in $(0,1],$ , there exists an \u03f5-net $\\mathcal{N}$ of $\\mathbb{S}_{2}^{d-1}$ with |N| \u2264 1 + \u03f52 d. ", "page_idx": 7}, {"type": "text", "text": "Proof of Theorem 5. Let $\\mathcal{N}_{\\epsilon}$ be a minimal $\\epsilon_{}$ -net of $\\mathbb{S}_{2}^{d-1}$ and define the event ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathcal{E}_{\\epsilon}=\\left\\{\\forall v\\in\\mathcal{N}_{\\epsilon},\\,\\forall t\\in[T],\\,\\frac{9}{100}\\leq R_{t}(v)\\leq\\frac{5}{3}\\right\\}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "We will now confirm that, for a suitable choice of $\\epsilon$ , $\\mathcal{E}_{\\epsilon}$ is a subset of the event in Theorem 5, and that $\\mathbb{P}(\\mathcal{E}_{\\epsilon})\\geq1-N T e^{-\\frac{m}{400}}$ , which establishes the theorem. ", "page_idx": 7}, {"type": "text", "text": "L\u221aet $u~\\in~\\mathbb{S}_{2}^{d-1}$ and $v~\\in~{\\mathcal{N}}_{\\epsilon}$ be such that $\\|u-v\\|_{2}\\,\\leq\\,\\epsilon$ . From Lemma 9, and choosing $\\epsilon=$ $\\sqrt{\\lambda}/(132\\sqrt{T+\\lambda})$ , and noting that $\\|V_{t}^{1/2}\\|\\leq\\sqrt{T+\\lambda}$ , we get ", "page_idx": 7}, {"type": "equation", "text": "$$\n|R_{t}(u)-R_{t}(v)|\\le\\frac{4\\|\\Gamma_{t}\\|^{2}\\|V_{t}^{1/2}\\|}{m\\sqrt{\\lambda}}\\epsilon\\le\\frac{\\|\\Gamma_{t}\\|^{2}}{33m}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Then on $\\mathcal{E}_{\\epsilon}$ , for our choice of $\\epsilon$ , ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\|\\Gamma_{t}\\|^{2}=m\\operatorname*{sup}_{u\\neq0}R_{t}(u)\\leq m\\operatorname*{sup}_{v\\in\\mathcal{N}_{\\epsilon}}R_{t}(v)+\\frac{\\|\\Gamma_{t}\\|^{2}}{33}\\leq\\frac{5}{3}m+\\frac{\\|\\Gamma_{t}\\|^{2}}{33}\\,,\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Solving for $\\|\\boldsymbol{\\Gamma}_{t}\\|^{2}$ , we have that $\\begin{array}{r}{\\|\\Gamma_{t}\\|^{2}=s_{1}^{2}(\\Gamma_{t})\\leq\\frac{165}{96}m}\\end{array}$ . The same argument also gives that, on $\\mathcal{E}_{\\epsilon}$ , ", "page_idx": 8}, {"type": "equation", "text": "$$\ns_{d}^{2}(\\Gamma_{t})\\geq m\\operatorname*{inf}_{v\\in\\mathcal{N}_{\\epsilon}}R_{t}(v)-\\frac{\\|\\Gamma_{t}\\|^{2}}{33}\\geq\\frac{9}{100}m-\\frac{\\|\\Gamma_{t}\\|^{2}}{33}\\geq\\frac{3}{100}m.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Loosening these bounds slightly, we have that on $\\mathcal{E}_{\\epsilon}$ $\\dot{\\bar{\\zeta}}_{\\epsilon},\\sqrt{m}/7\\leq s_{d}(\\Gamma_{t})\\leq s_{1}(\\Gamma_{t})\\leq10\\sqrt{m}/7.$ ", "page_idx": 8}, {"type": "text", "text": "The probability that $\\mathcal{E}_{\\epsilon}$ occurs is at least the probability that the event of Lemma 6 occurs and that the event of Lemma 8 occurs for each $u\\in\\mathcal N_{\\epsilon}$ , noting that the former ensures $\\begin{array}{r}{\\frac12\\leq R_{0}(u)\\leq\\frac32}\\end{array}$ for all $u\\in\\mathcal N_{\\epsilon}$ . Taking a union bound over these events, we have that ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\mathbb{P}(\\mathcal{E}_{\\epsilon})\\geq1-e^{-\\frac{m}{24}}-T|\\mathcal{N}_{\\epsilon}|e^{-\\frac{m}{400}}\\geq1-T(|\\mathcal{N}_{\\epsilon}|+1)e^{-\\frac{m}{400}}\\,.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "We conclude by noting that, by Lemma 10, for our choice of $\\epsilon$ , $N\\geq|\\mathcal{N}_{\\epsilon}|+1$ . ", "page_idx": 8}, {"type": "text", "text": "3.4 Proof of Lemma 8 ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Since we now consider a fixed $u\\,\\in\\,\\mathbb{S}_{2}^{d-1}$ , we will write $R_{t}^{j}\\;:=\\;R_{t}^{j}(u)$ and $R_{t}\\,:=\\,R_{t}(u)$ . Let be the filtration given by $\\mathcal{A}_{t}^{\\prime\\prime}=\\overline{{\\sigma}}(\\mathcal{A}_{t}\\cup\\sigma(\\xi_{t+1},J_{t+1}))$ for each $t\\,\\in\\,\\mathbb{N}$ , and let $\\mathbb{E}_{t}^{\\prime\\prime}$ denote the $\\sigma(\\mathcal{F}_{t}\\cup\\mathcal{A}_{t}^{\\prime\\prime}\\cup\\sigma(X_{t+1}))$ -conditional expectation, which will be used to integrate out the random targets U t1+1, . . . , U tm+1. ", "page_idx": 8}, {"type": "text", "text": "With that, we define ", "page_idx": 8}, {"type": "equation", "text": "$$\nD_{t}=\\mathbb{E}_{t}^{\\prime\\prime}R_{t+1}-R_{t}\\quad\\mathrm{and}\\quad W_{t+1}=R_{t+1}-\\mathbb{E}_{t}^{\\prime\\prime}R_{t+1}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "to be the drift and the noise of the process $(R_{t})_{t\\in\\mathbb{N}}$ , respectively. Also let ", "page_idx": 8}, {"type": "equation", "text": "$$\nQ_{t}=\\langle u,X_{t+1}\\rangle^{2}/\\|u\\|_{V_{t+1}}^{2}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "be the strength of the drift. These are related by the following two results: ", "page_idx": 8}, {"type": "text", "text": "Claim 11. $\\begin{array}{r}{D_{t}=(\\frac{2}{3}-R_{t})Q_{t}}\\end{array}$ for all $t\\in\\mathbb{N}$ . ", "page_idx": 8}, {"type": "text", "text": "Lemma 12. For any $T\\,\\in\\,\\mathbb{N}^{+}$ and $m\\,\\geq\\,400\\log(3+T)$ , there exists an event $\\mathcal{E}$ with $\\mathbb{P}(\\mathcal{E})\\ge$ $1-T e^{-\\frac{m}{400}}$ , such that on $\\mathcal{E}\\cap\\{R_{0}\\leq2\\}$ , for all $0\\leq\\tau\\leq t<T$ , ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\left|\\sum_{i=\\tau}^{t}W_{i+1}\\right|\\leq\\frac{1}{10}\\left(3+\\sum_{i=\\tau}^{t}Q_{i}R_{i}\\right)\\,.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "The lemma is proven in Appendix $\\mathrm{G}$ , and the claim in Appendix H. The constant $\\begin{array}{l}{{\\frac{2}{3}}}\\end{array}$ above is just the almost sure value of $\\mathbb{E}_{t}^{\\prime\\prime}[(U_{t+1}^{j})^{2}]$ ; see also Remark 12 for a discussion of the significance of the $(U_{t+1}^{j})^{2}$ terms and how we bound these in the proof of said lemma. ", "page_idx": 8}, {"type": "text", "text": "Proof of Lemma 8. Fix $0\\leq\\tau\\leq t<T$ . We can decompose $R_{t+1}$ as ", "page_idx": 8}, {"type": "equation", "text": "$$\nR_{t+1}=R_{t+1}-\\mathbb{E}_{t}^{\\prime\\prime}R_{t+1}+\\mathbb{E}_{t}^{\\prime\\prime}R_{t+1}-R_{t}+R_{t}=W_{t+1}+D_{t}+R_{t},\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "which unrolled back to $\\tau$ and combined with Claim 11 gives us that ", "page_idx": 8}, {"type": "equation", "text": "$$\nR_{t+1}=R_{\\tau}+\\sum_{i=\\tau}^{t}{\\bigg(}{\\frac{2}{3}}-R_{i}{\\bigg)}Q_{i}+\\sum_{i=\\tau}^{t}W_{i+1}\\,.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Observe from the above that the process $R_{0},R_{1},\\ldots$ drifts towards $\\frac{2}{3}$ , with strength proportional the level of deviation, scaled by $Q_{i}$ . We will now show that on the event of Lemma 12, whenever $R_{t}$ moves sufficiently far away from $\\begin{array}{l}{{\\frac{2}{3}}}\\end{array}$ , the drift will overwhelm the effect of the noises $(W_{t})_{t}$ . Assume henceforth that the aforementioned event holds. ", "page_idx": 8}, {"type": "text", "text": "Lower bound. We consider the excursions of $(R_{t})_{t}$ where it goes and stays below $1/2$ . Let $0\\leq\\tau<$ $s\\leq T$ be endpoints of such a maximal excursion, in the sense that $R_{\\tau}\\geq1/2$ , $R_{\\tau+1},\\ldots,R_{s}<1/2$ and if $s+1\\leq T$ then $R_{s+1}\\geq1/2$ . Our goal is to show that for any $\\tau\\le t<s$ , $R_{t+1}\\geq9/100$ , which suffices to prove the lower bound. Fix $t\\in[\\tau,s)$ . From equation (5) and since the event from Lemma 12 holds, ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{R_{t+1}\\geq R_{r}+\\displaystyle\\sum_{i=r}^{t}\\binom{2}{3}-R_{i}\\biggr)Q_{i}-\\alpha\\biggr(3+\\displaystyle\\sum_{i=r}^{t}Q_{i}R_{i}\\biggr)}\\\\ &{\\qquad=\\displaystyle(1-(1+\\alpha)Q_{r})R_{r}+\\frac{2}{3}Q_{r}-3\\alpha+\\displaystyle\\sum_{i=r+1}^{t}\\left(\\frac{2}{3}-(1+\\alpha)R_{i}\\right)Q_{i}}\\\\ &{\\qquad\\geq(1-(1+\\alpha)Q_{\\tau})R_{r}+\\frac{2}{3}Q_{\\tau}-3\\alpha+\\displaystyle\\sum_{i=r+1}^{t}\\left(\\frac{2}{3}-\\frac{11}{20}\\right)Q_{i}}\\\\ &{\\qquad\\qquad\\geq(1-(1+\\alpha)Q_{\\tau})R_{\\tau}-3\\alpha}\\\\ &{\\qquad\\geq(1-(1+\\alpha)Q_{\\tau})R_{\\tau}-3\\alpha}\\\\ &{\\qquad\\geq(1-\\frac{11}{50})\\frac{1}{2}-\\frac{3}{10}}\\\\ &{\\qquad=\\frac{9}{10}.}\\end{array}\n$$$\\begin{array}{r}{(Q_{i}\\geq0,\\frac{2}{3}-\\frac{11}{20}>0)}\\end{array}$ ", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "Upper bound. The upper bound follows near-verbatim, taking $\\tau$ with $\\begin{array}{r}{R_{\\tau}\\leq\\frac{3}{2}<R_{\\tau+1}}\\end{array}$ ", "page_idx": 9}, {"type": "text", "text": "Remark 11. The lower bound of Lemma 8 was the difficult direction; the upper bound follows straightforwardly from standard de la Pe\u00f1a-type bounds. See, for example, Theorem 20.4 of Lattimore and Szepesv\u00e1ri (2020). ", "page_idx": 9}, {"type": "text", "text": "Remark 12. The proof of Lemma 8 was where we use that the targets $(U_{t}^{j})$ are uniform\u2014or, in particular, that they are bounded random variables\u2014for each $W_{t+1}$ features $(U_{t}^{j})^{2}$ terms, and might otherwise be only sub-exponential. Of course, in that case, we would simply use a truncation argument: pick some truncation level $a>0$ , set $W_{t+1}^{\\prime}=W_{t+1}\\wedge a$ for each $t\\in\\mathbb{N}^{+}$ and work with the process given by the recursion $R_{t+1}^{\\prime}=W_{t+1}^{\\prime}+\\dot{D}_{t}+R_{t}^{\\prime}$ . Then, $R_{t}\\geq R_{t}^{\\prime}$ for all $t\\in\\mathbb{N}$ , and the truncated noises $(W_{t+1}^{\\prime})$ are once again subgaussian, so our approach to lower bounding $R_{t}$ would also work for $R_{t}^{\\prime}$ . We would establish the upper bound as in Remark $_{l l}$ , observing that the result cited therein does not require bounded targets. ", "page_idx": 9}, {"type": "text", "text": "4 Discussion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We showed that linear ensemble sampling works with small ensembles. Per Remark 6 and Remark 5, ours is the first useful theoretical result for linear ensemble sampling. However, our bound might be loose: we discuss this in Remark 2 and point out a potential source of looseness in Remark 10. Addressing this would make for an interesting direction for future work. As discussed in Section 2.2 and Remark 9, our algorithm uses a certain symmetrisation not used within the work of Lu and Van Roy (2017). While this symmetrisation comes with no particular downsides, we would nonetheless be curious to see whether there is a clean way of making the analysis go through without it. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "CS gratefully acknowledges funding from the Canada CIFAR AI Chairs Program, Amii and NSERC. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Y. Abbasi-Yadkori, D. P\u00e1l, and C. Szepesv\u00e1ri. Improved algorithms for linear stochastic bandits. Advances in Neural Information Processing Systems, 2011. Cited on page 6.   \nM. Abeille and A. Lazaric. Linear Thompson sampling revisited. Electronic Journal of Statistics, 11(2):5165\u20135197, 2017. Cited on pages 1, 2, 5, 6, 15, 16.   \nS. Agrawal and N. Goyal. Thompson sampling for contextual bandits with linear payoffs. In International Conference on Machine Learning, 2013. Cited on pages 1, 15.   \nS. Boucheron, G. Lugosi, and P. Massart. Concentration Inequalities: A Nonasymptotic Theory of Independence. Oxford University Press, 2013. Cited on pages 13, 17.   \nY. Burda, H. Edwards, A. Storkey, and O. Klimov. Exploration by random network distillation. arXiv preprint arXiv:1810.12894, 2018. Cited on page 1.   \nS. Curi, F. Berkenkamp, and A. Krause. Efficient model-based reinforcement learning through optimistic policy search and planning. Advances in Neural Information Processing Systems, 2020. Cited on page 1.   \nV. H. de la Pena, M. J. Klass, and T. Leung Lai. Self-normalized processes: exponential inequalities, moment bounds and iterated logarithm laws. The Annals of Probability, 2004. Cited on page 18.   \nM. Dimakopoulou and B. Van Roy. Coordinated exploration in concurrent reinforcement learning. In International Conference on Machine Learning, 2018. Cited on page 1.   \nV. Dwaracherla, X. Lu, M. Ibrahimi, I. Osband, Z. Wen, and B. Van Roy. Hypermodels for exploration. arXiv preprint arXiv:2006.07464, 2020. Cited on page 1.   \nD. Eckles and M. Kaptein. Bootstrap Thompson sampling and sequential decision problems in the behavioral sciences. Sage Open, 9(2):2158244019851675, 2019. Cited on page 1.   \nK. W. Fang. Symmetric multivariate and related distributions. Chapman and Hall/CRC, 1990. Cited on page 16.   \nS. Filippi, O. Cappe, A. Garivier, and C. Szepesv\u00e1ri. Parametric bandits: The generalized linear case. In Advances in Neural Information Processing Systems, 2010. Cited on page 2.   \nB. Hao, J. Zhou, Z. Wen, and W. W. Sun. Low-rank tensor bandits. arXiv preprint arXiv:2007.15788, 2020. Cited on page 1.   \nA. Jacot, F. Gabriel, and C. Hongler. Neural tangent kernel: Convergence and generalization in neural networks. Advances in Neural Information Processing Systems, 2018. Cited on page 2.   \nD. Janz, S. Liu, A. Ayoub, and C. Szepesv\u00e1ri. Exploration via linearly perturbed loss minimisation. International Conference on Artificial Intelligence and Statistics, 2024. Cited on page 1.   \nB. Kveton, C. Szepesv\u00e1ri, M. Ghavamzadeh, and C. Boutilier. Perturbed-History Exploration in Stochastic Linear Bandits. In Uncertainty in Artificial Intelligence Conference, 2020. Cited on page 1.   \nT. Lattimore and C. Szepesv\u00e1ri. Bandit algorithms. Cambridge University Press, 2020. Cited on pages 2, 10, 13, 18.   \nX. Lu and B. Van Roy. Ensemble sampling. Advances in Neural Information Processing Systems, 2017. Cited on pages 1\u20135, 10, 13.   \nX. Lu, Z. Wen, and B. Kveton. Efficient online recommendation via low-rank ensemble sampling. In ACM Conference on Recommender Systems, 2018. Cited on page 1.   \nI. Osband, J. Aslanides, and A. Cassirer. Randomized prior functions for deep reinforcement learning. Advances in Neural Information Processing Systems, 2018. Cited on pages 1, 4.   \nI. Osband, C. Blundell, A. Pritzel, and B. Van Roy. Deep exploration via bootstrapped DQN. Advances in Neural Information Processing Systems, 2016. Cited on pages 1, 4.   \nI. Osband, B. Van Roy, D. J. Russo, Z. Wen, et al. Deep Exploration via Randomized Value Functions. Journal of Machine Learning Research, 20(124):1\u201362, 2019. Cited on page 2.   \nG. Pisier. The volume of convex bodies and Banach space geometry, volume 94. Cambridge University Press, 1999. Cited on page 8.   \nC. Qin, Z. Wen, X. Lu, and B. Van Roy. An analysis of ensemble sampling. In Advances in Neural Information Processing Systems, 2022. Cited on pages 1, 4, 5.   \nM. Skorski. Bernstein-type bounds for beta distribution. Modern Stochastics: Theory and Applications, 10(2):211\u2013228, 2023. Cited on page 16.   \nN. Srinivas, A. Krause, S. M. Kakade, and M. Seeger. Gaussian process optimization in the bandit setting: No regret and experimental design. In International Conference on Machine Learning, 2010. Cited on page 2.   \nW. R. Thompson. On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. Biometrika, 25(3-4):285\u2013294, 1933. Cited on page 1.   \nR. Vershynin. High-dimensional probability: an introduction with applications in data science, volume 47. Cambridge University Press, 2018. Cited on page 16.   \nJ. Yang, D. Eckles, P. Dhillon, and S. Aral. Targeting for long-term outcomes. arXiv preprint arXiv:2010.15835, 2020. Cited on page 1.   \nA. Zanette, D. Brandfonbrener, E. Brunskill, M. Pirotta, and A. Lazaric. Frequentist regret bounds for randomized least-squares value iteration. In International Conference on Artificial Intelligence and Statistics, 2020. Cited on page 2.   \nZ. Zhu and B. Van Roy. Deep Exploration for Recommendation Systems. arXiv preprint arXiv:2109.12509, 2021. Cited on page 1. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A A reformulation of Algorithm 1 in the style of Lu and Van Roy (2017) ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Below in Algorithm 2, we list a reformulation of Algorithm 1 written using updates in the style of Lu and Van Roy (2017), with one difference: the algorithm here accepts only a single time-stationary perturbation scale $r\\,>\\,0$ (this is unavoidable with the Lu and Van Roy (2017) style of updates). Taking $r=7\\tilde{\\beta}_{T}^{\\delta}$ , where $\\beta_{T}^{\\delta}$ is the deterministic upper bound on $\\beta_{0}^{\\delta},\\dots,\\beta_{T}^{\\delta}$ given by ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\tilde{\\beta}_{T}^{\\delta}=\\sqrt{\\lambda}+\\sqrt{2\\log(1/\\delta)+d\\log((d+T/\\lambda)/d)}\\,,\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "allows for the same guarantee as given in Theorem 1, but with $\\tilde{\\beta}_{T}^{\\delta}$ in place of $\\beta_{\\tau-1}^{\\delta}$ . ", "page_idx": 12}, {"type": "text", "text": "Algorithm 2 Equivalent form of Algorithm 1 assuming time-stationary perturbation scale $r$ Input regularisation parameter $\\lambda>0$ , half-ensemble size $m\\in\\mathbb{N}^{+}$ , perturbation scale $r>0$ Let $V_{0}=\\lambda I$ , sample $(w_{0}^{j})\\sim\\mathcal{U}(\\sqrt{d}\\mathbb{S}_{2}^{d-1})^{\\otimes m}$ and let $w_{0}^{m+j}=-w_{0}^{j}$ for each $j\\in[m]$ for $t\\in\\mathbb{N}^{+}$ do Sample $J_{t}\\sim\\mathcal{U}(\\{1,\\{\\dots,2m\\})$ Compute an $X_{t}\\in\\arg\\operatorname*{max}_{x\\in\\mathcal{X}}\\langle x,w_{t-1}^{J_{t}}\\rangle$ , play action $X_{t}$ and receive reward $Y_{t}$ Sample $(U_{t}^{j})_{j\\in[m]}\\sim\\mathcal{U}([-1,1])^{\\otimes m}$ and let $U_{t}^{m+j}=-U_{t}^{j}$ for each $j\\in[m]$ Let $V_{t}=V_{t-1}+X_{t}X_{t}^{\\mathsf{T}}$ , and let $w_{t}^{j}=V_{t}^{-1}(V_{t-1}w_{t-1}^{j}+X_{t}(Y_{t}+r U_{t}^{j}))$ for each $j\\in[2m]$ ", "page_idx": 12}, {"type": "text", "text": "To confirm that Algorithm 2 is equivalent to Algorithm 1 if the sequence $r_{0},r_{1},\\ldots$ is set to the common value $r$ , it suffices to compare the list of parameters $w_{t-1}^{1},\\dots,w_{t-1}^{2m}$ used here with the list ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\hat{\\theta}_{t-1}\\pm r\\tilde{\\theta}_{t-1}^{1},\\dots,\\hat{\\theta}_{t-1}\\pm r\\tilde{\\theta}_{t-1}^{1}\\,,\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "at each step of the algorithm. We leave this as a simple algebraic exercise for the reader. ", "page_idx": 12}, {"type": "text", "text": "B Proof of Theorem 2: master regret bound ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "We need the following concentration inequality, a simple consequence of Exercise 20.8 in Lattimore and Szepesv\u00e1ri $(2020)^{2}$ and Hoeffding\u2019s lemma (Lemma 2.2, Boucheron et al., 2013), and the elliptical potential lemma (Lemma 19.4 in Lattimore and Szepesv\u00e1ri, 2020). ", "page_idx": 12}, {"type": "text", "text": "Lemma 13. Fix $0<\\delta\\leq1$ . Let $(\\xi_{t})_{t\\in\\mathbb{N}^{+}}$ be a real-valued martingale difference sequence satisfying $\\lvert\\xi_{t}\\rvert\\leq c$ almost surely for each $t\\in\\mathbb{N}^{+}$ and some $c>0$ . Then, ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\exists\\tau\\colon\\left(\\sum_{t=1}^{\\tau}\\xi_{t}\\right)^{2}\\geq2(c^{2}\\tau+1)\\log\\left(\\frac{\\sqrt{c^{2}\\tau+1}}{\\delta}\\right)\\right)\\leq\\delta.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Lemma 14 (Elliptical potential lemma). Let $(x_{t})_{t\\in\\mathbb{N}^{+}}$ be a sequence of vectors in $\\mathbb{B}_{2}^{d}$ , let $V_{0}=\\lambda I$ for some $\\lambda\\geq1$ and $\\begin{array}{r}{V_{t}=V_{0}+\\sum_{i=1}^{t}x_{i}x_{i}^{\\mathsf{T}}}\\end{array}$ for each $t\\in\\mathbb{N}^{+}$ . Then, for all $\\tau\\in\\mathbb{N}^{+}$ , ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{\\tau}\\|x_{t}\\|_{V_{t-1}^{-1}}^{2}\\leq2d\\log\\left(1+{\\frac{\\tau}{\\lambda d}}\\right).\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Recall that $J(\\theta)=\\operatorname*{max}_{x\\in\\mathcal{X}}\\langle x,\\theta\\rangle$ is the support function of $\\mathcal{X}$ , and observe the following: ", "page_idx": 12}, {"type": "text", "text": "Claim 15. For any $t\\in\\mathbb{N}^{+}$ , $X_{t}$ is a subgradient of $J$ at $\\theta_{t}$ . ", "page_idx": 12}, {"type": "text", "text": "Proof. Fix $t\\in\\mathbb{N}^{+}$ . For any $\\theta\\in\\ensuremath{\\mathbb{R}}^{d}$ , ", "page_idx": 12}, {"type": "equation", "text": "$$\nJ(\\theta_{t})+\\langle X_{t},\\theta-\\theta_{t}\\rangle=\\langle X_{t},\\theta_{t}\\rangle+\\langle X_{t},\\theta-\\theta_{t}\\rangle=\\langle X_{t},\\theta\\rangle\\leq\\operatorname*{max}_{x\\in\\mathcal{X}}\\langle x,\\theta\\rangle=J(\\theta),\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "which is the inequality that defines a subgradient. ", "page_idx": 12}, {"type": "text", "text": "Proof of Theorem 2. Throughout this proof, we work on the intersection $\\mathcal{E}_{T}\\cap\\mathcal{E}_{T}^{\\star}$ , and therefore in particular use that for any $t\\,\\in\\,[T],\\,\\overline{{\\theta}}_{t},\\theta^{\\star}\\,\\in\\,\\Theta_{t}$ . We let $\\gamma_{t}\\,=\\,2b_{t}$ , and observe that this is the $V_{t}$ -weighted Euclidean norm width of $\\Theta_{t}$ , in the sense that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\gamma_{t}=\\operatorname*{max}\\{\\|\\theta-\\theta^{\\prime}\\|_{V_{t}}\\colon\\theta,\\theta^{\\prime}\\in\\Theta_{t}\\}\\,.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "We will also use the shorthands $\\mathbb{F}_{t}^{\\prime}=\\sigma(\\mathcal{F}_{t}\\cup\\mathcal{A}_{t}^{\\prime}),\\mathbb{P}_{t}^{\\prime}=\\mathbb{P}(\\cdot\\mid\\mathbb{F}_{t}^{\\prime})$ and $\\mathbb{E}_{t}^{\\prime}=\\mathbb{E}[\\cdot\\mid\\mathbb{F}_{t}^{\\prime}]$ . ", "page_idx": 13}, {"type": "text", "text": "The proof is based on decomposing the regret into two parts, which we then control separately: ", "page_idx": 13}, {"type": "equation", "text": "$$\nR(\\tau)=\\sum_{t=1}^{\\tau}\\left(J(\\theta^{\\star})-J(\\theta_{t})\\right)+\\sum_{t=1}^{\\tau}\\left(J(\\theta_{t})-\\langle X_{t},\\theta^{\\star}\\rangle\\right).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Fix $t\\in[\\tau]$ and consider the second term, $J(\\theta_{t})-\\langle X_{t},\\theta^{\\star}\\rangle$ . We have that ", "page_idx": 13}, {"type": "equation", "text": "$$\nJ(\\theta_{t})-\\langle X_{t},\\theta^{\\star}\\rangle=\\langle X_{t},\\theta_{t}-\\theta^{\\star}\\rangle\\leq\\|X_{t}\\|_{V_{t-1}^{-1}}\\|\\theta_{t}-\\theta^{\\star}\\|_{V_{t-1}}\\leq\\gamma_{t-1}\\|X_{t}\\|_{V_{t-1}^{-1}},\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where the first inequality is by Cauchy-Schwartz, the second uses that $\\theta_{t},\\theta^{\\star}\\,\\in\\,\\Theta_{t-1}$ and then equation (7). ", "page_idx": 13}, {"type": "text", "text": "Now consider the first term, $J(\\theta^{\\star})\\mathrm{~-~}J(\\theta_{t})$ . Let $\\theta^{-}$ be a minimiser $J$ over $\\Theta_{t-1}$ (which is welldefined, since $J$ is continuous and $\\Theta_{t-1}$ closed) and let $\\theta^{+}$ be an element of $\\Theta^{\\mathrm{OPT}}\\cap\\Theta_{t-1}$ (which is non-empty, since it contains at least $\\theta^{\\star}$ ). Then, since ${\\theta}^{\\star},{\\theta}_{t}\\in\\Theta_{t-1}$ , we have the bound ", "page_idx": 13}, {"type": "equation", "text": "$$\nJ(\\theta^{\\star})-J(\\theta_{t})\\leq J(\\theta^{\\star})-J(\\theta^{-})\\leq J(\\theta^{+})-J(\\theta^{-})\\,.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "It follows that for any probability measure $Q$ over $\\Theta^{\\mathrm{OPT}}\\cap\\Theta_{t-1}$ , ", "page_idx": 13}, {"type": "equation", "text": "$$\nJ(\\theta^{\\star})-J(\\theta_{t})\\leq\\int J(\\theta^{+})-J(\\theta^{-})\\,\\mathrm{d}Q(\\theta^{+}).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Let $\\Theta_{t-1}^{\\mathrm{OPT}}=\\Theta^{\\mathrm{OPT}}\\cap\\Theta_{t-1}$ , and take $Q=Q_{t-1}$ in the integral above defined by ", "page_idx": 13}, {"type": "equation", "text": "$$\nQ_{t-1}=\\binom{\\mathbb{P}_{t-1}^{\\prime}\\left(\\theta_{t}\\in\\cdot\\cap\\Theta_{t-1}^{\\mathrm{OPT}}\\right)/p_{t-1}}{\\mathrm{any~arbitrary~probability~measure},}\\quad p_{t-1}>0\\,;\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "which yields the bound ", "page_idx": 13}, {"type": "equation", "text": "$$\nJ(\\theta^{\\star})-J(\\theta_{t})\\leq\\frac{1}{p_{t-1}}\\mathbb{E}_{t-1}^{\\prime}[(J(\\theta_{t})-J(\\theta^{-}))\\mathbf{1}[\\theta_{t}\\in\\Theta_{t-1}^{\\mathrm{OPT}}]],\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where for $p_{t-1}=0$ we take the upper bound to be positive infinity. Observing that $X_{t}$ is a subgradient of $J$ at $\\theta_{t}$ (Claim 15 and equation (6)) and applying Cauchy-Schwartz, we have that ", "page_idx": 13}, {"type": "equation", "text": "$$\nJ(\\theta_{t})-J(\\theta^{-})\\le\\langle X_{t},\\theta_{t}-\\theta^{-}\\rangle\\leq\\|X_{t}\\|_{V_{t-1}^{-1}}\\|\\theta^{-}-\\theta_{t}\\|_{V_{t-1}},\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Moreover, since $\\theta^{-}\\in\\Theta_{t-1}$ , observing that norms are non-negative, that $\\Theta_{t-1}^{\\mathrm{OPT}}\\subset\\Theta_{t-1}$ and using equation (7), ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\|\\theta^{-}-\\theta_{t}\\|_{V_{t-1}}\\mathbf{1}[\\theta_{t}\\in\\Theta_{t-1}^{\\mathrm{OPT}}]\\leq\\|\\theta^{-}-\\theta_{t}\\|_{V_{t-1}}\\mathbf{1}[\\theta_{t}\\in\\Theta_{t-1}]\\leq\\gamma_{t-1}\\,,\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "and therefore, since $\\gamma_{t-1}$ is $\\mathbb{F}_{t-1}^{\\prime}$ -measurable (by assumption), we have the bound ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\frac{1}{p_{t-1}}\\mathbb{E}_{t-1}^{\\prime}[(J(\\theta_{t})-J(\\theta^{-}))\\mathbf{1}[\\theta_{t}\\in\\Theta_{t-1}^{\\mathrm{OPT}}]]\\leq\\frac{\\gamma_{t-1}}{p_{t-1}}\\mathbb{E}_{t-1}^{\\prime}[\\|X_{t}\\|_{V_{t-1}^{-1}}].\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Chaining the above inequalities and writing these in terms of $\\Delta_{t}=\\mathbb{E}_{t-1}^{\\prime}[\\|X_{t}\\|_{V_{t-1}^{-1}}]-\\|X_{t}\\|_{V_{t-1}^{-1}}$ , we have the bound ", "page_idx": 13}, {"type": "equation", "text": "$$\nJ(\\theta^{\\star})-J(\\theta_{t})\\le\\frac{\\gamma_{t-1}}{p_{t-1}}\\mathbb{E}_{t-1}^{\\prime}[\\|X_{t}\\|_{V_{t-1}^{-1}}]=\\frac{\\gamma_{t-1}}{p_{t-1}}\\left(\\|X_{t}\\|_{V_{t-1}^{-1}}+\\Delta_{t}\\right),\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Combining equations (9) and (10) with the regret decomposition in equation (8), ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathfrak{I}(\\tau)\\leq\\sum_{t=1}^{T}\\left(\\left(\\gamma_{t-1}+\\frac{\\gamma_{t-1}}{p_{t-1}}\\right)\\|X_{t}\\|_{V_{t-1}^{-1}}+\\frac{\\gamma_{t-1}}{p_{t-1}}\\Delta_{t}\\right)\\leq\\operatorname*{max}_{i\\in[\\tau]}\\frac{\\gamma_{i-1}}{p_{i-1}}\\left(2\\sum_{t=1}^{\\tau}\\|X_{t}\\|_{V_{t-1}^{-1}}+\\sum_{t=1}^{\\tau}\\Delta_{t}\\right)\\,,\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "for any $\\tau\\in[T]$ . For the first sum in that upper bound, by Cauchy-Schwartz and the elliptical potential lemma (Lemma 14, which can be applied because by assumption $\\lambda\\geq1$ ), for any $\\tau\\in\\bar{\\mathbb{N}}^{+}$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{\\tau}\\left\\lVert X_{t}\\right\\rVert_{V_{t-1}^{-1}}\\leq\\left(\\tau\\sum_{t=1}^{\\tau}\\left\\lVert X_{t}\\right\\rVert_{V_{t-1}^{-1}}^{2}\\right)^{\\frac{1}{2}}\\leq\\sqrt{2\\tau d\\log\\left(1+\\frac{\\tau}{d\\lambda}\\right)}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "To deal with the second sum, observe that since for all $t\\in\\mathbb{N}^{+}$ , $V_{t-1}\\succeq\\lambda I$ and $X_{t}\\in\\mathbb{B}_{2}^{d}$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\|X_{t}\\|_{V_{t-1}^{-1}}^{2}=\\langle X_{t},V_{t-1}^{-1}X_{t}\\rangle\\leq\\|X_{t}\\|_{2}^{2}/\\lambda\\leq1/\\lambda\\quad{\\mathrm{and~so}}\\quad|\\Delta_{t}|\\leq2/{\\sqrt{\\lambda}}\\quad{\\mathrm{for~all~}}t\\in\\mathbb{N}^{+}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Moreover, observe that $(\\Delta_{t})_{t}$ is an $(\\mathbb{F}_{t}^{\\prime})_{t}$ -adapted martingale difference sequence. We thus apply Lemma 13 with $c=2/\\sqrt{\\lambda}$ , to obtain the deviation probability bound ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\exists\\tau\\in\\mathbb{N}^{+}\\colon\\sum_{t=1}^{\\tau}\\Delta_{t}\\geq\\sqrt{2(4\\tau/\\lambda+1)\\log\\left(\\frac{\\sqrt{4\\tau/\\lambda+1}}{\\delta}\\right)}\\right)\\leq\\delta.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The bounds on the two sums, equation (12) and equation (13), when inserted into equation (11) and combined with a union bound, yield the claimed result. \u53e3 ", "page_idx": 14}, {"type": "text", "text": "C Proof of Lemma 4: optimism for elliptical confidence sets ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Lemma 16. Let $F:\\ensuremath{\\mathbb{R}^{d}}\\to\\ensuremath{\\mathbb{R}}$ be a convex function and let u be its maximizer over the unit ball. Then, for any $v\\in H_{u}\\doteq\\{v\\in\\mathbb{R}^{d}\\colon\\langle v,u\\rangle\\ge1\\}$ , we have $F(v)\\geq F(u)$ . ", "page_idx": 14}, {"type": "text", "text": "Proof. For any $\\boldsymbol{v}\\in\\mathbb{R}^{d}$ with $\\langle v,u\\rangle>1$ , the ray from $v$ to $u$ enters the interior of the unit ball. Hence, for any such $v$ , there exists a $z\\in\\mathbb{B}_{2}^{d}$ and $\\alpha\\in(0,1)$ such that $u=\\alpha z+(1-\\alpha)v$ . By convexity and maximality, ", "page_idx": 14}, {"type": "equation", "text": "$$\nF(u)=F(\\alpha z+(1-\\alpha)v)\\leq\\alpha F(z)+(1-\\alpha)F(v)\\leq\\alpha F(u)+(1-\\alpha)F(v)\\,.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Hence, $F(u)\\leq F(v)$ . Since any finite convex function on an open set is continuous, the result holds for any $v\\in H_{u}$ . \u53e3 ", "page_idx": 14}, {"type": "text", "text": "Proof of Lemma 4. Write ${\\cal F}\\,=\\,{\\cal J}\\circ\\psi_{t}^{\\delta}$ ; since $J$ is convex and $\\psi_{t}^{\\delta}$ is affine, $F$ is convex. Let $u^{+}$ be the maximiser of $F$ over $\\mathbb{B}_{2}^{d}$ , and note that since $F$ is a convex function and $\\mathbb{B}_{2}^{d}$ is a convex set, $u^{+}\\,\\in\\,\\partial\\mathbb{B}_{2}^{d}\\,=\\,\\mathbb{S}_{2}^{d-1}$ . By assumption, $\\theta^{\\star}\\in\\,\\psi_{t}^{\\delta}(\\mathbb{B}_{2}^{d})$ , and so $J(\\theta^{\\star})\\,\\leq\\,F(u^{+})$ . By Lemma 16, $F(u^{+})\\leq F(u^{\\prime})$ for any $u^{\\prime}\\in H_{u^{+}}$ . Thus, $\\psi_{t}^{\\delta}(H_{u^{+}})\\subset\\Theta^{\\mathrm{OPT}}$ , and so ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\Theta^{\\mathrm{OPT}}\\cap\\psi_{t}^{\\delta}(a\\mathbb{B}_{2}^{d})\\supset\\psi_{t}^{\\delta}(H_{u^{+}})\\cap\\psi_{t}^{\\delta}(a\\mathbb{B}_{2}^{d})\\supset\\psi_{t}^{\\delta}(H_{u^{+}}\\cap a\\mathbb{B}_{2}^{d})\\,.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Therefore, for any measure $Q$ on $\\mathbb{R}^{d}$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\nQ(\\Theta^{\\mathrm{OPT}}\\cap\\psi_{t}^{\\delta}(a\\mathbb{B}_{2}^{d}))\\geq Q(\\psi_{t}^{\\delta}(H_{u^{+}}\\cap a\\mathbb{B}_{2}^{d}))\\geq\\operatorname*{inf}_{u\\in\\mathbb{S}_{2}^{d-1}}Q(\\psi_{t}^{\\delta}(H_{u}\\cap a\\mathbb{B}_{2}^{d}))\\,.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "D Analysis of Thompson sampling via our master theorem ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Algorithm 3 is a version of Thompson sampling we call confident linear Thompson sampling. It is extremely simple: at each step $t\\in[T]$ , it picks an a\u221action $X_{t}$ that is optimal according to an estimate $\\theta_{t}$ sampled uniformly on $\\psi_{t-1}^{\\delta}(\\sqrt{d}\\mathbb{B}_{2}^{d})=\\Theta_{t-1}$ , a $\\sqrt{d}$ -inflation of the ridge regression confidence set. It differs from usual linear Thompson sampling of Agrawal and Goyal (2013) through the use of a uniform sampling distribution. ", "page_idx": 14}, {"type": "text", "text": "Remark 13. Our use of the uniform distribution to generate perturbed parameters is purely for the sake of a clean exposition, and for easy comparison with our linear ensemble sampling algorithm. Observe that the usual analysis for the Gaussian (or subgaussian) case begins by restricting to a high-probability event where every $\\theta_{t}$ lands within some inflated version of the corresponding $\\Theta_{t-1}$ (as in Agrawal and Goyal, 2013; Abeille and Lazaric, 2017). ", "page_idx": 14}, {"type": "table", "img_path": "SO7fnIFq0o/tmp/4e68d30ef2f9b96837860b2c590a5aab300b292b3e2fb815428477792acb10ae.jpg", "table_caption": ["Algorithm 3 Confident linear Thompson sampling "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "Theorem 17. Let Assumption $^{\\,l}$ hold. Fix $\\delta\\,\\in\\,(0,1]$ and let $\\lambda\\geq1$ . There exist some universal constant $C>0$ such that with probability $1-\\delta$ , a learner using Algorithm 3 incurs regret satisfying ", "page_idx": 15}, {"type": "equation", "text": "$$\nR(T)\\leq C\\beta_{\\tau-1}^{\\delta}\\sqrt{d}\\left(\\sqrt{d\\tau\\log(1+\\tau/(\\lambda d))}+\\sqrt{(\\tau/\\lambda)\\log(\\tau/(\\lambda\\delta))}\\right)\\quad f o r\\,a l l\\quad\\tau\\in[T]\\,.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The above result recovers the same regret bound for confident linear Thompson sampling as that given by the analysis of Abeille and Lazaric (2017). The proof is as follows. ", "page_idx": 15}, {"type": "text", "text": "Proof of Theorem $I7.$ . Take $T=\\infty$ . Fix $b_{t}={\\sqrt{d}}$ for all $t\\in\\mathbb{N}$ . As each $\\theta_{t}$ is a uniform random variable on $\\Theta_{t-1}$ for all $t\\in\\mathbb N$ , and so event $\\mathcal{E}_{T}$ holds almost surely. Moreover, as for ensemble sampling, we take $\\mathcal{E}_{T}^{\\star}$ to be the event from the standar\u221ad concentration result for ridge regression, Lemma 3, observing as before that, $\\psi_{t-1}^{\\delta}(\\mathbb{B}_{2}^{d})\\subset\\psi_{t-1}^{\\delta}(\\sqrt{d}\\mathbb{B}_{2}^{d})=\\psi_{t-1}^{\\delta}(b_{t}\\mathbb{B}_{2}^{d}).$ . ", "page_idx": 15}, {"type": "text", "text": "We pick $\\mathcal{A}_{t-1}^{\\prime}=\\mathcal{A}_{t-1}$ for all $t\\in\\mathbb{N}^{+}$ . Now, to lower bound each $p_{t-1}$ , note that on $\\mathcal{E}_{T}^{\\star}$ , by Lemma 4 applied with ${\\dot{Q}}(A)=\\mathbb{P}(\\theta_{t}\\in A\\mid\\sigma({\\mathcal F}_{t-1}\\cup{\\mathcal A}_{t-1}^{\\prime}))=:\\mathbb{P}_{t-1}^{\\prime}(\\theta_{t}\\in A)$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\np_{t-1}=\\mathbb{P}_{t-1}^{\\prime}(\\theta_{t}\\in\\Theta^{\\mathrm{OPT}}\\cap\\Theta_{t-1})\\ge\\operatorname*{inf}_{u\\in\\mathbb{S}_{2}^{d-1}}\\mathbb{P}_{t-1}^{\\prime}(\\theta_{t}\\in\\psi_{t-1}^{\\delta}(H_{u}\\cap\\sqrt{d}\\mathbb{B}_{2}^{d}))\\,.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "And, since $\\psi_{t-1}^{\\delta}$ is a bijection and $U_{t}$ is uniform on $\\sqrt{d}\\mathbb{B}_{2}^{d}$ , and thus rotationally invariant, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{u\\in\\mathbb S_{2}^{d-1}}\\mathbb P_{t-1}^{\\prime}(\\theta_{t}\\in\\psi_{t-1}^{\\delta}(H_{u}\\cap\\sqrt{d}\\mathbb B_{2}^{d}))=\\operatorname*{inf}_{u\\in\\mathbb S_{2}^{d-1}}\\mathbb P_{t-1}^{\\prime}(U_{t}\\in H_{u}\\cap\\sqrt{d}\\mathbb B_{2}^{d})=\\mathcal U(H_{1}\\cap\\sqrt{d}\\mathbb B_{2}^{d})\\,.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "As established in Appendix A of Abeille and Lazaric (2017), $\\mathcal{U}(H_{1}\\cap\\sqrt{d}\\mathbb{B}_{2}^{d})\\ \\geq\\ 1/(16\\sqrt{3\\pi})$ , irnegdreept ebnoduenntdl yo fo fT $d$ .e oTrehims  2m eyiaenlsd ts htahte,  oclna $\\mathcal{E}_{T}^{\\star}$ ., $\\begin{array}{r}{\\frac{b_{t-1}}{p_{t-1}}\\leq16\\sqrt{3\\pi d}}\\end{array}$ for all $t\\in\\mathbb{N}^{+}$ ; plugging this into the ", "page_idx": 15}, {"type": "text", "text": "E Proof of Lemma 6: singular values at initialisation ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Lemma 6 follows by taking $\\begin{array}{r}{y=\\frac{81m}{2500}}\\end{array}$ and $\\begin{array}{r}{\\epsilon=\\frac{1}{20}}\\end{array}$ in Theorem 18. ", "page_idx": 15}, {"type": "text", "text": "Theorem 18. Let $U\\,\\in\\,\\mathbb{R}^{m\\times d}$ , $m\\,\\geq\\,d,$ , be a random matrix with independent rows $U_{1},\\ldots,U_{m}$ distributed uniformly on $\\sqrt{d}\\mathbb{S}_{2}^{d-1}$ . Then, for all $y>0$ , $\\epsilon\\in(0,1/2)$ , and with $c_{\\epsilon}=1/(1-2\\epsilon)$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}\\{\\sqrt{m}-c_{\\epsilon}\\sqrt{y}\\leq s_{d}(U)\\leq s_{1}(U)\\leq\\sqrt{m}+c_{\\epsilon}\\sqrt{y}\\}\\geq1-\\exp\\{-3y/8+\\log(2(1+2/\\epsilon)^{d})\\}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The proof of Theorem 18 follows the approach of Chapter 4 of Vershynin (2018), but makes the constants explicit. For these constants, we will need the following claim: ", "page_idx": 15}, {"type": "text", "text": "Claim 19. Fi $c\\,x\\in\\mathbb{S}_{2}^{d-1}$ , let $X\\sim\\mathcal{U}(\\mathbb{S}_{2}^{d-1})$ and $X_{x}^{2}=\\langle X,x\\rangle^{2}$ . Then, $\\mathbb{E}X_{x}^{2}=1/d,$ , and for some $\\nu,c>0$ satisfying $\\nu\\bar{\\leq}\\,2/d^{2}$ and $c\\leq4/d$ and all $0<s<1/c,$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{E}\\exp(s\\,|X_{x}^{2}-\\mathbb{E}X_{x}^{2}|)\\leq\\exp\\left(\\frac{s^{2}\\nu/2}{1-c s}\\right)\\,.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof. It is known that $\\begin{array}{r}{X_{x_{\\cdot}}^{2}\\sim\\mathrm{Beta}(\\frac{1}{2},\\frac{d-1}{2})}\\end{array}$ (see, for example, Theorem 1.5 and the discussion thereafter in Fang, 1990), the expectation of which is $1/d$ . We thus need only look up moment generating function bounds for beta random variables. Skorski (2023) derives such in their proof of their Theorem 1, and our result follows by substituting in the parameters of our beta distribution, and bounding the resulting $\\nu$ and $c$ crudely. \u53e3 ", "page_idx": 15}, {"type": "text", "text": "We will need the next two results from Vershynin (2018), appearing as Lemma 4.1.5 and Exercise 4.4.3 respectively. ", "page_idx": 15}, {"type": "text", "text": "Lemma 20 (Appropximate isometry). For any matrix $A\\in\\mathbb{R}^{m\\times d}$ and $\\epsilon\\geq0$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|A^{\\mathsf{T}}A-I\\|\\leq\\epsilon\\vee\\epsilon^{2}\\implies1-\\epsilon\\leq s_{d}(A)\\leq s_{1}(A)\\leq1+\\epsilon\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Lemma 21 (Quadratic form on a net). For a symmetric matrix $A\\in\\mathbb{R}^{d\\times d}$ and an \u03f5-net $\\mathcal{N}$ of $\\mathbb{S}_{2}^{d-1}$ with $\\epsilon\\in(0,1/2)$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\|A\\|\\leq{\\frac{1}{1-2\\epsilon}}\\operatorname*{sup}_{x\\in\\mathcal{N}}\\left|\\langle A x,x\\rangle\\right|.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof of Theorem $I8.$ .\u221a Fix $x\\in\\mathbb{S}_{2}^{d-1}$ , and consider $\\begin{array}{r}{Z_{x}^{2}\\,=\\,\\frac{1}{m}\\|U x\\|_{2}^{2}\\,=\\,\\frac{d}{m}\\sum_{j=1}^{m}\\langle U_{j}/\\sqrt{d},x\\rangle^{2}}\\end{array}$ . Observe that each $U_{j}/\\sqrt{d}\\sim\\mathcal{U}(\\mathbb{S}_{2}^{d-1})$ ; thus, by Claim 19 and since $U_{1},\\ldots,U_{m}$ are independent, adopting the notation of the claim, we have that, for all $0<s d/m<1/c$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{E}\\exp(s\\left|Z_{x}^{2}-1\\right|)=\\prod_{j=1}^{m}\\mathbb{E}\\exp\\left(\\frac{s d}{m}|X_{x}^{2}-\\mathbb{E}X_{x}^{2}|\\right)\\leq\\exp\\left(\\frac{s^{2}d^{2}\\nu/(2m)}{1-c s d/m}\\right).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Thus, $|Z_{x}^{2}\\mathrm{~-~}1|$ is what Boucheron et al. (2013) would term sub-gamma with parameters $(d^{2}\\nu/m,\\,c d/m)$ on both tails. Applying the maximal-form of the there-stated Bernstein-type bound for sub-gamma random variables, a union bound over the two tails, and the bounds $\\nu\\leq2/d^{2}$ and $c\\leq4/d$ from Claim 19, we have that, for all $y>0$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{P}(|Z_{x}^{2}-1|\\geq\\sqrt{y/m}\\vee y/m)\\leq2e^{-3y/8}\\,.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Now let $\\mathcal{N}_{\\epsilon}$ , $\\epsilon\\in(0,1/2)$ , be an $\\epsilon$ -net of $\\mathbb{S}_{2}^{d-1}$ . By Lemma 21, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{x\\in\\mathbb{S}_{2}^{d-1}}\\left|Z_{x}^{2}-1\\right|\\leq\\frac{1}{1-2\\epsilon}\\operatorname*{max}_{x\\in\\mathcal{N}_{\\epsilon}}\\left|Z_{x}^{2}-1\\right|.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Also, by our bound on nets from Lemma 10, $|\\mathcal{N}_{\\epsilon}|\\leq(1+2/\\epsilon)^{d}$ . Thus, taking a union bound over $x\\in\\mathcal{N}_{\\epsilon}$ , we have that for any $y>0$ and $\\epsilon\\in(0,1/2)$ , the probability that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\operatorname*{sup}_{x\\in\\mathbb{S}_{2}^{d-1}}|Z_{x}^{2}-1|\\leq\\frac{1}{1-2\\epsilon}\\left(\\sqrt{y/m}\\vee y/m\\right)\\right)\\geq1-\\exp\\{-3y/8+\\log(2(1+2/\\epsilon)^{d})\\}\\,.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We conclude the proof by observing that $\\begin{array}{r}{\\operatorname*{sup}_{x\\in\\mathbb{S}_{2}^{d-1}}|Z_{x}^{2}-1|\\;=\\;\\|U^{\\mathsf{T}}U/m-I\\|}\\end{array}$ and applying Lemma 20. ", "page_idx": 16}, {"type": "text", "text": "F Proof of Lemma 9: Lipschitzness result ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Proof of Lemma 9. Fix $u\\ne0$ and let $z=V_{t}^{1/2}u$ . Then, ", "page_idx": 16}, {"type": "equation", "text": "$$\nR_{t}(u)=\\frac{1}{m}\\sum_{j=1}^{m}R_{t}^{j}(u)=\\frac{1}{m}\\sum_{j=1}^{m}\\frac{\\langle z,\\Gamma_{t}^{j}\\rangle^{2}}{\\|z\\|_{2}^{2}}=\\frac{\\|\\Gamma_{t}^{\\top}z\\|_{2}^{2}}{m\\|z\\|_{2}^{2}}\\,.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Now note that for all non-negative $a,b,A,B$ with $b\\geq a$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\left|{\\frac{A^{2}}{a^{2}}}-{\\frac{B^{2}}{b^{2}}}\\right|=\\left|{\\frac{A^{2}(b^{2}-a^{2})+(A^{2}-B^{2})a^{2}}{a^{2}b^{2}}}\\right|\\leq{\\frac{2A^{2}}{a^{2}}}{\\frac{|b-a|}{b}}+{\\frac{|A-B|(A+B)}{b^{2}}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Let $u,v\\in\\mathbb{S}_{2}^{d-1}$ , $z\\,=\\,V_{t}^{1/2}u$ , $w\\,=\\,V_{t}^{1/2}v$ . Let $\\epsilon=\\|\\boldsymbol{u}-\\boldsymbol{v}\\|_{2}$ . Let $A\\,=\\,\\|\\Gamma_{t}^{\\mathsf{T}}z\\|_{2}$ , $B\\,=\\,\\|\\Gamma_{t}^{\\mathsf{T}}w\\|_{2}$ , $a=\\|z\\|_{2}$ , $b=\\|w\\|_{2}$ . Assume without loss of generality that $b\\geq a$ . Since $v\\in\\mathbb{S}_{2}^{d-1}$ and $b\\geq\\sqrt{\\lambda}$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n2\\frac{A^{2}}{a^{2}}\\frac{|b-a|}{b}\\leq\\frac{2\\|\\Gamma_{t}\\|^{2}\\|z-w\\|_{2}}{\\sqrt{\\lambda}}\\leq2\\|\\Gamma_{t}\\|^{2}\\frac{\\|V_{t}^{1/2}\\|}{\\sqrt{\\lambda}}\\,\\epsilon\\,,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and likewise ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{|A-B|(A+B)}{b^{2}}\\leq\\frac{2\\|\\Gamma_{t}\\|\\|\\Gamma_{t}^{\\mathsf{T}}(z-w)\\|_{2}}{\\sqrt{\\lambda}}\\leq2\\|\\Gamma_{t}\\|^{2}\\frac{\\|V_{t}^{1/2}\\|}{\\sqrt{\\lambda}}\\,\\epsilon\\,.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Putting things together, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n|R_{t}(u)-R_{t}(v)|\\leq\\frac{4\\|\\Gamma_{t}\\|^{2}\\|V_{t}^{1/2}\\|}{m\\sqrt{\\lambda}}\\epsilon\\,.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "G Proof of Lemma 12: concentration result ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "This proof will require the following definition of conditional subgaussianity, and the standard de la Pe\u00f1a-type concentration result for sequences of such random variables, stated thereafter. ", "page_idx": 17}, {"type": "text", "text": "Definition 22. Let $A,B$ be random variables and $\\mathcal{F}$ a $\\sigma$ -algebra. We say that $A$ is $\\mathcal{F}$ -conditionally $B$ -subgaussian when $B$ is non-negative and $\\mathcal{F}$ -measurable, and for all $s\\in\\mathbb{R}$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\exp\\{s A\\}\\mid{\\mathcal F}]\\le\\exp\\{s^{2}B^{2}/2\\}\\quad a l m o s t\\,s u r e l y.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Lemma 23. Let $(A_{i},B_{i},\\mathcal{H}_{i})_{i}$ be such that each $A_{i}$ is $\\mathcal{H}_{i}$ -conditionally $B_{i}$ -subgaussian. Then, for any $x,y>0$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left\\{\\exists\\tau\\in\\mathbb{N}\\colon\\left(\\sum_{i=1}^{\\tau}A_{i}\\right)^{2}\\geq\\left(\\sum_{i=1}^{\\tau}B_{i}^{2}+y\\right)\\left(x+\\log\\left(1+\\frac{1}{y}\\sum_{i=1}^{\\tau}B_{i}^{2}\\right)\\right)\\right\\}\\leq e^{-x/2}\\,.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Lemma 23 is an immediate consequence of Theorem 2.1 in de la Pena et al. (2004), in particular comparing our Definition 22 with their condition (1.4). For more background, see Lattimore and Szepesv\u00e1ri (2020), Lemma 20.2, and the surrounding discussion. ", "page_idx": 17}, {"type": "text", "text": "We will also need the following three claims, verified in Appendix H. Therein, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\sigma_{t}=\\sqrt{2Q_{t}^{2}+Q_{t}R_{t}}\\,.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Claim 24. Each $W_{t+1}$ is $\\sigma(\\mathcal{F}_{t}\\cup\\mathcal{A}_{t}^{\\prime\\prime})$ -conditionally $\\sigma_{t}/\\sqrt{m}$ -subgaussian. ", "page_idx": 17}, {"type": "text", "text": "Claim 25. For any $\\begin{array}{r}{0\\leq t\\leq\\tau<T,1+\\sum_{i=\\tau}^{t}\\sigma_{i}^{2}\\leq3+\\sum_{i=\\tau}^{t}Q_{i}R_{i}.}\\end{array}$ ", "page_idx": 17}, {"type": "text", "text": "Claim 26. For any $0\\leq t\\leq\\tau<T_{i}$ , on the event $\\begin{array}{r}{\\{R_{0}\\leq2\\},1+\\sum_{i=\\tau}^{t}\\sigma_{i}^{2}\\leq(3+2T)^{2}.}\\end{array}$ ", "page_idx": 17}, {"type": "text", "text": "Proof of Lemma $^{12}$ . For any $\\tau<T$ , by Claim 24 and Lemma 23 applied with $y=1/m$ and any $x>0$ , with probability $1-\\exp\\{-\\frac{x}{2}\\}$ , for all $t\\in\\{\\tau,\\dots,T-1\\}$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\left(\\sum_{i=\\tau}^{t}W_{i+1}\\right)^{2}\\leq\\left(\\frac{x}{m}+\\frac{1}{m}\\log\\left(\\sum_{i=\\tau}^{t}\\sigma_{t}^{2}+1\\right)\\right)\\left(\\sum_{i=1}^{\\tau}\\sigma_{i}^{2}+1\\right).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We take $x=2\\log(3+2T)$ , which by Claim 26 exceeds $\\scriptstyle\\log(\\sum_{i=\\tau}^{t}\\sigma_{t}^{2}+1)$ on $\\{R_{0}\\leq2\\}$ . Therefore, on the intersection of $\\{R_{0}\\leq2\\}$ and the event implicitly defined above, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\left|\\sum_{i=\\tau}^{t}W_{i+1}\\right|\\leq\\sqrt{\\frac{2x}{m}}\\sqrt{\\sum_{i=1}^{\\tau}\\sigma_{i}^{2}+1}\\,.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "By our assumption on $m$ , $\\sqrt{2x/m}\\leq1/10$ . Also, and using a trivial bound and Claim 25, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\sqrt{\\sum_{i=1}^{\\tau}\\sigma_{i}^{2}+1}\\leq\\sum_{i=1}^{\\tau}\\sigma_{i}^{2}+1\\leq3+\\sum_{i=\\tau}^{t}Q_{i}R_{i}\\,,\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "which combined with the display above yields the form of the claimed bound. ", "page_idx": 17}, {"type": "text", "text": "To conclude the proof, we take a union bound over $\\tau\\,\\in\\,\\{0,\\dots,T-1\\}$ and note that, by our assumption on $m$ and choice of $x$ , $x/2\\leq m/400$ . \u53e3 ", "page_idx": 17}, {"type": "text", "text": "H Proofs of Claims 11 and 24 to 26 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Let $\\mathbb{F}_{t}^{\\prime\\prime}=\\sigma(\\mathcal{F}_{t}\\cup\\mathcal{A}_{t}^{\\prime\\prime}\\cup\\sigma(X_{t+1}))$ , where we recall that $\\mathcal{A}_{t}^{\\prime\\prime}=\\sigma(\\mathcal{A}_{t}\\cup\\sigma(\\xi_{t+1},J_{t+1},X_{t+1}))$ for each $t\\in\\mathbb{N}$ , that $\\mathbb{E}_{t}^{\\prime\\prime}$ denotes the $\\mathbb{F}_{t}^{\\prime\\prime}$ -conditional expectation, and that the purpose of conditioning on $\\mathbb{F}_{t}^{\\prime\\prime}$ will be to integrate out the random targets $U_{t+1}^{1^{-}},\\dots,U_{t+1}^{m}$ \u00b7 ", "page_idx": 17}, {"type": "text", "text": "Proof of Claim $_{l l}$ . Fix $u\\in\\mathbb{S}_{2}^{d-1}$ and note that ", "page_idx": 18}, {"type": "equation", "text": "$$\nR_{t+1}^{j}=\\frac{\\langle u,S_{t}^{j}+U_{t+1}^{j}X_{t+1}\\rangle^{2}}{\\|u\\|_{V_{t+1}}^{2}}=\\frac{\\langle u,S_{t}^{j}\\rangle^{2}+(U_{t+1}^{j})^{2}\\langle u,X_{t+1}\\rangle^{2}+2U_{t+1}^{j}\\langle u,S_{t}^{j}\\rangle\\langle u,X_{t+1}\\rangle}{\\|u\\|_{V_{t}}^{2}+\\langle u,X_{t+1}\\rangle^{2}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Observe that $X_{t+1}$ and $\\begin{array}{r}{S_{t}^{j}=S_{0}^{j}+\\sum_{\\substack{s\\leq t}}U_{s}^{j}X_{s}}\\end{array}$ are $\\mathbb{F}_{t}^{\\prime\\prime}$ -measurable and that $U_{t+1}^{j}$ is independent of $\\mathbb{F}_{t}^{\\prime\\prime}$ . The latter of these gives $\\mathbb{E}_{t}^{\\prime\\prime}U_{t+1}^{j}=0$ and $\\mathbb{E}_{t}^{\\prime\\prime}(U_{t+1}^{j})^{2}=\\frac{2}{3}$ . With that, we have that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{t}^{\\eta}R_{t+1}^{j}-R_{t}^{j}=\\frac{\\langle u,S_{t}^{j}\\rangle^{2}+\\frac{2}{3}\\langle u,X_{t+1}\\rangle^{2}}{\\|u\\|_{V_{t}}^{2}+\\langle u,X_{t+1}\\rangle^{2}}-\\frac{\\langle u,S_{t}^{j}\\rangle^{2}}{\\|u\\|_{V_{t}}^{2}}}\\\\ &{\\quad\\quad\\quad\\quad\\quad=\\frac{2}{3}\\langle u,X_{t+1}\\rangle^{2}\\|u\\|_{V_{t}}^{2}-\\langle u,S_{t}^{j}\\rangle^{2}\\langle u,X_{t+1}\\rangle^{2}}\\\\ &{\\quad\\quad\\quad\\quad\\quad=\\frac{\\langle u,\\vert\\vert u\\vert_{V_{t}}^{2}\\big(\\vert u\\vert\\vert_{V_{t}}^{2}+\\langle u,X_{t+1}\\rangle^{2}\\big)}{\\vert u\\vert\\vert_{V_{t}}^{2}+\\langle u,X_{t+1}\\rangle^{2}}\\Big.}\\\\ &{\\quad\\quad\\quad\\quad\\quad=\\frac{\\langle u,X_{t+1}\\rangle^{2}}{\\|u\\vert\\vert_{V_{t}}^{2}+\\langle u,X_{t+1}\\rangle^{2}}\\left(\\frac{2}{3}-\\frac{\\langle u,S_{t}^{j}\\rangle^{2}}{\\|u\\vert\\vert_{V_{t}}^{2}}\\right)}\\\\ &{\\quad\\quad\\quad\\quad\\quad=Q_{t}\\left(\\frac{2}{3}-R_{t}^{j}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The statement follows by averaging over $j\\in\\{1,\\ldots,m\\}$ . ", "page_idx": 18}, {"type": "text", "text": "Proof of Claim 24. Subtracting the first expression on the right-hand side of equation (15) from equation (14) and averaging over $j\\in\\{1,\\ldots,m\\}$ , we see that ", "page_idx": 18}, {"type": "equation", "text": "$$\nW_{t+1}=R_{t+1}-\\mathbb{E}_{t}^{\\prime\\prime}R_{t+1}=\\frac{Q_{t}}{m}\\sum_{j=1}^{m}\\left((U_{t+1}^{j})^{2}-\\frac{2}{3}\\right)+\\frac{1}{m}\\sum_{j=1}^{m}U_{t+1}^{j}H_{t}^{j}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $H_{t}^{j}~=~\\langle u,X_{t+1}\\rangle\\langle u,S_{t}^{j}\\rangle/\\|u\\|_{V_{t+1}}^{2}$ . Note that $Q_{t}$ and $H_{t}$ are $\\mathbb{F}_{t}^{\\prime\\prime}$ measurable and that $U_{t+1}^{1},\\ldots,U_{t+1}^{m}$ are independent of $\\mathbb{F}_{t}^{\\prime\\prime}$ and one another, and that their absolute values are bounded by 1. Thus, examining the two terms in the sum we see that: ", "page_idx": 18}, {"type": "equation", "text": "$$\n(H_{t})^{2}:=\\frac{1}{m}\\sum_{j=1}^{m}(H_{t}^{j})^{2}=\\frac{1}{m}\\sum_{j=1}^{m}\\frac{\\langle u,X_{t+1}\\rangle^{2}\\langle u,S_{t}^{j}\\rangle^{2}}{\\|u\\|_{V_{t+1}}^{4}}=\\frac{Q_{t}}{m}\\sum_{j=1}^{m}\\frac{\\langle u,S_{t}^{j}\\rangle^{2}}{\\|u\\|_{V_{t+1}}^{2}}=Q_{t}R_{t}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The result follows by recalling that the sum of an $a$ -subgaussian random variable and a $b$ -subgaussian random variable is $\\sqrt{2(a^{2}+b^{2})}$ -subgaussian. \u53e3 ", "page_idx": 18}, {"type": "text", "text": "The proof of the final two claims will require the following simple lemma. ", "page_idx": 18}, {"type": "text", "text": "Lemma 27. Let $b_{1},b_{2},\\ldots$ . be a sequence of real numbers in $[0,1]$ . Then, for any $\\lambda>0$ and $n\\in\\mathbb{N}^{+}$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\sum_{j=1}^{n}\\frac{b_{j}}{\\lambda+\\sum_{i=1}^{j}b_{i}}\\leq\\log(1+n/\\lambda)\\quad a n d\\quad\\sum_{j=1}^{n}\\left(\\frac{b_{j}}{\\lambda+\\sum_{i=1}^{j}b_{i}}\\right)^{2}\\leq\\frac{n}{\\lambda(\\lambda+n)}\\leq\\frac{1}{\\lambda}\\,.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof. Let B0 = 0 and for j \u2208[n] let Bj = Bj\u22121 + bj. The function f(x) =\u03bb1+x is decreasing on $[0,\\infty)$ . Hence, $\\begin{array}{r}{\\int_{B_{j-1}}^{B_{j}}f(x)d x\\geq b_{j}f(B_{j})}\\end{array}$ . Summing these up, ", "page_idx": 18}, {"type": "equation", "text": "$$\n[\\log(\\lambda+x)]_{0}^{B_{n}}=\\int_{0}^{B_{n}}f(x)d x=\\sum_{j=1}^{n}\\int_{B_{j-1}}^{B_{j}}f(x)d x\\geq\\sum_{j=1}^{n}b_{j}f(B_{j})=\\sum_{j=1}^{n}{\\frac{b_{j}}{\\lambda+\\sum_{i=1}^{j}b_{i}}}\\,.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Evaluating the left-hand side and noting that $B_{n}\\leq n$ holds because $b_{i}\\leq1$ gives the first result. For the second sum, we use a similar argument with $g(x)=1/(\\lambda+x)^{2}$ : ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{j=1}^{n}\\left(\\frac{b_{j}}{\\lambda+\\sum_{i=1}^{j}b_{i}}\\right)^{2}=\\sum_{j=1}^{n}\\frac{b_{j}^{2}}{(\\lambda+B_{j})^{2}}}}\\\\ &{}&{\\leq\\displaystyle\\sum_{j=1}^{n}\\frac{b_{j}}{(\\lambda+B_{j})^{2}}}&{\\mathrm{(because\\}0\\leq b_{j}\\leq1)}\\\\ &{}&{\\leq\\displaystyle\\int_{0}^{n}g(x)d x}&{\\mathrm{(\\gis\\decreasing\\on\\}[0,\\infty),B_{n}\\leq n)}\\\\ &{}&{=\\left[\\frac{-1}{\\lambda+x}\\right]_{0}^{n}=\\frac{n}{\\lambda(\\lambda+n)}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof of Claim 25. Noting that since $\\|u\\|_{2}=1$ and $\\lambda\\geq5$ , by Lemma 27, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\sum_{i=\\tau}^{t}Q_{i}\\leq\\sum_{i=0}^{T-1}\\frac{\\langle u,X_{i+1}\\rangle^{2}}{\\lambda+\\sum_{j=0}^{i+1}\\langle u,X_{j}\\rangle^{2}}\\leq\\log(1+T/5)\\leq T\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\sum_{i=\\tau}^{t}Q_{i}^{2}\\leq\\sum_{i=0}^{T-1}Q_{i}^{2}=\\sum_{i=0}^{T-1}\\left(\\frac{\\langle u,X_{i+1}\\rangle^{2}}{\\lambda+\\sum_{j=0}^{i+1}\\langle u,X_{j}\\rangle^{2}}\\right)^{2}\\leq\\frac{1}{\\lambda}\\leq1\\,.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Using these, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n1+\\sum_{i=\\tau}^{t}\\sigma_{i}^{2}=1+2\\sum_{i=\\tau}^{t}Q_{i}^{2}+\\sum_{i=\\tau}^{t}R_{i}Q_{i}\\leq3+\\sum_{i=\\tau}^{t}R_{i}Q_{i}\\leq3+T\\operatorname*{max}_{\\tau\\leq i\\leq t}R_{i}\\,.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof of Claim 26. Since $(a+b)^{2}\\leq2a^{2}+2b^{2}$ and by symmetry, ", "page_idx": 19}, {"type": "equation", "text": "$$\nR_{i}^{j}=\\frac{\\langle u,S_{0}^{j}+\\sum_{i=1}^{i}U_{\\ell}^{j}X_{\\ell}\\rangle^{2}}{\\lambda+\\sum_{\\ell=1}^{i}\\langle u,X_{\\ell}\\rangle^{2}}\\leq2R_{0}^{j}+2\\frac{\\left(\\sum_{\\ell=1}^{i}\\langle u,X_{\\ell}\\rangle\\right)^{2}}{\\lambda+\\sum_{\\ell=1}^{i}\\langle u,X_{\\ell}\\rangle^{2}}\\leq2R_{0}^{j}+2\\operatorname*{max}_{b\\in[0,1]}\\frac{(i b)^{2}}{\\lambda+i b^{2}}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "By definition, $\\begin{array}{r}{R_{i}=\\frac{1}{m}\\sum_{j=1}^{m}R_{i}^{j}}\\end{array}$ , and by assumption $R_{0}\\leq2$ and $i\\leq T\\!-\\!1$ , so $R_{i}\\leq4{+}2i\\leq2{+}2T$ And so, ", "page_idx": 19}, {"type": "equation", "text": "$$\n3+T\\operatorname*{max}_{\\tau\\leq i\\leq t}R_{i}\\leq3+T(2+2T)\\leq(3+2T)^{2}\\,.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 20}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 20}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 21}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 21}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/pu blic/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 22}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 22}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 23}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 23}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to ", "page_idx": 23}, {"type": "text", "text": "generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. ", "page_idx": 24}, {"type": "text", "text": "\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 24}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 24}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 24}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 25}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 25}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] Justification: Guid ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 25}]