[{"figure_path": "OJxua0PAIo/figures/figures_9_1.jpg", "caption": "Figure 1: Experimental results on the (left) monotone and (right) strongly monotone examples, comparing the variants of SEG. For a fair comparison, we take the number of passes over the full dataset as the abscissae. In other words, we plot ||Fz/2||2/||Fz8||\u00b2 for SEG-FFA and SEG-FF, as they pass through the whole dataset twice every epoch, and ||Fz||\u00b2/||Fz8||\u00b2 for the other methods, as they pass once every epoch.", "description": "This figure shows the experimental results for both monotone and strongly monotone cases. The x-axis represents the number of passes over the dataset. The y-axis represents the squared norm of the saddle-point gradient, normalized by the squared norm of the initial saddle-point gradient.  The plots compare the performance of different stochastic extragradient (SEG) methods: SEG-FFA, SEG-FF, SEG-RR, SEG-US, SGDA-RR, and SGDA-US. In the monotone case (left), SEG-FFA is the only method that converges, while the others diverge. In the strongly monotone case (right), SEG-FFA shows faster convergence compared to the other methods. The different convergence rates highlight the benefits of SEG-FFA.", "section": "Experiments"}, {"figure_path": "OJxua0PAIo/figures/figures_70_1.jpg", "caption": "Figure 1: Experimental results on the (left) monotone and (right) strongly monotone examples, comparing the variants of SEG. For a fair comparison, we take the number of passes over the full dataset as the abscissae. In other words, we plot ||Fz/2||2/||Fz8||2 for SEG-FFA and SEG-FF, as they pass through the whole dataset twice every epoch, and ||Fzt||2/||Fz8||2 for the other methods, as they pass once every epoch.", "description": "This figure shows the experimental results for both monotone and strongly monotone cases.  It compares the performance of SEG-FFA, SEG-FF, SEG-RR, SEG-US, SGDA-RR, and SGDA-US in terms of the squared norm of the saddle gradient.  For SEG-FFA and SEG-FF, since two passes over the data occur per epoch, the x-axis represents the number of full dataset passes divided by two. For the other methods, the x-axis is the number of full dataset passes.", "section": "Experiments"}, {"figure_path": "OJxua0PAIo/figures/figures_71_1.jpg", "caption": "Figure 3: Experimental results in the monotone example, comparing SEG-FFA and the methods proposed by Hsieh et al. [25]. By the same reason as in Figure 2, we plot ||Fz2||\u00b2/||Fz8||\u00b2 for SEG-FFA only.", "description": "This figure compares the performance of the proposed SEG-FFA algorithm against two variants of the independent-sample double stepsize stochastic extragradient (DSEG) algorithm from Hsieh et al. [25] on a monotone example.  The x-axis represents the number of passes over the dataset, and the y-axis shows ||Fz||\u00b2/||Fz\u2080||\u00b2, which is a measure of the convergence to the solution (optimum).  SEG-FFA demonstrates significantly faster convergence compared to both DSEG variants.", "section": "Experiments"}, {"figure_path": "OJxua0PAIo/figures/figures_72_1.jpg", "caption": "Figure 1: Experimental results on the (left) monotone and (right) strongly monotone examples, comparing the variants of SEG. For a fair comparison, we take the number of passes over the full dataset as the abscissae. In other words, we plot ||Fz/2||2/||Fz8||\u00b2 for SEG-FFA and SEG-FF, as they pass through the whole dataset twice every epoch, and || Fz ||\u00b2/||Fz8||\u00b2 for the other methods, as they pass once every epoch.", "description": "The figure shows the experimental results for both monotone and strongly monotone cases.  It compares the performance of SEG-FFA against other SEG variants (SEG-US, SEG-RR, SEG-FF) and SGDA variants (SGDA-US, SGDA-RR).  The x-axis represents the number of passes over the full dataset, and the y-axis shows the normalized function value (||Fz||\u00b2).  For SEG-FFA and SEG-FF (which use two passes per epoch), the y-axis shows the average of the function values for the two passes. The figure demonstrates that SEG-FFA converges while others diverge in the monotone case and shows faster convergence in the strongly monotone case.", "section": "6 Experiments"}]