[{"figure_path": "fC2SV2sQ8J/figures/figures_1_1.jpg", "caption": "Figure 1: Figure 1(a) and Figure 1(b) show the prediction results of HiVT [60] and QCNet [59] on the Argoverse 1 [4] and Argoverse 2 [51] datasets by using observed trajectories of different lengths, respectively. Figure 1(c) and Figure 1(d) display scenarios where longer trajectories perform better and shorter trajectories perform better, respectively. The red line represents the ground-truth future trajectory. The solid green and blue lines depict the observed trajectories, while the dashed green and blue lines illustrate the predicted trajectories.", "description": "This figure compares the trajectory prediction performance of two models (HiVT and QCNet) using observed trajectories of varying lengths on two datasets (Argoverse 1 and Argoverse 2).  Subfigures (a) and (b) show the model performance as a function of the length of the observed trajectory. Subfigures (c) and (d) illustrate scenarios where longer or shorter observed trajectories lead to better prediction accuracy, highlighting the challenges of using fixed-length trajectory observations in real-world scenarios.", "section": "Introduction"}, {"figure_path": "fC2SV2sQ8J/figures/figures_3_1.jpg", "caption": "Figure 2: Illustration of our LaKD framework. During training, we randomly mask historical trajectories M times to generate observed trajectories of varying lengths. Subsequently, we design a length-agnostic knowledge distillation module to dynamically transfer knowledge across trajectories of different lengths. Finally, we devise a dynamic soft-masking mechanism during gradient updates to effectively prevent knowledge conflicts. During inference, random masking, knowledge distillation, and dynamic soft-masking are not implemented.", "description": "This figure illustrates the LaKD framework, showing the three main components: random masking of historical trajectories to create varying lengths, length-agnostic knowledge distillation for dynamic knowledge transfer between trajectories of different lengths, and dynamic soft masking to prevent knowledge collisions during the distillation process.  The diagram visually depicts the flow of information and the interactions between these components during both training and inference phases.", "section": "3 Method"}, {"figure_path": "fC2SV2sQ8J/figures/figures_9_1.jpg", "caption": "Figure 3: Qualitative results on the Argoverse 2 dataset using (a) QCNet-Orig, (b) QCNet-FLN, and (c) QCNet-LaKD. The observed trajectories, ground-truth trajectories and predicted trajectories are shown in green, red and blue, respectively. Our predicted future trajectories are closer to the ground-truth, compared to other methods.", "description": "This figure shows a qualitative comparison of trajectory prediction results on the Argoverse 2 dataset using three different methods: QCNet-Orig, QCNet-FLN, and QCNet-LaKD.  Each column represents a different method. Within each column, the top row shows a scenario at a T-junction, while the bottom row depicts a scenario at a road fork where a lane change is involved. The green lines represent observed trajectories, the red lines indicate ground truth trajectories, and the blue lines show the model's predicted trajectories. The figure visually demonstrates that QCNet-LaKD produces predictions that are closer to the ground truth compared to the other two methods, suggesting the superiority of LaKD in trajectory prediction accuracy.", "section": "4 Experiments"}, {"figure_path": "fC2SV2sQ8J/figures/figures_14_1.jpg", "caption": "Figure 1: Figure 1(a) and Figure 1(b) show the prediction results of HiVT [60] and QCNet [59] on the Argoverse 1 [4] and Argoverse 2 [51] datasets by using observed trajectories of different lengths, respectively. Figure 1(c) and Figure 1(d) display scenarios where longer trajectories perform better and shorter trajectories perform better, respectively. The red line represents the ground-truth future trajectory. The solid green and blue lines depict the observed trajectories, while the dashed green and blue lines illustrate the predicted trajectories.", "description": "This figure shows the performance of two trajectory prediction models, HiVT and QCNet, using observed trajectories of varying lengths on Argoverse 1 and Argoverse 2 datasets.  Subfigures (a) and (b) present quantitative results demonstrating that prediction accuracy can vary with the length of the observed trajectory.  Subfigures (c) and (d) illustrate examples of scenarios where either longer or shorter trajectories yield better predictions, highlighting the challenge of handling trajectories of arbitrary lengths.", "section": "Introduction"}, {"figure_path": "fC2SV2sQ8J/figures/figures_15_1.jpg", "caption": "Figure 2: Illustration of our LaKD framework. During training, we randomly mask historical trajectories M times to generate observed trajectories of varying lengths. Subsequently, we design a length-agnostic knowledge distillation module to dynamically transfer knowledge across trajectories of different lengths. Finally, we devise a dynamic soft-masking mechanism during gradient updates to effectively prevent knowledge conflicts. During inference, random masking, knowledge distillation, and dynamic soft-masking are not implemented.", "description": "This figure illustrates the LaKD framework, showing the three main components: random masking of historical trajectories, length-agnostic knowledge distillation (transferring knowledge between trajectories of different lengths), and dynamic soft-masking (preventing knowledge conflicts).  The process is detailed for training, but during inference, these steps are not used.", "section": "3 Method"}, {"figure_path": "fC2SV2sQ8J/figures/figures_15_2.jpg", "caption": "Figure 2: Illustration of our LaKD framework. During training, we randomly mask historical trajectories M times to generate observed trajectories of varying lengths. Subsequently, we design a length-agnostic knowledge distillation module to dynamically transfer knowledge across trajectories of different lengths. Finally, we devise a dynamic soft-masking mechanism during gradient updates to effectively prevent knowledge conflicts. During inference, random masking, knowledge distillation, and dynamic soft-masking are not implemented.", "description": "This figure illustrates the LaKD framework, showing its three main components: random masking, length-agnostic knowledge distillation, and dynamic soft masking. Random masking creates observed trajectories of different lengths, which are then used in the distillation process. Knowledge distillation dynamically transfers knowledge among trajectories of varying lengths to improve prediction accuracy. Finally, dynamic soft masking prevents knowledge collisions during the distillation process by selectively updating the gradients of different neurons based on their importance.  During inference, these three components are not used to make the process more efficient.", "section": "3 Method"}, {"figure_path": "fC2SV2sQ8J/figures/figures_16_1.jpg", "caption": "Figure 2: Illustration of our LaKD framework. During training, we randomly mask historical trajectories M times to generate observed trajectories of varying lengths. Subsequently, we design a length-agnostic knowledge distillation module to dynamically transfer knowledge across trajectories of different lengths. Finally, we devise a dynamic soft-masking mechanism during gradient updates to effectively prevent knowledge conflicts. During inference, random masking, knowledge distillation, and dynamic soft-masking are not implemented.", "description": "This figure illustrates the LaKD framework, showing its three main components: random masking of historical trajectories, length-agnostic knowledge distillation, and dynamic soft-masking. Random masking creates trajectories of various lengths for training. Length-agnostic knowledge distillation dynamically transfers knowledge between trajectories of different lengths, improving predictions from shorter ones. Dynamic soft-masking prevents conflicts during knowledge transfer.  Inference uses the framework without the masking and distillation steps.", "section": "3 Method"}, {"figure_path": "fC2SV2sQ8J/figures/figures_16_2.jpg", "caption": "Figure 2: Illustration of our LaKD framework. During training, we randomly mask historical trajectories M times to generate observed trajectories of varying lengths. Subsequently, we design a length-agnostic knowledge distillation module to dynamically transfer knowledge across trajectories of different lengths. Finally, we devise a dynamic soft-masking mechanism during gradient updates to effectively prevent knowledge conflicts. During inference, random masking, knowledge distillation, and dynamic soft-masking are not implemented.", "description": "This figure illustrates the LaKD framework, showing the process of randomly masking historical trajectories to create various lengths of observed trajectories, then dynamically transferring knowledge across these trajectories using length-agnostic knowledge distillation. A dynamic soft-masking mechanism is incorporated to prevent knowledge collision during training. The inference process omits these steps.", "section": "3 Method"}, {"figure_path": "fC2SV2sQ8J/figures/figures_17_1.jpg", "caption": "Figure 2: Illustration of our LaKD framework. During training, we randomly mask historical trajectories M times to generate observed trajectories of varying lengths. Subsequently, we design a length-agnostic knowledge distillation module to dynamically transfer knowledge across trajectories of different lengths. Finally, we devise a dynamic soft-masking mechanism during gradient updates to effectively prevent knowledge conflicts. During inference, random masking, knowledge distillation, and dynamic soft-masking are not implemented.", "description": "This figure illustrates the LaKD framework, which consists of three main parts: random masking of historical trajectories, length-agnostic knowledge distillation, and a dynamic soft-masking mechanism.  Random masking creates observed trajectories of various lengths. Length-agnostic knowledge distillation dynamically transfers knowledge between these trajectories, improving prediction accuracy regardless of trajectory length. Finally, the dynamic soft-masking mechanism prevents knowledge conflicts during the distillation process.  The illustration visually depicts the flow of information within the framework and the interaction between its different components during training and inference.", "section": "3 Method"}]