[{"type": "text", "text": "InfoRM: Mitigating Reward Hacking in RLHF via Information-Theoretic Reward Modeling ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yuchun Miao1, Sen Zhang2, Liang $\\mathbf{Ding^{2}}$ , Rong Bao3, Lefei Zhang1,\u2217 Dacheng Tao4 ", "page_idx": 0}, {"type": "text", "text": "1 National Engineering Research Center for Multimedia Software, School of Computer Science, Wuhan University 2 The University of Sydney 3 Fudan University 4 Nanyang Technological University ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Despite the success of reinforcement learning from human feedback (RLHF) in aligning language models with human values, reward hacking, also termed reward overoptimization, remains a critical challenge. This issue primarily arises from reward misgeneralization, where reward models (RMs) compute reward using spurious features that are irrelevant to human preferences. In this work, we tackle this problem from an information-theoretic perspective and propose a framework for reward modeling, namely InfoRM, by introducing a variational information bottleneck objective to fliter out irrelevant information. Notably, we further identify a correlation between overoptimization and outliers in the IB latent space of InfoRM, establishing it as a promising tool for detecting reward overoptimization. Inspired by this finding, we propose the Cluster Separation Index (CSI), which quantifies deviations in the IB latent space, as an indicator of reward overoptimization to facilitate the development of online mitigation strategies. Extensive experiments on a wide range of settings and RM scales (70M, 440M, 1.4B, and 7B) demonstrate the effectiveness of InfoRM. Further analyses reveal that InfoRM\u2019s overoptimization detection mechanism is not only effective but also robust across a broad range of datasets, signifying a notable advancement in the field of RLHF. Code is available at: https://github.com/miaoyuchun/InfoRM. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "With the advent of large language models (LLMs), reinforcement learning from human feedback (RLHF) has emerged as a pivotal technological paradigm to align models\u2019 behaviors with human values [57, 33, 4, 25]. One of the core stages of RLHF is reward modeling, where a proxy reward model (RM) is learned to mimic human preference by training on a preference dataset that contains sets of responses with human rankings. Then a reinforcement learning (RL) stage follows to align the LLM with human preferences by optimizing rewards from the learned proxy RM. Despite empirical success, RLHF has been criticized for its vulnerability and instability [6]. One widely revealed cause is reward hacking, also known as reward overoptimization, a phenomenon where the policy model\u2019s optimization, though seemingly effective under the proxy RM, actually diverges from the true human objectives [57, 41, 16]. This issue can be manifested in various ways, from copying styles without generating meaningful content to exhibiting excessive caution in responses [10, 51]. ", "page_idx": 0}, {"type": "text", "text": "One primary cause of reward overoptimization in the reward modeling process is reward misgeneralization [6], where RMs may incorrectly generalize training data, resulting in poor proxies for actual human preference. This problem arises because the same set of human feedback can be interpreted in multiple ways by RMs, even when ample training data is available [40]. Consequently, RMs tend to depend on spurious features\u2014those unexpected or contingent elements that correlate with the ranking labels but are irrelevant to actual human preferences, such as length bias [38]. Over-exploiting such information results in RM overftiting, which significantly undermines its generalizability and poses a notable challenge for RM in handling the dynamic response distribution during the RL stage, leading to an unstable RL process [45, 29]. ", "page_idx": 0}, {"type": "image", "img_path": "3XnBVK9sD6/tmp/bdc6cf9c9cb80496479d7a2223a0ea82cb81c4ec8e4849dc45114e667f22f246.jpg", "img_caption": ["Figure 1: Comparison between standard RM and our information-theoretic reward model (InfoRM). InfoRM distinguishes itself by enhancing RM generalizability through mutual information modeling. Additionally, a distinct feature of InfoRM is its overoptimization detection mechanism, which can guide parameter selection and algorithm design in subsequent RLHF. Specifically, the RM encoder is derived from the standard RM, with modification to the final layer. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Current efforts in mitigating reward overoptimization mainly include incorporating KullbackLeibler (KL) divergence as constraints [44, 49, 33], enlarging the scale of RM [16], employing composite RMs [10, 14, 30, 36], optimizing preference dataset [56], and specifically addressing response length bias [7, 38]. However, none of these approaches take the aforementioned reward misgeneralization issue into account. ", "page_idx": 1}, {"type": "text", "text": "In this work, we propose a new reward modeling framework from an information-theoretic perspective, namely, InfoRM, which effectively addresses the aforementioned reward misgeneralization issue. InfoRM takes inspiration from the recent advancements in deep variational inference and mutual information (MI)-based learn", "page_idx": 1}, {"type": "image", "img_path": "3XnBVK9sD6/tmp/b41f76f377f659e013e97ed1b64611f1964ad20d42917d9f04292fbee6f47c03.jpg", "img_caption": ["Figure 2: Response comparison on AnthropicHelpful between RLHF models using our InfoRM and other baselines, assessed by GPT-4, demonstrating the superior performance of our method. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "ing theory [34, 18, 52]. Specifically, we translate the reward modeling problem into optimizing a variational information bottleneck (IB) objective function. This approach aims to filter out information irrelevant to human preferences from the IB latent representation, which acts as a crucial intermediary between the RM outputs and the corresponding human preferences; please see Figure 1 for comparison between standard RM and InfoRM. ", "page_idx": 1}, {"type": "text", "text": "The advantages of our framework are two-fold: Firstly, benefiting from the MI modeling, InfoRM eliminates human preference-irrelevant information from the IB latent representation to achieve generalizable human preference modeling. This approach directly addresses the reward misgeneralization challenge by ensuring that only pertinent features that genuinely reflect human preferences are retained within the IB latent space. Supporting experiments are detailed in Appendix D. Secondly, InfoRM also stands out for its potential in overoptimization detection. In particular, we discover a correlation between reward overoptimization and the emergence of numerous outliers in the latent IB space of InfoRM, a phenomenon not observed in RM without IB. Motivated by this observation, we design the Cluster Separation Index (CSI) as an indicator of reward overoptimization, which identifies such outliers by quantifying the deviations of RLHF model-generated sample distributions; please see Section 5 for experimental validation. The proposed CSI not only facilitates parameter adjustments in InfoRM within real-world scenarios when lacking the gold RM but also provides an informative tool for online mitigation strategies such as early stopping; see Appendix E.2 and G. ", "page_idx": 1}, {"type": "text", "text": "Building on these advantages, our method mitigates the risk of reward overoptimization in RLHF, resulting in enhanced RLHF performance, as illustrated in Figure 2. We summarize our main contributions as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We introduce InfoRM, a new reward modeling framework based on information theory principles, to tackle the reward misgeneralization challenges by bottlenecking the irrelevant information. \u2022 We propose CSI, an effective indicator for reward overoptimization detection, derived from our insight into the correlation between overoptimization and outliers in the IB latent space of InfoRM. \u2022 We empirically demonstrate that InfoRM significantly outperforms standard RMs in RLHF performance, particularly in mitigating reward hacking. Furthermore, our metric for detecting reward overoptimization has proven both effective and robust, marking a significant advancement in RLHF. ", "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Our work draws inspiration from two lines of research, i.e., reward overoptimization in RLHF and information bottleneck-family methods. ", "page_idx": 2}, {"type": "text", "text": "2.1 Reward Overoptimization in RLHF ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Reward hacking, also termed reward overoptimization, presents a prominent challenge in RLHF, stemming from the limitations of imperfect proxy RM for human preference [21, 57, 41]. In practice, optimizing a learned proxy RM typically results in improvements according to this proxy. However, it only enhances performance in line with the gold RM\u2014actual human preference\u2014for an initial period, after which the performance often starts to deteriorate; please see Figure 3 for an illustration. ", "page_idx": 2}, {"type": "text", "text": "To mitigate this issue, a widely adopted strategy is introducing KL divergence penalty to regulate the output deviation of the policy model from the supervised finetuning (SFT) model [44, 49, 33]. Although this strategy occasionally works in alleviating reward overoptimization, it inherently restricts the optimization landspace and is prone to overfitting [3], resulting in degraded RLHF performance [16]. Alternatively, enlarging RM scale [16], implementing RM ensembles [10, 14], and composing RMs from multiple perspectives [30, 36], have been explored to address this issue. Scaling up network size or quantity, as proposed by these approaches, presents limited feasibility and may incur significant costs, especially for models with billions of parameters [51]. Moreover, recent efforts to optimize RM training datasets [56], and address the specific issue, i.e., response length bias [7, 38], continue to overlook the human preference-irrelevant information in reward modeling, which perpetuates the issue of reward misgeneralization. ", "page_idx": 2}, {"type": "image", "img_path": "3XnBVK9sD6/tmp/b71bffe9c78037fc60b86f3032c5f82cffbaa0dfa26171711752ba94bb8bc6cf.jpg", "img_caption": ["Figure 3: An example of reward overoptimization in RLHF characterized by a declining gold score (i.e., actual human preference) and a rising proxy score (i.e., proxy RM preference). "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Our approach is distinct from existing methods by specifically targeting the underlying challenge of reward misgeneralization\u2014a fundamental driver of reward overoptimization. Consequently, our InfoRM, not only significantly reduces reward overoptimization via a single RM, but offers a valuable tool for detecting this phenomenon during RL stage, which facilitates parameter selection in real scenarios without gold RM and development of online mitigation strategies, such as early stopping. ", "page_idx": 2}, {"type": "text", "text": "2.2 Information Bottleneck-Family Methods ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Information bottleneck (IB) is a well-established technique for learning an informative and compact latent representation as a balance between the conciseness and predictive power [42, 39, 43]. To address the challenge of optimizing the corresponding mutual information, Alemi et al. [1] presents a variational approximation to the IB objective. This paradigm has successfully extended to various scenarios [19, 18, 12, 52]. Inspired by these works, we introduce the IB principle into reward modeling in RLHF and derive an optimizable variational bound for this ranking problem. Notably, while the aforementioned methods primarily use IB for extracting target-related information, our work makes a step forward by further exploring the informative and compact nature of the learned IB latent representation space, leading to the development of a tool for detecting reward overoptimization. To the best of our knowledge, this is the first effort to connect IB with RLHF and demonstrate its effectiveness in the context of LLM. ", "page_idx": 2}, {"type": "text", "text": "3 Methodology ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "3.1 Preliminary ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Reward modeling aims to learn a proxy RM that mimics the underlying human objective, providing the human preference rankings $y$ of response sets from human preference datasets where each sample is denoted as $\\pmb{x}=\\left(\\pmb{x}^{w},\\pmb{x}^{l}\\right)$ . Here, $\\pmb{x}^{w}$ and $x^{l}$ denote the chosen and rejected samples, respectively.2 Following Bradley-Terry Model [5], by employing the learned proxy RM $r_{\\theta}\\left(x\\right)$ , the preference distribution $p_{\\pmb{\\theta}}\\left(y\\right)=p_{\\pmb{\\theta}}\\left(\\pmb{x}^{w}\\succ\\pmb{x}^{l}\\right)$ can be formulated as: ", "page_idx": 3}, {"type": "equation", "text": "$$\np_{\\theta}\\left(x^{w}\\succ x^{l}\\right)=\\frac{\\exp\\left(r_{\\theta}\\left(x^{w}\\right)\\right)}{\\exp\\left(r_{\\theta}\\left(x^{w}\\right)\\right)+\\exp\\left(r_{\\theta}\\left(x^{l}\\right)\\right)},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $r_{\\theta}\\left(\\cdot\\right)$ represents the learned proxy RM and $\\pmb{\\theta}$ collects the model parameters. Standard reward modeling approaches typically regard this problem as a binary classification task and optimize a negative log-likelihood loss [44, 49, 4]: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\pmb{\\theta}}=-\\mathbb{E}_{(\\pmb{x}^{w},\\pmb{x}^{l})\\sim\\mathcal{D}}\\left[\\log\\sigma\\left(r_{\\pmb{\\theta}}\\left(\\pmb{x}^{w}\\right)-r_{\\pmb{\\theta}}\\left(\\pmb{x}^{l}\\right)\\right)\\right],\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathbf{\\mathcal{D}}=\\{(\\pmb{x}_{i},y_{i})\\}_{i=1}^{N}=\\{(\\pmb{x}_{i}^{w},\\pmb{x}_{i}^{l})\\}_{i=1}^{N}$ is the human preference dataset,3 and $\\sigma(\\cdot)$ is the logistic function. Within the domain of LLM, the proxy RM is commonly initialized with the SFT model. Subsequently, it integrates an extra linear layer at the final transformer layer, producing a single scalar prediction for the reward value. Nonetheless, as discussed in Section 1, this paradigm is prone to reward misgeneralization during the training process, focusing too much on the trivial aspects of training samples while neglecting meaningful information relevant to human preferences. As a result, although the model may exhibit exceptional performance on training data, it tends to struggle with generalizing to unseen data. This limited generalizability of RM leads to the reward overoptimization phenomenon, a critical concern in the subsequent RL process, which necessitates the generalizability of RM to the constantly evolving sample distributions. ", "page_idx": 3}, {"type": "text", "text": "3.2 Information-Theoretic Reward Modeling ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Addressing the challenge of reward misgeneralization necessitates the capacity of RM to efficiently capture information pertinent to human preferences while discarding the irrelevant details, which aids in preventing overftiting to the human preferences-irrelevant information present in the training samples, thereby significantly enhancing model generalizability [52]. ", "page_idx": 3}, {"type": "text", "text": "To this end, we tackle these challenges by reformulating the reward modeling process from an information theoretic perspective. Specifically, we quantify the human preference irrelevance and the utility of a latent representation for reward prediction in information-theoretic language. We first denote the random variables corresponding to RM input, the latent representation, and the human preference ranking as $X,S.$ , and $Y$ , respectively.4 By assuming a Gaussian distribution for the latent representation $\\boldsymbol{S}$ , we define $I_{\\mathrm{bottleneck}}=\\mathbf{\\dot{\\boldsymbol{I}}}\\left(\\mathbf{\\boldsymbol{X}};\\dot{\\mathbf{\\boldsymbol{S}}}|\\boldsymbol{Y}\\right)$ and $I_{\\mathrm{preference}}=I\\left(S;Y\\right)$ to provide quantitative measures for the irrelevance of human preferences in latent representation and the utility of latent representation for reward prediction respectively, where $I$ denotes the MI. Therefore, the objective of our information-theoretic reward modeling framework $J(\\theta)$ can be formulated as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\theta}\\ \\ J(\\theta)=\\operatorname*{max}_{\\theta}\\ \\ I_{\\mathrm{preference}}-\\beta I_{\\mathrm{bouleneck}}=\\operatorname*{max}_{\\theta}\\ I(S;Y)-\\beta I(X;S|Y),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\beta$ is a trade-off parameter, and $\\pmb{\\theta}$ encompasses all the parameters in this objective. In Eqn. (3), the latent representation $\\boldsymbol{S}$ essentially provides an information bottleneck between the input samples $\\mathbf{\\deltaX}$ and the corresponding rankings $Y$ . Due to the high dimensionality of the input sample space, it is non-trivial to evaluate these two MI. Thus, given a human preference dataset $\\bar{\\mathcal{D}}=\\{(\\bar{\\pmb{x_{i}}},y_{i})\\}_{i=1}^{N}$ and $\\pmb{\\theta}=\\{\\phi,\\psi\\}$ , we instead optimize a variational lower bound $J_{\\mathrm{VLB}}$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{J(\\phi,\\psi)\\ge J_{\\mathrm{vLB}}(\\phi,\\psi)=\\mathbb{E}_{(\\boldsymbol{x},\\boldsymbol{y})\\sim\\mathcal{D}}\\left[J_{\\mathrm{preference}}-\\beta J_{\\mathrm{botteneck}}\\right]}\\\\ &{J_{\\mathrm{preference}}=\\displaystyle\\int p_{\\phi}(\\boldsymbol{s}|\\boldsymbol{x})\\log q_{\\psi}(\\boldsymbol{y}|\\boldsymbol{s})d\\boldsymbol{s}}\\\\ &{J_{\\mathrm{botleneck}}=\\mathrm{KL}\\left[p_{\\phi}(\\boldsymbol{S}|\\boldsymbol{x}),r(\\boldsymbol{S})\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "image", "img_path": "3XnBVK9sD6/tmp/e096af8ad1b4f9e4db81d7c49ed8a753402f18b2b51a97b832ca74ba81d2a4d3.jpg", "img_caption": ["Figure 4: Simulated RLHF results for different proxy RMs (1.4B). Solid and dashed lines represent the gold and proxy scores, respectively. In later RL stages, as KL divergence increases, Standard RM shows a declining gold score and a rising proxy score, indicating overoptimization. Conversely, our InfoRM maintains consistent growth in both scores, effectively mitigating overoptimization. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "where $r(S)$ , $J_{\\mathrm{preference}}$ , and $J_{\\mathrm{bottleneck}}$ denote the variational approximation of the marginal distribution $p(S)$ ,5 the lower bound of $I_{\\mathrm{preference}}$ , and the upper bound of $I_{\\mathrm{bottleneck}}$ , respectively. Here, $p_{\\phi}(s|x)$ extract latent representations, and $q_{\\psi}(y|s)$ handles ranking prediction based on the generated representation. The parameters of these two functions are collected in $\\phi$ and $\\psi$ , respectively. ", "page_idx": 4}, {"type": "text", "text": "In our practice, the functions $p_{\\phi}(s|x)$ and $q_{\\psi}(y|s)$ are modeled by an LLM with an extra head $f_{\\phi}(\\cdot)$ for representation generation, and an MLP $g_{\\psi}(\\cdot)$ for reward prediction, respectively. Notably, $p_{\\phi}(s|x)$ is modeled as a multivariate Gaussian with a diagonal covariance structure, where the mean and covariance are both determined by the output of the encoder $f_{\\phi}({\\pmb x})$ , i.e., $f_{\\phi}^{\\pmb\\mu}({\\pmb x})$ and $f_{\\phi}^{\\pmb\\sigma}({\\pmb x})$ . Referring to Eqn. (4), the objective for our information-theoretic reward modeling reads: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\{\\phi,\\psi\\}}{\\operatorname*{max}}\\,J_{\\mathrm{vLB}}(\\phi,\\psi)\\approx\\underset{\\{\\phi,\\psi\\}}{\\operatorname*{max}}\\,\\mathbb{E}_{(\\boldsymbol{x}^{w},\\boldsymbol{x}^{l})\\sim\\mathcal{D}}\\left[L_{\\mathrm{preference}}-\\beta L_{\\mathrm{botleneck}}\\right]}\\\\ &{L_{\\mathrm{preference}}=\\log\\sigma\\left(g_{\\psi}\\big(h_{\\phi}(\\boldsymbol{x}^{w},\\boldsymbol{\\epsilon}^{w})\\big)-g_{\\psi}\\big(h_{\\phi}(\\boldsymbol{x}^{l},\\boldsymbol{\\epsilon}^{l})\\big)\\right)}\\\\ &{L_{\\mathrm{botleneck}}=\\mathrm{KL}\\left[p_{\\phi}(S|\\boldsymbol{x}^{w}),r(S)\\right]+\\mathrm{KL}\\left[p_{\\phi}(S|\\boldsymbol{x}^{l}),r(S)\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $h_{\\phi}({\\pmb x},{\\pmb\\epsilon})=f_{\\phi}^{\\pmb\\mu}+f_{\\phi}^{\\pmb\\sigma}({\\pmb x}){\\pmb\\epsilon}$ . $\\epsilon^{w}$ and $\\epsilon^{l}$ are independently sampled from $\\mathcal{N}(\\mathbf{0},\\mathbf{I})$ for each input sample. Lpreference and $L_{\\mathrm{bottleneck}}$ are the estimates of $J_{\\mathrm{preference}}$ and $J_{\\mathrm{bottleneck}}$ in Eqn. (4), respectively. Detailed derivation is provided in Appendix A, and related pseudocode is provided in Appendix J.1. ", "page_idx": 4}, {"type": "text", "text": "Remark I: Although InfoRM focuses on reward modeling, our ultimate goal is to mitigate reward overoptimization in RLHF by addressing the reward misgeneralization issue. Thus in subsequent experiments, we evaluate RLHF model performance to demonstrate the effectiveness of InfoRM. ", "page_idx": 4}, {"type": "text", "text": "4 Experiments in Reward Optimization Mitigation ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we first validate InfoRM\u2019s efficacy through simulation experiments with access to the gold RM, allowing us to clearly observe its impact on mitigating overoptimization. We then proceed to real-world scenarios without a gold RM to further verify our approach\u2019s effectiveness. ", "page_idx": 4}, {"type": "text", "text": "4.1 Simulation Experiments ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Our simulation experiments follow [16, 10], where a fixed gold RM plays the human role, providing labels (i.e., rankings) to train a proxy RM. This setup enables to intuitively assess RLHF performance and observe overoptimization, which is unavailable in real-world settings. ", "page_idx": 4}, {"type": "text", "text": "4.1.1 Setup", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Models. In our simulations, we use the Pythia suite [4] for both the policy model and the proxy RM. Specifically, the 1.4B Pythia model serves as the universal policy model utilized everywhere. For the proxy RM, we remove the embedding layers from Pythia models sized 70M, 410M, and 1.4B, adding an MLP head to output a scalar reward. Moreover, the gold RM, based on Vicuna-7B-v1.5 [9], follows the RM training protocol in AlpacaFarm [13]. Considering Vicuna\u2019s size of 7B\u2014much larger than our maximum proxy RM size of 1.4B\u2014it is reasonable to employ it as the gold RM [10]. ", "page_idx": 4}, {"type": "image", "img_path": "3XnBVK9sD6/tmp/8c9668b04ce0d59dd2f3dc58fa7675b35a93e6b71f7129a5c7e04f24bb47699a.jpg", "img_caption": ["Figure 5: Final gold rewards in simulated RLHF experiments. Left: Using proxy RMs with varying parameter sizes. Right: Conducting RL on Alpaca (in-distribution) and Flan (out-of-distribution). The proxy RMs are all trained on the same simulated preference dataset with $25\\%$ label noise. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Pipeline. Our RLHF pipeline in the simulation experiments follows [16], consisting of several key stages. Initially, both the policy model SFT and the gold RM training are performed on AlpacaFarm [13]. Next, a simulated preference dataset for proxy RM training is generated by prompting the SFT model with instructions to produce two different responses, which are then ranked by the gold RM. In line with [10], we simulate the scenario of high disagreement rates among human annotators by intentionally mislabeling $25\\%$ of this dataset, leading to two versions: one w/ and one w/o label noise. The proxy RM is then trained on these datasets. Finally, policy optimization is conducted using the PPO algorithm [37]; please see Appendix J.3 for more implementation details. ", "page_idx": 5}, {"type": "text", "text": "Data. Following [10], the training data in our simulation experiments are from AlpacaFarm [13]. In particular, 10k instruction demonstrations are utilized for the policy model SFT and $20\\mathrm{k}$ preference data is used for gold RM training. In addition, the instructions of the 20k preference data are used for response generation via the SFT model, which is then labeled by the gold RM. The remaining $20\\mathbf{k}$ unlabeled data in AlpacaFarm are used for policy optimization. It\u2019s important to note that all training data in our simulation experiments is sourced exclusively from the AlpacaFarm dataset [13], ensuring consistency of the training data distribution across three stages. ", "page_idx": 5}, {"type": "text", "text": "Baselines. Our baseline models include Supervised Fine-Tuning model (SFT), RLHF model using standard RM (Standard RM), RLHF model using standard RM with KL divergence penalty (Standard RM w/ KL) [33], and the RLHF model using ensemble RM (Ensemble RM) [10].6 ", "page_idx": 5}, {"type": "text", "text": "4.1.2 Main Results ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Figure 4 presents the simulated RLHF results for different 1.4B proxy RM w/ and w/o label noise. InfoRM consistently prevents reward overoptimization and substantially enhances RLHF performance under both noisy and noiseless scenarios. Notably, Standard RM\u2019s stability is significantly compromised with the label noise, leading to notable reward overoptimization. In contrast, InfoRM maintains stability regardless of label noise, underscoring InfoRM\u2019s ability to extract human preference-relevant information from noisy data to improve the resilience of proxy RMs. ", "page_idx": 5}, {"type": "text", "text": "Previous research [16] demonstrates that increasing the RM size enhances the performance during the RL stage, as measured by the gold RM. In Figure 5 (left), we assess the impact of varying proxy RM sizes on the final RLHF performance measured by the gold RM.7 Our findings include: (1) Information-theoretic reward modeling significantly improves performance beyond merely enlarging the RM size, making InfoRM a cost-effective and practical solution for deployment without additional computational costs. (2) InfoRM performance consistently improves as the RM size increases, suggesting our method\u2019s benefits are complementary to those from scaling the RM. ", "page_idx": 5}, {"type": "text", "text": "To assess InfoRM\u2019s generalizability, we conduct experiments using both in-distribution (AlpacaFarm) and out-of-distribution (Flan) datasets in the RL stage. The results, shown in Figure 5 (right), demonstrate that InfoRM maintains relatively stable performance on the out-of-distribution Flan dataset, unlike Standard RM, which suffers significant deterioration. This consistently exceptional performance across different datasets highlights InfoRM\u2019s superior generalizability. ", "page_idx": 5}, {"type": "table", "img_path": "3XnBVK9sD6/tmp/c2cc08f0f0759b9c44229013cc40112b2db7697d2d90c41941f7da686e90039f.jpg", "table_caption": ["Table 1: Comparison results of win, tie, and lose ratios of RLHF models using different RMs with the optimal hyper-parameters (learning rate and kl penalty) under GPT-4 evaluation. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Figure 6 presents the simulated RLHF results comparing InfoRM with Standard RM w/ KL across various KL penalty values, under a $25\\%$ label noise condition on a 1.4B proxy RM. As shown, increasing the KL penalty for Standard RM $w/\\textrm{}\\textrm{K L}$ initially helps mitigate the hacking issue, leading to gradual improvements in stability. However, when the KL penalty exceeds 0.001, the approach\u2019s effectiveness diminishes, significantly compromising the final RLHF performance. In contrast, InfoRM consistently outperforms Standard RM w/ KL. Specifically, InfoRM not only provides stronger resistance to hacking but also achieves superior training stability and better RLHF performance. ", "page_idx": 6}, {"type": "image", "img_path": "3XnBVK9sD6/tmp/2e15f2418bffa9ab7e9071ef687d64a0c14890f912be59a93ae300ca9de41845.jpg", "img_caption": ["Figure 6: Simulated RLHF results for InfoRM and Standard RM w/ KL using different KL penalty values with $25\\%$ label noise on 1.4B proxy RM. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "4.2 Real-World Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Our real-world experiments closely follow [55, 54], where the actual human preference dataset, instead of the simulated preference dataset labeled by the gold RM in simulations experiments, is utilized for proxy RM training. RM hereafter refers to proxy RM since the gold RM is absent. ", "page_idx": 6}, {"type": "text", "text": "4.2.1 Setup", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Model and Training Data. In our real-world experiments, we evaluate InfoRM on two distinct tasks: the general dialogue task and the summarization task. For the general dialogue task, we utilize Vicuna-7B-v1.5 [9], an open-source chatbot fine-tuned on LLaMA2-7B [44], as the SFT model. We then build the RM upon the architecture and weights of Vicuna-7B-v1.5 and train the RM on Anthropic-RLHF-HH [4], a large-scale human preference dataset including both helpful and harmless data. In the RL stage, this dataset is also employed to optimize the policy model initialized from the SFT model. For the summarization task, we utilize the Reddit TL;DR dataset [41] for SFT, reward modeling, and policy model optimization in the RL phase. ", "page_idx": 6}, {"type": "text", "text": "Baseline. Similar to the simulated experiments, the baseline models in the real-world experiments include Supervised Fine-Tuning model (SFT), RLHF model using standard RM (Standard RM), standard RM with KL divergence penalty (Standard RM w/ KL) [33], Ensemble RM (Ensemble RM) [10], and Weight Averaged RMs (WARM) [36]. ", "page_idx": 6}, {"type": "text", "text": "Evaluation Data. For the general dialogue task, to thoroughly evaluate the proposed method, both in-distribution and out-of-distribution data are utilized for evaluation. Specifically, in-distribution data refers to the Anthropic-RLHF-HH test set, including both helpful and harmless samples. And the out-of-distribution data is the validation set of AlpacaFarm [13], consisting of samples from the self-instruct test set[47], Vicuna test set [9, 53], and Koala test set [17]. For the summarization task, the test set of Reddit TL;DR dataset [41] is utilized in our experiments. ", "page_idx": 6}, {"type": "text", "text": "GPT-4 Evaluation. We evaluate the effectiveness of InfoRM by comparing its win ratio against baselines. Previous studies have found that GPT-4\u2019s judgments are closely related to humans [8, 55]. ", "page_idx": 6}, {"type": "image", "img_path": "3XnBVK9sD6/tmp/dd110397f4e07b547285cc50b24b9c5f49a894475046bd0dd00b751b4bce83e9.jpg", "img_caption": ["Figure 7: T-SNE visualization of the response distribution in the latent IB space of InfoRM before and after RLHF (SFT model and RLHF model), as well as the distribution of overoptimized samples from the RLHF model as judged by GPT-4. From top to bottom: The datasets used for response generation are Anthropic-Harmless and Anthropic-Helpful, respectively. From left to right: The RMs applied in RLHF are Standard RM and InfoRM, respectively. Observations: (1) Outliers in the IB latent space of InfoRM usually signify overoptimized samples. (2) Using InfoRM significantly reduces the emergence of overoptimized samples. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Therefore, we employ GPT-4 to evaluate the performance of our method and the baselines. The GPT-4 prompt used in our study is the one with the highest human agreement in AlpacaEval [24]; please see Appendix J.4 for the detailed prompt. To eliminate the position bias [46, 11], each pair of samples is assessed twice, with the order of responses reversed in each instance. ", "page_idx": 7}, {"type": "text", "text": "4.2.2 Main Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Table 1 compares the win, tie, and lose ratios under GPT-4 evaluation for our method versus other baselines. Key findings include: (1) Our InfoRM significantly outperforms Standard RM without a KL divergence penalty due to its vulnerability to spurious features within training samples and distribution shifts in RL process, leading to severe reward overoptimization. Our InfoRM leverages IB theory to enhance model generalizability, as evidenced in Section 4.1, thus remarkably reducing overoptimization. (2) Our InfoRM continues to surpass Standard RM w/ KL, despite the introduced KL divergence noticeably improving its RLHF performance. We conjecture that the KL penalty, though stabilizing RL, may restrict the optimization landspace of the policy model, thereby affecting RL effectiveness; please see Appendix E.2 for parameter sensitivity analysis in such a real scenario. (3) InfoRM is a versatile and foundational framework that integrates seamlessly with other techniques to provide complementary benefits. InfoRM not only outperforms Ensemble RM and WARM in RLHF performance but also enhances results when combined with these methods. ", "page_idx": 7}, {"type": "text", "text": "5 Detecting Overoptimization: Additional Strength of OurInfoRM ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "It is noteworthy that our InfoRM not only fliters irrelevant information to human preference, thereby significantly enhancing the performance of RLHF, but also benefits from a highly informative and compact IB latent space, facilitating the establishment of a detection mechanism for reward overoptimization through latent representations. The capacity of our overoptimization detection mechanism hinges on two pivotal points: (1) Overoptimized samples manifest as outliers in the IB latent space of InfoRM. (2) The emergence of these outliers is quantitatively signaled by our proposed indicator. ", "page_idx": 7}, {"type": "text", "text": "5.1 Outlier Behavior of Overoptimizaed Samples in IB Latent Space ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "To examine the relationship between outliers in the latent IB space of InfoRM and the overoptimized samples in the RL process, the identification of overoptimized samples is highly challenging and under-explored. To address this issue, we pioneer the use of AI feedback, such as GPT-4, to identify overoptimized samples. Specifically, drawing upon the insights from [10, 51], we first summarize common overoptimization behaviors, including excessive caution, responses that deviate from user intent, and the generation of a large volume of repetitive and meaningless text. Based on this, we then design guidelines for GPT-4 to assess whether an RLHF model response is overoptimized. Detailed prompt designs are provided in Appendix J.4. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Figure 7 provides a t-SNE visualization of the response distributions in the latent IB space of InfoRM before and after RLHF, as well as the distribution of overoptimized samples from the RLHF model as judged by GPT-4. Our key conclusions include: (1) From the left column, outliers in the IB latent space are generally indicative of overoptimized samples, supported by the observation that most overoptimized samples significantly deviate from the distribution of samples before RLHF (depicted as blue points). (2) By comparing the left and right columns, it becomes evident that the incorporation of InfoRM leads to a substantial reduction in the number of outliers after RLHF, effectively preventing the appearance of overoptimized samples. This observation aligns seamlessly with the superior performance of InfoRM, as demonstrated in both simulated and realworld experiments. Appendix C.1 presents a more comprehensive validation of these observations, and related parameter sensitivity analysis in Appendix E.1 demonstrates their robustness. ", "page_idx": 8}, {"type": "text", "text": "5.2 Detection of Outlier Emergencies and Overoptimization by the CSI Indicator ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Based on the above observation, we design a detection metric for reward overoptimization, namely, Cluster Separation Index (CSI), by quantifying the deviations in the latent IB space of InfoRM. The computation process of CSI is elaborated as follows: ", "page_idx": 8}, {"type": "text", "text": "$\\bullet$ Step 1: Perform clustering on the RLHF model outputs within the latent space of our InfoRM. Denote the clusters as $C=\\{C_{1},C_{2},...,C_{n}\\}$ , where $C_{i}$ represents the $i$ -th cluster, and $n$ is the total number of clusters. For each $C_{i}$ , compute the geometric centroid $\\mathbf{c}_{i}$ by ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\mathbf{c}_{i}=\\frac{1}{|C_{i}|}\\sum_{\\mathbf{x}\\in C_{i}}\\mathbf{x},\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $|C_{i}|$ denotes the count of points in $C_{i}$ and $\\mathbf{x}$ represents the points within $C_{i}$ . ", "page_idx": 8}, {"type": "text", "text": "\u2022 Step 2: For each cluster centroid $\\mathbf{c}_{i}$ from Step 1, identify its nearest SFT model output. Calculate the Euclidean distance $d_{i}$ between each centroid $\\mathbf{c}_{i}$ and its nearest SFT output as: ", "page_idx": 8}, {"type": "equation", "text": "$$\nd_{i}=\\operatorname*{min}_{\\mathbf{s}\\in S}\\|\\mathbf{c}_{i}-\\mathbf{s}\\|,\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $S$ represents all SFT outputs and $\\|\\cdot\\|$ indicates Euclidean distance. ", "page_idx": 8}, {"type": "text", "text": "\u2022 Step 3: CSI is calculated as the sum of weighted distances by the number of the elements in each cluster: ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\operatorname{CSI}=\\sum_{i=1}^{n}|C_{i}|\\cdot d_{i}.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "In this work, we utilize DBSCAN [15] as the clustering algorithm due to its robust empirical performance and ability to operate without a predetermined number of clusters. The pseudocode of CSI calculation is provided in Appendix J.2 for better understanding. ", "page_idx": 8}, {"type": "text", "text": "Figure 8 compares CSI values during RLHF with Standard RM and InfoRM. As observed, between $600\\textrm{-}700$ training steps, there is a sudden and substantial increase in the CSI values of Standard RM, which then persist at the highlyelevated level in subsequent steps. This abrupt change corresponds to the outlier emergence in latent space, as highlighted by the green and red boxes in Figure 8. This indicates that the proposed CSI is highly sensitive to the emergence of outliers, thus offering timely and accurate detection of reward overoptimization. Furthermore, the RLHF process with InfoRM consistently exhibits much lower CSI values, suggesting that InfoRM can significantly mitigate the reward overoptimization phenomenon, ", "page_idx": 8}, {"type": "image", "img_path": "3XnBVK9sD6/tmp/720bdeac0a95de9f2c970e7057e51db783bb6d23f6d69e90c3b40c8342389799.jpg", "img_caption": ["Figure 8: CSI values in the RLHF processes of Standard RM and InfoRM across the training steps on Anthropic-Helpful dataset. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "aligning with our previous experimental findings. Further validations of our CSI\u2019s performance on various datasets are presented in Appendix C.2. ", "page_idx": 9}, {"type": "text", "text": "Remark II: Our overoptimization detection mechanism is closely tied to InfoRM\u2019s compact IB latent space. Other RMs without IB, showing weak correlations between latent space outliers and overoptimized samples, are incompatible with this mechanism; see Appendix F for related evidence. ", "page_idx": 9}, {"type": "text", "text": "Remark III: Our overoptimization detection mechanism enhances RLHF performance in three ways. First, it facilitates parameter adjustments in InfoRM for real-world scenarios; please see Appendix E.2 for an example. Additionally, it serves as a model-based metric for overoptimization detection as verified in Appendix C.2, thus guiding the optimization of any reward model during the RLHF process, including dataset selection and algorithm design. Finally, it provides a tool for online mitigation strategies like early stopping, helping to prevent overfitting and maintain model integrity. The automated early-stopping algorithm based on our CSI is elaborated in Appendix G. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this study, we introduce InfoRM, a novel framework designed to mitigate reward overoptimization in RLHF by applying information-theoretic principles to reward modeling. Unlike existing methods that focus on implementing KL divergence constraints, expanding reward model scales, and addressing specific issues like length biases, InfoRM directly addresses the primary cause of reward overoprimization in reward modeling, i.e., reward misgeneralization, by incorporating a variational information bottleneck objective. Our RM effectively filters out information irrelevant to human preferences, ensuring only key features reflecting human values are retained. Additionally, InfoRM features CSI, a quantitative indicator from the latent IB space for detecting reward overoptimization. Experiments across various scenarios and model sizes have demonstrated InfoRM\u2019s significant effectiveness in mitigating reward overoptimization. We also empirically validate CSI\u2019s effectiveness in detecting reward overoptimization on a wide range of datasets, offering valuable guidance for future research in RLHF algorithm design, and developing online overoptimization mitigation strategies. ", "page_idx": 9}, {"type": "text", "text": "Broader Impacts ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In reinforcement learning from human feedback, reward hacking or overoptimization occurs when the policy model\u2019s optimization diverges from true human objectives, reducing the helpfulness of large language models, from generating meaningful content to displaying excessive caution. This work introduces the information bottleneck into reward modeling, significantly reducing reward overoptimization. Additionally, we propose an indicator to support online mitigation strategies, aiming to better align large models with human preferences. Our study is ethical and poses no adverse effects on society. ", "page_idx": 9}, {"type": "text", "text": "Limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Our study presents several avenues for future research. Firstly, while our evaluation includes models up to 7 billion parameters, scaling our InfoRM framework to state-of-the-art models that are orders of magnitude larger remains an exciting and unexplored direction. Furthermore, our over-optimization monitoring mechanism exhibits some latency and requires inference on test datasets, highlighting the need for the development of real-time, lightweight over-optimization detection metrics. Such metrics are crucial for enhancing the effectiveness of Reinforcement Learning from Human Feedback (RLHF). Regarding evaluations, we also observe that the win rates computed by GPT-4 are influenced by the prompt structure. Future investigations could focus on identifying optimal ways to elicit high-quality judgments from automated systems, ensuring more reliable and consistent results. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We express our gratitude to Zuchao Li for his insightful feedback on the writing of this paper and to Yuqi Zhang for her assistance with proofreading. This research / project is supported by the National Natural Science Foundation of China under Grants 62122060, 62076188, and the National Research Foundation, Singapore, and Cyber Security Agency of Singapore under its National Cybersecurity R&D Programme and CyberSG R&D Cyber Research Programme Office. Any opinions, findings and conclusions or recommendations expressed in these materials are those of the author(s) and do not reflect the views of National Research Foundation, Singapore, Cyber Security Agency of Singapore as well as CyberSG R&D Programme Office, Singapore. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Alexander A Alemi, Ian Fischer, Joshua V Dillon, and Kevin Murphy. Deep variational information bottleneck. In International Conference on Learning Representations, 2016. URL https://arxiv.org/pdf/1612.00410.   \n[2] Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph, Ben Mann, Nova DasSarma, et al. A general language assistant as a laboratory for alignment. arXiv preprint arXiv:2112.00861, 2021. URL https://arxiv.org/ abs/2112.00861. [3] Mohammad Gheshlaghi Azar, Mark Rowland, Bilal Piot, Daniel Guo, Daniele Calandriello, Michal Valko, and R\u00e9mi Munos. A general theoretical paradigm to understand learning from human preferences. arXiv preprint arXiv:2310.12036, 2023. URL https://arxiv.org/pdf/ 2310.12036.   \n[4] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022. URL https://arxiv.org/pdf/2204.05862.   \n[5] Ralph Allan Bradley and Milton E Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324\u2013345, 1952. URL https://apps. dtic.mil/sti/pdfs/ADA417190.pdf.   \n[6] Stephen Casper, Xander Davies, Claudia Shi, Thomas Krendl Gilbert, J\u00e9r\u00e9my Scheurer, Javier Rando, Rachel Freedman, Tomasz Korbak, David Lindner, Pedro Freire, et al. Open problems and fundamental limitations of reinforcement learning from human feedback. arXiv preprint arXiv:2307.15217, 2023. URL https://arxiv.org/pdf/2307.15217.   \n[7] Lichang Chen, Chen Zhu, Davit Soselia, Jiuhai Chen, Tianyi Zhou, Tom Goldstein, Heng Huang, Mohammad Shoeybi, and Bryan Catanzaro. ODIN: Disentangled reward mitigates hacking in rlhf. arXiv preprint arXiv:2402.07319, 2024. URL https://arxiv.org/abs/2402.07319.   \n[8] Yi Chen, Rui Wang, Haiyun Jiang, Shuming Shi, and Ruifeng Xu. Exploring the use of large language models for reference-free text quality evaluation: A preliminary empirical study. arXiv preprint arXiv:2304.00723, 2023. URL https://arxiv.org/pdf/2304.00723.   \n[9] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al. Vicuna: An open-source chatbot impressing gpt-4 with $90\\%*$ chatgpt quality. See https://vicuna. lmsys. org (accessed 14 April 2023), 2023. URL https://lmsys.org/blog/2023-03-30-vicuna/.   \n[10] Thomas Coste, Usman Anwar, Robert Kirk, and David Krueger. Reward model ensembles help mitigate overoptimization. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=dcjtMYkpXx.   \n[11] Nick Craswell, Onno Zoeter, Michael Taylor, and Bill Ramsey. An experimental comparison of click position-bias models. In Proceedings of the 2008 international conference on web search and data mining, pages 87\u201394, 2008. URL https://citeseerx.ist.psu.edu/document? repid=rep1&type=pdf&doi=13d72ef522b405c18f7d228c5744687609b4c3a4.   \n[12] Bin Dai, Chen Zhu, Baining Guo, and David Wipf. Compressing neural networks using the variational information bottleneck. In International Conference on Machine Learning, pages 1135\u2013 1144. PMLR, 2018. URL http://proceedings.mlr.press/v80/dai18d/dai18d.pdf.   \n[13] Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, and Tatsunori B Hashimoto. Alpacafarm: A simulation framework for methods that learn from human feedback. arXiv preprint arXiv:2305.14387, 2023. URL https://arxiv.org/pdf/2305.14387.   \n[14] Jacob Eisenstein, Chirag Nagpal, Alekh Agarwal, Ahmad Beirami, Alex D\u2019Amour, DJ Dvijotham, Adam Fisch, Katherine Heller, Stephen Pfohl, Deepak Ramachandran, et al. Helping or herding? reward model ensembles mitigate but do not eliminate reward hacking. arXiv preprint arXiv:2312.09244, 2023. URL https://arxiv.org/pdf/2312.09244.   \n[15] Martin Ester, Hans-Peter Kriegel, J\u00f6rg Sander, Xiaowei Xu, et al. A density-based algorithm for discovering clusters in large spatial databases with noise. In kdd, volume 96, pages 226\u2013231, 1996. URL https://cdn.aaai.org/KDD/1996/KDD96-037.pdf.   \n[16] Leo Gao, John Schulman, and Jacob Hilton. Scaling laws for reward model overoptimization. In International Conference on Machine Learning, pages 10835\u201310866. PMLR, 2023. URL https://proceedings.mlr.press/v202/gao23h/gao23h.pdf.   \n[17] Xinyang Geng, Arnav Gudibande, Hao Liu, Eric Wallace, Pieter Abbeel, Sergey Levine, and Dawn Song. Koala: A dialogue model for academic research. Blog post, April 2023. URL https://bair.berkeley.edu/blog/2023/04/03/koala/.   \n[18] Anirudh Goyal, Riashat Islam, DJ Strouse, Zafarali Ahmed, Hugo Larochelle, Matthew Botvinick, Yoshua Bengio, and Sergey Levine. Infobot: Transfer and exploration via the information bottleneck. In International Conference on Learning Representations, 2018. URL https://arxiv.org/pdf/1901.10902.   \n[19] Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control: Learning behaviors by latent imagination. In International Conference on Learning Representations, 2019. URL https://arxiv.org/pdf/1912.01603.   \n[20] Shengding Hu, Yifan Luo, Huadong Wang, Xingyi Cheng, Zhiyuan Liu, and Maosong Sun. Won\u2019t get fooled again: Answering questions with false premises. arXiv preprint arXiv:2307.02394, 2023. URL https://aclanthology.org/2023.acl-long.309/.   \n[21] Borja Ibarz, Jan Leike, Tobias Pohlen, Geoffrey Irving, Shane Legg, and Dario Amodei. Reward learning from human preferences and demonstrations in atari. Advances in neural information processing systems, 31, 2018. URL https://proceedings.neurips.cc/paper_ files/paper/2018/file/8cbe9ce23f42628c98f80fa0fac8b19a-Paper.pdf.   \n[22] Jiaming Ji, Mickel Liu, Josef Dai, Xuehai Pan, Chi Zhang, Ce Bian, Boyuan Chen, Ruiyang Sun, Yizhou Wang, and Yaodong Yang. Beavertails: Towards improved safety alignment of llm via a human-preference dataset. Advances in Neural Information Processing Systems, 36, 2024. URL https://arxiv.org/abs/2307.04657.   \n[23] Andreas K\u00f6pf, Yannic Kilcher, Dimitri von R\u00fctte, Sotiris Anagnostidis, Zhi Rui Tam, Keith Stevens, Abdullah Barhoum, Duc Nguyen, Oliver Stanley, Rich\u00e1rd Nagyf,i et al. Openassistant conversations-democratizing large language model alignment. Advances in Neural Information Processing Systems, 36, 2024. URL https://arxiv.org/abs/2304.07327.   \n[24] Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Alpacaeval: An automatic evaluator of instruction-following models. https://github.com/tatsu-lab/alpaca_eval, 2023.   \n[25] Zuchao Li, Shitou Zhang, Hai Zhao, Yifei Yang, and Dongjie Yang. Batgpt: A bidirectional autoregessive talker from generative pre-trained transformer, 2023. URL https://arxiv.org/ abs/2307.00360.   \n[26] Stephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how models mimic human falsehoods. arXiv preprint arXiv:2109.07958, 2021. URL https://arxiv.org/abs/ 2109.07958.   \n[27] Shayne Longpre, Yi Lu, and Joachim Daiber. Mkqa: A linguistically diverse benchmark for multilingual open domain question answering. Transactions of the Association for Computational Linguistics, 9:1389\u20131406, 2021. URL https://aclanthology.org/2021.tacl-1.82.pdf.   \n[28] Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V Le, Barret Zoph, Jason Wei, et al. The flan collection: Designing data and methods for effective instruction tuning. In International Conference on Machine Learning, pages 22631\u201322648. PMLR, 2023. URL https://arxiv.org/abs/2301.13688.   \n[29] Eric J Michaud, Adam Gleave, and Stuart Russell. Understanding learned reward functions. arXiv preprint arXiv:2012.05862, 2020. URL https://arxiv.org/pdf/2012.05862.   \n[30] Ted Moskovitz, Aaditya K Singh, DJ Strouse, Tuomas Sandholm, Ruslan Salakhutdinov, Anca D Dragan, and Stephen McAleer. Confronting reward model overoptimization with constrained rlhf. arXiv preprint arXiv:2310.04373, 2023. URL https://arxiv.org/pdf/2310.04373.   \n[31] Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, and Ahmed Awadallah. Orca: Progressive learning from complex explanation traces of gpt-4. arXiv preprint arXiv:2306.02707, 2023. URL https://arxiv.org/abs/2306.02707.   \n[32] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332, 2021. URL https://arxiv.org/abs/2112.09332.   \n[33] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730\u201327744, 2022. URL https://proceedings.neurips.cc/paper_files/ paper/2022/file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf.   \n[34] Ben Poole, Sherjil Ozair, Aaron Van Den Oord, Alex Alemi, and George Tucker. On variational bounds of mutual information. In International Conference on Machine Learning, pages 5171\u2013 5180. PMLR, 2019. URL http://proceedings.mlr.press/v97/poole19a/poole19a.pdf.   \n[35] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations toward training trillion parameter models. In SC20: International Conference for High Performance Computing, Networking, Storage and Analysis, pages 1\u201316. IEEE, 2020. URL https://ieeexplore.ieee.org/abstract/document/9355301.   \n[36] Alexandre Ram\u00e9, Nino Vieillard, L\u00e9onard Hussenot, Robert Dadashi, Geoffrey Cideron, Olivier Bachem, and Johan Ferret. Warm: On the benefits of weight averaged reward models. arXiv preprint arXiv:2401.12187, 2024. URL https://arxiv.org/abs/2401.12187.   \n[37] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. URL https://arxiv. org/pdf/1707.06347.   \n[38] Wei Shen, Rui Zheng, Wenyu Zhan, Jun Zhao, Shihan Dou, Tao Gui, Qi Zhang, and Xuanjing Huang. Loose lips sink ships: Mitigating length bias in reinforcement learning from human feedback. In The 2023 Conference on Empirical Methods in Natural Language Processing, 2023. URL https://arxiv.org/abs/2310.05199.   \n[39] Ravid Shwartz-Ziv and Naftali Tishby. Opening the black box of deep neural networks via information. arXiv preprint arXiv:1703.00810, 2017. URL https://arxiv.org/pdf/1703. 00810.   \n[40] Joar Max Viktor Skalse, Matthew Farrugia-Roberts, Stuart Russell, Alessandro Abate, and Adam Gleave. Invariance in policy optimisation and partial identifiability in reward learning. In International Conference on Machine Learning, pages 32033\u201332058. PMLR, 2023. URL https://arxiv.org/abs/2203.07475.   \n[41] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F Christiano. Learning to summarize with human feedback. Advances in Neural Information Processing Systems, 33:3008\u20133021, 2020. URL https://proceedings.neurips.cc/paper/2020/file/ 1f89885d556929e98d3ef9b86448f951-Paper.pdf.   \n[42] Naftali Tishby and Noga Zaslavsky. Deep learning and the information bottleneck principle. In 2015 ieee information theory workshop (itw), pages 1\u20135. IEEE, 2015. URL https://arxiv. org/pdf/1503.02406.   \n[43] Naftali Tishby, Fernando C Pereira, and William Bialek. The information bottleneck method. arXiv preprint physics/0004057, 2000. URL https://arxiv.org/pdf/physics/0004057.   \n[44] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. URL https: //arxiv.org/pdf/2307.09288.   \n[45] Binghai Wang, Rui Zheng, Lu Chen, Yan Liu, Shihan Dou, Caishuang Huang, Wei Shen, Senjie Jin, Enyu Zhou, Chenyu Shi, et al. Secrets of rlhf in large language models part ii: Reward modeling. arXiv preprint arXiv:2401.06080, 2024. URL https://arxiv.org/pdf/2401. 06080.   \n[46] Xuanhui Wang, Nadav Golbandi, Michael Bendersky, Donald Metzler, and Marc Najork. Position bias estimation for unbiased learning to rank in personal search. In Proceedings of the eleventh ACM international conference on web search and data mining, pages 610\u2013618, 2018. URL https://dl.acm.org/doi/pdf/10.1145/3159652.3159732.   \n[47] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language model with self generated instructions. arXiv preprint arXiv:2212.10560, 2022. URL https://arxiv.org/pdf/2212.10560.   \n[48] Zhilin Wang, Yi Dong, Jiaqi Zeng, Virginia Adams, Makesh Narsimhan Sreedhar, Daniel Egert, Olivier Delalleau, Jane Polak Scowcroft, Neel Kant, Aidan Swope, et al. Helpsteer: Multi-attribute helpfulness dataset for steerlm. arXiv preprint arXiv:2311.09528, 2023. URL https://arxiv.org/abs/2311.09528.   \n[49] Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Ce Bian, Chao Yin, Chenxu Lv, Da Pan, Dian Wang, Dong Yan, et al. Baichuan 2: Open large-scale language models. arXiv preprint arXiv:2309.10305, 2023. URL https://arxiv.org/pdf/2309.10305.   \n[50] Sohee Yang, Jonghyeon Kim, Joel Jang, Seonghyeon Ye, Hyunji Lee, and Minjoon Seo. Improving probability-based prompt selection through unified evaluation and analysis. arXiv preprint arXiv:2305.14877, 2023. URL https://arxiv.org/abs/2305.14877.   \n[51] Yuanzhao Zhai, Han Zhang, Yu Lei, Yue Yu, Kele Xu, Dawei Feng, Bo Ding, and Huaimin Wang. Uncertainty-penalized reinforcement learning from human feedback with diverse reward lora ensembles. arXiv preprint arXiv:2401.00243, 2023. URL https://arxiv.org/pdf/ 2401.00243.   \n[52] Sen Zhang, Jing Zhang, and Dacheng Tao. Information-theoretic odometry learning. International Journal of Computer Vision, 130(11):2553\u20132570, 2022. URL https://link.springer. com/article/10.1007/s11263-022-01659-9.   \n[53] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. arXiv preprint arXiv:2306.05685, 2023. URL https://arxiv.org/pdf/2306. 05685.   \n[54] Rui Zheng, Shihan Dou, Songyang Gao, Yuan Hua, Wei Shen, Binghai Wang, Yan Liu, Senjie Jin, Yuhao Zhou, Limao Xiong, Lu Chen, Zhiheng Xi, Nuo Xu, Wenbin Lai, Minghao Zhu, Haoran Huang, Tao Gui, Qi Zhang, and Xuanjing Huang. Delve into PPO: Implementation matters for stable RLHF. In NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following, 2023. URL https://openreview.net/forum?id=rxEmiOEIFL.   \n[55] Rui Zheng, Wei Shen, Yuan Hua, Wenbin Lai, Shihan Dou, Yuhao Zhou, Zhiheng Xi, Xiao Wang, Haoran Huang, Tao Gui, et al. Improving generalization of alignment with human preferences through group invariant learning. In The Twelfth International Conference on Learning Representations, 2024. URL https://arxiv.org/html/2310.11971v3.   \n[56] Banghua Zhu, Michael I Jordan, and Jiantao Jiao. Iterative data smoothing: Mitigating reward overftiting and overoptimization in rlhf. arXiv preprint arXiv:2401.16335, 2024. URL https: //arxiv.org/abs/2401.16335.   \n[57] Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019. URL https://arxiv.org/pdf/1909.08593. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Derivation for the Loss of Our InfoRM ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Let $X,S.$ , and $Y$ denote the random variable of reward model input, latent representation, and human preference ranking, respectively. According to the well-established variational bounds for MI [1], the variational lower bound of our IB objective can be formulated as follows: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{J(\\pmb\\theta)=I(\\pmb S;Y)-\\beta I(\\pmb X;\\pmb S|Y)}\\\\ &{\\quad\\quad\\geq I(\\pmb S;Y)-\\beta I(\\pmb X;\\pmb S)}\\\\ &{\\quad\\quad\\geq\\mathbb E_{(\\pmb x,y)}\\left[\\int p_{\\phi}(\\pmb s|\\pmb x)\\log q_{\\psi}(y|\\pmb s)d s\\right]-\\beta\\,\\mathbb E_{\\pmb x}\\left[\\mathrm{KL}(p_{\\phi}(\\pmb S|\\pmb x),r(\\pmb S))\\right]\\stackrel{\\triangle}{=}L,}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $r(s)=\\mathcal{N}(s;\\mathbf{0},\\mathbf{I})$ is the variational approximation of the marginal distribution $p(s)$ . Notably, $p_{\\phi}(s|x)$ is modeled as a multivariate Gaussian with a diagonal covariance structure, where the mean and covariance are both determined by the output of the encoder $f_{\\phi}({\\pmb x})$ , i.e., $f_{\\phi}^{\\pmb\\mu}({\\pmb x})$ and $f_{\\phi}^{\\pmb\\sigma}({\\pmb x})$ . The first output, $f_{\\phi}^{\\pmb\\mu}({\\pmb x})$ , represents the $K$ -dimensional mean of the latent representation $\\pmb{s}$ . The second output, $f_{\\phi}^{\\pmb\\sigma}({\\pmb x})$ is squared to form the diagonal elements of the $K\\times K$ diagonal covariance matrix $\\Sigma$ . The relationship between $f_{\\phi}^{\\pmb{\\mu}}({\\pmb x})$ , $f_{\\phi}^{\\pmb{\\sigma}}({\\pmb x})$ , and $p_{\\phi}(s|x)$ can be formulated as follows: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle p_{\\phi}({\\pmb s}\\mid{\\pmb x})=\\mathcal{N}({\\pmb s}\\mid f_{\\phi}^{\\mu}({\\pmb x}),f_{\\phi}^{\\sigma}({\\pmb x}))}\\ ~}\\\\ {{\\displaystyle~~~~~~~~~~~=\\frac{1}{\\sqrt{(2\\pi)^{k}|{\\pmb\\Sigma}|}}\\exp\\left(-\\frac{1}{2}({\\pmb s}-f_{\\phi}^{\\mu}({\\pmb x}))^{\\top}{\\pmb\\Sigma}^{-1}({\\pmb s}-f_{\\phi}^{\\mu}({\\pmb x}))\\right).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Then, given a latent representation $\\pmb{s}$ drawn from $p_{\\phi}(s|x)$ , the decoder $g_{\\psi}(\\pmb{s})$ estimates the human preference ranking $y$ based on the distribution $q_{\\psi}(y|s)$ . ", "page_idx": 14}, {"type": "text", "text": "By estimating the expectation on $(\\pmb{x},y)$ using the sample estimate based on the preference dataset $\\pmb{\\mathcal{D}}=\\{\\pmb{x}_{n},y_{n}\\}_{n=1}^{N}$ , where ${\\pmb x}_{n}$ comprises a human-chosen sample ${\\pmb x}_{n}^{w}$ and a human-rejected sample $\\pmb{x}_{n}^{l}$ , with $y_{n}$ representing the corresponding human preference ranking, the variational lower bound of our IB objective can be approximated as follows: ", "page_idx": 14}, {"type": "equation", "text": "$$\nL\\approx\\frac{1}{N}\\sum_{n=1}^{N}\\left[\\int p_{\\phi}(\\pmb{s}|\\pmb{x}_{n})\\log q_{\\psi}(y_{n}|\\pmb{s})d\\mathbf{s}-\\beta\\,\\mathrm{KL}(p_{\\phi}(\\pmb{S}|\\pmb{x}_{n}),r(\\pmb{S}))\\right].\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Based on the Gaussian distribution assumption on $p_{\\phi}(s|x)$ , we can use the reparameterization trick to write $p(s|x)d s=p(\\epsilon)d\\epsilon$ , where $\\epsilon$ is an auxiliary Gaussian random variable with independent marginal $p(\\pmb\\epsilon)$ . In this way, $\\pmb{s}$ can be expressed by a deterministic function ", "page_idx": 14}, {"type": "equation", "text": "$$\n{\\pmb s}=h_{\\phi}({\\pmb x},{\\pmb\\epsilon})=f_{\\phi}^{\\pmb\\mu}({\\pmb x})+f_{\\phi}^{\\pmb\\sigma}({\\pmb x}){\\pmb\\epsilon}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Hence, we can get the following objective function: ", "page_idx": 14}, {"type": "equation", "text": "$$\nL\\approx\\frac{1}{N}\\sum_{n=1}^{N}\\left[\\mathbb{E}_{\\epsilon_{n}\\sim p(\\epsilon)}\\left[\\log q_{\\psi}(y_{n}|h_{\\phi}(\\mathbf{x}_{n},\\epsilon_{n}))\\right]-\\beta\\,\\mathrm{KL}\\left[p_{\\phi}(S|x_{n}),r(S)\\right]\\right].\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "In our experiments, we employ a sample estimate to determine $\\mathbb{E}_{\\epsilon_{n}\\sim p_{(}\\epsilon{})}\\left[\\log q_{\\psi}(y_{n}|h_{\\phi}({\\pmb x}_{n},{\\pmb\\epsilon}_{n}))\\right]$ , by sampling a $\\epsilon_{n}$ from $p(\\pmb\\epsilon)$ for ${\\pmb x}_{n}$ , balancing computational complexity. Thus our objective can be estimated as follows: ", "page_idx": 14}, {"type": "equation", "text": "$$\nL\\approx\\frac{1}{N}\\sum_{n=1}^{N}\\left[\\log q_{\\psi}(y_{n}|h_{\\phi}(\\pmb{x}_{n},\\pmb{\\epsilon}_{n}))-\\beta\\,\\mathrm{KL}\\left[p_{\\phi}(\\pmb{S}|\\pmb{x}_{n}),r(\\pmb{S})\\right]\\right].\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "According to the Bradley-Terry Model, the human preference distribution $p(y_{n})$ can be formulated as: ", "page_idx": 14}, {"type": "equation", "text": "$$\np(y_{n})=p(\\pmb{x}_{n}^{w}\\succ\\pmb{x}_{n}^{l})=\\sigma(r(\\pmb{x}_{n}^{w})-r(\\pmb{x}_{n}^{l})),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\sigma(\\cdot)$ is the logistic function, and $r(\\cdot)$ is the reward model. Notably, in this work, reward model $r(\\cdot)$ consists of the previously mentioned encoder $f_{\\phi}(\\cdot)$ and decoder $g_{\\psi}(\\cdot)$ and can be expressed as follows: ", "page_idx": 14}, {"type": "equation", "text": "$$\nr(\\pmb{x}_{n})=g_{\\psi}(h_{\\phi}(\\pmb{x}_{n},\\pmb{\\epsilon}_{n}))=g_{\\psi}(f_{\\phi}^{\\mu}(\\pmb{x}_{n})+f_{\\phi}^{\\sigma}(\\pmb{x}_{n})\\pmb{\\epsilon}_{n}).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Combining the two equations, we obtain: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\log q_{\\psi}(y_{n}|h_{\\phi}(\\pmb{x}_{n},\\pmb{\\epsilon}_{n}))=\\log\\sigma(g_{\\psi}(h_{\\phi}(\\pmb{x}_{n}^{w},\\pmb{\\epsilon}_{n}^{w}))-g_{\\psi}(h_{\\phi}(\\pmb{x}_{n}^{l},\\pmb{\\epsilon}_{n}^{l}))),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\epsilon_{n}^{w}$ and $\\epsilon_{n}^{l}$ are independently sampled from $\\mathcal{N}(\\mathbf{0},\\mathbf{I})$ for each input sample, ${\\pmb x}_{n}^{w}$ and $\\pmb{x}_{n}^{l}$ . Now, our estimation of the objective becomes: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\cal L}\\approx\\displaystyle\\frac{1}{N}\\sum_{n=1}^{N}\\left[\\log\\sigma(g_{\\psi}(h_{\\phi}(\\pmb{x}_{n}^{w},\\pmb{\\epsilon}_{n}^{w}))-g_{\\psi}(h_{\\phi}(\\pmb{x}_{n}^{l},\\pmb{\\epsilon}_{n}^{l})))\\right]}\\\\ &{\\quad-\\beta\\displaystyle\\frac{1}{N}\\sum_{n=1}^{N}\\left[\\mathrm{KL}\\left[p_{\\phi}(\\pmb{S}|\\pmb{x}_{n}^{w}),r(\\pmb{S})\\right]+\\mathrm{KL}\\left[p_{\\phi}(\\pmb{S}|\\pmb{x}_{n}^{l}),r(\\pmb{S})\\right]\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "in which $\\mathrm{KL}\\left[p_{\\phi}(S|\\pmb{x}_{n}),r(S)\\right]$ is replaced by $\\mathrm{KL}\\left[p_{\\phi}(S|x_{n}^{w}),r(S)\\right]+\\mathrm{KL}\\left[p_{\\phi}(S|x_{n}^{l}),r(S)\\right].$ Recalling that ", "page_idx": 15}, {"type": "equation", "text": "$$\nh_{\\phi}({\\pmb x},{\\pmb\\epsilon})=f_{\\phi}^{\\pmb\\mu}({\\pmb x})+f_{\\phi}^{\\pmb\\sigma}({\\pmb x}){\\pmb\\epsilon},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "we can get the final objective in our paper: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\cal L}\\approx\\frac{1}{N}\\sum_{n=1}^{N}\\left[\\log\\sigma\\left(g_{\\psi}(f_{\\phi}^{\\mu}({\\pmb x}_{n}^{w})+f_{\\phi}^{\\sigma}({\\pmb x}_{n}^{w})\\epsilon_{n}^{w})-g_{\\psi}(f_{\\phi}^{\\mu}({\\pmb x}_{n}^{l})+f_{\\phi}^{\\sigma}({\\pmb x}_{n}^{l})\\epsilon_{n}^{l})\\right)\\right]}}\\\\ {{\\displaystyle~~-\\beta\\,\\frac{1}{N}\\sum_{n=1}^{N}\\left[\\mathrm{KL}\\left[p_{\\phi}({\\pmb S}|{\\pmb x}_{n}^{w}),r({\\pmb S})\\right]+\\mathrm{KL}\\left[p_{\\phi}({\\pmb S}|{\\pmb x}_{n}^{l}),r({\\pmb S})\\right]\\right],}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\sigma(\\cdot)$ is the logistic function. ", "page_idx": 15}, {"type": "text", "text": "B Upper Bound of the Generalization Error for Our InfoRM ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The upper bound of the generalization error for our method is provided in Theorem 1 below, with the proof available in [52]. Theorem 1 demonstrates that the mutual information between the latent representation and observations, as well as the latent space dimensionality, upper bound the expected generalization error of our InfoRM method. ", "page_idx": 15}, {"type": "text", "text": "Theorem 1. Let $|S|$ be the cardinality of the latent representation space of InfoRM, $l(\\cdot)$ be the loss function following sub- $\\sigma$ -Gaussian distribution, $X$ be the reward model input, $S$ be the latent representation of InfoRM, and $\\Theta$ be the network parameters, we have the following upper bound for the expected generalization error of our InfoRM: ", "page_idx": 15}, {"type": "equation", "text": "$$\nE[R(\\Theta)-R_{T}(\\Theta)]\\leq\\exp\\left(-\\frac{L}{2}\\log\\frac{1}{\\eta}\\right)\\sqrt{\\frac{2\\sigma^{2}}{n}\\log I(X,S)}\\leq\\exp\\left(-\\frac{L}{2}\\log\\frac{1}{\\eta}\\right)\\sqrt{\\frac{2\\sigma^{2}}{n}\\log|S|},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $L,\\,\\eta_{:}$ , and n are the effective number of layers causing information loss, a constant smaller than $^{\\,l}$ , and the sample size, respectively. $R(\\Theta)=\\mathbb{E}_{X\\sim D}[l(X,\\Theta)]$ is the expected loss value given $\\Theta$ and $\\begin{array}{r}{R_{T}(\\Theta)=\\frac{1}{n}\\overleftarrow{\\sum_{i=1}^{n}l}(X_{i},\\overleftarrow{\\Theta})}\\end{array}$ is a sample estimate of $R(\\Theta)$ from the training data. ", "page_idx": 15}, {"type": "text", "text": "C Further Validations for Our Overoptimization Detection Machanism ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this section, we further validate the effectiveness and robustness of our overoptimization detection mechanism across a broad range of datasets. The core of our overoptimization detection mechanism relies on two main aspects: (1) Overoptimized samples appear as outliers in the IB latent space of our InfoRM. (2) The emergency of these outliers can be reflected through our proposed CSI indicator. We will next use sixteen diverse datasets to validate these two aspects respectively, including AlpacaFarm [13], FalseQA [20], Flan [28], HelpSteer [48], Anthropic-Helpful [4], Anthropic-Harmless [4], Mkqa [27], Oasst1 [23], OpenOrca [31], Piqa [50], PKU-SafeRLHF [22], ShareGPT8, SHP [2], Instruct- $\\mathrm{\\overline{{GPT^{9}}}}$ , TruthfulQA [26], and WebGPT [32] datasets, which encompass a wide range of scenarios. ", "page_idx": 15}, {"type": "text", "text": "C.1 Validations for Outlier Behavior of Overoptimizaed Samples in IB Latent Space ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this part, we explore the relationship between outliers in the IB latent space of InfoRM and overoptimized samples across various datasets used for response generation. The overoptimized samples are identified by GPT-4 as elaborated in Section 5. We provide visualizations of the sample distributions in the IB latent space before and after RLHF, along with the distribution of overoptimized samples, in Figures 9, 10, and 11. ", "page_idx": 16}, {"type": "text", "text": "From the left column of Figures 9, 10, and 11, it is evident that overoptimized samples consistently appear as prominent outliers in the latent IB space of InfoRM across these datasets. By comparing the left and right columns, we observe that the incorporation of InfoRM consistently results in a significant reduction in the number of outliers post-RLHF, effectively mitigating the emergence of overoptimized samples. These findings further corroborate the outlier behavior of overoptimized samples in the IB latent space, as well as the significant role of our InfoRM in mitigating overoptimization. ", "page_idx": 16}, {"type": "image", "img_path": "3XnBVK9sD6/tmp/52d75680684156b97333663b4754f4f485f3a5786374fc0189d0ef3f0ae155b6.jpg", "img_caption": ["Figure 9: T-SNE Visualization of the response distribution in the latent IB space of InfoRM before and after RLHF, as well as the distribution of overoptimized samples from the RLHF model as judged by GPT-4. From top to bottom: The datasets used for response generation are AlpacaFarm, FalseQA, Flan, and Helpsteer datasets, respectively. From left to right: The reward models applied in RLHF are Standard RM and InfoRM, respectively. ", "Dataset: Helpsteer & RM used in RLHF: Standard RM Dataset: Helpsteer & RM used in RLHF: InfoRM "], "img_footnote": [], "page_idx": 16}, {"type": "table", "img_path": "3XnBVK9sD6/tmp/c76894284a5e5f849061747040c3e8e270958d726c23bfab766956455adf20e0.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "3XnBVK9sD6/tmp/100e2e4017c13e08a6eef3bf88ab2cf972f28d986a739bd3246aa0ba643ac0c8.jpg", "img_caption": ["Dataset: Anth.-Harmless & RM used in RLHF: Standard RM "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "3XnBVK9sD6/tmp/859519f74afc96d60ee7f14856094ad559b22a3c11028a2b914fa49defe08bfb.jpg", "img_caption": ["Dataset: Anth.-Helpful & RM used in RLHF: Standard RM "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "3XnBVK9sD6/tmp/dfed0066a3d8f58f38e689491f08f88e8024b56fcc70b4c8eb4820b5764b42a2.jpg", "img_caption": ["Dataset: Mkqa & RM used in RLHF: Standard RM "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "3XnBVK9sD6/tmp/b445b8b7e4e3fc08b98c0ebafd4acc1362d742838b533ee9b629da1cf7d684c4.jpg", "img_caption": ["Dataset: Oasst1 & RM used in RLHF: Standard RM "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "3XnBVK9sD6/tmp/f07459eb48d57e55ca493b99eff9f7432f6c40e8776d8b48a8f6fe6da9c86f13.jpg", "img_caption": ["Dataset: OpenOrca & RM used in RLHF: Standard RM "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "3XnBVK9sD6/tmp/6e5ef9a87e1e4c12bc1b136aba3d484820adfa2bcf8f474bbb21f4641f7c8fec.jpg", "img_caption": ["Dataset: Piqa & RM used in RLHF: Standard RM "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "3XnBVK9sD6/tmp/a44b13fbc4ceb3348c24f6c741354cd87d225e543eb9bd8aef27a537a4f24ecc.jpg", "img_caption": [], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "Dataset: Anth.-Harmless & RM used in RLHF: InfoRM ", "page_idx": 17}, {"type": "image", "img_path": "3XnBVK9sD6/tmp/2605713631ca638ce8f1dd4ecf09970a22aab9cb1f013efaca7e5924ffb66b06.jpg", "img_caption": ["Dataset: Anth.-Helpful & RM used in RLHF: InfoRM "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "3XnBVK9sD6/tmp/a8f30155a52048c00b428860147553435140778a4b66617c958ee70cfb29fa37.jpg", "img_caption": ["Dataset: Mkqa & RM used in RLHF: InfoRM "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "3XnBVK9sD6/tmp/c54835894dc0f0d522ab66456fbdbcd9b4bd1d5bbef82b4b348e5c5085b14826.jpg", "img_caption": ["Dataset: Oasst1 & RM used in RLHF: InfoRM "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "3XnBVK9sD6/tmp/9a69c8805e775f7a6014242e81d7fb637710320d81f8b717890f427b8bb93d7d.jpg", "img_caption": ["Dataset: OpenOrca & RM used in RLHF: InfoRM "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "3XnBVK9sD6/tmp/ece14d69acf2bb054aa6fc285cf5f041faf76cae02e9f7f5089e4e18c2557e03.jpg", "img_caption": ["Dataset: Piqa & RM used in RLHF: InfoRM "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "Figure 10: T-SNE Visualization of the response distribution in the latent IB space of InfoRM before and after RLHF, as well as the distribution of overoptimized samples from the RLHF model as judged by GPT-4. From top to bottom: The datasets used for response generation are Anthropic-Helpful, Anthropic-Harmless, Mkqa, Oasst1, OpenOrca, and Piqa datasets, respectively. From left to right: The reward models applied in RLHF are Standard RM and InfoRM, respectively. ", "page_idx": 17}, {"type": "image", "img_path": "3XnBVK9sD6/tmp/8e73922f3a9b347de85de18188f0b39d4f9d668d5fd7629890c9f0ca34c19f94.jpg", "img_caption": ["Dataset: PKU-SafeRLHF & RM used in RLHF: Standard RM "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "3XnBVK9sD6/tmp/462f9438696ad0ea0214c389e73014afd64c02ac15f7d253d813c6ff1702aa64.jpg", "img_caption": ["Dataset: ShareGPT & RM used in RLHF: Standard RM "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "3XnBVK9sD6/tmp/ff6f6ded9958cfc2da74feaea30072541cbcce1d3b3016649f41c19e1c45f2a9.jpg", "img_caption": ["Dataset: SHP & RM used in RLHF: Standard RM "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "3XnBVK9sD6/tmp/742431d42e73aa0b3900392a12f4e349cb6c947a31a0e10e933cf0467d4f83ad.jpg", "img_caption": ["Dataset: Instruct-GPT & RM used in RLHF: Standard RM "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "3XnBVK9sD6/tmp/9bacf3adcf1e72d9840126c18337eb0e45fb14c409d7407e1cb465b5778b39d6.jpg", "img_caption": ["Dataset: TruthfulQA & RM used in RLHF: Standard RM "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "3XnBVK9sD6/tmp/e581dc44028e338049fc110390deea1478428f749f7d93357502a72f2699ac37.jpg", "img_caption": ["Dataset: WebGPT & RM used in RLHF: Standard RM "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "3XnBVK9sD6/tmp/5d182221e75d7f0661cbd064326a4bae05c91b65cd7c4887c6378c1677e966fd.jpg", "img_caption": ["Dataset: PKU-SafeRLHF & RM used in RLHF: InfoRM "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "3XnBVK9sD6/tmp/b818744558cb5f2d420cf5f85791dff21ff65ba7d587ab25c2d612847f2db7b9.jpg", "img_caption": ["Dataset: ShareGPT & RM used in RLHF: InfoRM "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "3XnBVK9sD6/tmp/f86fdbb8ff4ae9b91a3a7291c73e9a65784786d30486d95c7244ff8c749c2156.jpg", "img_caption": ["Dataset: SHP & RM used in RLHF: InfoRM "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "3XnBVK9sD6/tmp/884426b01abd522dbd3ed476ed4c50792a8175eab23d15f5c541033afd80ab21.jpg", "img_caption": ["Dataset: Instruct-GPT & RM used in RLHF: InfoRM "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "3XnBVK9sD6/tmp/4b9935ab39218a94cc5db72f1f59cb7ec43e479608b9d5e11b2941fea67e3de8.jpg", "img_caption": ["Dataset: TruthfulQA & RM used in RLHF: InfoRM "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "3XnBVK9sD6/tmp/d4e6365bb4bec4d0911ec90ac255d6a166e2f8e46afd94674d55ac087174d625.jpg", "img_caption": ["Dataset: WebGPT & RM used in RLHF: InfoRM "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "Figure 11: T-SNE Visualization of the response distribution in the latent IB space of InfoRM before and after RLHF, as well as the distribution of overoptimized samples from the RLHF model as judged by GPT-4. From top to bottom: The datasets used for response generation are PKU-SafeRLHF, ShareGPT, SHP, Instruct-GPT, TruthfulQA, and WebGPT datasets, respectively. From left to right: The reward models applied in RLHF are Standard RM and InfoRM, respectively. ", "page_idx": 18}, {"type": "text", "text": "C.2 Validations for Outlier Emergencies and Overoptimization Detection by the CSI Indicator ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In this part, we further validate the effectiveness of our CSI indicator in detecting outliers and overoptimization across various datasets used for response generation. The CSI values during the RL process using InfoRM and Standard RM on diverse datasets are illustrated in Figures 12 and 13. Regardless of the dataset, the abrupt changes in our CSI indicator consistently coincide with the emergence of outliers in the IB latent space. This consistency confirms the effectiveness of our proposed CSI indicator in identifying outlier emergencies, thus offering timely and accurate detection of reward overoptimization. Moreover, the RLHF process with InfoRM consistently shows significantly lower CSI values, indicating that InfoRM effectively mitigates reward overoptimization, corroborating our experimental results. ", "page_idx": 19}, {"type": "image", "img_path": "3XnBVK9sD6/tmp/cdae2ef1749d7d1e1597f51487cc66aa9e4d4b96955c2f938ff75097ff26565d.jpg", "img_caption": ["Figure 12: CSI values in the RLHF processes of Standard RM and InfoRM across the training steps. From left to right and from top to bottom: The dataset used for response generation is AlpacaFarm, FalseQA, Flan, HelpSteer, Anthropic-Helpful, Anthropic-Harmless, Mkqa, and Oasst1 datasets, respectively. "], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "3XnBVK9sD6/tmp/4981d65ca019b5231dac74867da0c0f9fe0cabbf44e8d547f87a6513cd74f7fb.jpg", "img_caption": ["Figure 13: CSI values in the RLHF processes of Standard RM and InfoRM across the training steps. From left to right and from top to bottom: The datasets used for response generation are OpenOrca, Piqa, PKU-SafeRLHF, ShareGPT, SHP, Instruct-GPT, TruthfulQA, and WebGPT datasets, respectively. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "D Analysis of Irrelevant Information Filtering Using Our InfoRM ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "This section delves into how our proposed approach effectively filters out information irrelevant to human preferences, thus enhancing the relevance and precision of model outputs. A salient example of human preference-irrelevant information is length bias [38]. Typically, human annotators may favor more detailed answers, leading reward models to erroneously equate longer responses with higher quality. This can result in RLHF models producing unduly verbose and excessively detailed outputs. Here, the detail is relevant to human preference, but the mere length is not. ", "page_idx": 20}, {"type": "text", "text": "To demonstrate our InfoRM\u2019s capability in eliminating such length bias, we calculate the average response length on diverse datasets by the models at different RLHF steps using our ", "page_idx": 20}, {"type": "text", "text": "InfoRM and Standard RM. The datasets used for response generation includes AlpacaFarm [13], FalseQA [20], Flan [28], HelpSteer [48], Anthropic-Helpful [4], Anthropic-Harmless [4], Oasst1 [23], OpenOrca [31], Piqa [50], PKU-SafeRLHF [22], SHP [2], TruthfulQA [26], and WebGPT [32] datasets. The results, presented in Figure 14, illustrate that the output lengths produced by the RLHF model optimizing our InfoRM are significantly shorter than those obtained through optimizing the Standard RM. This evidence supports the effectiveness of the IB method in mitigating length bias, further substantiating the claim that IB can indeed filter out irrelevant information. ", "page_idx": 21}, {"type": "text", "text": "It\u2019s worth noting that beyond length bias, we have empirically identified other examples that illustrate the efficacy of our approach in flitering out information irrelevant to human preferences. Specifically, in datasets with a high prevalence of harmful data, models tend to exhibit an overly cautious refusal to respond, even when the input itself is benign\u2014a phenomenon known as excessive caution. Our empirical observations indicate that the use of IB significantly reduces this phenomenon, highlighting its broader utility in enhancing model generalizability by flitering out extraneous information; please see Appendix K for the corresponding case studies. ", "page_idx": 21}, {"type": "image", "img_path": "3XnBVK9sD6/tmp/537904c6110b38e535d6f218bf1f655fae076b16223ec06311829be7619655b7.jpg", "img_caption": ["Figure 14: Average response length of the models at different RLHF steps using Standard RM and InfoRM. From left to right and from top to bottom: The dataset used for response generation is AlpacaFarm, FalseQA, Flan, Anthropic-Helpful, Anthropic-Harmless, Oasst1, OpenOrca, Piqa, PKU-SaveRLHF, SHP, TruthfulQA, and WebGPT datasets, respectively. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "E Sensitivity Analysis of hyperparameters in Our InfoRM ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In our approach, there are two parameters that require manual adjustment, namely, the IB dimensionality, and the IB tradeoff parameter $\\beta$ . IB latent dimensionality refers to the length of the IB representation vector. Next, we will analyze their impact on the overoptimization detection mechanism and RLHF performance, separately. ", "page_idx": 22}, {"type": "text", "text": "E.1 Impact on Overoptimization Detection Mechanism ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "First, we tested the impact of different hyperparameter settings on the performance of our overoptimization detection mechanism. The relevant results are displayed in Figure 15. We observe that regardless of the parameter settings, overoptimized samples consistently appear as outliers in the latent space of InfoRM. This demonstrates the robustness of our overoptimization detection mechanism against variations in InfoRM\u2019s hyperparameters. ", "page_idx": 22}, {"type": "image", "img_path": "3XnBVK9sD6/tmp/9b031f1d75e1797e41069fba2738c7966386363a1f265f58193df24cf134bbcc.jpg", "img_caption": ["Figure 15: Visualization of output distribution in InfoRM\u2019s IB latent space before and after RLHF of Standard RM. (a)-(c) correspond to different IB dimensionalities of InfoRM and (d)-(f) correspond to different tradeoff parameter $\\beta$ of InfoRM. The dataset used for response generation is the AnthropicHarmless dataset. Conclusion: Our overoptimization detection mechanism is robust against variations in InfoRM\u2019s hyperparameters. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "E.2 Impact on RLHF performance ", "text_level": 1, "page_idx": 22}, {"type": "image", "img_path": "3XnBVK9sD6/tmp/8da03fa532aae57533976a4e9cdb47a498412b1cc2731d4cac004428db9baf26.jpg", "img_caption": [], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "Figure 16: Win rate $(\\%)$ on Anthropic-Harmless dataset between the models after and before RLHF using our InfoRM with different hyper-parameters, according to GPT-4. In order to remove ties, we calculate the win rate as $w i n/(w i n+l o s s)$ . ", "page_idx": 22}, {"type": "text", "text": "In this part, we tested the impact of different hyperparameter settings on the RLHF performance of our InfoRM. Related results are shown in Figure 16. It can be observed that our model achieves its optimal performance when the IB dimensionality is 128 and the $\\beta$ value is 0.1. ", "page_idx": 22}, {"type": "text", "text": "Furthermore, to further illustrate the practical utility of our proposed overoptimization detection mechanism in facilitating parameter adjustments in real-world scenarios, we present the response distributions before and after RLHF using InfoRM, with varying IB dimensionality and $\\beta$ values in Figures 17. We observe that, at optimal parameter settings, i.e., IB dimensionality ${\\it=}128$ and $\\beta{=}0.1$ , the output of the RLHF model exhibits the smallest deviation in the IB latent space relative to the output of the SFT model. In addition, the CSI values in the RLHF processes of InfoRM with different IB dimensionalities and $\\beta$ are presented in Figure 18. As observed, at the optimal parameter setting, the CSI consistently maintains lower values compared to other parameter configurations. These observations validate our overoptimization detection mechanism\u2019s additional capability to assist in adjusting hyper-parameters in real-world scenarios. ", "page_idx": 23}, {"type": "image", "img_path": "3XnBVK9sD6/tmp/a0f8f2bdc03f64f1fc8ac2f7af291fd375ed5cdf4c8ee73b607e35b402f87fcd.jpg", "img_caption": ["Figure 17: Visualization of output distribution before and after RLHF with InfoRM, as well as the distribution of overoptimized samples from the RLHF model judged by GPT-4. (a)-(c) correspond to different IB dimensionalities of InfoRM and (d)-(f) correspond to different tradeoff parameter $\\beta$ of InfoRM. The best results are highlighted with a red border and the Anthropic-Harmless dataset is used for response generation. "], "img_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "3XnBVK9sD6/tmp/84d8cedb5ca7740e4177e26c6dde742f847e37a53286ec8bd46d23ad6b248b11.jpg", "img_caption": ["Figure 18: CSI values in the RLHF processes of InfoRM with different IB dimensionalities and $\\beta$ . (a)-(b) correspond to different IB dimensionalities and $\\beta$ of InfoRM, respectively. "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "F Universality of Our Overoptimization Detection Machanism ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In this section, we investigate the universality of our overoptimization detection mechanism across different RMs. The visualization of the response distribution before and after RLHF in the latent spaces of different RMs, as well as the distribution of overoptimized samples are provided in in Figure 19. ", "page_idx": 23}, {"type": "text", "text": "We find that outliers in the latent space of InfoRM consistently correspond to overoptimized samples. Conversely, the latent space distributions of the standard RM are more intricate, where outliers do not necessarily signify overoptimized samples, as illustrated by the green ovals in Figure 19 (b). This difference arises because InfoRM benefits from information bottleneck theory, resulting in a more compact latent space, whereas the latent spaces of standard RM are relatively dispersed. Therefore, ", "page_idx": 23}, {"type": "text", "text": "CSI, by detecting outliers in the latent space, effectively identifies overoptimization in our InfoRM.   \nHowever, it may not be applicable in the contexts of other RM without IB, such as standard RM. ", "page_idx": 24}, {"type": "image", "img_path": "3XnBVK9sD6/tmp/a7f2f315b924aa0ebb9fd3cb3a7f02217a08921dbbcc3eb1681f977a81e87144.jpg", "img_caption": ["Figure 19: The visualization of the response distribution before and after RLHF in the latent spaces of different RMs, as well as the distribution of overoptimized samples. (a)-(b) correspond to the results in the latent space of InfoRM and Standard RM, respectively. The green ovals highlight regions that demonstrate why our overoptimization detection mechanism is incompatible with the Standard RM. "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "G Early Stopping Algorithm Based on the Proposed CSI Metric ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "To explain how to use the CSI metric to select the stopping point during model training, in this section, we elaborate an automated early-stopping algorithm based on our CSI metric for executing early stopping. The CSI-based early stopping algorithm is detailed as follows: ", "page_idx": 24}, {"type": "text", "text": "\u2022 Step 1: Set a maximum tolerable CSI change rate, $\\epsilon_{\\mathrm{max}}$ , which is empirically set to a relatively large value of 10. Let $C_{t}$ represent the CSI value at the $t$ -th evaluation step. The change in CSI at this step is given by $\\Delta_{t}=|C_{t}-C_{t-1}|$ . ", "page_idx": 24}, {"type": "text", "text": "\u2022 Step 2: Calculate the ratio of the CSI change at the $t$ -th evaluation step, $\\Delta_{t}$ , to the average change across all previous steps, $\\textstyle{\\frac{1}{t-1}}\\sum_{i=1}^{t-1}\\Delta_{i}$ . This ratio is denoted as $\\begin{array}{r}{\\epsilon_{t}=\\Delta_{t}/(\\frac{1}{t-1}\\sum_{i=1}^{t-1}\\Delta_{i})}\\end{array}$ . ", "page_idx": 24}, {"type": "text", "text": "\u2022 Step 3: If $\\epsilon_{t}>\\epsilon_{\\operatorname*{max}}$ , trigger early stopping and exit the iteration. Otherwise, continue training. ", "page_idx": 24}, {"type": "text", "text": "To facilitate understanding, we summarize this algorithm as follows: ", "page_idx": 24}, {"type": "table", "img_path": "3XnBVK9sD6/tmp/4d5b149bc8bf0f7b2b5710bc9bd19785070feac4a89c7dbf955e7e04905867bf.jpg", "table_caption": [], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "H More Real-World Results with Different Hyper-parameters ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "To ensure the fairness and reliability of the experiments, we report the performance of each compared method under different hyperparameter settings in Table 2. As shown, our method consistently demonstrates significant advantages, regardless of the parameter configurations. ", "page_idx": 24}, {"type": "table", "img_path": "3XnBVK9sD6/tmp/9e8b2651517da2c5f0d5b3ae545f28b3da99eb4ea6c4650faf8ef9cd7c9f94b7.jpg", "table_caption": ["Table 2: Comparison results of RLHF models using various RMs with different hyper-parameters under GPT-4 evaluation. The best settings selected based on the win ratio in each group are highlighted in bold. "], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "I Performance of InfoRM on Reward Model Benchmarks ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "So far, we have validated the effectiveness of our InfoRM from the perspective of RLHF performance. In this section, to further demonstrate the superiority of InfoRM over Standard RM on reward model benchmarks, we report their accuracy on in-distribution reward model benchmarks (AnthropicHelpful and Anthropic-Harmless) and out-of-distribution reward model benchmarks (AlpacaEval and Truthful QA), as shown in Table 3. We can observe that while our InfoRM achieves comparable performance to the Standard RM on in-distribution reward model benchmarks (Anthropic-Helpful and Anthropic-Harmless), it significantly outperforms the Standard RM on out-of-distribution reward model benchmarks (AlpacaEval and Truthful QA). This observation further demonstrates that our InfoRM can significantly enhance the generalization of reward modeling. ", "page_idx": 25}, {"type": "text", "text": "Table 3: Accuracy on in-distribution datasets (Anthropic Helpful and Anthropic Harmless) and out-of-distribution datasets (AlpacaEval and Truthful QA). The best results are highlighted in bold. ", "page_idx": 25}, {"type": "table", "img_path": "3XnBVK9sD6/tmp/271acae1301b520aa904724586644d64d1204ca6e8f5b48c44054b1ceb27275f.jpg", "table_caption": [], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "J Experiments Details ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "In this part, we provide our experiments details in this work. ", "page_idx": 25}, {"type": "text", "text": "J.1 Implementation Details of Our InfoRM ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "To better demonstrate the implementation details of InfoRM, we provide the pseudocode of InfoRM\u2019s implementation in Algorithm 2. ", "page_idx": 25}, {"type": "text", "text": "J.2 Implementation Details of Our CSI ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "To better demonstrate the implementation details of our CSI, we provide the pseudocode of CSI calculation process in Algorithm 3. ", "page_idx": 25}, {"type": "text", "text": "J.3 Training Setup ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "In our study, all models were initialized from pre-trained checkpoints, ensuring that their architectural setup and hyperparameters remained aligned with those of their original pre-trained counterparts. ", "page_idx": 25}, {"type": "text", "text": "1: Class InfoRM inherits LlamaPreTrainedModel 2: function __INIT__(self, config, \\*\\*kwargs) 3: # Define the LLM backbone to extract hidden state. 4: self.model $\\leftarrow$ LlamaModel(config) 5: # Define the IB dimensionality of our InfoRM. 6: self.latent_dim $\\leftarrow$ kwargs.pop(\"latent_dim\", 128) 7: # Define the IB tradeoff parameter of our InfoRM. 8: self.beta $\\leftarrow$ kwargs.pop(\"beta\", 0.1)   \n9: # Define the last layer of RM encoder for IB representation generation from hidden state.   \n10: self.encode_head $\\leftarrow$ Linear(config.hidden_size, self.latent_dim $\\times\\,2_{.}$ )   \n11: # Define the MLP decoder for reward prediction from IB representation.   \n12: self.decode_head $\\leftarrow$ MLP(self.latent_dim, 1)   \n13: end function   \n14:   \n15: # This function is called in RLHF process for reward scores prediction.   \n16: function REWARD(self, input_ids, attention_mask, \\*\\*kwargs)   \n17: # Get hidden states using self.model.   \n18: hidden_states $\\leftarrow$ self.model(input_ids, attention_mask)[0]   \n19: # Get $I B$ representation using self.encode_head.   \n20: ib_representation $\\leftarrow$ get_representation(self.encode_head(hidden_states))   \n21: # Get final reward prediction using self.decode_head.   \n22: rewards $\\leftarrow$ extract_reward(self.decode_head(ib_representation))   \n23: return rewards   \n24: end function   \n25:   \n26: # This function is called in reward modeling process for RM training.   \n27: function FORWARD(self, input_ids, past_key_values, attention_mask, \\*\\*kwargs)   \n28: # Repeat Line 17, 19, and 21 to get ib_representation and rewards from inputs.   \n29: hidden_states $\\leftarrow$ self.model(input_ids, attention_mask)[0]   \n30: ib_representation $\\leftarrow$ get_representation(self.encode_head(hidden_states))   \n31: rewards $\\leftarrow$ extract_reward(self.decode_head(ib_representation))   \n32: # Compute normal reward loss (i.e., Lpreference) and KL loss (i.e., Lbottleneck).   \n33: compute Lpreference and $L_{b o t t l e n e c k}$ via Eqn. 5   \n34: $L_{t o t a l}\\gets\\bar{L_{p r e f e r e n c e}}+$ $^+$ self.beta \\* Lbottleneck   \n35: return Ltotal   \n36: end function ", "page_idx": 26}, {"type": "text", "text": "The fine-tuning process for the pre-trained models in simulation experiments was carried out on a solitary node outfitted with 8 A100-SXM80GB GPUs. We implemented Data Parallelism (DP) and made use of Automatic Mixed Precision (AMP) with bfloat16, capitalizing on the capabilities of the Deepspeed Zero framework [35]. During training, a learning rate of 5e-5 was used, along with only one epoch for the SFT phase and a global batch size of 64. ", "page_idx": 26}, {"type": "text", "text": "For reward modeling in simulation experiments and real-world experiments, we employed a learning rate of 5e-6, a global batch size of 64, and trained the model on human preference datasets for only 1 epoch to prevent overfitting. In addition, the IB trade-off parameter $\\beta$ is selected from $\\{0.1,0.01$ , 0.001}, and the IB dimensionality is selected from {32, 64, 128}, indicating that the final reward can be represented by a vector of this length. ", "page_idx": 26}, {"type": "text", "text": "Regarding the PPO training in simulation experiments, we utilized a learning rate of 5e-7 for the policy model and 1e-6 for the critic model. The number of epochs was set to 1, with a global batch size of 16. The sampling temperature was set to 0.8, top-p was set to 0.9, and the maximum output token length was set to 512. The critic model was initialized with the weight of the SFT model, as suggested in [54], and the Generalized Advantage Estimation parameter $\\lambda$ is set to 0.95. The clipping value in policy and critic optimization is set to 0.2, and the coefficient of KL divergence penalty is selected from the candidate {0.0001, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0}, manually adjusting to achieve optimal results. For the real-world experiments, the global batch size was increased to 64, with all other configurations remaining unchanged. ", "page_idx": 26}, {"type": "text", "text": "Algorithm 3 Pseudocode of Our CSI   \n1: # red_points represents the coordinates of the model response after RLHF in IB latent space.   \n2: # blue_points represents the coordinates of the model response before RLHF in IB latent space.   \n3: function CSI_INDICATOR(red_points, blue_points)   \n4: # Perform clustering on the red_points.   \n5: clusters_red $\\leftarrow$ DBSCAN().fit_predict(red_points)   \n6: CSI_value $\\gets0$   \n7: # traverse obtained clusters.   \n8: for cluster_id $\\in$ set(clusters_red) do   \n9: # Get corresponding sample points.   \n10: cluster_points $\\leftarrow$ red_points[clusters_red $==$ cluster_id]   \n11: # Get corresponding cluster size.   \n12: cluster_size $\\leftarrow$ len(cluster_points)   \n13: # Calculate the corresponding geometric centroid.   \n14: cluster_center $\\leftarrow$ np.mean(cluster_points, axis $=\\!0$ )   \n15: # Identify the nearest blue point.   \n16: closest_blue_point $\\leftarrow$ blue_points[np.argmin(distance(cluster_center, blue_points))]   \n17: # Calculate the distance between current red centroid and the nearest blur point.   \n18: dist $\\leftarrow$ distance.euclidean(cluster_center, closest_blue_point)   \n19: weighted_distance $\\leftarrow$ dist $\\times$ cluster_size   \n20: # Calculate the weighted distance.   \n21: CSI_value $\\leftarrow\\mathrm{CSI}_{-}$ value $^+$ weighted_distance   \n22: end for   \n23: return CSI_value   \n24: end function ", "page_idx": 27}, {"type": "text", "text": "J.4 GPT-4 Evaluation ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "We use GPT-4-1106-preview as the evaluator of AlpacaFarm\u2019s results, as well as the discriminator of hacking phenomenon. Detailed instructions provided to GPT-4 are illustrated in Figure 20. ", "page_idx": 27}, {"type": "image", "img_path": "3XnBVK9sD6/tmp/b1f476f34eb51db914002f3ab3ea3554967973f9498f5e27d19f53fa0ccb9045.jpg", "img_caption": ["Figure 20: GPT-4 prompts used in our experiments for (a) AlpacaFarm evaluation and (b) hacking samples identifying. "], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "K Qualitative Examples in Real-World Experiments ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "This section presents some practical examples in real-world experiments. These examples are from the AlpacaFarm, Anthropic Helpful, and Anthropic Harmless dataset. Overall, our InfoRM outperforms the compared methods in terms of incomplete information error (see Figures 21, 22, and 23), excessive caution error (see Figures 24, 25, and 26), and repeat information error (see Figures 27, 28, and 29). ", "page_idx": 28}, {"type": "text", "text": "K.1 Examples from AlpacaFarm Dataset ", "text_level": 1, "page_idx": 28}, {"type": "image", "img_path": "3XnBVK9sD6/tmp/8bb202c9d9504c07c72cdb4997b6d6f22f6d725bae167c0c21b05e5ef94048f8.jpg", "img_caption": [], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "Figure 21: Qualitative example I of RLHF models output with different RMs on AlpacaFarm dataset. Here we highlight the information in green that is covered by our InfoRM but missed by the competing methods (incomplete information error). ", "page_idx": 28}, {"type": "text", "text": "LM Input: How did mankind discover that the earth was spherical, and why did they initially believe it to be flat? ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 29}, {"type": "table", "img_path": "3XnBVK9sD6/tmp/bf36ff198615f9c51f0d27fec6706c3dd06a603c02cf7cd5d0634625276114aa.jpg", "table_caption": [], "table_footnote": [], "page_idx": 29}, {"type": "text", "text": "Figure 22: Qualitative example II of RLHF models output with different RMs on AlpacaFarm dataset. Here we highlight the information in green that is covered by our InfoRM but missed by the competing methods (incomplete information error). ", "page_idx": 29}, {"type": "table", "img_path": "3XnBVK9sD6/tmp/bc44dc077c57a32088c23121b7d59033a8096b38331db29ac4174410ef92f1b5.jpg", "table_caption": [], "table_footnote": [], "page_idx": 30}, {"type": "text", "text": "Figure 23: Qualitative example III of RLHF models output with different RMs on AlpacaFarm dataset. Here we highlight the information in green that is covered by our InfoRM but missed by the competing methods (incomplete information error). ", "page_idx": 30}, {"type": "image", "img_path": "3XnBVK9sD6/tmp/8f58a3ec2bc9e1edcea6798bf0629fd67e2dcebc0ac03db800e85a6e5f01fd05.jpg", "img_caption": [], "img_footnote": [], "page_idx": 31}, {"type": "text", "text": "Figure 24: Qualitative example IV of RLHF models output with different RMs on AlpacaFarm dataset. Here we highlight the information in green that is covered by our InfoRM but missed by the competing methods (incomplete information error). In addition, we mark the repeat information error and excessive caution error in the model outputs. [...] indicates that the response was trimmed to fit this page, but the generated text is actually longer. ", "page_idx": 31}, {"type": "image", "img_path": "3XnBVK9sD6/tmp/5492074fd8f2a96386b6e690dbebca4c8bba5e58196cd7a7e11bfbe5b492ca2c.jpg", "img_caption": [], "img_footnote": [], "page_idx": 32}, {"type": "text", "text": "Figure 25: Qualitative example $\\mathrm{v}$ of RLHF models output with different RMs on AlpacaFarm dataset. Here we highlight the information in green that is covered by our InfoRM but missed by the competing methods (incomplete information error). In addition, we mark the excessive caution error in the model outputs. [...] indicates that the response was trimmed to fti this page, but the generated text is actually longer. ", "page_idx": 32}, {"type": "table", "img_path": "3XnBVK9sD6/tmp/bbf7ebbc07dd993bf6fa384ea4ed19f31ae9f23bc384235ad110ad7736227920.jpg", "table_caption": [], "table_footnote": [], "page_idx": 33}, {"type": "text", "text": "Figure 26: Qualitative example VI of RLHF models output with different RMs on AlpacaFarm dataset. Here we highlight the information in green that is covered by our InfoRM but missed by the competing methods (incomplete information error). In addition, we mark the excessive caution error in the model outputs. [...] indicates that the response was trimmed to fit this page, but the generated text is actually longer. ", "page_idx": 33}, {"type": "image", "img_path": "3XnBVK9sD6/tmp/c55cfae200ff29ea04143b6dce842b20db3b2e2e07eb7cbaf71a6c8d00e50c22.jpg", "img_caption": [], "img_footnote": [], "page_idx": 34}, {"type": "text", "text": "Figure 27: Qualitative example VII of RLHF models output with different RMs on AlpacaFarm dataset. Here we highlight the information in green that is covered by our InfoRM but missed by the competing methods (incomplete information error). In addition, we mark the repeat information error in the model outputs. ", "page_idx": 34}, {"type": "image", "img_path": "3XnBVK9sD6/tmp/55a6966affcfef9012670e801e5cb2983ca180b3ae4f923aeefec81be2d954f9.jpg", "img_caption": ["Figure 28: Qualitative example VIII of RLHF models output with different RMs on AlpacaFarm dataset. Here we highlight the information in green that is covered by our InfoRM but missed by the competing methods (incomplete information error). In addition, we mark the repeat information error in the model outputs. "], "img_footnote": [], "page_idx": 34}, {"type": "image", "img_path": "3XnBVK9sD6/tmp/67c7b7235122592bd7906d6c0391b2ceda572f7d680cefdbe26e50acabfa0181.jpg", "img_caption": [], "img_footnote": [], "page_idx": 35}, {"type": "text", "text": "Figure 29: Qualitative example IX of RLHF models output with different RMs on AlpacaFarm dataset. Here we highlight the information in green that is covered by our InfoRM but missed by the competing methods (incomplete information error). In addition, we mark the repeat information error in the model outputs. ", "page_idx": 35}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: Please see abstract and Section 1. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 36}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Justification: Please see the part of Limitations. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 36}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: Please see Appendix A. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 37}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: Please see Appendix J. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 37}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Justification: We have released the code on github. Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 38}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: Please see Appendix J. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 38}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 38}, {"type": "text", "text": "Answer: [No] ", "page_idx": 38}, {"type": "text", "text": "Justification: The experiments were conducted using the same random seed to eliminate randomness, primarily due to the large size of the studied language models and limited computational resources. As a result, replicative experiments to establish statistical significance were not feasible, and measures of statistical variance are not reported. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 38}, {"type": "text", "text": "", "page_idx": 39}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: Please see Appendix J. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 39}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: This research strictly adheres to the NeurIPS Code of Ethics. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 39}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: Please see the part of Broader Impacts following Section 6. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 39}, {"type": "text", "text": "", "page_idx": 40}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 40}, {"type": "text", "text": "Justification: This research poses no such risks. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 40}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: All assets used in this paper, including code, data, and models, are properly credited to their original creators, and all licensing terms and conditions of use are explicitly acknowledged and fully respected. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 40}, {"type": "text", "text": "", "page_idx": 41}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 41}, {"type": "text", "text": "Justification: This research does not release new assets. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 41}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 41}, {"type": "text", "text": "Justification: This research does not involve crowdsourcing nor research with human subjects. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 41}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 41}, {"type": "text", "text": "Justification: This research does not involve crowdsourcing nor research with human subjects. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. ", "page_idx": 41}, {"type": "text", "text": "\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 42}]