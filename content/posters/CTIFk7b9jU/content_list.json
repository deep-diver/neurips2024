[{"type": "text", "text": "Bidirectional Recurrence for Cardiac Motion Tracking with Gaussian Process Latent Coding ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Jiewen Yang Yiqun Lin Bin Pu Xiaomeng Li The Hong Kong University of Science and Technology {jyangcu, ylindw}@connect.ust.hk {eebinpu, eexmli}@ust.hk ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Quantitative analysis of cardiac motion is crucial for assessing cardiac function. This analysis typically uses imaging modalities such as MRI and Echocardiograms that capture detailed image sequences throughout the heartbeat cycle. Previous methods predominantly focused on the analysis of image pairs lacking consideration of the motion dynamics and spatial variability. Consequently, these methods often overlook the long-term relationships and regional motion characteristic of cardiac. To overcome these limitations, we introduce the GPTrack, a novel unsupervised framework crafted to fully explore the temporal and spatial dynamics of cardiac motion. The GPTrack enhances motion tracking by employing the sequential Gaussian Process in the latent space and encoding statistics by spatial information at each time stamp, which robustly promotes temporal consistency and spatial variability of cardiac dynamics. Also, we innovatively aggregate sequential information in a bidirectional recursive manner, mimicking the behavior of diffeomorphic registration to better capture consistent long-term relationships of motions across cardiac regions such as the ventricles and atria. Our GPTrack significantly improves the precision of motion tracking in both 3D and 4D medical images while maintaining computational efficiency. The code is available at: https://github.com/xmed-lab/GPTrack. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Cardiac motion tracking from Cardiac Magnetic Resonance Imaging (MRI) and Echocardiograms is crucial in quantitative cardiac image processing. These imaging techniques provide comprehensive image sequences that cover an entire heartbeat cycle, allowing for detailed analysis of cardiac dynamics. Conventional non-parametric cardiac motion tracking approaches, such as B-splines [1], Demons algorithms [2] and optical-flow based methods [3, 4, 5], are commonly utilized due to their flexibility and ability to align detailed structures within images. However, these methods face significant challenges in motion tracking because they lack topology-preserving constraints and temporal coherence. The diffeomorphic registration method [6, 7, 8], which formulates the registration process as a group of diffeomorphisms in Lagrangian dynamics, is a nice candidate for topology-preserving motion tracking. However, traditional optimization-based diffeomorphic registration methods are computationally intensive and sensitive to noises, hindering their applications in efficient cardiac motion tracking. ", "page_idx": 0}, {"type": "text", "text": "Current deep learning-based techniques [9, 10, 11, 12, 13, 14, 15, 16] employ these advanced imaging modalities to register image pairs within the same patient, and some [14, 15, 16] adopt the diffeomorphic routine and learn the Lagrangian strain to describe the motion relationship between the reference frame and subsequent frames, aggregating dynamic information into consecutive cardiac motions as Lagrangian displacements. Although the above diffeomorphic methods better model the dynamic and continuous nature of cardiac motion, they still have room for improvement in handling the long-term temporal relationship in videos. For example, the approach [14] requires segmentation annotation to generate dense motion trajectories and calculate Lagrangian strain. Additionally, the methods outlined in [15, 16] are prone to error accumulation as Lagrangian displacements are integrated without regularizing temporal variations. In spatial views, while the global trajectory flow may follow a specific pattern, significant variations exist within each region regarding the phases, amplitudes, and intensities of the motion. For example, Figure 1 shows regions of the Right Atrium (red) and Myocardium (green) performing the opposite trajectories during the heartbeat cycle. Conversely, similar regions across different cases exhibit consistent motions. Hence, ignoring the regional scale may lead to a fragmented understanding of cardiac motion, underscoring the need for more nuanced analytical approaches. Furthermore, as shown in the right of Figure 1, the deformation is bounded in the space of periodically specific human cardiac motion variation. ", "page_idx": 0}, {"type": "image", "img_path": "CTIFk7b9jU/tmp/0af7441f9daf059cba62077c39acabf8131a63a5a34ba9e8568f3fd9339836b4.jpg", "img_caption": ["Figure 1: Regional Motions in Cardiac: The left sequential MRI frames within a heartbeat cycle illustrate that motion direction and intensity are completely different between the right atrium and myocardium during End-diastole and End-systole. Formulate Cardiac Motion as Prior Knowledge: The right figure depicts the regions of motion trajectory across the heartbeat cycle, alongside the probability distributions of motion trajectory. Curves (Middle) are the motion trajectory changes of different MRI sequences (Left). Highlighting the cardiac motion trajectory that follows a certain pattern can be modelled as prior knowledge via the Gaussian Process (Right). "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "To leverage discussed temporal and spatial information for cardiac motion tracking, we propose a novel framework named GPTrack. Our GPTrack has several appealing facets: 1) GPTrack employs the Gaussian Process (GP) to formulate the consistent temporal patterns in the latent space of diffeomorphic frameworks, promoting the consistency of cardiac motion; 2) GPTrack utilizes position information in the latent space to encode the statistics of the sequential Gaussian process, by which we model the region-specific motion and obtain a more precise estimation related to cardiac motion; 3) GPTrack leverages the inherent temporal continuity in cardiac motion by aggregating long-term relationships through forward and backward video flows, which mimics the forward-backward manner of classical diffeomorphic registration framework [6]. To evaluate the performance of GPTrack in cardiac motion tracking, we conduct experiments based on 3D Echocardiogram videos [17, 18] and 4D temporal MRI image [19]. Results in Tables 1, 2 and 3, show the GPTrack enhance the accuracy of motion tracking performance in a clear margin, without substantially increasing the computational cost in comparison to other state-of-the-art methods. Our contributions are summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "1. We propose a novel cardiac motion tracking framework named the GPTrack. This framework employs the Gaussian Process (GP) to promote temporal consistency and regional variability in compact latent space, establishing a robust regularizer to enhance cardiac motion tracking accuracy. ", "page_idx": 1}, {"type": "text", "text": "2. The GPTrack framework is designed to capture the long-term relationship of cardiac motion via a bidirectional recursive manner, and its forward-backward manner mimics the workflows of the classical diffeomorphic registration framework. By this approach, our method provides a more accurate and reliable estimation of cardiac motion. ", "page_idx": 1}, {"type": "text", "text": "3. Our GPTrack framework achieves state-of-the-art performance on both 3D Echocardiogram videos and 4D temporal MRI datasets, maintaining comparable computational efficiency. The results demonstrate that our method adapts effectively across different medical imaging modalities, proving its utility in different clinical settings. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2.1 Cardiac Motion Tracking via Non-parametric Registration Approach ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Extensive works have been proposed to address registration by optimising within the space of displacement vector fields. Models related to elastic matching were proposed by [20, 21]. [22] utilized statistical parametric mapping for improvement. Techniques incorporating free-form deformations with B-splines and Maxwell demons were adapted by [1] and [2], respectively. The Harmonic phase-based method, which utilizes spectral peaks in the Fourier domain for cardiac motion tracking, was introduced by [23]. This method calculates phase images from the inverse Fourier transforms and is specifically exploited in the analysis of tagged MRI. Popular formulations [6, 8, 24, 7] introduce topology-preserved diffeomorphic transforms. In the realm of diffeomorphic registration, inverse consistent diffeomorphic deformations have been estimated by [6] and [7]. Syn [24] proposed standard symmetric normalization. RDMM [8] considered regional parameterization based on Large Deformation Diffeomorphic Metric Mapping [6]. Despite their remarkable success in computational anatomy studies, these approaches are also time-consuming and susceptible to noise. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "2.2 Cardiac Motion Tracking with Deep-Learning based Registration Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Recent advancements in medical image registration have increasingly leveraged Deep Learning technologies. Pioneering studies [14, 25, 26, 27, 28] utilize ground truth displacement fields obtained by simulating deformations and deformed images, typically estimated using non-parametric methods. These approaches, however, may be limited by the types of deformations they can effectively model, which can affect both the quality and accuracy of the registration. Unsupervised methods, as discussed by [9, 10, 15, 29, 30, 31, 32] have shown promise by learning deformation through the warping of a fixed image to a moving image using spatial transformation functions [33]. These methods have been extended to include deformable models for single directional deformation field tracking [9, 26, 34] and diffeomorphic models for stationary velocity fields [32, 35, 36]. Further application of diffeomorphic models to cardiac motion tracking has been explored by [13, 15, 16, 30, 31]. These models predict motion fields that are both differentiable and invertible, ensuring one-to-one mappings and topology preservation. Recent studies in denoising diffusion probabilistic models (DDPM), such as [11] and [12], have achieved considerable success in registration tasks. However, DDPM-based methods face challenges in building temporal connections and demand substantial computational resources. The DL-based optical flow (OF) methods [37, 38, 39, 40] apply widely in nature image motion tracking. However, as illustrated in [7, 15, 16], due to annotations requirements and photometric constraints, they cannot be adopted in unsupervised cardiac motion tracking in medical image domains (See Section A1 in Appendix for detailed discussion). ", "page_idx": 2}, {"type": "text", "text": "3 Methodology ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Diffeomorphic Tracking of Cardiac Motion ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Diffeomorphic motion tracking and registration techniques are widely used in medical image analysis because they seek topology-preserving mapping between source and target images [6, 8]. Formally, given the source image $x_{0}$ and target image $x_{1}$ the diffeomorphic registration aims at a family of differentiable and invertible mappings $\\{\\phi_{t}\\}_{t\\in[0,1]}$ with the boundary condition $\\phi_{0}=\\mathrm{{Id}}$ and $\\phi_{1}(x_{0})=x_{1}$ , where Id is the identity mapping. The diffeomorphism $\\phi_{t}$ can be parameterized as its derivatives (velocity field) $\\mathbf{v}_{t}$ as follows: ", "page_idx": 2}, {"type": "image", "img_path": "CTIFk7b9jU/tmp/2f2e838c6a132a950bc782fddb4227cbdb578f6c7013572fc89f38d62aa6ddb7.jpg", "img_caption": ["Figure 2: Comparsion between our GPTrack (a) and conventional registration framework (b). "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "equation", "text": "$$\n\\frac{d\\phi_{t}}{d t}=\\mathbf{v}_{t}(\\phi_{t}):=\\mathbf{v}_{t}\\circ\\phi_{t}\\Longleftrightarrow\\phi_{t}=\\phi_{0}+\\int_{0}^{t}\\mathbf{v}_{s}(\\phi_{s})d s,\\;\\;s\\in[0,1],\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\circ$ is the composition operator. For numerical implementation, the associative property of the diffeomorphism group indicates $\\phi_{t_{1}+t_{2}}\\,=\\,\\phi_{t_{1}}\\circ\\phi_{t_{2}}$ , and the integral of Equation 1 can be approximated by $\\phi_{t+\\delta}\\,\\approx\\,\\phi_{t}+\\mathbf{v}_{t}\\delta$ for $t\\,\\in\\,[0,1)$ and small enough $\\delta$ . In this study, we follow the parameter settings of [7, 15, 16, 35, 36] and take $\\begin{array}{r}{\\delta\\,=\\,\\frac{1}{2^{N}}}\\end{array}$ , $N\\,=\\,7$ to discretize the path of diffeomorphic deformation. ", "page_idx": 2}, {"type": "image", "img_path": "CTIFk7b9jU/tmp/91064baff92beadf28c8587c7574076e90c671c9a3ad34f49092ea6f22dad3f9.jpg", "img_caption": ["Figure 3: The overview pipeline of GPTrack (one layer). The $x$ , $h$ , $\\dot{h}$ and $z$ denote input, forward hidden states, backward hidden states and latent coordinates. Feature $\\vec{f}_{t}$ with probabilistic prior on the latent space via Gaussian Process then enters the decoder to predict the motion field $\\phi$ . Subscript $t$ denotes the $t$ -th position in total $T$ moments. $e l u(\\cdot)$ represent the exponential linear units [42]. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "3.2 Motion Tracking with Gaussian Process Latent Coding ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Our proposed method adopts the generative variational autoencoder (VAE) framework as the backbone of a diffeomorphic tracking network, as suggested by previous methods [15, 16, 36]. However, as shown in Figure 2, the first difference between our method and other methods is that ours allows the registration network to aggregate the spatial information temporally, both forward and backward (see Figure 2(a)). The conventional approaches [9, 10, 15, 16, 24, 31] only conduct between two adjacent frames, which ignore the relationship of long-term dependency of the cardiac motion (see Figure 2(b)). Secondly, despite the large deformation through the path of diffeomorphism, the space of periodically specific human cardiac motion variation is bounded. Though methods [14, 15, 16] use Lagrangian strain to formulate the continuous dynamic of cardiac motion, however, without considering the motion consistency between two adjacent state spaces, the Lagrangian strain is prone to error accumulation and degrade the tracking performance. To address this problem, we take the simple yet efficient Bayesian approach, which employs the Gaussian Process (GP) to model cardiac motion dynamics in the compact feature space, predicting more consistent motion fields over dynamics parameters. Our proposed GPTrack can also be easily extended to other modalities or motion-tracking tasks, such as 3D Echocardiogram videos and 4D cardiac MRI. ", "page_idx": 3}, {"type": "text", "text": "In this paper, we follow the research [41] that employs the recursive manner in transformer for sequential data. As shown in Figure 3 left, the GPTrack pipeline comprised the GPTrack layer for feature extraction, the Gaussian Process (GP) layer for modelling the cardiac motion dynamics, and the Decoder for motion field estimation. Given the sequential 4D inputs $\\{\\boldsymbol{x}_{t}\\}_{t=1}^{T},\\boldsymbol{x}_{t}\\in\\dot{\\mathbb{R}}^{H\\times W\\times D\\times1}$ , where $H,W,D,T$ denote the height, width, depth, length of the input. For each $x_{t}$ , we first decompose it to P non-overlapping patches of shape p \u00d7 p \u00d7 p, where P = Hp \u00d7 Wp \u00d7 p p and p, p , p , p $\\begin{array}{r}{p,\\frac{H}{p},\\frac{W}{p},\\frac{D}{p}\\in\\mathbb{Z}^{+}}\\end{array}$ We then embed each patch as a feature with $C$ channels via embedding layers and disentangle patches with the dimension of $\\mathbb{R}^{P\\times C}$ from $x_{t}$ . The GPTrack layer then takes the $x$ , both forward and backward hidden states $\\vec{h},\\overleftarrow{h}\\in\\mathbb{R}^{P\\times C}$ as the input, then predicts the motion field $\\phi$ via the decoder after the GP layer. Note that the initial hidden states of forward $\\vec{h}_{0}$ and backward $\\overleftarrow{h}_{0}$ are set as zero. ", "page_idx": 3}, {"type": "text", "text": "3.3 Bidirectional Forward-Backward Recursive Cell ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The GPTrack layer consists of two independent GPTrack cells that respond to forward and backward computation. Similar to the [41, 43], adapting the hidden state to maintain and aggregate the sequential information allows the input with variable length. Meanwhile, the conventional convolutional neural network or vision in the transformer-based method is limited by the fixed input length. Furthermore, medical images such as Echocardiogram videos usually consist of hundreds of frames that cover multiple heartbeat cycles. Hence, parallel computing all frames requires a large amount of computational consumption, which hinders the application in real scenarios limited by lowcomputational devices. To this end, as shown in Tables 1 and 2, our GPTrack is able to formulate the variable temporal information while maintaining the comparable computational cost. ", "page_idx": 3}, {"type": "text", "text": "Using the forward GPTrack cell shown in the right of Figuer 3 as an example, the $x_{t}$ and $\\vec{h}_{t-1}$ followed by the addition of learnable position encoding $\\bar{\\mathrm{pos}_{t}}\\bar{\\in}\\mathbb{R}^{P\\times C}$ are respectively normalized by Layer Normalization. The linear self-attention then computes the attentive weight $A_{t}\\in\\mathbb{R}^{P\\times C}$ of combined $x_{t}$ and $\\vec{h}_{t-1}$ . The above operations can be formulated as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\nA_{t}=(\\delta(\\mathcal{W}_{Q}x)+1)(\\delta(\\mathcal{W}_{K}x)+1)^{\\top}\\mathcal{W}_{V}x;\\;x=\\mathbf{LN}(x_{t}+\\mathrm{pos}_{t})\\oplus\\mathbf{LN}(\\vec{h}_{t-1}+\\mathrm{pos}_{t})\\in\\mathbb{R}^{P\\times2C},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathcal{W}_{Q},\\mathcal{W}_{K},\\mathcal{W}_{V}\\in\\mathbb{R}^{2C\\times C}$ are learnable weights of different projection layers named query, key and value, $\\delta(\\cdot)$ represent the exponential linear units $e l u(\\cdot)$ [42], $\\mathrm{LN}(\\cdot)$ and $\\bigoplus$ are the layer normalization and the concatenation operation, respectively. ", "page_idx": 4}, {"type": "text", "text": "In order to raise the descendant hidden state $\\vec{h}_{t}$ that aggregates information before $t+1$ moment. The attentive weight $A_{t}$ then conducts the element-wise addition with ancestral positional encoded $\\vec{h}_{t-1}$ . Additionally, the $A_{t}$ takes the positional encoded $x_{t}$ as the residual connection and applies the addition operation. The Feed Forward Network denoted as $\\mathrm{FFN}(\\cdot)$ , is then introduced to output the feature of $t$ -th moment. The formulation can be written as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\vec{h}_{t}=A_{t}+\\mathbf{LN}(\\vec{h}_{t-1}+\\mathrm{pos}_{s})\\in\\mathbb{R}^{P\\times C},\\ \\vec{f}_{t}=\\mathbf{FFN}(\\mathbf{LN}(A_{t}+\\mathbf{LN}(x_{t}+\\mathrm{pos}_{s})))\\in\\mathbb{R}^{P\\times C}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "In the Bidirectional Forward-Backward Recursive Cell, both forward and backward share the same computation processes through the GPTrack cell. The only difference between the two directions is that the forward process starts from the first moment $x_{0}$ of input while the backward starts from the last moment $x_{T}$ . Hence, the feature $f_{t}$ in $t$ -th moment is formulated as $f_{t}={\\vec{f}}_{t}+{\\overleftarrow{f}}_{t}$ , $f_{t}\\in\\mathbb{R}^{P\\times C}$ , where $\\vec{f}_{t}$ aggregate the forward information from 0 to $t$ , while the $\\overleftarrow{f}_{t}$ aggregate the backward information from the last frame to the $t$ -th frame. ", "page_idx": 4}, {"type": "text", "text": "3.4 Gaussian Process in Cardiac Motion Tracking ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The primary objective of integrating the Gaussian Process (GP) is to establish a probabilistic prior in the latent space that incorporates prior knowledge. Specifically, it posits that cardiac motion across different individuals within the same region should yield similar motion fields in latent space encodings. Furthermore, as illustrated in Figure 1, cardiac structures within an individual that are spatially distant or exhibit motions in opposite directions should consistently adhere to the periodic pattern of motion. ", "page_idx": 4}, {"type": "text", "text": "Initially, we define a covariance (kernel) function for the GP layer as depicted in Figure 3. We design the prior for the latent space processes to be stationary, mean square continuous, and differentiable in sequential motion fields. This design stems from our expectation that the latent functions should model cardiac motion more prominently than visual features. Consequently, we anticipate the latent space to manifest continuous and relatively smooth behaviour. To this end, we employ the isotropic and stationary Matern kernel (refer to Equation 4) to fulfli the required covariance function structure: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\kappa(x_{t},x_{t-1})=\\sigma\\frac{2^{1-\\nu}}{\\Gamma(\\nu)}(\\sqrt{2\\nu}\\frac{D(x_{t},x_{t-1})}{l})^{\\nu}K_{\\nu}(\\sqrt{2\\nu}\\frac{D(x_{t},x_{t-1})}{l}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\nu,\\sigma,l>0$ are the smoothness, magnitude and length scale parameters, $K_{\\nu}$ is the modified Bessel function, and $D(\\cdot,\\cdot)$ denotes the distance metric between features of two consecutive motion fields. Our goal is to formulate cardiac motion as robust prior knowledge applicable to unseen data. To address this, we propose a position-related distance measurement. ", "page_idx": 4}, {"type": "text", "text": "As outlined in section 3.3 and referenced in [44], we utilize a learnable parameter $\\mathrm{pios}\\,\\in\\,\\mathbb{R}^{P\\times C}$ as the spatial positional encoding for each region, which provides relative or absolute positional information about the decomposed patches. To capture the periodic temporal positional information of cardiac motion, distinct from the spatial encodingp\u02d9os, we apply sine and cosine functions of various frequencies as temporal encoding $\\{\\mathbf{p}\\tilde{\\mathbf{o}}\\mathbf{s}_{t}\\}_{t=1}^{T},\\mathbf{\\dot{p}}\\tilde{\\mathbf{o}}\\mathbf{s}_{t}\\in\\mathbb{R}^{P\\overset{...}{C}}$ for each moment. The overall positional encoding at moment $t$ is formulated as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathrm{pos}_{t}=\\mathrm{pis}\\cdot\\mathrm{p\\tilde{o}s}_{t}\\in\\mathbb{R}^{P\\times C},\\ \\mathrm{where}\\ \\mathrm{p\\tilde{o}s}_{t}(t,i)=\\mathbb{I}_{(i=2k)}\\mathrm{sin}(t\\cdot n^{-\\frac{2k}{C}})+\\mathbb{I}_{(i=2k+1)}\\mathrm{cos}(t\\cdot n^{-\\frac{2k}{C}}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The $i$ denotes the $i$ -th position of $C$ channels, and $n$ is the scaling factor. We assign independent GP priors to all values in $\\{z_{t}\\}_{t=1}^{T},z_{t}\\in\\mathbb{R}^{P\\times C}$ to disseminate temporal information between frames ", "page_idx": 4}, {"type": "text", "text": "in the sequence. During this process, we regard the sequential output $\\{f_{t}\\}_{t=1}^{T}$ of GPTrack as noisecorrupted versions of the ideal latent space encodings, formulating the inference as the following GP regression model with noise observations: ", "page_idx": 5}, {"type": "equation", "text": "$$\nz_{t}\\sim\\mathrm{GP}\\left(\\mu(\\mathrm{pos}_{t}),\\kappa\\left(\\mathrm{pos}_{t-1},\\mathrm{pos}_{t}\\right)\\right),\\;f_{t}=z_{t}+\\epsilon_{t},\\;\\epsilon_{t}\\sim\\mathcal{N}(0,\\sigma^{2}),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\sigma^{2}$ is the noise variance of the likelihood model set as the learnable parameter in GPTrack. The above Gaussian process can be treated as the temporal sequence with intrinsic Markov property, and we adopt the methodology of connecting the Gaussian process with state space model [45] to decrease its computational complexity from $O(T^{3})$ to $O(T)$ , where $T$ is the number of time step. Concretely, the Gaussian process of Equation 6 corresponds to the following linear stochastic differential equation: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\frac{d}{d t}\\mathbf{z}(t)=\\mathbf{A}\\mathbf{z}(t)+\\mathbf{b}w(t),\\;\\;f(t)=\\mathbf{h}^{\\mathsf{T}}\\;\\mathbf{z}(t)+\\epsilon(t),\\;\\epsilon(t)\\sim\\mathcal{N}(0,\\sigma^{2}),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "with the solution as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbf{z}(t)=\\exp^{(t-r)\\mathbf{A}}\\mathbf{z}(r)+\\int_{r}^{t}\\exp^{(t-s)\\mathbf{A}}\\mathbf{b}w(s)d s,\\,\\forall r<t,}\\\\ {f(t)=\\mathbf{h}^{\\mathsf{T}}\\mathbf{z}(t)+\\epsilon(t),\\,\\epsilon(t)\\sim\\mathcal{N}(0,\\sigma^{2}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\begin{array}{r}{{\\mathbf z}(t):=(z(t),\\frac{d}{d t}z(t))}\\end{array}$ , $w(t)$ is the zero-mean Gaussian random process, and $\\mathbf{h}:=(0,1)^{\\top}$ is used for modelling observation model. The state transition matrix (vector) A and b can be calculated from the covariance function $\\kappa$ of the Gaussian process. For the Matern kernel shown in Equation 4, we take $\\begin{array}{r}{\\nu=\\frac{3}{2}}\\end{array}$ , and corresponding state transition matrix A and vector b [46] read as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbf{A}=\\left(\\begin{array}{c}{0,\\quad1}\\\\ {-\\frac{3}{l^{2}},-\\frac{2\\sqrt{3}}{l}}\\end{array}\\right),\\;\\mathbf{b}=(0,1)^{\\mathsf{T}}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Then we can discretize Equation 7 and get its weakly equivalent state-space model of Equation as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{{\\bf z}_{t}=\\Phi_{t}\\,{\\bf z}_{t-1}+{\\bf n}_{t},\\;f_{t}={\\bf h}^{\\top}{\\bf z}_{t}+\\epsilon_{t},\\;\\epsilon(t)\\sim\\mathcal{N}(0,\\sigma^{2}),\\;t=1,\\ldots,T,}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\Phi_{t}=\\exp^{D(\\mathrm{pos}_{\\mathrm{t}},\\mathrm{pos}_{t-1})\\mathbf{A}}$ , $\\mathbf{n}_{t}\\sim\\mathcal{N}(0,\\pmb{\\Phi}_{t}\\mathbf{b}Q_{w}(t-1,t)\\mathbf{b}^{\\top}\\pmb{\\Phi}_{t}^{\\top})$ , and $Q_{w}(t,t-1)$ is the covariance of $w(t)$ . Given the initial value $\\mathbf{z}_{0}\\sim\\mathcal{N}(\\pmb{\\mu}_{0},\\pmb{\\Sigma}_{0})$ with $\\pmb{\\mu}_{0}=0$ and $\\begin{array}{r}{\\Sigma_{0}=\\mathrm{diag}\\big(\\frac{\\sigma^{2}}{2},\\frac{3\\sigma^{2}}{l^{2}}\\big)}\\end{array}$ , we can sequentially calculate the posterior distribution $\\mathbf{z}_{t}|f_{1:t-1}\\sim\\mathcal{N}(\\overline{{\\pmb{\\mu}}}_{t},\\overline{{\\pmb{\\Sigma}}}_{t})$ and $z_{t}|f_{1:t}\\sim\\mathcal{N}(\\pmb{\\mu}_{t},\\pmb{\\Sigma}_{t})$ using update criterion of Kalman filter for state space model [47] as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\overline{{\\mu}}_{t}\\leftarrow\\Phi_{t}\\overline{{\\mu}}_{t-1},}&{\\overline{{\\Sigma}}_{t}\\leftarrow\\Phi_{t}\\overline{{\\Sigma}}_{t-1}\\Phi_{t}^{\\top}+\\Sigma_{0}-\\Phi_{t}\\Sigma_{0}\\Phi_{t}^{\\top},}\\\\ &{\\mu_{t}\\leftarrow\\overline{{\\mu}}_{t}+\\mathbf{k}_{t}(f_{t}-\\mathbf{h}^{\\top}\\overline{{\\mu}}_{t}),\\ \\ \\Sigma_{t}\\leftarrow\\overline{{\\Sigma}}_{t}-\\mathbf{k}_{t}\\mathbf{h}^{\\top}\\overline{{\\mu}}_{t},\\ t=1,\\dots,T,}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where kt := h\u03a3tht+\u03c32 is the optimal Kalman gain at time $t$ . The output of the GP layer in $t$ -th moment thus can be formulated as $\\begin{array}{r}{z_{t}^{G P}=\\mathrm{ReLU}({\\bf k}_{t}z_{t})}\\end{array}$ , where ReLU is the activation function. In the final, the $t$ -th motion field $\\theta_{t}$ is obtained by decoder from the $z_{t}^{G P}$ . ", "page_idx": 5}, {"type": "text", "text": "3.5 Overall Loss of Tracking a Time Sequence of Cardiac Motion ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The decoder takes the Gaussian-process coding {ztGP}tT=1 as velocity filed to composite the diffeo- morphic motion field $\\phi$ according to the criterion of Section 3.1. Here, we adopt the training loss of [15, 16], which minimizes the integration of four components summarized as follows: a) Dissimilarities of tracking results between adjacent states from both forward and backward; b) Smoothness of motion fields between adjacent states from both forward and backward; c) Dissimilarities of tracking results between the start state and each state; $\\mathbf{d}$ ) Smoothness of motion fields between the start state and each state. The overall loss function $\\mathcal{L}$ is formulated as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{n-1}[\\mathcal{L}_{k l}(x_{t},x_{t+1})}+\\alpha_{1}(\\underbrace{\\mathcal{L}_{s m}(\\phi_{t:t+1})+\\mathcal{L}_{s m}(\\phi_{t+1:t})}_{\\mathbf{b})})+\\alpha_{2}\\underbrace{\\mathcal{L}_{n c}(x_{t+1},x_{1}\\circ\\phi_{0:t+1})}_{\\mathbf{c})}+\\alpha_{3}\\underbrace{\\mathcal{L}_{s m}(\\phi_{1:t+1})}_{\\mathbf{d})}],\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\alpha_{1},\\ \\alpha_{2}$ and $\\alpha_{3}$ are loss weights, and $\\phi_{t_{1}:t_{2}}$ is the motion field from state $t_{1}$ to $t_{2}$ . $\\begin{array}{r}{\\check{\\lambda}_{k l}(x_{t},\\dot{x_{t+1}})\\overset{=}{=}\\mathbb{K L}(q(\\bar{z}_{t}^{G P}|x_{t};x_{t+1})||p(\\bar{z}_{t}^{G P}|x_{t};x_{t+1}^{\\cdots\\cdots}))+\\mathbb{K L}(q(\\bar{z}_{t}^{G P}|x_{t+1,\\lfloor x_{t}\\rfloor})||p(\\bar{z}_{t}^{G P}|x_{t+1}^{\\cdots};x_{t}))}\\end{array}$ is the summation of forward and backward VAE losses with latent coding $z_{t}^{G T}$ , posterior distribution $q$ and conditional distribution $p$ , $L_{n c}$ is the negative normalized local cross-correlation metric, and $\\dot{\\mathcal{L}}_{s m}(\\phi)=||\\nabla\\phi||_{2}^{2}$ is the $\\ell_{2}$ -total variation metric. ", "page_idx": 5}, {"type": "text", "text": "4 Experiment ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "4.1 Datasets ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "CardiacUDA [17]. The CardiacUDA dataset collected from two medical centers consists of 314 echocardiogram videos from patients. The video is scanned from the apical four-chamber heart (A4C) view. In this paper, we conduct training and validation in the A4C view that consists of 314 videos with 5 frame annotations in the Left/Right Ventricle and Atrium (LV, LA, RV, RA). For testing, we report our results in 10 videos with full annotation provided by the CardiacUDA. ", "page_idx": 6}, {"type": "text", "text": "CAMUS [18]. The CAMUS dataset provides pixel-level annotations for the left ventricle, myocardium, and left atrium in the Apical two-chamber view, which consists of 500 echocardiogram videos in total. There are 450 subjects in the training set with 2 frames annotated in the Left Ventricle (LV), Left Atrium (LA) and Myocardium (Myo) in the end-diastole (ED) and end-systole (ES) of the heartbeat cycle. The remaining 50 subjects without any annotation masks belong to the testing set. ", "page_idx": 6}, {"type": "text", "text": "ACDC [19]. The ACDC dataset consists of $100\\,4\\mathrm{D}$ temporal cardiac MRI cases. All data provide the segmentation annotations corresponding with the Left Ventricle (LV), Left Atrium (LA) and Myocardium (Myo) in the end-diastole (ED) and end-systole (ES) during the heartbeat cycle. ", "page_idx": 6}, {"type": "text", "text": "4.2 Implementation Details ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Training. We trained the model using the Adam optimizer with betas equal to 0.9 and 0.99. The training batch size of the model was set to 1. We trained for a total of 1000 epochs with an initial learning rate of $5e^{-4}$ and decay by a factor of 0.5 in every 50 epochs. During training, for CardiacUDA [17] and CAMUS [18], we resized each frame to $384\\times384$ and then randomly cropped them to $256\\times256$ . All frames were normalized to [0,1] during training. In temporal augmentation of datasets [17, 18], we randomly selected 32 frames from an echocardiogram video with a sampling ratio of either 1 or 2. For ACDC [19], we resampled all scans with a voxel spacing of $1.5\\times1.5\\times$ $3.15\\mathrm{mm}$ and cropped them to $128\\times128\\times32$ , normalized the intensity of all images to [-1, 1]. For spatial data augmentation of all datasets, we randomly applied filpping, rotation and Gaussian blurring. In CardiacUDA, we split the dataset into $8:2$ for training and validation. During testing, we reported results in 10 fully annotated videos. In the CAMUS [18] dataset, videos without annotation are used for only training, while we randomly split the remaining 450 annotated videos into 300/50/100 for training, validation and testing. In the ACDC [19], following the [11, 12], we split the training set in the ratio of 90 and 10 for training and testing. The reproduced methods strictly follow the official code and the description in the paper. For all experiments, We use Intel(R) Xeon(R) Platinum 8375C with $1\\times\\,{\\mathrm{RTX}}3090$ for both training and inference. All reproduced methods strictly followed the training settings with their original paper in the same experimental environment. ", "page_idx": 6}, {"type": "text", "text": "Inference. For CardiacUDA and CAMUS, we resized videos to $384\\times384$ , cropped to $256\\times256$ in central and normalized to [0,1]. We sample 32 frames that cover the segmentation annotation. When the sequence has more than 32 frames, the extra frames will be removed from the sequence, except for the first and the last one. The ACDC dataset remains the same sampling strategy as training in the inference stage, without any argumentation except for normalizing intensity to [-1,1]. ", "page_idx": 6}, {"type": "text", "text": "Evaluation Metrics. For the evaluation of the quality of registered target frames, we follow [12] to use the Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index (SSIM) [48] to measure whether the Lagrangian motion field is accurately estimated between the first frame and the following wrapped frames. We also use the Dice [49] score to measure the discrepancy between tracked and ground-truth cardiac segmentation. For CardiacUDA [17], only the first frame and corresponding segmentation are provided for tracking the following 32 frames, then report the averaged results by the above metrics in these 32 frames. For CAMUS [18] and ACDC [19], frame and segmentation of the ED stage are used to track the go-after frames, and we report all the metrics in the ES stage. We evaluate diffeomorphic property by computing the percentage of non-positive values of the Jacobian determinant $d e t(\\bar{J_{\\phi}})\\leq\\bar{0}\\;\\bar{(\\%)}$ on the Lagrangian motion field. In order to access the evaluation of comparing the physiological plausibility following the [50, 51], we also compute the mean absolute difference between the 1 and Jacobian determinant $(||J_{\\phi}|-1|)$ over the tracking areas. For a fair comparison, we evaluate the computational efficiency and report the computational time in seconds (Times), the parameter quantities in millions (Params), and the tera-floating point operations per second (TFlops). We also provide the result evaluated by Hussdorf Distance $(H D)$ in Tables B1, B2 and $B3$ of Appendix Section $B$ . ", "page_idx": 6}, {"type": "table", "img_path": "CTIFk7b9jU/tmp/cb074fb150c3e49b286f73ee8f66a3d5a77e95bc0e604eb5f30672131fa646ca.jpg", "table_caption": ["Table 1: The performance1 of different registration methods in Cardiac-UDA dataset [17]. Results were reported in structures (RV, RA, LV, LA) and the overall averaged Dice score (Avg. $\\%$ ). "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "CTIFk7b9jU/tmp/efef5d45b09b76736d998ffb466fefb77e1d1e602d4117425ae202da3aaa89a5.jpg", "table_caption": ["Table 2: The performance1 of different registration methods in ACDC [19] dataset. Results reported in structures (RV, LV, Myo) and overall averaged Dice score (Avg. $\\%$ ). "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "4.3 Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Result of 3D Echocardiogram Video. In Table 1, we compare our method with state-of-the-arts 2D registration [9, 10, 12, 15, 31, 52] and non deep learning [6, 8, 24] methods in CardiacUDA dataset. In comparison to DeepTag [15], our GPTrack-XL reach $\\bar{82}.37\\%$ and 12.75 in DICE and HD scores with the best non-positive Jacobian determinant value, which denotes the learned motion field is smooth. The registration quality of our method also achieved the best with 32.03 $(3.50\\,\\uparrow)$ and 80.04 $(3.64\\uparrow)$ in PSNR and SSIM compared to the second-best method, respectively. Though GP", "page_idx": 7}, {"type": "table", "img_path": "CTIFk7b9jU/tmp/8d07100a87d270c222d7746aeba3195bc6f6ccc16e2a7debbae3396fe864c05b.jpg", "table_caption": ["Table 3: The segmentation performance1 of different cardiac structures in the CAMUS [18]. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Track introduces more learnable parameters and requires a small amount of additional computation, slightly increases inference time and TFlops compared to DeepTag [15] and VM-DIF [9]. GPTrack surpasses all other methods in the registration and verifies the necessity of formulating a strong temporal relationship among frames by using the recursive manner. Tables 1 and 3 show registration results of cardiac structures (LV, RV, LA, RA, Myo). Compared to other methods, our GPTrack can outperform existing baseline methods by a substantial margin. In areas such as the left atrium (LA) and left ventricular (LV), which usually cause larger deformation, our method can also provide better alignment than other approaches. ", "page_idx": 7}, {"type": "image", "img_path": "CTIFk7b9jU/tmp/0943185a872e8f6e70a2978fdd22e9a6f0636c445dc25043a62cbe89aece1ad8.jpg", "img_caption": ["Figure 4: The visualization in 3D Echocardiogram video of motion tracking error. We visualised the last frame of tracking result and ground truth from 32 consecutive frames in CardiacUDA [17]. Colours Red, Blue, Green and Orange denote cardiac structures RA, RV, LV and LA, respectively. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "CTIFk7b9jU/tmp/1a0fec9b6d4b6d3e1ec7a4a03b8057c72675b29e31eb2d4f43040e23823f00e3.jpg", "img_caption": ["Figure 5: The visualization in 4D Cardiac MRI of motion tracking error. We visualised the result of the last frame tracking from ED to ES and corresponding ground truth in ACDC [17]. Colours Red, Blue, and Green denote cardiac structures MYO, LA, and LV, respectively. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "As shown in Figure 4, the tracking error of our GPTrack and other methods show a significant difference in LA (Labelled by Orange Colour) and LV (Labelled by Green Colour) when compared to the ground truth. The methods [24, 10, 9, 31] present inaccuracy tracking results due to lack of the constraints on the consecutive motion and ignore the long-term temporal information. These results further verify that our specifically designed GPTrack for modelling motion patterns is more suitable for cardiac motion tracking on echocardiography. ", "page_idx": 8}, {"type": "text", "text": "Result of 4D Temporal Cardiac MRI Dataset. We compare our GPTrack method against with the state-of-the-art deep learning based methods [9, 10, 11, 12, 15] and different Non-rigid approaches [6, 8, 24]. As illustrated in Table 2, our GPTrack-XL achieves the best average DICE score of 82.65 compared to FSDiffReg [11] with 82.30. In registration quality, our GPTrack-XL reaches the highest scores, 31.52 and 86.19, in PSNR and SSIM, respectively. Moreover, our Jacobian determinant on deformation fields shows numbers comparable to other methods with the diffeomorphic constraint. All results are based on our fast and lightweight model, reducing around $96.93\\%$ inference time, $17.2\\%$ model parameters and $76.02\\%$ computational consumption (TFlops) compared to the secondbest performance. In comparison to the diffusion-based method [12, 11], which requires enormous computation that hinders real-time inference and is nearly impossible to deploy in real scenarios, the GPTrack preserve light-weight and considerable performance by formulating cardiac motion patterns as the Gaussian process latent coding and bidirectionally understand the cardiac motion. The visualization result in Figure 5 also indicates our GPTrack can achieve better tracking accuracy. ", "page_idx": 8}, {"type": "text", "text": "4.4 Ablation Study ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "The Scale of Model Hyper-parameters. Table 4 shows the settings of the 2D/3D GPTrack-M/L/XL. For the 3D echocardiogram video dataset and 4D cardiac MRI dataset, the GPTrack with different scales has different patch sizes and dimension numbers. Referring to the result performed by Tables 3, 1 and 2, the registration result can be boosted by increasing the layers number and dimension size of GPTracks according to different requirements. ", "page_idx": 8}, {"type": "text", "text": "Table 4: The ablation study of the different configurations of our 2D / 3D GPTracks (M, L, XL). ", "page_idx": 9}, {"type": "table", "img_path": "CTIFk7b9jU/tmp/4fe8732ef883cacb482f24f38ec4c33147fb2312aa6f44133096395c811bb864.jpg", "table_caption": [], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "Table 5: Ablations of Bi-Directional (Bi-direct.) and Gaussian Process (GP) of GPTrack-XL. ", "page_idx": 9}, {"type": "table", "img_path": "CTIFk7b9jU/tmp/1cebe308eaec06ba10abb02e9bf667d1af63208f73eae8f645cb3b0ac43b7ddc.jpg", "table_caption": [], "table_footnote": [], "page_idx": 9}, {"type": "image", "img_path": "CTIFk7b9jU/tmp/9559a3971d48acd44262f6ad14891b69dceaccc3aa6690b4df1c426743c54c9f.jpg", "img_caption": ["Figure 6: The Tracking Error performed by different methods in 32 consecutive frames of CardiacUDA [17] full annotated set. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "The Ablation of Bidirectional Recursive Manner and Gaussian Process. Table 5 illustrates the Bi-directional layer in GPTracks can better formulate both forward and backward motion fields by aggregating the temporal information of the cardiac heartbeat cycle. The Gaussian Process models cardiac motion with strong prior knowledge from data, which makes more accurate predictions of the deformation field. The Figure 6 illustrate the tracking error from 1-st to 32-th frame in CardiacUDA [17] full annotated set. Our GPTrack and the DeepTag [15] both use the Lagrangian strain. However, the accuracy degrades significantly without aggregating the temporal information and GP when the input length increases. As shown in Figure 6, the tracking error after the 20-th frame becomes larger by using only Lagrangian strain, while introducing GP and bidirectional recursive methods can efficiently eliminate the tracking error by predicting more accurate motion fields. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion and Limitation ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we proposed a new framework named GPTrack to improve cardiac motion tracking accuracy. GPTrack innovatively aggregates both forward and backward temporal information by using the bidirectional recurrent transformer. Furthermore, we introduce the Gaussian Process to model the variability and predictability of cardiac motion. In experiments, our framework demonstrates the state-of-the-art in 3D echocardiogram and 4D cardiac MRI datasets. The limitation of our framework is that we use positional encoding as the prior knowledge of the cardiac motion, which may degrade the tracking performance in out-of-domain datasets. In our future work, we will build a more robust representation of cardiac motion and further our work across different medical domains. We also provide the illustration of Broader Impacts, please see Section A2 in Appendix. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was partially supported by grants from the National Natural Science Foundation of China (Grant No. 62306254), the Hetao Shenzhen-Hong Kong Science and Technology Innovation Cooperation Zone (Project No. HZQB-KCZYB-2020083), and the Research Grants Council of the Hong Kong Special Administrative Region, China (Project Reference Number: T45-401/22-N). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Daniel Rueckert, Luke I Sonoda, Carmel Hayes, Derek LG Hill, Martin O Leach, and David J Hawkes. Nonrigid registration using free-form deformations: application to breast mr images. IEEE transactions on medical imaging, 18(8):712\u2013721, 1999. ", "page_idx": 9}, {"type": "text", "text": "[2] J-P Thirion. Image matching as a diffusion process: an analogy with maxwell\u2019s demons. Medical image analysis, 2(3):243\u2013260, 1998.   \n[3] Alessandro Becciu, Hans van Assen, Luc Florack, Sebastian Kozerke, Vivian Roode, and Bart M ter Haar Romeny. A multi-scale feature based optic flow method for 3d cardiac motion estimation. In Scale Space and Variational Methods in Computer Vision: Second International Conference, SSVM 2009, Voss, Norway, June 1-5, 2009. Proceedings 2, pages 588\u2013599. Springer, 2009.   \n[4] Liang Wang, Patrick Clarysse, Zhengjun Liu, Bin Gao, Wanyu Liu, Pierre Croisille, and Philippe Delachartre. A gradient-based optical-flow cardiac motion estimation method for cine and tagged mr images. Medical image analysis, 57:136\u2013148, 2019.   \n[5] KY Esther Leung, Mikhail G Danilouchkine, Marijn van Stralen, Nico de Jong, Antonius FW van der Steen, and Johan G Bosch. Left ventricular border tracking using cardiac motion models and optical flow. Ultrasound in medicine & biology, 37(4):605\u2013616, 2011.   \n[6] M Faisal Beg, Michael I Miller, Alain Trouv\u00e9, and Laurent Younes. Computing large deformation metric mappings via geodesic flows of diffeomorphisms. International journal of computer vision, 61:139\u2013157, 2005.   \n[7] John Ashburner. A fast diffeomorphic image registration algorithm. Neuroimage, 38(1):95\u2013113, 2007.   \n[8] Zhengyang Shen, Fran\u00e7ois-Xavier Vialard, and Marc Niethammer. Region-specific diffeomorphic metric mapping. Advances in Neural Information Processing Systems, 32, 2019.   \n[9] Guha Balakrishnan, Amy Zhao, Mert R Sabuncu, John Guttag, and Adrian V Dalca. An unsupervised learning model for deformable medical image registration. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 9252\u20139260, 2018.   \n[10] Guha Balakrishnan, Amy Zhao, Mert R Sabuncu, John Guttag, and Adrian V Dalca. Voxelmorph: a learning framework for deformable medical image registration. IEEE transactions on medical imaging, 38(8):1788\u20131800, 2019.   \n[11] Yi Qin and Xiaomeng Li. Fsdiffreg: Feature-wise and score-wise diffusion-guided unsupervised deformable image registration for cardiac images. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 655\u2013665. Springer, 2023.   \n[12] Boah Kim, Inhwa Han, and Jong Chul Ye. Diffusemorph: Unsupervised deformable image registration using diffusion model. In European conference on computer vision, pages 347\u2013364. Springer, 2022.   \n[13] Hanchao Yu, Shanhui Sun, Haichao Yu, Xiao Chen, Honghui Shi, Thomas S Huang, and Terrence Chen. Foal: Fast online adaptive learning for cardiac motion estimation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 4313\u20134323, 2020.   \n[14] Nripesh Parajuli, Allen Lu, Kevinminh Ta, John Stendahl, Nabil Boutagy, Imran Alkhalil, Melissa Eberle, Geng-Shi Jeng, Maria Zontak, Matthew O\u2019Donnell, et al. Flow network tracking for spatiotemporal and periodic point matching: Applied to cardiac motion analysis. Medical image analysis, 55:116\u2013135, 2019.   \n[15] Meng Ye, Mikael Kanski, Dong Yang, Qi Chang, Zhennan Yan, Qiaoying Huang, Leon Axel, and Dimitris Metaxas. Deeptag: An unsupervised deep learning method for motion tracking on cardiac tagging magnetic resonance images. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 7261\u20137271, 2021.   \n[16] Meng Ye, Dong Yang, Qiaoying Huang, Mikael Kanski, Leon Axel, and Dimitris N Metaxas. Sequencemorph: A unified unsupervised learning framework for motion tracking on cardiac image sequences. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023.   \n[17] Jiewen Yang, Xinpeng Ding, Ziyang Zheng, Xiaowei Xu, and Xiaomeng Li. Graphecho: Graph-driven unsupervised domain adaptation for echocardiogram video segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 11878\u201311887, 2023.   \n[18] Sarah Leclerc, Erik Smistad, Joao Pedrosa, Andreas \u00d8stvik, Frederic Cervenansky, Florian Espinosa, Torvald Espeland, Erik Andreas Rye Berg, Pierre-Marc Jodoin, Thomas Grenier, et al. Deep learning for segmentation using an open large-scale dataset in 2d echocardiography. IEEE transactions on medical imaging, 38(9):2198\u20132210, 2019.   \n[19] Olivier Bernard, Alain Lalande, Clement Zotti, Frederick Cervenansky, Xin Yang, Pheng-Ann Heng, Irem Cetin, Karim Lekadir, Oscar Camara, Miguel Angel Gonzalez Ballester, et al. Deep learning techniques for automatic mri cardiac multi-structures segmentation and diagnosis: is the problem solved? IEEE transactions on medical imaging, 37(11):2514\u20132525, 2018.   \n[20] Ruzena Bajcsy and Stane Kova\u02c7ci\u02c7c. Multiresolution elastic matching. Computer vision, graphics, and image processing, 46(1):1\u201321, 1989.   \n[21] Christos Davatzikos. Spatial transformation and registration of brain images using elastically deformable models. Computer Vision and Image Understanding, 66(2):207\u2013222, 1997.   \n[22] John Ashburner and Karl J Friston. Voxel-based morphometry\u2014the methods. Neuroimage, 11(6):805\u2013821, 2000.   \n[23] Nael F Osman, William S Kerwin, Elliot R McVeigh, and Jerry L Prince. Cardiac motion tracking using cine harmonic phase (harp) magnetic resonance imaging. Magnetic Resonance in Medicine: An Official Journal of the International Society for Magnetic Resonance in Medicine, 42(6):1048\u20131060, 1999.   \n[24] Brian B Avants, Charles L Epstein, Murray Grossman, and James C Gee. Symmetric diffeomorphic image registration with cross-correlation: evaluating automated labeling of elderly and neurodegenerative brain. Medical image analysis, 12(1):26\u201341, 2008.   \n[25] Julian Krebs, Tommaso Mansi, Herv\u00e9 Delingette, Li Zhang, Florin C Ghesu, Shun Miao, Andreas K Maier, Nicholas Ayache, Rui Liao, and Ali Kamen. Robust non-rigid registration through agent-based action learning. In Medical Image Computing and Computer Assisted Intervention- MICCAI 2017: 20th International Conference, Quebec City, QC, Canada, September 11-13, 2017, Proceedings, Part I 20, pages 344\u2013352. Springer, 2017.   \n[26] Xiaohuan Cao, Jianhua Yang, Jun Zhang, Dong Nie, Minjeong Kim, Qian Wang, and Dinggang Shen. Deformable image registration based on similarity-steered cnn regression. In Medical Image Computing and Computer Assisted Intervention- MICCAI 2017: 20th International Conference, Quebec City, QC, Canada, September 11-13, 2017, Proceedings, Part I 20, pages 300\u2013308. Springer, 2017.   \n[27] Marc-Michel Roh\u00e9, Manasi Datar, Tobias Heimann, Maxime Sermesant, and Xavier Pennec. Svf-net: learning deformable image registration using shape matching. In Medical Image Computing and Computer Assisted Intervention- MICCAI 2017: 20th International Conference, Quebec City, QC, Canada, September 11-13, 2017, Proceedings, Part I 20, pages 266\u2013274. Springer, 2017.   \n[28] Hessam Sokooti, Bob De Vos, Floris Berendsen, Boudewijn PF Lelieveldt, Ivana I\u0161gum, and Marius Staring. Nonrigid image registration using multi-scale 3d convolutional neural networks. In Medical Image Computing and Computer Assisted Intervention- MICCAI 2017: 20th International Conference, Quebec City, QC, Canada, September 11-13, 2017, Proceedings, Part I 20, pages 232\u2013239. Springer, 2017.   \n[29] Bob D De Vos, Floris F Berendsen, Max A Viergever, Marius Staring, and Ivana I\u0161gum. End-to-end unsupervised deformable image registration with a convolutional neural network. In Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support: Third International Workshop, DLMIA 2017, and 7th International Workshop, ML-CDS 2017, Held in Conjunction with MICCAI 2017, Qu\u00e9bec City, QC, Canada, September 14, Proceedings 3, pages 204\u2013212. Springer, 2017.   \n[30] Kevinminh Ta, Shawn S Ahn, Allen Lu, John C Stendahl, Albert J Sinusas, and James S Duncan. A semisupervised joint learning approach to left ventricular segmentation and motion tracking in echocardiography. In 2020 IEEE 17th International Symposium on Biomedical Imaging (ISBI), pages 1734\u20131737. IEEE, 2020.   \n[31] Shawn S Ahn, Kevinminh Ta, Allen Lu, John C Stendahl, Albert J Sinusas, and James S Duncan. Unsupervised motion tracking of left ventricle in echocardiography. In Medical imaging 2020: Ultrasonic imaging and tomography, volume 11319, pages 196\u2013202. SPIE, 2020.   \n[32] Tony CW Mok and Albert CS Chung. Unsupervised deformable image registration with absent correspondences in pre-operative and post-recurrence brain tumor mri scans. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 25\u201335. Springer, 2022.   \n[33] Max Jaderberg, Karen Simonyan, Andrew Zisserman, et al. Spatial transformer networks. Advances in neural information processing systems, 28, 2015.   \n[34] Xiaohuan Cao, Jianhua Yang, Jun Zhang, Qian Wang, Pew-Thian Yap, and Dinggang Shen. Deformable image registration using a cue-aware deep regression network. IEEE Transactions on Biomedical Engineering, 65(9):1900\u20131911, 2018.   \n[35] Adrian V Dalca, Guha Balakrishnan, John Guttag, and Mert R Sabuncu. Unsupervised learning for fast probabilistic diffeomorphic registration. In Medical Image Computing and Computer Assisted Intervention\u2013 MICCAI 2018: 21st International Conference, Granada, Spain, September 16-20, 2018, Proceedings, Part I, pages 729\u2013738. Springer, 2018.   \n[36] Tony CW Mok and Albert Chung. Fast symmetric diffeomorphic image registration with convolutional neural networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 4644\u20134653, 2020.   \n[37] Alexey Dosovitskiy, Philipp Fischer, Eddy Ilg, Philip Hausser, Caner Hazirbas, Vladimir Golkov, Patrick Van Der Smagt, Daniel Cremers, and Thomas Brox. Flownet: Learning optical flow with convolutional networks. In Proceedings of the IEEE international conference on computer vision, pages 2758\u20132766, 2015.   \n[38] Pengpeng Liu, Michael Lyu, Irwin King, and Jia Xu. Selflow: Self-supervised learning of optical flow. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 4571\u20134580, 2019.   \n[39] Junhwa Hur and Stefan Roth. Iterative residual refinement for joint optical flow and occlusion estimation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 5754\u20135763, 2019.   \n[40] Eddy Ilg, Nikolaus Mayer, Tonmoy Saikia, Margret Keuper, Alexey Dosovitskiy, and Thomas Brox. Flownet 2.0: Evolution of optical flow estimation with deep networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2462\u20132470, 2017.   \n[41] Jiewen Yang, Xingbo Dong, Liujun Liu, Chao Zhang, Jiajun Shen, and Dahai Yu. Recurring the transformer for video action recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14063\u201314073, 2022.   \n[42] Djork-Arn\u00e9 Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and accurate deep network learning by exponential linear units (elus). arXiv preprint arXiv:1511.07289, 2015.   \n[43] Xingbo Dong, Jiewen Yang, Andrew Beng Jin Teoh, Dahai Yu, Xiaomeng Li, and Zhe Jin. Video-based face outline recognition. Pattern Recognition, 152:110482, 2024.   \n[44] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.   \n[45] Simo S\u00e4rkk\u00e4 and Arno Solin. Applied stochastic differential equations, volume 10. Cambridge University Press, 2019.   \n[46] Simo S\u00e4rkk\u00e4, Arno Solin, and Jouni Hartikainen. Spatiotemporal learning via infinite-dimensional bayesian filtering and smoothing: A look at gaussian process regression through kalman filtering. IEEE Signal Processing Magazine, 30(4):51\u201361, 2013.   \n[47] Simo Sarkka and Jouni Hartikainen. Infinite-dimensional kalman filtering approach to spatio-temporal gaussian process regression. In Artificial intelligence and statistics, pages 993\u20131001. PMLR, 2012.   \n[48] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 13(4):600\u2013612, 2004.   \n[49] Abdel Aziz Taha and Allan Hanbury. Metrics for evaluating 3d medical image segmentation: analysis, selection, and tool. BMC medical imaging, 15:1\u201328, 2015.   \n[50] Chen Qin, Shuo Wang, Chen Chen, Huaqi Qiu, Wenjia Bai, and Daniel Rueckert. Biomechanics-informed neural networks for myocardial motion tracking in mri. In Medical Image Computing and Computer Assisted Intervention\u2013MICCAI 2020: 23rd International Conference, Lima, Peru, October 4\u20138, 2020, Proceedings, Part III 23, pages 296\u2013306. Springer, 2020.   \n[51] Chen Qin, Shuo Wang, Chen Chen, Wenjia Bai, and Daniel Rueckert. Generative myocardial motion tracking via latent space exploration with biomechanics-informed prior. Medical Image Analysis, 83:102682, 2023.   \n[52] Mingyuan Meng, Lei Bi, Michael Fulham, Dagan Feng, and Jinman Kim. Non-iterative coarse-to-fine transformer networks for joint affine and deformable image registration. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 750\u2013760. Springer, 2023.   \n[53] Mingyuan Meng, Dagan Feng, Lei Bi, and Jinman Kim. Correlation-aware coarse-to-fine mlps for deformable medical image registration. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9645\u20139654, 2024.   \n[54] Zeyuan Chen, Yuanjie Zheng, and James C Gee. Transmatch: A transformer-based multilevel dual-stream feature matching network for unsupervised deformable image registration. IEEE transactions on medical imaging, 43(1):15\u201327, 2023.   \n[55] Noemi Carranza-Herrezuelo, Ana Bajo, Filip Sroubek, Cristina Santamarta, Gabriel Crist\u00f3bal, Andr\u00e9s Santos, and Mar\u00eda J Ledesma-Carbayo. Motion estimation of tagged cardiac magnetic resonance images using variational techniques. Computerized Medical Imaging and Graphics, 34(6):514\u2013522, 2010.   \n[56] Simon Meister, Junhwa Hur, and Stefan Roth. Unflow: Unsupervised learning of optical flow with a bidirectional census loss. In Proceedings of the AAAI conference on artificial intelligence, volume 32, 2018.   \n[57] Xiaoyu Shi, Zhaoyang Huang, Dasong Li, Manyuan Zhang, Ka Chun Cheung, Simon See, Hongwei Qin, Jifeng Dai, and Hongsheng Li. Flowformer++: Masked cost volume autoencoding for pretraining optical flow estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1599\u20131610, 2023.   \n[58] Zhaoyang Huang, Xiaoyu Shi, Chao Zhang, Qiang Wang, Ka Chun Cheung, Hongwei Qin, Jifeng Dai, and Hongsheng Li. Flowformer: A transformer architecture for optical flow. In European conference on computer vision, pages 668\u2013685. Springer, 2022.   \n[59] Rico Jonschkowski, Austin Stone, Jonathan T Barron, Ariel Gordon, Kurt Konolige, and Anelia Angelova. What matters in unsupervised optical flow. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part II 16, pages 557\u2013572. Springer, 2020.   \n[60] Ziyang Zheng, Jiewen Yang, Xinpeng Ding, Xiaowei Xu, and Xiaomeng Li. Gl-fusion: Global-local fusion network for multi-view echocardiogram video segmentation. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 78\u201388. Springer, 2023.   \n[61] Jiewen Yang, Yiqun Lin, Bin Pu, Jiarong Guo, Xiaowei Xu, and Xiaomeng Li. Cardiacnet: Learning to reconstruct abnormalities for cardiac disease assessment from echocardiogram videos. In European Conference on Computer Vision. Springer, 2024.   \n[62] Bin Pu, Kenli Li, Jianguo Chen, Yuhuan Lu, Qing Zeng, Jiewen Yang, and Shengli Li. Hfsccd: a hybrid neural network for fetal standard cardiac cycle detection in ultrasound videos. IEEE Journal of Biomedical and Health Informatics, 2024. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A1 Difference Between Optical Flow and Diffeomorphism Mapping ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Optical flow (OF) based methods, often applied in object tracking of video sequences, have been explored by [3, 4, 5]. However, their effectiveness in medical imaging is limited due to challenges in accommodating large deformations and the inherent low quality of certain medical imaging modalities [4, 55], such as echocardiogram videos. ", "page_idx": 14}, {"type": "text", "text": "With the progression of deep learning, neural networks have also been employed to predict optical flow, which is crucial for predicting dynamic motion trajectories in video sequences. Notable implementations include FlowNet [37], iterative methods by [39], and self-supervised learning approaches by [38] and [56]. However, while supervised methods require a ground truth annotation for training cost functions [37, 39, 40, 57, 58], unsupervised approaches depend on photometric loss to ensure motion consistency [38, 56, 59], which can be challenging to obtain in medical images. ", "page_idx": 14}, {"type": "text", "text": "Last but not least, OF-based methods do not necessarily preserve topology, non-globally one-to-one (objective) smooth and continuous mapping with derivatives that are invertible. In cardiac motion tracking, we consider the deformation in each point of the adjacent frame to remain one-on-one mapping and be invertible for forward and backward deformation fields. Directly using the OF-based method to predict the motion field of cardiac motion may lead to incorrect estimation. ", "page_idx": 14}, {"type": "text", "text": "A2 Broader Impacts ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Our work focuses on cardiac motion tracking with an unsupervised framework named GPTrack. The GPTrack framework has the potential to support medical imaging physicians, such as radiologists and sonographers, in observing the cardiac motion of patients. This is a fundamental task for assessing cardiac function, and we are able to provide decision support and improve analysis efficiency and analysis reproducibility in clinical scenarios. ", "page_idx": 14}, {"type": "text", "text": "Moreover, our new framework illustrates that cardiac motion can be formulated as strong prior knowledge, which is able to be utilised to enhance tracking accuracy. Also, our work presents several advantages that help us make progress towards these benefits, which improve the performance of automated motion field estimation algorithms. The method not only improves the precision of motion tracking and segmentation in both 3D and 4D medical image modailites [17, 18, 19, 60, 61, 62] but also provides a comprehensive observation of motion information to radiologists and sonographers to facilitate human assessment. However, this work may still remain gaps between real-world clinical utilization due to medical image analysis being a low failure tolerance application. The meaning of this work is to present a new direction for cardiac motion tracking, which is different from the conventional approach. In the current stage, the trained model in public datasets and the results presented are not specific to provide support for clinical use. ", "page_idx": 14}, {"type": "text", "text": "B Additional Quantitative Results and Visualization ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We provide the experiment reported on Hussdorf Distance (HD) for datasets CardiacUDA [17], CAMUS [18] and ACDC [19] in Tables B1, B2 and B3, respectively. Furthermore, the consequent tracking results of 3D echocardiogram and 4D Cardiac MRI are presented in Figures B1 and B2. ", "page_idx": 15}, {"type": "text", "text": "Table B1: The performance of different registration methods in CardiacUDA [17]. Results report in Structures (LV, RV, LA, RV) and overall averaged Dice score (Avg. $\\%$ ). Segmentation results are reported in the Hussdorf Distance (HD). Bold, underline denote the best results and the second-best performance, respectively. ", "page_idx": 15}, {"type": "table", "img_path": "CTIFk7b9jU/tmp/321c259f007c52424a33f26e8006225af2c9fc84b52851a92bcf156a64279f41.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "table", "img_path": "CTIFk7b9jU/tmp/d9dd640c6f1a71af7e0d492a25d8b2fdbf405086683370744d7dd02ea0f30cda.jpg", "table_caption": ["Table B2: The performance of different registration methods in CAMUS [18] dataset. Results report in Structures (LV, RV, Myo) and overall averaged Dice score (Avg. $\\%$ ). Segmentation results are reported in the Hussdorf Distance (HD). Bold, underline denote the best results and the second-best performance, respectively. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "Table B3: The performance of different registration methods in ACDC [19] dataset. Results report in Structures (LV, RV, Myo) and overall averaged Dice score (Avg. $\\%$ ). Segmentation results are reported in the Hussdorf Distance (HD). Bold, underline denote the best results and the second-best performance, respectively. ", "page_idx": 16}, {"type": "table", "img_path": "CTIFk7b9jU/tmp/9dbf309b7db7557da8e2f5cfc8a9e607c0e53a7686cfbeef18330afa9139b9d2.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "CTIFk7b9jU/tmp/b7735a1e9c9ee4a31e5dc5ef686a11d3722b250c20a21d90083682a193eda220.jpg", "img_caption": ["Figure B1: The visualization in 4D Cardiac MRI of estimated motion field and motion tracking results. We visualised the tracking result of the first frame (ED) to the last frame (ES) in ACDC [17]. Colours Red, Blue, and Green denote cardiac structures MYO, LA, and LV, respectively. "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "CTIFk7b9jU/tmp/68157f189cfb8eff0960114726eef8cb60e4ae2eda8883761962241c367beed2.jpg", "img_caption": ["", "Figure B2: The visualization in 3D Echocardiogram video of estimated motion field and motion tracking error. We visualised tracking results from the first frame to the last frame, with ground truth from 8 consecutive frames in CardiacUDA [17]. Colours Red, Blue, Green and Orange denote cardiac structures RA, RV, LV and LA, respectively. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: Main claims are presented in the abstract and introduction. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 18}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Justification: We already discuss the limiations in the last section. ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 18}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: We prove the full set of assumptions and a complete (and correct) proof in our paper. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 19}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: All the experimental results can be reproduced and we promise that all code will be made publicly. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 19}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: All datasets in experiments are public datasets, all code will be made publicly. Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 20}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We have described all the training and testing details. ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 20}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Justification: Error bars, standard deviation and all details of result are presented in paper. ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified. ", "page_idx": 20}, {"type": "text", "text": "\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). \u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 21}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: Please see the implementation details. ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 21}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We promise that all research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics. ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 21}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: Please refer to Appendix A.2. ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. ", "page_idx": 21}, {"type": "text", "text": "\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 22}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: No dataset or generative data. ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 22}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: For all datasets, we have acquired the license and permission for dataset usage. ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 22}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: N/A ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used. ", "page_idx": 22}, {"type": "text", "text": "\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 23}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: Not involve crowdsourcing nor research with human subjects. ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 23}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: Not involve crowdsourcing nor research with human subjects. ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 23}]