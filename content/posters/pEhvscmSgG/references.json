{"references": [{"fullname_first_author": "Sergey Levine", "paper_title": "Offline reinforcement learning: Tutorial, review, and perspectives on open problems", "publication_date": "2020-05-01", "reason": "This paper provides a comprehensive overview of offline reinforcement learning, which is the core topic of the current paper."}, {"fullname_first_author": "Tianhe Yu", "paper_title": "MOPO: model-based offline policy optimization", "publication_date": "2020-05-13", "reason": "This paper is a highly relevant model-based offline reinforcement learning method that is directly compared with in the current paper."}, {"fullname_first_author": "Rahul Kidambi", "paper_title": "MOREL: Model-based offline reinforcement learning", "publication_date": "2020-05-05", "reason": "This model-based offline reinforcement learning method is another key reference point for comparison in the current paper."}, {"fullname_first_author": "Yihao Sun", "paper_title": "Model-Bellman inconsistency for model-based offline reinforcement learning", "publication_date": "2023-00-00", "reason": "This paper addresses the issue of model-based offline RL and is highly relevant to the topic."}, {"fullname_first_author": "Wenxuan Zhou", "paper_title": "PLAS: latent action space for offline reinforcement learning", "publication_date": "2020-11-07", "reason": "This paper introduces a method using a latent action space, which is conceptually similar to the approach proposed in the current paper."}]}