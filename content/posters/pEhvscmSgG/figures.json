[{"figure_path": "pEhvscmSgG/figures/figures_1_1.jpg", "caption": "Figure 1: Overview of C-LAP. (a) The model is trained offline. It is encoding observations Ot and actions at to latent states (gray circle) and latent actions ut (green circle), and decoding them thereafter. Furthermore it is predicting rewards ft. (b) The policy is learned in the latent action space, but constrained to the support of the latent action prior, and uses the generative capabilities of the action decoder. Gradients are computed by back-propagating estimated values V\u2081 and rewards \u00eet through the imagined trajectories. (c) The policy is used in the real world, again using the generative action decoder.", "description": "This figure illustrates the three main steps of the Constrained Latent Action Policies (C-LAP) method.  Panel (a) shows the offline training phase where the model learns to encode observations and actions into latent variables, and then decode these to predict rewards. Panel (b) depicts the policy training phase.  The policy is trained in the latent action space, subject to constraints that keep generated actions within the observed data distribution.  Panel (c) shows how the trained policy is used in the real world, using the learned decoder to generate actions from latent representations.", "section": "3 Constrained Latent Action Policies"}, {"figure_path": "pEhvscmSgG/figures/figures_2_1.jpg", "caption": "Figure 2: Recurrent latent action state-space model. The generative process is shown by solid lines and inference by dashed lines. Stochastic variables are denoted by circles and deterministic variables by rectangles.", "description": "This figure illustrates the recurrent latent action state-space model used in the Constrained Latent Action Policies (C-LAP) method. It shows how observations (Ot), actions (at), latent states (st), and latent actions (ut) are interconnected through stochastic and deterministic processes.  The solid lines represent the generative process (model's forward pass), while the dashed lines represent the inference process (model's backward pass). Circles denote stochastic variables, and rectangles represent deterministic variables.  The model learns a joint distribution of states and actions, rather than a conditional dynamics model p(s|a), which is a key aspect of the C-LAP approach. The figure visually summarizes the core structure of the generative model which is detailed mathematically in the paper. ", "section": "3 Constrained Latent Action Policies"}, {"figure_path": "pEhvscmSgG/figures/figures_4_1.jpg", "caption": "Figure 3: Policy constraint through explicit parametrization by using a linear transformation g of the latent action prior po(ut | st) and the bounded policy \u03c0\u03c5(Ut | St) \u2208 [-1,1]. The generated actions at ~ p(at | St, g(\u03c0\u2084(Ut | St), Po(ut | st)) are implicitly constrained to the data distribution.", "description": "This figure illustrates how the policy is constrained to the data distribution by using a linear transformation. The prior distribution is shown in light blue. The policy distribution in orange is constrained to be within the range defined by the prior, ensuring the generated actions are within the bounds of the training data. The constraint is implemented through a linear transformation of the latent action prior and the bounded policy, as shown in the middle panel. This ensures that the policy is flexible but still respects the implicit constraint imposed by the data distribution. Finally, the fully trained policy distribution is shown in the right-most panel.", "section": "Constrained latent action policy"}, {"figure_path": "pEhvscmSgG/figures/figures_6_1.jpg", "caption": "Figure 4: Evaluation on low-dimensional feature observations using D4RL benchmark datasets. We plot mean and standard deviation of normalized returns over 4 seeds.", "description": "This figure shows the training curves for four different offline reinforcement learning algorithms (C-LAP, MOPO, MOBILE, PLAS) across various D4RL benchmark locomotion tasks (halfcheetah, walker2d, hopper) and navigation tasks (antmaze).  Each task has four datasets representing varying levels of data quality (medium-replay, medium, medium-expert, expert). The y-axis represents the normalized average return, and the x-axis represents the number of gradient steps taken during training.  Error bars show the standard deviation across four different training runs.  The figure demonstrates the performance comparison of C-LAP against state-of-the-art methods on low-dimensional feature observation datasets.", "section": "4.1 Benchmark results"}, {"figure_path": "pEhvscmSgG/figures/figures_7_1.jpg", "caption": "Figure 5: Evaluation on visual observations using V-D4RL benchmark datasets. We plot mean and standard deviation of normalized returns over 4 seeds.", "description": "This figure shows the performance of C-LAP, Offline DV2, and LOMPO on four different V-D4RL benchmark datasets with visual observations.  Each dataset represents a different level of data complexity (replay, medium, medium-expert, expert). The plot displays the mean and standard deviation of normalized returns over four separate runs (seeds), demonstrating the performance and variability of each method across different trials. This helps to illustrate how the methods perform on data with varying levels of complexity.", "section": "4.1 Benchmark results"}, {"figure_path": "pEhvscmSgG/figures/figures_7_2.jpg", "caption": "Figure 6: Ablation study, comparing C-LAP to the following variants: no constraint, C-LAP without enforcing the policy constraint dependent on the action prior; no latent action, C-LAP without a latent action space similar to Dreamer [29]. We plot mean and standard deviation of normalized returns and value estimates over 3 seeds. Moreover we add the dataset's average return and average maximum value estimate indicated by dashed lines.", "description": "This figure presents an ablation study comparing the performance of C-LAP against variations where the policy constraint is removed, and where a latent action space is not used, similar to Dreamer. The results are shown in terms of normalized returns and value estimates, along with dataset average return and maximum value estimate for comparison.  The study helps analyze the impact of the key components of the C-LAP model on its performance in offline reinforcement learning.", "section": "4.2 Value overestimation"}, {"figure_path": "pEhvscmSgG/figures/figures_8_1.jpg", "caption": "Figure 7: Sensitivity analysis of the support constraint parameter \u1ebd for the considered D4RL walker2d datasets. We plot mean and standard deviation of normalized returns over 4 seeds.", "description": "This figure shows the sensitivity analysis of the support constraint parameter (epsilon) on the performance of C-LAP across four walker2d datasets from the D4RL benchmark. The x-axis represents the epochs during training, and the y-axis represents the normalized returns. Different lines show the result for various values of epsilon, from 0.5 to 10.0, illustrating how this hyperparameter impacts the learning process and the model's final performance.  The shaded areas represent standard deviations over 4 seeds.", "section": "4 Experiments"}, {"figure_path": "pEhvscmSgG/figures/figures_17_1.jpg", "caption": "Figure 8: Comparison of the dataset's action distribution to the distribution of actions sampled from the latent action prior and latent action decoder. The figure shows the aggregated distributions (from leaving the ground to touching the ground) for one exemplary trajectory of the hopper-expert-v2 dataset", "description": "This figure compares the distribution of actions from the dataset and the distribution of actions generated by the model's latent action prior and decoder. The comparison is done for one trajectory from the hopper-expert-v2 dataset and shows that the model-generated actions are close to the dataset's actions distribution.", "section": "4 Experiments"}, {"figure_path": "pEhvscmSgG/figures/figures_17_2.jpg", "caption": "Figure 6: Ablation study, comparing C-LAP to the following variants: no constraint, C-LAP without enforcing the policy constraint dependent on the action prior; no latent action, C-LAP without a latent action space similar to Dreamer [29]. We plot mean and standard deviation of normalized returns and value estimates over 3 seeds. Moreover we add the dataset's average return and average maximum value estimate indicated by dashed lines.", "description": "This figure shows an ablation study comparing the performance of C-LAP with three variants: one without the constraint on the policy (allowing it to explore outside the data distribution), one without latent actions (using a simpler state-space model), and the original C-LAP model.  The plots show both the normalized returns and the value estimates over epochs for four different datasets (walker2d-medium-replay-v2, walker2d-medium-v2, walker2d-medium-expert-v2, and walker2d-expert-v2). Dashed lines represent the average return and the average maximum value estimate from the datasets, providing context for evaluating the performance of each model variant.", "section": "4.2 Value overestimation"}]