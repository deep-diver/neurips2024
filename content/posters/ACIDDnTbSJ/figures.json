[{"figure_path": "ACIDDnTbSJ/figures/figures_3_1.jpg", "caption": "Figure 1: An example of Palindrome-directed Generation Templates of Feint behaviors. The first row shows an action sequence of a cross-punch behavior. Three examples of templates are shown as 1, 2, and 3 to demonstrate physically realistic generation of Feint behaviors.", "description": "This figure demonstrates how to generate Feint behavior templates using the Palindrome-directed Generation of Feint Templates method.  It shows a three-stage decomposition of an attack behavior (cross-punch) into Stretch-out, Reward, and Retract sequences.  The key idea is that Sequence 1 (Stretch-out) and Sequence 3 (Retract) have semi-symmetric, reverse-order action patterns.  The figure illustrates three ways to generate Feint behavior templates by extracting and combining sections of these sequences, emphasizing the semi-symmetric nature and ensuring physically realistic movements.", "section": "3 Formalizing Feint Behavior"}, {"figure_path": "ACIDDnTbSJ/figures/figures_3_2.jpg", "caption": "Figure 2: Dual-action Model - high-level abstraction and demonstration of internal stage transitions", "description": "This figure illustrates the Dual-Behavior Model, showing how Feint behaviors and follow-up attack actions are combined to achieve temporal and spatial advantages.  The timeline shows the agent executing a Feint (stretch-out, retract) which deceives the opponent into a full defense. Then the agent follows up with a high-reward attack behavior (stretch-out, reward, retract) taking advantage of the opponent's temporary vulnerability and spatial misdirection. The key aspects highlighted are temporal advantage (opponent unable to react), spatial advantage (opponent's defense is ineffective against the follow-up attack), and the resulting reward gain for the agent.", "section": "3.2 Feint Behavior in Consecutive Game Steps"}, {"figure_path": "ACIDDnTbSJ/figures/figures_7_1.jpg", "caption": "Figure 3: Illustration of Feint behavior implementation in game iterations", "description": "This figure illustrates the implementation of Feint behaviors in game iterations.  It shows how an imaginary play module decides whether to initiate a Feint behavior, generating a Dual-Behavior Model using Palindrome-directed templates. The Feint and regular reward evaluations help to determine whether the Feint action sequence is effective. The figure also depicts the interaction with the environment, accumulation of rewards, and policy model updates, all within the framework of using both Feint and regular policies.", "section": "5 Proof-of-concept Implementation"}, {"figure_path": "ACIDDnTbSJ/figures/figures_9_1.jpg", "caption": "Figure 4: Comparison of Game Reward when using Feint and not using Feint in a 1 VS 1 scenario.", "description": "This figure compares the game rewards of using Feint behaviors versus not using them in a one-on-one scenario, using four different multi-agent reinforcement learning (MARL) models: MADDPG, MAD3PG, MASAC, and MATD3.  The results show the game rewards for each model, with and without the inclusion of Feint behaviors, across a significant number of training episodes.  The 'Good 1' player is using the Feint strategy while the 'Adv 1' is not. The purpose is to demonstrate that the use of Feint behaviors leads to improved rewards for the player utilizing them.", "section": "6.2 Major Results"}, {"figure_path": "ACIDDnTbSJ/figures/figures_9_2.jpg", "caption": "Figure 5: Comparison of Game Reward when using Feint and not using Feint in a 3 VS 3 scenario.", "description": "This figure compares the game rewards obtained when using Feint behaviors versus not using them in a 3 vs 3 multi-player game scenario.  The results are shown for four different multi-agent reinforcement learning (MARL) models (MADDPG, MAD3PG, MASAC, and MATD3).  Each model's performance is represented with two lines: one for the baseline (no Feint) and one incorporating Feint behaviors. The comparison allows assessment of the impact of Feint on reward gains across different MARL algorithms in a complex multi-agent environment.", "section": "6.2 Major Results"}, {"figure_path": "ACIDDnTbSJ/figures/figures_15_1.jpg", "caption": "Figure 6: Dual-action Model - snapshots of the full process", "description": "This figure shows a successful Feint behavior in a Dual-Behavior Model. The white agent (on the left) performs a fake punch (Feint) towards the opponent's head, causing the opponent to defend their head.  Simultaneously, the agent performs a hook towards the opponent's waist. Because the Feint created a temporal advantage (by deceiving the opponent) and a spatial advantage (by attacking a different area), the opponent is unable to defend the waist attack and is knocked down. The figure illustrates the action sequence of both the agent and opponent.", "section": "3.2 Feint Behavior in Consecutive Game Steps"}, {"figure_path": "ACIDDnTbSJ/figures/figures_16_1.jpg", "caption": "Figure 7: Demonstration of unsuccessful Feint behavior when its too short", "description": "This figure shows an example of unsuccessful Feint behavior. The key characteristic of this type of Feint is that the Feint action is too short. The time for the Feint behavior to end (tB1) is earlier than the time for the following attack to start gaining rewards (tA2).  As a result, the opponent is still in the defensive posture when the attack begins, reducing the effectiveness of the Feint. ", "section": "C.2 Demonstration of Successful and Unsuccessful Feint Behaviors"}, {"figure_path": "ACIDDnTbSJ/figures/figures_17_1.jpg", "caption": "Figure 8: Demonstration of successful Feint behavior with proper length", "description": "This figure demonstrates a successful Feint behavior in a Dual-Behavior Model. The key is that the Feint behavior (fake punch towards the opponent's head) is performed with a proper length, which creates both temporal and spatial advantages.  The temporal advantage comes from the opponent starting their defense reaction after the Feint's completion. The spatial advantage is gained because the opponent is mislead to defend the head while the attack is launched to the waist. This leads to a knockout of the opponent.", "section": "3.2 Feint Behavior in Consecutive Game Steps"}, {"figure_path": "ACIDDnTbSJ/figures/figures_17_2.jpg", "caption": "Figure 9: Demonstration of unsuccessful Feint behavior when its too long", "description": "This figure shows an example of unsuccessful Feint behavior due to its long duration. The timeline is divided into three key time points: tB1 (end of NPC B\u2019s first defense behavior), tA2 (estimated start of NPC A\u2019s second behavior\u2019s reward), and tB2 (estimated start of NPC B\u2019s second behavior\u2019s reward). In this case, tA2 > tB2 which means NPC B has already started its second behavior before NPC A\u2019s second behavior even starts. Therefore, NPC B\u2019s real attack interrupts NPC A\u2019s attack which leads to NPC A\u2019s unsuccessful Feint behavior.", "section": "C.2 Demonstration of Successful and Unsuccessful Feint Behaviors"}, {"figure_path": "ACIDDnTbSJ/figures/figures_18_1.jpg", "caption": "Figure 10: The full set of 22 behavior (action sequences) of a boxing game from Mixamo.", "description": "This figure shows the full set of 22 behavior (action sequences) used in the boxing game simulation. The behaviors are categorized into four types: offensive, defensive, reaction, and transition.  Each category contains several different action sequences, providing a rich set of actions for the agents in the game.", "section": "D.1 Details of Boxing Game Scenario"}, {"figure_path": "ACIDDnTbSJ/figures/figures_19_1.jpg", "caption": "Figure 11: Demonstration of the game rewards and action sequence lengths of 5 Mixamo behaviors.", "description": "This figure shows five example behaviors from the Mixamo dataset used in the paper's boxing game simulation.  Each behavior is depicted visually, along with its associated game reward and the number of action sequences it comprises.  This illustrates the variety of behaviors and their associated reward values used in the training and evaluation of the Feint strategy.", "section": "D.1 Details of Boxing Game Scenario"}, {"figure_path": "ACIDDnTbSJ/figures/figures_21_1.jpg", "caption": "Figure 12: Diversity gain for agents, in terms of the exploitablity and the negative population efficacy.", "description": "This figure shows the results of a comparative study between MARL training with and without Feint in AlphaStar games.  Exploitability measures how far a joint policy is from Nash Equilibrium, indicating diversity gains. Population Efficacy (PE) measures diversity of the whole policy space.  The results show that agents using Feint achieve lower Exploitability and higher PE (less negative), indicating increased diversity and effectiveness of the policy space.", "section": "F.1 Diversity Gain"}, {"figure_path": "ACIDDnTbSJ/figures/figures_22_1.jpg", "caption": "Figure 13: Overhead of Feint in the 1 VS 1 and 3 VS 3 scenarios using 4 MARL models.", "description": "This figure shows the overhead incurred by incorporating Feint into four different multi-agent reinforcement learning (MARL) models across two scenarios: 1 vs 1 and 3 vs 3.  The overhead is presented as a percentage increase in computation time. The figure illustrates that while there is some overhead in all cases, it generally remains under 5%, suggesting that the addition of Feint behaviors does not significantly impact computational efficiency.", "section": "F.2 Overhead Analysis"}]