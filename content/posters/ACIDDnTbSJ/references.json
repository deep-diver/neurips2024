{"references": [{"fullname_first_author": "Johannes Ackermann", "paper_title": "Johannesack/tf2multiagentrl", "publication_date": "2020-11-01", "reason": "This repository provides a unified framework for multi-agent reinforcement learning, which is fundamental to the paper's experimental setup and comparison of different MARL models."}, {"fullname_first_author": "Ryan Lowe", "paper_title": "Multi-agent actor-critic for mixed cooperative-competitive environments", "publication_date": "2017-12-04", "reason": "The MADDPG algorithm, introduced in this paper, is one of the primary MARL algorithms used in the paper's experiments, forming a crucial baseline for evaluating the effectiveness of the proposed Feint behavior integration."}, {"fullname_first_author": "Tuomas Haarnoja", "paper_title": "Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor", "publication_date": "2018-07-10", "reason": "The SAC algorithm is another key MARL algorithm used as a comparison baseline in the paper, providing insights into the performance of different MARL methods with and without Feint behaviors."}, {"fullname_first_author": "Xiangyu Liu", "paper_title": "Unifying behavioral and response diversity for open-ended learning in zero-sum games", "publication_date": "2021-06-01", "reason": "This paper introduces the concept of Behavior Diversity, which is directly adopted and extended in the current work to quantify the impact of Feint behaviors on the diversity of game strategies."}, {"fullname_first_author": "Jakob N. Foerster", "paper_title": "Counterfactual multi-agent policy gradients", "publication_date": "2018-02-02", "reason": "The COMA algorithm, presented in this paper, is another important MARL algorithm that is used in the paper's experimental evaluation, further enriching the comparative analysis and providing a broader perspective on the effectiveness of Feint integration."}]}