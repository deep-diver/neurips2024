{"references": [{"fullname_first_author": "Long Ouyang", "paper_title": "Training language models to follow instructions with human feedback", "publication_date": "2022-12-01", "reason": "This paper introduces the instruction tuning method, a crucial technique for aligning LLMs with human preferences, which is directly relevant to the proposed LISA method."}, {"fullname_first_author": "Colin Raffel", "paper_title": "Exploring the limits of transfer learning with a unified text-to-text transformer", "publication_date": "2020-07-01", "reason": "This paper introduces the text-to-text transformer framework, a foundational model architecture upon which many large language models are built, and which is thus highly relevant to the current paper."}, {"fullname_first_author": "Aakanksha Chowdhery", "paper_title": "Palm: Scaling language modeling with pathways", "publication_date": "2022-07-01", "reason": "This paper details the Pathways Language Model (PaLM), a large-scale language model that pushed the boundaries of model size and capabilities, which is a significant advancement informing the memory efficiency techniques investigated in the current paper."}, {"fullname_first_author": "Baptiste Rozi\u00e8re", "paper_title": "Code llama: Open foundation models for code", "publication_date": "2023-01-01", "reason": "This paper introduces Code Llama, a large language model specifically trained for code generation. The availability of Code Llama provides a powerful tool for validating the memory efficiency and performance gains of LISA on specific downstream tasks."}, {"fullname_first_author": "Hugo Touvron", "paper_title": "Llama: Open and efficient foundation language models", "publication_date": "2023-02-01", "reason": "This paper introduces the LLaMA model, a family of large language models, that is used as a case study in this paper to demonstrate the effectiveness of the proposed LISA method."}]}