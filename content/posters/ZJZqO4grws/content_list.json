[{"type": "text", "text": "Learning to Learn with Contrastive Meta-Objective ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 We propose a contrastive meta-objective to enable meta-learners to emulate human  \n2 like rapid learning capability through enhanced alignment and discrimination. Our   \n3 proposed approach, dubbed ConML, exploits task identity as additional supervision   \n4 signal for meta-training, benefiting meta-learner\u2019s fast-adaptation and task-level   \n5 generalization abilities. This is achieved by contrasting the outputs of meta-learner,   \n6 i.e, performing contrastive learning in the model space. Specifically, we introduce   \n7 metrics to minimize the inner-task distance, i.e., the distance among models learned   \n8 on varying data subsets of the same task, while maximizing the inter-task distance   \n9 among models derived from distinct tasks. ConML distinguishes itself through   \n10 versatility and efficiency, seamlessly integrating with episodic meta-training meth  \n11 ods and the in-context learning of large language models (LLMs). We apply   \n12 ConML to representative meta-learning algorithms spanning optimization-, metric-,   \n13 and amortization-based approaches, and show that ConML can universally and   \n14 significantly improve conventional meta-learning and in-context learning. ", "page_idx": 0}, {"type": "text", "text": "15 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "16 Meta-learning [37, 42], or learning to learn, is a powerful paradigm that aims to enable a learning   \n17 system to quickly adapt to new tasks. Meta-learning has been widely applied in different fields, like   \n18 few-shot learning [17, 50], reinforcement learning [56, 26] and neural architecture search [16, 38]. In   \n19 meta-training, a meta-leaner mimics the learning processes on many relevant tasks to gain experience   \n20 about how to make adaptation. In meta-testing, the meta-trained adaptation process is performed   \n21 on unseen tasks. The adaptation process is achieved by generating task-specific model by the meta  \n22 learner, which is given a set of training examples and returns a predictive model. People prefer   \n23 meta-learning to equip models with human\u2019s fast learning ability, so that a good model can be   \n24 achieved with a few examples [50].   \n25 The combination of two cognitive capabilities, namely, alignment and discrimination, is essential   \n26 for human\u2019s fast learning ability [23, 12, 13]. A good learner possesses the alignment [27] ability to   \n27 align different partial views of a certain object, which means they can integrate various aspects or   \n28 perspectives of information to form a coherent understanding. On the other hand, discrimination [34]   \n29 refers to the learner\u2019s capacity to distinguish between one stimulus and similar stimuli, responding   \n30 appropriately only to the correct stimuli. This is a fundamental ability that allows learners to   \n31 differentiate between what is relevant and what is not, ensuring that their responses are accurate   \n32 and based on the correct understanding of the stimuli presented. With alignment and discrimination,   \n33 learners can synthesize fragmented information to construct a complete picture of an object or   \n34 concept, while also being able to discern subtle differences between distinct but similar objects   \n35 or ideas. Such learners are not only efficient in processing information but also in applying their   \n36 knowledge accurately in varied contexts. This dual capability is crucial for effective learning.   \n37 We expect meta-learners to emulate the above combination of alignment and discrimination capa  \n38 bilities to approach human\u2019s fast learning ability. By equipping a meta-learner with the ability to   \n39 align, we enable it to capture the core essence of a task and being invariant to noises. Meanwhile,   \n40 discrimination ensures that a meta-learner can learn specific models for unique tasks, as it is a natural   \n4 supposition that different tasks enjoy distinguishable models. This reflects the natural diversity of   \n42 problems we encounter in the real world and the varied strategies we employ to solve them. Together,   \n43 alignment and discrimination empower a meta-learner to not only grasp the subtleties of individual   \n44 tasks but also to generalize its learning across a spectrum of challenges. This dual capability can   \n45 makes a meta-learner robust, versatile, and more aligned with the nuanced nature of human learning   \n46 and reasoning. However, existing meta-learning approaches conventionally follows the idea of \"train   \n47 as you test\", to minimize the validation loss [46] of meta-training tasks as meta-objective, where   \n48 supervision signal are directly produced by sample labels. To provide stronger supervision, there   \n49 are works assuming that the task-specific target models of meta-training tasks are available, then   \n50 the meta-training can be supervised by aligning the learned model and the corresponding target   \n51 model, with model weights [51, 52] or knowledge distillation [55]. However, as the target models are   \n52 expensive to learn, and even not available in many real world problems, meta-objectives requiring the   \n53 target models have very restricted applications. Moreover, the importance of discrimination ability of   \n54 meta-learner has not been noticed in the literature.   \n55 To achieve this, we propose contrastive meta-learning (ConML), by directly contrasting the outputs   \n56 of meta-learner in the model space, shown in Figure 1. Conventional contrastive learning (CL) [14,   \n57 48, 44] learns an encoder in unsupervised manner by equipping the model with alignment and   \n58 discrimination ability by exploiting the distinguishable identity of unlabeled samples. Considering   \n59 tasks in meta-learning are also unlabeled but have distinguishable identity, we are inspired to adopt   \n60 similar strategy in meta-learning. ConML exploits tasks as CL exploits unlabeled samples. Positive   \n61 pairs in ConML are different subsets of the same task, while negative pairs are datasets of different   \n62 tasks. In the model space output by meta-learner, inner-task distance can be measured between   \n63 positive pairs and inter-task distance can be measured between negative pairs. The contrastive   \n64 meta-objective is minimizing inner-task distance while maximizing inter-task distance, corresponding   \n65 to the expected alignment and discrimination ability respectively. The proposed ConML is universal   \n66 and cheap, as it can be plugged-in any meta-learning algorithms following the episodic training,   \n67 and does not require additional data nor model training. In this paper, we widely study ConML on   \n68 representative meta-learning algorithms from different categories: optimization-based (e.g., MAML   \n69 [17]), metric-based (e.g., ProtoNet [39]), amortization-based (e.g., Simple CNAPS [6]). We also   \n70 investigate in-context learning [8] with reformulating it into the meta-learning paradigm, and show   \n71 how ConML integrates and helps. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "image", "img_path": "ZJZqO4grws/tmp/6a01fb5ae0ecfb41f0b5e6cc5f3e6b6ac1c4637cf4da3823eded8126606b6c4e.jpg", "img_caption": ["Figure 1: ConML is performing contrastive learning in model space, where alignment and discrimination encourage the meta-learner\u2019s fast-adaptation and task-level generalize ability respectively. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "72 Our contributions are: ", "page_idx": 1}, {"type": "text", "text": "73 \u2022 We propose to emulate cognitive alignment and discrimination capabilities in meta-learning, to   \n74 narrow down the gap of fast learning ability between meta-learners and humans.   \n75 \u2022 We generalize contrastive learning from representation space of unsupervised learning to model   \n76 space of meta-learning. The exploiting task identity as additional supervision beneftis meta-learner\u2019s   \n77 fast-adaptation and task-level generalize abilities.   \n78 \u2022 ConML is algorithm-agnostic, that can be incorporated into any meta-learning algorithms with   \n79 episodic training. We empirically show ConML can bring universal improvement with cheap   \n80 implementation on a wide range of meta-learning algorithms and in-context learning. ", "page_idx": 1}, {"type": "text", "text": "81 2 Related Works ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "82 2.1 Learning to Learn ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "83 Meta-learning learns to improve the learning algorithm itself [37], i.e., learns to learn. Popular   \n84 meta-learning approaches can be roughly divided into three categories [7]: optimization-based,   \n85 metric-based and amortization-based. Optimization-based approaches [4, 17, 28] focus on learning   \n86 better optimization strategies for adapting to new tasks. For example MAML [17] learns initial   \n87 model parameters, where few steps of gradient descent can quickly make adaptaion for specific   \n88 tasks. Metric-based approaches [46, 39, 41] leverages learned similarity metrics. For example,   \n89 Prototypical Networks [39] and Matching Networks [46] learn global shared encoders to map training   \n90 set to embeddings, based on which task-specific model can be built. Amortization-based approaches   \n91 [19, 33, 6] seek to learn a shared representation across tasks. They amortize the adaptation process   \n92 by using neural networks to directly infer task-specific parameters from training set. Examples are   \n93 CNPs [19] and CNAPs [33].   \n94 In-context learning (ICL) [8] is designed for large language models, which integrates examples   \n95 (input-output pairs) in a task and a query input into the prompt, thus the language model can answer   \n96 the query. Recently, ICL has been studied as a general approach of learning to learn [2, 18, 47, 1],   \n97 which reduces meta-learning to conventional supervised learning via training a sequence model. It   \n98 considers training set as context to be provided along with the input to predict, forming a sequence to   \n99 feed the model. Training such a model can be viewed as an instance of meta-learning [18]. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "100 2.2 Contrastive Learning ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "101 Contrastive learning is a powerful technique in representation learning [29, 10, 48]. Its primary goal   \n102 is to learn useful representations, which are invariant to unnecessary details, and preserve as much   \n103 information as possible. This is achieved by maximizing alignment and discrimination (uniformity)   \n104 in representation space [48]. In conventional contrastive learning, alignment refers to bringing   \n105 positive pairs (e.g., augmentations of the same sample [54, 22, 5, 21, 10]) closer together in the   \n106 learned representation space. By maximizing alignment, the representations are encouraged to be   \n107 invariant to unneeded noise factors. Discrimination refers to separating negative pairs (e.g., different   \n108 samples) farther. Maximizing discrimination without any other knowledge results in uniformity, i.e.,   \n109 uniform distribution in the representation space. By maximizing discrimination, the representations   \n110 are encouraged to preserve as much information of the data as possible [43, 5], benefiting the   \n111 generalization ability. ", "page_idx": 2}, {"type": "text", "text": "112 3 Meta-Learning with Contrastive Meta-Objective ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "113 Meta-learning is a methodology considered with \"learning to learn\" machine learning algorithms.   \n114 Define $\\mathcal{L}(\\mathcal{D};h)$ as the loss obtained by evaluating model $h$ on dataset $\\mathcal{D}$ with function $\\ell(y,\\hat{y})$ (e.g.,   \n115 cross entropy or mean squared loss), $g(;\\theta)$ is a meta-learner that maps a dataset $\\mathcal{D}$ to a model $h$ ,   \n116 i.e, $h=g(\\mathcal{D};\\theta)$ . Given a distribution of tasks $p(\\tau)$ , where each task $\\tau$ consists of a training set   \n117 $\\mathcal{D}_{\\tau}^{\\mathrm{tr}}=\\{(x_{\\tau,i},y_{\\tau,i})\\}_{i=1}^{n}$ , and a validation set $\\mathcal{D}_{\\tau}^{\\mathrm{val}}=\\{(x_{\\tau,i},y_{\\tau,i})\\}_{i=n+1}^{m}$ , the goal of meta-learning is   \n118 to learn $g(;\\theta)$ to perform well on new task $\\tau^{\\prime}$ sampled from $p(\\tau^{\\prime})$ , evaluated by $\\mathcal{L}(\\mathcal{D}_{\\tau^{\\prime}}^{\\mathrm{val}};g(\\mathcal{D}_{\\tau^{\\prime}}^{\\mathrm{tr}};\\theta))$ . ", "page_idx": 2}, {"type": "text", "text": "119 3.1 A Unified View of Episodic Training ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "120 We aim to introduce \"learning to align and discriminate\" to universally improve the meta-learning process. The most conventional way of meta-training is taking the validation loss as meta-objective to optimize $\\theta$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\theta}\\mathbb{E}_{\\tau\\sim p(\\tau)}\\mathcal{L}(\\mathcal{D}_{\\tau}^{\\mathrm{val}};g(\\mathcal{D}_{\\tau}^{\\mathrm{tr}};\\theta)).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Different meta-learning algorithms tailor the function inside $g$ , while sharing the same episodic meta-training to achieve (1). Shown as Algorithm 1, in each episode, $B$ tasks are sampled from $p(\\tau)$ to form a batch $\\mathbf{b}$ , and validation loss of each task is aggregated as the supervision signal $\\begin{array}{r}{L_{v}\\,=\\,\\frac{1}{B_{-}}\\sum_{\\tau\\in b}\\mathcal{L}\\big(\\mathcal{D}_{\\tau}^{\\mathrm{val.}};\\check{g}(\\mathcal{D}_{\\tau}^{\\mathrm{tr}};\\theta)\\big)}\\end{array}$ t thom u p1 dcaaten $\\theta$ .e nBeyra slipzeec itfhye$g$ meta-training process of different meta-learning algorithms. ", "page_idx": 2}, {"type": "table", "img_path": "ZJZqO4grws/tmp/fd649bc1d675055e25f621b888fc714e4235f83a715710b9ef8300a698d36e45.jpg", "table_caption": [], "table_footnote": [], "page_idx": 2}, {"type": "table", "img_path": "", "table_caption": ["Table 1: Specifications of ConML "], "table_footnote": [], "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\! \n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "121 Specifications of optimization-based, metric-based and amortization-based algorithms are summa  \n122 rized in Table 1.   \n123 We design ConML to be integrated with Algorithm 1 without specifying $g$ , thus to be universally   \n124 applicable for meta-learning algorithms following the episodic manner. In Section 3.2, we introduce   \n125 how to measure the objective. Then in Section 3.3, we introduce specifications of ConML on a wide   \n126 range of meta-learning algorithms. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "127 3.2 Integration with Episodic Meta-Training ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "128 To equip meta-learners with the desired alignment and discrimination ability, we design contrastive   \n129 meta-objective measured in the output space of meta-learner, i.e., the model space of $h$ . Alignment   \n130 is achieved by minimizing inner-task distance, which is the distance among models generated from   \n131 different subsets of the same task. Discrimination is achieved by maximize the inter-task distance,   \n132 which is the distance among models generated from different tasks. Here we introduce how to   \n133 measure the contrastive objective and perform optimization.   \n134 Obtaining Model Representation. To train the meta-learner $g$ , the distances $D^{\\mathrm{in}}$ , $D^{\\mathrm{out}}$ are mea  \n135 sured in the output space of $g$ , i.e., the model space $\\mathcal{H}$ . A feasible way is to first represent model   \n136 $h=g(\\mathcal{D};\\theta)\\in\\bar{\\mathcal{H}}$ as fixed length vectors $\\boldsymbol{e}\\in\\mathbb{R}^{\\hat{d}}$ , then measure by explicit distance function $\\phi(\\cdot,\\cdot)$   \n137 (e.g., cosine distance). Note that $\\mathcal{H}$ is algorithm-specific. Here we only introduce a projection   \n138 $\\boldsymbol{\\psi}\\,:\\,\\boldsymbol{\\mathcal{H}}\\,\\rightarrow\\,\\mathbb{R}^{d}$ to obtain model representations $\\boldsymbol{e}\\,=\\,\\psi(h)$ . The $\\mathcal{H}$ and $\\psi$ will be elucidated and   \n139 specified for different meta-learning algorithms in Section 3.3.   \n140 Obtaining Inner-Task Distance. During meta-training, $\\mathcal{D}_{\\tau}^{\\mathrm{tr}}\\cup\\mathcal{D}_{\\tau}^{\\mathrm{val}}$ contains all the available in  \n141 formation about task $\\tau$ . The meta-learner is expected to learn similar model given any subset $\\kappa$ of   \n142 the task. Meanwhile those models from subsets are expected to be similar to the model learned   \n143 from the full supervision $\\mathcal{D}_{\\tau}^{\\mathrm{tr}}\\cup\\mathcal{D}_{\\tau}^{\\mathrm{val}}$ . We design the following inner-task distance to minimize that   \n144 encourages $g$ to learn a generalizable model even from a set containing only few or biased samples.   \n145 For $\\forall\\kappa\\subseteq\\mathcal{D}_{\\tau}^{\\mathrm{tr}}\\cup\\mathcal{D}_{\\tau}^{\\mathrm{val}}$ , we expect $e_{\\tau}^{\\kappa}=e_{\\tau}^{*}$ , where $\\pmb{e}_{\\tau}^{\\kappa}=\\psi(g(\\kappa;\\theta)),\\pmb{e}_{\\tau}^{*}=\\psi(g(\\mathcal{D}_{\\tau}^{\\mathrm{tr}}\\cup\\mathcal{D}_{\\tau}^{\\mathrm{val}};\\theta))$ . The   \n146 inner-task distance $D_{\\tau}^{\\mathrm{in}}$ of task $\\tau$ is defined as: ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\nD_{\\tau}^{\\mathrm{in}}=\\frac{1}{K}\\sum_{k=1}^{K}\\phi(e_{\\tau}^{\\kappa_{k}},e_{\\tau}^{*}),\\;s.t.,e_{\\tau}^{\\kappa_{k}}\\sim\\pi_{\\kappa}(\\mathcal{D}_{\\tau}^{\\mathrm{tr}}\\cup\\mathcal{D}_{\\tau}^{\\mathrm{val}}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "147 where $\\{\\kappa_{k}\\}_{k=1}^{K}$ are $K$ subsets sampled from $\\mathcal{D}_{\\tau}^{\\mathrm{tr}}\\cup\\mathcal{D}_{\\tau}^{\\mathrm{val}}$ by certain sampling strategy $\\pi_{\\kappa}$ . In each   \n148 episode given a batch of task b containing $B$ tasks, inner-task distance is averaged by $D^{\\mathrm{in}}\\,=$   \n149 B \u03c4\u2208b Di\u03c4n .   \n150 Obtaining Inter-Task Distance. Since the goal of meta-learning is improving the performance on   \n151 unseen tasks, it is important that the $g$ is generalizable for diverse tasks. With a natural supposition   \n152 that different tasks enjoy different task-specific models, it is necessary that $g$ can learn different   \n153 models from different tasks, i.e., discrimination. We define the following inter-task distance to   \n154 maximize to improve the task-level generalizability of $g$ . For two tasks $\\tau\\neq\\tau^{\\prime}$ during meta-training,   \n155 we expect to maximize the distance between $e_{\\tau}^{*}$ and $e_{\\tau^{\\prime}}^{*}$ . To be practical under the mini-batch episodic   \n156 training paradigm, we consider to measure inter-task distance among a batch of tasks: ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\nD^{\\mathrm{out}}=\\frac{1}{B(B-1)}\\sum_{\\tau\\in b}\\sum_{\\tau^{\\prime}\\in b\\setminus\\tau}\\phi(e_{\\tau}^{*},e_{\\tau^{\\prime}}^{*}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Training Procedure. ConML mea157 sures $D^{\\mathrm{in}}$ by (2) and $D^{\\mathrm{out}}$ by (3) in each episode, and minimizes a combination of the validation loss $L_{v}$ and contrastive meta-objective $D^{\\mathrm{in}}-D^{\\mathrm{out}}$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\nL=L_{v}+\\lambda(D^{\\mathrm{in}}-D^{\\mathrm{out}}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The training procedure of ConML is provided in Algorithm 2. Comparing with Algorithm 1, ConML introduces additional computation $\\psi(g(\\mathcal{D};\\theta))$ for $K\\!+\\!1$ times in each episode. Note that we implement $\\psi$ with very cheap function such as obtaining model weights (or a single probing, i.e., feeding-forward, for ICL), and $g(\\mathcal{D};\\theta)$ already exists in Algorithm 1 while multiple $g(\\mathcal{D};\\theta)$ can be parallel-computed. ConML could have very comparable time consumption. ", "page_idx": 4}, {"type": "text", "text": "Algorithm 2 Meta-Learning with Contrastive Meta-Object (ConML) ", "page_idx": 4}, {"type": "text", "text": "while Not converged do Sample a batch of tasks $\\pmb{b}\\sim p^{B}(\\tau)$ . for All $\\tau\\in b$ do for $k=1,2,\\cdots\\,,K\\,\\epsilon$ do Sample $\\kappa_{k}$ from $\\pi_{\\kappa}(\\mathcal{D}_{\\tau}^{\\mathrm{tr}}\\cup\\mathcal{D}_{\\tau}^{\\mathrm{val}})$ ; Get model representation $e_{\\tau}^{\\kappa_{k}}=\\psi(g(\\kappa_{k};\\theta))$ ; end for Get model representation $e_{\\tau}^{*}=\\psi(g(D_{\\tau}^{\\mathrm{tr}}\\cup D_{\\tau}^{\\mathrm{val}};\\theta))$ ; Get inner-task distance $D_{\\tau}^{\\mathrm{in}}$ by (2); Get task-specific model $h_{\\tau}=g(D_{\\tau}^{\\mathrm{tr}};\\theta)$ ; Get validation loss $\\mathcal{L}(\\mathcal{D}_{\\tau}^{\\mathrm{val}};h_{\\tau})$ ; end for Get $\\begin{array}{r}{D^{\\mathrm{in}}=\\frac{1}{B}\\sum_{\\tau\\in b}D_{\\tau}^{\\mathrm{in}}}\\end{array}$ and $D^{\\mathrm{out}}$ by (3); Get loss $L$ by (4); Update $\\theta$ by $\\theta\\leftarrow\\theta-\\nabla_{\\theta}L$ .   \nend while ", "page_idx": 4}, {"type": "text", "text": "158 3.3 Instantiations of ConML ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "159 Here we demonstrate specifications of $\\mathcal{H}$ and $\\psi(g(\\mathcal{D},\\theta))$ to obtain model representation to implement   \n160 ConML. We show examples on representative meta-learning algorithms from different categories:   \n161 optimization-based, metric-based and amortization-based. They are explicitly represented by model   \n162 weights, summarized in Table 1.   \n163 With Optimization-Based Methods. The representative algorithm of optimization-based meta  \n164 learning is MAML. It meta-learns an initialization from where gradient steps are taken to learn   \n165 task-specific models, i.e., $g(\\mathcal{D};\\theta)=h_{\\theta-\\nabla_{\\theta}\\mathcal{L}(\\mathcal{D};h_{\\theta})}$ . As $g$ directly generates the model weights, we   \n166 explicitly take the model weights as model representation. The representation of model learned   \n167 by $g$ given a dataset $\\mathcal{D}$ is $\\psi(g(\\mathcal{D};\\theta))=\\theta-\\nabla_{\\theta}\\mathcal{L}(\\mathcal{D};h_{\\theta})$ . Note that there are optimization-based   \n168 meta-learning algorithms which are based on first-order approximation of MAML, thus they do not   \n169 strictly follows Algorithm 1 to minimize validation loss (e.g., FOMAML [17] and Reptile [28]).   \n170 ConML can also be incorporated as long as it follows the episodic manner.   \n171 With Metric-Based Methods. Metric-based algorithms are feasible for classification tasks. Given   \n172 dataset $\\mathcal{D}$ of a $N$ -way classification task, metric-based algorithms can be summarized as classifying   \n173 according to distances with $\\{\\{f_{\\theta}(x_{i})\\}_{x_{i}\\in\\mathcal{D}_{j}}\\}_{j=1}^{N}$ and corresponding labels, where $f_{\\theta}$ is a meta  \n174 learned encoder and $\\mathcal{D}_{j}$ is the set of inputs belongs to class $j$ . We design to represent this metric  \n175 based classifier with the concatenation of mean embedding of each class in label-aware order. For   \n176 example, ProtoNet [39] computes the prototype $c_{j}$ , i.e., mean embedding of samples in each class.   \n177 $\\begin{array}{r}{\\pmb{c}_{j}\\,=\\,\\frac{1}{|\\mathcal{D}_{j}|}\\sum_{(\\boldsymbol{x}_{i},y_{i})\\in\\mathcal{D}_{j}}f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i})}\\end{array}$ . Then classifier $h_{\\theta,\\mathcal{D}}$ is built by giving prediction $p(y=j\\mid x)=$   \n178 $\\begin{array}{r}{\\exp(-d(f_{\\theta}(x),\\pmb{c}_{j}))/\\sum_{j^{\\prime}}\\exp(-d(f_{\\theta}(x),\\pmb{c}_{j^{\\prime}}))}\\end{array}$ . As the outcome model $h_{\\theta,\\mathcal{D}}$ depends on $\\mathcal{D}$ through   \n179 $\\{c_{j}\\}_{j=1}^{N}$ and corresponding labels, the representation is specified as $\\psi(g({\\mathcal{D}};\\theta))=[c_{1}|c_{2}|\\cdot\\cdot\\cdot|c_{N}]$ ,   \n180 where $[\\cdot|\\cdot]$ means concatenation.   \n181 With Amortization-Based Methods. Amortization-based approaches meta-learns a hypernetwork   \n182 $H_{\\theta}$ , which aggregates information from $\\mathcal{D}$ to task-specific parameter $\\alpha$ and serves as weights of   \n183 main-network $h$ , resulting in task-specific model $h_{\\alpha}$ . For example, Simple CNAPS [6] adopts the   \n184 hypernetwork to generate only a small amount of task-specific parameter, which performs feature-wise   \n185 linear modulation (FiLM) on convolution channels of the main-network. For contrasting we represent   \n186 $h_{\\alpha}$ by $\\alpha$ , i.e., the output of hypernetwork $H_{\\theta}$ : $\\psi(g(\\mathcal{D};\\theta))=H_{\\theta}(\\mathcal{D})$ . The detailed procedures of   \n187 different meta-learning algorithms with ConML are provided in Appendix A. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "188 4 In-Context Learning with Contrastive Meta-Objective ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "189 In-context learning (ICL) is first proposed for large language models [8], where examples in a task   \n190 are integrated into the prompt (input-output pairs) and given a new query input, the language model   \n191 can generate the corresponding output. This approach allows pre-trained model to address new tasks   \n192 without fine-tuning the model. For example, given \"happy->positive; sad- $>$ negative; blue- $\\cdot>\"$ , the   \n193 model can output \"negative\", while given \"green->cool; yellow- $>$ warm; blue- $>$ \" the model can   \n194 output \"cool\". ICL has the ability to learn from the prompt. Training ICL can be viewed as learning   \n195 to learn, like meta-learning [25, 18, 24]. More generally, the input and output are not necessarily   \n196 to be natural language. In ICL, a sequence model $T_{\\theta}$ (typically transformer [45]) is trained to map   \n197 sequence $\\left[x_{1},y_{1},x_{2},y_{2},\\cdot\\cdot\\cdot\\right.,x_{m-1},y_{m-1},x_{m}\\right]$ (prompt prefix) to prediction $y_{m}$ . Given distribution   \n198 $P$ of training prompt $t$ , then training ICL follows an auto-regressive manner: ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\theta}\\mathbb{E}_{t\\sim P(t)}\\frac{1}{m}\\sum_{i=0}^{m-1}\\ell\\big(y_{t,i+1},T_{\\theta}\\big([x_{t,1},y_{t,1},\\cdot\\cdot\\cdot\\mathbf{\\delta},x_{t,i+1}]\\big)\\big).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "199 It has been mentioned that the training of ICL can be viewed as an instance of meta-learning [18, 2]   \n200 as $T_{\\theta}$ learns to learn from prompt. In this section we first formally reformulate $T_{\\theta}$ to meta-learner   \n201 $g(;\\theta)$ , then introduce how ConML can be integrated with ICL. ", "page_idx": 5}, {"type": "text", "text": "202 4.1 A Meta-learning Reformulation ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "203 Denote a sequentialized $\\mathcal{D}$ as $\\vec{\\mathcal{D}}$ where the sequentializer is default to bridge $p(\\tau)$ and $P(t)$ . Then   \n204 the prompt $[x_{\\tau,1},y_{\\tau,1},\\cdot\\cdot\\cdot\\,,x_{\\tau,m},y_{\\tau,m}]$ can be viewed as $\\mathcal{D}_{\\tau}^{\\vec{t}r}$ which is providing task-specific infor  \n205 mation. Note that ICL does not specify an explicit output model $h(x)=g(D;\\theta)(x)$ ; instead, this   \n206 procedure exists only implicitly through the feeding-forward of the sequence model, i.e., task-specific   \n207 prediction is given by $g([\\vec{D},x];\\theta)$ . Thus we can reformulate the training of ICL (5) as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\theta}\\mathbb{E}_{\\tau\\sim p(\\tau)}\\frac{1}{m}\\sum_{i=0}^{m-1}\\ell\\big(y_{\\tau,i+1},g\\big([\\vec{\\mathcal{D}}_{\\tau,0:i},x_{\\tau,i+1}];\\theta\\big)\\big).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "208 Equation (6) can be regarded as the validation loss (1) in meta-learning, where each task in each   \n209 episode is sampled multiple times to form $\\mathcal{D}_{\\tau}^{\\mathrm{val}}$ and $\\mathcal{D}_{\\tau}^{\\mathrm{tr}}$ in an auto-regressive manner. The training   \n210 of ICL thus follows the episodic meta-training (Algorithm 1), where the validation loss with deter  \n211 mined $\\mathcal{D}_{\\tau}^{\\mathrm{tr}}$ and $\\mathcal{D}_{\\tau}^{\\mathrm{val}}\\colon\\mathcal{L}(\\bar{D_{\\tau}^{\\mathrm{val}}};g(\\mathcal{D}_{\\tau}^{\\mathrm{tr}};\\theta))$ , is replaced by loss validated in the auto-regressive manner:   \n212 $\\begin{array}{r}{\\frac{1}{m}\\sum_{i=0}^{m-1}\\ell(y_{\\tau,i+1},g([\\vec{\\mathcal{D}}_{\\tau,0:i},x_{\\tau,i+1}];\\theta))}\\end{array}$ . ", "page_idx": 5}, {"type": "text", "text": "213 4.2 Integration with ICL ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "214 Since the training of ICL could be reformulated as episodic meta-training, the three steps to measure   \n215 ConML proposed in Section 3.2 can be also adopted for ICL, but the first step to obtain model   \n216 representation $\\psi(g(\\mathcal{D},\\theta))$ needs modification. Due to the absence of an inner learning procedure for   \n217 a predictive model for prediction $h(x)=g(D;\\theta)(x)$ , representation by explicit model weights of $h$   \n218 is not feasible for ICL.   \n219 To represent what $g$ learns from $\\mathcal{D}$ , we design to incorporate $\\vec{\\mathcal{D}}$ with a dummy input $u$ , which   \n220 functions as a probe and its corresponding output can be readout as representation: ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\psi(g(\\mathcal{D};\\theta))=g([\\vec{\\mathcal{D}},u];\\theta),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "221 where $u$ is constrained to be in the same shape as $x$ , and has consistent value in an episode. The   \n222 complete algorithm of ConML for ICL is provided in Appendix A. From the perspective of learning   \n223 to learn, ConML encourages ICL to align and discriminate like it does for conventional meta-learning,   \n224 while the representations to evaluate inner- and inter- task distance are obtained by probing output   \n225 rather than explicit model weights. Thus, incorporating ConML into the training process of ICL   \n226 beneftis the fast-adaptation and task-level generalization ability. From the perspective of supervised   \n227 learning, ConML is performing unsupervised data augmentation that it introduces the dummy input   \n228 and contrastive objective as additional supervision to train ICL. ", "page_idx": 5}, {"type": "text", "text": "229 5 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "230 In this secrion, we first empirically investigate the alignment and discrimination empowered by   \n231 ConML. Then we show the effect of ConML that it significantly improve meta-learning performance   \n232 on a wide range of meta-learning algorithms on few-shot image classification, and the effect of   \n233 ConML-ICL with in-context learning general functions. Additionally, by applying ConML we provide   \n234 a SOTA approach for few-shot molecular property prediction problem, provided in Appendix B.   \n235 Code is provided in supplementary materials. ", "page_idx": 5}, {"type": "text", "text": "236 5.1 Impact of Alignment and Discrimination ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "237 There are two important questions to understand the way ConML works: First, does ConML equip   \n238 meta-learners with better alignment and discrimination as expected? Second, what is the contribution   \n239 of inner-task and inter-task distance respectively? We take ConML-MAML as example and investigate   \n240 above questions with few-shot regression problem following the same settings in [17], where each   \n241 task involves regressing from the input to the output of a sine wave. We use this synthetic regression   \n242 dataset to be able to sample data and vary the distribution as needed for investigation. The implement   \n243 of ConML-MAML is consistent with Section 5.2. Firstly the meta-testing performance in Table 2   \n244 shows that ConML is effective for the regression problem.   \n45 Clustering. If ConML enhances the alignment and discrimination abilities, ConML-MAML can   \n46 generate more similar models from different subsets of the same task, while generating more separable   \n47 models from different tasks. This can be verified by evaluating the clustering performance for model   \n48 representations $^e$ . During meta-testing, we randomly sample 10 different tasks, inside each we sample   \n49 10 different subsets, each one contains $N=10$ samples. Taking these 100 different $\\mathcal{D}^{\\mathrm{tr}}$ as input,   \n50 meta-learner generates 100 models. Figure 2(a) and 2(d) show the visualization of model distribution.   \n51 It can be obviously observed ConML-MAML performs better alignment and discrimination than   \n52 MAML. To quantity the results, we also evaluate the supervised clustering performance, where task   \n53 identity is used as label. Table 2 shows the supervised clustering performance of different metrics:   \n54 Silhouette score [35], Davies-Bouldin index (DBI) [15] and Calinski-Harabasz index (CHI) [9],   \n55 where ConML-MAML shows much better performance. ", "page_idx": 5}, {"type": "table", "img_path": "ZJZqO4grws/tmp/c34619634078d0a3d6593f94c780906221a1c076e422afe82633c9df728241aa.jpg", "table_caption": ["Table 2: Meta-testing and clustering performance of few-shot sinusoidal regression. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "image", "img_path": "ZJZqO4grws/tmp/c57c65f5c918ca5048651a63c32f15a99c697bc5cc8c44a2b8e5161fb608a2dd.jpg", "img_caption": ["(d) Model distribution of ConML- (e) Inter-task distance distribution. (f) Varying test distribution. MAML. Figure 2: Investigating the way ConML works. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "56 Decoupling Inner- and Inter-Task Distance. In conventional unsupervised contrastive learning,   \n7 where objective only relies on contrasting of positive pairs and negative pairs, positive and negative   \n8 pairs are both necessary to avoid learning representations without useful information. However, in   \n9 ConML, there is validation loss $L_{v}$ plays a necessary and fundamental role in \"learning to learn\",   \n0 and the contrastive objective is introduced as additional supervision to enhance alignment and discrimination. Thus, distance of positive pairs $(D^{\\mathrm{in}})$ and negative pairs $(D^{\\mathrm{out}})$ in ConML could be   \n2 decoupled and incorporated with $L_{v}$ respectively. We aim to understand how $D^{\\mathrm{in}}$ and $D^{\\mathrm{out}}$ contributes   \n3 respectively. This gives birth to two variants of ConML: in-MAML which optimize $L_{v}$ and $D^{\\mathrm{in}}$ ,   \n4 out-MAML which optimize $L_{v}$ and $D^{\\mathrm{out}}$ . During meta-testing, we randomly sample 1000 different   \n5 tasks, inside each we sample 10 different subsets each one contains $N=10$ samples. We aggregate   \n6 different subsets from the same task to form a $N\\,=\\,100$ set to obtaining $e_{\\tau}^{*}$ for each task. The distribution of $D^{\\mathrm{in}}$ and $D^{\\mathrm{out}}$ are shown in Figure 2(b) and 2(e) respectively, where the dashed lines   \n8 are mean values. We can find that: the alignment and discrimination ability corresponds to optimizing $D^{\\mathrm{in}}$ and $D^{\\mathrm{out}}$ respectively; the alignment and discrimination capabilities are generalizable; ConML shows the couple of both capabilities. Figure 2(c) shows the testing performance given different numbers of examples per task (shot), while the meta-leaner is trained with fixed $N=10$ . We can find that the improvement brought by $D^{\\mathrm{in}}$ is much more significant than $D^{\\mathrm{out}}$ under few-shot scenario,   \nwhich indicates that alignment is closely related to the fast-adaptation ability of the meta-learner. ", "page_idx": 6}, {"type": "table", "img_path": "ZJZqO4grws/tmp/69df612ccf7e89dd107ee8e0967e34a34bf1cc26e11e77aa08862b98eeb418ab.jpg", "table_caption": ["Table 3: Meta-testing accuracy on miniImageNet. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "274 Figure 2(f) shows the out-of-distribution testing performance. While meta-trained on tasks with   \n275 amplitudes that uniformly distribute on [0.1, 5], meta-testing is performed on tasks with amplitudes   \n276 that uniformly distribute on $[0.1+\\delta,5\\dot{+}\\delta]$ (the distribution shift $\\delta$ is indicated as $x$ -axis). We can   \n277 find that the improvement brought by $D^{\\mathrm{out}}$ is notably more significant as the distribution gap grows   \n278 than $D^{\\mathrm{in}}$ . This indicates that discrimination is closely related to the task-level generalization ability   \n279 of meta-learner. ConML takes both advantages brought by $D^{\\mathrm{in}}$ and $D^{\\mathrm{out}}$ . ", "page_idx": 7}, {"type": "text", "text": "280 5.2 Few-Shot Image Classification ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "281 To evaluate ConML on conventional meta-learning approaches, we follow existing works [46, 17, 39,   \n282 28, 6] to evaluate the meta-learning performance with few-shot image classification problem. We   \n283 consider representative meta-learning algorithms from different categories, including optimization  \n284 based: MAML [17], FOMAML [17], Reptile [28]; metric-based: MatchNet [46], ProtoNet [39];   \n285 and amortization-based: SCNAPs (Simple CNAPS) [6]. We evaluate their original meta-learning   \n286 performance (w/o ConML) and performance meta-trained with the proposed ConML (ConML-). The   \n287 implementation of ConML- follows the general Algorithm 2 and the specification for corresponding   \n288 category in Section 3.3.   \n289 Datasets and Settings. We consider two few-shot image classification benchmarks: miniImageNet   \n290 [46] and tieredImageNet [32]. 5-way 1-shot and 5-way 5-shot tasks are trained and evaluated   \n291 respectively. Note that we focus on the improvement comparing ConML- and the corresponding   \n292 algorithm without ConML, rather than performance comparison across different algorithms. So we   \n293 conduct the experiment on each algorithm following the originally reported settings. All baselines   \n294 share the same settings of hyperparameters related to the measurement of ConML: task batch   \n295 size $B\\:=\\:32$ , inner-task sampling $K\\,=\\,1$ and $\\pi_{\\kappa}(D_{\\tau}^{\\mathrm{tr}}\\cup D_{\\tau}^{\\mathrm{val}})\\,=\\,\\mathcal{D}_{\\tau}^{\\mathrm{tr}}$ , $\\phi(a,b)\\,=\\,1\\,-\\,a{\\cdot}b\\big/\\|a\\|\\|b\\|$   \n296 (cosine distance) and $\\lambda=0.1$ . For other settings of hyperparameters about model architecture and   \n297 training procedure, each baseline is consistent with its originally reported. Note that $K=1$ and   \n298 $\\pi_{\\kappa}(\\mathcal{D}_{\\tau}^{\\mathrm{tr}}\\cup\\mathcal{D}_{\\tau}^{\\mathrm{val}})=\\mathcal{D}_{\\tau}^{\\mathrm{tr}}$ is the most simple and efficient implementation, provided as Efficient-ConML   \n299 in Appendix A. In this case, considering the consumption of feeding-forward neural networks in each   \n300 task, Algorithm 1 takes $h=g(D_{\\tau}^{\\mathrm{tr}};\\theta)$ and $\\mathcal{L}(\\mathcal{D}_{\\tau}^{\\mathrm{val}};h)$ , while ConML only introduces an additional   \n301 $g(\\mathcal{D}_{\\tau}^{\\mathrm{tr}}\\cup\\mathcal{\\bar{D}}_{\\tau}^{\\mathrm{val}};\\theta)$ , which results in very comparable time consumption.   \n302 Results. Table 3 and 4 show the results on miniImageNet and tieredImageNet respectively. The   \n303 relative gain is calculated in terms of the summation of 1-shot and 5-shot accuracy. The relative   \n304 time is comparing the total time consumption of meta-training. Significant relative gain and very   \n305 comparable relative time consumption show that ConML brings universal improvement on different   \n306 meta-learning algorithms with cheap implementation. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "307 5.3 In-Context Learning General Functions ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "308 Following [18], we investigate ConML on ICL by learning to learn synthetic functions including   \n309 linear regression (LR), sparse linear regression (SLR), decision tree (DT) and 2-layer neural network   \n310 with ReLU activation (NN). We train the GPT-2 [30]-like transformer for each function with ICL and   \n311 ConML-ICL respectively and compare the inference (meta-testing) performance. We follow the same   \n312 model structure, data generation and training settings [18]. We implement ConML-ICL with $K=1$   \n313 and $\\pi_{\\kappa}([x_{1},y_{1},\\cdot\\cdot\\cdot\\cdot,\\bar{x_{n}},y_{n}])=[x_{1},y_{1},\\cdot\\cdot\\cdot\\cdot,x_{\\lfloor\\frac{n}{2}\\rfloor},\\bar{y}_{\\lfloor\\frac{n}{2}\\rfloor}]$ . To obtain the implicit representation (7),   \n314 we sample $u$ from a standard normal distribution (the same with $x$ \u2019s distribution) independently in   \n315 each episode. Since the output of (7) is a scalar, i.e., representation $e\\in\\mathbb R$ , we adopt distance measure   \n316 $\\phi(a,b)=\\sigma((a-b)^{2})$ , where $\\sigma(\\cdot)$ is sigmoid function to bound the squared error. $\\lambda=0.02$ .   \n317 Results. Figure 3 shows that varying the number of in-context examples during inference, ConML  \n318 ICL always makes more accurate predictions than ICL. Table 5 collects the two values to show the   \n319 effect ConML brings to ICL: Rel. Min. Error is ConML-ICL\u2019s minimal inference error given different   \n320 number of examples, divided by ICL\u2019s; Shot Spare is when ConML-ICL obtain an error no larger   \n321 than ICL\u2019s minimal error, the difference between the corresponding example numbers. Note that the   \n22 learning of different functions (different meta-datasets) share the same settings about ConML, which   \n323 shows ConML can bring ICL universal improvement with cheap implementation. We notice that   \n324 during training of LR and SLR $\\textstyle{\\left\\lfloor{\\frac{n}{2}}\\right\\rfloor}=5$ , which happens to equals to the dimension of the regression   \n325 task. This means sampling by $\\pi_{\\kappa}$ would results in the minimal sufficient information to learn the   \n326 task. In this case, minimizing $D^{\\mathrm{in}}$ is particularly beneficial for the fast-adaptation ability, shown as   \n327 Figure 3(a) and 3(b). This indicates that introducing prior knowledge to design the hyperparameter   \n328 settings of ConML could bring more advantage. The effect of ConML for ICL is without loss of   \n329 generalizability to real-world applications like pretraining large language models. ", "page_idx": 7}, {"type": "table", "img_path": "ZJZqO4grws/tmp/2bb143c811bb46b882db6e9adab8edefa81e6e6c8632ac32b4cd0a63294a94ac.jpg", "table_caption": ["Table 4: Meta-testing accuracy on tieredImageNet. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "image", "img_path": "ZJZqO4grws/tmp/dd55bd41648d3b40ef45e2d8cf7893c9d90e9ae5644496c5c5de3274c0b8f1de.jpg", "img_caption": ["Figure 3: In-context learning performance. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "330 6 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "331 In this work, we propose ConML that introduce an additional supervision for episodic meta-training   \n332 by exploiting task identity. The contrastive meta-objective is designed to emulate the alignment and   \n333 discrimination embodied in human\u2019s fast learning ability, and measured by performing contrastive   \n334 learning in the model space. Specifically, we design ConML to be integrated with the conventional   \n335 episodic meta-training, and then give specifications on a wide range of meta-learning algorithms.   \n336 We also reformulate training ICL into episodic meta-training to design ConML-ICL following the   \n337 same principle. Empirical results show that ConML can universally and significantly improve meta  \n338 learning performance by benefiting the meta-learner\u2019s fast-adaptation and task-level generalization   \n339 ability. This work lays the groundwork for contrastive meta-learning, by identifying the importance   \n340 of alignment and discrimination ability of meta-learner, and practicing contrastive learning in model   \n341 space. There also exists certain limitations, such as lack of investigating advanced contrastive strategy,   \n342 batch- and subset- sampling strategies. We would consider these as future directions. ", "page_idx": 8}, {"type": "text", "text": "343 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "344 [1] Kwangjun Ahn, Xiang Cheng, Hadi Daneshmand, and Suvrit Sra. Transformers learn to imple  \n345 ment preconditioned gradient descent for in-context learning. Advances in Neural Information   \n346 Processing Systems, 36, 2024.   \n347 [2] Ekin Aky\u00fcrek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. What   \n348 learning algorithm is in-context learning? investigations with linear models. arXiv preprint   \n349 arXiv:2211.15661, 2022.   \n350 [3] Han Altae-Tran, Bharath Ramsundar, Aneesh S Pappu, and Vijay Pande. Low data drug   \n351 discovery with one-shot learning. ACS Central Science, 3(4):283\u2013293, 2017.   \n352 [4] Marcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew W Hoffman, David Pfau, Tom   \n353 Schaul, Brendan Shillingford, and Nando De Freitas. Learning to learn by gradient descent by   \n354 gradient descent. Advances in neural information processing systems, 29, 2016.   \n355 [5] Philip Bachman, R Devon Hjelm, and William Buchwalter. Learning representations by   \n356 maximizing mutual information across views. Advances in neural information processing   \n357 systems, 32, 2019.   \n358 [6] Peyman Bateni, Raghav Goyal, Vaden Masrani, Frank Wood, and Leonid Sigal. Improved   \n359 few-shot visual classification. In Proceedings of the IEEE/CVF conference on computer vision   \n360 and pattern recognition, pages 14493\u201314502, 2020.   \n361 [7] John Bronskill, Daniela Massiceti, Massimiliano Patacchiola, Katja Hofmann, Sebastian   \n362 Nowozin, and Richard Turner. Memory efficient meta-learning with large images. Advances in   \n363 neural information processing systems, 34:24327\u201324339, 2021.   \n364 [8] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,   \n365 Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are   \n366 few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.   \n367 [9] Tadeusz Calin\u00b4ski and Jerzy Harabasz. A dendrite method for cluster analysis. Communications   \n368 in Statistics-theory and Methods, 3(1):1\u201327, 1974.   \n369 [10] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework   \n370 for contrastive learning of visual representations. In International conference on machine   \n371 learning, pages 1597\u20131607. PMLR, 2020.   \n372 [11] Wenlin Chen, Austin Tripp, and Jos\u00e9 Miguel Hern\u00e1ndez-Lobato. Meta-learning adaptive deep   \n373 kernel gaussian processes for molecular property prediction. In International Conference on   \n374 Learning Representations, 2022.   \n375 [12] Zhe Chen. Object-based attention: A tutorial review. Attention, Perception, & Psychophysics,   \n376 74:784\u2013802, 2012.   \n377 [13] Stella Christie. Learning sameness: object and relational similarity across species. Current   \n378 Opinion in Behavioral Sciences, 37:41\u201346, 2021.   \n379 [14] Ching-Yao Chuang, Joshua Robinson, Yen-Chen Lin, Antonio Torralba, and Stefanie Jegelka.   \n380 Debiased contrastive learning. Advances in neural information processing systems, 33:8765\u2013   \n381 8775, 2020.   \n382 [15] David L Davies and Donald W Bouldin. A cluster separation measure. IEEE transactions on   \n383 pattern analysis and machine intelligence, (2):224\u2013227, 1979.   \n384 [16] Thomas Elsken, Benedikt Staffler, Jan Hendrik Metzen, and Frank Hutter. Meta-learning of   \n385 neural architectures for few-shot learning. In Proceedings of the IEEE/CVF conference on   \n386 computer vision and pattern recognition, pages 12365\u201312375, 2020.   \n387 [17] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adap  \n388 tation of deep networks. In International conference on machine learning, pages 1126\u20131135.   \n389 PMLR, 2017.   \n390 [18] Shivam Garg, Dimitris Tsipras, Percy S Liang, and Gregory Valiant. What can transformers   \n391 learn in-context? a case study of simple function classes. Advances in Neural Information   \n392 Processing Systems, 35:30583\u201330598, 2022.   \n393 [19] Marta Garnelo, Dan Rosenbaum, Christopher Maddison, Tiago Ramalho, David Saxton, Murray   \n394 Shanahan, Yee Whye Teh, Danilo Rezende, and SM Ali Eslami. Conditional neural processes.   \n395 In International conference on machine learning, pages 1704\u20131713. PMLR, 2018.   \n396 [20] Zhichun Guo, Chuxu Zhang, Wenhao Yu, John Herr, Olaf Wiest, Meng Jiang, and Nitesh V   \n397 Chawla. Few-shot graph learning for molecular property prediction. In The Web Conference,   \n398 pages 2559\u20132567, 2021.   \n399 [21] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for   \n400 unsupervised visual representation learning. In Proceedings of the IEEE/CVF conference on   \n401 computer vision and pattern recognition, pages 9729\u20139738, 2020.   \n402 [22] R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil Bachman,   \n403 Adam Trischler, and Yoshua Bengio. Learning deep representations by mutual information   \n404 estimation and maximization. arXiv preprint arXiv:1808.06670, 2018.   \n405 [23] John E Hummel. Object recognition. Oxford handbook of cognitive psychology, 810:32\u201346,   \n406 2013.   \n407 [24] Louis Kirsch, James Harrison, Jascha Sohl-Dickstein, and Luke Metz. General-purpose in  \n408 context learning by meta-learning transformers. arXiv preprint arXiv:2212.04458, 2022.   \n409 [25] Sewon Min, Mike Lewis, Luke Zettlemoyer, and Hannaneh Hajishirzi. Metaicl: Learning to   \n410 learn in context. arXiv preprint arXiv:2110.15943, 2021.   \n411 [26] Anusha Nagabandi, Ignasi Clavera, Simin Liu, Ronald S Fearing, Pieter Abbeel, Sergey Levine,   \n412 and Chelsea Finn. Learning to adapt in dynamic, real-world environments through meta  \n413 reinforcement learning. arXiv preprint arXiv:1803.11347, 2018.   \n414 [27] VS Napper. Alignment of learning, teaching, and assessment. Encyclopedia of the sciences of   \n415 learning. Boston: Springer US, pages 200\u20132, 2012.   \n416 [28] Alex Nichol, Joshua Achiam, and John Schulman. On first-order meta-learning algorithms.   \n417 arXiv preprint arXiv:1803.02999, 2018.   \n418 [29] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive   \n419 predictive coding. arXiv preprint arXiv:1807.03748, 2018.   \n420 [30] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al.   \n421 Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.   \n422 [31] KI Ramachandran, Gopakumar Deepa, and Krishnan Namboori. Computational chemistry and   \n423 molecular modeling: principles and applications. Springer Science & Business Media, 2008.   \n424 [32] Mengye Ren, Eleni Triantafillou, Sachin Ravi, Jake Snell, Kevin Swersky, Joshua B Tenen  \n425 baum, Hugo Larochelle, and Richard S Zemel. Meta-learning for semi-supervised few-shot   \n426 classification. arXiv preprint arXiv:1803.00676, 2018.   \n427 [33] James Requeima, Jonathan Gordon, John Bronskill, Sebastian Nowozin, and Richard E Turner.   \n428 Fast and flexible multi-task classification using conditional neural adaptive processes. Advances   \n429 in Neural Information Processing Systems, 32, 2019.   \n430 [34] Donald Robbins. Stimulus selection in human discrimination learning and transfer. Journal of   \n431 Experimental Psychology, 84(2):282, 1970.   \n432 [35] Peter J Rousseeuw. Silhouettes: a graphical aid to the interpretation and validation of cluster   \n433 analysis. Journal of computational and applied mathematics, 20:53\u201365, 1987.   \n434 [36] Johannes Schimunek, Philipp Seidl, Lukas Friedrich, Daniel Kuhn, Friedrich Rippmann, Sepp   \n435 Hochreiter, and G\u00fcnter Klambauer. Context-enriched molecule representations improve few  \n436 shot drug discovery. arXiv preprint arXiv:2305.09481, 2023.   \n437 [37] J\u00fcrgen Schmidhuber. Evolutionary principles in self-referential learning, or on learning how to   \n438 learn: the meta-meta-... hook. PhD thesis, Technische Universit\u00e4t M\u00fcnchen, 1987.   \n439 [38] Albert Shaw, Wei Wei, Weiyang Liu, Le Song, and Bo Dai. Meta architecture search. Advances   \n440 in Neural Information Processing Systems, 32, 2019.   \n441 [39] Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning.   \n442 Advances in neural information processing systems, 30, 2017.   \n443 [40] Megan Stanley, John F Bronskill, Krzysztof Maziarz, Hubert Misztela, Jessica Lanini, Marwin   \n444 Segler, Nadine Schneider, and Marc Brockschmidt. FS-Mol: A few-shot learning dataset of   \n445 molecules. In Neural Information Processing Systems Track on Datasets and Benchmarks,   \n446 2021.   \n447 [41] Flood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Philip HS Torr, and Timothy M Hospedales.   \n448 Learning to compare: Relation network for few-shot learning. In Proceedings of the IEEE   \n449 conference on computer vision and pattern recognition, pages 1199\u20131208, 2018.   \n450 [42] Sebastian Thrun and Lorien Pratt. Learning to learn: Introduction and overview. In Learning to   \n451 learn, pages 3\u201317. Springer, 1998.   \n452 [43] Yonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive multiview coding. In Com  \n453 puter Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020,   \n454 Proceedings, Part XI 16, pages 776\u2013794. Springer, 2020.   \n455 [44] Yonglong Tian, Chen Sun, Ben Poole, Dilip Krishnan, Cordelia Schmid, and Phillip Isola. What   \n456 makes for good views for contrastive learning? Advances in neural information processing   \n457 systems, 33:6827\u20136839, 2020.   \n458 [45] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,   \n459 \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information   \n460 processing systems, 30, 2017.   \n461 [46] Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et al. Matching networks   \n462 for one shot learning. Advances in neural information processing systems, 29, 2016.   \n463 [47] Johannes Von Oswald, Eyvind Niklasson, Ettore Randazzo, Jo\u00e3o Sacramento, Alexander   \n464 Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by   \n465 gradient descent. In International Conference on Machine Learning, pages 35151\u201335174.   \n466 PMLR, 2023.   \n467 [48] Tongzhou Wang and Phillip Isola. Understanding contrastive representation learning through   \n468 alignment and uniformity on the hypersphere. In International conference on machine learning,   \n469 pages 9929\u20139939. PMLR, 2020.   \n470 [49] Yaqing Wang, Abulikemu Abuduweili, Quanming Yao, and Dejing Dou. Property-aware relation   \n471 networks for few-shot molecular property prediction. In Advances in Neural Information   \n472 Processing Systems, pages 17441\u201317454, 2021.   \n473 [50] Yaqing Wang, Quanming Yao, James T Kwok, and Lionel M Ni. Generalizing from a few   \n474 examples: A survey on few-shot learning. ACM computing surveys (csur), 53(3):1\u201334, 2020.   \n475 [51] Yu-Xiong Wang and Martial Hebert. Learning to learn: Model regression networks for easy   \n476 small sample learning. In Computer Vision\u2013ECCV 2016: 14th European Conference, Amster  \n477 dam, The Netherlands, October 11-14, 2016, Proceedings, Part VI 14, pages 616\u2013634. Springer,   \n478 2016.   \n479 [52] Yu-Xiong Wang, Deva Ramanan, and Martial Hebert. Learning to model the tail. Advances in   \n480 neural information processing systems, 30, 2017.   \n481 [53] Michael J Waring, John Arrowsmith, Andrew R Leach, Paul D Leeson, Sam Mandrell, Robert M   \n482 Owen, Garry Pairaudeau, William D Pennie, Stephen D Pickett, Jibo Wang, et al. An analysis   \n483 of the attrition of drug candidates from four major pharmaceutical companies. Nature Reviews   \n484 Drug discovery, 14(7):475\u2013486, 2015.   \n485 [54] Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin. Unsupervised feature learning via   \n486 non-parametric instance discrimination. In Proceedings of the IEEE conference on computer   \n487 vision and pattern recognition, pages 3733\u20133742, 2018.   \n488 [55] Han-Jia Ye, Lu Ming, De-Chuan Zhan, and Wei-Lun Chao. Few-shot learning with a strong   \n489 teacher. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022.   \n490 [56] Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea Finn, and   \n491 Sergey Levine. Meta-world: A benchmark and evaluation for multi-task and meta reinforcement   \n492 learning. In Conference on robot learning, pages 1094\u20131100. PMLR, 2020. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Algorithm 3 ConML Input: Task distribution $p(\\tau)$ , batch size $B$ , inner-task sample times $K$ and sampling strategy $\\pi_{\\kappa}$ . while Not converged do Sample a batch of tasks $\\pmb{b}\\sim p^{B}(\\tau)$ . for All $\\tau\\in b$ do for $k=1,2,\\cdots,K\\,\\mathbf{d}$ o Sample $\\kappa_{k}$ from $\\pi_{\\kappa}(\\mathcal{D}_{\\tau}^{\\mathrm{tr}}\\cup\\mathcal{D}_{\\tau}^{\\mathrm{val}})$ ; Get model representation $e_{\\tau}^{\\kappa_{k}}=\\psi(g(\\kappa_{k};\\theta))$ ; end for Get model representation $e_{\\tau}^{*}=\\psi(g(\\mathcal{D}_{\\tau}^{\\mathrm{tr}}\\cup\\mathcal{D}_{\\tau}^{\\mathrm{val}};\\theta))$ ; Get inner-task distance $D_{\\tau}^{\\mathrm{in}}$ by (2); Get task-specific model $h_{\\tau}=g(D_{\\tau}^{\\mathrm{tr}};\\theta)$ ; Get validation loss $\\mathcal{L}(\\mathcal{D}_{\\tau}^{\\mathrm{val}};h_{\\tau})$ ; end for Get $\\begin{array}{r}{D^{\\mathrm{in}}=\\frac{1}{B}\\sum_{\\tau\\in b}D_{\\tau}^{\\mathrm{in}}}\\end{array}$ and $D^{\\mathrm{out}}$ by (3); Get loss $L$ by (4); Update $\\theta$ by $\\theta\\leftarrow\\theta-\\nabla_{\\theta}L$ . end while ", "page_idx": 13}, {"type": "text", "text": "Algorithm 4 Efficient ConML ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Input: Task distribution $p(\\tau)$ , batch size $B$ (inner-task sample times $K=1$ and sampling strategy   \n$\\bar{\\pi_{\\kappa}(\\mathcal{D}_{\\tau}^{\\mathrm{tr}}\\cup\\mathcal{D}_{\\tau}^{\\mathrm{val}})}=\\mathcal{D}_{\\tau}^{\\mathrm{tr}})$ .   \nwhile Not converged do Sample a batch of tasks $\\pmb{b}\\sim p^{B}(\\tau)$ . for All $\\tau\\in b$ do Get task-specific model $h_{\\tau}=g(D_{\\tau}^{\\mathrm{tr}};\\theta)$ , and model representation $e_{\\tau}^{\\kappa_{k}}=\\psi(g(\\kappa_{k};\\theta))$ ; Get model representation $e_{\\tau}^{\\ast}=\\psi\\big(g(\\mathcal{D}_{\\tau}^{\\mathrm{tr}}\\cup\\mathcal{D}_{\\tau}^{\\mathrm{val}};\\theta)\\big)$ ; Get inner-task distance $D_{\\tau}^{\\mathrm{in}}$ by (2); Get validation loss $\\mathcal{L}(\\mathcal{D}_{\\tau}^{\\mathrm{val}};h_{\\tau})$ ; end for GGeett $\\begin{array}{r}{D^{\\mathrm{in}}=\\frac{1}{B}\\sum_{\\tau\\in b}D_{\\tau}^{\\mathrm{in}}}\\end{array}$ and $D^{\\mathrm{out}}$ by (3); $L$ Update $\\theta$ by $\\theta\\leftarrow\\theta-\\nabla_{\\theta}L$ .   \nend while   \nInput: Task distribution $p(\\tau)$ , batch size $B$ , inner-task sample times $K$ and sampling strategy $\\pi_{\\kappa}$ ,   \ndummy input $u$ (probe).   \nwhile Not converged do Sample a batch of tasks $\\pmb{b}\\sim p^{B}(\\tau)$ . for All $\\tau\\in b$ do for $k=1,2,\\cdots\\,,K$ do Sample $\\kappa_{k}$ from $\\pi_{\\kappa}(\\mathcal{D}_{\\tau})$ ; Get $e_{\\tau}^{\\kappa_{k}}=g([\\vec{\\kappa_{k}},u];\\theta)$ ; end for Get $\\boldsymbol{e}_{\\tau}^{*}=g([\\vec{D_{\\tau}},u];\\boldsymbol{\\theta})$ ; Get inner-task distance $D_{\\tau}^{\\mathrm{in}}$ by (2); Get task loss $\\begin{array}{r}{\\frac{1}{m}\\sum_{i=0}^{m-1}\\ell(y_{\\tau,i+1},g([\\vec{D}_{\\tau,0:i},x_{\\tau,i+1}];\\theta))}\\end{array}$ ; end for Get $\\begin{array}{r}{D^{\\mathrm{in}}=\\frac{1}{B}\\sum_{\\tau\\in b}D_{\\tau}^{\\mathrm{in}}}\\end{array}$ and $D^{\\mathrm{out}}$ by (3); GUeptd laotes $\\begin{array}{r}{L=\\frac{1}{\\mathcal{B}}\\sum_{\\tau\\in b}\\frac{1}{m}\\sum_{i=0}^{m-1}\\ell(y_{\\tau,i+1},g([\\vec{\\mathcal{D}}_{\\tau,0:i},x_{\\tau,i+1}];\\theta))+\\lambda(D^{\\mathrm{in}}-D^{\\mathrm{out}});}\\end{array}$ $\\theta$ $\\theta\\leftarrow\\theta\\bar{-}\\nabla_{\\theta}L$   \nend while ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "Algorithm 6 ConML-MAML ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Input: Task distribution $p(\\tau)$ , batch size $B$ , inner-task sample times $K=1$ and sampling strategy   \n\u03c0\u03ba   \nwhile Not converged do Sample a batch of tasks $\\pmb{b}\\sim p^{B}(\\tau)$ . for All $\\tau\\in b$ do for $k=1,2,\\cdots\\,,K$ do Sample $\\kappa_{k}$ from $\\pi_{\\kappa}(\\mathcal{D}_{\\tau}^{\\mathrm{tr}}\\cup\\mathcal{D}_{\\tau}^{\\mathrm{val}})$ ; Get model representation $e_{\\tau}^{\\kappa\\dot{k}}=\\theta-\\nabla_{\\theta}\\mathcal{L}(\\kappa_{k};h_{\\theta})$ ; end for Get model representation $e_{\\tau}^{*}=\\theta-\\nabla_{\\theta}\\mathcal{L}(\\mathcal{D}_{\\tau}^{\\mathrm{tr}}\\cup\\mathcal{D}_{\\tau}^{\\mathrm{val}};h_{\\theta})$ . Get inner-task distance $D_{\\tau}^{\\mathrm{in}}$ by (2); Get task-specific model h\u03b8\u2212\u2207\u03b8L(Dt\u03c4r ;\u03b8); Get validation loss L(D\u03c4va l; h\u03b8\u2212\u2207\u03b8L(Dtr ;h\u03b8)); end for Get $\\begin{array}{r}{D^{\\mathrm{in}}=\\frac{1}{B}\\sum_{\\tau\\in b}D_{\\tau}^{\\mathrm{in}}}\\end{array}$ and $D^{\\mathrm{out}}$ by (3); Get loss $L$ by (4); Update $\\theta$ by $\\theta\\leftarrow\\theta-\\nabla_{\\theta}L$ .   \nend while ", "page_idx": 14}, {"type": "text", "text": "Algorithm 7 ConML-Reptile ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Input: Task distribution $p(\\tau)$ , batch size $B$ . (inner-task sample times $K=1$ and sampling strategy   \n$\\bar{\\pi_{\\kappa}^{\\mathsf{\\bar{(}}}(\\mathcal{D}_{\\tau}^{\\mathrm{tr}}\\cup\\mathcal{D}_{\\tau}^{\\mathrm{val}})=\\mathcal{D}_{\\tau}^{\\mathrm{tr}})}$   \nwhile Not converged do Sample a batch of tasks $\\pmb{b}\\sim p^{B}(\\tau)$ . for All $\\tau\\in b$ do for $k=1,2,\\cdots\\,,K$ do Sample $\\kappa_{k}$ from $\\pi_{\\kappa}(\\mathcal{D}_{\\tau})$ ; Get model representation $\\begin{array}{r}{e_{\\tau}^{\\kappa_{k}}=\\theta-\\nabla_{\\theta}\\mathcal{L}(\\kappa_{k};h_{\\theta})}\\end{array}$ ; end for Get model representation $e_{\\tau}^{*}=\\theta-\\nabla_{\\theta}\\mathcal{L}(\\mathcal{D}_{\\tau}^{\\mathrm{tr}}\\cup\\mathcal{D}_{\\tau}^{\\mathrm{val}};h_{\\theta})$ . Get inner-task distance $D_{\\tau}^{\\mathrm{in}}$ by (2); end for Get $\\begin{array}{r}{D^{\\mathrm{in}}=\\frac{1}{B}\\sum_{\\tau\\in b}D_{\\tau}^{\\mathrm{in}}}\\end{array}$ and $D^{\\mathrm{out}}$ by (3); Get loss $L$ by (4); Update $\\theta$ by $\\begin{array}{r}{\\dot{\\theta}\\xleftarrow{\\theta}+\\frac{1}{B}\\sum_{\\tau\\in b}(e_{\\tau}^{*}-\\theta)-\\lambda\\nabla_{\\theta}(D^{\\mathrm{in}}-D^{\\mathrm{out}}).}\\end{array}$   \nend while   \nNote: Here $h_{w}$ corresponds to the feature extractor $f_{\\boldsymbol{\\theta}};H_{\\boldsymbol{\\theta}}$ corresponds to the task encoder $g_{\\phi}$ in   \n[6].   \nInput: Task distribution $p(\\tau)$ , batch size $B$ , inner-task sample times $K$ and sampling strategy $\\pi_{\\kappa}$ .   \nPretrain $h_{w}$ with the mixture of all meta-training data;   \nwhile Not converged do Sample a batch of tasks $\\pmb{b}\\sim p^{B}(\\tau)$ . for All $\\tau\\in b$ do for $k=1,2,\\cdots\\,,K\\,\\epsilon$ do Sample $\\kappa_{k}$ from $\\pi_{\\kappa}(\\mathcal{D}_{\\tau}^{\\mathrm{tr}}\\cup\\mathcal{D}_{\\tau}^{\\mathrm{val}})$ ; Get model representation $e_{\\tau}^{\\kappa_{k}}=H_{\\theta}(\\kappa_{k})$ ; end for Get model representation $e_{\\tau}^{*}=H_{\\theta}(\\mathcal{D}_{\\tau}^{\\mathrm{tr}}\\cup\\mathcal{D}_{\\tau}^{\\mathrm{val}})$ ; Get inner-task distance $D_{\\tau}^{\\mathrm{in}}$ by (2); Get task-specific model by FiLM $h_{\\tau}=h_{w,H_{\\theta}(\\mathcal{D}_{\\tau}^{\\mathrm{tr}})}$ ; Get validation loss $\\mathcal{L}(\\mathcal{D}_{\\tau}^{\\mathrm{val}};h_{\\tau})$ ; end for Get $\\begin{array}{r}{D^{\\mathrm{in}}=\\frac{1}{B}\\sum_{\\tau\\in b}D_{\\tau}^{\\mathrm{in}}}\\end{array}$ and $D^{\\mathrm{out}}$ by (3); Get loss $L$ by (4); Update $\\theta$ by $\\theta\\leftarrow\\theta-\\nabla_{\\theta}L$ .   \nend while   \nInput: Task distribution $p(\\tau)$ , batch size $B$ , inner-task sample times $K=1$ and sampling strategy   \n$\\pi_{\\kappa}$   \nwhile Not converged do Sample a batch of tasks $\\pmb{b}\\sim p^{B}(\\tau)$ . for All $\\tau\\in b$ do for $k=1,2,\\cdots\\,,K$ do Sample $\\kappa_{k}$ from $\\pi_{\\kappa}(\\mathcal{D}_{\\tau}^{\\mathrm{tr}}\\cup\\mathcal{D}_{\\tau_{*}}^{\\mathrm{val}})$ ; Calculate prototypes $\\begin{array}{r}{\\pmb{c}_{j}^{\\prime}=\\frac{1}{|\\kappa_{k,j}|}\\sum_{(x_{i},y_{i})\\in\\kappa_{k,j}}f_{\\theta}(x_{i})}\\end{array}$ for $j=1,\\cdot\\cdot\\cdot\\,,N$ ; Get model representation $\\pmb{e}_{\\tau}^{\\kappa_{k}}=[\\pmb{c}_{1}|\\pmb{c}_{2}|\\cdot\\cdot\\cdot|\\pmb{c}_{N}]$ ; end for Calculate prototypes $\\begin{array}{r}{\\pmb{c}_{j}=\\frac{1}{|\\mathcal{D}_{j}|}\\sum_{(\\boldsymbol{x}_{i},y_{i})\\in\\mathcal{D}_{j}}f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i})}\\end{array}$ for $j=1,\\cdot\\cdot\\cdot\\,,N$ ; Get model representation $\\pmb{e}_{\\tau}^{*}=[\\pmb{c}_{1}|\\pmb{c}_{2}|\\cdot\\cdot\\cdot\\cdot|\\pmb{c}_{N}]$ ; Get inner-task distance $D_{\\tau}^{\\mathrm{in}}$ by (2); Get task-specific model $h_{[{c_{1}}|{c_{2}}|\\cdots|{c_{N}}]}$ , which gives prediction by $p(y\\ \\,=\\ \\,j\\ \\ \\mid\\ x)\\ =$ $\\frac{e x p(-d(f_{\\theta}(x),\\pmb{c}_{j}))}{\\sum_{j^{\\prime}}e x p(-d(f_{\\theta}(x),\\pmb{c}_{j^{\\prime}}))}$ ; Get validation loss $\\mathcal{L}(\\mathcal{D}_{\\tau}^{\\mathrm{val}};h_{[c_{1}\\vert c_{2}\\vert\\cdots\\vert c_{N}]})$ ; end for Get $\\begin{array}{r}{D^{\\mathrm{in}}=\\frac{1}{B}\\sum_{\\tau\\in b}D_{\\tau}^{\\mathrm{in}}}\\end{array}$ and $D^{\\mathrm{out}}$ by (3); Get loss $L$ by (4); Update $\\theta$ by $\\theta\\leftarrow\\theta-\\nabla_{\\theta}L$ .   \nend while ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "495 Few-shot molecular property prediction (FSMPP) is an important real-world application where meta  \n496 learning has been widely applied recently [3, 20, 49, 11, 36]. Molecular property prediction, which   \n497 predicts whether desired properties will be active on given molecules, plays a crucial role in many   \n498 applications like computational chemistry [31] and drug discovery [53]. As wet-lab experiments   \n499 to evaluate the actual properties of molecules are expensive and risky, usually only a few labeled   \n500 molecules are available for a specific property. Molecular property prediction can be naturally   \n501 modeled as a few-shot learning problem [3], and meta-learning approaches has been successfully   \n502 adopted for FSMPP [3, 20, 49, 11].   \n503 Dataset and Settings. We use FS-Mol [40], a widely studied FSMPP benchmark consisting of   \n504 a large number of diverse tasks. We adopt the public data split [40]. Each training set contains 64   \n505 labeled molecules, and can be imbalanced where the number of labeled molecules from active and   \n506 inactive is not equal. All remaining molecules in the task form the validation set. The performance is   \n507 evaluated by $\\Delta$ AUPRC (change in area under the precision-recall curve) w.r.t. a random classifier [40],   \n508 averaged across meta-testing tasks.   \n509 Baselines. We consider the following meta-learning-based FSMPP approaches: MAML, ProtoNet,   \n510 CNP, IterRefLSTM, PAR, ADKF-IFT. Note that MHNfs [36] is not included as it uses additional   \n511 reference molecules from external datasets, which leads to unfair comparison, and ADKF-IFT is   \n512 the SOTA approach in literature. All baselines share the same GNN-based encoder provided by the   \n513 benchmark to meta-train from scratch, which maps molecular graphs to embedding vectors. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "Algorithm 10 Hypro ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Note: The main-network consists of two modules [40]: the molecular encoder $f_{\\theta}$ and the prototyp  \nical network classifier $h_{\\theta}$ .   \nInput: Task distribution $p(\\tau)$ , batch size $B$ .   \nwhile Not converged do Sample a batch of tasks $\\pmb{b}\\sim p^{B}(\\tau)$ . for All $\\tau\\in b$ do Encode all molecules $f_{\\boldsymbol{\\theta}}(\\boldsymbol{x})$ for $x\\in\\mathcal{D}_{\\tau}^{\\mathrm{tr}}\\cup\\mathcal{D}_{\\tau}^{\\mathrm{val}}$ Get task-specific parameters $\\alpha_{\\tau}=H_{\\theta}(\\{(f_{\\theta}(x_{i}),y_{i})\\}_{(x_{i},y_{i})\\in{\\mathcal{D}_{\\tau}^{\\mathrm{t}}}})$ ; Modulate all molecular embedding with $\\alpha_{\\tau}$ by FiLM, and classify with $h_{\\theta}$ ; (denote the function of this step as $h_{\\theta,\\alpha_{\\tau}}$ ) Get validation loss $\\mathcal{L}(D_{\\tau}^{\\mathrm{val}};h_{\\theta,\\alpha_{\\tau}})$ ; end for $\\begin{array}{r}{L_{v}=\\frac{1}{B}\\sum_{\\tau\\in b}\\mathcal{L}(\\mathcal{D}_{\\tau}^{\\mathrm{val}};h_{\\theta,\\alpha_{\\tau}})}\\end{array}$ Update $\\theta$ by $\\bar{\\theta}\\leftarrow\\theta-\\nabla_{\\theta}L_{v}$ .   \nend while ", "page_idx": 16}, {"type": "text", "text": "514 We introduce a new baseline ConML-Hypro, which achieves SOTA performance by incorporating   \n515 ConML with a simple backbone, Hypro. It is an amortization-based model built by modifying the   \n516 ProtoNet backbone, by plugging-in a hypernetwork $H$ with a set-encoder structure, i.e., $\\dot{H}(\\bar{\\mathcal{D}})=$   \n517 $\\begin{array}{r l}&{\\mathtt{M L P}_{2}\\big(\\frac{1}{|\\mathcal{D}|}\\sum_{\\mathcal{D}}\\mathtt{M L P}_{1}\\big([\\dot{x_{i}}\\mathbin{\\overbar{|}}\\ y_{i}\\mathbin{\\overbar{]}}\\big)\\big)}\\end{array}$ . We input the embedding vectors in $\\mathcal{D}^{\\mathrm{tr}}$ to the hypernetwork, and take   \n518 the output to modulate embedding vectors through FiLM before classification. This hypernetwork   \n519 and modulation is typical in amortization-based models. Viewing Hypro as an amortization-based   \n520 model, we apply the specification of ConML to form ConML-Hypro. The detailed procedure to train   \n521 Hypro and ConML-Hypro are provided in Algorithm 10 and 11. The structure of $H$ is provided   \n522 in Table 6, and two such hypernetworks are used for generate parameters for FiLM function. We   \n523 implement ConML with $B=16$ , $\\phi(a,b)=1-a\\cdot b/\\|a\\|\\,\\|b\\|$ (cosine distance) and $\\lambda=0.1$ . As for the   \n524 sampling strategy $\\pi_{\\kappa}$ and times $K$ , for every task, we sample subset with different sizes, including   \n525 each $m\\in\\{4,8,16,32,64\\}$ , for $128/m$ times respectively. A $m$ -sized subset contains $m/2$ positive   \n526 and $m/2$ negative samples sampled randomly. The other hyperparameters of model structure and   \n527 training procedure follow the benchmark\u2019s default setting [40].   \n528 Results. Table 7 shows the results. ConML-Hypro shows advantage over SOTA approach under   \n529 all meta-testing scenarios with different shots. Comparing Hypro and ProtoNet, we can find the ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "Note: Refer to Algorithm 10 for details about $H_{\\theta}(\\mathcal{D})$ and $h_{\\theta,\\alpha}$ .   \nInput: Task distribution $p(\\tau)$ , batch size $B$ , inner-task sample times $K$ and sampling strategy $\\pi_{\\kappa}$ .   \nwhile Not converged do Sample a batch of tasks $\\pmb{b}\\sim p^{B}(\\tau)$ . for All $\\tau\\in b$ do for $k=1,2,\\cdots\\,,K$ do Sample $\\kappa_{k}$ from $\\pi_{\\kappa}(\\mathcal{D}_{\\tau}^{\\mathrm{tr}}\\cup\\mathcal{D}_{\\tau}^{\\mathrm{val}})$ ; Get model representation $e_{\\tau}^{\\kappa\\dot{k}}=H_{\\theta}(\\kappa_{k})$ ; end for Get model representation $e_{\\tau}^{*}=H_{\\theta}(\\mathcal{D}_{\\tau}^{\\mathrm{tr}}\\cup\\mathcal{D}_{\\tau}^{\\mathrm{val}})$ ; Get inner-task distance $D_{\\tau}^{\\mathrm{in}}$ by (2); Get task-specific model h\u03b8,H\u03b8(Dt\u03c4r ); Get validation loss $\\mathcal{L}(\\mathcal{D}_{\\tau}^{\\mathrm{val}};h_{\\theta,H_{\\theta}(\\mathcal{D}_{\\tau}^{\\mathrm{tr}})})$ ; end for Get $\\begin{array}{r}{D^{\\mathrm{in}}=\\frac{1}{B}\\sum_{\\tau\\in b}D_{\\tau}^{\\mathrm{in}}}\\end{array}$ and $D^{\\mathrm{out}}$ by (3); Get loss $L$ by (4); Update $\\theta$ by $\\theta\\leftarrow\\theta-\\nabla_{\\theta}L$ .   \nend while ", "page_idx": 17}, {"type": "table", "img_path": "ZJZqO4grws/tmp/b00237f89efd186e88ccce595de89716a5f60a5c5c3b1fdb1912aabe903725f7.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "530 introduced hypernetwork can brings notable improvement. Comparing ConML-Hypro and Hypro,   \n531 we can find the effect of ConML is significant. ", "page_idx": 17}, {"type": "text", "text": "Table 7: Few-shot molecular property prediction performance (\u2206AUPRC) on FS-Mol. $^{\\dagger}$ indicates result from [36]. $^*$ indicates new approach proposed in this paper. ", "page_idx": 17}, {"type": "table", "img_path": "ZJZqO4grws/tmp/caa619bfc40da48efee2a01f0535ef9f178446400065af200a818db926422fd1.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "532 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "533 1. Claims   \n534 Question: Do the main claims made in the abstract and introduction accurately reflect the   \n535 paper\u2019s contributions and scope? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "537 Justification: [TODO] ", "page_idx": 18}, {"type": "text", "text": "538 Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 18}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "551 Justification: [TODO] ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 18}, {"type": "text", "text": "579 3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "80 Question: For each theoretical result, does the paper provide the full set of assumptions and   \n81 a complete (and correct) proof? ", "page_idx": 18}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 19}, {"type": "text", "text": "595 4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "596 Question: Does the paper fully disclose all the information needed to reproduce the main ex  \n597 perimental results of the paper to the extent that it affects the main claims and/or conclusions   \n598 of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "600 Justification: [TODO]   \n601 Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 19}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "34 Question: Does the paper provide open access to the data and code, with sufficient instruc  \n35 tions to faithfully reproduce the main experimental results, as described in supplemental   \n36 material? ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 20}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 20}, {"type": "text", "text": "637   \n638   \n639   \n640   \n641   \n642   \n643   \n644   \n645   \n646   \n647   \n648   \n649   \n650   \n651   \n652   \n653   \n654   \n655   \n656   \n657   \n658   \n659   \n660   \n661   \n662   \n663   \n664   \n665   \n666   \n667   \n668   \n669   \n670   \n671   \n672   \n673   \n674   \n675   \n676   \n677   \n678   \n679   \n680   \n681   \n682   \n683   \n684   \n685   \n686   \n687   \n688 ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: [TODO] ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 20}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 20}, {"type": "text", "text": "Answer:[Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: [TODO] Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "697 8. Experiments Compute Resources ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "698 Question: For each experiment, does the paper provide sufficient information on the com  \n699 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n00 the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 21}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: [TODO] ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 21}, {"type": "text", "text": "723 10. Broader Impacts ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "724 Question: Does the paper discuss both potential positive societal impacts and negative   \n725 societal impacts of the work performed? ", "page_idx": 21}, {"type": "text", "text": "Answer:[Yes] ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to ", "page_idx": 21}, {"type": "text", "text": "740 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n741 that a generic algorithm for optimizing neural networks could enable people to train   \n742 models that generate Deepfakes faster.   \n743 \u2022 The authors should consider possible harms that could arise when the technology is   \n744 being used as intended and functioning correctly, harms that could arise when the   \n745 technology is being used as intended but gives incorrect results, and harms following   \n746 from (intentional or unintentional) misuse of the technology.   \n747 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n748 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n749 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n750 feedback over time, improving the efficiency and accessibility of ML).   \n751 11. Safeguards   \n752 Question: Does the paper describe safeguards that have been put in place for responsible   \n753 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n754 image generators, or scraped datasets)?   \n755 Answer: [NA]   \n756 Justification: [TODO]   \n757 Guidelines:   \n758 \u2022 The answer NA means that the paper poses no such risks.   \n759 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n760 necessary safeguards to allow for controlled use of the model, for example by requiring   \n761 that users adhere to usage guidelines or restrictions to access the model or implementing   \n762 safety filters.   \n763 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n764 should describe how they avoided releasing unsafe images.   \n765 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n766 not require this, but we encourage authors to take this into account and make a best   \n767 faith effort.   \n768 12. Licenses for existing assets   \n769 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n770 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n771 properly respected?   \n772 Answer: [Yes]   \n773 Justification: [TODO]   \n774 Guidelines:   \n775 \u2022 The answer NA means that the paper does not use existing assets.   \n776 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n777 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n778 URL.   \n779 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n780 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n781 service of that source should be provided.   \n782 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n783 package should be provided. For popular datasets, paperswithcode.com/datasets   \n784 has curated licenses for some datasets. Their licensing guide can help determine the   \n785 license of a dataset.   \n786 \u2022 For existing datasets that are re-packaged, both the original license and the license of   \n787 the derived asset (if it has changed) should be provided.   \n788 \u2022 If this information is not available online, the authors are encouraged to reach out to   \n789 the asset\u2019s creators.   \n790 13. New Assets   \n791 Question: Are new assets introduced in the paper well documented and is the documentation   \n792 provided alongside the assets? ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 23}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "05 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n06 include the full text of instructions given to participants and screenshots, if applicable, as   \n07 well as details about compensation (if any)? ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 23}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: [TODO] ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 23}]