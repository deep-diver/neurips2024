[{"type": "text", "text": "Freya PAGE: First Optimal Time Complexity for Large-Scale Nonconvex Finite-Sum Optimization with Heterogeneous Asynchronous Computations ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Alexander Tyurin Kaja Gruntkowska Peter Richtarik KAUST\\* AIRI! Skoltech+ KAUST\\* KAUST\\* ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In practical distributed systems, workers are typically not homogeneous, and due to differences in hardware configurations and network conditions, can have highly varying processing times. We consider smooth nonconvex finite-sum (empirical risk minimization) problems in this setup and introduce a new parallel method, Freya PAGE, designed to handle arbitrarily heterogeneous and asynchronous computations. By being robust to \u201cstragglers\u201d and adaptively ignoring slow computations, Freya PAGE offers significantly improved time complexity guarantees compared to all previous methods, including Asynchronous SGD, Rennala SGD, SPIDER, and PAGE, while requiring weaker assumptions. The algorithm relies on novel generic stochastic gradient collection strategies with theoretical guarantees that can be of interest on their own, and may be used in the design of future optimization methods. Furthermore, we establish a lower bound for smooth nonconvex finite-sum problems in the asynchronous setup, providing a fundamental time complexity limit. This lower bound is tight and demonstrates the optimality of Freya PAGE in the large-scale regime, i.e., when ${\\sqrt{m}}\\geq n$ ,where $n$ is $\\#$ of workers, and $m$ is # of data samples. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In real-world distributed systems used for large-scale machine learning tasks, it is common to encounter device heterogeneity and variations in processing times among different computational units. These can stem from GPU computation delays, disparities in hardware configurations, network conditions, and other factors, resulting in different computational capabilities and speeds across devices [Chen et al., 2016, Tyurin and Richtarik, 2023]. As a result, some clients may execute computations faster, while others experience delays or even fail to participate in the training altogether. ", "page_idx": 0}, {"type": "text", "text": "Due to the above reasons, we aim to address the challenges posed by device heterogeneity in the context of solving finite-sum nonconvex optimization problems of the form ", "page_idx": 0}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{x\\in\\mathbb{R}^{d}}\\left\\{f(x):=\\frac{1}{m}\\sum_{i=1}^{m}f_{i}(x)\\right\\},\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "where $f_{i}:\\ensuremath{\\mathbb{R}^{d}}\\to\\ensuremath{\\mathbb{R}}$ can be viewed as the loss of a machine learning model $x$ on the $i^{\\mathrm{th}}$ example in a training dataset with $m$ samples. Our goal is to find an $\\varepsilon$ -stationary point, i.e., a (possibly random) point $\\hat{x}$ such that $\\mathbb{E}[\\|\\nabla f({\\hat{x}})\\|^{2}]\\leq\\varepsilon$ . We focus on the homogeneous distributed setup: ", "page_idx": 0}, {"type": "text", "text": "\u00b7 each worker has access to stochastic gradients $\\nabla f_{j},\\boldsymbol{j}\\in[m]$ \uff0c \u00b7 worker $i$ calculates $\\nabla f_{j}(\\cdot)$ in less or equal to $\\tau_{i}\\in[0,\\infty]$ seconds for all $i\\in[n],j\\in[m]$ ", "page_idx": 1}, {"type": "text", "text": "Without loss of generality, we assume that $\\tau_{1}\\,\\leq\\,.\\,.\\,\\leq\\,\\tau_{n}$ \uff0e One can think of $\\tau_{i}\\,\\in\\,[0,\\infty]$ as an upper bound on the computation time rather than a fixed deterministic time. Looking ahead, iteration complexity can be established even if $\\tau_{i}=\\infty$ for all $i\\in[n]$ (Theorem 4). We also provide results where the bounds $\\{\\tau_{i}^{k}\\}$ are dynamic and change with every iteration $k$ (Section 4.4). For simplicity of presentation, however, we assume that $\\tau_{i}^{k}=\\tau_{i}$ for $i\\in[n],k\\geq0$ , unless explicitly stated otherwise. ", "page_idx": 1}, {"type": "text", "text": "1.1  Assumptions ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We adopt two weak assumptions, which are standard for the problem (1) [Fang et al., 2018]. ", "page_idx": 1}, {"type": "text", "text": "Assumption 1. The function $f$ is $L_{-}$ -smooth and lower-bounded by $f^{*}\\in\\mathbb{R}$ ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{1}{m}\\displaystyle\\sum_{i=1}^{m}\\left\\|\\nabla f_{i}(x)-\\nabla f_{i}(y)\\right\\|^{2}\\leq L_{+}^{2}\\left\\|x-y\\right\\|^{2}\\forall x,y\\in\\mathbb{R}^{d}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "We also consider Assumption 3. Note that this assumption does not restrict the class of considered functions $\\{f_{i}\\}$ . Indeed, if Assumption 2 holds with $L_{+}$ , then Assumption 3 holds with some $L_{\\pm}\\leq$ $L_{+}$ . If one only wants to rely on Assumptions 1 and 2, it is sufficient to take $L_{\\pm}=L_{+}$ . However, Assumption 3 enables us to derive sharper rates, since $L_{\\pm}$ can be small or even O, even if $L_{-}$ and $L_{+}$ are large [Szlendak et al., 2021, Tyurin et al., 2023, Kovalev et al., 2022]. ", "page_idx": 1}, {"type": "text", "text": "Assumption 3 (Hessian variance [Szlendak et al., 2021]). There exists $L_{\\pm}\\ge0$ suchthat ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{1}{m}\\displaystyle\\sum_{i=1}^{m}\\|\\nabla f_{i}(x)-\\nabla f_{i}(y)-(\\nabla f(x)-\\nabla f(y))\\|^{2}\\leq L_{\\pm}^{2}\\left\\|x-y\\right\\|^{2}\\qquad\\forall x,y\\in\\mathbb{R}^{d}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "1.2  Gradient oracle complexities ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Iterative algorithms are traditionally evaluated based on their gradient complexity. Let us present a brief overview of existing theory. The classical result of Gradient Descent (GD) says that in the smooth nonconvex regime, the number of oracle calls needed to solve problem (1) is $\\mathcal{O}(m\\varepsilon^{-1})$ because GD converges in ${\\mathcal{O}}(\\varepsilon^{-1})$ iterations, and calculates the full gradient $\\begin{array}{r}{\\nabla f={1}/{m}\\sum_{i=1}^{m}\\nabla f_{i}}\\end{array}$ in each iteration. This was improved to $\\mathcal{O}(m+m^{2/3}\\varepsilon^{-1})$ by several variance-reduced methods, including SVRG and SCSG [Allen-Zhu and Hazan, 2016, Reddi et al., 2016, Lei et al., 2017, Horvath and Richtarik, 2019]. Since then, various other algorithms, such as SNVRG, SARAH, SPIDER, SpiderBoost, PAGE and their variants, have been developed [Fang et al., 2018, Wang et al., 2019, Nguyen et al., 2017, Li et al., 2021, Zhou et al., 2020, Horvath et al., 2022]. These methods achieve a gradient complexity of $O(m+\\sqrt{m}\\varepsilon^{-1})$ , matching the lower bounds [Fang et al., 2018, Li et al., 2021]. ", "page_idx": 1}, {"type": "text", "text": "That said, in practical scenarios, what often truly matters is the time complexity rather than the gradient complexity [Tyurin and Richtarik, 2023]. Although the latter metric serves as a natural benchmark for sequential methods, it seems ill-suited in the context of parallel methods. ", "page_idx": 1}, {"type": "text", "text": "1.3  Some previous time complexities ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Let us consider some examples to provide intuition about time complexities for problem (1). ", "page_idx": 1}, {"type": "text", "text": "GD with 1 worker (Hero GD). In principle, each worker can solve the problem on their own. Hence, one approach would be to select the fastest client (assuming it is known) and delegate the task to them exclusively. A well-known result says that for $L_{-}$ -smooth objective function $f$ (Assumption 1), GD converges in $\\delta^{0}L_{-}\\varepsilon^{-1}$ iterations, where $\\delta^{0}:=f(x^{0})-f^{*}$ , and $x^{0}$ is the starting point. Since at each iteration the method computes $m$ gradients $\\nabla f_{i}(\\cdot),i\\in[m].$ the time required to find an $\\varepsilon$ -stationary point is $\\delta^{0}L_{-}\\varepsilon^{-1}\\times m\\tau_{1}$ seconds. ", "page_idx": 1}, {"type": "text", "text": "GD with $n$ workers and equal data allocation (Soviet GD). The above strategy leaves the remaining $n-1$ workers idle, and thus potentially useful computing resources are wasted. A common approach is to instead divide the data into $n$ equal parts and assign one such part to each worker, so that each has to compute $m/n$ gradients (assuming for simplicity that $m$ is divisible by $n$ ). Since at each iteration the strategy needs to wait for the slowest worker, the total time is $\\delta^{0}\\bar{L_{-}}\\varepsilon^{-1}\\times m\\tau_{n}/n$ . Depending on the relationship between $\\tau_{1}$ and $\\tau_{n}/n$ , this could be more efficient or less efficient compared to Hero GD. This shows that the presence of stragglers can eliminate the potential speedup expected from parallelizing the training [Dutta et al., 2018]. ", "page_idx": 1}, {"type": "table", "img_path": "AUeTkSymOq/tmp/7893b61faf688adb5a829a53f410c4194d25959c9ef557a2da4e589fb76eea15.jpg", "table_caption": ["Table 1: Comparison of the worst-case time complexity guarantees of methods that work with asynchronous computations in the setup from Section 1 (up to smoothness constants). We assume that $\\tau_{i}\\in[0,\\infty]$ is the bound on the times required to calculate one stochastic gradient $\\nabla f_{j}$ by worker $i$ $\\tau_{1}\\leq...\\leq\\tau_{n}$ , and $m\\geq n\\log n$ Abbr: $\\delta^{0}:=f(\\dot{x}^{0})-f^{*}$ $m=\\#$ of data samples, $n=\\#$ of workers, $\\varepsilon=$ error tolerance. "], "table_footnote": ["Freya PAGE has universally better guarantees than all previous methods: the dependence on $\\varepsilon$ $\\bigcirc$ $(1/\\varepsilon)$ (unlike Rennala SGD and Asynchronous SGD), the dependence on $\\{\\tau_{i}\\}$ is harmonic-like and robust to slow workers (robust to $\\tau_{n}\\to\\infty)$ (unlike Soviet PAGE and SYNTHESIS), the assumptions are weak, and the time complexity of Freya PAGE is optimal when ${\\sqrt{m}}\\geq n$ ", "(@ In Line 3 of their Algorithm 3, they calculatethe full gradient, assuming that it can be done for free and not explaining how. (b)Theirconvergencerates inTheorems and3depend on abound on thedelays $\\Delta$ whichinturn depends onthe perfrmance of the slowest worker. Our method does not depend on the slowest worker if it is too slow (see Section 4.3), which is required for optimality. We prove bttertimecomplexity inTheorem6,but thisresult requires theknowledge of $\\{\\tau_{i}\\}$ in advance,unlikeTheorems 7and8. "], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "SPIDER/PAGE with 1 worker or $n$ workers and equal data allocation (Hero PAGE and Soviet PAGE). As mentioned in Section 1.2, SPIDER/PAGE can have better gradient complexity guarantees than GD. Using the result of Li et al. [2021], the equal data allocation strategy with $n$ workersleads to the time complexity of ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{T_{\\mathrm{Soviet\\,PAGE}}:=\\Theta\\left(\\tau_{n}\\operatorname*{max}\\left\\{\\frac{m}{n},1\\right\\}+\\tau_{n}\\frac{\\delta^{0}\\operatorname*{max}\\left\\{L_{-},L_{\\pm}\\right\\}}{\\varepsilon}\\operatorname*{max}\\left\\{\\frac{\\sqrt{m}}{n},1\\right\\}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "seconds. We refer to this method as Soviet PAGE. In practical regimes, when $\\varepsilon$ is small and $L_{-}\\approx L_{\\pm}$ this complexity can be $\\sqrt{m}$ better than that of GD. Running PAGE on the fastest worker (which we will call Hero PAGE), we instead get the time complexity $\\breve{T_{\\mathrm{Hero\\,PAGE}}}\\,:=\\,\\Theta\\left(\\tau_{1}m+\\tau_{1}\\delta^{0}/\\varepsilon\\sqrt{m}\\right)$ ", "page_idx": 2}, {"type": "text", "text": "Given these examples, the following question remains unanswered: what is the best possible time complexity in our setting? This paper aims to answer this question. ", "page_idx": 2}, {"type": "text", "text": "2 Contributions ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We consider the finite-sum optimization problem (1) under weak assumptions and develop a new method, Freya PAGE. The method works with arbitrarily heterogeneous and asynchronous computations on the clients without making any assumptions about the bounds on the processing times $\\tau_{i}$ .We show that the time complexity of Freya PAGE is provably better than that of all previously proposed synchronous/asynchronous methods (Table 1). Moreover, we prove a lower bound that guarantees optimality of Freya PAGE in the large-scale regime $({\\sqrt{m}}\\geq n)$ . The algorithm leverages new computation strategies, ComputeGradient (Alg. 2) and ComputeBatchDifference (Alg. 3), which are generic ", "page_idx": 2}, {"type": "text", "text": "1: Parameters: starting point $x^{0}\\in\\mathbb{R}^{d}$ , learning rate $\\gamma>0$ , minibatch size $S\\in\\mathbb{N}$ probability $p\\in(0,1]$ , initialization $g^{0}=\\nabla f(x^{0})$ using ComputeGradient $(x^{0})$ (Alg. 2)   \n2: for $k=0,1,\\ldots,K-1$ do   \n3: $x^{k+1}=x^{k}-\\gamma g^{k}$   \n4: Sample ck \\~ Bernoulli $(p)$   \n5: if $c^{k}=1$ then (with probability p)   \n6: $\\begin{array}{r l}&{\\nabla f(x^{k+1})=\\mathsf{C o m p u t e G r a d i e n t}(x^{k+1})}\\\\ &{g^{k+1}=\\nabla f(x^{k+1})}\\end{array}$ (Alg. 2)   \n7:   \n8: else (with probability. $1-p)$   \n9: \u2265(Vf;(xk+1) -Vf;(x)\uff09 = ComputeBatchDifference $(S,x^{k+1},\\Bar{x}^{k})$ (Alg. 3) iESk   \n10: g+= g+(Vf(x+1)-Vf(x) iESk   \n11: end if   \n12: end for (note): $S^{k}$ is a set of i.i.d. indices that are sampled from $[m]$ , uniformly with replacement, $\\left|S^{k}\\right|=S$ ", "page_idx": 3}, {"type": "text", "text": "and can be used in any other asynchronous method. These strategies enable the development of our new SGD method (Freya SGD); see Sections 6 and H. Experiments from Section A on synthetic optimization problems and practical logistic regression tasks support our theoretical results. ", "page_idx": 3}, {"type": "text", "text": "3  The Design of the New Algorithm ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "It is clear that to address the challenges arising in the setup under consideration and achieve optimality, a distributed algorithm has to adapt to and effectively utilize the heterogeneous nature of the underlying computational infrastructure. With this in mind, we now present a new algorithm, Freya PAGE, that can efficiently coordinate and synchronize computations across the $n$ devices, accommodating arbitrarily varying processing speeds, while mitigating the impact of slow devices or processing delays on the overall performance of the system. ", "page_idx": 3}, {"type": "text", "text": "Freya PAGE is formalized in Algorithm 1. The update rule is just the regular PAGE [Li et al., 2021] update: at each iteration, with some (typically small) probability $p$ , the algorithm computes the full gradient $\\nabla f(x^{k+1})$ , and otherwise, it samples a minibatch $S^{k}$ of size $S$ and reuses the gradient estimator $g^{k}$ from the previous iteration, updated by the cheaper-to-compute adjustment $\\frac{1}{S}\\sum_{i\\in S^{k}}$ $\\daleth_{i\\in S^{k}}\\left(\\nabla f_{i}(x^{k+1})-\\nabla f_{i}(x^{k})\\right)$ ", "page_idx": 3}, {"type": "text", "text": "Within Algorithm 1, at each iteration we call one of two subroutines: ComputeGradient (Alg. 2, performing the low-probability step), and ComputeBatchDifference (Alg. 3, performing the highprobability step). Let us focus on ComputeGradient, designed to collect the full gradient: it takes a point $x$ asiput andreturns $\\begin{array}{r}{\\nabla f(x)=\\frac{1}{m}\\sum_{i=1}^{m}{\\nabla f_{i}(x)}}\\end{array}$ Therexst many strategies forimplementing this calculation, some of which were outlined in Section 1.3. The most naive one is to assign the task of calculating the whole gradient $\\nabla f$ to a single worker $i$ , resulting in a worst-case running time of $m\\tau_{i}$ seconds for ComputeGradient. Another possible strategy is to distribute the functions $\\bar{\\{f_{i}\\}}$ evenly among the workers; in this case, calculating $\\nabla f$ takes $\\tau_{n}\\operatorname*{max}\\{m/n,1\\}$ seconds in the worst case. ", "page_idx": 3}, {"type": "text", "text": "Clearly, we could do better if we knew $\\{\\tau_{i}\\}$ in advance. Indeed, let us allocate to each worker $j$ a number of functions $\\{f_{i}\\}$ inversely proportional to $\\tau_{j}$ . This strategy is reasonable - the faster the worker, the more gradients it can compute. We can show that such a strategy finds $\\nabla f$ in ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\Theta\\left(\\operatorname*{min}_{j\\in[n]}\\left(\\left(\\sum_{i=1}^{j}\\frac{1}{\\tau_{i}}\\right)^{-1}(m+j)\\right)\\right)\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "seconds in the worst case (see the proof of Theorem 2). This complexity is better than $m\\tau_{1}$ and $\\tau_{n}\\operatorname*{max}\\{m/n,1\\}$ (Theorem 31). However, this approach comes with two major limitations: i) it requires knowledge of the upper bounds $\\{\\tau_{i}\\}$ , i) even if we have access to $\\{\\tau_{i}\\}$ , thecomputation environment can be adversarial: theoretically and practically, it is possible that at the beginning the first worker is the fastest and the last worker is the slowest, but after some time, their performances swap. Consequently, the first worker might end up being assigned the largest batch, despite now having the lowest performance. Thus, this strategy is not robust to time-varying speeds. ", "page_idx": 3}, {"type": "table", "img_path": "AUeTkSymOq/tmp/d48d6bbf67dc0a945a148a743617c6e855242ce96f98cb403cfe89d214675532.jpg", "table_caption": [], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "New gradient computation strategy.  The key innovation of this work lies in the introduction of new computation strategies: Algorithms 2 and 3. We start by examining Algorithm 2. It first broadcasts the input $\\boldsymbol{x}\\in\\mathbb{R}^{\\breve{d}}$ to all workers. Then, for each worker, it samples $j$ uniformly from $[m]$ and asks it to calculate $\\nabla f_{j}(x)$ (with a non-zero probability, two workers can be assigned the same computation). Then, the algorithm enters the loop and waits for any worker to finish their calculations. Once this happens, it asks this worker to compute a stochastic gradient with a new index sampled uniformly from the set $[m]\\backslash{\\mathcal{M}}$ of indices that have not yet been processed (again, it is possible to resample an index previously assigned to another worker). This continues until all indices $i\\in[m]$ have been processed and the full gradient $\\textstyle{\\frac{1}{m}}\\sum_{i=1}^{m}\\nabla f_{i}$ has been collected. Unlike the previous strategies, our Algorithm 2 does not use $\\{\\tau_{i}\\}$ ,thus being robust and adaptive to the changing compute times. Furthermore, we can prove that its time complexity (almost) equals (3): ", "page_idx": 4}, {"type": "text", "text": "Theorem1. TheexpetedienedbyAgorit talcul $g=\\textstyle{\\frac{1}{m}}\\sum_{i=1}^{m}\\nabla f_{i}$ is at most ", "page_idx": 4}, {"type": "equation", "text": "$$\n12\\operatorname*{min}_{j\\in[n]}\\left(\\left(\\sum_{i=1}^{j}\\frac{1}{\\tau_{i}}\\right)^{-1}(m+\\operatorname*{min}\\{m,n\\}\\log\\left(\\operatorname*{min}\\{m,n\\}\\right)+j)\\right)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "seconds. ", "page_idx": 4}, {"type": "text", "text": "The result (4) (the proof of which can be found in Section C) is slightly worse than (3) due to theextra $\\operatorname*{min}\\{m,n\\}\\log\\left(\\operatorname*{min}\\{m,n\\}\\right)$ term. This term arises because a worker may be assigned a gradient $\\nabla f_{j}(x)$ that was previously assigned to another worker (in Line 11 of Algorithm 2). Hence, with a small (but non-zero) probability, two workers can perform the same calculations. However, typically the number of samples $m$ is much larger than the number of workers $n$ . If we assume that $m\\geq n\\log n$ , which is satisfied in many practical scenarios, then the time complexity (4) equals ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\Theta\\left(\\operatorname*{min}_{j\\in[n]}\\left(\\left(\\sum_{i=1}^{j}\\frac{1}{\\tau_{i}}\\right)^{-1}(m+j)\\right)\\right)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "and the term $\\operatorname*{min}\\{m,n\\}\\log\\left(\\operatorname*{min}\\{m,n\\}\\right)$ never dominates. Since this complexity is not worse than (3), our strategy behaves as if it knew $\\{\\tau_{i}\\}$ in advance! To simplify formulas and avoid the logarithmic term, we use the following assumption throughout the main part of this paper: ", "page_idx": 4}, {"type": "text", "text": "Assumption 4. $m\\geq n\\log n$ , where m is the # of data samples and n is the # of workers. ", "page_idx": 4}, {"type": "text", "text": "We now proceed to discuss ComputeBatchDifference (Algorithm 3), designed to compute a minibatch of stochastic gradient differences. Both Algorithms 2 and 3 calculate sums. However the latter only waitsuntil thereare $S$ samples in the sum, where some indices in the batch may not be unique. On the other hand, Algorithm 2 must ensure the collection of a full batch of $m$ unique stochastic gradients. As a result, Algorithm 3 offers better complexity results and, unlike ComputeGradient, does not suffer from suboptimal guarantees and logarithmic terms, as demonstrated in the theorem below. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Theorem 2. The time needed by Algorithm $^3$ tocalculate $g$ is at most ", "page_idx": 5}, {"type": "equation", "text": "$$\n4\\operatorname*{min}_{j\\in[n]}\\left(\\left(\\sum_{i=1}^{j}{\\frac{1}{\\tau_{i}}}\\right)^{-1}(S+j)\\right)\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "seconds. ", "page_idx": 5}, {"type": "text", "text": "Algorithm 1 uses uniform sampling with replacement, implemented in Algorithm 3. However, one can modify the two algorithms slightly to support virtually any unbiased sampling (see Section E). ", "page_idx": 5}, {"type": "text", "text": "Note on asynchronicity. It is clear that to eliminate the need of waiting for very slow machines, some level of asynchronicity has to be injected into an algorithm for it to be efficient. Asynchronous SGD [Recht et al., 2011, Nguyen et al., 2018, Koloskova et al., 2022, Mishchenko et al., 2022] takes this concept to the extreme by never synchronizing and continually overwriting the updates. Consequently, the algorithm's time complexity is suboptimal. Conversely, imposing limitations on asynchronicity leads to optimal methods, both in our context (in the large-scale regime) and in the scenario examined by Tyurin and Richtarik [2023]. Freya PAGE seamlessly combines synchrony and asynchrony, getting the best out of the two worlds. ", "page_idx": 5}, {"type": "text", "text": "4  Time Complexities and Convergence Rates ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Formulas (3) and (4) will be used frequently throughout the paper. To lighten up the heavy notation, let us define the following mapping. ", "page_idx": 5}, {"type": "text", "text": "Definition 3 (Equilibrium time). A mapping $t^{*}:\\mathbb{R}_{\\geq0}\\times\\mathbb{R}_{\\geq0}^{n}\\rightarrow\\mathbb{R}_{\\geq0}$ defined by ", "page_idx": 5}, {"type": "equation", "text": "$$\nt^{*}(S,[\\bar{\\tau}_{i}]_{i=1}^{n}):=\\operatorname*{min}_{j\\in[n]}\\left(\\left(\\sum_{i=1}^{j}\\frac{1}{\\bar{\\tau}_{i}}\\right)^{-1}(S+j)\\right)\\in[0,\\infty]\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "is called the equilibrium time. We let $t^{*}(S)\\equiv t^{*}(S,[\\tau_{i}]_{i=1}^{n})$ when considering $\\{\\tau_{i}\\}$ from Section 1. ", "page_idx": 5}, {"type": "text", "text": "Returning to the algorithm, we guarantee the following iteration complexity. ", "page_idx": 5}, {"type": "text", "text": "Theorem 4 (Iteration complexity). Let Assumptions 1, 2 and $^3$ hold. Consider any minibatch size $S\\in\\mathbb{N}:=\\{1,2,\\ldots\\}$ any probability $p\\in(0,1]$ , and let thestepsize be $\\begin{array}{r}{\\gamma=\\left(L_{-}+L_{\\pm}\\sqrt{\\frac{1-p}{p S}}\\right)^{-1}}\\end{array}$ Then,after ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{K\\ge K_{\\mathsf{P A G E}}:=\\frac{2\\delta^{0}}{\\varepsilon}\\left(L_{-}+L_{\\pm}\\sqrt{\\frac{1-p}{p S}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "iterations of Algorithm $^{\\,l}$ we have $\\mathbb{E}\\left[\\left\\|\\nabla f({\\hat{x}}^{K})\\right\\|^{2}\\right]\\leq\\varepsilon_{}$ where $\\hat{x}^{K}$ is sampled uniformly at random from the iterates $\\{x^{0},\\ldots,x^{K-1}\\}$ ", "page_idx": 5}, {"type": "text", "text": "Theorem 4 states that the iteration complexity is the same as in the optimal PAGE method [Li et al., 2021]. Note that we can guarantee convergence even if the upper bounds $\\{\\tau_{i}\\}$ are unknown or infinite (as long as there exists some worker that can complete computations within a finite time). ", "page_idx": 5}, {"type": "text", "text": "We now derive time complexity guarantees. With probability $p$ the workers need to supply to the algorithm $m$ stochastic gradients at each of the $m$ data samples, which by Theorem 1 can be done in at most $t^{*}(m)$ seconds (up to a log factor). Otherwise, they compute $S$ differences of stochastic gradients, which by Theorem 2 takes at most $t^{*}(S)$ seconds (up to a constant factor). The resulting time complexity is given in the theorem below. ", "page_idx": 5}, {"type": "text", "text": "Theorem 5 (Time complexity with free parameters $p$ and $S$ ).Consider theassumptions and the parameters from Theorem 4, plus Assumption 4.The expected time complexity of Algorithm $^{\\,l}$ isat most ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{T(p,S,[\\tau_{i}]_{i=1}^{n}):=12\\cdot t^{*}(m,[\\tau_{i}]_{i=1}^{n})}\\\\ &{\\quad+\\,\\frac{48\\delta^{0}}{\\varepsilon}\\left(L_{-}+L_{\\pm}\\sqrt{\\frac{1-p}{p S}}\\right)\\times\\{p\\cdot t^{*}(m,[\\tau_{i}]_{i=1}^{n})+(1-p)\\cdot t^{*}(S,[\\tau_{i}]_{i=1}^{n})\\}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The first term comes from the preprocessing step, where the full gradient is calculated to obtain $g^{0}=\\nabla f(x^{0})$ . Here, we use Assumption 4 that $m\\geq n\\log n$ . The result (8) is valid even without this assumption, but at the cost of extra logarithmic factors. ", "page_idx": 6}, {"type": "text", "text": "4.1  Optimal parameters $S$ and $p$ ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "The time complexity (8) depends on two free parameters, $S\\in\\mathbb{N}$ and $p\\in(0,1]$ . The result below (following from Theorems 13 and 14) determines their optimal choice. ", "page_idx": 6}, {"type": "text", "text": "Theorem 6 (Main result). Consider the assumptions and parameters from Theorem $^{4}$ plusAssumption 4. Up to a constant factor, the time complexity (8) is at least $T([\\tau_{i}]_{i=1}^{n}):=t^{*}(m,[\\tau_{i}]_{i=1}^{n})$ ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{+\\frac{\\delta^{0}}{\\varepsilon}\\operatorname*{min}\\Bigg\\{L_{-}t^{*}(m,[\\tau_{i}]_{i=1}^{n}),\\underset{S\\in\\mathbb{N}}{\\operatorname*{min}}\\underbrace{\\left[L_{-}t^{*}(S,[\\tau_{i}]_{i=1}^{n})+L_{\\pm}\\frac{\\sqrt{t^{*}(m,[\\tau_{i}]_{i=1}^{n})t^{*}(S,[\\tau_{i}]_{i=1}^{n})}}{\\sqrt{S}}\\right]}_{F(S):=\\,}\\Bigg\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "and this lower bound is achieved with $S^{*}=\\arg\\operatorname*{min}_{S\\in\\mathbb{N}}F(S)$ and $p^{*}=p^{*}(S^{*})$ ,where ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{\\varphi}^{*}(S)=\\left\\{\\underset{t^{*}(D,[\\tau_{i}]_{i=1}^{n})}{\\mathrm{if}}~\\mathrm{~}L_{-}t^{*}(m,[\\tau_{i}]_{i=1}^{n})\\le L_{-}t^{*}(S,[\\tau_{i}]_{i=1}^{n})+L_{\\pm}\\frac{\\sqrt{t^{*}(m,[\\tau_{i}]_{i=1}^{n})t^{*}(S,[\\tau_{i}]_{i=1}^{n})}}{\\sqrt{S}},\\right.}\\\\ {\\left.\\frac{t^{*}(S,[\\tau_{i}]_{i=1}^{n})}{t^{*}(m,[\\tau_{i}]_{i=1}^{n})},\\right.\\ \\mathrm{~otherwise}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Result (9) is the best possible time complexity that can be achieved with the Freya PAGE method. Unfortunately, the final time complexity has non-trivial structure, and the optimal parameters depend on $\\{\\tau_{i}\\}$ in the general case. If we have access to all parameters and times $\\{\\tau_{i}\\}$ , then (9), $S^{*}$ , and $p^{*}$ can be computed efficiently. Indeed, the main problem is to find $\\mathrm{min}_{S\\in\\mathbb{N}}\\,F(S)$ , which can be solved, for instance, by using the bisection method, because $L_{-}t^{*}(S,[\\tau_{i}]_{i=1}^{n})$ is non-decreasing and $L_{\\pm}\\sqrt{t^{*}(m,[\\tau_{i}]_{i=1}^{n})t^{*}(S,[\\tau_{i}]_{i=1}^{n})}/\\sqrt{S}$ is non-increasing in $S$ ", "page_idx": 6}, {"type": "text", "text": "4.2  Optimal parameters $S$ and $p$ in the large-scale regime ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Surprisingly, we can significantly simplify the choice of the optimal parameters $S$ and $p$ in the large-scale regime, when ${\\sqrt{m}}\\geq{\\dot{n}}$ . This is a weak assumption, since typically the number of data samples is much larger than the number of workers. ", "page_idx": 6}, {"type": "text", "text": "Theorem 7 (Main result in the large-scale regime). Consider the assumptions and parameters from Theorem 4, plus Assumption 4. Up to a constant factor and smoothness constants, i $f{\\sqrt{m}}\\geq n$ ,then the optimal choice of parameters in (8) is $S^{*}=\\lceil{\\sqrt{m}}\\rceil$ and $p^{*}=1/\\sqrt{m}$ For this choice, the expected time complexity of Algorithm $^{\\,l}$ isatmost ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{T(^{1}/\\sqrt{m},\\sqrt{m},[\\tau_{i}]_{i=1}^{n})=12t^{*}(m,[\\tau_{i}]_{i=1}^{n})+\\frac{192\\delta^{0}\\operatorname*{max}\\{L_{-},L_{\\pm}\\}}{\\varepsilon}t^{*}(\\sqrt{m},[\\tau_{i}]_{i=1}^{n})}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "seconds. The iteration complexity with $S=\\lceil{\\sqrt{m}}\\rceil$ and $p=1/\\sqrt{m}$ $K_{\\sf P A G E}\\leq4\\delta^{0}\\operatorname*{max}\\{L-,L\\pm\\}/\\varepsilon.$ ", "page_idx": 6}, {"type": "text", "text": "We cannot guarantee that $S=\\lceil{\\sqrt{m}}\\rceil$ and $p={^{1}\\!/}{\\sqrt{m}}$ is the optimal pair when ${\\sqrt{m}}<n$ , but it is a valid choice for all $m\\geq1$ . Note that (10) is true if $m\\geq n\\log n$ , and it is true up to a log factor if $m<n\\log n$ . In light of Theorem 15, we can further refine Theorem 7 if the ratio $L_{\\pm}/L$ is known: ", "page_idx": 6}, {"type": "text", "text": "Theorem 8 (Main result in the large-scale regime using the ratio $L\\pm{\\mathord{\\left/{\\vphantom{L\\pm{L x}}}\\right.\\kern-\\nulldelimiterspace}L}$ ).Consider the assumptions and parameters fromTheorem 4,plus Assumption 4.The expected time complexity ofAlgorithm $^{\\,l}$ isatmost $\\Theta(t^{*}(\\bar{m},[\\tau_{i}]_{i=1}^{n})+\\delta^{0}L_{-}^{-}/\\varepsilon\\times t^{*}(\\operatorname*{min}\\{\\operatorname*{max}\\{\\lceil L\\pm\\sqrt{m}/L_{-}\\rceil\\,,1\\},m\\},[\\tau_{i}]_{i=1}^{n}))$ seconds,where $S=\\operatorname*{min}\\{\\operatorname*{max}\\{\\lceil L\\pm\\sqrt{m}/L_{-}\\rceil\\,,1\\},m\\}$ and $p=S/{m}$ ", "page_idx": 6}, {"type": "text", "text": "For brevity reasons, we will continue working with the result from Theorem 7 in the main part of this paper. Note that the optimal parameters do not depend on $\\{\\tau_{i}\\}$ , and can be easily calculated since the number of functions $m$ is known in advance. Hence, our method is fully adaptive to changing and heterogeneouscomputetimesof theworkers. ", "page_idx": 6}, {"type": "text", "text": "Even if the bounds are unknown and $\\tau_{i}~=~\\infty$ for all $\\textit{i}\\in\\ [n]$ , our method converges after $4\\delta^{0}\\operatorname*{max}\\{L_{-},L_{\\pm}\\}/\\varepsilon$ iterations, and calculates the optimal number of stochastic gradients equal to $\\mathcal{O}(m+\\sqrt{m}\\delta^{0}\\operatorname*{max}\\{L_{-},L_{\\pm}\\}/\\varepsilon)$ ", "page_idx": 6}, {"type": "text", "text": "4.3  Discussion of the time complexity ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Let us use Definition 3 and unpack the second term in the complexity (10), namely. ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\frac{\\delta^{0}\\operatorname*{max}\\{L_{-},L_{\\pm}\\}}{\\varepsilon}\\operatorname*{min}_{j\\in[n]}\\left(\\left(\\sum_{i=1}^{j}\\frac{1}{\\tau_{i}}\\right)^{-1}\\big(\\sqrt{m}+j\\big)\\right).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "A somewhat similar expression involving $\\operatorname*{min}_{j\\in[n]}$ and harmonic means was obtained by Tyurin and Richtarik [2023], Tyurin et al. [2024] for minimizing expectation under the bounded variance assumption. The term $\\delta^{0}\\operatorname*{max}\\{L_{-},L_{\\pm}\\}/\\varepsilon$ is standard in optimization [Nesterov, 2018, Lan, 2020] and describes the dificulty of the problem (1). The term $\\begin{array}{r}{\\operatorname*{min}_{j\\in[n]}((\\sum_{i=1}^{j}1/\\tau_{i})^{-1}(\\sqrt{m}+j))}\\end{array}$ represents the average time of one iteration and has some nice properties. For instance, if the last worker is slow and $\\tau_{n}~\\approx~\\infty$ then $\\begin{array}{r}{\\operatorname*{min}_{j\\in[n]}(\\cdot\\cdot\\cdot)~=~\\operatorname*{min}_{j\\in[n-1]}(\\cdot\\cdot\\cdot)}\\end{array}$ , so the complexity effectively ignores it. Moreover, if $j^{*}$ is an index that minimizes $\\operatorname*{min}_{j\\in[n]}(\\cdot\\cdot\\cdot)$ , then $\\begin{array}{r}{\\operatorname*{min}_{j\\in[n]}((\\sum_{i=1}^{j}1/\\tau_{i})^{-1}(\\sqrt{m}+j))=((\\sum_{i=1}^{j^{\\ast}}1/\\tau_{i})^{-1}(\\sqrt{m}+j^{\\ast}))}\\end{array}$ . The last formula, again, does not depend on the slowest workers $\\{j^{*}+1,\\ldots,n\\}$ , which are automatically excluded from the time complexity expression. The same reasoning applies to the term $t^{*}(m,[\\tau_{i}]_{i=1}^{n})$ . Let us now consider some extreme examples which are meant to shed some light on our time complexity result (10): ", "page_idx": 7}, {"type": "text", "text": "Example 1. [Equally Fast Workers] Suppose that the upper bounds on the processing times are equal, i.e., $\\tau_{j}=\\tau$ forall $j\\in[n]$ Then ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{T(^{1/}\\sqrt{m},\\sqrt{m},[\\tau_{i}]_{i=1}^{n})=\\Theta\\left(\\tau\\operatorname*{max}\\left\\{\\frac{m}{n},1\\right\\}+\\tau\\frac{\\delta^{0}\\operatorname*{max}\\{L_{-},L_{\\pm}\\}}{\\varepsilon}\\operatorname*{max}\\left\\{\\frac{\\sqrt{m}}{n},1\\right\\}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "The complexity in Example 1 matches that in (2), which makes sense since Soviet PAGE is a reasonable methodwhen $\\{\\tau_{i}\\}$ are equal. Note that the reduction happens without prior knowledge of $\\{\\tau_{i}\\}$ ", "page_idx": 7}, {"type": "text", "text": "Example 2.[Infinitely Fast Worker] If $\\tau_{1}=0$ .then $T(1/\\sqrt{m},\\sqrt{m},[\\tau_{i}]_{i=1}^{n})=0$ ", "page_idx": 7}, {"type": "text", "text": "Example 3. [Infinitely Slow Workers] If $\\tau_{j}=\\infty\\forall j\\in[n]$ then $T(1/\\sqrt{m},\\sqrt{m},[\\tau_{i}]_{i=1}^{n})=\\infty.$ ", "page_idx": 7}, {"type": "text", "text": "Example 4. [Extremely Slow Workers] Suppose that the times $\\tau_{j}\\,<\\,\\infty$ are fixed $\\forall j\\:\\leq\\:j_{B}$ and $\\tau_{j}\\geq B\\,\\forall j>j_{B}$ for some $B$ large enough. Then $T(1/\\sqrt{m},\\sqrt{m},[\\tau_{i}]_{i=1}^{n})=T(1/\\sqrt{m},\\sqrt{m},[\\tau_{i}]_{i=1}^{j_{B}})$ ", "page_idx": 7}, {"type": "text", "text": "Example 4 says that the workers whose processing time is too large are ignored, which supports the discussion preceding the examples. ", "page_idx": 7}, {"type": "text", "text": "4.4 Dynamic bounds ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "It turns out that the guarantes from Theorem 7 can be generalized to the situation where the compute times $\\{\\tau_{i}\\}$ are allowed to dynamically change throughout the iterations. Consider the $k^{\\mathrm{th}}$ iteration of Algorithm 1 and assume that worker $i$ calculates one $\\nabla f_{j}$ in at most $\\tau_{i}^{k}\\ \\in\\ [0,\\infty]$ seconds $\\forall i\\in[n],j\\in[m]$ . Clearly, $\\mathrm{max}_{k\\geq-1}\\,\\tau_{i}^{k}\\,\\leq\\,\\tau_{i}\\,\\,\\forall i\\,\\in\\,[n]$ (where $\\{\\tau_{i}^{-1}\\}$ are upper bounds from the preprocessing step), but $\\tau_{i}^{k}$ can be arbitrarily smaller than $\\tau_{i}$ (possibly $\\tau_{i}^{k}=0$ and $\\tau_{i}=\\infty$ ", "page_idx": 7}, {"type": "text", "text": "Theorem 9. Consider the assumptions and parameters from Theorem 4, plus Assumption 4. Up to $a$ constant factor, the expected time complexity of Algorithm $^{\\,l}$ With $S=\\bar{\\lceil}\\sqrt{m}\\rceil$ and $p={^{1}\\!/}{\\sqrt{m}}$ is at most ", "page_idx": 7}, {"type": "equation", "text": "$$\nt^{*}(m,[\\tau_{\\pi_{-1,i}}^{-1}]_{i=1}^{n})+\\sum_{k=0}^{\\left\\lceil4\\delta^{0}\\operatorname*{max}\\{L_{-},L_{\\pm}\\}/\\varepsilon\\right\\rceil}t^{*}(\\sqrt{m},[\\tau_{\\pi_{k,i}}^{k}]_{i=1}^{n}),\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\pi_{k_{\\mathrm{i}}}$ . is a permutation such that $\\tau_{\\pi_{k,1}}^{k}\\leq\\cdots\\leq\\tau_{\\pi_{k,n}}^{k}$ for all $k\\ge-1$ (This theorem follows from Theorem 22 with the chosen parameters). ", "page_idx": 7}, {"type": "text", "text": "Hence, our algorithm is adaptive to the dynamic compute times $\\{\\tau_{i}^{k}\\}$ . Let us consider an example with 2 workers. Assume that the first worker is stable: $\\tau_{1}^{k}=\\tau$ for all $k\\geq0$ , and the second worker is unstable: $\\tau_{2}^{0}=\\tau$ is small in the frst iteration, and $\\tau_{2}^{1}\\approx\\infty$ in the second iteration. For explanation purposes, we ignore the preprocessing term $t^{*}(m,\\cdot)$ , which is not a factor if $\\varepsilon$ is small. Then, ", "page_idx": 7}, {"type": "equation", "text": "$$\n(11)=t^{*}\\big(\\sqrt{m},[\\tau_{\\pi_{0,1}}^{0},\\tau_{\\pi_{0,2}}^{0}]\\big)+t^{*}\\big(\\sqrt{m},[\\tau_{\\pi_{1,1}}^{1},\\tau_{\\pi_{1,2}}^{1}]\\big)+\\ldots=t^{*}\\big(\\sqrt{m},[\\tau,\\tau]\\big)+t^{*}\\big(\\sqrt{m},[\\tau]\\big)+\\ldots\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "because $t^{*}(\\sqrt{m},[\\tau_{\\pi_{1,1}}^{1},\\tau_{\\pi_{1,2}}^{1}])=t^{*}(\\sqrt{m},[\\tau])$ when $\\tau_{2}^{1}\\approx\\infty$ . The time complexity in the second iteration depends on the first (stable) worker only, which is reasonable since $\\tau_{2}^{1}\\approx\\infty$ , and this happens automatically. At the same time, the first term $t^{*}(\\sqrt{m},[\\tau,\\tau])$ depends on both workers, and this iteration will be faster because $t^{*}(\\sqrt{m},[\\tau,\\tau])\\leq t^{*}(\\dot{\\sqrt{m}},[\\dot{\\tau}])$ ", "page_idx": 8}, {"type": "text", "text": "4.5  Comparison with previous strategies from Section 1.3 ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Our time complexities (9) and (10) are better than all known previous guarantees if $m\\geq n\\log n$ In particular, $\\bar{T}(1/\\sqrt{m},\\sqrt{m},[\\tau_{i}]_{i=1}^{n})\\,\\leq\\,T_{\\mathrm{Soviet\\,PAGE}}$ (from (2), because $\\bar{t}^{*}(\\sqrt{m},[\\tau_{i}]_{i=1}^{n})\\lesssim\\sqrt{m}\\bar{\\tau_{n}}/n$ (Theorem 31). In fact, since $\\begin{array}{r}{\\operatorname*{lim}_{\\tau_{n}\\to\\infty}t^{*}(\\sqrt{m},[\\tau_{i}]_{i=1}^{n})=t^{*}(\\sqrt{m},[\\tau_{i}]_{i=1}^{n-1})<\\infty}\\end{array}$ and $\\dim_{\\tau_{n}\\to\\infty}=$ $\\sqrt{m}\\tau_{n}/n=\\infty$ Tsoviet PAGE can be arbitrily larger. We also improve on Hero PAGE (see Remark 32). ", "page_idx": 8}, {"type": "text", "text": "4.6  Comparison with existing asynchronous variance reduced methods ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Several studies have explored asynchronous variance reduced algorithms. Essentially all of them are variants of the existing synchronous methods discussed in Section 1.2 and depend on the slowest worker in every iteration. There have been several attempts to combine variance reduction techniques with asynchronous computations. Perhaps the most relevant baseline is SYNTHESIS, an asynchronous variant of SPIDER [Fang et al., 2018] introduced by Liu et al. [2022]. The obtained gradient complexity matches that of SPIDER in terms of dependence on $m$ , but scales linearly with the bound on the time performance of the slowest worker, making it non-adaptive to slow computations. Moreover, in Line 3 of their Algorithm 3, the full gradient is calculated, assuming that it can be done for free. Lastly, the analysis assumes the gradients to be bounded. ", "page_idx": 8}, {"type": "text", "text": "5 Lower Bound ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In previous sections, we showed that Freya PAGE converges in at most (9) or (10) seconds, providing better time complexity guarantees compared to all previous methods. The natural question is: how good are these time complexities, and can they be improved? In Section J, we formalize our setup and prove Theorems 29 and 30, which collectively yield the following lower bound. ", "page_idx": 8}, {"type": "text", "text": "Theorem 10 (Less formal version of Theorems 29 and 30). Assume that $0<\\tau_{1}\\leq\\dots\\leq\\tau_{n}$ and take any $L_{+},\\delta^{0},\\varepsilon>0$ and $m\\in\\mathbb{N}$ such that $\\varepsilon<c_{1}L_{+}\\delta^{0}$ Then, for any (zero-respecting) algorithm, there existsafunction $f$ that satisfies $f(0)-f^{*}\\leq\\delta^{0}$ and Assumption 2, such that it is impossible to find an $\\varepsilon$ -stationarypointfasterthanin ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Omega\\left(t^{*}(m,[\\tau_{i}]_{i=1}^{n})+\\frac{\\delta^{0}L_{+}}{\\sqrt{m}\\varepsilon}t^{*}(m,[\\tau_{i}]_{i=1}^{n})\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "seconds using uniform sampling with replacement. ", "page_idx": 8}, {"type": "text", "text": "Comparing (10) and (12), we see that Freya PAGE is optimal under Assumptions 1 and 2 in the large-scale regime $({\\sqrt{m}}\\ \\geq\\ n)$ . Indeed, without Assumption 3, we have $\\operatorname*{max}\\{L_{-},L_{\\pm}\\}~=$ $\\operatorname*{max}\\{\\bar{L}_{-},L_{+}\\}=\\bar{L}_{+}$ . Up to constant factor, (10) is less or equal to (12) since ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{t^{*}(\\sqrt{m},[\\tau_{i}]_{i=1}^{n})=\\underset{j\\in[n]}{\\operatorname*{min}}\\left(\\left(\\sum_{i=1}^{j}\\frac{1}{\\tau_{i}}\\right)^{-1}(\\sqrt{m}+j)\\right)=\\frac{1}{\\sqrt{m}}\\underset{j\\in[n]}{\\operatorname*{min}}\\left(\\left(\\sum_{i=1}^{j}\\frac{1}{\\tau_{i}}\\right)^{-1}(m+\\sqrt{m}j)\\right)}\\\\ &{\\qquad\\qquad\\qquad\\sqrt{m}\\geq n\\underset{j\\in[n]}{\\operatorname*{min}}\\left(\\left(\\sum_{i=1}^{j}\\frac{1}{\\tau_{i}}\\right)^{-1}(m+j)\\right)=\\frac{2}{\\sqrt{m}}t^{*}(m,[\\tau_{i}]_{i=1}^{n}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "This is the first optimal method for the problem we consider, and Theorem 10 gives the first lower bound. ", "page_idx": 8}, {"type": "text", "text": "6 Using the Developed Strategies in Other Methods ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "ComputeBatchDifference (Algorithm 3) is a generic subroutine and can be used in other methods. In Section H, we introduce Freya SGD, a simple algorithm with update rule $x^{k+1}\\;=\\;$ $x^{k}-\\gamma\\times{\\mathsf{C o m p u t e B a t c h}}(S,x^{k})$ where $S$ is a batch size and ComputeBatch (Algorithm 4) is a minor modification of ComputeBatchDifference. Theorem 25 establishes that Freya SGD converges in $\\mathcal{O}\\left(1/\\varepsilon\\times t^{*}\\left(1/\\varepsilon,\\left[\\tau_{i}\\right]_{i=1}^{n}\\right)\\right)$ seconds (where we only keep the dependence on $\\varepsilon$ and $\\{\\tau_{i}\\}$ 0. For small $\\varepsilon$ this complexity is worse than (10), but it can be better, for instance, in the interpolation regime [Schmidt and Roux, 2013, Ma et al., 2018]. Freya SGD resembles Rennala SGD [Tyurin and Richtarik, 2023], but unlike the latter, it is specialized to work with finite-sum problems (1) and does not require the $\\sigma^{2}$ -bounded variance assumption on stochastic gradients (which is not satisfied in our setting). ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The research reported in this publication was supported by funding from King Abdullah University of Science and Technology (KAUST): i) KAUST Baseline Research Scheme, i) Center of Excellence for Generative AI, under award number 5940, ii) SDAIA-KAUST Center of Excellence in Artificial Intelligence and Data Science. The work of A.T. was partially supported by the Analytical center under the RF Government (subsidy agreement 000000D73032iP5Q0002, Grant No. 70-2021-00145 02.11.2021). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Z. Allen-Zhu and E. Hazan. Variance reduction for faster non-convex optimization. In International Conference on Machine Learning, pages 699-707. PMLR, 2016.   \nY. Arjevani, Y. Carmon, J. C. Duchi, D. J. Foster, N. Srebro, and B. Woodworth. Lower bounds for non-convex stochastic optimization. Mathematical Programming, pages 1-50, 2022.   \nY. Carmon, J. C. Duchi, O. Hinder, and A. Sidford. Lower bounds for finding stationary points I. Mathematical Programming, 184(1):71-120, 2020.   \nJ. Chen, X. Pan, R. Monga, S. Bengio, and R. Jozefowicz. Revisiting distributed synchronous SGD. arXiv preprint arXiv:1604.00981, 2016.   \nS. Dutta, G. Joshi, S. Ghosh, P. Dube, and P. Nagpurkar. Slow and stale gradients can win the race: Error-runtime trade-offs in distributed SGD. In International Conference on Artificial Intelligence and Statistics, pages 803-812. PMLR, 2018.   \nC. Fang, C. J. Li, Z. Lin, and T. Zhang. SPIDER: Near-optimal non-convex optimization via stochastic path-integrated differential estimator. Advances in Neural Information Processing Systems, 31, 2018.   \nS. Horvath and P. Richtarik. Nonconvex variance reduced optimization with arbitrary sampling. In International Conference on Machine Learning, pages 2781-2789. PMLR, 2019.   \nS. Horvath, L. Lei, P. Richtarik, and M. 1. Jordan. Adaptivity of stochastic gradient methods for nonconvex optimization. SIAM Journal on Mathematics of Data Science, 4(2):634-648, 2022.   \nA. Khaled and P. Richtarik. Better theory for SGD in the nonconvex world. Transactions on Machine Learning Research, 2022.   \nD. P. Kingma and J. Ba. Adam: A method for stochastic optimization. International Conference on Learning Representations, 2015.   \nA. Koloskova, S. U. Stich, and M. Jaggi. Sharper convergence guarantees for asynchronous SGD for distributed and federated learning. Advances in Neural Information Processing Systems, 2022.   \nD. Kovalev, A. Beznosikov, E. Borodich, A. Gasnikov, and G. Scutari. Optimal gradient sliding and its application to optimal distributed optimization under similarity. Advances in Neural Information Processing Systems, 35:33494-33507, 2022.   \nG. Lan. First-order and stochastic optimization methods for machine learning. Springer, 2020.   \nY. LeCun, C. Cortes, and C. Burges. Mnist handwritten digit database. ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist, 2, 2010.   \nL. Lei, C. Ju, J. Chen, and M. I. Jordan. Non-convex finite-sum optimization via SCSG methods. Advances in Neural Information Processing Systems, 30, 2017.   \nZ. Li, H. Bao, X. Zhang, and P Richtarik. PAGE: A simple and optimal probabilistic gradient estimator for nonconvex optimization. In International Conference on Machine Learning, pages 6286-6295. PMLR, 2021.   \nZ. Liu, X. Zhang, and J. Liu. Synthesis: A semi-asynchronous path-integrated stochastic gradient method for distributed learning in computing clusters. In Proceedings of the Twenty-Third International Symposium on Theory, Algorithmic Foundations, and Protocol Design for Mobile Networks and Mobile Computing, pages 151-160, 2022.   \nS. Ma, R. Bassily, and M. Belkin. The power of interpolation: Understanding the effectiveness of sgd inmoderm over-parametrized learning. In International Conference on Machine Learning, pages 3325-3334. PMLR, 2018.   \nK. Mishchenko, F. Bach, M. Even, and B. Woodworth. Asynchronous SGD beats minibatch SGD under arbitrary delays. Advances in Neural Information Processing Systems, 2022.   \nA. S. Nemirovskij and D. B. Yudin. Problem complexity and method efficiency in optimization. Wiley-Interscience, 1983.   \nY. Nesterov. Lectures on convex optimization, volume 137. Springer, 2018.   \nL. Nguyen, P. H. Nguyen, M. Dijk, P. Richtarik, K. Scheinberg, and M. Takac. SGD and hogwild! convergence without the bounded gradients assumption. In International Conference on Machine Learning, pages 3750-3758. PMLR, 2018.   \nL. M. Nguyen, J. Liu, K. Scheinberg, and M. Takac. SARAH: A novel method for machine learning problemsusing stochastic recursive gradient. In International Conference onMachine Learning, pages 2613-2621. PMLR, 2017.   \nB. Recht, C. Re, S. Wright, and F. Niu. Hogwild!: A lock-free approach to parallelizing stochastic gradient descent. Advances in Neural Information Processing Systems, 24, 2011.   \nS. J. Reddi, A. Hefny, S. Sra, B. Poczos, and A. Smola. Stochastic variance reduction for nonconvex optimization. In International Conference on Machine Learning, pages 314-323. PMLR, 2016.   \nM. Schmidt and N. L. Roux. Fast convergence of stochastic gradient descent under a strong growth condition. arXiv preprint arXiv: 1308.6370, 2013.   \nR. Szlendak, A. Tyurin, and P. Richtarik. Permutation compressors for provably faster distributed nonconvex optimization. In International Conference on Learning Representations, 2021.   \nA. Tyurin and P. Richtarik. Optimal time complexities of parallel stochastic optimization methods under a fixed computation model. Advances in Neural Information Processing Systems, 2023.   \nA. Tyurin, L. Sun, K. Burlachenko, and P. Richtarik. Sharper rates and flexible framework for nonconvex sgd with client and data sampling. Transactions on Machine Learning Research, 2023.   \nA. Tyurin, M. Pozzi, I. lin, and P. Richtarik. Shadowheart SGD: Distributed asynchronous SGD with optimal time complexity under arbitrary computation and communication heterogeneity. arXiv preprint arXiv:2402.04785, 2024.   \nZ. Wang, K. Ji, Y. Zhou, Y. Liang, and V. Tarokh. SpiderBoost and momentum: Faster variance reduction algorithms. Advances in Neural Information Processing Systems, 32, 2019.   \nD. Zhou, P. Xu, and Q. Gu. Stochastic nested variance reduction for nonconvex optimization. Journal of Machine Learning Research, 21(103):1-63, 2020. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "APPENDIX ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "Contents ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "1Introduction 1   \n1.1 Assumptions 2   \n1.2 Gradient oracle complexities 2   \n1.3 Some previous time complexities 2 ", "page_idx": 11}, {"type": "text", "text": "2 Contributions 3 ", "page_idx": 11}, {"type": "text", "text": "3  The Design of the New Algorithm ", "page_idx": 11}, {"type": "text", "text": "Time Complexities and Convergence Rates 6   \n4.1 Optimal parameters $S$ and $p$ 7   \n4.2 Optimal parameters $S$ and $p$ in the large-scale regime 7   \n4.3 Discussion of the time complexity 8   \n4.4 Dynamic bounds 8   \n4.5 Comparison with previous strategies from Section 1.3 . 9   \n4.6 Comparison with existing asynchronous variance reduced methods 9 ", "page_idx": 11}, {"type": "text", "text": "5Lower Bound 9 ", "page_idx": 11}, {"type": "text", "text": "6 Using the Developed Strategies in Other Methods 9   \nAExperiments 14   \nA.1  Experiments with nonconvex quadratic optimization . . . 14   \nA.2  Experiments with logistic regression 14   \nBThe Time Complexity Guarantees of Algorithms 3 and 4 16   \nC The Time Complexity Guarantees of Algorithms 2 and 5 17   \nD Proofs for Algorithm 1 (Freya PAGE) 20   \nE Freya PAGE with Other Samplings 25   \nE.1 Nice sampling . . . 26   \nE.2  Importance sampling 27   \nF  Dynamic Bounds 27   \nG  Examples 29   \nH A New Stochastic Gradient Method: Freya SGD 31   \nI Setup of the Experiments from Section A.1 33 ", "page_idx": 11}, {"type": "text", "text": "J Lower bound 34 ", "page_idx": 12}, {"type": "text", "text": "J.1 Time multiple oracles protocol 34   \nJ.2 The \u201cworst case\u201d function in the nonconvex world 35   \nJ.3 The first lower bound 36   \nJ.4 The second lower bound 39 ", "page_idx": 12}, {"type": "text", "text": "42 ", "page_idx": 12}, {"type": "text", "text": "A Experiments ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We compare Freya PAGE with Rennala SGD, Asynchronous SGD, and Soviet PAGE on nonconvex quadratic optimization tasks and practical machine learning problems. The experiments were conducted in Python 3.8 with Intel(R) Xeon(R) Gold 6248 CPU $\\textcircled{a}2.50\\mathrm{GHz}$ Wedeveloped alibrary that emulates the working behavior of thousands of nodes. ", "page_idx": 13}, {"type": "text", "text": "A.1  Experiments with nonconvex quadratic optimization ", "text_level": 1, "page_idx": 13}, {"type": "image", "img_path": "AUeTkSymOq/tmp/e09bc195c839e1b429ace040de64303860f3b8533ed1b21fcee37d0c0fb65ea4.jpg", "img_caption": [], "img_footnote": [], "page_idx": 13}, {"type": "image", "img_path": "AUeTkSymOq/tmp/bf446c6ff7dae4f02928a703cf972c8ad14f64a602ed7f09310c36f8b99571fd.jpg", "img_caption": [], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "Figure 1: Experiments with nonconvex quadratic optimization tasks. We plot function suboptimality against elapsed time. ", "page_idx": 13}, {"type": "text", "text": "In the first set of experiments, we compare the algorithms on a synthetic quadratic optimization task generated using the procedure from Section I. To ensure robust and fair comparison, we fix the performance of each worker and emulate our setup by assuming that the $i^{\\mathrm{th}}$ workerrequires $\\sqrt{i}$ seconds to calculate a stochastic gradient. For each algorithm, we fine-tune the step size from the set $\\{2^{i}\\mid i\\in[-20,20]\\}$ . Uniform sampling with replacement is used across all methods. In Freya PAGE, weset $\\bar{S^{\\mathrm{\\,}}}=\\bar{\\lceil}\\sqrt{m}\\bar{\\rceil}$ according to Theorem 7. We consider $n\\in\\{1000,10000\\}$ and in each case plot the best run of each method. ", "page_idx": 13}, {"type": "text", "text": "The results are presented in Figure 1. It is evident that our new method, Freya PAGE, has the best convergence performance among all algorithms considered. The convergence behavior of Rennala SGD and Asynchronous SGD is very noisy, and both achieve lower accuracy than Freya PAGE. Furthermore, the gap between Freya PAGE and Soviet PAGE widens with increasing $n$ becauseSoviet PAGE is not robust to the presence of slow workers. ", "page_idx": 13}, {"type": "text", "text": "A.2 Experiments with logistic regression ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Table 2: Mean and variance of algorithm accuracies on the MNIST test set during the final 100K seconds of the experiments from Figure 2b. ", "page_idx": 13}, {"type": "table", "img_path": "AUeTkSymOq/tmp/3ef02f1e3a7b27843ffa785b751dc737aeac7608670d86490c9a797a7e1a4f16.jpg", "table_caption": [], "table_footnote": [], "page_idx": 13}, {"type": "image", "img_path": "AUeTkSymOq/tmp/4a7efa1d46c2eca28c4696f896d0e58efd2130ee86e4a1ef34ea3cc57a94af9c.jpg", "img_caption": ["Figure 2: Experiments with the logistic regression problem on the MNIST dataset. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "We now consider the logistic regression problem on the MNIST dataset [LeCun et al., 2010], where each algorithm samples one data point at a time. The results of the experiments are presented in Figure 2. The difference between Freya PAGE and Rennala SGD/Asynchronous SGD is not as pronounced as in Section A.1: the methods have almost the same performance for this particular problem. However, our method still outperforms its competitors in the low accuracy regime, and is significantly better than Soviet PAGE. ", "page_idx": 14}, {"type": "text", "text": "A critical disadvantage of Rennala SGD and Asynchronous SGD is their noisy behavior, evident in both the plots and reflected in higher variance of the accuracy (see Table 2). In contrast, the iterations of Freya PAGE in Figure 2 are smooth, and its accuracy exhibits the lowest variance, as shown in Table 2. This stability can be attributed to the variance-reduction nature of Freya PAGE. ", "page_idx": 14}, {"type": "text", "text": "B  The Time Complexity Guarantees of Algorithms 3 and 4 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In addition to ComputeBatchDifference (Algorithm 3) introduced in the main part, we also analyze ComputeBatch (Algorithm 4) that is similar to ComputeBatchDifference, but calculates a minibatch of stochastic gradients $\\nabla f_{i}(x)$ instead of stochastic gradient differences $\\nabla f_{i}(x)-\\nabla f_{i}(y)$ ", "page_idx": 15}, {"type": "text", "text": "Algorithm 4 ComputeBatch(S, x) ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "1: Input: batch size $S\\in\\mathbb{N}$ point $\\boldsymbol{x}\\in\\mathbb{R}^{d}$   \n2:Init $g=0\\in\\mathbb{R}^{d}$   \n3: Broadcast $x$ to all workers   \n4: For each worker, sample $j$ from $[m]$ (uniformly) and ask it to calculate $\\nabla f_{j}(x)$   \n5: for $i=1,2,\\dots,S$ do   \n6: Wait for $\\nabla f_{p}(\\boldsymbol{x})$ from a worker   \n7: $\\begin{array}{r}{g\\gets g+\\frac{1}{S}\\nabla f_{p}(x)}\\end{array}$   \n8: Sample $j$ from $[m]$ (uniformly) and ask this worker to calculate $\\nabla f_{j}(x)$   \n9: end for   \n10: Return $g$ ", "page_idx": 15}, {"type": "text", "text": "Theorem 2. The time needed by Algorithm $^3$ to calculate $g$ is atmost ", "page_idx": 15}, {"type": "equation", "text": "$$\n4\\operatorname*{min}_{j\\in[n]}\\left(\\left(\\sum_{i=1}^{j}{\\frac{1}{\\tau_{i}}}\\right)^{-1}(S+j)\\right)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "seconds. ", "page_idx": 15}, {"type": "text", "text": "Proof. Let ", "page_idx": 15}, {"type": "equation", "text": "$$\nt=4\\operatorname*{min}_{j\\in[n]}\\left(\\left(\\sum_{i=1}^{j}{\\frac{1}{\\tau_{i}}}\\right)^{-1}(S+j)\\right).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "As soon as some worker finishes calculating the stochastic gradient difference, it immediately starts computing the difference of the next pair. Hence, by the time $t$ , all workers will have processed at least ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{n}\\left\\lfloor{\\frac{t}{2\\tau_{i}}}\\right\\rfloor\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "pairs. Assume that ", "page_idx": 15}, {"type": "equation", "text": "$$\nj^{*}=\\arg\\operatorname*{min}_{j\\in[n]}\\left(\\left(\\sum_{i=1}^{j}\\frac{1}{\\tau_{i}}\\right)^{-1}(S+j)\\right).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Using Lemma 4, we have $t\\geq4\\tau_{i}$ for all $i\\leq j^{*}$ . Thus, we get $\\begin{array}{r}{\\frac{t}{2\\tau_{i}}\\geq1}\\end{array}$ for all $i\\leq j^{*}$ and ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\sum_{i=1}^{n}\\left\\lfloor\\frac{t}{2\\tau_{i}}\\right\\rfloor\\ge\\sum_{i=1}^{j^{*}}\\left\\lfloor\\frac{t}{2\\tau_{i}}\\right\\rfloor\\ge\\sum_{i=1}^{j^{*}}\\frac{t}{4\\tau_{i}}}\\\\ {\\displaystyle=\\frac{1}{4}\\left(\\sum_{i=1}^{j^{*}}\\frac{1}{\\tau_{i}}\\right)\\left(4\\left(\\sum_{i=1}^{j^{*}}\\frac{1}{\\tau_{i}}\\right)^{-1}(S+j^{*})\\right)=S+j^{*}\\ge S.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We can conclude that by the time (5), the algorithm will have calculated $S$ pairs of stochastic gradients and exited the loop. \u53e3 ", "page_idx": 15}, {"type": "text", "text": "Theorem 11. The time needed by Algorithm $^{4}$ tocalculate $g$ is atmost ", "page_idx": 15}, {"type": "equation", "text": "$$\n2\\operatorname*{min}_{j\\in[n]}\\left(\\left(\\sum_{i=1}^{j}\\frac{1}{\\tau_{i}}\\right)^{-1}(S+j)\\right)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "seconds. ", "page_idx": 15}, {"type": "text", "text": "Proof. The proof of this theorem is essentially the same as the proof of Theorem 2. The only difference is that Algorithm 4 calculates $\\nabla f_{i}(x)$ instead of $\\nabla f_{i}(x)\\ {\\bar{-}}-\\nabla f_{i}(y)$ \u53e3 ", "page_idx": 16}, {"type": "text", "text": "C  The Time Complexity Guarantees of Algorithms 2 and 5 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Instead of Algorithm 2, we analyze a more general Algorithm 5 that reduces to Algorithm 2 when $S=[m]$ ", "page_idx": 16}, {"type": "table", "img_path": "AUeTkSymOq/tmp/5526a86daedb69e29949a5d581a1610898dd28e2e21499a7edbcaff43526241e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "Theorem12. Th expected te needed by Agorim $^{5}$ to calculate $\\begin{array}{r}{\\boldsymbol{g}=\\frac{1}{|\\mathcal{S}|}\\sum_{i\\in\\mathcal{S}}\\nabla{f}_{i}}\\end{array}$ is at most ", "page_idx": 16}, {"type": "equation", "text": "$$\n12\\operatorname*{min}_{j\\in\\left[n\\right]}\\left(\\left(\\sum_{i=1}^{j}\\frac{1}{\\tau_{i}}\\right)^{-1}\\left(\\left|S\\right|+\\operatorname*{min}\\{\\left|S\\right|,n\\}\\log\\left(\\operatorname*{min}\\{\\left|S\\right|,n\\right\\}\\right)+j\\right)\\right)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "seconds. ", "page_idx": 16}, {"type": "text", "text": "Proof Sketch: While the following proof is technical, the intuition and idea behind it and the algorithm are relatively simple. For simplicity, assume that $n\\geq|S|$ . The set $\\mathcal{S}\\backslash\\mathcal{M}$ includes all indices that have not yet been calculated. Each worker is assigned a new random index from $\\mathbf{\\mathcal{S}}\\backslash\\mathcal{M}$ and starts the calculation of the gradient. At the beginning of the algorithm, when the set $\\mathbf{\\mathcal{S}}\\backslash\\mathcal{M}$ is large, the probability that two workers are assigned the same index is very small. Hence, using the same idea as in the proof of Theorem 2, the workers will calculate $\\approx|\\boldsymbol{S}|-n$ stochastic gradients after ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\approx\\operatorname*{min}_{j\\in[n]}\\left(\\left(\\sum_{i=1}^{j}{\\frac{1}{\\tau_{i}}}\\right)^{-1}(|S|-n+j)\\right)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "seconds. However, once the size of $\\mathbf{\\mathcal{S}}\\backslash\\mathcal{M}$ becomes roughly equal to $n$ , the probability that two workers sample the same index increases. In the final steps of the algorithm, we encounter the same issue as in the famous coupon collector's problem, resulting in an additional factor of $n\\log n$ because some stochastic gradients will be calculated multiple times. ", "page_idx": 16}, {"type": "text", "text": "Proof. Let us define $S=|S|$ and take any $k\\in[n]$ . We refer to the workers with the upper bounds $\\tau_{i}$ such that $\\tau_{i}\\leq\\tau_{k}$ as \u201cfast\", and the others will be termed \u201cslow\". ", "page_idx": 16}, {"type": "text", "text": "Consider the moment when the algorithm samples $j$ from the set $\\mathbf{\\mathcal{S}}\\backslash\\mathcal{M}$ to allocate it to one of the \"fast'\" workers (Line 4 or 11). The probability of sampling $j$ such that $\\nabla f_{j}(x)$ is currently being calculated by another \u201cfast\u2019 worker is ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left|\\{\\mathrm{indices~from~}S\\backslash\\mathcal{M}\\mathrm{~taken~by~^*\\mathrm{fast\"}~w o r k e r s}\\}\\right|\\qquad\\qquad\\qquad\\qquad\\left|\\{\\mathrm{indices~from~}S\\backslash\\mathcal{M}\\mathrm{~taken~by~^*\\mathrm{fast\"}~w o r k e r s}\\}\\right|\\qquad}\\\\ {\\left\\{\\mathrm{indices~from~}S\\backslash\\mathcal{M}\\mathrm{~taken~by~^*\\mathrm{fast\"}~w o r k e r s}\\}\\right|+\\left|\\{\\mathrm{indices~from~}S\\backslash\\mathcal{M}\\mathrm{~not~taken~by~^*\\mathrm{fast\"}~w o r k e r s}\\}\\right|\\neq0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "equation", "text": "", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "because there are at most $k$ \"fast\u201d\u2019 workers and at most $S$ distinct stochastic gradients. Let us define the set ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathcal{U}:=\\{\\mathrm{indices~from}\\;S\\backslash\\mathcal{M}\\mathrm{~not~taken~by~^{*}f a s t^{*}~w o r k e r s}\\}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "A \u201cfast\u201d\u2019 worker can be \u201cunlucky\u201d and start calculating a stochastic gradient that is being computed by another \u201cfast\u201d\u2019 worker. However, with probability at least ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\geq\\frac{|\\mathcal{U}|}{\\operatorname*{min}\\{k,S\\}+|\\mathcal{U}|},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "it will take a new index $j$ that was not previously taken by another \u201cfast\" worker. ", "page_idx": 17}, {"type": "text", "text": "Thus, the while loop of the algorithm defines a Markov process that begins with some $\\mathcal{U}\\subseteq S$ . The size of $\\boldsymbol{\\mathcal{U}}$ decreases by one with probability at least $\\frac{|\\mathcal{U}|}{\\operatorname*{min}\\{k,S\\}+|\\mathcal{U}|}$ in iterations where the algorithm samples $j$ from $\\boldsymbol{\\mathcal{U}}$ and asks a \u201cfast\u201d\u2019 worker to calculate the stochastic gradient. Additionally, the size of $\\boldsymbol{\\mathcal{U}}$ can decrease by one when a \u201cslow\" worker finishes calculating a stochastic gradient from $\\boldsymbol{\\mathcal{U}}$ ", "page_idx": 17}, {"type": "text", "text": "Let $\\bar{t}$ be the time required for the Markov process to reach the state $\\mathcal{U}=\\emptyset$ . Then, the while loop in Algorithm 2 will finish after at most ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\bar{T}:=\\bar{t}+\\tau_{k}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "secondsbecause once $\\mathcal{U}=\\emptyset$ , all non-processed indices from $\\mathcal{S}\\backslash\\mathcal{M}$ are assigned to the \u201cfast\u201d\u2019 workers, so calculating the remaining stochastic gradients will take at most $\\tau_{k}$ seconds. ", "page_idx": 17}, {"type": "text", "text": "It remains to estimate $\\bar{t}$ Let $\\eta_{p}$ be the number of iterations of the while loop where the algorithm samples $j$ from $\\mathcal{S}\\backslash\\mathcal{M}$ and asks a \u201cfast\u2019 worker to calculate the stochastic gradient when $|\\mathcal{U}|=p$ .By the definition of the Markov chain, we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\eta_{p}\\right]\\leq\\left(\\frac{p}{\\operatorname*{min}\\{k,S\\}+p}\\right)^{-1}=1+\\frac{\\operatorname*{min}\\{k,S\\}}{p}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "because with probaity least $\\frac{p}{\\operatorname*{min}\\{k,S\\}+p}$ oe of the\u201clucky) fat workes ecives $j$ from $\\boldsymbol{\\mathcal{U}}$ and decreases the size of $\\boldsymbol{\\mathcal{U}}$ by $\\eta_{p}$ has a geometric-like distribution). ", "page_idx": 17}, {"type": "text", "text": "Since $|\\mathcal{U}|\\leq S$ at the beginning of the while loop, it is sufficient for the \u201cfast\u2019 workers to calculate at most ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\sum_{p=1}^{S}\\left(\\eta_{p}+1\\right)\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "stochastic gradients to ensure that $\\mathcal{U}\\,=\\,\\emptyset$ (it is possible that some stochastic gradients will be calculated many times). Indeed, if $|\\mathcal{U}|=p$ for the first moment, then after $\\eta_{p}+1$ calculations of stochastic gradients by the \u201cfast\" workers, the size of set will be at most $p-1$ . The last \u201cplus one' calculation can only happen when $|\\mathcal{U}|=p-1$ ", "page_idx": 17}, {"type": "text", "text": "The time required for the \u201cfast\u2019? workers to process this number of stochastic gradients is at most ", "page_idx": 17}, {"type": "equation", "text": "$$\nt^{\\prime}=2\\left(\\sum_{i=1}^{k}\\frac{1}{\\tau_{i}}\\right)^{-1}\\left(\\sum_{p=1}^{S}(\\eta_{p}+1)\\right)+\\tau_{k},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "because for this choice of $t^{\\prime}$ ,we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{k}\\left\\lfloor\\frac{t^{\\prime}}{\\tau_{i}}\\right\\rfloor\\ge\\frac{1}{2}\\sum_{i=1}^{k}\\frac{t^{\\prime}}{\\tau_{i}}\\ge\\sum_{p=1}^{S}(\\eta_{p}+1),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\left\\lfloor\\frac{t^{\\prime}}{\\tau_{i}}\\right\\rfloor$ isthensthastradi hat $i$ can calculaein $t^{\\prime}$ seconds. Taking expectation gives ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[t^{\\prime}\\right]=2\\left(\\sum_{i=1}^{k}\\frac{1}{\\tau_{i}}\\right)^{-1}\\left(\\sum_{p=1}^{S}\\mathbb{E}\\left[\\eta_{p}+1\\right]\\right)+\\tau_{k}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{160}{52}\\left(\\displaystyle\\sum_{i=1}^{k}\\frac{1}{v_{i}^{3}}\\right)^{-1}\\left(2s+\\displaystyle\\sum_{p=1}^{s}\\frac{\\operatorname*{min}\\{k,S\\}}{p}\\right)+\\tau_{k}}\\\\ &{\\leq2\\left(\\displaystyle\\sum_{i=1}^{k}\\frac{1}{v_{i}^{3}}\\right)^{-1}\\left(2s+\\displaystyle\\sum_{p=1}^{s}\\frac{\\operatorname*{min}\\{n,S\\}}{p}\\right)+\\tau_{k}}\\\\ &{=2\\left(\\displaystyle\\sum_{i=1}^{k}\\frac{1}{v_{i}^{3}}\\right)^{-1}\\left(2s+\\displaystyle\\sum_{p=1}^{s}\\displaystyle\\sum_{i=1}^{n}\\frac{\\operatorname*{min}\\{n,S\\}}{p}+\\operatorname*{min}\\{n,S\\}\\displaystyle\\sum_{p=1}^{m+1}\\frac{1}{p}\\right)+\\tau_{k}}\\\\ &{\\leq2\\left(\\displaystyle\\sum_{i=1}^{k}\\frac{1}{v_{i}^{3}}\\right)^{-1}\\left(3s+\\operatorname*{min}\\{n,S\\}\\displaystyle\\sum_{p=1}^{n}\\frac{1}{p}\\right)+\\tau_{k}}\\\\ &{\\leq2\\left(\\displaystyle\\sum_{i=1}^{k}\\frac{1}{v_{i}^{3}}\\right)^{-1}\\left(3s+\\operatorname*{min}\\{n,S\\}\\right)(2+\\log\\left(\\operatorname*{min}\\{S,S\\}\\right))\\right)+\\tau_{k}}\\\\ &{\\leq10\\left(\\displaystyle\\sum_{i=1}^{k}\\frac{1}{v_{i}^{3}}\\right)^{-1}\\left(3+\\operatorname*{min}\\{n,S\\}\\log\\left(\\operatorname*{min}\\{n,S\\}\\right)\\right)+\\tau_{k}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where we use the standard bound on the harmonic series. Thus, the expectation of the total time (15) canbeboundedby ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{E}\\left[\\bar{T}\\right]\\leq10\\left(\\sum_{i=1}^{k}\\frac{1}{\\tau_{i}}\\right)^{-1}\\left(S+\\operatorname*{min}\\{n,S\\}\\log\\left(\\operatorname*{min}\\{n,S\\}\\right)\\right)+2\\tau_{k}}\\\\ {\\displaystyle\\leq10\\left(\\sum_{i=1}^{k}\\frac{1}{\\tau_{i}}\\right)^{-1}\\left(S+\\operatorname*{min}\\{n,S\\}\\log\\left(\\operatorname*{min}\\{n,S\\}\\right)+k\\right)+2\\tau_{k},}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where in the last line we add $k\\geq0$ . Recall that $k$ is a parameter we can choose. Let us take ", "page_idx": 18}, {"type": "equation", "text": "$$\nk=\\arg\\operatorname*{min}_{j\\in[n]}\\left(\\sum_{i=1}^{j}\\frac{1}{\\tau_{i}}\\right)^{-1}\\left(S+\\operatorname*{min}\\{n,S\\}\\log\\left(\\operatorname*{min}\\{n,S\\}\\right)+j\\right).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Using Lemma 4, we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\tau_{k}\\leq\\operatorname*{min}_{j\\in[n]}\\left(\\sum_{i=1}^{j}\\frac{1}{\\tau_{i}}\\right)^{-1}(S+\\operatorname*{min}\\{n,S\\}\\log\\left(\\operatorname*{min}\\{n,S\\}\\right)+j)\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and hence ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\bar{T}\\right]\\leq12\\operatorname*{min}_{j\\in[n]}\\left(\\left(\\sum_{i=1}^{j}\\frac{1}{\\tau_{i}}\\right)^{-1}(S+\\operatorname*{min}\\{n,S\\}\\log\\left(\\operatorname*{min}\\{n,S\\}\\right)+j)\\right).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "DProofs for Algorithm 1 (Freya PAGE) ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "The proofs use the simplified notation $t^{*}(S):=t^{*}(S,[\\tau_{i}]_{i=1}^{n})$ from Definition 3. ", "page_idx": 19}, {"type": "text", "text": "Since the update rule of PAGE coincides with that of Freya PAGE, one can directly apply the iteration complexity results established in Tyurin et al. [2023]. ", "page_idx": 19}, {"type": "text", "text": "Theorem 4 (Iteration complexity). Let Assumptions 1, 2 and 3 hold. Consider any minibatch size $S\\in\\mathbb{N}:=\\{1,2,\\ldots\\}$ any probabity $p\\in(0,1]$ and lethestepsize be $\\begin{array}{r}{\\gamma=\\left(L_{-}+L_{\\pm}\\sqrt{\\frac{1-p}{p S}}\\right)^{-1}}\\end{array}$ Then,after ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{K\\ge K_{\\mathsf{P A G E}}:=\\frac{2\\delta^{0}}{\\varepsilon}\\left(L_{-}+L_{\\pm}\\sqrt{\\frac{1-p}{p S}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "iterations of Algorithm $^{\\,l}$ we have $\\mathbb{E}\\left[\\left\\|\\nabla f({\\hat{x}}^{K})\\right\\|^{2}\\right]\\leq\\varepsilon_{}$ where $\\hat{x}^{K}$ is sampled uniformly at random from the iterates $\\{x^{0},\\ldots,x^{K-1}\\}$ ", "page_idx": 19}, {"type": "text", "text": "Proof. The result follows from Theorem 6 of Tyurin et al. [2023], using the parameters from the \"Uniform With Replacement\u2019 line in Table 1 of the same work. \u53e3 ", "page_idx": 19}, {"type": "text", "text": "Theorem 5 (Time complexity with free parameters $p$ and $S$ ).Considertheassumptionsandthe parameters from Theorem 4, plus Assumption 4. The expected time complexity of Algorithm $^{\\,l}$ is at most ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{T(p,S,[\\tau_{i}]_{i=1}^{n}):=12\\cdot t^{*}(m,[\\tau_{i}]_{i=1}^{n})}\\\\ &{\\quad+\\,\\frac{48\\delta^{0}}{\\varepsilon}\\left(L_{-}+L_{\\pm}\\sqrt{\\frac{1-p}{p S}}\\right)\\times\\{p\\cdot t^{*}(m,[\\tau_{i}]_{i=1}^{n})+(1-p)\\cdot t^{*}(S,[\\tau_{i}]_{i=1}^{n})\\}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof. The result established in Theorem 4 says that the iteration complexity of the algorithm is ", "page_idx": 19}, {"type": "equation", "text": "$$\nK_{\\mathsf{P A G E}}:=\\frac{2\\delta^{0}}{\\varepsilon}\\left(L_{-}+L_{\\pm}\\sqrt{\\frac{1-p}{p S}}\\right).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "At each iteration, with probability $1-p$ , the workers compute $S$ differences of stochastic gradients, which by Theorem 2 takes ", "page_idx": 19}, {"type": "equation", "text": "$$\n4\\operatorname*{min}_{j\\in[n]}\\left(\\left(\\sum_{i=1}^{j}\\frac{1}{\\tau_{i}}\\right)^{-1}(S+j)\\right)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "seconds. Otherwise, they collect the full gradient, which can be done (Theorem 1) in ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l}{12\\displaystyle\\operatorname*{min}_{j\\in[n]}\\left(\\left(\\sum_{i=1}^{j}\\frac{1}{\\tau_{i}}\\right)^{-1}(m+\\operatorname*{min}\\{m,n\\}\\log\\left(\\operatorname*{min}\\{m,n\\}\\right)+j)\\right)}\\\\ {\\leq24\\displaystyle\\operatorname*{min}_{j\\in[n]}\\left(\\left(\\sum_{i=1}^{j}\\frac{1}{\\tau_{i}}\\right)^{-1}(m+j)\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "seconds, where the inequality uses Assumption 4. Hence, recalling the notation ", "page_idx": 19}, {"type": "equation", "text": "$$\nt^{*}(S):=\\operatorname*{min}_{j\\in[n]}\\left(\\left(\\sum_{i=1}^{j}\\frac{1}{\\tau_{i}}\\right)^{-1}(S+j)\\right),\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "the (expected) time complexity of the method is ", "page_idx": 19}, {"type": "equation", "text": "$$\nT(p,S,[\\tau_{i}]_{i=1}^{n})=24t^{*}(m)+24K_{\\mathsf{P A G E}}\\times(p t^{*}(m)+(1-p)t^{*}(S))\\,,\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where the term $t^{*}(m)$ corresponds to the preprocessing step, when the algorithm needs to calculate $\\begin{array}{r}{g^{0}=\\nabla f(x^{0})={^{1\\!}\\!/{m}}\\sum_{i=1}^{m}\\bar{\\nabla}f_{i}(x^{0})}\\end{array}$ \u53e3 ", "page_idx": 19}, {"type": "text", "text": "Theorem 13. Up to a constant factor, the time complexity $T(p,S,[\\tau_{i}]_{i=1}^{n})$ from (8) is at least ", "page_idx": 20}, {"type": "equation", "text": "$$\nt^{*}(m)+\\frac{\\delta^{0}}{\\varepsilon}\\operatorname*{min}\\left\\{L_{-}t^{*}(m),L_{-}t^{*}(S)+L_{\\pm}\\sqrt{\\frac{t^{*}(m)t^{*}(S)}{S}}\\right\\},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and attains thisvaluewith ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{p^{*}(S)=\\left\\{\\!\\!\\begin{array}{l l}{1,\\quad}&{L_{-}t^{*}(m)\\leq L_{-}t^{*}(S)+L_{\\pm}\\sqrt{\\frac{t^{*}(m)t^{*}(S)}{S}}}\\\\ {\\frac{t^{*}(S)}{t^{*}(m)},}&{\\mathrm{otherwise}.}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. Up to a constant factor, by Theorem 5, the time complexity of Freya PAGE is ", "page_idx": 20}, {"type": "equation", "text": "$$\nT(p,S,[\\tau_{i}]_{i=1}^{n}):=t^{*}(m)+\\frac{\\delta^{0}}{\\varepsilon}\\left(L_{-}+L_{\\pm}\\sqrt{\\frac{1-p}{p S}}\\right)\\left(p t^{*}(m)+(1-p)t^{*}(S)\\right).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Let us denote the second term in the above equation as $T_{p,S}$ . Then for all $p\\geq\\textstyle{\\frac{1}{2}}$ ,wehave ", "page_idx": 20}, {"type": "equation", "text": "$$\nT_{p,S}\\propto\\frac{\\delta^{0}}{\\varepsilon}\\left(L_{-}+L_{\\pm}\\sqrt{\\frac{1-p}{p S}}\\right)\\left(p t^{*}(m)+(1-p)t^{*}(S)\\right)\\geq\\frac{\\delta^{0}}{2\\varepsilon}L_{-}t^{*}(m),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and for all $S\\geq{\\frac{m}{2}}$ ", "page_idx": 20}, {"type": "equation", "text": "$$\nT_{p,S}\\geq\\frac{\\delta^{0}}{\\varepsilon}L_{-}\\left\\{p t^{*}(m)+(1-p)\\operatorname*{min}_{j\\in[n]}\\left(\\left(\\sum_{i=1}^{j}\\frac{1}{\\tau_{i}}\\right)^{-1}\\left(\\frac{m}{2}+j\\right)\\right)\\right\\}\\geq\\frac{\\delta^{0}}{2\\varepsilon}L_{-}t^{*}(m).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Otherwise, when $\\textstyle p<{\\frac{1}{2}}$ and $S<{\\frac{m}{2}}$ , we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{T_{p,S}\\geq\\displaystyle\\frac{\\delta^{0}}{2\\varepsilon}\\left(L_{-}+L_{\\pm}\\sqrt{\\frac{1}{p S}}\\right)(p t^{*}(m)+t^{*}(S))}\\\\ &{\\qquad\\geq\\displaystyle\\frac{\\delta^{0}}{2\\varepsilon}L_{-}t^{*}(S)+\\frac{\\delta^{0}}{2\\varepsilon}\\left(L_{\\pm}\\sqrt{\\frac{1}{p S}}\\right)(p t^{*}(m)+t^{*}(S))}\\\\ &{\\qquad\\geq\\displaystyle\\frac{\\delta^{0}}{2\\varepsilon}L_{-}t^{*}(S)+\\frac{\\delta^{0}}{\\varepsilon}\\left(L_{\\pm}\\sqrt{\\frac{1}{p S}}\\right)\\sqrt{p t^{*}(m)t^{*}(S)}}\\\\ &{\\qquad=\\displaystyle\\frac{\\delta^{0}}{2\\varepsilon}L_{-}t^{*}(S)+\\frac{\\delta^{0}}{\\varepsilon}L_{\\pm}\\sqrt{\\frac{t^{*}(m)t^{*}(S)}{S}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Hence, up to a constant factor, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\left.\\mathrm{~\\sum~}(p,S,[\\tau_{i}]_{i=1}^{n})=t^{*}(m)+T_{p,S}\\geq t^{*}(m)+\\frac{\\delta^{0}}{\\varepsilon}\\operatorname*{min}\\left\\{L_{-}t^{*}(m),L_{-}t^{*}(S)+L_{\\pm}\\frac{\\sqrt{t^{*}(m)t^{*}(S)}}{\\sqrt{S}}\\right\\}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "It can be easily verified that this bound can be attained (up to a constant factor) using the parameter $p$ as defined in (18). \u53e3 ", "page_idx": 20}, {"type": "text", "text": "Theorem 14. Up to a constant factor, the minimum of the time complexities (8) and (17) is ", "page_idx": 20}, {"type": "equation", "text": "$$\nt^{*}(m)+\\frac{\\delta^{0}}{\\varepsilon}\\operatorname*{min}\\left\\{L_{-}t^{*}(m),\\operatorname*{min}_{S\\in[m]}\\left[L_{-}t^{*}(S)+L_{\\pm}\\frac{\\sqrt{t^{*}(m)t^{*}(S)}}{\\sqrt{S}}\\right]\\right\\}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "andis achieved for ", "page_idx": 20}, {"type": "equation", "text": "$$\nS^{*}=\\arg\\operatorname*{min}_{S\\in[m]}\\operatorname*{min}\\left\\{L_{-}t^{*}(m),L_{-}t^{*}(S)+L_{\\pm}\\frac{\\sqrt{t^{*}(m)t^{*}(S)}}{\\sqrt{S}}\\right\\}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and $p^{*}=p^{*}(S^{*})$ , where $p^{*}(S)$ is defined in (18). ", "page_idx": 20}, {"type": "text", "text": "Proof. This is a simple corollary of Theorem 13: we take $S$ that minimizes (17). ", "page_idx": 20}, {"type": "text", "text": "In certain scenarios, we can derive the optimal parameter values explicitly. ", "page_idx": 21}, {"type": "text", "text": "Theorem 15. ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "$\\begin{array}{r}{n\\,\\leq\\,\\frac{L_{\\pm}\\sqrt{m}}{L_{-}}\\,\\leq\\,m,}\\end{array}$ then (up to constants) $\\begin{array}{r}{S^{*}=\\frac{L_{\\pm}\\sqrt{m}}{L_{-}}}\\end{array}$ and $\\begin{array}{r}{p^{*}\\,=\\,\\frac{L_{\\pm}}{L_{-}\\sqrt{m}}}\\end{array}$ are optimal parametersand ", "page_idx": 21}, {"type": "equation", "text": "$$\nT(p^{*},S^{*},[\\tau_{i}]_{i=1}^{n})=t^{*}(m)+\\frac{\\delta^{0}{\\cal L}_{-}}{\\varepsilon}t^{*}\\left(\\frac{{\\cal L}_{\\pm}\\sqrt{m}}{{\\cal L}_{-}}\\right).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "2. If $\\begin{array}{r}{\\mathbf{\\nabla}\\cdot\\frac{L\\pm\\sqrt{m}}{L_{-}}\\leq1,}\\end{array}$ then (up to constants) $S^{*}=1$ and $p^{*}={}^{1}/m$ are optimal parameters and ", "page_idx": 21}, {"type": "equation", "text": "$$\nT(p^{*},S^{*},[\\tau_{i}]_{i=1}^{n})=t^{*}(m)+\\frac{\\delta^{0}{\\cal L}_{-}}{\\varepsilon}t^{*}(1).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "3. 1f \u00b1Vm $\\textstyle{\\frac{L\\pm{\\sqrt{m}}}{L_{-}}}\\geq m,$ then (up to constants) $p^{*}=1$ is an optimal parameter and ", "page_idx": 21}, {"type": "equation", "text": "$$\nT(p^{*},S,[\\tau_{i}]_{i=1}^{n})=\\frac{\\delta^{0}{\\cal L}_{-}}{\\varepsilon}t^{*}(m)\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "for any $S\\in[m]$ ", "page_idx": 21}, {"type": "text", "text": "Proof. We fist consider the case when n \u2264 Lm $\\begin{array}{r}{n\\leq\\frac{L\\pm\\sqrt{m}}{L_{-}}\\leq m}\\end{array}$ Since $\\begin{array}{r}{j\\leq n\\leq\\frac{L\\pm\\sqrt{m}}{L_{-}}}\\end{array}$ ,we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\frac{L_{\\pm}\\sqrt{m}}{L_{-}}+\\frac{L_{\\pm}}{L_{-}\\sqrt{m}}j\\ge\\frac{1}{2}\\left(\\frac{L_{\\pm}\\sqrt{m}}{L_{-}}+j\\right),\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and from the assumption that $\\begin{array}{r}{L_{-}\\geq\\frac{L\\pm}{\\sqrt{m}}}\\end{array}$ it follows that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\frac{L_{\\pm}\\sqrt{m}}{L_{-}}+j\\ge\\frac{L_{\\pm}\\sqrt{m}}{L_{-}}+\\frac{L_{\\pm}}{L_{-}\\sqrt{m}}j\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "for all $j\\in[n]$ . Thus, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\frac{L_{\\pm}}{L_{-}\\sqrt{m}}t^{*}(m)=\\operatorname*{min}_{j\\in[n]}\\left(\\left(\\sum_{i=1}^{j}\\frac{1}{\\tau_{i}}\\right)^{-1}\\left(\\frac{L_{\\pm}\\sqrt{m}}{L_{-}}+\\frac{L_{\\pm}}{L_{-}\\sqrt{m}}j\\right)\\right)\\stackrel{(20)}{\\geq}\\frac{1}{2}t^{*}\\left(\\frac{L_{\\pm}\\sqrt{m}}{L_{-}}\\right),\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\frac{L_{\\pm}}{L_{-}\\sqrt{m}}t^{*}(m)=\\operatorname*{min}_{j\\in[n]}\\left(\\left(\\sum_{i=1}^{j}\\frac{1}{\\tau_{i}}\\right)^{-1}\\left(\\frac{L_{\\pm}\\sqrt{m}}{L_{-}}+\\frac{L_{\\pm}}{L_{-}\\sqrt{m}}j\\right)\\right)\\overset{(21)}{\\leq}t^{*}\\left(\\frac{L_{\\pm}\\sqrt{m}}{L_{-}}\\right).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "It follows that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\frac{1}{2}t^{*}\\left(\\frac{L_{\\pm}\\sqrt{m}}{L_{-}}\\right)\\leq\\frac{L_{\\pm}}{L_{-}\\sqrt{m}}t^{*}(m)\\leq t^{*}\\left(\\frac{L_{\\pm}\\sqrt{m}}{L_{-}}\\right).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Since $\\textstyle{\\frac{L\\pm{\\sqrt{m}}}{L_{-}}}\\leq m$ ,we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\operatorname*{min}_{S\\in[m]}\\left\\{L_{-}t^{*}(S)+L_{\\pm}\\frac{\\sqrt{t^{*}(m)t^{*}(S)}}{\\sqrt{S}}\\right\\}\\leq L_{-}\\operatorname*{min}_{S\\in[m]}\\left\\{t^{*}(S)+\\sqrt{m}\\frac{\\sqrt{t^{*}(m)t^{*}(S)}}{\\sqrt{S}}\\right\\}}}\\\\ &{}&{\\leq2L_{-}t^{*}(m),}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and thus, according to the result from Theorem 14, it is sufficient to minimize ", "page_idx": 21}, {"type": "equation", "text": "$$\nt^{\\prime}(S):=L_{-}t^{*}(S)+L_{\\pm}\\frac{\\sqrt{t^{*}(m)t^{*}(S)}}{\\sqrt{S}}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "First, let us note that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{t^{\\prime}\\left(\\frac{L_{\\pm}\\sqrt{m}}{L_{-}}\\right)=L_{-}t^{*}\\left(\\frac{L_{\\pm}\\sqrt{m}}{L_{-}}\\right)+L_{\\pm}\\sqrt{\\frac{L_{-}}{L_{\\pm}\\sqrt{m}}t^{*}(m)t^{*}\\left(\\frac{L_{\\pm}\\sqrt{m}}{L_{-}}\\right)}}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\frac{(22)}{\\mathrm{~\\ensuremath{\\leq~}}L_{-}t^{*}\\left(\\frac{L_{\\pm}\\sqrt{m}}{L_{-}}\\right)+L_{\\pm}\\sqrt{\\frac{L_{-}}{L_{\\pm}\\sqrt{m}}\\frac{L_{-}\\sqrt{m}}{L_{\\pm}}t^{*}\\left(\\frac{L_{\\pm}\\sqrt{m}}{L_{-}}\\right)t^{*}\\left(\\frac{L_{\\pm}\\sqrt{m}}{L_{-}}\\right)}}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ =2L_{-}t^{*}\\left(\\frac{L_{\\pm}\\sqrt{m}}{L_{-}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "If $\\begin{array}{r}{S\\ge\\frac{L\\pm\\sqrt{m}}{L_{-}}}\\end{array}$ L\u00b1Vm , then ", "page_idx": 22}, {"type": "equation", "text": "$$\nt^{\\prime}(S)=L_{-}t^{*}(S)+L_{\\pm}\\sqrt{\\frac{t^{*}(m)t^{*}(S)}{S}}\\geq L_{-}t^{*}\\left(\\frac{L_{\\pm}\\sqrt{m}}{L_{-}}\\right).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Otherwise, if $\\begin{array}{r}{S<\\frac{L\\pm\\sqrt{m}}{L_{-}}}\\end{array}$ L\u00b1Vm , then ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ell\\left(s\\right)=L_{s}r\\left(s\\right)+L_{s}\\sqrt{r(m)}\\sqrt{\\frac{m}{\\sqrt{r(m)}}}\\left(\\left(\\sum_{i=1}^{j}\\alpha_{i}^{\\star}\\right)^{-1}\\left(1+\\frac{j}{\\delta}\\right)\\right)}\\\\ &{\\qquad\\ge L_{s}\\sqrt{r(m)}\\sqrt{\\frac{m}{\\sqrt{r(m)}}}\\left(\\left(\\displaystyle\\sum_{i=1}^{j}\\frac{1}{n}\\right)^{-1}\\left(1+\\frac{L_{s}-j}{L_{s}\\sqrt{m}}\\right)\\right)}\\\\ &{\\qquad=L_{s}\\sqrt{r(m)}\\sqrt{\\frac{L_{s}}{L_{s}\\sqrt{m}}}\\frac{\\operatorname*{min}\\left(\\left(\\displaystyle\\sum_{i=1}^{j}\\frac{1}{n}\\right)^{-1}\\left(\\frac{L_{s}+\\sqrt{m}}{L_{s}}+j\\right)\\right)}{\\left(\\left(\\displaystyle\\sum_{i=1}^{j}\\frac{1}{n}\\right)^{-1}\\right)}}\\\\ &{\\qquad=L_{s}\\sqrt{r(m)}\\frac{L_{s}}{L_{s}\\sqrt{m}}\\sqrt{\\frac{L_{s}\\sqrt{m}}{L_{s}}}\\left(\\frac{L_{s}+\\sqrt{m}}{L_{s}}\\right)}\\\\ &{\\qquad\\overset{(a)}{\\ge}\\frac{L_{s}\\sqrt{r(m)}}{L_{s}\\sqrt{m}}\\frac{L_{s}\\sqrt{m}}{L_{s}}\\bigg)\\frac{L_{s}\\sqrt{m}}{L_{s}}\\frac{L_{s}}{L_{s}\\sqrt{m}}e^{\\displaystyle\\left(\\frac{L_{s}\\sqrt{m}}{L_{s}}\\right)}}\\\\ &{\\qquad=\\frac{L_{s}}{\\sqrt{r}}\\frac{L_{s}\\sqrt{m}}{L_{s}}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Therefore, the optimal choice is $\\begin{array}{r}{S^{*}=\\frac{L\\pm\\sqrt{m}}{L_{-}}}\\end{array}$ , and by Theorem 13 and inequality (23), $p$ should chosen to be ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\frac{t^{*}\\left(\\frac{L\\pm\\sqrt{m}}{L_{-}}\\right)}{t^{*}(m)}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Using (22), we can conclude that $\\begin{array}{r}{p^{*}=\\frac{L\\pm}{L-\\sqrt{m}}}\\end{array}$ is optimal, proving the first part of the Theorem. ", "page_idx": 22}, {"type": "text", "text": "Next, consider the case when Vm $\\begin{array}{r}{\\frac{L\\pm\\sqrt{m}}{L_{-}}\\leq1}\\end{array}$ . By the reasoning above, it is sufficient to minimize ", "page_idx": 22}, {"type": "equation", "text": "$$\nt^{\\prime}(S):=L_{-}t^{*}(S)+L_{\\pm}\\frac{\\sqrt{t^{*}(m)t^{*}(S)}}{\\sqrt{S}}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "First, let us note that ", "page_idx": 22}, {"type": "equation", "text": "$$\nt^{\\prime}(1)=L_{-}t^{*}(1)+L_{\\pm}\\sqrt{t^{*}(m)t^{*}(1)}\\le L_{-}t^{*}(1)+L_{-}\\sqrt{\\frac{t^{*}(m)t^{*}(1)}{m}}\\le2L_{-}t^{*}(1),\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the last inequality follows from the fact that for any $S\\in[m]$ $t^{*}(S)/s\\leq t^{*}(1)$ . On the other hand, if $S\\geq1$ ,then ", "page_idx": 22}, {"type": "equation", "text": "$$\nt^{\\prime}(S)=L_{-}t^{*}(S)+L_{\\pm}\\frac{\\sqrt{t^{*}(m)t^{*}(S)}}{\\sqrt{S}}\\geq L_{-}t^{*}(S)\\geq L_{-}t^{*}(1).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Therefore, the optimal choice is $S^{*}=1$ . Then ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\delta^{0}}{\\varepsilon}\\left(L_{-}+L_{\\pm}\\sqrt{m-1}\\right)\\left(\\frac{t^{*}(m)}{m}+\\left(1-\\displaystyle\\frac{1}{m}\\right)t^{*}(1)\\right)}\\\\ &{\\leq\\displaystyle\\frac{2\\delta^{0}}{\\varepsilon}\\left(L_{-}+\\frac{L_{-}}{\\sqrt{m}}\\sqrt{m-1}\\right)t^{*}(1)}\\\\ &{\\leq\\displaystyle\\frac{4\\delta^{0}L_{-}}{\\varepsilon}t^{*}(1),}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "while for any $p$ ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\delta^{0}}{\\varepsilon}\\left(L_{-}+L_{\\pm}\\sqrt{\\frac{1-p}{p}}\\right)(p t^{*}(m)+(1-p)t^{*}(1))}\\\\ &{\\geq\\frac{\\delta^{0}L_{-}}{\\varepsilon}\\left(p t^{*}(m)+(1-p)t^{*}(1)\\right)}\\\\ &{\\geq\\frac{\\delta^{0}L_{-}}{\\varepsilon}\\left(p t^{*}(1)+(1-p)t^{*}(1)\\right)}\\\\ &{=\\frac{\\delta^{0}L_{-}}{\\varepsilon}t^{*}(1).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Hence $p^{*}=1/m$ ", "page_idx": 23}, {"type": "text", "text": "It remains to prove th thd reslt. Suppe tht $\\begin{array}{r}{L_{-}\\,<\\,\\frac{L\\pm}{\\sqrt{m}}}\\end{array}$ . Then, the last part o the theorem follows from the fact that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{S\\in[m]}{\\operatorname*{min}}\\left\\lbrace L_{-}t^{*}(S)+L_{\\pm}\\frac{\\sqrt{t^{*}(m)t^{*}(S)}}{\\sqrt{S}}\\right\\rbrace\\geq L_{\\pm}\\underset{S\\in[m]}{\\operatorname*{min}}\\left\\lbrace\\frac{\\sqrt{t^{*}(m)t^{*}(S)}}{\\sqrt{S}}\\right\\rbrace}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=L_{\\pm}\\left\\lbrace\\frac{\\sqrt{t^{*}(m)t^{*}(m)}}{\\sqrt{m}}\\right\\rbrace}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\geq L_{-}t^{*}(m).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "In practice, the values of smoothness constants are often unknown. However, the algorithm can still be run with close to optimal parameters. ", "page_idx": 23}, {"type": "text", "text": "Theorem 7 (Main result in the large-scale regime). Consider the assumptions and parameters from Theorem 4, plus Assumption 4. Up to a constant factor and smoothness constants, if ${\\bigl\\langle}{\\sqrt{m}}\\geq n$ ,then the optimal choice of parameters in (8) is $S^{*}=\\lceil{\\sqrt{m}}\\rceil$ and $p^{*}=1/\\sqrt{m}$ For this choice, the expected time complexity ofAlgorithm $^{\\,l}$ is atmost ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{T(^{1}/\\sqrt{m},\\sqrt{m},[\\tau_{i}]_{i=1}^{n})=12t^{*}(m,[\\tau_{i}]_{i=1}^{n})+\\frac{192\\delta^{0}\\operatorname*{max}\\{L_{-},L_{\\pm}\\}}{\\varepsilon}t^{*}(\\sqrt{m},[\\tau_{i}]_{i=1}^{n})}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "seconds. The iteration complexity with $S=\\lceil{\\sqrt{m}}\\rceil$ and $p={^{1}\\!/\\!{\\sqrt{m}}}$ $K_{\\sf P A G E}\\leq4\\delta^{0}\\operatorname*{max}\\{L_{-},L_{\\pm}\\}/\\varepsilon.$ ", "page_idx": 23}, {"type": "text", "text": "Proof. The proof is the same as in Theorem 15. Indeed, up to a constant factor, the time complexity (8) can be bounded as ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{T(p,S,[\\tau_{i}]_{i=1}^{n})=t^{*}(m)+\\frac{\\delta^{0}}{{\\varepsilon}}\\left(L_{-}+L_{\\pm}\\sqrt{\\frac{1-p}{p S}}\\right)\\left(p t^{*}(m)+(1-p)t^{*}(S)\\right)}\\\\ &{\\qquad\\qquad\\qquad\\leq t^{*}(m)+\\frac{2\\delta^{0}\\operatorname*{max}\\{L_{-},L_{\\pm}\\}}{{\\varepsilon}}\\left(1+\\sqrt{\\frac{1-p}{p S}}\\right)\\left(p t^{*}(m)+(1-p)t^{*}(S)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Therefore, by setting $L_{\\pm}=L_{-}$ in Theorem 15, one can easily derive the parameters $p$ and $S$ that are optimal up to the smoothness constants. The time complexity (10) can be obtained by applying $S={\\bar{\\lceil}}{\\sqrt{m}}{\\rceil}$ and $p={^{1}\\!/}{\\sqrt{m}}$ to (8). \u53e3 ", "page_idx": 23}, {"type": "text", "text": "E Freya PAGE with Other Samplings ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Algorithm 1 can be adapted to accommodate other sampling methods. This brings us to the introduction of Algorithm 6, which supports virtually any sampling strategy, formalized by the following mapping: ", "page_idx": 24}, {"type": "text", "text": "Definition 16 (Sampling). A sampling is a random mapping $\\mathbf{S}_{S}$ , which takes as an input a set of indices $\\mathcal{T}:=\\{a_{1},...,a_{m}\\}$ and returns a (multi)set $\\{a_{i_{1}},\\ldots,a_{i_{S}}\\}$ ,where $a_{i_{j}}\\in\\mathcal{T}$ for ali $j\\in[S]$ ", "page_idx": 24}, {"type": "text", "text": "Algorithm 6 Freya PAGE (with virtually any sampling) ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "1: Parameters: starting point $x^{0}\\,\\in\\,\\mathbb{R}^{d}$ , learning rate $\\gamma\\,>\\,0$ , minibatch size $S$ . sampling $\\mathbf{S}_{S}$   \nprobability $p\\in(0,1]$ , initialization $g^{0}=\\nabla f(x^{\\overline{{0}}})$ using ComputeGradient $([m],x^{0})$ (Alg. 2)   \n2: for $k=0,1,\\ldots,K-1$ do   \n3: $x^{k+1}=\\dot{x}^{k}-\\gamma g^{k}$   \n4: Sample $c^{k}\\sim\\mathrm{Bernoulli}(p)$   \n5: $c^{k}=1$ then   \n6: $\\begin{array}{r l}&{\\nabla f(x^{k+1})=\\mathsf{C o m p u t e G r a d i e n t}(x^{k+1})}\\\\ &{g^{k+1}=\\nabla f(x^{k+1})}\\end{array}$ (Alg. 2)   \n7:   \n8: else   \n9: Sample indices $S^{k}=\\mathbf{S}_{S}([m])$   \n10: $\\begin{array}{r l}&{\\frac{1}{S}\\displaystyle\\sum_{i\\in S^{k}}\\left(\\nabla f_{i}(x^{k+1})-\\nabla\\tilde{f_{i}}(x^{\\dot{k}})\\right)}\\\\ &{=\\mathsf{C o m p u t e B a t c h D i f f e r e n c e A n y S a m p l i n g}(S^{k},x^{k+1},x^{k})}\\\\ &{g^{k+1}=g^{k}+\\frac{1}{S}\\displaystyle\\sum_{i\\in S^{k}}\\left(\\nabla f_{i}(x^{k+1})-\\nabla f_{i}(x^{k})\\right)}\\end{array}$   \n(Alg. 7)   \n11:   \n12: end if ", "page_idx": 24}, {"type": "text", "text": "Algorithm 7 ComputeBatchDifferenceAnySampling(S, \u03b1, y) ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "1: Input: multiset $\\boldsymbol{S}$ , points $x,y\\in\\mathbb{R}^{d}$ 2: Init $g=0\\in\\mathbb{R}^{d}$ , multiset $\\mathcal{M}=\\emptyset$ 3:Broadcast $x$ to all workers 4: For each worker, sample $j$ from $\\boldsymbol{S}$ (uniformly) and ask it to calculate $\\nabla f_{j}(x)-\\nabla f_{j}(y)$ 5: while $\\mathcal{M}\\ne\\mathcal{S}$ do 6: Wait for $\\nabla f_{p}(x)-\\nabla f_{p}(y)$ from a worker 7: if $p\\in{\\mathcal{S}}\\backslash{\\mathcal{M}}$ then 8: $\\begin{array}{r}{g\\gets\\dot{g}+\\frac{1}{|S|}\\left(\\nabla f_{p}(x)-\\nabla f_{p}(y)\\right)}\\end{array}$ 9: Update ${\\mathcal{M}}\\gets{\\mathcal{M}}\\cup\\{p\\}$ 10: end if 11: Sample $j$ from $\\mathbf{\\mathcal{S}}\\backslash\\mathcal{M}$ (uniformly) and ask this worker to calculate $\\nabla f_{j}(x)-\\nabla f_{j}(y)$ 12: end while 13: Returm $g=\\textstyle{\\frac{1}{|S|}}\\sum_{i\\in S}\\big(\\nabla f_{i}(x)-\\nabla f_{i}(y)\\big)$ ", "page_idx": 24}, {"type": "text", "text": "The only difference is that instead of ComputeBatchDifference (Algorithm 5), Algorithm 6 uses a new subroutine, called ComputeBatchDifferenceAnySampling (Algorithm 3). ", "page_idx": 24}, {"type": "text", "text": "For this algorithm, we can prove the following time complexity guarantees. ", "page_idx": 24}, {"type": "text", "text": "Theorem 17. The expected time needed by Algorithm 7 to calculate $g=\\textstyle{\\frac{1}{|S|}}\\sum_{i\\in S}(\\nabla f_{i}(x)-\\nabla f_{i}(y))$ isatmost ", "page_idx": 24}, {"type": "equation", "text": "$$\n24\\operatorname*{min}_{j\\in\\left[n\\right]}\\left(\\left(\\sum_{i=1}^{j}\\frac{1}{\\tau_{i}}\\right)^{-1}\\left(\\left|S\\right|+\\operatorname*{min}\\{\\left|S\\right|,n\\}\\log\\left(\\operatorname*{min}\\{\\left|S\\right|,n\\right\\}\\right)+j\\right)\\right)\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "seconds. ", "page_idx": 24}, {"type": "text", "text": "Proof. The proof of this theorem is the same as the proof of Theorem 12. We only have to multiply (14) by 2 because Algorithm 3 calculates $\\nabla f_{i}(x)\\ {\\bar{-}}-\\nabla f_{i}(y)$ instead of $\\nabla f_{i}(x)$ \u53e3 ", "page_idx": 25}, {"type": "text", "text": "While changing the sampling strategy might affect the iteration complexity of the method, for a fixedminibatch size $S$ , the time complexity of a single iteration remains unchanged. Thus, having established the expected time needed by the algorithm to perform a single iteration (i.e., to collect a minibatch of stochastic gradients of the required size), one can simply multiply it by the iteration complexity of the method determined for any supported sampling technique to obtain the resulting time complexity. ", "page_idx": 25}, {"type": "text", "text": "With this in mind, we now analyse the time complexity of Algorithm 6 with 2 different sampling techniques: nice sampling and importance sampling. However, it can be analyzed with virtually any other unbiased sampling [Tyurin et al., 2023]. ", "page_idx": 25}, {"type": "text", "text": "E.1  Nice sampling ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Nice sampling returns a random subset of fixed cardinality $S$ chosen uniformly from $[m]$ .Unlike uniform sampling with replacement used in Algorithm 5, which returns a random multiset (that can include repetitions), the samples obtained by nice sampling are distinct. The iteration complexity of Algorithm 6 with nice sampling is given by the following theorem. ", "page_idx": 25}, {"type": "text", "text": "Theorem 18 (Tyurin et al. [2023], Section 3). Let Assumptions 1, 2 and 3 hold. Choose a minibatch size $S\\in[m]$ aprobability $p\\in(0,1]$ andthestepsize ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\gamma=\\frac{1}{L_{-}+L_{\\pm}\\sqrt{\\frac{(1-p)(m-S)}{p(m-1)S}}}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Then, the number of iteration needed by Algorithm $^{6}$ with nice sampling to reach an $\\varepsilon$ -stationary pointis ", "page_idx": 25}, {"type": "equation", "text": "$$\nK=\\frac{2\\delta^{0}}{\\varepsilon}\\left(L_{-}+L_{\\pm}\\sqrt{\\frac{(1-p)(m-S)}{p(m-1)S}}\\right).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Proof. The result follows from Theorem 6 and Table 1 from [Tyurin et al., 2023]. ", "page_idx": 25}, {"type": "text", "text": "Theorem 19. Consider the assumptions and parameters from Theorem 18 and Assumption 4. Up to a constant factor, the time complexity of Algorithm $^{6}$ is ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{T(p,S,[\\tau_{i}]_{i=1}^{n})=t^{*}(m,[\\tau_{i}]_{i=1}^{n})+\\displaystyle\\frac{\\delta^{0}}{\\varepsilon}\\left(L_{-}+L_{\\pm}\\sqrt{\\frac{(1-p)(m-S)}{p(m-1)S}}\\right)\\times}\\\\ &{\\times\\left\\{p\\times t^{*}(m,[\\tau_{i}]_{i=1}^{n})+(1-p)\\times t^{*}(S+\\operatorname*{min}\\{S,n\\}\\log\\left(\\operatorname*{min}\\{S,n\\}\\right),[\\tau_{i}]_{i=1}^{n})\\right\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $t^{*}$ is defined from Definition 3. ", "page_idx": 25}, {"type": "text", "text": "Remark 20. Compared to Theorem $5$ . which uses uniform sampling with replacement, the guarantees for nice sampling are slightly worse: the term $t^{*}(S,[\\tau_{i}]_{i=1}^{n})$ from Theorem 5 here is replaced with $t^{*}(S+\\operatorname*{min}\\bar{\\{}S,\\bar{n}\\}\\log\\left(\\dot{\\operatorname*{min}}\\bar{\\{}S},n\\right\\}\\right),$ $[\\tau_{i}]_{i=1}^{n})$ : Ignoring the logarithmic term $\\log\\left(\\operatorname*{min}\\{S,n\\}\\right)$ $(\\leq$ $\\log\\left(\\operatorname*{min}\\{m,n\\}\\right)$ theresult fromTheorem $_{l9}$ is equivalent to that in Theorem 5. Thus, Theorems $^{6}$ and 7 hold also for the nice sampling (up to logarithmic factors). ", "page_idx": 25}, {"type": "text", "text": "Proof. We use the same reasoning as in the proof of Theorem 5. With probability $p$ , the algorithm calculates the full gradients, which by Theorem 1 requires ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Theta\\left(\\underset{j\\in[n]}{\\operatorname*{min}}\\left(\\left(\\sum_{i=1}^{j}\\frac{1}{\\tau_{i}}\\right)^{-1}(m+\\operatorname*{min}\\{m,n\\}\\log\\left(\\operatorname*{min}\\{m,n\\}\\right)+j)\\right)\\right)}\\\\ &{=\\Theta\\left(\\underset{j\\in[n]}{\\operatorname*{min}}\\left(\\left(\\sum_{i=1}^{j}\\frac{1}{\\tau_{i}}\\right)^{-1}(m+j)\\right)\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "seconds, where we use Assumption 4. With probability $1-p$ , the algorithm calls ComputeBatchDifferenceAnySampling, which by Theorem 17 requires ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\Theta\\left(\\operatorname*{min}_{j\\in[n]}\\left(\\left(\\sum_{i=1}^{j}\\frac{1}{\\tau_{i}}\\right)^{-1}(S+\\operatorname*{min}\\{S,n\\}\\log\\left(\\operatorname*{min}\\{S,n\\}\\right)+j)\\right)\\right)\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "seconds. One can obtain the result by multiplying the iteration complexity (25) by the expected time needed to collect the required number of stochastic gradients per iteration and adding the preprocessing time. \u53e3 ", "page_idx": 26}, {"type": "text", "text": "E.2   Importance sampling ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Here we additionally assume $L_{i}$ -smoothness of the local objective functions $f_{i}$ ", "page_idx": 26}, {"type": "text", "text": "Assumption 5. The functions $f_{i}$ are $L_{i}$ -smooth. We denote $\\begin{array}{r}{\\bar{L}\\;:=\\;\\frac{1}{m}\\sum_{j=1}^{m}L_{j}}\\end{array}$ and $L_{\\operatorname*{max}}~:=$ $\\operatorname*{max}_{i\\in[n]}L_{i}$ ", "page_idx": 26}, {"type": "text", "text": "Importance sampling is a sampling technique that returns a multiset of indices with repetitions. Index j is included in the multise with probablity , L ", "page_idx": 26}, {"type": "text", "text": "Theorem 21 (Tyurin et al. [2023], Section 3). Let Assumptions $^{\\,l}$ and $^{5}$ hold.Choosea minibatch size $S\\in[m]$ probability $p\\in(0,1]$ andthestepsize ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\gamma=\\frac{1}{L_{-}+\\bar{L}\\sqrt{\\frac{1-p}{p S}}}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Then, the number of iteration needed by Algorithm $^{\\,l}$ with importance sampling to reach an Estationarypointis ", "page_idx": 26}, {"type": "equation", "text": "$$\nK=\\frac{2\\delta^{0}}{\\varepsilon}\\left(L_{-}+\\bar{L}\\sqrt{\\frac{1-p}{p S}}\\right).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "The complexity (27) is nearly identical to (7) and (25), with the only difference being the dependence On $\\bar{L}$ rather than $L_{\\pm}$ . Thus, all the results up to constant and logarithmic factors can be derived using the same methodology as that outlined in Section 4, with the simple substitution of $L_{\\pm}$ With $\\bar{L}$ ", "page_idx": 26}, {"type": "text", "text": "F Dynamic Bounds ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "As noted in Section 4.4, the results from Section D can be easily generalized to iteration-dependent processing times. ", "page_idx": 26}, {"type": "text", "text": "Theorem 22.Consider the assumptions and the parameters from Theorem 4 and Assumption 4. Up to a constant factor, the time complexity of Freya PAGE(Algorithm 1) withiteration-dependent processingtimes $\\left\\{\\tau_{i}^{k}\\right\\}$ , which are defined in Section 4.4, is at most ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{j\\in[n]}{\\operatorname*{min}}\\left(\\left(\\displaystyle\\sum_{i=1}^{j}\\frac{1}{\\tau_{\\pi_{-1,i}}^{-1}}\\right)^{-1}(m+j)\\right)}\\\\ &{\\quad\\quad+\\displaystyle\\sum_{k=0}^{[K_{\\mathsf{p a c t}}]}\\left\\{p\\operatorname*{min}_{j\\in[n]}\\left(\\left(\\displaystyle\\sum_{i=1}^{j}\\frac{1}{\\tau_{\\pi_{k,i}}^{k}}\\right)^{-1}(m+j)\\right)+(1-p)\\operatorname*{min}_{j\\in[n]}\\left(\\left(\\displaystyle\\sum_{i=1}^{j}\\frac{1}{\\tau_{\\pi_{k,i}}^{k}}\\right)^{-1}(S+j)\\right)\\right\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "to find an $\\varepsilon$ -stationary point, where $\\mathit{p}\\in\\mathit{\\Theta}(0,1]$ and $\\cal{S}~\\in~\\mathbb{N}$ are free parameters, and $\\pi_{k}$ . is $a$ permutation such that $\\tau_{\\pi_{k,1}}^{k}\\leq\\cdots\\leq\\tau_{\\pi_{k,n}}^{k}$ for all $k\\ge-1$ ", "page_idx": 26}, {"type": "text", "text": "Remark 23. The theorem can be trivially extended to other samplings by changing KpAGE to the iterationcomplexitiesfromTheorems $I8$ and21. ", "page_idx": 26}, {"type": "text", "text": "Proof. The reasoning behind this result is exactly the same as in the proof of Theorem 5. The only difference is that in this more general setting, the expected time per iteration varies across iterations. Therefore, instead of simply multiplying, one needs to sum over the iterations to obtain the total time complexity. ", "page_idx": 27}, {"type": "text", "text": "We introducethe permutations to ensure that $\\{\\tau_{\\pi_{k,i}}^{k}\\}_{i=1}^{n}$ aresorted.When $\\tau_{i}^{k}=\\tau_{i}$ thereis no ned to introduce them because, throughout the paper, it is assumed $\\tau_{1}\\leq...\\leq\\tau_{n}$ (see Section 1). ", "page_idx": 27}, {"type": "text", "text": "G Examples ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Here we provide the proofs for the examples from Section 4.3. We will use the notation ", "page_idx": 28}, {"type": "equation", "text": "$$\nt(S,j):=\\left(\\sum_{i=1}^{j}\\frac{1}{\\tau_{i}}\\right)^{-1}(S+j)\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "for a fixed $S\\in[m]$ and all $j\\in[n]$ ", "page_idx": 28}, {"type": "text", "text": "Example 1. [Equally Fast Workers]Suppose that the upper bounds on the processing times are equal, i.e., $\\tau_{j}=\\tau$ for all $j\\in[n]$ Then ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{T(^{1/}\\sqrt{m},\\sqrt{m},[\\tau_{i}]_{i=1}^{n})=\\Theta\\left(\\tau\\operatorname*{max}\\left\\{\\frac{m}{n},1\\right\\}+\\tau\\frac{\\delta^{0}\\operatorname*{max}\\{L_{-},L_{\\pm}\\}}{\\varepsilon}\\operatorname*{max}\\left\\{\\frac{\\sqrt{m}}{n},1\\right\\}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Proof. First, when $\\tau_{j}=\\tau$ for all $j\\,\\in\\,[n]$ , then for any $S\\,\\in\\,[m],\\,t(S,j)$ is minimized by taking $j=n$ ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{t^{*}(S):=\\underset{j\\in[n]}{\\operatorname*{min}}\\,t(S,j)=\\underset{j\\in[n]}{\\operatorname*{min}}\\left(\\left(\\displaystyle\\sum_{i=1}^{j}\\frac{1}{\\tau_{i}}\\right)^{-1}(S+j)\\right)}\\\\ &{\\quad\\quad=\\underset{j\\in[n]}{\\operatorname*{min}}\\left(\\frac{\\tau}{j}(S+j)\\right)=\\Theta\\left(\\tau\\operatorname*{max}\\left\\{\\frac{S}{n},1\\right\\}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "It remains to substitute this equality in (10) ", "page_idx": 28}, {"type": "text", "text": "Example 2. [Infinitely Fast Worker] If $\\tau_{1}=0$ then $T(1/\\sqrt{m},\\sqrt{m},[\\tau_{i}]_{i=1}^{n})=0.$ ", "page_idx": 28}, {"type": "text", "text": "Proof. The statement follows easily from the fact that for any $S\\in[m]$ wehave ", "page_idx": 28}, {"type": "equation", "text": "$$\nt^{*}(S):=\\operatorname*{min}_{j\\in[n]}\\left(\\left(\\sum_{i=1}^{j}\\frac{1}{\\tau_{i}}\\right)^{-1}(S+j)\\right)\\le\\left(\\frac{1}{\\tau_{1}}\\right)^{-1}(S+j)=0.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Example 3. [Infinitely Slow Workers] If $\\tau_{j}=\\infty\\,\\forall j\\in[n],$ then $T(1/\\sqrt{m},\\sqrt{m},[\\tau_{i}]_{i=1}^{n})=\\infty.$ ", "page_idx": 28}, {"type": "text", "text": "Proof. This follows from the fact that for any $S\\in[m]$ and any $j\\in[n]$ wehave ", "page_idx": 28}, {"type": "equation", "text": "$$\nt(S,j):=\\left(\\sum_{i=1}^{j}\\frac{1}{\\tau_{i}}\\right)^{-1}(S+j)=\\infty.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Example 4. [Extremely Slow Workers] Suppose that the times $\\tau_{j}\\,<\\,\\infty$ are fixed $\\forall j\\le j_{B}$ and $\\tau_{j}\\geq B\\,\\forall j>j_{B}$ for some $B$ large enough. Then $T(1/\\sqrt{m},\\sqrt{m},[\\tau_{i}]_{i=1}^{n})=T(1/\\sqrt{m},\\sqrt{m},[\\tau_{i}]_{i=1}^{j_{B}})$ ", "page_idx": 28}, {"type": "text", "text": "Proo. Suppos that $\\begin{array}{r}{B\\ge\\frac{m+j_{B}}{\\sum_{i=1}^{j_{B}}\\frac{1}{\\tau_{i}}}}\\end{array}$ and fi any $k>j_{B}$ Then, since $\\tau_{j}\\geq B$ for ll $j>j_{B}$ , we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n{\\frac{k-j_{B}}{\\sum_{i=j_{B}+1}^{k}{\\frac{1}{\\tau_{i}}}}}\\geq{\\frac{k-j_{B}}{\\sum_{i=j_{B}+1}^{k}{\\frac{1}{B}}}}=B\\geq{\\frac{m+j_{B}}{\\sum_{i=1}^{j_{B}}{\\frac{1}{\\tau_{i}}}}}\\geq{\\frac{S+j_{B}}{\\sum_{i=1}^{j_{B}}{\\frac{1}{\\tau_{i}}}}}=t(S,j_{B})\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "for any S E [m]. Rearranging and adding (S + jB)\u2265=1 to both sides of the inequality, we obtain ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\left(S+j_{B}\\right)\\left(\\sum_{i=1}^{j_{B}}\\frac{1}{\\tau_{i}}\\right)+\\left(S+j_{B}\\right)\\left(\\sum_{i=j_{B}+1}^{k}\\frac{1}{\\tau_{i}}\\right)\\leq\\left(S+j_{B}\\right)\\left(\\sum_{i=1}^{j_{B}}\\frac{1}{\\tau_{i}}\\right)+\\left(k-j_{B}\\right)\\left(\\sum_{i=1}^{j_{B}}\\frac{1}{\\tau_{i}}\\right),\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "meaning that ", "page_idx": 29}, {"type": "equation", "text": "$$\nt(S,j_{B})=\\frac{S+j_{B}}{\\sum_{i=1}^{j_{B}}\\frac{1}{\\tau_{i}}}\\leq\\frac{S+k}{\\sum_{i=1}^{k}\\frac{1}{\\tau_{i}}}=t(S,k)\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "for any $k>j_{B}$ and any $S\\in[m]$ . Therefore, ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{j\\in[n]}\\left(\\left(\\sum_{i=1}^{j}\\frac{1}{\\tau_{i}}\\right)^{-1}(S+j)\\right)=\\operatorname*{min}_{j\\in[j_{B}]}\\left(\\left(\\sum_{i=1}^{j}\\frac{1}{\\tau_{i}}\\right)^{-1}(S+j)\\right)\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "for any $S\\in[m]$ , which proves the claim. ", "page_idx": 29}, {"type": "text", "text": "H A New Stochastic Gradient Method: Freya SGD ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "In this section, we introduce a new a non-variance reduced SGD method that we call Freya SGD. Freya SGD is closely aligned with Rennala SGD [Tyurin and Richtarik, 2023], but Freya SGD does not require the $\\sigma^{2}$ -bounded variance assumption on stochastic gradients. ", "page_idx": 30}, {"type": "text", "text": "Algorithm 8 Freya SGD ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "1: Parameters: starting point $x^{0}\\in\\mathbb{R}^{d}$ , learning rate $\\gamma>0$ , minibatch size $S$ ", "page_idx": 30}, {"type": "text", "text": "2: for $k=0,1,\\ldots,K-1$ do   \n3: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{S}\\displaystyle\\sum_{i\\in S^{k}}\\bar{\\nabla}f_{i}\\big(\\mathring{x}^{k}\\big)=\\mathsf{C o m p u t e B a t c h}(S,x^{k})}\\\\ &{x^{k+1}=x^{k}-\\gamma\\frac{1}{S}\\displaystyle\\sum_{i\\in S^{k}}\\nabla f_{i}(x^{k})}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "5: end for ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "(note): $S^{k}$ is a set of size $\\left|S^{k}\\right|=S$ of i.i.d. indices sampled from $[m]$ uniformly with replacement ", "page_idx": 30}, {"type": "text", "text": "Assumption 6. For all $i\\in[n]$ , there exists $f_{i}^{*}$ such that $f_{i}(x)\\geq f_{i}^{*}$ for all $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ ", "page_idx": 30}, {"type": "text", "text": "We define ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\Delta^{*}:=\\frac{1}{n}\\sum_{i=1}^{n}\\left(f^{*}-f_{i}^{*}\\right).\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Theorem 24. Let Assumptions $^{\\,l}$ \uff0c $^{5}$ and $^{6}$ hold. Choose minibatch size $S\\in[m]$ and stepsize ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\gamma=\\operatorname*{min}\\left\\{\\frac{\\sqrt{S}}{\\sqrt{L L_{\\operatorname*{max}}K_{\\mathrm{SGD}}}},\\frac{1}{L\\left(1-\\frac{1}{S}\\right)},\\frac{S\\varepsilon}{4L L_{\\operatorname*{max}}\\Delta^{*}}\\right\\},\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where ", "page_idx": 30}, {"type": "equation", "text": "$$\nK_{\\mathrm{SGD}}:=\\frac{12\\delta^{0}L}{\\varepsilon}\\operatorname*{max}\\left\\{1-\\frac{1}{S},\\frac{12L_{\\mathrm{max}}\\delta^{0}}{S\\varepsilon},\\frac{4L_{\\mathrm{max}}\\Delta^{*}}{S\\varepsilon}\\right\\}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Then, the number of iterations needed by Algorithm 8 to reach an $\\varepsilon$ -stationarypoint is $\\lceil K_{\\mathrm{SGD}}\\rceil$ ", "page_idx": 30}, {"type": "text", "text": "Proof. The iteration complexity can be proved using Corollary 1 and Proposition 3 (i) of Khaled and Richtarik [2022] (with $q_{i}=1/n_{\\l}$ \u53e3 ", "page_idx": 30}, {"type": "text", "text": "Theorem 25. Consider the assumptions and the parameters from Theorem 24. Up to a constant factor, the time complexity of Freya SGD (Algorithm 8) is at most ", "page_idx": 30}, {"type": "equation", "text": "$$\nT_{\\mathrm{SGD}}(S,[\\tau_{i}]_{i=1}^{n}):=\\frac{\\delta^{0}L_{-}}{\\varepsilon}\\left(1-\\frac{1}{S}+\\frac{L_{\\mathrm{max}}}{\\varepsilon S}\\left(\\delta^{0}+\\Delta^{*}\\right)\\right)\\times\\operatorname*{min}_{j\\in[n]}\\left(\\left(\\sum_{i=1}^{j}\\frac{1}{\\tau_{i}}\\right)^{-1}(S+j)\\right)\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "and is minimized by choosing ", "page_idx": 30}, {"type": "equation", "text": "$$\nS^{*}=\\frac{L_{\\operatorname*{max}}}{\\varepsilon}\\left(\\delta^{0}+\\Delta^{*}\\right).\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Up to a constant factor, we get ", "page_idx": 30}, {"type": "equation", "text": "$$\nT_{\\mathrm{SGD}}(S^{\\ast},[\\tau_{i}]_{i=1}^{n})=\\frac{\\delta^{0}L_{-}}{\\varepsilon}\\operatorname*{min}_{j\\in[n]}\\left(\\left(\\sum_{i=1}^{j}\\frac{1}{\\tau_{i}}\\right)^{-1}\\left(\\frac{L_{\\mathrm{max}}}{\\varepsilon}\\left(\\delta^{0}+\\Delta^{\\ast}\\right)+j\\right)\\right).\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Proof. At each iteration, the algorithm needs to collect a minibatch of stochastic gradients of size $S$ Multiplying the iteration complexity of Theorem 24 by the time needed to gather such a minibatch (Theorem 11), the resulting time complexity is ", "page_idx": 30}, {"type": "equation", "text": "$$\nT_{\\mathrm{SGD}}(S,[\\tau_{i}]_{i=1}^{n})=\\frac{\\delta^{0}L_{-}}{\\varepsilon}\\left(1-\\frac{1}{S}+\\frac{L_{\\mathrm{max}}}{\\varepsilon S}\\left(\\delta^{0}+\\Delta^{*}\\right)\\right)\\times\\operatorname*{min}_{j\\in[n]}\\left(\\left(\\sum_{i=1}^{j}\\frac{1}{\\tau_{i}}\\right)^{-1}(S+j)\\right).\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "We now find the optimal $S$ . Assume first that $\\begin{array}{r}{\\frac{L_{\\mathrm{max}}}{\\varepsilon}\\left(\\delta^{0}+\\Delta^{*}\\right)\\leq\\frac{1}{2}}\\end{array}$ .Assumption 5 ensures that the function $f$ is $L_{\\mathrm{max}}$ -smooth. Thus, we have $\\left\\|\\nabla f(x^{0})\\right\\|^{2}\\leq2L_{\\mathrm{max}}\\delta^{0}\\leq\\varepsilon$ and $x^{0}$ is an $\\varepsilon$ -solution. Therefore, we can take any $S\\geq1$ ", "page_idx": 31}, {"type": "text", "text": "Now, suppose that $\\frac{L_{\\mathrm{max}}}{\\varepsilon}\\left(\\delta^{0}+\\Delta^{*}\\right)>\\frac{1}{2}$ . Then we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{\\Lambda}_{\\mathrm{SGD}}^{r}(L_{\\mathrm{max}}/\\varepsilon\\left(\\delta^{0}+\\Delta^{*}\\right),[\\tau_{i}]_{i=1}^{n})=\\frac{\\delta^{0}L_{-}}{\\varepsilon}\\left(2-\\displaystyle\\frac{1}{S}\\right)\\operatorname*{min}_{j\\in[n]}\\left(\\left(\\displaystyle\\sum_{i=1}^{j}\\displaystyle\\frac{1}{\\tau_{i}}\\right)^{-1}\\left(\\displaystyle\\frac{L_{\\mathrm{max}}}{\\varepsilon}\\left(\\delta^{0}+\\Delta^{*}\\right)+j\\right)\\right)}\\\\ &{\\phantom{\\quad\\quad}\\leq\\displaystyle\\frac{2\\delta^{0}L_{-}}{\\varepsilon}\\operatorname*{min}_{j\\in[n]}\\left(\\left(\\displaystyle\\sum_{i=1}^{j}\\displaystyle\\frac{1}{\\tau_{i}}\\right)^{-1}\\left(\\displaystyle\\frac{L_{\\mathrm{max}}}{\\varepsilon}\\left(\\delta^{0}+\\Delta^{*}\\right)+j\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "For all $S>\\operatorname*{max}\\left\\{{\\frac{L_{\\operatorname*{max}}}{\\varepsilon}}\\left(\\delta^{0}+\\Delta^{*}\\right),1\\right\\}$ , we get $S\\geq2$ and hence ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{T_{\\mathrm{SGD}}(S,[\\tau_{i}]_{i=1}^{n})=\\frac{\\delta^{0}L_{-}}{\\varepsilon}\\left(1-\\displaystyle\\frac{1}{S}+\\frac{L_{\\mathrm{max}}}{\\varepsilon S}\\left(\\delta^{0}+\\Delta^{*}\\right)\\right)\\underset{j\\in[n]}{\\operatorname*{min}}\\left(\\left(\\displaystyle\\sum_{i=1}^{j}\\frac{1}{\\tau_{i}}\\right)^{-1}(S+j)\\right)}\\\\ &{\\phantom{\\sum{\\sum{\\limits{\\tau_{i}}\\in\\cal J}}}\\geq\\frac{\\delta^{0}L_{-}}{2\\varepsilon}\\underset{j\\in[n]}{\\operatorname*{min}}\\left(\\left(\\displaystyle\\sum_{i=1}^{j}\\frac{1}{\\tau_{i}}\\right)^{-1}(S+j)\\right)}\\\\ &{\\phantom{\\sum{\\limits{\\tau_{i}}\\in\\cal J}}\\geq\\frac{\\delta^{0}L_{-}}{2\\varepsilon}\\underset{j\\in[n]}{\\operatorname*{min}}\\left(\\left(\\displaystyle\\sum_{i=1}^{j}\\frac{1}{\\tau_{i}}\\right)^{-1}\\left(\\frac{L_{\\mathrm{max}}}{\\varepsilon}\\left(\\delta^{0}+\\Delta^{*}\\right)+j\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Let us now consider the case $1<S\\leq\\mathrm{max}\\left\\{\\frac{L_{\\mathrm{max}}}{\\varepsilon}\\left(\\delta^{0}+\\Delta^{*}\\right),1\\right\\}$ . We can additionally assume that $\\frac{L_{\\mathrm{max}}}{\\varepsilon}\\left(\\delta^{0}+\\Delta^{*}\\right)>1$ and $\\begin{array}{r}{S\\leq\\frac{L_{\\mathrm{max}}}{\\varepsilon}\\left(\\delta^{0}+\\overbar{\\Delta^{*}}\\right)}\\end{array}$ (otherwise, the set $S$ that satisfies the condition $\\begin{array}{r}{1<S\\le\\operatorname*{max}\\left\\{\\frac{L_{\\operatorname*{max}}}{\\varepsilon}\\left(\\delta^{0}+\\Delta^{*}\\right),1\\right\\}}\\end{array}$ is empty). We get ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{T_{\\mathrm{SGD}}(S,[\\tau_{i}]_{i=1}^{n})=\\frac{\\delta^{0}L_{-}}{\\varepsilon}\\left(1-\\frac{1}{S}+\\frac{L_{\\operatorname*{max}}}{\\varepsilon S}\\left(\\delta^{0}+\\Delta^{*}\\right)\\right)\\underset{j\\in[n]}{\\operatorname*{min}}\\left(\\left(\\displaystyle\\sum_{i=1}^{j}\\frac{1}{\\tau_{i}}\\right)^{-1}(S+j)\\right)}\\\\ &{\\phantom{\\sum}\\geq\\frac{\\delta^{0}L_{-}}{\\varepsilon}\\left(\\frac{L_{\\operatorname*{max}}}{\\varepsilon S}\\left(\\delta^{0}+\\Delta^{*}\\right)\\right)\\underset{j\\in[n]}{\\operatorname*{min}}\\left(\\left(\\displaystyle\\sum_{i=1}^{j}\\frac{1}{\\tau_{i}}\\right)^{-1}(S+j)\\right)}\\\\ &{\\phantom{\\sum}=\\frac{\\delta^{0}L_{-}}{\\varepsilon}\\underset{j\\in[n]}{\\operatorname*{min}}\\left(\\left(\\displaystyle\\sum_{i=1}^{j}\\frac{1}{\\tau_{i}}\\right)^{-1}\\left(\\frac{L_{\\operatorname*{max}}}{\\varepsilon}\\left(\\delta^{0}+\\Delta^{*}\\right)+j\\frac{L_{\\operatorname*{max}}}{\\varepsilon S}\\left(\\delta^{0}+\\Delta^{*}\\right)\\right)\\right)}\\\\ &{\\phantom{\\sum}\\geq\\frac{\\delta^{0}L_{-}}{\\varepsilon}\\underset{j\\in[n]}{\\operatorname*{min}}\\left(\\left(\\displaystyle\\sum_{i=1}^{j}\\frac{1}{\\tau_{i}}\\right)^{-1}\\left(\\frac{L_{\\operatorname*{max}}}{\\varepsilon}\\left(\\delta^{0}+\\Delta^{*}\\right)+j\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Finally, for $S=1$ , we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{T_{\\mathrm{SGD}}(S,[\\tau_{i}]_{i=1}^{n})=\\cfrac{\\delta^{0}L_{-}L_{\\mathrm{max}}}{\\varepsilon^{2}}\\left(\\delta^{0}+\\Delta^{*}\\right)\\underset{j\\in[n]}{\\operatorname*{min}}\\left(\\left(\\displaystyle\\sum_{i=1}^{j}\\frac{1}{\\tau_{i}}\\right)^{-1}\\left(1+j\\right)\\right)}\\\\ &{\\phantom{\\sum{\\sum_{\\alpha\\in\\mathrm{D}}\\left(S,[\\tau_{i}]_{i=1}^{n}\\right)}}=\\cfrac{\\delta^{0}L_{-}}{\\varepsilon}\\underset{j\\in[n]}{\\operatorname*{min}}\\left(\\left(\\displaystyle\\sum_{i=1}^{j}\\frac{1}{\\tau_{i}}\\right)^{-1}\\left(\\frac{L_{\\operatorname*{max}}}{\\varepsilon}\\left(\\delta^{0}+\\Delta^{*}\\right)+\\frac{L_{\\operatorname*{max}}}{\\varepsilon}\\left(\\delta^{0}+\\Delta^{*}\\right)j\\right)\\right)}\\\\ &{\\phantom{\\sum{\\sum_{\\alpha\\in\\mathrm{D}}\\left(S,[\\tau_{i}]_{i=1}^{n}\\right)}}\\geq\\cfrac{\\delta^{0}L_{-}}{2\\varepsilon}\\underset{j\\in[n]}{\\operatorname*{min}}\\left(\\left(\\displaystyle\\sum_{i=1}^{j}\\frac{1}{\\tau_{i}}\\right)^{-1}\\left(\\frac{L_{\\operatorname*{max}}}{\\varepsilon}\\left(\\delta^{0}+\\Delta^{*}\\right)+j\\right)\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "because we assume $\\frac{L_{\\mathrm{max}}}{\\varepsilon}\\left(\\delta^{0}+\\Delta^{*}\\right)>\\frac{1}{2}$ Therefore, an optimal choie is $\\begin{array}{r}{S^{*}=\\frac{L_{\\mathrm{max}}}{\\varepsilon}\\left(\\delta^{0}+\\Delta^{*}\\right)}\\end{array}$ ", "page_idx": 31}, {"type": "text", "text": "1 Setup of the Experiments from Section A.1 ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "We consider the optimization problem (1) with nonconvex quadratic functions. The matrices and vectors defining the objective functions $f_{i}$ are generated using Algorithm 9 with $m\\:=\\:10000$ $d=1000$ $\\lambda=1\\mathrm{e}{-6}$ , and $s=10$ . The output is used to construct ", "page_idx": 32}, {"type": "equation", "text": "$$\nf_{i}(x)=\\frac{1}{2}x^{\\top}\\mathbf{A}_{i}x-b_{i}^{\\top}x\\quad\\forall x\\in\\mathbb{R}^{d},\\,\\forall i\\in[m].\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Algorithm 9 Quadratic optimization task generation ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "1: Parameters: number of functions $m$ , dimension $d$ , regularizer $\\lambda$ , noise scale $s$   \n2: for $i=1,\\hdots,m$ do   \n3: Generate random noises $\\nu_{i}^{s}=1+s\\xi_{i}^{s}$ and $\\nu_{i}^{b}=s\\xi_{i}^{b}$ , i.i.d. $\\xi_{i}^{s},\\xi_{i}^{b}\\sim\\mathcal{N}(0,1)$   \n4: Let $\\begin{array}{r}{b_{i}=\\frac{\\nu_{i}^{s}}{4}(-1+\\nu_{i}^{b},0,\\cdots\\,,0)\\in\\mathbb{R}^{d}}\\end{array}$   \n5: Take the initial tridiagonal matrix ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathbf{A}_{i}=\\frac{\\nu_{i}^{s}}{4}\\left(\\begin{array}{c c c c}{2}&{-1}&&{0}\\\\ {-1}&{\\ddots}&{\\ddots}\\\\ &{\\ddots}&{\\ddots}&\\\\ {0}&&{-1}&{2}\\end{array}\\right)\\in\\mathbb{R}^{d\\times d}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "6: end for ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "7:Takethe meanof matrices $\\textstyle\\mathbf{A}={\\frac{1}{m}}\\sum_{i=1}^{m}\\mathbf{A}_{i}$   \n8: Find the minimum eigenvalue $\\lambda_{\\mathrm{min}}(\\mathbf{A})$   \n9: for $i=1,\\hdots,m$ do   \n10: Update matrix $\\mathbf{A}_{i}=\\mathbf{A}_{i}+(\\lambda-\\lambda_{\\operatorname*{min}}(\\mathbf{A}))\\mathbf{I}$ ", "page_idx": 32}, {"type": "text", "text": "[2: Take starting point $x^{0}=({\\sqrt{d}},0,\\cdots,0)$ [3: Output: matrices $\\mathbf{A}_{1},\\cdots,\\mathbf{A}_{m}$ , vectors $b_{1},\\cdot\\cdot\\cdot,b_{m}$ , starting point $x^{0}$ ", "page_idx": 32}, {"type": "text", "text": "Lower bound ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "J.1   Time multiple oracles protocol ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "The classical lower bound frameworks [Nemirovskij and Yudin, 1983, Carmon et al., 2020, Arjevani et al., 2022, Nesterov, 2018] are not convenient in the analysis of parallel algorithms since they are designed to estimate lower bounds on iteration complexities. In order to obtain time complexity lower bounds, we use the framework by Tyurin and Richtarik [2023]. Let us briefly explain the main idea. A more detailed explanation can be found in [Tyurin and Richtarik, 2023][Sections 3-6]. ", "page_idx": 33}, {"type": "text", "text": "We start by introducing an appropriate oracle for our setup: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{O_{\\tau}\\,:\\,\\underbrace{{\\mathbb{R}}_{\\geq0}\\times{\\underbrace{{\\mathbb{R}}^{d}}_{\\mathrm{point}}}}_{\\mathrm{inne}}\\times\\underbrace{({\\mathbb{R}}_{\\geq0}\\times{\\mathbb{R}}^{d}\\times\\{0,1\\})}_{\\mathrm{input~state}}\\rightarrow\\underbrace{({\\mathbb{R}}_{\\geq0}\\times{\\mathbb{R}}^{d}\\times\\{0,1\\})}_{\\mathrm{output~state}}\\times{\\mathbb{R}}^{d}}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "equation", "text": "$$\nO_{\\tau}(t,x,(s_{t},s_{x},s_{q}))=\\left\\{\\!\\!\\begin{array}{l l}{((t,x,1),}&{0),\\quad s_{q}=0,}\\\\ {((s_{t},s_{x},1),}&{0),\\quad s_{q}=1,t<s_{t}+\\tau,}\\\\ {((0,0,0),}&{\\nabla f_{j}(s_{x})),}&{s_{q}=1,t\\geq s_{t}+\\tau,}\\end{array}\\!\\!\\right.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where $j\\sim\\mathrm{Uniform}([m])$ , i.e., $j$ is a random index sampled uniformly from the set $[m]$ . We assume that all draws from Uniform $([m])$ are i.i.d.. ", "page_idx": 33}, {"type": "text", "text": "Next, we define the time multiple oracles protocol, first introduced in [Tyurin and Richtarik, 2023]. ", "page_idx": 33}, {"type": "table", "img_path": "AUeTkSymOq/tmp/e66e54c5770616c1037aa0fb98b778b0bf74e954e4878c421e8fddf28d04a93b.jpg", "table_caption": [], "table_footnote": [], "page_idx": 33}, {"type": "text", "text": "Let us explain the behavior of the protocol. At each iteration, the algorithm $A$ returns three outputs, based on the available information/gradients: time $t^{k+1}$ , the index of a worker $i^{k+1}$ , and a new point $x^{k}$ . Depending on the current time $t^{k+1}$ and the state of the worker, three options are possible (see (29)). If $s_{q}\\,=\\,0$ , then the worker is idle. It then starts calculations at the point $x^{k}$ , changes the state $s_{q}$ from 0 to 1, stores the point $x^{k}$ in $s_{x}$ (at which a new stochastic gradient should be calculated), and returns a zero vector. If $s_{q}=0$ and $t^{k+1}<s_{t}+\\tau$ then the worker is still calculating a stochastic gradient. It does not change the state and returns a zero vector because the computation has not finished yet. If $s_{q}=0$ and $t^{k+1}\\geq s_{t}+\\tau$ the worker can finally return a stochastic gradient at $s_{x}$ because sufficient time has passed since the worker was idle $\\(s_{q}=0)$ ). Note that with this oracle, the algorithm will never receive the first stochastic gradient before time $\\tau$ (assuming that all oracles have the same processing time $\\tau$ ; in general, we will assume that the processing times are different). ", "page_idx": 33}, {"type": "text", "text": "In the setting considered in this work, there are $n$ oracles that can do calculations in parallel, and an algorithm orchestrates their work. Let the processing times of the oracles be equal to $\\tau_{1},\\dots,\\tau_{n}$ :A reasonable strategy would be to call each oracle with $t^{k}=0$ , then to call the fastest worker with $t^{k}=\\mathrm{min}_{i\\in[n]}\\,\\tau_{i}$ to get the first stochastic gradients as soon as possible, then to call this worker again With $t^{k}=\\operatorname*{min}_{i\\in[n]}\\tau_{i}$ to request calculation of the next stochastic gradient, and so on. One unusual thing about this protocol is that the algorithm controls the time. The oracle is designed to force the algorithm to increase the time; otherwise, the algorithm would not receive new information about the function. ", "page_idx": 33}, {"type": "text", "text": "Our goal will be to bound the complexity measure ${\\mathfrak{m}}_{\\mathrm{time}}\\left({\\mathcal{A}},{\\mathcal{F}}\\right)$ , defined as ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathfrak{m}_{\\mathrm{time}}\\left(A,\\mathcal{F}\\right):=\\underset{A\\in\\mathcal{A}}{\\operatorname*{inf}}\\underset{f\\in\\mathcal{F}}{\\operatorname*{sup}}\\underset{\\left\\{O_{i}\\right\\}\\in\\mathcal{O}(f)}{\\operatorname*{sup}}\\,\\operatorname*{inf}\\left\\lbrace t\\geq0\\left|\\mathbb{E}\\left[\\underset{k\\in S_{t}}{\\operatorname*{inf}}\\,\\left\\Vert\\nabla f(x^{k})\\right\\Vert^{2}\\right]\\leq\\varepsilon\\right\\rbrace,}\\\\ &{S_{t}:=\\left\\lbrace k\\in\\mathbb{N}_{0}\\middle|t^{k}\\leq t\\right\\rbrace,}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where thesequences $t^{k}$ and $x^{k}$ are generated by Protocol 10. Hence, unlike the classical approach, where the lower bounds are obtained for the minimum number of iterations required to find an $\\varepsilon,$ -stationary point, we seek to find the minimum time needed to get an $\\varepsilon,$ -stationarypoint. ", "page_idx": 34}, {"type": "text", "text": "We consider a standard for our setup class of functions [Fang et al., 2018]: ", "page_idx": 34}, {"type": "text", "text": "Definition 26 (Funtion Clas $\\mathcal{F}_{\\delta^{0},L_{+}}^{m})$ .We say that $f\\in\\mathcal{F}_{\\delta^{0},L_{+}}^{m}$ ifitis $\\delta^{0}$ bounded, i.e., $f(0)\\mathrm{~-~}$ $\\operatorname*{inf}_{x\\in\\mathbb{R}^{d}}f(x)\\leq\\delta^{0}$ ,and ", "page_idx": 34}, {"type": "equation", "text": "$$\nf(x)={\\frac{1}{m}}\\sum_{i=1}^{m}f_{i}(x),\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where the functions $f_{i}\\,:\\,\\mathbb{R}^{d}\\to\\mathbb{R}$ are differentiable and satisfy ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\frac{1}{m}\\sum_{i=1}^{m}\\left\\|\\nabla f_{i}(x)-\\nabla f_{i}(y)\\right\\|^{2}\\leq L_{+}^{2}\\left\\|x-y\\right\\|^{2}\\quad\\forall x,y\\in\\mathbb{R}^{d}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Next, we define the class of algorithms we will analyze. ", "page_idx": 34}, {"type": "text", "text": "Definition 27 (Algorithm Class $\\mathcal{A}_{\\mathrm{zr}})$ . Let us consider Protocol 10. We say that a sequence of mappings $A=\\{A^{\\overline{{k}}}\\}_{k=0}^{\\infty}$ is a zero-respecting algorithm, if ", "page_idx": 34}, {"type": "text", "text": "We denote the set of all algorithms with these properties as $A_{\\mathrm{zr}}$ ", "page_idx": 34}, {"type": "text", "text": "In the above definition, property 1 defines the domain of the mappings $A^{k}$ , and property 2 ensures that our algorithm does not \u201ccheat\u2019 and does not \"travel into the past': the time can only go forward (see [Tyurin and Richtarik, 2023][Section 4, Definition 4.1]). Property 3 is a standard assumption for zero-respecting algorithms [Arjevani et al., 2022] that is satisfied by virtually all algorithms, including Adam [Kingma and Ba, 2015], SGD, PAGE [Li et al., 2021] and Asynchronous SGD. ", "page_idx": 34}, {"type": "text", "text": "It remains to define an oracle class for our problem that employs oracles from (29). We design oracles that emulate the real behavior of the workers. ", "page_idx": 34}, {"type": "text", "text": "Denition8CpuatorlCs $\\mathcal{O}_{\\tau_{1},\\dots,\\tau_{n}},$ .Forany $f\\in\\mathcal{F}_{\\delta^{0},L_{+}}^{m}$ $O_{\\tau_{1},\\dots,\\tau_{n}}$ returns oracles $O_{i}=O_{\\tau_{i}}$ \uff0c $i\\in[n]$ , where the mappings $O_{\\tau_{i}}$ are defined in (29). ", "page_idx": 34}, {"type": "text", "text": "J.2  The \u201cworst case\" function in the nonconvex world ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "The analysis uses a standard function, commonly employed to derive lower bounds in the nonconvex regime. First, let us define ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathrm{prog}(x):=\\operatorname*{max}\\{i\\geq0\\,|\\,x_{i}\\neq0\\}\\quad(x_{0}\\equiv1).\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Our choice of the underlying function $F$ follows the construction introduced in Carmon et al. [2020], Arjevani et al. [2022]: for any $T\\in\\mathbb N$ ,define ", "page_idx": 34}, {"type": "equation", "text": "$$\nF_{T}(x):=-\\Psi(1)\\Phi(x_{1})+\\sum_{i=2}^{T}\\left[\\Psi(-x_{i-1})\\Phi(-x_{i})-\\Psi(x_{i-1})\\Phi(x_{i})\\right],\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\Psi(x)=\\left\\{\\!\\!\\!\\begin{array}{l l}{0,}&{x\\leq1/2,}\\\\ {\\exp\\left(1-\\frac{1}{(2x-1)^{2}}\\right),}&{x\\geq1/2,}\\end{array}\\right.\\,\\,\\mathrm{and}\\quad\\Phi(x)=\\sqrt{e}\\int_{-\\infty}^{x}e^{-\\frac{1}{2}t^{2}}d t.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Throughout the proof, we only rely on the following properties of the function: ", "page_idx": 34}, {"type": "text", "text": "Lemma 1 (Carmon et al. [2020], Arjevani et al. [2022]). The function $F_{T}$ satisfies: ", "page_idx": 35}, {"type": "text", "text": "1. $F_{T}(0)-\\operatorname*{inf}_{x\\in\\mathbb{R}^{T}}F_{T}(x)\\leq\\Delta^{0}T_{!}$ where $\\Delta^{0}=12$   \n2.Thefunction $F_{T}$ is $l_{1}$ smooth,where $l_{1}=152$   \n3. For all $x\\in\\mathbb{R}^{T}$ \uff0c $\\|\\nabla F_{T}(x)\\|_{\\infty}\\leq\\gamma_{\\infty}$ , where $\\gamma_{\\infty}=23$   \n4. For all $x\\in\\mathbb{R}^{T}$ \uff0c $\\mathrm{prog}(\\nabla F_{T}(x))\\leq\\mathrm{prog}(x)+1.$   \n5. For all $x\\in\\mathbb{R}^{T}$ \uff0c $i f\\mathrm{prog}(x)<T$ then $\\|\\nabla F_{T}(x)\\|>1$ ", "page_idx": 35}, {"type": "text", "text": "J.3 The first lower bound ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "We are ready to present the main results of this section. ", "page_idx": 35}, {"type": "text", "text": "Theorem 29. Let us consider Protocol 10. Without loss of generality, assume that $0<\\tau_{1}\\leq\\cdot\\cdot\\leq\\tau_{n}$ and take any $L_{+},\\delta^{0},\\varepsilon>0$ and $m\\in\\mathbb{N}$ suchthat $\\varepsilon<c_{1}L_{+}\\delta^{0}$ and $\\frac{\\delta^{0}L_{+}}{\\varepsilon}>c_{2}\\sqrt{m}$ Then,for any algorithm $A\\in{\\mathcal{A}}_{\\mathrm{zr}}$ , there exists a function $f\\in\\mathcal{F}_{\\delta^{0},L_{+}}^{m}$ and computation oracles $(O_{1},\\dots,O_{n})\\in$ $\\mathcal{O}_{\\tau_{1},...,\\tau_{n}}(f)$ such that $\\mathbb{E}\\left[\\operatorname*{inf}_{k\\in S_{t}}\\left\\|\\nabla f(x^{k})\\right\\|^{2}\\right]>\\varepsilon$ where ${S_{t}:=\\left\\{k\\in\\mathbb{N}_{0}\\,|\\,t^{k}\\leq t\\right\\}}$ and ", "page_idx": 35}, {"type": "equation", "text": "$$\nt=c_{3}\\times\\frac{\\delta^{0}L_{+}}{\\sqrt{m}\\varepsilon}\\operatorname*{min}_{j\\in[n]}\\left[\\left(\\sum_{i=1}^{j}\\frac{1}{\\tau_{i}}\\right)^{-1}(m+j)\\right].\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Thequantities $c_{1},c_{2}$ ,and $c_{3}$ are universal constants. The sequences $x^{k}$ and $t^{k}$ are defined in Protocol 10. ", "page_idx": 35}, {"type": "text", "text": "Proof. (Step 1: Construction of a hard problem) ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "We start our proof by constructing an appropriate function $f\\in\\mathcal{F}_{\\delta^{0},L_{+}}^{m}$ . Let us fix any $T\\geq\\mathbb{N}$ and define $f_{i}\\,:\\,\\mathbb{R}^{T}\\rightarrow\\mathbb{R}$ such that ", "page_idx": 35}, {"type": "equation", "text": "$$\nf_{1}:=\\frac{\\sqrt{m}L_{+}\\lambda^{2}}{l_{1}}F_{T}\\left(\\frac{x}{\\lambda}\\right)\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "for all $x\\in\\mathbb{R}^{T}$ , and $f_{i}(x)=0$ for all $i\\in\\{2,\\ldots,m\\}$ and $x\\in\\mathbb{R}^{T}$ . Essentially, all information about the function =1 f is in the first function. Note that ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{1}{m}\\sum_{i=1}^{m}\\|\\nabla f_{i}(x)-\\nabla f_{i}(y)\\|^{2}=\\displaystyle\\frac{1}{m}\\left\\|\\nabla f_{1}(x)-\\nabla f_{1}(y)\\right\\|^{2}}\\\\ {\\displaystyle=\\frac{1}{m}\\left\\|\\frac{\\sqrt{m}L_{+}\\lambda}{l_{1}}\\nabla F_{T}\\left(\\frac{x}{\\lambda}\\right)-\\frac{\\sqrt{m}L_{+}\\lambda}{l_{1}}\\nabla F_{T}\\left(\\frac{y}{\\lambda}\\right)\\right\\|^{2}}\\\\ {\\displaystyle=\\frac{L_{+}^{2}\\lambda^{2}}{l_{1}^{2}}\\left\\|\\nabla F_{T}\\left(\\frac{x}{\\lambda}\\right)-\\nabla F_{T}\\left(\\frac{y}{\\lambda}\\right)\\right\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Then, using Lemma 1, we have ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\frac{1}{m}\\sum_{i=1}^{m}\\left\\|\\nabla f_{i}(x)-\\nabla f_{i}(y)\\right\\|^{2}\\leq L_{+}^{2}\\lambda^{2}\\left\\|\\frac{x}{\\lambda}-\\frac{y}{\\lambda}\\right\\|^{2}=L_{+}^{2}\\left\\|x-y\\right\\|^{2}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Taking ", "page_idx": 35}, {"type": "equation", "text": "$$\nT=\\left\\lfloor\\frac{\\sqrt{m}\\delta^{0}l_{1}}{L_{+}\\lambda^{2}\\Delta^{0}}\\right\\rfloor,\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "we ensure that ", "page_idx": 35}, {"type": "equation", "text": "$$\nf(0)-\\operatorname*{inf}_{x\\in\\mathbb{R}^{T}}f(x)=\\frac{1}{m}\\left(\\frac{\\sqrt{m}L_{+}\\lambda^{2}}{l_{1}}F_{T}\\left(0\\right)-\\operatorname*{inf}_{x\\in\\mathbb{R}^{T}}\\frac{\\sqrt{m}L_{+}\\lambda^{2}}{l_{1}}F_{T}\\left(x\\right)\\right)\n$$", "text_format": "latex", "page_idx": 35}, {"type": "equation", "text": "$$\n=\\frac{L_{+}\\lambda^{2}}{\\sqrt{m}l_{1}}\\left(F_{T}\\left(0\\right)-\\operatorname*{inf}_{x\\in\\mathbb{R}^{T}}F_{T}\\left(x\\right)\\right)\\le\\frac{L_{+}\\lambda^{2}\\Delta^{0}T}{\\sqrt{m}l_{1}}\\le\\delta^{0},\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where in the inequalities we use Lemma 1 and the choice of $T$ Now, inequalities (32) and (33) imply that $\\begin{array}{r}{f={\\frac{1}{m}}\\sum_{i=1}^{m^{\\star}}f_{i}\\in\\mathcal{F}_{\\delta^{0},L_{+}}^{m}}\\end{array}$ and hence, using Lemma again, weget ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{k\\in S^{t}}{\\operatorname*{inf}}\\left\\|\\nabla f(x^{k})\\right\\|^{2}=\\underset{k\\in S^{t}}{\\operatorname*{inf}}\\left\\|\\frac{1}{m}\\times\\frac{\\sqrt{m}L_{+}\\lambda}{l_{1}}\\nabla F_{T}\\left(\\frac{x^{k}}{\\lambda}\\right)\\right\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad=\\frac{L_{+}^{2}\\lambda^{2}}{m l_{1}^{2}}\\underset{k\\in S^{t}}{\\operatorname*{inf}}\\left\\|\\nabla F_{T}\\left(\\frac{x}{\\lambda}\\right)\\right\\|^{2}>\\frac{L_{+}^{2}\\lambda^{2}}{m l_{1}^{2}}\\underset{k\\in S^{t}}{\\operatorname*{inf}}\\mathbb{1}\\left[\\mathrm{prog}(x^{k})<T\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Let us take ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\lambda=\\frac{2l_{1}\\sqrt{m}\\sqrt{\\varepsilon}}{L_{+}}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Then ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{k\\in S^{t}}\\left\\|\\nabla f(x^{k})\\right\\|^{2}>4\\varepsilon\\operatorname*{inf}_{k\\in S^{t}}\\mathbb{1}\\left[\\mathrm{prog}(x^{k})<T\\right]\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "and ", "page_idx": 36}, {"type": "equation", "text": "$$\nT=\\left\\lfloor{\\frac{\\delta^{0}L_{+}}{4l_{1}\\Delta^{0}{\\sqrt{m}}\\varepsilon}}\\right\\rfloor.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "The last inequality means that while $\\mathrm{prog}(x^{k})\\,<\\,T$ for all $k\\;\\in\\;S^{t}$ , all gradients are large. The function $F_{T}$ is a zero-chain: due to Lemma 1, we know that $\\mathrm{prog}(\\nabla F_{T}(x))\\leq\\mathrm{prog}(x)+1$ for all $x\\in\\mathbb{R}^{T}$ . This implies that we can discover at most one new non-zero coordinate by calculating the gradient of the function $\\nabla f_{1}$ . Since the algorithm $A\\in{\\mathcal{A}}_{\\mathrm{zr}}$ is zero-respecting, by definition it cannot return a point $x^{k}$ with progress greater than that of the vectors returned by the oracles. In view of this, it is necessary to calculate the gradient of $f_{1}$ at least $T$ times to get $\\mathsf{p r o g}(x^{k})\\geq T$ ", "page_idx": 36}, {"type": "text", "text": "The gradient of $f_{1}$ can be calculated if and only if $j\\,=\\,1$ , where $j\\,\\sim\\,\\mathrm{Uniform}([m])$ (see (29)). Consider worker $i$ and define $\\eta_{i}^{1}$ to be the number of draws from Uniform $([m])$ until the index $j=1$ is sampled. Clearly, $\\eta_{i}^{1}$ is a Geometric random variable with parameter $\\textstyle\\mathbb{P}\\,{\\dot{(}\\,j=1)}={\\frac{1}{m}}$ . Recall that the workers can do the computations in parallel, and by the design of the oracles, worker $i$ needs at least $\\tau_{i}\\eta_{i}^{1}$ seconds to calculate $\\eta_{i}^{1}$ stochastic gradients. Hence, it is impossible to calculate $\\nabla f_{1}$ before the time ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{i\\in[n]}\\tau_{i}\\eta_{i}^{1}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Once the algorithm calculates $\\nabla f_{1}$ for the first time, it needs to do so at least $T-1$ times more to achieve $\\operatorname{prog}(x^{k})\\geq T$ . Thus, one should wait at least ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{T}\\operatorname*{min}_{i\\in[n]}\\tau_{i}\\eta_{i}^{k}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "seconds, where $\\eta_{i}^{k}\\overset{\\mathrm{i.i.d.}}{\\sim}$ Geometric $\\left(1/{\\mathfrak{m}}\\right)$ . We can conclude that ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\operatorname*{inf}_{k\\in S^{t}}\\mathbb{1}\\left[\\mathrm{prog}(x^{k})<T\\right]=0\\right)\\leq\\mathbb{P}\\left(\\sum_{k=1}^{T}\\operatorname*{min}_{i\\in[n]}\\tau_{i}\\eta_{i}^{k}\\leq t\\right).\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "(Step 2: The Chernoff Method) ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "The theorem's proof is now reduced to the analysis of the concentration of $\\begin{array}{r}{\\sum_{k=1}^{T}\\operatorname*{min}_{i\\in[n]}\\tau_{i}\\eta_{i}^{k}}\\end{array}$ Using the Chernoff method, for all $s>0$ wehave ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\mathbb{P}\\left(\\displaystyle\\sum_{k=1}^{T}\\operatorname*{min}_{i\\in[n]}\\tau_{i}\\eta_{i}^{k}\\leq t\\right)=\\mathbb{P}\\left(-s\\displaystyle\\sum_{k=1}^{T}\\operatorname*{min}_{i\\in[n]}\\tau_{i}\\eta_{i}^{k}\\geq-s t\\right)}\\\\ {=\\mathbb{P}\\left(e^{-s\\sum_{k=1}^{T}\\operatorname*{min}_{i\\in[n]}\\tau_{i}\\eta_{i}^{k}}\\geq e^{-s t}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "equation", "text": "$$\n\\leq e^{s t}\\mathbb{E}\\left[\\exp\\left(-s\\sum_{k=1}^{T}\\operatorname*{min}_{i\\in[n]}\\tau_{i}\\eta_{i}^{k}\\right)\\right].\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Independence gives ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\sum_{k=1}^{T}\\operatorname*{min}_{i\\in[n]}\\tau_{i}\\eta_{i}^{k}\\leq t\\right)\\leq e^{s t}\\prod_{k=1}^{T}\\mathbb{E}\\left[\\exp\\left(-s\\operatorname*{min}_{i\\in[n]}\\tau_{i}\\eta_{i}^{k}\\right)\\right]}\\\\ {\\overset{\\mathrm{i.i.d.}}{=}e^{s t}\\left(\\mathbb{E}\\left[\\exp\\left(-s\\operatorname*{min}_{i\\in[n]}\\tau_{i}\\eta_{i}^{1}\\right)\\right]\\right)^{T}.}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Let us consider the term in the last bracket separately. For a fixed $t^{\\prime}>0$ wehave ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\exp\\left(-\\operatorname*{sun}_{i\\in[n]}\\tau_{i}\\eta_{i}^{1}\\right)\\right]}\\\\ &{\\;\\;=\\;\\;\\mathbb{E}\\left[\\operatorname*{max}_{i\\in[n]}\\exp\\left(-s\\tau_{i}\\eta_{i}^{1}\\right)\\right]}\\\\ &{\\;\\;=\\;\\;\\mathbb{E}\\left[\\operatorname*{max}_{i\\in[n]}\\left(\\mathbb{I}\\left[\\tau_{i}\\eta_{i}^{1}\\leq t^{\\prime}\\right]\\exp\\left(-s\\tau_{i}\\eta_{i}^{1}\\right)+\\left(1-\\mathbb{I}\\left[\\tau_{i}\\eta_{i}^{1}\\leq t^{\\prime}\\right]\\right)\\exp\\left(-s\\tau_{i}\\eta_{i}^{1}\\right)\\right)\\right]}\\\\ &{\\;\\;\\leq\\;\\;\\mathbb{E}\\left[\\operatorname*{max}_{i\\in[n]}\\left(\\mathbb{I}\\left[\\tau_{i}\\eta_{i}^{1}\\leq t^{\\prime}\\right]+\\left(1-\\mathbb{I}\\left[\\tau_{i}\\eta_{i}^{1}\\leq t^{\\prime}\\right]\\right)\\exp\\left(-s t^{\\prime}\\right)\\right)\\right]}\\\\ &{\\;\\;=\\;\\;\\exp\\left(-s t^{\\prime}\\right)+\\left(1-\\exp\\left(-s t^{\\prime}\\right)\\mathbb{E}\\left[\\operatorname*{max}_{i\\in[n]}\\left(\\mathbb{I}\\left[\\tau_{i}\\eta_{i}^{1}\\leq t^{\\prime}\\right]\\right)\\right]\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "We now consider the last term. Due to the independence, we have ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\operatorname*{max}_{i\\in[n]}\\left(\\mathbb{1}\\left[\\tau_{i}\\eta_{i}^{1}\\leq t^{\\prime}\\right]\\right)\\right]=1-\\prod_{i=1}^{n}\\mathbb{P}\\left(\\tau_{i}\\eta_{i}^{1}>t^{\\prime}\\right)=1-\\prod_{i=1}^{n}\\left(1-p\\right)^{\\left\\lfloor\\frac{t^{\\prime}}{\\tau_{i}}\\right\\rfloor},\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where we use the cumulative distribution function of a geometric random variable and temporarily define p :=m\u00b7 . Using Lemma 3, we get ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\operatorname*{max}_{i\\in[n]}\\left(\\mathbb{1}\\left[\\tau_{i}\\eta_{i}^{1}\\leq t^{\\prime}\\right]\\right)\\right]\\leq p\\sum_{i=1}^{n}\\left\\lfloor\\frac{t^{\\prime}}{\\tau_{i}}\\right\\rfloor.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Let us take ", "page_idx": 37}, {"type": "equation", "text": "$$\nt^{\\prime}=\\frac{1}{8}\\times\\operatorname*{min}_{j\\in[n]}\\left(\\sum_{i=1}^{j}\\frac{1}{\\tau_{i}}\\right)^{-1}\\left(\\frac{1}{p}+j\\right)=\\frac{1}{8}\\times\\operatorname*{min}_{j\\in[n]}g(j),\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where $\\begin{array}{r}{g(j):=\\left(\\sum_{i=1}^{j}\\frac{1}{\\tau_{i}}\\right)^{-1}\\left(\\frac{1}{p}+j\\right)}\\end{array}$ for all $j\\in[n]$ and assume that $j^{*}$ is the largest index such that $\\begin{array}{r}{\\operatorname*{min}_{j\\in[n]}g(j)=g(j^{*})}\\end{array}$ . Then, Lemma 4 gives ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\tau_{j^{\\ast}}\\leq\\operatorname*{min}_{j\\in[n]}g(j)<\\tau_{j^{\\ast}+1},\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where we let $\\tau_{n+1}\\equiv\\infty$ . Therefore, $t^{\\prime}<\\tau_{j^{*}+1}$ and (39) gives ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\operatorname*{max}_{i\\in[n]}\\left(\\mathbb{1}\\left[\\tau_{i}\\eta_{i}^{1}\\leq t^{\\prime}\\right]\\right)\\right]\\leq p\\sum_{i=1}^{n}\\left\\lfloor\\frac{t^{\\prime}}{\\tau_{i}}\\right\\rfloor=p\\sum_{i=1}^{j^{*}}\\left\\lfloor\\frac{t^{\\prime}}{\\tau_{i}}\\right\\rfloor.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Using (40) we get obtain = mingm) \u2265foralli \u2264j\\* Since [] \u22642x-\u2193forall  \u2265;we ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\operatorname*{max}_{i\\in[n]}\\left(\\mathbb{1}\\left[\\tau_{i}\\eta_{i}^{1}\\leq t^{\\prime}\\right]\\right)\\right]\\leq p\\sum_{i=1}^{j^{*}}\\left(\\frac{2t^{\\prime}}{\\tau_{i}}-\\frac{1}{4}\\right)=2p t^{\\prime}\\left(\\sum_{i=1}^{j^{*}}\\frac{1}{\\tau_{i}}\\right)-\\frac{p j^{*}}{4}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle=2p\\times\\frac{1}{8}\\left(\\sum_{i=1}^{j^{*}}\\frac{1}{\\tau_{i}}\\right)^{-1}\\left(\\frac{1}{p}+j^{*}\\right)\\times\\left(\\sum_{i=1}^{j^{*}}\\frac{1}{\\tau_{i}}\\right)-\\frac{p j^{*}}{4}}\\\\ {\\displaystyle=\\frac{p}{4}\\left(\\frac{1}{p}+j^{*}\\right)-\\frac{p j^{*}}{4}=\\frac{1}{4}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Substituting the last inequality to (38) gives ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\exp\\left(-s\\operatorname*{min}_{i\\in[n]}\\tau_{i}\\eta_{i}^{1}\\right)\\right]\\leq\\exp\\left(-s t^{\\prime}\\right)+\\frac{1}{4}(1-\\exp\\left(-s t^{\\prime}\\right)).\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "We now take $s=1/t^{\\prime}$ to obtain ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\exp\\left(-s\\operatorname*{min}_{i\\in[n]}\\tau_{i}\\eta_{i}^{1}\\right)\\right]\\leq e^{-1}+\\frac{1}{4}(1-e^{-1})\\leq e^{-\\frac{1}{2}}.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Substituting this inequality in (35) and (36) gives ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\operatorname*{inf}_{k\\in S^{t}}\\mathbb{1}\\left[{\\mathrm{prog}}(x^{k})<T\\right]=0\\right)\\leq\\mathbb{P}\\left(\\sum_{k=1}^{T}\\operatorname*{min}_{i\\in[n]}\\tau_{i}\\eta_{i}^{k}\\leq t\\right)\\leq e^{\\frac{t}{t^{\\prime}}-\\frac{T}{2}}.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Therefore, ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\operatorname*{inf}_{k\\in S^{t}}\\mathbb{1}\\left[{\\mathrm{prog}(x^{k})<T}\\right]=0\\right)\\leq\\rho\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "for all ", "page_idx": 38}, {"type": "equation", "text": "$$\nt\\leq\\frac{1}{8}\\operatorname*{min}_{j\\in[n]}\\left[\\left(\\sum_{i=1}^{j}\\frac{1}{\\tau_{i}}\\right)^{-1}\\left(\\frac{1}{p}+j\\right)\\right]\\left(\\frac{T}{2}+\\log\\rho\\right)\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "and $\\rho>0$ .Using the bound on the probability with $\\rho=\\textstyle{\\frac{1}{2}}$ and (34), we finally conclude ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\operatorname*{inf}_{k\\in S^{t}}\\left\\|\\nabla f(x^{k})\\right\\|^{2}\\right]>4\\varepsilon\\mathbb{P}\\left(\\operatorname*{inf}_{k\\in S^{t}}\\mathbb{1}\\left[\\mathrm{prog}(x^{k})<T\\right]=1\\right)\\geq2\\varepsilon\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "for all ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{t\\leq\\frac{1}{8}\\operatorname*{min}_{j\\in[n]}\\left[\\left(\\displaystyle\\sum_{i=1}^{j}\\frac{1}{\\tau_{i}}\\right)^{-1}\\left(\\frac{1}{p}+j\\right)\\right]\\left(\\frac{T}{2}+\\log{\\frac{1}{2}}\\right)}}\\\\ {{\\,\\,=\\frac{1}{8}\\operatorname*{min}_{j\\in[n]}\\left[\\left(\\displaystyle\\sum_{i=1}^{j}\\frac{1}{\\tau_{i}}\\right)^{-1}(m+j)\\right]\\left(\\frac{1}{2}\\left\\lfloor\\frac{\\delta^{0}L_{+}}{4l_{1}\\Delta^{0}\\sqrt{m}\\varepsilon}\\right\\rfloor+\\log{\\frac{1}{2}}\\right).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "It remains to use the conditions $\\varepsilon\\;<\\;c_{1}L\\delta^{0}$ and $\\frac{\\delta^{0}L_{+}}{\\varepsilon}\\;>\\;c_{2}\\sqrt{m}$ from the theorem to finish the proof. \u53e3 ", "page_idx": 38}, {"type": "text", "text": "J.4The second lower bound ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Theorem 30.Let us consider Protocol 10.Without loss of generality, assume that $0<\\tau_{1}\\leq\\cdot\\cdot\\leq\\tau_{n}$ and take any $L_{+},\\delta^{0},\\varepsilon>0$ ,and $m\\in\\mathbb{N}$ such that $\\varepsilon<\\bar{c}_{1}\\bar{L}_{+}\\delta^{0}$ Then,foranyalgorithm $A\\in{\\mathcal{A}}_{\\mathrm{zr}}$ there exists afunction $f\\in\\mathcal{F}_{\\delta^{0},L_{+}}^{m}$ andcomputationoracles $(O_{1},\\ldots,O_{n})\\in\\mathcal{O}_{\\tau_{1},\\ldots,\\tau_{n}}(f)$ such that $\\mathbb{E}\\left[\\operatorname*{inf}_{k\\in S_{t}}\\left\\|\\nabla f(x^{k})\\right\\|^{2}\\right]>\\varepsilon$ where ${S_{t}:=\\left\\{k\\in\\mathbb{N}_{0}\\,|\\,t^{k}\\leq t\\right\\}}$ and ", "page_idx": 38}, {"type": "equation", "text": "$$\nt=c_{2}\\times\\operatorname*{min}_{j\\in[n]}\\left(\\sum_{i=1}^{j}\\frac{1}{\\tau_{i}}\\right)^{-1}(m+j)\\,.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "The quantities $c_{1}$ and $c_{2}$ are universal constants. The sequences $x^{k}$ and $t^{k}$ are defined in Protocol 10. ", "page_idx": 38}, {"type": "text", "text": "Proof. We use the same construction as in the proof of Theorem 2 of Li et al. [2021]. Let us consider ", "page_idx": 39}, {"type": "equation", "text": "$$\nf_{i}(x):=c\\left\\langle v_{i},x\\right\\rangle+\\frac{L_{+}}2\\left\\|x\\right\\|^{2}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "for all $x\\in\\mathbb{R}^{T}$ ,where $c\\in\\mathbb{R}$ and $v_{i}\\in\\mathbb{R}^{T}$ $i\\in[n]$ are parameters that we define later. Then ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\frac{1}{m}\\sum_{i=1}^{m}\\left\\|\\nabla f_{i}(x)-\\nabla f_{i}(y)\\right\\|^{2}\\leq L_{+}^{2}\\left\\|x-y\\right\\|^{2}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "for all $x,y\\in\\mathbb{R}^{d}$ and ", "page_idx": 39}, {"type": "equation", "text": "$$\nf(0)-\\operatorname*{inf}_{x\\in\\mathbb R^{T}}f(x)=-\\operatorname*{inf}_{x\\in\\mathbb R^{T}}\\left[c\\left\\langle\\frac{1}{m}\\sum_{i=1}^{m}v_{i},x\\right\\rangle+\\frac{L_{+}}{2}\\left\\Vert x\\right\\Vert^{2}\\right]=\\frac{c^{2}}{2L_{+}m^{2}}\\left\\Vert\\sum_{i=1}^{m}v_{i}\\right\\Vert^{2}=\\delta^{0},\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "where we take ", "page_idx": 39}, {"type": "equation", "text": "$$\nc:=\\sqrt{\\frac{2L_{+}m^{2}\\delta^{0}}{\\left\\|\\sum_{i=1}^{m}v_{i}\\right\\|^{2}}}.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Thus, we have $\\begin{array}{r}{f=\\frac{1}{m}\\sum_{i=1}^{m}f_{i}\\in\\mathcal{F}_{\\delta^{0},L_{+}}^{m}}\\end{array}$ Now, let ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{v_{1}=(\\underbrace{1,\\dots,1}_{\\overline{{\\pi}}},0,\\dots,0)^{\\top}\\in\\mathbb R^{T},}\\\\ &{v_{2}=(\\underbrace{0,\\dots,0}_{\\overline{{\\pi}}},\\underbrace{1,\\dots,1}_{\\overline{{\\pi}}},0,\\dots,0)^{\\top}\\in\\mathbb R^{T},}\\\\ &{v_{3}=(\\underbrace{0,\\dots,0}_{\\overline{{\\pi}}},\\underbrace{0,\\dots,0}_{\\overline{{\\pi}}},\\underbrace{1,\\dots,1}_{\\overline{{\\pi}}},0,\\dots,0)^{\\top}\\in\\mathbb R^{T}}\\\\ &{\\dots}\\\\ &{v_{n}=(0,\\dots,0,\\underbrace{1,\\dots,1}_{\\overline{{\\pi}}})^{\\top}\\in\\mathbb R^{T}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "and choose any $T\\in\\{s m\\,|\\,s\\in\\mathbb{N}\\}$ (one can always take $T=m$ ). Then ", "page_idx": 39}, {"type": "equation", "text": "$$\nc=\\sqrt{\\frac{2L_{+}m^{2}\\delta^{0}}{T}}.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Let us fix some time $t\\,>\\,0$ to be determined later and recall that the workers can calculate the stochastic gradients $\\nabla f_{i}(x)\\,=\\,c v_{i}\\,+{L}_{+}x$ in parallel. Suppose that up to time $t$ , fewer than $\\frac{m}{2}$ stochastic gradients have been computed. Then, since the algorithm is a zero-respecting algorithm, it cannot have discovered more than $\\textstyle{\\frac{m}{2}}\\times{\\frac{T}{m}}={\\frac{T}{2}}$ coordinates. Thus, at least $\\textstyle{\\frac{T}{2}}$ coordinates are equal to 0 and ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\left\\|\\nabla f(x^{k})\\right\\|^{2}=\\left\\|{\\frac{c}{m}}\\sum_{i=1}^{m}v_{i}+L_{+}x^{k}\\right\\|^{2}\\geq{\\frac{c^{2}}{m^{2}}}\\times{\\frac{T}{2}}.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "for all $k\\in\\left\\{k\\in\\mathbb{N}_{0}\\,|\\,t^{k}\\leq t\\right\\}$ . Therefore, substituting our choice of $c$ and using the assumptions from the theorem, we get ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\left\\|\\nabla f(x^{k})\\right\\|^{2}\\geq L_{+}\\delta^{0}>2\\varepsilon.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "It remains to find the time $t>0$ . The workers work in parallel, so in $t$ seconds they calculate at most ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{n}\\left\\lfloor{\\frac{t}{\\tau_{i}}}\\right\\rfloor\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "stochastic gradients. Now, let ", "page_idx": 40}, {"type": "equation", "text": "$$\nt=\\frac{1}{8}\\operatorname*{min}_{j\\in[n]}\\left(\\sum_{i=1}^{j}\\frac{1}{\\tau_{i}}\\right)^{-1}(m+j)\\,,\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "and define $\\begin{array}{r}{g(j):=\\left(\\sum_{i=1}^{j}\\frac{1}{\\tau_{i}}\\right)^{-1}(m+j)}\\end{array}$ for all $j\\in[n]$ . Assume that $j^{*}$ is the largest index such that $\\begin{array}{r}{\\operatorname*{min}_{j\\in[n]}g(j)=g(j^{*})}\\end{array}$ . Using Lemma 4, we have ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{n}\\left\\lfloor{\\frac{t}{\\tau_{i}}}\\right\\rfloor=\\sum_{i=1}^{j^{*}}\\left\\lfloor{\\frac{t}{\\tau_{i}}}\\right\\rfloor.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Since $\\begin{array}{r}{\\frac{t}{\\tau_{i}}\\geq\\frac{1}{8}}\\end{array}$ for all $i\\leq j^{*}$ and $[x]\\leq2x-{\\frac{1}{4}}$ for all $x\\geq\\textstyle{\\frac{1}{8}}$ , we obtain ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{n}\\left\\lfloor\\frac{t}{\\tau_{i}}\\right\\rfloor\\leq\\sum_{i=1}^{j^{*}}\\frac{2t}{\\tau_{i}}-\\frac{j^{*}}{4}=\\frac{1}{4}\\left(\\sum_{i=1}^{j^{*}}\\frac{1}{\\tau_{i}}\\right)^{-1}(m+j^{*})\\sum_{i=1}^{j^{*}}\\frac{1}{\\tau_{i}}-\\frac{j^{*}}{4}=\\frac{m}{4}.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Therefore, it is possible to calculate at most $\\textstyle{\\frac{m}{4}}$ stochastic gradient in time (43) and we can finally conclude that inequality (42) holds for any time that is less than or equal (43). \u53e3 ", "page_idx": 40}, {"type": "text", "text": "K   Useful Identities and Inequalities ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Lemma 2 ([Szlendak et al., 2021). It holds that $L_{-}\\leq L_{+}$ $\\begin{array}{r}{L_{-}\\,\\leq\\,\\frac{1}{m}\\sum_{i=1}^{m}L_{i}}\\end{array}$ and $L_{+}^{2}-L_{-}^{2}\\leq$ $\\begin{array}{r}{L_{\\pm}^{2}\\leq L_{+}^{2}\\leq\\frac{1}{m}\\sum_{i=1}^{m}L_{i}^{2}}\\end{array}$ ", "page_idx": 41}, {"type": "text", "text": "Lemma 3. Consider a sequence $q_{1},\\ldots,q_{n}\\in[0,1].$ Then ", "page_idx": 41}, {"type": "equation", "text": "$$\n1-\\sum_{m=1}^{n}q_{m}\\leq\\prod_{m=1}^{n}\\left(1-q_{m}\\right).\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Prof We prove the result by induction. It is clearly true for $n\\ =\\ 1$ $\\begin{array}{r l}{1\\,-\\,\\sum_{m=1}^{1}q_{m}}&{{}{=}}\\end{array}$ $\\textstyle\\prod_{m=1}^{1}\\left(1-q_{m}\\right)$ . Now, assume that that it holds for $n-1$ , meaning that ", "page_idx": 41}, {"type": "equation", "text": "$$\n1-\\sum_{m=1}^{n-1}q_{m}\\leq\\prod_{m=1}^{n-1}\\left(1-q_{m}\\right).\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Multiplying both sides of the inequality by $1-q_{n}\\in[0,1]$ gives ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\prod_{m=1}^{n}\\left(1-q_{m}\\right)\\geq\\left(1-q_{n}\\right)\\left(1-\\sum_{m=1}^{n-1}q_{m}\\right)=1-\\sum_{m=1}^{n-1}q_{m}-q_{n}+q_{n}\\left(\\sum_{m=1}^{n-1}q_{m}\\right)\\geq1-\\sum_{m=1}^{n}q_{m}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "since $q_{m}\\in[0,1]$ for all $m\\in[n]$ ", "page_idx": 41}, {"type": "text", "text": "Theorem 31. Let us consider the equilibrium time mapping from Definition 3. Then ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r}{I.\\,\\ t^{*}(S,[\\tau_{i}]_{i=1}^{n})\\le2\\tau_{n}\\operatorname*{max}\\left\\{\\frac{S}{n},1\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "equation", "text": "$$\n2.\\ t^{*}(S,[\\tau_{i}]_{i=1}^{n})\\leq2\\tau_{1}\\operatorname*{max}\\left\\{S,1\\right\\}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "for all $S\\geq0$ \uff0c $\\tau_{i}\\in[0,\\infty]$ for all $i\\in[n]$ , and $\\tau_{1}\\leq\\cdot\\cdot\\leq\\tau_{n}$ ", "page_idx": 41}, {"type": "text", "text": "Remark 32. Assume that $\\tau_{1}=\\cdot\\cdot\\cdot=\\tau_{n-1}=\\tau$ and $\\tau_{n}\\to\\infty$ ,then ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{\\tau_{n}\\to\\infty}t^{*}(S,[\\tau_{i}]_{i=1}^{n})=t^{*}(S,[\\tau_{i}]_{i=1}^{n-1})=\\tau\\left(\\frac{S}{n-1}+1\\right)\\le2\\tau\\operatorname*{max}\\left\\{\\frac{S}{n-1},1\\right\\},\n$$", "text_format": "latex", "page_idx": 41}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{\\tau_{n}\\rightarrow\\infty}2\\tau_{n}\\operatorname*{max}\\left\\{\\frac{S}{n},1\\right\\}=\\infty,\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "and ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{\\tau_{n}\\to\\infty}2\\tau_{1}\\operatorname*{max}\\left\\{S,1\\right\\}=2\\tau\\operatorname*{max}\\left\\{S,1\\right\\}.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Thus, $t^{*}(S,[\\tau_{i}]_{i=1}^{n})$ can be arbitrarily smaller than $2\\tau_{n}\\operatorname*{max}\\left\\{{\\frac{S}{n}},1\\right\\}$ and $2\\tau_{1}\\operatorname*{max}\\left\\lbrace S,1\\right\\rbrace$ This implies that our new complexities (9) and (10) can be arbitrarily better than Tsoviet PAGE and $T_{\\mathrm{Hero\\;PAGE}}$ ", "page_idx": 41}, {"type": "text", "text": "Lemma 4. Consider a sequence $0<\\tau_{1}\\leq...\\leq\\tau_{n}$ and fix some $S>0,$ For all $j\\in[n]$ define ", "page_idx": 41}, {"type": "equation", "text": "$$\ng(j):=\\left(\\sum_{i=1}^{j}\\frac{1}{\\tau_{i}}\\right)^{-1}(S+j)\\,.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "1. Let $j_{\\mathrm{max}}^{\\ast}$ be the largest index such that $\\operatorname*{min}_{j\\in[n]}g(j)=g(j_{\\operatorname*{max}}^{*}).\\,F o r\\,j_{\\operatorname*{max}}^{*}<n$ we have ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{j\\in[n]}g(j)<\\tau_{(j_{\\operatorname*{max}}^{*}+1)}.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "2. Let $j^{*}$ be any index such that $\\operatorname*{min}_{j\\in[n]}g(j)=g(j^{*})$ .For $j^{*}<n$ , we have ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{j\\in[n]}g(j)\\leq\\tau_{(j^{*}+1)}.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "3. Let $j_{\\mathrm{min}}^{*}$ be the smallest index such that $\\operatorname*{min}_{j\\in[n]}g(j)=g(j_{\\operatorname*{min}}^{*}).$ Then ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\tau_{j_{\\operatorname*{min}}^{\\ast}}<\\operatorname*{min}_{j\\in[n]}{g(j)}.\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "4. Let $j^{*}$ be any index such that $\\operatorname*{min}_{j\\in[n]}g(j)=g(j^{*})$ .Then ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\tau_{j^{*}}\\leq\\operatorname*{min}_{j\\in[n]}g(j).\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Proof. We first prove the frst inequality. Suppose that $j_{\\mathrm{max}}^{\\ast}<n$ . Then $g(j_{\\mathrm{max}}^{\\ast})\\,<\\,g(j_{\\mathrm{max}}^{\\ast}+1)$ \uff0c meaning that ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\left(\\sum_{i=1}^{j_{\\operatorname*{max}}^{*}}\\frac{1}{\\tau_{i}}\\right)^{-1}\\left(S+j_{\\operatorname*{max}}^{*}\\right)<\\left(\\sum_{i=1}^{j_{\\operatorname*{max}}^{*}+1}\\frac{1}{\\tau_{i}}\\right)^{-1}\\left(S+j_{\\operatorname*{max}}^{*}+1\\right).\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "This implies the following series of inequalities: ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(\\displaystyle\\sum_{i=1}^{j_{\\operatorname*{max}}+1}\\frac{1}{\\tau_{i}}\\right)\\left(S+j_{\\operatorname*{max}}^{*}\\right)<\\left(\\displaystyle\\sum_{i=1}^{j_{\\operatorname*{max}}}\\frac{1}{\\tau_{i}}\\right)(S+j_{\\operatorname*{max}}^{*}+1)}\\\\ &{\\Leftrightarrow\\quad\\displaystyle\\frac{1}{\\tau_{(j_{\\operatorname*{max}}^{*}+1)}}\\left(S+j_{\\operatorname*{max}}^{*}\\right)<\\displaystyle\\sum_{i=1}^{j_{\\operatorname*{max}}}\\frac{1}{\\tau_{i}}}\\\\ &{\\Leftrightarrow\\quad\\displaystyle\\tau_{(j_{\\operatorname*{max}}^{*}+1)}>\\left(\\displaystyle\\sum_{i=1}^{j_{\\operatorname*{max}}^{*}}\\frac{1}{\\tau_{i}}\\right)^{-1}(S+j_{\\operatorname*{max}}^{*})=g(j_{\\operatorname*{max}}^{*})=\\displaystyle\\operatorname*{min}_{j\\in[n]}g(j).}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "The proof of the second inequality is the same, but with non-strict inequalities. To prove the third inequality, first suppose that $j_{\\mathrm{min}}^{\\ast}=1$ .Then $g(j_{\\operatorname*{min}}^{*})=\\tau_{j_{\\operatorname*{min}}^{*}}\\left(S+1\\right)^{*}\\succi_{j_{\\operatorname*{min}}^{*}}$ as needed. On the Other hand, $j_{\\mathrm{min}}^{*}>1$ implies ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle9(\\hat{J}_{\\mathrm{min}}^{\\mathrm{sup}})<g(\\hat{J}_{\\mathrm{min}}^{\\mathrm{sup}}-1)}\\\\ &{\\displaystyle\\Leftrightarrow\\left(\\frac{\\hat{J}_{\\mathrm{min}}^{\\mathrm{sup}}}{t_{\\mathrm{in}}^{2}}\\frac{1}{\\hat{\\tau}_{i}}\\right)^{-1}\\left(S+\\hat{J}_{\\mathrm{min}}^{\\mathrm{sup}}\\right)<\\left(\\sum_{i=1}^{\\beta_{\\mathrm{max}}^{\\mathrm{sup}}-1}\\frac{1}{\\hat{\\tau}_{i}}\\right)^{-1}\\left(S+\\hat{J}_{\\mathrm{min}}^{\\mathrm{s}}-1\\right)}\\\\ {\\displaystyle\\Leftrightarrow}&{\\displaystyle\\left(\\sum_{i=1}^{\\beta_{\\mathrm{min}}^{\\mathrm{sup}}-1}\\frac{1}{\\hat{\\tau}_{i}}\\right)\\left(S+\\hat{J}_{\\mathrm{min}}^{\\mathrm{s}}\\right)<\\left(\\sum_{i=1}^{\\beta_{\\mathrm{min}}^{\\mathrm{sup}}}\\frac{1}{\\hat{\\tau}_{i}}\\right)\\left(S+\\hat{J}_{\\mathrm{min}}^{\\mathrm{s}}-1\\right)}\\\\ {\\displaystyle\\Leftrightarrow}&{\\displaystyle\\sum_{i=1}^{\\beta_{\\mathrm{min}}^{\\mathrm{sup}}}\\frac{1}{\\tau_{i}}<\\frac{1}{\\hat{\\tau}_{\\mathrm{grion}}^{\\mathrm{sup}}}\\left(S+\\hat{J}_{\\mathrm{min}}^{\\mathrm{s}}\\right)}\\\\ {\\displaystyle\\Leftrightarrow}&{\\displaystyle\\tau_{\\mathrm{grion}}^{\\mathrm{g}}<\\left(\\sum_{i=1}^{\\beta_{\\mathrm{min}}^{\\mathrm{sup}}}\\frac{1}{\\hat{\\tau}_{i}}\\right)^{-1}\\left(S+\\hat{J}_{\\mathrm{min}}^{\\mathrm{s}}\\right)=\\operatorname*{min}_{j\\in[n]}g(j).}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "The proof of the fourth inequality is the same, but with non-strict inequalities. ", "page_idx": 42}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? ", "page_idx": 43}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 43}, {"type": "text", "text": "Justification: Sections 4 and 5, Table 1 ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 43}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 43}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Justification: Sections 3, 4.1, and 5 ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\"\u2019 section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u00b7 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 43}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 43}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 43}, {"type": "text", "text": "Justification: Sections 1.1, 4, and the appendix ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 44}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 44}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 44}, {"type": "text", "text": "Justification: Section A Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. () If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 44}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 44}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 45}, {"type": "text", "text": "Justification: In the supplementary materials. ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please see the NeurIPS code and data submission guidelines (https: //nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https : / /nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 45}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 45}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 45}, {"type": "text", "text": "Justification: Section A ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u00b7 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 45}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 45}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 45}, {"type": "text", "text": "Justification: We provide two plots for each algorithm to ensure the soundness of our experiments. We also calculate the variance of accuracies in Table 2. ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The authors should answer \"Yes\u2019 if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative errorrates).   \n\u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 45}, {"type": "text", "text": "", "page_idx": 46}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 46}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 46}, {"type": "text", "text": "Justification: Section A ", "page_idx": 46}, {"type": "text", "text": "Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 46}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines? ", "page_idx": 46}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 46}, {"type": "text", "text": "Justification: We have thoroughly reviewed the code of ethics and we can confidently state that our paper is fully compliant with it. ", "page_idx": 46}, {"type": "text", "text": "Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 46}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 46}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 46}, {"type": "text", "text": "Justification: Our paper considers a mathematical topic ", "page_idx": 46}, {"type": "text", "text": "Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u00b7 The answer NA means that there is no societal impact of the work performed.   \n\u00b7 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u00b7 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 46}, {"type": "text", "text": "", "page_idx": 47}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 47}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 47}, {"type": "text", "text": "Justification: ", "page_idx": 47}, {"type": "text", "text": "Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u00b7 The answer NA means that the paper poses no such risks. ", "page_idx": 47}, {"type": "text", "text": "\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safetyfilters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 47}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 47}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 47}, {"type": "text", "text": "Justification: Section A ", "page_idx": 47}, {"type": "text", "text": "Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u00b7 'I'he answer NA means that the paper does not use existing assets.   \n\u00b7 The authors should cite the original paper that produced the code package or dataset.   \n\u00b7 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode . com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 47}, {"type": "text", "text": "\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset's creators. ", "page_idx": 48}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 48}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 48}, {"type": "text", "text": "Justification: In the supplementary materials. Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not release new assets.   \n\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose asset isused.   \n\u00b7 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 48}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 48}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 48}, {"type": "text", "text": "Justification: ", "page_idx": 48}, {"type": "text", "text": "Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPs Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 48}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)were obtained? ", "page_idx": 48}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 48}, {"type": "text", "text": "Justification: ", "page_idx": 48}, {"type": "text", "text": "Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 48}]