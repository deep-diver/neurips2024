[{"figure_path": "kpo6ZCgVZH/figures/figures_7_1.jpg", "caption": "Figure 1: Left: CFG sampled particles at different numbers of iterations on constrained domains (ring, cardioid, double-moon, block). Right: The convergence curves of MSVGD, CFG and MIED on the block constraint.", "description": "This figure demonstrates the effectiveness of the proposed CFG method on sampling from four different truncated Gaussian distributions within various constrained domains. The left panel shows the particle distributions at different iterations for each of the four domains: ring, cardioid, double-moon, and block. The particles successfully converge to their respective target distributions without escaping the constrained domains. The right panel displays the convergence curves of MSVGD, CFG, and MIED methods for the block constraint. These curves are plotted using Wasserstein-2 distance and energy distance metrics against the number of iterations and illustrate that CFG demonstrates superior convergence performance compared to the baseline methods, MIED and MSVGD.", "section": "6.1 Toy Experiments"}, {"figure_path": "kpo6ZCgVZH/figures/figures_8_1.jpg", "caption": "Figure 1: Left: CFG sampled particles at different numbers of iterations on constrained domains (ring, cardioid, double-moon, block). Right: The convergence curves of MSVGD, CFG and MIED on the block constraint.", "description": "The left part of the figure shows the effectiveness of the CFG method in sampling from four different truncated Gaussian distributions within various constrained domains. The right part shows the convergence of the CFG, MSVGD, and MIED methods on the block-shaped domain using Wasserstein-2 distance and energy distance as metrics.", "section": "6.1 Toy Experiments"}, {"figure_path": "kpo6ZCgVZH/figures/figures_8_2.jpg", "caption": "Figure 3: Left: Bayesian Lasso (q = 1) using Spherical HMC (upper left), CFG (upper middle) and MIED (upper right). Bayesian Bridge Regression (q = 1.2) using Spherical HMC (lower left) CFG (upper middle) and MIED (upper right). Right: Results of monotonic Bayesian neural network with  \u03b5 = 0.01. Only the portion below 0.02 is shown on the y-axis to better display the performance of models satisfying constraint.", "description": "This figure presents the comparison results of Bayesian Lasso and Bayesian Bridge Regression using three different methods: Spherical HMC, CFG, and MIED. The left part shows the estimated coefficients of the models, and the right part shows the test accuracy and test monotonic loss of a monotonic Bayesian neural network trained with different methods. The results indicate that CFG achieves comparable performance to MIED and better performance than Spherical HMC.", "section": "6.2 Bayesian Lasso"}, {"figure_path": "kpo6ZCgVZH/figures/figures_16_1.jpg", "caption": "Figure 4: Left: MSE of boundary integral estimation of distribution p1. Middle: MSE of boundary integral estimation of distribution p2. Right: MSE of boundary integral estimation of distribution p3.", "description": "This figure presents the mean squared error (MSE) of the band-wise approximation of the boundary integral for three different velocity fields (v1, v2, v3) and three different distributions (p1, p2, p3).  The x-axis represents the sample size (N), and the y-axis represents the log10(MSE). Each plot shows how the MSE decreases as the sample size increases for each combination of velocity and distribution. This illustrates the accuracy and convergence of the boundary integral estimation method used in the paper.", "section": "C Insights on Bandwidth Selection"}, {"figure_path": "kpo6ZCgVZH/figures/figures_17_1.jpg", "caption": "Figure 5: MSE of boundary integral estimation of distribution p\u2081 and velocity v\u2081 using fixed edgewidths and adaptive edgewidth.", "description": "This figure demonstrates the Mean Squared Error (MSE) of the boundary integral estimation method used in the paper.  Three different bandwidths are compared: a fixed bandwidth of 0.5, a fixed bandwidth of 0.5*(10^2)^(-1/3), and an adaptive bandwidth of 0.5*N^(-1/3), where N is the sample size.  The plot shows how the MSE changes as the sample size increases. The adaptive bandwidth shows better performance in terms of lower MSE across varying sample sizes.", "section": "C Insights on Bandwidth Selection"}, {"figure_path": "kpo6ZCgVZH/figures/figures_19_1.jpg", "caption": "Figure 6: Ablation sampling results of not estimating boundary integral (left), not using znet (middle left), using znet (middle right) and the ground truth (right).", "description": "This figure compares the sampling results of three ablation studies on four different constrained 2D distributions (ring, cardioid, double-moon, block) against the ground truth. The ablation studies involve: (1) not estimating the boundary integral, (2) not using znet, and (3) using znet.  The visualization helps understand how each of these components affects the performance of the constrained sampling method. The results show that using znet and estimating the boundary integral are both crucial for accurate results.", "section": "6.1 Toy Experiments"}, {"figure_path": "kpo6ZCgVZH/figures/figures_20_1.jpg", "caption": "Figure 7: Illustration of generalizing our method to accommodating equality and inequality constraints.", "description": "The figure shows the results of applying the proposed method to a 3D ring-shaped distribution. The initial distribution is a 3D standard Gaussian.  The particles are constrained to lie on the xOy plane (equality constraint: z = 0) and within a ring in the xOy plane (inequality constraint: 1 < x\u00b2 + y\u00b2 < 4).  The figure displays the particle positions at iterations 0, 100, and 1000, showing how the particles converge to the target distribution while satisfying both constraints.  The ground truth distribution is also shown.", "section": "6.3 Monotonic Bayesian Neural Network"}, {"figure_path": "kpo6ZCgVZH/figures/figures_20_2.jpg", "caption": "Figure 1: Left: CFG sampled particles at different numbers of iterations on constrained domains (ring, cardioid, double-moon, block). Right: The convergence curves of MSVGD, CFG and MIED on the block constraint.", "description": "This figure shows the performance of the proposed Constrained Functional Gradient Flow (CFG) method for sampling from truncated Gaussian distributions within various constrained domains. The left panel displays the particle distribution at different iteration numbers for ring-shaped, cardioid-shaped, double-moon-shaped, and block-shaped domains, illustrating how the particles converge to the target distribution without escaping the domain.  The right panel compares the convergence curves of CFG with MSVGD and MIED on a block-shaped domain using Wasserstein-2 distance and energy distance metrics. This demonstrates that CFG effectively handles constraints and achieves better sampling results compared to other baselines.", "section": "6.1 Toy Experiments"}, {"figure_path": "kpo6ZCgVZH/figures/figures_21_1.jpg", "caption": "Figure 9: Wasserstein-2 distance of CFG and MIED versus the training time using different number of particles.", "description": "This figure compares the performance of CFG and MIED in terms of Wasserstein-2 distance against training time for different numbers of particles (900, 2000, and 4000).  It demonstrates the scalability of CFG compared to MIED, showing that CFG's training time increases linearly with the number of particles while MIED's increases quadratically.  This is because the complexity of CFG is O(N) while MIED's is O(N^2), where N is the number of particles.  The figure highlights the superior computational efficiency of CFG, especially for larger datasets.", "section": "6 Experiments"}, {"figure_path": "kpo6ZCgVZH/figures/figures_21_2.jpg", "caption": "Figure 10: Choosing the adaptive bandwidth (red) against fixed bandwidths for the Bayesian Lasso experiment on a synthetic dataset.", "description": "This figure compares the performance of the proposed CFG method using different bandwidth selection strategies for the Bayesian Lasso problem. It shows the energy distance (a measure of the difference between the estimated and true distributions) plotted against the sample size (number of particles) for three different bandwidth methods: a fixed bandwidth of 0.1, a fixed bandwidth of 0.01, and an adaptive bandwidth of 0.1N\u207b\u00b3.  The adaptive bandwidth, represented in red, generally shows better performance (lower energy distance) across different sample sizes. This supports the claim that the adaptive bandwidth selection method is more effective for the Bayesian Lasso problem.", "section": "6.2 Bayesian Lasso"}]