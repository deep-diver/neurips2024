{"references": [{"fullname_first_author": "Julius Adebayo", "paper_title": "Sanity checks for saliency maps", "publication_date": "2018-12-03", "reason": "This paper is foundational for evaluating the reliability of post-hoc explanation methods, a core concern addressed by the main paper."}, {"fullname_first_author": "Christopher M. Bishop", "paper_title": "Pattern Recognition and Machine Learning", "publication_date": "2006-01-01", "reason": "This book is a highly cited standard text in machine learning, providing the background for probabilistic modeling crucial to the main paper's theoretical analysis."}, {"fullname_first_author": "Zachary C Lipton", "paper_title": "The mythos of model interpretability", "publication_date": "2018-01-01", "reason": "This paper highlights the ambiguity and challenges in defining and evaluating model interpretability, which motivates the central research question in the main paper."}, {"fullname_first_author": "Marco T\u00falio Ribeiro", "paper_title": "\"why should I trust you?\": Explaining the predictions of any classifier", "publication_date": "2016-08-13", "reason": "This paper introduces LIME, a widely used model-agnostic explanation method; understanding its limitations with spurious correlations informs the main paper's approach."}, {"fullname_first_author": "Cynthia Rudin", "paper_title": "Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead", "publication_date": "2019-01-01", "reason": "This paper advocates for using inherently interpretable models over post-hoc explanation methods for high-stakes applications, which aligns with the main paper's focus on improving model transparency."}]}