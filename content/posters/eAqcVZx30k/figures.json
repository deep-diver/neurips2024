[{"figure_path": "eAqcVZx30k/figures/figures_1_1.jpg", "caption": "Figure 1: The standard rationalization framework RNP. The task is binary sentiment classification about the hotel's location. X, Z,\u0176,Y represent the input, the extracted rationale candidate, the prediction and the ground truth label, respectively. \u03b8E, OP represent the parameters of the extractor and the predictor, respectively. He denotes cross-entropy.", "description": "This figure illustrates the Rationalizing Neural Predictions (RNP) framework, a model-agnostic rationalization approach.  The input text (X) is processed by an extractor that selects a subset of the most informative words (Z), called the rationale. This rationale is then used by a predictor to make a prediction (\u0176) of the target variable (Y).  The goal is to train the extractor and predictor cooperatively to maximize prediction accuracy, guided by the maximum mutual information (MMI) criterion.", "section": "1 Introduction"}, {"figure_path": "eAqcVZx30k/figures/figures_4_1.jpg", "caption": "Figure 3: Penalizing spurious features for more efficiently searching causal rationales.", "description": "This figure illustrates how penalty-based methods, which aim to improve the efficiency of searching causal rationales by penalizing spurious features, can result in various outcomes depending on the penalty's strength.  The x-axis represents the extractor's parameter \u03b8, while the y-axis represents the loss L.  Four scenarios are shown: (a) under-penalty, where the penalty is insufficient to prevent the extractor from being drawn to spurious features (S); (b) no penalty, where the extractor is equally likely to select noise (N) or spurious features; (c) appropriate penalty, where the loss landscape is optimized for selecting the causal rationale (C); and (d) over-penalty, where the penalty dominates the loss function, reducing the ability of the MMI criterion to distinguish between noise and causal features.", "section": "4 Treating spurious features as equivalent to plain noise"}, {"figure_path": "eAqcVZx30k/figures/figures_4_2.jpg", "caption": "Figure 2: The data-generating process of (a) a general classification dataset and (b) a specific dataset Beer-Appearance.", "description": "This figure illustrates two probabilistic graphical models representing data generation processes. (a) shows a general model with unobservable confounders U influencing both spurious features S and causal features C, which in turn affect the label Y. (b) illustrates the model for Beer-Appearance dataset, specifically.  Here, XT and XA represent Taste and Appearance features. XA is a direct cause of Y, but XT is correlated with Y due to a backdoor path via the confounder U (e.g., brand reputation). This example highlights the challenge of spurious correlations in rationale extraction, where XT might be mistaken for a causal feature.", "section": "4 Treating spurious features as equivalent to plain noise"}, {"figure_path": "eAqcVZx30k/figures/figures_6_1.jpg", "caption": "Figure 4: The architecture of our proposed MRD. The approximators for the two distributions are shared to reduce the model complexity.", "description": "This figure illustrates the architecture of the Maximizing Remaining Discrepancy (MRD) method proposed in the paper.  It shows how the model works to approximate two distributions: P(Y|X) and P(Y|X-Z). The model consists of an extractor that takes the input X and outputs a rationale Z and the remaining part X-Z. Two predictors share parameters to approximate P(Y|X) and P(Y|X-Z). The loss function is based on minimizing the KL divergence between the approximated and true distributions, thereby focusing on maximizing the remaining discrepancy after removing the rationale candidate from the full input.", "section": "5 The practical method"}, {"figure_path": "eAqcVZx30k/figures/figures_18_1.jpg", "caption": "Figure 3: Penalizing spurious features for more efficiently searching causal rationales.", "description": "This figure illustrates how different penalty strategies in the loss function affect the search for causal rationales when spurious features exist.  Panel (a) shows an under-penalty scenario where the penalty term is too weak, resulting in the gradient descent algorithm potentially being drawn toward spurious features instead of causal ones. Panel (b) shows the unpenalized case (vanilla MMI), where the gradient may move toward either causal or spurious features.  Panel (c) depicts an appropriate penalty scenario where the penalty term is balanced with the MMI criterion, leading to efficient identification of causal rationales. Panel (d) demonstrates an over-penalty scenario, where the penalty dominates the loss function, causing the gradient to potentially prioritize noise over causal features.  The figure effectively demonstrates how the balance of penalty terms affects performance.", "section": "4 Treating spurious features as equivalent to plain noise"}]