[{"figure_path": "QMaLS4VeY3/figures/figures_1_1.jpg", "caption": "Figure 1: A glimpse of the AVAgent workflow. Three steps (tool use, planning, and reflection) form a cyclic agentic workflow where audio signals are progressively aligned with the visual content for joint representation improvement.", "description": "The figure illustrates the AVAgent workflow, a three-step cyclic process for improving audio-visual joint representations.  The workflow starts with a \"tool use\" phase where a multimodal LLM processes both audio and video inputs separately to create textual descriptions.  Next, in the \"planning\" phase, the AVAgent (an LLM-based assistant) analyzes these descriptions, determines if alignment is needed, and plans audio editing actions. Finally, in the \"reflection\" phase, a vision-language model (VLM) evaluates the results, providing feedback to the AVAgent. This cyclic process of tool use, planning, and reflection continues until the audio and video are well-aligned.", "section": "1 Introduction"}, {"figure_path": "QMaLS4VeY3/figures/figures_2_1.jpg", "caption": "Figure 2: Overview of the AVAGENT framework. Tool use: For each audio-visual data pair, we employ a multi-modal Large Language Model (LLM) to convert audio and visual data into the language form, separately. Planning: The agent takes the AV data via text description and plans to edit the audio signal for alignment enhancement. Reflection: Subsequently, a Vision-Language Model (VLM) evaluates modifications to ensure that the audio adjustments appropriately match the visual content, and provides feedback to the agent. These steps form a cyclic agentic workflow where audio signals are progressively aligned with the visual content for enhanced joint representation.", "description": "This figure illustrates the AVAgent framework's workflow.  It uses a multimodal LLM to convert audio and video data into text descriptions. The AVAgent then plans audio edits (noise reduction, synchronization) based on these descriptions. Finally, a vision-language model (VLM) assesses the edits, providing feedback to refine the process iteratively, creating a cycle.", "section": "3 Proposed method"}, {"figure_path": "QMaLS4VeY3/figures/figures_4_1.jpg", "caption": "Figure 2: Overview of the AVAGENT framework. Tool use: For each audio-visual data pair, we employ a multi-modal Large Language Model (LLM) to convert audio and visual data into the language form, separately. Planning: The agent takes the AV data via text description and plans to edit the audio signal for alignment enhancement. Reflection: Subsequently, a Vision-Language Model (VLM) evaluates modifications to ensure that the audio adjustments appropriately match the visual content, and provides feedback to the agent. These steps form a cyclic agentic workflow where audio signals are progressively aligned with the visual content for enhanced joint representation.", "description": "This figure illustrates the AVAGENT framework, a cyclic workflow with three main steps: Tool Use, Planning, and Reflection.  In the Tool Use step, a multimodal LLM processes both audio and video data separately, generating textual descriptions.  The Planning step uses the LLMs to decide whether audio adjustments are needed and to plan appropriate audio editing actions.  Finally, the Reflection step employs a vision-language model (VLM) to evaluate the impact of those actions and provide feedback to the LLM for the next iteration.  The entire process gradually aligns audio signals with visual content, improving AV joint representations.", "section": "3 Proposed method"}, {"figure_path": "QMaLS4VeY3/figures/figures_17_1.jpg", "caption": "Figure 2: Overview of the AVAGENT framework. Tool use: For each audio-visual data pair, we employ a multi-modal Large Language Model (LLM) to convert audio and visual data into the language form, separately. Planning: The agent takes the AV data via text description and plans to edit the audio signal for alignment enhancement. Reflection: Subsequently, a Vision-Language Model (VLM) evaluates modifications to ensure that the audio adjustments appropriately match the visual content, and provides feedback to the agent. These steps form a cyclic agentic workflow where audio signals are progressively aligned with the visual content for enhanced joint representation.", "description": "This figure illustrates the AVAGENT workflow, a three-step cyclic process: 1) Tool use: A multimodal LLM processes audio and visual data separately into language descriptions.  2) Planning: The AVAgent uses these descriptions to decide if alignment is needed and plans audio edits (e.g., noise reduction, speed adjustment). 3) Reflection: A Vision-Language Model (VLM) evaluates the edited audio's match to the visual content, providing feedback for the next cycle. This iterative process refines the audio to better align with the visuals, improving the joint audio-visual representation.", "section": "3 Proposed method"}, {"figure_path": "QMaLS4VeY3/figures/figures_18_1.jpg", "caption": "Figure 2: Overview of the AVAGENT framework. Tool use: For each audio-visual data pair, we employ a multi-modal Large Language Model (LLM) to convert audio and visual data into the language form, separately. Planning: The agent takes the AV data via text description and plans to edit the audio signal for alignment enhancement. Reflection: Subsequently, a Vision-Language Model (VLM) evaluates modifications to ensure that the audio adjustments appropriately match the visual content, and provides feedback to the agent. These steps form a cyclic agentic workflow where audio signals are progressively aligned with the visual content for enhanced joint representation.", "description": "This figure illustrates the AVAGENT framework, a cyclic workflow composed of three stages: tool use, planning, and reflection.  In the tool use stage, a multimodal LLM processes audio and visual data separately, generating textual descriptions. These descriptions are then used by the AVAgent in the planning stage to determine necessary audio adjustments (e.g., noise reduction, synchronization). Finally, a Vision-Language Model (VLM) in the reflection stage evaluates the effectiveness of these adjustments, providing feedback to the AVAgent to refine the process iteratively.  The goal is to progressively align audio and visual signals for improved joint representation.", "section": "3 Proposed method"}, {"figure_path": "QMaLS4VeY3/figures/figures_19_1.jpg", "caption": "Figure 2: Overview of the AVAGENT framework. Tool use: For each audio-visual data pair, we employ a multi-modal Large Language Model (LLM) to convert audio and visual data into the language form, separately. Planning: The agent takes the AV data via text description and plans to edit the audio signal for alignment enhancement. Reflection: Subsequently, a Vision-Language Model (VLM) evaluates modifications to ensure that the audio adjustments appropriately match the visual content, and provides feedback to the agent. These steps form a cyclic agentic workflow where audio signals are progressively aligned with the visual content for enhanced joint representation.", "description": "This figure provides a high-level overview of the AVAgent framework, illustrating the three main steps involved in the agentic workflow: tool use, planning, and reflection.  The workflow starts with a multimodal LLM processing both audio and visual data independently, generating separate textual descriptions.  The AVAgent then uses these descriptions to plan necessary audio edits (e.g., noise reduction, synchronization). Finally, a Vision-Language Model (VLM) assesses the results of these edits to provide feedback for the next iteration of the cycle.  This cyclical process progressively improves the alignment between audio and visual data, resulting in enhanced joint representation.", "section": "3 Proposed method"}]