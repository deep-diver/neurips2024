[{"figure_path": "QMaLS4VeY3/tables/tables_5_1.jpg", "caption": "Table 1: Data analysis of visual alignment and temporal synchronization. True and False pairs denote the video corresponding to the modified and original audio separately.", "description": "This table presents a quantitative analysis of visual and temporal alignment scores before and after applying the proposed audio-visual alignment method.  It shows the impact of the number of \"true\" (aligned) and \"false\" (misaligned) audio-visual pairs on the alignment scores, demonstrating that the proposed method improves both visual and temporal alignment.", "section": "3.3 Data Analysis"}, {"figure_path": "QMaLS4VeY3/tables/tables_5_2.jpg", "caption": "Table 1: Data analysis of visual alignment and temporal synchronization. True and False pairs denote the video corresponding to the modified and original audio separately.", "description": "This table presents the results of data analysis focusing on visual alignment and temporal synchronization.  It compares the performance of the proposed method against a baseline.  'True Pairs' represent video-audio pairs where the audio has been modified by the proposed method, while 'False Pairs' represent pairs with original, unmodified audio. The 'T-Alignment (%)' column shows the percentage improvement in temporal alignment achieved by the proposed method, demonstrating its effectiveness in enhancing synchronization between audio and video.", "section": "3.3 Data Analysis"}, {"figure_path": "QMaLS4VeY3/tables/tables_6_1.jpg", "caption": "Table 2: Audio-visual classification on VGGSound-Music, VGGSound-All, and AudioSet datasets.", "description": "This table presents the performance comparison of the proposed AVAGENT model against various state-of-the-art baselines on three different audio-visual classification datasets: VGGSound-Music, VGGSound-All, and AudioSet.  The results are shown in terms of linear probing accuracy and finetune accuracy.  Higher percentages indicate better performance.  The table highlights the superior performance of the AVAGENT method in this task.", "section": "4.2 Comparison to Prior Work"}, {"figure_path": "QMaLS4VeY3/tables/tables_7_1.jpg", "caption": "Table 3: Sound source localization and segmentation. Quantitative results on Flickr-SoundNet and AVSBench.", "description": "This table presents a quantitative comparison of the proposed AVAGENT model against several state-of-the-art methods for sound source localization and segmentation tasks.  The evaluation is performed on two benchmark datasets, Flickr-SoundNet and AVSBench, using metrics such as Precision, Average Precision (AP), F1 score, mean Intersection over Union (mIoU), and F1 score for segmentation.  Higher scores indicate better performance.", "section": "4 Experiments"}, {"figure_path": "QMaLS4VeY3/tables/tables_7_2.jpg", "caption": "Table 4: Sound source separation. Quantitative results on MUSIC and VGGSound datasets.", "description": "This table presents a quantitative comparison of different sound source separation methods on two datasets: MUSIC and VGGSound.  The evaluation metrics used are SDR (Signal-to-Distortion Ratio) and SAR (Signal-to-Artifact Ratio).  Higher values of SDR and SAR indicate better separation performance. The table allows for a comparison of the proposed AVAGENT method against several established baselines in the field of audio source separation.", "section": "4.2 Comparison to Prior Work"}, {"figure_path": "QMaLS4VeY3/tables/tables_8_1.jpg", "caption": "Table 2: Audio-visual classification on VGGSound-Music, VGGSound-All, and AudioSet datasets.", "description": "This table presents the performance of the proposed AVAGENT model and several baseline models on three audio-visual classification datasets: VGGSound-Music, VGGSound-All, and AudioSet.  The results are shown in terms of linear probing accuracy and fine-tuning accuracy for each model on each dataset.  The table highlights the superior performance of the AVAGENT model compared to the baselines, demonstrating its effectiveness in improving audio-visual representation learning for classification tasks.", "section": "4.2 Comparison to Prior Work"}, {"figure_path": "QMaLS4VeY3/tables/tables_16_1.jpg", "caption": "Table 6: Ablation studies on LoRA tuning. Quantitative results on VGGSound-All, Flickr-SoundNet, AVS-Bench, and VGGS-Music datasets.", "description": "This ablation study investigates the impact of LoRA (Low-Rank Adaptation) tuning on the performance of the AVAgent model across four different audio-visual datasets. The results presented show the impact of LoRA tuning across various downstream tasks including linear probing, fine-tuning, precision, average precision (AP), F1 score, mean Intersection over Union (mIoU), and Signal-to-Distortion Ratio (SDR) and Signal-to-Artifact Ratio (SAR). The study compares the model's performance with and without LoRA tuning to highlight its effectiveness.", "section": "4.3 Experimental Analysis"}]