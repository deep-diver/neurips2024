[{"figure_path": "t7wvJstsiV/tables/tables_6_1.jpg", "caption": "Table 1: Comparison on LLaMA 2 model family. The best results are in bold for each dataset/metric. SLED outperforms DoLa and the vanilla greedy decoding.", "description": "This table presents a comparison of the performance of three decoding methods (greedy decoding, DoLa, and SLED) on several benchmark datasets using three different sizes of LLaMA 2 models.  The metrics used include accuracy on multiple-choice questions (MC1, MC2, MC3), accuracy on a factual accuracy dataset (FACTOR), and metrics measuring truthfulness, informativeness, rejection rate, and accuracy on various open-ended generation tasks (TruthfulQA, StrategyQA, and GSM8K).  The results show that SLED consistently outperforms both greedy decoding and DoLa across multiple metrics and datasets.", "section": "3.2 Evaluation on a Broad Range of LLM Benchmarks"}, {"figure_path": "t7wvJstsiV/tables/tables_7_1.jpg", "caption": "Table 1: Comparison on LLaMA 2 model family. The best results are in bold for each dataset/metric. SLED outperforms DoLa and the vanilla greedy decoding.", "description": "This table presents the results of experiments comparing the performance of SLED, DoLa, and vanilla greedy decoding on various datasets and metrics using the LLaMA 2 model family.  It shows the accuracy in multiple-choice questions (MC1, MC2, MC3) for the FACTOR dataset, and various metrics (%Truth, %Info, %T*I, %Reject) for the TruthfulQA dataset.  The best results for each metric are highlighted in bold, demonstrating SLED's superior performance compared to the baselines.", "section": "3.2 Evaluation on a Broad Range of LLM Benchmarks"}, {"figure_path": "t7wvJstsiV/tables/tables_8_1.jpg", "caption": "Table 1: Comparison on LLaMA 2 model family. The best results are in bold for each dataset/metric. SLED outperforms DoLa and the vanilla greedy decoding.", "description": "This table presents a comparison of different decoding methods (greedy decoding, DoLa, and SLED) on various metrics across three different sizes of LLaMA 2 models (7B, 13B, and 70B).  Each model is tested on several datasets (FACTOR, TruthfulQA (MC), TruthfulQA (Open-Ended), StrategyQA, GSM8K) using multiple metrics (accuracy, truthfulness, information, rejection).  The table demonstrates that SLED consistently outperforms both DoLa and the baseline greedy decoding method across most datasets and metrics. ", "section": "3.2 Evaluation on a Broad Range of LLM Benchmarks"}, {"figure_path": "t7wvJstsiV/tables/tables_8_2.jpg", "caption": "Table 1: Comparison on LLaMA 2 model family. The best results are in bold for each dataset/metric. SLED outperforms DoLa and the vanilla greedy decoding.", "description": "This table presents a comparison of the performance of three decoding methods (greedy decoding, DoLa, and SLED) on various tasks and metrics using different sizes of LLaMA 2 models.  The metrics evaluated include accuracy on multiple-choice questions (MC) across three different datasets (FACTOR, MC1, MC2, MC3), percentage of truthful and informative answers, rejection rate, and accuracy on open-ended question answering and chain-of-thought tasks across datasets like StrategyQA and GSM8K. The table highlights the superior performance of SLED compared to both DoLa and the standard greedy approach.", "section": "3.2 Evaluation on a Broad Range of LLM Benchmarks"}, {"figure_path": "t7wvJstsiV/tables/tables_9_1.jpg", "caption": "Table 5: Latency (ms/token) comparison across different configurations on different sizes of models.", "description": "This table presents the latency in milliseconds per token for various model sizes (LLaMA-2-7B, LLaMA-2-13B, LLaMA-2-70B) under different decoding methods.  It compares the latency of greedy decoding, DoLa, and SLED with varying evolution scales (topk).  The results show the added latency overhead of different decoding methods and different evolution scales.", "section": "3.5 Ablation Studies and Analysis"}, {"figure_path": "t7wvJstsiV/tables/tables_15_1.jpg", "caption": "Table 1: Comparison on LLaMA 2 model family. The best results are in bold for each dataset/metric. SLED outperforms DoLa and the vanilla greedy decoding.", "description": "This table presents a comparison of the performance of three decoding methods (greedy decoding, DoLa, and SLED) on the LLaMA 2 model family across multiple datasets and metrics. The datasets used include FACTOR, TruthfulQA (for both multiple-choice and open-ended questions), and Chain-of-Thought (COT) reasoning tasks (StrategyQA and GSM8K).  The metrics evaluated include accuracy (%Truth, %Info, %T*I, %Reject) and rejection rate.  The table highlights that SLED consistently outperforms DoLa and greedy decoding across various models and evaluation criteria, demonstrating its effectiveness in enhancing factual accuracy.", "section": "3.2 Evaluation on a Broad Range of LLM Benchmarks"}, {"figure_path": "t7wvJstsiV/tables/tables_16_1.jpg", "caption": "Table 1: Comparison on LLaMA 2 model family. The best results are in bold for each dataset/metric. SLED outperforms DoLa and the vanilla greedy decoding.", "description": "This table presents a comparison of different decoding methods (greedy, DoLa, and SLED) on various metrics across three sizes of LLaMA 2 models (7B, 13B, and 70B), both base and chat versions.  The metrics used include accuracy on multiple choice question tasks (FACTOR, TruthfulQA) and performance on open-ended generation tasks (TruthfulQA, StrategyQA, GSM8K), considering aspects like truthfulness, information, and rejection rate.  The results demonstrate that SLED consistently outperforms DoLa and the greedy decoding baseline, highlighting its effectiveness in improving the factuality of LLMs.", "section": "3.2 Evaluation on a Broad Range of LLM Benchmarks"}, {"figure_path": "t7wvJstsiV/tables/tables_17_1.jpg", "caption": "Table 1: Comparison on LLaMA 2 model family. The best results are in bold for each dataset/metric. SLED outperforms DoLa and the vanilla greedy decoding.", "description": "This table presents a comparison of the performance of three decoding methods (greedy decoding, DoLa, and SLED) on various metrics across three different sizes of LLaMA 2 models.  The metrics evaluated include accuracy on multiple-choice questions (MC1, MC2, MC3) and the percentage of truthful, informative, and truthful-and-informative answers on open-ended questions.  The table also includes rejection rate and various other metrics on different datasets (FACTOR, TruthfulQA, StrategyQA, GSM8K).  The results demonstrate that SLED consistently outperforms both DoLa and greedy decoding across most metrics and datasets.", "section": "3.2 Evaluation on a Broad Range of LLM Benchmarks"}, {"figure_path": "t7wvJstsiV/tables/tables_18_1.jpg", "caption": "Table 1: Comparison on LLaMA 2 model family. The best results are in bold for each dataset/metric. SLED outperforms DoLa and the vanilla greedy decoding.", "description": "This table presents the results of experiments comparing the performance of three decoding methods (greedy decoding, DoLa, and SLED) on various tasks using three sizes of LLaMA 2 models.  The tasks assess factuality using metrics such as accuracy, information content, and rejection rate across benchmarks like TruthfulQA (multiple choice and open-ended), FACTOR, StrQA, and GSM8K.  The bolded numbers highlight the best performance achieved for each metric and benchmark.", "section": "3.2 Evaluation on a Broad Range of LLM Benchmarks"}]