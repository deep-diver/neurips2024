[{"figure_path": "nK6OnCpd3n/tables/tables_4_1.jpg", "caption": "Table 1: Results for goal-achievement experiments on DeepMind Control Suite Dog and Humanoid environments. For rows with an associated ground-truth reward function, numerical results are listed; for performant approaches, we report mean and standard deviation across 5 seeds. For novel zero-shot text-conditioned behavior learning, checkmarks denote if the resulting policy is aligned with the provided text prompt according to human evaluation.", "description": "This table presents the results of goal-achievement experiments conducted on the DeepMind Control Suite's Dog and Humanoid environments.  It compares different methods for achieving various goals specified through natural language prompts.  Numerical results (mean and standard deviation over 5 seeds) are given where ground-truth reward functions exist. For novel, zero-shot learning tasks, checkmarks indicate whether the learned policy aligns with the text prompt, as judged by human evaluators.", "section": "4.1 Experimental Setup and Evaluation"}, {"figure_path": "nK6OnCpd3n/tables/tables_7_1.jpg", "caption": "Table 3: Results for continuous locomotion experiments. For rows with an associated ground-truth reward function, numerical results are listed; for performant approaches, we report mean and standard deviation across 5 seeds. Evaluation for text-alignment is also reported. Video-TADPoLe greatly outperforms ViCLIP-RM on both Humanoid and Dog.", "description": "This table presents the results of continuous locomotion experiments using different methods.  It compares the performance of several approaches, including LIV, Text2Reward, Video-TADPoLe, and ViCLIP-RM, across two environments (Humanoid and Dog) and two prompts (\"a person walking\" and \"a dog walking\").  The table shows numerical results (mean and standard deviation) for methods with ground-truth reward functions and qualitative assessment (checkmark/x-mark) for text alignment.  The key finding is that Video-TADPoLe significantly outperforms other methods in terms of continuous locomotion performance and text alignment.", "section": "4.3 Continuous Locomotion"}, {"figure_path": "nK6OnCpd3n/tables/tables_8_1.jpg", "caption": "Table 4: Average success rate for robotic manipulation tasks in Meta-World [42] over 30 evaluation rollouts. We compare between TADPoLe and VLM-RM, both approaches that do not utilize in-domain data or demonstrations, and find TADPoLe significantly outperforms VLM-RM. We report mean performance and standard deviation across 10 seeds for each task.", "description": "This table presents a comparison of the success rates achieved by TADPoLe and VLM-RM on various robotic manipulation tasks within the Meta-World environment.  Both methods operate without using in-domain data or demonstrations.  The results show that TADPoLe substantially outperforms VLM-RM across different tasks, demonstrating the effectiveness of the proposed method.", "section": "4.4 Robotic Manipulation"}, {"figure_path": "nK6OnCpd3n/tables/tables_9_1.jpg", "caption": "Table 3: Results for continuous locomotion experiments. For rows with an associated ground-truth reward function, numerical results are listed; for performant approaches, we report mean and standard deviation across 5 seeds. Evaluation for text-alignment is also reported. Video-TADPoLe greatly outperforms ViCLIP-RM on both Humanoid and Dog.", "description": "This table presents the results of continuous locomotion experiments using different methods.  It shows the performance (mean and standard deviation across 5 trials) of various methods, including Video-TADPoLe and ViCLIP-RM, on Humanoid and Dog locomotion tasks.  The table also indicates whether the resulting policy aligns with the text prompt used.  Video-TADPoLe significantly outperforms ViCLIP-RM.", "section": "4.3 Continuous Locomotion"}, {"figure_path": "nK6OnCpd3n/tables/tables_14_1.jpg", "caption": "Table 1: Results for goal-achievement experiments on DeepMind Control Suite Dog and Humanoid environments. For rows with an associated ground-truth reward function, numerical results are listed; for performant approaches, we report mean and standard deviation across 5 seeds. For novel zero-shot text-conditioned behavior learning, checkmarks denote if the resulting policy is aligned with the provided text prompt according to human evaluation.", "description": "This table presents the results of goal-achievement experiments using the DeepMind Control Suite's Dog and Humanoid environments.  For experiments where a ground-truth reward function was available, the table shows numerical results (mean and standard deviation across 5 seeds). For novel tasks learned using only text-based conditioning (zero-shot learning), a checkmark indicates whether the learned policy successfully matched the text description according to human evaluation.", "section": "4.1 Experimental Setup and Evaluation"}, {"figure_path": "nK6OnCpd3n/tables/tables_14_2.jpg", "caption": "Table 1: Results for goal-achievement experiments on DeepMind Control Suite Dog and Humanoid environments. For rows with an associated ground-truth reward function, numerical results are listed; for performant approaches, we report mean and standard deviation across 5 seeds. For novel zero-shot text-conditioned behavior learning, checkmarks denote if the resulting policy is aligned with the provided text prompt according to human evaluation.", "description": "This table presents the results of goal-achievement experiments using the DeepMind Control Suite's Dog and Humanoid environments.  It compares the performance of different methods, including the proposed TADPoLe, on various text-conditioned tasks. For tasks with ground truth reward functions, numerical results (mean and standard deviation) are shown, while for novel zero-shot tasks, the table shows whether the policy successfully aligned with the text prompt according to human evaluation.", "section": "4.1 Experimental Setup and Evaluation"}, {"figure_path": "nK6OnCpd3n/tables/tables_14_3.jpg", "caption": "Table 1: Results for goal-achievement experiments on DeepMind Control Suite Dog and Humanoid environments. For rows with an associated ground-truth reward function, numerical results are listed; for performant approaches, we report mean and standard deviation across 5 seeds. For novel zero-shot text-conditioned behavior learning, checkmarks denote if the resulting policy is aligned with the provided text prompt according to human evaluation.", "description": "This table presents the results of goal-achievement experiments conducted on the DeepMind Control Suite's Dog and Humanoid environments.  It compares different methods for learning policies based on natural language instructions. Numerical results (mean and standard deviation across 5 seeds) are shown where applicable (ground-truth reward function exists). For novel behaviors learned without ground-truth rewards, a checkmark indicates if the resulting policy aligned with the textual instructions according to human evaluation.", "section": "4.1 Experimental Setup and Evaluation"}, {"figure_path": "nK6OnCpd3n/tables/tables_15_1.jpg", "caption": "Table A4: Ablation over w\u2081 and w\u2082 on Humanoid Stand and Walk", "description": "This table presents the ablation study results on the impact of hyperparameters w\u2081 and w\u2082 on the Humanoid Stand and Walk tasks.  Different combinations of w\u2081 and w\u2082 were tested, and the resulting performance (in terms of cumulative reward) is reported for both tasks.  This helps to determine the optimal values of w\u2081 and w\u2082 that lead to the best performance in each task.", "section": "B Detailed Hyperparameters"}, {"figure_path": "nK6OnCpd3n/tables/tables_15_2.jpg", "caption": "Table 1: Results for goal-achievement experiments on DeepMind Control Suite Dog and Humanoid environments. For rows with an associated ground-truth reward function, numerical results are listed; for performant approaches, we report mean and standard deviation across 5 seeds. For novel zero-shot text-conditioned behavior learning, checkmarks denote if the resulting policy is aligned with the provided text prompt according to human evaluation.", "description": "This table presents the results of goal-achievement experiments conducted on the DeepMind Control Suite Dog and Humanoid environments.  For tasks with a ground-truth reward function, the table shows quantitative results (mean and standard deviation across 5 seeds). For novel tasks where a ground-truth reward isn't available, a qualitative assessment is provided, indicating whether the learned policy successfully aligns with the specified text prompt based on human evaluation.  The table compares different methods, including the proposed TADPoLe method, showing whether each method successfully achieved the text-specified goal.", "section": "4.1 Experimental Setup and Evaluation"}, {"figure_path": "nK6OnCpd3n/tables/tables_16_1.jpg", "caption": "Table 4: Average success rate for robotic manipulation tasks in Meta-World [42] over 30 evaluation rollouts. We compare between TADPoLe and VLM-RM, both approaches that do not utilize in-domain data or demonstrations, and find TADPoLe significantly outperforms VLM-RM. We report mean performance and standard deviation across 10 seeds for each task.", "description": "This table presents a comparison of the success rates of TADPoLe and VLM-RM on 12 robotic manipulation tasks from the Meta-World benchmark.  Both methods operate without using in-domain demonstrations or data.  The table shows the average success rate (percentage of successful task completions) and standard deviation across ten trials for each task, highlighting TADPoLe's superior performance.", "section": "4.4 Robotic Manipulation"}, {"figure_path": "nK6OnCpd3n/tables/tables_18_1.jpg", "caption": "Table 1: Results for goal-achievement experiments on DeepMind Control Suite Dog and Humanoid environments. For rows with an associated ground-truth reward function, numerical results are listed; for performant approaches, we report mean and standard deviation across 5 seeds. For novel zero-shot text-conditioned behavior learning, checkmarks denote if the resulting policy is aligned with the provided text prompt according to human evaluation.", "description": "This table presents the results of goal-achievement experiments conducted on the DeepMind Control Suite's Dog and Humanoid environments.  For tasks with ground truth reward functions, the table shows quantitative results (mean and standard deviation across 5 random seeds) of different methods.  For novel tasks where ground-truth rewards were not available, the table shows qualitative results, indicating whether the learned policies successfully matched the given text prompts according to human evaluation.", "section": "4.1 Experimental Setup and Evaluation"}]