[{"type": "text", "text": "Data-Faithful Feature Attribution: Mitigating Unobservable Confounders via Instrumental Variables ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Qiheng Sun1,2, Haocheng Xia3, Jinfei Liu1,2\u2217 ", "page_idx": 0}, {"type": "text", "text": "1Zhejiang University 2Hangzhou High-Tech Zone (Binjiang) Institute of Blockchain and Data Security 3Siebel School of Computing and Data Science University of Illinois Urbana-Champaign {qiheng_sun,jinfeiliu}@zju.edu.cn, hxia7@illinois.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The state-of-the-art feature attribution methods often neglect the influence of unobservable confounders, posing a risk of misinterpretation, especially when it is crucial for the interpretation to remain faithful to the data. To counteract this, we propose a new approach, data-faithful feature attribution, which trains a confounder-free model using instrumental variables. The cluttered effects of unobservable confounders in a model trained as such are decoupled from input features, thereby aligning the output of the model with the contribution of input features to the target feature in the data generation. Furthermore, feature attribution results produced by our method are more robust when focusing on attributions from the perspective of data generation. Our experiments on both synthetic and real-world datasets demonstrate the effectiveness of our approaches. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The increasing complexity and opacity of machine learning (ML) models in real-world applications boost the demand for feature attribution [8]. The feature attribution methods have been developed to help users understand why a model produces certain outputs from specific inputs. For instance, a loan applicant rejected by a bank\u2019s decision-making model might seek reasons behind the denial and what changes could potentially reverse the model\u2019s decision. Some recent studies [19, 9] have shifted the focus of feature attribution from a traditional model-centric perspective to a new perspective that is data-centric. Specifically, users may wish to assign importance values to features according to the data generation process, referring to the causal relationships through which features influence the target feature. For example, consider a medical application setting where a patient seeks to understand which personal features aggravated the illness and the cooperative impact of all features on the illness. What the patient really wants to know is how all features collaboratively contribute to the illness, rather than one diagnostic model\u2019s prediction output. These two aspects of feature attribution align with the concepts of model fidelity and data fidelity, respectively. Here, model fidelity refers to the attribution being consistent with the output of the explained model, while data fidelity pertains to the attribution being consistent with the data generation process. ", "page_idx": 0}, {"type": "text", "text": "SHapley Additive exPlanations (SHAP) [27] and Integrated Gradients (IG) [38] are prevalent representatives of two distinct series of feature attribution methods, each uniquely satisfying critical axioms including sensitivity, implementation invariance, completeness, and symmetry [28]. These properties are essential for ensuring reasonability and fairness in feature attribution. The SHAP-based methods, grounded in game theory and particularly the Shapley value [32], offer interpretations by evaluating all possible combinations of feature contributions in a discrete feature space. In contrast, the IG-based methods, which are an analog of the Aumann-Shapley method [39] from cost-sharing, focus on continuous feature spaces by using path integration over gradients. It is worth noting that even when we aim for interpretations that are faithful to the data, we still rely on a model to predict the target feature values when certain features are selected or excluded in the attribution process. However, when unobservable confounders exist, both SHAP-based and IG-based methods may lead to misunderstandings if applied directly to the widely used predictive model. This is because unobservable confounders, although impacting the output, are entirely overlooked from the process of attribution. Consequently, their influence is instead attributed to other correlated features. ", "page_idx": 0}, {"type": "image", "img_path": "jZv9A8Tg9p/tmp/c2a3cebb16c36af9832ba7a411a8c9ab55570464fd31ba2cb19ecdf14f5c8309.jpg", "img_caption": ["Figure 1: Arrows indicate direct effects. Since Ability s not directly observed and is correlated with education, the influence that should be attributed to Ability (arrow 1) is erroneously attributed to Education (arrow 2) in feature attribution. To fix this issue, Parental Education can be used as an instrumental variable for investigating the true impact of Education on Income. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Motivation example. As shown in Figure 1, suppose a model predicting personal income includes Education as an input feature, and Ability serves as a confounder if the model does not incorporate it as an input feature. Because Ability has an indirect impact on Income through its influence on Education and a direct impact on Income simultaneously, the existing feature attribution methods tend to incorrectly attach the direct impact of Ability on Income to the impact of Education on Income due to their correlation. This concealed correlation may lead to incorrect attribution on the role of educational level in personal income, resulting in an overestimation of the education returns, as demonstrated in our experiments using real datasets (\u00a7 5). ", "page_idx": 1}, {"type": "text", "text": "In this paper, we develop a method to eliminate unobservable confounder effects in feature attribution in order to achieve a deeper understanding from the perspective of data fidelity. The instrumental variable method is widely used for causal analysis [24]. It lies in identifying features that directly affect those influenced by confounders, while not having a direct impact on the outcome themselves. By using the instrumental variable to control confounders that influence specific features, any resulting changes in the outcome variable are driven solely by how the instrumental variable alters that feature. For example, in Figure 1, when examining the impact of Education on Income, a suitable instrumental variable could be the variable Parental Education as analyzed in appendix Section F.2. By observing the changes in Education resulting from variations in Parental Education, we can then discern the true effect of Education on Income, effectively isolating it from other confounders. Intuitively, the instrumental variable approach can help to mitigate the impact of unobservable confounders in feature attribution. However, the instrumental variable approach mainly focuses on evaluating the isolated impact of individual features influenced by unobservable confounders on the outcome variable, while feature attribution lies in considering the cooperative attribution of features. This means evaluating the combined contribution of Education and Other Features on Income. ", "page_idx": 1}, {"type": "text", "text": "To bridge this gap, we propose using a two-stage model training with the instrumental variable that disrupts the association between confounders and other features. Specifically, the model is trained using features re-estimated through instrumental variables and collaborative variables. This ensures that the influence of confounders remains consistent despite variations in feature coalitions. Therefore, the marginal contribution of each feature, which is determined by assessing the impact on the model\u2019s output with and without the feature, is not affected by any confounders. Furthermore, the attribution value of each feature, calculated as the average of its marginal contributions across different feature coalitions, remains influenced by confounders. This alignment allows the contribution of input features to the model output to mirror their contribution in data generation to the target feature, thereby facilitating the attribution to be faithful to the data. Feature attribution involves explaining a model output by assigning attribution scores to the input instance. However, our focus is on data-faithful feature attribution, i.e., we are not trying to explain the output of a specific model but trying to explain the target feature through a model. ", "page_idx": 1}, {"type": "text", "text": "Contributions. To the best of our knowledge, we are the first to identify a crucial issue: unobservable confounders compromise feature attribution, especially when data fidelity is essential. To tackle this challenge, we propose training models free of confounders using instrumental variables, ensuring the feature attribution will remain faithful to the data. We validate the effectiveness of our proposed methods using both real and synthetic datasets, observing that our method achieves up to a $67\\%$ relative improvement over the baseline methods in terms of the error of attribution ratio metric in the real dataset. ", "page_idx": 2}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 Problem Setup ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We aim to quantitatively assess the influence of each input feature on the target feature. This assessment can be viewed as a contribution assignment problem in the context of cooperative game theory [41]. Formally, given an explained input vector of $d$ features $\\pmb{x}^{*}=\\{x_{1}^{*},\\ldots,\\bar{x_{d}^{*}}\\}$ , a baseline input $\\mathbf{\\nabla}x^{\\prime}$ , and a model $f:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ which approximates the data generation equation for the target feature, our objective is to explain the difference in target feature, i.e., $y^{*}\\!-\\!y^{\\prime}$ , conducting data-faithful attribution for the input features. We assume $x^{*}$ and $\\mathbf{x}^{\\prime}$ are of the same dimensionality $d$ , and each entry can be either discrete or continuous. We denote by $X$ the set of input features and $Y$ the target feature, partitioning $X$ into two subsets: $\\tilde{X}$ , which is influenced by unobserved confounders (denoted as $\\mathcal{E}$ ), and $\\overline{{X}}$ , the set of other observable features. For clarity and convenience, we use $\\mathbf{\\Delta}x,\\,y,\\,\\tilde{\\mathbf{}x}$ , $\\overline{{\\pmb{x}}}$ , $\\epsilon$ to denote possible values within feature sets $X,Y,{\\tilde{X}},{\\overline{{X}}},{\\mathcal{E}},$ , respectively. For a given subset of features $\\boldsymbol{S}$ , we denote the subset of the original vector of values by using $\\boldsymbol{S}$ as a subscript, e.g., $\\pmb{x}_{S}:=\\{x_{i}\\}_{i:i\\in S}$ . ", "page_idx": 2}, {"type": "text", "text": "2.2 SHAP-based Attribution ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Shapley Value. Consider a set of players ${\\mathcal{N}}=\\{1,\\ldots,d\\}$ . A coalition $\\boldsymbol{S}$ is a subset of $\\mathcal{N}$ that cooperates to complete a task. A utility function $\\dot{\\mathcal{U}}(S)\\left(S\\subseteq\\mathcal{N}\\right)$ is the utility of a coalition $\\boldsymbol{S}$ for a task. The marginal contribution of player $i$ with respect to a coalition $\\boldsymbol{S}$ is $\\mathcal{U}(\\dot{S}\\cup\\{\\mathrm{i}\\}){-}\\mathcal{U}(S)$ . Shapley value is the unique metric that satisfies the properties of fair reward allocation, including balance, symmetry, additivity, and zero element [41]. It measures the expectation of marginal contribution by $i$ in all possible coalitions. That is, ", "page_idx": 2}, {"type": "equation", "text": "$$\nS\\mathcal V_{i}=\\frac{1}{|N|}\\sum_{\\boldsymbol{S}\\subseteq\\mathcal N\\setminus\\{i\\}}\\frac{\\mathcal{U}(\\boldsymbol{S}\\cup\\{i\\})-\\mathcal{U}(\\boldsymbol{S})}{\\binom{|N|-1}{|\\boldsymbol{S}|}}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Computing the exact Shapley value requires enumerating all utilities for all player subsets. Therefore, the computational complexity of exactly calculating the Shapley value is exponential [46]. ", "page_idx": 2}, {"type": "text", "text": "SHAP [27] utilizes the concept of Shapley values to attribute the contribution of each feature in a model. In the existing SHAP-based methods [27, 19], the definition of utility functions for interpreting an input $\\textbf{\\em x}$ can be divided into two categories [5], condition expectation Shapley and intervention Shapley. In condition expectation Shapley, following the assumption that the features are generated according to a distribution $D$ , the utility function is defined by $\\overbar{\\mathcal{U}}^{c}(\\pmb{S})=E_{D}[f(\\pmb{x})|\\pmb{x}_{S}=\\pmb{x}_{S}^{*}]$ [37] based on the condition expectation of the model prediction under feature set $\\boldsymbol{S}$ . In intervention Shapley, the utility function is defined by $\\mathcal{U}^{\\mathbb{Z}}(\\pmb{S})\\;=\\;E_{D}[f(\\pmb{x})|d o(\\pmb{x}_{S}\\;=\\;\\pmb{x}_{S}^{*})]$ [44] where the operation $d o(\\pmb{x}_{S}\\doteq\\pmb{x}_{S}^{*})$ means we intervene on the features $\\boldsymbol{S}$ in variable $\\textbf{\\em x}$ to be the same as the features in $x^{*}$ , while the features outside of $\\boldsymbol{S}$ in $\\textbf{\\em x}$ are influenced following the causal relationships of the features [30]. For conciseness, we omit the subscript $D$ in the expectation term in the rest of the paper. ", "page_idx": 2}, {"type": "text", "text": "2.3 IG-based Attribution ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "IG is a pivotal method for model attribution [38], particularly well-suited for deep neural networks due to its prerequisite that the model be continuously differentiable. This approach calculates the cumulative gradients along a straight-line path extending from a baseline input $\\pmb{x}^{\\prime}$ to the explained input $x^{*}$ . Mathematically, the attribution $\\mathbb{Z}\\mathcal{G}_{i}$ assigned to a particular feature $\\pmb{x}_{i}^{*}$ for a given input $x^{*}$ ", "page_idx": 2}, {"type": "text", "text": "and baseline $\\pmb{x}^{\\prime}$ is defined as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{Z G}_{i}\\left(\\pmb{x}^{*},\\pmb{x}^{\\prime},f\\right)=\\left(\\pmb{x}_{i}^{*}-\\pmb{x}_{i}^{\\prime}\\right)\\int_{\\alpha=0}^{1}\\frac{\\partial f\\left(\\pmb{x}^{\\prime}+\\alpha\\left(\\pmb{x}^{*}-\\pmb{x}^{\\prime}\\right)\\right)}{\\partial\\pmb{x}_{i}^{*}}d\\alpha.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Remarkably, IG shares similarities with the Aumann-Shapley approach [39] and satisfies several essential properties, including linearity, dummy attribution, Affine Scale Invariance (ASI), proportionality, and symmetry [37]. Recent research has enhanced IG through its application to complex models and the refinement of the integrated path [1, 28]. ", "page_idx": 3}, {"type": "text", "text": "3 Misattribution with Unobservable Confounders ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we carry out a theoretical analysis to demonstrate how unobservable confounders mislead the feature attribution of both SHAP-based and IG-based methods. To show the influence of unobservable confounders in feature attribution, we first employ a simple structural equation to characterize the data-generating process, expressed as ", "page_idx": 3}, {"type": "equation", "text": "$$\ny=g(\\tilde{\\pmb{x}},\\overline{{\\pmb{x}}})+\\epsilon,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $g(\\tilde{{\\boldsymbol x}},\\overline{{{\\boldsymbol x}}})$ can represent any linear or nonlinear continuous relationship involving both $\\tilde{\\pmb{x}}$ and $\\overline{{\\pmb{x}}}$ . The equation allows us to clearly recognize the individual contributions of $\\tilde{\\pmb{x}}$ and $\\overline{{\\pmb{x}}}$ to $y$ , while also considering the unobserved effects encapsulated in the error term $\\epsilon$ . Given that $\\tilde{X}$ is the set of features influenced by unobservable confounders $\\mathcal{E}$ , it generally follows that given two data instances $x_{1}$ and $\\pmb{x}_{2}$ , $\\mathbb{E}[\\epsilon|\\tilde{\\pmb{x}}_{1}]\\neq\\mathbb{E}[\\epsilon|\\tilde{\\pmb{x}}_{2}]$ when $\\tilde{\\mathbf{x}}_{1}\\neq\\tilde{\\mathbf{x}}_{2}$ since $\\tilde{X}$ is influenced by $\\mathcal{E}$ while $\\mathbb{E}[\\epsilon|\\overline{{\\pmb{x}}}_{1}]=\\mathbb{E}[\\epsilon|\\overline{{\\pmb{x}}}_{2}]$ is valid as the feature set $\\overline{{X}}$ is not affected by $\\mathcal{E}$ . ", "page_idx": 3}, {"type": "text", "text": "3.1 Example of Errors for Data-Faithful Feature Attribution ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We discuss how unobservable confounders introduce errors in data-faithful feature attribution with a toy example of Figure 1 in condition expectation Shapley. We can assume $\\epsilon$ as ability, $\\tilde{\\pmb{x}}$ as the measurement of education level, $\\overline{{\\pmb{x}}}$ the work time in a week (i.e., the other variable), and $y$ the weekly income. $\\tilde{\\mathbf{x}},\\overline{{\\mathbf{x}}}$ , and $\\epsilon$ each represents a single numerical variable. The data generation equations are defined as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\epsilon\\sim\\mathrm{Uniform}(0,1),\\quad\\tilde{x}\\sim\\mathrm{Uniform}(0,1)+\\epsilon,\\quad\\overline{{x}}\\sim\\mathrm{Uniform}(0,1),\\quad y=\\tilde{x}\\cdot\\overline{{x}}+\\epsilon.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "In this case, ability influences education, and the three features all have a direct influence on income. Consider a specific data instance $\\pmb{x}^{*}=[\\tilde{\\pmb{x}}^{*},\\overline{{\\pmb{x}}}^{*}]=[1.5,1]$ we are curious about the contributions of the individual\u2019s education level and work hours to their income compared to the features distribution. In this example, it means assigning a value to the individual\u2019s education level $\\tilde{\\pmb{x}}^{*}=1.5$ and work time $\\overline{{\\pmb{x}}}^{*}=1$ to evaluate their contribution to weekly income in comparison to the distribution of education levels and work hours of the population. ", "page_idx": 3}, {"type": "text", "text": "First, we conduct feature attribution with a model $f$ that is trained to fit $\\mathbb{E}[y|\\pmb{x}]\\,=\\,\\mathbb{E}[y|\\pmb{\\tilde{x}},\\pmb{\\overline{{x}}}]\\,=$ $g(\\tilde{\\pmb{x}},\\overline{{\\pmb{x}}})+\\mathbb{E}[\\epsilon|\\tilde{\\pmb{x}},\\overline{{\\pmb{x}}}]$ which simulates the widely used supervised machine learning model training paradigm in reality. The utility derived from the model is $\\mathcal{U}^{\\mathcal{C}}(\\mathcal{S})\\,=\\,\\mathbb{E}[f(\\pmb{x})|\\bar{\\pmb{x}}_{\\mathcal{S}}\\,=\\,\\pmb{x}_{\\mathcal{S}}^{*}]\\,=$ $\\mathbb{E}[g({\\tilde{\\pmb{x}}},{\\overline{{\\pmb{x}}}})|{\\pmb{x}}_{S}~=~{\\pmb{x}}_{S}^{*}]\\,+\\,\\mathbb{E}[\\epsilon|{\\pmb{x}}_{S}~=~{\\pmb{x}}_{S}^{*}]$ . The condition expectation Shapley value of $\\tilde{\\pmb{x}}^{*}$ is $\\begin{array}{r}{{\\mathcal{S}}{\\mathcal{V}}_{\\tilde{x}^{*}}^{c}\\,=\\,\\frac{1}{2}\\{[{\\mathcal{U}}^{\\mathcal{C}}(\\{{\\tilde{x}^{*}}\\})-{\\mathcal{U}}^{\\mathcal{C}}(\\emptyset)]+[{\\mathcal{U}}^{\\mathcal{C}}(\\{{\\tilde{x}^{*}},{\\overline{{x}}^{*}}\\})-{\\mathcal{U}}^{\\mathcal{C}}(\\{{\\overline{{x}}^{*}}\\})]\\}}\\end{array}$ . Replacing the according utilities, we have $\\begin{array}{r}{{\\mathcal S V}_{\\tilde{x}^{\\star}}^{c}\\,=\\,\\frac{1}{2}\\{\\mathbb{E}[g(\\tilde{x}^{\\star},\\overline{{{x}}})]+\\mathbb{E}[\\epsilon|\\tilde{x}^{\\ast},\\overline{{{x}}}]-\\mathbb{E}[g(\\tilde{x},\\overline{{{x}}})]-\\mathbb{E}[\\epsilon|\\tilde{x},\\overline{{{x}}}]+\\mathbb{E}[g(\\tilde{x}^{\\star},\\overline{{{x}}}^{\\ast})]+}\\end{array}$ $\\mathbb{E}[\\epsilon|\\tilde{\\pmb{x}}^{*},\\overline{{\\pmb{x}}}^{*}]-\\mathbb{E}[g(\\tilde{\\pmb{x}},\\overline{{\\pmb{x}}}^{*})]-\\mathbb{E}[\\epsilon|\\tilde{\\pmb{x}},\\overline{{\\pmb{x}}}^{*}]\\}$ . ", "page_idx": 3}, {"type": "text", "text": "Then, we conduct feature attribution for $\\tilde{\\pmb{x}}^{*}$ with the term which it really contribute to $y$ , i.e., $g(\\tilde{{\\boldsymbol{x}}},\\overline{{{\\boldsymbol{x}}}})$ . The utility derived is $\\overline{{\\mathcal{U}}}^{\\mathcal{C}}(\\mathcal{S})\\,=\\,\\mathbb{E}[g(\\tilde{{\\pmb x}},\\overline{{\\pmb x}})|{\\pmb x}_{\\mathcal{S}}\\,=\\,{\\pmb x}_{\\mathcal{S}}^{*}]$ . According to the definition of condition expectation Shapley, $\\begin{array}{r}{\\overline{{S\\mathcal{V}}}_{\\widetilde{x}^{*}}^{\\mathcal{C}}=\\frac{1}{2}\\{[\\overline{{\\mathcal{U}}}^{\\mathcal{C}}(\\{\\widetilde{x}^{*}\\})-\\overline{{\\mathcal{U}}}^{\\mathcal{C}}(\\emptyset)]+[\\overline{{\\mathcal{U}}}^{\\mathcal{C}}(\\{\\widetilde{x}^{*},\\overline{{x}}^{*}\\})-\\overline{{\\mathcal{U}}}^{\\mathcal{C}}(\\{\\overline{{x}}^{*}\\})]\\}}\\end{array}$ . Replacing the accrrording utilities, we have $\\begin{array}{r}{\\overline{{\\mathcal{S}\\mathcal{V}}}_{\\tilde{x}^{*}}^{c}=\\frac{1}{2}\\{\\mathbb{E}[g(\\tilde{x}^{*},\\overline{{x}})]-\\mathbb{E}[g(\\tilde{x},\\overline{{x}})]+\\mathbb{E}[g(\\tilde{x}^{*},\\overline{{x}}^{*})]-\\mathbb{E}[g(\\tilde{x},\\overline{{x}}^{*})]\\}.}\\end{array}$ ", "page_idx": 3}, {"type": "text", "text": "Errors in Feature Attribution Values. The values of each expectation term computed according to the data generation equations are shown in Table 1. Since $\\overline{{\\pmb{x}}}$ is independent from $\\epsilon$ , we have $\\mathbb{E}[\\epsilon|\\tilde{\\pmb{x}},\\overline{{\\pmb{x}}}^{*}]\\stackrel{=}{=}\\mathbb{E}[\\epsilon|\\tilde{\\pmb{x}},\\overline{{\\pmb{x}}}]$ and $\\mathbb{E}[\\epsilon|\\tilde{\\pmb{x}}^{*},\\pmb{\\overline{{x}}}^{*}]=\\mathbb{E}[\\epsilon|\\tilde{\\pmb{x}}^{*},\\pmb{\\overline{{x}}}]$ . By substituting the values for each expected term, we can obtain that $\\mathcal{S V}_{\\tilde{x}^{*}}^{\\mathcal{C}}=0.875,\\overline{{S\\mathcal{V}}}_{\\tilde{x}^{*}}^{\\mathcal{C}}=0.625,S\\mathcal{V}_{\\overline{{x}}^{*}}^{\\mathcal{C}}=0.325$ , and $\\overline{{S\\nu}}_{\\overline{{x}}^{*}}^{c}=0.325$ . It\u2019s clear that attribution based on the model trained to fti $\\mathbb{E}[y|\\pmb{x}]$ , which is a common training paradigm in supervised learning, tends to give a wrong attribution value of $\\tilde{\\pmb{x}}^{*}$ , which is the excessive attribution value of education level in this example. The intuitive reason is that the model associates the direct effect of ability on income with the education level, i.e., the influence of $\\epsilon$ is attached to $\\tilde{\\pmb{x}}$ . However, $\\mathbb{E}[\\epsilon|\\tilde{{\\pmb x}}^{*},\\overline{{{\\pmb x}}}]-\\dot{\\mathbb{E}}[\\epsilon|\\tilde{{\\pmb x}},\\overline{{{\\pmb x}}}]$ is not actually in the effect of $\\tilde{\\pmb{x}}^{*}$ on $y$ because $\\tilde{\\pmb{x}}$ does not influence $\\epsilon$ during the data generation process. ", "page_idx": 3}, {"type": "table", "img_path": "jZv9A8Tg9p/tmp/4606c26ef79854a9adfcbff17a99af54bbcd1863a5ac320f5e1eb64753d0f04f.jpg", "table_caption": ["Table 1: Value of each expectation term in $\\boldsymbol{S}\\boldsymbol{\\mathcal{V}}_{\\tilde{\\boldsymbol{x}}^{*}}^{\\mathcal{C}}$ "], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "3.2 Errors in Feature Attribution with Unobservable Confounders ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We analyze the attribution errors when the feature attribution is conducted on a model $f$ trained to fti $\\mathbb{E}[y|\\pmb{x}]=\\mathbb{E}[y|\\pmb{\\tilde{x}},\\pmb{\\overline{{x}}}]$ in supervised learning for SHAP-based method (Propositions 1 and 2) and IG (Proposition 3), respectively. ", "page_idx": 4}, {"type": "text", "text": "Proposition 1. The expected error for marginal contribution of feature $i$ in condition expectation Shapley with model $f$ trained to fit $\\mathbb{E}[y|x]$ is $\\mathbb{E}[\\epsilon|\\pmb{x}_{S\\cup\\{i\\}}=\\pmb{x}_{S\\cup\\{i\\}}^{*}]-\\mathbb{E}[\\epsilon|\\pmb{x}_{S}=\\pmb{x}_{S}^{*}]$ , resulting an expected deviation of attribution value by $\\begin{array}{r}{\\Delta S\\mathcal{V}_{i}\\,=\\,\\frac{1}{N}\\sum_{S\\subseteq{\\mathcal{N}}\\setminus\\{i\\}}\\binom{|{\\mathcal{N}}|-1}{|S|}^{-1}\\{\\mathbb{E}[\\epsilon|x_{S\\cup\\{i\\}}\\,=\\,}\\end{array}$ $\\pmb{x}_{S\\cup\\{i\\}}^{*}]-\\mathbb{E}[\\epsilon|\\pmb{x}_{S}=\\pmb{x}_{S}^{*}]\\}$ . ", "page_idx": 4}, {"type": "text", "text": "Proof. Due to the limited space, please see the appendix for detailed proof. The same to the following propositions. ", "page_idx": 4}, {"type": "text", "text": "Proposition 2. The expected error for marginal contribution of feature $i$ in intervention Shapley with mode $f$ trained to fit $\\mathbb{E}[y|\\pmb{x}]$ is $\\mathbb{E}_{D}[\\epsilon|d o(\\pmb{x}_{S\\cup\\{i\\}}=\\pmb{x}_{S\\cup\\{i\\}}^{*})]-\\mathbb{E}[\\epsilon|d o(\\pmb{x}_{S}=\\pmb{x}_{S}^{*})]$ )], resulting an expected deviation of attribution value by $\\begin{array}{r}{\\Delta S\\mathcal{V}_{i}=\\frac{1}{\\mathcal{N}}\\sum_{S\\subseteq\\mathcal{N}\\setminus\\{i\\}}\\binom{|\\mathcal{N}|-1}{|S|}^{-1}\\{\\mathbb{E}[\\epsilon|d o(\\pmb{x}_{S\\cup\\{i\\}}=}\\end{array}$ $\\begin{array}{r}{\\pmb{x}_{S\\cup\\{i\\}}^{*})]-\\mathbb{E}[\\epsilon|d o(\\pmb{x}_{S}=\\pmb{x}_{S}^{*})]\\}.}\\end{array}$ ", "page_idx": 4}, {"type": "text", "text": "Proposition 3. The expected error for attribution value of feature $i$ using $I G$ with model f trained to $f u\\mathbb{E}[y|x]$ is $\\begin{array}{r}{\\Delta\\mathcal{T}\\mathcal{G}_{i}=(x_{i}^{*}-x_{i}^{\\prime})\\int_{\\alpha=0}^{1}\\frac{\\partial f\\left(x^{\\prime}+\\alpha\\left(x^{*}-x^{\\prime}\\right)\\right)}{\\partial x_{i}}-\\frac{\\partial g\\left(x^{\\prime}+\\alpha\\left(x^{*}-x^{\\prime}\\right)\\right)}{\\partial x_{i}}d\\alpha.}\\end{array}$ . ", "page_idx": 4}, {"type": "text", "text": "4 Mitigating Unobservable Confounders via Instrumental Variables ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "As demonstrated in Section 3, when it pertains to unobservable confounders, the prevalent feature attribution methods including SHAP and IG inevitably lead to misunderstandings that are not faithful to the data, since they rely on predictive model $f$ trained to fti $\\mathbb{E}[y|x]$ . This is fundamental because the trained predictive model has already associated the unobservable confounders with the input features. Therefore, it is tempting to ask: how can we decouple the confounders from their correlations with other features in the used model $f$ ? ", "page_idx": 4}, {"type": "text", "text": "4.1 Motivation of Using Confounder-free Model ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "One may think a straightforward solution is directly training a model $f$ to fti $g(\\tilde{\\pmb{x}},\\overline{{\\pmb{x}}})$ . Unfortunately, it is nearly impossible since we cannot remove the influence of unobservable confounders in the target feature $y$ . To bridge the gap, we provide an alternative solution to train a model that gives the same attribution results for the input features as it is trained to fit $g(\\tilde{{\\boldsymbol{x}}},\\overline{{{\\boldsymbol{x}}}})$ . Denote by $\\hat{y}=\\bar{g}(\\tilde{\\pmb{x}},\\overline{{\\pmb{x}}})+\\mathbb{E}[\\epsilon]$ , and $f$ is trained to fit $\\mathbb{E}[\\hat{y}|\\tilde{{\\boldsymbol{x}}},\\overline{{{\\boldsymbol{x}}}}]$ . The influence of $\\tilde{\\pmb{x}}$ and $\\overline{{\\pmb{x}}}$ on $\\hat{y}$ is identical to their impact on $y$ , as both are encompassed within $\\bar{g}(\\tilde{\\pmb{x}},\\overline{{\\pmb{x}}})$ . ", "page_idx": 4}, {"type": "text", "text": "Example. For the toy example in Section 3, the utility calculated with $f$ trained to fit $\\mathbb{E}[\\hat{y}|\\pmb{\\tilde{x}},\\pmb{\\overline{{x}}}]=$ $g(\\tilde{\\pmb{x}},\\overline{{\\pmb{x}}})+\\mathbb{E}[\\epsilon]$ is $\\hat{\\mathcal{U}}^{\\mathcal{C}}(\\pmb{S})=\\mathbb{E}[g(\\pmb{\\tilde{x}},\\pmb{\\overline{{x}}})|\\pmb{x}_{S}=\\pmb{x}_{S}^{*}]+\\mathbb{E}[\\epsilon]$ . The condition expectation Shapley value of $\\tilde{\\pmb{x}}^{*}$ is $\\begin{array}{r}{\\hat{S}\\hat{\\mathcal{V}}_{\\tilde{x}^{*}}^{\\mathcal{C}}=\\frac{1}{2}\\{[\\hat{\\mathcal{U}}(\\{\\tilde{x}^{*}\\})-\\hat{\\mathcal{U}}(\\emptyset)]+[\\hat{\\mathcal{U}}(\\{\\tilde{x}^{*},\\overline{{x}}^{*}\\})-\\hat{\\mathcal{U}}(\\{\\overline{{x}}^{*}\\})]\\}}\\end{array}$ . By replacing the according utilitie $\\begin{array}{r}{\\mathfrak{z},\\;\\mathrm{we~have~}\\dot{\\mathcal{S}}\\dot{\\mathcal{V}}_{\\tilde{\\mathcal{X}}^{\\ast}}\\;=\\;\\frac{1}{2}\\{\\mathbb{E}[g(\\tilde{\\pmb{x}}^{\\ast},\\overline{{\\mathcal{X}}})]+\\mathbb{E}[\\epsilon]-\\mathbb{E}[g(\\tilde{\\pmb{x}},\\overline{{\\mathcal{X}}})]-\\mathbb{E}[\\epsilon]+\\mathbb{E}[g(\\tilde{\\pmb{x}}^{\\ast},\\overline{{\\mathcal{X}}}^{\\ast})]\\}}\\end{array}$ )] + E[\u03f5] \u2212 $\\begin{array}{r}{\\Sigma[g(\\tilde{x},\\overline{{x}}^{*})]-\\mathbb{E}[\\epsilon]\\}=\\frac{1}{2}\\{\\mathbb{E}[g(\\tilde{x}^{*},\\overline{{x}})]-\\mathbb{E}[g(\\tilde{x},\\overline{{x}})]+\\mathbb{E}[g(\\tilde{x}^{*},\\overline{{x}}^{*})]-\\mathbb{E}[g(\\tilde{x},\\overline{{x}}^{*})]\\}=\\overline{{S}}\\overline{{\\nu}}_{\\tilde{x}^{*}}^{\\star}.}\\end{array}$ ", "page_idx": 4}, {"type": "text", "text": "The advantage of attribution based on $\\hat{y}$ lies in the term $\\mathbb{E}[\\epsilon]$ being constant, thereby breaking the association between $\\epsilon$ and the input features. ", "page_idx": 5}, {"type": "text", "text": "Proposition 4. From the perspective of data generation, the contributions of features in $\\tilde{\\pmb{x}}$ and $\\overline{{\\pmb{x}}}$ to $y$ are equivalent to their contributions to $\\hat{y}$ . When using the condition expectation Shapley, intervention Shapley, and Integrated Gradients $(I G)$ methods, the attribution values of each feature in $\\tilde{\\pmb{x}}$ and $\\overline{{\\pmb{x}}}$ are identical for both models $f=g(\\tilde{\\boldsymbol{x}},\\overline{{\\boldsymbol{x}}})$ and $f=g(\\tilde{\\pmb{x}},\\overline{{\\pmb{x}}})+\\mathbb{E}[\\epsilon]$ . ", "page_idx": 5}, {"type": "text", "text": "4.2 Confounder-free Model Building ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "With Proposition 4, the problem becomes how to train the model $f$ to fit $\\mathbb{E}[\\hat{y}|\\tilde{{\\boldsymbol{x}}},\\overline{{{\\boldsymbol{x}}}}]$ now. To achieve this, we introduce the instrumental variables. ", "page_idx": 5}, {"type": "text", "text": "Instrumental Variable. The features that are used as instrumental variables, denoted as $\\Psi$ , can be effectively utilized in our model if they satisfy the following three key properties. 1) relevance: $\\Psi$ should correlate with $\\tilde{X}$ , ensuring that $\\Psi$ can serve as a reliable proxy for these features. 2) exogeneity: $\\Psi$ should be uncorrelated with the latent confounders $\\mathcal{E}$ , ensuring that it is not influenced by these unobserved factors. 3) exclusion restriction: $\\Psi$ should influence the outcome $Y$ solely through its effect on $\\tilde{X}$ . In other words, apart from its interaction with $\\tilde{X}$ , $\\Psi$ should not have any other direct or indirect pathways affecting $Y$ . This ensures that the effect of $\\Psi$ on $Y$ can be unambiguously attributed to its relationship with X\u02dc. The effectiveness of IV-SHAP and IV-IG may be reduced when the three assumptions of instrumental variables are violated. However, the extent of this reduction depends on how severely the assumptions are violated. ", "page_idx": 5}, {"type": "text", "text": "With the help of instrumental variables, we can establish the following equation by taking the expectation of $y$ given $\\overline{{\\pmb{x}}}$ and $\\psi$ , ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{E}[y|\\overline{{\\pmb{x}}},\\psi]=\\mathbb{E}[g(\\pmb{\\mathscr{x}},\\overline{{\\pmb{x}}})|\\overline{{\\pmb{x}}},\\psi]+\\mathbb{E}[\\epsilon]=\\int g(\\pmb{\\mathscr{x}},\\overline{{\\pmb{x}}})+\\mathbb{E}[\\epsilon]d M(\\pmb{\\mathscr{x}}|\\overline{{\\pmb{x}}},\\psi),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\psi$ is a possible value of $\\Psi$ and $d M({\\tilde{\\pmb{x}}}|{\\overline{{\\pmb{x}}}},\\psi)$ is the conditional distribution of $\\tilde{X}$ . Given the $T$ training data instances, the optimal parameters of model $f$ trained to fit $g(\\tilde{\\pmb{x}},\\overline{{\\pmb{x}}})+\\mathbb{E}[\\epsilon]$ within the function space $\\mathcal{H}$ are identified by minimizing the following objective: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{f\\in\\mathcal{H}}\\sum_{t=1}^{T}\\mathcal{L}\\left(y_{t}-\\int f\\left(\\tilde{\\pmb{x}},\\overline{{\\pmb{x}}}_{t}\\right)d M\\left(\\tilde{\\pmb{x}}|\\overline{{\\pmb{x}}}_{t},\\psi_{t}\\right)\\right)\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\mathcal{L}$ represents the loss metric we used to evaluate model performance and $t$ is the index of specific data instance. We provide the training methods for supervised neural network models which are extensively employed in the real world in Section 4.3. Specifically, we discuss their loss functions and gradient computations when the objectives are regression and classification problems. The training steps are inspired by the two-stage training in causal effect estimation [2] and counterfactual prediction [18]. The re-estimated unconfounded values are sampled from the first-stage trained model, the sampling has little influence on the implementation and computation complexity of the second-stage model training. Therefore, the two-stage training process does not limit the method\u2019s practical usability. Due to the limited space, we provide details of model training with discrete input features and non-gradient model training in appendix Section D. ", "page_idx": 5}, {"type": "text", "text": "4.3 Confounder-free Model Training ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Model Training for Continuous Feature Attribution. In the regression task, where the model $f$ is a neural network trained for forecasting continuous value, it is also denoted as $f_{\\theta}({\\tilde{x}},{\\overline{{x}}})$ , where $\\theta$ represents the model parameters. As our objective, we adopt a $l_{2}$ loss function. With the unknown conditional distribution of X\u02dc given $X$ and $\\Psi$ , we initially utilize a neural network model, denoted $\\hat{M}_{\\phi}$ , where $\\phi$ is the model parameters, to approximate this distribution. The $l_{2}$ loss function for determining the optimal model parameters $\\theta$ subsequently approximates as per the following equation ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}(T;\\theta)=|T|^{-1}\\sum_{t}\\left(y_{t}-\\int f_{\\theta}\\left(\\tilde{\\mathbf{\\alpha}},\\overline{{\\mathbf{x}}}_{t}\\right)d\\hat{M}_{\\phi}\\left(\\tilde{\\mathbf{\\alpha}}\\mid\\overline{{\\mathbf{x}}}_{t},\\psi_{t}\\right)\\right)^{2},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where the integral term estimates the expected output of $f_{\\theta}$ under the distribution approximated by $\\hat{M}_{\\phi}$ . By employing the relevant calculations, we ascertain that the gradient of the loss function with respect to the $t^{t h}$ training data point is ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\nabla_{\\theta}\\mathcal{L}_{t}=-\\left2\\mathbb{E}_{\\hat{M}_{\\phi}(\\tilde{\\mathbf{x}}|\\overline{{\\mathbf{x}}}_{t},\\psi_{t})}\\left[y_{t}-f_{\\theta}\\left(\\tilde{\\mathbf{x}},\\overline{{\\mathbf{x}}}_{t}\\right)\\right]\\cdot\\mathbb{E}_{\\hat{M}_{\\phi}(\\tilde{\\mathbf{x}}|\\overline{{\\mathbf{x}}}_{t},\\psi_{t}).}\\left[f_{\\theta}^{\\prime}\\left(\\tilde{\\mathbf{x}},\\overline{{\\mathbf{x}}}_{t}\\right)\\right].\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "In short, our training process comprises two fundamental steps: 1) an instrumental variable method is applied to re-estimate $\\tilde{\\pmb{x}}$ , mitigating the impact of unobservable confounders, and 2) this refined $\\tilde{\\pmb{x}}$ is utilized to calculate the gradients for $f$ . ", "page_idx": 6}, {"type": "text", "text": "Model Training for Discrete Feature Attribution. In the classification task, where $y$ is a discrete variable representing classes, we adapt the loss function to the multi-class cross-entropy ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{L}(T;\\theta)=|T|^{-1}\\sum_{t}\\sum_{r=1}^{R}\\left(\\int y_{t,r}\\cdot\\ln f_{\\theta,r}\\left(\\tilde{x},\\overline{{x}}_{t}\\right)\\!d\\hat{M}_{\\phi}\\left(\\tilde{x}\\mid\\overline{{x}}_{t},\\psi_{t}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "In this formulation, $y_{t,r}$ represents the true label of the $t^{t h}$ data point in the $r^{\\mathrm{th}}$ category. $R$ denotes the total number of distinct classes into which the target variable y can be classified. The probability of the model classifying a data point into the $r^{\\mathrm{th}}$ category is given by $f_{\\theta,r}\\left(\\widetilde{\\pmb{x}},\\overline{{\\pmb{x}}}_{t}\\right)$ . The gradient calculation for the $t^{t h}$ training data point, considering this loss function, is then ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\nabla_{\\theta}\\mathcal{L}_{t}=\\mathbb{E}_{\\hat{M}_{\\phi}(\\tilde{\\mathbf{x}}|\\overline{{\\mathbf{x}}}_{t},z_{t})}\\left[\\sum_{r=1}^{R}\\frac{y_{t,r}}{f_{\\theta,r}\\left(\\tilde{\\mathbf{x}},\\overline{{\\mathbf{x}}}_{t}\\right)}\\cdot f_{\\theta,r}^{\\prime}\\left(\\tilde{\\mathbf{x}},\\overline{{\\mathbf{x}}}_{t}\\right)\\right].\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "This adaptation of the loss function for discrete target variables ensures that our model can handle classification tasks, effectively optimizing its performance across multiple categories. ", "page_idx": 6}, {"type": "text", "text": "Feature Attribution Computation. The exact computation of Shapley value and integrated gradients needs huge cost while approximation methods are widely used. Towards practical applications, we further propose a Shapley value approximation method and an integrated gradients approximation method for saving computation costs in Sections E.1 and E.2, respectively. The correlation of input features may affect the data-faithfulness of IV-SHAP and IV-IG. We can combine methods which deal with the correlated input features to the two-stage model to better capture these correlations. For example, on-manifold Shapley [26] can be used to account for feature correlations, while causal Shapley [19] can be applied if the causal structure of the input features is known. ", "page_idx": 6}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we present our empirical evaluation in detail. We employ synthetic and real-world datasets to evaluate the faithfulness and robustness against unobservable confounders of feature attributions given by our proposed data-faithful feature attribution methods to prevalent SHAP-based and IG methods. Comparisons of feature attribution for classification problems, feature attribution on non-gradient training models, and the approximation methods of SHAP and IG are given in appendix Sections F.3, F.4, and F.5, respectively. Our code can be found in the repository at https://github.com/ZJU-DIVER/IV-SHAP. ", "page_idx": 6}, {"type": "text", "text": "5.1 Experiments on Synthetic Datasets ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We first conducted a data simulation experiment to validate our proposed methods\u2019 effectiveness. Synthetic datasets offer an advantage in studying feature attribution, as we can obtain the ground truth of attribution values according to the data generation equation which is unobtainable in most real datasets. ", "page_idx": 6}, {"type": "text", "text": "Dataset Generation Process. We generated two synthetic datasets, each containing four parts: an unobserved confounder $\\epsilon$ , a variable $\\tilde{\\pmb{x}}$ influenced by the unobserved confounder, collaborative variables $\\overline{{\\mathbf{\\emx}}}=\\{\\overline{{x}}_{i}\\}$ $(1\\leq i\\leq6)$ ), and the target feature $y$ . Notably, dataset A and dataset B share the same $\\overline{{\\pmb{x}}}$ and $\\psi$ . $\\epsilon$ is formulated by a uniform variable $v$ and a parameter $\\rho$ which controls the noise level. The generation of these features adhered to specific functional relationships, as illustrated in ", "page_idx": 6}, {"type": "image", "img_path": "jZv9A8Tg9p/tmp/41abb7648c6f6cb97c8c42e2b27e4c981deeabb426024455d7afe3fe2b36f5db.jpg", "img_caption": ["Figure 2: Evaluation results on synthetic Dataset A. "], "img_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "jZv9A8Tg9p/tmp/e0eb8b9073db2cc9782ad93546d5939a11ad8f5c90262e04f7db4c755876d129.jpg", "img_caption": ["Figure 3: Evaluation results on synthetic Dataset B. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "the equations below. ", "page_idx": 7}, {"type": "image", "img_path": "jZv9A8Tg9p/tmp/fc1ae9e6221a5de9ed3a4e7b0be83af11b334a7ca442196e029c4224f22ee3a8.jpg", "img_caption": [], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Compared Methods. We utilized two representative feature attribution algorithms, SHAP [27] and IG [38], as the baseline methods. Specifically, we trained a neural network model on synthetic datasets to fit $\\mathbb{E}[y|x]$ as the baseline model. Then we applied the two feature attribution methods to attribute contributions for input features. For our proposed approach, we employed the same neural network architecture but trained the model to fit $\\mathbb{E}[\\hat{y}|x]$ with instrumental variables. Our attribution approaches, applied to the model trained with the instrumental variable, are referred to as IV-SHAP and IV-IG, corresponding to SHAP and IG, respectively. It is worth noting that for the model, the explicitly input data features have no causal relationships among them. Therefore, Causal SHAP [19], Asymmetric SHAP [14], BSHAP [37] and SHAP are equivalent in this context. ", "page_idx": 7}, {"type": "text", "text": "Experimental Results. We randomly generated 1000 data points based on the data generation equations. We then adjusted features $\\tilde{\\pmb{x}}$ and $\\overline{{\\pmb{x}}}$ of each data point by subtracting a certain value as a baseline input. We conducted experiments with varied subtracted values set at 0.125, 0.25, 0.375, and 0.5. For each data point, we applied IV-SHAP, IV-IG, SHAP, and IG to attribute the contributions of the features. Subsequently, we compared the attribution values of feature $\\tilde{\\pmb{x}}$ against the ground truth obtained directly from the data generation equations. We observe that the errors in attribution results provided by IV-SHAP and IV-IG are significantly smaller than those of SHAP and IG. The absolute errors in the attribution values of each algorithm for every data point, as compared to the benchmark, are illustrated in Figures 2 and 3. ", "page_idx": 7}, {"type": "text", "text": "5.2 Experiments on Real-world Datasets ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We conducted experiments on two real-world datasets to demonstrate the efficacy of IV-SHAP and IV-IG in practical scenarios where data generation processes are black-box. Initially, we excluded specific features to simulate unobservable confounders. Subsequently, we trained a model using the complete set of features to establish ground-truth attribution values, thereby assessing the robustness and reliability of the compared methods under realistic conditions. ", "page_idx": 8}, {"type": "text", "text": "Real-world Datasets. The first real dataset we used is the Griliches76 dataset [17, 36], consisting of 758 entries with 20 variables each, gathered from the U.S. labour market. This dataset is extensively used in research to explore the impact of education on income. In the study examining the relationship between the logarithm of weekly earnings (lw) and other features such as educational years (edu), years of work experience (expr), tenure at the current organization (tenure), marital status (mr), residence in the South (rns), and urban residence (smsa), there exists a significant challenge. Ability, as an unobservable confounder, not only directly influences an individual\u2019s education level but also their income. Using feature attribution methods like SHAP and IG without accounting for the confounder might incorrectly attribute the effect of ability on income to correlated educational levels. To address this issue, we incorporated the educational years of the mother (medu) as an instrumental variable to affect an individual\u2019s education. Additionally, IQ scores (iq) and knowledge in the world of work test (kww) in the dataset, serving as crucial indicators of ability, offer a unique opportunity to approximate the ground truth of the real contribution of each feature. ", "page_idx": 8}, {"type": "text", "text": "Compared Methods. We assume a decrease in educational years for each individual as baseline inputs and execute a two-phase experiment. Initially, we omit IQ and the world of work test scores, calculating attribution values for IV-SHAP, IV-IG, SHAP, and IG. Note that the process of computing these attribution values using IV-SHAP, IV-IG, SHAP, and IG is consistent with the synthetic dataset experiments. These methods are employed to evaluate the impact of reduced education years. Subsequently, we incorporate IQ and the world of work test scores to train a new model and recalculate attribution values using SHAP and IG. Due to the absence of a real-world benchmark in the reality dataset, we adopt the attribution results from the model, which includes unobservable confounders in its training process, as our benchmark for comparison. ", "page_idx": 8}, {"type": "text", "text": "riwch.ere e ea tetrdiubcuatitioonn arla taittor ibofut iIoVn- SvaHluAeP f obr yth eE IdVaStHa ApPo $=$ $\\textstyle{\\frac{1}{n}}\\sum_{i=1}^{n}\\left|{\\frac{\\operatorname{IVSHAP}_{i}}{l w_{i}}}\\right|$ $\\mathrm{IVSHAP}_{i}$ $i^{t h}$ calculated by the IV-SHAP method. $\\mathrm{IVSHAP}_{i}$ represents the extent to which changes in educational years influence the income in the $i^{t h}$ data point. $l w_{i}$ is the income for the $i^{t h}$ data point. EARIVSHAP is the average of the absolute values of the ratios between the educational attribution values and income across all data points. This measure provides a comprehensive quantification of the impact of educational years on income. The average attribution ratio for reduced educational years, calculated by SHAP in the model trained with IQ and the world of work test scores, is denoted as EARBMSHAP $\\begin{array}{r}{=\\frac{1}{n}\\sum_{i=1}^{n}\\left|\\frac{\\mathrm{BMSHAP}_{i}}{l w_{i}}\\right|}\\end{array}$ where $\\mathrm{BMSHAP}_{i}$ represents the benchmark attribution of education on income, incorporating IQ and the world of work test. We then compute the absolute relative error between the attributions of IV-SHAP and the benchmark using the formula $\\Big|\\,\\frac{\\mathrm{EAR}_{\\mathrm{IVSHAP}}\\,{-}\\,\\mathrm{EAR}_{\\mathrm{BMSHAP}}}{\\mathrm{EAR}_{\\mathrm{BMSHAP}}}\\,\\Big|$ . For the SHAP algorithm, EARBMSHAP still serve as a benchmark for EARSHAP. For the attribution values calculated by IV-IG and IG, we use the average attribution ratio obtained by the IG algorithm on the model that includes IQ and kww as inputs as the benchmark. ", "page_idx": 8}, {"type": "text", "text": "Experimental Results. The experimental results are shown in Table 2, which demonstrates that our methods can significantly reduce the attribution error. The values in the table represent the mean of five independent runs, with the standard deviation following each mean. ", "page_idx": 8}, {"type": "table", "img_path": "jZv9A8Tg9p/tmp/e31f0f1ccf7654d117bd6e388af0d2b691cf988cf22b46404de1d3efebb67ba2.jpg", "table_caption": ["Table 2: Relative error of each attribution algorithm. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Empirical Analysis. The second real dataset we use is the Angrist and Krueger dataset [3], which is an American census dataset consisting of statistical data on people born in specific years, including variables such as age (AGE), an education level (EDUC), weekly wage (LWKLYWGE), marital status (MARRIED), and race (RACE). We employed this dataset to examine the confounder effects of the ability on the attribution of education for income. In this dataset, we used the quarter of birth as an instrumental variable for years of education. The rationale behind this is the compulsory education laws in various states, which typically mandate schooling until the age of 16. Students born early in the year often start school later, leading to systematic differences in educational duration based on birth quarter. However, this dataset lacks measures of intelligence or work capability to assess the factor of ability, so we cannot conduct the experiment like the previous one. Nevertheless, the average attribution ratio of one additional year, calculated by SHAP, IV-SHAP, IG, and IV-IG, are 0.0218, 0.0206, 0.0218, and 0.0207, respectively. This aligns with the observation that people may overestimate educational returns because of neglecting the confounder ability in [6]. ", "page_idx": 9}, {"type": "text", "text": "6 Limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Despite the strengths of our approach, there are several limitations to consider which are shown as follows: ", "page_idx": 9}, {"type": "text", "text": "\u2022 Dependence on the Availability of Instrumental Variables: Our approach assumes the presence of suitable instrumental variables for features affected by unobserved confounders. However, in practical scenarios, finding appropriate instrumental variables can be challenging sometimes. For further information on identifying instrumental variables, refer to works such as [4], [10], and [23].   \n\u2022 Linearity Assumption in Theoretical Derivations: Our theoretical derivations are based on the assumption that the influence of unobserved confounders on the target features is linear. This assumption does not hold in all real-world situations. Nevertheless, in our experiments with real datasets in Section 5.2, our attribution method showed significant improvements over existing methods, even when the influence of unobservable confounders on features was non-linear. ", "page_idx": 9}, {"type": "text", "text": "These limitations highlight areas for future research, particularly in developing methods that do not rely on the availability of instrumental variables and that can give theoretical analysis of nonlinear effects of unobserved confounders. Addressing these aspects can enhance the practicality and applicability of our methods. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we focus on addressing the effects of unobservable confounders in feature attribution, emphasizing feature attribution being faithful to data. The proposed method improves the understanding of the causal factors driving an outcome variable, going beyond standard attribution scores that simply describe predictive models. Our approach of training confounder-free models using instrumental variables effectively isolates the impact of confounders, enhancing the robustness of data-faithful feature attribution results. Our validations using real and synthetic datasets confirm the effectiveness of the proposed methods. For future work, we intend to develop methods that do not rely on the availability of instrumental variables and that can provide a theoretical analysis of the non-linear effects of unobserved confounders. For the broader impacts of the paper, please see Section A in the appendix due to the limited space. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgment ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The authors would like to thank the anonymous reviewers for their helpful comments. This work was supported in part by the National Key RD Program of China (2021YFB3101100), NSFC grants (62102352, U23A20306), The Zhejiang Province Pioneer Plan (2024C01074), and NSF grant (CNS-2125530). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] N. Akhtar and M. A. Jalwana. Towards credible visual model interpretation with path attribution. In International Conference on Machine Learning, pages 439\u2013457. PMLR, 2023.   \n[2] J. D. Angrist and G. W. Imbens. Two-stage least squares estimation of average causal effects in models with variable treatment intensity. Journal of the American statistical Association, 90(430):431\u2013442, 1995.   \n[3] J. D. Angrist and A. B. Krueger. The effect of age at school entry on educational attainment: an application of instrumental variables with moments from two samples. Journal of the American statistical Association, 87(418):328\u2013336, 1992.   \n[4] M. Baiocchi, J. Cheng, and D. S. Small. Instrumental variable methods for causal inference. Statistics in medicine, 33(13):2297\u20132340, 2014. [5] S. Bordt and U. von Luxburg. From shapley values to generalized additive models and back. In International Conference on Artificial Intelligence and Statistics, pages 709\u2013745. PMLR, 2023.   \n[6] D. Card. The causal effect of education on earnings. Handbook of labor economics, 3:1801\u2013 1863, 1999.   \n[7] J. Castro, D. G\u00f3mez, and J. Tejada. Polynomial calculation of the shapley value based on sampling. Computers & Operations Research, 36(5):1726\u20131730, 2009.   \n[8] H. Chen, I. C. Covert, S. M. Lundberg, and S.-I. Lee. Algorithms to estimate shapley value feature attributions. Nature Machine Intelligence, 5(6):590\u2013601, 2023.   \n[9] H. Chen, J. D. Janizek, S. Lundberg, and S.-I. Lee. True to the model or true to the data? arXiv preprint arXiv:2006.16234, 2020.   \n[10] N. M. Davies, G. D. Smith, F. Windmeijer, and R. M. Martin. Issues in the reporting and conduct of instrumental variable studies: a systematic review. Epidemiology, 24(3):363\u2013369, 2013.   \n[11] X. Deng and C. H. Papadimitriou. On the complexity of cooperative solution concepts. Math. Oper. Res., 19(2):257\u2013266, 1994.   \n[12] R. Dwivedi, D. Dave, H. Naik, S. Singhal, R. Omer, P. Patel, B. Qian, Z. Wen, T. Shah, G. Morgan, et al. Explainable ai (xai): Core ideas, techniques, and solutions. ACM Computing Surveys, 55(9):1\u201333, 2023.   \n[13] J. Enguehard. Sequential integrated gradients: a simple but effective method for explaining language models. In Findings of the Association for Computational Linguistics: ACL 2023, Toronto, Canada, July 9-14, 2023, pages 7555\u20137565. Association for Computational Linguistics, 2023.   \n[14] C. Frye, C. Rowat, and I. Feige. Asymmetric shapley values: incorporating causal knowledge into model-agnostic explainability. Advances in Neural Information Processing Systems, 33:1229\u20131239, 2020.   \n[15] H. Gao, J. Li, W. Qiang, L. Si, B. Xu, C. Zheng, and F. Sun. Robust causal graph representation learning against confounding effects. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 7624\u20137632, 2023.   \n[16] A. Ghorbani and J. Zou. Data shapley: Equitable valuation of data for machine learning. In International conference on machine learning, pages 2242\u20132251. PMLR, 2019.   \n[17] Z. Griliches. Wages of very young men. Journal of Political Economy, 84(4, Part 2):S69\u2013S85, 1976.   \n[18] J. Hartford, G. Lewis, K. Leyton-Brown, and M. Taddy. Deep iv: A flexible approach for counterfactual prediction. In International Conference on Machine Learning, pages 1414\u20131423. PMLR, 2017.   \n[19] T. Heskes, E. Sijben, I. G. Bucur, and T. Claassen. Causal shapley values: Exploiting causal knowledge to explain individual predictions of complex models. Advances in neural information processing systems, 33:4778\u20134789, 2020.   \n[20] N. Jethani, M. Sudarshan, I. Covert, S.-I. Lee, and R. Ranganath. Fastshap: Real-time shapley value estimation. ICLR 2022, 2022.   \n[21] Y. Jung, S. Kasiviswanathan, J. Tian, D. Janzing, P. Bl\u00f6baum, and E. Bareinboim. On measuring causal contributions via do-interventions. In International Conference on Machine Learning, pages 10476\u201310501. PMLR, 2022.   \n[22] D. Kaltenpoth and J. Vreeken. Nonlinear causal discovery with latent confounders. In International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pages 15639\u201315654. PMLR, 2023.   \n[23] Y. Kawakami. Instrumental variable-based identification for causal effects using covariate information. In Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, ThirtyThird Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event, February 2-9, 2021, pages 12131\u201312138. AAAI Press, 2021.   \n[24] Y. Kawakami, M. Kuroki, and J. Tian. Instrumental variable estimation of average partial causal effects. In International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pages 16097\u201316130. PMLR, 2023.   \n[25] R. Liu, C. Yin, and P. Zhang. Estimating individual treatment effects with time-varying confounders. In 2020 IEEE International Conference on Data Mining (ICDM), pages 382\u2013391. IEEE, 2020.   \n[26] S. M. Lundberg and S.-I. Lee. Consistent feature attribution for tree ensembles. arXiv preprint arXiv:1706.06060, 2017.   \n[27] S. M. Lundberg and S.-I. Lee. A unified approach to interpreting model predictions. Advances in neural information processing systems, 30, 2017.   \n[28] D. D. Lundstrom, T. Huang, and M. Razaviyayn. A rigorous study of integrated gradients method and extensions to internal neuron attributions. In International Conference on Machine Learning, pages 14485\u201314508. PMLR, 2022.   \n[29] S. Maleki, L. Tran-Thanh, G. Hines, T. Rahwan, and A. Rogers. Bounding the estimation error of sampling-based shapley value approximation with/without stratifying. CoRR, abs/1306.4265, 2013.   \n[30] J. Pearl. Causality. Cambridge university press, 2009.   \n[31] M. T. Ribeiro, S. Singh, and C. Guestrin. \" why should i trust you?\" explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, pages 1135\u20131144, 2016.   \n[32] L. S. Shapley. A value for n-person games. Contributions to the Theory of Games, 2(28):307\u2013 317, 1953.   \n[33] A. Shrikumar, P. Greenside, and A. Kundaje. Learning important features through propagating activation differences. In International conference on machine learning, pages 3145\u20133153. PMLR, 2017.   \n[34] R. Singal, G. Michailidis, and H. Ng. Flow-based attribution in graphical models: A recursive shapley approach. In International Conference on Machine Learning, pages 9733\u20139743. PMLR, 2021.   \n[35] D. Smilkov, N. Thorat, B. Kim, F. Vi\u00e9gas, and M. Wattenberg. Smoothgrad: removing noise by adding noise. arXiv preprint arXiv:1706.03825, 2017.   \n[36] H. Stock and W. Watson. Instructional stata datasets for econometrics. Boston College Department of Economics, 2003.   \n[37] M. Sundararajan and A. Najmi. The many shapley values for model explanation. In International conference on machine learning, pages 9269\u20139278. PMLR, 2020.   \n[38] M. Sundararajan, A. Taly, and Q. Yan. Axiomatic attribution for deep networks. In International conference on machine learning, pages 3319\u20133328. PMLR, 2017.   \n[39] Y. Tauman. The aumann-shapley prices: a survey. The shapley value, page 279, 1988.   \n[40] J. Wang, J. Wiens, and S. Lundberg. Shapley flow: A graph-based approach to interpreting model predictions. In International Conference on Artificial Intelligence and Statistics, pages 721\u2013729. PMLR, 2021.   \n[41] E. Winter. The shapley value. Handbook of game theory with economic applications, 3:2025\u2013 2054, 2002.   \n[42] A. Wu, K. Kuang, B. Li, and F. Wu. Instrumental variable regression with confounder balancing. In International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, volume 162 of Proceedings of Machine Learning Research, pages 24056\u2013 24075. PMLR, 2022.   \n[43] Y. Xu, J. Zhu, C. Shi, S. Luo, and R. Song. An instrumental variable approach to confounded off-policy evaluation. In Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 38848\u201338880. PMLR, 23\u201329 Jul 2023.   \n[44] A. Zern, K. Broelemann, and G. Kasneci. Interventional shap values and interaction values for piecewise linear regression trees. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 11164\u201311173, 2023.   \n[45] J. Zhang, D. Kumor, and E. Bareinboim. Causal imitation learning with unobserved confounders. Advances in neural information processing systems, 33:12263\u201312274, 2020.   \n[46] J. Zhang, Q. Sun, J. Liu, L. Xiong, J. Pei, and K. Ren. Efficient sampling approaches to shapley value approximation. Proceedings of the ACM on Management of Data, 1(1):1\u201324, 2023.   \n[47] J. Zhang, H. Xia, Q. Sun, J. Liu, L. Xiong, J. Pei, and K. Ren. Dynamic shapley value computation. In 39th IEEE International Conference on Data Engineering, ICDE 2023, Anaheim, CA, USA, April 3-7, 2023, pages 639\u2013652. IEEE, 2023. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In the appendix of our paper, we provide comprehensive additional content. We discuss the lbroader impacts of the paper in Section A, respectively. Section B reviews the related works. In Section D, we delve into the computation of gradients when dealing with discrete features and discuss training methods for non-neural network models. Following this, Section E presents the algorithm for computing Shapley values optimized using confidence intervals, along with an error analysis for unbiased sampling in integrated gradients. Subsequently, Section F offers supplementary material related to our experimental procedures. This includes a detailed analysis of the characteristics of our experimental dataset, justifying our experimental design. Additional results from classification experiments and non-neural network models are also provided. ", "page_idx": 13}, {"type": "text", "text": "A Broader Impacts ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "While we believe our paper has many positive social impacts, we think it can particularly affect: ", "page_idx": 13}, {"type": "text", "text": "\u2022 Fairness and Equity in Automated Systems: Reduces biases caused by unobservable confounders in feature attribution, promoting fairness in systems like credit scoring and hiring. This helps to build trust in these systems and supports fair decision-making in various areas.   \n\u2022 Improved Decision-Making in Healthcare: Our method enables more accurate identification of factors affecting patient outcomes, leading to better diagnosis, treatment plans, and personalized medicine. Healthcare professionals can make better decisions, which improves patient care and outcomes. ", "page_idx": 13}, {"type": "text", "text": "We do not think our paper has any negative social impacts. ", "page_idx": 13}, {"type": "text", "text": "B Related Work ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this section, we first introduce seminal works that have a significant influence on the feature attribution domain. This is followed by an exploration of works integrating causal knowledge into feature attribution. Additionally, we introduce the widespread presence of confounders in machine learning. Finally, we discuss advancements in computational optimization for SHAP-based methods. For a more detailed survey of feature attribution, please see [12]. ", "page_idx": 13}, {"type": "text", "text": "B.1 Classic Feature Attribution Techniques ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "LIME (Local Interpretable Model-agnostic Explanations) facilitates the understanding of individual predictions of complex models by creating explanatory models [31]. It reveals the impact of features on predictions by perturbing the input and observing the resultant changes in output. DeepLIFT (Deep Learning Important FeaTures) offers a method for assessing feature importance in deep neural networks by comparing the activation of each feature against a reference activation, proving particularly effective in interpreting deep learning models [33]. SmoothGrad enhances visual interpretations of gradient-based methods by applying multiple small random perturbations to the input data and averaging the gradients of these perturbations [35]. Meanwhile, researchers have increasingly recognized that for interpretability methods to be effective and credible, they need to satisfy axiomatic properties. SHAP (SHapley Additive exPlanations) employs Shapley values from cooperative game theory to measure feature contributions, offering a model-agnostic approach with broad applicability [27]. It adheres to desirable allocation properties, ensuring both consistency and equity in attributing feature influence on predictions. Meanwhile, Integrated Gradients (IG) calculates feature importance through the integration of gradients along a straight path from a baseline to the input, making it ideal for scenarios with continuous features and differentiable models [38]. ", "page_idx": 13}, {"type": "text", "text": "B.2 Causal Feature Attribution Techniques ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In the evolving field of feature attribution, the significance of causal relationships for data-faithful interpretations is increasingly recognized [21]. Asymmetric Shapley values (ASVs) are developed to infuse causal understanding into model explanations [14]. They achieve this by modifying the symmetry axiom in the Shapley value framework, allowing for the inclusion of causal relationships. Notably, ASVs can provide insights even without a complete causal graph. Causal Shapley values stand out in their capacity to distinguish between direct and indirect feature impacts on model predictions, offering a profound understanding of data generation [19]. Shapley Flow distinguishes itself by evaluating the entire causal graph, attributing influence across its edges rather than focusing solely on nodes [40]. Recursive Shapley Value (RSV) presents a specialized approach for graphical models, quantifying the propagation of changes from source nodes throughout the graph [34]. However, despite the advancements made by these methods in considering the causal relationships between model input features, these methods overlook the impact of unobservable confounders on feature attribution. ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "B.3 Confounders in Machine Learning ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Researchers have recently begun exploring methods to identify and adjust for confounders in algorithmic models to enhance decision-making quality [22, 42]. Gao et al. [15] identify that pre-trained graph neural networks perform better on pruned graphs than on full graphs due to confounders and introduce Robust Causal Graph Representation Learning (RCGRL) to effectively address this issue by eliminating confounders. Zhang et al. [45] introduce a method for causal imitation learning in the presence of unobservable confounders, featuring a graphical criterion to evaluate its feasibility despite partially observed decision variables behind expert actions. Deep Sequential Weighting (DSW) is proposed for estimating individual treatment effects in healthcare, accounting for time-varying hidden confounders using deep learning [25]. In confounded sequential decision-making, Xu et al. [43] study introduces an instrumental variable (IV) method for off-policy evaluation (OPE) to estimate policy returns accurately in infinite horizon settings. ", "page_idx": 14}, {"type": "text", "text": "B.4 Approximation of SHAP-based Methods ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "TreeSHAP [26], for tree-based models, enhances SHAP value calculation efficiency by utilizing tree structures to skip redundant feature combination evaluations. Dynamic Shapley, as discussed in the paper by [47], focuses on dealing with scenarios where the players may change. Kernel SHAP [27], suitable for various models, approximates SHAP values by sampling in the feature space and assessing the impact of different feature combinations. Among the recent advancements in optimizing SHAP computation are TMC (Truncated Monte Carlo) [16] and FastSHAP [20], each offering unique approaches to enhance efficiency. TMC employs a truncation technique for rapid, biased sampling approximations. FastSHAP is biased too, employing a pre-trained auxiliary model, speeds up SHAP value prediction. Moreover, unlike methods that approximate Shapley values through sampling, its ability to accurately estimate Shapley values does not improve with more samples, as the precision of the auxiliary model is predetermined upon training. Recently, researchers have proposed one Shapley value approximation method based on the complementary contribution which can be adapted to the general class of feature attribution scenarios [46]. ", "page_idx": 14}, {"type": "text", "text": "C Proofs ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "C.1 Proof of Proposition 1 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Proof. From the perspective of the data generation process for $y$ , the marginal contribution of feature $i$ is exclusively linked to the function $g$ . Thus, the marginal contribution of feature $i$ in condition expectation Shapley for the target feature generation is $\\overline{{\\mathcal{U}}}^{\\overline{{\\mathcal{C}}}}(\\mathcal{S}\\cup\\{i\\})\\,-\\overline{{\\mathcal{U}}}^{\\mathcal{C}}(\\mathcal{S})=\\mathbb{E}[g(\\tilde{\\mathbf{x}},\\overline{{\\mathbf{x}}})|x_{S\\cup\\{i\\}}=$ $x_{S\\cup\\{i\\}}]\\,-\\,\\mathbb{E}\\big[g(\\tilde{{\\pmb x}},\\overline{{{\\pmb x}}})\\big|{\\pmb x}_{S}\\;=\\;{\\pmb x}_{S}\\big]$ . For the model trained to fit $\\mathbb{E}[y|\\tilde{{\\boldsymbol x}},\\overline{{{\\boldsymbol x}}}]$ , we have $\\mathbb{E}[y|\\pmb{\\tilde{x}},\\pmb{\\overline{{x}}}]\\;=$ $\\mathbb{E}[g(\\pmb{\\tilde{x}},\\pmb{\\overline{{x}}})|\\pmb{\\tilde{x}},\\pmb{\\overline{{x}}}]+\\mathbb{E}[\\epsilon|\\pmb{\\tilde{x}},\\pmb{\\overline{{x}}}]=g(\\pmb{\\tilde{x}},\\pmb{\\overline{{x}}})+\\mathbb{E}[\\epsilon|\\pmb{\\tilde{x}}]$ . Therefore, given explained input $\\textbf{\\em x}$ , the marginal contribution of a particular feature $i$ with $\\boldsymbol{S}$ in condition expectation Shapley derived with model $f$ can be represented as follows $\\mathcal{U}^{\\mathcal{C}}(\\mathcal{S}\\cup\\{i\\})-\\mathcal{U}^{\\mathcal{C}}(\\mathcal{S})=\\mathbb{E}[f(\\pmb{x})|\\pmb{x}_{S\\cup\\{i\\}}=\\pmb{x}_{\\mathcal{S}\\cup\\{i\\}}^{\\star}]-\\mathbb{E}[f(\\pmb{x})|\\pmb{x}_{S}=\\pmb{\\dot{x}}_{S}]=$ $\\mathbb{E}[g(\\tilde{x},\\overline{{x}})|x_{S\\cup\\{i\\}}=x_{S\\cup\\{i\\}}]+\\mathbb{E}[e|x_{S\\cup\\{i\\}}=x_{S\\cup\\{i\\}}]-\\mathbb{E}[g(\\tilde{x},\\overline{{x}})|x_{S}=x_{S}]-\\mathbb{E}[e|x_{S}=x_{S\\cup\\{i\\}}].$ . Thus, the marginal contribution calculated by the model $f$ which is trained to fit $\\mathbb{E}[y|\\tilde{{\\boldsymbol x}},\\overline{{{\\boldsymbol x}}}]$ includes an error term $\\mathbb{E}_{D}[\\epsilon|\\pmb{x}_{S\\cup\\{i\\}}=\\pmb{x}_{S\\cup\\{i\\}}]-\\mathbb{E}[\\epsilon|\\pmb{x}_{S\\cup\\{i\\}}=\\pmb{x}_{S\\cup\\{i\\}}]$ , arises from the model\u2019s reliance on the correlations within features $\\tilde{X}$ and $\\mathcal{E}$ . By averaging the errors in the marginal contributions of feature $i$ with all possible cooperate coalitions, we can get the expected deviation of attribution value by $\\begin{array}{r}{\\Delta S\\mathcal{V}_{i}=\\frac{1}{N}\\sum_{S\\subseteq{\\mathcal{N}}\\setminus\\{i\\}}\\big(\\overset{|\\mathcal{N}|-1}{|S|}\\big)^{-1}\\big\\{\\mathbb{E}_{D}[\\epsilon|x_{S\\cup\\{i\\}}=x_{S\\cup\\{i\\}}^{*}]-\\mathbb{E}[\\epsilon|x_{S}=x_{S}^{*}]\\big\\}.}\\end{array}$ \u53e3 ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "C.2 Proof of Proposition 2 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Proof. From the data generation perspective, the marginal contribution of feature $i$ in intervention Shapley for the target feature generation should be $\\begin{array}{r}{\\overline{{\\mathcal{U}}}^{\\mathbb{Z}}(\\mathcal{S}\\cup\\{i\\})-\\overline{{\\mathcal{U}}}^{\\mathbb{Z}}(\\mathcal{S})=\\mathbb{E}[g(\\tilde{\\mathbf{x}},\\overline{{\\mathbf{x}}})|d o(\\mathbf{x}_{S\\cup\\{i\\}}=}\\end{array}$ $x_{S\\cup\\{i\\}})]-\\mathbb{E}[g({\\pmb x},\\pmb x)|d o({\\pmb x}_{S}={\\pmb x}_{S})]$ . However, for the intervention Shapley, the marginal contribution derived with a model trained to fit $\\mathbb{E}[y|\\tilde{{\\boldsymbol{x}}},\\overline{{{\\boldsymbol{x}}}}]$ is $\\mathcal{U}^{\\mathbb{Z}}(\\mathcal{S}\\cup\\{i\\})-\\mathcal{U}^{\\mathbb{Z}}(\\mathcal{S})=\\mathbb{E}[f(\\pmb{x})|d o(\\pmb{x}_{S\\cup\\{i\\}}=$ $z_{S\\cup\\{i\\}})]\\,-\\,\\mathbb{E}[f(x)|d o(x_{S}\\;=\\;x_{S})]\\;=\\;\\mathbb{E}[g(\\tilde{x},\\overline{{x}})|d o(x_{S\\cup\\{i\\}}\\;=\\;x_{S\\cup\\{i\\}})]\\,+\\,\\mathbb{E}[\\epsilon|d o(x_{S\\cup\\{i\\}}\\;=\\;\\tilde{x},\\{x_{S}\\})]$ $\\begin{array}{r}{\\pmb{x}_{S\\cup\\{i\\}})\\big]-\\mathbb{E}[g(\\pmb{\\tilde{x}},\\pmb{\\overline{{x}}})|d o(\\pmb{x}_{S}\\,=\\,\\pmb{x}_{S})]-\\mathbb{E}_{D}[\\epsilon|d o(\\pmb{x}_{S}\\,=\\,\\pmb{x}_{S})]}\\end{array}$ ]. Thus, we have the expected error for marginal contribution of feature $i$ in intervention Shapley with mode $f$ trained to fit $\\mathbb{E}[y|\\pmb{x}]$ is $\\mathbb{E}_{D}[\\epsilon|d o(\\pmb{x}_{S\\cup\\{i\\}}=\\pmb{x}_{S\\cup\\{i\\}}^{*})]-\\mathbb{E}_{D}[\\epsilon|d o(\\pmb{x}_{S}=\\pmb{x}_{S}^{*})]$ . By averaging the errors in all the marginal contributions of feature $i$ with all possible cooperative coalitions, we can get the expected deviation of attribution value by $\\begin{array}{r}{\\Delta S\\mathcal{V}_{i}=\\frac{1}{N}\\overset{\\cdot}{\\sum}_{S\\subseteq\\mathcal{N}\\setminus\\{i\\}}\\overset{\\cdot}{\\binom{|\\mathcal{N}|-1}{|S|}}^{-1}\\{\\mathbb{E}_{D}[\\epsilon|d o(\\pmb{x}_{S\\cup\\{i\\}}=\\pmb{x}_{S\\cup\\{i\\}}^{*})]-\\mathbb{E}_{D}[\\epsilon|d o(\\pmb{x}_{S}=\\pmb{x}_{S\\cup\\{i\\}}^{*})]\\}}\\end{array}$ $x_{S}^{*})]\\}$ . \u53e3 ", "page_idx": 15}, {"type": "text", "text": "C.3 Proof of Proposition 3 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "tPor oinofc.o rFpoorr IatGe,  twhhe eerfe $\\epsilon$ cits  tohf uonno ,e ravsa bitl ei sc otrnafionuendd teor  cfoitr i.t h $\\textbf{\\em x}$ , ntsheeq dueernitvlayt,i vteh $\\textstyle{\\frac{\\partial f}{\\partial\\mathbf{x}}}$ o ills oliwkienlgy $\\epsilon$ $\\textbf{\\em x}$ $g(\\pmb{x})+\\epsilon$   \ninequality typically holds $\\begin{array}{r}{\\frac{\\partial f}{\\partial\\mathbf{x}}~\\neq~\\frac{\\partial g}{\\partial\\mathbf{x}}}\\end{array}$ . Therefore, the attribution $\\mathcal{T G}_{i}(\\pmb{x},\\pmb{x}^{\\prime},f)$ derived from the predictive model $f$ generally differs from the attribution $\\mathcal{T G}_{i}(\\pmb{x},\\pmb{x}^{\\prime},g)$ that should be obtained based on the actual data generation process. When we accumulate the difference of $\\textstyle{\\frac{\\partial f}{\\partial\\mathbf{x}}}$ and $\\textstyle{\\frac{\\partial g}{\\partial\\mathbf{x}}}$ in the path, we can get the error for attribution value of feature $i$ using IG with model $f$ trained to fit $\\mathbb{E}[y|x]$ is $\\begin{array}{r}{\\Delta\\mathcal{I G}_{i}=(x_{i}^{*}-x_{i}^{\\prime})\\int_{\\alpha=0}^{1}\\frac{\\partial f\\left(\\pmb{x}^{\\prime}+\\alpha\\left(\\pmb{x}^{*}-\\pmb{x}^{\\prime}\\right)\\right)}{\\partial\\pmb{x}_{i}}-\\frac{\\partial g\\left(\\pmb{x}^{\\prime}+\\alpha\\left(\\pmb{x}^{*}-\\pmb{x}^{\\prime}\\right)\\right)}{\\partial\\pmb{x}_{i}}d\\alpha.}\\end{array}$ ", "page_idx": 15}, {"type": "text", "text": "C.4 Proof of Proposition 4 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Proof. The marginal contribution of a particular feature $i$ with $\\boldsymbol{S}$ in condition expectation Shapley derived with model $f\\,=\\,g({\\tilde{\\pmb x}},{\\overline{{\\pmb x}}})+\\mathbb{E}[\\epsilon]$ can be represented as follows $\\mathcal{U}^{c}(\\bar{S^{\\mathrm{~}}}\\cup\\{i\\})-\\mathcal{U}^{\\hat{c}}(\\bar{S^{\\mathrm{~}}}=$ $\\mathbb{E}[f(\\pmb{x})|{\\pmb x}_{S\\cup\\{i\\}}]\\ =\\ \\pmb{x}_{S\\cup\\{i\\}}]\\ -\\ \\mathbb{E}[f(\\pmb{x})|{\\pmb x}_{S}\\ =\\ \\pmb{x}_{S}]\\ =\\ \\mathbb{E}[g(\\pmb{x},\\pmb{\\overline{{x}}})|{\\pmb x}_{S\\cup\\{i\\}}\\ =\\ \\pmb{x}_{S\\cup\\{i\\}}]\\ +\\ \\pmb{x}_{S\\cup\\{i\\}}]\\ .$ E[e] \u2212 $\\mathbb{E}[g(\\tilde{x},\\overline{{x}})\\vert x_{S}=x_{S}]-\\mathbb{E}[e]\\,=\\,\\mathbb{E}[g(\\tilde{x},\\overline{{x}})\\vert x_{S\\cup\\{i\\}}\\,=\\,x_{S\\cup\\{i\\}}]-\\mathbb{E}[g(\\tilde{x},\\overline{{x}})\\vert x_{S}\\,=\\,x_{S}].$ Thus, the attribution are identical for models $f=g(\\tilde{\\pmb{x}},\\overline{{\\pmb{x}}})$ and $f=g({\\tilde{\\pmb x}},{\\overline{{\\pmb x}}})+\\mathbb{E}[\\epsilon]$ in condition expectation Shapley. ", "page_idx": 15}, {"type": "text", "text": "The marginal contribution derived with a model trained to fit $f=g(\\tilde{\\pmb{x}},\\overline{{\\pmb{x}}})+\\mathbb{E}[\\epsilon]$ is $\\mathcal{U}^{\\mathbb{Z}}(S\\cup\\{i\\})-$ $\\begin{array}{r}{i^{Z}({\\cal S})\\,=\\,\\mathbb{E}[f({\\pmb x})|d o({\\pmb x}_{\\cal S\\cup\\{i\\}}\\,=\\,{\\pmb x}_{\\cal S\\cup\\{i\\}})]-\\mathbb{E}[f({\\pmb x})|d o({\\pmb x}_{{\\cal S}}\\,=\\,{\\pmb x}_{{\\cal S}})]\\,=\\,\\mathbb{E}[g(\\tilde{{\\pmb x}},\\overline{{{\\pmb x}}})|d o({\\pmb x}_{\\cal S\\cup\\{i\\}}\\,=\\,{\\pmb x}_{{\\cal S}})].}\\end{array}$ $\\begin{array}{r}{?s_{\\cup\\{i\\}})]\\ +\\mathbb{E}[\\epsilon]\\ -\\ \\mathbb{E}[g(\\tilde{\\ x},\\ \\overline{{\\ x}})|d o({x_{\\mathcal{S}}\\ \\stackrel{}{=}\\ \\ x_{\\mathcal{S}}})]\\ -\\ \\mathbb{E}[\\epsilon]\\ =\\ \\mathbb{E}[g(\\tilde{x},\\overline{{x}})|d o({x_{\\mathcal{S}\\cup\\{i\\}}\\ }\\ =\\ {x_{\\mathcal{S}\\cup\\{i\\}}})]\\ -\\ \\mathcal{O}(\\epsilon^{2})\\ =\\ \\mathbb{E}[g(\\tilde{x},\\ \\overline{{x}})|d o({x_{\\mathcal{S}\\cup\\{i\\}}\\ }\\ =\\ {x_{\\mathcal{S}\\cup\\{i\\}}})]\\ -\\ \\mathcal{O}(\\epsilon^{3})\\ =\\ \\delta,}\\end{array}$ $\\mathbb{E}[g({\\tilde{\\pmb x}},{\\overline{{\\pmb x}}})|d o({\\pmb x}_{S}~=~{\\pmb x}_{S})]$ . Thus, the attribution are identical for models $f\\ \\ =\\ \\ g(\\tilde{\\pmb{x}},\\overline{{\\pmb{x}}})$ and $f=g(\\tilde{\\pmb{x}},\\overline{{\\pmb{x}}})+\\mathbb{E}[\\epsilon]$ in intervention Shapley. ", "page_idx": 15}, {"type": "text", "text": "The derivativ e \u2202\u2202fx = \u2202\u2202gx holds when model f = g(x)+E[\u03f5] as E[\u03f5] is a constant. Thus, the attribution are identical for models $f=g(\\tilde{\\boldsymbol{x}},\\overline{{\\boldsymbol{x}}})$ and $f=g(\\tilde{\\pmb{x}},\\overline{{\\pmb{x}}})+\\mathbb{E}[\\epsilon]$ in Integrated Gradients(IG). \u53e3 ", "page_idx": 15}, {"type": "text", "text": "D Discrete Confounded features and Gradient-Free Model Training ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "D.1 Training with Discrete Features ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We extend our discussion to scenarios where the targets predicted by $\\hat{M}_{\\phi}\\left(\\tilde{\\pmb{x}}\\mid\\overline{{\\pmb{x}}}_{t},\\psi_{t}\\right)$ are discrete. In cases where the prediction features $\\mathbf{P}$ of $\\hat{M}_{\\phi}$ are discrete, the fundamental approach to optimizing the loss function $L(T;\\theta)$ remains similar. The primary modification involves substituting the integral over the probability distribution of $\\mathbf{P}$ with a summation across discrete points. Assuming $\\mathbf{P}$ has $\\mathbf{K}$ categories, and denoting the probability of the $t^{t h}$ data point being classified into the $k^{t h}$ category by $\\hat{M}_{\\phi}\\left(\\tilde{\\pmb{x}}^{k}\\mid\\overline{{\\pmb{x}}}_{t},z_{t}\\right)$ , the loss function when y is continuous is reformulated as: ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathcal{L}(T;\\theta)=|T|^{-1}\\sum_{t}\\left(y_{t}-\\sum_{k=1}^{K}\\hat{M}_{\\phi}\\left(\\tilde{\\pmb x}^{k}\\mid\\overline{{\\boldsymbol x}}_{t},\\psi_{t}\\right)\\,f_{\\theta}\\left(\\tilde{\\pmb x}^{k},\\overline{{\\boldsymbol x}}_{t}\\right)\\right)^{2}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "For the $t^{t h}$ training data point, the gradient of this loss function is: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\nabla_{\\theta}{\\mathcal L}_{t}=-\\,2\\left[y_{t}-\\sum_{k=1}^{K}{\\hat{M}}_{\\phi}\\left({\\tilde{\\pmb x}}^{k}\\mid{\\overline{{\\pmb x}}}_{t},\\psi_{t}\\right)\\,f_{\\theta}\\left({\\tilde{\\pmb x}}^{k},{\\overline{{\\pmb x}}}_{t}\\right)\\right]\\cdot\\left[\\sum_{k=1}^{K}{\\hat{M}}_{\\phi}\\left({\\tilde{\\pmb x}}^{k}\\mid{\\overline{{\\pmb x}}}_{t},\\psi_{t}\\right)\\,f_{\\theta}^{\\prime}\\left({\\tilde{\\pmb x}}^{k}{\\overline{{\\pmb x}}}_{t}\\right)\\right].\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Furthermore, the gradients of a mini-batch comprising m training data tuples are computed as: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\nabla_{\\theta}^{m}\\mathcal{L}_{t}\\equiv m^{-1}\\sum_{t}-2\\left[\\left(y_{t}-\\sum_{k=1}^{K}\\hat{M}_{\\phi}\\left(\\hat{x}^{k}\\mid\\overline{{x}}_{t},\\psi_{t}\\right)\\,f_{\\theta}\\left(\\hat{x}^{k},\\overline{{x}}_{t}\\right)\\right)\\right]\\cdot\\left[\\sum_{k=1}^{K}\\hat{M}_{\\phi}\\left(\\hat{x}^{k}\\mid\\overline{{x}}_{t},\\psi_{t}\\right)\\,f_{\\theta}^{\\prime}\\left(\\hat{x}^{k},\\overline{{x}}_{t}\\right)\\right].\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "In situations where $\\mathbf{y}$ is a discrete variable, representing categories or classes, the multi-class crossentropy can be formulated as: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathcal{L}(T;\\theta)=|T|^{-1}\\sum_{t}\\sum_{r}^{R}\\sum_{k=1}^{K}\\hat{M}_{\\phi}\\left(\\tilde{x}^{k}\\mid\\overline{{x}}_{t},\\psi_{t}\\right)y_{t,r}\\cdot\\ln f_{\\theta,r}\\left(\\tilde{x}^{k},\\overline{{x}}_{t}\\right)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "In this formulation, $y_{t,r}$ represents the true label of the $t^{t h}$ data point in the $r^{\\mathrm{th}}$ category. $R$ denotes the total number of distinct categories or classes into which the target variable $y$ can be classified. The model\u2019s prediction for this category is given by $f_{\\theta,r}\\left(\\widetilde{\\pmb{x}},\\overline{{\\pmb{x}}}_{t}\\right)$ . The gradient calculation for the $t^{t h}$ training data point, considering this loss function, is then: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\nabla_{\\theta}\\mathcal{L}_{t}=\\sum_{r=1}^{R}\\sum_{k=1}^{K}\\hat{M}_{\\phi}\\left(\\tilde{\\pmb{x}}^{k}\\mid\\overline{{\\mathbf{x}}}_{t},\\psi_{t}\\right)\\frac{y_{t,r}}{f_{\\theta,r}\\left(\\tilde{\\pmb{x}}^{k},\\overline{{\\mathbf{x}}}_{t}\\right)}\\cdot f_{\\theta,r}^{\\prime}\\left(\\tilde{\\pmb{x}}^{k},\\overline{{\\mathbf{x}}}_{t}\\right).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Furthermore, the gradients of a mini-batch comprising m training data tuples are computed as: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\nabla_{\\theta}^{m}\\mathcal{L}_{t}\\equiv m^{-1}\\sum_{t}\\sum_{r=1}^{R}\\sum_{k=1}^{K}\\hat{M}_{\\phi}\\left(\\tilde{\\pmb x}^{k}\\mid\\overline{{\\mathbf x}}_{t},\\psi_{t}\\right)\\frac{y_{t,r}}{f_{\\theta,r}\\left(\\tilde{\\pmb x}^{k},\\overline{{\\mathbf x}}_{t}\\right)}\\cdot f_{\\theta,r}^{\\prime}\\left(\\tilde{\\pmb x}^{k},\\overline{{\\mathbf x}}_{t}\\right).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "This adaptation of the loss function for discrete target variables ensures that our model can handle classification tasks, effectively optimizing its performance across multiple categories. ", "page_idx": 16}, {"type": "text", "text": "D.2 Gradient-Free Model Training ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "When training models to fit $\\hat{y}$ in scenarios where gradient-based optimization is not feasible, we introduce an alternative approach that effectively addresses the influence of confounding factors. The essence of this approach lies in the generation of synthetic data, which is derived from the predicted distribution of p. By sampling each original data point B times, we create B synthetic data points for every original point. This process results in a synthetic dataset that embodies the controlled effects of the confounders. The creation of this dataset is a vital step towards ensuring that the subsequent model training is less influenced by confounding variables. ", "page_idx": 16}, {"type": "text", "text": "A key advantage of this method is its independence from any specific model type. The generated synthetic dataset can be utilized to train a variety of machine learning models, not limited to those that rely on gradient-based optimization. This model-agnostic nature significantly widens the applicability of our approach, making it suitable for various scenarios and models. Through this method, we ensure that the training of models occurs in an environment where the impact of confounders is mitigated, thereby enhancing the reliability of the feature attribution. ", "page_idx": 16}, {"type": "text", "text": "E Supplement to SHAP and IG approximation ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "SHAP-based methods and IG-based methods can be applied to the proposed confounder-free models. However, the computational complexity poses a significant barrier to real-world applications. The exact computation of the Shapley value is proved to be an #P-hard problem [11], and the exact computation of integrated gradients requires the antiderivative of the gradient, which is infeasible due to their complexity, necessitating the use of approximation methods. The approximation cost of the Shapley value is higher than the straightforward sampling in the path of integrated gradients due to extensive feature subset evaluations. To further enhance the applicability, we develop optimizations for the approximation of SHAP-based methods. Research has shown that Shapley values can be represented not only based on marginal contributions but also complementary contributions, which allows for reusing samples in estimations, offering an advantage [46]. We propose an enhanced approach for SHAP-based methods, optimizing complementary contribution-based sampling using confidence intervals. This optimization is designed to minimize estimation errors in all utility functions within SHAP-based methods. ", "page_idx": 17}, {"type": "text", "text": "E.1 Estimation Techniques for SHAP ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Recent work [46] suggests that the Shapley value formula can be equivalently transformed into a form expressed based on complementary contributions. Here, the complementary contribution refers to the difference in utility between complementary subsets. The Shapley expression is given by ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{S}\\mathcal{V}_{i}=\\displaystyle\\frac{1}{|\\mathcal{N}|}\\sum_{\\boldsymbol{s}\\subseteq\\mathcal{N}\\setminus\\{i\\}}\\frac{\\mathcal{U}(\\boldsymbol{S}\\cup\\{i\\})-\\mathcal{U}(\\boldsymbol{S})}{\\binom{|\\mathcal{N}|-1}{|\\mathcal{S}|}}}\\\\ &{\\quad=\\displaystyle\\frac{1}{n}\\sum_{\\boldsymbol{s}\\subseteq\\mathcal{N}\\setminus\\{\\boldsymbol{z}_{i}\\}}\\frac{\\mathcal{U}(\\boldsymbol{S}\\cup\\{\\boldsymbol{z}_{i}\\})-\\mathcal{U}(\\mathcal{N}\\setminus(\\mathcal{S}\\cup\\{\\boldsymbol{z}_{i}\\}))}{\\binom{n-1}{|\\mathcal{S}|}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The formulas based on complementary contributions offer advantages in terms of sample reusability during approximate computation. Building on this foundation, we propose a dynamic sampling adjustment based on the confidence intervals of Shapley value estimates in stratified sampling. ", "page_idx": 17}, {"type": "text", "text": "Denote by ${\\mathfrak{S}}_{N}^{i,j}=\\{S\\cup\\{z_{i}\\}|S\\subseteq N\\setminus\\{z_{i}\\},|S|=j-1\\}$ $(1\\leq j\\leq n)$ the set of $(z_{i},j)$ -coalitions, and by $\\boldsymbol{S}\\boldsymbol{\\nu}_{i,j}$ the expected complementary contributions of $(z_{i},j)$ -coalitions. That is, ", "page_idx": 17}, {"type": "equation", "text": "$$\nS\\mathcal{V}_{i,j}=\\sum_{S\\in\\mathfrak{S}_{\\mathcal{N}}^{i,j}}\\frac{\\mathcal{U}(S)-\\mathcal{U}(N\\setminus S)}{{\\binom{n-1}{j-1}}}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Complementary contributions $C C(S)=\\mathcal{U}(S\\cup\\{z_{i}\\})-\\mathcal{U}(\\mathcal{N}\\setminus(S\\cup\\{z_{i}\\}))$ are naturally stratified into $n$ strata $\\mathfrak{S}_{\\mathcal{N}}^{i,1},\\dots,\\mathfrak{S}_{\\mathcal{N}}^{i,n}$ according to the coalition size. We start by deriving the confidence interval of the estimator of $\\boldsymbol{S}\\boldsymbol{\\nu}_{i,j}$ using $t$ -test. ", "page_idx": 17}, {"type": "text", "text": "Lemma 5. According to the Central Limit Theorem, the sample mean approximates a normal distribution when the sample size is sufficiently large. Assuming a confidence level of $\\alpha$ , the confidence interval for $\\overline{{S\\mathcal{V}_{i,j}}}$ based on the $t$ -test is $\\begin{array}{r}{\\overline{{S\\mathcal{V}_{i,j}}}\\pm\\bar{A}_{\\alpha}\\frac{S_{i,j}}{\\sqrt{m_{i,j}}}}\\end{array}$ , which can also be represented as ", "page_idx": 17}, {"type": "equation", "text": "$$\nP(\\overline{{S\\mathcal{V}_{i,j}}}-A_{\\alpha}\\frac{S_{i,j}}{\\sqrt{m_{i,j}}}<S\\mathcal{V}_{i,j}<\\overline{{S\\mathcal{V}_{i,j}}}+A_{\\alpha}\\frac{S_{i,j}}{\\sqrt{m_{i,j}}})=\\alpha,\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $A_{\\alpha}$ is the t-score corresponding to $\\alpha$ , $m_{i,j}$ is the sample size of $\\mathfrak{S}_{\\mathcal{N}}^{i,j}$ and $S_{i,j}$ is the sampling variance of $\\overline{{S\\mathcal{V}_{i,j}}}$ . ", "page_idx": 17}, {"type": "text", "text": "Denote by $\\mathfrak{S}_{\\mathcal{N}}^{j}=\\{S|S\\subseteq\\mathcal{N},|S|=j\\}$ the set of $j$ -coalitions $(1\\leq j\\leq n)$ . After drawing a coalition $\\boldsymbol{S}$ from ${\\mathfrak{S}}_{{N}}^{j}$ , we can estimate the complementary contribution $C C_{\\mathcal{N}}(S)$ , which can be used in $\\boldsymbol{S}\\boldsymbol{\\mathcal{V}}_{i,j}$ for $z_{i}$ in $\\boldsymbol{S}$ and $s\\nu_{i,n-j}$ for $z_{i}$ in ${\\mathcal{N}}\\setminus{\\mathcal{S}}$ . In light of the fact that each sample can influence multiple strata, how should we allocate the number of samples to optimize the precision of the estimated values? We denote $I_{\\mathcal{N}}^{j}$ as the sum of the confidence intervals for strata which can be influenced by a random sample of ${\\mathfrak{S}}_{{\\mathcal{N}}}^{j}$ , that is ", "page_idx": 17}, {"type": "equation", "text": "$$\nI_{\\mathcal{N}}^{j}=2*(\\sum_{i=1}^{N}A_{\\alpha}\\frac{S_{i,j}}{\\sqrt{m_{i,j}}}+\\sum_{i=1}^{N}A_{\\alpha}\\frac{S_{i,n-j}}{\\sqrt{m_{i,n-j}}}).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "For each sampling iteration, we select the stratum that maximizes the sum of the corresponding confidence intervals. ", "page_idx": 18}, {"type": "text", "text": "Next, we will outline the algorithmic procedure, which is divided into two primary stages. Due to page limitations, the pseudo-code of the algorithm is presented in Algorithm 1 in the appendix. In the first stage, we sample at least $m_{\\mathrm{init}}$ samples for $\\boldsymbol{S}\\boldsymbol{\\nu}_{i,j}$ . We then compute unbiased estimations of $\\sigma_{i,j}^{2}$ using Bessel\u2019s correction based on samples collected in the first stage. In the second stage, the stratum for each individual sampling is determined based on the sum of confidence intervals $I_{\\mathcal{N}}^{j}(1\\,\\le\\,j\\,\\le\\,n/2)$ . Furthermore, this sum is updated upon the completion of each sampling. Specifically, let $C C_{N}(S_{1}\\cup\\{z_{i}\\}),\\dots,C C_{N}(S_{m_{i,j}}\\cup\\{z_{i}\\})$ be $m_{i,j}$ samples for computing $\\overline{{S\\mathcal{V}_{i,j}}}$ , $\\begin{array}{r}{\\overline{{\\sigma_{i,j}^{2}}}\\,=\\,\\frac{1}{m_{i,j}-1}\\sum_{k=1}^{m_{i,j}}(C C_{N}(S_{k}\\cup\\{z_{i}\\})-\\frac{1}{m_{i,j}}\\sum_{k=1}^{m_{i,j}}C C_{N}(S_{k}\\cup\\{z_{i}\\}))}\\end{array}$ . Let $m_{\\mathrm{{flrst}}}$ be the cWale crualnatdeo $I_{\\mathcal{N}}^{j}(1\\leq j\\leq n/2)$ ea csctroartduimn g wtiot hE tqhuea ltairogne (st1 7s)u ums ionf gc tohnef iduennbciae siendt esravamlps. $m-m_{\\mathrm{{first}}}$ $\\widehat{\\sigma_{i,j}^{2}}$ sampling, the sum of confidence intervals will be updated. We will continue to repeat this process until all samples have been utilized. The final estimation of Shapley value is the average of all complementary contribution means in each stratum. ", "page_idx": 18}, {"type": "text", "text": "SHAP experiences an exponential increase in computational complexity with the addition of more features. However, for IG, the increase in features does not significantly escalate the complexity of the integral path. Therefore, in terms of efficiency in approximate computations, IG generally outperforms SHAP. This makes IG a preferable choice in situations where computational complexity for interpretability is a critical concern. However, it is important to note that SHAP, as a modelagnostic method, is applicable for interpreting models that are not based on gradient optimization. ", "page_idx": 18}, {"type": "text", "text": "Algorithm of SHAP Computation Based on the Confidence Interval. For the algorithm process we propose, which utilizes confidence intervals to optimize the calculation of Shapley values, refer to Algorithm 1. It is important to note that our algorithm is utility function-agnostic [29], meaning it can be applied across various SHAP-based algorithm variants. This is achieved by simply substituting the utility function defined by each method into our calculation. Furthermore, we provide an unbiased proof of our method in Theorem 6. ", "page_idx": 18}, {"type": "text", "text": "Theorem 6. Given a set of players $\\mathcal{N}=\\{z_{1},\\ldots,z_{n}\\}$ , Algorithm $^{\\,I}$ gives an unbiased estimation of Shapley value for every player, that is, $E[\\overline{{S\\mathcal{V}_{i}}}]=S\\mathcal{V}_{i}$ $(1\\leq i\\leq n)$ ). ", "page_idx": 18}, {"type": "text", "text": "Proof. Denote by $C C_{\\mathcal{N}}(S_{1}),\\ldots,C C_{\\mathcal{N}}(S_{m_{i,j}})$ a sample of $\\mathfrak{S}_{\\mathcal{N}}^{i,j}$ $(1\\,\\leq\\,i,j\\,\\leq\\,n)$ drawn by Algorithm 1. The expectation of the sample SVi,j = m1i,j $\\begin{array}{r}{\\overline{{S\\mathcal{V}_{i,j}}}\\;=\\;\\frac{1}{m_{i,j}}\\sum_{k=1}^{m_{i,j}}C C_{\\mathcal{N}}(S_{k})}\\end{array}$ km=i,1j CCN (Sk). We can compute the expectation of $\\overline{{S\\mathcal{V}_{i,j}}}$ with ", "page_idx": 18}, {"type": "equation", "text": "$$\nE[\\overline{{{S}}}\\overline{{{V_{i,j}}}}]=E[\\frac{1}{m_{i,j}}\\sum_{k=1}^{m_{i,j}}C C_{N}(S_{k})]=\\frac{1}{m_{i,j}}\\sum_{k=1}^{m_{i,j}}E[C C_{N}(S_{k})]\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "According to Equation 15, $E[C C_{\\mathcal{N}}(S_{k})]=S\\mathcal{V}_{i,j}$ . Thus, $E[\\overline{{S\\mathcal{V}_{i,j}}}]=S\\mathcal{V}_{i,j}$ that means $\\overline{{S\\mathcal{V}_{i,j}}}$ is an unbiased estimation of SVi,j. ", "page_idx": 18}, {"type": "text", "text": "Then, we can compute the expectation of $\\overline{{s\\nu_{i}}}$ produced by Algorithm 1. We have ", "page_idx": 18}, {"type": "equation", "text": "$$\nE[\\overline{{S}}\\overline{{V_{i}}}]=E[\\frac{1}{n}\\sum_{j=1}^{n}\\overline{{S}}\\overline{{V_{i,j}}}]=\\frac{1}{n}\\sum_{j=1}^{n}E[\\overline{{S}}\\overline{{V_{i,j}}}]=\\frac{1}{n}\\sum_{j=1}^{n}S\\}_{i,j}=S\\}_{i}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "That is, $\\overline{{s\\nu_{i}}}$ is an unbiased estimation of $\\mathcal{S}\\mathcal{V}_{i}$ . ", "page_idx": 18}, {"type": "text", "text": "E.2 Unbiased Integrated Gradients Approximation ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In existing literature related to Integrated Gradients (IG), interpolation methods are commonly used for approximation [13], which already exhibit high efficiency compared to SHAP-like approximation methods. In contrast, our paper introduces the use of Monte Carlo methods for the integration of gradients. It\u2019s important to clarify that the aim of proposing an unbiased estimate for IG is not ", "page_idx": 18}, {"type": "text", "text": "Input: players $\\mathcal{N}=\\{z_{1},\\ldots,z_{n}\\}$ , $m_{i n i t}>1$ , and $m>0$   \nOutput: approximate Shapley value $\\overline{{s\\nu}}_{i}$ for each player $z_{i}$ $(1\\leq i\\leq n)$ )   \n$\\overline{{S\\mathcal{V}_{i}}},\\overline{{S\\mathcal{V}_{i,j}}},m_{i,j}\\gets0\\;(1\\leq i\\leq n);$ ;   \n$c\\gets-1$ ;   \nwhile $\\textstyle c\\neq\\sum_{j=1}^{n}m_{1,j}\\,\\mathbf{d}$ o $\\begin{array}{r}{c=\\sum_{j=1}^{n}\\bar{m}_{1,j}}\\end{array}$ ; for $\\mathrm{i}{=}1$ to n, $\\mathrm{j}=1$ to n do if $m_{i,j}<m_{i n i t}$ then let $\\boldsymbol{S}$ be a sample drawn from ${\\mathfrak{S}}_{{\\mathcal{N}}}^{j}$ ; $u\\gets\\mathcal{U}(\\mathcal{S})-\\mathcal{U}(\\mathcal{N}\\setminus\\mathcal{S})$ ; for $z_{i}\\in S$ do $\\overline{{S\\mathcal{V}_{i,|S|}}}+=u;m_{i,|S|}+=1;$ end for for $z_{i}\\in\\mathcal{N}\\backslash\\mathcal{S}$ do $\\overline{{S\\mathcal{V}_{i,|\\mathcal{N}\\backslash S|}}}-=u$ ; $m_{i,|\\mathcal{N}\\backslash S|}+=1$ ; end for end if end for   \nend while   \ncompute ${\\widehat{S_{i,j}^{2}}}\\ (1\\leq i,j\\leq n);$   \nmfirst \u2190 jn=1 m1,j;   \nfor $\\mathrm{k}=0$ to $m-m_{f i r s t}$ do for $\\scriptstyle{\\mathrm{j}}=1$ to n do $\\begin{array}{r}{\\bar{I_{N}^{j}}=2*(\\sum_{i=1}^{N}A_{\\alpha}\\frac{S_{i,j}}{\\sqrt{m_{i,j}}}+\\sum_{i=1}^{N}A_{\\alpha}\\frac{S_{i,n-j}}{\\sqrt{m_{i,n-j}}});}\\end{array}$ end for let $\\boldsymbol{S}$ be a sample drawn from ${\\mathfrak{S}}_{{\\mathcal{N}}}^{j}$ where j corresponding to the stratum with the maximum $I_{\\mathcal{N}}^{j}$ ; $u\\gets\\mathcal{U}(\\mathcal{S})-\\bar{\\mathcal{U}}(\\mathcal{N}\\backslash\\mathcal{S})$ ; for $z_{i}\\in S$ do $\\overline{{S\\mathcal{V}_{i,|S|}}}+=u;m_{i,|S|}+=1;$ ; end for for $z_{i}\\in\\mathcal{N}\\backslash\\mathcal{S}$ do $\\overline{{S\\mathcal{V}_{i,|N\\backslash S|}}}-=u;m_{i,|N\\backslash S|}+=1;$ end for update $\\widehat{S_{i,j}^{2}}\\ (1\\leq i,j\\leq n)$ ;   \nend for   \nfor $\\mathrm{i}{=}1$ to n do $\\begin{array}{r}{\\overline{{S\\mathcal{V}_{i}}}=\\frac{1}{n}\\sum_{j=1}^{n}\\overline{{S\\mathcal{V}_{i,j}}}/m_{i,j};}\\end{array}$ ;   \nend for   \nreturn $\\overline{{S\\nu_{1}}},...,\\overline{{S\\nu_{n}}}$ . ", "page_idx": 19}, {"type": "text", "text": "to optimize sampling efficiency but rather to provide an error analysis for the sampling process. Our proposed approach not only provides an unbiased estimate of the integrated gradients but also includes this crucial error analysis. ", "page_idx": 20}, {"type": "text", "text": "Let u be a uniformly distributed random variable over the interval $[0,1]$ , where ", "page_idx": 20}, {"type": "equation", "text": "$$\nh(u)=(x_{i}-x_{i}^{\\prime})\\frac{\\partial f\\left(x^{\\prime}+u\\left(x-x^{\\prime}\\right)\\right)}{\\partial x_{i}}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Then, $\\mathrm{\\mathbf{h}}(\\mathbf{u})$ is an unbiased estimator of $I G_{i}(x,x^{\\prime},f)$ due to ", "page_idx": 20}, {"type": "equation", "text": "$$\nE[h(u)]=(x_{i}-x_{i}^{\\prime})\\int_{u=0}^{1}\\frac{\\partial f\\left(x^{\\prime}+u\\left(x-x^{\\prime}\\right)\\right)}{\\partial x_{i}}d u=(x_{i}-x_{i}^{\\prime})\\int_{\\alpha=0}^{1}\\frac{\\partial f\\left(x^{\\prime}+\\alpha\\left(x-x^{\\prime}\\right)\\right)}{\\partial x_{i}}d\\alpha.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Next, we can obtain a more accurate unbiased estimate through the following steps: First, we generate $m_{i}$ random samples $u_{1},\\cdots,u_{m_{i}}$ of $\\mathbf{u}$ . Then, we sample $\\mathbf{h}(\\mathbf{u})$ based on these random numbers to get an independent and identically distributed sample $h(u_{1}),\\cdot\\cdot\\cdot\\,,h(u_{m_{i}})$ . Finally, we use the observed values of $\\begin{array}{r}{\\overline{{I G_{i}(x,x^{\\prime},f)}}\\,=\\,\\frac{1}{m_{i}}\\sum_{i=1}^{m_{i}}h(u_{i})}\\end{array}$ as the estimate for $I G_{i}(x,x^{\\prime},f)$ . The proof that $\\begin{array}{r}{\\frac{1}{m_{i}}\\sum_{i=1}^{m_{i}}h(u_{i})}\\end{array}$ is an unbiased estimator of $I G_{i}(x,x^{\\prime},f)$ is straightforward, due to the fact that $\\begin{array}{r}{E[\\frac{1}{m_{i}}\\sum_{i=1}^{m_{i}}h(u_{i})]=\\frac{1}{m_{i}}\\sum_{i=1}^{m_{i}}E[h(u_{i})]=I G_{i}(x,x^{\\prime},f).}\\end{array}$ ", "page_idx": 20}, {"type": "text", "text": "Lemma 7. The probability that $\\overline{{I G_{i}(x,x^{\\prime},f)}}(1\\leq i\\leq n)$ deviates from $I G_{i}(x,x^{\\prime},f)$ be equal to or greater than any fixed $\\epsilon\\geq0$ given the sample size $m_{i}$ is bounded by ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{P}(|\\overline{{I G_{i}(x,x^{\\prime},f)}}-I G_{i}(x,x^{\\prime},f)|\\geq\\epsilon|m_{i})\\leq2\\exp(-\\frac{2m_{i}\\epsilon^{2}}{r_{i,j}^{2}})\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\begin{array}{r}{r_{i,j}=\\operatorname*{max}_{u\\in[0,1]}h(u)-\\operatorname*{min}_{u\\in[0,1]}h(u).}\\end{array}$ ", "page_idx": 20}, {"type": "text", "text": "Proof. According to Hoeffding\u2019s inequality, we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{~~~\\mathbb{P}(|\\overline{{I G_{i}(x,x^{\\prime},f)}}-I G_{i}(x,x^{\\prime},f)|\\geq\\epsilon|m_{i})}\\\\ &{=\\!\\mathbb{P}(|\\overline{{I G_{i}(x,x^{\\prime},f)}}-\\mathbb{E}[\\overline{{I G_{i}(x,x^{\\prime},f)}}]|\\geq\\epsilon|m_{i})}\\\\ &{=\\!\\mathbb{P}(|\\sum_{i=1}^{m_{i}}h(u_{i})-\\mathbb{E}[\\displaystyle\\sum_{i=1}^{m_{i}}h(u_{i})]|\\geq m_{i}\\epsilon|m_{i})\\leq2\\exp(-\\frac{2m_{i}\\epsilon^{2}}{r_{i,j}^{2}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "F Supplement to Experiments ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "F.1 Experiments Compute Resources ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We conduct experiments on a machine with 2 Montage(R) Jintide(R) C6226R $\\ @\\ 2.90\\mathrm{GHz}$ and 256GB memory. Our experiments do not require high-end hardware, and our algorithm is not time-consuming. For feature attribution experiments on synthetic datasets, each attribution algorithm takes about 10 seconds to attribute one data point. Thus, an attribution algorithm takes several hours to attribute an entire dataset. The time consumption on our real dataset is similar. Also, since our algorithm uses very little memory, it is easy to run multiple processes and algorithms in parallel on a machine, so the time cost for reproduction is friendly. ", "page_idx": 20}, {"type": "text", "text": "F.2 Analysis of Experiment Design ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Statistical Characteristics of the Synthetic Datasets. We present the mean and variance of each feature in our synthetic datasets to provide crucial insights into their characteristics, as shown in Table 3. The mean offers an understanding of the average behaviour of features, while the variance indicates their variability. This information is vital for assessing the data\u2019s overall distribution and quality, and it plays a key role in interpreting the results of our proposed methods and baseline algorithms. ", "page_idx": 20}, {"type": "table", "img_path": "jZv9A8Tg9p/tmp/10c3f13ee9b85d7f2ca4445e0c1a4a778a8924a6ef7222a0fe629966a69fba3e.jpg", "table_caption": ["Table 3: Mean and Std.Dev. of Features as a Function of $\\rho$ . "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "These statistics help validate the synthetic data\u2019s consistency and reliability, which is essential for the credibility of our experimental findings. ", "page_idx": 21}, {"type": "text", "text": "Statistical Characteristics of the Real Datasets. In the analysis of the Griliches76 dataset [36], we observe various degrees of correlation between the mother\u2019s years of education (denoted as med) and several key variables. Firstly, the correlation coefficient between the mother\u2019s education and marital status (variables mrt and $\\mathrm{\\mrt80}\\$ ) is close to 0, indicating almost no correlation between the mother\u2019s level of education and her marital status. The correlation coefficients with urban residence status (variables smsa and smsa80) are 0.098 and 0.031, respectively, suggesting a slight positive correlation. This implies that there is a weak but positive association between the mother\u2019s educational attainment and living in an urban area. A moderate positive correlation is observed with IQ, as indicated by a correlation coefficient of 0.226 with the mother\u2019s education. This suggests that higher maternal education is somewhat associated with higher IQ scores. Similarly, the correlation between the mother\u2019s education and scores in the world of work test is 0.195, which also reflects a moderate positive correlation. This indicates that higher maternal education levels might be linked to better performance in job-related knowledge. Most notably, the correlation coefficients with personal education years (variables s and s80) are 0.340 and 0.341, respectively, indicating a relatively strong positive correlation. This suggests that the mother\u2019s level of education is considerably associated with the individual\u2019s own educational attainment. While there is a certain degree of correlation between maternal education and both IQ and job knowledge test scores, factors like regression to the mean in intelligence suggest that the correlation between a mother\u2019s education and her child\u2019s IQ is weaker than the correlation between a mother\u2019s education and the child\u2019s own educational attainment. Therefore, we posit that selecting maternal education as an instrumental variable, although not perfectly ideal, still holds validity and can be utilized to verify our methodology. ", "page_idx": 21}, {"type": "text", "text": "In the Angrist dataset, a child must be six years old within the current year to enroll in school under the U.S. Compulsory Education Law. In the U.S., the school year typically starts in August, meaning a child turning six in December can still commence their education in the same year. Consequently, a child born in the fourth quarter, such as December, can start school before reaching six. Conversely, a child born in the first quarter, like January, must wait until the autumn term after their sixth birthday to begin school. U.S. law mandates students must be at least 16 years old to legally drop out of school. Therefore, students dropping out at 16 may have varying years of education based on their birth month. For instance, those born between 1920-1929 have average educational years of 11.39, 11.44, 11.55, and 11.57 for each quarter, respectively. Parents, when deciding to have children, seldom consider such subtle differences in birth months. Thus, the month of a child\u2019s birth, independent of other factors affecting educational levels like intelligence, family background, and environment, can be seen as a random assignment. This inadvertently creates variations in education duration based on birth month \u2013 akin to a randomized controlled trial where children born in the fourth quarter represent the \"experimental group\" with longer education, while those in the first quarter are the \"control group\" with shorter education. Hence, the birth quarter serves as an instrumental variable in this context. ", "page_idx": 21}, {"type": "text", "text": "F.3 Classification Task with DNN Model ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In this experiment, we continued to utilize synthetic datasets a and b for a classification study. This time, the labels were processed for binary classification. Specifically, we computed the probabilities for data points being classified into category 1 by applying a sigmoid function to the y values in the datasets; otherwise, the labels were assigned to category 0. Due to the inherent randomness in generating labels, it was not feasible to directly determine a benchmark for each feature\u2019s contribution to the data classification. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "To validate the efficacy of our proposed method, we designed a comparative experiment based on the symmetric properties of SHAP and IG. In this experiment, we set baseline inputs by reducing the values of feature t and the collaborative variables c in each data point. The reduction followed a specific rule: the decrease in t and the decrease in c should result in equivalent Shapley/IG values for the change in y. Under this setup, the SHAP/IG values attributed to the classification into category 1 should be identical for both t and c. We assessed the effectiveness of our approach by comparing the difference in SHAP/IG values for t and c between our method and baseline algorithms. The results indicated that our approach significantly reduced errors compared to baseline algorithms. This outcome suggests that even though $t$ and $c$ might have similar Shapley values in altering y, the baseline training method may inaccurately estimate changes in latent confounders, leading to different impacts of $t$ and $c$ on the final classification. ", "page_idx": 22}, {"type": "image", "img_path": "jZv9A8Tg9p/tmp/33ac52f9938d7d2fb0676e0b99ff8e4f6bd1ba8aaf3d027142a3df42bfa86f8e.jpg", "img_caption": ["Figure 4: Evaluation results on synthetic Dataset A with DNN Classifier. "], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "jZv9A8Tg9p/tmp/39bca49f69b267e6c314358d49547cf3f7c917e972f0fed42df6d1ff2c61a908.jpg", "img_caption": ["Figure 5: Evaluation results on synthetic Dataset B with DNN Classifier. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "F.4 Regression Task with XGBoost Model ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We opted for XGBoost as the representative of non-deep learning models for our experiments. As gradient accumulation is not feasible on XGBoost, the Integrated Gradients (IG) algorithm cannot be applied. Hence, our comparisons were primarily focused on SHAP-based algorithms. The experimental results indicate that our method outperforms the baseline in most scenarios. When the impact of the unobservable confounder is minimal, our method is less effective compared to the baseline. This is considered reasonable, as there are inherent errors in training the model with features re-estimated using instrumental variables. In such scenarios, the influence of these errors on the model surpasses that of the unobservable confounder. ", "page_idx": 22}, {"type": "image", "img_path": "jZv9A8Tg9p/tmp/186c2adcd0902a14df010a394b741b60dd08fbbd599d643a505051d9d6450d0a.jpg", "img_caption": ["Figure 6: Evaluation results on synthetic dataset a with XGBoost. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "F.5 Efficiency of Our Approximation Methods ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We utilized widely-used algorithms MC (Monte Carlo, referring to the Monte Carlo sampling method based on marginal contributions) [7], CC (Complementary Contribution, referring to the stratified sampling method based on complementary contribution), and the state-of-the-art CCN (Complementary Contribution Neyman, referring to the sampling based on Neyman allocation with complementary contribution) [46] as baseline methods for our experiment on the real-world Spambase dataset. We chose the Spambase dataset for its larger number of features (56), compared to the two other real datasets we previously used. This higher feature count offers a better testbed to evaluate our proposed methods. It is worth noting that both our proposed method and these baseline methods are unbiased sampling estimation approaches. We trained a regression neural network model on a random selection of 1000 data points from this dataset. The efficiency of each method was assessed by comparing the Mean Squared Error (MSE) of SHAP values against a benchmark for the same number of samples. For this, we denoted the SHAP value of the $j^{t h}$ feature for the $i^{t h}$ data point as $\\boldsymbol{S}\\boldsymbol{\\nu}_{i,j}$ in the benchmark, and $\\overline{{S\\mathcal{V}_{i,j}}}$ in the estimation algorithm, with the MSE calculated using $\\frac{\\sum_{i=1}^{n}\\sum_{j=1}^{m}(S\\mathcal{V}_{i,j}-\\overline{{S\\mathcal{V}_{i,j}}})^{2}}{n{*m}}$ , where $\\scriptstyle{\\mathtt{n}=1000}$ and $\\scriptstyle{\\mathrm{m}}=56$ . Given that the computation of exact SHAP values requires exponential time complexity, we used the results obtained from extensive sampling via the CC method as our benchmark, involving $10{,}000{\\times}56$ samples. These results served as a specific benchmark, separate from the baseline CC method used earlier in the experiment for comparative analysis. We then analyzed errors for sampling algorithms at various sample sizes, ranging from $100\\!\\times\\!56$ to $500\\!\\times\\!56$ . Our findings revealed a decrease in error for all algorithms as sample size increased, with our method exhibiting the lowest error as shown in Table 4. This superior performance is attributed to our method\u2019s unique ability to estimate each stratum\u2019s confidence interval from sample variance during sampling. ", "page_idx": 22}, {"type": "image", "img_path": "jZv9A8Tg9p/tmp/4623ed0e13bdb479ce2b01ef8f5bde5e3ca1d8b88f4bf24c70d7e3074fdbeb3c.jpg", "img_caption": ["Figure 7: Evaluation results on synthetic dataset b with XGBoost. "], "img_footnote": [], "page_idx": 23}, {"type": "table", "img_path": "jZv9A8Tg9p/tmp/3f4c49941b70bf015e339c053212dbed4bc1daeced09bfb6cf566b4e860991f8.jpg", "table_caption": ["Table 4: MSE of the SHAP values estimation. "], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: The introduction of our paper clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 23}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: Please see Section 6. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 24}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: We provide the full set of assumptions and a complete (and correct) proof. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 24}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: The experiments sections (Section 5 and F in the appendix) provided in the paper disclose all the information needed to reproduce the main experimental results. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 25}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: Our code can be found in the repository at https://github.com/ ZJU-DIVER/IV-SHAP. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. ", "page_idx": 25}, {"type": "text", "text": "\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). \u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 26}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: The full details are provided with the code. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 26}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: For the results on synthetic datasets, we used bar charts, which effectively demonstrate the stability of the experimental results. For the experimental results presented in tabular format, we included the standard deviation to show statistical significance. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 26}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: Please see Section F.1 in the appendix for the information of computer resources. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 27}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: We have thoroughly reviewed the NeurIPS Code of Ethics and confirm that our research conforms to it in every respect. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 27}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: Please see Section A in the appendix for societal impacts. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 27}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: The paper does not pose such risks. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 28}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We have properly cited the original papers that produced the real datasets used in the paper. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 28}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: The paper introduces new code assets, which are well documented. The documentation includes details about the usage, the limitations, and the corresponding license. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 28}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 29}, {"type": "text", "text": "Answer:[NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. We used two real-world datasets that contain information about education and income collected from the market, which are anonymized and do not involve direct human subjects research. Therefore, the requirements for providing instructions, screenshots, and compensation details do not apply. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 29}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: The paper uses two real-world datasets that contain information about education and income collected from the market. These datasets are anonymized and do not involve direct human subjects research. Therefore, IRB approval is not required. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 29}]