{"references": [{"fullname_first_author": "Long Ouyang", "paper_title": "Training language models to follow instructions with human feedback", "publication_date": "2022-03-02", "reason": "This paper is foundational to the field, introducing a method for aligning LLMs with human values through Reinforcement Learning from Human Feedback (RLHF), a technique heavily discussed and contrasted with the paper's proposed method."}, {"fullname_first_author": "Andy Zou", "paper_title": "Representation engineering: A top-down approach to AI transparency", "publication_date": "2023-10-01", "reason": "This paper introduces Representation Engineering (RepE), a key prior work that the current paper builds upon; the proposed method directly addresses limitations of RepE in handling multiple, simultaneous control tasks."}, {"fullname_first_author": "Nicholas Goldowsky-Dill", "paper_title": "Localizing model behavior with path patching", "publication_date": "2023-04-05", "reason": "The core methodological innovation of the current paper, Sparse Activation Control, relies on the path patching technique for identifying task-relevant components within LLMs, making this a crucial foundational reference."}, {"fullname_first_author": "Chenyu Shi", "paper_title": "Navigating the overkill in large language models", "publication_date": "2024-01-17", "reason": "This work directly addresses the problem of over-alignment in LLMs, a significant challenge also tackled by the current paper; the findings and analysis in this paper provide valuable context and comparison for evaluating the success of the proposed approach."}, {"fullname_first_author": "Lichao Sun", "paper_title": "TrustLLM: Trustworthiness in large language models", "publication_date": "2024-01-05", "reason": "This paper introduces a comprehensive benchmark, TrustLLM, for evaluating the trustworthiness of LLMs; the current paper uses TrustLLM for its experimental setup and evaluation metrics, making it a critical reference for understanding the context and scope of the presented work."}]}