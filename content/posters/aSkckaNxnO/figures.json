[{"figure_path": "aSkckaNxnO/figures/figures_1_1.jpg", "caption": "Figure 1: Left. Control conflict of representation engineering for multiple tasks, i.e., the performance of single control consistently increases while the simultaneous control of multiple behaviors decreases on all tasks. Right. Sparsity and uniqueness of related components in LLMs for different behaviors, i.e., the corresponding heads for different tasks are sparse and independent.", "description": "The figure illustrates the limitations of traditional representation engineering methods for controlling multiple aspects of LLM behavior simultaneously.  The left panel shows that while controlling a single aspect (e.g., preference) improves performance, attempting to control multiple aspects concurrently leads to performance degradation across all aspects. This highlights a control conflict. The right panel demonstrates that specific components within LLMs (attention heads) exhibit sparsity and independence in relation to different tasks. This sparsity suggests that near-independent control over multiple tasks might be possible by focusing on these specific components, addressing the limitations highlighted in the left panel.", "section": "1 Introduction"}, {"figure_path": "aSkckaNxnO/figures/figures_8_1.jpg", "caption": "Figure 2: Left. The variance ratio of the top-10 components of the top 10 layers that have the highest classification accuracy. Right. We collect the output activation from one head being controlled, and we plot the Gaussian distribution results after TSNE clustering. The blue and red dots represent the distribution of activations of X and X<sub>e</sub> samples.", "description": "The figure shows two visualizations. The left panel displays the variance ratio of the top 10 principal components obtained from principal component analysis (PCA) of the top 10 layers with the highest classification accuracy for 10 different heads. Each line represents a different head, showing how much variance is explained by each principal component. The right panel shows a t-SNE visualization of stimulus pairs (experimental and reference prompts) for one controlled head. Red dots represent the experimental prompts, while blue dots represent the reference prompts. The Gaussian Mixture Model (GMM) is used to model the distribution of the prompts.", "section": "4.3 Ablation Studies"}, {"figure_path": "aSkckaNxnO/figures/figures_18_1.jpg", "caption": "Figure 1: Left. Control conflict of representation engineering for multiple tasks, i.e., the performance of single control consistently increases while the simultaneous control of multiple behaviors decreases on all tasks. Right. Sparsity and uniqueness of related components in LLMs for different behaviors, i.e., the corresponding heads for different tasks are sparse and independent.", "description": "This figure illustrates two key aspects of the paper's proposed method. The left panel shows the control conflict problem in traditional representation engineering, where attempting to control multiple behaviors simultaneously leads to performance degradation across all tasks. The right panel highlights the sparsity and independence of components (attention heads) in LLMs, which enables near-independent control over different tasks through sparse activation control.", "section": "1 Introduction"}, {"figure_path": "aSkckaNxnO/figures/figures_18_2.jpg", "caption": "Figure 1: Left. Control conflict of representation engineering for multiple tasks, i.e., the performance of single control consistently increases while the simultaneous control of multiple behaviors decreases on all tasks. Right. Sparsity and uniqueness of related components in LLMs for different behaviors, i.e., the corresponding heads for different tasks are sparse and independent.", "description": "The figure demonstrates two key aspects of the proposed Sparse Activation Control method.  The left panel shows the limitations of traditional representation engineering when attempting to control multiple LLM behaviors simultaneously; individual controls are effective, but simultaneous control leads to performance degradation. The right panel highlights the sparsity and independence of components (attention heads) within LLMs, enabling near-independent control over multiple behaviors via sparse activation.", "section": "1 Introduction"}, {"figure_path": "aSkckaNxnO/figures/figures_23_1.jpg", "caption": "Figure 1: Left. Control conflict of representation engineering for multiple tasks, i.e., the performance of single control consistently increases while the simultaneous control of multiple behaviors decreases on all tasks. Right. Sparsity and uniqueness of related components in LLMs for different behaviors, i.e., the corresponding heads for different tasks are sparse and independent.", "description": "The figure demonstrates two key aspects of the proposed Sparse Activation Control method. The left panel illustrates that traditional representation engineering struggles when trying to control multiple aspects of LLM behavior simultaneously (e.g. safety, factuality, preference).  Performance declines across all tasks in this scenario.  In contrast, the right panel shows that certain components within LLMs, specifically attention heads, exhibit sparsity and independence. This means the heads related to different tasks are distinct and thus easier to control independently, which is the key idea behind the proposed sparse activation control approach.", "section": "1 Introduction"}, {"figure_path": "aSkckaNxnO/figures/figures_23_2.jpg", "caption": "Figure 1: Left. Control conflict of representation engineering for multiple tasks, i.e., the performance of single control consistently increases while the simultaneous control of multiple behaviors decreases on all tasks. Right. Sparsity and uniqueness of related components in LLMs for different behaviors, i.e., the corresponding heads for different tasks are sparse and independent.", "description": "This figure shows two key findings about the limitations of representation engineering and the potential of sparse activation control. The left panel demonstrates that controlling multiple aspects of LLM behavior (like preference, safety, and factuality) simultaneously using representation engineering leads to performance decrease across all controlled aspects, which highlights a conflict between control targets. The right panel shows that attention heads in LLMs, when involved in different tasks, tend to be sparse and not overlap heavily with one another, offering opportunities for independently controlling each aspect without causing negative side effects on other tasks.", "section": "1 Introduction"}, {"figure_path": "aSkckaNxnO/figures/figures_23_3.jpg", "caption": "Figure 1: Left. Control conflict of representation engineering for multiple tasks, i.e., the performance of single control consistently increases while the simultaneous control of multiple behaviors decreases on all tasks. Right. Sparsity and uniqueness of related components in LLMs for different behaviors, i.e., the corresponding heads for different tasks are sparse and independent.", "description": "The figure demonstrates the challenges of controlling multiple aspects of LLM behavior simultaneously using traditional representation engineering methods.  The left panel shows a performance decrease when attempting to control multiple behaviors (preference, safety, factuality) concurrently, indicating control conflict. The right panel highlights the sparsity and independence of components (attention heads) within LLMs, suggesting that independent control of each aspect may be feasible. This observation motivates the proposed 'Sparse Activation Control' method, which leverages these sparse characteristics for independent and concurrent control.", "section": "1 Introduction"}, {"figure_path": "aSkckaNxnO/figures/figures_23_4.jpg", "caption": "Figure 1: Left. Control conflict of representation engineering for multiple tasks, i.e., the performance of single control consistently increases while the simultaneous control of multiple behaviors decreases on all tasks. Right. Sparsity and uniqueness of related components in LLMs for different behaviors, i.e., the corresponding heads for different tasks are sparse and independent.", "description": "The figure demonstrates two key aspects. The left panel shows the conflict arising when using representation engineering to control multiple LLM behaviors simultaneously. It indicates that while single-behavior control consistently improves performance, attempting to control multiple behaviors concurrently leads to a performance decrease across all tasks. The right panel highlights the sparsity and independence of LLM components (attention heads) related to specific behaviors. This suggests that individual control over different tasks is feasible due to the limited overlap between the components involved.", "section": "1 Introduction"}, {"figure_path": "aSkckaNxnO/figures/figures_23_5.jpg", "caption": "Figure 1: Left. Control conflict of representation engineering for multiple tasks, i.e., the performance of single control consistently increases while the simultaneous control of multiple behaviors decreases on all tasks. Right. Sparsity and uniqueness of related components in LLMs for different behaviors, i.e., the corresponding heads for different tasks are sparse and independent.", "description": "The figure demonstrates the challenges of using representation engineering to control multiple aspects of LLM behavior simultaneously. The left panel shows that while single-task control consistently improves performance, multi-task control leads to performance degradation across all tasks. The right panel illustrates the sparse and independent nature of components within LLMs responsible for different behaviors, suggesting that individual control over these components might be more effective than holistic representation engineering.", "section": "1 Introduction"}, {"figure_path": "aSkckaNxnO/figures/figures_23_6.jpg", "caption": "Figure 1: Left. Control conflict of representation engineering for multiple tasks, i.e., the performance of single control consistently increases while the simultaneous control of multiple behaviors decreases on all tasks. Right. Sparsity and uniqueness of related components in LLMs for different behaviors, i.e., the corresponding heads for different tasks are sparse and independent.", "description": "The figure illustrates the challenges of applying representation engineering to control multiple aspects of LLM behavior simultaneously (left).  Attempting to control multiple behaviors leads to performance decline on all tasks due to control conflicts. In contrast, the right panel shows that different components within LLMs (attention heads in this case) exhibit sparse and independent relationships with different behavioral tasks, suggesting the possibility of more independent control.", "section": "1 Introduction"}, {"figure_path": "aSkckaNxnO/figures/figures_24_1.jpg", "caption": "Figure 1: Left. Control conflict of representation engineering for multiple tasks, i.e., the performance of single control consistently increases while the simultaneous control of multiple behaviors decreases on all tasks. Right. Sparsity and uniqueness of related components in LLMs for different behaviors, i.e., the corresponding heads for different tasks are sparse and independent.", "description": "This figure illustrates two key findings. The left panel shows that while representation engineering can improve individual aspects of LLM behavior (preference, safety, factuality), attempting to simultaneously control multiple aspects leads to performance degradation in all areas.  The right panel demonstrates that attention heads in LLMs exhibit sparsity and uniqueness; specific attention heads are primarily associated with specific tasks, which allows for independent control over multiple dimensions of trustworthiness.", "section": "1 Introduction"}]