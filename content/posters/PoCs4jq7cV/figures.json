[{"figure_path": "PoCs4jq7cV/figures/figures_1_1.jpg", "caption": "Figure 1: We apply temporal contrastive learning to observation pairs to obtain representations (\u03c8(xo), \u03c8(Xt+k)) such that \u0391\u03c8(xo) is close to (xt+k). While inferring waypoints in the high-dimensional observation space is challenging, we show that the distribution over intermediate latent representations has a closed form solution corresponding to linear interpolation between the initial and final representations.", "description": "This figure illustrates the core idea of the paper: using contrastive learning to learn low-dimensional representations of high-dimensional time series data.  The learned representations are shown to follow a Gauss-Markov chain, allowing for efficient inference (e.g., prediction, planning) by simple matrix inversion or linear interpolation in the latent space. The figure visually depicts the mapping from high-dimensional observation space to a low-dimensional latent space where planning becomes easier.", "section": "1 Introduction"}, {"figure_path": "PoCs4jq7cV/figures/figures_4_1.jpg", "caption": "Figure 1: We apply temporal contrastive learning to observation pairs to obtain representations (\u03c8(xo), \u03c8(Xt+k)) such that \u0391\u03c8(xo) is close to (xt+k). While inferring waypoints in the high-dimensional observation space is challenging, we show that the distribution over intermediate latent representations has a closed form solution corresponding to linear interpolation between the initial and final representations.", "description": "This figure illustrates the concept of contrastive learning applied to time series data.  Pairs of observations (x0 and xt+k) are used to learn representations (\u03c8(x0) and \u03c8(xt+k)). The key idea is that a linear transformation (A) applied to the initial representation (\u03c8(x0)) should closely approximate the final representation (\u03c8(xt+k)). The figure highlights the advantage of working in a lower-dimensional latent space. While directly interpolating waypoints in the original high-dimensional space is difficult, it becomes a simple linear interpolation in the latent space.", "section": "1 Introduction"}, {"figure_path": "PoCs4jq7cV/figures/figures_4_2.jpg", "caption": "Figure 1: We apply temporal contrastive learning to observation pairs to obtain representations (\u03c8(xo), \u03c8(Xt+k)) such that \u0391\u03c8(xo) is close to (xt+k). While inferring waypoints in the high-dimensional observation space is challenging, we show that the distribution over intermediate latent representations has a closed form solution corresponding to linear interpolation between the initial and final representations.", "description": "This figure illustrates the core idea of the paper: using temporal contrastive learning to learn low-dimensional representations of high-dimensional time series data.  The learned representations, denoted \u03c8(x), are designed such that the transformation A\u03c8(x0) approximates the representation of a future state, \u03c8(xt+k). The key result is that the distribution over intermediate representations between \u03c8(x0) and \u03c8(xt+k) has a closed-form solution and can be computed via linear interpolation, simplifying inference tasks.", "section": "1 Introduction"}, {"figure_path": "PoCs4jq7cV/figures/figures_6_1.jpg", "caption": "Figure 4: Numerical simulation of our analysis. (Top Left) Toy dataset of time-series data consisting of many outwardly-spiraling trajectories. We apply temporal contrastive learning to these data. (Top Right) For three initial observations ( ), we use the learned representations to predict the distribution over future observations. Note that these distributions correctly capture the spiral structure. (Bottom Left) For three observations (+), we use the learned representations to predict the distribution over preceding observations. (Bottom Right) Given an initial and final observation, we plot the inferred posterior distribution over the waypoint (Section 4.3). The representations capture the shape of the distribution.", "description": "This figure shows the results of numerical simulations on a toy dataset of outwardly spiraling trajectories.  Temporal contrastive learning is applied, and the learned representations are used for forward prediction, backward prediction (inferring past states), and planning (inferring intermediate waypoints between a start and end state).  The results demonstrate that the learned representations effectively capture the underlying structure of the data, allowing for accurate probabilistic inference.", "section": "5 Numerical Simulation"}, {"figure_path": "PoCs4jq7cV/figures/figures_7_1.jpg", "caption": "Figure 5: Using inferred paths over our contrastive representations for control boosts success rates by 4.5\u00d7 on the most difficult goals (18% \u2192 84%). Alternative representation learning techniques fail to improve performance when used for planning.", "description": "This figure shows the results of an experiment where the authors used inferred paths from contrastive representations for a control task in a maze environment.  The x-axis represents the initial Euclidean distance to the goal in the maze, while the y-axis shows the success rate of reaching the goal.  Different colored lines represent different methods: contrastive (ours), VIP, PCA, autoencoder, and no planning. The figure demonstrates that using the inferred paths from contrastive representations significantly improves the success rate, particularly for goals that are farther away.", "section": "5.2 Solving Mazes with Inferred Representations"}, {"figure_path": "PoCs4jq7cV/figures/figures_8_1.jpg", "caption": "Figure 6: Planning for 39-dimensional robotic door opening. (Top Left) We use a dataset of trajectories demonstrating door opening from prior work [31] to learn representations. (Top Right) We use our method and three baselines to infer one intermediate waypoint between the first and last observation in a trajectory from a held-out validation set. Errors are measured using the mean squared error with the true waypoint observation; predicted representations are converted to observations using nearest neighbors on a validation set. (Bottom) We visualize a TSNE [89] of the states along the sampled trajectory as blue circles, with the transparency indicating the index along the trajectory. The inferred plan is shown as red circles connected by arrows. Our method generates better plans than alternative representation learning methods (PCA, VIP).", "description": "This figure shows the results of a planning task in a 39-dimensional robotic door opening scenario.  It compares the proposed contrastive planning method with three baselines (no planning, PCA planning, and VIP planning) in terms of waypoint prediction accuracy.  The figure displays a dataset of trajectories, the mean squared error (MSE) of waypoint prediction for each method, and a t-SNE visualization showing the learned representations and the inferred plans.", "section": "5.3 Higher dimensional tasks"}, {"figure_path": "PoCs4jq7cV/figures/figures_18_1.jpg", "caption": "Figure 7: Our approach enables a goal-conditioned policy to reach farther targets (red) from the start (green) by planning over intermediate waypoints (orange).", "description": "This figure shows five examples of a robot navigating a maze. The robot starts at a green square and must reach a red star.  The blue lines indicate the path that the robot would take without the use of the proposed method. The orange dots and lines represent the planned path based on the proposed approach using contrastive representations to interpolate between the start and goal. The result shows that the use of the inferred waypoints helps the robot successfully navigate to the goal, even when it is more distant than in previous examples.. This illustrates how contrastive representations can make planning simpler in complex environments.", "section": "5.2 Solving Mazes with Inferred Representations"}, {"figure_path": "PoCs4jq7cV/figures/figures_19_1.jpg", "caption": "Figure 8: Planning for 46-dimensional robotic hammering. (a) Contrastive representations. (b) PCA representations.", "description": "This figure demonstrates the results of applying contrastive learning and PCA to a 46-dimensional robotic hammering dataset.  Panel (a) shows the learned contrastive representations, visualized as a trajectory with intermediate states inferred using the method described in the paper. The transparency of the points indicates the position in the time series. The inferred plan (intermediate states) are represented by red circles connected by arrows. Panel (b) shows the results obtained using PCA, which does not capture the same nonlinear trajectory. The figure aims to illustrate that contrastive representations are superior to PCA representations for the task of planning or interpolating in high-dimensional state spaces.", "section": "5 Numerical Simulation"}, {"figure_path": "PoCs4jq7cV/figures/figures_19_2.jpg", "caption": "Figure 4: Numerical simulation of our analysis. (Top Left) Toy dataset of time-series data consisting of many outwardly-spiraling trajectories. We apply temporal contrastive learning to these data. (Top Right) For three initial observations (), we use the learned representations to predict the distribution over future observations. Note that these distributions correctly capture the spiral structure. (Bottom Left) For three observations (+), we use the learned representations to predict the distribution over preceding observations. (Bottom Right) Given an initial and final observation, we plot the inferred posterior distribution over the waypoint (Section 4.3). The representations capture the shape of the distribution.", "description": "This figure shows numerical simulations validating the paper's theoretical results.  A toy dataset of spiraling trajectories is used to demonstrate the effectiveness of contrastive learning in predicting future and past states, as well as inferring intermediate states (waypoints) between given initial and final states. The distributions generated from the learned representations accurately reflect the structure of the data.", "section": "5 Numerical Simulation"}]