[{"type": "text", "text": "Energy-based Hopfield Boosting for Out-of-Distribution Detection ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Claus Hofmann 1 Simon Schmid 2 Bernhard Lehner 3 Daniel Klotz 4 Sepp Hochreiter 1 ", "page_idx": 0}, {"type": "text", "text": "1 Institute for Machine Learning, JKU LIT SAL IWS Lab, Johannes Kepler University, Linz, Austria 2 Software Competence Center Hagenberg GmbH, Austria 3 Silicon Austria Labs, JKU LIT SAL IWS Lab, Linz, Austria 4 Department of Computational Hydrosystems, Helmholtz Centre for Environmental Research\u2013UFZ, Leipzig, Germany ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Out-of-distribution (OOD) detection is critical when deploying machine learning models in the real world. Outlier exposure methods, which incorporate auxiliary outlier data in the training process, can drastically improve OOD detection performance compared to approaches without advanced training strategies. We introduce Hopfield Boosting, a boosting approach, which leverages modern Hopfield energy to sharpen the decision boundary between the in-distribution and OOD data. Hopfield Boosting encourages the model to focus on hard-to-distinguish auxiliary outlier examples that lie close to the decision boundary between in-distribution and auxiliary outlier data. Our method achieves a new state-of-the-art in OOD detection with outlier exposure, improving the FPR95 from 2.28 to 0.92 on CIFAR-10, from 11.76 to 7.94 on CIFAR-100, and from 50.74 to 36.60 on ImageNet-1K. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Out-of-distribution (OOD) detection is crucial when using machine learning systems in the real world (Ruff et al., 2021; Yang et al., 2021; Liu et al., 2021). Deployed models will \u2014 sooner or later \u2014 encounter inputs that deviate from the training distribution. For example, a system trained to recognize music genres might also hear a sound clip of construction site noise. In the best case, a naive deployment can then result in overly confident predictions. In the worst case, we will get erratic model behavior and completely wrong predictions (Hendrycks & Gimpel, 2017). The purpose of OOD detection is to classify these inputs as OOD, such that the system can then, for instance, notify users that no prediction is possible. In this paper we propose Hopfield Boosting, a novel OOD detection method that leverages the energy component of modern Hopfield networks (MHNs; Ramsauer et al., 2021) and advances the state-of-the-art of OOD detection. This energy represents a measure of dissimilarity between a set of data instances $\\mathbf{\\deltaX}$ and a query instance $\\xi$ . It is therefore a natural fit for doing OOD detection (as shown in Zhang et al., 2023a). ", "page_idx": 0}, {"type": "text", "text": "Hopfield Boosting uses an auxiliary outlier data set (AUX) to boost the model\u2019s OOD detection capacity. This allows the training process to learn a boundary around the in-distribution (ID) data, improving the performance at the OOD detection task. In summary, our contributions are as follows: ", "page_idx": 0}, {"type": "image", "img_path": "VLQYtVMTYz/tmp/967ca3426e1c0a5692f142668fe20be7a56bf214bf74601ec4ae1de641cc10cb.jpg", "img_caption": ["Figure 1: The Hopfield Boosting concept. The first step (weight) creates weak learners by firstly choosing in-distribution samples (ID, orange), and by secondly choosing auxiliary outlier samples (AUX, blue) according to their assigned probabilities; the second step (evaluate) computes the losses for the resulting predictions (Section 3); and the third step (update) assigns new probabilities to the AUX samples according to their position on the hypersphere (see Figure 2). "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "2. Hopfield Boosting achieves a new state-of-the-art in OOD detection. It improves the average false positive rate at $95\\%$ true positives (FPR95) from 2.28 to 0.92 on CIFAR-10, from 11.38 to 7.94 on CIFAR-100, and from 50.74 to 36.60 on ImageNet-1K. ", "page_idx": 1}, {"type": "text", "text": "3. We provide theoretical background that motivates Hopfield Boosting for OOD detection. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Some authors (e.g., Bishop, 1994; Roth et al., 2022; Yang et al., 2022) distinguish between anomalies, outliers, and novelties. These distinctions reflect different goals within applications (Ruff et al., 2021). For example, when an anomaly is found, it will usually be removed from the training pipeline. However, when a novelty is found it should be studied. We focus on the detection of samples that are not part of the training distribution and consider sample categorization as a downstream task. ", "page_idx": 1}, {"type": "text", "text": "Post-hoc OOD detection. A common and straightforward OOD detection approach is to use a post-hoc strategy, where one employs statistics obtained from a classifier. The perhaps most well known and simplest approach in this class is the Maximum Softmax Probability (MSP; Hendrycks & Gimpel, 2017), where one utilizes $p(\\textbf{\\em y}|\\textbf{\\em x})$ of the most likely class $y$ given a feature vector $\\pmb{x}\\in\\mathbb{R}^{D}$ to estimate whether a sample is OOD. Despite good empirical performances, this view is intrinsically limited, since OOD detection should focus on $p(x)$ (Morteza & Li, 2022). A wide range of post-hoc OOD detection approaches have been proposed to address the shortcomings of MSP (e.g., Lee et al., 2018; Hendrycks et al., 2019a; Liu et al., 2020; Sun et al., 2021, 2022; Wang et al., 2022; Zhang et al., 2023a; Djurisic et al., 2023; Liu et al., 2023; Xu et al., 2024). Most related to Hopfield Boosting is the work of Zhang et al. (2023a) \u2014 to our knowledge, they are the first to apply the MHE to OOD detection. Specifically, they use the ID data set to produce stored patterns and then use a modified version of MHE as the OOD score. While post-hoc approaches can be deployed out of the box on any model, a crucial limitation is that their performance heavily depends on the employed model itself. ", "page_idx": 1}, {"type": "text", "text": "Training methods. In contrast to post-hoc strategies, training-based methods modify the training process to improve the model\u2019s OOD detection capability (e.g., Hendrycks et al., 2019c; Tack et al., 2020; Sehwag et al., 2021; Du et al., 2022; Hendrycks et al., 2022; Wei et al., 2022a; Ming et al., 2023; ", "page_idx": 1}, {"type": "text", "text": "Tao et al., 2023; Lu et al., 2024). For example, Self-Supervised Outlier Detection (SSD; Sehwag et al., 2021) leverages contrastive self-supervised learning to train a model for OOD detection. ", "page_idx": 2}, {"type": "text", "text": "Auxiliary outlier data and outlier exposure. A third group of OOD detection approaches are outlier exposure (OE) methods. Like Hopfield Boosting, they incorporate AUX data in the training process to improve the detection of OOD samples (e.g., Hendrycks et al., 2019b; Liu et al., 2020; Ming et al., 2022; Zhang et al., 2023b; Wang et al., 2023a; Zhu et al., 2023; Jiang et al., 2024). We provide more detailed discussions on a range of OE methods in Appendix C.1. As far as we know, all OE approaches optimize an objective $(\\mathcal{L}_{\\mathrm{OOD}})$ , which aims at improving the model\u2019s discriminative power between ID and OOD data using the AUX data set as a stand-in for the OOD case. Hendrycks et al. (2019b) were the first to use the term OE to describe a more restrictive OE concept. Since their approach uses the MSP for incorporating the AUX data we refer to it as MSP-OE. Further, we refer to the OE approach introduced in Liu et al. (2020) as EBO-OE (to differentiate it from EBO, their post-hoc approach). In general, OE methods conceptualize the AUX data set as a large and diverse data set (e.g., ImageNet for vision tasks). As a consequence, usually, only a small subset of the samples bear semantic similarity to the ID data set \u2014 most data points are easily distinguishable from the ID data. Recent approaches therefore actively try to find informative samples for the training. The aim is to refine the decision boundary, ensuring the ID data is more tightly encapsulated (e.g., Chen et al., 2021; Ming et al., 2022). For example, Posterior Sampling-based Outlier Mining (POEM; Ming et al., 2022) selects samples close to the decision boundary using Thompson sampling: They first sample a linear decision boundary between ID and AUX data and then select those data instances which are closest to the sampled decision boundary. Hopfield Boosting also makes use of samples close to the boundary by giving them higher weights for the boosting step. ", "page_idx": 2}, {"type": "text", "text": "Continuous modern Hopfield networks. MHNs are energy-based associative memory networks. They advance conventional Hopfield networks (Hopfield, 1984) by introducing continuous queries and states with the MHE as a new energy function. MHE leads to exponential storage capacity, while retrieval is possible with a one-step update (Ramsauer et al., 2021). The update rule of MHNs coincides with attention as it is used in the Transformer (Vaswani et al., 2017). Examples for successful applications of MHNs are Widrich et al. (2020); F\u00fcrst et al. (2022); Sanchez-Fernandez et al. (2022); Paischer et al. (2022); Sch\u00e4fl et al. (2022); Schimunek et al. (2023) and Auer et al. (2023). Section 3.2 gives an introduction to MHE for OOD detection. For further details on MHNs, we refer to Appendix A. ", "page_idx": 2}, {"type": "text", "text": "Boosting for classification. Boosting, in particular, AdaBoost (Freund & Schapire, 1995), is an ensemble learning technique for classification. It is designed to focus ensemble members toward data instances that are hard to classify by assigning them higher weights. These challenging instances often lie near the maximum margin hyperplane (R\u00e4tsch et al., 2001), akin to support vectors in support vector machines (SVMs; Cortes & Vapnik, 1995). Popular boosting methods include Gradient boosting (Breiman, 1997), LogitBoost (Friedman et al., 2000), and LPBoost (Demiriz et al., 2002). ", "page_idx": 2}, {"type": "text", "text": "Radial basis function networks. Radial basis function networks (RBF networks; Moody & Darken, 1989) are function approximators of the form ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\varphi(\\pmb\\xi)=\\sum_{i=1}^{N}\\omega_{i}\\exp\\left(-\\frac{||\\pmb\\xi-\\pmb\\mu_{i}||_{2}^{2}}{2\\sigma_{i}^{2}}\\right),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\omega_{i}$ are linear weights, $\\pmb{\\mu}_{i}$ are the component means and $\\sigma_{i}^{2}$ are the component variances. RBF networks can be described as a weighted linear superposition of $N$ radial basis functions and have previously been used as hypotheses for boosting (R\u00e4tsch et al., 2001). If the linear weights are strictly positive, RBF networks can be viewed as an unnormalized weighted mixture of Gaussian distributions $\\bar{p}_{i}(\\pmb{\\xi})=\\mathcal{N}(\\pmb{\\xi};\\pmb{\\mu}_{i},\\sigma_{i}^{2}\\pmb{I})$ with $i=\\{1,\\dots,N\\}$ . Appendix H.1 explores the connection between RBF networks and MHNs via Gaussian mixtures in more depth. We refer to Bishop (1995) and M\u00fcller et al. (1997) for more general information on RBF networks. ", "page_idx": 2}, {"type": "text", "text": "3 Method ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "This section presents Hopfield Boosting: First, we formalize the OOD detection task. Second, we give an overview of the MHE and why it is suitable for OOD detection. Finally, we introduce the AUX-based boosting framework. Figure 1 shows a summary of the Hopfield Boosting concept. ", "page_idx": 3}, {"type": "text", "text": "3.1 Classification and OOD Detection ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Consider a multi-class classification task denoted as $(X^{\\mathcal{D}},Y_{-}^{\\mathcal{D}},\\mathcal{D})$ , where $\\pmb{X}^{\\mathcal{D}}\\in\\mathbb{R}^{D\\times N}$ represents a set of $N\\;D.$ -dimensional feature vectors $(x_{1}^{D},x_{2}^{D},\\cdot\\cdot\\cdot,x_{N}^{D})$ , which are i.i.d. samples $x_{i}^{\\hat{D}}\\sim p_{\\mathrm{ID}}$ $\\mathbf{Y}^{\\mathcal{D}}\\in\\mathcal{Y}^{N}$ denotes the labels associated with these feature vectors, and $\\boldsymbol{\\wp}$ is a set containing possible classes $(||\\mathcal{Y}||=K$ signifies the number of distinct classes). We consider observations $\\pmb{\\xi}^{\\mathcal{D}}\\in\\mathbb{R}^{D}$ that deviate considerably from the data generation $p_{\\mathrm{ID}}(\\pmb{\\xi}^{\\mathcal{D}})$ that defines the \u201cnormality\u201d of our data as OOD. Following Ruff et al. (2021), an observation is OOD if it pertains to the set ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbb{O}\\;=\\;\\{\\pmb{\\xi}^{D}\\in\\mathbb{R}^{D}\\;\\vert\\;p_{\\mathrm{ID}}(\\pmb{\\xi}^{D})<\\epsilon\\}\\;\\mathrm{where}\\;\\epsilon\\ge0.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Since the probability density of the data generation $p_{\\mathrm{ID}}$ is in general not known, one needs to estimate $p_{\\mathrm{ID}}(\\pmb{\\xi}^{\\mathcal{D}})$ . In practice, it is common to define an outlier score $s(\\xi)$ that uses an encoder $\\phi$ , where $\\pmb{\\xi}=\\phi(\\pmb{\\xi}^{D})$ . The outlier score should \u2014 in the best case \u2014 preserve the density ranking. In contrast to a density estimation, the score $s(\\pmb\\xi)$ does not have to fulflil all requirements of a probability density (like proper normalization or non-negativity). Given $s(\\xi)$ and $\\phi$ , OOD detection can be formulated as a binary classification task with the classes ID and OOD: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\hat{B}(\\pmb\\xi^{\\mathcal D},\\gamma)\\;=\\;\\left\\{\\vphantom{\\sum_{000}^{\\infty}}\\mathrm{ID}\\;\\mathrm{\\ensuremath{\\operatorname{if}}\\ }s(\\phi(\\pmb\\xi^{\\mathcal D}))\\geq\\gamma\\right.\\mathrm{\\;.}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "It is common to choose the threshold $\\gamma$ so that a portion of $95\\%$ of $\\mathrm{ID}$ samples from a previously unseen validation set are correctly classified as ID. However, metrics like the area under the receiver operating characteristic (AUROC) can be directly computed on $s(\\xi)$ without specifying $\\gamma$ since the AUROC computation sweeps over the threshold. ", "page_idx": 3}, {"type": "text", "text": "3.2 Modern Hopfield Energy ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The log-sum-exponential (lse) function is defined as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\sec(\\beta,z)=\\ \\beta^{-1}\\ \\log\\left(\\sum_{i=1}^{N}\\exp(\\beta z_{i})\\right),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\beta$ is the inverse temperature and $z\\in\\mathbb{R}^{N}$ is a vector. The lse can be seen as a soft approximation to the maximum function: As $\\beta\\to\\infty$ , the lse approaches $\\operatorname*{max}_{i}z_{i}$ . ", "page_idx": 3}, {"type": "text", "text": "Given a set of $N\\,d\\cdot$ -dimensional stored patterns $(x_{1},x_{2},\\ldots,x_{N})$ arranged in a data matrix $\\mathbf{\\deltaX}$ , and a $d$ -dimensional query $\\xi$ , the MHE is defined as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname{E}(\\pmb{\\xi};\\pmb{X})\\;=\\;-\\,\\operatorname{lse}(\\beta,\\pmb{X}^{T}\\pmb{\\xi})+\\;{\\frac{1}{2}}\\,\\pmb{\\xi}^{T}\\pmb{\\xi}\\;+\\;C,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $C=\\beta^{-1}\\log N\\,+\\frac{1}{2}M^{2}$ and $M$ is the largest norm of a pattern: $M=\\operatorname*{max}_{i}||x_{i}||$ . $\\mathbf{\\deltaX}$ is also called the memory of the MHN. Intuitively, Equation (5) can be explained as follows: The dot-product within the lse computes a similarity for a given $\\pmb{\\xi}\\in\\mathbb{R}^{d}$ to all patterns in the memory $X\\in\\mathbb{R}^{d\\times N}$ . The lse function aggregates the similarities to form a single value, where the $\\beta$ parameterizes the aggregation operation: If $\\beta\\to\\infty$ , the maximum similarity of $\\xi$ to the patterns in $\\mathbf{\\deltaX}$ is returned. ", "page_idx": 3}, {"type": "text", "text": "rTaow u sdea ttah ien sMtaHncEe fs $(\\pmb{x}_{1}^{D},\\pmb{x}_{2}^{D},\\dots,\\pmb{x}_{N}^{D})$ H oofp tfhieel Id DB doaotsat isnegt  aarcrqaunigreesd  tihn et hme edmatoar ym patartitxe $X^{\\mathcal{D}}\\in\\dot{\\mathbb{R}}^{D\\times N}$ $\\mathbf{\\deltaX}$ ntgo an encoder $\\phi:\\mathbb{R}^{D}\\to\\mathbb{R}^{d}$ (e.g., ResNet): $\\pmb{x}_{i}=\\phi(\\pmb{x}_{i}^{D})$ . We denote the component-wise application of $\\phi$ to the patterns in $X^{\\mathcal{D}}$ as $X=\\phi(X^{\\mathcal{D}})$ . Similarly, a raw query $\\pmb{\\xi}^{D}\\in\\mathbb{R}^{D}$ is fed through the encoder to obtain the query pattern: $\\pmb{\\xi}=\\phi(\\pmb{\\xi}^{D})$ . One can now use $\\operatorname{E}(\\xi;X)$ to estimate whether $\\xi$ is $\\mathrm{ID}$ or OOD: A low energy indicates $\\xi$ is $\\mathrm{ID}$ , and a high energy signifies that $\\xi$ is OOD. ", "page_idx": 3}, {"type": "text", "text": "3.3 Boosting Framework ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Sampling of informative outlier data. Hopfield Boosting uses AUX data to learn a decision boundary between the ID and OOD region during the training. Similar to Chen et al. (2021) and Ming et al. (2022), Hopfield Boosting selects informative outliers close to the ID-OOD decision boundary. For this selection, Hopfield Boosting weights the AUX data similar to AdaBoost (Freund & Schapire, 1995) by sampling data instances close to the decision boundary more frequently. We consider samples close to the decision boundary as weak learners \u2014 their nearest neighbors consist of samples from their own class as well as from the foreign class. An individual weak learner represents a classifier that is only slightly better than random guessing (Figure 6). Vice versa, a strong learner can be created by forming an ensemble of a set of weak learners (Figure 2). ", "page_idx": 4}, {"type": "text", "text": "We denote the matrix containing the raw AUX data instances $(\\sigma_{1}^{\\mathcal{D}},\\sigma_{2}^{\\mathcal{D}},\\ldots,\\sigma_{N}^{\\mathcal{D}})$ as $O^{\\mathcal{D}}\\in\\mathbb{R}^{D\\times M}$ , and the memory containing the encoded AUX patterns as $O\\,=\\,\\phi(O^{D})$ . The boosting process proceeds as follows: There exists a weight $(w_{1},w_{2},\\dots,w_{N})$ for each data point in $O^{\\mathcal{\\breve{D}}}$ and the individual weights are aggregated into the weight vector $\\pmb{w}_{t}$ . Hopfield Boosting uses these weights to draw mini-batches $O_{s}^{\\mathcal{D}}$ from $O^{\\mathcal{D}}$ for training, where weak learners are sampled more frequently. ", "page_idx": 4}, {"type": "text", "text": "We introduce an MHE-based energy function which Hopfield Boosting uses to determine how weak a specific learner $\\xi$ is (with higher energy indicating a weaker learner): ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname{E}_{b}({\\boldsymbol\\xi};{\\boldsymbol{X}},{\\boldsymbol{O}})\\;=\\;-\\;2\\operatorname{lse}(\\beta,({\\boldsymbol{X}}\\parallel{\\boldsymbol{O}})^{T}{\\boldsymbol\\xi})\\;+\\;\\operatorname{lse}(\\beta,X^{T}{\\boldsymbol\\xi})\\;+\\;\\operatorname{lse}(\\beta,O^{T}{\\boldsymbol\\xi}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\pmb{X}\\,\\in\\,\\mathbb{R}^{d\\times N}$ contains ID patterns, $O\\,\\in\\,\\mathbb{R}^{d\\times M}$ contains AUX patterns, and $(X\\parallel O)\\in$ $\\mathbb{R}^{d\\times(N+M)}$ denotes the concatenated data matrix containing the patterns from both $\\mathbf{\\deltaX}$ and $o$ . Before computing $\\operatorname{E}_{b}$ , we normalize the feature vectors in $X,o$ , and $\\xi$ to unit length. Figure 3 displays the energy landscape of $\\operatorname{E}_{b}({\\boldsymbol{\\xi}};X,{\\boldsymbol{O}})$ using exemplary data on a 3-dimensional sphere. $\\operatorname{E}_{b}$ is maximal at the decision boundary between ID and AUX data and decreases with increasing distance from the decision boundary in both directions. ", "page_idx": 4}, {"type": "text", "text": "As we show in our theoretical discussion in Appendix G, when modeling the class-conditional densities of the $\\mathrm{ID}$ and AUX data set as mixtures of Gaussian distributions ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{p({\\boldsymbol{\\xi}}\\mid\\mathrm{{ID}})=\\displaystyle\\frac{1}{N}\\sum_{i=1}^{N}\\mathcal{N}({\\boldsymbol{\\xi}};\\boldsymbol{x}_{i},\\beta^{-1}\\boldsymbol{I}),}\\\\ {p({\\boldsymbol{\\xi}}\\mid\\mathrm{{AUX}})=\\displaystyle\\frac{1}{N}\\sum_{i=1}^{N}\\mathcal{N}({\\boldsymbol{\\xi}};\\boldsymbol{o}_{i},\\beta^{-1}\\boldsymbol{I}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "with equal class priors $p(\\mathrm{ID})=p(\\mathrm{AUX})=1/2$ and normalized patterns $||\\pmb{x}_{i}||=1$ and $||\\pmb{\\omega}_{i}||=1$ , we obtain $\\operatorname{E}_{b}({\\pmb\\xi};{\\pmb X},{\\pmb O})\\stackrel{C}{=}\\beta^{-1}\\log(p(\\mathrm{~ID~}|\\textbf{\\^{\\xi}})\\cdot p(\\mathrm{~AUX~}|\\textbf{\\^{\\xi}}))$ , where $\\underline{{\\underline{{C}}}}$ denotes equality up to an irrelevant additive constant. The exponential of $\\operatorname{E}_{b}$ is the variance of a Bernoulli random variable with the outcomes $\\{\\mathrm{ID},\\mathrm{AUX}\\}$ conditioned on $\\xi$ . Thus, according to $\\operatorname{E}_{b}$ , the weak learners are situated at locations where the model defined in Equations (7) and (8) is uncertain. ", "page_idx": 4}, {"type": "text", "text": "Given a set of query values $(\\xi_{1},\\xi_{2},\\ldots,\\xi_{n})$ assembled in a query matrix $\\Xi\\in\\mathbb{R}^{d\\times n}$ , we denote a vector of energies $e\\in\\mathbb{R}^{n}$ with $e_{i}=\\mathrm{E}_{b}(\\pmb{\\xi}_{i};\\pmb{X},\\pmb{O})$ as ", "page_idx": 4}, {"type": "equation", "text": "$$\ne=\\mathrm{E}_{b}(\\Xi;X,O).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "To calculate the weights $\\pmb{w}_{t+1}$ , we use the memory of AUX patterns as a query matrix $\\Xi=O$ and compute the respective energies $\\operatorname{E}_{b}$ of those patterns. The resulting energy vector $\\mathrm{E}_{b}(\\Xi;X,O)$ is then normalized by a softmax. This computation provides the updated weights: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\pmb{w}_{t+1}=\\mathrm{softmax}(\\beta\\mathrm{E}_{b}(\\Xi;X,O)).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Appendix J provides theoretical background on how informative samples close to the decision boundary are beneficial for training an OOD detector. ", "page_idx": 4}, {"type": "image", "img_path": "VLQYtVMTYz/tmp/4f238b68f8e519870a81f499e23c22bacd0d5dec1c3db4e82458d3bf821900ca.jpg", "img_caption": ["Figure 2: Synthetic example of the adaptive resampling mechanism. Hopfield Boosting forms a strong learner by sampling and combining a set of weak learners close to the decision boundary. The heatmap on the background shows $\\exp(\\beta\\mathrm{E}_{b}(\\xi;X,O))$ , where $\\beta$ is 60. Only the sampled (i.e., highlighted) points serve as memories $\\mathbf{\\deltaX}$ and $o$ . "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Training the model with MHE. In this section, we introduce how Hopfield Boosting uses the sampled weak learners to improve the detection of patterns outside the training distribution. We follow the established training method for OE (Hendrycks et al., 2019b; Liu et al., 2020; Ming et al., 2022): Train a classifier on the ID data using the standard cross-entropy loss and add an OOD loss that uses the AUX data set to sharpen the decision boundary between the ID and OOD regions. Formally, this yields the loss ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}=\\mathcal{L}_{\\mathrm{CE}}+\\lambda\\mathcal{L}_{\\mathrm{OOD}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\lambda$ is a hyperparamter indicating the relative importance of $\\mathcal{L}_{\\mathrm{ooD}}$ . Hopfield Boosting explicitly minimizes $\\operatorname{E}_{b}$ (which is also the energy function Hopfield Boosting uses to sample weak learners). Given the weight vector $\\pmb{w}_{t}$ , and the data sets $X^{\\mathcal{D}}$ and $O^{\\mathcal{D}}$ , we obtain a mini-batch $X_{s}^{\\mathcal{D}}$ containing $_\\mathrm{N}$ samples from $X^{\\mathcal{D}}$ by uniform sampling, and a mini-batch of $\\Nu$ weak learners $\\breve{O}_{s}^{\\mathcal{D}}$ from $O^{\\mathcal{\\breve{D}}}$ by sampling according to $\\pmb{w}_{t}$ with replacement. We then feed the respective mini-batches into the neural network $\\phi_{\\mathrm{base}}$ to create a latent feature (in our experiments, we always use the feature of the penultimate layer of a ResNet). Our proposed approach then uses two heads: ", "page_idx": 5}, {"type": "text", "text": "1. A linear classification head that maps the latent feature to the class logits for $\\mathcal{L}_{\\mathrm{CE}}$ .   \n2. A 2-layer MLP $\\phi_{\\mathrm{proj}}$ maps the features from the penultimate layer to the output for $\\mathcal{L}_{\\mathrm{ooD}}$ . ", "page_idx": 5}, {"type": "text", "text": "Hopfield Boosting computes $\\mathcal{L}_{\\mathrm{ooD}}$ on the representations it obtains from $\\phi=\\phi_{\\mathrm{proj}}\\circ\\phi_{\\mathrm{base}}$ as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{0OD}}=\\frac{1}{2N}\\sum_{\\xi}\\mathrm{E}_{b}(\\pmb{\\xi};\\pmb{X}_{s},\\pmb{O}_{s}),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where the memories $X_{s}$ and $O_{s}$ contain the encodings of the sampled data instances: $X_{s}=\\phi(X_{s}^{\\mathcal{D}})$ and $O_{s}\\,=\\,\\phi(O_{s}^{D})$ . The sum is taken over the observations $\\xi$ , which are drawn from $({\\cal X}_{s}\\parallel{\\cal O}_{s})$ Hopfield Boosting computes $\\mathcal{L}_{\\mathrm{ooD}}$ for each mini-batch by first calculating the pairwise similarity matrix between the patterns in the mini-batch, followed by determining the $\\operatorname{E}_{b}$ values of the individual observations $\\xi$ , and, finally a mean reduction. To the best of our knowledge, Hopfield Boosting is the first method that uses Hopfield networks in this way to train a deep neural network. We note that there is a relation between Hopfield Boosting and SVMs with an RBF kernel (see Appendix H.3). However, the optimization procedure of SVMs is in general not differentiable. In contrast, our novel energy function is fully differentiable. This allows us to use it to train neural networks. ", "page_idx": 5}, {"type": "text", "text": "Summary. Algorithm 1 provides an outline of Hopfield Boosting. Each iteration $t$ consists of three main steps: 1. weight, 2. evaluate, and 3. update. First, Hopfield Boosting samples a mini-batch from the ID data and weights the AUX data by sampling a mini-batch according to $\\pmb{w}_{t}$ . Second, Hopfield Boosting evaluates the composite loss on the sampled mini-batch. Third, Hopfield Boosting updates the model parameters and, every $N$ -th step, also the sampling weights for the AUX data set $\\pmb{w}_{t+1}$ . ", "page_idx": 5}, {"type": "text", "text": "Inference. At inference time, the OOD score $s(\\pmb\\xi)$ is ", "page_idx": 5}, {"type": "equation", "text": "$$\ns(\\pmb\\xi)\\;=\\;\\mathrm{lse}(\\beta,X^{T}\\pmb\\xi)\\;-\\;\\mathrm{lse}(\\beta,O^{T}\\pmb\\xi).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Require: $T,N,X,O,Y,\\mathcal{L}_{\\mathrm{CE}},\\mathrm{E}_{b},\\beta$ Set all weights $w_{1}$ to $1/|O|$ for $t=1$ to $T$ do 1. Weight. Get hypothesis ${\\cal X}_{s}\\parallel{\\cal O}_{s}\\rightarrow\\{\\mathrm{ID},\\mathrm{AUX}\\}$ : 1.a. Mini-batch sampling $X_{s}$ from $\\mathbf{\\deltaX}$ , and 1.b. Sub-sampling of weak learners $O_{s}$ from $o$ according to the weighting $\\pmb{w}_{t}$ . 2. Evaluate. Compute loss from Equation (11) on $X_{s}$ and $O_{s}$ . 3. Update. Update model for the next iteration: 3.a. At every step, update the full model (backbone, classification head, and MHE). 3.b. At every $t*N$ step calculate new weights for $^o$ with $\\pmb{w}_{t+1}=\\operatorname{softmax}(\\beta\\mathrm{E}_{b}(\\pmb{O};\\pmb{X},\\pmb{O}))$ . end for ", "page_idx": 6}, {"type": "text", "text": "For computing $s(\\xi)$ , Hopfield Boosting uses the 50,000 random samples from the ID and AUX data sets, respectively. As we show in Appendix I.8, this step entails only a very moderate computational overhead in relation to a complete forward pass (e.g., an overhead of $7.5\\%$ for ResNet-18 on an NVIDIA Titan V GPU with 50,000 patterns stored in each of the memories $\\mathbf{\\deltaX}$ and $^o$ ). We additionally experimented with using only $\\mathrm{lse}({\\bar{\\beta}},X^{T}{\\pmb\\xi})$ as a score, which also gives reasonable results. However, the approach in Equation (13) has turned out to be superior. Equation (13) uses information from both ID and AUX samples. This can, for example, be beneficial for handling query patterns $\\xi$ that are dissimilar from both the memory patterns in $\\mathbf{\\deltaX}$ as well as from the memory patterns in $o$ . ", "page_idx": 6}, {"type": "text", "text": "3.4 Comparison of Hopfield Boosting to HE and SHE ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Zhang et al. (2023a) propose two post-hoc methods for OOD detection with MHE:\u201cHopfield Energy\u201d (HE) and \u201cSimplified Hopfield Energy\u201d (SHE). In contrast to Hopfield Boosting, HE and SHE do not use AUX data to get a better boundary between ID and OOD data. Rather, their methods evaluate the MHE on ID patterns only to determine whether a sample is ID or OOD. Additional differences include the selection of patterns stored in the memory or the normalization of the patterns. The OE process of Hopfield Boosting drastically improves the OOD detection performance compared to HE and SHE. We verify that the unique contributions of Hopfield Boosting (like the energy-based loss and the boosting process) are responsible for the superior performance with two extensions of HE that include AUX data (the comparison can be found in Appendix I.9). For further information on the differences to HE and SHE, we refer to Appendix H.4. ", "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "4.1 Toy Example ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "This section presents a toy example illustrating the main intuitions behind Hopfield Boosting. For the sake of clarity, the toy example does not consider the inlier classification task that would induce secondary processes, which would obscure the explanations. Formally, we do not consider the first term on the right-hand side of Equation (11). For further toy examples, we refer to Appendix F. ", "page_idx": 6}, {"type": "text", "text": "Figure 2 demonstrates how the weighting in Hopfield Boosting allows good estimations of the decision boundary, even if Hopfield Boosting only samples a small number of weak learners. This is advantageous because the AUX data set contains a large number of data instances that are uninformative for the OOD detection task. For small, low dimensional data, one can always use all the data to compute $\\operatorname{E}_{b}$ (Figure 2, a). For large problems (like in Ming et al., 2022), this strategy is difficult, and the naive solution of uniformly sampling N data points would also not work. This will yield many uninformative points (Figure 2, b). When using Hopfield Boosting and sampling N weak learners according to $\\pmb{w}_{t}$ , the result better approximates the decision boundary of the full data (Figure 2, c). ", "page_idx": 6}, {"type": "table", "img_path": "VLQYtVMTYz/tmp/c2b2b7aeace5408a6ee7e51a34f3b667e1281cea304b19b76b328d79c4c4dcb4.jpg", "table_caption": ["Table 1: OOD detection performance on CIFAR-10. We compare results from Hopfield Boosting, DOS (Jiang et al., 2024), DOE (Wang et al., 2023b), DivOE (Zhu et al., 2023), DAL (Wang et al., 2023a), MixOE (Zhang et al., 2023b), POEM (Ming et al., 2022), EBO-OE (Liu et al., 2020), and MSP-OE (Hendrycks et al., 2019b) on ResNet-18. $\\downarrow$ indicates \u201clower is better\u201d and $\\uparrow$ \u201chigher is better\u201d. All values in $\\%$ . Standard deviations are estimated across five training runs. "], "table_footnote": ["ID Accuracy \u2191 $94.02^{\\pm0.09}$ $94.74^{\\pm0.13}$ $94.93^{\\pm0.12}$ $94.72^{\\pm0.17}$ 95.11\u00b10.05 96.60\u00b11.50 89.20\u00b11.30 91.32\u00b10.35 94.83\u00b10.23 "], "page_idx": 7}, {"type": "text", "text": "Table 2: OOD detection performance on ImageNet-1K. We compare results from Hopfield Boosting, DOS (Jiang et al., 2024), DOE (Wang et al., 2023b), DivOE (Zhu et al., 2023), DAL (Wang et al., 2023a), MixOE (Zhang et al., 2023b), POEM (Ming et al., 2022), EBO-OE (Liu et al., 2020), and MSP-OE (Hendrycks et al., 2019b) on ResNet-50. $\\downarrow$ indicates \u201clower is better\u201d and $\\uparrow$ \u201chigher is better\u201d. All values in $\\%$ . Standard deviations are estimated across five training runs. ", "page_idx": 7}, {"type": "table", "img_path": "VLQYtVMTYz/tmp/bf4d2415ae8f252ab084693898736850741248601dec540df449e39d2cecce95.jpg", "table_caption": [], "table_footnote": ["ID Accuracy \u2191 76.30\u00b10.04 76.04\u00b10.02 64.36\u00b16.94 74.86\u00b10.03 75.46\u00b10.04 75.47\u00b10.20 $75.66^{\\pm0.05}$ $75.64^{\\pm0.09}$ 75.71\u00b10.03 "], "page_idx": 7}, {"type": "text", "text": "4.2 Data & Setup", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "CIFAR-10 & CIFAR-100. Our training and evaluation proceeds as follows: We train Hopfield Boosting with ResNet-18 (He et al., 2016) on the CIFAR-10 and CIFAR-100 data sets (Krizhevsky, 2009), respectively. In these settings, we use ImageNet-RC (Chrabaszcz et al., 2017) (a low-resolution version of ImageNet) as the AUX data set. For testing the OOD detection performance, we use the data sets SVHN (Street View House Numbers) (Netzer et al., 2011), Textures (Cimpoi et al., 2014), iSUN (Xu et al., 2015), Places 365 (L\u00f3pez-Cifuentes et al., 2020), and two versions of the LSUN data set (Yu et al., 2015) \u2014 one where the images are cropped, and one where they are resized to match the resolution of the CIFAR data sets ( $32\\mathrm{x}32$ pixels). We refer to the two LSUN data sets as LSUN-Crop and LSUN-Resize, respectively. We compute the scores $s(\\xi)$ as described in Equation (13) and then evaluate the discriminative power of $s(\\pmb\\xi)$ between CIFAR and the respective OOD data set using the FPR95 and the AUROC. We use a validation process with different OOD data for model selection. Specifically, we validate the model on MNIST (LeCun et al., 1998), and ImageNet-RC with different pre-processing than in training (resize to 32x32 pixels instead of crop to $32\\mathrm{x}32$ pixels), as well as Gaussian and uniform noise. ", "page_idx": 7}, {"type": "text", "text": "ImageNet-1K. We evaluate Hopfield Boosting on the large-scale benchmark: We use ImageNet-1K (Russakovsky et al., 2015) as ID data set and ImageNet-21K (Ridnik et al., 2021) as AUX data set. The OOD test data sets are Textures (Cimpoi et al., 2014), SUN (Xu et al., 2015), Places 365 (L\u00f3pezCifuentes et al., 2020), and iNaturalist (Van Horn et al., 2018). In this setting, all images are scaled to a resolution of $224\\!\\!\\!\\times\\!224$ . To keep our method comparable to other OE methods, we closely follow the training and evaluation protocol of (Zhu et al., 2023). This implies htat we fine-tune a ResNet-50 that was pre-trained on the ImageNet-1K ID classification task (as provided by TorchVision, 2016). ", "page_idx": 7}, {"type": "text", "text": "Table 3: Ablated training procedures on CIFAR-10. We compare the result of Hopfield Boosting to the results of our method when not using weighted sampling, the projection head, or the OOD loss. $\\downarrow$ indicates \u201clower is better\u201d and $\\uparrow$ \u201chigher is better\u201d. All values in $\\%$ . Standard deviations are estimated across five training runs. ", "page_idx": 8}, {"type": "table", "img_path": "VLQYtVMTYz/tmp/16e038bc9ce40e9b1cfe78065855e75ed4feedb1da88528be09ee91f79b9091f.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Baselines. As mentioned earlier, previous works offer vast experimental evidence that OE methods offer superior OOD detection compared to methods without OE (see e.g., Ming et al., 2022; Wang et al., 2023a). Our experiments in Appendix I.14 confirm this. Thus, we focus on a comprehensive comparison of Hopfield Boosting to eight OE methods: MSP-OE (Hendrycks et al., 2019b), EBO-OE (Liu et al., 2020), POEM (Ming et al., 2022), MixOE (Zhang et al., 2023b), DAL (Wang et al., 2023a), DivOE (Zhu et al., 2023), DOE (Wang et al., 2023b) and DOS (Jiang et al., 2024). ", "page_idx": 8}, {"type": "text", "text": "Training setup. The network trains for 100 epochs (CIFAR-10/100) or 4 epochs (ImageNet-1K), respectively. In each epoch, the model processes the entire ID data set and a selection of AUX samples (sampled according to $\\pmb{w}_{t}$ ). We sample mini-batches of size 128 per data set, resulting in a combined batch size of 256. We evaluate the composite loss from Equation (11) for each resulting mini-batch and update the model accordingly. After an epoch, we update the sample weights, yielding $\\pmb{w}_{t+1}$ . For efficiency reasons, we only compute the weights for 500,000 AUX data instances ${\\sim}40\\%$ of ImageNet), which we denote as \u039e. The weights of the remaining samples are set to 0. During the sample weight update, Hopfield Boosting does not compute gradients or update model parameters. The update of the sample weights $\\pmb{w}_{t+1}$ proceeds as follows: First, we fill the memories $\\mathbf{\\deltaX}$ and $o$ with $50{,}000\\,\\mathrm{ID}$ samples and 50,000 AUX samples, respectively. Second, we use the obtained $\\mathbf{\\deltaX}$ and $o$ to get the energy $\\mathrm{E}_{b}(\\Xi;X,O)$ for each of the 500,000 AUX samples in $\\Xi$ and compute $\\pmb{w}_{t+1}$ according to Equation (10). In the following epoch, Hopfield Boosting samples the mini-batches $O_{s}^{\\mathcal{D}}$ according to $\\pmb{w}_{t+1}$ with replacement. To allow the storage of even more patterns in the Hopfield memory during the weight update process, one could incorporate a vector similarity engine (e.g., Douze et al., 2024) into the process. This would potentially allow a less noisy estimate of the sample weights. For the sake of simplicity, we did not opt to do this in our implementation of Hopfield Boosting. As we show in section 4.3, Hopfield Boosting achieves state-of-the-art OOD detection results and can scale to large datasets (ImageNet-1K) even without access to a similarity engine. ", "page_idx": 8}, {"type": "text", "text": "Hyperparameters & Model Selection. Like Yang et al. (2022), we use SGD with an initial learning rate of 0.1 and a weight decay of $5\\cdot10^{-4}$ . We decrease the learning rate during the training process with a cosine schedule (Loshchilov & Hutter, 2016). Appendix I.2 describes the image transformations and pre-processing. We apply optimizer, weight decay, learning rate, scheduler, and transformations consistently to all OOD detection methods of the comparison. For training Hopfield Boosting, we use a single value for $\\beta$ throughout the training and evaluation process and for all OOD data sets. For model selection, we use a grid search with $\\lambda$ , chosen from the set $\\{0.1,0.25,0.5,1.0\\}$ , and $\\beta$ , chosen from the set $\\{2,4,8,16,32\\}$ . From these hyperparameter configurations, we select the model with the lowest mean FPR95 metric (where the mean is taken over the validation OOD data sets) and do not consider the ID classification accuracy for model selection. In our experiments, $\\beta=4$ and $\\lambda=0.5$ yields the best results for CIFAR-10 and CIFAR-100. For ImageNet-1K, we set $\\beta=32$ and $\\lambda=0.25$ . ", "page_idx": 8}, {"type": "text", "text": "4.3 Results & Discussion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Table $1$ summarizes the results for CIFAR-10. Hopfield Boosting achieves equal or better performance compared to the other methods regarding the FPR95 metric for all OOD data sets. It surpasses POEM (the previously best OOD detection approach with OE in our comparison), improving the mean FPR95 metric from 2.28 to 0.92. On CIFAR-100 (Appendix I.1), Hopfield Boosting improves the mean FPR95 metric from 11.76 to 7.94. It is notable that all methods achieve perfect FPR95 results on the LSUN-Resize and iSUN data sets. This is somewhat problematic since there exists evidence that the LSUN-Resize data set can give misleading results due to image artifacts resulting from the resizing procedure (Tack et al., 2020; Yang et al., 2022). We hypothesize that a similar issue exists with the iSUN data set, as in our experiments, LSUN-Resize and iSUN behave very similarly. ", "page_idx": 9}, {"type": "text", "text": "On ImageNet-1K (Table 2), Hopfield Boosting surpasses all methods in our comparison in terms of both mean FPR95 and mean AUROC. Compared to POEM (the previously best method) Hopfield Boosting improves the mean FPR95 from 50.74 to 36.60. This demonstrates that Hopfield Boosting scales very favourably to large-scale settings. ", "page_idx": 9}, {"type": "text", "text": "We observe that all methods tested perform worst on the Places 365 data set. To gain more insights regarding this behavior, we look at the data instances from the Places 365 data set that Hopfield Boosting trained on CIFAR-10 most confidently classifies as in-distribution (i.e., which receive the highest scores $s(\\pmb{\\xi})$ ). Visual inspection shows that among those images, a large portion contains objects from semantic classes included in CIFAR-10 (e.g., airplanes, horses, automobiles). We refer to Appendix I.6 for more details. ", "page_idx": 9}, {"type": "text", "text": "We evaluate the performance of the following 3 ablated training procedures on the CIFAR-10 benchmark to gauge the importance of the individual contributions of Hopfield Boosting: (a) Random sampling instead of weighted sampling, (b) Random sampling instead of weighted sampling and no projection head, (c) No application of $\\mathcal{L}_{\\mathrm{ooD}}$ . The results (Table 3) show that all contributions (i.e, weighted sampling, the projection head, and $\\mathcal{L}_{\\mathrm{OOD}}$ ) are important factors for Hopfield Boosting \u2019s performance. For additional ablations, we refer to Appendix I. ", "page_idx": 9}, {"type": "text", "text": "When subjecting Hopfield Boosting to data sets that were designed to show the weakness of OOD detection approaches (Appendix I.7), we identify instances where a substantial number of outliers are wrongly classified as inliers. Testing with EBO-OE yields comparable outcomes, indicating that this phenomenon extends beyond Hopfield Boosting. ", "page_idx": 9}, {"type": "text", "text": "5 Limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Lastly, we would like to discuss two limitations that we found: First, we see an opportunity to improve the evaluation procedure for OOD detection. Specifically, it remains unclear how reliably the performance on specific data sets can indicate the general ability to detect OOD inputs. Our results from iSUN and LSUN-Resize (Section 4.3) indicate that issues like image artifacts in data sets greatly influence model evaluation. Second, although OE-based approaches improve the OOD detection capability, their reliance on AUX data can limit their applicability. For one, the selection of the AUX data is crucial (since it determines the characteristics of the decision boundary surrounding the inlier data). Furthermore, the use of AUX data can be prohibitive in domains where only a few or no outliers at all are available for training the model. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We introduce Hopfield Boosting: an approach for OOD detection with OE. Hopfield Boosting uses an energy term to boost a classifier between inlier and outlier data by sampling weak learners that are close to the decision boundary. We illustrate how Hopfield Boosting shapes the energy surface to form a decision boundary. Additionally, we demonstrate how the boosting mechanism creates a sharper decision boundary than with random sampling. We compare Hopfield Boosting to eight modern OOD detection approaches using OE. Overall, Hopfield Boosting shows the best results. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "We thank Christian Huber for helpful feedback and fruitful discussions. ", "page_idx": 10}, {"type": "text", "text": "The ELLIS Unit Linz, the LIT AI Lab, the Institute for Machine Learning, are supported by the Federal State Upper Austria. We thank the projects AI-MOTION (LIT-2018-6-YOU-212), DeepFlood (LIT2019-8-YOU-213), Medical Cognitive Computing Center (MC3), INCONTROL-RL (FFG-881064), PRIMAL (FFG-873979), S3AI (FFG-872172), DL for GranularFlow (FFG-871302), EPILEPSIA (FFG-892171), AIRI FG 9-N (FWF-36284, FWF-36235), AI4GreenHeatingGrids(FFG- 899943), INTEGRATE (FFG-892418), ELISE (H2020-ICT-2019-3 ID: 951847), Stars4Waters (HORIZONCL6-2021-CLIMATE-01-01). We thank Audi.JKU Deep Learning Center, TGW LOGISTICS GROUP GMBH, Silicon Austria Labs (SAL), University SAL Labs initiative, FILL Gesellschaft mbH, Anyline GmbH, Google, ZF Friedrichshafen AG, Robert Bosch GmbH, UCB Biopharma SRL, Merck Healthcare KGaA, Verbund AG, GLS (Univ. Waterloo) Software Competence Center Hagenberg GmbH, T\u00dcV Austria, Frauscher Sensonic, Borealis AG, TRUMPF and the NVIDIA Corporation. This work has been supported by the \"University SAL Labs\" initiative of Silicon Austria Labs (SAL) and its Austrian partner universities for applied fundamental research for electronic-based systems. Daniel Klotz acknowledges funding from the Helmholtz Initiative and Networking Fund (Young Investigator Group COMPOUNDX, grant agreement no. VH-NG-1537) We acknowledge EuroHPC Joint Undertaking for awarding us access to Karolina at IT4Innovations, Czech Republic and Leonardo at CINECA, Italy. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Abbott, L. F. and Arian, Y. Storage capacity of generalized networks. Physical review A, 36(10): 5091, 1987.   \nAhn, S., Korattikara, A., and Welling, M. Bayesian posterior sampling via stochastic gradient Fisher scoring. In Proceedings of the 29th International Coference on International Conference on Machine Learning, pp. 1771\u20131778, Madison, WI, USA, 2012. Omnipress.   \nAnderson, B. G. and Sojoudi, S. Certified robustness via locally biased randomized smoothing. In Learning for Dynamics and Control Conference, pp. 207\u2013220. PMLR, 2022.   \nAuer, A., Gauch, M., Klotz, D., and Hochreiter, S. Conformal prediction for time series with modern hopfield networks. Advances in Neural Information Processing Systems, 36:56027\u201356074, 2023.   \nBaldi, P. and Venkatesh, S. S. Number of stable points for spin-glasses and neural networks of higher orders. Physical Review Letters, 58(9):913, 1987.   \nBishop, C. Neural Networks for Pattern Recognition. Oxford University Press, 1995.   \nBishop, C. M. Novelty detection and neural network validation. IEE Proceedings-Vision, Image and Signal processing, 141(4):217\u2013222, 1994.   \nBreiman, L. Arcing the edge. Technical report, Citeseer, 1997.   \nCaputo, B. and Niemann, H. Storage capacity of kernel associative memories. In Artificial Neural Networks\u2014ICANN 2002: International Conference Madrid, Spain, August 28\u201330, 2002 Proceedings 12, pp. 51\u201356. Springer, 2002.   \nChen, H., Lee, Y., Sun, G., Lee, H., Maxwell, T., and Giles, C. L. High order correlation model for associative memory. In AIP Conference Proceedings, volume 151, pp. 86\u201399. American Institute of Physics, 1986.   \nChen, J., Li, Y., Wu, X., Liang, Y., and Jha, S. Atom: Robustifying out-of-distribution detection using outlier mining. In Machine Learning and Knowledge Discovery in Databases. Research Track: European Conference, ECML PKDD 2021, Bilbao, Spain, September 13\u201317, 2021, Proceedings, Part III 21, pp. 430\u2013445. Springer, 2021.   \nChen, T., Kornblith, S., Norouzi, M., and Hinton, G. A simple framework for contrastive learning of visual representations. In International conference on machine learning, pp. 1597\u20131607. PMLR, 2020. ", "page_idx": 10}, {"type": "text", "text": "Chrabaszcz, P., Loshchilov, I., and Hutter, F. A downsampled variant of imagenet as an alternative to the CIFAR datasets. arXiv preprint arXiv:1707.08819, 2017. ", "page_idx": 11}, {"type": "text", "text": "Cimpoi, M., Maji, S., Kokkinos, I., Mohamed, S., , and Vedaldi, A. Describing textures in the wild. In Proceedings of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2014.   \nCortes, C. and Vapnik, V. Support-vector networks. Machine learning, 20(3):273\u2013297, 1995.   \nCubuk, E. D., Zoph, B., Shlens, J., and Le, Q. V. Randaugment: Practical automated data augmentation with a reduced search space. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops, pp. 702\u2013703, 2020.   \nDemiriz, A., Bennett, K. P., and Shawe-Taylor, J. Linear programming boosting via column generation. Machine Learning, 46:225\u2013254, 2002.   \nDjurisic, A., Bozanic, N., Ashok, A., and Liu, R. Extremely simple activation shaping for out-ofdistribution detection. In The Eleventh International Conference on Learning Representations, 2023.   \nDouze, M., Guzhva, A., Deng, C., Johnson, J., Szilvasy, G., Mazar\u00e9, P.-E., Lomeli, M., Hosseini, L., and J\u00e9gou, H. The faiss library. 2024.   \nDu, X., Wang, Z., Cai, M., and Li, Y. Vos: Learning what you don\u2019t know by virtual outlier synthesis. arXiv preprint arXiv:2202.01197, 2022.   \nFreund, Y. and Schapire, R. E. A decision-theoretic generalization of on-line learning and an application to boosting. In Computational Learning Theory: Eurocolt \u201995, pp. 23\u201337. SpringerVerlag, 1995.   \nFriedman, J., Hastie, T., and Tibshirani, R. Additive logistic regression: a statistical view of boosting (with discussion and a rejoinder by the authors). The annals of statistics, 28(2):337\u2013407, 2000.   \nF\u00fcrst, A., Rumetshofer, E., Lehner, J., Tran, V. T., Tang, F., Ramsauer, H., Kreil, D., Kopp, M., Klambauer, G., Bitto, A., et al. CLOOB: Modern Hopfield networks with InfoLOOB outperform clip. Advances in neural information processing systems, 35:20450\u201320468, 2022.   \nGardner, E. Multiconnected neural network models. Journal of Physics A: Mathematical and General, 20(11):3453, 1987.   \nHe, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770\u2013778, 2016.   \nHe, K., Fan, H., Wu, Y., Xie, S., and Girshick, R. Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 9729\u20139738, 2020.   \nHendrycks, D. and Gimpel, K. A baseline for detecting misclassified and out-of-distribution examples in neural networks. In International Conference on Learning Representations, 2017. URL https://openreview.net/forum?id=Hkg4TI9xl.   \nHendrycks, D., Basart, S., Mazeika, M., Zou, A., Kwon, J., Mostajabi, M., Steinhardt, J., and Song, D. Scaling out-of-distribution detection for real-world settings. arXiv preprint arXiv:1911.11132, 2019a.   \nHendrycks, D., Mazeika, M., and Dietterich, T. Deep anomaly detection with outlier exposure. In International Conference on Learning Representations, 2019b. URL https://openreview. net/forum?id=HyxCxhRcY7.   \nHendrycks, D., Mazeika, M., Kadavath, S., and Song, D. Using self-supervised learning can improve model robustness and uncertainty. Advances in neural information processing systems, 32, 2019c.   \nHendrycks, D., Zou, A., Mazeika, M., Tang, L., Li, B., Song, D., and Steinhardt, J. Pixmix: Dreamlike pictures comprehensively improve safety measures. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 16783\u201316792, 2022. ", "page_idx": 11}, {"type": "text", "text": "Hopfield, J. J. Neural networks and physical systems with emergent collective computational abilities. Proceedings of the National Academy of Sciences, 79(8):2554\u20132558, 1982. ", "page_idx": 12}, {"type": "text", "text": "Hopfield, J. J. Neurons with graded response have collective computational properties like those of two-state neurons. Proceedings of the National Academy of Sciences, 81(10):3088\u20133092, 1984. doi: 10.1073/pnas.81.10.3088.   \nHorn, D. and Usher, M. Capacities of multiconnected memory models. Journal de Physique, 49(3): 389\u2013395, 1988.   \nHu, J. Y.-C., Chang, P.-H., Luo, H., Chen, H.-Y., Li, W., Wang, W.-P., and Liu, H. Outlier-efficient hopfield layers for large transformer-based models. In Forty-first International Conference on Machine Learning, 2024.   \nIsola, P., Xiao, J., Torralba, A., and Oliva, A. What makes an image memorable? In CVPR 2011, pp. 145\u2013152. IEEE, 2011.   \nJiang, W., Cheng, H., Chen, M., Wang, C., and Wei, H. DOS: Diverse outlier sampling for out-ofdistribution detection. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id $\\equiv$ iriEqxFB4y.   \nKrizhevsky, A. Learning multiple layers of features from tiny images. Master\u2019s thesis, Deptartment of Computer Science, University of Toronto, 2009.   \nKrotov, D. and Hopfield, J. J. Dense associative memory for pattern recognition. In Lee, D. D., Sugiyama, M., Luxburg, U. V., Guyon, I., and Garnett, R. (eds.), Advances in Neural Information Processing Systems, pp. 1172\u20131180. Curran Associates, Inc., 2016.   \nLeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278\u20132324, 1998.   \nLee, K., Lee, K., Lee, H., and Shin, J. A simple unified framework for detecting out-of-distribution samples and adversarial attacks. In Bengio, S., Wallach, H., Larochelle, H., Grauman, K., CesaBianchi, N., and Garnett, R. (eds.), Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018. URL https://proceedings.neurips.cc/paper/2018/file/ abdeb6f575ac5c6676b747bca8d09cc2-Paper.pdf.   \nLiu, J., Shen, Z., He, Y., Zhang, X., Xu, R., Yu, H., and Cui, P. Towards out-of-distribution generalization: A survey. arXiv preprint arXiv:2108.13624, 2021.   \nLiu, W., Wang, X., Owens, J., and Li, Y. Energy-based out-of-distribution detection. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H. (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 21464\u201321475. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/ f5496252609c43eb8a3d147ab9b9c006-Paper.pdf.   \nLiu, X., Lochman, Y., and Zach, C. Gen: Pushing the limits of softmax-based out-of-distribution detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 23946\u201323955, 2023.   \nL\u00f3pez-Cifuentes, A., Escudero-Vinolo, M., Besc\u00f3s, J., and Garc\u00eda-Mart\u00edn, \u00c1. Semantic-aware scene recognition. Pattern Recognition, 102:107256, 2020.   \nLoshchilov, I. and Hutter, F. Sgdr: Stochastic gradient descent with warm restarts. arXiv preprint arXiv:1608.03983, 2016.   \nLu, H., Gong, D., Wang, S., Xue, J., Yao, L., and Moore, K. Learning with mixture of prototypes for out-of-distribution detection. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=uNkKaD3MCs.   \nMcInnes, L., Healy, J., and Melville, J. Umap: Uniform manifold approximation and projection for dimension reduction. arXiv preprint arXiv:1802.03426, 2018.   \nMing, Y., Fan, Y., and Li, Y. POEM: Out-of-distribution detection with posterior sampling. In Chaudhuri, K., Jegelka, S., Song, L., Szepesvari, C., Niu, G., and Sabato, S. (eds.), Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pp. 15650\u201315665. PMLR, 17\u201323 Jul 2022. URL https://proceedings. mlr.press/v162/ming22a.html.   \nMing, Y., Sun, Y., Dia, O., and Li, Y. How to exploit hyperspherical embeddings for out-of-distribution detection? In The Eleventh International Conference on Learning Representations, 2023.   \nMoody, J. and Darken, C. J. Fast learning in networks of locally-tuned processing units. Neural computation, 1(2):281\u2013294, 1989.   \nMorteza, P. and Li, Y. Provable guarantees for understanding out-of-distribution detection. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pp. 7831\u20137840, 2022.   \nM\u00fcller, K.-R., Smola, A. J., R\u00e4tsch, G., Sch\u00f6lkopf, B., Kohlmorgen, J., and Vapnik, V. Predicting time series with support vector machines. In International conference on artificial neural networks, pp. 999\u20131004. Springer, 1997.   \nNetzer, Y., Wang, T., Coates, A., Bissacco, A., Wu, B., and Ng, A. Y. Reading digits in natural images with unsupervised feature learning. 2011.   \nOord, A. v. d., Li, Y., and Vinyals, O. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018.   \nPaischer, F., Adler, T., Patil, V., Bitto-Nemling, A., Holzleitner, M., Lehner, S., Eghbal-Zadeh, H., and Hochreiter, S. History compression via language models in reinforcement learning. In International Conference on Machine Learning, pp. 17156\u201317185. PMLR, 2022.   \nPark, G. Y., Kim, J., Kim, B., Lee, S. W., and Ye, J. C. Energy-based cross attention for Bayesian context update in text-to-image diffusion models. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \nPsaltis, D. and Park, C. H. Nonlinear discriminant functions and associative memories. In AIP conference Proceedings, volume 151, pp. 370\u2013375. American Institute of Physics, 1986.   \nRamsauer, H., Sch\u00e4f,l B., Lehner, J., Seidl, P., Widrich, M., Gruber, L., Holzleitner, M., Pavlovic\u00b4, M., Sandve, G. K., Greiff, V., Kreil, D., Kopp, M., Klambauer, G., Brandstetter, J., and Hochreiter, S. Hopfield networks is all you need. In 9th International Conference on Learning Representations (ICLR), 2021. URL https://openreview.net/forum?id $\\equiv$ tL89RnzIiCd.   \nR\u00e4tsch, G., Onoda, T., and M\u00fcller, K.-R. Soft margins for AdaBoost. Machine learning, 42:287\u2013320, 2001.   \nRidnik, T., Ben-Baruch, E., Noy, A., and Zelnik-Manor, L. Imagenet-21k pretraining for the masses. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1), 2021.   \nRoth, K., Pemula, L., Zepeda, J., Sch\u00f6lkopf, B., Brox, T., and Gehler, P. Towards total recall in industrial anomaly detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14318\u201314328, 2022.   \nRuff, L., Kauffmann, J. R., Vandermeulen, R. A., Montavon, G., Samek, W., Kloft, M., Dietterich, T. G., and M\u00fcller, K.-R. A unifying review of deep and shallow anomaly detection. Proceedings of the IEEE, 109(5):756\u2013795, 2021.   \nRussakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., et al. Imagenet large scale visual recognition challenge. International journal of computer vision, 115:211\u2013252, 2015.   \nSaleh, R. A. and Saleh, A. Statistical properties of the log-cosh loss function used in machine learning. arXiv preprint arXiv:2208.04564, 2022.   \nSanchez-Fernandez, A., Rumetshofer, E., Hochreiter, S., and Klambauer, G. CLOOME: a new search engine unlocks bioimaging databases for queries with chemical structures. bioRxiv, pp. 2022\u201311, 2022.   \nSch\u00e4f,l B., Gruber, L., Bitto-Nemling, A., and Hochreiter, S. Hopular: Modern Hopfield networks for tabular data. arXiv preprint arXiv:2206.00664, 2022.   \nSchimunek, J., Seidl, P., Friedrich, L., Kuhn, D., Rippmann, F., Hochreiter, S., and Klambauer, G. Context-enriched molecule representations improve few-shot drug discovery. In The Eleventh International Conference on Learning Representations, 2023.   \nSehwag, V., Chiang, M., and Mittal, P. Ssd: A unified framework for self-supervised outlier detection. arXiv preprint arXiv:2103.12051, 2021.   \nsmeschke. Four Shapes. https://www.kaggle.com/datasets/smeschke/four-shapes/, 2018. URL https://www.kaggle.com/datasets/smeschke/four-shapes/.   \nSun, Y., Guo, C., and Li, Y. React: Out-of-distribution detection with rectified activations. Advances in Neural Information Processing Systems, 34:144\u2013157, 2021.   \nSun, Y., Ming, Y., Zhu, X., and Li, Y. Out-of-distribution detection with deep nearest neighbors. In International Conference on Machine Learning, pp. 20827\u201320840. PMLR, 2022.   \nTack, J., Mo, S., Jeong, J., and Shin, J. Csi: Novelty detection via contrastive learning on distributionally shifted instances. Advances in neural information processing systems, 33:11839\u201311852, 2020.   \nTao, L., Du, X., Zhu, X., and Li, Y. Non-parametric outlier synthesis. arXiv preprint arXiv:2303.02966, 2023.   \nTeh, Y. W., Thiery, A. H., and Vollmer, S. J. Consistency and fluctuations for stochastic gradient Langevin dynamics. J. Mach. Learn. Res., 17(1):193\u2013225, 2016.   \nTorchVision. Torchvision: Pytorch\u2019s computer vision library. https://github.com/pytorch/ vision, 2016.   \nVan Horn, G., Mac Aodha, O., Song, Y., Cui, Y., Sun, C., Shepard, A., Adam, H., Perona, P., and Belongie, S. The inaturalist species classification and detection dataset. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 8769\u20138778, 2018.   \nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention is all you need. In Guyon, I., Luxburg, U. V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., and Garnett, R. (eds.), Advances in Neural Information Processing Systems 30, pp. 5998\u20136008. Curran Associates, Inc., 2017.   \nWang, H., Li, Z., Feng, L., and Zhang, W. Vim: Out-of-distribution with virtual-logit matching. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 4921\u20134930, 2022.   \nWang, Q., Fang, Z., Zhang, Y., Liu, F., Li, Y., and Han, B. Learning to augment distributions for out-of-distribution detection. In NeurIPS, 2023a. URL https://openreview.net/forum?id= OtU6VvXJue.   \nWang, Q., Ye, J., Liu, F., Dai, Q., Kalander, M., Liu, T., Hao, J., and Han, B. Out-of-distribution detection with implicit outlier transformation. arXiv preprint arXiv:2303.05033, 2023b.   \nWang, T. and Isola, P. Understanding contrastive representation learning through alignment and uniformity on the hypersphere. In International Conference on Machine Learning, pp. 9929\u20139939. PMLR, 2020.   \nWei, H., Xie, R., Cheng, H., Feng, L., An, B., and Li, Y. Mitigating neural network overconfidence with logit normalization. In International conference on machine learning, pp. 23631\u201323644. PMLR, 2022a.   \nWei, X.-S., Cui, Q., Yang, L., Wang, P., Liu, L., and Yang, J. Rpc: a large-scale and fine-grained retail product checkout dataset, 2022b. URL https://rpc-dataset.github.io/.   \nWelling, M. and Teh, Y. W. Bayesian learning via stochastic gradient Langevin dynamics. In Proceedings of the 28th International Conference on Machine Learning, pp. 681\u2013688, Madison, WI, USA, 2011. Omnipress.   \nWidrich, M., Sch\u00e4f,l B., Pavlovi\u00b4c, M., Ramsauer, H., Gruber, L., Holzleitner, M., Brandstetter, J., Sandve, G. K., Greiff, V., Hochreiter, S., and Klambauer, G. Modern Hopfield networks and attention for immune repertoire classification. ArXiv, 2007.13505, 2020.   \nXu, K., Chen, R., Franchi, G., and Yao, A. Scaling for training time and post-hoc out-of-distribution detection enhancement. In The Twelfth International Conference on Learning Representations, 2024.   \nXu, P., Ehinger, K. A., Zhang, Y., Finkelstein, A., Kulkarni, S. R., and Xiao, J. Turkergaze: Crowdsourcing saliency with webcam based eye tracking. arXiv preprint arXiv:1504.06755, 2015.   \nXu, P., Chen, J., Zou, D., and Gu, Q. Global convergence of Langevin dynamics based algorithms for nonconvex optimization. In Bengio, S., Wallach, H., Larochelle, H., Grauman, K., Cesa-Bianchi, N., and Garnett, R. (eds.), Advances in Neural Information Processing Systems, volume 31, pp. 3122\u20133133. Curran Associates, Inc., 2018.   \nYang, J., Zhou, K., Li, Y., and Liu, Z. Generalized out-of-distribution detection: A survey. arXiv preprint arXiv:2110.11334, 2021.   \nYang, J., Wang, P., Zou, D., Zhou, Z., Ding, K., Peng, W., Wang, H., Chen, G., Li, B., Sun, Y., et al. Openood: Benchmarking generalized out-of-distribution detection. Advances in Neural Information Processing Systems, 35:32598\u201332611, 2022.   \nYu, F., Seff, A., Zhang, Y., Song, S., Funkhouser, T., and Xiao, J. LSUN: Construction of a large-scale image dataset using deep learning with humans in the loop. arXiv preprint arXiv:1506.03365, 2015.   \nYun, S., Han, D., Oh, S. J., Chun, S., Choe, J., and Yoo, Y. Cutmix: Regularization strategy to train strong classifiers with localizable features. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 6023\u20136032, 2019.   \nZhang, H., Cisse, M., Dauphin, Y. N., and Lopez-Paz, D. mixup: Beyond empirical risk minimization. In International Conference on Learning Representations, 2018.   \nZhang, J., Fu, Q., Chen, X., Du, L., Li, Z., Wang, G., Han, S., Zhang, D., et al. Out-of-distribution detection based on in-distribution data patterns memorization with modern Hopfield energy. In The Eleventh International Conference on Learning Representations, 2023a.   \nZhang, J., Inkawhich, N., Linderman, R., Chen, Y., and Li, H. Mixture outlier exposure: Towards out-of-distribution detection in fine-grained environments. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), pp. 5531\u20135540, January 2023b.   \nZhang, R., Li, C., Zhang, J., Chen, C., and Wilson, A. G. Cyclical stochastic gradient MCMC for Bayesian deep learning. In International Conference on Learning Representations, 2020.   \nZheng, Y., Zhao, Y., Ren, M., Yan, H., Lu, X., Liu, J., and Li, J. Cartoon face recognition: A benchmark dataset. In Proceedings of the 28th ACM International Conference on Multimedia, pp. 2264\u20132272, 2020.   \nZhu, J., Geng, Y., Yao, J., Liu, T., Niu, G., Sugiyama, M., and Han, B. Diversified outlier exposure for out-of-distribution detection via informative extrapolation. Advances in Neural Information Processing Systems, 36, 2023. ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "A Details on Continuous Modern Hopfield Networks ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "The following arguments are adopted from F\u00fcrst et al. (2022) and Ramsauer et al. (2021). Associative memory networks have been designed to store and retrieve samples. Hopfield networks are energybased, binary associative memories, which were popularized as artificial neural network architectures in the 1980s (Hopfield, 1982, 1984). Their storage capacity can be considerably increased by polynomial terms in the energy function (Chen et al., 1986; Psaltis & Park, 1986; Baldi & Venkatesh, 1987; Gardner, 1987; Abbott & Arian, 1987; Horn & Usher, 1988; Caputo & Niemann, 2002; Krotov & Hopfield, 2016). In contrast to these binary memory networks, we use continuous associative memory networks with far higher storage capacity. These networks are continuous and differentiable, retrieve with a single update, and have exponential storage capacity (and are therefore scalable, i.e., able to tackle large problems; Ramsauer et al., 2021). ", "page_idx": 16}, {"type": "text", "text": "Formally, we denote a set of patterns $\\{\\pmb{x}_{1},\\ldots,\\pmb{x}_{N}\\}\\subset\\mathbb{R}^{d}$ that are stacked as columns to the matrix $\\pmb{X}=(\\pmb{x}_{1},\\pmb{\\ldots},\\pmb{x}_{N})$ and a state pattern (query) $\\pmb{\\xi}\\in\\mathbb{R}^{d}$ that represents the current state. The largest norm of a stored pattern is $M=\\operatorname*{max}_{i}\\|\\pmb{x}_{i}\\|$ . Then, the energy $\\mathrm{E}$ of continuous Modern Hopfield Networks with state $\\xi$ is defined as (Ramsauer et al., 2021) ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathrm{~\\boldmath~{~\\cal~E~}~}=\\mathrm{~-~}\\beta^{-1}\\;\\log\\left(\\sum_{i=1}^{N}\\exp(\\beta{\\pmb x}_{i}^{T}{\\pmb\\xi})\\right)+\\mathrm{~\\frac{1}{2}~}{\\pmb\\xi}^{T}{\\pmb\\xi}\\;+\\;\\mathrm{C},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\mathrm{C}=\\beta^{-1}\\log N~+~{\\textstyle\\frac{1}{2}}~M^{2}$ . For energy $\\mathrm{E}$ and state $\\xi$ , Ramsauer et al. (2021) proved that the update rule ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\pmb{\\xi}^{\\mathrm{new}}\\;=\\;\\pmb{X}\\;\\mathrm{softmax}(\\beta\\pmb{X}^{T}\\pmb{\\xi})\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "converges globally to stationary points of the energy $\\mathrm{E}$ and coincides with the attention mechanisms of Transformers (Vaswani et al., 2017; Ramsauer et al., 2021). ", "page_idx": 16}, {"type": "text", "text": "The separation $\\Delta_{i}$ of a pattern $\\pmb{x}_{i}$ is its minimal dot product difference to any of the other patterns: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\Delta_{i}=\\operatorname*{min}_{j,j\\neq i}\\left(\\pmb{x}_{i}^{T}\\pmb{x}_{i}-\\pmb{x}_{i}^{T}\\pmb{x}_{j}\\right).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "A pattern is well-separated from the data if $\\Delta_{i}$ is above a given threshold (specified in Ramsauer et al., 2021). If the patterns $\\pmb{x}_{i}$ are well-separated, the update rule Equation 15 converges to a fixed point close to a stored pattern. If some patterns are similar to one another and, therefore, not well-separated, the update rule converges to a fixed point close to the mean of the similar patterns. ", "page_idx": 16}, {"type": "text", "text": "The update rule of a Hopfield network thus identifies sample\u2013sample relations between stored patterns. This enables similarity-based learning methods like nearest neighbor search (see Sch\u00e4f let al., 2022), which Hopfield Boosting leverages to detect samples outside the distribution of the training data. ", "page_idx": 16}, {"type": "text", "text": "Hopfield networks have recently been used for OOD detection (Zhang et al., 2023a). Hu et al. (2024) introduces Hopfield layers for outlier-efficient memory update. ", "page_idx": 16}, {"type": "text", "text": "B Notes on Langevin Sampling ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Another method that is appropriate for earlier acquired models is to sample the posterior via the Stochastic Gradient Langevin Dynamics (SGLD) (Welling & Teh, 2011). This method is efficient since it iteratively learns from small mini-batches Welling & Teh (2011); Ahn et al. (2012). See basic work on Langevin dynamics Welling & Teh (2011); Ahn et al. (2012); Teh et al. (2016); Xu et al. (2018). A cyclical stepsize schedule for SGLD was very promising for uncertainty quantification Zhang et al. (2020). Larger steps discover new modes, while smaller steps characterize each mode and perform the posterior sampling. ", "page_idx": 16}, {"type": "text", "text": "C Related work ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "C.1 Details on further OE approaches ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "This section gives details about related works from the area of OE in OOD detection. With OE, we refer to the usage of AUX for training an OOD detector in general. ", "page_idx": 16}, {"type": "text", "text": "MSP-OE. Hendrycks et al. (2019b) were the first to introduce the term OE in the context of OOD detection. Specifically, they improve an MSP-based OOD detection (Hendrycks & Gimpel, 2017): They train a classifier on the ID data set and maximize the entropy of the predictive distribution of the classifier for the AUX data. The combined loss they employ is ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}\\,=\\,\\mathcal{L}_{\\mathrm{CE}}\\,+\\,\\lambda\\mathcal{L}_{\\mathrm{OOD}}\\qquad\\qquad}\\\\ {\\mathcal{L}_{\\mathrm{OOD}}\\,=\\,\\underset{o^{\\mathcal{D}}\\sim p_{\\mathrm{AUX}}}{\\mathbb{E}}[H(\\mathcal{U},p_{\\theta}(o))]}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $H$ denotes the cross-entropy, $\\boldsymbol{\\mathcal{U}}$ denotes the uniform distribution over $\\mathbf{K}$ classes, and $p_{\\theta}$ is the model mapping the features to the predictive distribution over the K classes. ", "page_idx": 17}, {"type": "text", "text": "EBO-OE. Liu et al. (2020) propose a post-hoc and an OE approach. Their post-hoc approach (EBO) is to use the classifier\u2019s energy to perform OOD detection: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\mathrm{E}(\\pmb{\\xi}^{D})\\,=\\,-\\,\\beta^{-1}\\mathrm{lse}(\\beta,f_{\\pmb{\\theta}}({\\pmb{\\xi}}^{D}))}}\\\\ {{s({\\pmb{\\xi}}^{D})\\,=\\,-\\mathrm{E}({\\pmb{\\xi}}^{D},f_{\\pmb{\\theta}})}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $f_{\\theta}$ outputs the model\u2019s logits as a vector. Their OE approach (EBO-OE) promotes a low energy on ID samples and a high energy on AUX samples: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{OOD}}\\;=\\;\\underset{x^{D}\\sim p_{\\mathrm{ID}}}{\\mathbb{E}}[(\\operatorname*{max}(0,\\mathrm{E}({x^{D}})\\,\\,-\\,\\,m_{\\mathrm{ID}}))^{2}]\\;+\\;\\underset{o^{D}\\sim p_{\\mathrm{AUX}}}{\\mathbb{E}}[(\\operatorname*{max}(0,m_{\\mathrm{AUX}}\\,\\,-\\,\\,\\mathrm{E}(o^{D})))^{2}]\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $m_{\\mathrm{ID}}$ and $m_{\\mathrm{AUX}}$ are margin hyperparameters. ", "page_idx": 17}, {"type": "text", "text": "POEM. Ming et al. (2022) propose to incorporate Thompson sampling into the OE process. More specifically, they sample a linear decision boundary in embedding space between the ID and AUX data using Bayesian linear regression and then select those samples from the AUX data set that are closest to the sampled decision boundary. In the following epoch, they sample the AUX data uniformly from the selected data instances without replacement and optimize the model with the EBO-OE loss (Equation (21)). ", "page_idx": 17}, {"type": "text", "text": "MixOE. Zhang et al. (2023b) employ mixup (Zhang et al., 2018) between the ID and AUX samples to augment the OE task. Formally, this results in the following: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\tilde{\\mathbf{\\boldsymbol{x}}}\\,=\\,\\lambda\\mathbf{\\boldsymbol{x}}^{D}\\,+\\,(1-\\lambda)\\mathbf{\\boldsymbol{o}}^{D}}\\\\ {\\tilde{\\boldsymbol{y}}\\,=\\,\\lambda y\\,+\\,(1-\\lambda)\\mathcal{U}}\\\\ {\\mathcal{L}_{\\mathrm{00D}}\\,=\\,\\frac{\\mathbb{E}}{(\\mathbf{\\boldsymbol{x}}^{D},\\boldsymbol{y})\\sim p_{\\mathrm{ID}}}[H(\\tilde{\\boldsymbol{y}},\\tilde{\\mathbf{\\boldsymbol{x}}})]}\\\\ {\\mathbf{\\boldsymbol{\\sigma}}^{D}\\!\\sim\\!p_{\\mathrm{AUX}}\\,\\,}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Alternatively, they also propose to employ CutMix (Yun et al., 2019) instead of mixup (which would change the mixing operation in Equation (22)). ", "page_idx": 17}, {"type": "text", "text": "DAL. Wang et al. (2023a) augment the AUX data by defining a Wasserstein-1 ball around the AUX data and performing OE using this Wasserstein ball. DAL is motivated by the concept of distribution discrepancy: The distribution of the real OOD data will in general be different from the distribution of the AUX data. The authors argue that their approach can make OOD detection more reliable if the distribution discrepancy is large. ", "page_idx": 17}, {"type": "text", "text": "DivOE. Zhu et al. (2023) pose the question of how to utilize the given outliers from the AUX data set if the auxiliary outliers are not informative enough to represent the unseen OOD distribution. They suggest solving this problem by diversifying the AUX data using extrapolation, which should result in better coverage of the OOD space of the resultant extrapolated distribution. Formally, they employ a loss using a synthesized distribution with a manipulation $\\Delta$ : ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{OOD}}\\;=\\;\\underset{o^{D}\\sim p_{\\mathrm{AUX}}}{\\mathbb{E}}[(1\\,\\,-\\,\\,\\gamma)H(\\mathcal{U},p_{\\theta}(o^{D}))\\,+\\,\\gamma\\operatorname*{max}_{\\Delta}[H(\\mathcal{U},p_{\\theta}(o^{D}+\\Delta))\\,-\\,H(\\mathcal{U},p_{\\theta}(o^{D}))]]\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "DOE. Wang et al. (2023b) implicitly synthesize auxiliary outlier data using a transformation of the model weights. They argue that perturbing the model parameters has the same effect as transforming the data. ", "page_idx": 18}, {"type": "text", "text": "DOS. Jiang et al. (2024) apply K-means clustering to the features of the AUX data set. They then employ a balanced sampling from the K obtained clusters by selecting the same number of samples from each cluster for training. More specifically, they select those n samples from each cluster which are closest to the decision boundary between the ID and OOD regions. ", "page_idx": 18}, {"type": "text", "text": "D Future Work ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "D.1 Smooth and Sharp Decision Boundaries ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Our work treats samples close to the decision boundary as weak learners. However, the wider ramifications of this behavior remain unclear. One can also view the sampling of data instances close to the decision boundary as a form of adversarial training in that we search for something like \u201cnatural adversarial examples\u201d: Loosely speaking, the usual adversarial example case starts with a given ID sample and corrupts it in a specific way to get the classifier to output the wrong class probabilities. In our case, we start with a large set of potential adversarial instances (the AUX data) and search for the ones that could be either ID or OOD samples. That is, the sampling process will more likely select data instances that are hard to discriminate for the model \u2014 for example, if the model is uncertain whether a leaf in an auxiliary outlier sample is a frog or not. ", "page_idx": 18}, {"type": "text", "text": "This process can be viewed as \u201csharpening\u201d the decision boundary: The boosting process frequently samples data instances with high uncertainty. $\\mathcal{L}_{\\mathrm{ooD}}$ encourages the model to assign less uncertainty to the sampled data instances. After training, few training data instances will have high uncertainty. Nevertheless, a closer systematic evaluation of the sharpened decision boundary of Hopfield Boosting is important to fully understand the potential implications w.r.t. adversarial examples. We view such an investigation as out-of-scope for this work. However, we consider it an interesting avenue for future work. ", "page_idx": 18}, {"type": "text", "text": "Anderson & Sojoudi (2022) show that a smooth decision boundary helps with \u201cclassical\u201d adversarial examples. In this framing, our approach would produce different adversarial examples that are not based on noise but are more akin to \u201cnatural adversarial examples\u201d. For example, it is perfectly fine for us that an OOD sample close to the boundary does not correspond to any of the ID classes. Furthermore, the noise based smoothing leads to adversarial robustness at the (potential) cost of degrading classification performance. Similarly, our sharpening of the boundaries leads to better discrimination between ID and OOD region at the (potential) cost of degrading ID classification performance. ", "page_idx": 18}, {"type": "text", "text": "E Societal Impact ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "This section discusses the potential positive and negative societal impacts of our work. As our work aims improves the state-of-the-art in OOD detection, we focus on potential societal impact of OOD detection in general. ", "page_idx": 18}, {"type": "text", "text": "\u2022 Postive Impacts ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Improved model reliability: OOD detection aims to detect unfamiliar inputs that have little support in the model\u2019s training distribution. When these samples are detected, one can, for example, notify the user that no prediction is possible, or trigger a manual intervention. This can lead to an increase in a model\u2019s reliability. ", "page_idx": 18}, {"type": "text", "text": "\u2013 Abstain from doing uncertain predictions: When a model with appropriate OOD detection recognizes that a query sample has limited support in the training distribution, it can abstain from performing a prediction. This can, for example, increase trust in ML models, as they will rather tell the user they are uncertain than report a confidently wrong prediction. ", "page_idx": 19}, {"type": "text", "text": "\u2022 Negative Impacts ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "\u2013 Wrong sense of safety: Having OOD detection in place could cause users to wrongly assume that all OOD inputs will be detected. However, like most systems, also OOD detection methods can make errors. It is important to consider that certain OOD examples could remain undetected. ", "page_idx": 19}, {"type": "text", "text": "\u2013 Potential for misinterpretation: As with many other ML systems, the outcomes of OOD detection methods are prone to misinterpretation. It is important to acquaint oneself with the respective method before applying it in practice. ", "page_idx": 19}, {"type": "text", "text": "F Toy Examples ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "F.1 3D Visualizations of $\\operatorname{E}_{b}$ on a hypersphere ", "page_idx": 20}, {"type": "image", "img_path": "VLQYtVMTYz/tmp/090b7afba3d1cc5042dcb038469490bdc5c03e234436b6b00f3278cdc59ec3e3.jpg", "img_caption": ["Figure 3: Depiction of the energy function $\\mathrm{E}_{b}(\\pmb{\\xi};\\pmb{X},\\pmb{O})$ on a hypersphere. (a) shows $\\mathrm{E}_{b}(\\pmb{\\xi},\\pmb{X},\\pmb{O})$ with exemplary inlier (orange) and outlier (blue) points; and (b) shows $\\exp(\\beta\\mathrm{E}_{b}(\\xi,X,O))$ . $\\beta$ was set to 128. Both, (a) and (b), rotate the sphere by 0, 90, 180, and 270 degrees around the vertical axis. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "This example depicts how inliers and outliers shape the energy surface (Figure 3). We generated patterns so that $\\mathbf{\\deltaX}$ clusters around a pole and the outliers populate the remaining perimeter of the sphere. This is analogous to the idea that one has access to a large AUX data set, where some data points are more and some less informative for OOD detection (e.g., as conceptualized in Ming et al., 2022). ", "page_idx": 20}, {"type": "text", "text": "F.2 Dynamics of $\\mathcal{L}_{00\\mathbf{D}}$ on Patterns in Euclidean Space ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In this example, we applied our out-of-distribution loss $\\mathcal{L}_{\\mathrm{ooD}}$ on a simple binary classification problem. As we are working in Euclidean space and not on a sphere, we use a modified version of MHE, which uses the negative squared Euclidean distance instead of the dot-product-similarity. For the formal relation between Equation (26) and MHE, we refer to Appendix H.1: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname{E}(\\pmb{\\xi};\\pmb{X})=\\ -\\ \\beta^{-1}\\ \\log\\left(\\sum_{i=1}^{N}\\exp(-{\\frac{\\beta}{2}}\\ ||\\pmb{\\xi}-\\pmb{x}_{i}||_{2}^{2})\\right)\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Figure $4\\mathrm{a}$ shows the initial state of the patterns and the decision boundary $\\exp(\\beta E_{b}(\\xi;X,O))$ . We store the samples of the two classes as stored patterns in $\\mathbf{\\deltaX}$ and $o$ , respectively, and compute $\\mathcal{L}_{\\mathrm{ooD}}$ for all samples. We then set the learning rate to 0.1 and perform gradient descent with $\\mathcal{L}_{\\mathrm{ooD}}$ on the data points. Figure $4\\mathrm{b}$ shows that after 25 steps, the distance between the data points and the decision boundary has increased, especially for samples that had previously been close to the decision boundary. After 100 steps, as shown in Figure 4d, the variability orthogonal to the decision boundary has almost completely vanished, while the variability parallel to the decision boundary is maintained. ", "page_idx": 20}, {"type": "image", "img_path": "VLQYtVMTYz/tmp/9fb1bfb61d20dadf40fc662fdf5d929f512498d84e07c65d86f504a73e83d60b.jpg", "img_caption": ["Figure 4: $\\mathcal{L}_{\\mathrm{ooD}}$ applied to exemplary data points on euclidean space. Gradient updates are applied to the data points directly. We observe that the variance orthogonal to the decision boundary shrinks while the variance parallel to the decision boundary does not change to this extent. $\\beta$ is set to 2. "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "VLQYtVMTYz/tmp/8960c83789ac13638d7b5d4a6059003742f835d212d84318773bf951a5dfe866.jpg", "img_caption": ["F.3 Dynamics of $\\mathcal{L}_{00\\mathbf{D}}$ on Patterns on the Sphere ", "Figure 5: $\\mathcal{L}_{\\mathrm{ooD}}$ applied to exemplary data points on a sphere. Gradients are applied to the data points directly. We observe that the geometry of the space forces the patterns to opposing poles of the sphere. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "F.4 Learning Dynamics of Hopfield Boosting on Patterns on a Sphere - Video ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "The example video1 demonstrates the learning dynamics of Hopfield Boosting on a 3-dimensional sphere. We randomly generate ID patterns $\\mathbf{\\deltaX}$ clustering around one of the sphere\u2019s poles and AUX patterns $^o$ on the remaining surface of the sphere. We then apply Hopfield Boosting on this data set. First, we sample the weak learners close to the decision boundary for both classes, $\\mathbf{\\deltaX}$ and $o$ . Then, we perform 2000 steps of gradient descent with $\\mathcal{L}_{\\mathrm{ooD}}$ on the sampled weak learners. We apply the gradient updates to the patterns directly and do not propagate any gradients to an encoder. Every 50 gradient steps, we re-sample the weak learners. For this example, the initial learning rate is set to 0.02 and increased after every gradient step by $0.1\\%$ . ", "page_idx": 22}, {"type": "text", "text": "F.5 Location of Weak Learners near the Decision Boundary ", "text_level": 1, "page_idx": 23}, {"type": "image", "img_path": "VLQYtVMTYz/tmp/fcc63872b5c982e469390303002845c6e2f59571897bfd01d20dfd0310174c0b.jpg", "img_caption": [], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "Figure 6: A prototypical classifier (red circle) that is constructed with a sample close to the decision boundary. Classifiers like this one will only perform slightly better than random guessing (as indicated by the radial decision boundaries) and are, therefore, well-suited for weak learners. ", "page_idx": 23}, {"type": "text", "text": "F.6 Interaction between ID and OOD losses ", "text_level": 1, "page_idx": 23}, {"type": "image", "img_path": "VLQYtVMTYz/tmp/7835e67c919a661222f094d305e3def08778f35aeee9e0d5be62587f888a04c1.jpg", "img_caption": [], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "Figure 7: Synthetic example of Hopfield Boosting training dynamics. The ID data is split in two classes (shown in red and orange); the AUX data (blue) is sampled uniformly. We minimize the loss $\\mathcal{L}\\;=\\;\\mathcal{L}_{\\mathrm{CE}}\\;+\\;\\mathcal{L}_{\\mathrm{OOD}}$ , and apply the gradient updates on the patterns directly. The left Figure shows the initial pattern positions, the rightmost Figure shows the positions after 1000 gradient updates: The classes are well-separated; $\\operatorname{E}_{b}$ forms a tight decision boundary around the ID data. ", "page_idx": 23}, {"type": "text", "text": "To demonstrate how Hopfield Boosting can interact in a classification task we created a toy example that resembles the ID classification setting (Figure 7): The example shows the decision boundary and the inlier samples organized in two classes (shown in red and orange). We sample uniformly distributed auxiliary outliers. Then, we minimize the compound objective (applying the gradient updates on the patterns directly). This shows that Hopfield Boosting is able to separate the two classes well and that still forms a tight decision boundary around the ID data. ", "page_idx": 23}, {"type": "text", "text": "G Notes on $\\operatorname{E}_{b}$ ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "G.1 Probabilistic Interpretation of $\\operatorname{E}_{b}$ ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "We model the class-conditional densities of the in-distribution data and auxiliary data as mixtures of Gaussians with the patterns as the component means and tied, diagonal covariance matrices with $\\beta^{-1}$ in the main diagonal. ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{p(\\,\\pm\\mid\\mathrm{ID}\\,)\\,=\\,\\displaystyle\\frac{1}{N}\\sum_{i=1}^{N}\\!\\mathcal{N}\\left(\\pmb{\\xi};\\pmb{x}_{i},\\beta^{-1}\\pmb{I}\\right)}}\\\\ {{\\displaystyle p(\\,\\pmb{\\xi}\\mid\\mathrm{AUX}\\,)\\,=\\,\\displaystyle\\frac{1}{M}\\sum_{i=1}^{M}\\!\\mathcal{N}\\left(\\pmb{\\xi};\\pmb{o}_{i},\\beta^{-1}\\pmb{I}\\right)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Further, we assume the distribution $p(\\pmb\\xi)$ as a mixture of $p(\\,\\pmb{\\xi}\\mid\\mathbb{D}\\,)$ and $p(\\,\\pmb{\\xi}\\mid\\mathrm{AUX}\\,)$ with equal prior probabilities (mixture weights): ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle{p(\\pmb{\\xi})\\,=\\,p(\\mathrm{ID})\\,p(\\,\\pmb{\\xi}\\mid\\mathrm{ID}\\,)+p(\\mathrm{AUX})\\,p(\\,\\pmb{\\xi}\\mid\\mathrm{AUX}\\,)}}\\\\ {\\displaystyle{=\\,\\frac{1}{2}\\,p(\\,\\pmb{\\xi}\\mid\\mathrm{ID}\\,)\\,+\\,\\frac{1}{2}\\,p(\\,\\pmb{\\xi}\\mid\\mathrm{AUX}\\,)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "The probability of an unknown sample $\\xi$ being an AUX sample is given by ", "page_idx": 24}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}{p(\\operatorname{AUX}|\\,\\xi)\\,=\\,{\\frac{p(\\,\\xi\\,|\\,\\operatorname{AUX})\\,\\,p(\\operatorname{AUX})}{p(\\xi)}}}\\\\ &{={\\frac{p(\\,\\xi\\,|\\,\\operatorname{AUX})}{2\\,p(\\xi)}}}\\\\ &{={\\frac{p(\\,\\xi\\,|\\,\\operatorname{AUX})}{p(\\,\\xi\\,|\\,\\operatorname{AUX})\\,+\\,p(\\,\\xi\\,|\\,\\operatorname{ID})}}}\\\\ &{={\\frac{1}{1\\,+\\,{\\frac{p(\\,\\xi\\,|\\,\\operatorname{ID})}{p(\\,\\xi\\,|\\,\\operatorname{AUX})}}}}}\\\\ &{={\\frac{1}{1\\,+\\,\\exp(\\log(p(\\,\\xi\\,|\\,\\operatorname{ID}))-1)}}}\\end{array}}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where in line (34) we have used that $p(\\,\\pmb{\\xi}\\mid\\mathrm{AUX}\\,)>0$ for all $\\pmb{\\xi}\\in\\mathbb{R}^{d}$ . The probability of $\\xi$ being an ID sample is given by ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{p(\\mathrm{\\bf~ID}\\mid\\xi\\;)=\\displaystyle\\frac{p(\\xi\\mid\\mathrm{ID}\\;)}{2\\,p(\\xi)}}}\\\\ {{{\\displaystyle~~~~~~~~~~~~=\\frac{1}{1+\\exp(\\log(p(\\xi\\mid\\mathrm{AUX}\\;))-\\log(p(\\xi\\mid\\mathrm{ID}\\;)))}}}}\\\\ {{{\\displaystyle~~~~~~~~~~=1-p(\\mathrm{\\bf~AUX}\\mid\\xi\\;)}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Consider the function ", "page_idx": 24}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}&{f_{b}({\\pmb\\xi})\\;=\\;p(\\mathrm{\\bf~AUX}\\;|\\;{\\pmb\\xi})\\cdot p(\\mathrm{\\bf~ID}\\;|\\;{\\pmb\\xi})}\\\\ &{={\\frac{p(\\;{\\pmb\\xi}\\;|\\;{\\bf A U X}\\;)\\cdot p(\\;{\\pmb\\xi}\\;|\\;{\\bf I D}\\;)}{4p({\\pmb\\xi})^{2}}}}\\end{array}}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "By taking the log of Equation (40) we obtain the following. We use = to denote equality up to an additive constant that does not depend on $\\xi$ . ", "page_idx": 24}, {"type": "text", "text": "Multiplication by $\\beta^{-1}$ is equivalent to a change of base of the log. The ter $\\mathrm{m}\\mathrm{~-~}\\beta^{-1}\\,\\log(p(\\pmb{\\xi}))$ is equivalent to the MHE (Ramsauer et al., 2021) (up to an additive constant) when assuming normalized patterns, i.e. $||\\pmb{x}_{i}||_{2}=1$ and $||o_{i}||_{2}=1$ , and an equal number of patterns $M=N$ in the two Gaussian mixtures $p(\\,\\pmb{\\xi}\\mid\\mathbb{D}\\,)$ and $p(\\,\\pmb\\xi\\,|$ AUX ): ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{-\\beta^{-1}\\log(\\wp)=-\\beta^{-1}\\log\\left(\\frac{1}{\\alpha^{2}}\\wp\\left(1\\ln\\right)+\\frac{1}{2}\\rho^{\\prime}\\left(\\xi\\ln\\left[\\Delta\\pi\\right]\\right)\\right)}\\\\ {\\overset{c}{=}-\\beta^{-1}\\log(\\rho\\xi\\left[\\ln\\mathbf{D}\\right]+p(\\xi\\ln))}\\\\ {=-\\beta^{-1}\\log\\left(\\frac{1}{N}\\sum_{i}N\\left(\\xi\\ln,j^{-1},i^{-1}\\right)+\\frac{1}{N}\\sum_{i=1}^{N}\\xi\\left(\\xi\\cdot\\alpha_{i},\\beta^{-1}\\eta\\right)\\right)}\\\\ {\\overset{c}{=}-\\beta^{-1}\\log\\left(\\frac{N}{N}\\sum_{i}N\\left(\\xi\\cdot\\alpha_{i},\\beta^{-1}\\right)+\\frac{\\displaystyle\\sum_{i=1}^{N}}{\\displaystyle\\sum_{i=1}^{N}}N\\left(\\xi\\cdot\\alpha_{i},\\beta^{-1}\\eta\\right)\\right)}\\\\ {\\overset{c}{=}-\\beta^{-1}\\log\\left(\\frac{N}{\\displaystyle\\sum_{i=1}^{N}}\\exp(-\\frac{\\beta}{2}|\\xi|^{-1}-\\alpha_{i}|^{2})+\\displaystyle\\sum_{i=1}^{N}\\exp(-\\frac{\\beta}{2}|\\xi-\\alpha_{i}|^{2})\\right)}\\\\ {\\overset{c}{=}-\\beta^{-1}\\log\\left(\\frac{\\displaystyle\\sum_{i=1}^{N}\\exp(2\\xi^{2}\\frac{i}{2}\\xi^{2})}{\\displaystyle\\sum_{i=1}^{N}\\exp(2\\xi^{2}\\frac{i}{2})+\\displaystyle\\sum_{i=1}^{N}\\exp(2\\xi^{2}\\frac{i}{2}\\xi^{2})}\\right)}\\\\ {=-\\beta^{-1}\\log\\left(\\frac{\\displaystyle\\sum_{i=1}^{N}\\exp(2\\beta\\tau/\\xi)}{\\displaystyle\\sum_{i=1}^{N}\\exp(2\\xi^{2}\\xi)+\\displaystyle\\sum_{i=1}^{N}\\exp(2\\beta\\tau/\\xi)}\\right)+\\frac{1}{2}\\xi^{\\prime}\\xi}\\\\ {\\overset{c}{=}-\\ln(\\delta_{i}(\\xi))\\log^{2}\\xi_{i}+\\frac{1}{2}\\xi^{\\prime}\\xi+\\beta^{-1}\\log(3\\xi)+\\frac{1}{2}M^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Analogously, $\\beta^{-1}\\,\\log(p(\\,\\pmb{\\xi}\\,|\\,\\mathrm{ID}\\,))$ and $\\beta^{-1}\\,\\log(p(\\,\\pmb{\\xi}\\mid\\mathrm{AUX}\\,))$ also yield MHE terms. Therefore, $\\operatorname{E}_{b}$ is equivalent to $\\beta^{-1}\\log\\bigl(f_{b}(\\pmb{\\xi})\\bigr)$ under the assumption that $||{\\pmb x}_{i}||_{2}=1$ and $||\\pmb{\\ o}_{i}||_{2}=1$ and $M=N$ The $\\textstyle{\\frac{1}{2}}\\pmb{\\xi}^{T}\\pmb{\\xi}$ terms that are contained in the three MHEs cancel out. ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\beta^{-1}\\,\\log\\,(f_{b}(\\pmb\\xi))\\stackrel{C}{=}-2\\,\\mathrm{lse}(\\beta,(\\pmb X\\,\\|\\,\\pmb{O})^{T}\\pmb\\xi)\\,+\\,\\mathrm{lse}(\\beta,\\pmb X^{T}\\pmb\\xi)\\,+\\,\\mathrm{lse}(\\beta,\\pmb{O}^{T}\\pmb\\xi)=\\mathrm{E}_{b}(\\pmb\\xi;\\pmb X,\\pmb O)\\,\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "$f_{b}(\\pmb\\xi)$ can also be interpreted as the variance of a Bernoulli distribution with outcomes $\\mathrm{ID}$ and AUX: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\operatorname{\\rho}_{b}(\\xi)=\\ p({\\mathrm{~AUX~}}|\\ \\xi)\\ p({\\mathrm{~ID~}}|\\ \\xi)=p({\\mathrm{~ID~}}|\\ \\xi\\ )(1-p({\\mathrm{~ID~}}|\\ \\xi\\ ))\\ =\\ p({\\mathrm{~AUX~}}|\\ \\xi\\ )(1-p({\\mathrm{~AUX~}}|\\ \\xi\\ ))\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "In other words, minimizing $\\operatorname{E}_{b}$ means to drive a Bernoulli-distributed random variable with the outcomes ID and AUX towards minimum variance, i.e., $p(\\,\\mathbf{ID}\\mid\\boldsymbol{\\xi}\\,)$ is driven towards 1 if $p(\\,\\mathbf{ID}\\mid\\pmb{\\xi}\\,)>$ 0.5 and towards 0 if $p(\\mathrm{~ID~}|\\boldsymbol{\\xi}\\textbf{)}<0.5$ . Conversely, the same is true for $p(\\mathbf{\\xi}\\mathbf{A}\\mathbf{U}\\mathbf{X}\\mid\\boldsymbol{\\xi}\\mathbf{\\xi})$ . ", "page_idx": 25}, {"type": "text", "text": "From Equation (35), under the assumptions that $||\\pmb{x}_{i}||_{2}\\,=\\,1$ and $||o_{i}||_{2}\\,=\\,1$ and $M\\,=\\,N$ , the conditional probability $p(\\mathbf{\\DeltaAUX}\\mid\\boldsymbol{\\xi}\\mid\\)$ can be computed as follows: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{p(\\mathrm{\\normalfont~AUX~}|\\,\\xi\\,)\\,=\\,\\sigma(\\log(p(\\,\\xi\\,|\\,\\mathrm{AUX}\\,))-\\log(p(\\,\\xi\\,|\\,\\mathrm{ID}\\,)))}\\\\ &{\\,=\\,\\sigma(\\beta\\,(\\log(\\beta,O^{T}\\xi)-\\log(\\beta,X^{T}\\xi)))}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $\\sigma$ denotes the logistic sigmoid function. Similarly, $p(\\mathbf{\\sigma}\\mathbf{ID}\\mid\\boldsymbol{\\xi}\\mathbf{\\varepsilon})$ can be computed using ", "page_idx": 26}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}&{p(\\operatorname{ID}|\\left\\{\\,\\pmb{\\xi}\\,\\right\\})\\;=\\;\\sigma(\\beta\\left(\\operatorname{lse}(\\beta,{\\pmb{X}}^{T}{\\pmb{\\xi}})-\\operatorname{lse}(\\beta,{\\pmb{O}}^{T}{\\pmb{\\xi}})\\right))}\\\\ &{=1-p(\\operatorname{AUX}|\\left\\{\\,{\\pmb{\\xi}}\\,\\right\\})}\\end{array}}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "G.2 Alternative Formulations of $\\operatorname{E}_{b}$ and $f_{b}$ ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "$\\operatorname{E}_{b}$ can be rewritten as follows. ", "page_idx": 26}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}&{\\operatorname{E}_{b}(\\xi;X,O)=-\\ 2\\operatorname{lse}(\\beta,(X\\parallel O)^{T}\\xi)\\ +\\ \\operatorname{lse}(\\beta,X^{T}\\xi)\\ +\\ \\operatorname{lse}(\\beta,O^{T}\\xi)}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ =\\ -\\ 2\\beta^{-1}\\ \\log\\cosh\\left({\\frac{\\beta}{2}}\\left(\\operatorname{lse}(\\beta,X^{T}\\xi)\\ -\\operatorname{lse}(\\beta,O^{T}\\xi)\\right)\\right)\\ -\\ 2\\beta^{-1}\\ \\log(2)}\\end{array}}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "To prove this, we first show the following: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\quad-}&{\\beta^{-1}\\,\\log\\big(\\exp(\\beta\\,\\log(\\beta,X^{T}\\xi)\\,)+\\exp(\\beta\\,\\log(\\beta,O^{T}\\xi)\\,)\\,\\big)}\\\\ &{=}&{\\beta^{-1}\\,\\log\\left(\\exp\\left(\\beta\\,\\beta^{-1}\\log\\left(\\displaystyle\\sum_{i=1}^{N}\\exp(\\beta x_{i}^{T}\\xi)\\right)\\right)+\\exp\\left(\\beta\\,\\beta^{-1}\\log\\left(\\displaystyle\\sum_{i=1}^{N}\\exp(\\beta o_{i}^{T}\\xi)\\right)\\right)\\right)}\\\\ &{=}&{\\beta^{-1}\\,\\log\\left(\\displaystyle\\sum_{i=1}^{N}\\exp(\\beta x_{i}^{T}\\xi)+\\displaystyle\\sum_{i=1}^{N}\\exp(\\beta o_{i}^{T}\\xi)\\right)}\\\\ &{=}&{\\log(\\beta,(X\\|O)^{T}\\xi)\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\quad(61)^{T}\\xi)}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathrm{E}_{\\mathrm{t}}(\\xi;X,O)=-2\\,\\mathrm{lse}(\\beta,(X\\parallel O)^{T}\\xi)+\\mathrm{lse}(\\beta,X^{T}\\xi)+\\mathrm{lse}(\\beta,O^{T}\\xi)}\\\\ {=}&{-2\\beta^{-1}\\,\\log(\\exp(-\\beta\\,E x))+\\exp(-\\beta\\,E O)\\ )-\\,E x\\ -\\ \\mathit{E o}}\\\\ {=}&{-2\\beta^{-1}\\,\\log\\bigg(\\exp(-\\frac{\\beta}{2}\\,E x)+\\exp(-\\beta\\,E o+\\frac{\\beta}{2}\\,E x)\\bigg)\\ -\\ \\mathit{E o}}\\\\ {=}&{-2\\beta^{-1}\\,\\log\\bigg(\\exp(-\\frac{\\beta}{2}\\,E x+\\frac{\\beta}{2}\\,E o)+\\exp(-\\frac{\\beta}{2}\\,E o+\\frac{\\beta}{2}\\,E x\\ )\\bigg)}\\\\ {=}&{-2\\beta^{-1}\\,\\log\\cosh\\bigg(\\frac{\\beta}{2}(-E x+\\frac{\\gamma}{2}\\ O)\\bigg)\\ -\\ 2\\beta^{-1}\\,\\log(2)}\\\\ {=}&{-2\\beta^{-1}\\,\\log\\cosh\\bigg(\\frac{\\beta}{2}\\,\\mathrm{(lse}(\\beta,X^{T}\\xi)\\ -\\ \\mathrm{lse}(\\beta,O^{T}\\xi))\\bigg)\\ -\\ 2\\beta^{-1}\\,\\log(2)}\\\\ {=}&{-2\\beta^{-1}\\,\\log\\cosh\\bigg(\\frac{\\beta}{2}\\,\\mathrm{(lse}(\\beta,O^{T}\\xi)\\ -\\ \\mathrm{lse}(\\beta,X^{T}\\xi))\\bigg)\\ -\\ 2\\beta^{-1}\\,\\log(2)}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "By exponentiation of the above result we obtain ", "page_idx": 26}, {"type": "equation", "text": "$$\nf_{b}(\\pmb\\xi)\\propto\\exp(\\beta\\mathrm{E}_{b}(\\pmb\\xi;\\pmb X,\\pmb O))\\ =\\ \\frac{1}{4\\cosh^{2}\\left(\\frac{\\beta}{2}\\left(\\mathrm{lse}(\\beta,\\pmb X^{T}\\pmb\\xi)\\ -\\ \\mathrm{lse}(\\beta,O^{T}\\pmb\\xi)\\right)\\right)}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "The function $\\log\\cosh(x)$ is related to the negative log-likelihood of the hyperbolic secant distribution (see e.g. Saleh & Saleh, 2022). For values of $x$ close to 0, log cosh can be approximated by $\\textstyle{\\frac{x^{2}}{2}}$ , and for values far from 0, the function behaves as $|x|-\\log(2)$ . ", "page_idx": 26}, {"type": "image", "img_path": "VLQYtVMTYz/tmp/d93f487f7a231a8d005350f09a019c9c43c3800c2237d58089b9d0b247a77769.jpg", "img_caption": ["(a) Probabilities "], "img_footnote": [], "page_idx": 27}, {"type": "image", "img_path": "VLQYtVMTYz/tmp/c1d5fa42c42c217417d27c1626156b60678123a1eda8c482e1d54026ed1fb994.jpg", "img_caption": ["(b) Log-probabilities "], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "Figure 8: The product of two logistic sigmoids yields $f_{b}$ (a); the sum of two log-sigmoids yields $\\log(f_{b})=\\operatorname{E}_{b}$ (b). ", "page_idx": 27}, {"type": "text", "text": "G.3 Derivatives of $\\operatorname{E}_{b}$ ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "In this section, we investigate the derivatives of the energy function $\\operatorname{E}_{b}$ . The derivative of the lse is: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\nabla_{z}\\,\\mathrm{lse}(\\beta,z)\\;=\\;\\nabla_{z}\\;\\beta^{-1}\\;\\,\\mathrm{log}\\left(\\sum_{i=1}^{N}\\exp(\\beta z_{i})\\right)\\;=\\;\\mathrm{softmax}(\\beta\\;z)\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Thus, the derivative of the MHE $\\operatorname{E}(\\xi;X)$ w.r.t. $\\xi$ is: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\nabla_{\\xi}\\operatorname{E}(\\xi;X)\\;=\\;\\nabla_{\\xi}\\left(-\\mathrm{lse}(\\beta,X^{T}\\xi)\\;+\\;{\\frac{1}{2}}\\xi^{T}\\xi\\;+\\;C\\right)\\;=\\;-\\;X\\mathrm{softmax}(\\beta X^{T}\\xi)\\;+\\;\\xi\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "The update rule of the MHN ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\pmb{\\xi}^{t+1}\\:=\\:\\pmb{X}\\mathrm{softmax}(\\beta\\pmb{X}^{T}\\pmb{\\xi}^{t})\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "is derived via the concave-convex procedure. It coincides with the attention mechanisms of Transformers and has been proven to converge globally to stationary points of the energy $\\operatorname{E}(\\xi;X)$ (Ramsauer et al., 2021). It can also be shown that the update rule emerges when performing gradient descent on $\\operatorname{E}(\\xi;X)$ with step size $\\eta=1$ Park et al. (2023): ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{\\xi}^{t+1}\\pmb{\\xi}^{t}-\\,\\pmb{\\eta}\\,\\nabla_{\\pmb{\\xi}}\\mathrm{E}(\\pmb{\\xi}^{t};\\pmb{X})}\\\\ {\\pmb{\\xi}^{t+1}\\pmb{\\mathrm{\\Omega}}=\\,\\pmb{X}\\mathrm{softmax}(\\beta\\pmb{X}^{T}\\pmb{\\xi}^{t})}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "From Equation (71), we can see that the gradient of $\\operatorname{E}_{b}({\\boldsymbol{\\xi}};X,{\\boldsymbol{O}})$ w.r.t. $\\xi$ is: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{7_{\\xi}\\mathrm{E}_{b}(\\xi;X,O)\\,=\\,\\nabla_{\\xi}\\,(-\\,2\\operatorname{lse}(\\beta,(X\\parallel O)^{T}\\xi)\\,+\\,\\operatorname{lse}(\\beta,X^{T}\\xi)\\,+\\,\\operatorname{lse}(\\beta,O^{T}\\xi))\\qquad\\qquad\\qquad(75)}\\\\ &{=\\,-\\,2\\,\\left(X\\parallel O\\right)\\operatorname{softmax}(\\beta(X\\parallel O)^{T}\\xi)\\,+\\,X\\operatorname{softmax}(\\beta X^{T}\\xi)\\,+\\,O\\mathrm{softmax}(\\beta O)}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "When $X\\mathrm{softmax}(\\beta X^{T}{\\pmb\\xi})$ , $O\\mathrm{softmax}(\\beta O^{T}\\pmb{\\xi})$ , $\\mathrm{lse}(\\beta,X^{T}\\pmb{\\xi})$ and $\\mathrm{lse}(\\beta,O^{T}\\pmb{\\xi})$ are available, one can efficiently compute $\\left(X\\parallel O\\right)$ softmax $(\\beta(\\mathbf{\\dot{X}}\\parallel O)^{T}\\pmb{\\xi})$ as follows: ", "page_idx": 27}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}&{(X\\parallel O)\\operatorname{softmax}(\\beta(X\\parallel O)^{T}\\xi)}\\\\ &{\\qquad\\qquad={\\scriptsize\\nabla}_{\\xi}\\operatorname{lse}(\\beta,(X\\parallel O)^{T}\\xi)}\\\\ &{\\qquad\\quad={\\scriptsize\\nabla}_{\\xi}\\,\\beta^{-1}\\log\\left(\\exp(\\beta\\mathrm{lse}(\\beta,X^{T}\\xi))\\,+\\,\\exp(\\beta\\mathrm{lse}(\\beta,O^{T}\\xi))\\right)}\\\\ &{\\qquad\\quad=\\left(X\\operatorname{softmax}(\\beta X^{T}\\xi)\\,\\,\\,\\,\\,O\\operatorname{softmax}(\\beta O^{T}\\xi)\\right)\\operatorname{softmax}\\left(\\beta\\left({\\stackrel{\\mathrm{lse}(\\beta,X^{T}\\xi)}{\\mathrm{lse}(\\beta,O^{T}\\xi)}}\\right)\\right)}\\end{array}}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "We can also compute the gradient of $\\operatorname{E}_{b}(\\pmb{\\xi};\\pmb{X},\\pmb{O})$ w.r.t. $\\xi$ via the log cosh-representation of $\\operatorname{E}_{b}$ (see Equation (68)). The derivative of the log cosh function is ", "page_idx": 28}, {"type": "equation", "text": "$$\n{\\frac{\\mathrm{d}}{\\mathrm{d}x}}\\;\\beta^{-1}\\log\\cosh(\\beta x)\\;=\\;\\operatorname{tanh}(\\beta x)\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Therefore, we can compute the gradient of $\\mathrm{E}_{b}(\\xi;X,O)$ as ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{\\xi}\\operatorname{E}_{b}(\\xi;X,O)}\\\\ &{\\quad=\\begin{array}{l}{\\nabla_{\\xi}\\,-\\,2\\beta^{-1}\\,\\log\\cosh\\left(\\frac{\\beta}{2}\\,(\\log(\\beta,O^{T}\\xi)\\,-\\,\\log(\\beta,X^{T}\\xi))\\right)}\\\\ {\\qquad=\\,-\\operatorname{tanh}\\left(\\frac{\\beta}{2}(\\log(\\beta,O^{T}\\xi)\\,-\\,\\log(\\beta,X^{T}\\xi))\\right)\\bigl(O\\mathrm{softmax}(\\beta O^{T}\\xi)-X\\mathrm{softmax}(\\beta X^{T}\\xi)\\bigr)}\\end{array}}\\\\ &{\\quad=\\begin{array}{l}{\\qquad=\\qquad\\operatorname{tanh}\\left(\\frac{\\beta}{2}(\\log(\\beta,X^{T}\\xi)\\,-\\,\\log(\\beta,O^{T}\\xi))\\right)\\bigl(X\\mathrm{softmax}(\\beta X^{T}\\xi)-O\\mathrm{softmax}(\\beta O^{T}\\xi)\\bigr)}\\\\ {\\qquad=\\,-\\operatorname{tanh}\\left(\\frac{\\beta}{2}(\\log(\\beta,X^{T}\\xi)\\,-\\,\\log(\\beta,O^{T}\\xi))\\right)\\bigl(X\\mathrm{softmax}(\\beta X^{T}\\xi)-O\\mathrm{softmax}(\\beta O^{T}\\xi)\\bigr)}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Next, we would like to compute the gradient of $\\operatorname{E}_{b}({\\boldsymbol{\\xi}};X,{\\boldsymbol{O}})$ w.r.t. the memory matrices $\\mathbf{\\deltaX}$ and $o$ . For this, let us first look at the gradient of the MHE $\\operatorname{E}(\\xi;X)$ w.r.t. a single stored pattern $\\pmb{x}_{i}$ (where $\\mathbf{\\deltaX}$ is the matrix of concatenated stored patterns $(x_{1},x_{2},\\ldots,x_{N}))$ ): ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\nabla_{\\pmb{x}_{i}}\\mathrm{E}(\\pmb{\\xi};\\pmb{X})\\;=\\;-\\;\\xi\\mathrm{softmax}(\\beta X^{T}\\pmb{\\xi})_{i}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Thus, the gradient w.r.t. the full memory matrix $\\mathbf{\\deltaX}$ is ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\nabla_{X}\\mathrm{E}(\\pmb{\\xi};\\pmb{X})\\;=\\;-\\pmb{\\xi}\\mathrm{softmax}(\\beta\\pmb{X}^{T}\\pmb{\\xi})^{T}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "We can now also use the log cosh formulation of $\\mathrm{E}_{b}(\\pmb{\\xi};\\pmb{X},\\pmb{O})$ to compute the gradient of $\\mathrm{E}_{b}(\\pmb{\\xi};\\pmb{X},\\pmb{O})$ , w.r.t $\\mathbf{\\deltaX}$ and $^o$ : ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{X}\\mathrm{E}_{b}(\\xi;X,O)\\,=\\,\\nabla_{X}-\\,2\\beta^{-1}\\,\\log\\cosh\\left(\\frac{\\beta}{2}\\,(\\mathrm{lse}(\\beta,X^{T}\\xi)\\,-\\,\\mathrm{lse}(\\beta,O^{T}\\xi))\\right)}\\\\ &{\\mathrm{~=~-~tanh\\left(\\frac{\\beta}{2}(\\mathrm{lse}(\\beta,X^{T}\\xi)-\\mathrm{lse}(\\beta,O^{T}\\xi))\\right)~\\xi\\mathrm{softmax}(\\beta X^{T}\\xi)^{T}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Analogously, the gradient w.r.t $o$ is ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\nabla_{O}\\mathrm{E}_{b}(\\xi;X,O)\\;=\\;-\\;\\operatorname{tanh}\\left({\\frac{\\beta}{2}}(\\mathrm{lse}(\\beta,O^{T}\\xi)-\\mathrm{lse}(\\beta,X^{T}\\xi))\\right)\\xi\\mathrm{softmax}(\\beta O^{T}\\xi)^{T}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "H Notes on the Relationship between Hopfield Boosting and other methods ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "H.1 Relation to Radial Basis Function Networks ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "This section shows the relation between radial basis function networks (RBF networks; Moody & Darken, 1989) and modern Hopfield energy (following Sch\u00e4f let al., 2022). Consider an RBF network with normalized linear weights: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\varphi(\\pmb\\xi)=\\sum_{i=1}^{N}\\omega_{i}\\exp(-\\frac{\\beta}{2}||\\pmb\\xi-\\pmb{\\mu}_{i}||_{2}^{2})\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where $\\beta$ denotes the inverse tied variance $\\begin{array}{r}{\\beta=\\frac{1}{\\sigma^{2}}}\\end{array}$ , and the $\\omega_{i}$ are normalized using the softmax function: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\omega_{i}=\\mathrm{softmax}(\\beta\\pmb{a})_{i}=\\frac{\\exp(\\beta a_{i})}{\\sum_{j=1}^{N}\\exp(\\beta a_{j})}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "An energy can be obtained by taking the negative log of $\\varphi(\\pmb\\xi)$ : ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{E}(\\xi)\\ =\\ -\\beta^{-1}\\ \\log\\big(\\varphi(\\xi)\\big)}\\\\ &{\\quad=\\ -\\beta^{-1}\\ \\log\\left(\\displaystyle{\\sum_{i=1}^{N}\\omega_{i}\\,\\mathrm{exp}(-\\frac{\\beta}{2}\\,\\|\\xi-\\mu_{i}\\|_{2}^{2})}\\right)}\\\\ &{\\quad=\\ -\\ \\beta^{-1}\\ \\log\\left(\\displaystyle{\\sum_{i=1}^{N}\\mathrm{exp}\\big(\\beta(-\\frac{1}{2}\\|\\xi-\\mu_{i}\\|_{2}^{2})+\\beta^{-1}\\log\\operatorname{softmax}(\\beta a)_{i}\\big)}\\right)}\\\\ &{\\quad=\\ -\\ \\beta^{-1}\\ \\log\\left(\\displaystyle{\\sum_{i=1}^{N}\\mathrm{exp}\\big(\\beta(-\\frac{1}{2}\\|\\xi-\\mu_{i}\\|_{2}^{2}+a_{i}-\\log(\\beta,a))\\Big)}\\right)}\\\\ &{\\quad=\\ -\\ \\beta^{-1}\\ \\log\\left(\\displaystyle{\\sum_{i=1}^{N}\\mathrm{exp}\\big(\\beta(-\\frac{1}{2}\\xi^{T}\\xi+\\mu_{i}^{T}\\xi-\\frac{1}{2}\\mu_{i}^{T}\\mu_{i}+a_{i})\\big)}\\right)+\\log\\big(\\beta,a\\big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Next, we define $\\begin{array}{r}{a_{i}=\\frac{1}{2}\\pmb{\\mu}_{i}^{T}\\pmb{\\mu}_{i}}\\end{array}$ ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\operatorname{E}(\\pmb{\\xi})\\;=\\;-\\;\\beta^{-1}\\,\\log\\left(\\sum_{i=1}^{N}\\exp(\\beta\\pmb{\\mu}_{i}^{T}\\pmb{\\xi})\\right)+{\\frac{1}{2}}\\pmb{\\xi}^{T}\\pmb{\\xi}+\\operatorname{lse}(\\beta,\\pmb{a})\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Finally, we use the fact that $\\operatorname{lse}(\\beta,\\pmb{a})\\leq\\operatorname*{max}_{i}a_{i}+\\beta^{-1}\\log N$ ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\operatorname{E}(\\pmb{\\xi})\\~=~-\\,\\beta^{-1}\\,\\log\\left(\\sum_{i=1}^{N}\\exp(\\beta\\pmb{\\mu}_{i}^{T}\\pmb{\\xi})\\right)+\\frac{1}{2}\\pmb{\\xi}^{T}\\pmb{\\xi}+\\beta^{-1}\\log N+\\frac{1}{2}\\pmb{M}^{2}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where $M=\\operatorname*{max}_{i}||\\mu_{i}||_{2}$ ", "page_idx": 29}, {"type": "text", "text": "H.2 Contrastive Representation Learning ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "A commonly used loss function in contrastive representation learning (e.g., Chen et al., 2020; He et al., 2020) is the InfoNCE loss (Oord et al., 2018): ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathcal{L}_{\\mathrm{NCE}}=}&{\\underset{\\left\\{x,y\\right\\}\\sim p_{\\mathrm{pos}}}{\\mathbb{E}}\\left[-\\log\\frac{e^{f(x)^{T}f(y)/\\tau}}{e^{f(x)^{T}f(y)/\\tau}+\\sum_{i}e^{f(x_{i}^{-})^{T}f(y)/\\tau}}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{NCE}}=\\underset{(x,y)\\sim p_{p o s}}{\\mathbb{E}}\\left[-f(x)^{T}f(y)/\\tau\\right]\\quad+\\underset{(x_{i}^{-})_{i=1}^{M}\\sim p_{d a t a}}{\\mathbb{E}}\\left[\\log\\Biggl(e^{f(x)^{T}f(y)/\\tau}+\\sum_{i}e^{f(x_{i}^{-})^{T}f(x)/\\tau}\\Biggr)\\right]\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Alignment enforces that features from positive pairs are similar, while uniformity encourages a uniform distribution of the samples over the hypersphere. ", "page_idx": 30}, {"type": "text", "text": "In comparison, our proposed loss, $\\mathcal{L}_{\\mathrm{ooD}}$ , does not visibly enforce alignment between samples within the same class. Instead, we can observe that it promotes uniformity to the instances of the foreign class. Due to the constraints that are imposed by the geometry of the space the optimization is performed on, that is, $||f(x)||=1$ when the samples move on a hypersphere, the loss encourages the patterns in the ID data have maximum distance to the samples of the AUX data, i.e., they concentrate on opposing poles of the hypersphere. A demonstration of this mechanism can be found in Appendix F.2 and F.3 ", "page_idx": 30}, {"type": "text", "text": "H.3 Support Vector Machines ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "In the following, we will show the relation of Hopfield Boosting to support vector machines (SVMs;   \nCortes & Vapnik, 1995) with RBF kernel. We adopt and expand the arguments of Sch\u00e4f let al. (2022). ", "page_idx": 30}, {"type": "text", "text": "Assume we apply an SVM with RBF kernel to model the decision boundary between ID and AUX data. We train on the features ${\\pmb Z}~=~({\\pmb x}_{1},\\pmb\\dots,{\\pmb x}_{N},{\\pmb o}_{1},\\pmb\\dots,{\\pmb o}_{M})$ and assume that the patterns are normalized, i.e., $||\\pmb{x}_{i}||_{2}=1$ and $||\\pmb{\\ o}_{i}||_{2}=1$ . We define the targets $\\displaystyle\\bigl(y_{1},\\ldots,y_{(N+M)}\\bigr)$ as 1 for ID and $-1$ for AUX data. The decision rule of the SVM equates to ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\hat{B}(\\pmb\\xi)\\,=\\,\\left\\{\\!\\!\\begin{array}{l l}{{\\mathrm{ID}}}&{{\\mathrm{if}\\;s(\\pmb\\xi)\\geq0}}\\\\ {{\\mathrm{OOD}}}&{{\\mathrm{if}\\;s(\\pmb\\xi)<0}}\\end{array}\\!\\!\\right.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r}{s(\\pmb{\\xi})\\;=\\;\\displaystyle\\sum_{i=1}^{N+M}\\alpha_{i}y_{i}k(z_{i},\\pmb{\\xi})\\medskip}\\\\ {k(z_{i},\\pmb{\\xi})=\\exp\\left(-\\displaystyle\\frac{\\beta}{2}||\\pmb{\\xi}\\,-\\,z_{i}||_{2}^{2}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "We assume that there is at least one support vector for both ID and AUX data, i.e., there exists at least one index $i$ s.t. $\\alpha_{i}y_{i}>0$ and at least one index $j$ s.t. $\\alpha_{j}y_{j}<0$ . We now split the samples $z_{i}$ in $s(\\pmb\\xi)$ according to their label: ", "page_idx": 30}, {"type": "equation", "text": "$$\ns({\\pmb\\xi})\\;=\\;\\sum_{i=1}^{N}\\alpha_{i}k({\\pmb x}_{i},{\\pmb\\xi})\\;-\\;\\sum_{i=1}^{M}\\alpha_{N+i}k({\\pmb o}_{i},{\\pmb\\xi})\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "We define an alternative score: ", "page_idx": 30}, {"type": "equation", "text": "$$\ns_{\\mathrm{frac}}(\\pmb{\\xi})\\;=\\;\\frac{\\sum_{i=1}^{N}\\alpha_{i}k(\\pmb{x}_{i},\\pmb{\\xi})}{\\sum_{i=1}^{M}\\alpha_{N+i}k(\\pmb{o}_{i},\\pmb{\\xi})}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Because we assumed there is at least one support vector for both ID and AUX data and as the $\\alpha_{i}$ are constrained to be non-negative and because $\\bar{k}(\\cdot,\\cdot)>0$ , the numerator and denominator are strictly positive. We can, therefore, specify a new decision rule $\\hat{B}_{\\mathrm{frac}}(\\pmb{\\xi})$ . ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\hat{B}_{\\mathrm{frac}}(\\xi)\\;=\\;\\left\\{\\!\\!\\begin{array}{l l}{{\\mathrm{ID}}}&{{\\mathrm{if}\\;s_{\\mathrm{frac}}(\\xi)\\geq1}}\\\\ {{\\mathrm{OOD}}}&{{\\mathrm{if}\\;s_{\\mathrm{frac}}(\\xi)<1}}\\end{array}\\!\\!\\right.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Although the functions $s(\\pmb\\xi)$ and $s_{\\mathrm{frac}}(\\pmb{\\xi})$ are different, the decision rules $\\hat{B}(\\pmb\\xi)$ and $\\hat{B}_{\\mathrm{frac}}(\\pmb{\\xi})$ are equivalent. Another possible pair of score and decision rule is the following: ", "page_idx": 31}, {"type": "equation", "text": "$$\ns_{\\mathrm{log}}({\\pmb\\xi})\\;=\\;\\beta^{-1}\\log(s_{\\mathrm{frac}}({\\pmb\\xi}))\\;=\\;\\beta^{-1}\\log\\left(\\sum_{i=1}^{N}\\alpha_{i}k({\\pmb x}_{i},{\\pmb\\xi})\\right)\\;-\\;\\beta^{-1}\\log\\left(\\sum_{i=1}^{M}\\alpha_{N+i}k({\\pmb\\sigma}_{i},{\\pmb\\xi})\\right)\n$$", "text_format": "latex", "page_idx": 31}, {"type": "equation", "text": "$$\n\\hat{B}_{\\mathrm{log}}(\\pmb{\\xi})\\;=\\;\\left\\{\\!\\!\\begin{array}{l l}{\\displaystyle\\mathrm{ID}}&{\\mathrm{if}\\;s_{\\mathrm{log}}(\\pmb{\\xi})\\geq0}\\\\ {\\displaystyle\\mathrm{OOD}}&{\\mathrm{if}\\;s_{\\mathrm{log}}(\\pmb{\\xi})<0}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Let us more closely examine the term $\\begin{array}{r}{\\beta^{-1}\\log\\left(\\sum_{i=1}^{N}\\alpha_{i}k(\\pmb{x}_{i},\\pmb{\\xi})\\right)}\\end{array}$ . We define $a_{i}=\\beta^{-1}\\log(\\alpha_{i})$ . ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\beta^{-1}\\log\\Bigg(\\displaystyle\\sum_{i=1}^{N}\\alpha_{i}k(x_{i},\\xi)\\Bigg)}&{=\\beta^{-1}\\log\\Bigg(\\displaystyle\\sum_{i=1}^{N}\\exp(\\beta a_{i})\\exp\\left(-\\displaystyle\\frac{\\beta}{2}||\\xi-x_{i}||_{2}^{2}\\right)\\Bigg)\\qquad\\qquad\\mathrm{~o~r~}\\quad\\beta^{-1}={\\frac{1}{N}},}\\\\ &{=\\beta^{-1}\\log\\left(\\displaystyle\\sum_{i=1}^{N}\\exp(\\beta a_{i})\\exp\\left(-\\displaystyle\\frac{\\beta}{2}\\xi^{T}\\xi+\\beta x_{i}^{T}\\xi-\\displaystyle\\frac{\\beta}{2}x_{i}^{T}x_{i}\\right)\\right)}\\\\ &{=\\ \\beta^{-1}\\log\\left(\\displaystyle\\sum_{i=1}^{N}\\exp\\left(-\\displaystyle\\frac{\\beta}{2}\\xi^{T}\\xi+\\beta x_{i}^{T}\\xi-\\displaystyle\\frac{\\beta}{2}x_{i}^{T}x_{i}+\\beta a_{i}\\right)\\right)\\ \\mathrm{~(~}}\\\\ &{=\\ \\beta^{-1}\\log\\left(\\displaystyle\\sum_{i=1}^{N}\\exp\\left(\\beta x_{i}^{T}\\xi+\\beta a_{i}\\right)\\right)\\ -\\ \\displaystyle\\frac{1}{2}\\xi^{T}\\xi-\\displaystyle\\frac{1}{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "We now construct a memory $X_{H}$ and query $\\xi_{H}$ such that we can compute (115) using the MHE (Equation (5)): ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{{\\bf{X}}_{H}\\;=\\;\\left(\\begin{array}{c c c}{{{\\pmb{x}}_{1}}}&{{\\ldots}}&{{{\\pmb{x}}_{N}}}\\\\ {{a_{1}}}&{{\\ldots}}&{{a_{N}}}\\end{array}\\right)}}\\\\ {{{\\pmb{\\xi}}_{H}\\;=\\;\\left(\\begin{array}{c}{{{\\pmb\\xi}}}\\\\ {{1}}\\end{array}\\right)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "We obtain ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathrm{E}(\\xi_{H};X_{H})\\,=\\,-\\,\\mathrm{lse}(\\beta,X_{H}^{T}\\xi_{H})\\,+\\,\\frac12\\xi_{H}^{T}\\xi_{H}\\,+\\,C}\\\\ {\\,=\\,-\\,\\beta^{-1}\\log\\left(\\sum_{i=1}^{N}\\exp\\left(\\beta x_{i}^{T}\\xi+1\\beta a_{i}\\right)\\right)\\,+\\,\\frac12\\xi^{T}\\xi\\,+\\frac12\\cdot1^{2}\\,+\\,C}\\\\ {\\,=\\,-\\,\\beta^{-1}\\log\\left(\\sum_{i=1}^{N}\\exp\\left(\\beta x_{i}^{T}\\xi+\\beta a_{i}\\right)\\right)\\,+\\,\\frac12\\xi^{T}\\xi\\,+\\,\\frac12\\,+\\,C}\\\\ {\\,=\\,-\\,\\beta^{-1}\\log\\left(\\sum_{i=1}^{N}\\alpha_{i}k(x_{i},\\xi)\\right)\\,+\\,C}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "We construct $O_{H}$ analogously to Equation (116) and thus can compute ", "page_idx": 32}, {"type": "equation", "text": "$$\ns_{\\mathrm{log}}(\\xi)\\;=\\;\\mathrm{E}(\\xi_{H};O_{H})\\;-\\;\\mathrm{E}(\\xi_{H};X_{H})\\;=\\;\\mathrm{lse}(\\beta,X_{H}^{T}\\xi_{H})\\;-\\;\\mathrm{lse}(\\beta,O_{H}^{T}\\xi_{H})\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "which is exactly the score Hopfield Boosting uses for determining whether a sample is OOD (Equation (13)). In contrast to SVMs, Hopfield Boosting uses a uniform weighting of the patterns in the memory when computing the score. However, Hopfield Boosting can emulate a weighting of the patterns by more frequently sampling patterns with high weights into the memory. ", "page_idx": 32}, {"type": "text", "text": "H.4 HE and SHE ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Zhang et al. (2023a) introduce two post-hoc methods for OOD detection using MHE, which are called \u201cHopfield Energy\u201d (HE) and \u201cSimplified Hopfield Energy\u201d (SHE). Like Hopfield Boosting, HE and SHE both employ the MHE to determine whether a sample is ID or OOD. However, unlike Hopfield Boosting, HE and SHE offer no possibility to include AUX data in the training process to improve the OOD detection performance of their method. The rest of this section is structured as follows: First, we briefly introduce the methods HE and SHE, second, we formally analyze the two methods, and third, we relate them to Hopfield Boosting. ", "page_idx": 32}, {"type": "text", "text": "Hopfield Energy (HE) The method HE (Zhang et al., 2023a) computes the OOD score $s_{\\mathrm{HE}}(\\pmb{\\xi})$ as follows: ", "page_idx": 32}, {"type": "equation", "text": "$$\ns_{\\mathrm{HE}}(\\pmb{\\xi})\\;=\\;\\mathrm{lse}(\\beta,X_{c}^{T}\\pmb{\\xi})\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where $\\pmb{X}_{c}\\in\\mathbb{R}^{d\\times N_{c}}$ denotes the memory $(x_{c1},.~.~,x_{c N_{c}})$ containing $N_{c}$ encoded data instances of class $c$ . HE uses the prediction of the ID classification head to determine which patterns to store in the Hopfield memory: ", "page_idx": 32}, {"type": "equation", "text": "$$\nc\\;=\\;\\underset{y}{\\mathrm{argmax}}\\;p(\\;y\\;|\\;\\boldsymbol{\\xi}^{D}\\;)\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Simplified Hopfield Energy (SHE) The method SHE (Zhang et al., 2023a) employs a simplified score $s_{\\mathrm{SHE}}(\\pmb{\\xi})$ : ", "page_idx": 32}, {"type": "equation", "text": "$$\ns_{\\mathrm{SHE}}(\\pmb{\\xi})\\;=\\;m_{c}^{T}\\pmb{\\xi}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where $m_{c}\\in\\mathbb{R}^{d}$ denotes the mean of the patterns in memory $X_{c}$ : ", "page_idx": 32}, {"type": "equation", "text": "$$\nm_{c}\\;=\\;\\frac{1}{N_{c}}\\sum_{i=1}^{N_{c}}x_{c i}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Relation between HE and SHE In the following, we show a simple yet enlightening relation between the scores $s_{\\mathrm{HE}}$ and $s_{\\mathrm{SHE}}$ . For mathematical convenience, we first slightly modify the score $s_{\\mathrm{HE}}$ : ", "page_idx": 32}, {"type": "equation", "text": "$$\ns_{\\mathrm{HE}}(\\pmb{\\xi})\\;=\\;\\mathrm{lse}(\\beta,X_{c}^{T}\\pmb{\\xi})\\;-\\;\\beta^{-1}\\log N_{c}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "All data sets which were employed in the experiments of Zhang et al. (2023a) (CIFAR-10 and CIFAR-100) are class-balanced. Therefore, the additional term $\\beta^{-1}\\log N_{c}$ does not change the result of the OOD detection on those data sets, as it only amounts to the same constant offset for all classes. ", "page_idx": 32}, {"type": "text", "text": "The function ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\operatorname{lse}(\\beta,z)-\\beta^{-1}\\log N\\;=\\;\\beta^{-1}\\log\\left(\\frac{1}{N}\\sum_{i=1}^{N}\\exp(\\beta z_{i})\\right)\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "converges to the mean function as $\\beta\\to0$ : ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{\\beta\\to0}\\,\\bigl(\\mathrm{lse}(\\beta,z)-\\beta^{-1}\\log N\\bigr)\\;=\\;\\frac{1}{N}\\sum_{i=1}^{N}z_{i}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "We now investigate the behavior of $s_{\\mathrm{HE}}$ in this limit: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\operatorname*{lim}_{\\beta\\rightarrow0}\\,(\\mathrm{lse}(\\beta,\\pmb{X}_{c}^{T}\\pmb{\\xi})\\ -\\ \\beta^{-1}\\log N)=\\frac{1}{N}\\sum_{i=1}^{N}(\\pmb{x}_{c i}^{T}\\pmb{\\xi})}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad=\\ \\left(\\displaystyle\\frac{1}{N}\\sum_{i=1}^{N}\\pmb{x}_{c i}\\right)^{T}\\pmb{\\xi}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad=\\pmb{m}_{c}^{T}\\pmb{\\xi}}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where ", "page_idx": 33}, {"type": "equation", "text": "$$\nm_{c}\\;=\\;\\frac{1}{N}\\sum_{i=1}^{N}x_{c i}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Therefore, we have shown that ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{\\beta\\to0}s_{\\mathrm{HE}}(\\pmb{\\xi})=s_{\\mathrm{SHE}}(\\pmb{\\xi})\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Relation of HE and SHE to Hopfield Boosting. In contrast to HE and SHE, Hopfield Boosting uses an AUX data set to learn a decision boundary between the ID and OOD regions during the training process. To do this, our work introduces a novel MHE-based energy function, $\\mathrm{E}_{b}(\\xi;\\bar{X^{,}}O)$ , to determine how close a sample is to the learnt decision boundary. Hopfield Boosting uses this energy function to frequently sample weak learners into the Hopfield memory and for computing a novel Hopfield-based OOD loss $\\mathcal{L}_{\\mathrm{ooD}}$ . To the best our knowledge, we are the first to use MHE in this way to train a neural network. ", "page_idx": 33}, {"type": "text", "text": "The OOD detection score of Hopfield Boosting is ", "page_idx": 33}, {"type": "equation", "text": "$$\ns(\\pmb\\xi)\\;=\\;\\mathrm{lse}(\\beta,X^{T}\\pmb\\xi)\\;-\\;\\mathrm{lse}(\\beta,O^{T}\\pmb\\xi).\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where $\\boldsymbol{X}\\in\\mathbb{R}^{d\\times N}$ contains the full encoded training set $(\\pmb{x}_{1},\\dots,\\pmb{x}_{N})$ of all classes and $O\\in\\mathbb{R}^{d\\times M}$ contains AUX samples. While certainly similar to $s_{\\mathrm{HE}}$ , the Hopfield Boosting score $s$ differs from $s_{\\mathrm{HE}}$ in three crucial aspects: ", "page_idx": 33}, {"type": "text", "text": "1. Hopfield Boosting uses AUX data samples in the OOD detection score in order to create a sharper decision boundary between the ID and OOD regions.   \n2. Hopfield Boosting normalizes the patterns in the memories $\\mathbf{\\deltaX}$ and $^o$ and the query $\\xi$ to unit length, while HE and SHE use unnormalized patterns to construct their memories $X_{c}$ and their query pattern $\\xi$ . ", "page_idx": 33}, {"type": "text", "text": "3. The score of Hopfield Boosting, $s(\\pmb\\xi)$ , contains the full encoded training data set, while $s_{\\mathrm{HE}}$ only contains the patterns of a single class. Therefore Hopfield Boosting computes the similarities of a query sample $\\xi$ to the entire ID data set. In Appendix I.8, we show that this process only incurs a moderate overhead of $7.5\\%$ compared to the forward pass of the ResNet-18. ", "page_idx": 34}, {"type": "text", "text": "The selection of the score function $s(\\xi)$ is only a small aspect of Hopfield Boosting. Hopfield Boosting additionally samples informative AUX data close to the decision boundary, optimizes an MHE-based loss function, and thereby learns a sharp decision boundary between ID and OOD regions. Those three aspects are novel contributions of Hopfield Boosting. In contrast, the work of Zhang et al. (2023a) solely focuses on the selection of a suitable Hopfield-based OOD detection score for post-hoc OOD detection. ", "page_idx": 34}, {"type": "text", "text": "Table 4: OOD detection performance on CIFAR-100. We compare results from Hopfield Boosting, DOS (Jiang et al., 2024), DOE (Wang et al., 2023b), DivOE (Zhu et al., 2023), DAL (Wang et al., 2023a), MixOE (Zhang et al., 2023b), POEM (Ming et al., 2022), EBO-OE (Liu et al., 2020), and MSP-OE (Hendrycks et al., 2019b) on ResNet-18. $\\downarrow$ indicates \u201clower is better\u201d and $\\uparrow$ \u201chigher is better\u201d. All values in $\\%$ . Standard deviations are estimated across five training runs. ", "page_idx": 35}, {"type": "table", "img_path": "VLQYtVMTYz/tmp/1f4ae988a3751905121fe53167de993dc07c586c5d3d715bfceaec5b4b7020e8.jpg", "table_caption": [], "table_footnote": [""], "page_idx": 35}, {"type": "text", "text": "I Additional Experiments & Experimental Details ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "I.1 Results on CIFAR-100 ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "When applying Hopfield Boosting on CIFAR-100 (Table 4), Hopfield Boosting surpasses POEM (the previously best method), improving the mean FPR95 from 11.76 to 7.95. On the SVHN data set, Hopfield Boosting improves the FPR95 metric the most, decreasing it from 33.59 to 13.27. For the LSUN-Resize and iSUN data sets, we observe a similar behavior as we saw in our CIFAR-10 evaluation \u2014 almost all methods achieve a perfect result with regard to the FPR95 metric. ", "page_idx": 35}, {"type": "text", "text": "I.2 Pre-Processing and Transformations ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "For evaluating OOD detection methods, consistent pre-processing and image transformation is crucial. An inconsistent application of image transformations will skew results when comparing different OOD detection methods. We, therefore, apply the same pre-processing steps and transformations to all OOD detection methods we compare. ", "page_idx": 35}, {"type": "text", "text": "CIFAR-10 & CIFAR-100. For CIFAR-10 and CIFAR-100, we apply the following transformations: ", "page_idx": 35}, {"type": "text", "text": "1. RandomCrop (32x32, padding 4)   \n2. RandomHorizontalFlip ", "page_idx": 35}, {"type": "text", "text": "ImageNet-RC. For ImageNet-RC (used as AUX data set for the ID data sets CIFAR-10 and CIFAR-100), we apply the following transformations: ", "page_idx": 35}, {"type": "text", "text": "1. RandomCrop (32x32)   \n2. RandomCrop (32x32, padding 4)   \n3. RandomHorizontalFlip ", "page_idx": 35}, {"type": "text", "text": "ImageNet-1K. For ImageNet-1K, we apply the following transformations. We closely follow the transformations used in the experiments of Zhu et al. (2023): ", "page_idx": 35}, {"type": "text", "text": "1. Resize (224x224)   \n2. RandomCrop (224x224, padding 4)   \n3. RandomHorizontalFlip ", "page_idx": 35}, {"type": "table", "img_path": "VLQYtVMTYz/tmp/7a2d67279959bc25cbfdd560399d087d2049933cc373a45e867870aab957aa0b.jpg", "table_caption": ["Table 5: Comparison between HE, SHE and our version. $\\downarrow$ indicates \u201clower is better\u201d and $\\uparrow$ indicates \u201chigher is better\u201d. "], "table_footnote": [], "page_idx": 36}, {"type": "text", "text": "ImageNet-21K. For ImageNet-21K (used as AUX data set for the ID data sets ImageNet-1K), we apply the following transformations. We closely follow the transformations used in the experiments of Zhu et al. (2023): ", "page_idx": 36}, {"type": "text", "text": "1. RandAugment (Cubuk et al., 2020)   \n2. Resize (224x224)   \n3. RandomCrop (224x224, padding 4)   \n4. RandomHorizontalFlip ", "page_idx": 36}, {"type": "text", "text": "I.3 Comparison HE/SHE ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Since Hopfield Boosting shares similarities with the MHE-based methods HE and SHE (Zhang et al., 2023a), we also looked at the approach as used for their methods. We use the same ResNet-18 as a backbone network as we used in the experiments for Hopfield Boosting, but train it on CIFAR-10 without OE. We modify the approach of Zhang et al. (2023a) to not only use the penultimate layer, but perform a search over all layer activation combinations of the backbone for the best-performing combination. We also do not use the classifier to separate by class. From the search, we see that the concatenated activations of layers 3 and 5 give the best performance on average, so we use this setting. We experience a quite noticeable drop in performance compared to their results (Table 5). Since the computation of the MHE is the same, we assume the reason for the performance drop is the different training of the ResNet-18 backbone network, where (Zhang et al., 2023a) used strong augmentations. ", "page_idx": 36}, {"type": "text", "text": "I.4 Ablations ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "We investigate the impact of different encoder backbone architectures on OOD detection performance with Hopfield Boosting. The baseline uses a ResNet-18 as the encoder architecture. For the ablation, the following architectures are used as a comparison: ResNet-34, ResNet-50, and Densenet-100. It can be observed, that the larger architectures lead to a slight increase in OOD performance (Table 6). We also see that a change in architecture from ResNet to Densenet leads to a different OOD behavior: The result on the Places365 data set is greatly improved, while the performance on SVHN is noticeably worse than on the ResNet architectures. The FPR95 of Densenet on SVHN also shows a high variance, which is due to one of the five independent training runs performing very badly at detecting SVHN samples as OOD: The worst run scores an FPR95 5.59, while the best run achieves an FPR95 of 0.24. ", "page_idx": 36}, {"type": "text", "text": "I.5 Effect on Learned Representation ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "In order to analyze the impact of Hopfield Boosting on learned representations, we utilize the output of our model\u2019s embedding layer (see 4.2) as the input for a manifold learning-based visualization. Uniform Manifold Approximation and Projection (UMAP) McInnes et al. (2018) is a non-linear ", "page_idx": 36}, {"type": "text", "text": "Table 6: Comparison of OOD detection performance on CIFAR-10 of Hopfield Boosting on different encoders. $\\downarrow$ indicates \u201clower is better\u201d and $\\uparrow$ indicates \u201chigher is better\u201d. Standard deviations are estimated across five independent training runs. ", "page_idx": 37}, {"type": "table", "img_path": "VLQYtVMTYz/tmp/dec2f22709040f4a47964e8f51418b853012f8e5c24594fc573fb049eb11c840.jpg", "table_caption": [], "table_footnote": [], "page_idx": 37}, {"type": "image", "img_path": "VLQYtVMTYz/tmp/1325374eaeca636d3f3bff5ddf732c148615f5b1af4e082bb7ffea2266d071e9.jpg", "img_caption": [], "img_footnote": [], "page_idx": 37}, {"type": "text", "text": "Figure 9: UMAP embeddings of ID (CIFAR-10) and OOD (AUX and SVHN) data based on our model trained without (a) and with Hopfield Boosting (b). Clearly, without Hopfield Boosting, the embedded OOD data points tend to overlap with the ID data points, making it impossible to distinguish between ID and OOD. On the other hand, Hopfield Boosting shows a clear separation of ID and OOD data in the embedding. ", "page_idx": 37}, {"type": "text", "text": "dimensionality reduction technique known for its ability to preserve both global and local structure in high-dimensional data. ", "page_idx": 37}, {"type": "text", "text": "First, we train two models \u2013 with and without Hopfield Boosting\u2013 and extract the embeddings of both ID and OOD data sets from them. This results in a 512-dimensional vector representation for each data point, which we further reduce to two dimensions with UMAP. The training data for UMAP always corresponds to the training data of the respective method. That is, the model trained without Hopfield Boosting is solely trained on CIFAR-10 data, and the model trained with Hopfield Boosting is presented with CIFAR-10 and AUX data during training, respectively. We then compare the learned representations concerning ID and OOD data. ", "page_idx": 37}, {"type": "text", "text": "Figure 9 shows the UMAP embeddings of ID (CIFAR-10) and OOD (AUX and SVHN) data based on our model trained without (a) and with Hopfield Boosting (b). Without Hopfield Boosting, OOD data points typically overlap with ID data points, with just a few exceptions, making it difficult to differentiate between them. Conversely, Hopfield Boosting allows to distinctly separate ID and OOD data in the embedding. ", "page_idx": 37}, {"type": "text", "text": "I.6 OOD Examples from the Places 365 Data Set with High Semantic Similarity to CIFAR-10 ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "We observe that Hopfield Boosting and all competing methods struggle with correctly classifying the samples from the Places 365 data set as OOD the most. Table 1 shows that for Hopfield Boosting, the FPR95 for the Places 365 data set with CIFAR-10 as the ID data set is at 4.28. The second worst FPR95 for Hopfield Boosting was measured on the LSUN-Crop data set at 0.82. ", "page_idx": 37}, {"type": "text", "text": "We inspect the 100 images from Places 365 that perform worst (i.e., that achieve the highest score $s(\\pmb{\\xi})\\}$ ) on a model trained with Hopfield Boosting on the CIFAR-10 data set as the in-distribution data set. Figure 10 shows that within those 100 images, the Places 365 data set contains a non-negligible amount of data instances that show objects from semantic classes contained in CIFAR-10 (e.g., horses, automobiles, dogs, trucks, and airplanes). We argue that data instances that clearly show objects of semantic classes contained in CIFAR-10 should be considered as in-distribution, which Hopfield Boosting correctly recognizes. Therefore, a certain amount of error can be anticipated on the Places 365 data set for all OOD detection methods. We leave a closer evaluation of the amount of the anticipated error up to future work. ", "page_idx": 38}, {"type": "text", "text": "For comparison, Figure 11 shows the 100 images from Places 365 with the lowest score $s(\\pmb\\xi)$ , as evaluated by a model trained with Hopfield Boosting on CIFAR-10. There are no objects visible that have clear semantic overlap with the CIFAR-10 classes. ", "page_idx": 38}, {"type": "image", "img_path": "VLQYtVMTYz/tmp/c61b40cbb8ed6e26fcf22c67a558d65c5075b06acca3aceaba0f2e75b8d820a2.jpg", "img_caption": ["Figure 10: The set of top-100 images from the Places 365 data set which Hopfield Boosting recognized as in-distribution. The image captions show $s(\\xi)$ of the respective image below the caption. "], "img_footnote": [], "page_idx": 38}, {"type": "image", "img_path": "VLQYtVMTYz/tmp/194f7824cce1cabb6e6a230ac11f2fa81b70e096f49de12f50ca3772ef0570ed.jpg", "img_caption": ["Figure 11: The set of top-100 images from the Places 365 data set which Hopfield Boosting recognized as out-of-distribution. The image captions show $s(\\pmb\\xi)$ of the respective image below the caption. "], "img_footnote": [], "page_idx": 39}, {"type": "text", "text": "The choice of additional data sets should not be driven by a desire to showcase good performance; rather, we suggest opting for data that highlights weaknesses, as it holds the potential to drive investigations and uncover novel insights. Simple toy data is preferable due to its typically clearer and more intuitive characteristics compared to complex natural image data. In alignment with these considerations, the following data sets captivated our interest: iCartoonFace (Zheng et al., 2020), Four Shapes (smeschke, 2018), and Retail Product Checkout (RPC) (Wei et al., 2022b). In Figure 12, we show random samples from these data sets to demonstrate the noticeable differences compared to CIFAR-10. ", "page_idx": 39}, {"type": "text", "text": "Table 7: Comparison between EBO-OE (Liu et al., 2020) and our version. $\\downarrow$ indicates \u201clower is better\u201d and $\\uparrow$ indicates \u201chigher is better\u201d. ", "page_idx": 40}, {"type": "table", "img_path": "VLQYtVMTYz/tmp/dd3652a414288350a20b4696578558d331273e44624726ecfc925ef63f0f2524.jpg", "table_caption": [], "table_footnote": [], "page_idx": 40}, {"type": "image", "img_path": "VLQYtVMTYz/tmp/05df0ccd30d20ef234eaf2b84fb4b3bf262e38d55fc429c512608ee1ccda3ad5.jpg", "img_caption": ["Figure 12: Random samples from three data sets, each noticeably different from CIFAR-10. First row: iCartoonFace; Second row: Four shapes; Third row: RPC. "], "img_footnote": [], "page_idx": 40}, {"type": "text", "text": "In Table 7, we present some preliminary results using models trained with the respective method on CIFAR-10 as ID data set (as in Table 1). Results for comparison are presented for EBO-OE only, as time constraints prevented experimenting with additional baseline methods. Although one would expect near-perfect results due to the evident disparities with CIFAR-10, Four Shapes (smeschke, 2018) and RPC (Wei et al., 2022b) seem to defy that expectation. Their results indicate a weakness in the capability to identify outliers robustly since many samples are classified as inliers. Only iCartoonFace (Zheng et al., 2020) is correctly detected as OOD, at least to a large degree. Interestingly, the weakness uncovered by this data is present in both methods, although more pronounced in EBO-OE. Therefore, we suspect that this specific behavior may be a general weakness when training OOD detectors using OE, an aspect we plan to investigate further in our future work. ", "page_idx": 40}, {"type": "text", "text": "I.8 Runtime Considerations for Inference ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "When using Hopfield Boosting in inference, an additional inference step is needed to check whether a given sample is ID or OOD. Namely, to obtain the score (Equation (13)) of a query sample $\\xi^{\\mathcal{D}}$ , Hopfield Boosting computes the dot product similarity of the embedding obtained from $\\pmb{\\xi}\\ =\\ \\dot{\\phi}(\\pmb{\\xi}^{D})$ to all samples in the Hopfield memories $\\mathbf{\\deltaX}$ and $^o$ . In our experiments, $\\mathbf{\\deltaX}$ contains the full indistribution data set (50,000 samples) and $^o$ contains a subset of the AUX data set of equal size. We investigate the computational overhead of computing the dot-product similarity to 100,000 samples in relation to the computational load of the encoder. For this, we feed 100 batches of size 1024 to an encoder (1) without using the score and (2) with using the score, measure the runtimes per batch, and compute the mean and standard deviation. We conduct this experiment with four different encoders on an NVIDIA Titan V GPU. The results are shown in Figure 13 and Table 8. One can see that, especially for larger models, the computational overhead of determining the score is very moderate in comparison. ", "page_idx": 40}, {"type": "text", "text": "Table 9: OOD detection performance on CIFAR-10. We compare results from Hopfield Boosting with two extensions of HE (Zhang et al., 2023a) on ResNet-18: $\\mathrm{HE}{+}\\mathrm{AUX}$ includes AUX data in the OOD score. $\\mathrm{HE+OE}$ applies OE (Hendrycks et al., 2019b) during the training process. $\\downarrow$ indicates \u201clower is better\u201d and $\\uparrow$ \u201chigher is better\u201d. All values in $\\%$ . ", "page_idx": 41}, {"type": "table", "img_path": "VLQYtVMTYz/tmp/bd7ffa486d53b20f2e77fc278003283301462efe11a54b3014e8a39ed94ca148.jpg", "table_caption": ["OOD Dataset Metric HB (ours) HE+AUX HE+OE "], "table_footnote": [], "page_idx": 41}, {"type": "image", "img_path": "VLQYtVMTYz/tmp/263f39b0f4b3bb83c6322ffaf9da83c155c0a19f80703963850b8b5cc30e6887.jpg", "img_caption": ["Figure 13: Mean inference runtimes for Hopfield Boosting on four different encoders on an NVIDIA Titan V GPU. We plot the contributions to the total runtime of the encoder and the MHE-based score (Equation (13)) separately. The evaluation shows that the score computation adds a negligible amount of computational overhead to the total runtime. "], "img_footnote": [], "page_idx": 41}, {"type": "table", "img_path": "VLQYtVMTYz/tmp/df7a5370eb2b460664d157d98961a640b5b0e00b72b828e9ee196e91d452806a.jpg", "table_caption": ["Table 8: Inference runtimes for Hopfield Boosting with four different encoders on an NVIDIA Titan V GPU. We compare the runtime of the encoder only and the runtime of the encoder with the MHE-based score computation (Equation (13)) combined. "], "table_footnote": [], "page_idx": 41}, {"type": "text", "text": "I.9 HE and SHE Extensions with AUX Data ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "To show that the unique contributions of Hopfield Boosting (like the energy-based loss $\\mathcal{L}_{\\mathrm{ooD}}$ and the boosting process) are responsible for the superior performance of Hopfield Boosting, we devise two extensions of HE that include AUX data and compare them to Hopfield Boosting. ", "page_idx": 41}, {"type": "image", "img_path": "VLQYtVMTYz/tmp/0170f785ee67c9a193b7cb32872dceb8d81809153890314494bfdcb439531c6c.jpg", "img_caption": ["Figure 14: Tradeoff between classification error and Mean OOD FPR95 for different values of $\\lambda$ . Decreasing the value of $\\lambda$ to 0.1 improves the classification error while maintaining low OOD FPR95. $\\lambda=0$ (i.e., training only the ID classifier) achieves low classification error but dramatically increases the OOD FPR95. "], "img_footnote": [], "page_idx": 42}, {"type": "text", "text": "The first extension $\\mathrm{(HE+AUX)}$ uses a model trained only on the ID data and adapts HE to include an MHE term that measures the energy of $\\xi$ on the AUX data $^o$ : ", "page_idx": 42}, {"type": "equation", "text": "$$\ns_{\\mathrm{mod}}(\\pmb{\\xi})\\;=\\;s_{\\mathrm{HE}}(\\pmb{\\xi})\\;-\\;\\mathrm{lse}(\\beta,\\pmb{O}^{T}\\pmb{\\xi})\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "The second extension $\\scriptstyle(\\mathrm{HE+OE})$ ) applies OE (Hendrycks et al., 2019b) while training the model. It then uses $s_{\\mathrm{HE}}$ to estimate whether a sample is ID or OOD. For both extensions, we select $\\beta$ by minimizing the mean FPR95 on the OOD test data sets to obtain an upper bound of the possible performance of these extensions. The $\\beta$ we selected for $\\mathrm{HE+}$ AUX is 0.001, and for $\\mathrm{HE+OE}$ is 0.01. ", "page_idx": 42}, {"type": "text", "text": "Our results (Table 9) show that Hopfield Boosting is superior to both extensions. $\\mathrm{HE}{+}\\mathrm{AUX}$ results in a mean FPR95 of 19.91, $\\mathrm{HE+OE}$ achieves a mean FPR95 of 2.72. Hopfield Boosting improves on both extensions, achieving a mean FPR95 of 0.92. ", "page_idx": 42}, {"type": "text", "text": "I.10 Ablation on the Hyperparameter $\\lambda$ ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "There is usually an inherent tradeoff between ID accuracy and OOD detection performance when employing OE methods. In practice one can always improve the tradeoff by using models with more capacity \u2014 in the extreme case practitioners can even train a separate ID network. Hence, the model selection process we employed only considered the OOD detection performance and did not take the ID accuracy into account. To investigate if and how this tradeoff can be controlled by changing the hyperparameters of Hopfield Boosting, we conduct the following experiment: ", "page_idx": 42}, {"type": "text", "text": "We (1) ablate the hyperparameter (the weight of the out-of-distribution loss) and run Hopfield Boosting on the CIFAR-10 benchmark; (2) select $\\lambda$ from the range $[0,1]$ with a step size of 0.1; and (3) record the OOD detection performance (the mean FPR95 where the mean is taken over the OOD test data sets) and the ID classification error for the individual settings of $\\lambda$ . ", "page_idx": 42}, {"type": "text", "text": "The results indicate that decreasing the hyperparameter $\\lambda$ improves the ID classification accuracy of Hopfield Boosting (Figure 14). At the same time, the mean OOD AUROC is only moderately influenced: When setting, the hyperparameter setting reported in the original manuscript, the mean ID classification error is $5.98\\%$ , and the mean FPR95 is $0.92\\%$ . When decreasing $\\lambda$ to 0.1, the mean ID classification error improves to $5.02\\%$ . Similarly, the FPR95 only slightly increases to $1.08\\%$ (which is still substantially better than the second-best outlier exposed method, POEM, which achieves a mean FPR95 of $2.28\\%$ ). Hence, practitioners can control the tradeoff between ID classification accuracy and OOD detection performance. ", "page_idx": 42}, {"type": "image", "img_path": "VLQYtVMTYz/tmp/0cbd4091c610e9f8fcc40645c3cf07eaddc77bf73c3abf780aaaac64438958c7.jpg", "img_caption": ["Figure 15: Ablating the number of patterns stored in the Hopfield memories during inference. AUROC on SVHN based on the number of patterns in the Hopfield memory. In (a), $\\mathbf{\\deltaX}$ and $o$ contain the same number of patterns; in (b) $\\mathbf{\\deltaX}$ contains 50, 000 patterns, and we vary the number of patterns in $o$ . The variability of the AUROC is reduced when $\\mathbf{\\deltaX}$ and $o$ contain 50,000 patterns, respectively. "], "img_footnote": [], "page_idx": 43}, {"type": "text", "text": "I.11 Ablation on the Number of Patterns Stored in the Memories during Inference ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "In our implementation of Hopfield Boosting, we fill the memories $\\mathbf{\\deltaX}$ and $o$ with $\\textsl{N}=~50,000$ patterns to compute the score $s(\\xi)$ , respectively. To investigate the robustness of Hopfield Boosting when changing the number of patterns $N$ , we conduct the following experiments: ", "page_idx": 43}, {"type": "text", "text": "1. We train Hopfield Boosting on CIFAR-10 (ID data) and ImageNet (AUX data). During the weight update process, we store 50,000 patterns in the memories $\\mathbf{\\deltaX}$ and $o$ , and then ablate the number of patterns stored in the memories for computing the score $s(\\pmb\\xi)$ at inference time. We evaluate the discriminative power of $s(\\pmb\\xi)$ on SVHN with 1, 5, 10, 50, 100, 500, 1,000, 5,000, 10,000, and 50,000 patterns stored in the memories $\\mathbf{\\deltaX}$ and $^o$ . To investigate the influence of the stochastic process of sampling $N$ patterns from the ID and AUX data sets, we conduct 50 runs for all of the and create boxplots of the runs. The results (Figure 15a) show that sampling 50, 000 patterns has the lowest variability of the individual trials. We argue that the reason for this is that by this time the entire ID data set is stored in the Hopfield memory \u2014 which effectively eliminates stochasticity from randomly selecting $N$ patterns from the ID data. ", "page_idx": 43}, {"type": "text", "text": "2. To verify that we can use $s(\\xi)$ when the number of patterns in $\\mathbf{\\deltaX}$ and $o$ is imbalanced, we fill $\\mathbf{\\deltaX}$ with all 50,000 data instances of CIFAR-10 and fill $^o$ with 1, 5, 10, 50, 100, 500, 1000, 5000, 10,000, and 50,000 data instances of the AUX data set. Then, we evaluate the discriminative power of $s(\\pmb\\xi)$ for the different instances. Our results (Figure 15b) show that Hopfield Boosting is robust to an imbalance in the number of samples in $\\mathbf{\\deltaX}$ and $^o$ . The setting with 50,000 samples in both memories (which is the setting we use in the experiments in our original manuscript) incurs the least variability. ", "page_idx": 43}, {"type": "text", "text": "I.12 Compute Ressources ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Our experiments were conducted on an internal cluster equipped with a variety of different GPU types (ranging from the NVIDIA Titan V to the NVIDIA A100-SXM-80GB). For our experiments on ImageNet-1K, we additionally used resources of an external cluster that is equipped with NVIDIA A100-SXM-64GB GPUs. ", "page_idx": 43}, {"type": "text", "text": "For our experiments with Hopfield Boosting on CIFAR-10 and CIFAR-100, one run (100 epochs) of Hopfield Boosting trained for about 8.0 hours on a single NVIDIA RTX 2080 Ti GPU and required 4.3 GB of VRAM. Fnding the hyperparameters required 160h of compute for CIFAR-10 and CIFAR-100, respectively. These were divided across four RTX 2080 Ti. Estimating the standard deviation required 40 hours of compute on a single RTX 2080 Ti for CIFAR-10 and CIFAR-100 respectively. ", "page_idx": 43}, {"type": "text", "text": "For ImageNet-1K, one run (4 epochs) of Hopfield Boosting trained for about 4.4 hours on a single NVIDIA A-100-SXM64GB GPU and required $26.9\\,\\mathrm{GB}$ of VRAM. Finding the optimal hyperparameters required a total of 86h of compute, divided across 20 NVIDIA A-100-SXM64GB GPUs. Estimating the standard deviation required 22 hours of compute, divided across 5 NVIDIA A-100- SXM64GB GPUs. ", "page_idx": 44}, {"type": "text", "text": "The amount of resources reported above cover the compute for obtaining the results of Hopfield Boosting reported in the paper. The total amount of compute resources for the project is substantially higher. Notable additional compute expenses are preliminary training runs during the development of Hopfield Boosting, and the training runs for tuning the hyperparameters and evaluating the results of the methods we compare Hopfield Boosting to. ", "page_idx": 44}, {"type": "text", "text": "I.13 Data Sets and Licenses ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "We provide a list of the data sets we used in our experiments and, where applicable, specify their licenses: ", "page_idx": 45}, {"type": "text", "text": "\u2022 CIFAR-10 (Krizhevsky, 2009): License unknown \u2022 CIFAR-100 (Krizhevsky, 2009): License unknown \u2022 ImageNet-RC (Chrabaszcz et al., 2017): Custom License2 \u2022 SVHN (Netzer et al., 2011): Creative Commons (CC) \u2022 Textures (Cimpoi et al., 2014): Custom License3 \u2022 iSUN (Xu et al., 2015): License unknown \u2022 Places 365 (L\u00f3pez-Cifuentes et al., 2020): License unknown \u2022 LSUN (Yu et al., 2015): License unknown \u2022 ImageNet-1K (Russakovsky et al., 2015): Custom License2 \u2022 ImagetNet-21K (Ridnik et al., 2021): Custom License2 \u2022 SUN (Isola et al., 2011): License unknown \u2022 iNaturalist (Van Horn et al., 2018): Custom License4 ", "page_idx": 45}, {"type": "table", "img_path": "VLQYtVMTYz/tmp/9480cd4985513fec62cd4f3e2ed552a0e2735bccb1cadddfb1510053aafa8392.jpg", "table_caption": ["Table 10: OOD detection performance on CIFAR-10. We compare results from Hopfield Boosting, PALM (Lu et al., 2024), NPOS (Tao et al., 2023), $S S\\mathrm{D}+$ (Sehwag et al., 2021), ASH (Djurisic et al., 2023), GEN (Liu et al., 2023), EBO (Liu et al., 2020), MaxLogit (Hendrycks et al., 2019a), and MSP (Hendrycks & Gimpel, 2017) on ResNet-18. $\\downarrow$ indicates \u201clower is better\u201d and $\\uparrow$ \u201chigher is better\u201d. All values in $\\%$ . Standard deviations are estimated across five training runs. "], "table_footnote": [], "page_idx": 46}, {"type": "text", "text": "I.14 Non-OE Baselines ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "To confirm the prevailing notion that OE methods can improve the OOD detection capability in general, we compare Hopfield Boosting to 3 training methods (Sehwag et al., 2021; Tao et al., 2023; Lu et al., 2024) and 5 post-hoc methods (Hendrycks & Gimpel, 2017; Hendrycks et al., 2019b; Liu et al., 2020, 2023; Djurisic et al., 2023). For all methods, we train a ResNet-18 on CIFAR-10. For Hopfield Boosting, we use the same training setup as described in section 4.2. For the post-hoc methods, we do not use the auxiliary outlier data. For the training methods, we use the training procedures described in the respective publications for 100 epochs. Notably, all training methods employ stronger augmentations than the OE or the post-hoc methods. The OE and post-hoc methods use the following augmentations (denoted as \u201cWeak\u201d): ", "page_idx": 46}, {"type": "text", "text": "1. RandomCrop (32x32), padding 4   \n2. RandomHorizontalFlip ", "page_idx": 46}, {"type": "text", "text": "The training methods use the following augmentations (denoted as \u201cStrong\u201d): ", "page_idx": 46}, {"type": "text", "text": "1. RandomResizedCrop (32x32), scale 0.2-1   \n2. RandomHorizontalFlip   \n3. ColorJitter applied with probability 0.8   \n4. RandomGrayscale applied with probability 0.2 ", "page_idx": 46}, {"type": "text", "text": "Table 10 shows the results of the comparison of Hopfield Boosting to the post-hoc and training methods. Hopfield Boosting is better at OOD detection than all non-OE baselines on CIFAR-10 in terms of both mean AUROC and mean FPR95 by a large margin. Further, Hopfield Boosting achieves the best OOD detection on all OOD data sets in terms of FPR95 and AUROC, except for SVHN and LSUN-Crop, where PALM (Lu et al., 2024) shows better AUROC results. An interesting avenue for future work is to combine one of the non-OE based training methods with the OE method Hopfield Boosting. ", "page_idx": 46}, {"type": "text", "text": "J Informativeness of Sampling with High Boundary Scores ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "This section adopts and expands the arguments of Ming et al. (2022) on sampling with high boundary scores. ", "page_idx": 47}, {"type": "text", "text": "We assume the extracted features of a trained deep neural network to approximately equal a Gaussian mixture model with equal class priors: ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle p(\\pmb\\xi)\\,=\\,\\frac12\\mathcal{N}(\\pmb\\xi;\\pmb\\mu,\\sigma^{2}I)+\\frac12\\mathcal{N}(\\pmb\\xi;-\\pmb\\mu,\\sigma^{2}I)}\\\\ {\\displaystyle p_{\\mathrm{ID}}(\\pmb\\xi)=p(\\pmb\\xi|\\mathrm{ID})\\,=\\,\\mathcal{N}(\\pmb\\xi;\\pmb\\mu,\\sigma^{2}I)}\\\\ {\\displaystyle p_{\\mathrm{AUX}}(\\pmb\\xi)=p(\\pmb\\xi|\\mathrm{AUX})\\,=\\,\\mathcal{N}(\\pmb\\xi;-\\pmb\\mu,\\sigma^{2}I)}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Using the MHE and sufficient data from those distributions, we can estimate the densities $p(\\pmb\\xi)$ , $p(\\pmb{\\xi}|\\mathrm{ID})$ and $p(\\pmb{\\xi}|\\mathrm{AUX})$ . ", "page_idx": 47}, {"type": "text", "text": "Lemma J.1. (see Lemma $E.I$ in Ming et al. (2022)) Assume the $M$ sampled data points $o_{i}\\sim p_{A U X}$ satisfy the following constraint on high boundary scores $\\operatorname{E}_{b}(\\pmb{\\xi})$ ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\frac{-\\sum_{i=1}^{M}\\mathrm{E}_{b}(\\pmb{o}_{i})}{M}\\leq\\epsilon\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Then they have ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{M}|2\\pmb{\\mu}^{T}\\pmb{o}_{i}|\\leq M\\epsilon\\sigma^{2}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Proof. They first obtain the expression for $\\operatorname{E}_{b}({\\pmb\\xi})$ under the Gaussian mixture model described above and can express $p\\big(\\mathbf{AUX}|\\pmb{\\xi}\\big)$ as ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{p(\\mathrm{AUX}|\\xi)\\!\\!}&{=\\frac{p(\\xi|\\mathrm{AUX})p(\\mathrm{AUX})}{p(\\xi)}}\\\\ &{=\\frac{\\frac{1}{2}p(\\xi|\\mathrm{AUX})}{\\frac{1}{2}p(\\xi|\\mathrm{ID})\\,+\\,\\frac{1}{2}p(\\xi|\\mathrm{AUX})}}\\\\ &{=\\frac{(2\\pi\\sigma^{2})^{-d/2}\\exp(-\\frac{1}{2\\sigma^{2}})^{-d/2}\\exp(-\\frac{1}{2\\sigma^{2}}||\\xi-\\mu||_{2}^{2})}{(2\\pi\\sigma^{2})^{-d/2}\\exp(-\\frac{1}{2\\sigma^{2}}||\\xi+\\mu||_{2}^{2})\\,+\\,\\,(2\\pi\\beta^{-1})^{-d/2}\\exp(-\\frac{1}{2\\sigma^{2}}||\\xi-\\mu||_{2}^{2})}}\\\\ &{=\\frac{1}{1\\,+\\,\\exp(-\\frac{1}{2\\sigma^{2}}(||\\xi-\\mu||_{2}^{2}-||\\xi+\\mu||_{2}^{2}))}}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "When defining $\\begin{array}{r}{f_{\\mathrm{AUX}}(\\pmb{\\xi})~=~\\frac{1}{2\\sigma^{2}}(||\\pmb{\\xi}-\\pmb{\\mu}||_{2}^{2}-||\\pmb{\\xi}+\\pmb{\\mu}||_{2}^{2})}\\end{array}$ such that $p(\\mathbf{AUX}|\\pmb{\\xi})\\ =\\sigma(f_{\\mathrm{AUX}}(\\pmb{\\xi}))\\ =$ 1 + exp(1\u2212fAUX(\u03be)), they define Eb as follows: ", "page_idx": 47}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}{\\operatorname{E}_{b}({\\pmb\\xi})\\,=\\,-|f_{\\mathrm{AUX}}({\\pmb\\xi})|}\\\\ {\\,=\\,-{\\frac{1}{2\\sigma^{2}}}\\,|\\,|\\pmb{\\xi}-{\\pmb\\mu}||_{2}^{2}-||\\pmb{\\xi}+{\\pmb\\mu}||_{2}^{2}\\mid}\\\\ {\\,=\\,-{\\frac{1}{2\\sigma^{2}}}\\,|\\,\\xi^{T}\\pmb{\\xi}-2{\\pmb\\mu}^{T}\\pmb{\\xi}+{\\pmb\\mu}^{T}{\\pmb\\mu}-(\\pmb{\\xi}^{T}\\pmb{\\xi}+2{\\pmb\\mu}^{T}\\pmb{\\xi}+{\\pmb\\mu}^{T}{\\pmb\\mu})|}\\\\ {\\,=\\,-{\\frac{|2{\\pmb\\mu}^{T}\\pmb{\\xi}|}{\\sigma^{2}}}}\\end{array}}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Therefore, the constraint in Equation (141) is translated to ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{M}|2\\boldsymbol{\\mu}^{T}\\pmb{o}_{i}|\\;\\leq\\;M\\epsilon\\sigma^{2}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "As $\\begin{array}{r}{\\operatorname*{max}_{i\\in M}|{\\pmb\\mu}^{T}{\\pmb o}_{i}|\\leq\\sum_{i=1}^{M}|{\\pmb\\mu}^{T}{\\pmb o}_{i}|}\\end{array}$ given a fixed $M$ , the selected samples can be seen as generated from $p_{\\mathrm{{AUX}}}$ with the constraint that all samples lie within the two hyperplanes in Equation (150). ", "page_idx": 48}, {"type": "text", "text": "Parameter estimation. Now they show the benefit of such constraint in controlling the sample complexity. Assume the signal/noise ratio is large: ${\\frac{||\\pmb{\\mu}||}{\\sigma}}=r\\gg1$ , and $\\epsilon\\leq1$ is some constant. Assume the classifier is given by ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\pmb{\\theta}\\;=\\;\\frac{1}{N+M}(\\sum_{i=1}^{M}\\pmb{x}_{i}-\\sum_{i=1}^{N}\\pmb{o}_{i})\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "where $o_{i}\\sim p_{\\mathrm{AUX}}$ and $x_{i}\\sim p_{\\mathrm{ID}}$ . One can decompose $\\pmb{\\theta}$ . Assuming $M=N$ : ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\theta=\\mu+\\frac{1}{2}\\eta+\\frac{1}{2}\\omega}\\\\ {\\displaystyle\\eta\\,=\\,\\frac{1}{N}(\\sum_{i=1}^{N}x_{i})-\\mu}\\\\ {\\displaystyle\\omega\\,=\\,\\frac{1}{N}(\\sum_{i=1}^{M}-o_{i})-\\mu}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "We would now like to determine the distributions of the random variables $||\\boldsymbol{\\eta}||_{2}^{2}$ and $\\pmb{\\mu}^{T}\\pmb{\\eta}$ ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{||\\eta||_{2}^{2}=\\displaystyle\\sum_{i=1}^{d}\\eta_{i}^{2}}}\\\\ {{\\eta_{i}\\,\\sim\\!N(0,\\displaystyle\\frac{\\sigma^{2}}{N})}}\\\\ {{\\displaystyle\\frac{\\sqrt{N}}{\\sigma}\\eta_{i}\\,\\sim\\,\\mathcal{N}(0,1)}}\\\\ {{\\displaystyle(\\frac{\\sqrt{N}}{\\sigma}\\eta_{i})^{2}\\,\\sim\\,\\chi_{1}^{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Therefore, for $||\\boldsymbol{\\eta}||_{2}^{2}$ we have ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\frac{N}{\\sigma^{2}}||\\pmb{\\eta}||_{2}^{2}=\\sum_{i=1}^{d}(\\frac{\\sqrt{N}}{\\sigma}\\eta_{i})^{2}\\;\\sim\\;\\chi_{d}^{2}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Now we would like to determine the distribution of $\\pmb{\\mu}^{T}\\pmb{\\eta}$ : ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\displaystyle\\mu^{T}\\eta~{=}~\\sum_{i=1}^{d}{\\mu_{i}}~\\eta_{i}}}\\\\ {{\\displaystyle\\mu_{i}\\ \\eta_{i}~{\\sim}~N(0,\\frac{\\sigma^{2}\\mu_{i}^{2}}{N})}}\\\\ {{\\displaystyle\\sum_{i=1}^{d}{\\mu_{i}}~\\eta_{i}~{\\sim}~N(0,\\sum_{i=1}^{d}\\frac{\\sigma^{2}\\mu_{i}^{2}}{N})}}\\\\ {{\\displaystyle\\sum_{i=1}^{d}{\\mu_{i}}\\ \\eta_{i}~{\\sim}~N(0,\\frac{\\sigma^{2}}{N}\\sum_{i=1}^{d}\\mu_{i}^{2})}}\\\\ {{\\displaystyle\\frac{\\mu^{T}\\eta}{\\|\\mu\\|}~\\sim\\ N(0,\\frac{\\sigma^{2}}{N})}}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Concentration bounds. They now develop concentration bounds for $||\\boldsymbol{\\eta}||_{2}^{2}$ and $\\pmb{\\mu}^{T}\\pmb{\\eta}$ . First, we look at $||\\boldsymbol{\\eta}||_{2}^{2}$ . A concentration bound for $\\chi_{d}^{2}$ is: ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\mathbb{P}(X-d\\geq2{\\sqrt{d x}}+2x)\\leq\\exp(-x)\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "By assuming $\\begin{array}{r}{x=\\frac{d}{8\\sigma^{2}}}\\end{array}$ we obtain ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}(X-d\\geq2\\sqrt{d\\frac{d}{8\\sigma^{2}}}+2\\frac{d}{8\\sigma^{2}})\\leq\\exp(-\\frac{d}{8\\sigma^{2}})}\\\\ {\\mathbb{P}(X\\geq\\frac{d}{\\sqrt{2}\\sigma}+\\frac{d}{4\\sigma^{2}}+d)\\leq\\exp(-\\frac{d}{8\\sigma^{2}})}\\\\ {\\mathbb{P}(\\frac{N}{\\sigma^{2}}||\\pmb{\\eta}||_{2}^{2}\\geq\\frac{d}{\\sqrt{2}\\sigma}+\\frac{d}{4\\sigma^{2}}+d)\\leq\\exp(-\\frac{d}{8\\sigma^{2}})}\\\\ {\\mathbb{P}(||\\pmb{\\eta}||_{2}^{2}\\geq\\frac{\\sigma^{2}}{N}(\\frac{d}{\\sqrt{2}\\sigma}+\\frac{d}{4\\sigma^{2}}+d))\\leq\\exp(-\\frac{d}{8\\sigma^{2}})}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "If $d\\geq2$ we have that5 ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\frac{d}{\\sqrt{2}\\sigma}+\\frac{d}{4\\sigma^{2}}>\\frac{1}{\\sigma}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "and thus, the above bound can be simplified when assuming $d\\geq2$ as follows: ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\mathbb{P}(||\\pmb{\\eta}||_{2}^{2}\\geq\\frac{\\sigma^{2}}{N}(\\frac{1}{\\sigma}+d))\\leq\\exp(-\\frac{d}{8\\sigma^{2}})\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "For $||\\boldsymbol{\\omega}||_{2}^{2}$ , since all $\\pmb{o}_{i}$ is drawn i.i.d. from $p_{\\mathrm{{AUX}}}$ , under the constraint in Equation (150), the distribution of $\\omega$ can be seen as a truncated distribution of $\\eta$ . Thus, with some finite positive constant $c$ , we have ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\mathbb{P}(||\\boldsymbol\\omega||_{2}^{2}\\geq\\frac{\\sigma^{2}}{N}(d+\\frac{1}{\\sigma}))\\,\\leq\\,c\\mathbb{P}(||\\boldsymbol\\eta||_{2}^{2}\\geq\\frac{\\sigma^{2}}{N}(d+\\frac{1}{\\sigma}))\\leq c\\exp(-\\frac{d}{8\\sigma^{2}})\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Now, we develop a bound for $\\pmb{\\mu}^{T}\\pmb{\\eta}$ . A concentration bound for $\\mathcal{N}(\\mu,\\sigma^{2})$ is ", "page_idx": 49}, {"type": "text", "text": "5Strictly, the bound is valid for $d>\\sqrt{2}$ ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\mathbb{P}(X-\\mu\\geq t)\\leq\\exp(\\frac{-t^{2}}{2\\sigma^{2}})\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "By applying $\\begin{array}{r}{\\frac{\\pmb{\\mu}^{T}\\pmb{\\eta}}{||\\pmb{\\mu}||}\\sim\\mathcal{N}(0,\\frac{\\sigma^{2}}{N})}\\end{array}$ to the above bound we obtain ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\mathbb{P}(\\frac{\\pmb{\\mu}^{T}\\pmb{\\eta}}{\\vert\\vert\\pmb{\\mu}\\vert\\vert}\\geq t)\\leq\\exp(\\frac{-t^{2}N}{2\\sigma^{2}})\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "Assuming $t=(\\sigma||\\pmb{\\mu}||)^{1/2}$ we obtain ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{\\mathbb{P}(\\frac{{\\pmb\\mu}^{T}{\\pmb\\eta}}{||{\\pmb\\mu}||}\\geq(\\sigma||{\\pmb\\mu}||)^{1/2})\\leq\\exp(\\frac{-(\\sigma||{\\pmb\\mu}||)N}{2\\sigma^{2}})}\\\\ {\\mathbb{P}(\\frac{{\\pmb\\mu}^{T}{\\pmb\\eta}}{||{\\pmb\\mu}||}\\geq(\\sigma||{\\pmb\\mu}||)^{1/2})\\leq\\exp(\\frac{-||{\\pmb\\mu}||N}{2\\sigma})}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "Due to symmetry, we have ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{P}(-\\frac{\\mu^{T}\\eta}{||\\mu||}\\le-(\\sigma||\\mu||)^{1/2})\\le\\exp(\\frac{-||\\mu||N}{2\\sigma})}\\\\ {\\displaystyle\\mathbb{P}(-\\frac{\\mu^{T}\\eta}{||\\mu||}\\le-(\\sigma||\\mu||)^{1/2})+\\mathbb{P}(\\frac{\\mu^{T}\\eta}{||\\mu||}\\ge(\\sigma||\\mu||)^{1/2})\\le2\\exp(\\frac{-||\\mu||N}{2\\sigma})}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "We can rewrite the above bound using the absolute value function. ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\mathbb{P}(\\frac{|\\pmb{\\mu}^{T}\\pmb{\\eta}|}{||\\pmb{\\mu}||}\\geq(\\sigma||\\pmb{\\mu}||)^{1/2})\\leq2\\exp(\\frac{-||\\pmb{\\mu}||N}{2\\sigma})\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "Benefit of high boundary scores. We will now show why sampling with high boundary scores is beneficial. Recall the results from Equations (150) and (154): ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\sum_{i=1}^{M}|2\\mu^{T}o_{i}|\\,\\leq\\,M\\epsilon\\sigma^{2}}\\\\ {\\displaystyle\\qquad\\qquad\\omega\\,=\\,\\frac{1}{M}(-\\sum_{i=1}^{M}o_{i})-\\mu}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "The triangle inequality is ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|a+b|\\leq|a|+|b|}\\\\ {|a+(-b)|\\leq|a|+|b|}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "Using the two facts above and the triangle inequality we can bound $|\\pmb{\\mu}^{T}\\omega|$ : ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\displaystyle\\frac{1}{M}|\\displaystyle\\sum_{i=1}^{M}\\mu^{T}o_{i}|\\leq\\frac{\\sigma^{2}\\epsilon}{2}}}\\\\ {{\\displaystyle\\frac{1}{M}|-\\displaystyle\\sum_{i=1}^{M}\\mu^{T}o_{i}|\\leq\\frac{\\sigma^{2}\\epsilon}{2}}}\\\\ {{\\displaystyle\\frac{1}{M}|-\\displaystyle\\sum_{i=1}^{M}\\mu^{T}o_{i}|+||\\mu||_{2}^{2}\\leq\\frac{\\sigma^{2}\\epsilon}{2}+||\\mu||_{2}^{2}}}\\\\ {{\\displaystyle\\frac{1}{M}|-\\displaystyle\\sum_{i=1}^{M}\\mu^{T}o_{i}-\\mu^{T}\\mu|\\leq\\frac{\\sigma^{2}\\epsilon}{2}+||\\mu||_{2}^{2}}}\\\\ {{\\displaystyle\\vert\\mu^{T}\\omega\\vert\\leq\\vert\\vert\\mu\\vert\\vert_{2}^{2}+\\frac{\\sigma^{2}\\epsilon}{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "Developing a lower bound. Let ", "text_level": 1, "page_idx": 51}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\displaystyle||\\pmb{\\eta}||_{2}^{2}\\leq\\frac{\\sigma^{2}}{N}(d+\\frac{1}{\\sigma})}\\\\ {\\displaystyle||\\pmb{\\omega}||_{2}^{2}\\leq\\frac{\\sigma^{2}}{N}(d+\\frac{1}{\\sigma})}\\\\ {\\displaystyle\\frac{|\\pmb{\\mu}^{T}\\pmb{\\eta}|}{||\\pmb{\\mu}||}\\leq(\\sigma||\\pmb{\\mu}||)^{1/2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "hold simultaneously. The probability of this happening can be bounded as follows: We define $T$ and its complement $\\bar{T}$ : ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle{T=\\{||\\eta||_{2}^{2}\\leq\\frac{\\sigma^{2}}{N}(d+\\frac{1}{\\sigma})\\}\\cap\\{||\\omega||_{2}^{2}\\leq\\frac{\\sigma^{2}}{N}(d+\\frac{1}{\\sigma})\\}\\cap\\{\\frac{|\\mu^{T}\\eta|}{||\\mu||}\\leq(\\sigma||\\mu||)^{1/2}\\}}}\\\\ {\\displaystyle{\\bar{T}=\\{||\\eta||_{2}^{2}>\\frac{\\sigma^{2}}{N}(d+\\frac{1}{\\sigma})\\}\\cup\\{||\\omega||_{2}^{2}>\\frac{\\sigma^{2}}{N}(d+\\frac{1}{\\sigma})\\}\\cup\\{\\frac{|\\mu^{T}\\eta|}{||\\mu||}>(\\sigma||\\mu||)^{1/2}\\}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "With $\\mathbb{P}(T)+\\mathbb{P}(\\bar{T})=1$ . The probability $\\mathbb{P}(\\bar{T})$ can be bounded using Boole\u2019s inequality and the results in Equations (171), (172) and (179): ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}(\\bar{T})\\leq\\exp(-d/8\\sigma^{2})+c\\exp(-d/8\\sigma^{2})+2\\exp(\\frac{-||\\mu||N}{2\\sigma})}\\\\ &{\\mathbb{P}(\\bar{T})\\leq(1+c)\\exp(-d/8\\sigma^{2})+2\\exp(\\frac{-||\\mu||N}{2\\sigma})}\\end{array}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "Further, we can bound the probability $\\mathbb{P}(T)$ : ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{~~~~~\\mathbb{P}(\\bar{T})\\leq(1+c)\\exp(-d/8\\sigma^{2})+2\\exp(\\frac{-||\\mu||N}{2\\sigma})}\\\\ &{1-\\mathbb{P}(T)\\leq(1+c)\\exp(-d/8\\sigma^{2})+2\\exp(\\frac{-||\\mu||N}{2\\sigma})}\\\\ &{~~~~~\\mathbb{P}(T)\\geq1-(1+c)\\exp(-d/8\\sigma^{2})-2\\exp(\\frac{-||\\mu||N}{2\\sigma})}\\end{array}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "Therefore, the probability of the assumptions in Equations (189), (190), and (191) occuring simultneously is at least $\\begin{array}{r}{1-(1+c)\\exp(-d/8\\sigma^{2})-2\\exp(\\frac{-||\\mu||N}{2\\sigma})}\\end{array}$ . ", "page_idx": 51}, {"type": "text", "text": "By using the triangle inequality, Equation (152) and the Assumptions (189) and (190) they can bound $||\\dot{\\pmb{\\theta}}||_{2}^{2}$ : ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle|\\theta||_{2}^{2}=||\\,\\mu\\,+\\,\\frac12\\,\\eta\\,+\\,\\frac12\\,\\omega||_{2}^{2}}}\\\\ {{\\displaystyle|\\theta||_{2}^{2}\\le\\,||\\mu||_{2}^{2}+\\,||\\frac12\\,\\eta||_{2}^{2}+\\,||\\frac12\\,\\omega||_{2}^{2}}}\\\\ {{\\displaystyle||\\theta||_{2}^{2}\\le\\,||\\mu||_{2}^{2}+\\,\\frac14||\\eta||_{2}^{2}+\\,\\frac14||\\omega||_{2}^{2}}}\\\\ {{\\displaystyle||\\theta||_{2}^{2}\\le\\,||\\mu||_{2}^{2}+\\,\\frac12\\frac{\\sigma^{2}}{N}(d+\\frac1\\sigma)}}\\\\ {{\\displaystyle||\\theta||_{2}^{2}\\le\\,||\\mu||_{2}^{2}+\\,\\frac{\\sigma^{2}}{N}(d+\\frac1\\sigma)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "The reverse triangle inequality is defined as ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|x-y|\\geq||x|-|y|}\\\\ {|x-(-y)|\\geq||x|-|y|}\\end{array}\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "Using the reverse triangle inequality, Equations (152), (188) and Assumption (191) we have that ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|\\mu^{T}\\theta|=|\\mu^{T}\\mu\\ +\\ \\frac{1}{2}\\mu^{T}\\eta\\ +\\ \\frac{1}{2}\\ \\mu^{T}\\omega|}\\\\ &{|\\mu^{T}\\theta|\\geq\\big||\\mu^{T}\\mu|\\ -\\ |\\frac{1}{2}\\mu^{T}\\eta|\\ -\\ |\\frac{1}{2}\\,\\mu^{T}\\omega|\\big|}\\\\ &{|\\mu^{T}\\theta|\\geq\\big|||\\mu||_{2}^{2}\\ -\\ \\frac{1}{2}\\sigma^{1/2}||\\mu||^{3/2}\\ -\\ \\frac{1}{2}||\\mu||_{2}^{2}\\ -\\ \\frac{1}{2}\\frac{\\sigma^{2}\\epsilon}{2}\\big|}\\\\ &{|\\mu^{T}\\theta|\\geq\\big|\\frac{1}{2}||\\mu||_{2}^{2}\\ -\\ \\frac{1}{2}\\sigma^{1/2}||\\mu||^{3/2}\\ -\\ \\frac{1}{2}\\frac{\\sigma^{2}\\epsilon}{2}|}\\\\ &{|\\mu^{T}\\theta|\\geq\\big|\\frac{1}{2}(||\\mu||_{2}^{2}\\ -\\ \\sigma^{1/2}||\\mu||^{3/2}\\ -\\ \\frac{\\sigma^{2}\\epsilon}{2})\\big|}\\end{array}\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "They have assumed that the signal/noise ratio is large: ${\\frac{||\\pmb{\\mu}||}{\\sigma}}=r\\gg1$ . Thus, we can drop the absolute value, because we assume that the term inside the $||$ is larger than zero: ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{|\\pmb{\\mu}^{T}\\pmb{\\theta}|\\geq\\big|\\frac{1}{2}(||\\pmb{\\mu}||_{2}^{2}\\,-\\,\\frac{1}{r}||\\pmb{\\mu}||^{1/2}||\\pmb{\\mu}||^{3/2}\\,-\\,\\frac{||\\pmb{\\mu}||_{2}^{2}\\epsilon}{2r^{2}})\\big|}}\\\\ {{|\\pmb{\\mu}^{T}\\pmb{\\theta}|\\geq\\big|(1-\\frac{1}{r}-\\frac{\\epsilon}{2r^{2}})\\frac{1}{2}(||\\pmb{\\mu}||_{2}^{2})\\big|}}\\end{array}\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "We have ", "page_idx": 52}, {"type": "equation", "text": "$$\n(1-\\frac{1}{r}-\\frac{\\epsilon}{2r^{2}})\\geq0\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "if r \u22651.36602540378443 . . . and $\\epsilon\\leq1$ , and therefore ", "page_idx": 52}, {"type": "equation", "text": "$$\n|\\pmb{\\mu}^{T}\\pmb{\\theta}|\\geq\\frac{1}{2}(||\\pmb{\\mu}||_{2}^{2}~-~\\sigma^{1/2}||\\pmb{\\mu}||^{3/2}~-~\\frac{\\sigma^{2}\\epsilon}{2})\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "Because of Equation (203) and the fact that if $x\\leq y$ and $\\operatorname{sgn}\\left(x\\right)=\\operatorname{sgn}\\left(y\\right)$ then $x^{-1}\\geq y^{-1}$ we have ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\frac{1}{||\\pmb{\\theta}||}\\geq\\frac{1}{\\sqrt{||\\pmb{\\mu}||_{2}^{2}\\,+\\,\\frac{\\sigma^{2}}{N}(d+\\frac{1}{\\sigma})}}\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "We can combine the Equations (214) and (215) to give a single bound: ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\frac{|\\pmb{\\mu}^{T}\\pmb{\\theta}|}{||\\pmb{\\theta}||}\\geq\\frac{||\\pmb{\\mu}||_{2}^{2}\\,-\\,\\sigma^{1/2}||\\pmb{\\mu}||^{3/2}\\,-\\,\\frac{\\sigma^{2}\\epsilon}{2}}{2\\sqrt{||\\pmb{\\mu}||_{2}^{2}\\,+\\,\\frac{\\sigma^{2}}{N}(d+\\frac{1}{\\sigma})}}\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "we define $\\pmb{\\theta}$ such that $\\pmb{\\mu}^{T}\\pmb{\\theta}>0$ and thus ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\frac{\\pmb{\\mu}^{T}\\pmb{\\theta}}{||\\pmb{\\theta}||}\\ge\\frac{||\\pmb{\\mu}||_{2}^{2}\\,-\\,\\sigma^{1/2}||\\pmb{\\mu}||^{3/2}\\,-\\,\\frac{\\sigma^{2}\\epsilon}{2}}{2\\sqrt{||\\pmb{\\mu}||_{2}^{2}\\,+\\,\\frac{\\sigma^{2}}{N}(d+\\frac{1}{\\sigma})}}\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "The false negative rate $\\operatorname{FNR}(\\pmb\\theta)$ and false positive rate $\\operatorname{FPR}(\\pmb\\theta)$ are ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathrm{FNR}(\\pmb\\theta)=\\int_{-\\infty}^{0}\\mathcal{N}(x;\\frac{\\pmb\\mu^{T}\\pmb\\theta}{||\\pmb\\theta||},\\sigma^{2})\\,\\mathrm{d}x}\\\\ {\\displaystyle\\mathrm{FPR}(\\pmb\\theta)=\\int_{0}^{\\infty}\\mathcal{N}(x;\\frac{-\\pmb\\mu^{T}\\pmb\\theta}{||\\pmb\\theta||},\\sigma^{2})\\,\\mathrm{d}x}\\end{array}\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "As $N(x;\\mu,\\sigma^{2})=N(-x;-\\mu,\\sigma^{2})$ , we have $\\mathrm{FNR}(\\pmb\\theta)=\\mathrm{FPR}(\\pmb\\theta)$ . From Equation (217) we can see that as \u03f5 decreases, the lower bound of |\u00b5|T\u03b8|\u03b8| will increase. Thus, the mean of the Gaussian distribution in Equation (218) will increase and therefore, the false negative rate will decrease, which shows the benefit of sampling with high boundary scores. This completes the extended proof adapted from (Ming et al., 2022). ", "page_idx": 53}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 54}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 54}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 54}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 54}, {"type": "text", "text": "Justification: The claims made in the abstract and introduction regarding improvement in OOD detection capabilities are backed by extensive experiments in Section 4. Our theoretical results in the Appendix (most notably Sections G.1, H.1, and J) support the applicability of our method for OOD detection with OE. ", "page_idx": 54}, {"type": "text", "text": "Guidelines: ", "page_idx": 54}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 54}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 54}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 54}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 54}, {"type": "text", "text": "Justification: Section 4.3 and Appendix I.7 show that when subjecting Hopfield Boosting to data sets that have been designed to show the weaknesses of OE methods, we identify instances where a substantial number of outliers are wrongly classified as inliers by Hopfield Boosting and EBO-OE. ", "page_idx": 54}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 54}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 54}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 55}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 55}, {"type": "text", "text": "Justification: Our theoretical results include the probabilistic interpretation of Equation (6) in Appendix G.1, the suitability of Hopfield Boosting for OOD detection in Appendix J, and the connection of Hopfield Boosting to RBF networks (Appendix H.1) and SVM (Appendix H.3). The proofs of our claims are contained in the respective Appendices. ", "page_idx": 55}, {"type": "text", "text": "Guidelines: ", "page_idx": 55}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 55}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 55}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 55}, {"type": "text", "text": "Justification: The details of the experiments we conducted are explained in section 4.2, and the data sets are publicly available. The code of Hopfield Boosting is included in the submission. ", "page_idx": 55}, {"type": "text", "text": "Guidelines: ", "page_idx": 55}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). ", "page_idx": 55}, {"type": "text", "text": "(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 56}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 56}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 56}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 56}, {"type": "text", "text": "Justification: We provide the code to reproduce the experimental results of Hopfield Boosting in the submission. All data sets used are publicly available. Descriptions how to run the code and how to obtain the data sets come with the code. ", "page_idx": 56}, {"type": "text", "text": "Guidelines: ", "page_idx": 56}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 56}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 56}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 56}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 56}, {"type": "text", "text": "Justification: We provide the training and OOD test data sets, the optimizer, and the hyperparameters we tested and selected, as well as the selection procedure in Section 4.2. ", "page_idx": 56}, {"type": "text", "text": "Guidelines: ", "page_idx": 56}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 56}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 56}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 56}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 56}, {"type": "text", "text": "Justification: We provide estimates of the standard deviation for the main results of our paper on Hopfield Boosting and on the compared methods in the Tables 1, 4, 2, 3, 6, 10. The standard deviations were estimated across five training runs, which is stated in the captions of the respective tables. ", "page_idx": 57}, {"type": "text", "text": "Guidelines: ", "page_idx": 57}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 57}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 57}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 57}, {"type": "text", "text": "Justification: We provide a detailed description of the compute resources we used as well as the amount of compute our experiments required in Appendix I.12. ", "page_idx": 57}, {"type": "text", "text": "Guidelines: ", "page_idx": 57}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 57}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 57}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 57}, {"type": "text", "text": "Justification: We have read the NeurIPS Code of Ethics and found that our work conforms with the code of ethics. More specifically, we did not include any human subjects in our work. We excluded deprecated data sets from our work. ", "page_idx": 57}, {"type": "text", "text": "Guidelines: ", "page_idx": 57}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. \u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. ", "page_idx": 57}, {"type": "text", "text": "The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 58}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 58}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 58}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 58}, {"type": "text", "text": "Justification: We provide a discussion of positive and negative societal impacts in Appendix E. ", "page_idx": 58}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 58}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 58}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 58}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 58}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 58}, {"type": "text", "text": "Justification: Our work is concerned with the improvement of out-of-distribution (OOD) detection algorithms on small- to mid-scale vision data sets, and is therefore not considered to pose a high risk. ", "page_idx": 58}, {"type": "text", "text": "Guidelines: ", "page_idx": 58}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 58}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 58}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 58}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 59}, {"type": "text", "text": "Justification: We provide a list of the data sets we use (and cite the original authors) in Section 4.2. We provide a list of the respective licenses (where applicable) in Appendix I.13. ", "page_idx": 59}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 59}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 59}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 59}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 59}, {"type": "text", "text": "Justification: The code that is included in the submission comes with a READE.md file, which contains all instructions how to run the code to reproduce the experiments. ", "page_idx": 59}, {"type": "text", "text": "Guidelines: ", "page_idx": 59}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 59}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 59}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 59}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 59}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 59}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 59}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 59}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 60}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 60}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 60}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 60}]