[{"type": "text", "text": "Gated Slot Attention for Efficient Linear-Time Sequence Modeling ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yu Zhang1\u2217 Songlin Yang2\u2217 Ruijie Zhu3 Yue Zhang1 Leyang $\\mathbf{Cui^{4}}$ Yiqiao Wang5 Bolun Wang5 Freda Shi6 Bailin Wang2 Wei $\\mathbf{Bi^{4}}$ Peng Zhou5\u2020 Guohong $\\mathbf{Fu}^{1}$ \u2020 ", "page_idx": 0}, {"type": "text", "text": "1School of Computer Science and Technology, Soochow University, China 2Massachusetts Institute of Technology 3University of California, Santa Cruz 4Tencent AI Lab 5LuxiTech 6University of Waterloo yzhang.cs@outlook.com yangsl66@mit.edu https://github.com/sustcsonglin/flash-linear-attention https://huggingface.co/fla-hub ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Linear attention Transformers and their gated variants, celebrated for enabling parallel training and efficient recurrent inference, still fall short in recall-intensive tasks compared to traditional Transformers and demand significant resources for training from scratch. This paper introduces Gated Slot Attention (GSA), which enhances Attention with Bounded-memory-Control (ABC [63]) by incorporating a gating mechanism inspired by Gated Linear Attention (GLA [96]). Essentially, GSA comprises a two-layer GLA linked via softmax, utilizing context-aware memory reading and adaptive forgetting to improve memory capacity while maintaining compact recurrent state size. This design greatly enhances both training and inference efficiency through GLA\u2019s hardware-efficient training algorithm and reduced state size. Additionally, retaining the softmax operation is particularly beneficial in \u201cfinetuning pretrained Transformers to RNNs\u201d (T2R [41]) settings, reducing the need for extensive training from scratch. Extensive experiments confirm GSA\u2019s superior performance in scenarios requiring in-context recall and in T2R settings. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Transformers [88] have emerged as the predominant architecture for most, if not all, sequence modeling tasks. Nevertheless, the quadratic complexity of softmax-based standard attention (SA) poses significant challenges for long sequence modeling (e.g., video understanding and biological sequence modeling). In the context of language modeling, where sequence lengths are moderate, training efficiency is generally not a primary concern. However, during inference, the Key-Value (KV) cache [34, 64] grows linearly with the generation length, resulting in substantial memory burdens and throughput bottlenecks due to high I/O costs. ", "page_idx": 0}, {"type": "text", "text": "Linear (kernelized) attention [42] and its gated variants [96, 82, 68, 61, 16, 69] have received interest as promising alternatives to softmax attention. These models demonstrate strong performance in language modeling and understanding tasks. Notably, they can be reframed as RNNs during inference, achieving constant memory complexity and thereby significantly enhancing inference efficiency. ", "page_idx": 0}, {"type": "text", "text": "However, two key issues persist with these models: (i) Performance-wise, recent research indicates that linear recurrent models still struggle with tasks requiring in-context retrieval or learning [2, 1, 37, ", "page_idx": 0}, {"type": "text", "text": "28], and there is a fundamental recall-memory trade-off [3, 91] where all inference-time-constantmemory models face inherent limitations. (ii) In terms of training efficiency, while linear attention supports hardware-efficient chunkwise training [96] as implemented in FlashLinearAttention (FLA [95]), training from scratch on trillions of tokens remains prohibitively expensive. A paradigm, \u201cfinetuning pretrained Transformers to RNNs\u201d (short for T2R [41]), has recently gained great attention [101, 10, 54, 13, 7, 90]. This approach circumvents the high cost of training from scratch by requiring only a few billion tokens for finetuning\u2014about $1-3\\%$ of the total cost. However, linear attention uses a different kernel method from softmax, leading to performance discrepancies when finetuning pretrained softmax attention models to linear attention [101]. ", "page_idx": 1}, {"type": "text", "text": "To address these issues, we revisit the Attention with Bounded-Memory Control (ABC) model [63], which retains the softmax operation, thereby reducing training-finetuning discrepancies between standard and linear attention, making it ideal for T2R settings. Additionally, ABC enables more effective state utilization, requiring less state size to achieve similar performance, as observed in Peng et al. [63]. This results in more efficient inference and potentially expands the Pareto frontier of the recall-memory tradeoff [3]. However, ABC has not gained significant attention due to its mediocre language modeling performance and slow training speed. ", "page_idx": 1}, {"type": "text", "text": "In this work, we first reformulate ABC as two-pass linear attention linked via softmax, allowing us to leverage the hardware-efficient chunkwise implementation from FLA [95] for more efficient training. We then identify several limitations of ABC and propose a new model, dubbed Gated Slot Attention (GSA), which is essentially a gated version of ABC, following the recent trend of enhancing linear attention with gating mechanisms [96, 69, 61, 16, 6, 62, 52, 65]. ", "page_idx": 1}, {"type": "text", "text": "Our extensive evaluation shows that GSA not only matches performance in language modeling and understanding tasks but also significantly outperforms other linear models in in-context recallintensive tasks [3, 4], without requiring a large state size like RetNet [82] or GLA [96]. In the T2R finetuning setting, we found that finetuning Mistral-7B [39] to GSA surpasses large recurrent language models (e.g., RWKV6-7B, Mamba-7B) and also outperforms finetuning Mistral-7B to other linear models (e.g., RetNet, GLA) and other T2R methods like SUPRA [54], verifying the importance of retaining the softmax operator. Finally, we remark that GSA achieves similar training speeds to GLA while offering an inference speedup due to its smaller state size. ", "page_idx": 1}, {"type": "text", "text": "2 Background and Preliminary ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2.1 Transformers as Unbounded Key-Value Memories ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Given $\\mathbf{X}=\\left[\\pmb{x}_{1},\\dots,\\pmb{x}_{T}\\right]^{\\top}\\in\\mathbb{R}^{T\\times d}$ , where $T$ is the sequence length and $\\mathbf{\\boldsymbol{x}}_{i}\\in\\mathbb{R}^{d}$ is the $i$ -th input vector with $d$ dimensions, SA with causal masking computes the output matrix: ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\mathbf{O}=f((\\mathbf{Q}\\mathbf{K}^{\\top})\\odot\\mathbf{M})\\mathbf{V},\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $\\mathbf{Q},\\mathbf{K},\\mathbf{V}\\in\\mathbb{R}^{T\\times d}$ are linear mappings of the input $\\mathbf{X}$ via learnable weights $\\mathbf{W}_{q},\\mathbf{W}_{k},\\mathbf{W}_{v}\\in$ $\\mathbb{R}^{d\\times d}$ , $\\mathbf{M}=\\{M_{i j}=1$ if $i\\ge j\\ \\mathrm{o.w.\\-\\infty}\\}$ is the causal mask to prevent future information leakage, $\\odot$ denotes element-wise production, and $f(\\cdot)$ is softmax $(\\cdot)$ . ", "page_idx": 1}, {"type": "text", "text": "Generally, $\\mathbf{K},\\mathbf{V}$ can be viewed as neural key-value memories $\\widetilde{\\mathbf{K}}_{t},\\widetilde{\\mathbf{V}}_{t}\\in\\mathbb{R}^{m\\times d}$ , respectively [81, 24], where $m$ is the number of memory slots. At step $t$ , the quer y $\\pmb{q}_{t}=\\mathbf{W}_{q}\\pmb{x}_{t}\\in\\mathbb{R}^{d}$ first attends to the key memories $\\widetilde{\\mathbf{K}}_{t}$ to retrieve relevant information, which is then summarized into $\\pmb{o}_{t}$ by computing a weighted sum of the value memories $\\widetilde{\\mathbf{V}}_{t}$ [104], where the weights are the normalized attention scores: ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\begin{array}{r}{{\\pmb{o}}_{t}=\\widetilde{{\\bf V}}_{t}^{\\top}f(\\widetilde{{\\bf K}}_{t}{\\pmb{q}}_{t}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "From this perspective, Transformers are equipped with an unbounded number of memory slots, which grow linearly with respect to the sequence length [57] (i.e., $m=t$ for step $t$ )\u2014a new key $\\pmb{k}_{t}=\\bar{\\mathbf{W}}_{k}\\pmb{x}_{t}\\in\\mathbb{R}^{d}$ is assigned with a unique memory slot upon its introduction. This leads to a simple memory updating rule: $\\widetilde{\\mathbf{K}}_{t}=\\widetilde{\\mathbf{K}}_{t-1}\\cup\\{k_{t}\\}$ . The value memories $\\widetilde{\\mathbf{V}}_{t}$ are updated in a similar way. This mechanism, howev er, co mes at the cost of quadratic time com plexity in terms of the sequence length for training and ${\\cal O}(T d)$ time/memory complexity for inference [64], posing challenges for large-scale models. ", "page_idx": 1}, {"type": "text", "text": "2.2 ABC [63]: Linearizing Attention with Bounded Memory Control ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "From a key-value memory perspective, the training and inference complexity of self-attention (SA) can be reduced by fixing the number of memory slots to a constant size $m\\ll T$ [27, 51, 63]. One straightforward way to achieve this is by employing a first-in-first-out memory management strategy, commonly known as sliding window attention (SWA). However, SWA is inefficient because it discards all information outside the window, leading to poor performance in balancing the recall-memory tradeoff [3]. To achieve acceptable performance, SWA often requires a large window size (e.g., 4,096 tokens in Mistral [39]), which diminishes its advantage over to global attention. ", "page_idx": 2}, {"type": "text", "text": "When the number of tokens in a sequence exceeds the number of memory slots, it becomes necessary to store information from multiple tokens in a single slot. To address this challenge, Peng et al. [63] propose the Attention-with-Bounded-memory-Control (ABC) mechanism, which allows multiple tokens to be written into a single slot: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\widetilde{\\mathbf{K}}_{t}=\\widetilde{\\mathbf{K}}_{t-1}+\\phi_{t}\\otimes k_{t}\\in\\mathbb{R}^{m\\times d},\\quad\\widetilde{\\mathbf{V}}_{t}=\\widetilde{\\mathbf{V}}_{t-1}+\\phi_{t}\\otimes v_{t}\\in\\mathbb{R}^{m\\times d},\\quad\\mathbf{o}_{t}=\\widetilde{\\mathbf{V}}^{T}f(\\widetilde{\\mathbf{K}}_{t}^{T}\\mathbf{q}_{t})\\in\\mathbb{R}^{d}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\alpha_{i}=\\exp\\left(\\mathbf{W}_{\\phi}\\mathbf{x}_{i}\\right)\\in\\mathbb{R}^{m},\\quad\\phi_{i}=\\frac{\\alpha_{i}}{\\sum_{j=1}^{i}\\alpha_{j}}\\in(0,1)^{m}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Here, $(\\phi_{i})_{j}$ represents the writing intensity of the ith token to the $j$ th slot, obtained using a cumulative softmax function (cf. [63, footnote 5]), which can be computed with a prefix sum. ", "page_idx": 2}, {"type": "text", "text": "ABC as two-pass linear attention. The outer-product-based additive memory update rule in Eq. 3 bears a resemblance to linear attention [42], which involves the following recurrence3: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\boldsymbol{\\mathbf{S}}_{t}=\\boldsymbol{\\mathbf{\\check{S}}}_{t-1}+\\boldsymbol{\\mathbf{\\check{k}}}_{t}\\otimes\\boldsymbol{\\mathbf{\\check{v}}}_{t}\\in\\mathbb{R}^{d\\times d},\\qquad\\boldsymbol{o}_{t}=\\boldsymbol{\\mathbf{\\check{S}}}_{t}^{T}\\boldsymbol{\\mathbf{\\check{q}}}_{t}\\in\\mathbb{R}^{d}}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "We denote this linear attention operator that computes $\\pmb{o}_{i}$ from $\\pmb{q}_{i},\\pmb{k}_{i}$ and $\\pmb{v}_{i}$ (Eq. 5) by $\\{o_{i}\\}_{i=1}^{T}=$ $\\mathrm{LA}(\\{\\pmb{q}_{i},\\pmb{k}_{i},\\pmb{v}_{i}\\}_{i=1}^{T})$ . We show that the ABC operations can be written as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\{\\pmb{o}_{i}^{\\prime}\\}_{i=1}^{T}=\\mathrm{LA}(\\{\\pmb{q}_{i},\\pmb{k}_{i},\\phi_{i}\\}_{i=1}^{T}),}\\\\ &{\\{\\pmb{o}_{i}\\}_{i=1}^{T}=\\mathrm{LA}(\\{\\mathrm{softmax}(\\pmb{o}_{i}^{\\prime}),\\phi_{i},\\pmb{v}_{i}\\}_{i=1}^{T}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\pmb{o}_{i}^{\\prime}\\,\\in\\,\\mathbb{R}^{m},\\pmb{o}_{i}\\,\\in\\,\\mathbb{R}^{d}$ . Therefore, ABC can enjoy hardware-efficient linear-time chunkwise training [96], as implemented in the FLA library [95]. ", "page_idx": 2}, {"type": "text", "text": "Remarks on state size. Peng et al. [63] empirically demonstrated that ABC requires a smaller state size to achieve comparable performance to other linear attention models, resulting in improved inference efficiency. We offer the following intuitive explanation: the new query $o^{\\prime}$ aggregates the entire history through the initial pass of linear attention, making it more context-aware and better at locating desired items for retrieval. The subsequent softmax operator helps mitigate the attention dilution issue [66]. From the perspective of Hopfield networks, softmax can exponentially increase the memory size [45]. Together, these factors suggest that ABC may possess an implicit large memory capacity, even with a small actual recurrent state size. ", "page_idx": 2}, {"type": "text", "text": "2.3 GLA [96]: Linear Attention with Gating Mechanism ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Linear attentions underperform softmax-attention Transformers in language modeling by a notable margin. RetNet [82] and TransnormerLLM [68] incorporate a data-independent exponential decay factor for memory update as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbf{S}_{t}=\\gamma\\mathbf{S}_{t-1}+\\pmb{k}_{t}\\otimes\\pmb{v}_{t}\\in\\mathbb{R}^{d\\times d},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\gamma\\in(0,1)$ is a scalar data-independent decaying factor; that is, the decay rate is fixed across time steps and hidden channels (under the same head), disrespect to the input tokens. RetNet has shown better language modeling performance compared to vanilla linear attentions thanks to the decaying mechanism. ", "page_idx": 2}, {"type": "text", "text": "However, research in recurrent neural networks (RNNs) has shown that data-dependent decay (or forget gates) is crucial for selectively retaining and forgetting information [22, 26], thus better leveraging the fixed recurrent hidden state. This selective mechanism has been revisited in recent state-space models [29, 16]. Inspired by LSTMs, Gated Linear Attention (GLA) [52, 96] introduces data-dependent decay parameters $\\mathbf{G}_{t}\\in(0,1)^{d\\times d}$ to gate the hidden state as follows, ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{S}_{t}=\\mathbf{G}_{t}\\odot\\mathbf{S}_{t-1}+\\mathbf{k}_{t}\\otimes\\boldsymbol{v}_{t}\\in\\mathbb{R}^{d\\times d},\\quad\\boldsymbol{o}_{t}=\\mathbf{S}_{t}^{T}\\boldsymbol{q}_{t}\\in\\mathbb{R}^{d}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "[96] show that if gates are parameterized in an outer product form $\\mathbf{G}_{t}=\\pmb{\\alpha}_{t}\\otimes\\pmb{\\beta}_{i}$ , and $\\alpha_{t},\\beta_{t}\\in[0,1]^{d}$ depend solely on input $\\pmb{x}_{t}$ , such recurrence can be rewritten as matrix multiplication, allowing for hardware-efficient training with a chunkwise parallel form. In what follows, we will use the following notation $\\mathrm{GLA}(\\{\\pmb q_{i},\\pmb k_{i},\\pmb v_{i},\\alpha_{i},\\beta_{i}\\}_{i=1}^{T})=\\{\\dot{\\pmb o}_{i}\\}_{i=1}^{T}$ to denote this computation. It is common to set $\\boldsymbol{\\beta}_{i}=\\mathbf{1}$ as in [96, 69, 61], which is also often written in the following equivalent form: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{S}_{t}=\\mathrm{Diag}(\\alpha_{t})\\mathbf{S}_{t-1}+\\mathbf{k}_{t}\\otimes\\pmb{v}_{t}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Here $k_{t}$ can be viewed as the input gate, and $\\alpha_{t}$ can be viewed as the forget gate. In gated RNN literature, it is common to couple these two gates via $k_{t}=1-\\alpha_{t}$ [12, 106, 67]. In particular, Qin et al. [69] proposed HGRN2, which uses this strategy as an improved parameterization of GLA, showing better performance in language modeling. ", "page_idx": 3}, {"type": "text", "text": "3 Method ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "3.1 Motivation: Issues with ABC ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We identify two primary limitations in ABC\u2019s memory update rule. Firstly, it lacks a forgetting mechanism, resulting in indefinite retention of items once written into memory slots. This prevents efficient memory reuse by impeding the prompt clearance of slots for new information. ", "page_idx": 3}, {"type": "text", "text": "Secondly, the rule introduces an unwarranted inductive bias favoring tokens at the sentence\u2019s beginning. This contradicts the recency bias in natural language, where more recent information is often more relevant. Prioritizing initial tokens over the recent ones confilcts with this inherent tendency in natural language processing. ", "page_idx": 3}, {"type": "text", "text": "Specifically, for the first token, the writing strength to all slots is maximized (i.e., $\\phi_{1}=\\mathbf{1}\\in\\mathbb{R}^{m}.$ ), causing every memory slot to retain a copy of the first token\u2019s representation. The absence of a forgetting mechanism exacerbates this issue. For subsequent tokens, the writing strength diminishes due to the influence of earlier tokens, as a result of the cumulative softmax in Eq. 4. This makes it challenging for the model to retain later tokens without learning a significantly large $\\alpha_{i}$ , potentially leading to instability in long-context settings, as observed by Zhang et al. [100]. ", "page_idx": 3}, {"type": "text", "text": "3.2 Gated Slot Attention (GSA): ABC with gating mechanism ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "To address these limitations, we propose Gated Slot Attention (GSA), which incorporates a gating mechanism to simultaneously resolve both issues by: (i) enabling the forgetting of historical information, and (ii) introducing a recency inductive bias, as detailed below. ", "page_idx": 3}, {"type": "text", "text": "For each memory slot, the update rule is a simple gated RNN with a scalar data-dependent gating value $\\alpha_{i}\\in[0,1]$ , ", "page_idx": 3}, {"type": "equation", "text": "$$\n(\\widetilde{\\mathbf{K}}_{t})_{i}=\\alpha_{i}(\\widetilde{\\mathbf{K}}_{t-1})_{i}+(1-\\alpha_{i})k_{t}\\in\\mathbb{R}^{d},\\qquad(\\widetilde{\\mathbf{V}}_{t})_{i}=\\alpha_{i}(\\widetilde{\\mathbf{V}}_{t-1})_{i}+(1-\\alpha_{i}){\\boldsymbol{v}}_{t}\\in\\mathbb{R}^{d}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "and these can be written in matrix form, which is reminiscent of HGRN2 [69]. ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widetilde{\\mathbf{K}}_{t}=\\mathrm{Diag}(\\pmb{\\alpha}_{t})\\cdot\\widetilde{\\mathbf{K}}_{t-1}+(1-\\pmb{\\alpha}_{t})\\otimes\\pmb{k}_{t}\\in\\mathbb{R}^{m\\times d}}\\\\ &{\\widetilde{\\mathbf{V}}_{t}=\\mathrm{Diag}(\\pmb{\\alpha}_{t})\\cdot\\widetilde{\\mathbf{V}}_{t-1}+(1-\\pmb{\\alpha}_{t})\\otimes\\pmb{v}_{t}\\in\\mathbb{R}^{m\\times d}}\\\\ &{\\pmb{\\upsigma}_{t}=\\widetilde{\\mathbf{V}}^{T}\\,\\mathrm{softmax}(\\widetilde{\\mathbf{K}}_{t}^{T}\\mathbf{q}_{t})\\in\\mathbb{R}^{d}}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "GSA as two-pass GLA. It is straightforward to see that we can write GSA as a two-pass GLA as shown below: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\{o_{t}^{\\prime}\\}_{t=1}^{T}=\\mathrm{GLA}\\left(\\{q_{t},k_{t},1-\\alpha_{t},\\alpha_{t},\\mathbf{1}\\}_{t=1}^{T}\\right)}\\\\ &{\\{o_{t}\\}_{t=1}^{T}=\\mathrm{GLA}\\left(\\{\\mathrm{softmax}(o_{t}^{\\prime}),1-\\alpha_{t},v_{t},\\mathbf{1},\\alpha_{t}\\}_{t=1}^{T}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Therefore, we can adapt GLA\u2019s hardware-efficient chunkwise training algorithm for GSA training, as shown in $\\S\\mathrm{~A~}$ and $\\S\\mathrm{~B~}$ . We illustrate the recurrent representation of GSA in Figure 1. ", "page_idx": 3}, {"type": "image", "img_path": "jY4PhQibmg/tmp/ecfb8a1597bdca437d760478e222db8f7941f1280ef6485158bc64edfd911bf6.jpg", "img_caption": ["Figure 1: The recurrent representation of GSA. means taking $\\pmb{x}_{t}$ as input. "], "img_footnote": [], "page_idx": 4}, {"type": "image", "img_path": "jY4PhQibmg/tmp/89b602a53a2c924a903acaa784fa58f4209f06e7ef3d613fbba8465c47e45d9a.jpg", "img_caption": ["Figure 2: The backbone of our proposed GSA models. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "The overall architecture of our proposed model, GSA, is shown in Figure 2. Following the Llama architecture [86], we use a stack of $L$ GSA blocks, each comprising a GSA token mixing layer followed by a Gated Linear Unit (GLU) channel mixing layer [19, 33]. ", "page_idx": 4}, {"type": "text", "text": "We utilize the multi-head attention mechanism [88] to capture different aspects of the input. For each head $h$ , the input to GSA token mixing is defined as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{q}_{i}^{h},\\mathbf{k}_{i}^{h},\\mathbf{v}_{i}^{h}=\\phi(\\mathbf{W}_{q}^{h}\\mathbf{x}_{i}),\\phi(\\mathbf{W}_{k}^{h}\\mathbf{x}_{i}),\\phi(\\mathbf{W}_{v}^{h}\\mathbf{x}_{i})\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\phi$ is the Swish activation following [68]. The forget gate is obtained by a linear transformation followed by a sigmoid activation $\\sigma$ with a damping factor $\\tau$ [96, 83]: $\\pmb{\\alpha}_{i}^{h}=\\sigma(\\mathbf{W}_{\\alpha}^{h}\\pmb{x}_{i})^{1/\\tau}$ , 4 where the damping factor is to regulate the forget gate value to one, which has been shown to be crucial for long-term dependency modeling [30, 67]. We feed them into a GSA layer to obtain outputs as described in Eq. 7: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\{o_{i}^{h}\\}_{i=1}^{T}=\\operatorname{GSA}(\\{q_{i}^{h},k_{i}^{h},v_{i}^{h},\\alpha_{i}^{h}\\}_{i=1}^{T})\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Finally, we obtain output via ", "page_idx": 4}, {"type": "equation", "text": "$$\ny_{i}=\\mathbf{W}_{o}\\left(\\mathrm{RMSNorm}\\left(\\mathrm{Swish}\\left(\\mathrm{Concat}\\left(o_{i}^{1},\\cdot\\cdot\\cdot,o_{i}^{H}\\right)\\right)\\right)\\right)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The total number of parameters for $\\mathbf{W}_{q},\\mathbf{W}_{k},\\mathbf{W}_{v}$ , and ${\\bf W}o$ is already $4d^{2}$ , which is the same as in a single standard softmax-attention layer. To control the overall parameter count, we aim to keep the parameters for $\\mathbf{W}_{\\alpha}$ , which amount to $d H m$ , relatively small. In practice, we set $m=64$ to achieve a balance between efficiency and effectiveness $(\\S\\,4.1.4)$ . One way to further manage the total parameter count is by reducing the number of heads. In practice, we set $H=4$ , ensuring that $H m\\ll d$ . This keeps the total number of parameters approximately equal to $4d^{2}$ . 5 ", "page_idx": 4}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "4.1 Language Modeling ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We perform moderate-scale language modeling experiments with 1.3B and 2.7B parameters on Slimpajama corpus [79] for 100B tokens each. ", "page_idx": 4}, {"type": "text", "text": "We compare the performance of GSA against Llama Transformer architecture (i.e., $X_{\\mathrm{fmr++}}$ [86] and recent subquadratic architectures including: Mamba [29], RetNet [82], GLA [96] and HGRN2 [69]. We refer readers to $\\S\\,C$ for more details on baselines and other experimental setups. ", "page_idx": 4}, {"type": "text", "text": "4.1.1 Results on commonsense reasoning tasks ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Following [29, 96], we report the perplexities and zero-shot performance of commonsense reasoning tasks including $\\mathbf{ARC}_{e}$ & $\\mathrm{ARC}_{c}$ (ARC-easy, ARC-challenge) [14]; Hella. (Hellaswag) [99], Lamb. (Lambada) [59], PIQA [8], Wiki. (Wikitext) [55], and Wino. (Winograde) [73]. We note that these tasks are typically short in length and do not require in-context learning capabilities, thus they do not adequately reflect long-context modeling or in-context learning retrieval abilities. Nevertheless, as shown in Table 1, we found that GSA performs comparably to the recent strong model HGRN2 with an equally sized hidden state, while outperforming GLA and RetNet even with a smaller state size. ", "page_idx": 5}, {"type": "table", "img_path": "jY4PhQibmg/tmp/05a89258ee6040246ba23f5dc23cef55dde6605212d8b01f154c587fae116d72.jpg", "table_caption": ["Table 1: The zero-shot results of 1.3B and 2.7B models evaluated by lm-evaluation-harness [21]. $L$ denotes number of layer while $d$ denotes the model dimension. "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "4.1.2 Results on in-context recall-intensive tasks ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "While subquadratic models can achieve comparable performance to (softmax-based) Transformers in language modeling and understanding tasks, their performance on recall-intensive tasks significantly lags behind Transformers and varies greatly across different subquadratic models, as observed in many recent studies [3, 4, 96, 97]. Therefore, it is crucial to improve linear models on in-context recall-intensive tasks. ", "page_idx": 5}, {"type": "image", "img_path": "jY4PhQibmg/tmp/88b5d074a72ec033e9735fc05b901d002cde6f19d2433cfaa2d66ca208113b66.jpg", "img_caption": ["(a) Results on the synthetic MQAR task. We adopt the most challenging settings in [2], utilizing a sequence length of 512 and $64\\mathrm{~key-}$ value pairs. $X_{\\mathrm{fmr++}}$ with standard attention achieves near-perfect results in this settings and is thus omitted for brevity. "], "img_footnote": [], "page_idx": 5}, {"type": "table", "img_path": "jY4PhQibmg/tmp/2d116e12d1df73c475de63e565697f533c3aa514d1401e9b16702dc255f65eef.jpg", "table_caption": ["(b) Results on the recall-intensive tasks used in [4]. We truncate the input to a maximum of 2K tokens. "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "MQAR. We first present the results on the multi-query associative recall (MQAR) task [2], a diagnostic synthetic task that requires models to retrieve multiple associative key-value pairs from the context. This task has been shown to strongly correlate with language modeling performance [2]. The results in Table 3a validate the effectiveness of GSA. ", "page_idx": 6}, {"type": "text", "text": "Real-world tasks. Next, we evaluate the zero-shot incontext learning performance on recall-intensive tasks, as used in Arora et al. [4].6 Specifically, we assess information retrieval on FDA [93] and SWDE [49], which are designed to evaluate retrieval from in-context passages scraped from HTML/PDFs. We also evaluate question answering on SQuAD [70], NQ [46], TriviaQA [40], and Drop [20], where models must ground their answers in in-context documents. ", "page_idx": 6}, {"type": "text", "text": "As shown in Table 3b, $X_{\\mathrm{fmr++}}$ achieves the best average performance, as expected. Meanwhile, GSA outperforms all other subquadratic baseline models by a notable margin without requiring a larger state size. We believe this advantage stems from GSA\u2019s context-aware memory readout mechanism (as discussed in $\\S2.2)$ and its forgetting mechanism (i.e., the gating mechanism), enabling it to manipulate finite-sized memory more effectively. ", "page_idx": 6}, {"type": "table", "img_path": "jY4PhQibmg/tmp/e5e4d92b81fde1fe232dc9e628d0756c2f6f48ebc2b808190a6204622e1e2b25.jpg", "table_caption": ["Table 2: Ablation study results for 340M models trained on 10B Slimpajama tokens. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "4.1.3 Ablation ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Table 2 presents the results of our ablation studies. Our findings indicate that: (i) the inclusion of the gating mechanism in GSA is crucial for improving language modeling perplexity; (ii) applying softmax non-linearities after the first recurrent pass is beneficial; and (iii) using 64 slots strikes an optimal balance between performance and efficiency. 7 ", "page_idx": 6}, {"type": "text", "text": "4.1.4 Efficiency ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Fig. 4a illustrates the training throughput for four models on a single $\\mathrm{H}800\\;\\mathrm{GPU}^{8}$ . To optimize memory usage, we employ the technique of recomputing the recurrent hidden state during the backward pass, as done in FLA [95] and Mamba2 [16]. This approach results in reduced memory consumption (Fig. 4b) at the cost of slightly lower training throughputs (Fig. 4a). ", "page_idx": 6}, {"type": "text", "text": "Despite requiring two GLA passes, GSA maintains comparable training throughputs to GLA due to its reduced state size. Since inference is primarily memory-bound, inference speed highly correlates with state size. As a result, GSA, with its smaller state size compared to RetNet and GLA, achieves faster inference speeds, as shown in Figure 4c. ", "page_idx": 6}, {"type": "text", "text": "4.2 Finetuning Pretrained Transformers to RNNs ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "The concept of finetuning pretrained Transformers to linear Transformers for recurrent inference was first introduced in T2R [41]. This approach uses pretrained language model weights to initialize all parameters, leveraging the similarity between linear attention and softmax attention, and finetunes all parameters, significantly reducing the total training time compared to training from scratch. Kasai et al. [41] also introduced a parametric feature map, implemented as a learnable MLP layer followed by ReLU, applied after the query/key projections. SUPRA, a follow-up to T2R, found that the original T2R approach did not perform well in the era of LLMs, and highlighted the importance of output normalization and a decay mechanism\u2014adopted from RetNet [82]\u2014as critical for finetuning performance. As a result, SUPRA essentially combines T2R and RetNet by finetuning pretrained Transformers into a RetNet architecture, though it excludes the Swish output gate. ", "page_idx": 6}, {"type": "image", "img_path": "jY4PhQibmg/tmp/f67eabb9301bb2492609fbd4f3e30244ba67e21e04e0ac09da5557ee2450f798.jpg", "img_caption": ["Figure 4: (a) Training throughput of various 1.3B models on a single H800 GPU, with a fixed batch size containing 16K tokens. \u201cGSA w/o recomp.\u201d indicates the use of the GSA kernel without hidden state recomputation during the backward pass. (b) Memory footprint (in GiB) of each 1.3B model during training with a batch size containing 16K tokens. (c) Inference latency (in seconds) of each 1.3B model on a single H800 GPU with 2K prefix tokens and a batch size of 1. "], "img_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "jY4PhQibmg/tmp/aaab102615d672080bc8974e1ae4efaec0a05b06e8d96b827f6003f6e9ff6a50.jpg", "table_caption": ["Table 3: Performance comparison across various 7B models. \u2663denotes models using softmax-attention. \u2020 denotes our results. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Settings. In our preliminary experiments, we found that the learnable MLP layer was unnecessary and could be merged into the query and key projections, similar to the approach in Peng et al. [63]. We finetuned the pretrained Transformer Mistral 7B [39] to RetNet, as well as to GLA and GSA models. Following SUPRA, we add ReLU as the feature map activation for RetNet and GLA, which originally used an identity feature map without activation 9, and also excluded the Swish output gate. For RetNet, there were no additional parameters; for GLA, the low-rank forget gate, and for GSA, the $W_{\\alpha}$ matrix are trainable parameters, though both are small in parameter count and negligible in terms of the total model size. We set the peak learning rate to $3\\bar{\\times}10^{-5}$ with 1K steps of linear warmup following SUPRA. The training length was set to 2K tokens, with a batch size of 2M tokens. For convenience, we trained on the SlimPajama corpus, while SUPRA used RefineWeb [60], a higher-quality corpus. We leave the use of RefineWeb for future work. ", "page_idx": 7}, {"type": "text", "text": "Main results. Following Jiang et al. [39], Touvron et al. [87], we evaluated the models on commonsense reasoning tasks: ${\\mathrm{ARC}}_{e}$ and $\\mathrm{ARC}_{c}$ [14], Hellaswag [99], PIQA [8], and Winogrande [73]; world knowledge tasks: NQ [46] and TriviaQA [40]; and popular aggregated benchmarks: MMLU [31] and BBH [85]. Results are shown in Table 3. We observed a clear advantage in finetuning Mistral to GSA compared to GLA or RetNet, confirming our intuition that preserving softmax is beneficial in T2R settings. When trained with 100B tokens, Mistral-to-GSA outperforms RWKV6 and Mamba on average, even though those models were trained on over 1T tokens, thereby reducing the required training data size. ", "page_idx": 8}, {"type": "text", "text": "Long-context ability evaluation. Following Xiong et al. [94], we evaluated the models on long-sequence tasks, including Qasper [18], NarrativeQA [44], QuALITY [58], and QMSum [105]. For each task, the input was truncated to 16K tokens, which is $8\\times$ the training length. ", "page_idx": 8}, {"type": "text", "text": "The results are shown in Table 4. Notably, GSA consistently outperforms other subquadratic models across all four tasks. We attribute this to the same factors observed in in-context recallintensive task settings. Interestingly, Mistralto-GSA also demonstrates overall better perfor", "page_idx": 8}, {"type": "table", "img_path": "jY4PhQibmg/tmp/5b0d7dd602e83ad811ba80383aebadb63f20b02855fd104e6f5e8e7e3582b2fd.jpg", "table_caption": ["Table 4: Long-context performance comparison. "], "table_footnote": ["mance compared to RWKV6 and Mamba, which were trained from scratch on ${>}1\\mathrm{T}$ token. "], "page_idx": 8}, {"type": "text", "text": "5 Related works ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Matrix-valued linear RNNs with hardware-efficient training. Traditional RNNs (e.g., LSTM [32], GRU [12]) maintain 1-dimensional hidden states, which are often too small to capture sufficient information. Recent work emphasizes the importance of expanding the size of recurrent states [29, 69, 96, 82, 61, 16]. However, naive state expansion dramatically increases FLOPs and I/O costs, making training impractical. To address this, Mamba introduces an I/O-aware approach, reducing I/O costs by materializing parameters and hidden states only on SRAM (instead of HBM). However, Mamba\u2019s recurrence cannot be expressed in matmul form, leading to two key issues: (i) high FLOP count cannot be optimized via tensor cores (the GPU\u2019s fast matmul unit), resulting in slower runtimes; and (ii) the recurrent hidden states cannot be compactly represented and must be materialized on SRAM during backpropagation, limiting the recurrent state size due to SRAM constraints. ", "page_idx": 8}, {"type": "text", "text": "Mamba2 [16] addresses these limitations by adopting a linear attention [42]-like approach that enables hardware-efficient training. Linear attention expands the state using outer products, allowing for both parallel attention-style computation and recurrent inference (also known as state-space duality in Mamba2). The chunkwise algorithm interpolates between parallel and recurrent forms, enabling hardware-efficient, linear-time training [33, 82, 96]. However, vanilla linear attention underperforms softmax attention in various tasks. Recent research has explored incorporating various decay or gating mechanisms to enhance model expressiveness and performance while maintaining matmul-based parallelism and chunkwise training. These include head-wise data-independent decay [82, 68]; head-wise data-dependent decay [62, 16, 6, 83]; and channel-wise data-dependent decay [96, 52, 43, 69, 61]. GSA leverages two-pass gated linear attention to further enhance capacity while allowing hardware-efficient training. ", "page_idx": 8}, {"type": "text", "text": "Fast weight RNNs. Fast weight programming [77], a classical concept intensively investigated in deep learning [5, 103, 74, 76, 56, 75, 35, 36, 52], has been shown to be closely related to (linear) Transformers [75]. The core idea involves using a slow network to produce rapid context-dependent weight modifications for the fast network. In linear attention, the fast network is a single-layer FFN with weight matrix $\\mathbf{S}_{t}$ (Eq. 5), while the slow networks are the query/key/value projections. ", "page_idx": 8}, {"type": "text", "text": "Linear attention is known to suffer from limited memory capacity [75], potentially due to the constraints of a single-layer FFN without a large representation. In contrast, ABC and GSA can be viewed as implementing a two-layer fast FFN with either additive update rule or gated update rule [74, 52], where the weight matrices are $\\widetilde{\\mathbf{K}}_{t}$ and $\\widetilde{\\mathbf{V}}_{t}$ connected by the softmax activation function (Eq. 3 and Eq. 6). This structure resembles DeltaMLP [35], which uses a delta update rule [92, 75, 98] and a multi-layer (potentially beyond two layers) fast FFN. The greater capacity of a two-layer FFN compared to a similarly sized single-layer FFN could explain why GSA requires a smaller state size to achieve similar or even better performance, especially in long sequence and recall-intensive tasks. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Finetuning Transformers to RNNs. As discussed, this paradigm could significantly reduce the training cost for large-scale recurrent language models. The idea of distilling Transformers to RNNs to improve inference efficiency can be traced back to Gerstenberger et al. [23]. In the following, we briefly introduce some recent works that complement those already mentioned in $\\S4.2$ . Zhang et al. [101] highlight the desirable properties of softmax, such as attention spikiness and dot-product monotonicity, and employ a learnable MLP layer to approximate softmax behavior using logit distillation loss (while freezing other parameters). Chen et al. [10] introduce DiJiang, an effective method for approximating attention distributions using the Discrete Cosine Transform (DCT) to enable frequency-domain kernelization, leading to faster feature mapping. Bick et al. [7] propose a multi-stage distillation approach, aligning attention distributions (similar to Hedgehog [101]), hidden states, and output logits to transfer knowledge from a pretrained Transformer teacher to a student Mamba model. Wang et al. [90] distill Transformer-based LLMs into hybrid Mamba-Attention architectures in the spirit of Ren et al. [71], Lieber et al. [47], Waleffe et al. [89]. However, they freeze the FFN weights, while Choi [13] suggest that it might be more effective to unfreeze them. In this work, we highlight the importance of the softmax operator, as discussed in Zhang et al. [101], except that GSA directly incorporates softmax, while Zhang et al. [101] learns a feature map to mimic softmax, without actually including any softmax operator in the resulting model. ", "page_idx": 9}, {"type": "text", "text": "6 Limitations and future work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Due to the relatively small scale of our pretrained models (compared to large-scale models trained on trillions of tokens), we did not report any results on long-context tasks, as the performance would all be poor. However, we believe Table 4 provides positive indications of GSA\u2019s long-context capabilities, and training on a larger token horizon and with larger models would address this. For copy-oriented tasks, we observed negative results on the Phonebook Lookup [38] and Needle-InHaystack evaluations compared to Transformers, revealing the fundamental limitations of linear recurrent models in handling \u201cprecise local token shifts and comparison\u201d, as discussed in Arora et al. [3]. Nonetheless, we expect this limitation could be significantly mitigated by pretraining a hybrid GSA-attention model, as recently explored [3, 71, 89, 47, 98], or by distilling pretrained Transformers into hybrid GSA-attention models, as in Wang et al. [90], or using different training objectives with JRT prompts, as in Arora et al. [4], or combining with YOCO [83, 25]. ", "page_idx": 9}, {"type": "text", "text": "GSA follows GLA in using a gated update rule, although we acknowledge recent work on Parallel DeltaNet [98], which parallelizes the delta update rule computations in DeltaNet [75] over sequence length, significantly enhancing training efficiency. The delta rule is known to improve in-context retrieval ability [75, 98], aligning with one of the objectives of this work. We did not explore the analogous two-pass DeltaNet, but we leave this for future investigation, which would bring the approach closer to the original DeltaMLP [35], as discussed earlier. It would also be beneficial to compare GSA with more recent strong RNN models, such as xLSTM [6], Mamba2 [16], TTT [84], and Longhorn [48]. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work introduces Gated Slot Attention (GSA), which enhances ABC [63] with a gating mechanism inspired by Gated Linear Attention (GLA [96]). By framing GSA as a two-pass GLA, we can leverage hardware-efficient implementations of GLA [95] to train GSA. As such, GSA beneftis from context-aware memory reading and forgetting, implicitly increasing the model\u2019s capacity despite a small actual state size, which improves training and inference efficiency. Through extensive experiments, we demonstrate the advantages of GSA in in-context recall-intensive tasks [4] and in \u201cfinetuning pretrained Transformers to RNNs\u201d [41] scenarios. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "We would like to thank Zhen Qin and Yikang Shen for their insightful discussions, Houquan Zhou and Kazuki Irie for providing valuable feedback on this manuscript. ", "page_idx": 10}, {"type": "text", "text": "We gratefully acknowledge the support by LuxiTech for computational resources; and the support by the National Natural Science Foundation of China (No. 62076173, 62476187), the High-level Entrepreneurship and Innovation Plan of Jiangsu Province (No. JSSCRC2021524), and the Project Funded by the Priority Academic Program Development of Jiangsu Higher Education Institutions. Yu Zhang was partially supported by Tencent AI Lab under Wei Bi\u2019s mentorship. Songlin Yang was supported by Xianhong Wu Fellowship from MIT. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] E. Aky\u00fcrek, B. Wang, Y. Kim, and J. Andreas. In-context language learning: Architectures and algorithms. In Proceedings of ICML, 2024. URL https://openreview.net/forum?id= 3Z9CRr5srL.   \n[2] S. Arora, S. Eyuboglu, A. Timalsina, I. Johnson, M. Poli, J. Zou, A. Rudra, and C. R\u00e9. Zoology: Measuring and improving recall in efficient language models, 2023. [3] S. Arora, S. Eyuboglu, M. Zhang, A. Timalsina, S. Alberti, D. Zinsley, J. Zou, A. Rudra, and C. R\u00e9. Simple linear attention language models balance the recall-throughput tradeoff, 2024.   \n[4] S. Arora, A. Timalsina, A. Singhal, B. Spector, S. Eyuboglu, X. Zhao, A. Rao, A. Rudra, and C. R\u00e9. Just read twice: closing the recall gap for recurrent language models, 2024. URL https://arxiv.org/abs/2407.05483.   \n[5] J. Ba, G. Hinton, V. Mnih, J. Z. Leibo, and C. Ionescu. Using fast weights to attend to the recent past, 2016.   \n[6] M. Beck, K. P\u00f6ppel, M. Spanring, A. Auer, O. Prudnikova, M. Kopp, G. Klambauer, J. Brandstetter, and S. Hochreiter. xlstm: Extended long short-term memory, 2024.   \n[7] A. Bick, K. Y. Li, E. P. Xing, J. Z. Kolter, and A. Gu. Transformers to ssms: Distilling quadratic knowledge to subquadratic models. 2024. URL https://api.semanticscholar. org/CorpusID:271903923.   \n[8] Y. Bisk, R. Zellers, R. L. Bras, J. Gao, and Y. Choi. Piqa: Reasoning about physical commonsense in natural language. In In Proceedings of AAAI, 2020.   \n[9] G. E. Blelloch. Prefix sums and their applications. Technical report, School of Computer Science, Carnegie Mellon University, Pittsburgh, PA, 1993. URL https://www.cs.cmu.edu/ \\~guyb/papers/Ble93.pdf.   \n[10] H. Chen, Z. Liu, X. Wang, Y. Tian, and Y. Wang. Dijiang: Efficient large language models through compact kernelization. ArXiv, abs/2403.19928, 2024. URL https://api. semanticscholar.org/CorpusID:268793982.   \n[11] T. Chen, B. Xu, C. Zhang, and C. Guestrin. Training deep nets with sublinear memory cost, 2016.   \n[12] K. Cho, B. van Merri\u00ebnboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk, and Y. Bengio. Learning phrase representations using RNN encoder\u2013decoder for statistical machine translation. In Proceedings of EMNLP, pages 1724\u20131734, 2014. URL https: //aclanthology.org/D14-1179.   \n[13] S. Choi. Cross-architecture transfer learning for linear-cost inference transformers, 2024. URL https://arxiv.org/abs/2404.02684.   \n[14] P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick, and O. Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv:1803.05457v1, 2018.   \n[15] T. Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. In The Twelfth International Conference on Learning Representations, 2024. URL https: //openreview.net/forum?id=mZn2Xyh9Ec.   \n[16] T. Dao and A. Gu. Transformers are ssms: Generalized models and efficient algorithms through structured state space duality. CoRR, abs/2405.21060, 2024. doi: 10.48550/ARXIV. 2405.21060. URL https://doi.org/10.48550/arXiv.2405.21060.   \n[17] T. Dao, D. Fu, S. Ermon, A. Rudra, and C. R\u00e9. Flashattention: Fast and memoryefficient exact attention with io-awareness. In Advances in NIPS, pages 16344\u2013 16359, 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/file/ 67d57c32e20fd0a7a302cb81d36e40d5-Paper-Conference.pdf.   \n[18] P. Dasigi, K. Lo, I. Beltagy, A. Cohan, N. A. Smith, and M. Gardner. A dataset of informationseeking questions and answers anchored in research papers. In Proceedings of NAACL, pages 4599\u20134610, 2021. URL https://aclanthology.org/2021.naacl-main.365.   \n[19] Y. N. Dauphin, A. Fan, M. Auli, and D. Grangier. Language modeling with gated convolutional networks. In Proceedings of ICML, pages 933\u2013941, 2017. URL https://proceedings.mlr. press/v70/dauphin17a.html.   \n[20] D. Dua, Y. Wang, P. Dasigi, G. Stanovsky, S. Singh, and M. Gardner. DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs. In Proceedings of NAACL, pages 2368\u20132378, 2019. URL https://aclanthology.org/N19-1246.   \n[21] L. Gao, J. Tow, B. Abbasi, S. Biderman, S. Black, A. DiPof,i C. Foster, L. Golding, J. Hsu, A. Le Noac\u2019h, H. Li, K. McDonell, N. Muennighoff, C. Ociepa, J. Phang, L. Reynolds, H. Schoelkopf, A. Skowron, L. Sutawika, E. Tang, A. Thite, B. Wang, K. Wang, and A. Zou. A framework for few-shot language model evaluation, 2023. URL https://zenodo.org/ records/10256836.   \n[22] F. Gers, J. Schmidhuber, and F. Cummins. Learning to forget: continual prediction with lstm. In Proceedings of ICANN, pages 850\u2013855, 1999.   \n[23] A. Gerstenberger, K. Irie, P. Golik, E. Beck, and H. Ney. Domain robust, fast, and compact neural language models. In Proceedings of ICASSP, Barcelona, Spain, 2020.   \n[24] M. Geva, R. Schuster, J. Berant, and O. Levy. Transformer feed-forward layers are key-value memories. In Proceedings of EMNLP, pages 5484\u20135495, Online and Punta Cana, Dominican Republic, 2021. URL https://aclanthology.org/2021.emnlp-main.446.   \n[25] D. Goldstein, F. Obeid, E. Alcaide, G. Song, and E. Cheah. Goldfinch: High performance rwkv/transformer hybrid with linear pre-fill and extreme kv-cache compression. ArXiv, abs/2407.12077, 2024. URL https://api.semanticscholar.org/CorpusID:271244694.   \n[26] A. Graves. Generating sequences with recurrent neural networks, 2014.   \n[27] A. Graves, G. Wayne, and I. Danihelka. Neural turing machines, 2014.   \n[28] R. Grazzi, J. N. Siems, S. Schrodi, T. Brox, and F. Hutter. Is mamba capable of in-context learning? ArXiv, abs/2402.03170, 2024. URL https://api.semanticscholar.org/CorpusID: 267412719.   \n[29] A. Gu and T. Dao. Mamba: Linear-time sequence modeling with selective state spaces, 2023.   \n[30] A. Gu, C. Gulcehre, T. Paine, M. Hoffman, and R. Pascanu. Improving the gating mechanism of recurrent neural networks. In H. D. III and A. Singh, editors, Proceedings of ICML, pages 3800\u20133809. PMLR, 2020. URL https://proceedings.mlr.press/v119/gu20a.html.   \n[31] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring massive multitask language understanding, 2021. URL https://arxiv.org/abs/2009. 03300.   \n[32] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Computation, 9(8): 1735\u20131780, 1997.   \n[33] W. Hua, Z. Dai, H. Liu, and Q. Le. Transformer quality in linear time. In K. Chaudhuri, S. Jegelka, L. Song, C. Szepesvari, G. Niu, and S. Sabato, editors, Proceedings of ICML, pages 9099\u20139117. PMLR, 2022. URL https://proceedings.mlr.press/v162/hua22a.html.   \n[34] K. Irie, A. Gerstenberger, R. Schl\u00fcter, and H. Ney. How much self-attention do we need? Trading attention for feed-forward layers. In Proceedings of ICASSP, Virtual only, May 2020.   \n[35] K. Irie, I. Schlag, R. Csord\u2019as, and J. Schmidhuber. Going beyond linear transformers with recurrent fast weight programmers. ArXiv, abs/2106.06295, 2021. URL https://api. semanticscholar.org/CorpusID:235417174.   \n[36] K. Irie, I. Schlag, R. Csord\u2019as, and J. Schmidhuber. A modern self-referential weight matrix that learns to modify itself. In International Conference on Machine Learning, 2022. URL https://api.semanticscholar.org/CorpusID:246823084.   \n[37] S. Jelassi, D. Brandfonbrener, S. M. Kakade, and E. Malach. Repeat after me: Transformers are better than state space models at copying. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=duRRoGeoQT.   \n[38] S. Jelassi, D. Brandfonbrener, S. M. Kakade, and E. Malach. Repeat after me: Transformers are better than state space models at copying, 2024.   \n[39] A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. de las Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier, L. R. Lavaud, M.-A. Lachaux, P. Stock, T. L. Scao, T. Lavril, T. Wang, T. Lacroix, and W. E. Sayed. Mistral 7b, 2023.   \n[40] M. Joshi, E. Choi, D. Weld, and L. Zettlemoyer. TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension. In R. Barzilay and M.-Y. Kan, editors, Proceedings of ACL, pages 1601\u20131611, Vancouver, Canada, July 2017. doi: 10.18653/v1/P17-1147. URL https://aclanthology.org/P17-1147.   \n[41] J. Kasai, H. Peng, Y. Zhang, D. Yogatama, G. Ilharco, N. Pappas, Y. Mao, W. Chen, and N. A. Smith. Finetuning pretrained transformers into RNNs. In M.-F. Moens, X. Huang, L. Specia, and S. W.-t. Yih, editors, Proceedings of EMNLP, pages 10630\u201310643, 2021. URL https://aclanthology.org/2021.emnlp-main.830.   \n[42] A. Katharopoulos, A. Vyas, N. Pappas, and F. Fleuret. Transformers are RNNs: Fast autoregressive transformers with linear attention. In H. D. III and A. Singh, editors, Proceedings of ICML, pages 5156\u20135165. PMLR, 2020. URL https://proceedings.mlr.press/v119/ katharopoulos20a.html.   \n[43] T. Katsch. Gateloop: Fully data-controlled linear recurrence for sequence modeling, 2024.   \n[44] T. Koc\u02c7isk\u00fd, J. Schwarz, P. Blunsom, C. Dyer, K. M. Hermann, G. Melis, and E. Grefenstette. The NarrativeQA reading comprehension challenge. TACL, pages 317\u2013328, 2018. URL https://aclanthology.org/Q18-1023.   \n[45] D. Krotov and J. Hopfield. Large Associative Memory Problem in Neurobiology and Machine Learning, 2021. URL http://arxiv.org/abs/2008.06996.   \n[46] T. Kwiatkowski, J. Palomaki, O. Redfield, M. Collins, A. Parikh, C. Alberti, D. Epstein, I. Polosukhin, J. Devlin, K. Lee, K. Toutanova, L. Jones, M. Kelcey, M.-W. Chang, A. M. Dai, J. Uszkoreit, Q. Le, and S. Petrov. Natural questions: A benchmark for question answering research. TACL, pages 452\u2013466, 2019. URL https://aclanthology.org/Q19-1026.   \n[47] O. Lieber, B. Lenz, H. Bata, G. Cohen, J. Osin, I. Dalmedigos, E. Safahi, S. Meirom, Y. Belinkov, S. Shalev-Shwartz, O. Abend, R. Alon, T. Asida, A. Bergman, R. Glozman, M. Gokhman, A. Manevich, N. Ratner, N. Rozen, E. Shwartz, M. Zusman, and Y. Shoham. Jamba: A hybrid transformer-mamba language model, 2024. URL https: //arxiv.org/abs/2403.19887.   \n[48] B. Liu, R. Wang, L. Wu, Y. Feng, P. Stone, and Q. Liu. Longhorn: State space models are amortized online learners. ArXiv, abs/2407.14207, 2024. URL https://api.semanticscholar. org/CorpusID:271310065.   \n[49] C. Lockard, P. Shiralkar, and X. L. Dong. OpenCeres: When Open Information Extraction Meets the Semi-Structured Web. In J. Burstein, C. Doran, and T. Solorio, editors, Proceedings of NAACL, pages 3047\u20133056, Minneapolis, Minnesota, 2019. doi: 10.18653/v1/N19-1309. URL https://aclanthology.org/N19-1309.   \n[50] I. Loshchilov and F. Hutter. Decoupled weight decay regularization, 2019.   \n[51] X. Ma, X. Kong, S. Wang, C. Zhou, J. May, H. Ma, and L. Zettlemoyer. Luna: Linear unified nested attention. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P. Liang, and J. W. Vaughan, editors, Advances in NIPS, volume 34, pages 2441\u20132453. Curran Associates, Inc., 2021. URL https://proceedings.neurips.cc/paper_files/paper/2021/file/ 14319d9cfc6123106878dc20b94fbaf3-Paper.pdf.   \n[52] H. H. Mao. Fine-tuning pre-trained transformers into decaying fast weights. In Y. Goldberg, Z. Kozareva, and Y. Zhang, editors, Proceedings of EMNLP, pages 10236\u201310242, Abu Dhabi, United Arab Emirates, 2022. URL https://aclanthology.org/2022.emnlp-main.697.   \n[53] E. Martin and C. Cundy. Parallelizing linear recurrent neural nets over sequence length. In Proceedings of ICLR, 2018. URL https://openreview.net/forum?id=HyUNwulC-.   \n[54] J. Mercat, I. Vasiljevic, S. Keh, K. Arora, A. Dave, A. Gaidon, and T. Kollar. Linearizing large language models, 2024.   \n[55] S. Merity, C. Xiong, J. Bradbury, and R. Socher. Pointer sentinel mixture models, 2016.   \n[56] T. Munkhdalai, A. Sordoni, T. Wang, and A. Trischler. Metalearned neural memory. ArXiv, abs/1907.09720, 2019. URL https://api.semanticscholar.org/CorpusID:198179407.   \n[57] M. Oren, M. Hassid, Y. Adi, and R. Schwartz. Transformers are multi-state rnns, 2024.   \n[58] R. Y. Pang, A. Parrish, N. Joshi, N. Nangia, J. Phang, A. Chen, V. Padmakumar, J. Ma, J. Thompson, H. He, and S. R. Bowman. Quality: Question answering with long input texts, yes!, 2022. URL https://arxiv.org/abs/2112.08608.   \n[59] D. Paperno, G. Kruszewski, A. Lazaridou, Q. N. Pham, R. Bernardi, S. Pezzelle, M. Baroni, G. Boleda, and R. Fern\u00e1ndez. The lambada dataset, 2016.   \n[60] G. Penedo, Q. Malartic, D. Hesslow, R. Cojocaru, A. Cappelli, H. Alobeidli, B. Pannier, E. Almazrouei, and J. Launay. The RefinedWeb dataset for Falcon LLM: outperforming curated corpora with web data, and web data only. arXiv preprint arXiv:2306.01116, 2023. URL https://arxiv.org/abs/2306.01116.   \n[61] B. Peng, D. Goldstein, Q. Anthony, A. Albalak, E. Alcaide, S. Biderman, E. Cheah, X. Du, T. Ferdinan, H. Hou, P. Kazienko, K. K. GV, J. Koco\u00b4n, B. Koptyra, S. Krishna, R. M. J. au2, N. Muennighoff, F. Obeid, A. Saito, G. Song, H. Tu, S. Woz\u00b4niak, R. Zhang, B. Zhao, Q. Zhao, P. Zhou, J. Zhu, and R.-J. Zhu. Eagle and finch: Rwkv with matrix-valued states and dynamic recurrence, 2024.   \n[62] H. Peng, N. Pappas, D. Yogatama, R. Schwartz, N. Smith, and L. Kong. Random feature attention. In Proceedings of ICLR, 2021. URL https://openreview.net/forum?id= QtTKTdVrFBB.   \n[63] H. Peng, J. Kasai, N. Pappas, D. Yogatama, Z. Wu, L. Kong, R. Schwartz, and N. A. Smith. ABC: Attention with bounded-memory control. In Proceedings of ACL, pages 7469\u20137483, 2022. URL https://aclanthology.org/2022.acl-long.515.   \n[64] R. Pope, S. Douglas, A. Chowdhery, J. Devlin, J. Bradbury, A. Levskaya, J. Heek, K. Xiao, S. Agrawal, and J. Dean. Efficiently scaling transformer inference, 2022.   \n[65] S. Pramanik, E. Elelimy, M. C. Machado, and A. White. Recurrent linear transformers, 2023. URL https://arxiv.org/abs/2310.15719.   \n[66] Z. Qin, X. Han, W. Sun, D. Li, L. Kong, N. Barnes, and Y. Zhong. The devil in linear transformer. In Y. Goldberg, Z. Kozareva, and Y. Zhang, editors, Proceedings of EMNLP, pages 7025\u20137041, Abu Dhabi, United Arab Emirates, 2022. doi: 10.18653/v1/2022.emnlp-main.473. URL https://aclanthology.org/2022.emnlp-main.473.   \n[67] Z. Qin, S. Yang, and Y. Zhong. Hierarchically gated recurrent neural network for sequence modeling. In Advances in NIPS, 2023. URL https://openreview.net/forum?id=P1TCHxJwLB.   \n[68] Z. Qin, D. Li, W. Sun, W. Sun, X. Shen, X. Han, Y. Wei, B. Lv, X. Luo, Y. Qiao, and Y. Zhong. Transnormerllm: A faster and better large language model with improved transnormer, 2024.   \n[69] Z. Qin, S. Yang, W. Sun, X. Shen, D. Li, W. Sun, and Y. Zhong. Hgrn2: Gated linear rnns with state expansion, 2024.   \n[70] P. Rajpurkar, R. Jia, and P. Liang. Know What You Don\u2019t Know: Unanswerable Questions for SQuAD. In Proceedings of ACL, Melbourne, Australia, 2018. Association for Computational Linguistics.   \n[71] L. Ren, Y. Liu, Y. Lu, Y. Shen, C. Liang, and W. Chen. Samba: Simple hybrid state space models for efficient unlimited context language modeling. CoRR, abs/2406.07522, 2024. doi: 10.48550/ARXIV.2406.07522. URL https://doi.org/10.48550/arXiv.2406.07522.   \n[72] A. Rush. Torch-struct: Deep structured prediction library. In A. Celikyilmaz and T.-H. Wen, editors, Proceedings of ACL, pages 335\u2013342, Online, July 2020. doi: 10.18653/v1/2020. acl-demos.38. URL https://aclanthology.org/2020.acl-demos.38.   \n[73] K. Sakaguchi, R. L. Bras, C. Bhagavatula, and Y. Choi. Winogrande: An adversarial winograd schema challenge at scale, 2019. URL https://arxiv.org/abs/1907.10641.   \n[74] I. Schlag and J. Schmidhuber. Gated fast weights for on-the-fly neural program generation. In Proceedings of ICLR, 2017. URL https://api.semanticscholar.org/CorpusID: 216094255.   \n[75] I. Schlag, K. Irie, and J. Schmidhuber. Linear transformers are secretly fast weight programmers. In M. Meila and T. Zhang, editors, Proceedings of ICML, pages 9355\u20139366. PMLR, 18\u201324 Jul 2021. URL https://proceedings.mlr.press/v139/schlag21a.html.   \n[76] I. Schlag, T. Munkhdalai, and J. Schmidhuber. Learning associative inference using fast weight memory, 2021. URL https://arxiv.org/abs/2011.07831.   \n[77] J. Schmidhuber. Learning to control fast-weight memories: An alternative to dynamic recurrent networks. Neural Computation, 4(1):131\u2013139, 1992.   \n[78] N. Shazeer. Glu variants improve transformer, 2020.   \n[79] D. Soboleva, F. Al-Khateeb, R. Myers, J. R. Steeves, J. Hestness, and N. Dey. Slimpajama: A 627b token cleaned and deduplicated version of redpajama, 2023. URL https: //huggingface.co/datasets/cerebras/SlimPajama-627B.   \n[80] J. Su, Y. Lu, S. Pan, A. Murtadha, B. Wen, and Y. Liu. Roformer: Enhanced transformer with rotary position embedding, 2023.   \n[81] S. Sukhbaatar, a. szlam, J. Weston, and R. Fergus. End-to-end memory networks. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in NIPS. Curran Associates, Inc., 2015. URL https://proceedings.neurips.cc/paper_files/paper/ 2015/file/8fb21ee7a2207526da55a679f0332de2-Paper.pdf.   \n[82] Y. Sun, L. Dong, S. Huang, S. Ma, Y. Xia, J. Xue, J. Wang, and F. Wei. Retentive network: A successor to transformer for large language models, 2023.   \n[83] Y. Sun, L. Dong, Y. Zhu, S. Huang, W. Wang, S. Ma, Q. Zhang, J. Wang, and F. Wei. You only cache once: Decoder-decoder architectures for language models, 2024. URL https://arxiv.org/abs/2405.05254.   \n[84] Y. Sun, X. Li, K. Dalal, J. Xu, A. Vikram, G. Zhang, Y. Dubois, X. Chen, X. Wang, O. Koyejo, T. Hashimoto, and C. Guestrin. Learning to (learn at test time): Rnns with expressive hidden states. ArXiv, abs/2407.04620, 2024. URL https://api.semanticscholar.org/CorpusID: 271039606.   \n[85] M. Suzgun, N. Scales, N. Sch\u00e4rli, S. Gehrmann, Y. Tay, H. W. Chung, A. Chowdhery, Q. Le, E. Chi, D. Zhou, and J. Wei. Challenging BIG-bench tasks and whether chain-of-thought can solve them. In Findings of the ACL, pages 13003\u201313051, Toronto, Canada, 2023. URL https://aclanthology.org/2023.findings-acl.824.   \n[86] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozi\u00e8re, N. Goyal, E. Hambro, F. Azhar, A. Rodriguez, A. Joulin, E. Grave, and G. Lample. Llama: Open and efficient foundation language models, 2023.   \n[87] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, D. Bikel, L. Blecher, C. C. Ferrer, M. Chen, G. Cucurull, D. Esiobu, J. Fernandes, J. Fu, W. Fu, B. Fuller, C. Gao, V. Goswami, N. Goyal, A. Hartshorn, S. Hosseini, R. Hou, H. Inan, M. Kardas, V. Kerkez, M. Khabsa, I. Kloumann, A. Korenev, P. S. Koura, M.-A. Lachaux, T. Lavril, J. Lee, D. Liskovich, Y. Lu, Y. Mao, X. Martinet, T. Mihaylov, P. Mishra, I. Molybog, Y. Nie, A. Poulton, J. Reizenstein, R. Rungta, K. Saladi, A. Schelten, R. Silva, E. M. Smith, R. Subramanian, X. E. Tan, B. Tang, R. Taylor, A. Williams, J. X. Kuan, P. Xu, Z. Yan, I. Zarov, Y. Zhang, A. Fan, M. Kambadur, S. Narang, A. Rodriguez, R. Stojnic, S. Edunov, and T. Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023. URL https://arxiv.org/abs/2307.09288.   \n[88] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. u. Kaiser, and I. Polosukhin. Attention is all you need. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in NIPS. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper_files/paper/2017/file/ 3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf.   \n[89] R. Waleffe, W. Byeon, D. Riach, B. Norick, V. Korthikanti, T. Dao, A. Gu, A. Hatamizadeh, S. Singh, D. Narayanan, G. Kulshreshtha, V. Singh, J. Casper, J. Kautz, M. Shoeybi, and B. Catanzaro. An empirical study of mamba-based language models, 2024. URL https: //arxiv.org/abs/2406.07887.   \n[90] J. Wang, D. Paliotta, A. May, A. M. Rush, and T. Dao. The mamba in the llama: Distilling and accelerating hybrid models, 2024. URL https://arxiv.org/abs/2408.15237.   \n[91] K. Wen, X. Dang, and K. Lyu. Rnns are not transformers (yet): The key bottleneck on in-context retrieval. ArXiv, abs/2402.18510, 2024. URL https://api.semanticscholar. org/CorpusID:268041425.   \n[92] B. Widrow and M. E. Hoff. Adaptive switching circuits. 1988. URL https://api. semanticscholar.org/CorpusID:60830585.   \n[93] E. Wu, K. Wu, R. Daneshjou, D. Ouyang, D. E. Ho, and J. Zou. How medical AI devices are evaluated: limitations and recommendations from an analysis of FDA approvals. Nature Medicine, pages 582\u2013584, 2021. URL https://doi.org/10.1038/s41591-021-01312-x.   \n[94] W. Xiong, J. Liu, I. Molybog, H. Zhang, P. Bhargava, R. Hou, L. Martin, R. Rungta, K. A. Sankararaman, B. Oguz, M. Khabsa, H. Fang, Y. Mehdad, S. Narang, K. Malik, A. Fan, S. Bhosale, S. Edunov, M. Lewis, S. Wang, and H. Ma. Effective long-context scaling of foundation models, 2023. URL https://arxiv.org/abs/2309.16039.   \n[95] S. Yang and Y. Zhang. FLA: A Triton-Based Library for Hardware-Efficient Implementations of Linear Attention Mechanism, 2024. URL https://github.com/sustcsonglin/ flash-linear-attention.   \n[96] S. Yang, B. Wang, Y. Shen, R. Panda, and Y. Kim. Gated linear attention transformers with hardware-efficient training. In Proceedings of ICML. PMLR, 2024. [97] S. Yang, B. Wang, Y. Zhang, Y. Shen, and Y. Kim. Parallelizing linear transformers with the delta rule over sequence length. CoRR, abs/2406.06484, 2024. doi: 10.48550/ARXIV.2406. 06484. URL https://doi.org/10.48550/arXiv.2406.06484. [98] S. Yang, B. Wang, Y. Zhang, Y. Shen, and Y. Kim. Parallelizing linear transformers with the delta rule over sequence length. ArXiv, abs/2406.06484, 2024. URL https://api. semanticscholar.org/CorpusID:270371554. [99] R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and Y. Choi. Hellaswag: Can a machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, 2019.   \n[100] J. Zhang, S. Jiang, J. Feng, L. Zheng, and L. Kong. Cab: Comprehensive attention benchmarking on long sequence modeling. ArXiv, abs/2210.07661, 2022. URL https: //api.semanticscholar.org/CorpusID:252907545.   \n[101] M. Zhang, K. Bhatia, H. Kumbong, and C. R\u00e9. The hedgehog & the porcupine: Expressive linear attentions with softmax mimicry, 2024.   \n[102] P. Zhang, G. Zeng, T. Wang, and W. Lu. Tinyllama: An open-source small language model, 2024.   \n[103] W. Zhang and B. Zhou. Learning to update auto-associative memory in recurrent neural networks for improving sequence memorization. ArXiv, abs/1709.06493, 2017. URL https: //api.semanticscholar.org/CorpusID:22458497.   \n[104] Y. Zhang and D. Cai. Linearizing transformer with key-value memory. In Y. Goldberg, Z. Kozareva, and Y. Zhang, editors, Proceedings of EMNLP, pages 346\u2013359, Abu Dhabi, United Arab Emirates, 2022. doi: 10.18653/v1/2022.emnlp-main.24. URL https: //aclanthology.org/2022.emnlp-main.24.   \n[105] M. Zhong, D. Yin, T. Yu, A. Zaidi, M. Mutuma, R. Jha, A. H. Awadallah, A. Celikyilmaz, Y. Liu, X. Qiu, and D. Radev. QMSum: A new benchmark for query-based multi-domain meeting summarization. In Proceedings of NAACL, pages 5905\u20135921, Online, 2021. doi: 10.18653/v1/2021.naacl-main.472. URL https://aclanthology.org/2021.naacl-main. 472.   \n[106] G.-B. Zhou, J. Wu, C.-L. Zhang, and Z.-H. Zhou. Minimal gated unit for recurrent neural networks, 2016. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "A Linear Attention and its Chunkwise Form ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Linear Attention (LA) [42, 66, 68] emerges as an alternative to resolve the quadratic complexity of self-attention (SA). The key idea is to use the kernel trick, which replaces softmax with a decomposable kernel function, resulting the following parallel form:10 ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbf{O}=\\big((\\phi(\\mathbf{Q})\\phi(\\mathbf{K})^{\\top})\\odot\\mathbf{M}\\big)\\mathbf{V}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\phi:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}^{m}$ functions as feature mapping applied to each input. Unfolding Eq. 10, we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{q}_{t},k_{t},v_{t}=\\mathbf{W}_{q}\\mathbf{x}_{t},\\mathbf{W}_{k}\\mathbf{x}_{t},\\mathbf{W}_{v}\\mathbf{x}_{t}\\in\\mathbb{R}^{d},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "equation", "text": "$$\no_{t}=\\sum_{i=1}^{t}\\boldsymbol{v}_{i}f(\\boldsymbol{k}_{i}^{\\top}\\boldsymbol{q}_{t})=\\sum_{i=1}^{t}\\boldsymbol{v}_{i}\\phi(\\boldsymbol{k}_{i})^{\\top}\\phi(\\boldsymbol{q}_{t})=\\left[\\mathbf{S}_{t}\\equiv\\sum_{i=1}^{t}\\phi(\\boldsymbol{k}_{i})\\otimes\\boldsymbol{v}_{i}\\right]^{\\top}\\phi(\\boldsymbol{q}_{t}).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "$\\otimes$ means outer product operation. It is clear that by leveraging the associativity, LA admits simple recurrent updating rules with matrix-valued hidden states $\\bar{\\mathbf{S}_{t}}\\in\\bar{\\mathbb{R}^{m\\times d}}$ : ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{o}_{t}=\\mathbf{S}_{t}^{\\top}\\phi(\\pmb{q}_{t});\\;\\mathbf{S}_{t}=\\mathbf{S}_{t-1}+\\phi(\\pmb{k}_{t})\\otimes\\pmb{v}_{t}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "By reserving bounded $m$ memory slots only, the overall computation complexity is reduced from $\\dot{O(T^{2}d)}$ to $\\bar{O}(T m d)$ . When the sequence length is $T\\gg m,d$ , the $m d$ factor has a minor impact on the complexity, and LA can be much more efficient than its counterpart with quadratic complexity. ", "page_idx": 17}, {"type": "text", "text": "During inference, LA enjoys the merits of RNNs, which only need to maintain $O(m d)$ hidden memories, helping avoid the memory-cost KV cache management in SA mechanisms. However, Eq. 12 employs a simple additive updating rule and can be hard to \u201cforget\u201d unrelated information if necessary [62], making the limited memory states vulnerable to be chaotic. ", "page_idx": 17}, {"type": "text", "text": "Gating mechanism has played a key role in classical RNNs [32, 22, 12], which serves as a mechanism to control the information flows in the network and help read and write from the memory selectively. [82] propose to apply a data-independent gate to LA, significantly narrowing the gap between LA and SA: $\\mathbf{S}_{t}\\,=\\,\\lambda\\mathbf{S}_{t-1}+\\phi(k_{t})\\otimes\\pmb{v}_{t}$ , $\\bar{\\lambda}\\in\\,[0,1]$ is a non-learnable scalar. Recent work [96, 43] further imposes a finer-grained data-dependent gate: ", "page_idx": 17}, {"type": "equation", "text": "$$\n{\\bf S}_{t}=\\mathrm{Diag}({\\pmb\\alpha}_{t}){\\bf S}_{t-1}+\\phi({\\pmb k}_{t})\\otimes{\\pmb v}_{t},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where each $\\alpha_{t}\\in[0,1]^{m}$ from $\\mathbf{A}:=\\{\\pmb{\\alpha}_{i}\\}_{i=1}^{T}\\in[0,1]^{T\\times m}$ is dependent on the input. Alternatively, we can couple the key values with the forget gates by allowing $\\phi(k_{t})=1-\\alpha_{t}$ in spirit of [12, 106] and [69], which reduces the number of parameters and improves efficiency accordingly. ", "page_idx": 17}, {"type": "text", "text": "A.1 Hardware-Efficient Training ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Despite the theoretical advantages of linear complexity, the recurrent form of Eq. 12 is still inefficient during training. Such recurrent computation prevents the full utilization of modern GPU parallelism over sequence lengths [53, 72]. On the other hand, the parallel form (Eq. 10) can be parallelized in similar vein as in flash attention [17, 15]. However, due to the existence of the casual mask M, we can not rearrange its computation order by $\\mathbf{KV}$ first, so that the parallel form still adheres to the quadratic complexity, which can hardly be scaled to very-long training context (e.g., sequences with more than 8K tokens). ", "page_idx": 17}, {"type": "text", "text": "Chunkwise form recurrences have been carried forward by [82], and achieve a good trade-off between the recurrent and parallel forms. [96] further disclose that the element-wise gating of Eq. 13 also satisfies the associative property required by parallel scan [9] and derive a parallelized chunkwise gated linear attention in a similar vein. The key idea is to partition the sequence into $\\begin{array}{r}{N=\\left\\lceil\\frac{T}{C}\\right\\rceil}\\end{array}$ chunks of size $C$ with $\\mathbf{Q}_{[t]}=q_{t C},q_{t C+1},\\dots,q_{t C+C}$ , and so forth for $\\mathbf{K}_{[t]},\\mathbf{V}_{[t]}\\in\\mathbb{R}^{C\\times d}$ , $\\mathbf{A}_{[t]}\\in\\mathbb{R}^{C\\times m}$ . Firstly, unrolling the $i$ -th hidden state in the $t$ -th chunk in Eq. 13, we get ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbf{S}_{[t],i}=\\mathrm{Diag}\\left(\\mathbf{A}_{[t],i}\\right)\\mathbf{S}_{[t],i-1}+\\phi\\left(\\mathbf{K}_{[t],i}\\right)\\otimes\\mathbf{V}_{[t],i}=\\cdot\\cdot}\\\\ {\\displaystyle=\\mathrm{Diag}\\left(\\prod_{j=1}^{i}\\mathbf{A}_{[t],j}\\right)\\mathbf{S}_{[t-1],C}+\\displaystyle\\sum_{k=1}^{i}\\left(\\phi(\\mathbf{K}_{[t],k})\\odot\\prod_{j=k+1}^{i}\\mathbf{A}_{[t],j}\\right)\\otimes\\mathbf{V}_{[t],k}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "10There is a normalization term in vanilla LA similar to softmax, [66] reveal that removing it could avoid potential gradient explosions. ", "page_idx": 17}, {"type": "text", "text": "We write the last hidden in the chunk $\\mathbf{S}_{[t],C}$ as $\\mathbf{S}_{[t]}$ interchangeably for simplicity. Define $\\vec{\\mathcal{A}}_{[t],i}=$ $\\begin{array}{r}{\\prod_{j=1}^{i}\\mathbf{A}_{[t],j}\\,\\in\\,[0,1]^{d}}\\end{array}$ as the cumulative decay from the start of chunk to $i$ , and likewise $\\overleftarrow{A}_{[t],i}=$ $\\begin{array}{r}{\\prod_{j=i+1}^{C}\\mathbf{A}_{[t],j}\\in[0,1]^{d}}\\end{array}$ from $i+1$ to the end of the chunk, then ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbf{S}_{[t]}=\\mathrm{Diag}(\\vec{\\mathcal{A}}_{[t],C})\\mathbf{S}_{[t-1]}+(\\mathbf{K}_{[t]}\\odot\\overleftarrow{\\mathcal{A}}_{[t]})^{\\top}\\mathbf{V}_{[t]}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "$\\vec{\\mathcal{A}},\\overleftarrow{\\mathcal{A}}$ can be absorbed into $\\mathbf{Q},\\mathbf{K}\\;\\mathrm{first}:\\overline{{\\mathbf{Q}}}_{[t]}=\\phi(\\mathbf{Q}_{[t]})\\odot\\vec{\\mathcal{A}}_{[t]}$ , $\\overline{{\\mathbf{K}}}_{[t]}=\\phi(\\mathbf{K}_{[t]})\\odot(\\overleftarrow{A}_{[t]}/\\overrightarrow{A}_{[t],C})$ . Combining them with Eq. 10 and Eq. 14, we derive the following vectorized updating rules ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbf{O}_{[t]}=\\overline{{\\mathbf{Q}}}_{[t]}\\mathbf{S}_{[t-1]}+\\left(\\overline{{\\mathbf{Q}}}_{[t]}\\overline{{\\mathbf{K}}}_{[t]}^{\\top}\\odot\\mathbf{M}_{[t]}\\right)\\mathbf{V}_{[t]}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The first term is referred to as the inter chunk part and the second term is the intra chunk part. The process to get this intra part is a little more involved as the cumulative productions of $\\overleftarrow{A}_{[t]}/\\overrightarrow{A}_{[t],C}$ is greater than 1, which can lead to numerical instability. [96] deal with this issue by proposing a secondary-chunking strategy, and we refer readers to their paper for more details. ", "page_idx": 18}, {"type": "text", "text": "Hardward considerations Modern GPU architectures, such as the NVIDIA A100, offer highly optimized matrix multiplication (matmul) operations through specialized Tensor Cores, achieving up to $16\\times$ higher throughput than non-matmul operations [17]. However, this incurs IO overheads due to data transfer from slower, off-chip global high bandwidth memory (HBM) to on-chip shared memory (SRAM). The chunkwise form balances I/O and computation complexity tradeoffs. As shown in Eq.16, it improves parallelism over the sequence dimension while reducing non-matmul FLOPs greatly. Also, the chunk recurrent updating conducts the query and hidden states reduction in an online manner, requiring only $O(N d\\bar{m})$ hidden states materialized into HBMs, so that it can significantly reduce the memory/IO overheads. While LA enjoys much lower overall running FLOPs than SA, the chunkwise form displays a practical significant wall-clock speedup against SA, due to its hardware-efficient implementations [95]. ", "page_idx": 18}, {"type": "text", "text": "B Details for GSA ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Beyond the recurrent GSA form provided in Figure. 1, we give detailed, hardware-efficient procedures for the forward and backward passes of Gated Slot Attention (GSA) in Algorithm 1. For simplicity, we define $\\mathbf{A}\\,=\\,\\{\\pmb{\\alpha}_{i}\\}_{i=1}^{T}\\,\\in\\,[\\dot{0},1]^{T\\times m}$ , and $\\textbf{I}=$ $\\{\\mathbf{1}-\\alpha_{i}\\}_{i=1}^{T}\\,\\in\\,[0,1]^{T\\times m}$ . The algorithm demonstrates that GSA can be modeled as a two-pass GLA, as illustrated in Fig. 5. ", "page_idx": 18}, {"type": "text", "text": "In the preprocessing step, we precompute the chunkwise cumulative sum of the forget gate, resulting in $\\vec{\\mathcal A}$ . Subsequently, $\\bar{\\vec{A}}$ along with the queries, keys, and values are passed to engage in two ", "page_idx": 18}, {"type": "image", "img_path": "jY4PhQibmg/tmp/54d13ca972f3079337aa5193a7eb7c15b25ec980d068ec61a97ed30d8f08f141.jpg", "img_caption": ["Figure 5: Diagrams of the recurrence and updating rules in Gated Slot Attention. The outputs of the first pass is taken as queries of the second pass. : query nodes : key/value nodes output nodes : recurrent hidden states "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "GLA passes. For each chunk of size $C$ , we define $\\overleftarrow{\\boldsymbol{A}}_{[i]}:=\\overrightarrow{\\boldsymbol{A}}_{[i],C}/\\overrightarrow{\\boldsymbol{A}}_{[i]}$ as in Eq. 15 and Eq. 16. ", "page_idx": 18}, {"type": "text", "text": "In the first pass, $\\vec{A},\\overleftarrow{A}$ is absorbed into $\\mathbf{Q},\\mathbf{K}:\\bar{\\mathbf{Q}}_{[i]}=\\mathbf{Q}_{[i]}\\odot\\vec{\\mathcal{A}}_{[i]}$ , $\\bar{\\mathbf{K}}_{[i]}=\\mathbf{K}_{[i]}\\odot(\\overleftarrow{A}_{[i]}/\\overrightarrow{\\lambda}_{[i],C})$ , then $\\bar{\\bf Q}$ and $\\bar{\\bf K}$ function as usual queries and keys, and the slot representations $\\mathbf{I}$ serve as the value vectors. ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbf{O}_{[i]}^{k}\\ =\\underbrace{\\bar{\\mathbf{Q}}_{[i]}\\ \\ \\mathbf{S}_{[i-1]}^{k}}_{\\mathbf{O}_{[i]}^{\\mathrm{inter}}}+\\underbrace{((\\bar{\\mathbf{\\Delta}}\\bar{\\mathbf{Q}}_{[i]}\\ \\ \\bar{\\mathbf{K}}_{[i]}^{\\top})\\odot\\mathbf{M})\\mathbf{I}_{[i]}}_{\\mathbf{O}_{[i]}^{\\mathrm{intra}}}\\in\\mathbb{R}^{C\\times m}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We use different notations from those presented in Eq.6 to enhance clarity in the chunkwise updating rules. The output $\\mathbf{O}^{k}$ is decomposed into the inter-chunk recurrence and intra-chunk parallel computations [96]. ", "page_idx": 18}, {"type": "image", "img_path": "jY4PhQibmg/tmp/a9dfd4475d77fa74b6e045b6248b84af69727cf4777875adf95f4d30c7439e81.jpg", "img_caption": [], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "In the second pass, the output $\\mathbf{O}^{k}$ from the first pass, after the application of the softmax function, serves as the queries $\\mathbf{Q}^{v}$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\textbf{Q}^{v}\\,=\\,\\mathrm{softmax}(\\textbf{O}^{k}\\,)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and I/V are used as the key/value vectors, respectively. The final GSA output $\\mathbf{O}$ is obtained as follows: ", "page_idx": 19}, {"type": "equation", "text": "$$\n{\\bf O}_{[i]}\\;=\\;{\\bf Q}_{[i]}^{v}\\;{\\bf\\delta S}_{[i-1]}^{v}\\;+((\\;{\\bf Q}_{[i]}^{v}\\;{\\bf\\delta I}_{[i]}^{\\top})\\odot{\\bf M})\\bar{\\bf V}_{[i]}\\;\\in\\mathbb R^{C\\times d}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Unlike in the first pass, $\\vec{\\mathcal{A}},\\overleftarrow{\\mathcal{A}}$ is absorbed into $\\mathbf{V},\\mathbf{O}$ rather than $\\mathbf{Q},\\mathbf{K}$ ", "page_idx": 19}, {"type": "text", "text": "During the backward pass, computing the gradients of $\\mathbf{Q},\\mathbf{K},\\mathbf{V},\\mathbf{I},\\mathbf{A}$ involves variables already computed in the forward pass. However, directly saving all intermediate results can pose severe challenges for memory management. To address this issue, we adopt gradient checkpointing [11] to trade off memory consumption for recomputation. In addition to the input $\\mathbf{Q},\\mathbf{K},\\mathbf{V},\\mathbf{I},\\mathbf{A}$ , we selectively save only the output of the first GLA pass, which significantly reduces memory consumption (Figure 4b). ", "page_idx": 19}, {"type": "text", "text": "Similar to the forward pass, the backward pass involves two GLA backward passes as well, but in the reverse order. The final gradient $\\mathrm{d}\\mathbf{I}$ is obtained by combining the gradients from these computations, i.e., $\\mathrm{d}\\mathbf{I}=\\mathrm{d}\\mathbf{I}^{k}+\\mathrm{d}\\mathbf{I}^{v}$ . The forget gate gradient can be decomposed into two parts: $\\mathbf{Q}\\odot\\mathrm{d}\\mathbf{Q}-\\mathbf{K}\\odot\\mathrm{d}\\mathbf{K}$ and $\\mathbf{O}\\odot\\mathrm{d}\\mathbf{O}-\\mathbf{V}\\odot\\mathrm{d}\\mathbf{V}$ (cf. $\\S C$ in [96]). The reversed cumulative sum in the backward pass corresponds to the cumulative sum computed in the preprocessing step of the forward pass. ", "page_idx": 19}, {"type": "text", "text": "We provide a PyTorch implementation for the above algorithm with chunkwise parallelism in Listing 1. ", "page_idx": 19}, {"type": "text", "text": "1 def gsa_fwd_k(q, k, v, g, C):   \n2   \n3 q/k/v:   \n4 query, key, value of shape [NC, C, K|V]   \ng:   \n6 local cumulative product of forget gate in log space   \n7 C:   \n8 chunk size   \n'''   \n9   \n10 # NC: number of chunks   \n11 # K: query/key head dimension   \n12 # V: value head dimension   \n13 NC, C, K, $v=\\ast\\mathfrak{q}$ .shape, v.shape[-1]   \n14 # [K, V]   \n15 s = q.new_zeros(K, V)   \n16 # [NC, C, V]   \n17 $\\textsf{o}=$ torch.empty_like(v)   \n18   \n19 for i in range(0, NC):   \n20 # [C, K|V] chunking   \n21 c_q, c_k, c_v, $\\mathsf{c}_{-}\\mathsf{g}\\;=\\;\\mathsf{q}[\\mathsf{i}]$ , k[i], v[i], g[i]   \n22 # the last g of each chunk   \n23 $\\mathsf{c\\_g n}\\;=\\;\\mathsf{c\\_g}\\mathsf{[-1\\,]}$   \n24 # inter-chunk w/ matmul   \n25 c_vg, c_gn $=$ c_v $\\star$ (c_gn - c_g).exp(), c_gn.exp()   \n26 # [C, V]   \n27 c_o_inter $=(c_{-}q\\,\\,\\odot\\,\\,\\mathsf{s})\\,\\,\\star\\,\\,\\mathsf{c}_{-}\\mathsf{g}\\,.\\,\\mathsf{e x p}()$   \n28 # hidden state update   \n29 $s=c_{-}\\mathsf{g n}\\,\\star\\,\\mathsf{s}\\,+\\,c_{-}\\mathsf{k}\\,.\\,\\mathsf{t}()\\,\\mathsf{~@~\\mathsf{c}\\_{-}\\mathsf{v g}~}$   \n30   \n31 # intra-chunk   \n32 # [C, C]   \n33 ${\\mathsf{c}}\\_{\\mathsf{A}}\\;=\\;{\\mathsf{c}}\\_{{\\mathsf{d}}}\\;\\Theta\\;\\;{\\mathsf{c}}\\_{{\\mathsf{k}}\\cdot{\\mathsf{t}}}(\\k)$   \n34 # [C, V]   \n35 c_o_intra $=$ torch.zeros_like(c_v)   \n36 for j in range(0, C // 16):   \n37 $\\sf t~=~s1i c e(j~\\times~16,~j~\\times~16~+~16)$   \n38 # [16, K|V] subchunking   \n39 s_A, s_v, s_g = c_A[t], c_v[t], c_g[t]   \n40 $s_{-}0\\ =\\ {\\mathfrak{q}}$ .new_zeros(16, V)   \n41   \n42 # inter-subchunk w/ matmul   \n43 $s_{-8}\\mathsf{n}~=~\\mathsf{s}_{-8}[\\otimes]$   \n44 for si in range(0, j):   \n45 $\\textsf{u}=$ slice(si $\\star\\,\\,\\,1\\,6$ , $\\sin\\ \\star\\ \\ 16\\ \\,+\\ \\ 16)$   \n46 ${\\sf S_{-}0}\\mathrel{+}={\\sf\\ S_{-}A L}$ :, u] @ (c_v[u] $\\star$ (s_gn - c_g[u]).exp())   \n47 ${\\tt S\\_O}\\;\\;\\star=\\;\\;({\\tt s\\_g}\\;\\;-\\;\\;{\\tt s\\_g n})\\ldots\\tt e x p(\\k)$   \n48 # intra-subchunk w/o matmul   \n49 for si in range(16):   \n50 for sj in range $(\\mathrm{\\Delta_{Si}+\\Delta\\Omega}1)$ ):   \n51 s_o[si] $+=$ s_A[si, j \\* 16 + sj] $\\star$ s_v[sj] $\\star$ (s_g[si] - s_g[sj]).exp()   \n52 c_o_intra[t] $=~{\\mathsf{S}}_{-0}$   \n53 # [C, V]   \n54 $\\circ[\\mathfrak{i}]\\ =\\ \\mathsf{c}_{-0_{-}}$ inter $^+$ c_o_intra   \n55 return o   \n56   \n57   \n58 def gsa_fwd_v(q, k, v, g, C):   \n59 NC, C, K, $v=\\ast\\mathfrak{q}$ .shape, v.shape[-1]   \n60 $s=\\ q$ .new_zeros(K, V)   \n61 o $=$ torch.empty_like(v)   \n62   \n64 # [C, K|V] chunking   \n65 c_q, c_k, c_v, $\\mathsf{c}_{-}\\mathsf{g}\\;=\\;\\mathsf{q}[\\mathsf{i}]$ , k[i], v[i], g[i]   \n66 # the last g of each chunk   \n67 $\\mathsf{c\\_g n}\\;=\\;\\mathsf{c\\_g}\\mathsf{[-1\\,]}$   \n68 # inter-chunk w/ matmul   \n69 c_qg, c_kg, c_gn $=$ c_q $\\star$ c_g.exp(), c_k $\\star$ (c_gn - c_g).exp(), c_gn.exp()   \n70 # [C, V]   \n71 c_o_inter $=$ c_qg @ s   \n72 # hidden state update   \n73 s = c_gn[:, None] \\* s + c_kg.t() @ c_v   \n74   \n75 # intra-chunk   \n76 $c_{-}\\mathsf{A}\\ =\\ \\mathsf{q}$ .new_zeros(C, C)   \n77 for j in range(0, C // 16):   \n78 $\\sf t~=~s1i c e(j~\\times~16,~j~\\times~16~+~16)$   \n79 # [16, K|V] subchunking   \n80 s_q, s_k, ${\\sf s}_{-}{\\sf g}\\ =\\ {\\sf c}_{-}{\\sf q}[{\\sf t}]$ , c_k[t], c_g[t]   \n81 s_A $=~\\mathfrak{q}$ .new_zeros(16, 16)   \n82   \n83 # intra-subchunk w/o matmul   \n84 for si in range(16):   \n85 for sj in range(si $+7$ ):   \n86 s_A[si, sj] $=$ torch.sum(s_q[si] $\\star$ s_k[sj] $\\star$ (s_g[si] - s_g[sj]).exp())   \n87 c_A[t, t $\\mathrm{~\\mathsf~{~J~}~}=\\mathsf{\\textbf~{~s~}~}\\mathsf{A}$   \n88 # inter-subchunk w/ matmul   \n89 $s_{-8}\\mathsf{n}~=~\\mathsf{s}_{-8}[\\otimes]$   \n90 ${\\sf s\\!\\!\\_q g}\\;=\\;{\\sf s\\!\\!\\_q}\\;\\star\\;\\left({\\sf s\\!\\!_{-}g}\\;-\\frac{}{}{}\\sf s\\!\\!\\_g{n}\\right).\\sf e x p()$   \n91 for si in range(0, j):   \n92 $\\textsf{u}=$ slice(si $\\star\\,\\,\\,1\\,6$ , si $.\\star\\,\\,\\,16\\,\\,+\\,\\,\\,16)$   \n93 c_A[t, u] $=$ s_qg @ (c_k[u] $\\star$ (s_gn - c_g[u]).exp()).t()   \n94 c_o_intra $=$ c_A @ c_v   \n95 # [C, V]   \n96 o[i] $=$ c_o_inter $^+$ c_o_intra   \n97 return o   \n98   \n99   \n100 def gsa(q, k, v, s, g):   \n101 T, $\\textsf{M}=\\textsf{s}$ .shape   \n102 # reshape each input to [NC, C, K|V]   \n103 q, k, v, s, $\\textsf{g}=$ map(lambda x: x.view(-1, C, x.shape[-1]), (q, k, v, s, g))   \n104 # local compute of cumulative product of decay   \n105 # [NC, C, K]   \n106 $_{\\mathtt{g}}=\\mathtt{g}$ .cumsum(1)   \n107 ok $=$ gsa_fwd_k(q, k, s, g, M)   \n108 qv $=$ ok.softmax(-1)   \n109 o = gsa_fwd_v(qv, s, v, g, M)   \n10 return o.view(T, -1) ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "Listing 1: Pseudo PyTorch-style code snippet for GSA with chunkwise parallelism. For brevity, we omit the dimensions of batch size and number of heads. Notably, unlike Algorithm 1, we obtain the intra outputs via a secondary chunking strategy in Line 31-52 and Line 75-94, as utilized by GLA [96], to ensure numerical stability. ", "page_idx": 21}, {"type": "text", "text": "C Experimental Setup ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "C.1 Language Modeling ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We compare GSA with the following strong Transformers with modern architectural recipes as well as other recent subquadratic architectures: ", "page_idx": 21}, {"type": "text", "text": "\u2022 $X_{\\mathrm{fmr++}}$ [86]: Llama-like architectures that enhance the vanilla Transformer by using Rotary position embeddings [80] and GLU [78]. ", "page_idx": 21}, {"type": "text", "text": "\u2022 Mamba [29]: State-space models with data-dependent decay.   \n\u2022 RetNet [82]: Linear attention with non-learnable, data-independent head-wise decay and rotary embedding.   \n\u2022 GLA [96]: Linear attention with elementwise data-dependent decay.   \n\u2022 HGRN2 [69]: Gated Linear RNN with state expansion, or GLA with improved parameterization. ", "page_idx": 22}, {"type": "text", "text": "Setup. For a fair comparison, all models are trained from scratch with the same training recipes. We utilize a subset of 100B tokens picked from the Slimpajama dataset [79]. The input tokens are processed using the Mistral tokenizer [39] 11. We use AdamW [50] with a weight decay 0.01 as the optimizer. During training, the learning rate is first warmed up to $3\\times10^{-4}$ in the first 1B tokens, and then decayed to $\\bar{3}\\times10^{-5}$ gradually with a cosine schedule. The number of attention heads is set to 4 and 5 for 1.3B and 2.7B models, respectively. The number of memory slots is uniformly set to 64 for all models. We utilize the open-sourced Triton-based library FLA [95] to run all compared models. ", "page_idx": 22}, {"type": "text", "text": "We ran all models on 32 Nvidia H800 GPUs. To facilitate distributed training and accelerate the process, we utilized the DeepSpeed framework and fused all necessary modules, including ROPE, cross-entropy, and LayerNorm, following the practice of [102]. The training of a GSA model with 2.7B parameters took approximately 2 days, while the 1.3B model required 1 day to complete training. ", "page_idx": 22}, {"type": "text", "text": "Remark on state size. Let the model dimension be denoted as $d$ . Mamba expands the value projection to $2d$ and uses a state expansion ratio of 16, resulting in a state size of $32d$ per layer. Since Mamba also replaces the FFN with a Mamba layer, this effectively doubles both the number of recurrent layers and the state size, leading to a total recurrent state size of $64L d$ . ", "page_idx": 22}, {"type": "text", "text": "Similarly, RetNet expands the value projection to $2d$ and sets the head dimension of queries/keys to be half that of the value head dimension. RetNet also reduces the number of heads to increase the head dimensions of queries and keys. We fix the query/key head dimension to 256 and adjust the number of heads accordingly, resulting in a recurrent state size of $512d$ per layer and $512L d$ in total. ", "page_idx": 22}, {"type": "text", "text": "GLA does not expand the value projection but reduces the head dimensions of queries and keys to half of the value head dimension to save parameters for the Swish output gate, ensuring each layer contains $4d^{2}$ parameters. We fix the query/key head dimension to 256 and adjust the number of heads accordingly, resulting in a recurrent state size of $256d$ per layer and $256L d$ in total. ", "page_idx": 22}, {"type": "text", "text": "HGRN2 follows a similar approach to GLA but without the Swish output gate, keeping the head dimensions of queries/keys and values equal, as in standard softmax attention, while still retaining $4d^{2}$ total parameters per recurrent layer. We set the head dimension to 128, resulting in a recurrent state size of $128d$ per layer and $128L d$ in total. ", "page_idx": 22}, {"type": "text", "text": "GSA maintains hidden states for both keys and values, so each layer contains a recurrent state size of $2\\times64\\times d$ . We fix the state expansion (i.e., number of slots) to $64^{12}$ , resulting in a total recurrent state size of $128L d$ . ", "page_idx": 22}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: This paper\u2019s contributions and scope are reflected in abstract and introduction part clearly. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 23}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: We discuss the limitations of this work in Section 6. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 23}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: This paper does not include theoretical results that require a full proof. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 24}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: The paper provides sufficient details on hyperparameters and training procedures to reproduce the results supporting its main conclusions. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 24}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: The code, data and pretrained models are publicly available at GitHub and Huggingface. The training processes are reproducible following the guidance in the paper. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 25}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We have detailed all the training and evaluation settings before the main results in the experimental part. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 25}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [No] ", "page_idx": 25}, {"type": "text", "text": "Justification: We do not have enough resources to obtain error bars as running the experiments multiple times is computationally expensive due to the large model size. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We provide information of GPU type and number of GPUs used for running our experiments. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 26}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: This work follows the NeurIPS Code of Ethics. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 26}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: We foresee no potential societal impact of this work. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: We foresee no such risks posed by this work. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 27}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: All of the datasets we use are publicly available at huggingface site, and we have properly cited all the training and evaluation datasets we used. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 27}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 28}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: This work does not release new assets. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 28}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: This work does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 28}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: This work does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 28}]