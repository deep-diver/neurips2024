[{"type": "text", "text": "LinNet: Linear Network for Efficient Point Cloud Representation Learning ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Hao Deng1 Kunlei $\\mathbf{J}\\mathbf{ing}^{2,3*}$ Shengmei Cheng1 Cheng Liu1 Jiawei ${\\bf R}{\\bf u}^{1}$ Jiang $\\mathbf{Bo}^{1}$ Lin Wang1\u2217 ", "page_idx": 0}, {"type": "text", "text": "1State-Province Joint Engineering and Research Center of Advanced Networking and Intelligent Information Services, School of Information Science and Technology, Northwest University 2School of Software Engineering, Xi\u2019an Jiaotong University ", "page_idx": 0}, {"type": "text", "text": "3Department of Computing, The Hong Kong Polytechnic University ", "page_idx": 0}, {"type": "text", "text": "denghao@stumail.nwu.edu.cn, kunlei.jing@xjtu.edu.cn 1615241805@qq.com, lc@nwu.edu.cn rujiawei@stumail.nwu.edu.cn, {jiangbo, wanglin}@nwu.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Point-based methods have made significant progress, but improving their scalability in large-scale 3D scenes is still a challenging problem. In this paper, we delve into the point-based method and develop a simpler, faster, stronger variant model, dubbed as LinNet. In particular, we first propose the disassembled set abstraction (DSA) module, which is more effective than the previous version of set abstraction. It achieves more efficient local aggregation by leveraging spatial anisotropy and channel anisotropy separately. Additionally, by mapping 3D point clouds onto 1D space-filling curves, we enable parallelization of downsampling and neighborhood queries on GPUs with linear complexity. LinNet, as a purely point-based method, outperforms most previous methods in both indoor and outdoor scenes without any extra attention, and sparse convolution but merely relying on a simple MLP. It achieves the mIoU of $73.7\\%$ , $81.4\\%$ , and $69.1\\%$ on the S3DIS Area5, NuScenes, and SemanticKITTI validation benchmarks, respectively, while speeding up almost $10\\mathbf{x}$ times over PointNeXt. Our work further reveals both the efficacy and efficiency potential of the vanilla pointbased models in large-scale representation learning. Our code will be available at https://github.com/DengH293/LinNet. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Appealed by the ongoing evolution progress of technologies in robotics, autonomous driving, augmented reality, etc., LiDAR sensors are incrementally integrated into hardware-constrained devices such as mobile devices and AR headsets. This has led to a growing interest in efficient point cloud processing models. Given the limited computational power of mobile devices and embedded systems, the design of mobile-friendly point cloud representation learning algorithms should not only focus on performance but also pay attention to high computational efficiency. ", "page_idx": 0}, {"type": "text", "text": "Unlike images, point cloud data is irregular and unordered. There are various methods for processing point cloud data in 3D vision. Common approaches include multi-view methods [1, 2, 3] and voxel-based methods [4, 5]. Converting irregular data into the required formal representations often requires additional computation and memory and thus results in the loss of geometric information [6]. Therefore, point-based methods that directly operate on point clouds have emerged. PointNet [7] and PointNet+ $^+$ [8] are the pioneers of this approach, introducing a general point cloud learning paradigm from local to global. The paradigm consists of two parts: the first part is a spatial neighborhood search strategy, which utilizes algorithms such as furthest point sampling (FPS) and K-nearest neighbors (KNN) to implement sub-sampling and neighborhood grouping of point clouds, respectively. The second part is a trainable local feature extractor. Following them, subsequent extensive research [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19] have shown promising results by focusing on the design of more sophisticated extractors. ", "page_idx": 0}, {"type": "image", "img_path": "ehfCxpDsrw/tmp/058e03ebf5231726c025d48e0bf9fb1654e98bc5d374485f1695ca367aef2148.jpg", "img_caption": ["(a) Latency decomposition. "], "img_footnote": [], "page_idx": 1}, {"type": "image", "img_path": "ehfCxpDsrw/tmp/f0ed393bb14d6a1aada236ea38e7aa9532b604bb09c7cc6c5aad6204be37ce22.jpg", "img_caption": ["Figure 1: Latency decomposition. We show the inference run time decomposition. (a) SA: set abstraction; DSA: disassembled set abstraction; LS: linearization sampling; HQ: hash query; Oth.: others. (b) LinNet achieves the highest mIoU with extremely low latency compared to the comparative point-based approaches. The latency of each network is measured on a single Nvidia 3090 GPU, taking a batch of $80\\mathrm{k}$ points. ", "(b) Accuracy-speed tradeoff. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Despite the impressive results that have been achieved in object recognition and semantic segmentation, most of these methods are limited to applications in small-scale 3D point clouds. The main reason is discouraged by the high time complexity of the neighborhood search strategy that the point-based methods adopted. As shown in Fig. 1a. FPS and KNN occupy $46\\%$ of the runtime. This draws forth the main motivation of this paper: enhancing the scalability of point-based approaches in large-scale scenes, while maintaining their excellent performance in small-scale tasks. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we introduce a novel model, named Linear Net (LinNet). Our LinNet derives from inheriting the innovations and overcoming the drawbacks of the PointNet++ paradigm, including a disassembled set abstraction (DSA) module and an efficient point search strategy. Specifically, inspired by MobileNet [20], we first use two independent MLPs to separately learn depth-wise geometric features of the neighborhood and point-wise semantic features. Then, the geometric features are assigned as biases to the queried neighbors\u2019 semantic features, achieving spatially anisotropic neighborhood aggregation. Since the learning of high-dimensional semantic features is point-wise and does not involve the neighborhood, the required floating-point operations (FLOPs) are significantly lower than those needed for SA. Besides, a hash query and linearization sampling strategy are proposed for speeding up point searching. The core of our method is to map the 3D search space onto a segmented curve for acceleration. A sparse point cloud is ordered on that curve, and points adjacent to each other in the curve are also adjacent in space. For neighborhood queries, we store each segmented curve as a bucket in a hash table. When querying the neighborhood, we only need to search in the buckets corresponding to the neighboring curves, which drastically cuts down the search range. Linear sampling ensures uniform sampling by taking the point closest to the origin within each grid as the new sampling point. The method reduces the time complexity to be linear and supports GPU parallelism, resulting in very fast sampling. As shown in Fig. 1, the additional point cloud search operations take up less than $10\\%$ of the model\u2019s runtime. By employing these techniques, our method achieves efficient point cloud representation learning and scalability, providing significant performance improvements for large-scale point cloud analysis. ", "page_idx": 1}, {"type": "text", "text": "The contribution of our paper can be summarized in the following three folds: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We analyze the feature aggregation of the vanilla SA module and introduce a novel efficient and effective DSA module. This strategy effectively reduces computational overhead and achieves performance gains. Moreover, we discuss the superiority of this method from the perspective of weight initialization, emphasizing how these adjustments crucially enhance the overall performance of the network. ", "page_idx": 1}, {"type": "image", "img_path": "ehfCxpDsrw/tmp/435f8368c50c71b24e4f30106b9871ca5b48976206824bcf590a4347d709ece5.jpg", "img_caption": ["Figure 2: Overall architecture. (a) Overview of the framework. The whole network consists of an embedding layer and four stages, each containing a downsampling layer and $N_{i}$ disassembled SA blocks. (b) Structure of the DSA blocks. Each DSA block consists of a DSA module and extra MLPs. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "\u2022 To improve scalability in large-scale scenes, a linear complexity point cloud search strategy is introduced, mapping a 3D point cloud to a 1D space-fliling curve. This approach drastically reduces the time consumption associated with sub-sampling and neighbors query. ", "page_idx": 2}, {"type": "text", "text": "\u2022 Experiments show that our approach achieves state-of-the-art performance on widely adopted 3D large-scale semantic segmentation benchmarks (S3DIS, NuScenes) and competitive results on small-scale classification tasks (ScanObjectNN and ModelNet40). Extensive ablation studies have also validated the effectiveness of our proposed components. ", "page_idx": 2}, {"type": "text", "text": "2 Related Works ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Point cloud analysis. Point cloud analysis is primarily approached in two ways. Another approach, exemplified by the PointNet family, directly processes raw point clouds. They introduce a hierarchical feature learning paradigm to recursively capture local geometric structures. By adopting local point representation and multi-scale information, PointNet+ $^+$ has demonstrated excellent performance and has become a cornerstone of modern point cloud methods [9, 16, 21, 17, 22, 23]. Our LinNet follows the design philosophy of PointNet+ $^{-+}$ but explores a simpler yet deeper network architecture. ", "page_idx": 2}, {"type": "text", "text": "Voxel-based methods. Instead of learning directly on discrete points, the sparse convolution [5, 24] first converts the point cloud into a regular grid and then constructs a full convolutional neural network using the discrete sparse tensor. By building a hash table of discrete rasters, neighborhood query and sampling can be performed efficiently with a constant time complexity of $\\bar{\\mathcal O}(1)$ . In addition, the hash table construction and query can be implemented in parallel on CUDA, which significantly improves the computational efficiency. However, even though sparse convolution performs well in many large-scale point cloud tasks, it still faces challenges in capturing the fine-grained patterns of point clouds. This is due to the quantization artifacts that may be introduced during voxelization, resulting in the extracted features being limited by the voxel size [25]. ", "page_idx": 2}, {"type": "text", "text": "Efficient network in computer vision. In computer vision, an efficient network typically refers to a deep learning architecture designed to balance performance and operational efficiency, including aspects like latency, FLOPs, memory, and power consumption. MobileNet [20] use depthwise separable convolutions, making them particularly efficient for mobile and embedded devices. EfficientNet [26] systematically scales the network\u2019s width, depth, and resolution, achieving a balance between efficiency and accuracy. Many works in 3D vision [27, 18, 19, 25] are dedicated to optimizing the efficiency and performance of point cloud processing. While such networks often slightly compromise on performance for reduced computational load, our network, as detailed in Section 4, uniquely enhances both efficiency and accuracy. ", "page_idx": 2}, {"type": "text", "text": "3 Linear Net ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "3.1 Problem Formulation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Given a 3D point cloud $\\mathcal{V}\\,=\\,(\\mathcal{P},\\mathcal{F})$ consisting of $n$ points $\\textbf{\\em x}$ . For the $\\pmb{i}$ -th point $\\pmb{x}_{i}\\,=\\,(\\pmb{p}_{i},\\pmb{f}_{i})$ , $p_{i}\\in\\mathbb{R}^{3}$ and $f_{i}\\in\\mathbb{R}^{c}$ are the space coordinates and features, respectively. The task of point cloud semantic segmentation involves assigning a class label to each point $\\pmb{x}_{i}$ , while scene classification entails predicting a class label for the entire scene $\\mathcal{C}$ . The point-based methods usually employ several stages to classify the points or point clouds. In each stage, a downsample layer is first applied to sample the points, reducing the density of the point cloud. ", "page_idx": 3}, {"type": "text", "text": "3.2 Disassembled Set Abstraction ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we progressively introduce the DSA modules. Initially, we explore the direct application of separable convolutions in 3D vision. Subsequently, we adopt a balanced approach to separate channel and spatial anisotropy. Finally, we explain the superiority of this method from the perspective of weight initialization, highlighting how these adjustments enhance the overall performance of the network. ", "page_idx": 3}, {"type": "text", "text": "Revisiting Local Aggregation of Computer Vision. In a standard convolutional kernel, the anisotropy of the weights plays a critical role in capturing local information. This anisotropy can be classified into two categories: spatial and channel anisotropy. Spatial anisotropy refers to the variations among the features of neighboring points within the same feature channel, while channel anisotropy reflects the differences across various feature channels. To improve efficiency, MobileNet [20] achieves speedup by decomposing the standard convolutional kernel: it divides the convolutional kernel into point-wise convolution (PWConv) for channel anisotropy and depth-wise convolution (DWConv) for spatial anisotropy. Due to the sparsity of point clouds, it is impractical to apply DWConv to handle them directly. Instead of achieving anisotropy through parameters, point-based methods approach this by manipulating the input directly and adding anisotropy to the input data. Given a point cloud $\\pmb{x}_{i}$ , a typical local aggregation in 3D vision can be formulated as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\pmb{f}_{i}^{\\prime}=\\mathcal{R}_{j:(i,j)\\in\\mathcal{N}}\\{\\mathrm{PWConv}^{3+c\\mapsto c}(f_{j}||(\\pmb{p}_{j}-\\pmb{p}_{i}))\\},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathcal{R}$ is the aggregation function (usually max-pooling) that aggregates the local feature from the neighbors of anchor point $\\pmb{x}_{i}$ denoted as $\\{j:\\bar{(i,j)}\\in\\mathcal{N}\\}$ . $||$ is the concatenate operation in channels. PWConv $3\\!+\\!c\\!\\mapsto\\!c$ : $\\mathbb{R}^{3+c}\\mapsto\\mathbb{R}^{c}$ is an MLP that consists of pointwise convolution, a batch normalization layer, and a ReLU activation function. Here, the neighborhood features of the different anchors come from the same query set, so they are isotropic. The coordinates are anisotropic as they are relative to their respective anchor. Concatenating together the isotropic features and anisotropic relative coordinates gives the input anisotropy. ", "page_idx": 3}, {"type": "text", "text": "Depth-wise Separate Set Abstraction. Corresponding to the separate weights, we initially chose to separate the inputs directly as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\nf_{i}^{\\prime}=\\mathcal{R}_{j:(i,j)\\in\\mathcal{N}}\\{\\mathrm{PWConv}^{c\\mapsto c}(f_{j})\\}+\\mathcal{R}_{j:(i,j)\\in\\mathcal{N}}\\{\\mathrm{PWConv}^{3\\mapsto c}((p_{j}-p_{i}))\\}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Since all $f_{j}^{\\prime}$ come from the same set, Eq. (2) is identity to Eq. (3): ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\overline{{f}}_{i}=\\mathsf{P W C o n v}^{c\\mapsto c}(f_{i});}\\\\ &{f_{i}^{\\prime}=\\mathcal{R}_{j:(i,j)\\in\\mathcal{N}}\\{\\overline{{f}}_{j}\\}+\\mathcal{R}_{j:(i,j)\\in\\mathcal{N}}\\{\\mathsf{P W C o n v}^{3\\mapsto c}((p_{j}-p_{i}))\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "This separation is necessary for several reasons. First, since the features are derived from the same query set, there is no need to apply a shared-weight PWConv on the neighboring features [18]. If the feature dimension is $c$ and the number of neighbors is $k$ , the computational complexity would be $k c^{2}$ . After separation, this complexity is reduced to $c^{2}$ Second, the distribution patterns of coordinates and features differ significantly, and using independent convolutional kernels allows for better capture of these differences. However, as encountered with MobileNet, the speedup often comes at the cost of reduced accuracy. As illustrated in Fig. 3, DSSA accelerates the model\u2019s convergence during the initial few epochs. Yet, as training progresses, the convergence slows down, and the model\u2019s overall performance starts to degrade. The main reason lies in the lack of spatial-wise anisotropy between neighbor features. When features are processed independently of their spatial relationships, it can lead to a loss of contextual information critical for certain tasks, such as those involving complex spatial structures or detailed textural information. ", "page_idx": 3}, {"type": "image", "img_path": "ehfCxpDsrw/tmp/1d1187c9ff796ab7626659c05b19110bc1b28e0b3c90fb5bd705932d47f5a7c2.jpg", "img_caption": ["Figure 3: Training on S3DIS. "], "img_footnote": [], "page_idx": 3}, {"type": "image", "img_path": "ehfCxpDsrw/tmp/56311a1ff795cd3e1c748642e9e19744ce12c0c52417e877ebc6c9f1209c33e2.jpg", "img_caption": ["Figure 4: Comparison of various local aggregation. Each square represents the semantic feature, while each rectangle represents relative coordinates. The number of neighbors is 3. The blue line indicates a mapping in a high-dimensional space (e.g., from $3+c$ to $c$ , or $c$ to $c$ ), and the red line indicates a mapping from a lower dimension to a higher dimension (e.g., from 3 to $c$ ). More blue lines indicate more computation. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Disassembled set abstraction. Building on the principles outlined above, we propose a novel method for lightweight local aggregation that addresses the inherent challenges of separating coordinates and features. The proposed disassembled set abstraction (DSA) module can be formulated as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\overline{{f}}_{i}=\\mathrm{PWConv}^{c\\mapsto c}(f_{i});}\\\\ &{f_{i}^{\\prime}=\\mathrm{BN}\\{\\mathcal{R}_{j:(i,j)\\in\\mathcal{N}}\\{\\overline{{f}}_{j}+\\mathrm{PWConv}^{3\\mapsto c}((p_{j}-p_{i}))\\}\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where BN is a batch normalization layer [28]. The spatial anisotropy derived from relative positions is integrated into the neighborhood feature aggregation as a manner of bias. This integration ensures that the aggregation of neighborhood features is closely linked to the spatial distribution of the point cloud, thereby enhancing the model\u2019s robustness under varying spatial distributions. Interestingly, the Eq. (1) and Eq. (4) are actually mathematically equivalent during forward propagation. However, the DSA module exhibits faster convergence and lower loss compared to the SA modules (see Fig. 3). This phenomenon can be attributed to variations in the initialization of weights as follows. ", "page_idx": 4}, {"type": "text", "text": "Excluding bias, Eq. (1) uses a combined weight matrix W (dimensions $c\\!\\times\\!(c\\!+\\!3))$ to process semantic and geometric data simultaneously. For a given neighbor $j$ , the input vector $\\mathbf{x}_{j}=\\left[f_{j},p_{j}-p_{i}\\right]$ results in the output: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{y}_{j}=\\mathbf{W}\\mathbf{x}_{j}^{\\mathrm{{T}}}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "In this case, Kaiming initialization sets $\\mathbf{W}$ as a normal distribution $\\textstyle{\\mathcal{N}}(0,{\\sqrt{\\frac{2}{c+3}}})$ , potentially reducing the impact of geometric data due to its smaller proportional weight. In stark contrast, the DSA module separates the processing of semantic and geometric data using two distinct weight matrices, $\\mathbf{W}_{f}$ for semantic (dimensions $c\\times c$ ) and $\\mathbf{W}_{p}$ for geometric data (dimensions $c\\times3$ ), leading to: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{y}_{j}=\\mathbf{W}_{f}\\mathbf{f}_{j}^{\\mathrm{T}}+\\mathbf{W}_{p}(\\boldsymbol{p}_{j}-\\boldsymbol{p}_{i})^{\\mathrm{T}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The specific initialization $\\mathbf{W}_{f}\\sim{\\mathcal{N}}(0,\\sqrt{\\frac{2}{c}})$ and $\\mathbf{W}_{p}\\sim{\\mathcal{N}}(0,\\sqrt{\\frac{2}{3}})$ allows for a more balanced influence of geometric data, thus enhancing the network\u2019s ability to extract and utilize geometric information effectively. It is noteworthy that the PWConv between high-dimensional semantic features is applied directly to the point cloud features, rather than to the neighborhood. Additionally, the number of input channels for PWConv applied to the neighborhood is only 3, which is significantly smaller than $c$ (with a minimum of 64 in the segmentation model). As illustrated in Fig 4, DSA requires substantially fewer FLOPs compared to SA, thereby improving computational efficiency and making it more suitable for large-scale applications. ", "page_idx": 4}, {"type": "image", "img_path": "ehfCxpDsrw/tmp/333911a620ba98eb61df8e6104007f339ca7a609db8baff3281dbcd1fe8450a2.jpg", "img_caption": ["Figure 5: Efficient point clouds searching for query and sampling. (a) The input point cloud. (b) Point cloud after linearization by space-filling curves. Points connected by solid arrows are within the same grid, while dashed lines connect points between different grids. (c) Store each segment of the solid line as a hash table. (d) The point closest to the center of each region represented by segments connected by solid arrows is chosen as the new sampling point. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "3.3 Point Searching Strategy ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Recent literature [29, 30] employ space-filling curves to serialize point clouds, which are then uniformly divided into patches and fed into a transformer architecture. Inspired by this, we map points in 3D space onto a space-fliling curve for accelerating point searching. Denote the coordinates $\\pmb{p}$ as $\\left(x,y,z\\right)$ and the batch index as $b$ . The shuffled key is defined as a 64-bit integer: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathrm{Key}=(b\\ll54)\\,|\\,(\\lfloor z/s\\rfloor\\ll36)\\,|\\,(\\lfloor y/s\\rfloor\\ll18)\\,|\\,(\\lfloor x/s\\rfloor),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\ll$ denotes left bit-shift, $s$ denotes the grid size, and | denotes bitwise OR. It is worth noting that, unlike PTv3 [30], which contains only one point per grid cell, our approach allows multiple points to share the same key. We utilize shuffled keys to store these points in memory in an ordered manner, ensuring that elements sharing the same key are close to each other in memory. As shown in Fig. 5b, points on the same solid line are in the same grid. Through this, the point cloud $\\mathcal{V}$ is partitioned into $m$ sub-regions $[\\nu_{1},\\nu_{2},...\\nu_{m}]$ . ", "page_idx": 5}, {"type": "text", "text": "Hash query. By storing coordinates in spatial order, a hash table can be constructed to efficiently manage and query neighbors. Each bucket represents a non-empty grid. The key is the shuffled key of the grid, and the value contains two parts: the index of the first point in the grid and the count of points in that grid. If the number of grids is $m$ , the time complexity of building the hash table is $\\mathcal{O}(m)$ . During the query phase, for each point, we find the 27 (i.e., $3\\times3\\times3)$ neighboring grids and query the hash table with these keys. Finally, the top $k$ nearest points are selected. Assuming each point\u2019s 27-grid neighborhood contains $p$ points on average, identifying the closest $k$ points involves maintaining a heap with a complexity of ${\\mathcal{O}}(p\\log k)$ and a final sorting step costing ${\\mathcal{O}}(k\\log k)$ . Thus, the total computational complexity is $\\mathcal{O}(m+N(p\\log k+k\\log k))$ , while that of $k\\mathrm{NN}$ is $\\mathcal{O}(k N^{2})$ . ", "page_idx": 5}, {"type": "text", "text": "Linearization sampling. To achieve uniform and fast sampling, we select a point from each subset according to the following rule: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathrm{id}\\mathbf{x}_{i}=\\arg\\operatorname*{min}_{\\substack{j\\,:\\,(j)\\in\\mathcal{M}_{i}}}\\left(\\|p_{j}-\\lfloor p_{j}/s\\rfloor\\times s\\|_{2}\\right),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $(j)\\in\\mathcal{M}_{i}$ are the points of $i$ -th sub-regions. This rule ensures that the newly sampled points are the closest to the origin within their respective grids, guaranteeing uniform sampling. The method has a linear time complexity and supports GPU parallelism, resulting in very fast sampling speeds. ", "page_idx": 5}, {"type": "text", "text": "3.4 Network Architecture ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The overall architecture is illustrated in Fig. 2. For segmentation tasks, we use both encoders and decoders. To ensure a fair comparison, in the indoor dataset S3DIS, we configure the encoder depth as [4, 7, 4, 4], which is the same as PointNeXt. For the outdoor dataset, the encoder depth is set to [4, 4, 7, 4]. Specifically, the channel numbers for these stages are set to [C, 2C, 4C, 8C], with C being 64. For the classification task, only the encoder is used. Considering that the dataset for the classification task is small and PointNeXt is already capable of real-time response, we do not use the proposed linear search strategy, ensuring a fairer comparison between the DSA module and the SA module. ", "page_idx": 5}, {"type": "table", "img_path": "ehfCxpDsrw/tmp/15d3533c699ac92ada3cb3d6cf88a8da23734fde594dc39373aafefa4af477e7.jpg", "table_caption": ["Table 1: Indoor sem. seg. on S3DIS Area 5. "], "table_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "ehfCxpDsrw/tmp/cdb06e4f92bb60bda2dfdc95c1b0dc1e1f0a8adf6f0cfb9e002fb0bb70872da1.jpg", "table_caption": ["Table 2: Outdoor sem. seg. on NuScenes. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "To validate the effectiveness of LinNet, we conduct experiments in 3D semantic segmentation and 3D object classification tasks. We also conduct an extensive ablation study to analyze each component in LinNet. More details of the experiments can be found in the Appendix. ", "page_idx": 6}, {"type": "text", "text": "4.1 Semantic Segmentation ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Data and metric. S3DIS [49] (Stanford Large-Scale 3D Indoor Spaces) is a challenging benchmark that comprises 6 extensive indoor areas, 271 rooms, and 13 semantic categories, which represent different types of objects and room elements commonly found in indoor environments. Each point in the dataset is labeled with one of the 13 semantic categories, such as table, door, chair, column, and window, in addition to clutter. Following previous work [22], we subsample the grid before sending the point cloud to the network. The grid size and maximum number of points are set to $0.04\\mathrm{m}$ and 24000 respectively. During training, we crop the center point by the pre-set maximum number of points and discard the rest. During testing, the entire scene is processed. The experiment results are shown in Tab. 1. For evaluation metrics, we choose class-wise intersection over union (mIoU), mean of class-wise accuracy (mAcc), and overall point-wise accuracy (OA). Given that S3DIS is relatively small, we conduct further experiments on the NuScenes [50] dataset to validate the efficiency of our model. In this dataset, each point is annotated with one of the 16 semantic categories. The dataset encompasses 1,000 scenes collected in Boston and Singapore, reflecting diverse urban environments. We adhered to the official segmentation protocol, allocating 700 scenes for training, 150 for validation, and another 150 for testing, ensuring a balanced and comprehensive evaluation of our model\u2019s performance across varied scenes. ", "page_idx": 6}, {"type": "text", "text": "Performance. The results are shown in Tab. 1 and Tab. 2. Following Point Transformer v2 [11] and CondaFormer [35], we also employ the test time augmentation (TTA) strategy to achieve fairer comparisons, and results using the TTA strategy are labeled with $^{\\ddagger}$ . In indoor dataset S3DIS, our LinNet outperforms the SoTA point-based method PointNeXt [22] by $2.9\\%$ , $0.2\\%$ , $2.5\\%$ in terms of mIoU, OA, and mAcc, respectively. We also visualize the segment result in Fig. 6. In largescale dataset NuScenes, LinNet also outperforms all previous methods. It is worth noting that our approach utilizes a pure MLP network, employing solely point-wise convolutions. This highlights that sophisticated feature extractors, such as attention mechanisms and graph structures, are not essential for achieving robust segmentation capabilities. Moreover, our method is strictly based on point data, devoid of any sparse convolutions, which further underscores the scalability of point-based methods in managing large-scale point clouds effectively. ", "page_idx": 6}, {"type": "image", "img_path": "ehfCxpDsrw/tmp/b709b445ca12f5fdc418cc029c5df93a0870c70f4518ab7eb3d7a14ae09a4181.jpg", "img_caption": ["Figure 6: Comparative Visualization of Semantic Segmentation on S3DIS. "], "img_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "ehfCxpDsrw/tmp/75174b48f2c19d6c15b6fd924b25088543371b0ce7347fc7039b02a78ffd6854.jpg", "table_caption": ["Table 3: 3D object classification in ScanObjectNN and ModelNet40. Averaged results in three random runs using 1024 points as input without normals and without voting are reported. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "4.2 Object classification ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We chose the ScanObjectNN [53] and ModelNet40 [54] datasets to assess the classification capabilities of our model, and the results are shown in Fig. 3. We also report the parameters, FlOPs, and throughput. Following PointNeXt, the input channels of the models used in ScanobjectNN and ModelNet40 are 32 and 64 respectively. The model parameters are computed for $C=32$ . ", "page_idx": 7}, {"type": "text", "text": "ScanObjectNN. It contains approximately 15,000 real scanned objects divided into 15 categories with 2,902 instances, which presents substantial challenges due to occlusions and noise. We conduct experiments on PB_T50_RS, the most challenging and frequently used variant of ScanObjectNN. According to the report, the proposed LinNet significantly outperformed existing methods in Overall Accuracy (OA) and mean Accuracy (mAcc), using fewer model parameters and achieving faster processing speeds. LinNet achieved an OA of $88.6\\%$ and a mAcc of $87.3\\%$ on ScanObjectNN. Note that we employ the same training protocols and experimental conditions as the SOTA benchmark, PointNeXt. Nonetheless, we still achieve an OA improvement of $0.4\\%$ , while other PointNeXt style architectures (e.g., PointMetaBase [17]) using the same experimental setup only achieve an OA improvement of $0.1\\%$ . ", "page_idx": 7}, {"type": "text", "text": "ModelNet40. This dataset is a widely-used object classification dataset, comprises 12,311 3D computer graphics CAD models across 40 categories. Our results, as indicated, are highly competitive and exceed those of most previous methods. LinNet achieved an Overall Accuracy of $93.9\\%$ on ModelNet40, surpassing graph-based models like DGCNN [13], transformer-based models such as Point Transformer [9], and KPConv [33]. ", "page_idx": 7}, {"type": "text", "text": "4.3 Model Efficiency ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We evaluate the efficiency of our model at four different scales of points: 20k, 50k, 100k, and $200\\mathbf{k}$ . The down sampling rate is about 4. The experiments are performed on an RTX 3090. The models we compared include PointNeXt-XL [22] and Point Transformer v2 [11]. As shown in Fig. 7a, the latency of LinNet grows linearly with the scale of the point cloud. Point Transformer v2 employs grid pooling, a technique similar to our linearization sampling, both characterized by linear time complexity. However, thanks to the simplicity and efficiency of our disassembled set aggregation, our model exhibits only half the latency of Point Transformer v2. Notably, at the 200k level, our LinNet model operates 13 times faster than PointNeXt, demonstrating significant improvements in processing speed. We also explore the latency of the proposed point cloud search strategy. Note that the horizontal axis is on a logarithmic scale. As shown in Fig. 7b, linearization sampling and hash query based on space-filling curves leads to greater speedup as the point cloud size increases. The proposed linearization sampling can be up to a thousand times faster than FPS. ", "page_idx": 7}, {"type": "image", "img_path": "ehfCxpDsrw/tmp/512751f38fa1cc262bd8f007895f37d2e25c8df2b9e00c6a7c8c7272265787cc.jpg", "img_caption": ["(a) Efficiency comparison of different models. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "ehfCxpDsrw/tmp/140a52fcda87fcccdc7b1b59d4bd5e918ff52979136276f982009864269201e4.jpg", "img_caption": ["(b) Efficiency comparison of different components. "], "img_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "ehfCxpDsrw/tmp/24c8a0ad9cf823706b55a5ae12c5ed0ac9d1272c02bfd898c4e396cc8584141b.jpg", "table_caption": ["Figure 7: Efficiency comparisons. The horizontal axis represents the number of points in the input tensor and the vertical axis represents the running time in milliseconds. ", "Table 4: Model design ablation. ", "Table 5: Ablation on the DSA design. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "4.4 Ablation Study ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We conduct ablation experiments of the model to verify the validity of each component, and all experimental results are averaged over three times in the S3DIS area 5 unless otherwise stated. ", "page_idx": 8}, {"type": "text", "text": "LinNet. We perform ablation experiments on different modules introduced in LinNet: linearization sampling (LS), hash query (HQ), depth-wise separate set abstraction (DSSA), and disassembled set abstraction (DSA), with the results shown in Tab. 4. The delay was measured on a $20\\mathbf{k}$ number of point clouds. The model used in Exp. I is PointNeXt, which is the baseline result of our design. In Exp. II through VI, we progressively incorporated each of our proposed components, improving the baseline accuracy to $73.1\\%$ and reducing the latency to $34\\;\\mathrm{ms}$ . ", "page_idx": 8}, {"type": "text", "text": "Disassembled set abstraction. In Tab. 5, we investigate the design of the feature aggregation module to improve the aggregation of semantic and geometric information. We utilize ASSA [18] and PosPool [19], instead of DSA module. Additionally, we evaluate the performance impact of replacing max pooling with average pooling. Exp. (1) and (2) show a $1\\%$ decrease in accuracy when using simple separation of inputs directly. Comparing Exp. (1) with Exp. (6) indicates that the proposed DSA module performs better than the SA module. Exp. (3) and (4), which employ ASSA and PosPool respectively, demonstrate performance degradation, highlighting that our datadriven approach outperforms the parameter-free strategy in merging low-dimensional coordinates and high-dimensional semantics. Exp. (5) shows that max pooling is more compatible with our network. ", "page_idx": 8}, {"type": "text", "text": "Model scalability. We refer to the default LinNet as LinNet-Base and designed two variants with different numbers of trainable parameters: LinNet-Small, which has one-tenth the parameters of LinNet-Base, and LinNet-Large, which has four times the parameters of LinNet-Base. We test the performance of these models on the outdoor dataset NuScenes, and the results are summarized in Tab. 6. The mIoU on the validation set steadily improves with increasing model size. Additionally, since the models are linear, the increase in parameters does not result in significantly higher latency. Notably, without using TTA, our LinNet-Small achieves a validation accuracy of $77.6\\%$ with only 1.7M parameters, surpassing the 38M parameter sparse convolution method MinkUnet [4]. ", "page_idx": 8}, {"type": "table", "img_path": "ehfCxpDsrw/tmp/f49baa5666bfcdc2c769ea09c110715c55a37611493896ffee821d8545f953f5.jpg", "table_caption": ["Table 6: Model scalability. Latency and FLOPs are measured with $24\\mathtt{k}$ points. "], "table_footnote": [], "page_idx": 9}, {"type": "table", "img_path": "ehfCxpDsrw/tmp/48e42cca6153da5c0a1ec8c9b8cd83ffa93ace354e3dfb4a6f0baee03059d591.jpg", "table_caption": ["Table 7: Memory footprint during training and inference on the NuScenes dataset. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Memory footprint. We conduct experiments to evaluate memory usage during both training and inference phases on the NuScenes dataset, utilizing an RTX 4090 graphics card with all tests conducted at a batch size of 1. We include comparisons with the baseline model PointNeXt [22] and the sparse convolution method MinkUNet [4]. Our findings reveal that PointNeXt suffers from out-of-memory issues when handling large-scale scenes, highlighting scalability challenges. In contrast, our DSA module significantly reduces memory consumption by avoiding high-dimensional feature transformations on neighboring point clouds. Given that MinkUNet starts with 32 input channels, we conducted similar tests with our LinNet-Small model, which also has 32 initial feature channels, for a direct comparison. The results are shown in Tab. 7. Although LinNet-Small consumes more memory than MinkUNet, it is crucial to note that LinNet-Small, with only 1.7M parameters, achieves a validation accuracy of $77.6\\%$ , surpassing the 38M parameter sparse convolution method MinkUNet, which achieves $73.3\\%$ . ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Conclusion. In this paper, we implement a point-based approach with linear complexity. Unlike current point-based methods, our framework uses space-fliling curves to achieve neighbor query and downsampling with linear complexity. Additionally, we introduce a disassembled set aggregation module, which aggregates local features simply and elegantly, significantly reducing the redundant computations in neighborhoods and greatly enhancing scalability. Extensive experiments on multiple benchmarks demonstrate the efficiency and state-of-the-art performance of our method. ", "page_idx": 9}, {"type": "text", "text": "Limitation and Future Work. Although the proposed approach largely addresses the scalability challenges of point-based approaches in large-scale scenes, the distribution of point cloud data in memory in point-based approaches tends to be discontinuous, leading to inefficient memory access. This increases the cache miss rate, which in turn reduces the processing speed. Point-wise neighborhood aggregation also consumes a significant amount of memory. We hope that future work will address the significantly higher memory footprint than sparse convolution methods. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgment. The authors would like to thank the reviewers of NeurIPS\u201924 for their constructive suggestions. This work was supported by the Key Research and Development Program of Shaanxi Province of China under Grant 2024GX-YBXM-149, in part by the National Natural Science Foundation of China under Grant 42271140, and in part by Northwest University Graduate Innovation Project under Grant CX2024194. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Abdullah Hamdi, Silvio Giancola, and Bernard Ghanem. Mvtn: Multi-view transformation network for 3d shape recognition. In CVPR, 2021. 1, 8 [2] Hang Su, Subhransu Maji, Evangelos Kalogerakis, and Erik Learned-Miller. Multi-view convolutional neural networks for 3d shape recognition. In ICCV, 2015. 1 [3] Ankit Goyal, Hei Law, Bowei Liu, Alejandro Newell, and Jia Deng. Revisiting point cloud shape classification with a simple and effective baseline. In ICML, 2021. 1, 8 [4] Christopher Choy, JunYoung Gwak, and Silvio Savarese. 4d spatio-temporal convnets: Minkowski convolutional neural networks. In CVPR, 2019. 1, 7, 10, 15 [5] Daniel Maturana and Sebastian Scherer. Voxnet: A 3d convolutional neural network for real-time object recognition. In IROS, 2015. 1, 3 [6] Zhijian Liu, Haotian Tang, Yujun Lin, and Song Han. Point-voxel cnn for efficient 3d deep learning. In NeurIPS, 2019. 1 [7] Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas. Pointnet: Deep learning on point sets for 3d classification and segmentation. In CVPR, 2017. 1, 7, 8, 15 [8] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J Guibas. Pointnet++: Deep hierarchical feature learning on point sets in a metric space. NeurIPS, 2017. 2, 7, 15 [9] Hengshuang Zhao, Li Jiang, Jiaya Jia, Philip HS Torr, and Vladlen Koltun. Point transformer. In CVPR,   \n2021. 2, 3, 7, 8, 15 [10] Meng-Hao Guo, Jun-Xiong Cai, Zheng-Ning Liu, Tai-Jiang Mu, Ralph R Martin, and Shi-Min Hu. Pct: Point cloud transformer. CVM, 7:187\u2013199, 2021. 2, 8 [11] Xiaoyang Wu, Yixing Lao, Li Jiang, Xihui Liu, and Hengshuang Zhao. Point transformer v2: Grouped vector attention and partition-based pooling. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, NeurIPS, 2022. 2, 7, 8 [12] Lei Wang, Yuchun Huang, Yaolin Hou, Shenman Zhang, and Jie Shan. Graph attention convolution for point cloud semantic segmentation. In CVPR, 2019. 2 [13] Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E Sarma, Michael M Bronstein, and Justin M Solomon. Dynamic graph cnn for learning on point clouds. ACM TOG, 38(5):1\u201312, 2019. 2, 8, 15 [14] Yangyan Li, Rui Bu, Mingchao Sun, Wei Wu, Xinhan Di, and Baoquan Chen. Pointcnn: Convolution on x-transformed points. NeurIPS, 2018. 2, 8, 15 [15] Yongcheng Liu, Bin Fan, Shiming Xiang, and Chunhong Pan. Relation-shape convolutional neural network for point cloud analysis. In CVPR, 2019. 2 [16] Xin Deng, WenYu Zhang, Qing Ding, and XinMing Zhang. Pointvector: A vector representation in point cloud analysis. In CVPR, 2023. 2, 3, 7 [17] Haojia Lin, Xiawu Zheng, Lijiang Li, Fei Chao, Shanshan Wang, Yan Wang, Yonghong Tian, and Rongrong Ji. Meta architecure for point cloud analysis. arXiv:2211.14462, 2022. 2, 3, 7, 8 [18] Guocheng Qian, Hasan Hammoud, Guohao Li, Ali Thabet, and Bernard Ghanem. Assanet: An anisotropic separable set abstraction for efficient point cloud representation learning. NeurIPS, 2021. 2, 3, 4, 7, 8, 9, 15 [19] Ze Liu, Han Hu, Yue Cao, Zheng Zhang, and Xin Tong. A closer look at local aggregation operators in point cloud analysis. In ECCV, 2020. 2, 3, 9 [20] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional neural networks for mobile vision applications. arXiv preprint arXiv:1704.04861, 2017. 2, 3, 4 [21] Xin Lai, Jianhui Liu, Li Jiang, Liwei Wang, Hengshuang Zhao, Shu Liu, Xiaojuan Qi, and Jiaya Jia. Stratified transformer for 3d point cloud segmentation. In CVPR, 2022. 3, 7 [22] Guocheng Qian, Yuchen Li, Houwen Peng, Jinjie Mai, Hasan Hammoud, Mohamed Elhoseiny, and Bernard Ghanem. Pointnext: Revisiting pointnet++ with improved training and scaling strategies. In NeurIPS,   \n2022. 3, 7, 8, 10, 15   \n[23] Xu Ma, Can Qin, Haoxuan You, Haoxi Ran, and Yun Fu. Rethinking network design and local geometry in point cloud: A simple residual mlp framework. ICLR, 2022. 3, 8   \n[24] Benjamin Graham, Martin Engelcke, and Laurens van der Maaten. 3d semantic segmentation with submanifold sparse convolutional networks. In CVPR, 2018. 3   \n[25] Chunghyun Park, Yoonwoo Jeong, Minsu Cho, and Jaesik Park. Fast point transformer. In CVPR, 2022. 3, 7   \n[26] Mingxing Tan and Quoc Le. Efficientnet: Rethinking model scaling for convolutional neural networks. In ICML, 2019. 3   \n[27] Qingyong Hu, Bo Yang, Linhai Xie, Stefano Rosa, Yulan Guo, Zhihua Wang, Niki Trigoni, and Andrew Markham. Learning semantic segmentation of large-scale point clouds with random sampling. PAMI, 44(11):8338\u20138354, 2021. 3, 7   \n[28] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In ICML, 2015. 5   \n[29] Peng-Shuai Wang. Octformer: Octree-based transformers for 3D point clouds. ACM Transactions on Graphics (SIGGRAPH), 42(4), 2023. 6   \n[30] Xiaoyang Wu, Li Jiang, Peng-Shuai Wang, Zhijian Liu, Xihui Liu, Yu Qiao, Wanli Ouyang, Tong He, and Hengshuang Zhao. Point transformer v3: Simpler, faster, stronger. In CVPR, 2024. 6, 7   \n[31] Wenxuan Wu, Zhongang Qi, and Li Fuxin. Pointconv: Deep convolutional networks on 3d point clouds. In CVPR, 2019. 7   \n[32] Hengshuang Zhao, Li Jiang, Chi-Wing Fu, and Jiaya Jia. Pointweb: Enhancing local neighborhood features for point cloud processing. In CVPR, 2019. 7   \n[33] Hugues Thomas, Charles R Qi, Jean-Emmanuel Deschaud, Beatriz Marcotegui, Fran\u00e7ois Goulette, and Leonidas J Guibas. Kpconv: Flexible and deformable convolution for point clouds. In ICCV, 2019. 7, 8, 15   \n[34] Liyao Tang, Yibing Zhan, Zhe Chen, Baosheng Yu, and Dacheng Tao. Contrastive boundary learning for point cloud segmentation. In CVPR, 2022. 7   \n[35] Lunhao Duan, Shanshan Zhao, Nan Xue, Mingming Gong, Gui-Song Xia, and Dacheng Tao. Condaformer: Disassembled transformer with local structure enhancement for 3d point cloud understanding. In NeurIPS, 2023. 7   \n[36] Andres Milioto, Ignacio Vizzo, Jens Behley, and Cyrill Stachniss. Rangenet $^{++}$ : Fast and accurate lidar semantic segmentation. In IROS, 2019. 7, 15   \n[37] Tiago Cortinhal, George Tzelepis, and Eren Erdal Aksoy. Salsanext: Fast, uncertainty-aware semantic segmentation of lidar point clouds. In ISVC, 2020. 7, 15   \n[38] Yang Zhang, Zixiang Zhou, Philip David, Xiangyu Yue, Zerong Xi, Boqing Gong, and Hassan Foroosh. Polarnet: An improved grid representation for online lidar point clouds semantic segmentation. In CVPR, 2020. 7, 15   \n[39] Yuenan Hou, Xinge Zhu, Yuexin Ma, Chen Change Loy, and Yikang Li. Point-to-voxel knowledge distillation for lidar semantic segmentation. In CVPR, 2022. 7, 15   \n[40] Venice Erin Liong, Thi Ngoc Tho Nguyen, Sergi Widjaja, Dhananjai Sharma, and Zhuang Jie Chong. Amvnet: Assertion-based multi-view fusion network for lidar semantic segmentation, 2020. 7, 15   \n[41] Xinge Zhu, Hui Zhou, Tai Wang, Fangzhou Hong, Yuexin Ma, Wei Li, Hongsheng Li, and Dahua Lin. Cylindrical and asymmetrical 3d convolution networks for lidar segmentation. In CVPR, 2021. 7, 15   \n[42] Haotian Tang, Zhijian Liu, Shengyu Zhao, Yujun Lin, Ji Lin, Hanrui Wang, and Song Han. Searching efficient 3d architectures with sparse point-voxel convolution. In ECCV, 2020. 7, 15   \n[43] Jianyun Xu, Ruixiang Zhang, Jian Dou, Yushi Zhu, Jie Sun, and Shiliang Pu. Rpvnet: A deep and efficient range-point-voxel fusion network for lidar point cloud segmentation. In CVPR, 2021. 7, 15   \n[44] Xu Yan, Jiantao Gao, Chaoda Zheng, Chao Zheng, Ruimao Zhang, Shuguang Cui, and Zhen Li. 2dpass: 2d priors assisted semantic segmentation on lidar point clouds. In ECCV, 2022. 7, 15   \n[45] Lingdong Kong, Youquan Liu, Runnan Chen, Yuexin Ma, Xinge Zhu, Yikang Li, Yuenan Hou, Yu Qiao, and Ziwei Liu. Rethinking range view representation for lidar segmentation. In CVPR, 2023. 7, 15   \n[46] Xin Lai, Yukang Chen, Fanbin Lu, Jianhui Liu, and Jiaya Jia. Spherical transformer for lidar-based 3d recognition. In CVPR, 2023. 7, 15   \n[47] Gilles Puy, Alexandre Boulch, and Renaud Marlet. Using a waffle iron for automotive point cloud semantic segmentation. In CVPR, 2023. 7, 15   \n[48] Bohao Peng, Xiaoyang Wu, Li Jiang, Yukang Chen, Hengshuang Zhao, Zhuotao Tian, and Jiaya Jia. Oa-cnns: Omni-adaptive sparse cnns for 3d semantic segmentation. In CVPR, 2024. 7   \n[49] Iro Armeni, Ozan Sener, Amir R Zamir, Helen Jiang, Ioannis Brilakis, Martin Fischer, and Silvio Savarese. 3d semantic parsing of large-scale indoor spaces. In CVPR, 2016. 7, 14   \n[50] Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. nuscenes: A multimodal dataset for autonomous driving. In CVPR, 2020. 7, 14   \n[51] Guohao Li, Matthias Muller, Ali Thabet, and Bernard Ghanem. Deepgcns: Can gcns go as deep as cnns? In CVPR, 2019. 8, 15   \n[52] Tiange Xiang, Chaoyi Zhang, Yang Song, Jianhui Yu, and Weidong Cai. Walk in the cloud: Learning curves for point clouds shape analysis. In CVPR, 2021. 8   \n[53] Mikaela Angelina Uy, Quang-Hieu Pham, Binh-Son Hua, Thanh Nguyen, and Sai-Kit Yeung. Revisiting point cloud classification: A new benchmark dataset and classification model on real-world data. In ICCV, 2019. 8, 14   \n[54] Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Linguang Zhang, Xiaoou Tang, and Jianxiong Xiao. 3d shapenets: A deep representation for volumetric shapes. In CVPR, 2015. 8, 14   \n[55] Jens Behley, Martin Garbade, Andres Milioto, Jan Quenzel, Sven Behnke, Cyrill Stachniss, and Jurgen Gall. Semantickitti: A dataset for semantic scene understanding of lidar sequences. In CVPR, 2019. 14   \n[56] Li Yi, Vladimir G Kim, Duygu Ceylan, I-Chao Shen, Mengyan Yan, Hao Su, Cewu Lu, Qixing Huang, Alla Sheffer, and Leonidas Guibas. A scalable active framework for region annotation in 3d shape collections. ToG, 35(6):1\u201312, 2016.   \n[57] Haoxi Ran, Jun Liu, and Chengjie Wang. Surface representation for point clouds. In CVPR, 2022. 15   \n[58] Yifan Xu, Tianqi Fan, Mingye Xu, Long Zeng, and Yu Qiao. Spidercnn: Deep learning on point sets with parameterized convolutional filters. In ECCV, 2018. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In the appendix, we provide more experiment details in Sec. A, and more experiment results in Sec.   \nB. ", "page_idx": 13}, {"type": "text", "text": "A Experimental Details ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "This section provides a detailed description of the experimental setup for each dataset. ", "page_idx": 13}, {"type": "text", "text": "Experimental environment. ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "\u2022 CUDA version: 11.3 \u2022 PyTorch version: 1.12.1 \u2022 GPU: Nvidia RTX $4090\\mathrm{D}\\times4$ \u2022 CPU: AMD EPYC 9754 128-Core ", "page_idx": 13}, {"type": "text", "text": "Training details. The specific model training settings are shown in Tab. 8 and Tab. 9. We used cross-entropy loss in all experiments. ", "page_idx": 13}, {"type": "table", "img_path": "ehfCxpDsrw/tmp/554c76486120800e87728eae83d0ab78f3c8e224916ddb686507d0df6bbf4904.jpg", "table_caption": ["Table 8: Data augmentation. "], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "Data license. Our experiments use open-source datasets widely applied for 3D recognition research. The ScanObjectNN [53], SemanticKITTI [55], dataset is under the MIT license, while S3DIS [49], NuScenes [50], and ModelNet40 [54] have custom licenses that only allow academic use. ", "page_idx": 13}, {"type": "text", "text": "B Additional Quantitative Results ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this section, we present additional quantitative results of SemanticKITT [55] for 3D semantic segmentation. In addition, we provide semantic segmentation results for each category of NuScenes (see Tab. 10) and S3DIS Area 5 (see Tab. 11). ", "page_idx": 13}, {"type": "text", "text": "SemanticKITTI. The SemanticKITTI dataset consists of sequences from the original KITTI dataset, comprising a total of 22 sequences. Each sequence contains approximately 1,000 LiDAR scans, amounting to around 20,000 individual frames. The result is shown in Tab. 12. The mIoU of validation set and test set are $69.1\\%$ and $70.4\\%$ respectively. ", "page_idx": 13}, {"type": "text", "text": "S3DIS 6-fold cross-validation. To evaluate the generalization capabilities, we perform 6-fold crossvalidation on the S3DIS dataset to ensure a robust assessment of our model\u2019s performance across different subsets of data. The results are shown in Fig. 13. ", "page_idx": 13}, {"type": "text", "text": "Nomalization layer type. We conducted ablation studies on the S3DIS dataset to further assess the necessity and effectiveness of BN. As shown in Tab. 14, models with BN outperform those with Layer Normalization (LN) and without any normalization, indicating that BN is particularly effective for our specific architecture. ", "page_idx": 13}, {"type": "table", "img_path": "ehfCxpDsrw/tmp/331000b646dd6b5cef3ccc7f7bc6202cfe208fe97ffe23637288038bf67b6260.jpg", "table_caption": ["Table 10: Semantic segmentation results on NuScenes val set. \u2021 denotes using rotation and translation testing-time augmentations. "], "table_footnote": [], "page_idx": 14}, {"type": "table", "img_path": "ehfCxpDsrw/tmp/92714c33f5ad9d027cc0ef8266ecb9442e4d7fd06050e30ee5319e546b76af34.jpg", "table_caption": ["Table 11: Semantic segmentation results on S3DIS Area 5. "], "table_footnote": [], "page_idx": 14}, {"type": "table", "img_path": "ehfCxpDsrw/tmp/0ba327020e377cf9616625bf20922475be9564506abfb679a3e1911c55ea0e5d.jpg", "table_caption": ["Table 12: Sem. seg. on Sem. KITTI. "], "table_footnote": [], "page_idx": 14}, {"type": "table", "img_path": "ehfCxpDsrw/tmp/33686e2b94a8b19d6d1819e23602ee82c8c74976f54642364bf729476712e27a.jpg", "table_caption": ["Table 13: S3DIS 6-fold cross-validation. "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Claims ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 15}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 15}, {"type": "text", "text": "Justification: [TODO] ", "page_idx": 15}, {"type": "text", "text": "Guidelines: ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 15}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 15}, {"type": "text", "text": "Justification: [TODO] ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Guidelines: ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 15}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 15}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 15}, {"type": "text", "text": "Justification: [TODO] ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 16}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: [TODO] ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 16}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 16}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 17}, {"type": "text", "text": "Justification: Our code will be available upon publication. Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 17}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: [TODO] ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 17}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: [TODO] ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: [TODO] ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 18}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 18}, {"type": "text", "text": "Answer:[Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: [TODO] ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 18}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: [TODO] ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake proflies, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate ", "page_idx": 18}, {"type": "text", "text": "deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. ", "page_idx": 19}, {"type": "text", "text": "\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 19}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 19}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 19}, {"type": "text", "text": "Justification: The paper poses no such risks ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 19}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: [TODO] ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 19}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: [TODO] Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 20}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: [TODO] ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 20}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: [TODO] ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 20}]