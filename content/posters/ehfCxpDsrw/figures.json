[{"figure_path": "ehfCxpDsrw/figures/figures_1_1.jpg", "caption": "Figure 1: Latency decomposition. We show the inference run time decomposition. (a) SA: set abstraction; DSA: disassembled set abstraction; LS: linearization sampling; HQ: hash query; Oth.: others. (b) LinNet achieves the highest mIoU with extremely low latency compared to the comparative point-based approaches. The latency of each network is measured on a single Nvidia 3090 GPU, taking a batch of 80k points.", "description": "This figure compares the latency of LinNet with other point-based methods (PointNext, Point Transformer V2, PointVector, Stratified Transformer) for point cloud semantic segmentation on S3DIS Area5.  It shows a breakdown of the inference time for each method, highlighting the components contributing to latency (Set Abstraction (SA), KNN search, etc).  It demonstrates LinNet's significantly improved speed while achieving higher mIoU (mean Intersection over Union).", "section": "1 Introduction"}, {"figure_path": "ehfCxpDsrw/figures/figures_1_2.jpg", "caption": "Figure 1: Latency decomposition. We show the inference run time decomposition. (a) SA: set abstraction; DSA: disassembled set abstraction; LS: linearization sampling; HQ: hash query; Oth.: others. (b) LinNet achieves the highest mIoU with extremely low latency compared to the comparative point-based approaches. The latency of each network is measured on a single Nvidia 3090 GPU, taking a batch of 80k points.", "description": "This figure demonstrates the efficiency of LinNet.  Subfigure (a) breaks down the inference time of LinNet and other methods into components such as set abstraction (SA), the novel disassembled set abstraction (DSA), linearization sampling (LS), hash query (HQ), and other operations.  Subfigure (b) shows a comparison of LinNet's performance against other point-based methods in terms of accuracy (mIoU) and speed (latency). LinNet achieves the highest accuracy with significantly lower latency.", "section": "Abstract"}, {"figure_path": "ehfCxpDsrw/figures/figures_2_1.jpg", "caption": "Figure 2: Overall architecture. (a) Overview of the framework. The whole network consists of an embedding layer and four stages, each containing a downsampling layer and Ni disassembled SA blocks. (b) Structure of the DSA blocks. Each DSA block consists of a DSA module and extra MLPs.", "description": "This figure illustrates the overall architecture of LinNet, a linear network for efficient point cloud representation learning.  (a) shows a high-level overview of the network's structure, highlighting the embedding layer, four stages of downsampling and DSA (disassembled set abstraction) blocks, and the sequential nature of the processing. Each stage involves downsampling the point cloud to reduce computational load and increase efficiency. (b) provides a detailed view of a single DSA block, outlining its internal components: a DSA module and multiple MLPs (multilayer perceptrons). The DSA module is a key innovation in LinNet, improving local feature aggregation efficiency.", "section": "3 Linear Net"}, {"figure_path": "ehfCxpDsrw/figures/figures_3_1.jpg", "caption": "Figure 3: Training on S3DIS.", "description": "This figure shows the training loss curves for three different set abstraction modules: SA (standard set abstraction), DSSA (depth-wise separate set abstraction), and DSA (disassembled set abstraction).  The x-axis represents the training epochs, and the y-axis represents the training loss.  The plot shows that DSA achieves a lower training loss compared to SA and DSSA.  A zoomed-in section of the plot highlights the early training stages, further emphasizing the faster convergence of DSA.", "section": "3.2 Disassembled Set Abstraction"}, {"figure_path": "ehfCxpDsrw/figures/figures_4_1.jpg", "caption": "Figure 4: Comparison of various local aggregation. Each square represents the semantic feature, while each rectangle represents relative coordinates. The number of neighbors is 3. The blue line indicates a mapping in a high-dimensional space (e.g., from 3 + c to c, or c to c), and the red line indicates a mapping from a lower dimension to a higher dimension (e.g., from 3 to c). More blue lines indicate more computation.", "description": "This figure compares three different local aggregation methods: SA (standard set abstraction), DSSA (depth-wise separated set abstraction), and DSA (disassembled set abstraction).  It illustrates how each method processes semantic features and relative coordinates of neighboring points to aggregate local features.  The number of neighbors considered is 3 in all cases.  Blue lines represent high-dimensional mappings (more computation), and red lines represent low-dimensional mappings (less computation). DSA is shown to be more efficient with fewer high-dimensional mappings than the other two methods.", "section": "3.2 Disassembled Set Abstraction"}, {"figure_path": "ehfCxpDsrw/figures/figures_5_1.jpg", "caption": "Figure 5: Efficient point clouds searching for query and sampling. (a) The input point cloud. (b) Point cloud after linearization by space-filling curves. Points connected by solid arrows are within the same grid, while dashed lines connect points between different grids. (c) Store each segment of the solid line as a hash table. (d) The point closest to the center of each region represented by segments connected by solid arrows is chosen as the new sampling point.", "description": "This figure illustrates the process of efficient point cloud searching and sampling using space-filling curves and a hash table. (a) shows the input point cloud. (b) shows how the points are linearized using a space-filling curve, grouping points in the same grid with solid arrows and points in neighboring grids with dashed lines. (c) shows the hash table created by storing each segment of the curve as a bucket, enabling efficient neighborhood queries. (d) shows the linearization sampling strategy, where the point closest to the center of each grid is selected as a new sampling point, ensuring uniform sampling and reducing computation.", "section": "3.3 Point Searching Strategy"}, {"figure_path": "ehfCxpDsrw/figures/figures_7_1.jpg", "caption": "Figure 6: Comparative Visualization of Semantic Segmentation on S3DIS.", "description": "This figure shows a qualitative comparison of semantic segmentation results on the S3DIS dataset.  For several example scenes, the input point cloud, ground truth segmentation, and the segmentation generated by LinNet are shown side-by-side. This allows for a visual comparison of the accuracy of LinNet's predictions against the ground truth labels for various indoor scene types, illustrating the model's performance on different types of objects and scene layouts.", "section": "4.1 Semantic Segmentation"}, {"figure_path": "ehfCxpDsrw/figures/figures_8_1.jpg", "caption": "Figure 7: Efficiency comparisons. The horizontal axis represents the number of points in the input tensor and the vertical axis represents the running time in milliseconds.", "description": "This figure compares the efficiency of LinNet with PointNeXt and Point Transformer V2.  The x-axis shows the number of points in the input point cloud (20k, 50k, 100k, 200k), and the y-axis represents the inference time in milliseconds.  It demonstrates that LinNet's inference time scales linearly with the number of input points, significantly outperforming the other two methods, especially as the point cloud size increases. This highlights LinNet's efficiency and scalability in handling large-scale point clouds.", "section": "4.3 Model Efficiency"}, {"figure_path": "ehfCxpDsrw/figures/figures_8_2.jpg", "caption": "Figure 7: Efficiency comparisons. The horizontal axis represents the number of points in the input tensor and the vertical axis represents the running time in milliseconds.", "description": "This figure compares the efficiency of different point cloud search strategies: KNN, Hash Query, Linearization Sampling, and FPS.  The x-axis shows the number of points in the input point cloud, ranging from 20k to 200k. The y-axis represents the time taken in milliseconds for each method to complete its search.  The graph clearly demonstrates the superior efficiency of Linearization Sampling, showing significantly lower processing times compared to the other methods, particularly as the number of points increases.  This highlights the advantage of the Linearization Sampling method proposed by the authors in enhancing the scalability and speed of their LinNet model.", "section": "4.4 Ablation Study"}]