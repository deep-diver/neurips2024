[{"type": "text", "text": "Learning Successor Features the Simple Way ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Raymond Chua\u21e4\u00a7 Arna Ghosh \u00a7 ", "page_idx": 0}, {"type": "text", "text": "Christos Kaplanis \u00b6 Blake A. Richards \u2020 \u2021 \u00a7 Doina Precup \u2021 \u00a7 \u00b6 ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In Deep Reinforcement Learning (RL), it is a challenge to learn representations that do not exhibit catastrophic forgetting or interference in non-stationary environments. Successor Features (SFs) offer a potential solution to this challenge. However, canonical techniques for learning SFs from pixel-level observations often lead to representation collapse, wherein representations degenerate and fail to capture meaningful variations in the data. More recent methods for learning SFs can avoid representation collapse, but they often involve complex losses and multiple learning phases, reducing their efficiency. We introduce a novel, simple method for learning SFs directly from pixels. Our approach uses a combination of a Temporaldifference (TD) loss and a reward prediction loss, which together capture the basic mathematical definition of SFs. We show that our approach matches or outperforms existing SF learning techniques in both 2D (Minigrid), 3D (Miniworld) mazes and Mujoco, for both single and continual learning scenarios. As well, our technique is efficient, and can reach higher levels of performance in less time than other approaches. Our work provides a new, streamlined technique for learning SFs directly from pixel observations, with no pretraining required1. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Deep reinforcement learning (RL) [Sutton and Barto, 2018] is important to modern artificial intelligence (AI), but standard approaches to deep RL can struggle when deployed for continual learning [Parisi et al., 2019, Hadsell et al., 2020, Khetarpal et al., 2022]. When either the reward function or the transition dynamics of the environment changes, standard deep RL techniques, such as deep Q-learning, will either struggle to adapt to the changes or they will exhibit catastrophic forgetting [Kirkpatrick et al., 2017, Kaplanis et al., 2018]. Given that the real-world is often non-stationary, better techniques for deep RL in continual learning are a major goal in AI research [Rusu et al., 2016, Rolnick et al., 2019, Powers et al., 2022, Abbas et al., 2023, Anand and Precup, 2024]. ", "page_idx": 0}, {"type": "text", "text": "One potential solution that researchers have explored is the use of Successor Features (SFs). Successor Features, the function approximation variant of Successor Representations (SRs) [Barreto et al., 2017], decompose the value function into a separate reward function and transition dynamics representation [Dayan, 1993]. In doing so, they make it easier to adapt to changes in the environment, because the network can relearn either the reward function or the transition dynamics separately [Borsa et al., 2018, Barreto et al., 2018, 2020, Hansen et al., 2019, Lehnert and Littman, 2019, Liu and Abbeel, 2021]. Furthermore, there are theoretical guarantees that SFs can improve generalization in multi-task ", "page_idx": 0}, {"type": "image", "img_path": "rI7oZj1WMc/tmp/ab69d62e8a7cc141f016727267ae8fa6f8f72ba81d29fb84c125eed7db3526fd.jpg", "img_caption": [], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Figure 1: (a) Results from a single task within a 2D two-room environment, illustrating the suboptimal performance of the canonical Successor Features (SF) learning rule (Eq. 4) due to representation collapse. (b) In the canonical SF approach, the average cosine similarity between pairs of SFs converges towards a value of 1, demonstrating representation collapse occurs. (c) The canonical SF learning rule does not develop distinct clusters in its representations, as evidenced by lower silhouette scores and higher Davies-Bouldin scores, which again indicates representation collapse. A mathematical proof can be found in section 3.4. ", "page_idx": 1}, {"type": "text", "text": "settings [Barreto et al., 2017]. SFs are therefore a promising candidate for deep RL in non-stationary settings. ", "page_idx": 1}, {"type": "text", "text": "However, learning SFs is non-trivial. The most straightforward solution, which is to use a temporaldifference (TD) error on subsequent observations [Barreto et al., 2018], can lead to representational collapse, where the artificial neural network maps all inputs to the same point in a high-dimensional representation space. This phenomenon is commonly observed in various deep learning pipelines that end up learning similar or identical latent representations for very different inputs [Aghajanyan et al., 2020]. In RL, representation collapse can lead to different states or state-action pairs being mapped to similar representations, leading to suboptimal policy decisions or inaccurate estimation of values. ", "page_idx": 1}, {"type": "text", "text": "To solve this problem, a variety of solutions have been proposed. One solution is to use an additional reconstruction loss [Kulkarni et al., 2016, Zhang et al., 2017, Machado et al., 2020] to force the network to maintain information about the inputs in its representations. Another solution is to use extensive pretraining coupled with additional loss terms to encourage high-entropy representations [Hansen et al., 2019, Liu and Abbeel, 2021]. More recently, an alternative solution using loss terms to promote orthogonal representations has been put forward [Mahadevan and Maggioni, 2007, Machado et al., 2017b]. Finally, an unconventional approach integrates Q-learning and reward prediction losses with the SF-TD loss, enhancing the learning process by providing additional supervisory signals that improve the robustness and effectiveness of the successor features [Janz et al., 2019]. This method allows the network to simultaneously learn the basis features, successor features, and task encoding vector, with the hope that the learned variables will satisfy their respective constraints. ", "page_idx": 1}, {"type": "text", "text": "Though these solutions prevent representational collapse, they can impair learning, introduce additional training phases, or add expensive covariance calculations to the loss function [Touati et al., 2022]. Ideally, there would be a way to learn deep SFs directly during task engagement with a simple, easy to calculate loss function. ", "page_idx": 1}, {"type": "text", "text": "Here, we introduce a simple technique for learning SFs directly during task engagement. We designed a neural network architecture specifically to achieve this training objective. Our approach leverages the mathematical definition of SFs and constructs a loss function with two terms: one that learns the value function with a TD-error, and another that enforces representations that make the rewards linearly predictable. By mathematical definition, this loss is minimized when the system has learned a set of SFs. We show that training with this loss during task engagement, facilitated by our neural network architecture, leads to the learning of deep SFs as well as, or better than, other approaches. It does so with no pretraining required and very minimal computational overhead. As well, we show that our technique improves continual reinforcement learning in dynamic environments, in both 2D grid worlds and 3D mazes. Altogether, our simple deep SF learning approach is an effective way to achieve the benefits of deep SFs without any of the drawbacks. ", "page_idx": 1}, {"type": "text", "text": "2 Related work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Our work builds on an extensive literature on decomposing the value function dating back to the 1990s [Dayan, 1993]. More recent work on learning deep SFs falls broadly into three categories of solutions. The first are solutions that use a reconstruction term in the loss function in order to avoid representational collapse [Kulkarni et al., 2016, Zhang et al., 2017, Machado et al., 2020]. This general approach is effective at avoiding collapse, but it can lead to impaired performance on the actual RL task, as we show below. The next set of solutions rely on hand-crafted features [Lehnert et al., 2017, Barreto et al., 2018, Borsa et al., 2018, Madarasz and Behrens, 2019, Machado et al., 2021, Emukpere et al., 2021, Nemecek and Parr, 2021, Brantley et al., 2021, McLeod et al., 2022, Alegre et al., 2022, Reinke and Alameda-Pineda, 2021] or hand-crafted task knowledge [Hansen et al., 2019, Filos et al., 2021, Liu and Abbeel, 2021, Carvalho et al., 2023a]. In these cases, the networks can learn and generalize well, but hand-crafted solutions cannot scale-up to real-world applications. Another category of solutions uses pretraining of the features in the deep neural network before any engagement with the actual RL task [Fujimoto et al., 2021, Abdolshah et al., 2021, Touati et al., 2022, Carvalho et al., 2023b]. Such solutions are not as applicable for continual RL because they introduce the need to engage in new pretraining when the environment changes, which assumes some form of oracle knowledge of the environment. Finally, there are solutions that rely on additional loss terms to encourage orthogonal representations, since SRs are built off of purely orthogonal tabular inputs [Touati et al., 2022, Farebrother et al., 2023]. These techniques can improve SF learning, but they require computationally expensive calculations of orthogonality in the basis features. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Among these prior approaches, the work most closely related to ours is the application of multiple losses to jointly learn the SFs, a task-encoding vector, and Q-values [Ma et al., 2020]. However, there are several key differences: (1) Our approach does not require the agent to be provided with a goal\u2014it is learned through interaction with the environment; (2) We provide direct evidence that our method works with pixel inputs; (3) We demonstrate that our approach eliminates the need for an SF loss; and (4) By removing the SF loss, we reduce the number of hyperparameters required, thereby simplifying the model. ", "page_idx": 2}, {"type": "text", "text": "In our results below, we compare our method to these classes of solutions described, namely reconstruction solutions [Machado et al., 2020], pretraining solutions [Liu and Abbeel, 2021], and orthogonality solutions [Touati et al., 2022]. ", "page_idx": 2}, {"type": "text", "text": "3 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Reinforcement Learning ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The RL setting is formalized as a Markov Decision Process defined by a tuple $(S,A,p,r,\\gamma)$ , where $\\boldsymbol{S}$ is the set of states, $\\boldsymbol{\\mathcal{A}}$ is the set of actions, $r:S\\to\\mathbb{R}$ is the reward function, $p:S\\times A\\to[0,1]$ is the transition probability function and $\\gamma\\in[0,1)$ is the discount factor which is being to used to balance the importance of immediate and future rewards [Sutton and Barto, 2018]. ", "page_idx": 2}, {"type": "text", "text": "At each time step $t$ , the agent observes state $S_{t}\\in S$ and takes an action $A_{t}\\in\\mathcal A$ sampled from a policy $\\pi:S\\times A\\rightarrow[\\bar{0},1]$ , resulting in to a transition of next state $S_{t+1}$ with probability $p(S_{t+1}\\mid\\bar{S}_{t},\\bar{A_{t}})$ and the reward $R_{t+1}$ . ", "page_idx": 2}, {"type": "text", "text": "3.2 Successor Features ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "SFs are defined via a decomposition of the state-action value function (i.e. the expected return), $Q$ , into the reward function and a representation of expected features occupancy for each state $S_{t}$ and action $A_{t}$ of time step $t$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\nQ(S_{t},A_{t},\\pmb{w})=\\psi(S_{t},A_{t},\\pmb{w})^{\\top}\\pmb{w}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\psi\\in\\mathbb{R}^{n}$ are the SFs that capture expected feature occupancy and $\\pmb{w}\\in\\mathbb{R}^{n}$ is a vector of the task encoding, which can be considered a representation of the reward function [Borsa et al., 2018]. ", "page_idx": 2}, {"type": "text", "text": "Canonically, the SFs for a state-action pair $(s,a)$ under a policy $\\pi$ are defined as: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\psi^{\\pi}(s,a)\\equiv\\mathrm{E}^{\\pi}\\left[\\sum_{i=t}^{\\infty}\\gamma^{i-t}\\phi_{i+1}\\mid S_{t}=s,A_{t}=a\\right]\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\phi\\in\\mathbb{R}^{n}$ is a set of basis features, and $\\pi$ is the policy [Barreto et al., 2017]. ", "page_idx": 2}, {"type": "text", "text": "However, as shown by Borsa et al. [2018], we can treat the task encoding vector $\\mathbf{\\nabla}w$ as a way to encode policy $\\pi$ . This results in Universal $S F s$ , $\\psi(s,a,w)$ , on which we base our work. ", "page_idx": 3}, {"type": "text", "text": "The task encoding vector $\\mathbf{\\nabla}w$ can also be related directly to the rewards themselves via the underlying basis features $\\left(\\phi\\right)$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\nR_{t+1}=\\phi(S_{t+1})^{\\top}\\pmb{w}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "3.3 Canonical Approach to Learning Successor Features and its Limitations ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The canonical approach for learning the basis features $\\phi$ and successor features $\\psi$ for each state $S_{t}$ and action $A_{t}$ of time step $t$ , with respect to policy $\\pi$ , are achieved by optimizing the following SF Temporal-Difference loss: ", "page_idx": 3}, {"type": "equation", "text": "$$\nL_{\\phi,\\psi}=\\frac{1}{2}\\left\\|\\phi(S_{t+1})+\\gamma\\psi(S_{t+1},a,{\\pmb w})\\right)-\\psi(S_{t},{\\cal A}_{t},{\\pmb w})\\right\\|^{2}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where action $a\\sim\\pi(S_{t+1})$ . The basis features $\\phi$ are typically defined as the normalized output of an encoder, which the SFs $\\psi$ learn from concurrently (see Figure 2 for an example). ", "page_idx": 3}, {"type": "text", "text": "However, when the basis features, $\\phi$ , must be learned from high-dimensional, complex observations such as pixels, optimizing Eq. 4 may result in the basis features, $\\phi$ , converging to a constant vector. This outcome occurs because it can minimize the loss, as noted by Machado et al. [2020], which we will also prove mathematically below. ", "page_idx": 3}, {"type": "text", "text": "3.4 Proof by Contradiction: Representation Collapse in Successor Features ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Consider the basis features function $\\phi(\\cdot)$ and the Successor Features $\\psi(\\cdot)$ , omitting the inputs for clarity. The canonical SF-TD loss (Eq. 4) is defined as: ", "page_idx": 3}, {"type": "equation", "text": "$$\nL_{\\phi,\\psi}=\\frac{1}{2}\\left\\|\\phi(\\cdot)+\\gamma\\psi(\\cdot)-\\psi(\\cdot)\\right\\|^{2}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Using proof by contradiction, we aim to show that when both $\\phi(\\cdot)$ and $\\psi(\\cdot)$ are constants across all states $S$ , specifically when $\\phi(\\cdot)=c_{1}$ and $\\psi(\\cdot)=c_{2}$ with $c_{1}=(1-\\gamma)c_{2}$ , the system satisfies the zero-loss conditions, leading to representation collapse. ", "page_idx": 3}, {"type": "text", "text": "We start with the assumption that if $\\phi(\\cdot)=c_{1},\\psi(\\cdot)=c_{2}$ , then $L_{\\phi,\\psi}\\neq0\\,\\forall c_{1},c_{2}\\in\\mathbb{R}$ . ", "page_idx": 3}, {"type": "text", "text": "Substituting $\\phi(\\cdot)=c_{1}$ and $\\psi(\\cdot)=c_{2}$ into the loss function: ", "page_idx": 3}, {"type": "equation", "text": "$$\nL_{\\phi,\\psi}=\\frac{1}{2}\\left\\|c_{1}+\\gamma c_{2}-c_{2}\\right\\|^{2}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "It is trivial to observe that if $c_{1}=(1-\\gamma)c_{2}$ , the expression for $L_{\\phi,\\psi}$ is as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\cal L}_{\\phi,\\psi}=\\frac12\\left\\|(1-\\gamma)c_{2}+\\gamma c_{2}-c_{2}\\right\\|^{2}}}\\\\ {{\\displaystyle=\\frac12\\left\\|0\\right\\|^{2}}}\\\\ {{\\displaystyle=0}}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "This contradicts our assumption that $L_{\\phi,\\psi}\\not=0$ for a particular relationship between $c_{1}$ and $c_{2}$ . \uff1a\u53e3 ", "page_idx": 3}, {"type": "text", "text": "Thus, we have shown that there exist constants $c_{1},c_{2}$ such that when $\\phi(\\cdot)=c_{1}$ and $\\psi(\\cdot)=c_{2}$ with $c_{1}=(1-\\gamma)c_{2}$ , the system does satisfy the zero-loss conditions, resulting in degenerate solutions for $L_{\\phi,\\psi}$ , i.e. causing representation collapse. In this collapsed state, $\\phi(\\cdot)$ loses its ability to distinguish between different states effectively, causing the model to lose critical discriminative information and thus impairing its generalization capabilities. ", "page_idx": 3}, {"type": "text", "text": "Additionally, we also show empirically in Figure 1(a-c) of the presence of representation collapse when learning using Eq. 4. In this work, our method aims to mitigate these issues with a novel, simple approach for learning SFs directly from pixels. ", "page_idx": 3}, {"type": "text", "text": "4 Proposed Method ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The key insight from the proof above (section 3.4) is that preventing representation collapse requires avoiding the scenario where the basis features $\\phi$ become a constant vector for all states $S$ , which would minimize the loss without contributing to meaningful learning. Below, we will describe the steps taken in our approach to mitigate these issues causing representation collapse. ", "page_idx": 4}, {"type": "text", "text": "We note that when the representations $\\psi$ form a set of SFs, Eq. (1) is satisfied for some $\\pmb{w}$ that also satisfies Eq. (3). Therefore, the approach we take to learn SFs is simply to ensure that over the course of the learning $\\psi$ and $\\mathbf{\\nabla}w$ come to satisfy both of these equations, which can be achieved by using the following loss functions: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{L_{w}=\\displaystyle\\frac{1}{2}\\left\\|R_{t+1}-\\overline{{\\phi}}(S_{t+1})^{\\top}{w}\\right\\|^{2}~,}\\\\ {L_{\\psi}=\\displaystyle\\frac{1}{2}\\left\\|\\widehat{y}-\\psi(S_{t},A_{t},{\\pmb w})^{\\top}{\\pmb w}\\right\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "image", "img_path": "rI7oZj1WMc/tmp/c90c1703a92e15b70bf8148ae285851ade55a46b39b1d05a4606df5beee069eb.jpg", "img_caption": [], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "where $\\overline{{\\phi}}(S_{t+1})$ is treated as a constant in Eq. 8 using a stop-gradient operator, and $\\hat{y}$ is the bootstrapped target: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\hat{y}=R_{t+1}+\\gamma\\operatorname*{max}_{a^{\\prime}}\\psi(S_{t+1},a^{\\prime},\\boldsymbol{w})^{\\top}\\boldsymbol{w}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Here, $\\pmb{w}$ is only altered by $L_{w}$ , whereas $\\operatorname{SF}\\psi$ and the basis features $\\phi$ are learned via $L_{\\psi}$ . ", "page_idx": 4}, {"type": "text", "text": "Specifically, our proposed approach can overcome representation collapse by treating the basis features $\\phi$ as the $L2$ normalized output from the encoder of the $S F\\ \\psi$ network (Figure 2), because unlike in Eq. 4, Eq. 8 and Eq. 9 are not minimized by setting $\\phi$ to a constant value, given that $\\hat{y}$ and $R_{t+1}$ are not constants for all states $S$ . Hence, there is nothing encouraging the network to con", "page_idx": 4}, {"type": "text", "text": "Figure 2: Our proposed model for learning SFs. Starting from the top, the representations of state $S_{t}$ are learned using the shared encoder, resulting in $h_{t}$ . The basis features $\\bar{\\phi(S_{t+1})}$ are the normalized output of the encoder using state $S_{t+1}$ . The task-encoding vector $\\mathbf{\\nabla}w$ is learned through the reward prediction loss (Eq. 8). Concatenated with $w$ , the basis features and successor features are learned through computing the Q-values with $\\mathbf{\\nabla}w$ and minimizing the $\\mathcal{Q}{-}S F{-}T D$ loss function (Eq. 9). A schematic for continuous actions and previous approaches can be found in Appendix G and H respectively. ", "page_idx": 4}, {"type": "text", "text": "verge to a constant vector, naturally avoiding representational collapse. ", "page_idx": 4}, {"type": "text", "text": "When the basis features $\\phi$ are needed to learn the task encoding vector $w$ through the reward prediction loss (Eq. 8), we apply a stop-gradient operator to treat the basis features $\\phi$ as fixed. As we will demonstrate in section 7 \u201cAnalysis of Efficiency and Efficacy\u201d, this inclusion of a stop-gradient operator is crucial. Without it, learning both the basis features $\\phi$ and the task encoding vector $w$ concurrently can lead to learning instability. ", "page_idx": 4}, {"type": "text", "text": "Next, we will clarify how our approach relates to learning SFs, as they are defined mathematically. Given the straightforward nature of our approach, we refer to the SFs learned as \u201cSimple SFs.\u201d ", "page_idx": 4}, {"type": "text", "text": "4.1 Bridging Simple SFs and Universal Successor Features ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In Proposition 1 (Appendix C), we show that our approach ultimately produces true SFs, equivalent to the SFs learned using Eq. 4. Proposition 1 does this by proving that minimizing our losses (Eq. 8 & Eq.9) also minimizes the canonical SF loss used in Universal Successor Features (Eq. 4). Furthermore, Proposition 1 supports the proof above (Section 3.4) that our approach minimizes these losses in a manner such that setting the basis features $\\phi$ to a constant is not a solution. Once again, if $\\psi=c_{2}$ and $\\phi=c_{1}=(1-\\gamma)c_{2}$ then Eq. 8 & Eq. 9 are not minimized, due to the fact that $\\hat{y}$ and $R_{t+1}$ in Eq. 10 are not constants for all states $S$ . ", "page_idx": 4}, {"type": "image", "img_path": "rI7oZj1WMc/tmp/f25a4da33218177b93eafe35d2a71e5c71e9188c3810038225be56d405a306b1.jpg", "img_caption": ["Figure 3: Continual Reinforcement Learning Evaluation with pixel observations in 2D Minigrid and 3D Four Rooms environment. Replay buffer resets at each task transitions to simulate drastic distribution shifts: Agents face two sequential tasks (Task 1 & Task 2), each repeated twice (Exposure 1 & Exposure 2). (a-c): The total cumulative returns accumulated during training. Overall, our agent, Simple SF (orange), shows notable superiority and exhibited better transfer in later tasks over both DQN (blue) and agents with added constraints. Importantly, constraints like reconstruction and orthogonality on basis features can impede learning. The plots for moving average episode returns are available in Appendix J.6 for additional insights. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "5 Learning Successor Features the Simple Way ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The architecture for our network is shown in Figure 2, which is broadly inspired by Liu and Abbeel [2021]. Pixel-level observations, $S_{t}$ , are fed into a convolutional encoder that outputs a latent representation $h(S_{t})$ , which is used both to construct the basis features and the SFs. To construct the basis features, $\\phi(S_{t})$ , we simply normalize the latent representations $h$ (via $L2$ normalization, following Machado et al. [2020]). To calculate the representations $\\psi(S_{t},A_{t},\\pmb{w})$ , the latent representation is combined with the task encoding vector, $\\pmb{w}$ , and fed into a multilayer perceptron that generates one set of representations for each possible action, $A_{t}$ . These representations are then combined with the task encoding via a dot product operation to estimate the $Q$ -value function, $Q(S_{t},A_{t},\\pmb{w})=\\psi(S_{t},A_{t},\\pmb{w})^{\\top}\\pmb{w}$ . The policy is then simply an $\\epsilon_{}$ -greedy policy based on the $Q$ -value function. ", "page_idx": 5}, {"type": "text", "text": "To learn the basis features $\\left(\\phi\\right)$ and representations $(\\psi)$ , we minimize the losses in Eq. 8 and Eq. 9 using minibatch samples of experience tuples $(S_{t},A_{t},R_{t+1},S_{t+1},\\pmb{w})$ , collected while interacting with the environment and sampled from a replay buffer which is similar to Mnih et al. [2015]. Critically, only the task-encoding vector $\\mathbf{\\nabla}w$ is learned by optimizing Eq. 8, so a stop gradient operator is applied to the basis features $\\phi(S_{t})$ (see Figure 2). The successor features, $\\psi$ , in the bootstrap target, $\\hat{y}$ , actually come from a target network, $\\overline{{\\psi}}$ , which is updated periodically by using the actual network, a common approach in deep RL [Mnih et al., 2015]. The successor features $\\psi$ , and all of the upstream network parameters $\\theta$ , are learned by minimizing Eq. 9. The full algorithm used for training our network is given in Algorithm 1 in Appendix B. ", "page_idx": 5}, {"type": "text", "text": "6 Experimental results ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The environments used in our studies are $\\mathrm{10}\\times\\mathrm{10}\\,2\\mathrm{D}$ grid worlds, 3D Four Rooms environments (Figure 9 in Appendix D) and Mujoco. All studies were conducted exclusively using pixel observations, as the primary motivation for this paper is to address representation collapse when learning with pixel observations. ", "page_idx": 5}, {"type": "text", "text": "The grid worlds offer both egocentric (partially) and allocentric (fully observable) scenarios while the 3D Four Rooms environments provide exclusively egocentric observations. The rationale behind selecting these environments was threefold: first, to evaluate the agent\u2019s learning capabilities across varying levels of environmental visibility, second, to examine its ability to interpret spatial relationships and distances, and third, to provide a set of tasks where the transition dynamics are easy to quantify for constructing SRs that can serve as a comparison to evaluate the SFs with. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "For a more complex setting, we considered the Mujoco environment because it allows for direct manipulation of the reward function and domains switching, such as moving from half-cheetah to walker, given that they both have the same action dimensions. ", "page_idx": 6}, {"type": "text", "text": "To evaluate the generalization capabilities of the learned SFs, our studies focus on continual learning setting. In the 2D grid worlds and 3D Four Rooms environment, agents are exposed to two cycles of two distinct tasks. These tasks involves changes in reward locations (as shown in Figure 9b & Figure 9d) and/or changes in environment dynamics (as shown in Figure 9a & Figure 9e). Additionally, we explored two different scenarios to better simulate real-world conditions. The first scenario involves resetting the replay buffer at each task transition, which emulates drastic distribution shifts typically encountered in real-world applications. The second scenario maintains the replay buffer across task transitions, allowing us to assess the agent\u2019s learning continuity in a more stable data setting. ", "page_idx": 6}, {"type": "text", "text": "In the Mujoco environment, agents are exposed to one cycle of two distinct task as in this setting, we primarily wish to evaluate how well the agents can adapt to new tasks and mitigating interference. ", "page_idx": 6}, {"type": "text", "text": "In all experiments, we make comparisons with several baselines, namely, a Double Deep Q-Network (DQN) agent [Van Hasselt et al., 2016] and agents learning SFs $(\\psi)$ with constraints on their basis features $\\left(\\phi\\right)$ , including reconstruction loss [Machado et al., 2020], orthogonal loss [Touati et al., 2022], and unlearnable random features [Touati et al., 2022]. Additionally, we compare with an agent that learns SFs through a non-task-engaged pre-training regime [Liu and Abbeel, 2021]. The mathematical definitions of the constraints can be found in Appendix F. To ensure the robustness of our results, all experiments are conducted across 5 different random seeds. ", "page_idx": 6}, {"type": "text", "text": "6.1 2D Grid world ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "The 2D Gridworld environments were developed based on 2D Minigrid [Chevalier-Boisvert et al., 2023]. We created two different layouts of the 2D Gridworld environment, namely Center-Wall (Figure 9a) and Inverted-LWalls (Figure 9b). In order to align the setting more closely with the canonical Gridworld environment as described by Sutton and Barto [2018], we altered the reward function such that it returns a reward of $+1$ when the agent successfully reaches the goal state and 0 otherwise. For the 2D Gridworld environments, the agents were trained for one million steps per task. ", "page_idx": 6}, {"type": "text", "text": "Figure 3a presents the cumulative returns for the Center-Wall environment with egocentric observations, while Figure 3b shows the results for allocentric observations. ", "page_idx": 6}, {"type": "text", "text": "The results show that our agent learns as well as, if not better than, the baseline models. Furthermore, when analysing the cumulative total returns during training, our model, SF Simple, exhibited better transfer that the baseline models. Particularly, SFs that are learned with constraints on the basis features clearly struggle to learn effectively, either due to the additional computational overhead or because representations that fulfill those constraints do not lead to effective policy learning. ", "page_idx": 6}, {"type": "text", "text": "6.2 3D Four Rooms environment ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We developed the 3D Four Rooms environments (Figure 9d) using Miniworld [Chevalier-Boisvert et al., 2023]. In this environment, the state and action spaces are continuous. In the first task, the agent receives a reward of $+1$ when it reaches the green box and a reward of -1 when it reaches the yellow box and this alternates for the second task. The agents were trained for five million steps per task. Similarly, the results in Figure 3c show that our agent is able to learn effectively using egocentric pixel observations in a 3D environment. ", "page_idx": 6}, {"type": "text", "text": "6.3 Mujoco ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In order to demonstrates our model\u2019s capabilities with continuous actions, we consider the Mujoco environment. We followed the established protocol in Yarats et al. [2021] for effective learning with pixels observations in this environment. We started in the half-cheetah domain, rewarding agents for running forward in Task 1. For Task 2, we introduced scenarios with running backwards, running faster, and switching to the walker domain. The results are presented in Figure 4. ", "page_idx": 6}, {"type": "image", "img_path": "rI7oZj1WMc/tmp/e94feb9665c0af9274059a8349b56cbff57e81263f606cec3283e6dddcc78ac6.jpg", "img_caption": ["Figure 4: Continual Reinforcement Learning results using pixel observations in Mujoco environment across 5 random seeds. Replay buffer resets at each task transitions to simulate drastic distribution shifts. we started with the half-cheetah domain in Task 1 where agents were rewarded for running forward. We then introduced three different scenarios in Task 2: (a) agents were rewarded for running backwards, (b) running faster, and, in the most drastic change, (c) switching from the half-cheetah to the walker domain with a forward running task. To ensure comparability across these diverse scenarios, we normalized the returns, considering that each task has different maximum attainable returns per episode. We did not evaluate APS (Pre-train) here because it struggles in the Continual RL setting, even in simpler environments such as the 2D Minigrid and 3D Miniworld. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Across all scenarios, our model not only maintained high performance but consistently outperformed all baselines in both Task 1 and Task 2, highlighting its superior adaptability and effectiveness in complex environments. This contrasted sharply with other SF-related baseline models, which struggled to adapt under these conditions. ", "page_idx": 7}, {"type": "text", "text": "7 Analysis of Efficacy and Efficiency ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "7.1 Comparison to Successor Representations ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Can our SF-learning technique, like traditional SRs, effectively capture the transition dynamics of the environment [Stachenfeld et al., 2017]? To investigate, we first sought a quantitative measure to compare SFs to SRs. To do this, we trained a simple non-linear decoder to assess which model\u2019s SFs can be most effectively decode into SRs. We conducted this evaluation using both allocentric and egocentric observations within the center-wall environment. The results, depicted in Figure 5, shows that our model demonstrate consistently high accuracy (lower errors) across both settings. This contrasts sharply with SFs developed using reconstruction constraints or random basis features, which, while effective in egocentric settings, perform poorly in allocentric settings where feature sparsity is greater. ", "page_idx": 7}, {"type": "text", "text": "We next utilized 2D visualizations with geospatial color mapping to differentiate environmen", "page_idx": 7}, {"type": "image", "img_path": "rI7oZj1WMc/tmp/f4de54b04ba191319515a22fcdd80c6c3794e1e742284b0d2550f38594a26724.jpg", "img_caption": ["Figure 5: Decoding performance comparison of models\u2019 SFs into SRs using a non-linear decoder in the Center-Wall environment. Ground truth SRs are generated analytically using Eq. 21, described in Appendix N. Lower Mean Squared Error values on the y-axis indicate better performance. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "tal locations, aiming to see if similar SFs that are proximate in neural space are proximate in physical space. Using UMAP [McInnes et al., 2018] for dimension reduction, our results (Figure 6) suggest that our simple approach captures environmental statistics comparably to other models, but with less overhead for calculating the loss. Moreover, our technique consistently forms organized spatial clusters across partially, fully, and egocentric observational settings. ", "page_idx": 7}, {"type": "text", "text": "Additionally, we performed a correlation analysis in 2D Gridworld environments, comparing each spatial position and head direction against analytically computed SRs [Dayan, 1993], further confirming the robustness and adaptability of our model\u2019s SFs in various observational contexts (Table 6 and Table 7 in Appendix N). ", "page_idx": 7}, {"type": "image", "img_path": "rI7oZj1WMc/tmp/73b7d6a92a5f1a4ebcf7e8d0a8dfa2bbb5b1d4039dcfcad7e7e327eafbd0b787.jpg", "img_caption": ["Figure 6: 2D visualization of Successor Features in (a) the fully-observable Center-Wall environment and ${\\bf(b)}$ the 3D Four Rooms environment. Each row represents different models\u2019 visualizations post-training, starting with geospatial color mapping of the layout in the first column, followed by comparisons of SF-based models. Clustering indicates the capture of environmental statistics. Despite this, well-clustered SF models, especially those with orthogonality constraints, may not always translate to effective policy learning, as seen in Figure 3. In allocentric scenarios, SFs with reconstruction constraints struggle with minimal pixel variations, unlike in the distinct pixel changes in the Four Rooms environment. For more visualizations, see Appendix M. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "7.2 Is Stop Gradient critical for learning? ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Previous methods that concurrently learn the basis features, $\\phi$ , and the task-encoding vector $w$ , often face challenges with learning efficiency and stability, particularly in environments characterized by sparse rewards. This issue is illustrated in Figure 10 in Appendix D of Ma et al. [2020], where optimizing the reward prediction loss (Eq. 8) can inadvertently drive the basis features towards zero $(\\phi\\to{\\vec{0}}$ ), causing significant representational collapse. Representational collapse not only reduces the discriminative capabilities of $\\phi$ but also undermines the agent\u2019s ability to differentiate between distinct states, thus severely impacting the overall learning process. ", "page_idx": 8}, {"type": "image", "img_path": "rI7oZj1WMc/tmp/254aceb9a56b2e42cf12f78ef13e0be2e9a714c08e0465b1d95c05df5fd29aba.jpg", "img_caption": ["Figure 7: Efficacy of the Stop Gradient Operator in the Four Rooms Environment. Agents without a stop gradient operator exhibit degraded learning. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "As depicted in Figure 2, our solution involves the strategic use of a stop gradient operator applied to the basis features $\\phi$ . This operator prevents the gradient from the reward prediction loss from updating basis features $\\phi$ , effectively decoupling the learning of $\\phi$ from $w$ , thus ensuring that it retains its critical discriminative statistics, allowing for effective learning as demonstrated in Figure 7. ", "page_idx": 8}, {"type": "text", "text": "7.3 Robustness to Stochasticity within the environment ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "How robust are the SFs learned using our approach as the environment dynamics become noisier? To explore this question and verify the robustness of our technique, we also created a slippery variant of the Four Rooms environment (Figure 9e). Specifically, in the top right and bottom left rooms, the agent experiences a \"slippery\" dynamic: chosen actions have a certain probability of being replaced with random, alternative actions. This design mimics the effects of a low-friction or slippery surface, creating a scenario where the agent\u2019s intended movements might lead to unpredictable outcomes. ", "page_idx": 8}, {"type": "image", "img_path": "rI7oZj1WMc/tmp/886b8686d95ebb54a1567f52884d4ff57c4d250c05e631f9bdd704a8e89f3ba0.jpg", "img_caption": [], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Figure 8: Efficiency analysis using 3D Slippery Four Rooms environment. (a-d): Robustness analysis to increasing levels of stochasticity. (e) Bar plot showing efficiency in learning, measured as steps to achieve a policy that produces a reasonable level of performance, with low values indicating higher efficiency. (f) Bar plot showing frames per second achieved by the agent during gradient computation, back-propagation, and interaction with the environment. These metrics provide insights into the computational efficiency and the real-time interaction capabilities of the agent across different tasks or conditions. (g) Bar plot showing the total training duration for completing two exposures of two tasks, demonstrating overall time efficiency. Collectively, these plots reveal that our agent not only learns tasks effectively but also excels in computational efficiency. ", "page_idx": 9}, {"type": "text", "text": "The results in Figure 8a-d demonstrate that our approach is robust to increasing levels of stochasicity. Notably, when the stochasicity is high (slippery probability $>=0.3)$ ), all other SF methods fail to learn effectively in the second task, whereas our approach continues to perform well. ", "page_idx": 9}, {"type": "text", "text": "7.4 Efficiency analysis ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "How do alternative SF learning methods with extra loss functions, like orthogonality constraints, stack up against our approach in terms of efficiency? We analyzed the number of steps each method takes to learn an effective policy, using a performance threshold defined by the shortest expected episode length from the last 10 episodes. A shorter episode length indicates better performance, as it signifies quicker goal achievement. We noted the timestep when each model first met or exceeded this threshold. Our results, shown in Figure 8e, demonstrate that our method outperforms all baselines in learning efficiency. Additionally, our method leverages simpler compute blocks and loss functions, enhancing computational speed and reducing training duration, as shown by faster frame processing rates (Figure 8f) and shorter overall training times (Figure 8g). Therefore, our approach is more efficient than the baseline methods for learning SFs. ", "page_idx": 9}, {"type": "text", "text": "8 Discussion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we developed a method for learning SFs from pixel-level observations without pretraining or complex auxiliary losses. By applying the mathematical principles of SFs, our system effectively learns during task engagement using standard losses based on typical training returns and rewards. This simplicity and efficiency are key advantages of our approach. ", "page_idx": 9}, {"type": "text", "text": "Our experiments demonstrate that our method learns SFs effectively under various conditions and surpasses baseline models in continual RL scenarios. It effectively captures environmental transition dynamics and correlates well with analytically computed Successor Representations (SRs), offering a streamlined, efficient strategy for integrating SFs into RL models. Future work could build on this to create more sophisticated models that leverage SFs for enhanced flexibility in RL. ", "page_idx": 9}, {"type": "text", "text": "9 Limitations and Broader Impact ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "The algorithms we developed were evaluated predominantly in simulated environments, which may not capture the diverse complexity of real-world scenarios. A key assumption in our approach is that pixel observations are of good quality. This assumption is critical as poor image quality could substantially degrade the performance and applicability of our algorithms. ", "page_idx": 10}, {"type": "text", "text": "The use of Successor Features in learning algorithms, as demonstrated in this work, offers significant advantages, particularly in mitigating catastrophic interference. This capability is crucial for the development of machine learning systems that require continuous learning, such as in dynamic environments. For instance, autonomous vehicles operating in ever-changing conditions can retain learned knowledge while adapting to new information, enhancing their safety and reliability. ", "page_idx": 10}, {"type": "text", "text": "However, the enhanced capabilities of these systems also raise concerns. The ability of machine learning models to continuously adapt and learn can lead to challenges in predictability and control, potentially making outcomes less transparent. As systems become more autonomous and capable of adapting over time, there\u2019s a risk that errors or biases in the learning process could propagate more extensively before detection, especially if oversight does not keep pace with the rate of learning. ", "page_idx": 10}, {"type": "text", "text": "10 Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Raymond Chua was supported by the DeepMind Graduate Award and UNIQUE Excellence Scholarship (PhD). We extend our gratitude to the FRQNT Strategic Clusters Program (2020-RS4-265502 - Centre UNIQUE - Quebec Neuro-AI Research Center). ", "page_idx": 10}, {"type": "text", "text": "Arna Ghosh was supported by the Vanier Canada Graduate scholarship and Healthy Brains, Healthy Lives Doctoral Fellowship. ", "page_idx": 10}, {"type": "text", "text": "Blake A. Richards was also supported by NSERC (Discovery Grant RGPIN-2020- 05105, RGPIN2018-04821; Discovery Accelerator Supplement: RGPAS-2020-00031; Arthur B. McDonald Fellowship: 566355-2022), Healthy Brains, Healthy Lives (New Investigator Award: 2b-NISU-8), and CIFAR (Canada AI Chair; Learning in Machine and Brains Fellowship). ", "page_idx": 10}, {"type": "text", "text": "This research was further enabled by computational resources provided by Calcul Qu\u00e9bec 2 and the Digital Research Alliance of Canada 3, along with the computational resources support from NVIDIA Corporation. ", "page_idx": 10}, {"type": "text", "text": "We are grateful to Gheorghe Comanici, Pranshu Malviya, Xing Han Lu, Isabeau Pr\u00e9mont-Schwarz and the anonymous reviewers whose insightful comments and suggestions significantly enhanced the quality of this manuscript. Additionally, our discussions with members of the LiNC lab 4, Mila 5, and early collaborators from Microsoft Research (MSR) have been invaluable in shaping this research. Special thanks to Ida Momennejad, Geoff Gordon and Mehdi Fatemi from MSR for their substantial insights and contributions during the initial phases of this work. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Zaheer Abbas, Rosie Zhao, Joseph Modayil, Adam White, and Marlos C. Machado. Loss of plasticity in continual deep reinforcement learning. arXiv.org, 2023. doi: 10.48550/arxiv.2303.07507. ", "page_idx": 10}, {"type": "text", "text": "Majid Abdolshah, Hung Le, T. G. Karimpanal, Sunil Gupta, Santu Rana, and S. Venkatesh. A new representation of successor features for transfer across dissimilar environments. International Conference on Machine Learning, 2021. ", "page_idx": 10}, {"type": "text", "text": "Armen Aghajanyan, Akshat Shrivastava, Anchit Gupta, Naman Goyal, Luke Zettlemoyer, and Sonal Gupta. Better fine-tuning by reducing representational collapse. arXiv preprint arXiv:2008.03156, 2020. ", "page_idx": 10}, {"type": "text", "text": "L. N. Alegre, A. Bazzan, and Bruno C. Da Silva. Optimistic linear support and successor features as a basis for optimal policy transfer. International Conference on Machine Learning, 2022. doi: 10.48550/arxiv.2206.11326.   \nNishanth Anand and Doina Precup. Prediction and control in continual reinforcement learning. Advances in Neural Information Processing Systems, 36, 2024.   \nAndr\u00e9 Barreto, Will Dabney, R\u00e9mi Munos, Jonathan J Hunt, Tom Schaul, Hado P van Hasselt, and David Silver. Successor features for transfer in reinforcement learning. Advances in neural information processing systems, 30, 2017.   \nAndre Barreto, Diana Borsa, John Quan, Tom Schaul, David Silver, Matteo Hessel, Daniel Mankowitz, Augustin Zidek, and Remi Munos. Transfer in deep reinforcement learning using successor features and generalised policy improvement. In International Conference on Machine Learning, pages 501\u2013510. PMLR, 2018.   \nAndr\u00e9 Barreto, Shaobo Hou, Diana Borsa, David Silver, and Doina Precup. Fast reinforcement learning with generalized policy updates. Proceedings of the National Academy of Sciences, 117 (48):30079\u201330087, 2020.   \nLukas Biewald. Experiment tracking with weights and biases, 2020. URL https://www.wandb. com/. Software available from wandb.com.   \nDiana Borsa, Andr\u00e9 Barreto, John Quan, Daniel Mankowitz, R\u00e9mi Munos, Hado Van Hasselt, David Silver, and Tom Schaul. Universal successor features approximators. arXiv preprint arXiv:1812.07626, 2018.   \nJames Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. JAX: composable transformations of Python+NumPy programs, 2018. URL http://github.com/google/jax.   \nKiant\u00e9 Brantley, Soroush Mehri, and Geoff J Gordon. Successor feature sets: Generalizing successor representations across policies. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 11774\u201311781, 2021.   \nWilka Carvalho, Angelos Filos, Richard L. Lewis, Honglak Lee, and Satinder Singh. Composing task knowledge with modular successor feature approximators. International Conference on Learning Representations, 2023a. doi: 10.48550/arxiv.2301.12305.   \nWilka Carvalho, Andre Saraiva, Angelos Filos, Andrew Kyle Lampinen, Loic Matthey, Richard L Lewis, Honglak Lee, Satinder Singh, Danilo J Rezende, and Daniel Zoran. Combining behaviors with the successor features keyboard. arXiv preprint arXiv:2310.15940, 2023b.   \nMaxime Chevalier-Boisvert, Bolun Dai, Mark Towers, Rodrigo de Lazcano, Lucas Willems, Salem Lahlou, Suman Pal, Pablo Samuel Castro, and Jordan Terry. Minigrid & miniworld: Modular & customizable reinforcement learning environments for goal-oriented tasks. CoRR, abs/2306.13831, 2023.   \nPeter Dayan. Improving generalization for temporal difference learning: The successor representation. Neural Computation, 1993. doi: 10.1162/neco.1993.5.4.613.   \nDavid Emukpere, Xavier Alameda-Pineda, and Chris Reinke. Successor feature neural episodic control. arXiv.org, 2021.   \nJesse Farebrother, Joshua Greaves, Rishabh Agarwal, Charline Le Lan, Ross Goroshin, Pablo Samuel Castro, and Marc G. Bellemare. Proto-value networks: Scaling representation learning with auxiliary tasks. International Conference on Learning Representations, 2023. doi: 10.48550/arxiv. 2304.12567.   \nAngelos Filos, Clare Lyle, Yarin Gal, Sergey Levine, Natasha Jaques, and Gregory Farquhar. Psiphilearning: Reinforcement learning with demonstrations using successor features and inverse temporal difference learning. null, 2021. doi: 10.48550/arxiv.2102.12560.   \nScott Fujimoto, D. Meger, and Doina Precup. A deep reinforcement learning approach to marginalized importance sampling with the successor representation. International Conference on Machine Learning, 2021.   \nJonathan Godwin\\*, Thomas Keck\\*, Peter Battaglia, Victor Bapst, Thomas Kipf, Yujia Li, Kimberly Stachenfeld, Petar Veli\u02c7ckovi\u00b4c, and Alvaro Sanchez-Gonzalez. Jraph: A library for graph neural networks in jax., 2020. URL http://github.com/deepmind/jraph.   \nRaia Hadsell, Dushyant Rao, Andrei A Rusu, and Razvan Pascanu. Embracing change: Continual learning in deep neural networks. Trends in cognitive sciences, 24(12):1028\u20131040, 2020.   \nSteven Hansen, Will Dabney, Andre Barreto, Tom Van de Wiele, David Warde-Farley, and Volodymyr Mnih. Fast task inference with variational intrinsic successor features. arXiv preprint arXiv:1906.05030, 2019.   \nTom Hennigan, Trevor Cai, Tamara Norman, Lena Martens, and Igor Babuschkin. Haiku: Sonnet for JAX, 2020. URL http://github.com/deepmind/dm-haiku.   \nJ. D. Hunter. Matplotlib: A 2d graphics environment. Computing in Science & Engineering, 9(3): 90\u201395, 2007. doi: 10.1109/MCSE.2007.55.   \nDavid Janz, Jiri Hron, Przemys\u0142aw Mazur, Katja Hofmann, Jos\u00e9 Miguel Hern\u00e1ndez-Lobato, and Sebastian Tschiatschek. Successor uncertainties: exploration and uncertainty in temporal difference learning. Advances in Neural Information Processing Systems, 32, 2019.   \nChristos Kaplanis, Murray Shanahan, and Claudia Clopath. Continual reinforcement learning with complex synapses. In International Conference on Machine Learning, pages 2497\u20132506. PMLR, 2018.   \nKhimya Khetarpal, Matthew Riemer, Irina Rish, and Doina Precup. Towards continual reinforcement learning: A review and perspectives. Journal of Artificial Intelligence Research, 75:1401\u20131476, 2022.   \nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.   \nJames Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, 114 (13):3521\u20133526, 2017.   \nYehuda Koren. On spectral graph drawing. In International Computing and Combinatorics Conference, pages 496\u2013508. Springer, 2003.   \nTejas D. Kulkarni, Ardavan Saeedi, Simanta Gautam, and Samuel J. Gershman. Deep successor reinforcement learning. arXiv: Machine Learning, 2016.   \nMichael Laskin, Denis Yarats, Hao Liu, Kimin Lee, Albert Zhan, Kevin Lu, Catherine Cang, Lerrel Pinto, and Pieter Abbeel. Urlb: Unsupervised reinforcement learning benchmark. arXiv preprint arXiv:2110.15191, 2021.   \nLucas Lehnert and Michael L Littman. Successor features support model-based and model-free reinforcement learning. CoRR abs/1901.11437, 2019.   \nLucas Lehnert, Stefanie Tellex, and Michael L Littman. Advantages and limitations of using successor features for transfer in reinforcement learning. arXiv preprint arXiv:1708.00102, 2017.   \nTimothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015.   \nHao Liu and Pieter Abbeel. Aps: Active pretraining with successor features. In International Conference on Machine Learning, pages 6736\u20136747. PMLR, 2021.   \nChen Ma, Dylan R Ashley, Junfeng Wen, and Yoshua Bengio. Universal successor features for transfer reinforcement learning. arXiv preprint arXiv:2001.04025, 2020.   \nMarlos C Machado, Marc G Bellemare, and Michael Bowling. A laplacian framework for option discovery in reinforcement learning. In International Conference on Machine Learning, pages 2295\u20132304. PMLR, 2017a.   \nMarlos C Machado, Clemens Rosenbaum, Xiaoxiao Guo, Miao Liu, Gerald Tesauro, and Murray Campbell. Eigenoption discovery through the deep successor representation. arXiv preprint arXiv:1710.11089, 2017b.   \nMarlos C Machado, Marc G Bellemare, and Michael Bowling. Count-based exploration with the successor representation. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 5125\u20135133, 2020.   \nMarlos C. Machado, Andr\u00e9 Barreto, and Doina Precup. Temporal abstraction in reinforcement learning with the successor representation. Journal of machine learning research, 2021.   \nTamas Madarasz and Tim Behrens. Better transfer learning with inferred successor maps. Advances in neural information processing systems, 32, 2019.   \nSridhar Mahadevan and Mauro Maggioni. Proto-value functions: A laplacian framework for learning representation and control in markov decision processes. Journal of Machine Learning Research, 8(10), 2007.   \nL. McInnes, J. Healy, and J. Melville. UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction. ArXiv e-prints, February 2018.   \nLeland McInnes, John Healy, Nathaniel Saul, and Lukas Grossberger. Umap: Uniform manifold approximation and projection. The Journal of Open Source Software, 3(29):861, 2018.   \nMatt McLeod, Chun-Ping Lo, M. Schlegel, Andrew Jacobsen, Raksha Kumaraswamy, Martha White, and Adam White. Continual auxiliary task learning. Neural Information Processing Systems, 2022.   \nVolodymyr Mnih, Volodymyr Mnih, Koray Kavukcuoglu, Koray Kavukcuoglu, David Silver, David Silver, Andrei Rusu, Andrei A. Rusu, Joel Veness, Joel Veness, Marc G. Bellemare, Marc G. Bellemare, Alex Graves, Alex Graves, Martin Riedmiller, Martin Riedmiller, Andreas K. Fidjeland, Andreas K. Fidjeland, Georg Ostrovski, Georg Ostrovski, Stig Petersen, Stig Petersen, Charles Beattie, Charles Beattie, Amir Sadik, Amir Sadik, Ioannis Antonoglou, Ioannis Antonoglou, Ioannis Antonoglou, Helen King, Helen King, Dharshan Kumaran, Dharshan Kumaran, Daan Wierstra, Daan Wierstra, Shane Legg, Shane Legg, Demis Hassabis, and Demis Hassabis. Humanlevel control through deep reinforcement learning. Nature, 2015. doi: 10.1038/nature14236.   \nMark W. Nemecek and R. Parr. Policy caches with successor features. International Conference on Machine Learning, 2021.   \nGerman I Parisi, Ronald Kemker, Jose L Part, Christopher Kanan, and Stefan Wermter. Continual lifelong learning with neural networks: A review. Neural networks, 113:54\u201371, 2019.   \nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019.   \nF. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825\u20132830, 2011.   \nSam Powers, Eliot Xing, Eric Kolve, Roozbeh Mottaghi, and Abhinav Gupta. Cora: Benchmarks, baselines, and metrics as a platform for continual reinforcement learning agents. In Conference on Lifelong Learning Agents, pages 705\u2013743. PMLR, 2022.   \nChris Reinke and Xavier Alameda-Pineda. Successor feature representations. arXiv preprint arXiv:2110.15701, 2021.   \nDavid Rolnick, Arun Ahuja, Jonathan Schwarz, Timothy Lillicrap, and Gregory Wayne. Experience replay for continual learning. Advances in Neural Information Processing Systems, 32, 2019.   \nAndrei A. Rusu, Neil C. Rabinowitz, Guillaume Desjardins, Hubert Soyer, J. Kirkpatrick, K. Kavukcuoglu, Razvan Pascanu, and R. Hadsell. Progressive neural networks. arXiv.org, 2016.   \nKimberly L. Stachenfeld, Matthew Botvinick, and Samuel J. Gershman. The hippocampus as a predictive map. Nature Neuroscience, 2017. doi: 10.1038/nn.4650.   \nRichard S Sutton. Learning to predict by the methods of temporal differences. Machine learning, 3: 9\u201344, 1988.   \nRichard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.   \nAhmed Touati, J\u00e9r\u00e9my Rapin, and Yann Ollivier. Does zero-shot reinforcement learning exist? arXiv preprint arXiv:2209.14935, 2022.   \nHado Van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double qlearning. In Proceedings of the AAAI conference on artificial intelligence, 2016.   \nGuido Van Rossum and Fred L. Drake. Python 3 Reference Manual. CreateSpace, Scotts Valley, CA, 2009. ISBN 1441412697.   \nMichael L. Waskom. seaborn: statistical data visualization. Journal of Open Source Software, 6(60): 3021, 2021. doi: 10.21105/joss.03021. URL https://doi.org/10.21105/joss.03021.   \nOmry Yadan. Hydra - a framework for elegantly configuring complex applications. Github, 2019. URL https://github.com/facebookresearch/hydra.   \nDenis Yarats, Rob Fergus, Alessandro Lazaric, and Lerrel Pinto. Mastering visual continuous control: Improved data-augmented reinforcement learning. arXiv preprint arXiv:2107.09645, 2021.   \nJingwei Zhang, Jost Tobias Springenberg, Joschka Boedecker, and Wolfram Burgard. Deep reinforcement learning with successor features for navigation across similar environments. In 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 2371\u20132378. IEEE, 2017. ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "A Appendix ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "This supplementary section provides detailed insights and additional information that supports the findings and methodology discussed in the main paper. Below is a brief overview of what each section contains: ", "page_idx": 15}, {"type": "table", "img_path": "rI7oZj1WMc/tmp/b8f56e696fa451f6ad0ca1cbd92c0f1f6edf4e47b5fc2d479608a09df0968476.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "B Pseudocode Implementation ", "text_level": 1, "page_idx": 15}, {"type": "table", "img_path": "rI7oZj1WMc/tmp/b3dac7fa49788f190377492acb54663467aebfcc43d123c70970b01373dfa540.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "C Proofs and Theoretical Justifications ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this section, we provide a proof sketch to show that minimizing the Q-SF-TD loss (Eq. 9) will also result in minimizing the canonical universal SF-TD loss [Borsa et al., 2018] for learning the SFs $(\\psi(\\cdot)\\in\\mathbb{R}^{n})$ . For the sake of brevity, we consider a tabular RL setting, where state $s$ is the current state, $s^{\\prime}$ is the next state, $a$ is the current action, and $r$ is the reward of the transition tuple $(s,a,s^{\\prime},r)$ and as per defined in the main text, $\\pmb{w}\\in\\mathbb{R}^{n}$ is the task encoding vector and $\\phi(\\cdot)\\in\\mathbb{R}^{n}$ is the set of basis features. ", "page_idx": 15}, {"type": "text", "text": "Let $L_{\\mathrm{SF}}$ be the canonical universal SF-TD loss [Borsa et al., 2018]: ", "page_idx": 15}, {"type": "equation", "text": "$$\n{\\cal L}_{\\mathtt{S F}}=\\frac{1}{2}\\left\\|\\phi(s^{\\prime})+\\gamma\\overline{{\\psi}}(s^{\\prime},a^{\\prime},{\\pmb w})\\right)-\\psi(s,a,{\\pmb w})\\right\\|^{2}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $a^{\\prime}\\,=\\,\\arg\\operatorname*{max}_{b}\\,Q(s^{\\prime},b,\\pmb{w})\\,=\\,\\arg\\operatorname*{max}_{b}\\,\\psi(s^{\\prime},b)^{\\top}\\pmb{w}$ and $\\gamma$ is the discount factor. We treat $\\overline{{\\psi}}(s^{\\prime},a^{\\prime},{\\pmb w})$ as part of the bootstrapped target: $\\hat{y}_{\\mathrm{SF}}=\\phi(s^{\\prime})+\\gamma\\overline{{{\\psi}}}(s^{\\prime},a^{\\prime},{\\pmb w})$ , which results in semigradient methods [Sutton and Barto, 2018]. Subsequently, the gradient $\\nabla_{\\psi}$ for $L_{\\mathrm{SF}}$ (Eq. 4) is defined ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\nabla_{\\psi}L_{\\mathtt{S F}}=-\\left(\\phi(s^{\\prime})+\\gamma\\overline{{\\psi}}(s^{\\prime},a^{\\prime},\\mathbf{w})-\\psi(s,a,\\mathbf{w})\\right)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Next, as previously discussed in section 3, the Q-SF-TD loss $L_{\\psi}$ which we used to learn the successor features $(\\psi)$ is defined as: ", "page_idx": 16}, {"type": "equation", "text": "$$\nL_{\\psi}=\\frac{1}{2}\\left\\|\\hat{y}-\\psi(s,a,\\pmb{w})^{\\top}\\pmb{w}\\right\\|^{2}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\hat{y}=r+\\gamma\\operatorname*{max}_{a^{\\prime}}\\overline{{\\psi}}(s^{\\prime},a^{\\prime},\\pmb{w})^{\\top}\\pmb{w}$ is the bootstrapped target. ", "page_idx": 16}, {"type": "text", "text": "(Note: Eq. 13 and the bootstrapped target $\\hat{y}$ is the same as Eq. 9 and Eq. 10 respectively, presented in Section 3 of the main text) ", "page_idx": 16}, {"type": "text", "text": "Following the same reasoning in Eq. 4, the gradient $\\nabla_{\\psi}$ for $L_{\\psi}$ is defined as: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\nabla_{\\psi}L_{\\psi}=-\\left(r+\\gamma\\overline{{\\psi}}(s^{\\prime},a^{\\prime},{\\pmb w})^{\\top}{\\pmb w}-\\psi(s,a,{\\pmb w})^{\\top}{\\pmb w}\\right){\\pmb w}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proposition 1 Optimizing $\\nabla_{\\psi}L_{\\psi}\\simeq{\\pmb w}^{\\top}\\nabla_{\\psi}L_{S F}{\\pmb w}$ , where $L_{S F}$ is the canonical loss for universal successor features [Borsa et al., 2018]. ", "page_idx": 16}, {"type": "text", "text": "Proof. Now, assuming that for any given state $s$ , the reward $r$ for state $s$ can be linearly decomposed into the dot product of the basis features $\\phi(s)$ and the task encoding vector $\\mathbf{\\nabla}w$ , as suggested by Sutton [1988], Dayan [1993], it follows that there exists an optimal set of basis features $\\phi^{*}(s)$ . This optimal set ensures that the reward $r$ can be accurately represented as the dot product of $\\phi^{*}(\\cdot)$ and the task encoding vector $\\pmb{w}$ : ", "page_idx": 16}, {"type": "equation", "text": "$$\nr=\\phi^{*}(s^{\\prime})^{\\top}{\\pmb w}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $s^{\\prime}$ is the next state. ", "page_idx": 16}, {"type": "text", "text": "Thereafter, let us recall that the reward prediction loss $L_{w}$ is defined as: ", "page_idx": 16}, {"type": "equation", "text": "$$\nL_{w}=\\frac{1}{2}\\left\\|r-\\phi(s^{\\prime})^{\\top}{\\pmb w}\\right\\|^{2}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "(Note: This equation is the same as Eq. 8 presented in Section 3 of the main text.) ", "page_idx": 16}, {"type": "text", "text": "Substituting the assumption that we made in Eq. 15 into the reward prediction loss (Eq. 16), ", "page_idx": 16}, {"type": "equation", "text": "$$\n{\\begin{array}{l}{L_{w}={\\cfrac{1}{2}}\\left(r-\\phi(s^{\\prime})^{\\top}{\\boldsymbol{w}}\\right)^{2}}\\\\ {\\qquad={\\cfrac{1}{2}}\\left(\\phi^{*}(s^{\\prime})^{\\top}{\\boldsymbol{w}}-\\phi(s^{\\prime})^{\\top}{\\boldsymbol{w}}\\right)^{2}\\qquad{\\mathrm{~(Subst.~}}r=\\phi^{*}(s^{\\prime})^{\\top}{\\boldsymbol{w}}{\\mathrm{~following~Eq.~}}15{\\mathrm{)}}}\\\\ {\\qquad={\\cfrac{1}{2}}\\left((\\phi^{*}(s^{\\prime})-\\phi(s^{\\prime}))^{\\top}{\\boldsymbol{w}}\\right)^{2}}\\\\ {\\qquad={\\cfrac{1}{2}}\\left(\\epsilon(s^{\\prime})^{\\top}{\\boldsymbol{w}}\\right)^{2}}\\end{array}}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Where $\\epsilon(s^{\\prime})$ is the difference between $\\phi^{*}(s^{\\prime})$ and $\\phi(s^{\\prime})$ . Furthermore, if $L_{w}\\simeq0$ , then $\\epsilon(s^{\\prime})^{\\top}w=$ $w^{\\top}\\epsilon(s^{\\prime})\\simeq0$ . ", "page_idx": 16}, {"type": "text", "text": "Shifting our focus back to the gradient $\\nabla_{\\psi}L_{\\psi}$ of our Q-SF-TD loss function (Eq. 14), ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{\\psi}L_{\\psi}=-\\left(r+\\gamma\\overline{{\\psi}}(s^{\\prime},a^{\\prime},w)^{\\top}w-\\psi(s,a,w)^{\\top}w\\right)w}\\\\ &{\\quad\\quad=-w^{\\top}(\\phi^{*}(s^{\\prime})+\\gamma\\overline{{\\psi}}(s^{\\prime},a^{\\prime},w)-\\psi(s,a,w))w}\\\\ &{\\quad\\quad=-w^{\\top}(\\phi(s^{\\prime})+\\epsilon(s^{\\prime})+\\gamma\\overline{{\\psi}}(s^{\\prime},a^{\\prime},w)-\\psi(s,a,w))w}\\\\ &{\\quad\\quad=-w^{\\top}\\left(-\\nabla_{\\psi}L_{\\mathsf{S F}}+\\epsilon(s^{\\prime})\\right)w}\\\\ &{\\quad\\quad=w^{\\top}\\nabla_{\\psi}L_{\\mathsf{S F}}w-w^{\\top}\\epsilon(s^{\\prime})w}\\\\ &{\\quad\\quad=w^{\\top}\\nabla_{\\psi}L_{\\mathsf{S F}}w-2\\sqrt{L_{w}}w}\\\\ &{\\quad\\quad\\simeq w^{\\top}\\nabla_{\\psi}L_{\\mathsf{S F}}w}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "(Eq. 14) (Subst. $r=\\phi^{*}(s^{\\prime})^{\\top}{\\pmb w}$ following Eq. 15) (Subst. $\\phi^{*}(s^{\\prime})=\\phi(s^{\\prime})+\\epsilon(s^{\\prime})$ from Eq. 17) (Subst. definition from Eq. 12) $(\\mathrm{Subst}.w^{\\top}\\epsilon(s^{\\prime})=2\\sqrt{L_{w}}$ from Eq. 17) \u21e4 (18) ", "page_idx": 16}, {"type": "text", "text": "In conclusion, this proof demonstrates that the gradients $\\nabla_{\\psi}L_{\\psi}$ computed using our proposed QSF-TD loss function (Eq. 13) effectively project the gradients $\\dot{\\nabla}_{\\psi}L_{\\mathrm{SF}}$ from the canonical universal SF-TD loss function (Eq. 12) onto the task encoding vector $\\pmb{w}$ . This indicates that our loss function maintains the essential characteristics of the canonical form while aligning closely with the specific direction of the task encoding vector $\\mathbf{\\nabla}w$ . ", "page_idx": 17}, {"type": "text", "text": "D Environments ", "text_level": 1, "page_idx": 17}, {"type": "image", "img_path": "rI7oZj1WMc/tmp/e6a93cd6b04762c6866adac0f8290bda2b49822c9188c1880ded82c358acfc03.jpg", "img_caption": ["Figure 9: Overview of 2D Minigrid, 3D Four Rooms environments and Mujoco used in our studies with changing dynamics and rewards. Both 2D Minigrid and 3D Four Rooms environments utilize discrete actions while Mujoco utilizes continuous actions. All studies in this paper were done using only pixel observations. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "E Experimental details ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In this section, we provide more details about the environments used in our experiments. ", "page_idx": 18}, {"type": "text", "text": "E.1 2D Gridworld Environments. ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "The specific parameters defining the 2D Gridworld environments are detailed in Table 1. ", "page_idx": 18}, {"type": "text", "text": "Table 1: 2D Minigrid Environment Specific Parameters ", "page_idx": 18}, {"type": "table", "img_path": "rI7oZj1WMc/tmp/b26c93ceafdd48d1dfc7ec9b589cf5d9a57b97090e2ad7ab8e100b97a876644f.jpg", "table_caption": [], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "E.1.1 Center-Wall environment ", "text_level": 1, "page_idx": 18}, {"type": "image", "img_path": "rI7oZj1WMc/tmp/822f75862de8ca6ce8737ce3a1551c70bc8fe8b987ec324ba287ed7376595492.jpg", "img_caption": ["Figure 10: Center-Wall environment and Geospatial Color Mapping "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "In the Center-Wall environment, a vertical wall splits the area into two distinct regions. Task 1 features a passage from the left to the right side at the bottom, with the goal state located in the top left corner Figure 10a). In Task 2, the layout is modified: the passage is moved to the top, while the goal state is relocated to the bottom right corner Figure 10b). These changes are strategically implemented to evaluate the agents\u2019 ability to adapt to simultaneous alterations in both the environmental structure and the goal location. To aid in visual analysis, we use a geospatial color mapping initially developed for Task 1 (Figure 10c). This mapping effectively illustrates the spatial positioning within the environment and is particularly useful in the 2D visualization of the Successor Features and DQN Representations, providing a clearer understanding of how agents interpret and navigate the modified environment (Figures 6, 29 and 30). ", "page_idx": 18}, {"type": "text", "text": "E.1.2 Inverted-Lwalls environment ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In the Inverted-Lwalls environment, we placed two L-shaped walls within the environment, one on the left and the other on the right, creating a unique layout. This design results in a single, central path acting as a bottleneck, which the agent must navigate to reach the goal states. Specifically, to access the goal state located on the left side of the environment, the agent is required to traverse this central path while facing north (Figure 11a). Conversely, to reach the goal state situated on the right, the agent must navigate the same path but facing south (Figure 11b). This layout ensures that the agent consistently encounters and must maneuver this bottleneck area, regardless of the goal state\u2019s location. ", "page_idx": 18}, {"type": "image", "img_path": "rI7oZj1WMc/tmp/0af47ef4fcbb70a7db3785fc27ef5d4a0b40124d3a852f3782e115c412d1833d.jpg", "img_caption": ["Figure 11: Inverted-Lwalls environment "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "E.2 3D Miniworld Environments. ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "The actions in this environment consists of moving Forward and Backwards, turning Left and Right.   \nThe specific parameters defining the 3D Miniworld environments are detailed in Table 2. ", "page_idx": 19}, {"type": "table", "img_path": "rI7oZj1WMc/tmp/a9548bfcb613b089a227b70006c3df28cabdcc9bdeff0c6c56f3494cf79890ac.jpg", "table_caption": ["Table 2: 3D Miniworld Four Rooms Environment Specific Parameters "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "E.2.1 Four Rooms environment ", "text_level": 1, "page_idx": 19}, {"type": "image", "img_path": "rI7oZj1WMc/tmp/abdedd39a4ab294eccce1971454c9a329dd1b9de02d198a7f473ba434e5d203b.jpg", "img_caption": ["Figure 12: Four Rooms (3D) "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "The Four Rooms environment consists of four identical square rooms arranged in a $2\\mathtt{x}2$ grid, with passages connecting the rooms and allowing an agent to move between the rooms (Figure 12a). Each room in our 3D environment is designed with unique textures, a deliberate choice to reduce the complexity associated with localization ambiguities often encountered in more uniform settings. This variation in textures aids the agent in distinguishing between rooms based solely on visual cues, thereby simulating more realistic navigation scenarios. This setup also allows us to observe how visual diversity impacts the agent\u2019s ability to infer its location and navigate to specific goals, providing insights into the interplay between environmental features and SFs learning in a 3D spatial context. Depending on the task, the agent receives a reward of either $+1$ or -1 when it reaches the yellow or green box. Similar to the Center-Wall environment, we also create a geospatial color mapping for the 2D visualization of the Successor Features and DQN Representations (Figure 12b). ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "E.2.2 Slippery Four Rooms environment ", "text_level": 1, "page_idx": 20}, {"type": "image", "img_path": "rI7oZj1WMc/tmp/d3fd3cd68b35efec3676a414956708d785c3c64c89aa34f380049381d7fd207d.jpg", "img_caption": ["Figure 13: Slippery Four Rooms (3D) layout "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "In the slippery variant of the Four Rooms environment, our goal is to rigorously test the robustness of agents in learning SFs under challenging conditions. Specifically, in the top right and bottom left rooms of this setup, the agent experiences a \u2019slippery\u2019 dynamic: chosen actions have a certain probability of being replaced with random, alternative actions. This design mimics the effects of a low-friction or slippery surface, creating a scenario where the agent\u2019s intended movements might lead to unpredictable outcomes. Such a setup is instrumental in assessing the agent\u2019s adaptability and the robustness of SF learning in the face of environmental unpredictability. This variant not only challenges the agent to adapt to unexpected changes but also provides valuable insights into the flexibility and resilience of the SFs when navigating environments where control and predictability are compromised. ", "page_idx": 20}, {"type": "text", "text": "E.3 Mujoco ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In this work, we only utilised pixels inputs from Mujoco since our focus is on learning SFs directly from pixel observations. For domains, we chose both walker and half-cheetah. We broadly follow the same setup as Yarats et al. [2021], and included their model as a baseline, which we denote as \"DDPG\" in our results (Figure 4). ", "page_idx": 20}, {"type": "text", "text": "The codebase from their model is provided in the Unsupervised Reinforcement Learning (URL) Benchmark repository[Laskin et al., $2021]^{6}$ , which we further described in the APS Agent in section F. The specific parameters we used for training in the Mujoco environment are detailed in Table 3. ", "page_idx": 20}, {"type": "table", "img_path": "rI7oZj1WMc/tmp/3ed7727178e14227e585d0e819f07e707290aa219c1432214d415686f6ec5d80.jpg", "table_caption": ["Table 3: Mujoco Environment Specific Parameters "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "F Agents ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In this section, we describe how we create our agent as well as the ones we used for comparisons. In addition, we provide the mathematical definitions of the constraints used on the basis features. For all agents, we swept the learning rates for both the SF network and the task encoding (specific for all SFs agents) using a gridsearch. The values ranged from 1e-1 to 1e-6, and the process was repeated using 5 random seeds in both 2D Gridworld and 3D Four Rooms environments. The same was also applied to the Double DQN agent [Van Hasselt et al., 2016] and we took extra care to ensure that the architecture and its number of parameters were as similar as possible to our model. Detailed hyperparameters for learning SFs and the task encoding $w$ for our agent are outlined in Tables 4 and 5. ", "page_idx": 21}, {"type": "text", "text": "F.1 APS Agent ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In our study, we take inspiration from the neural network architecture from Liu and Abbeel [2021] from the Unsupervised Reinforcement Learning (URL) Benchmark repository[Laskin et al., $2021]^{7}$ , which utilizes PyTorch [Paszke et al., 2019]. This repository was chosen for its robust implementation and served as the foundation for all SF-variant agents, including ours. Within the URL Benchmark, the encoder follows the Deep Deterministic Policy Gradient (DDPG) network architecture [Lillicrap et al., 2015]. Notably, there is a discrepancy in the network architecture hyperparameters between the APS paper [Liu and Abbeel, 2021]) and the URL Benchmark repository. Given the practical implications of these differences, our implementation aligns with the hyperparameters specified in the URL Benchmark. ", "page_idx": 21}, {"type": "text", "text": "In line with the URL Benchmark\u2019s methodology, we initially employed the least squares method to determine the optimal task encoding $w$ . However, we observed that this analytical approach was excessively sensitive in our experimental context, particularly due to its reliance on the mini-batch samples. This sensitivity was especially pronounced in environments with sparse rewards, like those in our study, suggesting that the least squares method might be less suited for such settings. This challenge was not present in the original APS framework [Liu and Abbeel, 2021], which was structured around distinct pre-training and fine-tuning phases. In contrast, our research focuses exclusively on continuous online learning, introducing unique challenges and dynamics not addressed in the APS paper [Liu and Abbeel, 2021]. ", "page_idx": 21}, {"type": "text", "text": "F.2 Reconstruction constraints ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "At each time step $t$ , the basis features $\\phi(S_{t})$ are generated from the current state $S_{t}$ using an encoder. Together with the action $A_{t}$ , these features are fed into a reconstruction decoder to predict the next state $\\hat{S}_{t+1}$ . Both the encoder and decoder are optimized using the reconstruction loss: ", "page_idx": 21}, {"type": "equation", "text": "$$\nL_{r e c o n}=||S_{t+1}-\\hat{S}_{t+1}||^{2}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $S_{t+1}$ is the ground truth of the next state. The same set of basis features $\\phi$ is also utilized in optimizing the Reward Prediction Loss (Eq. 8) and the Q-SF-TD Loss (Eq. 9). ", "page_idx": 21}, {"type": "text", "text": "F.3 Orthogonality constraints ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "At each time step $t$ , the basis features $\\phi$ are generated from the current state $S_{t}$ using an encoder. Besides being utilized to optimize the Reward Prediction Loss (Eq. 8) and the Q-SF-TD Loss (Eq. 9), the basis features $\\phi$ are also optimized with the orthogonality loss [Koren, 2003, Mahadevan and Maggioni, 2007, Machado et al., 2017b,a]: ", "page_idx": 21}, {"type": "equation", "text": "$$\nL_{o r t}=\\mathbb{E}_{(S_{t},S_{t+1})\\sim\\mathcal{D}}\\left[\\left\\|\\phi\\left(S_{t}\\right)-\\phi\\left(S_{t+1}\\right)\\right\\|^{2}\\right]+\\lambda\\operatorname{\\mathbb{E}}_{s^{\\sim}\\sim\\mathcal{D}}^{2}\\left[\\left(\\phi(s)^{\\top}\\phi\\left(s^{\\prime}\\right)\\right)^{2}-\\left\\|\\phi(s)\\right\\|^{2}-\\left\\|\\phi\\left(s^{\\prime}\\right)\\right\\|^{2}\\right]\n$$", "text_format": "latex", "page_idx": 21}, {"type": "table", "img_path": "rI7oZj1WMc/tmp/08f391b1496f32f8ec8815ed9fc5df7f58f661cc1a3612c09ba17d85f93db8b8.jpg", "table_caption": [], "table_footnote": [], "page_idx": 22}, {"type": "table", "img_path": "rI7oZj1WMc/tmp/fadb00542c53b0d4ad7072eedff5f3c95d856938433e384ca960103da7c32d5a.jpg", "table_caption": ["Table 5: Task $w$ encoding Hyperparameters "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "where states $s$ and $s^{\\prime}$ are two different states sampled from the replay buffer $\\mathcal{D}$ . The first term encourages the basis features $\\phi(S_{t})$ and $\\phi(S_{t+1})$ to be similar and the second term promotes orthogonality by ensuring that the basis features of the different states $\\phi(s)$ and $\\phi(s^{\\prime})$ are distinct and decorrelated. Following Touati et al. [2022], we set the regularization factor $\\lambda=1$ . ", "page_idx": 22}, {"type": "text", "text": "F.4 Random constraints ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In this agent, the basis features $\\phi$ are constrained to be unlearnable random features, which are defined during initialization. The SFs $\\psi$ are subsequently learned on top of these predefined basis features. To guarantee that the basis features $\\phi$ remain unlearnable throughout the training process, a stop gradient operator is employed. ", "page_idx": 22}, {"type": "text", "text": "F.5 Learning SFs through integrating all losses ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "This agent learns Successor Features using a complex learning strategy that integrates three distinct losses: the SF-TD loss (Eq. 4), the reward prediction loss (Eq. 8) and the Q-SF-TD loss (Eq. 9). This multifaceted approach, proposed by Janz et al. [2019] aims to ensure that the learnt SFs satisfy all desired constraints. ", "page_idx": 22}, {"type": "text", "text": "G Our Architecture for Continuous Control ", "text_level": 1, "page_idx": 23}, {"type": "image", "img_path": "rI7oZj1WMc/tmp/45c1dafc6034e2d6587f8344817ea20d2ce32a5f229e1c6f8322b67bcc063f49.jpg", "img_caption": [], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "Figure 14: Our model adapted for continuous action spaces, based on the Actor-Critic architecture commonly used in DDPG [Lillicrap et al., 2015] and implemented following the URL benchmark [Laskin et al., 2021]. This design modifies our original architecture to accommodate continuous action environments, enabling the model to handle a broader range of control tasks. The model incorporates a linear decomposition of Successor Features $\\psi$ and the task encoding vector $\\pmb{w}$ to compute Q-value, following Eq.1. Following the DDPG implementation in URL benchmark, actions are sampled from a truncated normal distribution, and LayerNorms are applied to normalize inputs to a unit distribution. ", "page_idx": 23}, {"type": "text", "text": "H Models of Previous Approaches ", "text_level": 1, "page_idx": 24}, {"type": "image", "img_path": "rI7oZj1WMc/tmp/7a7a745c811a8beac696e919d5a4a84d49a69013cbe8445c61cff8734fd87df3.jpg", "img_caption": [], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "Figure 15: In order to prevent representation collapse in the basis features $\\phi$ , previous methods on learning SFs from pixel observations often relied on an additional loss, such as reconstructing the state of the next time step $\\hat{S}_{t+1}$ after executing action $A_{t}$ [Machado et al., 2020]. Recent approaches in learning SFs include encouraging orthogonal representations in the basis features [Touati et al., 2022]. A stop gradient operator is also used to prevent the SFs from updating the basis features $\\phi$ when optimizing the SF-TD loss. [Kulkarni et al., 2016] ", "page_idx": 24}, {"type": "text", "text": "I Impact of Learning Rate Variations on Task Encoding Vector ", "text_level": 1, "page_idx": 25}, {"type": "image", "img_path": "rI7oZj1WMc/tmp/17a7f9d8fc46e84ee25099feb6da288b769dee445a227d81a00a20252c981929.jpg", "img_caption": [], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "Figure 16: Comparison of learning rates for the task encoding vector in grid worlds and 3D Four Rooms environments. Generally, a lower learning rate is required for the task encoding vector, despite its use of a simple reward prediction loss (Eq. 8), compared to the SF network, which needs more steps to converge due to its involvement in capturing complex environmental dynamics. ", "page_idx": 25}, {"type": "text", "text": "J Further Experimental Results ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "In this section, we present expanded illustrations of the results initially introduced in the main paper. These larger visual figures provide a clearer and more detailed view to enhance the reader\u2019s understanding of our findings. Additionally, we include additional supplementary experimental results that were not featured in the main paper due to space limitations. ", "page_idx": 26}, {"type": "text", "text": "J.1 Single task results for 2D Minigrid and 3D Four Room environment ", "text_level": 1, "page_idx": 26}, {"type": "image", "img_path": "rI7oZj1WMc/tmp/33a39f2143bff477e1aa4c85a68416350702b734d7af6e2107868e32287d34bc.jpg", "img_caption": [], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "Figure 17: Performance of agents trained on a single task in both 2D Minigrid and 3D Four Rooms environments across 5 random seeds. The Y-axis represents the moving average of the average episode returns. Our model, Simple SF (orange), performs comparably to DQN (blue), even though it learns two functions\u2014Successor Features (SFs) and the task encoding vector\u2014while DQN only learns a single function, the Q-value. ", "page_idx": 26}, {"type": "text", "text": "J.2 Continual RL results for Inverted-LWalls environment ", "text_level": 1, "page_idx": 26}, {"type": "image", "img_path": "rI7oZj1WMc/tmp/c5766842039bde409c859c78b9ca7ac4a0914063e3bbe33270d61f81859da896.jpg", "img_caption": [], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "Figure 18: Evaluation in a Continual Reinforcement Learning setting across 5 random seeds, without replay buffer resets at each task transition in the Inverted L-Walls environment. Here, the goal location alternates between the left and right sides with each task change, while the environment dynamics remain constant. (a) In the partially-observable scenario, our agent demonstrates a faster re-learning ability for new tasks compared to other agents. (b) In the fully-observable scenario, while our agent shows performance comparable to the DQN agent, it is slightly outperformed by the agent employing SFs with orthogonality constraints on its basis features. Notably, despite the superior performance of this latter agent in later tasks during Exposure 2, it initially faces difficulties in developing an effective policy, attributed to the added complexity of adhering to orthogonality constraints. ", "page_idx": 26}, {"type": "image", "img_path": "rI7oZj1WMc/tmp/f8cfa44b63720e5b74749ccff59e9f3d20cc49424bf4a84597c6b9d5ec797059.jpg", "img_caption": ["J.3 Continual RL results for Center-Wall environment "], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "Figure 19: Evaluation in a Continual Reinforcement Learning setting across 5 random seeds, without replay buffer resets at each task transition in the Center-Wall environment. In this setup, both the goal location and environment dynamics change with each task switch. (a) In the partiallyobservable scenario, our agent demonstrates performance comparable to that of other agents. (b) In the fully-observable scenario, our agent outperformed all others, with the agent employing SFs with orthogonality constraints on its basis features coming in as a close second. Notably, while this latter agent shows improved performance in later tasks of Exposure 2, it initially encounters difficulties in developing an effective policy, which can be attributed to the added complexity of adhering to orthogonality constraints. ", "page_idx": 27}, {"type": "image", "img_path": "rI7oZj1WMc/tmp/e399368cf516c6a1a452bb706360e19fc040029d44826ac59abb8ac1e1b3c1b9.jpg", "img_caption": [], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "Figure 20: Evaluation in a Continual Reinforcement Learning setting across 5 random seeds, without replay buffer resets at each task transition in the 3D Four Rooms environment. ", "page_idx": 28}, {"type": "image", "img_path": "rI7oZj1WMc/tmp/ca80122c6f130578eba4f4d91fdbbfa4b649980998a310f021fc1899af64c6d1.jpg", "img_caption": ["J.5 Continual RL results for Slippery Four Rooms environment ", "Figure 21: Evaluation in a slippery Four-Rooms environment with varied slipperiness probabilities, without replay buffer resets at each task transition. This environment features slippery conditions in the top-right and bottom-left rooms for both tasks, Task 1 and Task 2. Both tasks have differing reward structures: In Task 1, rewards are set at $+1$ for the green box and -1 for the yellow box; in Task 2, this reward scheme is reversed (green box: $^-1$ , yellow box: $+1$ ). The diagram illustrates the layout of the environment can be found in Figure 9. Note: The APS Pre-trained agent was tested only at a slippery probability of 0.15; higher probabilities were not evaluated due to performance decline beyond Task 1 of Exposure 1 when the slippery probability is 0.15. "], "img_footnote": [], "page_idx": 29}, {"type": "image", "img_path": "rI7oZj1WMc/tmp/446ff439a26834a92070fe2892a2db943916ce5cfd5b55f9ab3a989b7ab74be0.jpg", "img_caption": ["Figure 22: Evaluation in a Continual Reinforcement Learning setting across 5 random seeds in the Four-Rooms Environment with environmental changes, without replay buffer resets at each task transition. Task 1 adheres to the canonical Four Rooms environment dynamics, while Task 2 employs the slippery variant, where chosen actions are altered based on the slippery probability to simulate environmental changes. Throughout both tasks, reward associations remain consistent: $+1$ for the green box and -1 for the yellow box. The layout of this environment is depicted in Figure 9. Note: The APS Pre-trained agent was tested only at a slippery probability of 0.15; higher probabilities were not evaluated due to performance decline beyond Task 1 of Exposure 1 when the slippery probability is 0.15. "], "img_footnote": [], "page_idx": 30}, {"type": "image", "img_path": "rI7oZj1WMc/tmp/c9fde879204fd74bf194ac0fdc93ddea114b4419636c1bfa0e3d3965937c40d3.jpg", "img_caption": ["J.6 Continual RL results for 2D Minigrid and 3D Four Rooms environment with Replay resets "], "img_footnote": [], "page_idx": 31}, {"type": "text", "text": "Figure 23: Continual Reinforcement Learning Evaluation with pixel observations in 2D Minigrid and 3D Four Rooms enviroment. Replay buffer resets at each task transitions to simulate drastic distribution shifts: Agents face two sequential tasks (Task 1 & Task 2), each repeated twice (Exposure 1 & Exposure 2). Moving average episode returns using most recent episodes in both egocentric and allocentric 2D Minigrid environments and egocentric 3D Four Rooms environment. ", "page_idx": 31}, {"type": "text", "text": "K Experimental results of $\\mathbf{S}\\mathbf{F}+\\mathbf{Q}\\mathbf{-}\\mathbf{T}\\mathbf{D}+\\mathbf{R}$ eward vs SF Simple (Ours) ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "In this section, we present the experimental results of our agent (SF Simple) and the agent which optimizes the three losses $\\mathrm{(SF+Q-TD+R}$ eward) simultaneously. For more information about this agent, see section F.5. ", "page_idx": 32}, {"type": "text", "text": "K.1 Continual RL results for Inverted-LWalls environment ", "text_level": 1, "page_idx": 32}, {"type": "image", "img_path": "rI7oZj1WMc/tmp/57b55005241769cd67bf01634db452ce5dd35609140501fa5c136f6165dc1c4a.jpg", "img_caption": [], "img_footnote": [], "page_idx": 32}, {"type": "text", "text": "Figure 24: Evaluation in a Continual Reinforcement Learning setting across 5 random seeds, with replay buffer resets at each task transition in the Inverted L-Walls environment. Here, the goal location alternates between the left and right sides with each task change, while the environment dynamics remain constant. (a & b) In both partially-observable and fully-observable scenario, the agent, which optimizes three losses simultaneously $\\mathrm{(SF+Q{\\mathrm{-}}T D+R e w a r d)}$ , experiences learning instabilities due to the higher complexity involved in managing all constraints. ", "page_idx": 32}, {"type": "image", "img_path": "rI7oZj1WMc/tmp/f3b033bf9250d50201a2be4c2cd3f706942f7d9769ff100bf8fd3f7a8cde9faa.jpg", "img_caption": ["K.2 Continual RL results for Center-Wall environment "], "img_footnote": [], "page_idx": 33}, {"type": "text", "text": "Figure 25: Evaluation in a Continual Reinforcement Learning setting across 5 random seeds, with replay buffer resets at each task transition in the Center-Wall environment. In this setup, both the goal location and environment dynamics change with each task switch. (a & b) In both partiallyobservable and fully-observable scenario, the agent, which optimizes three losses simultaneously $\\mathrm{(SF+Q-TD+Reward)}$ , experiences learning instabilities due to the higher complexity involved in managing all constraints. ", "page_idx": 33}, {"type": "image", "img_path": "rI7oZj1WMc/tmp/25e3b25d8c113c28dd91eb74cb68e292d99c834fdb895f5e6a0df43a7dd9dd03.jpg", "img_caption": ["K.3 Continual RL results for Four Rooms environment "], "img_footnote": [], "page_idx": 34}, {"type": "text", "text": "Figure 26: Evaluation in a Continual Reinforcement Learning setting across 5 random seeds, with replay buffer resets at each task transition in the 3D Four Rooms environment. Simultaneous optimization of three losses $\\mathrm{(SF+Q-TD+R}$ eward) slows the learning process, as the agent requires more time to learn an effective policy. ", "page_idx": 34}, {"type": "image", "img_path": "rI7oZj1WMc/tmp/63e4704bfc8506a45ecbb3e84e494d23738794b03761bc705c538ca54c6e2e77.jpg", "img_caption": ["K.4 Continual RL results for Slippery Four Rooms environment ", "Figure 27: Evaluation in a Continual Reinforcement Learning setting across 5 random seeds, with replay buffer resets at each task transition in the 3D Four Rooms environment. This environment features slippery conditions in the top-right and bottom-left rooms for both tasks, Task 1 and Task 2. Both tasks have differing reward structures: In Task 1, rewards are set at $+1$ for the green box and $^{-1}$ for the yellow box; in Task 2, this reward scheme is reversed (green box: $^{-1}$ , yellow box: $+1\\rangle$ ). The diagram illustrates the layout of the environment can be found in Figure 9. As observed, simultaneous optimization of three losses $\\mathrm{(SF+Q-TD+R}$ eward) significantly impedes the agent\u2019s ability to learn effectively in a stochastic environment. "], "img_footnote": [], "page_idx": 35}, {"type": "image", "img_path": "rI7oZj1WMc/tmp/48eeb0c04d09ee5c4aab0377dadb360648166e0dad628f9ad962c352b118c00f.jpg", "img_caption": ["Figure 28: Evaluation in a Continual Reinforcement Learning setting across 5 random seeds, with replay buffer resets at each task transition in the 3D Four Rooms environment. Task 1 adheres to the canonical Four Rooms environment dynamics, while Task 2 employs the slippery variant, where chosen actions are altered based on the slippery probability to simulate environmental changes. Throughout both tasks, reward associations remain consistent: $+1$ for the green box and -1 for the yellow box. The layout of this environment is depicted in Figure 9. Once again, simultaneous optimization of three losses $\\mathrm{(SF+Q-TD+R}$ eward) significantly impedes the agent\u2019s ability to learn effectively. "], "img_footnote": [], "page_idx": 36}, {"type": "text", "text": "L Implementation Details ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "For our experimental setup, we utilized Python 3 [Van Rossum and Drake, 2009] as the primary programming language. The agent creation and computational components were developed using Jax [Bradbury et al., 2018, Godwin\\* et al., 2020], while Haiku [Hennigan et al., 2020] was employed for implementing the neural network components. For data visualization, we used Matplotlib [Hunter, 2007] and Seaborn [Waskom, 2021] to generate line plots. Additionally, we utilized Plotly8 for creating the violin plots and heat maps used in our correlation analysis, as well as the 2D visualizations of the SFs and DQN Representations. We utilized Scikit-learn [Pedregosa et al., 2011] in our correlation analysis studies as well as the open-source Uniform Manifold Approximation and Projection (UMAP) tool [McInnes et al., 2018] to generate the 2D embeddings of the SFs. The configuration and management of our experiments were facilitated by Hydra [Yadan, 2019] and Weights & Biases [Biewald, 2020]. All experiments, particularly those in the continual learning setting, were conducted using Nvidia V100 GPUs and completed within a maximum of one day. The code used in the study will be released in the near future, following an internal review process. ", "page_idx": 36}, {"type": "text", "text": "M Visualizations of Successor Features ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Given that Successor Features (SFs) are action-dependent, and considering the space constraints in the main paper, our visualizations here are more comprehensive. In the main paper, we primarily showcased visualizations for the forward action due to these limitations. However, in this section, we expand our focus to include visual representations for a variety of actions, providing a more holistic view of the SFs\u2019 behavior and their influence across different action scenarios. This expanded visualization not only enhances our understanding of the SFs\u2019 multidimensional nature but also offers deeper insights into the agent\u2019s decision-making process and its interaction with the environment. ", "page_idx": 37}, {"type": "table", "img_path": "rI7oZj1WMc/tmp/1fe9455c9157261c69d279b34ac6b19064b07c9e1d53a508dead85b142ff6743.jpg", "table_caption": ["M.1 Center-wall Environment (Fully-observable) "], "table_footnote": [], "page_idx": 38}, {"type": "text", "text": "Figure 29: 2D Geospatial Color-Mapped Visualizations of initial and action-based Successor Features in the Fully-Observable Center-Wall Environment. This figure displays the successor features of various RL agents, each panel representing a different agent and action.The first column illustrates the initial state of successor features before training, using geospatial color mapping for clear visualization. Subsequent columns correspond to successor features developed for specific actions: Forward, Turn Left, and Turn Right, also visualized using geospatial color mapping. In this scenario, only the agent learning SFs with orthogonality constraints as well as our agent (Simple SF) learned well-clustered representations after training. It\u2019s crucial to recognize, however, that while clustered representations may suggest effective learning, they do not automatically equate to successful policy development. These visualizations highlight the varied encoding strategies of agents in response to full observability and different actions. ", "page_idx": 38}, {"type": "table", "img_path": "rI7oZj1WMc/tmp/32d3dedad8ef884338b5ca2a80dde403a9fc0d3bf3268a41c293cdd387d22805.jpg", "table_caption": ["M.2 Center-wall Environment (Partially-observable) "], "table_footnote": [], "page_idx": 39}, {"type": "text", "text": "Figure 30: 2D Geospatial Color-Mapped Visualizations of initial and action-based Successor Features in the Partially-Observable Center-Wall Environment. This figure displays the successor features of various RL agents, each panel representing a different agent and action. The first column illustrates the initial state of successor features before training, using geospatial color mapping for clear visualization. Subsequent columns correspond to successor features developed for specific actions: Forward, Turn Left, and Turn Right, also visualized using geospatial color mapping. Some agents demonstrate well-clustered representations after training, which typically correlates with improved performance compared to agents with more dispersed or noisy features. It\u2019s crucial to recognize, however, that while clustered, color-mapped representations may suggest effective learning, they do not automatically equate to successful policy development. These visualizations highlight the varied encoding strategies of agents in response to partial observability and different actions. ", "page_idx": 39}, {"type": "image", "img_path": "rI7oZj1WMc/tmp/d72d43c2cfa216c0ba0b511e06bb0c9650e2da7a0bf9d44ebc26aab66b470df8.jpg", "img_caption": ["M.3 Four Rooms Environment "], "img_footnote": [], "page_idx": 40}, {"type": "text", "text": "Figure 31: 2D Geospatial Color-Mapped Visualizations of initial and action-based Successor Features in the 3D Four Rooms Environment. Agents operate with solely egocentric observations. Each panel represents the successor features of a different RL agent and action. The first column, using red, green, blue, and yellow to distinguish the four rooms, shows the initial state of successor features pretraining. Subsequent columns depict features for specific actions: Move Forward, Move Backwards, Turn Left, and Turn Right. Except for SF learned using unlearnable random basis features, most agents exhibit well-clustered representations post-training. However, it\u2019s important to note that such clustered, color-mapped representations, while indicative of effective learning, do not necessarily translate into successful policy development. ", "page_idx": 40}, {"type": "text", "text": "N Correlation Analysis ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Considering that the SRs are not normally distributed [Stachenfeld et al., 2017], we conduct our correlation analysis in the Grid world environments (Figure 9a and b) using the Spearman\u2019s rank correlation. The SRs were analytically computed using the transition matrix $T$ where $T(s^{\\prime}\\mid s,a)$ denotes the probability of transitioning from state $s$ to state $s^{\\prime}$ given an action $a\\sim\\pi(\\cdot\\mid s)$ : ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\mathbf{S}\\mathbf{R}=(I-\\gamma T)^{-1}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "where $0\\leq\\gamma<1$ is the discount factor and $I$ is the identity matrix. The same policy $\\pi$ was used to generate the transition matrix $T$ and to adjust the final correlations. These adjustments account for less frequently chosen actions and for positions and head directions less likely to be encountered by the agent, as outlined in the main text. Statistics regarding positions and head directions were collected using policy $\\pi$ . ", "page_idx": 41}, {"type": "text", "text": "In the remaining part of this section, we provide additional detailed violin plots to depict the correlation dynamics in both the Center-wall and Inverted-LWalls environments, covering scenarios that are both partially-observable and fully-observable. These plots are segmented into different stages: before training, after training, and the differences post-training. This segmentation offers a comprehensive view of the agents\u2019 learning progression over time. Specifically for the InvertedLWalls environment, a table is included to provide a summary of mean and standard deviation statistics for these correlations, thus offering a clear quantitative perspective of our findings. Additionally, we present heatmaps that showcase the correlation at each spatial position in the environment for various SF agents. These heatmaps further enrich our analysis by visually representing the spatial distribution of correlation values, highlighting how different agents adapt to the environment. ", "page_idx": 41}, {"type": "image", "img_path": "rI7oZj1WMc/tmp/2ca1f03e213434c26e41e26df5c5b8cbc34836e3d8cf53407538de973ee4f5de.jpg", "img_caption": ["N.1 Center-wall Environment (Partially-observable) "], "img_footnote": [], "page_idx": 42}, {"type": "text", "text": "Figure 32: Correlation analysis between learned Successor Features and analytically computed Successor Representation for all positions in the Center-Wall Environment under the Partiallyobservable scenario. ", "page_idx": 42}, {"type": "image", "img_path": "rI7oZj1WMc/tmp/0c16d9d9c1d3122db5058f74d8617f8fe6522d3f6a129cb362333a4b640f70fa.jpg", "img_caption": ["N.2 Center-wall Environment (Fully-observable) "], "img_footnote": [], "page_idx": 43}, {"type": "text", "text": "Figure 33: Correlation Analysis between Successor Features and Successor Representation for all positions in the Center-Wall Environment (Fully-observable). ", "page_idx": 43}, {"type": "image", "img_path": "rI7oZj1WMc/tmp/3f9f4d7f68ebf05aa1ca17a246e8c2a4d550b3ab6a3d9071bee5524f41e00e89.jpg", "img_caption": ["N.3 Inverted-LWalls Environment (Partially-observable) "], "img_footnote": [], "page_idx": 44}, {"type": "text", "text": "Figure 34: Correlation Analysis between Successor Features and Successor Representation for all positions in the Inverted-LWalls-Grid Environment (Partially-observable). ", "page_idx": 44}, {"type": "image", "img_path": "rI7oZj1WMc/tmp/bd544b2e59a622c715a0ee8543522023c58e4196763fc9fc780291733b30c2eb.jpg", "img_caption": ["N.4 Inverted-LWalls Environment (Fully-observable) "], "img_footnote": [], "page_idx": 45}, {"type": "text", "text": "Figure 35: Correlation Analysis between Successor Features and Successor Representation for all positions in the Inverted-LWalls-Grid Environment (Fully-observable). ", "page_idx": 45}, {"type": "text", "text": "N.5 Summary Statistics of the Correlation Analysis ", "text_level": 1, "page_idx": 46}, {"type": "table", "img_path": "rI7oZj1WMc/tmp/2e74eca40d3864b1648499a53b9fc3b91051eeefad3ea343342e0266bc6e7dc5.jpg", "table_caption": ["Table 6: Correlation Analysis against analytically computed Successor Representation in the CenterWall Environment with mean and standard deviation of the correlations. The data are categorized into three stages: before training, after training, and the observed differences post-training. The left column: Partially-observable scenarios in which our agent shows the highest correlation and greatest improvement post-training. The right column: Fully-observable scenarios where our agent and the agent with orthogonality constraints on basis features exhibit high correlation and significant post-training improvement. "], "table_footnote": [], "page_idx": 46}, {"type": "text", "text": "Table 7: Correlation Analysis against analytically computed Successor Representation in the InvertedLWalls-Grid Environment with mean and standard deviation of the correlations. The data are categorized into three stages: before training, after training, and the observed differences posttraining. Notably, our agent demonstrated the largest improvement in correlation as well as the highest resulting correlation after the training period in both Partially-observable scenarios (left) and Fully-observable scenarios (right). ", "page_idx": 46}, {"type": "table", "img_path": "rI7oZj1WMc/tmp/7c01370251bbd9bd31b3013d7b877f870afe2b9e556cef135e73cbfd7ca82083.jpg", "table_caption": [], "table_footnote": [], "page_idx": 46}, {"type": "text", "text": "N.6 Heatmap Visualization of SF Correlation in the Center-Wall Environment (Partially-Observable) ", "page_idx": 47}, {"type": "image", "img_path": "rI7oZj1WMc/tmp/601ecdb8bda781d6490d08bd6d1677e48f7a976114c6db229d6d0c1dcc68fed5.jpg", "img_caption": [], "img_footnote": [], "page_idx": 47}, {"type": "text", "text": "Figure 36: Correlation Analysis between Simple Successor Features (our model) and Successor Representation in the Center-Wall Environment (Partially-observable). ", "page_idx": 47}, {"type": "image", "img_path": "rI7oZj1WMc/tmp/fe1c9a5355f4d1111b5442cf2f341e8faa76dcbe70437d007effa320288a1c6a.jpg", "img_caption": ["(a) Center-wall environment "], "img_footnote": [], "page_idx": 48}, {"type": "image", "img_path": "rI7oZj1WMc/tmp/e17dac1a6e94fcc0391fb228660b54ffe7295cdc4e0610612d08b63fcc43852f.jpg", "img_caption": ["(b) Before Training "], "img_footnote": [], "page_idx": 48}, {"type": "image", "img_path": "rI7oZj1WMc/tmp/eeccbec8bd5d5ac0089ed4daf21110fc6c53c31261871476dde32a612990670e.jpg", "img_caption": ["(c) After Training "], "img_footnote": [], "page_idx": 48}, {"type": "image", "img_path": "rI7oZj1WMc/tmp/4b12f99b68e00ad8661f119f2936d8b30a562944bced2368de209cce424ae0f2.jpg", "img_caption": ["(d) Difference (Before vs After) "], "img_footnote": [], "page_idx": 48}, {"type": "text", "text": "Figure 37: Correlation Analysis between Successor Features with orthogonality constraints $S\\mathrm{F}+$ Orthogonality) and Successor Representation in the Center-Wall Environment (Partially-observable) ", "page_idx": 48}, {"type": "image", "img_path": "rI7oZj1WMc/tmp/9817e85061330858ade892e2f9fbe251ec7b8b16b9acb32822733d48dfc31c83.jpg", "img_caption": ["(a) Center-wall environment "], "img_footnote": [], "page_idx": 49}, {"type": "image", "img_path": "rI7oZj1WMc/tmp/4b3ef243c838bcb361bdb7379540e58728859fdc53cd5a0c3ddc34468ae35e3a.jpg", "img_caption": ["(b) Before Training "], "img_footnote": [], "page_idx": 49}, {"type": "image", "img_path": "rI7oZj1WMc/tmp/025e49ec09d612239acca5cfaac405f72d3b985ca3673b378c7176d6334d291f.jpg", "img_caption": ["(c) After Training "], "img_footnote": [], "page_idx": 49}, {"type": "image", "img_path": "rI7oZj1WMc/tmp/40d8dfc7c1093e0a9edf6a7a8fa531d0410b12f4d921cbb8e7ccbb7e27172a38.jpg", "img_caption": ["(d) Difference (Before vs After) "], "img_footnote": [], "page_idx": 49}, {"type": "text", "text": "Figure 38: Correlation Analysis between Successor Features with Random un-learnable constraints ${}^{\\mathrm{SF}+}$ Random) and Successor Representation in the Center-Wall Environment (Partially-observable) ", "page_idx": 49}, {"type": "image", "img_path": "rI7oZj1WMc/tmp/eb493cbd6c6a4fa7ca803f08f9c0e650d5896eb656e39e72e2d3e5f73c2a18cd.jpg", "img_caption": ["(a) Center-wall environment "], "img_footnote": [], "page_idx": 50}, {"type": "image", "img_path": "rI7oZj1WMc/tmp/d407dac7ab4876f238b5ce308c12f141a7b1307b6aeefd46a74ca9df32500c27.jpg", "img_caption": ["(b) Before Training "], "img_footnote": [], "page_idx": 50}, {"type": "image", "img_path": "rI7oZj1WMc/tmp/0321e7f1706e8f66cc30627ce5a6cee4c3b021a822bedf6d4733109959026ae3.jpg", "img_caption": ["(c) After Training "], "img_footnote": [], "page_idx": 50}, {"type": "image", "img_path": "rI7oZj1WMc/tmp/49cb0335d191ea2c2bdd05f1fec5c9171e08d7bad6b3f5543c1a2ce1b57ef93d.jpg", "img_caption": ["(d) Difference (Before vs After) "], "img_footnote": [], "page_idx": 50}, {"type": "text", "text": "Figure 39: Correlation Analysis between Successor Features with reconstruction constraints $(\\mathrm{SF}+$ Reconstruction) and Successor Representation in the Center-Wall Environment (Partially-observable) ", "page_idx": 50}, {"type": "image", "img_path": "rI7oZj1WMc/tmp/8ff10c1d589b0b5c7d2b9f28c54d6b4eee5688bf79ae78305b33887b89d3fa6a.jpg", "img_caption": ["(a) Center-wall environment "], "img_footnote": [], "page_idx": 51}, {"type": "image", "img_path": "rI7oZj1WMc/tmp/b06f1e2b90ce5ce31de04e28715c97ace851d4b2f6cf62f18ef443adb054bb1d.jpg", "img_caption": ["(b) Before Training "], "img_footnote": [], "page_idx": 51}, {"type": "image", "img_path": "rI7oZj1WMc/tmp/eb4d93cbfbd824fc2ae956f94205b3a7df628723adb306d1f2d10b8ddda65b7e.jpg", "img_caption": ["(c) After Training "], "img_footnote": [], "page_idx": 51}, {"type": "image", "img_path": "rI7oZj1WMc/tmp/a5011a5b64aa85778b3cddca4762be49f135acd63cb3d6a97bbee6686757d7b9.jpg", "img_caption": ["(d) Difference (Before vs After) "], "img_footnote": [], "page_idx": 51}, {"type": "text", "text": "Figure 40: Correlation Analysis between APS Pre-train Successor Features [Liu and Abbeel, 2021] and Successor Representation in the Center-Wall Environment (Partially-observable) ", "page_idx": 51}, {"type": "text", "text": "N.7 Heatmap Visualization of SF Correlation in the Center-Wall Environment (Fully-Observable) ", "page_idx": 52}, {"type": "image", "img_path": "rI7oZj1WMc/tmp/13aaedf0cdabb5847a54859e711e11529321f0aa2bad87fa54fcfce2ee8f23e6.jpg", "img_caption": [], "img_footnote": [], "page_idx": 52}, {"type": "text", "text": "Figure 41: Correlation Analysis between Simple Successor Features (our model) and Successor Representation in the Center-Wall Environment (Fully-observable) ", "page_idx": 52}, {"type": "image", "img_path": "rI7oZj1WMc/tmp/c77ebe838ed0b839d24c5813038cbf1241cd57b30511d9bc73ac39bde0b7f699.jpg", "img_caption": ["(c) After Training ", "(d) Difference (Before vs After) "], "img_footnote": [], "page_idx": 53}, {"type": "text", "text": "Figure 42: Correlation Analysis between Successor Features with orthogonality constraints $(\\mathrm{SF}+$ Orthogonality) and Successor Representation in the Center-Wall Environment (Fully-observable) ", "page_idx": 53}, {"type": "image", "img_path": "rI7oZj1WMc/tmp/190e1d54f19d03a4e9f53ee1ec2cd7982fda60cf12e2167859bc425fe2ae62e4.jpg", "img_caption": ["(a) Center-wall environment "], "img_footnote": [], "page_idx": 54}, {"type": "image", "img_path": "rI7oZj1WMc/tmp/83bb4303b2b0f96c198bb14ba655d90e0ee749befa6962c54d3623a3897bd753.jpg", "img_caption": ["(b) Before Training "], "img_footnote": [], "page_idx": 54}, {"type": "image", "img_path": "rI7oZj1WMc/tmp/2a9abb99fb78fb7ad7c39f07c5a0f14b80de144dbd9b01fedd0238c6f3836917.jpg", "img_caption": ["(c) After Training "], "img_footnote": [], "page_idx": 54}, {"type": "image", "img_path": "rI7oZj1WMc/tmp/5dc4cef9858d6b8ec339c70dd6a6b5eca3ab157994dac618e61725d1c773fbf1.jpg", "img_caption": ["(d) Difference (Before vs After) "], "img_footnote": [], "page_idx": 54}, {"type": "text", "text": "Figure 43: Correlation Analysis between Successor Features with Random un-learnable constraints $\\mathrm{SF+}$ Random) and Successor Representation in the Center-Wall Environment (Fully-observable) ", "page_idx": 54}, {"type": "image", "img_path": "rI7oZj1WMc/tmp/b89d0048e60fe88097461a23a5b602bb417572ccee8a0288745f105058803779.jpg", "img_caption": ["(a) Center-wall environment "], "img_footnote": [], "page_idx": 55}, {"type": "image", "img_path": "rI7oZj1WMc/tmp/4478439717d74484b9e5bb57babebe51c160dc01950f66c0f6dec00bf7bfb506.jpg", "img_caption": ["(b) Before Training "], "img_footnote": [], "page_idx": 55}, {"type": "image", "img_path": "rI7oZj1WMc/tmp/1ce20f7d97afad21b318c0e26fc0f6fb14c2452e0d139d2a1f5293eb7e569042.jpg", "img_caption": ["(c) After Training "], "img_footnote": [], "page_idx": 55}, {"type": "image", "img_path": "rI7oZj1WMc/tmp/7f075e336503832af6dba4eafa7358e39ad84ea52594f3de94efe0b5839eefdb.jpg", "img_caption": ["(d) Difference (Before vs After) "], "img_footnote": [], "page_idx": 55}, {"type": "text", "text": "Figure 44: Correlation Analysis between Successor Features with reconstruction constraints $(\\mathrm{SF}+$ Reconstruction) and Successor Representation in the Center-Wall Environment (Fully-observable) ", "page_idx": 55}, {"type": "image", "img_path": "rI7oZj1WMc/tmp/95560c6cc2d14b8810189d1da0e5c3e4e8a5c19644fe54c95b1227b55c9ac011.jpg", "img_caption": ["(a) Center-wall environment "], "img_footnote": [], "page_idx": 56}, {"type": "image", "img_path": "rI7oZj1WMc/tmp/8cb98a055bf0ccf5f9221f455713f5f5c3bcd8ec6d69dd1a3bc8c8da04a0b1dd.jpg", "img_caption": ["(b) Before Training "], "img_footnote": [], "page_idx": 56}, {"type": "image", "img_path": "rI7oZj1WMc/tmp/04e920c7206208d6ad336c17aebd61f4e4055d41e66caf7a5faab48050a01499.jpg", "img_caption": ["(c) After Training "], "img_footnote": [], "page_idx": 56}, {"type": "image", "img_path": "rI7oZj1WMc/tmp/2e4e1a0ac609eae35a6df519a62472bd88390b69e42e47bae7692c079f0de107.jpg", "img_caption": ["(d) Difference (Before vs After) "], "img_footnote": [], "page_idx": 56}, {"type": "text", "text": "Figure 45: Correlation Analysis between APS Pre-train Successor Features [Liu and Abbeel, 2021] and Successor Representation in the Center-Wall Environment (Fully-observable) ", "page_idx": 56}, {"type": "text", "text": "N.8 Heatmap Visualization of SF Correlation in the Inverted-LWalls Environment (Partially-Observable) ", "page_idx": 57}, {"type": "image", "img_path": "rI7oZj1WMc/tmp/3c3a837833f280a8bf16e19b269c1ea94d82748589f372348425ce8e9a056d1a.jpg", "img_caption": [], "img_footnote": [], "page_idx": 57}, {"type": "text", "text": "Figure 46: Correlation Analysis between Simple Successor Features (our model) and Successor Representation in the Inverted-LWalls-Grid Environment (Partially-observable). ", "page_idx": 57}, {"type": "image", "img_path": "rI7oZj1WMc/tmp/77a0725934f28e55fbdbca2bcef2ba74e8a8483b5bc941e24042a973db93e0ec.jpg", "img_caption": ["(a) Inverted-LWalls environment "], "img_footnote": [], "page_idx": 58}, {"type": "image", "img_path": "rI7oZj1WMc/tmp/189758915cc14cd2c58c905068df1a7d952ffec7f7ce06dd10dfe2da83bb6d93.jpg", "img_caption": ["(b) Before Training "], "img_footnote": [], "page_idx": 58}, {"type": "image", "img_path": "rI7oZj1WMc/tmp/9d07b0af65c7bba1c414506395a19421ffdae2cd3443da0210afc215cb8ce43e.jpg", "img_caption": ["(c) After Training "], "img_footnote": [], "page_idx": 58}, {"type": "image", "img_path": "rI7oZj1WMc/tmp/9bbb2b725fa244b8d7b3cd0114b395e560a32cf631da837ec70580fed3c2b46a.jpg", "img_caption": ["(d) Difference (Before vs After) "], "img_footnote": [], "page_idx": 58}, {"type": "text", "text": "Figure 47: Correlation Analysis between Successor Features with orthogonality constraints $\\mathrm{SF+}$ Orthogonality) and Successor Representation in the Inverted-LWalls-Grid Environment (Partiallyobservable) ", "page_idx": 58}, {"type": "image", "img_path": "rI7oZj1WMc/tmp/cbd07744530c9f1ad80bff367c974ac57eb31adb3fac2a84d4db99d91241875c.jpg", "img_caption": ["(a) Inverted-LWalls environment "], "img_footnote": [], "page_idx": 59}, {"type": "image", "img_path": "rI7oZj1WMc/tmp/d6e7f9a0be72f4db3a374ebed317bd6ab9b5cee34cf41170f162b5d439af1528.jpg", "img_caption": ["(b) Before Training "], "img_footnote": [], "page_idx": 59}, {"type": "image", "img_path": "rI7oZj1WMc/tmp/d3fa3de183feb4c4de678f74e3b92e43994a959566fc92ce822841bc5579eab7.jpg", "img_caption": ["(c) After Training "], "img_footnote": [], "page_idx": 59}, {"type": "image", "img_path": "rI7oZj1WMc/tmp/6ec983abb0040a974dece29efb972559622108d0f1a933c6b57a61aa31f69385.jpg", "img_caption": ["(d) Difference (Before vs After) "], "img_footnote": [], "page_idx": 59}, {"type": "text", "text": "Figure 48: Correlation Analysis between Successor Features with Random un-learnable constraints ${}^{\\mathrm{SF}+}$ Random) and Successor Representation in the Inverted-LWalls-Grid Environment (Partiallyobservable) ", "page_idx": 59}, {"type": "image", "img_path": "rI7oZj1WMc/tmp/13176f65c4a03b94b5ce161d64632d54c3f564504d9cb6b79e41e5fb6784db79.jpg", "img_caption": ["(a) Inverted-LWalls environment "], "img_footnote": [], "page_idx": 60}, {"type": "image", "img_path": "rI7oZj1WMc/tmp/39abe0c0353032036bef49a4068a42c0f0e6ed4be1e48ccc3c5b27ffcc27811a.jpg", "img_caption": ["(b) Before Training "], "img_footnote": [], "page_idx": 60}, {"type": "image", "img_path": "rI7oZj1WMc/tmp/c0a4a685dc31a6d2ca8428cef847320bb6383005609050e4603283c0472b8087.jpg", "img_caption": ["(c) After Training "], "img_footnote": [], "page_idx": 60}, {"type": "image", "img_path": "rI7oZj1WMc/tmp/81783bfa5100b42ac14d189ebe5025f5e45626dd973a720c939258dc4e3992d6.jpg", "img_caption": ["(d) Difference (Before vs After) "], "img_footnote": [], "page_idx": 60}, {"type": "text", "text": "Figure 49: Correlation Analysis between Successor Features with reconstruction constraints $(\\mathrm{SF}+$ Reconstruction) and Successor Representation in the Inverted-LWalls-Grid Environment (Partiallyobservable) ", "page_idx": 60}, {"type": "image", "img_path": "rI7oZj1WMc/tmp/0212cf76c360b6870398efcaa84dad41d1d5b85dcaedfd65c9917d5de3cdb543.jpg", "img_caption": ["(a) Inverted-LWalls environment "], "img_footnote": [], "page_idx": 61}, {"type": "image", "img_path": "rI7oZj1WMc/tmp/0bd4b8b3a62f50f8f6faad7c1f49c5c15e0fae74b005526d9b236f5cfa4eefba.jpg", "img_caption": ["(b) Before Training "], "img_footnote": [], "page_idx": 61}, {"type": "image", "img_path": "rI7oZj1WMc/tmp/de177f8ced206f814bb5645feb9a2d80b585305796e984e0cb6d9cf3a1f8d2cb.jpg", "img_caption": ["(c) After Training "], "img_footnote": [], "page_idx": 61}, {"type": "image", "img_path": "rI7oZj1WMc/tmp/1f14bdcd53d2dc5c92df8c2780ca97304eb210d3c948ac88504c65efb846c12c.jpg", "img_caption": ["(d) Difference (Before vs After) "], "img_footnote": [], "page_idx": 61}, {"type": "text", "text": "Figure 50: Correlation Analysis between APS Pre-train Successor Features [Liu and Abbeel, 2021] and Successor Representation in the Inverted-LWalls-Grid Environment (Partially-observable) ", "page_idx": 61}, {"type": "text", "text": "N.9 Heatmap Visualization of SF Correlation in the Inverted-LWalls Environment (Partially-Observable) ", "page_idx": 62}, {"type": "image", "img_path": "rI7oZj1WMc/tmp/bd1955c81b67f7f484dac5e96ae60f6b70790dc6d6dd2f21a81151d818d6e83f.jpg", "img_caption": [], "img_footnote": [], "page_idx": 62}, {"type": "text", "text": "Figure 51: Correlation Analysis between Simple Successor Features (our model) and Successor Representation in the Inverted-LWalls-Grid Environment (Fully-observable) ", "page_idx": 62}, {"type": "image", "img_path": "rI7oZj1WMc/tmp/ac4b0a88217b4fea640cc675fcf8b9b900672f57ca1265da5372eebe85e1c453.jpg", "img_caption": ["(a) Inverted-LWalls environment "], "img_footnote": [], "page_idx": 63}, {"type": "image", "img_path": "rI7oZj1WMc/tmp/d98cd8ba6025dba179ca933f23a86ea6011dbcca8a97a297a2d11ce7c7ab9413.jpg", "img_caption": ["(b) Before Training "], "img_footnote": [], "page_idx": 63}, {"type": "image", "img_path": "rI7oZj1WMc/tmp/523a975939560177e5bc493f04d685105b5b203b3cf7f0e8b996adc288792862.jpg", "img_caption": ["(c) After Training "], "img_footnote": [], "page_idx": 63}, {"type": "image", "img_path": "rI7oZj1WMc/tmp/9e6edc8afd0f3fdafb49f66728a1cd2819660a1156069fc3357f3e1c5ed2686c.jpg", "img_caption": ["(d) Difference (Before vs After) "], "img_footnote": [], "page_idx": 63}, {"type": "text", "text": "Figure 52: Correlation Analysis between Successor Features with orthogonality constraints (SF $^+$ Orthogonality) and Successor Representation in the Inverted-LWalls-Grid Environment (Fullyobservable) ", "page_idx": 63}, {"type": "image", "img_path": "rI7oZj1WMc/tmp/623ea46c7e1179e0012a14fb1efa1459c3adbe7bc47dfc0948581bbdcd76dac0.jpg", "img_caption": ["(a) Inverted-LWalls environment "], "img_footnote": [], "page_idx": 64}, {"type": "image", "img_path": "rI7oZj1WMc/tmp/0ef38f779e32229fbeff2224d6da1ce4e6a626f811ea22f42b5e61ef9828725a.jpg", "img_caption": ["(b) Before Training "], "img_footnote": [], "page_idx": 64}, {"type": "image", "img_path": "rI7oZj1WMc/tmp/8a1e6c3949dd15988c91232f3f169727e8fa48f0b852883d27cb42f8ae2a53fa.jpg", "img_caption": ["(c) After Training "], "img_footnote": [], "page_idx": 64}, {"type": "image", "img_path": "rI7oZj1WMc/tmp/2774bae95074dc4919015f602ee4321ae887d8ccb71c16ae2720e0cc8e66489b.jpg", "img_caption": ["(d) Difference (Before vs After) "], "img_footnote": [], "page_idx": 64}, {"type": "text", "text": "Figure 53: Correlation Analysis between Successor Features with Random un-learnable constraints $^{\\mathrm{SF}+}$ Random) and Successor Representation in the Inverted-LWalls-Grid Environment (Fullyobservable) ", "page_idx": 64}, {"type": "image", "img_path": "rI7oZj1WMc/tmp/cfa48ebe1137dd250baf7065f08826141df74873dc08eb3f76dcd4ac13ed0568.jpg", "img_caption": ["(a) Inverted-LWalls environment "], "img_footnote": [], "page_idx": 65}, {"type": "image", "img_path": "rI7oZj1WMc/tmp/3fa98ac9e007b5cdcfa871b4a2129de6a45aae790991d6fd51aaa1ebaaf1c6f9.jpg", "img_caption": ["(b) Before Training "], "img_footnote": [], "page_idx": 65}, {"type": "image", "img_path": "rI7oZj1WMc/tmp/b0ee49cc97ec8e7f38a4946f975084dcdb724c2d77554330ebd1ef1acce2c510.jpg", "img_caption": ["(c) After Training "], "img_footnote": [], "page_idx": 65}, {"type": "image", "img_path": "rI7oZj1WMc/tmp/a5f996f1d639cd8d03f0de0388d16862c84eab12acf32399f88826e15255f173.jpg", "img_caption": ["(d) Difference (Before vs After) "], "img_footnote": [], "page_idx": 65}, {"type": "text", "text": "Figure 54: Correlation Analysis between Successor Features with reconstruction constraints (SF $^+$ Reconstruction) and Successor Representation in the Inverted-LWalls-Grid Environment (Fullyobservable) ", "page_idx": 65}, {"type": "image", "img_path": "rI7oZj1WMc/tmp/b3444a0b1f840d65661cd7a4cad6a7708dbc04eea329990b94979feb734d6768.jpg", "img_caption": ["(a) Inverted-LWalls environment "], "img_footnote": [], "page_idx": 66}, {"type": "image", "img_path": "rI7oZj1WMc/tmp/59836c2f66881e51fc0289f7c6c204de5e46cef78b3f51853f49ddac2d62f955.jpg", "img_caption": ["(b) Before Training "], "img_footnote": [], "page_idx": 66}, {"type": "image", "img_path": "rI7oZj1WMc/tmp/3d21e63fa50c5c43f11125645209f5ddbfb832d27d0621ff9350e46b1a184a88.jpg", "img_caption": ["(c) After Training "], "img_footnote": [], "page_idx": 66}, {"type": "image", "img_path": "rI7oZj1WMc/tmp/9088797460fbc3a855a892fbfa5a05331c88e857dd908570c2ae70ebf747639c.jpg", "img_caption": ["(d) Difference (Before vs After) "], "img_footnote": [], "page_idx": 66}, {"type": "text", "text": "Figure 55: Correlation Analysis between APS Pre-train Successor Features [Liu and Abbeel, 2021] and Successor Representation in the Inverted-LWalls-Grid Environment (Fully-observable) ", "page_idx": 66}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 67}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 67}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 67}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 67}, {"type": "text", "text": "Justification: The abstract and introduction accurately reflect the paper\u2019s contributions and scope by clearly outlining our main achievement: the development of an algorithm designed to overcome representation collapse in learning Successor Features from pixel observations efficiently. We provide comprehensive evidence of representation collapse and review various existing approaches to address this issue, highlighting their computational demands and limitations in continual learning settings. Our contributions are precisely these comparative analyses and the introduction of a more efficient algorithm suitable for both single task and continual learning environments. ", "page_idx": 67}, {"type": "text", "text": "Guidelines: ", "page_idx": 67}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 67}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 67}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 67}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 67}, {"type": "text", "text": "Justification: We have included a section on the limitations and broader impact of our work in section 9 in the main paper. ", "page_idx": 67}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 67}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best ", "page_idx": 67}, {"type": "text", "text": "judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 68}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 68}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 68}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 68}, {"type": "text", "text": "Justification: The assumptions and the complete proof are provided in Appendix C. ", "page_idx": 68}, {"type": "text", "text": "Guidelines: ", "page_idx": 68}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 68}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 68}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 68}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 68}, {"type": "text", "text": "Justification: The details of our architecture are provided in Figure 2. The pseudocode of our algorithm can be found in Appendix B. In addition, the hyperparameters used in our experiments can be found in Appendix E. We intend to release the codebase in the near future after the conclusion of an internal review. ", "page_idx": 68}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 68}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.   \n(c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).   \n(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 68}, {"type": "text", "text": "", "page_idx": 69}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 69}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 69}, {"type": "text", "text": "Answer: [No] ", "page_idx": 69}, {"type": "text", "text": "Justification: We used opensource libraries to perform the experiments in this paper. Details of these software can be found in the Appendix L. We intend to release the codebase in the near future after the conclusion of an internal review. ", "page_idx": 69}, {"type": "text", "text": "Guidelines: ", "page_idx": 69}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 69}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 69}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 69}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 69}, {"type": "text", "text": "Justification: The hyperparameters for our experiments are listed in Appendix E for the environment specifics and F for the agents-specifics. ", "page_idx": 69}, {"type": "text", "text": "Guidelines: ", "page_idx": 69}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 69}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 69}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 69}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 70}, {"type": "text", "text": "Justification: All computational experiments are performed over 5 random seeds and all statistical plots include error bars, which represent the standard deviation of the data. ", "page_idx": 70}, {"type": "text", "text": "Guidelines: ", "page_idx": 70}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 70}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 70}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 70}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 70}, {"type": "text", "text": "Justification: We provide the information regarding the compute resources in the Appendix L. ", "page_idx": 70}, {"type": "text", "text": "Guidelines: ", "page_idx": 70}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 70}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 70}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 70}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 70}, {"type": "text", "text": "Justification: The data are collected using computational simulation, which does not involve any humans or animals. Most of our studies focus on navigational tasks, which are critical for robotics and self-driving cars. ", "page_idx": 70}, {"type": "text", "text": "Guidelines: ", "page_idx": 70}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. \u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. ", "page_idx": 70}, {"type": "text", "text": "\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 71}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 71}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 71}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 71}, {"type": "text", "text": "Justification: We included a section on the limitations and broader impact of our work in section 9. ", "page_idx": 71}, {"type": "text", "text": "Guidelines: ", "page_idx": 71}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 71}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 71}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 71}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 71}, {"type": "text", "text": "Justification: This paper focuses on studying reinforcement learning agents in a computational simulation, therefore we do not foresee such risks with regards to this research paper. ", "page_idx": 71}, {"type": "text", "text": "Guidelines: ", "page_idx": 71}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 71}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 71}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 71}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 72}, {"type": "text", "text": "Justification: We have cited the papers of the environments used, in the main paper as well as in Appendix E and L. ", "page_idx": 72}, {"type": "text", "text": "Guidelines: ", "page_idx": 72}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 72}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 72}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 72}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 72}, {"type": "text", "text": "Justification: This paper does not release new assets. ", "page_idx": 72}, {"type": "text", "text": "Guidelines: ", "page_idx": 72}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 72}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 72}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 72}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 72}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects Guidelines: ", "page_idx": 72}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 72}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 72}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 73}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 73}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects Guidelines: ", "page_idx": 73}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 73}]