[{"figure_path": "HS0faHRhWD/tables/tables_5_1.jpg", "caption": "Table 1: Forecasting performance comparisons. All results are averaged over four prediction windows, i.e., F\u2081 \u2208 {24,36, 48, 60} for ILI and { 96, 192, 336, 720} for others. Yellow : the best in TY1; Blue: the second best in TY1. Underline: the best over all types; Bold: the second best over all types. Full results are presented in Table 13.", "description": "This table presents a comparison of forecasting performance metrics (MSE and MAE) across multiple methods categorized into three types (TY1, TY2, TY3).  Each method's performance is averaged over four different prediction window lengths.  Color-coding highlights the best and second-best performing methods within TY1, while underlining and bolding indicate the best and second-best methods across all three types.", "section": "4.1 Main Results"}, {"figure_path": "HS0faHRhWD/tables/tables_6_1.jpg", "caption": "Table 1: Forecasting performance comparisons. All results are averaged over four prediction windows, i.e., F\u2081 \u2208 {24,36, 48, 60} for ILI and { 96, 192, 336, 720} for others. Yellow : the best in TY1; Blue: the second best in TY1. Underline: the best over all types; Bold: the second best over all types. Full results are presented in Table 13.", "description": "This table presents a comparison of the forecasting performance of TIME-FFM against several other methods across eight benchmark datasets.  The metrics used are Mean Squared Error (MSE) and Mean Absolute Error (MAE).  The methods are categorized into three types: federated fine-tuning methods (TY1), across-dataset centralized methods (TY2), and dataset-specific centralized methods (TY3).  The table highlights the best and second-best performing methods within each category and overall.", "section": "4.1 Main Results"}, {"figure_path": "HS0faHRhWD/tables/tables_6_2.jpg", "caption": "Table 1: Forecasting performance comparisons. All results are averaged over four prediction windows, i.e., F\u2081 \u2208 {24,36, 48, 60} for ILI and { 96, 192, 336, 720} for others. Yellow : the best in TY1; Blue: the second best in TY1. Underline: the best over all types; Bold: the second best over all types. Full results are presented in Table 13.", "description": "This table presents a comparison of forecasting performance across various methods categorized into three types (TY1, TY2, TY3).  The metrics used are MSE (Mean Squared Error) and MAE (Mean Absolute Error), averaged over four prediction window lengths for most datasets (except ILI). The table highlights the best (Yellow) and second best (Blue) performing methods within TY1 (Federated fine-tuning methods). It also shows the overall best and second best performing methods across all three types, regardless of category.  The full results can be found in Table 13.", "section": "4.1 Main Results"}, {"figure_path": "HS0faHRhWD/tables/tables_6_3.jpg", "caption": "Table 16: 5% few-shot forecasting results. Yellow: the best in TY1; Blue: the second best in TY1. Underline: the best over both types; Bold: the second best over both types. '-' means 5% time series is not sufficient to constitute a training set.", "description": "This table presents the results of a 5% few-shot forecasting experiment.  It compares the performance of different models (TIME-FFM and several baselines) across various time series datasets, in a setting where only 5% of the data is used for training.  The table highlights the best performing model within two categories (TY1 and TY2) and overall, using color-coding to differentiate the top performers.", "section": "4.2 Few-Shot Forecasting"}, {"figure_path": "HS0faHRhWD/tables/tables_7_1.jpg", "caption": "Table 4: Zero-shot forecasting results. All results are averaged across four prediction windows, i.e., F\u2081 \u2208 {96, 192, 336, 720}. Yellow : the best in TY1; Blue : the second best in TY1. Underline: the best over both types; Bold: the second best over both types. Full results are presented in Table 17.", "description": "This table presents the results of zero-shot forecasting experiments.  The models were trained on ETTh1, ETTm1, and ETTm2 and then tested on ETTh2, Electricity, and Weather datasets without further fine-tuning.  The table compares the performance of TIME-FFM against other federated learning (TY1) and centralized (TY2) methods using MSE and MAE metrics.  Color-coding highlights the best and second-best results within each category, and underlines highlight the overall best performers.", "section": "4.3 Zero-Shot Forecasting"}, {"figure_path": "HS0faHRhWD/tables/tables_7_2.jpg", "caption": "Table 5: Ablation studies of TIME-FFM on ETTh1 and ILI datasets with F\u2081 \u2208 {336,720} and F\u2081 \u2208 {48, 60} respectively. Bold: the best.", "description": "This table presents the ablation study results for the proposed TIME-FFM model. It shows the impact of different components of the model on forecasting performance using ETTh1 and ILI datasets with two different prediction window sizes.  The variations studied include removing the prompt adaption module, using explicit instructions instead of adaptive prompts, removing the personalized prediction heads, removing all the proposed components, and using a distributed version of TIME-FFM without global model aggregation.  The results are presented as Mean Squared Error (MSE) and Mean Absolute Error (MAE) metrics.", "section": "4.4 Model Analysis"}, {"figure_path": "HS0faHRhWD/tables/tables_8_1.jpg", "caption": "Table 6: Ablation studies of LM on ETTh1 and Weather datasets with F\u2081 \u2208 {96,192} and F\u2081 \u2208 {336, 720} respectively. Bold: the best.", "description": "This ablation study investigates the impact of using different Language Model (LM) variants on the forecasting performance.  Three different LM optimization modes were tested: freezing all parameters, fine-tuning positional embeddings and layer normalization, and full fine-tuning.  Additionally, two different GPT2 backbone sizes (6 and 12 layers) were evaluated. The results demonstrate the impact of each approach on Mean Squared Error (MSE) and Mean Absolute Error (MAE) across different forecasting horizons (F\u2081).", "section": "4.4 Model Analysis"}, {"figure_path": "HS0faHRhWD/tables/tables_8_2.jpg", "caption": "Table 7: Efficiency analysis of TIME-FFM on ETTh1 dataset.", "description": "This table presents the efficiency analysis of the proposed TIME-FFM model and other baseline models (FedLoRA, FedAdapterH, FedAdapterP, and GPT(12)) on the ETTh1 dataset. The metrics presented include training parameters (in millions), total parameters (in millions), training parameter percentage, training time per iteration (in seconds), and communication parameters (in millions). This table demonstrates the efficiency of the TIME-FFM model compared to other methods in terms of the number of parameters used and training time.", "section": "4. Experiments"}, {"figure_path": "HS0faHRhWD/tables/tables_14_1.jpg", "caption": "Table 8: Detailed descriptions of datasets. The dataset size is organized in (training, validation, test).", "description": "This table provides detailed information about the eight benchmark datasets used in the paper's experiments.  For each dataset, it lists the number of channels (c<sub>i</sub>), the dataset size (number of samples in training, validation, and test sets), the batch size used during training, the oversampling times (relevant to data augmentation), the frequency of the time series data (e.g., 1 hour, 15 minutes), and the application domain from which the data originates.  This information is crucial for understanding the experimental setup and the generalizability of the results.", "section": "A Experimental Details"}, {"figure_path": "HS0faHRhWD/tables/tables_14_2.jpg", "caption": "Table 9: Effectiveness evaluation of oversampling. All results are averaged over four prediction windows, i.e., F\u2081 \u2208 {24, 36, 48, 60} for ILI and { 96, 192, 336, 720} for others. Bold: Better.", "description": "This table presents the results of an experiment to evaluate the effectiveness of oversampling in the TIME-FFM model.  It compares the model's performance with and without oversampling across eight benchmark datasets from various domains.  The metrics used for evaluation are Mean Squared Error (MSE) and Mean Absolute Error (MAE). The bold values indicate better performance compared to the no oversampling strategy.", "section": "4.1 Main Results"}, {"figure_path": "HS0faHRhWD/tables/tables_15_1.jpg", "caption": "Table 10: Performance comparison with PatchTST-FL and DLinear-FL. Bold: the best.", "description": "This table compares the performance of TIME-FFM against two other federated learning methods, PatchTST-FL and DLinear-FL, across eight benchmark datasets representing various domains.  The metrics used are MSE (Mean Squared Error) and MAE (Mean Absolute Error).  The results highlight the superior performance of TIME-FFM in terms of both MSE and MAE on average across the datasets, indicating its effectiveness and robustness compared to the other federated learning methods.", "section": "4.1 Main Results"}, {"figure_path": "HS0faHRhWD/tables/tables_15_2.jpg", "caption": "Table 11: Performance comparison with iTransformer, N-BEATS, and Crossformer. Bold: the best.", "description": "This table compares the forecasting performance of TIME-FFM with three other state-of-the-art time series forecasting models: iTransformer, N-BEATS, and Crossformer.  The metrics used are Mean Squared Error (MSE) and Mean Absolute Error (MAE). The results are averaged across multiple datasets (ETTm2, Weather, and Exchange).  The bold values indicate the best performance achieved by any method for each metric and dataset.", "section": "4.1 Main Results"}, {"figure_path": "HS0faHRhWD/tables/tables_16_1.jpg", "caption": "Table 12: Performance comparison with MOIRAI and Moment. Bold: the best.", "description": "This table compares the performance of TIME-FFM against two other foundation models, Moirai and MOMENT, across six benchmark datasets. The metrics used are MSE and MAE, representing Mean Squared Error and Mean Absolute Error respectively, which are common metrics for evaluating the accuracy of time series forecasting models. Lower values indicate better performance.  The table highlights the competitive performance of TIME-FFM, particularly showing better performance in several instances. This comparison aims to illustrate TIME-FFM's performance against existing foundation models trained from scratch on large-scale time series data.", "section": "4.1 Main Results"}, {"figure_path": "HS0faHRhWD/tables/tables_17_1.jpg", "caption": "Table 1: Forecasting performance comparisons. All results are averaged over four prediction windows, i.e., F\u2081 \u2208 {24,36, 48, 60} for ILI and { 96, 192, 336, 720} for others. Yellow : the best in TY1; Blue: the second best in TY1. Underline: the best over all types; Bold: the second best over all types. Full results are presented in Table 13.", "description": "This table presents a comparison of forecasting performance across various methods categorized into three types (TY1, TY2, and TY3).  It shows Mean Squared Error (MSE) and Mean Absolute Error (MAE) for eight different datasets and four prediction window lengths (F1).  The best and second-best performing methods within TY1, and overall best and second-best methods across all three types are highlighted.", "section": "4.1 Main Results"}, {"figure_path": "HS0faHRhWD/tables/tables_18_1.jpg", "caption": "Table 1: Forecasting performance comparisons. All results are averaged over four prediction windows, i.e., F\u2081 \u2208 {24,36, 48, 60} for ILI and { 96, 192, 336, 720} for others. Yellow : the best in TY1; Blue: the second best in TY1. Underline: the best over all types; Bold: the second best over all types. Full results are presented in Table 13.", "description": "This table presents a comparison of forecasting performance across various methods categorized into three types: federated fine-tuning methods (TY1), across-dataset centralized methods (TY2), and dataset-specific centralized methods (TY3).  The metrics used are Mean Squared Error (MSE) and Mean Absolute Error (MAE), averaged over four different prediction window lengths.  The table highlights the best (yellow) and second-best (blue) performing methods within each category (TY1) and overall (underlined and bolded).  Complete results can be found in Table 13.", "section": "4.1 Main Results"}, {"figure_path": "HS0faHRhWD/tables/tables_18_2.jpg", "caption": "Table 1: Forecasting performance comparisons. All results are averaged over four prediction windows, i.e., F\u2081 \u2208 {24,36, 48, 60} for ILI and { 96, 192, 336, 720} for others. Yellow : the best in TY1; Blue: the second best in TY1. Underline: the best over all types; Bold: the second best over all types. Full results are presented in Table 13.", "description": "This table presents a comparison of the forecasting performance of TIME-FFM against various baseline methods across eight benchmark datasets.  The metrics used are Mean Squared Error (MSE) and Mean Absolute Error (MAE), averaged over four different prediction window lengths. The table highlights the best (yellow) and second best (blue) performers within the three method categories (TY1, TY2, and TY3), as well as the overall best and second best performers across all categories.  Full results are provided in a separate table (Table 13).", "section": "4.1 Main Results"}, {"figure_path": "HS0faHRhWD/tables/tables_19_1.jpg", "caption": "Table 1: Forecasting performance comparisons. All results are averaged over four prediction windows, i.e., F1 \u2208 {24, 36, 48, 60} for ILI and {96, 192, 336, 720} for others. Yellow : the best in TY1; Blue: the second best in TY1. Underline: the best over all types; Bold: the second best over all types. Full results are presented in Table 13.", "description": "This table compares the forecasting performance of TIME-FFM against several baseline methods across eight benchmark datasets.  The metrics used are MSE and MAE, averaged over four prediction windows.  The table highlights the best and second-best performing models within each category (TY1, TY2, TY3), overall best, and overall second-best.  Color-coding indicates performance within the TY1 category.  Full results can be found in Table 13.", "section": "4.1 Main Results"}, {"figure_path": "HS0faHRhWD/tables/tables_19_2.jpg", "caption": "Table 1: Forecasting performance comparisons. All results are averaged over four prediction windows, i.e., F\u2081 \u2208 {24,36, 48, 60} for ILI and { 96, 192, 336, 720} for others. Yellow : the best in TY1; Blue: the second best in TY1. Underline: the best over all types; Bold: the second best over all types. Full results are presented in Table 13.", "description": "This table presents a comparison of the forecasting performance of TIME-FFM against various baseline methods across eight benchmark datasets.  The metrics used are MSE and MAE, averaged across four prediction window lengths for each dataset.  The table highlights the best performing methods within three categories: federated fine-tuning methods (TY1), centralized cross-dataset methods (TY2), and dataset-specific centralized methods (TY3).  Color-coding indicates the best and second-best performance within TY1 and overall.", "section": "4.1 Main Results"}, {"figure_path": "HS0faHRhWD/tables/tables_20_1.jpg", "caption": "Table 18: Mean values and standard deviations of TY1.", "description": "This table presents the mean and standard deviation of the MSE and MAE metrics across three runs for each of the eight datasets in TY1 (federated fine-tuning methods).  It provides a quantitative measure of the stability and reliability of the results obtained by different federated learning methods. Lower values generally indicate better performance.", "section": "4.1 Main Results"}]