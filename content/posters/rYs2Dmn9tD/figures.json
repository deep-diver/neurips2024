[{"figure_path": "rYs2Dmn9tD/figures/figures_1_1.jpg", "caption": "Figure 1: Learning Example in Battleship. Trace automatically optimizes heterogeneous parameters (multiple codes) to train an agent to Battleship. Means and standard errors are computed over 10 random seeds.", "description": "This figure shows an example of using Trace to optimize a Battleship-playing agent.  The agent's policy consists of two parts: a reasoning component that analyzes the game board, and an action component that selects a target coordinate.  Trace automatically optimizes both the code for the reasoning and action components. The graph shows that Trace outperforms OPRO (a state-of-the-art LLM-based optimizer) and a simple enumeration baseline by learning to effectively enumerate all squares and balance exploration and exploitation.  The results are averaged over 10 different random seeds.", "section": "1.2 Example of Trace in Action"}, {"figure_path": "rYs2Dmn9tD/figures/figures_2_1.jpg", "caption": "Figure 2: Python Code of the Battleship Example. To build a self-adapting agent with Trace, we only need to annotate some empty functions (reason, act) and set up an optimizer following PyTorch semantics. For space, we trim the docstrings of the empty functions with \"...\" and list them in Appendix J. Trace then builds a DAG as the workflow executes and updates the parameters (see Fig. 1 for the result).", "description": "This figure shows the Python code for a Battleship example using the Trace library.  It demonstrates how to define a trainable policy using Trace operators (node and bundle), setting up an optimizer with PyTorch-like syntax, and how Trace automatically records the workflow execution as a directed acyclic graph (DAG). The figure highlights the simplicity and ease of use of the Trace framework for building self-adapting agents, requiring only minimal code annotations.", "section": "3 Trace: The Next AutoDiff"}, {"figure_path": "rYs2Dmn9tD/figures/figures_3_1.jpg", "caption": "Figure 3: Iterations of OPTO. When \u03b8 \u2208 \u0398 is selected, the Trace Oracle T returns trace feedback \u03c4 = (f, g), where g is a computational graph using \u03b8 as input and f is the feedback given to the output of g.", "description": "This figure illustrates the iterative process of Optimization with Trace Oracle (OPTO).  In each iteration, the optimizer chooses a parameter \u03b8 from the parameter space \u0398. The Trace Oracle T then provides feedback \u03c4, consisting of a computational graph g (which uses \u03b8 as input) and feedback f (provided to the output of g). This feedback is used by the optimizer to update the parameter and proceed to the next iteration. The figure visually depicts this iterative process across three iterations, showcasing the evolving structure of the computational graph g and associated feedback f.", "section": "2.1 Problem Definition of OPTO"}, {"figure_path": "rYs2Dmn9tD/figures/figures_6_1.jpg", "caption": "Figure 4: An example pseudo-code report generated by Trace for a program of x = Node(-1.0); z = bar(x) * (bar(x)+1) and the objective of maxx z.", "description": "This figure shows an example of pseudo-code that Trace automatically generates to represent a program's computational graph for an LLM.  It includes the code, operator definitions, inputs, intermediate values, output, and feedback. The LLM uses this information to optimize the program's parameters (here, x) to maximize z.", "section": "4 Design of the First OPTO Optimizer"}, {"figure_path": "rYs2Dmn9tD/figures/figures_7_1.jpg", "caption": "Figure 5: Numerical Optimization and Traffic Optimization Results.", "description": "This figure presents the results of three experiments comparing Trace with other optimization methods. (a) Numerical Optimization: Shows the absolute error of numerical optimization problems solved using Trace, Trace Masked (without access to the computation graph), and Adam. Trace achieves similar performance to Adam, while Trace Masked struggles. (b) Trace vs. Other optimizers: Compares Trace's performance in a traffic optimization task with other optimizers such as SCATS, GP, PSO, and OPRO. Trace quickly converges to a better solution than others. (c) Ablations of Trace: Demonstrates the impact of memory and access to the computational graph on Trace's performance. OptoPrime with memory and access to the graph performs best.", "section": "5 Experiments"}, {"figure_path": "rYs2Dmn9tD/figures/figures_8_1.jpg", "caption": "Figure 6: Learning the feedback control policy (code) for a simulated Sawyer manipulator in LLF-Bench Meta-World. In each iteration (x-axis), one episode of rollout (10 steps) is performed, and then the policy is updated. The mean and standard error of the success rate over 10 seeds are shown.", "description": "This figure displays the success rates for three different tasks (Reach, Pick-place, and Push) in a simulated robotic manipulation environment.  The x-axis represents training iterations, and the y-axis shows the success rate, averaged over 10 random seeds with error bars representing the standard error.  The graph compares the performance of different optimization methods (OPRO, Trace, Trace NoMem, Trace Masked).  Each iteration involves running a 10-step episode and updating the robot control policy. The results illustrate how Trace, especially with memory, learns more effectively than other approaches.", "section": "5 Experiments"}, {"figure_path": "rYs2Dmn9tD/figures/figures_18_1.jpg", "caption": "Figure 4: An example pseudo-code report generated by Trace for a program of x = Node(-1.0); z = bar(x) * (bar(x)+1) and the objective of maxx z.", "description": "This figure illustrates how Trace represents the computational workflow as a pseudo-algorithm problem. The subgraph is shown as code execution with information about computed values and operator descriptions.  The LLM uses this information to update the parameters based on feedback given to the output.  It highlights that while the code representation looks like a real program, it's actually a representation of the computational graph.", "section": "Design of the First OPTO Optimizer"}, {"figure_path": "rYs2Dmn9tD/figures/figures_19_1.jpg", "caption": "Figure 1: Learning Example in Battleship. Trace automatically optimizes heterogeneous parameters (multiple codes) to train an agent to Battleship. Means and standard errors are computed over 10 random seeds.", "description": "This figure shows the results of training a Battleship-playing AI agent using the Trace framework.  The x-axis represents training iterations, and the y-axis shows the percentage of ships hit.  The figure demonstrates that Trace successfully optimizes multiple code components (heterogeneous parameters) of the AI agent to achieve improved performance in the game. Error bars represent the standard error over 10 different random seeds, showcasing the stability of the results.", "section": "1.2 Example of Trace in Action"}, {"figure_path": "rYs2Dmn9tD/figures/figures_20_1.jpg", "caption": "Figure 1: Learning Example in Battleship. Trace automatically optimizes heterogeneous parameters (multiple codes) to train an agent to Battleship. Means and standard errors are computed over 10 random seeds.", "description": "This figure shows an example of using Trace to optimize a Battleship-playing AI agent.  The agent's policy consists of multiple code components that are optimized simultaneously by Trace. The x-axis represents training iterations, while the y-axis shows the percentage of ships hit.  The plot demonstrates how Trace improves the agent's performance over time by optimizing the different code components, ultimately leading to a higher success rate in hitting the ships.", "section": "1.2 Example of Trace in Action"}, {"figure_path": "rYs2Dmn9tD/figures/figures_20_2.jpg", "caption": "Figure 1: Learning Example in Battleship. Trace automatically optimizes heterogeneous parameters (multiple codes) to train an agent to Battleship. Means and standard errors are computed over 10 random seeds.", "description": "This figure shows the result of applying the Trace optimizer to a Battleship game.  The x-axis represents the number of training iterations, and the y-axis shows the percentage of ships hit.  The plot shows that the Trace optimizer successfully learns to improve the agent's policy (represented by multiple codes which are automatically updated) over time, leading to a higher percentage of ships hit.  Error bars indicating standard deviation are included in the plot.", "section": "1.2 Example of Trace in Action"}, {"figure_path": "rYs2Dmn9tD/figures/figures_21_1.jpg", "caption": "Figure 1: Learning Example in Battleship. Trace automatically optimizes heterogeneous parameters (multiple codes) to train an agent to Battleship. Means and standard errors are computed over 10 random seeds.", "description": "This figure shows an example of how Trace optimizes a Battleship-playing agent.  The agent's policy consists of multiple code components ('reason' and 'act') which are optimized by Trace. The x-axis shows the number of training iterations, and the y-axis shows the percentage of ships hit. The graph demonstrates how Trace improves the agent's performance over time by automatically adjusting the agent's code, resulting in a higher percentage of ships hit.", "section": "1.2 Example of Trace in Action"}, {"figure_path": "rYs2Dmn9tD/figures/figures_24_1.jpg", "caption": "Figure 1: Learning Example in Battleship. Trace automatically optimizes heterogeneous parameters (multiple codes) to train an agent to Battleship. Means and standard errors are computed over 10 random seeds.", "description": "This figure shows the results of training an AI agent to play the game Battleship using the Trace framework.  The x-axis represents the number of training iterations, and the y-axis shows the percentage of ships hit.  The graph demonstrates that the Trace optimizer successfully learns to improve the agent's performance over time by automatically optimizing different code variations (heterogeneous parameters) for the agent's reasoning and action components.  Error bars represent the standard error across 10 different random seeds, illustrating the consistency and reliability of the learning process.", "section": "1.2 Example of Trace in Action"}, {"figure_path": "rYs2Dmn9tD/figures/figures_26_1.jpg", "caption": "Figure 1: Learning Example in Battleship. Trace automatically optimizes heterogeneous parameters (multiple codes) to train an agent to Battleship. Means and standard errors are computed over 10 random seeds.", "description": "This figure shows the results of using Trace to train an AI agent to play the game Battleship.  The agent's policy is composed of multiple code components that are treated as heterogeneous parameters and are optimized by Trace. The graph displays the percentage of ships hit by the agent over the course of training iterations. The mean and standard deviation of this success rate over 10 independent runs are presented, illustrating the effectiveness of Trace in optimizing the code to learn an effective Battleship strategy.", "section": "1.2 Example of Trace in Action"}, {"figure_path": "rYs2Dmn9tD/figures/figures_35_1.jpg", "caption": "Figure 1: Learning Example in Battleship. Trace automatically optimizes heterogeneous parameters (multiple codes) to train an agent to Battleship. Means and standard errors are computed over 10 random seeds.", "description": "This figure shows the results of training a Battleship-playing AI agent using the Trace framework.  The x-axis represents the number of training iterations, and the y-axis represents the percentage of ships hit.  The figure demonstrates that Trace effectively optimizes multiple code snippets (heterogeneous parameters) to improve the agent's performance over time.  The error bars indicate standard errors calculated across 10 different random seeds.", "section": "1.2 Example of Trace in Action"}, {"figure_path": "rYs2Dmn9tD/figures/figures_39_1.jpg", "caption": "Figure 1: Learning Example in Battleship. Trace automatically optimizes heterogeneous parameters (multiple codes) to train an agent to Battleship. Means and standard errors are computed over 10 random seeds.", "description": "This figure demonstrates the results of applying Trace, a novel optimization framework, to a Battleship game.  The goal is to train an AI agent to play Battleship effectively.  The key point is that Trace optimizes multiple heterogeneous parameters simultaneously: in this case, different versions of the agent's 'reason' and 'act' code. The graph shows how the percentage of ships hit improves over training iterations.  Each point represents the mean performance across 10 different random seeds, illustrating the robustness and efficiency of Trace.", "section": "1.2 Example of Trace in Action"}]