[{"figure_path": "rYs2Dmn9tD/tables/tables_5_1.jpg", "caption": "Table 1: Token counts.", "description": "This table presents the number of tokens used by OPRO and OptoPrime during the first iteration of optimization for different tasks.  It highlights the significantly higher token usage of OptoPrime compared to OPRO.  However, the paper notes that even with significantly more iterations of OPRO to equalize token costs, OptoPrime still outperforms OPRO, indicating that OptoPrime's superior performance isn't solely due to increased computational resources but rather its utilization of additional information from the execution trace.", "section": "5.1 Validating with Numerical Optimization"}, {"figure_path": "rYs2Dmn9tD/tables/tables_5_2.jpg", "caption": "Table 1: Token counts.", "description": "This table presents the number of tokens used by OPRO and OptoPrime in different optimization tasks, including numerical optimization, BigBench-Hard, traffic optimization, Meta-World, and Battleship.  The token count reflects the input prompt length given to the respective LLMs, revealing that OptoPrime consistently uses more tokens than OPRO. This difference is attributed to OptoPrime's reliance on the more information-rich execution trace feedback, which is not used in OPRO.", "section": "5.1 Validating with Numerical Optimization"}, {"figure_path": "rYs2Dmn9tD/tables/tables_7_1.jpg", "caption": "Table 1: End-to-end workflow optimization for an LLM benchmark (Big-Bench Hard) in 0-shot setup. CoT refers to Chain-of-Thought prompting and PO refers to DSPy's own prompt optimizer (COPRO). We use Trace to optimize a DSPy program, starting from the same program and prompt template specified by DSPy.", "description": "This table presents the results of an experiment comparing different methods for optimizing large language model (LLM) workflows using the Big-Bench Hard benchmark.  It shows the performance (accuracy) of three methods (DSPy, DSPy with Chain-of-Thought prompting, and Trace) in three categories of tasks (all tasks, natural language processing tasks, and algorithmic tasks).  The results demonstrate that the Trace method, which uses the novel optimization approach introduced in the paper, achieves superior performance compared to the other methods, especially on algorithmic tasks.", "section": "5.3 Unifying Prompts and Functions Optimization"}, {"figure_path": "rYs2Dmn9tD/tables/tables_9_1.jpg", "caption": "Table 2: Comparison between Trace and TextGrad. The optimizer is GPT-4-0-2024-08-06, and the student model is GPT-35-turbo-1106. The results show the mean and the standard error of success rate of the last iterate computed by 5 seeds. The experiment time reported is in minutes (the time involves not just training but also validation and testing by running TextGrad's original pipeline); the time of GSM8K experiment is omitted as the experiment time (>8hrs) is determined primarily by the evaluation not optimization.", "description": "This table compares the performance of Trace and TextGrad on several tasks.  It shows the mean and standard error of success rates, along with the time taken for optimization.  Note that TextGrad's time includes validation and testing.", "section": "5.5 Comparison with TextGrad"}, {"figure_path": "rYs2Dmn9tD/tables/tables_19_1.jpg", "caption": "Table 1: Token counts.", "description": "This table shows the number of tokens used in the prompts for OPRO and OptoPrime in different experiments.  It highlights the significantly higher token usage of OptoPrime compared to OPRO,  but emphasizes that despite this higher token cost, OptoPrime's performance surpasses OPRO's even when OPRO is allowed many more iterations (thus equalizing the overall token expenditure). The difference is attributed to OptoPrime's more informative use of the Trace oracle.", "section": "I Experiment Details"}, {"figure_path": "rYs2Dmn9tD/tables/tables_22_1.jpg", "caption": "Table A.1: Token counts.", "description": "This table shows the number of tokens consumed by the OPRO and OptoPrime prompts across different experimental domains.  OptoPrime consistently uses more tokens than OPRO, but achieves superior performance even when OPRO is given many more iterations to use a comparable number of tokens.", "section": "I Experiment Details"}, {"figure_path": "rYs2Dmn9tD/tables/tables_25_1.jpg", "caption": "Table 1: End-to-end workflow optimization for an LLM benchmark (Big-Bench Hard) in 0-shot setup. CoT refers to Chain-of-Thought prompting and PO refers to DSPy's own prompt optimizer (COPRO). We use Trace to optimize a DSPy program, starting from the same program and prompt template specified by DSPy.", "description": "This table presents the results of an experiment comparing different methods for optimizing an LLM workflow on the Big-Bench Hard dataset.  It shows the performance (accuracy) achieved by various methods: DSPy, DSPy with Chain-of-Thought prompting (CoT), DSPy-PO (DSPy's own prompt optimizer), DSPy-PO with CoT, Trace, and Trace with CoT. The results are broken down by task type (NLP, algorithmic, and overall) to provide more granular insights into how different approaches perform across various complexities.", "section": "5.3 Unifying Prompts and Functions Optimization"}, {"figure_path": "rYs2Dmn9tD/tables/tables_28_1.jpg", "caption": "Table 1: End-to-end workflow optimization for an LLM benchmark (Big-Bench Hard) in 0-shot setup. CoT refers to Chain-of-Thought prompting and PO refers to DSPy's own prompt optimizer (COPRO). We use Trace to optimize a DSPy program, starting from the same program and prompt template specified by DSPy.", "description": "This table compares the performance of different optimization methods on the Big-Bench Hard benchmark, a dataset of challenging tasks for large language models.  It specifically looks at optimizing a workflow implemented using the DSPy library.  The methods compared include DSPy's own prompt optimizer (COPRO), DSPy with Chain-of-Thought prompting, and Trace (the proposed method in the paper) with and without Chain-of-Thought prompting. The results show the accuracy achieved by each method on different categories of tasks within the benchmark: NLP, algorithmic, and a combined set of all tasks.", "section": "5.3 Unifying Prompts and Functions Optimization"}]