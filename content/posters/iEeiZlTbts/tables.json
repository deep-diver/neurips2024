[{"figure_path": "iEeiZlTbts/tables/tables_5_1.jpg", "caption": "Table 4: Learning Hyperparameters.", "description": "This table shows the hyperparameters used in the paper's experiments, specifying values for different parameters related to PPO (Proximal Policy Optimization), Adam (optimizer), PLR (Prioritized Level Replay), and ACCEL (Automated Curriculum Learning) across different environments (JaxNav single-agent, JaxNav multi-agent, Minigrid, and XLand).  It details settings for the number of updates, learning rate, batch size, buffer size, scoring function, and other relevant parameters for each algorithm and environment.", "section": "C Hyperparameters"}, {"figure_path": "iEeiZlTbts/tables/tables_15_1.jpg", "caption": "Table 1: JaxNav Parameters", "description": "This table lists the parameters used to define the JaxNav environment.  These parameters control aspects such as the size of the grid, the robot's movement capabilities, the LiDAR sensor specifications, reward structure, and other relevant details. The table is broken into sections for \"Dynamics\" and \"Reward Signal\" to organize parameters by their function in the simulation.", "section": "Appendix B Environment Description"}, {"figure_path": "iEeiZlTbts/tables/tables_16_1.jpg", "caption": "Table 4: Learning Hyperparameters.", "description": "This table lists the hyperparameters used in the paper's experiments.  It includes hyperparameters for the Proximal Policy Optimization (PPO) algorithm, Prioritized Level Replay (PLR), Accelerated Curriculum Learning (ACCEL), and Sampling for Learnability (SFL) methods, separately detailing settings for single-agent and multi-agent JaxNav, Minigrid, and XLand environments.  Each hyperparameter's value is specified for each method and environment.", "section": "C Hyperparameters"}, {"figure_path": "iEeiZlTbts/tables/tables_23_1.jpg", "caption": "Table 5: The learnability and success rates for levels within the PLR/ACCEL/SFL buffers averaged over training. At each evaluation step, the average and median values for the entire buffer are calculated and then averaged over training. The mean and standard deviation across three different seeds are reported.", "description": "This table presents a statistical analysis of the learnability and success rates of levels selected by different curriculum learning methods (PLR, ACCEL, SFL) during training.  The analysis is performed by averaging the learnability and success rate of levels within each method's buffer at different training steps and averaging these across three different seeds. The results highlight the difference in level selection strategies between the various methods, particularly SFL's focus on more challenging, learnable levels.", "section": "G.3 Analysing the Learnability of Levels"}, {"figure_path": "iEeiZlTbts/tables/tables_25_1.jpg", "caption": "Table 6: Mean and standard deviation of time taken for single-agent JaxNav over 3 seeds.", "description": "This table shows the mean and standard deviation of the compute time for training RL agents using different curriculum learning methods (DR, RobustACCEL, RobustPLR, ACCEL, PLR, and SFL) in the single-agent JaxNav environment.  The compute time is reported for each method across three different seeds, providing a measure of the variability in runtime. The results are crucial for understanding the computational efficiency and scalability of each method.", "section": "H Timing Results and Speed Analysis"}, {"figure_path": "iEeiZlTbts/tables/tables_25_2.jpg", "caption": "Table 7: Mean and standard deviation of time taken for Minigrid over 3 seeds.", "description": "This table shows the mean and standard deviation of compute time for different reinforcement learning methods on the Minigrid environment.  The experiments were run over three seeds to ensure statistical reliability. The methods compared include Domain Randomization (DR), Robust Prioritized Level Replay (RobustPLR), Robust ACCEL, Prioritized Level Replay (PLR), ACCEL, and Sampling for Learnability (SFL).  The results show the average compute time for each method, indicating the efficiency of each algorithm in terms of computational resource usage.", "section": "H Timing Results and Speed Analysis"}, {"figure_path": "iEeiZlTbts/tables/tables_26_1.jpg", "caption": "Table 8: PLR and SFL timings for a single minigrid iteration", "description": "This table compares the time it takes for a single iteration, including training and evaluation, for both PLR and SFL methods on Minigrid.  It breaks down the time for each step: the training step, getting learnable levels (which only applies to SFL), and the evaluation step. The total time for one iteration is also presented.", "section": "H Timing Results and Speed Analysis"}, {"figure_path": "iEeiZlTbts/tables/tables_26_2.jpg", "caption": "Table 9: Mean and standard deviation of time taken for XLand-Minigrid over 5 seeds.", "description": "This table shows the average compute time and standard deviation for three different UED methods (DR, PLR, and SFL) on the XLand-Minigrid environment. The compute time is measured over five different random seeds, indicating the variability in the time taken for each method to complete training.  This provides a measure of the computational cost of each method on this specific environment.", "section": "H Timing Results and Speed Analysis"}]