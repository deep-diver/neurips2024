[{"figure_path": "QoWf3lo6m7/figures/figures_7_1.jpg", "caption": "Figure 1: Experiment results of reversal curse under default configuration (see Table 3). The curves represent the (average) negative log probability of the model predicting the next token to be B\u2081 when the input is \u201cAi \u2192\u201d, or to be A\u2081 when the input is \u201cBi \u2190\u201d. While the sentences in the training set can be learned nearly perfectly (as shown by the training curve where the next token probability converges to one), the model is not able to predict the correct next token in the validation set better than a uniformly random guess. Both curves are averaged over 10 random seeds.", "description": "This figure shows the training and validation loss for a model trained on a reversal curse task.  The training loss decreases sharply, indicating the model learns the forward relationship well. However, the validation loss remains high, showing the model fails to generalize to the reversed relationship. This demonstrates the reversal curse phenomenon.", "section": "5 Experiments"}, {"figure_path": "QoWf3lo6m7/figures/figures_8_1.jpg", "caption": "Figure 2: Visualization of the weights (logits) of the model with default configurations trained after 3000 epochs for the reversal curse experiment. For the top-left matrix, the i-th row corresponds to an entity token A\u1d62 for a training pair, and the i-th column corresponds to an entity token B\u1d62 for a training pair. The (i, j)-th entry represents the model weights from the token A\u1d62 to B\u2c7c, i.e., the logits of B\u2c7c when the input sequence consists of only A\u1d62. Similarly, for the bottom-left matrix, the row corresponds to the input entity tokens of the seen direction (the direction included in the training set) of validation pairs, and the column corresponds to output entity tokens. The two matrices on the right are obtained by swapping row tokens and column tokens of their corresponding left matrices. Note that the diagonals of the bottom-right matrix are all close to zero, while the diagonals of other matrices all have large values. This implies that if a pair of tokens (A, B) only appear in the training set in one direction, then the model weights associated with the other direction will hardly get trained.", "description": "This figure visualizes the weights (logits) of a model trained on a reversal curse task. The heatmaps show the model's learned weights for predicting a token given another token, differentiating between training and validation data, and seen vs. unseen directions.  The diagonals of the matrices highlight the asymmetry: strong weights in the seen direction during training but weak weights in the reverse direction and in unseen validation data.", "section": "5 Experiments"}, {"figure_path": "QoWf3lo6m7/figures/figures_27_1.jpg", "caption": "Figure 1: Experiment results of reversal curse under default configuration (see Table 3). The curves represent the (average) negative log probability of the model predicting the next token to be B\u2081 when the input is \u201cA\u2081 \u2192\u201d, or to be A\u2081 when the input is \u201cB\u2081 \u2190\u201d. While the sentences in the training set can be learned nearly perfectly (as shown by the training curve where the next token probability converges to one), the model is not able to predict the correct next token in the validation set better than a uniformly random guess. Both curves are averaged over 10 random seeds.", "description": "The figure shows the training and validation loss curves for a reversal curse experiment.  The training curve shows the model learns to predict the next token almost perfectly, achieving a negative log probability close to zero. However, the validation curve demonstrates that the model fails to generalize to unseen reverse examples; its performance is no better than random guessing, indicating the presence of a reversal curse.", "section": "5 Experiments"}, {"figure_path": "QoWf3lo6m7/figures/figures_28_1.jpg", "caption": "Figure 4: Visualization of the weights (logits) of the model with default configurations trained after 3000 epochs for COT experiment. The matrices are similar to Figure 2. The row tokens for the top matrices are A\u1d62, B\u1d62, A\u1d62 and column tokens are B\u1d62, C\u1d62, C\u1d62 for training triples respectively. Similarly, the bottom matrices correspond to validation triples. For validation triples (A\u1d62, B\u1d62, C\u1d62), the weights from A\u1d62 to C\u1d62 get hardly trained as indicated by the diagonals of the last matrix.", "description": "This figure visualizes the weights (logits) of a model trained for the chain-of-thought (COT) experiment.  It uses heatmaps to represent the weights from one token to another. The top row shows the weights for training data, illustrating strong weights along the diagonal (A\u1d62 to B\u1d62, B\u1d62 to C\u1d62). The bottom row displays the weights for validation data; here, the weights from A\u1d62 to C\u1d62 are significantly weaker, highlighting the model's struggle to directly infer A\u1d62 \u2192 C\u1d62 without intermediate steps, thus demonstrating the importance of COT.", "section": "Experiments"}, {"figure_path": "QoWf3lo6m7/figures/figures_29_1.jpg", "caption": "Figure 5: Results for reversal curse for different vocabulary sizes. All other configurations are set as default values as in Table 3. The training set sizes for the above four experiments are 9, 20, 85, 850 respectively, and the validation set sizes are 1, 4, 15, 150 respectively.", "description": "This figure displays the results of experiments on the reversal curse phenomenon, varying the vocabulary size while keeping other parameters consistent with Table 3.  The training and validation set sizes are adjusted proportionally to the vocabulary size.  The plots show negative log probability over epochs for both training and validation sets, illustrating the model's performance at different vocabulary sizes.", "section": "5 Experiments"}, {"figure_path": "QoWf3lo6m7/figures/figures_29_2.jpg", "caption": "Figure 1: Experiment results of reversal curse under default configuration (see Table 3). The curves represent the (average) negative log probability of the model predicting the next token to be B\u2081 when the input is \"Ai \u2192\", or to be A\u2081 when the input is \u201cBi \u2190\u201d. While the sentences in the training set can be learned nearly perfectly (as shown by the training curve where the next token probability converges to one), the model is not able to predict the correct next token in the validation set better than a uniformly random guess. Both curves are averaged over 10 random seeds.", "description": "The figure shows the training and validation loss curves for a reversal curse experiment. The training loss converges to a low value, indicating successful learning of forward and backward relationships. However, the validation loss remains high, suggesting a failure to generalize to unseen reverse relationships. The results are averaged over ten random seeds.", "section": "5 Experiments"}, {"figure_path": "QoWf3lo6m7/figures/figures_29_3.jpg", "caption": "Figure 1: Experiment results of reversal curse under default configuration (see Table 3). The curves represent the (average) negative log probability of the model predicting the next token to be B\u2081 when the input is \u201cAi \u2192\u201d, or to be A\u2081 when the input is \u201cBi \u2190\u201d. While the sentences in the training set can be learned nearly perfectly (as shown by the training curve where the next token probability converges to one), the model is not able to predict the correct next token in the validation set better than a uniformly random guess. Both curves are averaged over 10 random seeds.", "description": "The figure shows the training and validation loss curves for a reversal curse experiment. The training loss converges to near zero, indicating that the model learns the training examples well. However, the validation loss remains high, showing that the model fails to generalize to unseen examples, indicating the reversal curse phenomenon.", "section": "5 Experiments"}, {"figure_path": "QoWf3lo6m7/figures/figures_29_4.jpg", "caption": "Figure 1: Experiment results of reversal curse under default configuration (see Table 3). The curves represent the (average) negative log probability of the model predicting the next token to be B\u2081 when the input is \u201cA\u1d62 \u2192\u201d, or to be A\u1d62 when the input is \u201cB\u1d62 \u2190\u201d. While the sentences in the training set can be learned nearly perfectly (as shown by the training curve where the next token probability converges to one), the model is not able to predict the correct next token in the validation set better than a uniformly random guess. Both curves are averaged over 10 random seeds.", "description": "The figure displays the training and validation loss curves for a reversal curse experiment using a default configuration. The model successfully learns to predict the next token for training sentences, but performs no better than random on unseen validation sentences.  This demonstrates the reversal curse phenomenon where the model struggles to generalize from A \u2192 B to B \u2190 A.", "section": "5 Experiments"}, {"figure_path": "QoWf3lo6m7/figures/figures_29_5.jpg", "caption": "Figure 1: Experiment results of reversal curse under default configuration (see Table 3). The curves represent the (average) negative log probability of the model predicting the next token to be B\u2081 when the input is \u201cAi \u2192\u201d, or to be A\u2081 when the input is \u201cBi \u2190\u201d. While the sentences in the training set can be learned nearly perfectly (as shown by the training curve where the next token probability converges to one), the model is not able to predict the correct next token in the validation set better than a uniformly random guess. Both curves are averaged over 10 random seeds.", "description": "The figure shows the training and validation loss curves for a reversal curse experiment.  The training curve demonstrates that the model learns to predict the next token accurately during training.  However, the validation curve shows that the model fails to generalize this ability to unseen data, performing no better than random chance. This illustrates the reversal curse phenomenon where a model trained on a forward relationship fails to predict the reverse relationship.", "section": "5 Experiments"}, {"figure_path": "QoWf3lo6m7/figures/figures_30_1.jpg", "caption": "Figure 8: Results for reversal curse for different entity lengths. Each entity A or B consists of multiple tokens and different entities may have overlapped tokens. The \u201cTrain\u201d curve represents the negative log probability of predicting the first token of the output entity, and \u201cTrain (word)\u201d represents the negative log probability of predicting all tokens one by one of the output entity. All other configurations are set as default values as in Table 3. The training set sizes for the above two experiments are 680 and 250, respectively, and the validation set sizes are 120 and 50, respectively.", "description": "The figure shows the results of the reversal curse experiment when each entity consists of multiple tokens. The training curves show the model\u2019s ability to learn the training sentences effectively. However, the validation curves indicate that the model struggles to predict the correct tokens in the unseen direction, even when entities contain multiple tokens, which is similar to the result obtained when each entity is represented by one token. This is consistent with the findings presented in the paper, which highlights the reversal curse phenomenon even when entities consist of multiple tokens.", "section": "E.2 Additional experimental results for the reversal curse"}, {"figure_path": "QoWf3lo6m7/figures/figures_30_2.jpg", "caption": "Figure 1: Experiment results of reversal curse under default configuration (see Table 3). The curves represent the (average) negative log probability of the model predicting the next token to be B\u2081 when the input is \u201cAi \u2192\u201d, or to be A\u2081 when the input is \u201cBi \u2190\u201d. While the sentences in the training set can be learned nearly perfectly (as shown by the training curve where the next token probability converges to one), the model is not able to predict the correct next token in the validation set better than a uniformly random guess. Both curves are averaged over 10 random seeds.", "description": "The figure shows the training and validation loss curves for a reversal curse experiment.  The training loss converges to near zero, indicating successful learning of the forward relationship. However, the validation loss remains high, showing the model's failure to generalize to the reverse relationship.", "section": "5 Experiments"}, {"figure_path": "QoWf3lo6m7/figures/figures_31_1.jpg", "caption": "Figure 10: Results for reversal curse for different numbers of layers and different embedding dimensions. All other configurations are set as default values as in Table 3.", "description": "This figure shows the results of the reversal curse experiment under different numbers of layers and embedding dimensions.  The x-axis represents the number of epochs, and the y-axis represents the negative log probability. Three different settings are shown: one layer with embedding dimension 20, one layer with embedding dimension 768, and 24 layers with embedding dimension 20.  The training and validation curves are plotted for each setting.  The results demonstrate that the reversal curse persists across various model configurations, suggesting the phenomenon is robust and not simply an artifact of specific hyperparameter choices.", "section": "E.2.1 The reversal curse under small embedding dimensions"}, {"figure_path": "QoWf3lo6m7/figures/figures_31_2.jpg", "caption": "Figure 11: A heat map of cosine similarity between token embeddings (all A\u1d62 and B\u1d62) after 3000 epochs training under different settings. The settings are the same as Figure 10. Under different numbers of layers or embedding dimensions, most of the non-diagonal entries are close to 0, which shows that the embeddings of different tokens are nearly orthogonal.", "description": "This figure shows heatmaps visualizing the cosine similarity between token embeddings for various experimental settings.  The settings vary the number of layers and embedding dimensions of a transformer model. The heatmaps demonstrate that even with different model configurations, most of the non-diagonal entries show values close to zero. This finding supports the conclusion that embeddings of distinct tokens remain nearly orthogonal, which is a key assumption for the theoretical analysis presented in the paper.", "section": "E.2.2 Near orthogonal embeddings"}, {"figure_path": "QoWf3lo6m7/figures/figures_32_1.jpg", "caption": "Figure 1: Experiment results of reversal curse under default configuration (see Table 3). The curves represent the (average) negative log probability of the model predicting the next token to be B\u2081 when the input is \u201cA\u1d62 \u2192\u201d, or to be A\u1d62 when the input is \u201cB\u1d62 \u2190\u201d. While the sentences in the training set can be learned nearly perfectly (as shown by the training curve where the next token probability converges to one), the model is not able to predict the correct next token in the validation set better than a uniformly random guess. Both curves are averaged over 10 random seeds.", "description": "The figure shows the training and validation loss for a model trained on a reversal curse task.  The training loss converges to 0, indicating the model learns the forward and backward relationships perfectly in the training set. However, the validation loss remains high, showing the model fails to generalize to unseen instances.", "section": "5 Experiments"}, {"figure_path": "QoWf3lo6m7/figures/figures_32_2.jpg", "caption": "Figure 1: Experiment results of reversal curse under default configuration (see Table 3). The curves represent the (average) negative log probability of the model predicting the next token to be B\u2081 when the input is \u201cAi \u2192\u201d, or to be A\u2081 when the input is \u201cBi \u2190\u201d. While the sentences in the training set can be learned nearly perfectly (as shown by the training curve where the next token probability converges to one), the model is not able to predict the correct next token in the validation set better than a uniformly random guess. Both curves are averaged over 10 random seeds.", "description": "This figure shows the training and validation loss curves for a reversal curse experiment. The training loss converges to a low value, indicating that the model learns the training data well. However, the validation loss remains high, indicating that the model fails to generalize to unseen data. This demonstrates the reversal curse phenomenon.", "section": "5 Experiments"}, {"figure_path": "QoWf3lo6m7/figures/figures_33_1.jpg", "caption": "Figure 1: Experiment results of reversal curse under default configuration (see Table 3). The curves represent the (average) negative log probability of the model predicting the next token to be B\u2081 when the input is \u201cA\u1d62 \u2192\u201d, or to be A\u1d62 when the input is \u201cB\u1d62 \u2190\u201d. While the sentences in the training set can be learned nearly perfectly (as shown by the training curve where the next token probability converges to one), the model is not able to predict the correct next token in the validation set better than a uniformly random guess. Both curves are averaged over 10 random seeds.", "description": "This figure shows the training and validation loss for the reversal curse experiment. The training loss converges to 0, indicating that the model learns the training data perfectly. However, the validation loss remains high, suggesting that the model fails to generalize to unseen data and suffers from the reversal curse.", "section": "5 Experiments"}, {"figure_path": "QoWf3lo6m7/figures/figures_33_2.jpg", "caption": "Figure 1: Experiment results of reversal curse under default configuration (see Table 3). The curves represent the (average) negative log probability of the model predicting the next token to be B\u2081 when the input is \u201cAi \u2192\u201d, or to be A\u2081 when the input is \u201cBi \u2190\u201d. While the sentences in the training set can be learned nearly perfectly (as shown by the training curve where the next token probability converges to one), the model is not able to predict the correct next token in the validation set better than a uniformly random guess. Both curves are averaged over 10 random seeds.", "description": "The figure shows the training and validation loss curves for a reversal curse experiment. The training loss converges to 0, indicating successful learning of the forward direction. However, the validation loss remains high, indicating a failure to generalize to the reverse direction, demonstrating the reversal curse phenomenon.", "section": "5 Experiments"}, {"figure_path": "QoWf3lo6m7/figures/figures_33_3.jpg", "caption": "Figure 1: Experiment results of reversal curse under default configuration (see Table 3). The curves represent the (average) negative log probability of the model predicting the next token to be B\u2081 when the input is \u201cA\u2081 \u2192\u201d, or to be A\u2081 when the input is \u201cB\u2081 \u2190\u201d. While the sentences in the training set can be learned nearly perfectly (as shown by the training curve where the next token probability converges to one), the model is not able to predict the correct next token in the validation set better than a uniformly random guess. Both curves are averaged over 10 random seeds.", "description": "This figure shows the training and validation loss curves for a reversal curse experiment.  The training loss converges to near zero, indicating successful learning of the forward direction.  However, the validation loss remains high, showing that the model fails to generalize to reverse direction inference.", "section": "5 Experiments"}, {"figure_path": "QoWf3lo6m7/figures/figures_33_4.jpg", "caption": "Figure 1: Experiment results of reversal curse under default configuration (see Table 3). The curves represent the (average) negative log probability of the model predicting the next token to be B\u2081 when the input is \u201cAi \u2192\u201d, or to be A\u2081 when the input is \u201cBi \u2190\u201d. While the sentences in the training set can be learned nearly perfectly (as shown by the training curve where the next token probability converges to one), the model is not able to predict the correct next token in the validation set better than a uniformly random guess. Both curves are averaged over 10 random seeds.", "description": "The figure shows the training and validation loss curves for a reversal curse experiment. The training loss converges to 0, indicating that the model learns the training data well. However, the validation loss remains high, indicating that the model fails to generalize to unseen data. This demonstrates the reversal curse phenomenon.", "section": "5 Experiments"}, {"figure_path": "QoWf3lo6m7/figures/figures_34_1.jpg", "caption": "Figure 1: Experiment results of reversal curse under default configuration (see Table 3). The curves represent the (average) negative log probability of the model predicting the next token to be B\u2081 when the input is \u201cA\u1d62 \u2192\u201d, or to be A\u1d62 when the input is \u201cB\u1d62 \u2190\u201d. While the sentences in the training set can be learned nearly perfectly (as shown by the training curve where the next token probability converges to one), the model is not able to predict the correct next token in the validation set better than a uniformly random guess. Both curves are averaged over 10 random seeds.", "description": "This figure shows the training and validation loss curves for the reversal curse experiment.  The training loss converges to a low value, indicating successful learning of the forward relationships.  However, the validation loss remains high, demonstrating that the model struggles to generalize to the reverse relationships, illustrating the reversal curse phenomenon.", "section": "5 Experiments"}, {"figure_path": "QoWf3lo6m7/figures/figures_34_2.jpg", "caption": "Figure 1: Experiment results of reversal curse under default configuration (see Table 3). The curves represent the (average) negative log probability of the model predicting the next token to be B\u2081 when the input is \u201cAi \u2192\u201d, or to be A\u2081 when the input is \u201cBi \u2190\u201d. While the sentences in the training set can be learned nearly perfectly (as shown by the training curve where the next token probability converges to one), the model is not able to predict the correct next token in the validation set better than a uniformly random guess. Both curves are averaged over 10 random seeds.", "description": "The figure shows the training and validation loss curves for the reversal curse experiment.  The training loss decreases to near zero, indicating successful learning of the forward direction. However, the validation loss remains high, demonstrating a failure to generalize to the reverse direction. This confirms the model's struggle with the reversal curse phenomenon.", "section": "5 Experiments"}]