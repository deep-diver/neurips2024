{"references": [{"fullname_first_author": "Tom Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-12-01", "reason": "This paper is foundational in establishing the capabilities of large language models for few-shot learning, a technique central to the current paper's analysis of the reversal curse."}, {"fullname_first_author": "Jason Wei", "paper_title": "Chain-of-thought prompting elicits reasoning in large language models", "publication_date": "2022-12-01", "reason": "This paper introduces chain-of-thought prompting, a technique directly relevant to the current paper's exploration of logical reasoning and its limitations in LLMs."}, {"fullname_first_author": "Lukas Berglund", "paper_title": "The reversal curse: LLMs trained on \"a is b\" fail to learn \"b is a\"", "publication_date": "2023-09-12", "reason": "This paper directly defines and names the \"reversal curse\" phenomenon, which is the central focus of the current research paper."}, {"fullname_first_author": "Yuandong Tian", "paper_title": "Scan and snap: Understanding training dynamics and token composition in 1-layer transformer", "publication_date": "2023-12-01", "reason": "This paper provides a theoretical framework for analyzing the training dynamics of one-layer transformers, which is directly used as a model for the current paper's analysis."}, {"fullname_first_author": "Zeyuan Allen-Zhu", "paper_title": "Physics of language models: Part 3.2, knowledge manipulation", "publication_date": "2023-09-14", "reason": "This paper offers a theoretical perspective on the limitations of LLMs' ability to manipulate knowledge, directly complementing the current paper's investigation into the reversal curse."}]}