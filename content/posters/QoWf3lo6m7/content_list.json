[{"type": "text", "text": "Towards a Theoretical Understanding of the \u2018Reversal Curse\u2019 via Training Dynamics ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Hanlin Zhu\u2217 Baihe Huang\u2217 UC Berkeley UC Berkeley hanlinzhu@berkeley.edu baihe_huang@berkeley.edu ", "page_idx": 0}, {"type": "text", "text": "Shaolun Zhang Michael Jordan Jiantao Jiao UC Berkeley UC Berkeley UC Berkeley shaolun_zhang@berkeley.edu jordan@cs.berkeley.edu jiantao@berkeley.edu ", "page_idx": 0}, {"type": "text", "text": "Yuandong Tian Meta AI yuandong@meta.com ", "page_idx": 0}, {"type": "text", "text": "Stuart Russell UC Berkeley russell@cs.berkeley.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Auto-regressive large language models (LLMs) show impressive capacities to solve many complex reasoning tasks while struggling with some simple logical reasoning tasks such as inverse search: when trained on $'A\\rightarrow B\"$ (e.g., Tom is the parent of John), LLM fails to directly conclude \u201c $:B\\gets A^{\\,,\\bullet}$ (e.g., John is the child of Tom) during inference even if the two sentences are semantically identical, which is known as the \u201creversal curse\u201d. In this paper, we theoretically analyze the reversal curse via the training dynamics of (stochastic) gradient descent for two auto-regressive models: (1) a bilinear model that can be viewed as a simplification of a one-layer transformer; (2) one-layer transformers under certain assumptions. Our analysis reveals that for both models, the reversal curse is a consequence of the (effective) model weights asymmetry, i.e., the increase of weights from a token $A$ to token $B$ during training does not necessarily cause the increase of the weights from $B$ to $A$ , which is caused by the training dynamics under certain choice of loss function and the optimization space of model parameters. Moreover, our analysis can be naturally applied to other logical reasoning tasks such as chain-of-thought (COT), which provides a new perspective different from previous work that focuses on expressivity. Finally, we conduct experiments to validate our theory on multi-layer transformers under different settings. Our code is available at https://github.com/marlo-z/reversal_curse_analysis/. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "1 Introductions ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Large language models (LLMs) have shown great performance in solving complex reasoning tasks that require multiple reasoning steps through in-context learning (ICL), such as zero-shot learning [1, 2], few-shot learning [3, 4, 5], or via further fine-tuning [6, 7, 8]. However, without the above inferencetime techniques or model fine-tuning (probably combined with data manipulations), an auto-regressive LLM might struggle with simple logical reasoning tasks that require multiple reasoning steps learned during training separately [9], where the reversal curse [10] serves as a well-known example. ", "page_idx": 0}, {"type": "text", "text": "The reversal curse refers to the phenomenon that an auto-regressive LLM that learns $\\begin{array}{r}{{}^{\\;\\prime}A\\rightarrow B^{\\;\\prime}{}^{\\;\\prime}}\\end{array}$ (e.g., Tom is the parent of John) during training fails to generalize to the reverse direction \u201c $:B\\gets A^{\\,\\circ}$ (e.g., John is the child of Tom) even if the pair of relationship $^{\\bullet\\bullet}\\rightarrow^{\\bullet}$ and $\\mathbf{\\omega}^{\\bullet\\bullet}\\leftarrow\\mathbf{\\omega}^{\\bullet\\bullet}$ are reverse to each other and the two sentences are semantically identical. Although some previous works propose different methods to mitigate the reversal curse, including reversing the training dataset [11, 12] and training on different objectives such as autoregressive blank infliling [13], these methods might negatively affect the model performance on other tasks since they either alter the dataset or the model architecture. Without dataset manipulation or changing the auto-regressive nature (causal structure) of the model, there are two other candidate solutions to mitigate the reversal curse. ", "page_idx": 1}, {"type": "text", "text": "First, one might constrain the model parameters to satisfy a higher-level regularity for specific relationships. For example, a reversal-type regularity can be viewed as a pair of relationships $(\\rightarrow,$ $\\leftarrow)$ and two sets $A,B$ such that a model trained on $^{\\bullet\\bullet}A\\:\\rightarrow\\:B^{\\bullet}$ will also increase its probability of \u201c $^{\\star}B\\leftarrow A^{\\bullet}$ for all $A\\in{\\mathcal{A}},B\\in B$ , which induces a subspace of model parameters that satisfy this regularity. If one can train the model within this subspace, then training on $'A\\rightarrow B\"$ can, by definition, help to learn $^{\\star}B\\leftarrow A^{\\bullet}$ . However, for a general LLM, it is extremely challenging to find the subspace and manually hard-code the constraint during optimization even for one pair of relationships, not to mention there are numerous relationships. Since it is intractable to manually hard-code the constraints to the model parameter, one can alternatively expect the model to learn the higher-level regularity by training samples under unconstrained optimization. However, this is also hard, according to our analysis, through the popular cross-entropy (CE) loss that aims to maximize the next token prediction probability for the models studied in our paper. ", "page_idx": 1}, {"type": "text", "text": "Second, one can use a different loss function which is \u201csymmetric\u201d, rather than the popular CE loss. However, the \u201csymmetric\u201d loss might drive the model to learn meaningless sentences. For example, when trained on the sentence \u201cJohn is tall\u201d, a \u201csymmetric\u201d loss function might drive the model to learn \u201ctall is John\u201d, which is not what we expect. To prevent the model from the above undesired behavior, in practice, CE loss is still widely-used. ", "page_idx": 1}, {"type": "text", "text": "Therefore, in this paper, we analyze the reversal curse via training dynamics of the widely-used unconstrained optimization for the CE loss. We summarize our main contributions as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We theoretically analyze the reversal curse where training or test sequences have the form \u201c $'A\\rightarrow B\"$ or $^{\\bullet\\bullet}B\\:\\leftarrow\\:A^{\\bullet}$ via training dynamics of (stochastic) gradient descent under two auto-regressive models: a bilinear model (Section 3) and one-layer transformers under certain assumptions similar to [14] (Section 4). The analysis of both models reveals that the widely-used unconstrained optimization for CE loss leads to model weights asymmetry, i.e., the increase of (effective) weights (after reparameterization) from the token $A$ to token $\\dot{B}^{1}$ during training does not necessarily cause the increase of the weights from $B$ to $A$ , which further causes the reversal curse. Although the (effective) weights from $A$ to $B$ and from $B$ to $A$ might be related to some extent due to reparameterization, their correlation is weak and thus show asymmetry as empirically verified in Section 5. ", "page_idx": 1}, {"type": "text", "text": "\u2022 The techniques we used to analyze the reversal curse can be applied to other logical reasoning tasks. In particular, we use the above framework to analyze chain-of-thought (COT) [4], and we show that a model trained on \u201c $'A\\rightarrow B\"$ and \u201c $:B\\rightarrow C^{\\bullet}$ separately struggles to directly conclude \u201c $A\\sim C^{\\circ}$ without COT even if it is logically true (Section 4.2). Different from the previous work [15] that theoretically studies COT through the expressivity of transformers, our work provides a new perspective through training dynamics. ", "page_idx": 1}, {"type": "text", "text": "\u2022 We also empirically validate our theoretical results on multi-layer transformers (Section 5). ", "page_idx": 1}, {"type": "text", "text": "The asymmetry of auto-regressive model weights caused by widely-used unconstrained optimization for CE loss indicates that auto-regressive LLMs might not automatically deduce certain types of conclusions using separate knowledge learned during training under current popular training paradigms: to make a model predicting token $B$ where the input token is $A$ , the model might need to see $B$ following $A$ in the same sequence during the training set. This also highlights the importance of ICL, data augmentation, or planning for LLMs with the current popular causal transformer-based structures to solve complex reasoning tasks. ", "page_idx": 1}, {"type": "text", "text": "1.1 Related works ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "LLM Reasoning. The strong performance of LLMs on reasoning tasks [3, 7, 16, 2, 17, 4, 18, 19] has prompted many studies on the reasoning capabilities of LLMs. [20] argues that transformers perform implicit Bayesian inference in ICL. [21] shows that transformers implement a specific type of circuits called \u201cinduction heads\u201d that are key to the ICL abilities of LLMs. [22] proves that causal structures are encoded in transformer layers during the training dynamics. [23] identifies a backward chaining mechanism of transformers in deductive reasoning. Apart from in-context reasoning, LLMs still demonstrate limitations in other types of reasoning tasks [24, 25, 26]. ", "page_idx": 2}, {"type": "text", "text": "Reversal Curse. [10] identifies the phenomenon of reversal curse. This drawback of LLMs is also demonstrated in [27]. [9] studies a similar phenomenonin which LLMs face difficulty in manipulating already learned knowledge. Several paper studies eliminating the reversal curse by extending causal attention to bidirectional attention [13], training on reversed samples [12], permuting semantic units [11], or introducing reverse logic data [28]. Given all the empirical works, theoretical analysis of the reversal curse phenomenon remains scarce. ", "page_idx": 2}, {"type": "text", "text": "Expressivity of LLMs. There is a long line of works [29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 21, 45] studying the behavior of LLMs through the expressivity of transformers. It has been shown that transformers can implement simple functions such as sparse linear functions, two-layer neural networks, and decision trees [42], gradient descent [37, 44, 46], automata [47], Turing machines [48], variational inference [49], and bandit algorithms [50]. Different from [15] that study COT via expressivity, we analyze reversal curse and COT via training dynamics. ", "page_idx": 2}, {"type": "text", "text": "Training dynamics of LLMs. There are rich literatures in the optimization of attention layers [51, 52, 53, 54, 55, 56, 57]. [58, 59] study the dynamics of a single linear attention layer in in-context linear regression. [60] proves convergence of one-layer transformers in random feature regime. [61] shows the convergence of gradient descent on one-layer transformers in in-context linear regression with orthogonal data. [14] studies the convergence of one-layer transformers in a class of next-token prediction tasks. [62] studies training dynamics of multi-layer transformers. [22] studies gradient descent on a class of two-layer transformers in in-context learning tasks with latent causal structures. Our paper studies the reversal curse via training dynamics under both bilinear settings and one-layer transformers. For one-layer transformers, we use the same framework as [14] without the need for certain technical assumptions such as long input sequences, different learning rates for different parameters (except for Appendix C.3), or weak correlations that are required for [14]. Besides, we focus on the generalization ability of models for logical reasoning tasks while [14] mainly focus on optimization, and we identify the asymmetry and intransitivity properties of model weights, which are the core reasons for the failure of LLM for certain types of logical reasoning. Moreover, our analysis of the bilinear model only requires the embedding to be almost orthonormal, while [14] essentially assumed the embedding vectors to be fixed and one-hot. ", "page_idx": 2}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Basic notations. For any integer $N>0$ , we use $[N]$ to denote the set $\\{1,2,\\ldots,N\\}$ . Let $\\mathbb{R},\\,\\mathbb{N}$ denote the set of real numbers and natural numbers, respectively. For real variables $x_{1},\\ldots,x_{n}$ , we use po $\\operatorname{ly}(x_{1},\\ldots,x_{n})$ to denote the polynomial of $x_{1},\\ldots,x_{n}$ . We use $f(n)\\lesssim g(n)$ if there exists a constant $C>0$ s.t. $f(n)\\leq C g(n),\\bar{\\forall}n$ ; we say $g(n)\\gtrsim f(n)$ if $f(n)\\lesssim g(n)$ . ", "page_idx": 2}, {"type": "text", "text": "We use $e_{i}$ to denote one-hot vectors where only the $i$ -th entry of $e_{i}$ equals one and all other entries are zero. We use 1 to denote all-one vectors, 0 to denote zero vectors or zero matrices, and $I$ to denote the identity matrix. We will also add subscripts when we want to explicitly show the dimension, such as ${\\bf0}_{d}$ , $I_{d}$ for $d_{\\cdot}$ -dimensional zero vector and $d\\times d$ identity matrix. We use $\\otimes$ to denote tensor product of vectors or matrices and use $\\pmb{x}^{\\otimes2}$ and $A^{\\otimes2}$ to denote $\\mathbf{\\mathcal{x}}\\otimes\\mathbf{\\mathcal{x}}$ and $A\\otimes A$ for vector $\\textbf{\\em x}$ and matrix $A$ ", "page_idx": 2}, {"type": "text", "text": "We use $\\mathcal{N}(\\pmb{\\mu},\\Sigma)$ (or adding subscripts such as $\\mathcal{N}_{d}(\\cdot,\\cdot)$ if we want to show dimensions explicitly) to denote the (multi-variate) Gaussian distribution with mean $\\pmb{\\mu}$ and covariance $\\Sigma$ . Also, we use $\\Delta(\\mathcal{X})$ to denote the set of distributions over a set $\\mathcal{X}$ and use $\\mathbb{E}[\\cdot]$ to denote expectation. For any dataset ${\\cal D}=\\{x_{1},x_{2},\\ldots,x_{n}\\}$ where $x_{i}\\in\\mathcal{X}$ and a function $f:\\mathcal{X}\\rightarrow\\mathbb{R}.$ , we define the empirical expectation over the dataset as $\\begin{array}{r}{\\mathbb E\\hat{\\boldsymbol{\\mathcal{D}}}[f]=\\frac{1}{n}\\sum_{i=1}^{n}f({\\boldsymbol{x}}_{i})}\\end{array}$ . See additional notations in Appendix A. ", "page_idx": 2}, {"type": "table", "img_path": "QoWf3lo6m7/tmp/7910922fcc4cf6d845eda603953f230ab78c5e2de33ca918f0296541bcfdbc19.jpg", "table_caption": [], "table_footnote": [], "page_idx": 3}, {"type": "text", "text": "Table 1: Notations for tokens in Section 4. $^{\\omega}\\rightarrow^{\\gamma}$ and $\"\\leftarrow\"$ denote forward and backward relationships for the reversal curse. $^{\\omega}\\rightarrow^{\\gamma}$ and $\\leftrightarrow$ \u201d denote direct and indirect implication for COT. $\\mathtt{R}_{1}$ and $\\mathtt{R_{2}}$ are relationship tokens in Section 4.3. A, B, C, ${\\mathtt A}_{i}$ , ${\\mathtt{B}}_{i}$ , $\\mathsf{C}_{i}$ denote tokens representing entities. ", "page_idx": 3}, {"type": "text", "text": "Auto-regressive models. Define the vocabulary $\\mathcal{V}=[M]$ for a positive integer $M>0$ which is the size of the vocabulary. Let ${\\boldsymbol{x}}=(x_{1},x_{2},\\ldots,x_{T})$ be a sequence of tokens of length $T$ where each token $x_{t}\\in\\mathcal{V}$ , $\\forall t\\in[T]$ . See Table 1 for notations of different tokens used in Section 4. We study auto-regressive models $p_{\\theta}(\\cdot|x)\\in\\Delta(\\mathcal{V})$ parameterized by $\\theta$ that take the sequence $x$ as input and predict the distribution of the next token $x_{T+1}\\in\\mathcal{V}$ . For both models that we study in this paper, the next token probability is modeled as the softmax applied to the logits $l_{\\theta}(\\cdot|x)\\,\\dot{\\in}\\,\\mathbb{R}^{M}$ of each token in the vocabulary, i.e., $\\begin{array}{r}{p_{\\theta}(y|x)=\\frac{\\exp(l_{\\theta}(y|x))}{\\sum_{v\\in\\mathcal{V}}\\exp(l_{\\theta}(v|x))},}\\end{array}$ $\\forall y\\in\\mathcal{V}$ . Also, each token $v\\in\\mathcal{V}$ has a corresponding (fixed or learnable) embedding vector $\\pmb{u}_{v}\\in\\mathbb{R}^{d}$ . ", "page_idx": 3}, {"type": "text", "text": "3 Bilinear Models ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We start analyzing the reversal curse under bilinear models, which can be viewed as simplified one-layer transformers with input length one and decoder layer only. Also, in this section, we assume the embeddings of each token are fixed, so we directly use the embedding vector to represent a token. ", "page_idx": 3}, {"type": "text", "text": "Datasets. Assume the vocabulary has size $m$ where each token $\\begin{array}{r}{v_{1},v_{2},\\ldots,v_{m}\\overset{i.i.d.}{\\sim}\\mathcal{N}_{d}(0_{d},\\frac{1}{d}I_{d})}\\end{array}$ . Let $\\mathcal{V}=\\{v_{1},\\ldots,v_{m}\\}$ and let $\\mathcal{X}=\\{x_{1},\\ldots,x_{n}\\}$ and $\\mathcal{Y}=\\{y_{1},\\ldots,y_{n}\\}$ be disjoint random subsets of $\\mathcal{V}$ . Assume all training and test sequences have a length of two. For any $2\\leq i\\leq n$ , the training dataset contains both sequence $(x_{i},y_{i})$ and $(y_{i},x_{i})$ . In addition, the training set contains $\\left(x_{1},y_{1}\\right)$ while the test set only contain one example $(y_{1},x_{1})$ . During training, the model learns both $(x_{i},y_{i})$ and $(y_{i},x_{i})$ for $i\\geq2$ to conclude that $\\left({x_{i},y_{i}}\\right)$ is equivalent to $(y_{i},x_{i})$ . For example, $\\mathcal{X}$ is a set of names and $\\boldsymbol{\\wp}$ is a set of books. The sequence $\\left({x_{i},y_{i}}\\right)$ means $\"x_{i}$ is the author of $y_{i}\\rrangle^{,}$ , and the sentence $(y_{i},x_{i})$ means $\"y_{i}$ is written by $x_{i}{\\,}^{,}$ . We test whether the model is able to infer an unseen sequence $(y_{1},x_{1})$ given the training data which includes the other direction $(x_{1},y_{1})$ . ", "page_idx": 3}, {"type": "text", "text": "Bilinear model. We consider a bilinear model parameterized by $\\Theta~\\in~\\mathbb{R}^{d\\times d}$ of which the input contains single token $x\\in\\textbf{\\textit{V}}$ . The logits of the next token $\\textit{y}\\in\\textit{\\V}$ is defined as $l_{\\Theta}^{\\;\\bullet}(y|x)=x^{\\top}\\Theta y$ which is bilinear in $x$ and $y$ , and thus the next token probability is $p_{\\Theta}(y|x)\\,=$ ve\u2208xVp e(lx\u0398p((ly|\u0398x()v)|x)). The training loss for the bilinear model is the cross-entropy loss L(\u0398) = $\\begin{array}{r}{\\frac{1}{2n-1}\\left(\\sum_{i=1}^{n}-\\log p_{\\Theta}(y_{i}|x_{i})+\\sum_{i=2}^{n}-\\log p_{\\Theta}(x_{i}|y_{i})\\right)}\\end{array}$ and the test loss (reversal loss) is $\\mathcal{L}^{\\mathrm{rev}}(\\Theta)=$ $-\\log p\\Theta(x_{1}|y_{1})$ . We study the training dynamics of gradient flow $\\begin{array}{r}{\\frac{d\\Theta_{t}}{d t}=-\\nabla\\mathcal{L}(\\Theta_{t})}\\end{array}$ with the initialization $\\Theta_{0}$ that can be either randomly sampled from $\\mathcal{N}(\\mathbf{0}^{\\otimes2},\\sigma^{2}I^{\\otimes2})$ or set as a pretrained parameter satisfying $\\begin{array}{r}{\\frac{1}{2m}\\,<\\,p_{\\Theta_{0}}\\big(y_{i}|x_{i}\\big),p_{\\Theta_{0}}\\big(\\dot{x_{i}}|y_{i}\\big)\\,<\\,\\frac{2}{m}}\\end{array}$ for all $i~\\in~[n]$ . The following theorem shows a separation during training dynamics. ", "page_idx": 3}, {"type": "text", "text": "Theorem 1 (Separation of training dynamics (informal statement of Theorem 5)). Fix any $\\delta,\\epsilon\\in(0,1)$ . For small $\\sigma$ and $d\\ge\\mathrm{poly}(n,m,\\bar{1}/\\acute{\\epsilon},\\log(1/\\delta))$ , with probability at least $1-\\delta$ , we have ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}^{r e v}(\\Theta_{t})/\\mathcal{L}^{r e v}(\\Theta_{0})\\geq(\\mathcal{L}(\\Theta_{t})/\\mathcal{L}(\\Theta_{0}))^{\\epsilon},\\,\\forall t\\geq0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Theorem 1 shows that the reversal loss is lower bounded by the training loss. Note that for large $d$ and small $\\epsilon$ close to 0, $(\\mathcal{L}(\\Theta_{t})/\\mathcal{L}(\\Theta_{0}))^{\\epsilon}$ is close to 1 and thus $\\mathcal{L}^{\\mathtt{r e v}}(\\Theta_{t})\\gtrsim\\mathcal{L}^{\\mathtt{r e v}}(\\Theta_{0})$ which implies that $p_{\\Theta}(x_{1}|y_{1})$ remains small during training. We summarize the above argument in Theorem 2. ", "page_idx": 3}, {"type": "text", "text": "Theorem 2 (Lower bound of reversal loss (informal statement of Theorem 6). Fix arbitrary $c>0$ and $C\\leq\\log(m/2)$ . Suppose $\\sigma$ is small and $d\\geq\\mathrm{poly}(n,m,\\log(1/\\delta),\\log c,1/\\log C)$ . With probability at least $1-\\delta$ , it holds that $\\mathcal{L}^{r e v}(\\Theta_{\\tau})\\geq C$ , where $\\tau$ denotes the first time such that $\\mathcal{L}(\\Theta_{t})\\leq c$ . ", "page_idx": 3}, {"type": "text", "text": "The proofs of Theorems 1 and 2 are deferred to Appendix B. Theorem 2 implies that for large $d^{2}$ , while the training loss can be trained to be arbitrarily small, the reversal loss remains large. In other words, the model fails to infer an unseen sequence $(y_{1},x_{1})$ given the training data which includes the other direction $\\left(x_{1},y_{1}\\right)$ . Furthermore, even if the model is fine-tuned on new data from a pre-trained parameter $\\Theta_{0}$ that initially grasps the concept of reversal and satisfies $\\textstyle{\\frac{1}{2m}}<$ $\\begin{array}{r}{p_{\\Theta_{0}}(y_{i}|x_{i}),p_{\\Theta_{0}}(x_{i}|y_{i})<\\frac{2}{m}}\\end{array}$ for new data, it fails to extend this understanding to new, unseen data. ", "page_idx": 4}, {"type": "text", "text": "A core reason that the reversal curse happens on the above bilinear model is that the parameter matrix $\\Theta_{t}$ is asymmetric. Consequently, the logits $l_{\\Theta_{t}}(y|x)=x^{\\top}\\Theta_{t}y$ and $l_{\\Theta_{t}}(x|y)=\\dot{y}^{\\top}\\Theta_{t}x$ generally differ. Consider a special case where each $v_{i}$ is a one-hot vector. Then $l_{\\Theta}(\\boldsymbol{y}|\\boldsymbol{x})=\\boldsymbol{x}^{\\top}\\Theta\\boldsymbol{y}=\\Theta_{i j}$ and $l_{\\Theta}(x|y)=y^{\\top}\\Theta x=\\Theta_{j i}$ for $x=e_{i},y=e_{j}$ . Training on $(x,y)$ can increase $\\Theta_{i j}$ but not $\\Theta_{j i}$ , which means the model does not automatically learn the reversal direction $(y,x)$ from $(x,y)$ . In Section 4, we will show that for one-layer transformers, the reversal curse is mainly caused by the same reason, i.e., the asymmetry of the model weights. ", "page_idx": 4}, {"type": "text", "text": "4 One-Layer Transformers ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In Section 3, we analyzed the reversal curse under a bilinear model. In this section, we analyze the reversal curse for one-layer transformers in a similar setting to [14] via training dynamics. We also extend our analysis to chain-of-thought in Section 4.2. ", "page_idx": 4}, {"type": "text", "text": "Basic notations. Let $\\nu\\,=\\,[M]$ be the vocabulary. For any token $x\\,\\in\\,[M]$ , we also use the corresponding one-hot vector $\\pmb{x}=\\pmb{e}_{x}\\in\\mathbb{R}^{M}$ to represent it. Let $U=[\\pmb{u}_{1},\\pmb{u}_{2},\\dots,\\pmb{u}_{M}]^{\\top}\\in\\mathbb{R}^{M\\times d}$ be the embedding matrix, where $\\pmb{u}_{x}\\in\\mathbb{R}^{d}$ is the embedding of token $x\\in[M]$ . Note that $U^{\\top}\\pmb{x}=\\pmb{u}_{x}$ . Consider the $i$ -th training sample in the dataset, $x[i]=(x_{1}[i],\\ldots,x_{T[i]}[i],x_{T[i]+1}[i])$ , a sequence of tokens of length $T[i]+1$ . Here, $x_{T[i]+1}[i]$ is the next token (or equivalently the label) to be predicted, $x_{T[i]}[i]$ is the query token, and $(\\dot{x_{1}[i]},\\cdot\\cdot\\cdot,x_{T[i]-1}[i])$ are contextual tokens. For token $x_{t}[i]$ , we also use its one-hot vector $\\pmb{x}_{t}[i]=\\pmb{e}_{x_{t}[i]}\\in\\mathbb{R}^{M}$ to represent it. Define the contextual token matrix $X[i]=[\\pmb{x}_{1}[i],\\pmb{\\mathscr{s}}...\\,\\pmb{x}_{T[i]-1}[i]]^{\\top}\\,\\in\\mathbb{R}^{(T[i]-1)\\times M}$ . We omit all $i$ in notations when the context is clear. ", "page_idx": 4}, {"type": "text", "text": "One-layer transformer. For a training sample $x\\,=\\,\\left(x_{1},\\ldots,x_{T},x_{T+1}\\right)$ , its contextual token matrix $X\\,=\\,[\\pmb{x}_{1},\\dots,\\pmb{x}_{T-1}]^{\\top}$ and thus $X U\\,=\\,\\bigl[{\\pmb u}_{x_{1}},\\dots,{\\pmb u}_{x_{T-1}}\\bigr]^{\\top}$ contains the contextual token embeddings. We study one-layer transformers in the same setting as [14]. In particular, for an input token sequence $\\left(x_{1},\\ldots,x_{T}\\right)$ , after the one-layer self-attention, we can obtain $\\tilde{\\pmb u}_{T}=\\boldsymbol U^{\\top}\\mathrm{LN}(\\boldsymbol X^{\\top}\\dot{\\boldsymbol b}_{T})$ where btT = tT\u2032 \u2212=11 expx(Tu x\u22a4 QWQ K W Kx\u22a4 tuxt\u2032 /\u221ad), bT = [b1T , . . . , bT \u22121,T ]\u22a4 contains attention scores (after softmax) that query token $x_{T}$ attend to each contextual token3, $\\mathrm{LN}(\\pmb{x})=\\pmb{x}/\\|\\pmb{x}\\|_{2}$ is the $\\ell_{2}$ - normalization operator, and $W_{Q},W_{K}\\in\\mathbb{R}^{d\\times d_{k}}$ are trainable query and key matrices respectively. The logit of $x\\in[M]$ is then calculated by a decoder layer, i.e., $l_{\\boldsymbol{\\theta}}(x|x_{1},\\dots,x_{T})=\\mathbf{u}_{x}^{\\top}W_{V}\\tilde{\\mathbf{u}}_{T}$ , where $\\theta$ encodes all parameters in the transformer, and $W_{V}\\in\\mathbb{R}^{d\\times d}$ can be viewed as a reparameterization of value and output matrices. Finally, the next token prediction probability is obtained by ", "page_idx": 4}, {"type": "equation", "text": "$$\np_{\\theta}(x|x_{1},\\dots,x_{T})=\\frac{\\exp(l_{\\theta}(x|x_{1},\\dots,x_{T}))}{\\sum_{x^{\\prime}\\in[M]}\\exp(l_{\\theta}(x|x_{1},\\dots,x_{T}))}=\\frac{\\exp(u_{x}^{\\top}W_{V}\\tilde{u}_{T})}{\\sum_{x^{\\prime}\\in[M]}\\exp(u_{x^{\\prime}}^{\\top}W_{V}\\tilde{u}_{T})}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We use the cross-entropy loss function to train the model over the whole training set $\\ensuremath{\\mathcal{D}}_{\\mathrm{train}}$ , i.e., $\\begin{array}{r}{\\operatorname*{max}_{U,W_{K},W_{Q},W_{V}}J\\triangleq\\mathbb{E}_{\\mathcal{D}_{\\operatorname*{min}}}[\\log p_{\\theta}(x_{T+1}|x_{1},\\dots,x_{T})]}\\end{array}$ . ", "page_idx": 4}, {"type": "text", "text": "Reparameterization. Similar to [14], we define $Z\\,=\\,U W_{Q}W_{K}^{\\top}U^{\\top}/\\sqrt{d}\\,\\in\\,\\mathbb{R}^{M\\times M}$ and $Y=$ $U W_{V}^{\\top}U^{\\top}\\in\\mathbb{R}^{M\\times M}$ and are interested in their dynamics after reperameterization. Then, the attention score (after softmax) and next token probability become ", "page_idx": 4}, {"type": "equation", "text": "$$\nb_{t T}=\\frac{\\exp(x_{T}^{\\top}Z x_{t})}{\\sum_{t^{\\prime}=1}^{T-1}\\exp(x_{T}^{\\top}Z x_{t^{\\prime}})},\\;\\;p_{\\theta}(x|x_{1},\\ldots,x_{T})=\\frac{\\exp\\left(x^{\\top}Y^{\\top}\\mathrm{LN}(X^{\\top}b_{T})\\right)}{\\sum_{x^{\\prime}}\\exp\\left(x^{\\prime^{\\top}}Y^{\\top}\\mathrm{LN}(X^{\\top}b_{T})\\right)},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "and the objective can be written as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{Y,Z}J=\\mathbb{E}_{\\mathcal{D}_{\\mathrm{train}}}[{\\pmb x}_{T+1}^{\\top}Y^{\\top}\\mathrm{LN}(X^{\\top}b_{T})-\\log\\sum_{x^{\\prime}\\in[M]}{\\pmb x}^{\\prime^{\\top}}Y^{\\top}\\mathrm{LN}(X^{\\top}b_{T})].\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Let $\\eta_{Y},\\eta_{Z}$ be the learning rate of matrices $Y$ and $Z$ respectively. Then the gradient of $Y$ and $Z$ can be characterized by the following lemma: ", "page_idx": 5}, {"type": "text", "text": "Lemma 1 (Gradient of $Y$ and $Z$ for 1-layer transformer, Lemma 1 of [14]). The gradient of $Y$ and $Z$ w.r.t. (2) of batch size $^{\\,I}$ and learning rate $\\eta_{Y}$ and $\\eta_{Z}$ can be written as $\\dot{Y}=\\eta_{Y}L N(X^{\\top}b_{T})({x_{T+1}}-\\alpha)^{\\top},\\;\\;\\dot{Z}=\\eta_{Z}{x_{T}}({x_{T+1}}-\\alpha)^{\\top}Y^{\\top}\\frac{P_{X^{\\top}b_{T}}^{\\bot}}{\\|X^{\\top}b_{T}\\|_{2}}X^{\\top}\\operatorname{diag}(b_{T})X,$ where $P_{v}^{\\perp}\\;\\;\\triangleq\\;\\;I\\;-\\;v v^{\\top}/\\|v\\|_{2}^{2}$ projects any vector to orthogonal complement of $\\pmb{v}$ , $\\alpha\\quad=$ $\\left[\\alpha_{1},\\alpha_{2},\\ldots,\\alpha_{M}\\right]^{\\top}\\in\\mathbb{R}^{M}$ with $\\overleftarrow{\\alpha}=\\exp\\left(Y^{\\top}L N(X^{\\top}b_{T})\\right)/\\mathbf{1}^{\\top}\\exp\\left(Y^{\\top}L N(X^{\\top}b_{T})\\right)$ . ", "page_idx": 5}, {"type": "text", "text": "Proof. ${\\dot{Y}},{\\dot{Z}}$ can be obtained by direct calculation. One can refer to the proof of Lemma 1 of [14]. ", "page_idx": 5}, {"type": "text", "text": "4.1 Main results for the reversal curse ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we analyze the reversal curse where data points are three-token sentences $^{\\;\\leftarrow}\\mathtt{A}\\rightarrow\\mathtt{B}^{\\;\\bullet}$ or $\\mathbf{\\nabla}^{\\circ}\\mathbf{B}\\leftarrow\\mathbf{A}^{\\circ}\\mathbf{\\Phi}$ . For each sentence, A and B are two distinct tokens that represent two entities, and $\\omega^{,}$ and \u201c $\\leftarrow$ \u201d are two special tokens representing a pair of relationships inverse to each other. ", "page_idx": 5}, {"type": "text", "text": "Datasets. Let $N_{\\mathrm{train}}>0$ , $N_{\\mathrm{test}}^{(1)}>0$ and $N_{\\mathrm{test}}^{(2)}>0$ and denote $N_{\\mathrm{total}}=N_{\\mathrm{train}}+N_{\\mathrm{test}}^{(1)}+N_{\\mathrm{test}}^{(2)}$ . Let $\\mathtt{A}_{i},\\mathtt{B}_{i}\\in\\mathcal{V},\\forall i\\in[N_{\\mathrm{total}}]$ be $2N_{\\mathrm{total}}$ distinct tokens representing distinct entities. $\\operatorname{Let}\\to,\\leftarrow\\in\\mathcal{V}$ be two additional different tokens that represent two inverse relationships. Specifically, we have $\\mathtt{A}_{i}\\to\\mathtt{B}_{i}$ and ${\\tt B}_{i}\\gets{\\tt A}_{i}$ for all $i\\in[N_{\\mathrm{total}}]$ . For notation convenience, we define the following three index sets ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l r l r l}&{\\mathcal{Z}_{\\mathrm{train}}=[N_{\\mathrm{train}}],}&&{\\mathcal{Z}_{\\mathrm{test}}^{(1)}=[N_{\\mathrm{train}}+N_{\\mathrm{train}}^{(1)}]\\backslash\\mathcal{Z}_{\\mathrm{train}},}&&{\\mathcal{Z}_{\\mathrm{test}}^{(2)}=[N_{\\mathrm{total}}]\\backslash(\\mathcal{Z}_{\\mathrm{train}}\\cup\\mathcal{Z}_{\\mathrm{test}}^{(1)}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The training set $\\ensuremath{\\mathcal{D}}_{\\mathrm{train}}$ consists of all $\\mathtt{A}_{i}\\to\\mathtt{B}_{i}$ and ${\\tt B}_{i}\\gets{\\tt A}_{i}$ for $i\\in\\mathcal{T}_{\\mathrm{train}}$ . In addition, $\\mathcal{D}_{\\mathrm{train}}$ contains $\\mathtt{A}_{i}\\to\\mathtt{B}_{i}$ for $i\\in\\mathcal{T}_{\\mathrm{test}}^{(1)}$ and ${\\tt B}_{i}\\gets{\\tt A}_{i}$ for $i\\in\\mathcal{T}_{\\mathrm{test}}^{(2)}$ . For convenience, we let $N=|\\mathcal{D}_{\\operatorname{train}}|$ to be the size of the training set. The test set $\\ensuremath{\\mathcal{D}}_{\\mathrm{test}}$ consists of ${\\tt B}_{i}\\gets{\\tt A}_{i}$ for $i\\in\\mathcal{T}_{\\mathrm{test}}^{(1)}$ and $\\mathtt{A}_{i}\\to\\mathtt{B}_{i}$ for $i\\in\\mathcal{T}_{\\mathrm{test}}^{(2)}$ I(2)  Under . our construction of the dataset, the LLM will learn the relationship between ${\\mathtt A}_{i}$ and $\\mathtt{B}_{i}$ for $i\\in\\mathcal{T}_{\\mathrm{train}}$ in both directions to deduce that $\\rightarrow$ is reverse to $\\leftarrow$ , and learn the relationship between ${\\mathtt A}_{i}$ and ${\\mathtt{B}}_{i}$ for $i\\in\\mathcal{I}_{\\mathrm{test}}^{(1)}\\cup\\mathcal{I}_{\\mathrm{test}}^{(2)}$ in one direction and will be tested for the other. ", "page_idx": 5}, {"type": "text", "text": "We use $p_{\\theta}(\\mathsf{A}_{i}|\\mathsf{B}_{i}\\gets)$ and $p_{\\theta}(\\mathsf{B}_{i}|\\mathsf{A}_{i}\\to)$ to more compactly represent $p_{\\theta}(x_{3}=\\mathtt{A}_{i}|x_{1}=\\mathtt{B}_{i},x_{2}=\\leftarrow)$ and $p_{\\theta}(x_{3}=\\mathtt{B}_{i}|x_{1}=\\mathtt{A}_{i},x_{2}=\\rightarrow)$ , respectively. Our goal is to prove through the training dynamics owfe  oanree -ilnatyeerre strteadn sifno $p_{\\theta}(\\mathbf{A}_{i}|\\mathbf{B}_{i}\\leftarrow),\\forall i\\in\\mathcal{T}_{\\mathrm{test}}^{(1)}$ aabnidli $p_{\\theta}(\\mathrm{B}_{i}|\\mathrm{A}_{i}\\rightarrow),\\forall i\\in\\mathcal{I}_{\\mathrm{test}}^{(2)}$ .uring training. In particular, ", "page_idx": 5}, {"type": "text", "text": "For convenience, we assume zero-initialization $Y(0)=\\mathbf{0}$ and $Z(0)=\\mathbf{0}$ . This is the same as [14] and is reasonable since empirically, $Y$ and $Z$ are usually initialized as inner products of $d$ -dimensional vectors with i.i.d Gaussian entries, and thus are almost zero (Lemma 8 in Appendix B). The following proposition shows the initial train/test probabilities are uniform over the vocabulary $\\mathcal{V}$ . ", "page_idx": 5}, {"type": "text", "text": "Proposition 4.1 (Initial probability under zero initializaion). Assume the transformer is under zero-initialization $\\theta(0)=\\^{\\circ}\\!(Y(0),Z(0))$ with $Y(0)=\\mathbf{0}$ and $Z(0)=\\mathbf{0}$ . For any $i\\in\\left[N_{t o t a l}\\right]$ , we have ", "page_idx": 5}, {"type": "equation", "text": "$$\np_{\\theta(0)}\\bigl(B_{i}\\bigl|A_{i}\\to\\bigr)=p_{\\theta(0)}\\bigl(A_{i}\\bigl|B_{i}\\leftarrow\\bigr)=1/M.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The proof is deferred to Appendix C.1.1. Proposition 4.1 shows that initially, the probability of predicting any B (or $\\mathtt{A}$ , respectively) given any $\\mathsf{A}\\to(\\mathrm{or}\\;\\mathsf{B}\\gets\\$ , respectively) as input is uniform over the whole vocabulary. When $Y(0)$ and $Z(0)$ are not exactly 0 but close to 0, the initial prediction will still be close to the uniform distribution, which is similar to Lemma 6. Next we analyze the dynamics of $p_{\\theta(t)}\\big(\\mathtt{B}_{i}|\\mathtt{A}_{i}\\to\\big)$ and $p_{\\theta(t)}(\\mathtt{A}_{i}|\\mathtt{B}_{i}\\gets)$ . ", "page_idx": 5}, {"type": "text", "text": "Proposition 4.2 (Next token probability). For input sequence $(x_{1},x_{2})$ , the next token probability under parameters $\\theta(t)$ is $\\begin{array}{r}{p_{\\theta(t)}(x|x_{1},x_{2})=\\exp\\left(Y(t)_{x_{1},x}\\right)/\\!\\sum_{x^{\\prime}\\in[M]}\\exp\\left(Y(t)_{x_{1},x^{\\prime}}\\right)}\\end{array}$ , where $Y(t)_{i,j}$ is the entry of the matrix $Y(t)$ at row $i$ and column $j$ . ", "page_idx": 5}, {"type": "text", "text": "The proof is also deferred to Appendix C.1.1. According to Proposition 4.2, the next token probability when the entity in the input is $x_{1}$ is determined by the $x_{1}$ -th row of the matrix $Y(t)$ . Another nice property indicated by Proposition 4.2 is that we don\u2019t need to keep track of the dynamics of $Z(t)$ , which could greatly simplify the analysis. The following lemma shows the dynamics of $Y(t)$ . ", "page_idx": 6}, {"type": "text", "text": "Lemma 2 (Dynamics of $Y(t),$ ). Assume we run SGD with batch size $I^{\\mathrm{~4~}}$ , and assume $M\\gg100$ and $\\begin{array}{r}{\\frac{1}{M^{0.99}}\\ll\\eta_{Y}<1.}\\end{array}$ . Let $\\begin{array}{r}{t\\gtrsim\\frac{N\\ln M}{\\eta_{Y}}}\\end{array}$ and let $Y(t)_{i}$ denote the $i$ -th row of $Y(t)$ and $Y(t)_{i j}$ denote the $(i,j)$ -th entry of $Y(t)$ . Then for training sequence $(x_{1},x_{2},x_{3})\\in\\mathcal{D}_{t r a i n}$ at time $t_{:}$ , we have ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{Y(t)_{x_{1},x_{3}}\\gtrsim\\ln\\left(M\\eta_{Y}t/N\\right),\\quad a n d\\quad Y(t)_{x_{1},x}\\lesssim-\\ln\\left(M\\eta_{Y}t/N\\right)/M,\\quad\\forall x\\neq x_{3},}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "and for any test sequence $(x_{1},x_{2},x_{3})\\in\\mathcal{D}_{t e s t},$ , we have $Y(t)_{x_{1},x}=0,\\forall x\\in[M].$ . ", "page_idx": 6}, {"type": "text", "text": "The proof of Lemma 2 is presented in Appendix C.1.2. Lemma 2 implies the asymmetry of the model weights $Y(t)$ : for two tokens $x_{1},x_{3}$ , when $x_{1}$ appears as a contextual token and $x_{3}$ serves as the next token in the same training sequence, the model weights $Y(t)_{x_{1},x_{3}}$ gets increased during training while $Y(t)_{x_{3},x_{1}}$ will not get increased. Combining Proposition 4.2, we can obtain our main theorem for the reversal curse. ", "page_idx": 6}, {"type": "text", "text": ".e rLseatl sduemneo twe et hreu tni mSeG sDte pw iwthh ibcaht calhs soi zsea $^{\\,l}$ s, faiens $M\\gg100$ aFnodr $\\begin{array}{r}{\\frac{1}{M^{0.99}}\\ll\\eta_{Y}<1.}\\end{array}$ $\\begin{array}{r}{t\\gtrsim\\frac{\\dot{N}\\ln M}{\\eta_{Y}}}\\end{array}$ $\\ln t\\gtrsim\\ln(N M/\\eta_{Y})$ training sequence $(x_{1},x_{2},x_{3})\\in\\mathcal{D}_{t r a i n}$ at time $t_{;}$ , we have ", "page_idx": 6}, {"type": "equation", "text": "$$\np_{\\theta(t)}(x_{3}|x_{1},x_{2})\\ge1-(M-1)(M\\eta_{Y}t/N)^{-c}\\to1,\\quad a s\\ t\\to\\infty\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "for some constant $c>0$ , and for any test sequence $(x_{1},x_{2},x_{3})\\in\\mathcal{D}_{t e s t}$ that is not included in the training set $\\mathcal{D}_{t r a i n}$ , we have $p_{\\theta(t)}(x_{3}|x_{1},x_{2})\\leq1/M$ . ", "page_idx": 6}, {"type": "text", "text": "Theorem 3 shows that although the direction presented in the training set can be learned nearly perfectly, the model\u2019s next token prediction of the reverse direction is almost a random guess. The proof is deferred to Appendix C.1.3. We also empirically validate the above results for multi-layer transformers in Section 5. ", "page_idx": 6}, {"type": "text", "text": "4.2 Chain-of-thought ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we extend our analysis in Section 4.1 to study other logical relationships. In particular, we study chain-of-thought (COT) [4] and show its importance via training dynamics. COT encourages LLMs to output a series of intermediate reasoning steps to increase their performance. Consider the simplest example, where the model learns two facts that $\\textsf{A}\\to\\textsf{B}$ and $\\mathtt{B}\\rightarrow\\mathtt{C}$ , and we want to test whether the model is able to directly conclude that $\\tt A\\sim C$ . COT indicates that if an LLM is only trained on $\\mathtt{A}\\to\\mathtt{B}$ and $\\mathtt{B}\\rightarrow\\mathtt{C}$ , it would be easier for the model to deduce $\\tt A\\sim C$ during the inference time if the model can first output the intermediate steps $\\mathtt{A}\\to\\mathtt{B}$ and $\\mathtt{B}\\rightarrow\\mathtt{C}$ , instead of directly predicting the next token C given the input $^{\\bullet\\bullet}\\mathbb{A}\\sim^{\\bullet\\bullet}$ . The failure of directly deducing $\\tt A\\sim C$ is also empirically observed by [9]. ", "page_idx": 6}, {"type": "text", "text": "Theoretically, [15] shows the importance of COT for some complex reasoning tasks through the lens of the expressivity of transformers. In this section, we show the importance of COT through a different angle, i.e., training dynamics. We show that for the above simplest two-step reasoning, without COT, the model is not able to directly predict C given the input \u201c $\\mathbb{A}\\sim$ \u201d even if it learns $\\tt A\\rightarrow B$ and $\\mathtt{B}\\rightarrow\\mathtt{C}$ . ", "page_idx": 6}, {"type": "text", "text": "Theorem 4 (Importance of chain-of-thought, informal statement of Theorem 7). Under certain assumptions as stated in Theorem 7, for any $A_{i},B_{i}$ , $c_{i}$ s.t. $A_{i}\\to B_{i}$ and $B_{i}\\to\\,C_{i}$ are in the training set but $A_{i}\\to\\,C_{i}$ is not, we have ", "page_idx": 6}, {"type": "equation", "text": "$$\np_{\\theta(t)}(B_{i}|A_{i}\\to)\\to1,\\quad p_{\\theta(t)}(C_{i}|B_{i}\\to)\\to1,\\quad p_{\\theta(t)}(C_{i}|A_{i}\\sim)\\leq1/M,\\quad a s\\ t\\to\\infty.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "We defer the details of the dataset construction and proof to Appendix C.2. Theorem 4 shows that although the LLM learns $\\mathtt{A}_{i}\\to\\mathtt{B}_{i}$ and $\\mathsf{B}_{i}\\to\\mathsf{C}_{i}$ nearly perfectly, it cannot directly deduce $\\mathtt{A}_{i}\\sim\\mathtt{C}_{i}$ . Analogous to the asymmetry of causal transformer weights as we discussed in Section 4.1, our analysis of COT reveals another property, i.e., intransitivity: training the weights associated with A to B and B to C does not necessarily increase the weights associated with A to C. ", "page_idx": 6}, {"type": "text", "text": "We also emphasize that the model fails to directly deduce $\\mathtt{A}_{i}\\sim\\mathtt{C}_{i}$ when the two intermediate steps $\\mathtt{A}_{i}\\to\\mathtt{B}_{i}$ and $\\mathtt{B}_{i}\\to\\mathtt{C}_{i}$ are trained separately. If the two steps are concatenated into a single training sequence, it is possible that the model learns $\\mathtt{A}_{i}\\sim\\mathtt{C}_{i}$ directly [19]. ", "page_idx": 7}, {"type": "text", "text": "4.3 Roles of the attention score matrix ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "During the analysis of Sections 4.1 and 4.2, we show that the reversal curse and the importance of COT are largely due to the asymmetry and intransitivity of causal transformer weights (in our case, the weight matrix $Y(t))$ . However, it seems that the dynamics of the attention score matrix $Z(t)$ do not impact the model performance. Below, we briefly discuss the role of the attention score matrix $Z(t)$ . ", "page_idx": 7}, {"type": "text", "text": "In (1), the attention score is used to calculate the weights $b_{t T}$ , where a contextual token $x_{t}$ with a larger attention score attended by the query token $x_{T}$ has a larger weight. Note that we use the same formulation as the previous work [14] where the query token will not attend to itself. Therefore, for a three-token training sequence, the weights $b_{12}$ is always one since there is only one contextual token $x_{1}$ , no matter whether the value of the attention score is high or low. ", "page_idx": 7}, {"type": "text", "text": "However, consider a slightly different setting, where the relationship is represented by two tokens. In that case, $x_{1}=\\mathtt{A}_{i},x_{2}=\\mathtt{R}_{1},x_{3}=\\mathtt{R}_{2},x_{4}=\\mathtt{B}_{i}$ , and there are two contextual tokens ${\\mathtt A}_{i}$ and $\\mathtt{R}_{1}$ . The role of the attention score is then to select the important token, i.e., ${\\mathtt A}_{i}$ , by putting more weights on it. Theorem 2 of [14] showed that under certain assumptions, the query token $\\mathtt{R}_{2}$ will attend more to \u201cdistinct tokens\u201d ${\\mathtt A}_{i}$ and less to the \u201ccommon token\u201d $\\mathtt{R}_{1}$ . Therefore, the query token $\\mathtt{R}_{2}$ will eventually put all weights to ${\\mathtt A}_{i}$ , and the remaining analysis remains the same as in Sections 4.1 and 4.2. See Appendix C.3 for a more rigorous analysis. ", "page_idx": 7}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we conduct experiments to further validate our theoretical results in Section 4 on multi-layer transformers. We show experimental results of the reversal curse in this section and COT in Appendix D. Note that in Sections 3 and 4, we theoretically proved the reversal curse for both the bilinear model and one-layer transformer under certain assumptions. Now, we empirically show that the reversal curse still happens even for multi-layer transformers. In Appendix E.2.3, we also provide empirical results that the reversal curse does not happen in ICL settings. ", "page_idx": 7}, {"type": "image", "img_path": "QoWf3lo6m7/tmp/942475bc19725bff16d3448f8c531ba15cf359e12c0061b6eaafb6eb5ba98362.jpg", "img_caption": ["Figure 1: Experiment results of reversal curse under default configuration (see Table 3). The curves represent the (average) negative log probability of the model predicting the next token to be $\\mathtt{B}_{i}$ when the input is $^{\\bullet\\bullet}\\mathbb{A}_{i}\\to^{\\bullet\\bullet}$ , or to be ${\\mathtt A}_{i}$ when the input is $\\mathrm{^{\\bullet}B_{\\it i}\\leftarrow}\\mathrm{^{\\bullet}}$ . While the sentences in the training set can be learned nearly perfectly (as shown by the training curve where the next token probability converges to one), the model is not able to predict the correct next token in the validation set better than a uniformly random guess. Both curves are averaged over 10 random seeds. "], "img_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "QoWf3lo6m7/tmp/c335dbb2c7135541c7367d49fe59c5da7062e2d3de49c964f352f1084d3105a6.jpg", "img_caption": ["Figure 2: Visualization of the weights (logits) of the model with default configurations trained after 3000 epochs for the reversal curse experiment. For the top-left matrix, the $i$ -th row corresponds to an entity token ${\\mathtt A}_{i}$ for a training pair, and the $i^{\\th}$ -th column corresponds to an entity token $\\mathtt{B}_{i}$ for a training pair. The $(i,j)$ -th entry represents the model weights from the token ${\\mathtt A}_{i}$ to $\\mathtt{B}_{j}$ , i.e., the logits of $\\mathtt{B}_{j}$ when the input sequence consists of only ${\\mathtt A}_{i}$ . Similarly, for the bottom-left matrix, the row corresponds to the input entity tokens of the seen direction (the direction included in the training set) of validation pairs, and the column corresponds to output entity tokens. The two matrices on the right are obtained by swapping row tokens and column tokens of their corresponding left matrices. Note that the diagonals of the bottom-right matrix are all close to zero, while the diagonals of other matrices all have large values. This implies that if a pair of tokens $(\\mathtt{A},\\mathtt{B})$ only appear in the training set in one direction, then the model weights associated with the other direction will hardly get trained. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Dataset construction. Below, we describe how we generate our synthetic dataset for experiments on the reversal curse. We choose the vocabulary $\\dot{\\mathcal{V}}\\,\\,{=}\\,\\,\\{0,1,\\dots,\\dot{N}\\}$ for a specified $N>0$ . We randomly sample two disjoint sets of entities $A,B\\subset\\nu$ with $\\lvert A\\rvert=\\lvert B\\rvert=\\lvert\\mathcal{V}\\rvert/4$ , and reserve two additional tokens for relationships $\\rightarrow\\mathrm{and}\\leftarrow$ , respectively. 5 Next, we specify a bijection from $\\boldsymbol{\\mathcal{A}}$ to $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ uniformly at random. For each $\\mathtt{A}_{i}\\in\\mathcal{A}$ and its corresponding $\\mathsf{B}_{i}\\in\\mathcal{B}$ , we can obtain a pair of sequence $\\mathtt{A}_{i}\\to\\mathtt{B}_{i}$ , ${\\tt B}_{i}\\gets{\\tt A}_{i}$ ). We split the set of all pairs into training pairs and validation pairs. For each training pair, both sequences will be included in the training set, while for the validation pair, we randomly select one sequence for the training set and the other for the validation set. Therefore, the model will learn both directions for the training pairs and only one direction for each validation pair while being tested in the unseen direction. ", "page_idx": 8}, {"type": "text", "text": "Model architectures. We train multi-layer transformers based on GPT-2 architecture [63]. Figure 1 shows the results where the model has 24 layers, 12 attention heads per layer, uses absolute positional encoding, and we choose the vocabulary size of 800. The training set size is 340, and the validation set size is 60 (resulting from 140 training pairs and 60 validation pairs). We also conducted experiments with various model configurations and vocabulary sizes in Appendix E.2. Besides, all hyperparameters and different model configurations are presented in Appendix E.1. ", "page_idx": 8}, {"type": "text", "text": "Results. Figure 1 shows that during the training, the next token probability for training data increases a lot while the next token probability for validation data remains unchanged or gets even smaller. This is consistent with our theoretical results of Theorem 3. ", "page_idx": 8}, {"type": "text", "text": "According to our theoretical analysis, the reversal curse happens due to the asymmetry of model (reparameterized) weights (i.e., logits of a token given another token as input), and we also empirically validate the asymmetry for multi-layer transformers. Figure 2 shows the model weights from a token $x_{1}$ to $x_{3}$ is trained large for a training sequence $(x_{1},x_{2},x_{3})$ as represented by the diagonals of the first three matrices, while the weights from a token $x_{1}$ to $x_{3}$ remains nearly zero for a validation sequence $(x_{1},x_{2},x_{3})$ as represented by the diagonals of the last matrix, which is consistent with Lemma 2. This implies that if a pair of tokens $(\\mathtt{A},\\mathtt{B})$ only appear in the training set in one direction, then the model weights associated with the other direction will hardly get trained. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "6 Conclusions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we study the reversal curse theoretically via training dynamics of (1) a bilinear model, which is a simplification of the one-layer transformer; (2) one-layer transformers under certain technical assumptions similar to [14]. Our theoretical results suggest that a core reason the reversal curse happens in auto-regressive LLMs is the asymmetry of the model weights, and we apply our technique to prove the necessity of COT for one-layer transformers, which is mainly due to the intransitivity of model weights. The asymmetry and intransitivity of model weights caused by unconstrained optimization of CE loss indicate that an auto-regressive LLM might mainly focus on learning text sequences during training separately instead of automatically deducing indirect conclusions under the current popular training paradigms. This highlights the importance of ICL, data augmentation, or planning for current auto-regressive LLMs to solve complex reasoning tasks. ", "page_idx": 9}, {"type": "text", "text": "As for future directions, it would be interesting and important to study: (1) What is a unified way to characterize and study the reversal curse, COT, and other similar logical reasoning tasks? (2) Our paper mainly focuses on three-token sequences, where each entity or relationship is represented by a single token. While we empirically explored the setting where each entity might consist of multiple tokens and distinct entities might share a few tokens, it would be interesting to analyze the multiple-token setting theoretically. (3) We theoretically analyzed the bilinear model and one-layer transformer, and it would be an important future direction to extend the analysis to multi-layer transformers. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was partially supported by a gift from Open Philanthropy to the Center for HumanCompatible AI (CHAI) at UC Berkeley and by NSF Grants IIS-1901252 and CCF-2211209. The work was done when HZ was a visiting researcher at Meta. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Laria Reynolds and Kyle McDonell. Prompt programming for large language models: Beyond the few-shot paradigm. In Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems, pages 1\u20137, 2021.   \n[2] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35:22199\u201322213, 2022.   \n[3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.   \n[4] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824\u201324837, 2022.   \n[5] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022.   \n[6] Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. Show your work: Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114, 2021.   \n[7] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.   \n[8] Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. Star: Bootstrapping reasoning with reasoning. Advances in Neural Information Processing Systems, 35:15476\u201315488, 2022.   \n[9] Zeyuan Allen-Zhu and Yuanzhi Li. Physics of language models: Part 3.2, knowledge manipulation. arXiv preprint arXiv:2309.14402, 2023.   \n[10] Lukas Berglund, Meg Tong, Max Kaufmann, Mikita Balesni, Asa Cooper Stickland, Tomasz Korbak, and Owain Evans. The reversal curse: Llms trained on\" a is b\" fail to learn\" b is a\". arXiv preprint arXiv:2309.12288, 2023.   \n[11] Qingyan Guo, Rui Wang, Junliang Guo, Xu Tan, Jiang Bian, and Yujiu Yang. Mitigating reversal curse via semantic-aware permutation training. arXiv preprint arXiv:2403.00758, 2024.   \n[12] Olga Golovneva, Zeyuan Allen-Zhu, Jason Weston, and Sainbayar Sukhbaatar. Reverse training to nurse the reversal curse. arXiv preprint arXiv:2403.13799, 2024.   \n[13] Ang Lv, Kaiyi Zhang, Shufang Xie, Quan Tu, Yuhan Chen, Ji-Rong Wen, and Rui Yan. Are we falling in a middle-intelligence trap? an analysis and mitigation of the reversal curse. arXiv preprint arXiv:2311.07468, 2023.   \n[14] Yuandong Tian, Yiping Wang, Beidi Chen, and Simon Du. Scan and snap: Understanding training dynamics and token composition in 1-layer transformer, 2023.   \n[15] Guhao Feng, Bohang Zhang, Yuntian Gu, Haotian Ye, Di He, and Liwei Wang. Towards revealing the mystery behind chain of thought: a theoretical perspective. Advances in Neural Information Processing Systems, 36, 2024.   \n[16] Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. In International Conference on Machine Learning, pages 9118\u20139147. PMLR, 2022.   \n[17] Jaehun Jung, Lianhui Qin, Sean Welleck, Faeze Brahman, Chandra Bhagavatula, Ronan Le Bras, and Yejin Choi. Maieutic prompting: Logically consistent reasoning with recursive explanations. arXiv preprint arXiv:2205.11822, 2022.   \n[18] Simon Jerome Han, Keith J Ransom, Andrew Perfors, and Charles Kemp. Inductive reasoning in humans and large language models. Cognitive Systems Research, 83:101155, 2024.   \n[19] Xinyi Wang, Alfonso Amayuelas, Kexun Zhang, Liangming Pan, Wenhu Chen, and William Yang Wang. Understanding the reasoning ability of language models from the perspective of reasoning paths aggregation. arXiv preprint arXiv:2402.03268, 2024.   \n[20] Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of in-context learning as implicit bayesian inference. arXiv preprint arXiv:2111.02080, 2021.   \n[21] Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, et al. In-context learning and induction heads. arXiv preprint arXiv:2209.11895, 2022.   \n[22] Eshaan Nichani, Alex Damian, and Jason D Lee. How transformers learn causal structure with gradient descent. arXiv preprint arXiv:2402.14735, 2024.   \n[23] Jannik Brinkmann, Abhay Sheshadri, Victor Levoso, Paul Swoboda, and Christian Bartelt. A mechanistic analysis of a transformer trained on a symbolic multi-step reasoning task. arXiv preprint arXiv:2402.11917, 2024.   \n[24] Karthik Valmeekam, Alberto Olmo, Sarath Sreedharan, and Subbarao Kambhampati. Large language models still can\u2019t plan (a benchmark for llms on planning and reasoning about change). arXiv preprint arXiv:2206.10498, 2022.   \n[25] Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, et al. A survey on evaluation of large language models. ACM Transactions on Intelligent Systems and Technology, 2023.   \n[26] Dylan Zhang, Curt Tigges, Zory Zhang, Stella Biderman, Maxim Raginsky, and Talia Ringer. Transformer-based models are not yet perfect at learning to emulate structural recursion. arXiv preprint arXiv:2401.12947, 2024.   \n[27] Chengwen Qi, Bowen Li, Binyuan Hui, Bailin Wang, Jinyang Li, Jinwang Wu, and Yuanjun Laili. An investigation of llms\u2019 inefficacy in understanding converse relations. arXiv preprint arXiv:2310.05163, 2023.   \n[28] Ruilin Luo, Tianle Gu, Haoling Li, Junzhe Li, Zicheng Lin, Jiayi Li, and Yujiu Yang. Chain of history: Learning and forecasting with llms for temporal knowledge graph completion. arXiv preprint arXiv:2401.06072, 2024.   \n[29] Chulhee Yun, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank J Reddi, and Sanjiv Kumar. Are transformers universal approximators of sequence-to-sequence functions? arXiv preprint arXiv:1912.10077, 2019.   \n[30] Satwik Bhattamishra, Kabir Ahuja, and Navin Goyal. On the ability and limitations of transformers to recognize formal languages. arXiv preprint arXiv:2009.11264, 2020.   \n[31] Satwik Bhattamishra, Arkil Patel, and Navin Goyal. On the computational power of transformers and its implications in sequence modeling. arXiv preprint arXiv:2006.09286, 2020.   \n[32] Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and \u0141ukasz Kaiser. Universal transformers. arXiv preprint arXiv:1807.03819, 2018.   \n[33] Jorge P\u00e9rez, Pablo Barcel\u00f3, and Javier Marinkovic. Attention is turing-complete. Journal of Machine Learning Research, 22(75):1\u201335, 2021.   \n[34] Benjamin L Edelman, Surbhi Goel, Sham Kakade, and Cyril Zhang. Inductive biases and variable creation in self-attention mechanisms. In International Conference on Machine Learning, pages 5793\u20135831. PMLR, 2022.   \n[35] Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, et al. A mathematical framework for transformer circuits. Transformer Circuits Thread, 1:1, 2021.   \n[36] Valerii Likhosherstov, Krzysztof Choromanski, and Adrian Weller. On the expressive power of self-attention matrices. arXiv preprint arXiv:2106.03764, 2021.   \n[37] Ekin Aky\u00fcrek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. What learning algorithm is in-context learning? investigations with linear models. arXiv preprint arXiv:2211.15661, 2022.   \n[38] Haoyu Zhao, Abhishek Panigrahi, Rong Ge, and Sanjeev Arora. Do transformers parse while predicting the masked word? arXiv preprint arXiv:2303.08117, 2023.   \n[39] Shunyu Yao, Binghui Peng, Christos Papadimitriou, and Karthik Narasimhan. Self-attention networks can process bounded hierarchical languages. arXiv preprint arXiv:2105.11115, 2021.   \n[40] Cem Anil, Yuhuai Wu, Anders Andreassen, Aitor Lewkowycz, Vedant Misra, Vinay Ramasesh, Ambrose Slone, Guy Gur-Ari, Ethan Dyer, and Behnam Neyshabur. Exploring length generalization in large language models. arXiv preprint arXiv:2207.04901, 2022.   \n[41] Boaz Barak, Benjamin Edelman, Surbhi Goel, Sham Kakade, Eran Malach, and Cyril Zhang. Hidden progress in deep learning: Sgd learns parities near the computational limit. Advances in Neural Information Processing Systems, 35:21750\u201321764, 2022.   \n[42] Shivam Garg, Dimitris Tsipras, Percy S Liang, and Gregory Valiant. What can transformers learn in-context? a case study of simple function classes. Advances in Neural Information Processing Systems, 35:30583\u201330598, 2022.   \n[43] Johannes Von Oswald, Eyvind Niklasson, Ettore Randazzo, Jo\u00e3o Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by gradient descent. arXiv preprint arXiv:2212.07677, 2022.   \n[44] Yu Bai, Fan Chen, Huan Wang, Caiming Xiong, and Song Mei. Transformers as statisticians: Provable in-context learning with in-context algorithm selection. arXiv preprint arXiv:2306.04637, 2023.   \n[45] Shuai Li, Zhao Song, Yu Xia, Tong Yu, and Tianyi Zhou. The closeness of in-context learning and weight shifting for softmax regression. arXiv preprint arXiv:2304.13276, 2023.   \n[46] Johannes Von Oswald, Eyvind Niklasson, Ettore Randazzo, Jo\u00e3o Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by gradient descent. In International Conference on Machine Learning, pages 35151\u201335174. PMLR, 2023.   \n[47] Bingbin Liu, Jordan T Ash, Surbhi Goel, Akshay Krishnamurthy, and Cyril Zhang. Transformers learn shortcuts to automata. arXiv preprint arXiv:2210.10749, 2022.   \n[48] Colin Wei, Yining Chen, and Tengyu Ma. Statistically meaningful approximation: a case study on approximating turing machines with transformers. Advances in Neural Information Processing Systems, 35:12071\u201312083, 2022.   \n[49] Song Mei and Yuchen Wu. Deep networks as denoising algorithms: Sample-efficient learning of diffusion models in high-dimensional graphical models. arXiv preprint arXiv:2309.11420, 2023.   \n[50] Licong Lin, Yu Bai, and Song Mei. Transformers as decision makers: Provable in-context reinforcement learning via supervised pretraining. arXiv preprint arXiv:2310.08566, 2023.   \n[51] Jingzhao Zhang, Tianxing He, Suvrit Sra, and Ali Jadbabaie. Why gradient clipping accelerates training: A theoretical justification for adaptivity. In International Conference on Learning Representations, 2020.   \n[52] Jiri Hron, Yasaman Bahri, Jascha Sohl-Dickstein, and Roman Novak. Infinite attention: Nngp and ntk for deep attention networks. In International Conference on Machine Learning, pages 4376\u20134386. PMLR, 2020.   \n[53] Greg Yang, Edward J Hu, Igor Babuschkin, Szymon Sidor, Xiaodong Liu, David Farhi, Nick Ryder, Jakub Pachocki, Weizhu Chen, and Jianfeng Gao. Tensor programs v: Tuning large neural networks via zero-shot hyperparameter transfer. arXiv preprint arXiv:2203.03466, 2022.   \n[54] Enric Boix-Adsera, Etai Littwin, Emmanuel Abbe, Samy Bengio, and Joshua Susskind. Transformers learn through gradual rank increase. arXiv preprint arXiv:2306.07042, 2023.   \n[55] Alberto Bietti, Vivien Cabannes, Diane Bouchacourt, Herve Jegou, and Leon Bottou. Birth of a transformer: A memory viewpoint. arXiv preprint arXiv:2306.00802, 2023.   \n[56] Samy Jelassi, Michael Sander, and Yuanzhi Li. Vision transformers provably learn spatial structure. Advances in Neural Information Processing Systems, 35:37822\u201337836, 2022.   \n[57] Charlie Snell, Ruiqi Zhong, Dan Klein, and Jacob Steinhardt. Approximating how single head attention learns. arXiv preprint arXiv:2103.07601, 2021.   \n[58] Arvind Mahankali, Tatsunori B Hashimoto, and Tengyu Ma. One step of gradient descent is provably the optimal in-context learner with one layer of linear self-attention. arXiv preprint arXiv:2307.03576, 2023.   \n[59] Ruiqi Zhang, Spencer Frei, and Peter L Bartlett. Trained transformers learn linear models in-context. arXiv preprint arXiv:2306.09927, 2023.   \n[60] Hengyu Fu, Tianyu Guo, Yu Bai, and Song Mei. What can a single attention layer learn? a study through the random features lens. Advances in Neural Information Processing Systems, 36, 2024.   \n[61] Yu Huang, Yuan Cheng, and Yingbin Liang. In-context convergence of transformers. arXiv preprint arXiv:2310.05249, 2023.   \n[62] Yuandong Tian, Yiping Wang, Zhenyu Zhang, Beidi Chen, and Simon Du. Joma: Demystifying multilayer transformers via joint dynamics of mlp and attention. arXiv preprint arXiv:2310.00535, 2023.   \n[63] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-theart natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38\u201345, Online, October 2020. Association for Computational Linguistics. URL https://www.aclweb.org/anthology/ 2020.emnlp-demos.6.   \n[64] Beatrice Laurent and Pascal Massart. Adaptive estimation of a quadratic functional by model selection. Annals of Statistics, pages 1302\u20131338, 2000.   \n[65] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Additional Notations ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Let $\\delta_{i j}\\;=\\;1$ for $i\\;=\\;j$ and $\\delta_{i j}~=~0$ for $i\\ne j$ . For a squared matrix $A\\,\\in\\,\\mathbb{R}^{d\\times d}$ , its trace is $\\textstyle\\operatorname{Tr}(A)\\,=\\,\\sum_{i=1}^{d}A_{i i}$ . For two matrices $A,B\\,\\in\\,\\mathbb{R}^{m\\times n}$ of the same shape, their inner product is defined as $\\langle{\\dot{A}},{\\dot{B}}\\rangle\\,=\\,\\mathrm{Tr}(A B^{\\top})$ . For any matrix $A\\,\\in\\,\\mathbb{R}^{m\\times n}$ , its (Frobenius) norm is defined as $\\|A\\|={\\sqrt{\\langle A,A\\rangle}}$ . For any vector $\\pmb{x}\\,=\\,(x_{1},\\dots,x_{d})^{\\top}\\,\\in\\,\\mathbb{R}^{d}$ or a matrix $A\\,\\in\\,\\mathbb{R}^{m\\times n}$ , we define the zero-norm as $\\begin{array}{r}{\\|\\pmb{x}\\|_{0}=\\sum_{i=1}^{d}\\mathbb{1}\\{\\pmb{x}_{i}\\neq0\\}}\\end{array}$ or $\\begin{array}{r}{\\|A\\|_{0}{=}\\sum_{i=1}^{m}\\sum_{j=1}^{n}\\mathbb{1}\\{A_{i j}\\neq0\\}}\\end{array}$ where $\\mathbb{1}\\{\\cdot\\}$ is the indicator function. ", "page_idx": 13}, {"type": "text", "text": "B Missing Proofs of Section 3 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Theorem 5 (Separation of training dynamics, formal statement of Theorem 1). Fix arbitrary $\\delta,\\epsilon\\in$ $(0,1)$ . Let $v_{1},\\ldots,v_{m}$ be independently sampled from $\\begin{array}{r}{\\mathcal{N}_{d}(\\mathbf{0}_{d},\\frac{1}{d}I_{d})}\\end{array}$ . Let $x_{1},\\ldots,x_{n}$ and $y_{1},\\ldots,y_{n}$ be sampled uniformly at random from $\\{v_{1},\\ldots,v_{m}\\}$ without replacement. Define ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathcal{L}(\\Theta)=\\displaystyle\\frac{1}{2n-1}\\left(\\sum_{i=1}^{n}-\\log p_{\\Theta}(y_{i}|x_{i})+\\sum_{i=2}^{n}-\\log p_{\\Theta}(x_{i}|y_{i})\\right)}&{}\\\\ {\\mathcal{L}^{r e v}(\\Theta)=\\displaystyle-\\log p_{\\Theta}(x_{1}|y_{1}).}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Consider the gradient flow $\\Theta_{t}:t\\geq0$ ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\frac{d\\Theta_{t}}{d t}=-\\nabla\\mathcal{L}(\\Theta_{t}).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\Theta_{0}\\sim\\mathcal{N}(\\mathbf{0}^{\\otimes2},\\sigma^{2}\\cdot I^{\\otimes2})$ or $\\Theta_{0}$ satisfies $\\begin{array}{r}{\\frac{1}{2m}\\,<\\,p\\Theta_{0}\\big(y_{i}|x_{i}\\big),p\\Theta_{0}\\big(x_{i}|y_{i}\\big)\\,<\\,\\frac{2}{m}}\\end{array}$ for all $i\\,\\in\\,[n]$ Suppose $\\begin{array}{r}{\\sigma\\leq\\frac{1}{100\\ln(64m^{2}/\\delta)}}\\end{array}$ and ", "page_idx": 13}, {"type": "equation", "text": "$$\nd\\geq\\frac{10^{6}n^{4}m^{2}\\log^{4}(2m)\\log(64m^{2}n^{2}/\\delta)}{\\epsilon^{2}}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "With probability at least $1-\\delta$ , we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\frac{\\mathcal{L}^{r e v}(\\Theta_{t})}{\\mathcal{L}^{r e v}(\\Theta_{0})}\\geq\\left(\\frac{\\mathcal{L}(\\Theta_{t})}{\\mathcal{L}(\\Theta_{0})}\\right)^{\\epsilon},\\,\\forall t\\geq0.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Proof. Let 400n2m2 logd(64m2n2/\u03b4). By Lemma 3 and Lemma 4, with probability at least 1 \u2212\u03b4, ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}^{\\mathrm{rev}}(\\Theta_{t})\\geq\\mathcal{L}^{\\mathrm{rev}}(\\Theta_{0})\\cdot\\bigg(1+\\cfrac{\\mathcal{L}(\\Theta_{0})\\cdot t}{8(2n-1)\\log^{2}(2m)}\\bigg)^{-8v(2n-1)\\log^{2}(2m)}\\,.}\\\\ &{\\qquad\\qquad\\geq\\mathcal{L}^{\\mathrm{rev}}(\\Theta_{0})\\cdot\\bigg(\\cfrac{\\mathcal{L}(\\Theta_{t})}{\\mathcal{L}(\\Theta_{0})}\\bigg)^{8v(2n-1)\\log^{2}(2m)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "By definition of d, we have 8v(2n \u22121) log2(2m) \u2264\u03f5. Notice that LL((\u0398\u03980t)) , thus ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathcal{L}^{\\mathrm{rev}}(\\Theta_{t})\\geq\\mathcal{L}^{\\mathrm{rev}}(\\Theta_{0})\\cdot\\left(\\frac{\\mathcal{L}(\\Theta_{t})}{\\mathcal{L}(\\Theta_{0})}\\right)^{\\epsilon}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Theorem 6 (Lower bound of reversal loss, formal statement of Theorem 2). Fix arbitrary $c>0$ and $C\\leq\\log(m/2)$ . Under the setting of Theorem 5, suppose $\\begin{array}{r}{\\sigma\\leq\\frac{1}{100\\ln\\left(64m^{2}/\\delta\\right)}}\\end{array}$ and ", "page_idx": 13}, {"type": "equation", "text": "$$\nd\\geq10^{6}n^{4}m^{2}\\log^{4}(2m)\\log(64m^{2}n^{2}/\\delta)\\cdot{\\frac{\\log^{2}{\\frac{c}{\\log(2m)}}}{\\log^{2}{\\frac{C}{\\log(m/2)}}}}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "With probability at least $1-\\delta$ , ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathcal{L}^{r e v}(\\Theta_{\\tau})\\geq C.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\tau$ denotes the first time such that $\\mathcal{L}(\\Theta_{t})\\leq c$ . ", "page_idx": 13}, {"type": "text", "text": "Proof. By continuity, $\\mathcal{L}(\\Theta_{\\tau})=c$ . By Theorem 5, when ", "page_idx": 14}, {"type": "equation", "text": "$$\nd\\geq{\\frac{10^{6}n^{4}m^{2}\\log^{4}(2m)\\log(64m^{2}n^{2}/\\delta)}{\\epsilon^{2}}}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "with probability at least $1-\\delta$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}^{\\mathrm{rev}}(\\Theta_{\\tau})\\geq\\mathcal{L}^{\\mathrm{rev}}(\\Theta_{0})\\cdot\\left(\\cfrac{\\mathcal{L}(\\Theta_{\\tau})}{\\mathcal{L}(\\Theta_{0})}\\right)^{\\epsilon}}\\\\ {\\geq\\mathcal{L}^{\\mathrm{rev}}(\\Theta_{0})\\cdot\\left(\\cfrac{c}{\\mathcal{L}(\\Theta_{0})}\\right)^{\\epsilon}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Under this event, applying Lemma 6, we can obtain ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathcal{L}^{\\mathrm{rev}}(\\Theta_{\\tau})\\geq\\log(m/2)\\cdot\\left(\\frac{c}{\\log(2m)}\\right)^{\\epsilon}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "To ensure that the right hand side is $C$ , we set $\\begin{array}{r}{\\epsilon=\\frac{\\log\\frac{C}{\\log(m/2)}}{\\log\\frac{c}{\\log(2m)}}}\\end{array}$ . One may check that the definition of $d$ satisfies Eq. (4). It follows that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathcal{L}^{\\mathrm{rev}}(\\Theta_{\\tau})\\geq C.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "B.1 Training dynamics ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Lemma 3 (Dynamics of the forward loss). Let $x_{1},\\ldots,x_{n},\\ y_{1},\\ldots,y_{n},\\ {\\mathcal{L}}(\\Theta)$ , and $\\Theta_{t}$ $\\mathit{\\Pi}^{\\prime}t\\;\\geq\\;0$ ) be defined as in Theorem 5. When $\\begin{array}{r}{\\sigma\\,\\leq\\,\\frac{1}{100\\ln(16n^{2}/\\delta)}}\\end{array}$ and $d\\,\\geq\\,1600n^{3}m^{2}\\log(8m^{2}n^{2}/\\dot{\\delta})$ , with probability at least $1-\\delta$ , we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\Theta_{t})\\leq\\frac{1}{\\frac{t}{8(2n-1)\\log^{2}(2m)}+\\frac{1}{\\mathcal{L}(\\Theta_{0})}},\\;\\forall t\\geq0.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Furthermore, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname*{inf}\\left\\{p_{\\Theta_{t}}(x_{i}|y_{i}),p_{\\Theta_{t}}(y_{i}|x_{i}):t\\geq0,i\\in[n]\\right\\}>\\frac{1}{2m}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Proof. For convenience, we assume $x_{1},\\ldots,x_{n}\\ =\\ v_{1},\\ldots,v_{n}$ and $y_{1},...\\,,y_{n}~=~v_{n+1},...\\,v_{2n}$ WLOG. ", "page_idx": 14}, {"type": "text", "text": "Let 400n2m2 lodg(8m2n2/\u03b4). Then \u03f5 \u2264 2\u221a1n. Define li(\u0398) = \u2212log p\u0398(yi|xi) and lirev(\u0398) = \u2212log p\u0398(xi|yi). Let \u03b1i(,tj) $\\alpha_{i,j}^{(t)}=-p_{\\Theta_{t}}(v_{j}|v_{i})+\\delta_{i,j-n},\\beta_{i,j}^{(t)}=-p_{\\Theta_{t}}(v_{j}|v_{i})+\\delta_{i-n,j}.$ By Lemma 5, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{\\displaystyle\\frac{d\\mathcal{L}(\\boldsymbol{\\Theta}_{t})}{d t}=\\left\\langle\\nabla\\mathcal{L}(\\boldsymbol{\\Theta}_{t}),\\frac{d\\boldsymbol{\\Theta}_{t}}{d t}\\right\\rangle=-\\left\\langle\\nabla\\mathcal{L}(\\boldsymbol{\\Theta}_{t}),\\nabla\\mathcal{L}(\\boldsymbol{\\Theta}_{t})\\right\\rangle}\\\\ {\\displaystyle\\qquad=\\,-\\left\\|\\frac{1}{2n-1}\\left(\\sum_{i=1}^{n}x_{i}(y_{i}-\\mathbb{E}_{p_{\\boldsymbol{\\Theta}_{t}}(\\cdot|x_{i})}[y])^{\\top}+\\sum_{i=2}^{n}y_{i}(x_{i}-\\mathbb{E}_{p_{\\boldsymbol{\\Theta}_{t}}(\\cdot|y_{i})}[x])^{\\top}\\right)\\right\\|^{2}}\\\\ {\\displaystyle\\qquad=\\,-\\left\\|\\frac{1}{2n-1}\\left(\\sum_{i=1}^{n}\\sum_{j=1}^{m}\\alpha_{i,j}^{(t)}v_{i}v_{j}^{\\top}+\\sum_{i=n+2}^{2n}\\sum_{j=1}^{m}\\beta_{i,j}^{(t)}v_{i}v_{j}^{\\top}\\right)\\right\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Similarly, we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\frac{d l_{i}(\\Theta_{t})}{d t}}\\\\ &{=\\left\\langle\\nabla l_{i}(\\Theta_{t}),\\frac{d\\Theta_{t}}{d t}\\right\\rangle=-\\left\\langle\\nabla l_{i}(\\Theta_{t}),\\nabla\\mathcal{L}(\\Theta_{t})\\right\\rangle}\\\\ &{=\\,-\\left\\langle x_{i}(y_{i}-\\mathbb{E}_{p_{\\Theta_{t}}(\\cdot|x_{i})}[y])^{\\top},\\frac{1}{2n-1}\\left(\\displaystyle\\sum_{i=1}^{n}x_{i}(y_{i}-\\mathbb{E}_{p_{\\Theta_{t}}(\\cdot|x_{i})}[y])^{\\top}+\\displaystyle\\sum_{i=2}^{n}y_{i}(x_{i}-\\mathbb{E}_{p_{\\Theta_{t}}(\\cdot|y_{i})}[x])\\right)}\\\\ &{=\\,-\\left\\langle\\displaystyle\\sum_{j=1}^{m}\\alpha_{i,j}v_{i}v_{j}^{\\top},\\frac{1}{2n-1}\\left(\\displaystyle\\sum_{i=1}^{n}\\sum_{j=1}^{m}\\alpha_{i,j}^{(t)}v_{i}v_{j}^{\\top}+\\displaystyle\\sum_{i=n+2}^{2n}\\sum_{j=1}^{m}\\beta_{i,j}^{(t)}v_{i}v_{j}^{\\top}\\right)\\right\\rangle,}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\frac{d l_{i}^{\\mathrm{rev}}(\\boldsymbol{\\Theta}_{t})}{d t}}\\\\ &{=\\left\\langle\\nabla l_{i}^{\\mathrm{rev}}(\\boldsymbol{\\Theta}_{t}),\\frac{d\\boldsymbol{\\Theta}_{t}}{d t}\\right\\rangle=-\\left\\langle\\nabla l_{i}^{\\mathrm{rev}}(\\boldsymbol{\\Theta}_{t}),\\nabla\\mathcal{L}(\\boldsymbol{\\Theta}_{t})\\right\\rangle}\\\\ &{=\\,-\\left\\langle y_{i}(x_{i}-\\mathbb{E}_{p\\in_{t}(\\cdot\\vert y_{i})}[x])^{\\top},\\frac{1}{2n-1}\\left(\\displaystyle\\sum_{i=1}^{n}x_{i}\\big(y_{i}-\\mathbb{E}_{p\\in_{t}(\\cdot\\vert x_{i})}[y]\\big)^{\\top}+\\displaystyle\\sum_{i=2}^{n}y_{i}\\big(x_{i}-\\mathbb{E}_{p\\in_{t}(\\cdot\\vert y_{i})}[x]\\big)^{\\top}\\right)}\\\\ &{=\\,-\\left\\langle\\displaystyle\\sum_{j=1}^{m}\\beta_{i+n,j}^{(t)}v_{i+n}v_{j}^{\\top},\\frac{1}{2n-1}\\left(\\displaystyle\\sum_{i=1}^{n}\\sum_{j=1}^{m}\\alpha_{i,j}^{(t)}v_{i}v_{j}^{\\top}+\\displaystyle\\sum_{i=n+2}^{2n}\\sum_{j=1}^{m}\\beta_{i,j}^{(t)}v_{i}v_{j}^{\\top}\\right)\\right\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Applying Lemma 7, with probability at least $1-\\delta/2$ , for any $t\\geq0$ we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\left|\\frac{d\\mathcal{L}(\\Theta_{t})}{d t}+\\frac{1}{(2n-1)^{2}}\\left(\\sum_{i=1}^{n}\\sum_{j=1}^{m}(\\alpha_{i,j}^{(t)})^{2}+\\sum_{i=n+2}^{2n}\\sum_{j=1}^{m}(\\beta_{i,j}^{(t)})^{2}\\right)\\right|}\\\\ &{\\leq\\!\\epsilon\\cdot\\frac{1}{(2n-1)^{2}}\\left(\\sum_{i=1}^{n}\\sum_{j=1}^{m}(\\alpha_{i,j}^{(t)})^{2}+\\sum_{i=n+2}^{2n}\\sum_{j=1}^{m}(\\beta_{i,j}^{(t)})^{2}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\left|\\frac{d l_{i}(\\Theta_{t})}{d t}+\\frac{1}{2n-1}\\sum_{j=1}^{m}(\\alpha_{i,j}^{(t)})^{2}\\right|}\\\\ &{\\displaystyle\\leq\\epsilon\\cdot\\frac{1}{2n-1}\\left(\\sum_{j=1}^{m}(\\alpha_{i,j}^{(t)})^{2}\\right)^{1/2}\\left(\\sum_{i=1}^{n}\\sum_{j=1}^{m}(\\alpha_{i,j}^{(t)})^{2}+\\sum_{i=n+2}^{2n}\\sum_{j=1}^{m}(\\beta_{i,j}^{(t)})^{2}\\right)^{1/2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\left|\\frac{d l_{i}^{\\mathrm{\\tiny{rev}}}(\\Theta_{t})}{d t}+\\frac{1}{2n-1}\\sum_{j=1}^{m}(\\beta_{i+n,j}^{(t)})^{2}\\right|}\\\\ &{\\displaystyle\\leq\\epsilon\\cdot\\frac{1}{2n-1}\\left(\\sum_{j=1}^{m}(\\beta_{i+n,j}^{(t)})^{2}\\right)^{1/2}\\left(\\sum_{i=1}^{n}\\sum_{j=1}^{m}(\\alpha_{i,j}^{(t)})^{2}+\\sum_{i=n+2}^{2n}\\sum_{j=1}^{m}(\\beta_{i,j}^{(t)})^{2}\\right)^{1/2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Furthermore, Lemma 6 implies that with probability at least $1-\\delta/2$ ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{1}{2m}<p\\L_{\\Theta_{0}}(y_{i}|x_{i}),p\\L_{\\Theta_{0}}(x_{i}|y_{i})<\\frac{2}{m}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The following arguments are based on the event that the above inequalities hold. ", "page_idx": 15}, {"type": "text", "text": "We first show that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{inf}\\left\\{p_{\\Theta_{t}}(x_{i}|y_{i}),p_{\\Theta_{t}}(y_{i}|x_{i}):t\\geq0,i\\in[n]\\right\\}>\\frac{1}{2m}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We prove this by contradiction. Let ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\tau=\\operatorname*{inf}\\left\\{t\\geq0:\\exists i\\in[n],\\operatorname{s.t.}\\operatorname*{min}\\{p_{\\Theta_{t}}(y_{i}|x_{i}),p_{\\Theta_{t}}(x_{i}|y_{i})\\}\\leq{\\frac{1}{2m}}\\right\\}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "IBt yf oEllqo. w(s7 )t,h iatt  itsh eorbev ieoxuisst st $\\tau\\,>\\,0$ c. h Athssatu ilso sas  doefc rgeeanseirnagl itfyu ntchtaito $\\begin{array}{r}{p_{\\Theta_{\\tau}}(y_{1}|x_{1})\\,\\le\\,\\frac{1}{2m}}\\end{array}$ . $\\delta>0$ $p_{\\Theta_{t}}(y_{1}|x_{1})$ $(\\tau-\\delta,\\tau)$ $p_{\\Theta_{t}}(y_{1}|x_{1})\\,=\\,\\mathrm{min}\\left\\{p_{\\Theta_{t}}(x_{i}|y_{i}),p_{\\Theta_{t}}(y_{i}|x_{i})\\,\\stackrel{!}{:}i\\,\\stackrel{!}{\\in}\\,\\overline{{[n]}}\\right\\}$ for any $t\\,\\in\\,(\\tau-\\delta,\\tau)$ . It follows that for $t\\in(\\tau-\\delta,\\tau)$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{l_{1}(\\theta_{t})}{d t}\\leq\\frac{1}{2n-1}\\left(-\\sum_{j=1}^{m}(\\alpha_{1,j}^{(t)})^{2}+\\epsilon\\cdot\\left(\\sum_{j=1}^{m}(\\alpha_{1,j}^{(t)})^{2}\\right)^{1/2}\\left(\\sum_{i=1}^{n}\\sum_{j=1}^{m}(\\alpha_{i,j}^{(t)})^{2}+\\sum_{i=n+2}^{2n}\\sum_{j=1}^{m}(\\beta_{i,j}^{(t)})^{2}\\right)^{1/2}\\right.}\\\\ &{\\displaystyle\\qquad\\qquad\\leq\\frac{1}{2n-1}\\left(-\\sum_{j=1}^{m}(\\alpha_{1,j}^{(t)})^{2}+\\epsilon\\cdot\\left(\\sum_{j=1}^{m}(\\alpha_{1,j}^{(t)})^{2}\\right)^{1/2}\\left(2\\sum_{i=1}^{n}(\\alpha_{i,i+n}^{(t)})^{2}+2\\sum_{i=n+2}^{2n}(\\beta_{i,i-n}^{(t)})^{2}\\right)^{1/2}}\\\\ &{\\displaystyle\\qquad\\qquad\\leq\\frac{1}{2n-1}\\left(\\sum_{j=1}^{m}(\\alpha_{1,j}^{(t)})^{2}\\right)^{1/2}\\cdot\\left(-\\left(\\sum_{j=1}^{m}(\\alpha_{1,j}^{(t)})^{2}\\right)^{1/2}+\\epsilon\\cdot\\left(4n(\\alpha_{1,n+1}^{(t)})^{2}\\right)^{1/2}\\right)}\\\\ &{\\displaystyle\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\left.\\,}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the first inequality is from Eq. (6); the second inequality is due to $\\begin{array}{r l}{\\sum_{j\\neq i+n}\\lvert\\alpha_{i,j}^{(t)}\\rvert{=}\\alpha_{i,i+n}^{(t)}=}&{{}}\\end{array}$ $\\begin{array}{r}{1\\,-\\,p_{\\Theta_{t}}(y_{i}|x_{i}),\\sum_{j\\neq i}\\lvert\\beta_{i+n,j}^{(t)}\\rvert{=\\,\\beta_{i+n,i}^{(t)}\\,=\\,1-p_{\\Theta_{t}}(x_{i}|y_{i})}}\\end{array}$ for all $i\\;\\in\\;[n]$ ; the third inequality is because $p_{\\Theta_{t}}(y_{1}|x_{1})=\\operatorname*{min}\\left\\{p_{\\Theta_{t}}(x_{i}|y_{i}),p_{\\Theta_{t}}(y_{i}|x_{i}):i\\in[n]\\right\\}$ . However, $p_{\\Theta_{t}}(y_{1}|x_{1})$ is a decreasing function in $(\\tau-\\delta,\\tau)$ , a contradiction. Therefore, we conclude that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{inf}\\left\\{p_{\\Theta_{t}}(x_{i}|y_{i}),p_{\\Theta_{t}}(y_{i}|x_{i}):t\\geq0,i\\in[n]\\right\\}>\\frac{1}{2m}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Now we show ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\Theta_{t})\\leq\\frac{1}{\\frac{t}{8(2n-1)\\log^{2}(2m)}+\\frac{1}{\\mathcal{L}(\\Theta_{0})}},\\;\\forall t\\geq0.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "By Eq. (5), ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{d\\mathcal{L}(\\boldsymbol{\\Theta}_{t})}{d t}\\leq-\\,\\frac{1-\\epsilon}{(2n-1)^{2}}\\left(\\sum_{i=1}^{n}\\sum_{j=1}^{m}(\\alpha_{i,j}^{(t)})^{2}+\\sum_{i=n+2}^{2n}\\sum_{j=1}^{m}(\\beta_{i,j}^{(t)})^{2}\\right)}\\\\ &{\\qquad\\,\\leq\\,-\\,\\frac{1-\\epsilon}{(2n-1)^{2}}\\left(\\sum_{i=1}^{n}(1-p_{\\Theta_{t}}(y_{i}|x_{i}))^{2}+\\sum_{i=2}^{n}(1-p_{\\Theta_{t}}(x_{i}|y_{i}))^{2}\\right)}\\\\ &{\\qquad\\,\\leq\\,-\\,\\frac{1-\\epsilon}{(2n-1)^{3}}\\left(\\sum_{i=1}^{n}(1-p_{\\Theta_{t}}(y_{i}|x_{i}))+\\sum_{i=2}^{n}(1-p_{\\Theta_{t}}(x_{i}|y_{i}))\\right)^{2}}\\\\ &{\\qquad\\,\\leq\\,-\\,\\frac{1-\\epsilon}{8(2n-1)\\log^{2}(2m)}Z(\\boldsymbol{\\Theta}_{t})^{2}}\\\\ &{\\qquad\\,\\leq\\,-\\,\\frac{1}{8(2n-1)\\log^{2}(2m)}Z(\\boldsymbol{\\Theta}_{t})^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the second inequality is due to $\\begin{array}{r}{\\sum_{j\\neq i+n}\\lvert\\alpha_{i,j}^{(t)}\\rvert{=\\alpha_{i,i+n}^{(t)}=1-p_{\\Theta_{t}}(y_{i}\\lvert x_{i}),\\sum_{j\\neq i}\\lvert\\beta_{i+n,j}^{(t)}\\rvert}=}\\end{array}$ $\\beta_{i+n,i}^{(t)}=1-p_{\\Theta_{t}}(x_{i}|y_{i})$ for all $i\\,\\in\\,[n]$ ; the third inequality applies Cauchy-Schwarz inequality; ", "page_idx": 16}, {"type": "text", "text": "the last inequality uses the fact that $\\begin{array}{r}{p_{\\Theta_{t}}(x_{i}|y_{i}),p_{\\Theta_{t}}(y_{i}|x_{i})\\,>\\,\\frac{1}{2m}}\\end{array}$ for any $t\\,\\geq\\,0,i\\,\\in\\,[n]$ and the inequality $\\begin{array}{r}{1-x\\geq\\frac{\\log x}{2\\log(1/(2m))}}\\end{array}$ for $\\textstyle x\\in\\left({\\frac{1}{2m}},1\\right)$ . ", "page_idx": 17}, {"type": "text", "text": "By Lemma 10, we conclude that $\\forall t\\ge0$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}(\\Theta_{t})\\leq\\frac{1}{\\frac{t}{8(2n-1)\\log^{2}(2m)}+\\frac{1}{\\mathcal{L}(\\Theta_{0})}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "which completes the proof. ", "page_idx": 17}, {"type": "text", "text": "Lemma 4 (Dynamics of the reversal loss). Let $x_{1},\\ldots,x_{n},\\,y_{1},\\ldots,y_{n},\\,\\mathcal{L}^{r e v}(\\Theta)$ , and $\\Theta_{t}$ $[\\ t\\geq0]$ ) be defined as in Theorem $5$ . When $\\begin{array}{r}{\\sigma\\,\\leq\\,\\frac{1}{100\\ln(16n^{2}/\\delta)}}\\end{array}$ and $d\\stackrel{\\cdot}{\\geq}400\\bar{n}^{2}m^{2}\\log(8{\\dot{m}}^{2}n^{2}/\\delta)/\\epsilon^{2}$ , with probability at least $1-\\delta$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathcal{L}^{r e v}(\\Theta_{t})\\geq\\mathcal{L}^{r e v}(\\Theta_{0})\\cdot\\left(1+\\frac{\\mathcal{L}(\\Theta_{0})\\cdot t}{8(2n-1)\\log^{2}(2m)}\\right)^{-8\\epsilon(2n-1)\\log^{2}(2m)}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof. Similar to Lemma 3, we assume $x_{1},\\ldots,x_{n}=v_{1},\\ldots,v_{n}$ and $y_{1},\\ldots,y_{n}\\,=\\,v_{n+1},\\ldots v_{2n}$ WLOG. Let \u03b1i(,tj) $\\alpha_{i,j}^{(t)}=-p_{\\Theta_{t}}(v_{j}|v_{i})+\\delta_{i,j-n},\\beta_{i,j}^{(t)}=-p_{\\Theta_{t}}(v_{j}|v_{i})+\\delta_{i-n,j}$ . By Lemma 5, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{c l}{\\displaystyle}&{=\\left<\\nabla\\mathcal{L}^{\\mathrm{rev}}(\\Theta_{t}),\\frac{d\\Theta_{t}}{d t}\\right>}\\\\ {\\displaystyle}&{=\\left.-\\left\\langle y_{1}(x_{1}-\\mathbb{E}_{p\\Theta_{t}(\\cdot\\vert y_{1})}[x])^{\\top},\\frac{1}{2n-1}\\left(\\sum_{i=1}^{n}x_{i}(y_{i}-\\mathbb{E}_{p\\Theta_{t}(\\cdot\\vert x_{i})}[y])^{\\top}+\\sum_{i=2}^{n}y_{i}(x_{i}-\\mathbb{E}_{p\\Theta_{t}(\\cdot\\vert y_{i})}[x])^{\\top}\\right)\\right.}\\\\ {\\displaystyle}&{\\displaystyle=\\left.-\\left\\langle\\sum_{j=1}^{m}\\beta_{n+1,j}^{(t)}v_{n+1}v_{j}^{\\top},\\frac{1}{2n-1}\\left(\\sum_{i=1}^{n}\\sum_{j=1}^{m}\\alpha_{i,j}^{(t)}v_{i}v_{j}^{\\top}+\\sum_{i=n+2}^{2n}\\sum_{j=1}^{m}\\beta_{i,j}^{(t)}v_{i}v_{j}^{\\top}\\right)\\right\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Applying Lemma 7, with probability at least $1-\\delta/2$ , for any $t\\geq0$ we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\left|\\frac{d\\mathcal{L}^{\\sf r e v}(\\Theta_{t})}{d t}\\right|\\leq\\epsilon\\cdot\\frac{1}{2n-1}\\left(\\sum_{j=1}^{m}(\\beta_{n+1,j}^{(t)})^{2}\\right)^{1/2}\\left(\\sum_{i=1}^{n}\\sum_{j=1}^{m}(\\alpha_{i,j}^{(t)})^{2}+\\sum_{i=n+2}^{2n}\\sum_{j=1}^{m}(\\beta_{i,j}^{(t)})^{2}\\right)^{1/2},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\left|\\frac{d\\mathcal{L}(\\Theta_{t})}{d t}+\\frac{1}{(2n-1)^{2}}\\left(\\sum_{i=1}^{n}\\sum_{j=1}^{m}(\\alpha_{i,j}^{(t)})^{2}+\\sum_{i=n+2}^{2n}\\sum_{j=1}^{m}(\\beta_{i,j}^{(t)})^{2}\\right)\\right|}\\\\ &{\\leq\\epsilon\\cdot\\displaystyle\\frac{1}{(2n-1)^{2}}\\left(\\sum_{i=1}^{n}\\sum_{j=1}^{m}(\\alpha_{i,j}^{(t)})^{2}+\\sum_{i=n+2}^{2n}\\sum_{j=1}^{m}(\\beta_{i,j}^{(t)})^{2}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "as well as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\left\\lvert\\frac{d I_{i}(\\Theta_{t})}{d t}+\\frac{1}{2n-1}\\sum_{j=1}^{m}(\\alpha_{i,j}^{(t)})^{2}\\right\\rvert}\\\\ {\\displaystyle\\leq\\epsilon\\cdot\\frac{1}{2n-1}\\left(\\sum_{j=1}^{m}(\\alpha_{i,j}^{(t)})^{2}\\right)^{1/2}\\left(\\sum_{i=1}^{n}\\sum_{j=1}^{m}(\\alpha_{i,j}^{(t)})^{2}+\\sum_{i=n+2}^{2n}\\sum_{j=1}^{m}(\\beta_{i,j}^{(t)})^{2}\\right)^{1/2},}\\\\ {\\displaystyle\\left\\lvert\\frac{d I_{i}^{\\mathrm{rer}}(\\Theta_{t})}{d t}+\\frac{1}{2n-1}\\sum_{j=1}^{m}(\\beta_{i+n,j}^{(t)})^{2}\\right\\rvert}\\\\ {\\displaystyle\\leq\\epsilon\\cdot\\frac{1}{2n-1}\\left(\\sum_{j=1}^{m}(\\beta_{i+n,j}^{(t)})^{2}\\right)^{1/2}\\left(\\sum_{i=1}^{n}\\sum_{j=1}^{m}(\\alpha_{i,j}^{(t)})^{2}+\\sum_{i=n+2}^{2n}\\sum_{j=1}^{m}(\\beta_{i,j}^{(t)})^{2}\\right)^{1/2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Furthermore, Lemma 6 implies that with probability at least $1-\\delta/2$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{1}{2m}\\leq p_{\\Theta_{0}}(y_{i}|x_{i}),p_{\\Theta_{0}}(x_{i}|y_{i})\\leq\\frac{2}{m}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The following arguments are based on the event that the above inequalities hold. By Eq. (8), ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{d\\mathcal{L}^{\\mathrm{rev}}(\\Theta_{t})}{d t}\\ge-\\epsilon\\cdot\\frac{1}{2n-1}\\left(\\underbrace{\\sum_{j=1}^{m}(\\beta_{n+1,j}^{(t)})^{2}}_{A_{t}}\\right)^{1/2}\\left(\\underbrace{\\sum_{i=1}^{n}\\sum_{j=1}^{m}(\\alpha_{i,j}^{(t)})^{2}+\\sum_{i=n+2}^{2n}\\sum_{j=1}^{m}(\\beta_{i,j}^{(t)})^{2}}_{B_{t}}\\right)^{1/2}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Notice that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle B_{t}\\leq2\\sum_{i=1}^{n}(1-p_{\\Theta_{t}}(y_{i}|x_{i}))^{2}+2\\sum_{i=2}^{n}(1-p_{\\Theta_{t}}(x_{i}|y_{i}))^{2}}\\\\ {\\displaystyle\\leq2\\left(\\sum_{i=1}^{n}(1-p_{\\Theta_{t}}(y_{i}|x_{i})+\\sum_{i=2}^{n}(1-p_{\\Theta_{t}}(x_{i}|y_{i}))\\right)^{2}}\\\\ {\\displaystyle}&{\\leq2(2n-1)^{2}\\mathcal{L}(\\Theta_{t})^{2}}\\\\ {\\displaystyle}&{\\leq2(2n-1)^{2}\\left(\\frac{1}{\\frac{t}{8(2n-1)\\log^{2}(2m)}}+\\frac{1}{\\mathcal{L}(\\Theta_{0})}\\right)^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the first inequality uses $\\begin{array}{r}{\\sum_{j\\neq i+n}\\lvert\\alpha_{i,j}^{(t)}\\rvert=\\alpha_{i,i+n}^{(t)}=1-p_{\\Theta_{t}}(y_{i}\\lvert x_{i}),\\sum_{j\\neq i}\\lvert\\beta_{i+n,j}^{(t)}\\rvert=\\beta_{i+n,i}^{(t)}=1}\\end{array}$ $1\\!-p_{\\Theta_{t}}(x_{i}|y_{i})$ for all $i\\in[n]$ ; the third inequality uses the fact that $1-x\\leq-\\log x$ ; the last inequality applies Lemma 3. ", "page_idx": 18}, {"type": "text", "text": "Similarly, ", "page_idx": 18}, {"type": "equation", "text": "$$\nA_{t}\\leq2(1-p_{\\Theta_{t}}(x_{1}|y_{1}))^{2}\\leq2\\mathcal{L}^{\\mathrm{rev}}(\\Theta_{t})^{2}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Combining, we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{d\\mathcal{L}^{\\mathtt{r e v}}(\\Theta_{t})}{d t}\\ge-\\epsilon\\cdot\\frac{1}{2n-1}\\cdot2\\mathcal{L}^{\\mathtt{r e v}}(\\Theta_{t})\\cdot(2n-1)\\cdot\\frac{1}{\\overline{{8(2n-1)\\log^{2}(2m)}}+\\frac{1}{\\mathcal{L}(\\Theta_{0})}}}\\\\ &{\\qquad\\qquad\\ge\\,-\\,8\\epsilon(2n-1)\\log^{2}(2m)\\cdot\\mathcal{L}^{\\mathtt{r e v}}(\\Theta_{t})\\cdot\\frac{1}{t+\\frac{8(2n-1)\\log^{2}(2m)}{\\mathcal{L}(\\Theta_{0})}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "By Lemma 10, we conclude that $\\forall t\\ge0$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathcal{L}^{\\mathrm{rev}}(\\Theta_{t})\\geq\\mathcal{L}^{\\mathrm{rev}}(\\Theta_{0})\\cdot\\left(1+\\frac{\\mathcal{L}(\\Theta_{0})\\cdot t}{8(2n-1)\\log^{2}(2m)}\\right)^{-8\\epsilon(2n-1)\\log^{2}(2m)},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "This completes the proof. ", "page_idx": 18}, {"type": "text", "text": "Lemma 5 (Gradient of the loss function). Define ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}(\\Theta)=\\displaystyle\\frac{1}{2n-1}\\left(\\sum_{i=1}^{n}-\\log p_{\\Theta}(y_{i}|x_{i})+\\sum_{i=2}^{n}-\\log p_{\\Theta}(x_{i}|y_{i})\\right),}\\\\ {\\mathcal{L}^{r e v}(\\Theta)=\\displaystyle-\\log p_{\\Theta}(x_{1}|y_{1}).\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Then we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\nabla\\mathcal{L}(\\Theta)=\\mathrm{~-~}\\frac{1}{2n-1}\\left(\\displaystyle\\sum_{i=1}^{n}x_{i}\\big(y_{i}-\\mathbb{E}_{p\\Theta(\\cdot|x_{i})}[y]\\big)^{\\top}+\\displaystyle\\sum_{i=2}^{n}y_{i}\\big(x_{i}-\\mathbb{E}_{p\\Theta(\\cdot|y_{i})}[x]\\big)^{\\top}\\right),}\\\\ &{\\nabla\\mathcal{L}^{r e v}(\\Theta)=\\mathrm{~-~}y_{1}\\big(x_{1}-\\mathbb{E}_{p\\Theta(\\cdot|y_{1})}[x]\\big)^{\\top}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof. We have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla(-\\log p(y|x))}\\\\ &{=-\\frac{\\nabla p_{\\Theta}(y|x)}{p_{\\Theta}(y|x)}}\\\\ &{=-\\frac{1}{p_{\\Theta}(y|x)}\\cdot\\frac{x y^{\\top}\\exp(x^{\\top}\\Theta)y\\Big(\\sum_{y\\in\\mathcal{Y}}\\exp(x^{\\top}\\Theta)y\\Big)-\\exp(x^{\\top}\\Theta)y\\big(\\sum_{y\\in\\mathcal{Y}}x y^{\\top}\\exp(x^{\\top}\\Theta y)\\Big)}{\\left(\\sum_{y\\in\\mathcal{Y}}\\exp(x^{\\top}\\Theta y)\\right)^{2}}}\\\\ &{=-\\frac{1}{p_{\\Theta}(y|x)}\\left(p_{\\Theta}(y|x)x y^{\\top}-p_{\\Theta}(y|x)\\sum_{y\\in\\mathcal{Y}}p_{\\Theta}(y|x)x y^{\\top}\\right)}\\\\ &{=-\\frac{1}{p_{\\Theta}(y|x)}\\left(p_{\\Theta}(y|x)x y^{\\top}-p_{\\Theta}(y|x)\\sum_{y\\in\\mathcal{Y}}\\exp(y|x)x y^{\\top}\\right)}\\\\ &{=-x\\left(y-\\sum_{y\\in\\mathcal{Y}}p_{\\Theta}(y|x)y\\right)^{\\top}}\\\\ &{=-x\\left(y-\\frac{x}{y\\in\\mathcal{Y}}p_{\\Theta}(x)\\right)^{\\top}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The statements follow immediately. ", "page_idx": 19}, {"type": "text", "text": "B.2 Initialization ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Lemma 6 (Initial distributions are all close to uniform). Fix any $\\delta\\in(0,1)$ . Let $x_{1},x_{2},\\ldots,x_{n}$ i.i\u223c.d. $\\begin{array}{r}{\\mathcal{N}_{d}(\\mathbf{0}_{d},\\frac{1}{d}I_{d})}\\end{array}$ . Let $\\Theta\\in\\mathbb{R}^{d\\times d}$ , where each $\\Theta_{i j}\\overset{i.i.d.}{\\sim}\\mathcal{N}(0,\\sigma^{2})$ independent of $x_{1},\\ldots,x_{n}$ . For any $i,j\\in[n]$ , define ", "page_idx": 19}, {"type": "equation", "text": "$$\np_{\\Theta}(x_{j}|x_{i})=\\frac{\\exp(l_{\\Theta}(x_{j}|x_{i}))}{\\sum_{k=1}^{n}\\exp(l_{\\Theta}(x_{k}|x_{i}))},\\;\\;w h e r e\\;\\;l_{\\Theta}(x_{j}|x_{i})=x_{i}^{\\top}\\Theta x_{j}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Then when $\\begin{array}{r}{\\sigma^{2}\\leq\\frac{1}{100\\ln(4n^{2}/\\delta)}}\\end{array}$ and $d\\geq400\\log(1/(2\\delta n^{2}))/\\epsilon^{2}$ , with probability at least $1-\\delta$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n|p_{\\Theta}(x_{j}|x_{i})-1/n|\\!\\le\\frac{1}{2n},\\,\\forall i,j\\in[n].\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof. Let $v=0.2$ . By Lemma 8, with probability at least $1-\\delta$ , we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n|\\langle x_{i},x_{j}\\rangle-\\delta_{i j}|\\leq v,\\,\\forall i,j\\in[n].\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Conditioned on the above high-probability event, we can further obtain that for any $j\\in[n]$ ", "page_idx": 19}, {"type": "equation", "text": "$$\np_{\\Theta}(x_{j}|x_{i})=\\frac{\\exp(l_{\\Theta}(x_{j}|x_{i}))}{\\sum_{k=1}^{n}\\exp(l_{\\Theta}(x_{k}|x_{i}))}\\leq\\frac{\\exp(v)}{\\sum_{k=1}^{n}\\exp(-v)}=\\frac{\\exp(2v)}{n},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and ", "page_idx": 19}, {"type": "equation", "text": "$$\np_{\\Theta}(x_{j}|x_{i})=\\frac{\\exp(l_{\\Theta}(x_{j}|x_{i}))}{\\sum_{k=1}^{n}\\exp(l_{\\Theta}(x_{k}|x_{i}))}\\geq\\frac{\\exp(-v)}{\\sum_{k=1}^{n}\\exp(v)}=\\frac{\\exp(-2v)}{n},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "It follows that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{1}{2n}<p_{\\Theta}(x_{j}|x_{i})<\\frac{3}{2n}\\implies|p_{\\Theta}(x_{j}|x_{i})-1/n|<\\frac{1}{2n}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "which completes the proof. ", "page_idx": 19}, {"type": "text", "text": "B.3 Subspace embedding ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Lemma 7 ( $\\ell_{1}$ -subspace embedding of Gaussian second-order tensors). Let $z_{1},\\ldots,z_{n}$ be independently sampled from $\\textstyle\\bigwedge_{d}(\\pmb{\\theta}_{d},\\,\\frac{1}{d}I_{d})$ . Let $\\mathbb{Z}_{1},\\mathbb{Z}_{2}\\subseteq[n]\\times[n]$ be two index sets. Let $\\mathcal{T}_{0}=\\mathcal{T}_{1}\\cap\\mathcal{T}_{2}$ . If $d\\geq64\\log(2n^{2}/\\delta)/\\epsilon^{2}$ , then with probability at least $1-\\delta$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\left\\langle\\sum_{(i,j)\\in{\\cal Z}_{1}}\\alpha_{i,j}z_{i}z_{j}^{\\top},\\sum_{(i,j)\\in{\\cal Z}_{2}}\\beta_{i,j}z_{i}z_{j}^{\\top}\\right\\rangle-\\sum_{(i,j)\\in{\\cal Z}_{0}}\\alpha_{i,j}\\beta_{i,j}\\left|\\leq\\epsilon\\cdot\\left(\\sum_{(i,j)\\in{\\cal Z}_{1}}|\\alpha_{i,j}|\\right)\\left(\\sum_{(i,j)\\in{\\cal Z}_{2}}|\\beta_{i,j}|\\right)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "holds for any $\\alpha_{i,j},\\beta_{i,j}$ . Furthermore, if $\\dot{\\mathbf{\\xi}}d\\geq64k^{2}\\log(2n^{2}/\\delta)/\\epsilon^{2}$ , then with probability at least $1-\\delta$ , ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\left\\langle\\sum_{(i,j)\\in\\cal Z_{1}}\\alpha_{i,j}z_{i}z_{j}^{\\top},\\sum_{(i,j)\\in\\cal Z_{2}}\\beta_{i,j}z_{i}z_{j}^{\\top}\\right\\rangle-\\sum_{(i,j)\\in\\cal Z_{0}}\\alpha_{i,j}\\beta_{i,j}\\right\\rvert\\leq\\epsilon\\cdot\\left(\\sum_{(i,j)\\in\\cal Z_{1}}\\alpha_{i,j}^{2}\\right)^{1/2}\\left(\\sum_{(i,j)\\in\\cal Z_{2}}\\beta_{i,j}^{2}\\right)^{1/2}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "holds for any $\\alpha_{i,j},\\beta_{i,j}$ such that $\\|\\alpha\\|_{0}\\!\\leq k,\\|\\beta\\|_{0}\\!\\leq k$ . ", "page_idx": 20}, {"type": "text", "text": "Proof. Using Lemma 8 and Cauchy-Schwarz inequality, we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\Bigg|\\Bigg\\langle\\sum_{i,j\\in\\mathcal{D}_{i}}\\alpha_{i,j}z_{i,j}^{\\top},\\underset{(i,j)\\in\\mathcal{Z}_{2}}{\\sum}\\beta_{i,j}z_{i,j}z_{j}^{\\top}\\Bigg\\rangle-\\underset{(i,j)\\in\\mathcal{D}_{i}}{\\sum}\\alpha_{i,j}\\beta_{i,j}\\Bigg|}\\\\ &{\\leq\\Bigg|\\sum_{i,j\\in\\mathcal{D}_{i}}\\alpha_{i,j}\\beta_{i,j}\\big(\\|z_{i}\\|^{2}\\|z_{j}\\|^{2}-1\\big)+\\underset{(i,j)\\in\\mathcal{Z}_{1}}{\\sum}\\alpha_{i,j}\\beta_{i,j}\\underline{{\\lambda}}_{\\lambda}z_{i,j}^{\\top}z_{i,j}}\\\\ &{\\leq\\sqrt{\\frac{64\\log(2\\pi^{2}/\\delta)}{d}}\\cdot\\Bigg\\langle\\sum_{i,j\\in\\mathcal{E}_{1}}\\alpha_{i,j}\\beta_{i,j}\\big|+\\underset{(i,j)\\in\\mathcal{Z}_{1}}{\\sum}\\sum_{i,j\\in\\mathcal{E}_{2}}\\underline{{1}}\\alpha_{i,j}\\beta_{i,j}\\big|}\\\\ &{=\\sqrt{\\frac{64\\log(2\\pi^{2}/\\delta)}{d}}\\cdot\\Bigg\\langle\\sum_{i,j\\in\\mathcal{E}_{1}}\\alpha_{i,j}\\Bigg|\\right\\rangle\\cdot\\Bigg\\langle(\\underset{(i,j)\\in\\mathcal{Z}_{2}}{\\sum}|\\beta_{i,j}|)}\\\\ &{\\leq\\sqrt{\\frac{64\\log(2\\pi^{2}/\\delta)}{d}}\\cdot\\Bigg(\\underset{(i,j)\\in\\mathcal{Z}_{2}}{\\sum}\\alpha_{i,j}^{2}\\Bigg)^{1/2}\\Bigg(\\underset{(i,j)\\in\\mathcal{Z}_{2}}{\\sum}\\beta_{i,j}^{2}\\Bigg)^{1/2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "The statements follow directly from plugging in suitable values of $d$ . ", "page_idx": 20}, {"type": "text", "text": "Lemma 8 (Almost orthonormal). Le $\\begin{array}{r}{{}^{\\dagger}x_{1},x_{2},\\ldots,x_{n}\\overset{i.i.d.}{\\sim}\\mathcal{N}_{d}(\\mathbf{0}_{d},\\frac{1}{d}I_{d})}\\end{array}$ . For any $\\epsilon,\\delta\\in(0,1)$ , when $d\\geq16\\log(2n^{2}/\\delta)/\\epsilon^{2}$ , it holds that with probability at least $1-\\delta$ , ", "page_idx": 20}, {"type": "equation", "text": "$$\n|\\langle x_{i},x_{j}\\rangle-\\delta_{i j}|\\!\\le\\epsilon,\\,\\forall i,j\\in[n].\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. Fix $i\\neq j\\in[n]$ . Notice ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|\\langle x_{i},x_{j}\\rangle|\\!=|\\langle x_{i}+x_{j},x_{i}+x_{j}\\rangle-\\langle x_{i}-x_{j},x_{i}-x_{j}\\rangle|/4}\\\\ &{\\qquad\\qquad\\leq(|\\langle x_{i}+x_{j},x_{i}+x_{j}\\rangle-1|\\!+\\!|\\langle x_{i}-x_{j},x_{i}-x_{j}\\rangle-1|)/4.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Using Lemma 9, we have that with probability at least $1-\\delta/n^{2}$ , ", "page_idx": 20}, {"type": "equation", "text": "$$\n|\\langle x_{i},x_{j}\\rangle|\\!\\le\\epsilon.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Furthermore, fix $i\\in[n]$ , with probability at least $1-\\delta/n^{2}$ , ", "page_idx": 20}, {"type": "equation", "text": "$$\n|\\langle x_{i},x_{i}\\rangle-1|\\leq\\epsilon.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "The statement then follows from union bound over $i,j\\in[n]$ . ", "page_idx": 20}, {"type": "text", "text": "Lemma 9 (Almost normal for a fixed vector). For a $d$ -dimensional random vector $\\begin{array}{r}{x\\sim\\mathcal{N}_{d}(\\mathbf{0}_{d},\\frac{1}{d}I_{d})}\\end{array}$ and any $v\\in(0,1/2)$ , ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}\\left(|\\langle x,x\\rangle-1|\\ge v\\right)\\le2e^{-v^{2}d/16}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "In particular, when $d\\geq16\\log(1/(2\\delta))/v^{2}$ , we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(|\\langle x,x\\rangle-1|\\geq v\\right)\\leq\\delta\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. Let $\\boldsymbol{x}=\\left(x_{1},\\ldots,x_{d}\\right)$ . By Lemma 11 (letting $x=v^{2}d/16)$ , ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\left(|\\langle x,x\\rangle-1|\\geq v\\right)\\leq\\mathbb{P}\\left(\\left|\\displaystyle\\sum_{i=1}^{d}(\\sqrt{d}\\cdot x_{i})^{2}-d\\right|\\geq v d\\right)}\\\\ &{\\qquad\\qquad\\qquad\\leq2e^{-v^{2}d/16}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "The second inequality follows from simple arithmetics. ", "page_idx": 20}, {"type": "text", "text": "B.4 Useful results ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Lemma 10 (ODE bound). Let $c_{1},c_{2},c_{3}\\;>\\;0.$ . Suppose the function $f_{1},f_{2}:\\mathbb{R}_{+}\\,\\rightarrow\\,\\mathbb{R}$ satisfies $f_{1}(0)>0,f_{2}(0)>0$ and ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{d f_{1}(t)}{d t}\\leq\\,-\\,c_{1}\\cdot f_{1}(t)^{2},}\\\\ {\\displaystyle\\frac{d f_{2}(t)}{d t}\\geq\\,-\\,c_{2}\\cdot f_{2}(t)\\cdot\\frac{1}{t+c_{3}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Then ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{f_{1}(t)\\le\\displaystyle\\frac{1}{c_{1}t+\\frac{1}{f_{1}(0)}},}\\\\ {f_{2}(t)\\ge f_{2}(0)\\cdot\\left(1+\\displaystyle\\frac{t}{c_{3}}\\right)^{-c_{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof. The conditions imply that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\frac{d f_{1}^{-1}(t)}{d t}=\\,-\\,\\frac{1}{f_{1}^{2}(t)}\\cdot\\frac{d f_{1}(t)}{d t}\\geq c_{1},}\\\\ {\\displaystyle\\frac{d\\log f_{2}(t)}{d t}=\\frac{1}{f_{2}(t)}\\cdot\\frac{d f_{2}(t)}{d t}\\geq-c_{2}/(t+c_{3}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "It follows that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad f_{1}^{-1}(t)\\geq c_{1}t+f_{1}^{-1}(0),}\\\\ &{\\log f_{2}(t)\\geq\\,-\\,c_{2}\\log(1+t/c_{3})+\\log f_{2}(0).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Rearranging the above inequalities, one can obtain the desired results. ", "page_idx": 21}, {"type": "text", "text": "Lemma 11 ( $\\chi^{2}$ -concentration bound, Lemma 1 of [64]). Let $g_{1},\\ldots,g_{t}$ be i.i.d. ${\\mathcal{N}}(0,1)$ random variables. Then for any $x\\geq0$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left[\\sum_{i=1}^{t}g_{i}^{2}\\geq t+2\\sqrt{t x}+2x\\right]\\leq\\exp(-x),\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left[\\sum_{i=1}^{t}g_{i}^{2}\\leq t-2{\\sqrt{t x}}\\right]\\leq\\exp(-x).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "C Missing Proofs of Section 4 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In this section, we show missing proofs in Section 4. ", "page_idx": 21}, {"type": "text", "text": "C.1 Proofs of Section 4.1 ", "page_idx": 21}, {"type": "text", "text": "C.1.1 Proofs of Proposition 4.1 and Proposition 4.2 ", "page_idx": 21}, {"type": "text", "text": "We first show the proofs of Proposition 4.1 and Proposition 4.2, respectively. ", "page_idx": 21}, {"type": "text", "text": "Proof of Proposition 4.1. Actually, for any three tokens $x_{1},x_{2},x_{3}$ , it holds that ", "page_idx": 21}, {"type": "equation", "text": "$$\np_{\\theta(0)}(x_{3}|x_{1},x_{2})=\\frac{\\exp\\left(x_{3}^{\\top}Y(0)^{\\top}\\mathrm{LN}(X^{\\top}b_{2})\\right)}{\\sum_{x^{\\prime}\\in[M]}\\exp\\left(x^{\\prime^{\\top}}Y(0)^{\\top}\\mathrm{LN}(X^{\\top}b_{2})\\right)}=\\frac{\\exp\\left(0\\right)}{\\sum_{x^{\\prime}\\in[M]}\\exp\\left(0\\right)}=1/M,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "since $Y(0)=\\mathbf{0}$ . ", "page_idx": 21}, {"type": "text", "text": "Proof of Proposition 4.2. Note that the input sequence length $T=2$ . By (1), ", "page_idx": 22}, {"type": "equation", "text": "$$\np_{\\theta(t)}(x|x_{1},x_{2})=\\frac{\\exp\\left(x^{\\top}Y(t)^{\\top}\\mathrm{LN}(X^{\\top}b_{2})\\right)}{\\sum_{x^{\\prime}\\in[M]}\\exp\\left({x^{\\prime}}^{\\top}Y(t)^{\\top}\\mathrm{LN}(X^{\\top}b_{2})\\right)}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $b_{2}=[b_{12}]$ and $b_{12}=1$ . Also, $X=[{\\pmb x}_{1}]^{\\intercal}$ is a one-hot row vector. Therefore, $\\ L{\\mathsf{N}}(X^{\\top}b_{2})=$ $\\mathrm{LN}(\\pmb{x}_{1}b_{12})=\\mathrm{LN}(\\pmb{x}_{1})=\\pmb{x}_{1}$ , and thus ", "page_idx": 22}, {"type": "equation", "text": "$$\np_{\\theta(t)}(x|x_{1},x_{2})=\\frac{\\exp\\left(x^{\\top}Y(t)^{\\top}x_{1}\\right)}{\\sum_{x^{\\prime}\\in[M]}\\exp\\left(x^{\\prime^{\\top}}Y(t)^{\\top}x_{1}\\right)}=\\frac{\\exp\\left(Y(t)_{x_{1},x}\\right)}{\\sum_{x^{\\prime}\\in[M]}\\exp\\left(Y(t)_{x_{1},x^{\\prime}}\\right)}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $Y(t)_{i,j}$ is the entry of the matrix $Y(t)$ at row $i$ and column $j$ . ", "page_idx": 22}, {"type": "text", "text": "C.1.2 Proof of Lemma 2 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Proof of Lemma 2. We first calculate the gradient of $Y$ when the current batch is a sequence $(x_{1},x_{2},x_{3})$ . Note that the input sequence length $T\\ =\\ 2$ and by the proof of Proposition 4.2, we have $\\mathrm{LN}(X^{\\top}b_{T})=\\mathrm{LN}(X^{\\top}b_{2})=x_{1}$ . By Lemma 1, we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\dot{Y}=\\eta_{Y}\\mathbf{LN}(\\boldsymbol{X}^{\\top}b_{T})(\\pmb{x}_{T+1}-\\pmb{\\alpha})^{\\top}=\\eta_{Y}\\pmb{x}_{1}(\\pmb{x}_{3}-\\pmb{\\alpha})^{\\top}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $\\pmb{\\alpha}=[\\alpha_{1},\\alpha_{2},\\dots,\\alpha_{M}]^{\\top}\\in\\mathbb{R}^{M}$ with $\\alpha=\\exp\\left({Y_{x_{1}}^{\\top}}\\right)/\\mathbf{1}^{\\top}\\exp\\left({Y_{x_{1}}^{\\top}}\\right)$ . Note that $\\pmb{x}_{1}(\\pmb{x}_{3}-\\pmb{\\alpha})^{\\top}$ is a matrix with only $x_{1}$ -th row non-zero since $x_{1}$ is one-hot. Therefore, the update of each row of $Y$ are independent and only $x_{1}$ -th row of $Y$ gets updated at the current time step. ", "page_idx": 22}, {"type": "text", "text": "Now we consider any fixed $x_{1}\\in\\mathcal{V}$ . Let $t_{x_{1},i}$ be the time step that the $x_{1}$ -th row of $\\mathrm{\\bfY}$ gets updated (i.e., the first token of the training data is $x_{1}$ for the current batch) for the $i$ -th time and let $t_{x_{1},0}=0$ for notation convenience, then ", "page_idx": 22}, {"type": "equation", "text": "$$\nY(t_{x_{1},i})_{x_{1}}=Y(t_{x_{1},i-1})_{x_{1}}+\\eta_{Y}(\\mathbf{x}_{3}-\\pmb{\\alpha})^{\\top}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "For convenience, we denote $\\pmb{y}(i)=Y(t_{x_{1},i})_{x_{1}}^{\\top}$ , and thus ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\pmb{y}(i)=\\pmb{y}(i-1)+\\eta_{Y}(\\pmb{x}_{3}-\\pmb{\\alpha}(i-1))\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where ${\\pmb y}(0)={\\bf0}$ and $\\pmb{\\alpha}(i-1)=\\exp(\\pmb{y}(i-1))/\\pmb{1}^{\\top}\\exp(\\pmb{y}(i-1))$ . Note that for a fixed $x_{1},\\,x_{3}$ is also fixed by our construction of the dataset. By Lemma 5 of [14], we can obtain that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\pmb{y}(i)=(M-1)h^{\\ast}(i)\\pmb{\\xi}_{x_{3}},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $\\begin{array}{r}{\\pmb{\\xi}_{x_{3}}=\\frac{M}{M-1}(\\pmb{x}_{3}-\\frac{1}{M}\\pmb{1})}\\end{array}$ and $h^{*}(i)$ can be derived recursively as ", "page_idx": 22}, {"type": "equation", "text": "$$\nh^{\\ast}(i)=h^{\\ast}(i-1)+\\frac{\\eta_{Y}}{(M-1)+\\exp(M h^{\\ast}(i-1))}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "with $h^{*}(0)=0$ . Combining Lemma 7 and 9 in [14], we have ", "page_idx": 22}, {"type": "equation", "text": "$$\nh^{*}(i)\\gtrsim\\frac{1}{M}\\ln(M\\eta_{Y}i),\\quad\\forall i\\gtrsim\\frac{\\ln M}{\\eta_{Y}}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Note that the update of each row of $Y$ are independent, the training set has size $N$ , and the batch size is 1, we have ", "page_idx": 22}, {"type": "equation", "text": "$$\nY(t)_{x_{1}}^{\\top}=(M-1)h^{*}\\left(\\lceil t/N\\rceil\\right){\\xi}_{x_{3}}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the training data at time step $t$ is $(x_{1},x_{2},x_{3})$ . Combining (11), we can obtain that ", "page_idx": 22}, {"type": "equation", "text": "$$\nY(t)_{x_{1},x_{3}}\\gtrsim(M-1)\\cdot\\frac{M}{M-1}(1-\\frac{1}{M})\\cdot\\frac{1}{M}\\ln\\left(M\\eta_{Y}\\lceil t/N\\rceil\\right)\\geq\\ln\\left(\\frac{M\\eta_{Y}t}{N}\\right)\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "and ", "page_idx": 22}, {"type": "equation", "text": "$$\nY(t)_{x_{1},x}\\lesssim(M-1)\\cdot\\frac{M}{M-1}(-\\frac{1}{M})\\cdot\\frac{1}{M}\\ln\\left(M\\eta_{Y}\\lceil t/N\\rceil\\right)\\leq-\\frac{1}{M}\\ln\\left(\\frac{M\\eta_{Y}t}{N}\\right),\\quad\\forall x\\neq x_{3}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "On the other hand, for any sequence $(x_{1},x_{2},x_{3})$ in the test set, since the $x_{1}$ -th row of $Y$ has never been updated, we have ", "page_idx": 22}, {"type": "equation", "text": "$$\nY(t)_{x_{1},x}=Y(0)_{x_{1},x}=0,\\quad\\forall x\\in[M].\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "C.1.3 Proof of Theorem 3 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Proof. We first consider training sequence $(x_{1},x_{2},x_{3})$ at time $t$ . By Proposition 4.2, we have ", "page_idx": 23}, {"type": "equation", "text": "$$\np_{\\theta(t)}(x_{3}|x_{1},x_{2})=\\frac{\\exp{(Y(t)_{x_{1},x_{3}})}}{\\sum_{x^{\\prime}\\in[M]}\\exp{(Y(t)_{x_{1},x^{\\prime}})}}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "and by Lemma 2, we have ", "page_idx": 23}, {"type": "equation", "text": "$$\nY(t)_{x_{1},x_{3}}\\geq c\\ln\\left(\\frac{M\\eta_{Y}t}{N}\\right),\\quad\\mathrm{and}\\quad Y(t)_{x_{1},x}\\leq-\\frac{c}{M}\\ln\\left(\\frac{M\\eta_{Y}t}{N}\\right),\\quad\\forall x\\neq x_{3}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "for some constant $c>0$ . Therefore, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{p_{\\theta(t)}(x_{3}|x_{1},x_{2})\\geq\\frac{\\exp\\Big(c\\ln\\left(\\frac{M\\eta_{\\mathrm{V}}t}{N}\\right)\\Big)}{\\exp\\Big(c\\ln\\left(\\frac{M\\eta_{\\mathrm{V}}t}{N}\\right)\\Big)+(M-1)\\ln\\left(-\\frac{c}{M}\\ln\\left(\\frac{M\\eta_{\\mathrm{V}}t}{N}\\right)\\right)}}\\\\ &{\\geq\\frac{\\left(\\frac{M\\eta_{\\mathrm{V}}t}{N}\\right)^{c}}{\\left(\\frac{M\\eta_{\\mathrm{V}}t}{N}\\right)^{c}+(M-1)}}\\\\ &{=1-\\frac{M-1}{\\left(\\frac{M\\eta_{\\mathrm{V}}t}{N}\\right)^{c}+(M-1)}}\\\\ &{\\geq1-\\frac{M-1}{2\\left(\\frac{M\\eta_{\\mathrm{V}}t}{N}\\right)^{c}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where the last inequality holds since ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\ln t\\gtrsim\\ln(N M/\\eta_{Y})\\implies t\\geq{\\frac{N(M-1)^{1/c}}{M\\eta_{Y}}}\\implies\\left({\\frac{M\\eta_{Y}t}{N}}\\right)^{c}\\geq M-1.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Finally, for any sequence $(x_{1},x_{2},x_{3})\\in\\mathcal{D}_{\\mathrm{test}}$ , since $Y(t)_{x_{1},x}=0,\\forall x\\in[M]$ according to Lemma 2, we have ", "page_idx": 23}, {"type": "equation", "text": "$$\np_{\\theta(t)}(x_{3}|x_{1},x_{2})=\\frac{\\exp{(Y(t)_{x_{1},x_{3}})}}{\\sum_{x^{\\prime}\\in[M]}\\exp{(Y(t)_{x_{1},x^{\\prime}})}}=\\frac{\\exp(0)}{M\\cdot\\exp(0)}=1/M,\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "which completes the proof. ", "page_idx": 23}, {"type": "text", "text": "C.2 Additional details and proof of Section 4.2 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Datasets. Let $N_{\\mathrm{train}}>0$ , $N_{\\mathrm{test}}>0$ be two positive integers and let $N_{\\mathrm{total}}\\,=\\,N_{\\mathrm{train}}+N_{\\mathrm{test}}$ . Let $\\mathtt{A}_{i},\\mathtt{B}_{i},\\mathtt{C}_{i}\\in\\mathcal{V},\\forall i\\in[N_{\\mathrm{total}}]$ be $3N_{\\mathrm{total}}$ distinct tokens. Let $\\to,\\lnot\\lnot\\in\\mathcal{V}=[M]$ be two additional different tokens that represent \u201cdirect implication\u201d and \u201cindirect implication\u201d respectively. Specifically, we have $\\mathtt{A}_{i}\\to\\mathtt{B}_{i}$ , $\\mathsf{B}_{i}\\to\\mathsf{C}_{i}$ and $\\mathtt{A}_{i}\\sim\\mathtt{C}_{i}$ for all $i\\in[N_{\\mathrm{total}}]$ . For notation convenience, we define the following two index sets ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{Z}_{\\mathrm{train}}=\\{1,2,\\dotsc,N_{\\mathrm{train}}\\},\\ \\mathcal{Z}_{\\mathrm{test}}=\\{N_{\\mathrm{train}}+1,\\dotsc,N_{\\mathrm{total}}\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "The training set $\\ensuremath{\\mathcal{D}}_{\\mathrm{train}}$ consists of all $\\mathtt{A}_{i}\\to\\mathtt{B}_{i}$ , $\\mathsf{B}_{i}\\to\\mathsf{C}_{i}$ and $\\mathtt{A}_{i}\\sim\\mathtt{C}_{i}$ for $i\\in\\mathcal{T}_{\\mathrm{train}}$ . In addition, $\\ensuremath{\\mathcal{D}}_{\\mathrm{train}}$ contains $\\mathtt{A}_{i}\\to\\mathtt{B}_{i}$ and $\\mathsf{B}_{i}\\to\\mathsf{C}_{i}$ for $i\\in\\mathcal{T}_{\\mathrm{test}}$ . For convenience, we let $N=|\\mathcal{D}_{\\operatorname{train}}|$ to be the size of the training set. The test set $\\ensuremath{\\mathcal{D}}_{\\mathrm{test}}$ consists of $\\mathtt{A}_{i}\\sim\\mathtt{C}_{i}$ for $i\\in\\mathcal{Z}_{\\mathrm{test}}$ . Under our construction of the dataset, the LLM will learn the relationship between ${\\mathtt A}_{i}$ , $\\mathtt{B}_{i}$ and $\\mathsf{C}_{i}$ for $i\\in\\mathcal{T}_{\\mathrm{train}}$ in both direct and indirect implication, and learn the relationship between ${\\mathtt A}_{i}$ , ${\\mathtt{B}}_{i}$ and $\\mathsf{C}_{i}$ for $i\\in\\mathcal{Z}_{\\mathrm{test}}$ only in direct implication and will be tested for indirect implication. ", "page_idx": 23}, {"type": "text", "text": "Similar to the reversal curse in Section 4.1, we aim to prove through the training dynamics of onelayer transformers that the test probability remains negligible during training. In particular, we are interested in ", "page_idx": 23}, {"type": "equation", "text": "$$\np_{\\theta}(x_{3}=\\mathsf{C}_{i}|x_{1}=\\mathsf{A}_{i},x_{2}=\\sim\\sim),\\quad i\\in\\mathcal{T}_{\\mathrm{test}}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "We also use $p_{\\theta}\\big(\\mathtt{B}_{i}|\\mathtt{A}_{i}\\ \\rightarrow\\big)$ , $p_{\\theta}(\\mathsf{C}_{i}|\\mathsf{B}_{i}\\to)$ and $p_{\\theta}(\\mathsf{C}_{i}|\\mathsf{A}_{i}\\rightsquigarrow)$ to more compactly represent $p_{\\theta}(x_{3}\\,=$ $\\mathsf{B}_{i}\\vert x_{1}=\\mathsf{A}_{i},x_{2}=\\overset{\\cdot}{\\longrightarrow}\\,)$ , $p_{\\theta}(x_{3}=\\mathsf{C}_{i}|x_{1}=\\mathsf{B}_{i},x_{2}=\\rightarrow)$ and $p_{\\theta}(x_{3}=\\mathsf{C}_{i}|x_{1}=\\mathsf{A}_{i},x_{2}=\\sim\\sim)$ , respectively. The following theorem shows the importance of the chain-of-thought method: ", "page_idx": 23}, {"type": "text", "text": "Theorem 7 (Importance of chain-of-thought, formal statement of Theorem 4). Assume we run SGD with batch size $^{\\,l}$ , and assume $M\\gg100$ and $\\begin{array}{r}{\\frac{1}{M^{0.99}}\\ll\\eta_{Y}<1.}\\end{array}$ . Let $t\\gtrsim\\frac{N\\ln M}{\\eta_{Y}}$ denote the time step which also satisfies $\\ln t\\gtrsim\\ln(N M/\\eta_{Y})$ . For any test index $i\\in\\mathcal{T}_{t e s t}$ , we have ", "page_idx": 24}, {"type": "equation", "text": "$$\np_{\\theta(t)}(B_{i}|A_{i}\\rightarrow)\\geq1-\\frac{M-1}{2\\left(\\frac{M\\eta_{Y}t}{N}\\right)^{c}},\\qquad p_{\\theta(t)}(C_{i}|B_{i}\\rightarrow)\\geq1-\\frac{M-1}{2\\left(\\frac{M\\eta_{Y}t}{N}\\right)^{c}}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "for some constant $c>0$ and ", "page_idx": 24}, {"type": "equation", "text": "$$\np_{\\theta(t)}(c_{i}|A_{i}\\sim)\\leq\\frac{1}{M}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proof. Recall that by Proposition 4.2, we have ", "page_idx": 24}, {"type": "equation", "text": "$$\np_{\\theta(t)}(x_{3}|x_{1},x_{2})=\\frac{\\exp{(Y(t)_{x_{1},x_{3}})}}{\\sum_{x^{\\prime}\\in[M]}\\exp{(Y(t)_{x_{1},x^{\\prime}})}}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "and by Lemma 2, we have ", "page_idx": 24}, {"type": "equation", "text": "$$\nY(t)_{\\mathbb{A}_{i},\\mathbb{B}_{i}}\\geq c\\ln\\left({\\frac{M\\eta_{Y}t}{N}}\\right),\\quad{\\mathrm{and}}\\quad Y(t)_{\\mathbb{A}_{i},x}\\leq-{\\frac{c}{M}}\\ln\\left({\\frac{M\\eta_{Y}t}{N}}\\right),\\quad\\forall x\\neq\\mathbb{B}_{i}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "for some constant $c>0$ . Therefore, using the same proof as Theorem 3, we have ", "page_idx": 24}, {"type": "equation", "text": "$$\np_{\\theta(t)}\\big(\\mathtt{B}_{i}\\big|\\mathtt{A}_{i}\\to\\big)\\geq1-\\frac{M-1}{2\\left(\\frac{M\\eta_{Y}t}{N}\\right)^{c}}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Additionally, according to the proof of Lemma 2, $Y(t)_{{\\tt A}_{i},x}$ has the same value across all $x\\neq\\mathtt{B}_{i}$ , which implies ", "page_idx": 24}, {"type": "equation", "text": "$$\np_{\\theta(t)}\\bigl(\\mathsf{C}_{i}|\\mathsf{A}_{i}\\searrow\\bigr)\\leq\\frac{1}{M}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Similarly, applying Lemma 2 to $Y(t)_{\\mathtt{B}_{i}}$ , we can obtain that ", "page_idx": 24}, {"type": "equation", "text": "$$\np_{\\theta(t)}(\\mathsf{C}_{i}|\\mathsf{B}_{i}\\to)\\geq1-\\frac{M-1}{2\\left(\\frac{M\\eta_{Y}t}{N}\\right)^{c}}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "C.3 Analysis of the reversal curse for the four-token sequences ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "In this section, we analyze the reversal curse where data points are four-token sentences $\\mathbf{\\nabla}\\mathbf{A}\\mathbf{R}_{1}\\mathbf{R}_{2}\\mathbf{B}^{\\circ}$ or ${}^{\\mathbf{\\hat{\\mu}}}\\mathbf{B}\\mathbf{R}_{1}\\mathbf{R}_{2}\\mathbf{A}^{\\mathbf{\\hat{\\mu}}}{}^{,\\;\\;}$ . For each sentence, A and B are two distinct tokens that represent two entities, and $\\mathtt{R}_{1}$ , $\\mathtt{R}_{2}$ are two special tokens jointly representing a relationship that is inverse to itself (e.g., $\\mathtt{R}_{1}\\mathtt{R}_{2}$ represents \u201cis the friend of\u201d, then $\\tt A R_{1}R_{2}B$ means $^{\\bullet\\bullet}\\mathtt{A}$ is the friend of $\\vec{\\mathsf{B}}^{\\bullet\\bullet}$ and $\\mathtt{B R}_{1}\\mathtt{R}_{2}\\mathtt{A}$ means $\\mathbf{\\mu}^{\\leftarrow}\\mathbf{B}$ is the friend of $\\mathtt{A}^{,,}$ ). ", "page_idx": 24}, {"type": "text", "text": "Datasets. Let $N_{\\mathrm{train}}~>~0$ and $N_{\\mathrm{test}}~>~0$ and denote $N_{\\mathrm{total}}~=~N_{\\mathrm{train}}\\,+\\,N_{\\mathrm{test}}$ . Let $\\mathtt{A}_{i},\\mathtt{B}_{i}\\ \\ \\in$ $\\nu,\\forall i\\in[N_{\\mathrm{total}}]$ be $K\\triangleq2N_{\\mathrm{total}}$ distinct tokens representing distinct entities. WLOG, we assume $\\mathtt{A}_{i},\\mathtt{B}_{i}\\in[K],\\forall\\dot{i}\\in[N_{\\mathrm{total}}]$ . Let $\\mathbb{R}_{1},\\mathbb{R}_{2}\\in\\mathcal{V}$ be two additional different tokens that jointly represent a relationship that is inverse to itself. Specifically, we have $\\mathtt{A}_{i}\\mathtt{R}_{1}\\mathtt{R}_{2}\\mathtt{B}_{i}$ and ${\\tt B}_{i}{\\tt R}_{1}{\\tt R}_{2}{\\tt A}_{i}$ for all $i\\in[N_{\\mathrm{total}}]$ . For notation convenience, we define the following two index sets ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{Z}_{\\mathrm{train}}=[N_{\\mathrm{train}}],\\qquad\\mathcal{Z}_{\\mathrm{test}}=[N_{\\mathrm{total}}]\\backslash\\mathcal{Z}_{\\mathrm{train}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "The training set $\\ensuremath{\\mathcal{D}}_{\\mathrm{train}}$ consists of all $\\mathtt{A}_{i}\\mathtt{R}_{1}\\mathtt{R}_{2}\\mathtt{B}_{i}$ and ${\\tt B}_{i}{\\tt R}_{1}{\\tt R}_{2}{\\tt A}_{i}$ for $i\\in\\mathcal{T}_{\\mathrm{train}}$ . In addition, $\\ensuremath{\\mathcal{D}}_{\\mathrm{train}}$ contains $\\mathtt{A}_{i}\\mathtt{R}_{1}\\mathtt{R}_{2}\\mathtt{B}_{i}$ for $i\\in\\mathcal{Z}_{\\mathrm{test}}$ . For convenience, we let $N=|\\mathcal{D}_{\\operatorname{train}}|$ to be the size of the training set. The test set $\\ensuremath{\\mathcal{D}}_{\\mathrm{test}}$ consists of ${\\tt B}_{i}{\\tt R}_{1}{\\tt R}_{2}{\\tt A}_{i}$ for $i\\in\\mathcal{Z}_{\\mathrm{test}}$ . Under our construction of the dataset, the LLM will learn the relationship between ${\\mathtt A}_{i}$ and $\\mathtt{B}_{i}$ for $i\\in\\mathcal{T}_{\\mathrm{train}}$ in both directions to deduce that the relationship $\\mathrm{{^{\\circ}R_{1}R_{2}}^{\\prime}}$ is reverse to itself, and learn the relationship between ${\\mathtt A}_{i}$ and $\\mathtt{B}_{i}$ for $i\\in\\mathcal{Z}_{\\mathrm{test}}$ in one direction and will be tested for the other. WLOG, we assume for each training sequence $(x_{1},\\mathsf{R}_{1},\\mathsf{R}_{2},x_{4})\\in\\mathcal{D}_{\\mathrm{train}}$ , $x_{4}\\in[N]$ . ", "page_idx": 24}, {"type": "text", "text": "Now we assume that the learning rate $\\eta_{Y}\\gg\\eta_{Z}$ and therefore can treat $X^{\\top}b_{T}$ as fixed when analyzing the dynamics of $Y$ . For any sequence $(x_{1},x_{2}=\\mathsf{R}_{1},x_{3}=\\mathsf{R}_{2},x_{4}=n)$ in training or test dataset with $T=3$ , we define ${\\pmb f}_{n}={\\mathrm{LN}}(X^{\\top}{\\pmb b}_{T})$ . Then the gradient of $Y$ in (3) becomes ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\dot{Y}=\\eta_{Y}\\pmb{f}_{n}(\\pmb{x}_{T+1}-\\pmb{\\alpha})^{\\top}=\\eta_{Y}\\pmb{f}_{n}(\\pmb{e}_{n}-\\pmb{\\alpha})^{\\top}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Note that for three-token sequences, $\\pmb{f}_{n}$ is a one-hot vector, and thus $\\dot{Y}$ has only one non-zero row, and each row of $Y$ can be analyzed independently. For the four-token sequences, we use the same reparameterization strategy as in [14], where we denote $W=[\\pmb{w}_{1},\\pmb{w}_{2},\\dots,\\pmb{w}_{K}]^{\\top}\\triangleq F^{\\top}Y\\in$ $\\mathbb{R}^{K\\times M}$ with $F=[f_{1},\\dots,f_{K}]\\overset{\\smile}{\\in}\\mathbb{R}^{M\\times K}$ . ", "page_idx": 25}, {"type": "text", "text": "Note that the parameter $W$ can be viewed as a combination of $Y$ and $Z$ where $Z$ is fixed. The following lemma shows the dynamics of $W$ , assuming we are performing gradient updates on $W$ instead of $Y$ . ", "page_idx": 25}, {"type": "text", "text": "Lemma 12 (Dynamics of $W$ ). Assume we perform gradient updates directly on $W$ instead of $Y$ with learning rate $\\eta_{Y}$ and batch size $^{\\,l}$ , and assume $M\\gg100$ and M 01.99 \u226a\u03b7Y < 1. Let t \u2273 N \u03b7lnY M and let $W(t)_{i}$ denote the $i$ -th row of $W(t)$ and $W(t)_{i j}$ denote the $(i,j)$ -th entry of $W(t)$ . Then for training sequence $(x_{1},R_{1},R_{2},x_{4})\\in\\mathcal{D}_{t r a i n}$ at time $t$ , we have ", "page_idx": 25}, {"type": "equation", "text": "$$\nW(t)_{x_{4},x_{4}}\\gtrsim\\ln\\left(M\\eta_{Y}t/N\\right),\\quad a n d\\quad W(t)_{x_{4},x}\\lesssim-\\ln\\left(M\\eta_{Y}t/N\\right)/M,\\quad\\forall x\\neq x_{4},\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "and for any test sequence $\\left(x_{1},R_{1},R_{2},x_{4}\\right)\\in\\mathcal{D}_{t e s t}$ , we have $W(t)_{x_{4},x}=0,\\forall x\\in[M].$ . ", "page_idx": 25}, {"type": "text", "text": "Proof. According to Lemma 3 of [14], for training sequence $(x_{1},\\mathsf{R}_{1},\\mathsf{R}_{2},x_{4})\\in\\mathcal{D}_{\\mathrm{train}}$ at time $t$ , only the $x_{4}$ -th row of $W$ will be updated and the gradient ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\dot{\\pmb{w}}_{x_{4}}(t)=\\eta_{Y}(\\pmb{x}_{4}-\\pmb{\\alpha}_{x_{4}}(t))\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $\\pmb{\\alpha}(t)=\\exp(\\pmb{w}_{x_{4}}(t))/(\\pmb{1}^{\\top}\\exp(\\pmb{w}_{x_{4}}(t)))$ . Therefore, the dynamics of $W$ is nearly identical to the dynamics of $Y$ in Lemma 2, and we can use the proof of Lemma 2 to conclude the results. ", "page_idx": 25}, {"type": "text", "text": "With the dynamics of $W$ , we can obtain the following result: ", "page_idx": 25}, {"type": "text", "text": "Proposition C.1 (Reversal curse for the four-token sequences). Assume we perform gradient updates directly on $W$ instead of $Y$ with learning rate $\\eta_{Y}$ and batch size $^{\\,l}$ , and assume $M\\gg100$ and $\\begin{array}{r}{\\frac{1}{M^{0.99}}\\overset{\\cdot}{\\ll}\\eta_{Y}<1.}\\end{array}$ $t\\overset{*}{\\underset{\\sim}{\\sim}}\\frac{N\\ln M}{\\eta_{Y}}$ $\\ln t\\gtrsim\\ln(N M/\\eta_{Y})$ training sequence $(x_{1},R_{1},R_{2},x_{4})\\in\\mathcal{D}_{t r a i n}$ at time $t_{;}$ , we have ", "page_idx": 25}, {"type": "equation", "text": "$$\np_{\\theta(t)}(x_{4}|x_{1},R_{1},R_{2})\\ge1-\\frac{M-1}{(M\\eta_{Y}t/N)^{c}}\\to1,\\quad a s\\ t\\to\\infty\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "for some constant $c>0$ , and for any test sequence $\\left(x_{1},R_{1},R_{2},x_{4}\\right)\\in\\mathcal{D}_{t e s t}$ that is not included in the training set $\\mathcal{D}_{t r a i n}$ , we have ", "page_idx": 25}, {"type": "equation", "text": "$$\np_{\\theta(t)}(x_{4}|x_{1},R_{1},R_{2})\\leq1/M.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Proof. For any sequence $(x_{1},x_{2}=\\mathsf{R}_{1},x_{3}=\\mathsf{R}_{2},x_{4})$ where $T=3$ , ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{p_{\\theta(t)}(x|x_{1},\\mathbb{R}_{1},\\mathbb{R}_{2})=\\frac{\\exp\\big(x^{\\top}Y(t)^{\\top}\\mathrm{LN}(X^{\\top}b_{T})\\big)}{\\sum_{x^{\\prime}\\in[M]}\\exp\\big(x^{\\prime^{\\top}\\top}V(t)^{\\top}\\mathrm{LN}(X^{\\top}b_{T})\\big)}}\\\\ &{\\qquad\\qquad\\qquad=\\frac{\\exp\\big(x^{\\top}Y(t)^{\\top}f_{x_{d}}\\big)}{\\sum_{x^{\\prime}\\in[M]}\\exp\\big(x^{\\prime^{\\top}\\top}V(t)^{\\top}f_{x_{d}}\\big)}}\\\\ &{\\qquad\\qquad=\\frac{\\exp\\big(x^{\\top}w_{x_{d}}(t)\\big)}{\\sum_{x^{\\prime}\\in[M]}\\exp\\big(x^{\\prime^{\\top}\\top}w_{x_{d}}(t)\\big)}}\\\\ &{\\qquad\\qquad=\\frac{\\exp\\big(W(t)_{x_{d},x}\\big)}{\\sum_{x^{\\prime}\\in[M]}\\exp\\big(W(t)_{x_{d},x^{\\prime}}\\big)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "The above next token probability formulation is almost identical to Proposition 4.2 after replacing $Y$ with $W$ . Combining the dynamics of $W$ as shown in Lemma 12, we can use the proof of Theorem 3 to conclude the result. \u53e3 ", "page_idx": 25}, {"type": "text", "text": "Finally, we discuss the role of $Z$ in the four-token-sequence settings. Note that Proposition C.1 assumes gradient update on $W$ instead of $Y$ . While we are not able to perform gradient updates on $W$ directly, it is equivalent to modifying the gradient of $Y$ to be ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\dot{Y}=\\eta_{Y}(\\pmb{f}_{n}-F E^{\\prime}\\pmb{e}_{n})(\\pmb{e}_{n}-\\pmb{\\alpha}_{n})^{\\top}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "according to Lemma 3 of [14], where the next token $x_{T+1}=n$ $n,E^{\\prime}=(I+E)^{-1}-I,E=F^{\\top}F-I,$ , and $F=[\\pmb{f}_{1},\\dots,\\pmb{f}_{N}]\\in\\bar{\\mathbb{R}}^{M\\times N}$ which only contains the next token that appears in the training set. Compared to the original gradient of $Y$ ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\dot{Y}=\\eta_{Y}\\,{\\pmb f}_{n}({\\pmb e}_{n}-{\\pmb\\alpha}_{n})^{\\top},\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "one can obtain that the modification of the gradient of $Y$ is small if $\\lambda_{1}(E)$ is small. ", "page_idx": 26}, {"type": "text", "text": "Note that $E_{i i}=\\|\\pmb{f}_{i}\\|_{2}^{2}{-}1=0$ and $E_{i j}=f_{i}^{\\top}f_{j}$ . Also, for any sequence $(x_{1}=n^{\\prime},x_{2}=\\mathsf{R}_{1},x_{3}=$ $\\ R_{2},x_{4}=n)$ in training set, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\pmb{f}_{n}=\\mathrm{LN}(\\boldsymbol{X}^{\\top}\\pmb{b}_{T})=\\frac{b_{13}\\pmb{e}_{n^{\\prime}}+b_{23}\\pmb{e}_{\\mathrm{R_{1}}}}{\\sqrt{b_{13}^{2}+b_{23}^{2}}},\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where ", "page_idx": 26}, {"type": "equation", "text": "$$\nb_{13}=\\exp(Z_{\\tt R_{2},n^{\\prime}}),b_{13}=\\exp(Z_{\\tt R_{2},R_{1}}).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Note that $\\mathtt{R}_{1}$ is a common token that appears in each training sentence, and $n^{\\prime}$ is a distinct token that only appears in one training sentence as a contextual token. By Theorem 2 of [14], under certain technical assumptions, $\\dot{Z}_{\\mathrm{R}_{2},n^{\\prime}}>0$ and $\\dot{Z}_{\\mathtt{R}_{1},n}<0$ and one can expect $Z_{\\mathrm{R}_{2},n^{\\prime}}-Z_{\\mathrm{R}_{1},n}$ to be sufficiently large after sufficient time of training. Therefore, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\pmb{f}_{n}=\\tilde{b}_{13}\\pmb{e}_{n^{\\prime}}+\\tilde{b}_{23}\\pmb{e}_{\\mathrm{R}_{1}}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "with $\\tilde{b}_{23}$ close to 0. Consider a simple case where for each $n\\in[N]$ , ${\\pmb f}_{n}=\\sqrt{1-c^{2}}{\\pmb e}_{n^{\\prime}}+c{\\pmb e}_{\\mathrm{R}_{1}}$ for $c$ sufficiently small. Then ", "page_idx": 26}, {"type": "equation", "text": "$$\nE_{i j}=f_{i}^{\\top}f_{j}=(\\sqrt{1-c^{2}}e_{i^{\\prime}}+c e_{\\tt R_{1}})^{\\top}(\\sqrt{1-c^{2}}e_{j^{\\prime}}+c e_{\\tt R_{1}})=c^{2}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Therefore, one can calculate that $\\lambda_{1}(E)=c^{2}(N-1)$ . When $\\begin{array}{r}{c\\ll\\frac{1}{\\sqrt{N}}}\\end{array}$ \u221a1N , we have \u03bb1(E) \u226a1, and thus the gradient update of $W$ and gradient update of $Y$ are almost the same. ", "page_idx": 26}, {"type": "text", "text": "D Experiments for chain-of-thought ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "In this section, we conduct experiments for COT on multi-layer transformers to validate theoretical results in Section 4.2. ", "page_idx": 26}, {"type": "text", "text": "Dataset construction. Similar to Section 5, we randomly sample three disjoint sets of entities $A,B,C\\,\\subset\\,\\nu$ , and reverse two additional tokens for $\\rightarrow$ and $\\rightsquigarrow$ , respectively. Next, we specify a bijection from $\\boldsymbol{\\mathcal{A}}$ to $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ , and a bijection from $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ to $\\mathcal{C}$ randomly. For each $\\mathtt{A}_{i}\\in\\mathcal{A}$ and its corresponding $\\mathtt{B}_{i}\\in\\mathcal{B}$ and $\\mathsf{C}_{i}\\in\\mathcal{C}$ , we can obtain a triple of sequences $\\left(\\mathsf{A}_{i}\\to\\mathsf{B}_{i},\\mathsf{B}_{i}\\to\\mathsf{C}_{i},\\mathsf{A}_{i}\\\\to\\mathsf{C}_{i}\\right)$ ), and split the set of all triples into training triples and validation triples. All three sequences of a training triple will be added to the training set, while for a validation triple, we add $\\mathtt{A}_{i}\\to\\mathtt{B}_{i}$ and $\\mathsf{B}_{i}\\to\\mathsf{C}_{i}$ to the training set and add $\\mathtt{A}_{i}\\sim\\mathtt{C}_{i}$ to the validation set. Therefore, the model will learn both direct and indirect implications for the training triples and only learn the direct implications for each validation triple while being tested on the indirect implication. ", "page_idx": 26}, {"type": "text", "text": "Results. Figure 3 shows the experiment results for COT using the same model architecture and configurations as in Figure 1 (the training set size is 540, and the validation set size is 60 resulting from 140 training triples and 60 validation triples), which is consistent with Theorem 7. One can refer to Appendix E.3 for additional experiments with various model configurations and vocabulary sizes. We also empirically validate the intransitivity of model weights (i.e., logits) for multi-layer transformers in Figure 4, which shows that for a validation triple $\\left(\\mathsf{A}_{i},\\mathsf{B}_{i},\\mathsf{C}_{i}\\right)$ of which only the direct implication $^{\\leftarrow}\\mathtt{A}_{i}\\to\\mathtt{B}_{i}{}^{,}$ and $^{\\bullet\\bullet}\\mathbb{B}_{i}\\to\\mathbb{C}_{i}{}^{,}$ appears in the training set, although the weights from ${\\mathtt A}_{i}$ to $\\mathtt{B}_{i}$ and from $\\mathtt{B}_{i}$ to $\\mathsf{C}_{i}$ are trained large as indicated by the diagonals of the first two bottom matrices, the weights from ${\\mathtt A}_{i}$ to $\\mathsf{C}_{i}$ gets hardly trained as indicated by the diagonals of the last matrix. We also emphasize that another reason that COT is necessary is that all tokens ${\\mathtt A}_{i}$ , $\\mathtt{B}_{i}$ , and $\\mathsf{C}_{i}$ are different tokens with randomly initialized embedding and thus irrelevant. When these tokens are relevant and show specific patterns, the validation loss can also get better. See more details in Appendix E.4. ", "page_idx": 26}, {"type": "image", "img_path": "QoWf3lo6m7/tmp/ea05b037cff0a7fd2788dd9df7b77df016ec12c41e6282981d4da50e42f795ac.jpg", "img_caption": ["Figure 3: Experiment results of COT under default configuration (see Table 3). The curves represent the (average) negative log probability of the model predicting the next token to be: (1) $\\mathtt{B}_{i}$ given the input ${^{\\bullet}\\mathbf{A}_{i}}\\rightarrow{^{\\bullet}}$ , (2) $\\mathsf{C}_{i}$ given the input ${}^{\\bullet}\\mathbf{B}_{i}\\rightarrow{}^{\\bullet}$ , or (3) $\\mathsf{C}_{i}$ given the input ${^{\\bullet}\\mathbb{A}_{i}}\\sim{^{\\circ}}^{\\circ}$ . Similar to the reversal curse experiment, while the sentences in the training set can be learned nearly perfectly, the model is not able to predict the correct next token in the validation set better than a uniformly random guess. Both curves are averaged over 10 random seeds. ", "Table 2: Full list of hyperparameters for AdamW optimizer and training "], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "E Additional Experimental Results ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "In this section, we show additional experimental results for Section 5. ", "page_idx": 27}, {"type": "text", "text": "E.1 Details of model architectures and hyperparameters ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "For both the reversal curse and COT experiments, we used the GPT2 model architecture $[63]^{6}$ and trained the model with the AdamW optimizer for 3000 epochs of batch size 64. See Table 2 for a full list of hyperparameters. We also conducted experiments under various model configurations and vocabulary sizes to show that the results in Section 5 and appendix D are consistent under different settings. See Table 3 for a complete list of different configurations, where the default choices are boldened. For each curve in all figures, the results are averaged over 10 trials, and the error bar is calculated using standard deviation. We run each trial on an Nvidia A100 GPU and it typically takes 0.5-1.5 hours for each trial. ", "page_idx": 27}, {"type": "table", "img_path": "QoWf3lo6m7/tmp/fa462088f8de870e3f64325781c252dc223f93b786e5a2d7640a10cbc0e91c0c.jpg", "table_caption": [], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "E.2 Additional experimental results for the reversal curse ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "In this section, we show additional experimental results for the reversal curse under different configurations, including different vocabulary sizes (Figure 5), different number of layers (Figure 6), ", "page_idx": 27}, {"type": "image", "img_path": "QoWf3lo6m7/tmp/9f3268a922561f89b7f25bb146af2f64d14e94431de86ba176444a3cb2573183.jpg", "img_caption": [], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "Figure 4: Visualization of the weights (logits) of the model with default configurations trained after 3000 epochs for COT experiment. The matrices are similar to Figure 2. The row tokens for the top matrices are ${\\mathtt A}_{i}$ , $\\mathtt{B}_{i}$ , ${\\mathtt A}_{i}$ and column tokens are $\\mathtt{B}_{i}$ , $\\mathsf{C}_{i}$ , $\\mathsf{C}_{i}$ for training triples respectively. Similarly, the bottom matrices correspond to validation triples. For validation triples $(\\mathsf{A}_{i},\\mathsf{B}_{i},\\mathsf{C}_{i})$ , the weights from ${\\mathtt A}_{i}$ to $\\mathsf{C}_{i}$ get hardly trained as indicated by the diagonals of the last matrix. ", "page_idx": 28}, {"type": "table", "img_path": "QoWf3lo6m7/tmp/e3bc343024b7444ef1d15e4dec90ef6bc64e80245ce7d273152b2609adcf65c4.jpg", "table_caption": [], "table_footnote": [], "page_idx": 28}, {"type": "text", "text": "Table 3: The list of different configurations for experiments in Appendices E.2 and E.3. Default choices are boldened for each row. ", "page_idx": 28}, {"type": "text", "text": "different positional encoding (Figure 7), different entity lengths (Figure 8) and whether token and positional embeddings are trainable or fixed (Figure 9). Our experimental results consistently show that the reversal curse happens under different settings. ", "page_idx": 28}, {"type": "text", "text": "We provide additional experimental results that (1) the reversal curse still happens even if the embedding dimension is much smaller than the vocabulary size in Appendix E.2.1; (2) the embeddings of different tokens are nearly orthogonal; (3) the reversal curse does not happen under the in-context learning settings. ", "page_idx": 28}, {"type": "text", "text": "E.2.1 The reversal curse under small embedding dimensions ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Although for theoretical analysis in Section 3, we assumed the embedding dimension is polynomial in the vocabulary size, in practice, the embedding dimension only needs to be the order of logarithm of the vocabulary size. Figure 10 shows that for a much smaller embedding size, the reversal curse still happens. ", "page_idx": 28}, {"type": "image", "img_path": "QoWf3lo6m7/tmp/0a1ead444b710c8927d823ec12c8db143a8606d044782ed86b610edaa56dabdc.jpg", "img_caption": ["(c) Vocabulary size 200 ", "(d) Vocabulary size 2000 "], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "Figure 5: Results for reversal curse for different vocabulary sizes. All other configurations are set as default values as in Table 3. The training set sizes for the above four experiments are 9, 20, 85, 850 respectively, and the validation set sizes are 1, 4, 15, 150 respectively. ", "page_idx": 29}, {"type": "image", "img_path": "QoWf3lo6m7/tmp/b8d12c937d9f81baddebf2a3f8c619e4517ca13770e8d1018dbf8a0fbd7cd834.jpg", "img_caption": [], "img_footnote": [], "page_idx": 29}, {"type": "image", "img_path": "QoWf3lo6m7/tmp/f489a2a55118d0c44d7d6c8c986a93680473d4b83a6fb93243884dbf860cad99.jpg", "img_caption": [], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "Figure 6: Results for reversal curse for different numbers of layers of the transformer. All other configurations are set as default values as in Table 3. ", "page_idx": 29}, {"type": "image", "img_path": "QoWf3lo6m7/tmp/9862233978b273d018a81785175efe1b0119fc052377c8b5fe562bb84f6135df.jpg", "img_caption": ["(a) No positional encoding "], "img_footnote": [], "page_idx": 29}, {"type": "image", "img_path": "QoWf3lo6m7/tmp/4b5735044d3dd6110730e724ea2de5005a37073e5ef97fa9e0260de87f0bf512.jpg", "img_caption": ["(b) Relative positional encoding "], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "Figure 7: Results for reversal curse with no positional encoding or relative positional encoding. For relative positional encoding, we follow the Rotary Position Embedding (RoPE) method proposed by [65]. We use the implementation of this repo, MIT license. All other configurations are set as default values as in Table 3. ", "page_idx": 29}, {"type": "image", "img_path": "QoWf3lo6m7/tmp/e72c0213949268af8fb7b4bc75166aaeaaf11293f4b546361937bf03bb890f9a.jpg", "img_caption": ["Figure 8: Results for reversal curse with different entity lengths. Each entity ${\\mathtt A}_{i}$ or $\\mathtt{B}_{i}$ consists of multiple tokens and different entities may have overlapped tokens. The \u201cTrain\u201d curve represents the negative log probability of predicting the first token of the output entity, and \u201cTrain (word)\u201d represents the negative log probability of predicting all tokens one by one of the output entity. All other configurations are set as default values as in Table 3. The training set sizes for the above two experiments are 680 and 250, respectively, and the validation set sizes are 120 and 50, respectively. "], "img_footnote": [], "page_idx": 30}, {"type": "image", "img_path": "QoWf3lo6m7/tmp/f126617730d73f0fb690bbc365a5ade828c46a37dc3f0d52d2db57da07adef65.jpg", "img_caption": ["Figure 9: Results for reversal curse with fixed token embedding and fixed positional embedding. All other configurations are set as default values as in Table 3. "], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "E.2.2 Near orthogonal embeddings ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Note that in Section 3, our analysis relies on the fact that embeddings of different tokens are nearly orthogonal, and in Section 4, the embeddings are effectively one-hot. In Figure 11, We show that in practice, even if the embedding dimension is much smaller than the vocabulary size, the near orthogonality condition still holds. ", "page_idx": 30}, {"type": "text", "text": "E.2.3 The reversal curse does not happen in ICL settings ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "We also emphasize that the reversal curse does not happen in ICL settings, which means if $^{\\bullet\\bullet}\\mathbb{A}\\to\\mathbb{B}^{\\bullet\\bullet}$ is provided as part of the prompt, then the model is able to answer $\\mathbf{\\omega^{6}B}\\leftarrow\\mathbf{\\DeltaA^{\\circ}}$ . Figure 12 shows preliminary results of ICL. All the sentences in the dataset have the format of $\\begin{array}{r}{^{\\leftarrow}\\mathtt{A}_{i}\\mathtt{R}\\mathtt{B}_{j}\\Leftrightarrow\\mathtt{B}_{j}\\mathtt{R}^{-1}\\mathtt{A}_{i}^{\\,,}}\\end{array}$ , which is a seven-token sentence where $\\mathtt{R}$ and $\\mathtt{R}^{-1}$ is a pair of relationships inverse to each other, and $\\leftrightarrow$ is another reserved token representing equivalence. There are ten different $\\mathtt{B}_{j}$ and $n$ different ${\\mathtt A}_{i}$ where $n=100$ for the left figure in Figure 12 and $n=200$ for the right figure. For each $A_{i}$ , we construct ten sentences using different $B_{j}$ , and we randomly chose three of them to be included in the validation set and seven other sentences in the training set. The result of Figure 12 shows that the reversal curse does not happen during the ICL setting. ", "page_idx": 30}, {"type": "image", "img_path": "QoWf3lo6m7/tmp/12cbc6febb50b20962a02be2766f9d71324193047c731b1e88400e1148504870.jpg", "img_caption": [], "img_footnote": [], "page_idx": 31}, {"type": "text", "text": "Figure 10: Results for reversal curse for different numbers of layers and different embedding dimensions. All other configurations are set as default values as in Table 3. ", "page_idx": 31}, {"type": "image", "img_path": "QoWf3lo6m7/tmp/d13896fe52c59f69f294c9b65b692da7733001c48dbdf68d38269f0a8e04b27a.jpg", "img_caption": [], "img_footnote": [], "page_idx": 31}, {"type": "text", "text": "Figure 11: A heat map of cosine similarity between token embeddings (all ${\\mathtt A}_{i}$ and $\\mathtt{B}_{i}$ ) after 3000 epochs training under different settings. The settings are the same as Figure 10. Under different numbers of layers or embedding dimensions, most of the non-diagonal entries are close to 0, which shows that the embeddings of different tokens are nearly orthogonal. ", "page_idx": 31}, {"type": "text", "text": "E.3 Additional experimental results for chain-of-thought ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "In this section, we show additional experimental results for COT under different configurations, including different vocabulary sizes (Figure 13), different number of layers (Figure 14), different positional encoding (Figure 15), different entity lengths (Figure 16) and whether token and positional embeddings are trainable or fixed (Figure 17). Note that our experimental results consistently show the necessity of COT under different settings. ", "page_idx": 31}, {"type": "text", "text": "E.4 Chain-of-thought with relevant tokens ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "In Appendix D, we briefly mentioned that the irrelevance of different entity tokens is one of the reasons that COT is necessary. Now, we show that if the entity tokens are correlated and show specific patterns, it is possible for a model to deduce indirect implications automatically. ", "page_idx": 31}, {"type": "text", "text": "Instead of using single tokens ${\\mathtt A}_{i}$ , $\\mathtt{B}_{i}$ , $\\mathsf{C}_{i}$ to represent each entity, now we use two tokens Ai, Bi, Ci to represent entities, where A, B and C are three common tokens shared by each triple, and token i are distinct for each triple. Figure 18 shows that for the above version of COT where tokens in the same chain are correlated, the model is able to \u201cdeduce\u201d Ai $\\rightsquigarrow\\,\\complement{\\dot{1}}$ after training on $\\tt A i\\rightarrow B i$ , $\\mathtt{B i}\\to\\mathtt{C i}$ and other training samples. ", "page_idx": 31}, {"type": "image", "img_path": "QoWf3lo6m7/tmp/1732b0905af417f75503d72b9a6f026a7b0d4bcba1399969df3238ff42ff27eb.jpg", "img_caption": [], "img_footnote": [], "page_idx": 32}, {"type": "image", "img_path": "", "img_caption": ["Figure 12: Training and validation loss under in-context learning (ICL) settings. All sentences consist of seven tokens and have the form of $\\begin{array}{r}{{^{\\star}\\mathtt{A}_{i}\\mathtt{R}\\mathtt{B}_{j}}\\Leftrightarrow\\mathtt{B}_{j}\\mathtt{R}^{-1}\\mathtt{A}_{i}^{\\,,}{^{,}}}\\end{array}$ . The loss is calculated on the last token. For the left figure, the training set size is 700, the validation set size is 300, and the vocabulary size is 400; for the right figure, the training set size is 1400, the validation set size is 600, and the vocabulary size is 800. All other configurations are set as default values as in Table 3. The result shows that the reversal curse does not happen in ICL settings, i.e., if $\\^{\\leftarrow}\\mathtt{A}_{i}\\mathtt{R B}_{j}\\\"$ is provided as part of the prompt, then the model is able to recognize $\\mathsf{^{\\bullet}B_{j}R^{-1}A_{i}}^{\\bullet}$ . "], "img_footnote": [], "page_idx": 32}, {"type": "image", "img_path": "QoWf3lo6m7/tmp/f42af3797917c75c03bcff4b5d1cf09e4ee662e3ad0e31d60edeb83503c717e6.jpg", "img_caption": ["Figure 13: Results for COT for different vocabulary sizes. All other configurations are set as default values as in Table 3. The training set sizes for the above four experiments are 14, 32, 135, 1350 respectively, and the validation set sizes are 1, 4, 15, 150 respectively. ", "(c) Vocabulary size 200 ", "(d) Vocabulary size 2000 "], "img_footnote": [], "page_idx": 32}, {"type": "image", "img_path": "QoWf3lo6m7/tmp/adca9ab025ae24c392252cdd36cb6c9510f5e69d71310a5762b656e1303179b9.jpg", "img_caption": [], "img_footnote": [], "page_idx": 33}, {"type": "text", "text": "Figure 14: Results for COT for different number of layers of the transformer. All other configurations are set as default values as in Table 3. ", "page_idx": 33}, {"type": "image", "img_path": "QoWf3lo6m7/tmp/fac0660782b6b87a242a1e01868be91f40cd097f44925b19fd378f5bab56f49d.jpg", "img_caption": ["(a) No positional encoding "], "img_footnote": [], "page_idx": 33}, {"type": "image", "img_path": "QoWf3lo6m7/tmp/afd2fedd0563326e08ecd2ab2e2372588c0991d7972394f2d2c3255b35dd3a39.jpg", "img_caption": ["(b) Relative positional encoding "], "img_footnote": [], "page_idx": 33}, {"type": "text", "text": "Figure 15: Results for COT with no positional encoding or relative positional encoding. For relative positional encoding, we follow the Rotary Position Embedding (RoPE) method proposed by [65]. All other configurations are set as default values as in Table 3. ", "page_idx": 33}, {"type": "image", "img_path": "QoWf3lo6m7/tmp/c7f009bd2093a6f5d6d3d14ac77da035c48a6271a8aadafb623424eb3ca24cc2.jpg", "img_caption": [], "img_footnote": [], "page_idx": 33}, {"type": "text", "text": "Figure 16: Results for COT with different entity lengths. The setting and curves are similar to Figure 8. All other configurations are set as default values as in Table 3. The training set sizes for the above two experiments are 1080 and 400, respectively, and the validation set sizes are 120 and 50. ", "page_idx": 33}, {"type": "image", "img_path": "QoWf3lo6m7/tmp/20a1bc254d40fe370060d1c922a776f59faa941b750ca34ae8a6f4d2896e9ea9.jpg", "img_caption": ["Figure 17: Results for COT with fixed token embedding and fixed positional embedding. All other configurations are set as default values as in Table 3. "], "img_footnote": [], "page_idx": 34}, {"type": "image", "img_path": "QoWf3lo6m7/tmp/4bfe6a144c120b0489fa7ae380aeafc4e8cdf31af8a79575300a9ee8ea0682b1.jpg", "img_caption": ["(c) Training set size 400 ", "(d) Training set size 1600 "], "img_footnote": [], "page_idx": 34}, {"type": "text", "text": "Figure 18: Results for COT where each entity is represented by two tokens, i.e., Ai, Bi, or Ci. The validation set sizes are 50. The model is able to \u201cdeduce\u201d unseen Ai $\\rightsquigarrow\\rightsquigarrow$ by learning underlying patterns. ", "page_idx": 34}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: See abstract and Section 1. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 35}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Justification: See Sections 1 and 6. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 35}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: See Sections 3 and 4 and appendices B and C. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 36}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: See Section 5 and appendices D and E. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 36}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Justification: Our code is available at https://github.com/marlo-z/reversal_ curse_analysis/. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 37}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: See Section 5 and appendices D and E. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 37}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: See Appendix E.1. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 37}, {"type": "text", "text": "", "page_idx": 38}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: See Appendix E.1. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 38}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: The research conducted in the paper conforms, in every respect, with the NeurIPS code of Ethics. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 38}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: See Section 1 and Section 6. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 38}, {"type": "text", "text": "", "page_idx": 39}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 39}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 39}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: See Appendix E. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 39}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 40}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: Our code is available at https://github.com/marlo-z/reversal_ curse_analysis/. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 40}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 40}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing or research with human subjects. Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 40}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing or research with human subjects. Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 40}]