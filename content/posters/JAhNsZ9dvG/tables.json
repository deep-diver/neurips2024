[{"figure_path": "JAhNsZ9dvG/tables/tables_5_1.jpg", "caption": "Table 1: Inference speed with RAM offloading, A100 GPU, Chat / Instruct models, using SpecExec (SX) and SpecInfer (SI) methods. Generation rate (\"Gen. rate\") denotes the average number of draft model tokens accepted for one target model iteration.", "description": "This table presents the inference speed of the SpecExec and SpecInfer methods on an A100 GPU with RAM offloading for various large language models (LLMs).  It shows the speed in tokens per second, the generation rate (number of draft model tokens accepted per target model iteration), and the speedup achieved compared to a baseline. The table includes results for different LLMs (Llama 2, Mistral, Llama 3) with and without quantization, and uses two different datasets (OASST, MTBench).  The 't' column indicates whether temperature sampling was used (0.6 or 0).", "section": "5.3 Inference Speed"}, {"figure_path": "JAhNsZ9dvG/tables/tables_8_1.jpg", "caption": "Table 1: Inference speed with RAM offloading, A100 GPU, Chat / Instruct models, using SpecExec (SX) and SpecInfer (SI) methods. Generation rate (\"Gen. rate\") denotes the average number of draft model tokens accepted for one target model iteration.", "description": "This table presents the inference speed results obtained using Speculative Execution (SpecExec) and SpecInfer methods on an A100 GPU with RAM offloading.  The experiments were conducted using several large language models (LLMs) in both Chat and Instruct configurations. The table shows the inference speed (tokens per second), generation rate (average number of draft tokens accepted per target model iteration), the draft model budget size, and the speedup achieved compared to a baseline method (SpecInfer).", "section": "5.3 Inference Speed"}, {"figure_path": "JAhNsZ9dvG/tables/tables_8_2.jpg", "caption": "Table 3: SpecExec inference speed on consumer GPUs with offloading, chat/instruct models, Llama 2 70B-GPTQ target model, t = 0.6, OpenAssistant dataset.", "description": "This table presents the inference speed of the SpecExec model on various consumer-grade GPUs with offloading enabled. It uses the Llama 2 70B-GPTQ model as the target model and the OpenAssistant dataset for evaluation.  The table shows the generation rate (tokens per second) and speedup compared to a baseline (SpecInfer).  Different draft models and budget sizes are tested, demonstrating the effects of hardware and parameter choices on performance.", "section": "5.3 Inference Speed"}, {"figure_path": "JAhNsZ9dvG/tables/tables_9_1.jpg", "caption": "Table 3: SpecExec inference speed on consumer GPUs with offloading, chat/instruct models, Llama 2 70B-GPTQ target model, t = 0.6, OpenAssistant dataset.", "description": "This table presents the inference speed of the SpecExec model on various consumer-grade GPUs using offloading.  It shows the impact of different GPUs (RTX 4090, RTX 4060, RTX 3090, and RTX 2080Ti) and draft model choices (Llama 2-7B and ShearedLlama-1.3B) on the generation rate (tokens per second) and speedup compared to a baseline. The budget refers to the number of tokens considered in the draft tree. The table highlights the effectiveness of SpecExec across various hardware configurations, showcasing its potential for interactive LLM inference on consumer-grade devices.", "section": "5.3 Inference Speed"}, {"figure_path": "JAhNsZ9dvG/tables/tables_19_1.jpg", "caption": "Table 4: SpecExec Inference speed without offloading, A100 GPU.", "description": "This table presents the results of experiments evaluating the inference speed of SpecExec without offloading, using an A100 GPU. It shows the generation rate (average number of draft model tokens accepted per target model iteration), speed in tokens per second, and speedup compared to a baseline for different models (SL-1.3B / Vicuna-33B), datasets (OASST-1, C4, WikiText-2), and temperature settings (t = 0.6 and t = 0). The budget refers to the maximum number of tokens in the draft tree.", "section": "5.3 Inference Speed"}]