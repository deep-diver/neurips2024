[{"figure_path": "E2BYPreuU8/figures/figures_9_1.jpg", "caption": "Figure 1: Simulations results on Gaussian and Example 4.1 show that the convergence of ab satisfies Theorem 4.1. In addition, the trained transformer can recover the sequence with the initial token from Example 4.1, but fails to recover the Gaussian initial token, which verifies Theorem 4.2 and Proposition 4.1, respectively.", "description": "This figure presents simulation results to verify the theoretical findings of the paper.  It demonstrates the convergence behavior of the parameter 'ab' under two different initial token distributions: a Gaussian distribution and a distribution as defined in Example 4.1.  The plots show that the convergence of 'ab' aligns with Theorem 4.1 in both cases.  Importantly, it highlights that the model successfully recovers the sequence when using the Example 4.1 initial token but fails to do so when using the Gaussian initial token, thus validating Theorem 4.2 and Proposition 4.1.", "section": "Simulation results"}, {"figure_path": "E2BYPreuU8/figures/figures_36_1.jpg", "caption": "Figure 1: Simulations results on Gaussian and Example 4.1 show that the convergence of ab satisfies Theorem 4.1. In addition, the trained transformer can recover the sequence with the initial token from Example 4.1, but fails to recover the Gaussian initial token, which verifies Theorem 4.2 and Proposition 4.1, respectively.", "description": "This figure presents simulation results that validate the theoretical findings of the paper.  The plots show the dynamics of the product 'ab' (a parameter related to the trained transformer's learning process) during the training process for different initial conditions.  The results for two scenarios are depicted:\n\n1. **Gaussian Initial Token:** The initial token is sampled from a Gaussian distribution. This demonstrates the convergence behavior according to Theorem 4.1 and illustrates the limitations described in Proposition 4.1 showing that the trained model fails to learn the true distribution.\n2. **Sparse Initial Token (Example 4.1):** The initial token is sampled from a distribution defined in Example 4.1 in the paper. This condition leads to the convergence to the true distribution, highlighting a sufficient condition for mesa-optimization to emerge successfully.", "section": "Simulation results"}, {"figure_path": "E2BYPreuU8/figures/figures_37_1.jpg", "caption": "Figure 1: Simulations results on Gaussian and Example 4.1 show that the convergence of ab satisfies Theorem 4.1. In addition, the trained transformer can recover the sequence with the initial token from Example 4.1, but fails to recover the Gaussian initial token, which verifies Theorem 4.2 and Proposition 4.1, respectively.", "description": "This figure presents simulation results that validate the theoretical findings of the paper.  The plots show the convergence of the product 'ab' (a parameter from the linear transformer model) under different initializations and data distributions.  The key observation is that the model's performance is dependent on the data distribution, specifically verifying Theorem 4.1, 4.2 and Proposition 4.1 which describe when and how well the transformer learns to minimize an OLS problem.", "section": "Simulation results"}, {"figure_path": "E2BYPreuU8/figures/figures_38_1.jpg", "caption": "Figure 7: WKQ and WPV of full-one start points with different diagonal initialization. The read blocks in Assumption 3.1 are presented, which are related to the final prediction.", "description": "This figure displays heatmaps of the matrices WKQ and WPV obtained after training a one-layer linear transformer with a full-one initial token. Different diagonal initializations (ao, bo) were used: (0.1, 0.1), (0.5, 1.5), and (2, 2).  The heatmaps visualize the values of the matrices, highlighting the diagonal structure emphasized by the diagonal initialization strategy (Assumption 3.1). The highlighted (red) blocks in the matrices are those relevant to the next-token prediction task (\u0177t).  The dynamics of ab (the product of diagonal elements of WPV and WKQ) over the epochs are also presented, showing its convergence to a certain value based on the specific initialization.", "section": "Simulation results"}, {"figure_path": "E2BYPreuU8/figures/figures_39_1.jpg", "caption": "Figure 5: Results of Gaussian start point (\u03c3 = 1) and standard Gaussian initialization with different variance \u03c3\u03c9. The read blocks in Assumption 3.1 are presented, which are related to the final prediction. The parameter matrices retain the same strong diagonal structure and test performance as those of the diagonal initialization.", "description": "This figure displays simulation results that verify the theoretical findings of the paper. It demonstrates the convergence behavior of the parameters (WKQ and WPV) under different variance (\u03c3\u03c9) of Gaussian initialization.  The results show that even with varied Gaussian initialization, the key parameters maintain a strong diagonal structure, aligning with the theoretical prediction. This suggests robustness to the initialization method and the convergence to the mesa-optimizer hypothesis.", "section": "Simulation results"}, {"figure_path": "E2BYPreuU8/figures/figures_40_1.jpg", "caption": "Figure 5: Results of Gaussian start point (\u03c3 = 1) and standard Gaussian initialization with different variances \u03c3\u03c9. The read blocks in Assumption 3.1 are presented, which are related to the final prediction. The parameter matrices retain the same strong diagonal structure and test performance as those of the diagonal initialization.", "description": "This figure visualizes the results of experiments using Gaussian start points (\u03c3 = 1) with standard Gaussian initialization, but varying the variance (\u03c3\u03c9).  It shows the learned matrices WKQ and WPV for different values of \u03c3\u03c9.  The key observation is that despite the change in variance, the resulting matrices maintain a strong diagonal structure, similar to the results obtained with diagonal initialization. This supports the paper's claim that the diagonal structure is a significant factor in the model's behavior and that this behavior is relatively robust to changes in the initialization.", "section": "Simulation results"}, {"figure_path": "E2BYPreuU8/figures/figures_41_1.jpg", "caption": "Figure 7: WKQ and WPV of full-one start points with different diagonal initialization. The red blocks in Assumption 3.1 are presented, which are related to the final prediction.", "description": "This figure visualizes the learned weight matrices WKQ and WPV of a one-layer linear transformer with a full-one initial token under different diagonal initializations. The color intensity in the heatmap represents the magnitude of the weights. The red blocks highlight the parameters that are directly related to the next-token prediction, as specified in Assumption 3.1 of the paper.  The figure demonstrates how the weight matrices evolve during training with different starting points, providing insights into the impact of initialization on model learning.  The patterns in the matrices can be related to the overall mesa-optimization behavior discussed in the paper. ", "section": "Simulation results"}]