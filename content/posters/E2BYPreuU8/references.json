{"references": [{"fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-12-01", "reason": "This paper introduced the Transformer architecture, the foundation of the autoregressive models discussed in the paper."}, {"fullname_first_author": "Tom B. Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-12-01", "reason": "This paper highlighted the in-context learning (ICL) ability of transformers, a key phenomenon the current paper investigates."}, {"fullname_first_author": "Johannes von Oswald", "paper_title": "Uncovering mesa-optimization algorithms in transformers", "publication_date": "2023-09-01", "reason": "This paper is a seminal work that introduced the mesa-optimization hypothesis to explain in-context learning, which is the core concept this paper builds upon."}, {"fullname_first_author": "Ruiqi Zhang", "paper_title": "Trained transformers learn linear models in-context", "publication_date": "2024-01-01", "reason": "This paper provides theoretical foundations linking transformers trained via in-context learning to gradient descent methods for linear regression, offering a critical theoretical perspective for the current paper."}, {"fullname_first_author": "Michael Eli Sander", "paper_title": "How do transformers perform in-context autoregressive learning?", "publication_date": "2024-01-01", "reason": "This paper offers a detailed characterization of the loss landscape of autoregressively trained transformers, which the current paper further investigates."}]}