[{"type": "text", "text": "Almost Surely Asymptotically Constant Graph Neural Networks ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Sam Adam-Day\\* Michael Benedikt ismail ilkan Ceylan Ben Finkelshtein Department of Computer Science University of Oxford Oxford, UK ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We present a new angle on the expressive power of graph neural networks (GNNs) by studying how the predictions of real-valued GNN classifiers, such as those classifying graphs probabilistically, evolve as we apply them on larger graphs drawn from some random graph model. We show that the output converges to a constant function, which upper-bounds what these classifiers can uniformly express. This strong convergence phenomenon applies to a very wide class of GNNs, including state of the art models, with aggregates including mean and the attention-based mechanism of graph transformers. Our results apply to a broad class of random graph models, including sparse and dense variants of the Erd6s-R\u00e9nyi model, the stochastic block model, and the Barabasi-Albert model. We empirically validate these findings, observing that the convergence phenomenon appears not only on random graphs but also on some real-world graphs. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Graph neural networks (GNNs) [44, 21] have become prominent for graph machine learning with applications in domains such as life sciences [47, 11, 25, 55]. Their empirical success motivated work investigating their theoretical properties, pertaining to their expressive power [52, 39, 4, 8, 1, 43, 23, 19, 54], generalization capabilities [18, 33, 38, 45, 40], and convergence properties [28, 27, 38, 32, 2]. ", "page_idx": 0}, {"type": "text", "text": "We consider GNNs outputting real-valued vectors, such as those which classify graphs probabilistically, and ask: how do the outputs of these GNNs evolve as we apply them on larger graphs drawnfrom a random graph model? ", "page_idx": 0}, {"type": "text", "text": "Our study provides a surprising answer to this question: the output of many GNNs eventually ", "page_idx": 0}, {"type": "image", "img_path": "Dn68qdfTry/tmp/9073f603c1fcb94bcb3f6923164633fe7248c852c1993c71836d1deda29ca96b.jpg", "img_caption": ["Figure 1: The output of the considered GNNs eventually become constant as the graph sizes increase. "], "img_footnote": [], "page_idx": 0}, {"type": "text", "text": "become independent of their inputs - each model eventually outputs the same values on all graphs - as graph sizes increase (Figure 1). This \u201calmost sure convergence\u201d to a constant distribution is much stronger than convergence to some limit object [28, 27, 32]. The immediate consequence of this strong convergence phenomenon is to upper bound the uniform expressiveness of the considered model architectures: these architectures can uniformly express only the classifiers that are almost surely asymptotically constant. In other words, our results provide impossibility results for what tasks are in principle learnable by GNNs. While our top-level results are for graph classification, in the process we provide strong limitations on what node- and edge-classification can be performed by GNNs on random graphs: see, for example Theorem 5.3. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Scope of the result. The core approach in graph machine learning is based on iteratively updating node representations of an input graph by an aggregate of messages flowing from the node's neighbours [20]. This approach can be extended to global aggregates [5]. Our main result holds for all architectures that use weighted mean as an aggregation function and it is extremely robust in the following two dimensions: ", "page_idx": 1}, {"type": "text", "text": "1. Model architectures: Our result is very general and abstracts away from low-level architectural design choices. To achieve this, we introduce an aggregate term language using weighted mean aggregation and provide an \u201calmost sure optimization\" result for this language: our result states that every term in the language can be simplified to Lipschitz functions for most inputs. Thus, any architecture that can be expressed in this language follows the same convergence law. This includes graph attention networks (GATs) [49], as well as popular (graph) transformers, such as the General, Powerful, Scalable Graph Transformer (GPS) with random walk encodings [41]. The term language can seamlessly capture common design choices, such as skip and jumping knowledge connections [51], or global aggregation schemes [5]. ", "page_idx": 1}, {"type": "text", "text": "2. Random graph models: All results apply to a wide class of random graph models, including Erds-Renyi models of various sparsity levels, the Barabasi-Albert preferential attachment model, and the stochastic block model. The sparse models are more realistic than their dense counterparts which makes it typically harder to obtain results for them. This is also reflected in our study, as the results for sparse and dense models require very different proofs. ", "page_idx": 1}, {"type": "text", "text": "Contributions. The key contributions of this paper are as follows: ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "\u00b7 We introduce a flexible aggregate term language with attractive closure properties (Section 4) and prove an \u201calmost sure convergence\u201d result for this language relative to a wide class of random graph models (Section 5). This result is of independent interest since it pushes the envelope for convergence results from classical logical languages [15, 36] to include aggregation. \u00b7 We show that a diverse class of architectures acting as real-valued graph classifiers can be expressed in the term language (Section 4). In Section 5 we present \u201calmost sure convergence\" results for our term language, from which we derive results about GNNs (Corollary 5.2). The results are robust to many practical architectural design choices and even hold for architectures using mixtures of layers from different architectures. We also show strong convergence results for real-valued node classifiers in many graph models. \u00b7 We validate these results empirically, showing the convergence of these graph classifiers in practice (Section 6) on graphs drawn from the random models studied. In addition, we probe the real-world significance of our results by testing for convergence on a dataset with varying size dataset splits. Across all experiments we observe rapid convergence to a constant distribution. Interestingly, we note some distinctions between the convergence of the sparse and non-sparse Erd6s-R\u00e9nyi model, which we can relate to the proof strategies for our convergence laws. ", "page_idx": 1}, {"type": "text", "text": "2 Related work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Uniform expressiveness. The expressive power of MPNNs is studied from different angles, including their power in terms of graph distinguishability [52, 39]. The seminal results of Xu et al. [52], Morris et al. [39] show that MPNNs are upper bounded by the 1-dimensional Weisfeiler Leman graph isomorphism test (1-WL) in terms of graph distinguishability. WL-style expressiveness results are inherently non-uniform, i.e., the model construction is dependent on the graph size. There are also recent studies that focus on uniform expressiveness [4, 42, 2]. In particular, Adam-Day et al. [2] investigate the uniform expressive power of GNNs with randomized node features, which are known to be more expressive in the non-uniform setting [1, 43]. They show that for classical Erd6s-Renyi graphs, GNN binary classifiers display a zero-one law, assuming certain restrictions on GNN weights and the random graph model. We focus on real-valued classifiers, where their results do not apply, while dealing with a wider class of random graph models, subsuming popular architectures such as graph transformers. ", "page_idx": 1}, {"type": "text", "text": "Convergence laws for languages. Our work situates GNNs within a rich term language built up from graph and node primitives via real-valued functions and aggregates. Thus it relates to convergence laws for logic-based languages on random structures, dating back to the zero-one law of Fagin [15], including [35, 46, 31]. We are not aware of any prior convergence laws for languages with aggregates; the only work on numerical term languages is by Gradel et al. [22], which deals with a variant of first-order logic in general semi-rings. ", "page_idx": 2}, {"type": "text", "text": "Other notions of convergence on random graphs. The works of Cordonnier et al. [9], Keriven et al. [28, 27], Maskey et al. [38], Levie [32] consider convergence to continuous analogues of GNNs, often working within metrics on a function space. The results often focus on dense random graph models, such as graphons [34]. Our approach is fundamentally different in that we can use the standard notion of asymptotic convergence in Euclidean space, comparable to traditional language-based convergence results outlined above, such as those by Fagin [15] and Lynch [36]. The key point is that a.a.s. constancy is a very strong notion of convergence and it does not follow from convergence in the senses above. In fact, obtaining such a strong convergence result depends heavily on the details of the term language, as well as the parameters that control the random graph: see Section 7 for further discussion of the line between a.a.s. convergence and divergence. Our study gives particular emphasis to sparse random graph models, like Barabasi-Albert, which are closer to graphs arising in practice. ", "page_idx": 2}, {"type": "text", "text": "3 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Featured random graphs and convergence ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Random graphs. We consider simple, undirected graphs $G=(V_{G},E_{G},H_{G})$ where each node is associated with a vector of node features given by $H_{G}:V_{G}\\to\\mathbb{R}^{d}$ . We refer to this as a featured graph. We are interested in random graph models, specifying for each number $n$ a distribution $\\mu_{n}$ on graphs with $n$ nodes, along with random graph feature models, where we have a distribution $\\mu_{n}$ on featured graphs with $n$ nodes. Given a random graph model and a distribution $\\nu$ over $\\mathbb{R}^{d}$ , we get a random graph feature model by letting the node features be chosen independently of the graph structure via $\\nu$ ", "page_idx": 2}, {"type": "text", "text": "Erdos-R\u00e9nyi and the stochastic block model. The most basic random graph model we deal with is theErdos-R\u00e9nyi distribution $\\mathrm{ER}(n,p(n))$ , where an edge is included in the graph with $n$ nodes with probability $p(n)$ [14]. The classical case is when $p(n)$ is independent of the graph size $n$ , which we refer as the dense ER distribution. We also consider the stochastic block model $\\operatorname{SBM}(n_{1},\\dotsc,n_{M},P)$ , which contains $m$ communities of sizes $n_{1},\\ldots,n_{M}$ and an edge probability matrix between communities $\\boldsymbol{P}\\:\\in\\:\\mathbb{R}^{M\\times M}$ A community $i$ is sampled from the Erdos-Renyi distribution $\\mathrm{ER}(n,p(n)=P_{i,i})$ and an edge to a node in another community $j$ is included with probability $P_{i,j}$ ", "page_idx": 2}, {"type": "text", "text": "The Barabasi-Albert preferential attachment model. Many graphs encountered in the real world obey a power law, in which a few vertices are far more connected than the rest [7]. The BarabasiAlbert distribution was developed to model this phenomenon [3]. It is parametrised by a single integer $m$ ,and the $n$ -vertexgraph $\\bar{\\mathrm{BA}}(n,m)$ is generated sequentially, beginning with a fully connected $m$ -vertex graph. Nodes are added one at a time and get connected via $m$ new edges to previous nodes, where the probability of attaching to a node is proportional to its degree. ", "page_idx": 2}, {"type": "text", "text": "Almost sure convergence. Given any function $F$ from featured graphs to real vectors and a random featured graph model $(\\mu_{n})_{n\\in\\mathbb{N}}$ , we say $F$ converges asymptotically almost surely (converges a.a.s.) to a vector $_{\\textit{z}}$ with respect to $\\bar{\\mu}$ if for all $\\epsilon,\\theta>0$ there is $N\\in\\mathbb{N}$ such that for all $n\\,\\geq\\,N$ ,with probability at least $1-\\theta$ when drawing featured graphs $G$ from $\\mu_{n}$ , we have that $\\|F(G)-z\\|<\\epsilon$ ", "page_idx": 2}, {"type": "text", "text": "3.2  Graph neural networks and graph transformers ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We first briefly introduce message passing neural networks (MPNNs) [20], which include the vast majority of graph neural networks, as well as (graph) transformers. ", "page_idx": 2}, {"type": "text", "text": "Message passing neural networks. Given a featured graph $G=(V_{G},E_{G},H_{G})$ , an MPNN sets the initial features $h_{v}^{(0)}\\;=\\;H_{G}(v)$ and iteratively updates the feature $h_{v}^{(\\ell)}$ of each node $v$ ,for ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathrm{UPD}^{(\\ell)}\\left(h_{v}^{(\\ell)},\\mathsf{A G G}^{(\\ell)}\\Big(h_{v}^{(\\ell)},\\big\\{\\!\\|h_{u}^{(\\ell)}\\;\\big|\\;u\\in\\mathcal{N}(v)\\!\\big\\}\\!\\right),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\{\\!\\!\\left\\{\\cdot\\right\\}\\!\\!\\}$ denotes a multiset and ${\\mathrm{UPD}}^{(\\ell)}$ and $\\mathrm{AGG}^{(\\ell)}$ are differentiable update and aggregation functions, respectively. The final node embeddings are pooled to form a graph embedding vector $z_{G}^{(L)}$ to predict properties of entire graphs. A MEANGNN is an MPNN where the agregate is mean. ", "page_idx": 3}, {"type": "text", "text": "GATs. One class of MPNNs are graph attention networks (GATs) [49], where each node is updated with a weighted average of its neighbours\u2032 representations, letting (e+1)be: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\sum_{\\boldsymbol{u}\\in\\mathcal{N}(\\boldsymbol{v})}\\frac{\\exp\\left(\\operatorname{score}\\left(h_{\\boldsymbol{v}}^{(\\ell)},h_{\\boldsymbol{u}}^{(\\ell)}\\right)\\right)}{\\sum_{\\boldsymbol{w}\\in\\mathcal{N}(\\boldsymbol{v})}\\exp\\left(\\operatorname{score}\\left(h_{\\boldsymbol{v}}^{(\\ell)},h_{\\boldsymbol{w}}^{(\\ell)}\\right)\\right)}W^{(\\ell)}h_{\\boldsymbol{u}}^{(\\ell)},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where score is a certain learnable Lipschitz function. ", "page_idx": 3}, {"type": "text", "text": "Graph transformers. Beyond traditional MPNNs, graph transformers extend the well-known transformer architecture to the graph domain. The key ingredient in transformers is the self-attention mechanism. Given a featured graph, a single attention head computes a new representation for every (query)node $v$ ,ineverylayer $\\ell>0$ asfollows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname{ATT}(v)=\\sum_{u\\in V_{G}}{\\frac{\\exp\\left(\\operatorname{scale}\\left(h_{v}^{(\\ell)},h_{u}^{(\\ell)}\\right)\\right)}{\\sum_{w\\in V_{G}}\\exp\\left(\\operatorname{scale}\\left(h_{v}^{(\\ell)},h_{w}^{(\\ell)}\\right)\\right)}}W^{(\\ell)}h_{u}^{(\\ell)}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where scale is another learnable Lipschitz function (the scaled dot-product) ", "page_idx": 3}, {"type": "text", "text": "The vanilla transformer architecture ignores the graph structure. Graph transformer architectures [53, 41, 37] address this by explicitly encoding graph inductive biases, most typically in the form of positional encodings (PEs). In their simplest form, these encodings are additional features $\\scriptstyle p_{v}$ for everynode $v$ that encode a node property (e.g., node degree) which are concatenated to the node features $\\boldsymbol{h}_{v}$ . The random walk positional encoding (Rw) [12] of each node $v$ is givenby: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{rw}_{v}=[\\mathbf{rw}_{v,1},\\mathbf{rw}_{v,2},\\ldots\\mathbf{rw}_{v,k}],}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where ${\\bf r w}_{v,i}$ is the probability of an $i$ -length random walk that starts at $v$ to end at $v$ ", "page_idx": 3}, {"type": "text", "text": "The GPS architecture [41] is a representative graph transformer, which applies a parallel computation in every layer: a transformer layer (with or without PEs) and an MPNN layer are applied in parallel and their outputs are summed to yield the node representations. By including a standard MPNN in this way, a GPS layer can take advantage of the graph topology even when there is no positional encoding. In the context of this paper, we write GPS to refer to a GPS architecture that uses an MPNN with mean aggregation, and $\\mathrm{GPS+RW}$ if the architecture additionally uses a random-walk PE. ", "page_idx": 3}, {"type": "text", "text": "Probabilistic classifiers. We are looking at models that produce a vector of reals on each graph. All of these models can be used as probabilistic graph classifiers. We only need to ensure that the final layer is a softmax or sigmoid applied to pooled representations. ", "page_idx": 3}, {"type": "text", "text": "4   Model architectures via term languages ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We demonstrate the robustness and generality of the convergence phenomenon by defining a term language consisting of compositions of operators on graphs. Terms are formal sequences of symbols which when interpreted in a given graph yield a real-valued function on the graph nodes. ", "page_idx": 3}, {"type": "text", "text": "Definition 4.1 (Term language). AGG[WMEAN] is a term language which contains node variables $x,y,z,\\ldots$ and terms defined inductively:2 ", "page_idx": 3}, {"type": "text", "text": "\u00b7 The basic terms are of the form $\\operatorname{H}(x)$ , representing the features of the node $x$ , and constants $^c$ ", "page_idx": 3}, {"type": "text", "text": "\u00b7 Let $h\\colon\\mathbb{R}^{d}\\to(0,\\infty)^{d}$ be a function which is Lipschitz continuous on every compact domain. Given terms $\\tau$ and $\\pi$ , the local $h$ -weighted mean for node variable $x$ is: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\sum_{y\\in\\mathcal{N}(x)}\\tau(y)\\star h(\\pi(y))\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The interpretation of $\\star$ will be defined below. The global $h$ -weighted mean is the term: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\sum_{y}\\tau(y)\\star h(\\pi(y))\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "\u00b7Terms are closed under applying a function symbolfor each Lipschitz continuous $F\\colon\\mathbb{R}^{d\\times k}\\rightarrow\\mathbb{R}^{d}$ for any $k\\in\\mathbb{N}^{+}$ ", "page_idx": 4}, {"type": "text", "text": "The weighted mean operator takes a weighted average of the values returned by $\\tau$ .It uses $\\pi$ to perform a weighting, normalizing the values of $\\pi$ using $h$ to ensure that we are not dividing by zero (see below for the precise definition). ", "page_idx": 4}, {"type": "text", "text": "To avoid notational clutter, we keep the dimension of each term fixed at $d$ It is possible to simulate terms with different dimensions by letting $d$ be the maximum dimension and padding the vectors with zeros, noting that the padding operation is Lipschitz continuous. ", "page_idx": 4}, {"type": "text", "text": "We make the interpretation of the terms precise as follows. See Figure 2 for a graphical example of evaluating a term on a graph. ", "page_idx": 4}, {"type": "text", "text": "Definition 4.2. Let $G=(V_{G},E_{G},H_{G})$ be a featured graph. Let $\\tau$ be a term with free variables $x_{1},\\ldots,x_{k}$ and $\\boldsymbol{\\bar{u}}=(u_{1},\\dots,u_{k})$ a tuple of nodes. The interpretation $[\\![\\tau(\\bar{u})]\\!]_{G}$ of term $\\tau$ graph $G$ for tuple $\\bar{u}$ is defined recursively: ", "page_idx": 4}, {"type": "text", "text": "$[\\![c(\\bar{u})]\\!]_{G}=c$ for any constant $^c$ \u00b7 $\\mathbb{I H}(x_{i})(\\bar{u})\\mathbb{J}_{G}=H_{G}(u_{i})$ for the $i^{\\mathrm{th}}$ node's features. $\\begin{array}{r}{\\|F(\\tau_{1},\\dots,\\tau_{k})(\\bar{u})\\|_{G}=F(\\[\\tau_{1}(\\bar{u})]_{G},\\dots,\\[\\tau_{k}(\\bar{u})]_{G})}\\end{array}$ for any function symbol $F$ ", "page_idx": 4}, {"type": "text", "text": "\u00b7 For any term composed using $\\star$ ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\left\\|\\sum_{y\\in\\mathcal{N}(x_{i})}\\tau\\star h(\\pi)(\\bar{u})\\right\\|_{G}=\\left\\{\\begin{array}{l l}{\\displaystyle\\frac{\\sum_{v\\in\\mathcal{N}(u_{i})}[\\tau(\\bar{u},v)]_{G}h([\\pi(\\bar{u},v)]_{G})}{\\sum_{v\\in\\mathcal{N}(u_{i})}h([\\pi(\\bar{u},v)]_{G})}}&{\\mathrm{if}\\,\\mathcal{N}(u_{i})\\neq\\emptyset;}\\\\ {\\displaystyle\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\mathbf{0}}&{\\mathrm{if}\\,\\mathcal{N}(u_{i})=\\emptyset.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The semantics of global weighted mean is defined analogously and omitted for brevity. ", "page_idx": 4}, {"type": "image", "img_path": "Dn68qdfTry/tmp/21fd92fbedd0616df2c72b9e532a2e3eea951a825a49d768032669eedbdd47d4.jpg", "img_caption": [], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Figure 2: Evaluation of the term $\\begin{array}{r}{\\sum_{x\\in\\mathcal{N}(y)}(2\\mathrm{H}(x)+2)\\star1.0}\\end{array}$ on a small graph with scalar features. The term computes the mean of $z\\mapsto2z+2$ on each of a node's neighbours. As each sub-term has one free variable, we can represent the intermediate results as scalar values for each node. ", "page_idx": 4}, {"type": "text", "text": "A closed term has all node variables bound by a weighted mean operator: so the implicit input is just a featured graph. ", "page_idx": 4}, {"type": "text", "text": "Definition 4.3. We augment the term language to AGG[WMEAN, RW] by adding the random walk operator $\\operatorname{rw}(x)$ . The interpretation of $\\operatorname{rw}(x_{i})$ given a graph $G$ and a tuple of nodes $\\bar{u}$ is: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbb{\\left[\\mathbf{rw}(x_{i})(\\bar{u})\\right]}_{G}=\\left[\\mathbf{rw}_{u_{i},1},\\ldots,\\mathbf{rw}_{u_{i},d}\\right]\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "4.1  How powerful is the term language? ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Various architectures can be described using this term language. The core idea is always the same: we show that all basic building blocks of the architecture can be captured in the term language and applying this inductively yields the desired result. Let us first note that all linear functions and all commonly used activation functions are Lipschitz continuous, and therefore included in the language. ", "page_idx": 5}, {"type": "text", "text": "MPNNs with mean aggregation. Consider an $L$ -layer MPNN with mean aggregation, update functions ${\\mathrm{UPD}}^{(\\ell)}$ consisting of an activation function applied to a linear transformation, with mean pooling at the end. First, note that mean aggregation can be expressed as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathsf{M E A N}_{\\boldsymbol{y}}\\pi(\\boldsymbol{y}):=\\sum_{\\boldsymbol{y}}\\pi(\\boldsymbol{y})\\star1.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "For each layer $0\\leq\\ell<L$ we define a term $\\tau^{(\\ell)}(x)$ which will compute the representation of a node at layer $\\ell$ of the MPNN: ", "page_idx": 5}, {"type": "text", "text": "\u00b7Initialization. Let $\\tau^{(0)}(x):=\\mathrm{H}(x)$ . Then the value $\\mathbb{[}\\tau^{(0)}(x)(u)]\\mathbb{G}$ at a node $u$ is the initial node representation $H(u)$ \u00b7 Layers. For $1\\leq\\ell<L$ define $\\tau^{(\\ell+1)}(x):=\\mathrm{UPD}^{(\\ell)}\\left(x,\\mathrm{MEAN}_{y\\in\\mathcal{N}(x)}\\tau^{(\\ell)}(y)\\right)$ Then the value at node $u$ is the following, which conforms with the inductive construction of the MPNN: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{I}\\tau^{(\\ell+1)}(x)(u)\\mathbb{J}_{G}=\\mathbf{U}\\mathbf{P}\\mathrm{D}^{(\\ell)}\\left(\\mathbb{I}\\tau^{(\\ell)}(x)(u)\\mathbb{J}_{G},\\frac{1}{|N(u)|}\\sum_{v\\in\\mathcal{N}(u)}\\mathbb{I}\\tau^{(\\ell)}(x)(v)\\mathbb{J}_{G}\\right)\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "\u00b7 Final mean pooling. The final graph representation is computed as $\\tau:=\\mathsf{M E A N}_{x}\\tau^{(L)}(x)$ ", "page_idx": 5}, {"type": "text", "text": "The idea is similar for the other architectures, where the difference lies in the aggregation functions.   \nThus below we only present how the term language captures their respective aggregation functions. ", "page_idx": 5}, {"type": "text", "text": "Graph transformers. We can express the self-attention mechanism of transformers using the following aggregator: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname{ATT}_{y}\\pi(y):=\\sum_{y}\\pi(y)\\star\\exp(\\operatorname{scale}(\\pi(x),\\pi(y)))\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Thefunction $\\operatorname{scale}(\\pi(x),\\pi(y))$ is a term in the language, since scaled dot product attention is a Lipschitz function. To see how graph transformer architectures such as GPS can be expressed, it suffices to note that we can express both self-attention layers and MPNN layers with mean aggregation, since the term language is closed under addition. The random walk positional encoding can also be expressed using the rw operator. ", "page_idx": 5}, {"type": "text", "text": "Graph attention networks. The attention mechanism of GAT is local to a node's neighbours and can be expressed in our term language using similar ideas, except using the local aggregate terms. ", "page_idx": 5}, {"type": "text", "text": "Additional architectural features. Because the term language allows the arbitrary combination of graph operations, it can robustly capture many common architectural choices used in graph learning architectures. For example, a skip connection or residual connection from layer $\\ell_{1}$ tolayer $\\ell_{2}$ can be expressed by including a copy of the term for layer $\\ell_{1}$ in the term for the layer $\\ell_{2}$ [51]. Global readout can be captured using a global mean aggregation [5]. Attention conditioned on computed node or node-pair representations can be captured by including the term which computes these representations in the mean weight [37]. ", "page_idx": 5}, {"type": "text", "text": "Capturing probabilistic classification. Our term language defines bounded vector-valued functions over graphs. Standard normalization functions, like softmax and sigmoid, are easily expressible in our term language, so probabilistic classifiers are subsumed. ", "page_idx": 5}, {"type": "text", "text": "Graph convolutional networks. In Appendix A we show how to extend the term language to incorporate graph convolutional networks (GCNs) [29] by adding a new aggregator. ", "page_idx": 5}, {"type": "text", "text": "5 Convergence theorems ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We start by presenting the convergence theorem. ", "page_idx": 5}, {"type": "text", "text": "Theorem 5.1. Consider $(\\mu_{n})_{n\\in\\mathbb{N}}$ samplingagraph $G$ from any of the following models and node features independently from i.i.d. bounded distributions on $d$ features. ", "page_idx": 6}, {"type": "text", "text": "1. The Erdos-Renyi distribution $\\mathrm{ER}(n,p(n))$ where $p$ satisfies any of the following properties. ", "page_idx": 6}, {"type": "text", "text": "\u00b7 Density. p converges to $\\tilde{p}>0$ \u00b7 Root growth. For some $K>0$ and $0<\\beta<1$ we have: $p(n)=K n^{-\\beta}$ \u00b7 Logarithmic growth. For some $K>0$ we have: $\\begin{array}{r}{p(n)=K\\frac{\\log(n)}{n}}\\end{array}$ \u00b7 Sparsity. For some $K>0$ we have: $p(n)=K n^{-1}$ \uff1a ", "page_idx": 6}, {"type": "text", "text": "2. The Barabasi-Albert model $\\operatorname{BA}(n,m)$ for any $m\\geq1$ ", "page_idx": 6}, {"type": "text", "text": "3. The stochastic block model $\\operatorname{SBM}(n_{1}(n),\\ldots,n_{m}(n),P)$ where $n_{1},...,n_{m}\\colon\\ensuremath{\\mathbb{N}}\\to\\ensuremath{\\mathbb{N}}$ are such that $n_{1}(n)+\\cdots+n_{m}(n)=n$ and each $\\frac{n_{i}}{n}$ converges, and $_{P}$ is any symmetric $m\\times m$ edge probability matrix. ", "page_idx": 6}, {"type": "text", "text": "Then every AGG[WMEAN, RW] term converges a.a.s. to a constant with respect to $(\\mu_{n})_{n\\in\\mathbb{N}}$ ", "page_idx": 6}, {"type": "text", "text": "Concretely, this result shows that for any probabilistic classifier, or other real-valued classifier, which can be expressed within the term language, when drawing graphs from any of these distributions, eventually the output of the classifier will be the same regardless of the input graph, asymptotically almost surely. Thus, the only probabilistic classifiers which can be expressed by such models are those which are asymptotically constant. ", "page_idx": 6}, {"type": "text", "text": "Corollary 5.2. For any of the random graph featured models above, for any MeanGNN, GAT, or GPS $+\\,R W,$ thereisa distribution $\\bar{p}$ on the classes such that the class probabilities converge asymptotically almostsurelyto $\\bar{p}$ ", "page_idx": 6}, {"type": "text", "text": "We now discuss briefly how the results are proven. The cases divide into two groups: the denser cases (the first three ER distributions and the SBM) and the sparser cases (the fourth ER distribution and the BA model). Each is proved with a different strategy. ", "page_idx": 6}, {"type": "text", "text": "5.1Overview of the technical constructions for the denser cases ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "While the theorem is about closed terms, naturally we need to prove it inductively on the term language, which requires consideration of terms with free variables. We show that each open term in some sense degenerates to a Lipschitz function almost surely. The only caveat is that we may need to distinguish based on the \u201ctype\u201d of the node - for example, nodes $u_{1},u_{2},u_{3}$ that form a triangle may require a different function from nodes that do not. Formally, for node variables $\\textstyle{\\bar{x}}$ , an $\\textstyle{\\bar{x}}$ graph type is a conjunction of expressions $E(x_{i},x_{j})$ and their negations. The graph type of tuple $\\bar{u}$ in a graph, denoted $\\mathrm{GrTp}(\\bar{u})$ is the set of all edge relations and their negations that hold between elements of $\\bar{u}$ A $(\\bar{x},d)$ feature-type controller is a Lipschitz function taking as input pairs consisting of $d_{\\cdot}$ -dimensional real vector and an $\\textstyle{\\bar{x}}$ graph type. ", "page_idx": 6}, {"type": "text", "text": "The key theorem below shows that to each term $\\pi$ we can associate a feature-type controller $e_{\\pi}$ which captures the asymptotic behaviour of $\\pi$ , in the sense that with high probability, for most of the tuples $\\bar{u}$ the value of $\\dot{\\boldsymbol{e}}_{\\pi}(\\dot{H}_{G}(\\bar{u}),\\mathrm{GrTp}(\\bar{u}))$ is close to $\\left[\\pi(\\bar{u})\\right]$ ", "page_idx": 6}, {"type": "text", "text": "Theorem 5.3 (Aggregate Elimination for Non-Sparse Graphs). For all terms $\\pi({\\bar{x}})$ over featured graphs with $d$ features, there is a $(\\bar{x},d)$ feature-type controller $e_{\\pi}$ such that for every $\\epsilon,\\delta,\\theta>0$ there is $N\\in\\mathbb N$ such that for all $n\\geq N$ withprobabilityat least $1-\\theta$ in the space of graphs of size $n,$ out of all the tuples $\\bar{u}$ at least $1-\\delta$ satisfy that $\\lVert e_{\\pi}(H_{G}(\\bar{u}),\\mathrm{GrTp}(\\bar{u}))-\\bar{[}\\pi(\\bar{u})]\\rVert<\\epsilon.$ ", "page_idx": 6}, {"type": "text", "text": "This can be seen as a kind of \u201calmost sure quantifier elimination\" (thinking of aggregates as quantifiers), in the spirit of Kaila [24], Keisler and Lotfallah [26]. It is proven by induction on term depth, with the log neighbourhood bound playing a critical role in the induction step for weighted mean. ", "page_idx": 6}, {"type": "text", "text": "Theorem 5.3 highlights an advantage of working with a term language having nice closure properties, rather than directly with GNNs: it allows us to use induction on term construction, which may be more natural and more powerful than induction on layers. Theorem 5.3 also gives strong limitations on node and link classification using GNNs: on most nodes in (non-sparse) random graphs, GNNs can only classify based on the features of a node, they cannot make use of any graph structure. ", "page_idx": 6}, {"type": "text", "text": "5.2  Overview of the technical constructions for the sparser cases ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In the sparser cases, the analysis is a bit more involved. Instead of graph types over $\\bar{u}$ whichonly specify graph relations among the $\\bar{u}$ , we require descriptions of local neighbourhoods of $\\bar{u}$ ", "page_idx": 7}, {"type": "text", "text": "Definition 5.4. Let $G$ be a graph, $\\bar{u}$ a tuple of nodes in $G$ and $\\ell\\in\\mathbb{N}$ .The $\\ell$ -neighbourhoodof $\\bar{u}$ in $G$ denoted $\\mathcal{N}_{\\ell}(\\bar{u})$ is the subgraph of $G$ induced by the nodes of distance at most $\\ell$ from some node in $\\bar{u}$ ", "page_idx": 7}, {"type": "text", "text": "The \u201ctypes\"\u201d' are now graphs $T$ with $k$ distinguished elements $\\bar{w}$ , which we call $k$ -rootedgraphs.Two $k$ -rooted graphs $(T,{\\bar{w}})$ and $\\left(U,{\\bar{z}}\\right)$ are isomorphic is there is a structure-preserving bijection $T\\rightarrow U$ which maps $\\bar{w}$ to $\\bar{z}$ . The combinatorial tool here is a fact known in the literature as \u2018weak local convergence': the percentage of local neighbourhoods of any given type converges. ", "page_idx": 7}, {"type": "text", "text": "Lemma 5.5 (Weak local convergence). Consider sampling a graph $G$ from either the sparse ER or BA distributions. Let $(T,{\\bar{w}})$ be a $k$ -rootedgraphandtake $\\ell\\in\\mathbb{N}$ There is $q_{T}\\in[0,1]$ such that for all $\\epsilon,\\theta>0$ there is $N\\in\\mathbb N$ suchthatforall $n\\geq N$ withprobabilityatleast $1-\\theta$ we have that: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\left|\\frac{|\\{\\bar{u}\\mid{\\mathcal{N}}_{\\ell}(\\bar{u})\\cong(T,\\bar{w})\\}|}{n}-q_{T}\\right|<\\epsilon\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Our \u201caggregate elimination\", analogous to Theorem 5.3, states that every term can be approximated by a Lipschitz function of the features and the neighbourhood type. ", "page_idx": 7}, {"type": "text", "text": "Theorem 5.6 (Aggregate Elimination for Sparser Graphs). For every $\\pi({\\bar{x}})$ , letting $\\ell$ be the maximum aggregator nesting depth in $\\pi$ for all $k$ -rooted graphs $(T,{\\bar{w}})$ there is a Lipschitz function $e_{\\pi}^{T}\\colon\\mathbb{R}^{|T|\\cdot d}\\to\\mathbb{R}^{d}$ such that for each $\\epsilon,\\theta>0,$ there is $N\\in\\mathbb{N}$ such that for all $n\\geq N$ with probability at least $1-\\theta$ in the space of graphs of size n, for every $k$ -tuple of nodes $\\bar{u}$ in the graph such that $\\mathcal{N}_{\\ell}(\\bar{u})\\cong(T,\\bar{w})$ we have that $\\left\\|e_{\\pi}^{\\bar{T}}(H_{G}(\\mathcal{N}_{\\ell}(\\bar{u})))-\\left[\\pi(\\bar{u})\\right]\\right\\|<\\epsilon.$ ", "page_idx": 7}, {"type": "text", "text": "Compared to Theorem 5.3 the result is much less limiting in what node-classifying GNNs can express. Although combining the sparse and non-sparse conditions covers many possible growth rates, it is not true that one gets convergence for Erd?s-R\u00e9nyi with arbitrary growth functions: ", "page_idx": 7}, {"type": "text", "text": "Theorem 5.7.There are functions $p(n)$ converging to zero and a term $\\tau$ inourlanguagesuchthat $\\tau$ does not converge even in distribution (and hence does not converge a.a.s.) over ER random graphs withgrowthrate $p$ ", "page_idx": 7}, {"type": "text", "text": "Proof. Let $p$ alternate between $\\frac{1}{2}$ on even $n$ and $\\scriptstyle{\\frac{1}{n}}$ on odd $n$ . Consider $\\tau_{1}(x)$ that returns O if $x$ has a neighbour and 1 otherwise, and let $\\tau$ be the global average of $\\tau_{1}$ . So $\\tau$ is the percentage of isolated nodes in the graph. Then $\\tau$ clearly goes to zero in the non-sparse case. However the probability that a particular node is not isolated in a random sparse graph of size $n$ .is $\\textstyle{\\big(}1-{\\frac{1}{n}}{\\big)}^{n-1}$ , which goes to $\\textstyle{\\frac{1}{e}}<1$ Thus $\\left[\\![\\tau]\\!\\right]\\!$ diverges on $\\mathrm{ER}(n,p(n))$ \u53e3 ", "page_idx": 7}, {"type": "text", "text": "6   Experimental evaluation ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We first empirically verify our findings on random graphs and then on a real-world graph to answer the following questions: Q1. Do we empirically observe convergence? Q2. What is the impact of the different weighted mean aggregations on the convergence? Q3. What is the impact of the graph distribution on the convergence? Q4. Can these phenomena arise within large real-world graphs? ", "page_idx": 7}, {"type": "text", "text": "All our experiments were run on a single NVidia GTX V100 GPU. We made our codebase available online at https: //github.com/benfinkelshtein/GNN-Asymptotically-Constant. ", "page_idx": 7}, {"type": "text", "text": "Setup. We report experiments for the architectures MeanGNN, GAT [49], and $\\mathrm{GPS+RW}$ [41] with random walks of length up to 5. Our setup is carefully designed to eliminate confounding factors: ", "page_idx": 7}, {"type": "text", "text": "\u00b7 We consider five models with the same architecture, each having randomly initialized weights, utilizing a ReLU non-linearity, and applying a softmax function to their outputs. Each model uses a hidden dimension of 128, 3 layers and an output dimension of 5. We experimentwith disributions $\\mathrm{ER}(n,p(n)=0.1)$ $\\begin{array}{r}{\\mathrm{ER}(n,p(n)=\\frac{\\log n}{n})}\\end{array}$ $\\begin{array}{r}{\\mathrm{ER}(n,p(n)=\\frac{1}{50n})}\\end{array}$ $\\mathrm{BA}(n,m=5)$ . We also experiment with an SBM of 10 communities with equal size, where an edge between nodes within the same community is included with probability 0.7 and an edge between nodes of different communities is included with probability 0.1. ", "page_idx": 7}, {"type": "text", "text": "\u00b7 We draw graphs of sizes up to 10,000, where we take 100 samples of each graph size. Node features are independently drawn from $U[0,1]$ and the initial feature dimension is 128. ", "page_idx": 8}, {"type": "text", "text": "To understand the behaviour of the respective models, we will draw larger and larger graphs from the graph distributions. We use five different models to ensure this is not a model-specific behaviour. Further experimental results are reported in Appendix F. ", "page_idx": 8}, {"type": "text", "text": "6.1  Empirical results on random graph models ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In Figure 3, a single model initialization of the MeanGNN, GAT and $\\mathrm{GPS+RW}$ architectures is used With $\\mathrm{ER}(n,p(n)=0.1)$ $\\begin{array}{r}{\\mathrm{ER}(n,p(n)\\,=\\,\\frac{\\log n}{n})}\\end{array}$ $\\begin{array}{r}{\\mathrm{ER}(n,p(n)\\,=\\,\\frac{1}{50n})}\\end{array}$   \ncorresponds to a different class probability, depicting the average of 100 samples for each graph size along with the standard deviation shown in lower opacity. ", "page_idx": 8}, {"type": "text", "text": "The convergence of class probabilities is apparent across all models and graph distributions, as illustrated in Figure 3, in accordance with our main theorems (Q1). The key differences between the plots are the convergence time, the standard deviation and the converged values. ", "page_idx": 8}, {"type": "text", "text": "One striking feature of Figure 3 is that the eventual constant output of each model is the same for the dense and logarithmic growth distributions, but not for the sparse distribution (Q3). We can relate this to the distinct proof strategy employed in the sparse case, which uses convergence of the proportions of local isomorphism types. There are many local isomorphism types, and the experiments show that what we converge to depends the proportion of these. In all other cases the neighbourhood sizes are unbounded, so there is asymptotically almost surely one \u201clocal graph type'. ", "page_idx": 8}, {"type": "text", "text": "We observe that attention-based models such as GAT and $\\mathrm{GPS+RW}$ exhibit delayed convergence and greater standard deviation in comparison to MeanGNN (Q2). A possible explanation is that because some nodes are weighted more than others, the attention aggregation has a higher variance than regular mean aggregation. For instance, if half of the nodes have weights close to O, then the ", "page_idx": 8}, {"type": "image", "img_path": "Dn68qdfTry/tmp/96ca03abf92de0cbd91a3a4381005a81873b97bf401717f030ebcd06c4ebbbc0.jpg", "img_caption": [], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 3: Each plot shows the five mean class probabilities (in different colours) with standard deviations of a single model initialization over $\\mathrm{ER}(n,p(n)\\,=\\,0.1)$ $\\begin{array}{r}{\\mathrm{ER}(n,p(n)\\ =\\ \\frac{\\log n}{n})}\\end{array}$ lo\u53f7), and $\\begin{array}{r}{\\mathrm{ER}(n,p(n)=\\frac{1}{50n})}\\end{array}$ , as we draw graphs of increasing ize. ", "page_idx": 8}, {"type": "image", "img_path": "Dn68qdfTry/tmp/c587af2b355779482d9d684f2e61f0032fcdb710de5da5ec1f3dd5fc1506eaa6.jpg", "img_caption": ["Figure 4: Each plot depicts the standard deviation of Euclidean distances between class probabilities and their respective means across various samples of each graph size for $\\mathrm{GPS+RW}.$ "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "attention aggregation effectively takes a mean over half of the available nodes. Eventually, however, the attention weights themselves converge, and thus convergence cannot be postponed indefinitely. ", "page_idx": 9}, {"type": "text", "text": "Figure 4 depicts the outcomes of the $\\mathrm{GPS+RW}$ architecture for various graph distributions. The analysis involves calculating the Euclidean distance between the class probabilities of the $\\mathrm{GPS+RW}$ architecture and the mean class probabilities over the different graph samples. The standard deviation across the different graph samples is then derived for each of the 5 different model initializations and presented in Figure 4. The decrease of standard deviation across the different model initializations in Figure 4 indicates that all class probabilities converge across the different model initializations, empirically verifying the phenomenon for varying model initializations (Q1 and Q3). ", "page_idx": 9}, {"type": "text", "text": "6.2  Empirical results on large real-world graphs ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Towards Q4, we investigated a large real-world dataset. Many commonly studied graph datasets (e.g. ZINC [13], QM9 [6]) do not exhibit sufficient graph size variance and provide no obvious means to add scaling. We used the TIGER-Alaska dataset [16] of geographic faces. The original dataset has 93366 nodes, while Dimitrov et al. [10] extracted smaller datasets with graphs having 1K, 5K, 10K, 25K and 90K nodes. We chose the modified dataset as it is split by graph size, and consists of graphs differing from our random models (in particular all graphs are planar). Figure 5 shows the results of applying the same five MeanGNNs to graphs of increasing sizes. Strikingly, we again observe a convergence phenomenon, but at a slower pace. ", "page_idx": 9}, {"type": "image", "img_path": "Dn68qdfTry/tmp/5d66f348d9d7218ee549a77ca5dd49f2bd21fe5e611075f0ee4f39a12b55b30c.jpg", "img_caption": ["Figure 5: Standard deviation of distances between class probabilities and their means across TIGER-Alaska graph sizes for MeanGNN. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "7   Discussion and limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We have demonstrated a wide convergence phenomenon for real-valued classifiers expressed even in very advanced GNN architectures, and it applies to a great variety of random graph models. Rather than having separate proof techniques per GNN model, our paper introduces a broad language where such models can be situated, and provides techniques at the level of term languages. Although our top-level theorems deal with graph-level tasks, along the way we provide strong limitative results on what can be achieved on random graphs for node- or edge-level real-valued tasks: see Theorem 5.3. ", "page_idx": 9}, {"type": "text", "text": "The principal limitations of our work come our assumptions. In particular, we assume that the initial node embeddings are i.i.d. This assumption is used in the application concentration inequalities throughout the proofs, so loosening it would require careful consideration. ", "page_idx": 9}, {"type": "text", "text": "Our main results show that many GNN architectures cannot distinguish large graphs. To overcome this limitation, one could consider moving beyond our term language. For example, if we add sum aggregation, the term values clearly diverge, and similarly if we allow non-smooth functions, such as linear inequalities. Further we emphasize that a.a.s. convergence is not universal for our term language, and it does not hold even for ER with arbitrary $p(n)$ going to 0: see Theorem 5.7. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1]  R. Abboud, i. i. Ceylan, M. Grohe, and T. Lukasiewicz. The Surprising Power of Graph Neural Networks with Random Node Initialization. In IJCAI, 2021.   \n[2] S. Adam-Day, T. M. Iiant, and i. i. Ceylan. Zero-one laws of graph neural networks. In NeurIPS, 2023.   \n[3]  A.-L. Barabasi and R. Albert. Emergence of scaling in random networks. Science, 286(5439): 509-512, 1999.   \n[4] P. Barcelo, E. Kostylev, M. Monet, J. Perez, J. Reutter, and J. P. Silva. The Logical Expressiveness of Graph Neural Networks. In ICLR, 2020.   \n[5] P. W. Battaglia, J. B. Hamrick, V. Bapst, A. Sanchez-Gonzalez, V. F. Zambaldi, M. Malinowski, A. Tacchetti, D. Raposo, A. Santoro, R. Faulkner, C. Gulcehre, H. F. Song, A. J. Ballard, J. Gilmer, G. E. Dahl, A. Vaswani, K. R. Allen, C. Nash, V. Langston, C. Dyer, N. Heess, D. Wierstra, P. Kohli, M. Botvinick, O. Vinyals, Y. Li, and R. Pascanu. Relational inductive biases, deep learning, and graph networks. CoRR, abs/1806.01261, 2018.   \n[6]  M. Brockschmidt. GNN-FiLM: Graph neural networks with feature-wise linear modulation. In ICML, 2020.   \n[7]  G. Caldarelli. Scale-free networks complex webs in nature and technology. Oxford University Press, Oxford, 2007.   \n[8]  Z. Chen, L. Chen, S. Villar, and J. Bruna. Can graph neural networks count substructures? In NeurIPS, 2020.   \n[9]  M. Cordonnier, N. Keriven, N. Tremblay, and S. Vaiter. Convergence of message passing graph neural networks with generic agregation on large random graphs. CoRR, 2304.11140, 2023.   \n[10] R. Dimitrov, Z. Zhao, R.Abboud, and ismail Ikan Ceylan. PlanE: representation learning over planar graphs. In NeurIPS, 2023.   \n[11] D. Duvenaud, D. Maclaurin, J. Aguilera-Iparraguirre, R. Gomez-Bombarelli, T. Hirzel, A. Aspuru-Guzik, and R. P. Adams. Convolutional networks on graphs for learning molecular fingerprints. In NeurIPS, 2015.   \n[12] V. P. Dwivedi, A. T. Luu, T. Laurent, Y. Bengio, and X. Bresson. Graph neural networks with learnable structural and positional representations. In ICLR, 2021.   \n[13]  V. P. Dwivedi, C. K. Joshi, A. T. Luu, T. Laurent, Y. Bengio, and X. Bresson. Benchmarking graph neural networks. Journal of Machine Learning Research, 24(1), 2024.   \n[14] P. Erdos and A. Renyi. On Random Graphs I. Publicationes Mathematicae Debrecen, 6: 290-297, 1959.   \n[15] R. Fagin. Probabilities on finite models. Journal of Symbolic Logic, 41(1):50-58, 1976.   \n[16] J. Fuentes and G. Navarro. Tiger-Alaska dataset, 2021. http: //www.inf .udec.cl/ \\~ jfuentess/datasets/graphs. php, accessed 20 May 2024.   \n[17]  . Garavaglia. Preferential attachment models for dynamic networks. Phd thesis 1 (research tu/e / graduation tu/e), Mathematics and Computer Science, Jan. 2019. Proefschrift.   \n[18]  V. K. Garg, S. Jegelka, and T.S. Jaakkola. Generalization and representational limits of graph neural networks. In ICLR, 2020.   \n[19] F. Geerts and J. L. Reutter. Expressiveness and approximation properties of graph neural networks. In ICLR, 2022.   \n[20] J. Gilmer, S. S. Schoenholz, P. E. Riley, O. Vinyals, and G. E. Dahl. Neural message passing for quantum chemistry. In ICML, 2017.   \n[21] M. Gori, G. Monfardini, and F. Scarselli. A new model for learning in graph domains. In IJCNN, 2005.   \n[22] E. Gradel, H. Helal, M. Naaf, and R. Wilke. Zero-one laws and almost sure valuations of frst-order logic in semiring semantics. In LICS, 2022.   \n[23]  M. Grohe. The Descriptive Complexity of Graph Neural Networks. In LICS, 2023.   \n[24]  R. Kaila. On almost sure elimination of numerical quantifiers. Journal of Logic and Computation, 13(2):273-285, 2003.   \n[25] S. M. Kearnes, K. McCloskey, M. Berndl, V. S. Pande, and P. Riley. Molecular graph convolutions: moving beyond fingerprints. Journal of Computer Aided Molecular Design, 30(8): 595-608, 2016.   \n[26]  H. J. Keisler and W. B. Lotfallah. Almost Everywhere Elimination of Probability Quantifiers. Journal of Symbolic Logic, 74(4):1121-42, 2009.   \n[27]  N. Keriven, A. Bietti, and S. Vaiter. Convergence and stability of graph convolutional networks on large random graphs. In NeurIPS, 2020.   \n[28] N. Keriven, A. Bieti, and S. Vaiter. On the universality of graph neural networks on large random graphs. In NeurIPS, 2021.   \n[29]  T. Kipf and M. Welling. Semi-supervised classification with graph convolutional networks. In ICLR, 2017.   \n[30] T. N. Kipf and M. Welling. Semi-supervised classification with graph convolutional networks. In ICLR, 2017.   \n[31] L. A. Larrauri, T. Muller, and M. Noy. Limiting probabilities of frst order properties of random sparse graphs and hypergraphs. Random Structures and Algorithms, 60(3):506-526, 2022.   \n[32] R. Levie. A graphon-signal analysis of graph neural networks. In NeurIPS, 2023.   \n[33] R. Liao, R. Urtasun, and R.S. Zemel. A PAC-Bayesian approach to generalization bounds for graph neural networks. In ICLR, 2021.   \n[34] L. Lovasz. Large Networks and Graph Limits, volume 60 of Colloquium Publications. American Mathematical Society, 2012.   \n[35] J. Lynch. Convergence laws for random words. Australian J. Comb., 7:145-156, 1993.   \n[36] J.F Lynch. Probabilitiesof sentences about very sparse randm graps. Random Structures and Algorithms, 3(1):33-54, 1992.   \n[37] L. Ma, C. Lin, D. Lim, A. Romero-Soriano, P. K. Dokania, M. Coates, P. Torr, and S.-N. Lim. Graph inductive biases in transformers without message pasing. In ICML, 2023.   \n[38] S. Maskey, R Lvie, Y. Lee, and G.Kutynik. Generalization analysis of message passing neural networks on large random graphs. In NeurIPS, 2022.   \n[39] C. Morris, M. Ritzert, M. Fey, W. L. Hamilton, J. E. Lenssen, G. Rattan, and M. Grohe. Weisfeiler and Leman go neural: Higher-order graph neural networks. In AAAI, 2019.   \n[40] C. Morris, F. Geerts, J. Tonshoff, and M. Grohe. WL meet VC. In ICML, 2023.   \n[41] L. Rampasek, M. Galkin, V. P. Dwivedi, A. T. Luu, G. Wolf, and D. Beaini. Recipe for a general, powerful, scalable graph transformer. In NeurIPS, 2022.   \n[42] E. Rosenbluth, J. Tonshoff, and M. Grohe. Some might say all you need is sum. In IJCAI, 2023.   \n[43] R. Sato, M. Yamada, and H. Kashima. Random features strengthen graph neural networks. In SDM, 2021.   \n[44] F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and G. Monfardini. The graph neural network model. IEEE Transactions on Neural Networks, 20(1):61-80, 2009.   \n[45] F. Scarselli, A. C. Tsoi, and M. Hagenbuchner. The Vapnik-Chervonenkis dimension of graph and recursive neural networks. Neural Networks, pages 248-259, 2018.   \n[46] S. Shelah and J. Spencer. Zero-one laws for sparse random graphs. Journal of the American Mathematical Society, 1(1):97-115, 1988.   \n[47] J. Shlomi, P. Battaglia, and J.-R. Vlimant. Graph neural networks in particle physics. Machine Learning: Science and Technology, 2(2):021001, 2021.   \n[48] R. van der Hofstad. Random Graphs and Complex Networks. Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press, 2024.   \n[49] P. Velickovic, G. Cucurull, A. Casanova, A. Romero, P. Lio, and Y. Bengio. Graph attention networks. In ICLR, 2018.   \n[50]  R. Vershynin. High-dimensional probability : an introduction with applications in data science. Cambridge University Press, Cambridge, 2018.   \n[51]  K. Xu, C. Li, Y. Tian, T. Sonobe, K.-i. Kawarabayashi, and S. Jegelka. Representation learning on graphs with jumping knowledge networks. In ICML, 2018.   \n[52] K. Xu, W. Hu, J. Leskovec, and S. Jegelka. How powerful are graph neural networks? In ICLR, 2019.   \n[53] C. Ying, T. Cai, S. Luo, S. Zheng, G. Ke, D. He, Y. Shen, and T. Liu. Do transformers really perform badly for graph representation? In NeurlPS, 2021.   \n[54] B. Zhang, S. Luo, L. Wang, and D. He. Rethinking the Expressive Power of GNNs via Graph Biconnectivity. In ICLR, 2023.   \n[55] M. Zitnik, M. Agrawal, and J. Leskovec. Modeling polypharmacy side effects with graph convolutional networks. Bioinformatics, 34(13):i457-i466, 2018. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Graph Convolutional Networks ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In the body of the paper, we mentioned that our results can be extended to graph convolutional networks (GCNs) [29]. In this section we show how to extend our term language to capture them. Later in the appendix when providing proof details for our convergence laws, we will utilize the extended language, thus showing the applicability to GCNs. ", "page_idx": 13}, {"type": "text", "text": "GCNs are instances of the message passing neural network in which the update function is defined as follows. ", "page_idx": 13}, {"type": "equation", "text": "$$\nh_{v}^{(\\ell+1)}=\\sum_{u\\in\\mathcal{N}(v)}\\frac{1}{\\sqrt{|\\mathcal{N}(v)||\\mathcal{N}(u)|}}W^{(\\ell)}h_{u}^{(\\ell)}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "To allow our term language to capture GCNs, we add the new aggregator: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathrm{GCN}_{y\\in\\mathcal{N}(x)}\\tau(y)\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "The language AGG[WMEAN, GCN, RW] is the result of closing AGG[WMEAN, RW] under this operator.   \nThe semantics are extended as follows. ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\big\\|\\mathrm{GCN}_{y\\in\\mathcal{N}(x_{i})}\\tau(\\bar{u})\\big\\|_{G}=\\sum_{v\\in\\mathcal{N}(u_{i})}\\frac{1}{\\sqrt{|\\mathcal{N}(u_{i})||\\mathcal{N}(v)|}}\\big\\|\\tau(\\bar{u},v)\\big\\|_{G}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "With this operator it is possible to capture GCNs in the same way as MeanGNNs are captured (Section 4). Moreover, the extended term language permits arbitrary combinations of the GCN aggregator with other language features. ", "page_idx": 13}, {"type": "text", "text": "B  Concentration Inequalities ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Throughout the appendix, we make use of a few basic inequalities. ", "page_idx": 13}, {"type": "text", "text": "Theorem B.1 (Markov's Inequality). Let $X$ be a positive random variable with finite mean. Then for any $\\lambda>0$ wehave: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbb{P}(X\\geq\\lambda)\\leq{\\frac{\\mathbb{E}[X]}{\\lambda}}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Proof. See Proposition 1.2.4, p. 8 of [50]. ", "page_idx": 13}, {"type": "text", "text": "Theorem B.2 (Chebyshev's Inequality). Let $X$ be a randomvariablewithfinitemean $\\mu$ andfinite variance $\\sigma^{2}$ .Thenfor any $\\lambda>0$ wehave: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(|X-\\mu|\\geq\\lambda\\right)\\leq{\\frac{\\sigma^{2}}{\\lambda^{2}}}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Proof. See Corollary 1.2.5, p. 8 of [50]. ", "page_idx": 13}, {"type": "text", "text": "Theorem B.3 (Chernoff's Inequality). Let $X_{i}$ for $i\\leq n$ be i.i.d. Bernoulli random variables with parameter $p$ ", "page_idx": 13}, {"type": "text", "text": "(1)For any $\\lambda>p$ we have: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\sum_{i=1}^{n}X_{i}\\geq\\lambda\\right)\\leq e^{-n p}\\left({\\frac{e n p}{\\lambda}}\\right)^{\\lambda}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "(2)For any $\\lambda<p$ we have: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\sum_{i=1}^{n}X_{i}\\leq\\lambda\\right)\\leq e^{-n p}\\left({\\frac{e n p}{\\lambda}}\\right)^{\\lambda}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Proof. See Theorem 2.3.1, p. 17 of [50] and Exercise 2.3.2, p. 18 of [50]. ", "page_idx": 13}, {"type": "text", "text": "Theorem B.4 (Hoeffding's Inequality for bounded random variables). Let $X_{i}$ for $i\\leq n$ be i.i.d. bounded random variables taking values in $[a,b]$ with common mean $\\mu$ Then for any $\\lambda>0$ we have: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\left|\\sum_{i=1}^{n}X_{i}-n\\mu\\right|\\geq\\lambda\\right)\\leq2\\exp\\left(-\\frac{2\\lambda^{2}}{n(b-a)^{2}}\\right)\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Proof. See Theorem 2.2.6, p. 16 of [50]. ", "page_idx": 14}, {"type": "text", "text": "Corollary B.5 (Hoeffding's Inequality for Bernoulli random variables). Let $X_{i}$ for $i\\leq n$ be i.i.d. Bernoullirandomvariableswithparameter $p$ Thenforany $\\lambda>0$ wehave: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\left|\\sum_{i=1}^{n}X_{i}-n p\\right|\\geq\\lambda\\right)\\leq2\\exp\\left(-\\frac{2\\lambda^{2}}{n}\\right)\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "C Proof for Erdos-R\u00e9nyi distributions, the non-sparse cases ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We new begin with the proof of convergence for the first three cases of the Erdos-R\u00e9nyi distribution in Theorem 5.1, which we restate here for convenience: ", "page_idx": 14}, {"type": "text", "text": "Theorem C.1. Consider $(\\mu_{n})_{n\\in\\mathbb{N}}$ sampling a graph $G$ from theErdos-Renyidistribution $\\mathrm{ER}(n,p(n))$ and node features independently from i.i.d. bounded distributions on $d$ features, where $p$ satisfies any of the following properties. ", "page_idx": 14}, {"type": "text", "text": "\u00b7 Density. p converges to $\\tilde{p}>0$ . Root growth. For some $K>0$ and $0<\\beta<1$ we have: $p(n)=K n^{-\\beta}$ \u00b7 Logarithmic growth. For some $K>0$ we have: $\\begin{array}{r}{p(n)=K\\frac{\\log(n)}{n}}\\end{array}$ ", "page_idx": 14}, {"type": "text", "text": "Then for every AGG[WMEAN, RW] term converges a.a.s. with respect to $(\\mu_{n})_{n\\in\\mathbb{N}}$ ", "page_idx": 14}, {"type": "text", "text": "Note that throughout the following we use $u$ and $v$ to denote both nodes and node variables. ", "page_idx": 14}, {"type": "text", "text": "C.1 Combinatorial and growth lemmas ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Remark C.2. At various points in the following we will make statements concerning particular nodes of an Erdos-R\u00e9nyi graph without fixing a graph. For example, we state that for any $u$ we have that its degree $d(u)$ is the sum of $n-1$ Bernoulli random variables. To make sense of such statements we can fix a canonical enumeration of the nodes of every graph and view our node meta-variables as ranging over indices. So we would translate the previous statement as \u201cfor any node index $u$ the degree of the $u^{\\mathrm{th}}$ node in $\\mathrm{ER}(n,p(n))$ is the sum of $n-1$ Bernoulli random variables.?' ", "page_idx": 14}, {"type": "text", "text": "Both the combinatorial and growth lemmas, and the main inductive results themselves require saying that certain events hold with high probability. Inductively, we will need strengthenings of these compared with the statements given in the body. We will need to consider conditional probabilities, where the conditioning is on a conjunction of atomic statements about the nodes. ", "page_idx": 14}, {"type": "text", "text": "Definition C.3. A $\\wedge$ -description $\\phi(\\bar{u})$ is a conjunction of statements about the variables $\\bar{u}$ of the form $u_{i}E u_{j}$ , which expresses that there exists an edge between $u_{i}$ and $u_{j}$ .A $\\wedge$ -description $u_{i_{1}}E u_{j_{1}}\\wedge\\cdot\\cdot\\cdot\\wedge u_{i_{m}}E u_{j_{m}}$ holds if and only if there is an edge between $u_{i_{\\ell}}$ and $u_{j\\ell}$ for each $\\ell$ ", "page_idx": 14}, {"type": "text", "text": "The reason for strengthening the results in this way will become clearer when we are proving the local weighted mean induction step $\\begin{array}{r}{\\sum_{v\\in\\mathcal{N}(u_{i})}\\rho(\\bar{u},\\bar{v})\\star h(\\eta(\\bar{u},v))}\\end{array}$ .There we will need our auxiliary results to hold conditioned on $v E u_{i}$ , along with any additional requirements coming from previous inductionsteps. ", "page_idx": 14}, {"type": "text", "text": "We will first need a few auxiliary lemmas about the behaviour of random graphs in the different ER models. ", "page_idx": 14}, {"type": "text", "text": "We will need to know that in the non-sparse case, neighborhoods are fairly big: ", "page_idx": 14}, {"type": "text", "text": "Lemma C.4 (Log neighbourhood bound). Let $p\\colon\\ensuremath{\\mathbb{N}}\\to[0,1]$ satisfy density, root growth or logarithmic growth. There is $R>0$ such that for all $\\delta,\\theta>0$ there is $N\\in\\mathbb{N}$ such that for all $n\\geq N$ with probability at least $1-\\theta$ when drawing graphs from $\\mathrm{ER}(n,p(n))$ ,we have that the proportion of all nodesu such that $|{\\mathcal{N}}(u)|\\geq R\\log(n)$ is at least $1-\\delta$ ", "page_idx": 14}, {"type": "text", "text": "We prove a stronger result, which allows us finer control over the growth in each of the non-sparse cases. The strengthening will involve the $\\wedge$ -descriptions of Definition C.3. ", "page_idx": 15}, {"type": "text", "text": "Lemma C.5. Take any $\\wedge$ -description $\\phi$ on $k$ variables, and choose $i\\leq k$ We consider $k$ -tuplesof nodes u satisfying $\\phi$ and the degrees of the $i^{t h}$ node $u_{i}$ ", "page_idx": 15}, {"type": "text", "text": "(1) Let $p$ satisfy the density condition, converging to $\\tilde{p}$ Thenfor every $\\epsilon,\\theta>0$ there is $N\\in\\mathbb{N}$ such that for all $n\\geq N$ with probability at least $1-\\theta$ when drawing graphs from $\\mathrm{ER}(n,p(n))$ for all tuples $\\bar{u}$ which satisfy $\\phi$ we have that: ", "page_idx": 15}, {"type": "equation", "text": "$$\nn(\\tilde{p}-\\epsilon)<d(u_{i})<n(\\tilde{p}+\\epsilon)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "(2) Let $p$ satisfy the root growth condition, with $p=K n^{-\\beta}$ Then for every $R_{1}\\in(0,1)$ and $R_{2}>1$ andforevery $\\theta>0$ there is $N\\in\\mathbb{N}$ such that for all $n\\geq N$ ,with probability at least $1-\\theta$ when drawing graphs from $\\operatorname{ER}(n,p(n))$ , for all tuples $\\bar{u}$ which satisfy $\\phi$ we have that: ", "page_idx": 15}, {"type": "equation", "text": "$$\nR_{1}K n^{1-\\beta}<d(u_{i})<R_{2}K n^{1-\\beta}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "(3) Let $p$ satisfy the log growth condition, with $p=K{\\frac{\\log(n)}{n}}$ Then for every $R_{1}\\in(0,1)$ and $R_{2}>1$ and for every $\\delta,\\theta>0$ there is $N\\in\\mathbb{N}$ such that for all $n\\geq N$ with probability at least $1-\\theta$ when drawing graphs from $\\mathrm{ER}(n,p(n))$ for at least $1-\\delta$ of the tuples $\\bar{u}$ which satisfy $\\phi$ we have that: ", "page_idx": 15}, {"type": "equation", "text": "$$\nR_{1}K\\log(n)<d(u_{i})<R_{2}K\\log(n)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Moreover, there is $R_{3}>0$ such that for every $\\theta>0$ there is $N\\in\\mathbb{N}$ such that for all $n\\geq N$ with probability at least $1-\\theta$ when drawing graphs from $\\mathrm{ER}(n,p(n))$ for all tuples $\\bar{u}$ which satisfy $\\phi$ we have that: ", "page_idx": 15}, {"type": "equation", "text": "$$\nd(u_{i})<R_{3}K\\log(n)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Note that in the first two cases, we only have claims about all tuples, while in the third case we make an assertion about most tuples, while also asserting an upper bound on all tuples. The upper bound on all tuples does not subsume the upper bound on most tuples, since the former states the existence Oof an $R_{3}$ , while the latter gives a bound for an arbitrary $R_{2}$ ", "page_idx": 15}, {"type": "text", "text": "We now give the proof of the lemma: ", "page_idx": 15}, {"type": "text", "text": "Proof. ", "page_idx": 15}, {"type": "text", "text": "(1) Take $\\epsilon,\\theta>0$ . First, there is $N_{1}$ such that for all $n\\geq N_{1}$ ", "page_idx": 15}, {"type": "equation", "text": "$$\n|p(n)-\\tilde{p}|<\\epsilon\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Now, any $d(u)$ is the sum of $n-1$ i.i.d. Bernoulli random variables with parameter $p(n)$ : see Remark C.2 for the formalization. Hence by Hoeffding's Inequality (Theorem B.4) and a union bound, there is $N_{2}$ such that for all $n\\geq N_{2}$ , with probability at least $1-\\theta$ ,foreverynode $u$ we have: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\left|{\\frac{d(u)}{n}}-{\\tilde{p}}\\right|<\\epsilon\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Letting $N=\\operatorname*{max}(N_{1},N_{2})$ , the result follows. ", "page_idx": 15}, {"type": "text", "text": "(2)Take $R_{1}\\in(0,1)$ and $R_{2}>1$ and fix $\\theta>0$ . Again, any $d(u)$ is the sum of $n-1$ i.i.d. Bernoulli random variables with parameter $p(n)$ . By Chernoff's Inequality (Theorem B.3), for any node $u$ wehave that: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(d(u)\\leq R_{1}K n^{1-\\beta}\\right)\\leq\\exp\\left(K n^{1-\\beta}\\left(R_{1}-1-R_{1}\\log(R_{1})\\right)\\right)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Since $R_{1}-1-R_{1}\\log(R_{1})<0$ , by a union bound there is $N_{1}$ such that for all $n\\geq N_{1}$ , with probability at least $1-\\theta$ , for every node $u$ we have: ", "page_idx": 15}, {"type": "equation", "text": "$$\nd(u)>R_{1}K n^{1-\\beta}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Similarly there is $N_{2}$ such that for all $n\\geq N_{2}$ ,with probability at least $1-\\theta$ ,foreverynode $u$ wehave: ", "page_idx": 15}, {"type": "equation", "text": "$$\nd(u)<R_{2}K n^{1-\\beta}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Letting $N=\\operatorname*{max}(N_{1},N_{2})$ , the result follows. ", "page_idx": 15}, {"type": "text", "text": "(3) Take $R_{1}\\in(0,1)$ and $R_{2}>1$ and fix $\\delta,\\theta>0$ . By Chernoff's Inequality (Theorem B.3), for any node $u$ we have that: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(d(u)\\leq R_{1}K n^{1-\\beta}\\right)\\leq n^{K\\left(R_{1}-1-R_{1}\\log(R_{1})\\right)}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Since $R_{1}-1-R_{1}\\log(R_{1})<0$ this probability tends to O as $n$ tends to infinity. Hence there is $N_{1}$ such that for all $n\\geq N_{1}$ , for all nodes $u$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(d(u)\\leq R_{1}\\log(n)\\right)<\\theta\\delta\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Let $B_{n}$ be the proportion, our of tuples $\\bar{u}$ which satisfy $\\phi$ , such that: ", "page_idx": 16}, {"type": "equation", "text": "$$\nd(u_{i})\\leq R\\log(n)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "(the \u201cbad\u2019 tuples). Then for $n\\geq N_{1}$ , by linearity of expectation $\\mathbb{E}[B_{n}]\\leq\\theta\\delta$ ", "page_idx": 16}, {"type": "text", "text": "By Markov's Inequality (Theorem B.1) we have that: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(B_{n}\\geq\\delta\\right)\\leq{\\frac{\\theta\\delta}{\\delta}}=\\theta\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "That is, with probability at least $1-\\theta$ we have that the proportionout of tuples $\\bar{u}$ which satisfy $\\phi$ such that: ", "page_idx": 16}, {"type": "equation", "text": "$$\nd(u_{i})>R_{1}\\log(n)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "is at least $1-\\delta$ ", "page_idx": 16}, {"type": "text", "text": "Similarly, there is $N_{2}$ such that for all $n\\geq N_{2}$ , with probability at least $1-\\theta$ we have that the proportion out of tuples $\\bar{u}$ which satisfy $\\phi$ such that: ", "page_idx": 16}, {"type": "equation", "text": "$$\nd(u_{i})<R_{2}\\log(n)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "is at least $1-\\delta$ . Letting $N=\\operatorname*{max}(N_{1},N_{2})$ the first statement follows. ", "page_idx": 16}, {"type": "text", "text": "To show the \u2018moreover part, note that by Chernoff's Inequality (Theorem B.3), for any $R_{3}>0$ and for anynode $u$ wehavethat: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(d(u)\\geq R_{3}K n^{1-\\beta}\\right)\\leq n^{K\\left(R_{3}-1-R_{3}\\log(R_{3})\\right)}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "For $R_{3}$ large enough we have that $K(R_{3}-1-R_{3}\\log(R_{3}))\\leq-2.$ Hence taking a union bound for every $\\theta>0$ there is $N\\in\\mathbb{N}$ such that for all $n\\geq N$ , with probability at least $1-\\theta$ when drawing graphs from $\\mathrm{ER}(n,p(n))$ , for all tuples $\\bar{u}$ which satisfy $\\phi$ we have that: ", "page_idx": 16}, {"type": "equation", "text": "$$\nd(u_{i})<R_{3}K\\log(n)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The basic form of the following \u201cregularity lemma\u201d states that for every $k\\in\\mathbb{N}^{+}$ and $i\\leq k$ , whenever we have a large subset $S$ of the $\\left(k+1\\right)$ -tuples $(\\bar{u},v)$ with each $u_{i}$ adjacent to $v$ , there is a large subset of the $k$ -tuples $\\bar{u}$ such that each $u_{i}$ is adjacent to a large set of $v$ 's such that $(\\bar{u},v)\\in\\bar{S}$ The full regularity lemma states that this is the case also when we restrict the tuples $\\bar{u}$ to a certain $\\wedge$ -description. ", "page_idx": 16}, {"type": "text", "text": "Lemma C.6 (Regularity Lemma). Let $p\\colon\\ensuremath{\\mathbb{N}}\\to[0,1]$ satisfy one of theconditions other than sparsity. Take a $\\wedge$ -description $\\phi$ on $k$ variables and fix $i\\leq k$ . Then for every $\\gamma,\\theta>0$ there is $N\\in\\mathbb{N}$ and $\\delta^{\\prime}>0$ such that for all $n\\geq N$ and $\\delta<\\delta^{\\prime}$ with probability at least $1-\\theta$ we have that, whenever $S\\subseteq\\{(\\bar{u},v)\\mid\\phi(\\bar{u})\\wedge u_{i}E v\\}$ is such that: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{|S|}{|\\{(\\bar{u},v)\\mid\\phi(\\bar{u})\\wedge u_{i}E v\\}|}\\geq1-\\delta\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "then there is $S^{\\prime}\\subseteq\\{\\bar{u}\\mid\\phi(\\bar{u})$ and $d(u_{i})>0\\}$ such that: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{|S^{\\prime}|}{|\\{\\bar{u}\\mid\\phi(\\bar{u})\\,a n d\\,d(u_{i})>0\\}|}\\geq1-\\gamma\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and for every ${\\bar{u}}\\in S^{\\prime}$ we have: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{\\left|\\{v\\mid(\\bar{u},v)\\in S\\}\\right|}{d(u_{i})}\\geq1-(\\delta+\\delta^{2})\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "For this we make use of the following purely combinatorial lemma. ", "page_idx": 17}, {"type": "text", "text": "Lemma C.7. Take $\\delta,\\sigma>0$ Let $(B_{1},\\hdots,B_{m})$ be a sequence of disjoint finite sets of minimum size a and maximum size $b$ Take $\\textstyle S\\subseteq\\bigcup_{i=1}^{m}B_{i}$ suchthat: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\frac{|S|}{\\sum_{i=1}^{m}\\lvert B_{i}\\rvert}\\geq1-\\delta\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Let: ", "page_idx": 17}, {"type": "equation", "text": "$$\nS^{\\prime}:=\\left\\{i\\left|\\begin{array}{l}{\\left|S\\cap B_{i}\\right|}\\\\ {\\left|B_{i}\\right|}\\end{array}\\right.\\geq1-\\sigma\\right\\}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Then: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\frac{|S^{\\prime}|}{m}\\geq1-\\frac{(\\sigma-\\delta)a}{(\\sigma-\\delta)a+\\delta b}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof. Let $\\bar{B}:=(B_{1},\\ldots,B_{m})$ . We look for an upper bound on: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\gamma_{\\bar{B},S}:=1-\\frac{\\vert S^{\\prime}\\vert}{m}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "To do this, take any $({\\bar{B}},S)$ with proportion at least $1-\\delta$ which minimises $\\gamma_{\\bar{B},S}$ . By performing a series of transformations, we show that we can assume that $({\\bar{B}},S)$ is of a particular form. ", "page_idx": 17}, {"type": "text", "text": "\u00b7 First, ensuring that $S\\cap B_{i}=B_{i}$ for each $i\\in S^{\\prime}$ does not increase $\\gamma_{\\bar{B},S}$ and does not decrease the proporion $\\frac{\\left|S\\right|}{\\sum_{i=1}^{m}\\left|B_{i}\\right|}$ ", "page_idx": 17}, {"type": "text", "text": "\u00b7 We can also make sure that $|B_{i}|=b$ for each $i\\in S^{\\prime}$ ", "page_idx": 17}, {"type": "text", "text": "\u00b7 Similarly, we can ensure that: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\frac{|S\\cap B_{i}|}{|B_{i}|}=\\lfloor1-\\sigma\\rfloor\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "for each $i\\notin S^{\\prime}$ ", "page_idx": 17}, {"type": "text", "text": "\u00b7 Finally, we can make sure that $|B_{i}|=a$ for each $i\\notin S^{\\prime}$ ", "page_idx": 17}, {"type": "text", "text": "Now, using this nice form for $({\\bar{B}},S)$ , we can compute: ", "page_idx": 17}, {"type": "equation", "text": "$$\n1-\\delta\\le\\frac{|S|}{\\sum_{i=1}^{m}\\lvert B_{i}\\rvert}\\le\\frac{\\gamma_{\\bar{B},S}b+(1-\\gamma_{\\bar{B},S})(1-\\sigma)a}{\\gamma_{\\bar{B},S}b+(1-\\gamma_{\\bar{B},S})a}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Rearranging we get: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\frac{|S^{\\prime}|}{m}=1-\\gamma_{\\bar{B},S}\\geq1-\\frac{(\\sigma-\\delta)a}{(\\sigma-\\delta)a+\\delta b}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "as required. ", "page_idx": 17}, {"type": "text", "text": "Proof of Lemma C.6. We proceed differently depending on which of the non-sparse conditions are satisfied. ", "page_idx": 17}, {"type": "text", "text": "We begin with the Density condition. Say $p$ converges to some $\\tilde{p}>0$ .Fix $\\gamma,\\theta>0$ Nowchoose $\\epsilon,\\delta^{\\prime}>0$ small enough. We will decide how small later, but we need at least $\\epsilon<\\tilde{p}$ ", "page_idx": 17}, {"type": "text", "text": "By Lemma C.5 (1) there is $N$ such that for all $n\\geq N$ , with probability at least $1-\\theta$ , for all tuples $\\bar{u}$ which satisfy $\\phi(\\bar{u})$ we have that: ", "page_idx": 17}, {"type": "equation", "text": "$$\nn(\\tilde{p}-\\epsilon)<d(u_{i})<n(\\tilde{p}+\\epsilon)\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Condition on the event that this holds. ", "page_idx": 17}, {"type": "text", "text": "Take any $\\delta<\\delta^{\\prime}$ and $S\\subseteq\\{(\\bar{u},v)\\mid\\phi(\\bar{u})\\wedge u_{i}E v\\}$ such that: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\frac{|S|}{|\\{(\\bar{u},v)\\mid\\phi(\\bar{u})\\wedge u_{i}E v\\}|}\\geq1-\\delta\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Let: ", "page_idx": 18}, {"type": "equation", "text": "$$\nS^{\\prime}:=\\left\\{\\bar{u}\\,\\,\\Big|\\,\\,\\phi(\\bar{u})\\mathrm{~and~}\\frac{|\\{v\\mid(\\bar{u},v)\\in S\\}|}{d(u_{i})}\\geq1-(\\delta+\\delta^{2})\\right\\}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Applying Lemma C.7 with $\\sigma=\\delta+\\delta^{2}$ \uff0c $a=n(\\tilde{p}-\\epsilon)$ and $n=n(\\tilde{p}+\\epsilon)$ , we get that: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{|S^{\\prime}|}{|\\{\\bar{u}~|~\\phi(\\bar{u})~a n d~}d(u_{i})>0\\}|\\geq1-\\frac{(\\delta+\\delta^{2}-\\delta)n(\\tilde{p}-\\epsilon)}{(\\delta+\\delta^{2}-\\delta)n(\\tilde{p}-\\epsilon)+\\delta n(\\tilde{p}+\\epsilon)}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=1-\\frac{\\delta(\\tilde{p}-\\epsilon)}{\\delta(\\tilde{p}-\\epsilon)+(\\tilde{p}+\\epsilon)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "By making $\\epsilon$ and $\\delta^{\\prime}$ small enough, we can make this greater than $1-\\gamma$ . This completes the proof for the dense case. ", "page_idx": 18}, {"type": "text", "text": "We now give the proof for the case of Root growth. Let $p(n)=K n^{-\\beta}$ .Fix $\\mathfrak{c}\\,\\gamma,\\theta>0$ Choose $\\delta^{\\prime}>0$ small enough. Also choose $R_{1}\\in(0,1)$ and $R_{2}>1$ ", "page_idx": 18}, {"type": "text", "text": "By Lemma C.5 (2) there is $N\\in\\mathbb{N}$ such that for all $n\\geq N$ , with probability at least $1-\\theta$ , for all tuples $\\bar{u}$ which satisfy $\\phi(\\bar{u})$ we have that: ", "page_idx": 18}, {"type": "equation", "text": "$$\nR_{1}K n^{1-\\beta}<d(u_{i})<R_{2}K n^{1-\\beta}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proceeding analogously to the dense case, we have that for all $\\delta<\\delta^{\\prime}$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\frac{|S^{\\prime}|}{|\\{\\bar{u}\\mid\\phi(\\bar{u})\\mathrm{~and~}d(u_{i})>0\\}|}\\geq1-\\frac{\\delta R_{1}K n^{1-\\beta}}{\\delta n R_{1}K n^{1-\\beta}+R_{2}K n^{1-\\beta}}}}\\\\ &{=1-\\frac{\\delta R_{1}}{\\delta R_{1}+R_{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "By making $\\delta^{\\prime}$ and $R_{2}-R_{1}$ small enough, we can make this greater than $1-\\gamma$ . This completes the proof for the root growth case. ", "page_idx": 18}, {"type": "text", "text": "Finally, we give the argument for the case of Logarithmic growth. Let $\\begin{array}{r}{p(n)\\,=\\,K\\frac{\\log(n)}{n}}\\end{array}$ .Fix $\\gamma,\\theta>0$ .Choose $\\zeta,\\delta^{\\prime}>0$ small enough. Also choose $R_{1}\\in(0,1)$ and $R_{2}>1$ ", "page_idx": 18}, {"type": "text", "text": "By Lemma C.5 (3), there is $R_{3}>0$ and $N\\in\\mathbb{N}$ such that for all $n\\geq N$ ,withprobability at least $1-\\theta$ when drawing graphs from $\\mathrm{ER}(n,p(n))$ , for at least $1-\\delta$ of the tuples $\\bar{u}$ which satisfy $\\phi(\\bar{u})$ we have that: ", "page_idx": 18}, {"type": "equation", "text": "$$\nR_{1}K\\log(n)<d(u_{i})<R_{2}K\\log(n)\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and moreover for all tuples $\\bar{u}$ which satisfy $\\phi(\\bar{u})$ we have that: ", "page_idx": 18}, {"type": "equation", "text": "$$\nd(u_{i})<R_{3}K\\log(n)\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Now, let: ", "page_idx": 18}, {"type": "equation", "text": "$$\nQ:=\\{\\bar{u}\\mid R_{1}K\\log(n)<d(u_{i})<R_{2}K\\log(n)\\}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "For $n$ large enough we have both: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{|Q|}{|\\{\\bar{u}\\mid\\phi(\\bar{u})\\mathrm{~and~}d(u_{i})>0\\}|}\\geq1-\\zeta\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and (using the fact that outside of $Q$ all nodes $u_{i}$ have degree at most $R_{3}K\\log(n))$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{|\\{(\\bar{u},v)\\mid\\phi(\\bar{u})\\wedge u_{i}E v\\mathrm{~and~}\\bar{u}\\in Q\\}|}{|\\{(\\bar{u},v)\\mid\\phi(\\bar{u})\\wedge u_{i}E v\\}|}\\geq1-\\zeta\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Since we have control over $\\zeta$ , it suffices to restrict attention to: ", "page_idx": 18}, {"type": "equation", "text": "$$\nS\\subseteq\\{(\\bar{u},v)\\mid\\phi(\\bar{u})\\wedge u_{i}E v\\ \\mathrm{and}\\ \\bar{u}\\in Q\\}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Then, analogously to the dense case, we have that for the worst case, for every $\\delta<\\delta^{\\prime}$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{|S^{\\prime}|}{|\\{\\bar{u}\\mid\\phi(\\bar{u})\\mathrm{~and~}d(u_{i})>0\\}|}\\geq1-\\frac{\\delta R_{1}}{\\delta R_{1}+R_{2}}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "By making $\\delta^{\\prime}$ and $R_{2}-R_{1}$ small enough, we can make this greater than $1-\\gamma$ ", "page_idx": 18}, {"type": "text", "text": "This completes the proof of Lemma C.6 for the logarithmic growth case. ", "page_idx": 18}, {"type": "text", "text": "The following lemma will be needed only in the analysis of random walk embeddings. It gives the asymptotic behaviour of the random walk embeddings in the non-sparse cases. We see that the embeddings are almost surely zero. ", "page_idx": 19}, {"type": "text", "text": "Lemma C.8. Let $p\\colon\\mathbb{N}\\rightarrow\\mathbb{N}$ satisfy density, root growth or logarithmic growth. Then for every $k\\in\\mathbb N$ and every $\\epsilon,\\delta,\\theta>0$ there is $N\\in\\mathbb{N}$ such that for all $n\\geq N$ with probability at least $1-\\theta$ theproportion of nodes $v$ such that: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\|r w_{k}(v)\\|<\\epsilon\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "is at least $1-\\delta$ ", "page_idx": 19}, {"type": "text", "text": "Proof. We start with the Dense case. Let $p$ converge to $\\tilde{p}>0$ . Take $\\epsilon,\\delta,\\theta>0$ .By Hoeffding's Inequality (Theorem B.4) and taking a union bound, there is $N$ such that for all $n~\\geq~N$ with probability at least $1-\\theta$ we have that: ", "page_idx": 19}, {"type": "equation", "text": "$$\n|d(v)-\\tilde{p}n|<\\epsilon n\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Condition on this event. ", "page_idx": 19}, {"type": "text", "text": "For any node $v$ the number of length- $k$ walks from $v$ is at least: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\left((\\tilde{p}-\\epsilon)n\\right)^{k}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "By removing the last node of the walk, the number of length- $k$ walksfrom $v$ to itself is at most the number of length- $(k-1)$ walks from $v$ ", "page_idx": 19}, {"type": "text", "text": "The number of length- $(k-1)$ walks from $v$ is at most: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\left(\\left(\\tilde{p}+\\epsilon\\right)n\\right)^{k-1}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Thus the proportion of length- $k$ walks from $v$ which return to $v$ is at most: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{\\left((\\tilde{p}+\\epsilon)n\\right)^{k-1}}{\\left((\\tilde{p}-\\epsilon)n\\right)^{k}}=\\frac{(\\tilde{p}+\\epsilon)^{k-1}}{(\\tilde{p}-\\epsilon)^{k}}n^{-1}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "This tends to O an $n$ tends to infinity. ", "page_idx": 19}, {"type": "text", "text": "We now argue this for the Root growth case. Let $p=K n^{-\\beta}$ ", "page_idx": 19}, {"type": "text", "text": "As in the proof of Lemma C.6, by Chernoff's Inequality (Theorem B.3) and taking a union bound, there are $0<R_{1}<1<R_{2}$ and $N$ such that for all $n\\geq N$ with probability at least $1-\\theta$ we have that for all $v$ ", "page_idx": 19}, {"type": "equation", "text": "$$\nR_{1}K n^{1-\\beta}<d(v)<R_{2}K n^{1-\\beta}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Then, as in the dense case, the proportion of length- $k$ walks from $v$ which return to $v$ is at most: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{\\left(R_{2}K n^{1-\\beta}\\right)^{k-1}}{\\left(R_{1}K n^{1-\\beta}\\right)^{k}}=\\frac{R_{2}^{k-1}}{R_{1}^{k}K}n^{\\beta-1}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "which tends to O as $n$ tends to infinity. ", "page_idx": 19}, {"type": "text", "text": "Finally, we argue this for the Logarithmic growth case. Let $p=K\\log(n)$ .Take $\\epsilon,\\delta,\\theta>0$ ", "page_idx": 19}, {"type": "text", "text": "As in the proof of Lemma C.6, by Chernoff's Inequality (Theorem B.3), there are $0<R_{1}<1<R_{2}$ and $N$ such that for all $n\\geq N_{1}$ with probability at least $1-\\theta$ we have that for at least $1-\\delta$ proportion of nodes $v$ ", "page_idx": 19}, {"type": "equation", "text": "$$\nR_{1}K\\log(n)<d(v)<R_{2}K\\log(n)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Moreover, by Chernoff's Inequality and a union bound, there is $R_{3}>0$ such that for all $n\\geq N_{2}$ with probability at least $1-\\theta$ we have that for all $v$ ", "page_idx": 19}, {"type": "equation", "text": "$$\nd(v)<R_{3}K\\log(n)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Let $N:=\\operatorname*{max}(N_{1},N_{2})$ .Take $n\\geq N$ . We will condition on the event that the above inequalities hold. ", "page_idx": 19}, {"type": "text", "text": "Take any node $v$ such that equation $(\\star)$ holds. Then, the number of length- $k$ walks from $v$ is at least: ", "page_idx": 20}, {"type": "equation", "text": "$$\nR_{1}K\\log(n)((1-\\delta)(R_{1}K\\log(n)))^{k-1}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "The number of length- $k$ walks from $v$ to itself is at most: ", "page_idx": 20}, {"type": "equation", "text": "$$\n(R_{3}K\\log(n))^{k-1}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Thus the proportion of length- $k$ walks from $v$ which return to $v$ is at most: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\frac{(R_{3}K\\log(n))^{k-1}}{R_{1}K\\log(n)((1-\\delta)(R_{1}K\\log(n)))^{k-1}}=\\frac{R_{3}^{k-1}}{R_{1}^{k}(1-\\delta)^{k-1}}\\cdot\\frac{1}{\\log(n)}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "This tends to O as $n$ tends to infinity. ", "page_idx": 20}, {"type": "text", "text": "Finally, we need a simple lemma about division, which has a straightforward proof: ", "page_idx": 20}, {"type": "text", "text": "Lemma C.9. Let $x,y,z,w\\,\\in\\,\\mathbb{R}$ and $\\zeta,\\xi,\\nu,\\Omega\\;>\\;0$ with $\\nu>\\xi$ be such that $|x-y|\\,<\\,\\zeta$ and $|z-w|<\\xi$ while $|x|,|z|<\\Omega$ and $z>\\nu$ . Then: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\left|\\frac{x}{z}-\\frac{y}{w}\\right|<\\frac{\\Omega(\\zeta+\\xi)}{\\nu(\\nu-\\xi)}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "C.2  Proving the inductive invariant for the non-sparse cases, and proving the convergence theorem for these cases ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "With the growth lemmas for the non-sparse cases in hand, we return to presenting the main proof of Theorem C.1, the convergence theorem for non-sparse ER distributions. ", "page_idx": 20}, {"type": "text", "text": "Throughout the following subsection, for notational convenience we allow empty tuples. ", "page_idx": 20}, {"type": "text", "text": "For a tuple $\\bar{u}$ in a graph let $\\mathrm{GrTp}(\\bar{u})$ be the graph type of $\\bar{u}$ . For $k\\in\\mathbb{N}$ let $\\mathsf{G r T}\\mathsf{p}_{k}$ be the set of such types with $k$ free variables. For any $t\\in{\\mathsf{G r T}}{\\mathsf{p}}_{k}$ let: ", "page_idx": 20}, {"type": "equation", "text": "$$\n{\\mathsf{E x t}}(t):=\\{t^{\\prime}({\\bar{u}},v)\\in{\\mathsf{G r T p}}_{k+1}\\mid t^{\\prime}({\\bar{u}},v)\\models t({\\bar{u}})\\}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "For any $u_{i}$ free in $t$ let: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathsf{E x t}_{u_{i}}(t):=\\{t^{\\prime}(\\bar{u},v)\\in\\mathsf{G r T p}_{k+1}\\mid t^{\\prime}(\\bar{u},v)\\models t(\\bar{u})\\land u_{i}E v\\}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We now are ready to begin the proof of Theorem 5.1 for the non-sparse cases. This will involve defining controllers and proving that they satisfy the inductive invariant. ", "page_idx": 20}, {"type": "text", "text": "Let $\\mathbb{F}$ be the probability distribution of the node features, which, for the sake of convenience, we assume to have domain $\\dot{[0,1]}^{d}$ . The domain $[0,1]^{d}$ can be replaced by the more general domain $[a,b]^{d}$ in the results and proofs without further modification. But we note that the domain $[0,1]^{d}$ is already sufficient for the results to hold in the general case. Indeed, suppose $\\tau({\\bar{x}})$ is a term which we apply to a graph distribution $\\mathcal{D}$ with features in $[a,b]^{d}$ . Modify this distribution to $\\mathcal{D}^{\\prime}$ by applying the function $\\bar{z}\\mapsto(\\bar{z}-a)/(b-a)$ to the features. This is now a distribution with features in $[0,1]^{d}$ . Modify $\\tau$ to $\\tau^{\\prime}$ by replacing each $\\mathrm{H}(x)$ by $F(\\mathrm{H}(x))$ , where $F$ is the function $\\bar{z}\\mapsto(b-a)\\bar{z}+a$ . Then evaluating $\\tau^{\\prime}$ on $\\mathcal{D}^{\\prime}$ is equivalent to evaluating $\\tau$ on $\\mathcal{D}$ ", "page_idx": 20}, {"type": "text", "text": "Note that in each case the probability function $p(n)$ converges to some $\\tilde{p}$ (which is 0 in the root growth and logarithmic growth cases). ", "page_idx": 20}, {"type": "text", "text": "Recall from the body of the paper that a $(\\bar{u},d)$ feature-type controller is a Lipschitz function taking as input pairs consisting of a $d$ -dimensional real vector and a $\\bar{u}$ graph type. Recall from Theorem 5.3 that for each term $\\pi$ , we need to define a controller $e_{\\pi}$ that approximates it. We first give the construction of $e_{\\pi}$ and then recall and verify what it means for $e_{\\pi}$ to approximate $\\pi$ ", "page_idx": 20}, {"type": "text", "text": "Take $\\bar{\\pmb{a}}\\in([0,1]^{d})^{k}$ and $t\\in{\\mathsf{G r T}}{\\mathsf{p}}_{k}$ ,where $k$ is the number of free variables in $\\pi$ ", "page_idx": 20}, {"type": "text", "text": "When $\\pi=\\mathrm{H}(u)$ define: ", "page_idx": 20}, {"type": "equation", "text": "$$\ne_{\\pi}(\\bar{\\pmb{a}},t):=\\bar{\\pmb{a}}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "When $\\pi=c$ , a constant, define: ", "page_idx": 21}, {"type": "equation", "text": "$$\ne_{\\pi}(\\bar{a},t):=c\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "When $\\pi=\\operatorname{rw}(u)$ define: ", "page_idx": 21}, {"type": "equation", "text": "$$\ne_{\\pi}({\\bar{a}},t):={\\bf0}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "When $\\pi=f(\\rho_{1},...,\\rho_{r})$ define: ", "page_idx": 21}, {"type": "equation", "text": "$$\ne_{\\pi}(\\bar{\\pmb{a}},t):=f(e_{\\rho_{1}}(\\bar{\\pmb{a}},t),\\dots,e_{\\rho_{r}}(\\bar{\\pmb{a}},t))\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "We start with the construction for the global weighted mean. For any $t(\\bar{u})\\in\\mathsf{G r T}_{\\mathsf{P}_{k}}$ and $t^{\\prime}(\\bar{u},v)\\in$ $\\mathsf{E x t}(t)$ , define $\\alpha(t,t^{\\prime})$ as follows. As an extension type, $t^{\\prime}$ specifies some number, say $r$ , of edges between the nodes $\\bar{u}$ and the new node $v$ . Define: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\alpha(t,t^{\\prime}):=\\tilde{p}^{r}(1-\\tilde{p})^{k-r}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Note that in both the root growth and logarithmic growth cases, $\\alpha(t,t^{\\prime})$ is non-zero precisely when $t^{\\prime}$ specifies no edges between $\\bar{u}$ and $v$ . These relative atomic type weights will play a key role in the construction. ", "page_idx": 21}, {"type": "text", "text": "Now consider a term $\\begin{array}{r}{\\pi=\\sum_{v}\\rho(\\bar{u},v)\\star h(\\eta(\\bar{u},v))}\\end{array}$ ", "page_idx": 21}, {"type": "text", "text": "Recall that the semantics are: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\frac{\\sum_{v\\in G}[\\rho(\\bar{u},v)]_{G}h([\\eta(\\bar{u},v)]_{G})}{\\sum_{v\\in G}h([\\eta(\\bar{u},v)]_{G})}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Define: ", "page_idx": 21}, {"type": "equation", "text": "$$\ne_{\\pi}(\\bar{\\pmb{a}},t):=\\frac{f_{\\pi}(\\bar{\\pmb{a}},t)}{g_{\\pi}(\\bar{\\pmb{a}},t)}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where: ", "page_idx": 21}, {"type": "equation", "text": "$$\nf_{\\pi}(\\bar{\\pmb{a}},t):=\\sum_{t^{\\prime}\\in\\mathsf{E x t}(t)}\\alpha(t,t^{\\prime})\\sum_{\\pmb{b}\\sim\\mathbb{F}}\\left[e_{\\rho}(\\bar{\\pmb{a}},\\pmb{b},t^{\\prime})h\\big(e_{\\eta}(\\bar{\\pmb{a}},\\pmb{b},t^{\\prime})\\big)\\right]\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and: ", "page_idx": 21}, {"type": "equation", "text": "$$\ng_{\\pi}(\\bar{\\pmb{a}},t):=\\sum_{t^{\\prime}\\in\\mathsf{E x t}(t)}\\alpha(t,t^{\\prime})\\sum_{\\pmb{b}\\sim\\mathbb{F}}\\left[h(e_{\\eta}(\\bar{\\pmb{a}},\\pmb{b},t^{\\prime}))\\right]\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Note that when $\\tilde{p}=0$ (as in the root growth and logarithmic growth cases), we have that: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\alpha(t,t^{\\prime})={\\left\\{\\begin{array}{l l}{1}&{{\\mathrm{if~}}t^{\\prime}{\\mathrm{~specifies~no~edges~between~}}{\\bar{u}}{\\mathrm{~and~}}v}\\\\ {0}&{{\\mathrm{otherwise}}}\\end{array}\\right.}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Therefore the controller becomes, letting $t^{\\mathcal{B}}$ be the type with no edges between $\\bar{u}$ and $v$ ", "page_idx": 21}, {"type": "equation", "text": "$$\ne_{\\pi}(\\bar{\\pmb{a}},t)=\\frac{\\mathbb{E}_{\\pmb{b}\\sim\\mathbb{F}}\\left[e_{\\rho}(\\bar{\\pmb{a}},\\pmb{b},t^{\\mathcal{O}})h(e_{\\eta}(\\bar{\\pmb{a}},\\pmb{b},t^{\\mathcal{O}}))\\right]}{\\mathbb{E}_{\\pmb{b}\\sim\\mathbb{F}}\\left[h(e_{\\eta}(\\bar{\\pmb{a}},\\pmb{b},t^{\\mathcal{O}}))\\right]}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "For the local weighted mean case, given $t(\\bar{u})\\in\\mathsf{G r T}\\mathsf{p}_{k}$ and $t^{\\prime}(\\bar{u},v)\\in\\mathsf{E x t}_{u_{i}}(t)$ , we define $\\alpha_{u_{i}}(t,t^{\\prime})$ as follows. Let $r$ be the number of edges specified by $t^{\\prime}$ between ${\\bar{u}}\\setminus u_{i}$ and $v$ . Define: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\alpha_{u_{i}}(t,t^{\\prime}):=\\tilde{p}^{r}(1-\\tilde{p})^{k-r-1}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Consider $\\begin{array}{r}{\\pi=\\sum_{v\\in N(u_{i})}\\rho(\\bar{u},v)\\star h(\\eta(\\bar{u},v))}\\end{array}$ Define: ", "page_idx": 21}, {"type": "equation", "text": "$$\ne_{\\pi}(\\bar{\\pmb{a}},t):=\\frac{f_{\\pi}(\\bar{\\pmb{a}},t)}{g_{\\pi}(\\bar{\\pmb{a}},t)}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where: ", "page_idx": 21}, {"type": "equation", "text": "$$\nf_{\\pi}(\\bar{\\pmb{a}},t):=\\sum_{t^{\\prime}\\in\\mathsf{E x t}_{u_{i}}(t)}\\alpha_{u_{i}}(t,t^{\\prime})\\sum_{b\\sim\\mathbb{F}}[e_{\\rho}(\\bar{\\pmb{a}},\\pmb{b},t^{\\prime})h(e_{\\eta}(\\bar{\\pmb{a}},\\pmb{b},t^{\\prime}))]\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and: ", "page_idx": 21}, {"type": "equation", "text": "$$\ng_{\\pi}(\\bar{\\pmb{a}},t):=\\sum_{t^{\\prime}\\in\\mathsf{E x t}_{u_{i}}(t)}\\alpha_{u_{i}}(t,t^{\\prime})\\sum_{\\pmb{b}\\sim\\mathbb{F}}\\left[h\\!\\left(e_{\\eta}(\\bar{\\pmb{a}},\\pmb{b},t^{\\prime})\\right)\\right]\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "With the controllers $e_{\\pi}$ defined, we now prove that they satisfy Theorem (Theorem 5.3). We state the strengthened version of this result here using the notation of $\\wedge$ -decriptions from Definition C.3: ", "page_idx": 21}, {"type": "text", "text": "Lemma C.10. For every $\\epsilon,\\delta,\\theta>0$ and $\\wedge$ -description $\\psi(\\bar{u})$ , there is $N\\in\\mathbb{N}$ such that for all $n\\geq N$ \uff0c with probability at least $1-\\theta$ in the space of graphs of size $n$ , out of all the tuples $\\bar{u}$ such that $\\psi(\\bar{u})$ at least $1-\\delta$ satisfy: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\lVert e_{\\pi}(\\mathrm{H}(\\bar{u}),\\mathrm{GrTp}(\\bar{u}))-[\\pi(\\bar{u})]\\rVert<\\epsilon\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof. Naturally, we prove the lemma by induction. ", "page_idx": 22}, {"type": "text", "text": "\u00b7 Base cases $\\mathrm{H}(v)$ and constant $^c$ . These are immediate from the definition. ", "page_idx": 22}, {"type": "text", "text": "\u00b7 Base case $\\operatorname{rw}(v)$ . This follows by Lemma C.8. ", "page_idx": 22}, {"type": "text", "text": "\u00b7 Induction step for Lipschitz functions $f(\\rho_{1},\\ldots,\\rho_{r})$ ", "page_idx": 22}, {"type": "text", "text": "Take $\\epsilon,\\delta,\\theta>0$ and $\\wedge$ -description $\\psi(\\bar{u})$ . By the induction hypothesis there is $N\\in\\mathbb{N}$ such that for all $n\\geq N$ and for every $i\\leq r$ , with probability at least $1-\\theta$ , we have that out of all the tuples $\\bar{u}$ such that $\\psi(\\bar{u})$ , at least $1-\\delta$ satisfy: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\lVert e_{\\rho_{i}}(\\mathrm{H}(\\bar{u}),\\mathrm{GrTp}(\\bar{u}))-[\\rho_{i}(\\bar{u})]\\rVert<\\epsilon\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Hence, with probability at least $1-r\\theta$ , out of all the tuples $\\bar{u}$ such that $\\psi(\\bar{u})$ , at least $1-r\\delta$ satisfy this for every $i\\le r$ . Condition on this event. ", "page_idx": 22}, {"type": "text", "text": "Take any such tuple $\\bar{u}$ . Then by Lipschitzness of $f$ we have that the normed distance between: ", "page_idx": 22}, {"type": "equation", "text": "$$\ne_{\\pi}(\\mathrm{H}(\\bar{u}),\\mathrm{GrTp}(\\bar{u}))=f(e_{\\rho_{1}}(\\mathrm{H}(\\bar{u}),\\mathrm{GrTp}(\\bar{u})),\\dots,e_{\\rho_{r}}(\\mathrm{H}(\\bar{u}),\\mathrm{GrTp}(\\bar{u})))\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "and: ", "page_idx": 22}, {"type": "equation", "text": "$$\n[\\pi(\\bar{u})]=f([\\rho_{1}(\\bar{u})],\\dots,[\\rho_{r}(\\bar{u})])\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "is at most $L_{f}\\epsilon$ , where $L_{f}$ is the Lipschitz constant of $f$ . Since $L_{f}$ is a constant, this case follows. ", "page_idx": 22}, {"type": "text", "text": "\u00b7 Inductive step for $\\begin{array}{r}{\\sum_{v}\\rho(\\bar{u},v)\\star h(\\eta(\\bar{u},v))}\\end{array}$ ", "page_idx": 22}, {"type": "text", "text": "Take $\\epsilon,\\delta,\\theta>0$ . Take a $\\wedge$ -description $\\psi(\\bar{u})$ ", "page_idx": 22}, {"type": "text", "text": "By the induction hypothesis, there is $N_{1}$ such that for all $n\\geq N_{1}$ , with probability at least $1-\\theta$ out of all the tuples $(\\bar{u},v)$ which satisfy $\\psi(\\bar{u})$ , at least $1-\\delta$ satisfy: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\|e_{\\rho}(\\mathrm{H}(\\bar{u},v),\\mathrm{GrTp}(\\bar{u},v))-[\\rho(\\bar{u},v)]\\|<\\epsilon\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "and: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\|e_{\\eta}(\\mathrm{H}(\\bar{u},v),\\mathrm{GrTp}(\\bar{u},v))-[\\eta(\\bar{u},v)]\\|<\\epsilon\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Take $\\gamma>0$ . We will choose how small $\\gamma$ is later. ", "page_idx": 22}, {"type": "text", "text": "By Lemma C.6 there is $N_{\\gamma}>N_{1}$ such that for all $n\\geq N_{\\gamma}$ , with probability at least $1-\\theta$ , out of all the tuples $\\bar{u}$ such that $\\psi(\\bar{u})$ , at least $1-\\gamma$ , at least $1-(\\delta+\\delta^{2})$ of the $v$ 's satisfy equation $(\\dagger_{\\rho})$ and equation $(\\dagger_{\\eta})$ ", "page_idx": 22}, {"type": "text", "text": "Now consider $t(\\bar{u})\\in\\mathsf{G r T}_{\\mathsf{P}_{k}}$ and take any $t^{\\prime}(\\bar{u},v)\\in\\mathsf{E x t}(t)$ . For any tuple $\\bar{u}$ of nodes such that $\\mathrm{GrTp}(\\bar{u})=t$ , define: ", "page_idx": 22}, {"type": "equation", "text": "$$\n[\\![t^{\\prime}(\\bar{u})]\\!]:=\\left\\{v\\mid t^{\\prime}(\\bar{u},v)\\right\\}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Since $p(n)$ converges to $\\tilde{p}$ and $\\mathsf{E x t}(t)$ is finite, there is $N_{3}$ such that for all $n\\geq N_{3}$ with probability at least $1-\\theta$ we have that for every pair $(t,t^{\\prime})$ and tuple $\\bar{u}$ of nodes such that $t(\\bar{u})$ ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\left|\\frac{|[t^{\\prime}(\\bar{u})]|}{n}-\\alpha(t,t^{\\prime})\\right|<\\frac{1}{2^{k+1}}\\epsilon\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Next, for any tuple $\\bar{u}$ consider the function: ", "page_idx": 22}, {"type": "equation", "text": "$$\nf_{\\pi}^{\\circ}(\\bar{u},v):=e_{\\rho}(H_{G}(\\bar{u},v),\\operatorname{GrTp}(\\bar{u},v))h(e_{\\eta}(H_{G}(\\bar{u},v),\\operatorname{GrTp}(\\bar{u},v)))\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Note that the function: ", "page_idx": 22}, {"type": "equation", "text": "$$\n(\\bar{\\pmb{a}},\\pmb{b},t^{\\prime})\\mapsto e_{\\rho}(\\bar{\\pmb{a}},\\pmb{b},t^{\\prime})h(e_{\\eta}(\\bar{\\pmb{a}},\\pmb{b},t^{\\prime}))\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "is bounded. Let $\\Lambda$ be the diameter of its range. This also bounds $f_{\\pi}^{\\circ}$ ", "page_idx": 22}, {"type": "text", "text": "We will now apply Hoeffding's Inequality to $\\textstyle\\sum_{v\\mid t^{\\prime}(\\bar{u},v)}f_{\\pi}^{\\circ}(\\bar{u},v)$ .Note that the number of summands is $|[\\![t^{\\prime}(\\bar{u})]\\!]|$ and the summands are bounded by $\\Lambda$ . Furthermore, the summands are independent and have expected value: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\underset{b\\sim\\mathbb{F}}{\\mathbb{E}}[e_{\\rho}(H_{G}(\\bar{u},\\pmb{b}),t^{\\prime})h(e_{\\eta}(H_{G}(\\bar{u},\\pmb{b}),t^{\\prime}))]\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Hence, by Hoeffding's Inequality (Theorem B.4) for any $\\bar{u}$ the probability that: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\left\\lVert\\frac{1}{n}\\sum_{v\\mid t^{\\prime}(\\bar{u},v)}f_{\\pi}^{\\circ}(\\bar{u},v)-\\frac{|\\mathbb{I}^{\\prime}(\\bar{u})]|}{n}\\sum_{b\\sim\\mathbb{F}}[e_{\\rho}(H_{G}(\\bar{u},b),t^{\\prime})h(e_{\\eta}(H_{G}(\\bar{u},b),t^{\\prime}))]\\right\\rVert\\geq\\frac{1}{2^{k+1}}e^{\\frac{\\rho+n}{2}t},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "is at most: ", "page_idx": 23}, {"type": "equation", "text": "$$\n2d\\exp\\left(-\\frac{\\epsilon^{2}|\\mathbb{I}^{t^{\\prime}}(\\bar{u})\\mathbb{I}|}{2\\Lambda^{2}}\\right)\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "By equation $(*)$ there is $N_{4}$ such that for all $n\\geq N_{4}$ , with probability at least $1-\\theta$ for all $t^{\\prime}$ such that $\\alpha(t,t^{\\prime})>0$ we have that for every tuple $\\bar{u}$ ", "page_idx": 23}, {"type": "equation", "text": "$$\n2d\\exp\\left(-\\frac{\\epsilon^{2}|[t^{\\prime}(\\bar{u})]|}{2\\Lambda^{2}}\\right)<\\theta\\delta\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Let $B_{n}$ be the proportion, out of all tuples $\\bar{u}$ for which $\\psi(\\bar{u})$ holds4, such that: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\left\\|\\frac{1}{n}\\sum_{v\\mid t^{\\prime}(\\bar{u},v)}f_{\\pi}^{\\circ}(\\bar{u},v)-\\frac{|\\mathbb{I}^{\\prime}(\\bar{u})]|}{n}\\sum_{b\\sim\\mathbb{F}}\\left[e_{\\rho}(H_{G}(\\bar{u},b),t^{\\prime})h(e_{\\eta}(H_{G}(\\bar{u},b),t^{\\prime}))\\right]\\right\\|\\geq\\frac{1}{2^{k+1}}e_{\\eta}^{2(\\eta+\\delta)}+O(\\delta^{2}),\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Note that the property above is exactly the event whose probability is bounded in equation $(\\heartsuit)$ We can express $B_{n}$ as the mean of a set of indicator variables, one for each tuple $\\bar{u}$ such that $\\psi(\\bar{u})$ holds. Each indicator variable has expected value at most $\\theta\\delta$ by equation $(\\heartsuit)$ and equation $(\\clubsuit)$ ", "page_idx": 23}, {"type": "text", "text": "Then by linearity of expectation, $\\mathbb{E}[B_{n}]\\leq\\theta\\delta$ for all $n\\geq N_{4}$ , and hence by Markov's Inequality (Theorem B.1): ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(B_{n}\\geq\\delta\\right)\\leq{\\frac{\\theta\\delta}{\\delta}}=\\theta\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Therefore, for all $n\\geq\\operatorname*{max}(N_{3},N_{4})$ with probability at least $1-\\theta$ for at least $1-\\delta$ of the tuples $\\bar{u}$ for which $\\psi(\\bar{u})$ holds we have that: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\left\\|\\frac{1}{n}\\sum_{v\\mid t^{\\prime}(\\bar{u},v)}f_{\\pi}^{\\circ}(\\bar{u},v)-\\frac{|\\mathbb{I}^{\\prime}(\\bar{u})]|}{n}\\sum_{b\\sim\\mathbb{F}}\\left[e_{\\rho}(H_{G}(\\bar{u},b),t^{\\prime})h(e_{\\eta}(H_{G}(\\bar{u},b),t^{\\prime}))\\right]\\right\\|<\\frac{1}{2^{k+1}}\\epsilon,\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "and therefore, by equation $(*)$ and the definition of $f_{\\pi}$ ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\left\\|\\frac{1}{n}\\sum_{v}f_{\\pi}^{\\circ}(\\bar{u},v)-f_{\\pi}(H_{G}(\\bar{u}),\\mathrm{GrTp}(\\bar{u}))\\right\\|<\\epsilon\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Similarly, for any tuple $\\bar{u}$ consider the function: ", "page_idx": 23}, {"type": "equation", "text": "$$\ng_{\\pi}^{\\circ}(\\bar{u},v):=h(e_{\\eta}(H_{G}(\\bar{u},v),\\mathrm{GrTp}(\\bar{u},v)))\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "As above there is $N_{5}\\geq N_{3}$ such that for all $n\\geq N_{5}$ with probability at least $1-\\theta$ we have that the proportion out of tuples $\\bar{u}$ for which $\\psi(\\bar{u})$ holds such that: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\left\\|\\frac{1}{n}\\sum_{v}g_{\\pi}^{\\circ}(\\bar{u},v)-g_{\\pi}(H_{G}(\\bar{u}),\\mathrm{{GrTp}}(\\bar{u}))\\right\\|<\\epsilon\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "is at least $1-\\delta$ ", "page_idx": 23}, {"type": "text", "text": "4Recall that $\\psi$ is the $\\wedge$ -description on which we condition the tuples $\\bar{u}$ in the inductive invariant. ", "page_idx": 23}, {"type": "text", "text": "Take $n\\geq\\operatorname*{max}(N_{1},N_{\\gamma},N_{3},N_{4},N_{5})$ .For such an $n$ , these events above hold with probability at least $1-5\\theta$ . So from now on we will condition on the event that they hold. ", "page_idx": 24}, {"type": "text", "text": "Take any such tuple $\\bar{u}$ : Let $t\\,:=\\,\\mathrm{GrTp}(\\bar{u})$ . It suffices to show, using the definition of the interpretation of the weighted mean operator, that: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\left\\|\\frac{\\sum_{v}\\[\\rho(\\bar{u},v)]h(\\mathbb{[}\\eta(\\bar{u},v)\\mathbb{]})}{\\sum_{v}h(\\mathbb{[}\\eta(\\bar{u},v)\\mathbb{]})}-\\frac{f_{\\pi}(H_{G}(\\bar{u}),t)}{g_{\\pi}(H_{G}(\\bar{u}),t)}\\right\\|<\\iota(\\epsilon,\\delta,\\gamma)\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "for $\\iota$ which we can make arbitrarily small by choosing $\\epsilon,\\delta,\\gamma$ small enough. ", "page_idx": 24}, {"type": "text", "text": "By Lemma C.9 it suffices to find $\\zeta(\\epsilon,\\delta,\\gamma),\\xi(\\epsilon,\\delta,\\gamma)>0$ which we can make arbitrarily small and constants $\\nu,\\Omega>0$ such that: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\displaystyle\\frac{1}{n}\\sum_{v^{\\prime}}[p(\\bar{u},v)]h([\\eta(\\bar{u},v)])-f_{\\pi}(H_{G}(\\bar{u}),t)\\right|<\\zeta(\\epsilon,\\delta,\\gamma)}\\\\ &{\\left|\\displaystyle\\frac{1}{n}\\sum_{v^{\\prime}}h([p(\\bar{u},v)])-g_{\\pi}(H_{G}(\\bar{u}),t)\\right|<\\xi(\\epsilon,\\delta,\\gamma)}\\\\ &{\\qquad\\displaystyle\\left\\lVert\\displaystyle\\sum_{v^{\\prime}}[p(\\bar{u},v)]h([\\eta(\\bar{u},v)])\\right\\rVert<\\Omega n}\\\\ &{\\qquad\\displaystyle\\left\\lVert\\displaystyle\\sum_{v^{\\prime}}h([p(\\bar{u},v)])\\right\\rVert<\\Omega n}\\\\ &{\\qquad\\displaystyle\\qquad\\left\\lVert\\displaystyle\\sum_{v^{\\prime}}h([p(\\bar{u},v)])\\right\\rVert<\\Omega n}\\\\ &{\\qquad\\displaystyle\\forall i\\leq d:\\left[\\displaystyle\\sum_{v^{\\prime}}h([p(\\bar{u},v)])\\right]_{i}>\\nu n}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "For equation (3) and equation (4) we can use the fact that $\\mathbb{[}\\rho(\\bar{u},v)]$ and $\\mathbb{[}\\eta(\\bar{u},v)]$ are bounded and that $h$ is Lipschitz on bounded domains. For equation (5) we use the fact that $\\mathbb{[}\\eta(\\bar{u},v)]$ is bounded and that the codomain of $h$ is $(0,\\infty)^{d}$ ", "page_idx": 24}, {"type": "text", "text": "The proofs of equation (1) and equation (2) are very similar. We prove equation (2) since it is slightly notationally lighter. Let: ", "page_idx": 24}, {"type": "equation", "text": "$$\ng_{\\pi}^{*}(\\bar{u}):=\\frac{1}{n}\\sum_{v}h(e_{\\eta}(H_{G}(\\bar{u},v),t))\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Let $\\kappa$ be a bound on the norms of $h(e_{\\eta}(\\bar{\\pmb{a}},\\pmb{b},t^{\\prime}))$ and $h(\\mathbb{I}\\eta(\\bar{u},v)\\mathbb{I})$ . By equation $(\\dagger_{\\eta})$ we have that: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\left\\|\\frac{1}{n}\\sum_{v}h(\\mathbb{I}\\eta(\\bar{u},v)\\mathbb{I})-g_{\\pi}^{*}(\\bar{u})\\right\\|<\\left(1-(\\delta+\\delta^{2})\\right)\\epsilon+(\\delta+\\delta^{2})2\\kappa^{2}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "We can make the right-hand-side as small as we like by taking $\\epsilon$ and $\\delta$ sufficiently small. ", "page_idx": 24}, {"type": "text", "text": "Now note that: ", "page_idx": 24}, {"type": "equation", "text": "$$\ng_{\\pi}^{*}(\\bar{u})=\\frac{1}{n}\\sum_{v}g_{\\pi}^{\\circ}(\\bar{u},v)\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Hence by equation $(\\triangle_{g})$ we have that: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\|g_{\\pi}^{*}(\\bar{u})-g_{\\pi}(H_{G}(\\bar{u}),t)\\|<\\epsilon\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Therefore: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\left\\|g_{\\pi}(H_{G}(\\bar{u}),t)-\\frac{1}{n}\\sum_{v}h(\\mathbb{I}\\eta(\\bar{u},v)\\mathbb{I})\\right\\|<\\left(1-(\\delta+\\delta^{2})\\right)\\epsilon+(\\delta+\\delta^{2})2\\kappa^{2}+\\epsilon\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "which we can make as small as we like by taking $\\epsilon$ and $\\delta$ small enough. ", "page_idx": 24}, {"type": "text", "text": "\u00b7 Inductiv step for $\\begin{array}{r}{\\sum_{v\\in\\mathcal{N}(u_{i})}\\rho(\\bar{u},v)\\star h(\\eta(\\bar{u},v))}\\end{array}$ ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "We proceed similarly to the global weighted mean case, this time making use of the conditioning $\\wedge$ -description in the inductive invariant. Indeed, notice that when we apply the inductive invariant below,weadd $u_{i}E v$ to our condition. ", "page_idx": 25}, {"type": "text", "text": "Take $\\epsilon,\\delta,\\theta>0$ . Take a $\\wedge$ -description $\\psi(\\bar{u})$ ", "page_idx": 25}, {"type": "text", "text": "By the induction hypothesis, there is $N_{1}$ such that for all $n\\geq N_{1}$ , with probability at least $1-\\theta$ out of all the tuples $(\\bar{u},v)$ which satisfy $\\psi(\\bar{u})\\wedge u_{i}E v$ , at least $1-\\delta$ satisfy: ", "page_idx": 25}, {"type": "text", "text": "and: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|e_{\\rho}(H_{G}(\\bar{u},v),\\mathrm{GrTp}(\\bar{u},v))-\\mathbb{I}\\rho(\\bar{u},v)\\mathbb{I}\\|<\\epsilon}\\\\ &{}\\\\ &{\\|e_{\\eta}(H_{G}(\\bar{u},v),\\mathrm{GrTp}(\\bar{u},v))-\\mathbb{I}\\eta(\\bar{u},v)\\mathbb{I}\\|<\\epsilon}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{(\\dagger_{\\rho}^{\\mathrm{loc}})}}\\\\ {{\\ }}\\\\ {{(\\dagger_{\\eta}^{\\mathrm{loc}})}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Take $\\gamma>0$ . We will choose how small $\\gamma$ is later. ", "page_idx": 25}, {"type": "text", "text": "By Lemma C.6 there is $N_{\\gamma}>N_{1}$ such that for all $n\\geq N_{\\gamma}$ , with probability at least $1-\\theta$ the following event happens. Out of all the tuples. $\\bar{u}$ such that $\\psi(\\bar{u})\\wedge\\exists v\\colon u_{i}E v$ , a proportion of at least $1-\\gamma$ of them satisfy the following. At least $1-(\\delta+\\delta^{2})$ of the nodes $v$ for which $\\psi(\\bar{u})\\wedge u_{i}E v$ holds also satisfy equation $(\\dagger_{\\rho}^{\\mathrm{loc}})$ and equation $(\\dagger_{\\eta}^{\\mathrm{loc}})$ ", "page_idx": 25}, {"type": "text", "text": "Now consider $t(\\bar{u})\\in\\mathsf{G r T}_{\\mathsf{P}_{k}}$ and take any $t^{\\prime}(\\bar{u},v)\\in\\mathsf{E x t}_{u_{i}}(t)$ . For any tuple $\\bar{u}$ of nodes such that $\\mathrm{GrTp}(\\bar{u})=t$ , define: ", "page_idx": 25}, {"type": "equation", "text": "$$\n[\\![t^{\\prime}(\\bar{u})]\\!]:=\\{v\\in\\mathcal{N}(u_{i})\\mid t^{\\prime}(\\bar{u},v)\\}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "By Lemma C.5 we have that in all the non-sparse cases there is $R>0$ and $N_{3}$ such that for all $n\\geq N_{3}$ with probability at least $1-\\theta$ , the proportion of all tuples $\\bar{u}$ for which $\\psi(\\bar{u})$ holds such that: ", "page_idx": 25}, {"type": "equation", "text": "$$\nd(u_{i})\\geq R\\log(n)\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "is at least $1-\\delta$ . Then, as before, there is $N_{4}\\geq N_{3}$ such that for all $n\\geq N_{4}$ , with probability at least $1-\\theta$ , for all pairs $(t,t^{\\prime})$ the proportion of all tuples $\\bar{u}$ for which $\\psi(\\bar{u})$ and $t(\\bar{u})$ hold such that: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\left|\\frac{|[t^{\\prime}(\\bar{u})]|}{d(u_{i})}-\\alpha(t,t^{\\prime})\\right|<\\frac{1}{2^{k+1}}\\epsilon\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "For any tuple $\\bar{u}$ define the function: ", "page_idx": 25}, {"type": "equation", "text": "$$\nf_{\\pi}^{\\circ}(\\bar{u},v):=e_{\\rho}(H_{G}(\\bar{u},v),\\operatorname{GrTp}(\\bar{u},v))h(e_{\\eta}(H_{G}(\\bar{u},v),\\operatorname{GrTp}(\\bar{u},v)))\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Taking $\\Lambda$ as before, by Hoeffding's Inequality for any $\\bar{u}$ the probability that: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\left\\|\\frac{1}{d(u_{i})}\\sum_{v\\in\\mathcal{N}(u_{i})|t^{\\prime}(\\bar{u},v)}f_{\\pi}^{\\circ}(\\bar{u},v)-\\frac{|\\![t^{\\prime}(\\bar{u})]\\!|}{d(u_{i})}\\underset{b\\sim\\mathcal{F}}{\\mathbb{E}}[e_{\\rho}(H_{G}(\\bar{u},b),t^{\\prime})h(e_{\\eta}(H_{G}(\\bar{u},b),t^{\\prime}))]\\right\\|}}\\\\ &{}&{\\ge\\frac{1}{2^{k+1}}\\epsilon}\\\\ &{}&{\\mathrm{most.}}\\\\ &{}&{2d\\exp\\left(-\\frac{\\epsilon^{2}|[t^{\\prime}(\\bar{u})]\\!|}{2\\Lambda^{2}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "is at ", "page_idx": 25}, {"type": "text", "text": "Using a similar argument to the global weighted mean case, there is $N_{5}$ such that for all $n\\geq N_{5}$ with probability at least $1-\\theta$ for all $t^{\\prime}$ such that $\\alpha(t,t^{\\prime})>0$ we have that for at least $1-\\delta$ of the tuples $\\bar{u}$ such that $\\psi(\\bar{u})$ and $t(\\bar{u})$ hold: ", "page_idx": 25}, {"type": "equation", "text": "$$\n2d\\exp\\left(-\\frac{\\epsilon^{2}|[t^{\\prime}(\\bar{u})]|}{2\\Lambda^{2}}\\right)<\\theta\\delta\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "We now proceed as in the global weighted mean case, creating a random variable similar to $B_{n}$ and making using of linearity of expectation. We get that for all $n\\geq\\operatorname*{max}(N_{3},N_{4},N_{5})$ , with probability at least $1-\\theta$ for at least $1-\\delta$ of the tuples $\\bar{u}$ for which $\\psi(\\bar{u})$ holds we have that: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\left\\|\\frac{1}{n}\\sum_{v}f_{\\pi}^{\\circ}(\\bar{u},v)-f_{\\pi}(H_{G}(\\bar{u}),\\mathrm{GrTp}(\\bar{u}))\\right\\|<\\epsilon\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Similarly, with $g_{\\pi}^{\\circ}$ defined as above, there is $N_{6}$ such that for all $n\\geq N_{6}$ , with probability at least $1-\\theta$ for at least $1-\\delta$ of the tuples $\\bar{u}$ for which $\\psi(\\bar{u})$ holds we have that: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\left\\|\\frac{1}{n}\\sum_{v}g_{\\pi}^{\\circ}(\\bar{u},v)-g_{\\pi}(H_{G}(\\bar{u}),\\mathrm{{GrTp}}(\\bar{u}))\\right\\|<\\epsilon\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "With equations $(\\dagger_{\\rho}^{\\mathrm{loc}}),(\\dagger_{\\eta}^{\\mathrm{loc}}),(\\triangle_{f}^{\\mathrm{loc}})$ and $(\\triangle_{g}^{\\mathrm{loc}})$ established, we can nw prceed as before, making use of Lemma C.9. ", "page_idx": 26}, {"type": "text", "text": "This completes the proof of Theorem 5.3. ", "page_idx": 26}, {"type": "text", "text": "Applying the lemma to prove the theorem. To prove Theorem C.1, note that $e_{\\tau}$ is only a function of the ${\\mathsf{G r T p}}_{0}$ (since $\\tau$ is a closed term), while ${\\sf G r T p}_{\\mathrm{0}}$ consists of a single type, $\\intercal$ , which is satisfied by all graphs. ${\\bf S}\\boldsymbol{0}\\,e_{\\tau}$ is a constant. ", "page_idx": 26}, {"type": "text", "text": "D  Sparse Erdos-R\u00e9nyi and Barabasi-Albert ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "We now give the proof for the sparse Erd6s-R\u00e9nyi and Barabasi-Albert cases of Theorem 5.1. In addition, we extend the result to the language AGG[WMEAN, GCN, RW] defined in Appendix A. We state the full result here. ", "page_idx": 26}, {"type": "text", "text": "Theorem D.1. Consider $(\\mu_{n})_{n\\in\\mathbb{N}}$ sampling a graph $G$ from either of the following models and node features independently from i.i.d. bounded distributions on $d$ features. ", "page_idx": 26}, {"type": "text", "text": "1.TheErdos-Renyidistribution $\\mathrm{ER}(n,p(n))$ where $p$ satisfiesSparsity:for some $K>0$ we have: $p(n)=K n^{-1}$ \uff1a   \n2.TheBarabasi-Albertmodel $\\operatorname{BA}(n,m)$ for any $m\\geq1$   \nThen for every AGG[WMEAN, GCN, RW] term converges a.a.s. with respect to $(\\mu_{n})_{n\\in\\mathbb{N}}$ ", "page_idx": 26}, {"type": "text", "text": "As discussed in the body, this requires us to analyze neighbourhood isomorphism types, which implicitly quantify over local neighbourhoods, rather than atomic types as in the non-sparse cases. Remark D.2. Formally, a $k$ -rooted graph is a tuple $(T,{\\bar{u}})$ . At various points below to lighten the notation we drop the $\\acute{\\bullet}\\acute{u}\\,$ and refer simply to a $k$ -rooted graph $T$ ", "page_idx": 26}, {"type": "text", "text": "D.1  Combinatorial tools for the sparse case ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "As in the dense case, before we turn to analysis of our term language, we prove some results about the underlying random graph model that we can use as tools. ", "page_idx": 26}, {"type": "text", "text": "The key combinatorial tool we will use is the following, which states that for any tuple of nodes $\\bar{u}$ the proportion of nodes $v$ such that $(\\bar{u},v)$ has any particular neighbourhood type converges. ", "page_idx": 26}, {"type": "text", "text": "Definition D.3. Let $(T,\\bar{w})$ be a $k$ -rooted graph and let $(T^{\\prime},\\bar{w},y)$ be a $(k+1)$ -rooted graph. Then $T^{\\prime}$ extends $T$ $T$ is a subgraph of $T^{\\prime}$ . Let $\\mathsf{E x t}(T)$ be the set of all $(k+1)$ -rooted graphs extending $(T,{\\bar{w}})$ ", "page_idx": 26}, {"type": "text", "text": "Theorem D.4. Consider sampling a graph $G$ from either the sparse Erdos-Renyi or Barabasi-Albert distributions. Let $(T,{\\bar{w}})$ be a $k$ -rootedgraph and take $\\ell\\in\\mathbb{N}$ Then for every $(k+1)$ -rooted graph $(T^{\\prime},\\bar{w},y)$ which extends $(T,{\\bar{w}})$ there is $q_{T^{\\prime}|T}\\in[0,1]$ such that for all $\\epsilon,\\theta>0$ there is $N\\in\\mathbb{N}$ such that for all $n\\geq N$ with probability at least $1-\\theta$ we have that for all $k$ -tuplesofnodes $\\bar{u}$ such that $\\mathcal{N}_{\\ell}^{G}(\\bar{u})\\cong(T,\\bar{w})$ ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\left|\\frac{|\\{v\\ |\\ N_{\\ell}^{G}(\\bar{u},v)\\cong(T^{\\prime},\\bar{w},y)\\}|}{n}-q_{T}\\right|<\\epsilon\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Moreover, we have that: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\sum_{T^{\\prime}\\in\\mathsf{E x t}(T)}q_{T^{\\prime}|T}=1\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "We refer to the $q_{T^{\\prime}}|T$ as relative neighborhood weights. They will play a role in defining the controllers, analogous to that of the relative atomic type weights $\\alpha(t,t^{\\prime})$ in the non-sparse case. ", "page_idx": 26}, {"type": "text", "text": "Theorem D.4 follows from what is known as \u201cweak local convergence\" in the literature. It is essentially a non-parametrised version of Theorem D.4. ", "page_idx": 27}, {"type": "text", "text": "Definition D.5. A sequence $(\\mu_{n})_{n\\in\\mathbb{N}}$ of graph distributions has weak local convergence if for every $k$ -rooted graph $(T,{\\bar{w}})$ and $\\ell\\in\\mathbb{N}$ there is $q_{T}\\in[0,1]$ such that for all $\\epsilon,\\theta>0$ there is $N\\in\\mathbb{N}$ such that for all $n\\geq N$ with probability at least $1-\\theta$ we have that: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\left|\\frac{|\\{\\bar{u}\\mid\\mathcal{N}_{\\ell}^{G}(\\bar{u})\\cong(T,\\bar{w})\\}|}{n}-q_{T}\\right|<\\epsilon\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "and moreover: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\sum_{T}q_{T}=1\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Theorem D.6. The sparse Erdos-Renyi and Barabasi-Albert distributions have weak local convergence. ", "page_idx": 27}, {"type": "text", "text": "Proof. See [48, Theorem 2.18] for the sparse Erd6s-R\u00e9nyi distribution and [17, Theorem 4.2.1] for the Barabasi-Albert distribution. \u53e3 ", "page_idx": 27}, {"type": "text", "text": "Proof of Theorem D.4. Take $T^{\\prime}\\in\\mathsf{E x t}(T)$ . There are two cases. ", "page_idx": 27}, {"type": "text", "text": "Case 1. There is a path from $y$ to some node $w_{i}$ of $\\bar{w}$ in $T^{\\prime}$ ", "page_idx": 27}, {"type": "text", "text": "Set $q_{T^{\\prime}|T}=0$ . Note that in this case, for all tuples $\\bar{u}$ such that $\\mathcal{N}_{\\ell}^{G}(\\bar{u})\\cong T$ we have that: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\{v\\mid{\\mathcal{N}}_{\\ell}^{G}(\\bar{u},v)\\cong(T^{\\prime},\\bar{w},y)\\}\\subseteq{\\mathcal{N}}_{\\ell}^{G}(u_{i})\\}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Since $\\mathcal{N}_{\\ell}^{G}(u_{i})$ is determined by $T$ and is thus of fixed size, the proportion: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\left|\\frac{|\\{v\\ |\\ \\mathcal{N}_{\\ell}^{G}(\\bar{u},v)\\cong(T^{\\prime},\\bar{w},y)\\}|}{n}\\right|\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "tends to $0$ as $n$ grows. This completes the proof for Case 1. ", "page_idx": 27}, {"type": "text", "text": "Case 2. There is no path from $y$ to any node of $\\bar{w}$ in $T^{\\prime}$ . Then for all tuples $\\bar{u}$ such that $\\mathcal{N}_{\\ell}^{G}(\\bar{u})\\cong T$ we have that: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\{v\\mid\\mathcal{N}_{\\ell}^{G}(\\bar{u},v)\\cong(T^{\\prime},\\bar{w},y)\\}=\\{v\\mid\\mathcal{N}_{\\ell}^{G}(v)\\cong\\mathcal{N}_{\\ell}^{T^{\\prime}}(y)\\}\\setminus\\mathcal{N}_{\\ell}^{G}(\\bar{u})\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Hence: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\frac{|\\{v\\mid\\mathcal{N}_{\\ell}^{G}(v)\\cong\\mathcal{N}_{\\ell}^{T^{\\prime}}(y)\\}|}{n}-\\frac{|\\mathcal{N}_{\\ell}^{G}(\\bar{u})|}{n}\\leq\\frac{|\\{v\\mid\\mathcal{N}_{\\ell}^{G}(\\bar{u},v)\\cong(T^{\\prime},\\bar{w},y)\\}|}{n}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Also: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\frac{|\\{v\\mid{\\mathcal{N}}_{\\ell}^{G}(\\bar{u},v)\\cong(T^{\\prime},\\bar{w},y)\\}|}{n}\\leq\\frac{|\\{v\\mid{\\mathcal{N}}_{\\ell}^{G}(v)\\cong{\\mathcal{N}}_{\\ell}^{T^{\\prime}}(y)\\}|}{n}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "By Lemma 5.5 (with $k=1$ ) we have that: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\frac{|\\{v\\ |\\ \\mathcal{N}_{\\ell}^{G}(v)\\cong\\mathcal{N}_{\\ell}^{T^{\\prime}}(y)\\}|}{n}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "converges to some limit $q_{T|T^{\\prime}}:=q_{\\mathcal{N}_{\\ell}^{T^{\\prime}}(y)}$ asymptotically almost surely, while: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\frac{\\left|\\mathcal{N}_{\\ell}^{G}(\\bar{u})\\right|}{n}=\\frac{|T|}{n}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "converges to 0. This completes the proof for Case 1. ", "page_idx": 27}, {"type": "text", "text": "Finally, to show that $\\sum_{T^{\\prime}\\in\\mathsf{E x t}(T)}q_{T^{\\prime}|T}=1$ note the set: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\{\\mathcal{N}_{\\ell}^{T^{\\prime}}(y)\\mid(T^{\\prime},\\bar{w},y)\\in\\mathsf{E x t}(T^{\\prime})\\}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "contains all 1-rooted graphs up to isomorphism which can be the $\\ell$ -neighbourhood of a node. Therefore, by the \u201cmoreover? part of Definition D.5 we have that: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\sum_{T^{\\prime}\\in\\mathsf{E}\\times\\mathsf{t}(T)}q_{T^{\\prime}|T}=\\sum_{T^{\\prime}\\in\\mathsf{E}\\times\\mathsf{t}(T)}q_{\\mathcal{N}_{\\ell}^{T^{\\prime}}(y)}=1\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "D.2  Proving the inductive invariant for the sparse cases, and proving the convergence theorem for these cases ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "We now begin the proof of Theorem D.1. Recall from the body that we will do this via Theorem 5.6, which requires us to define some Lipschitz controllers on neighbourhood types that approximate a giventerm $\\pi$ relative to a neighbourhood type. ", "page_idx": 28}, {"type": "text", "text": "Throughout the following, it will be convenient to assume for every $k$ -rooted graph isomorphism type $(T,\\bar{u})$ a canonical ordering nodes $T=\\{s_{1},\\ldots,s_{|T|}\\}$ such that: ", "page_idx": 28}, {"type": "text", "text": "\u00b7 the first $k$ nodes in the ordering are $\\bar{u}$ and   \n\u00b7 for any graph $G$ , tuple of nodes $\\bar{u}$ and $\\ell,\\ell^{\\prime}\\in\\mathbb{N}$ with $\\ell^{\\prime}<\\ell$ the canonical ordering of $\\mathcal{N}_{\\ell^{\\prime}}(\\bar{u})$ is an initial segment of the canonical ordering of $\\mathcal{N}_{\\ell}(\\bar{u})$ ", "page_idx": 28}, {"type": "text", "text": "Again we will use $\\mathbb{F}$ for the probability distribution of the node features. For each subterm $\\pi$ of $\\tau$ ,its reach,denoted $\\operatorname{Reach}(\\pi)$ is a natural number defined as follows: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Reach}(\\mathrm{H}(u))=0}\\\\ &{\\qquad\\qquad\\mathrm{Reach}(c)=0}\\\\ &{\\qquad\\qquad\\mathrm{Reach}(\\mathrm{rw}(v))=d}\\\\ &{\\mathrm{Reach}(f(\\rho_{1},\\dots,\\rho_{r}))=\\underset{i\\leq r}{\\operatorname{max}}\\mathrm{Reach}(\\rho_{i})}\\\\ &{\\mathrm{Reach}\\left(\\displaystyle\\sum_{v\\in\\mathcal{N}(u)}\\rho\\star h(\\eta)\\right)=\\operatorname*{max}(\\mathrm{Reach}(\\rho),\\mathrm{Reach}(\\eta))+1}\\\\ &{\\qquad\\mathrm{Reach}\\left(\\displaystyle\\sum_{v\\,\\in\\mathcal{N}^{\\star}(u)}\\rho\\star h(\\eta)\\right)=0}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Take a subterm $\\pi$ of $\\tau$ . Let $k$ be the number of free variables in $\\pi$ . We now define the controller at $\\pi$ for every $k$ -rooted graph $(T,{\\bar{w}})$ . The controller will be of the form: ", "page_idx": 28}, {"type": "equation", "text": "$$\ne_{\\pi}^{T}\\colon([0,1]^{d})^{|T|}\\to\\ensuremath{\\mathbb{R}}^{d}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Note that when $|T|=0$ this is a constant. ", "page_idx": 28}, {"type": "text", "text": "Recall from Theorem 5.6 that our goal is to ensure the following correctness property for our conrolersr $e_{\\pi}^{T}$ ", "page_idx": 28}, {"type": "text", "text": "Property D.7. Let $\\pi$ be any subterm of $\\tau$ and let $\\ell=\\operatorname{Reach}(\\pi)$ .Consider sampling a graph $G$ from either the sparse Erdos-Renyi or Barabasi-Albert distributions. For every $k$ -rootedgraph $(T,{\\bar{w}})$ and $\\epsilon,\\theta>0,$ there is $N\\in\\mathbb{N}$ such that for all $n\\geq N$ withprobabilityatleast $1-\\theta$ we have that for every $k$ -tuple of nodes $\\bar{u}$ in the graph such that $\\mathcal{N}_{\\ell}(\\bar{u})\\cong(T,\\bar{w})$ ,taking the canonical ordering $\\mathcal{N}_{\\ell}(\\bar{u})=\\{s_{1},\\dots,s_{|T|}\\}$ ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\left|\\left|e_{\\pi}^{T}(H_{G}(s_{1}),\\dots,H_{G}(s_{|T|}))-\\mathbb{I}\\pi(\\bar{u})\\right|\\right|<\\epsilon\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "For notational convenience, we allow each $e_{\\pi}^{T}$ to take additional arguments as input, which it will ignore. ", "page_idx": 28}, {"type": "text", "text": "Proof of Theorem 5.6. We give the construction of the controllers in parallel with the proof of correctness. ", "page_idx": 28}, {"type": "text", "text": "\u00b7 Base case $\\pi=\\mathrm{H}(u_{i})$ ", "page_idx": 28}, {"type": "text", "text": "Here $\\ell=\\operatorname{Reach}(\\pi)=0$ Let $e_{\\pi}^{T}(\\bar{\\pmb{a}}):=\\pmb{a}_{i}$ , the feature value of the $i^{\\mathrm{th}}$ node in the tuple. Note that for any $k$ -tuple of nodes $\\bar{u}$ in the graph we have: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{e_{\\pi}^{T}(H_{G}(s_{1}),\\dots,H_{G}(s_{|T|}))=H_{G}(s_{i})}\\\\ {=[\\pi(\\bar{u})]\\quad}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "\u00b7Base case \u03c0 = C. ", "page_idx": 28}, {"type": "text", "text": "Let $e_{\\pi}^{T}(\\bar{\\pmb{a}}):=\\pmb{c}$ ", "page_idx": 28}, {"type": "text", "text": "\u00b7 Base case $\\pi=\\operatorname{rw}(u_{i})$ ", "page_idx": 29}, {"type": "text", "text": "Then $\\ell=\\mathrm{Reach}(\\pi)=d$ . Note that the random walk embeddings up to length $d$ are entirely determined by the $d$ -neighbourhood. Therefore, given rooted graph $(T,{\\bar{w}})$ , there is $r_{T}$ such that: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\left[\\operatorname{\\![rw}(u_{i})]\\!\\right]=r_{T}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "for all tuples $\\bar{u}$ such that $\\mathcal{N}_{\\ell}(\\bar{u})=T$ . So set: ", "page_idx": 29}, {"type": "equation", "text": "$$\ne_{\\pi}^{T}(\\bar{a})=r_{T}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "for any $\\bar{\\pmb{a}}$ ", "page_idx": 29}, {"type": "text", "text": ". Inductive step for Lipschitz functions $\\pi=F(\\rho_{1},...,\\rho_{r})$ ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Note that $\\ell\\,=\\,\\mathrm{max}_{j\\le r}\\,\\mathrm{Reach}(\\rho_{i})$ . Take a $k$ -rooted graph $(T,{\\bar{w}})$ . For each $i\\,\\le\\,r$ let $T_{j}:=$ $\\mathcal{N}_{\\mathrm{Reach}(\\rho_{j})}^{T}(\\bar{w})$ Defin: ", "page_idx": 29}, {"type": "equation", "text": "$$\ne_{\\pi}^{T}(\\bar{\\pmb{a}}):=F(e_{\\rho_{1}}^{T_{1}}(\\bar{\\pmb{a}}),\\dots,e_{\\rho_{r}}^{T_{r}}(\\bar{\\pmb{a}}))\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Now take $\\epsilon,\\theta\\;>\\;0$ .By the induction hypothesis, there is $N$ such that for all $n~\\geq~N$ with probability at least $1\\!-\\!\\theta$ we have that for every $k$ -tuple of nodes $\\bar{u}$ in the graph, letting $T=\\mathcal{N}_{\\ell}(\\bar{u})$ for every $j\\le r$ ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\left|\\left|e_{\\rho_{j}}^{T_{j}}(H_{G}(s_{1}),\\dots,H_{G}(s_{|T_{j}|}))-[\\rho_{j}(\\bar{u})]\\right|\\right|<\\epsilon\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Hence, by the Lipschitzness of $F$ we have: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\left\\|e_{\\pi}^{T}(H_{G}(s_{1}),\\dots,H_{G}(s_{|T|}))-\\|\\pi(\\bar{u})\\|\\right\\|<L_{F}\\epsilon\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where $L_{F}$ is the Lipschitz constant of $F$ ", "page_idx": 29}, {"type": "text", "text": "\u00b7 Inductive step for local weighted mean $\\begin{array}{r}{\\pi=\\sum_{v\\in\\mathcal{N}(u_{i})}\\rho\\star h(\\eta)}\\end{array}$ ", "page_idx": 29}, {"type": "text", "text": "In this step we use that the value of a local aggregator is determined by the values of the terms it is aggregating in the local neighbourhood. ", "page_idx": 29}, {"type": "text", "text": "Take a $k$ -rooted graph $(T,{\\bar{w}})$ ,where $k$ is the number of free variables in $\\pi$ . Note that $\\ell=$ $\\mathrm{max}(\\mathrm{Reach}(\\rho),\\mathrm{Reach}(\\eta))+1$ ", "page_idx": 29}, {"type": "text", "text": "When $w_{i}$ has no neighbours in $T$ , define: ", "page_idx": 29}, {"type": "equation", "text": "$$\ne_{\\pi}^{T}(\\bar{\\pmb{a}}):=\\mathbf{0}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "and note that for any $k$ -tuple of nodes $\\bar{u}$ in the graph such that $\\mathcal{N}_{\\ell}(\\bar{u})=T$ we have: ", "page_idx": 29}, {"type": "equation", "text": "$$\n[\\pi(\\bar{u})]=\\mathbf{0}=e_{\\pi}^{T}(H_{G}(s_{1}),\\ldots,H_{G}(s_{|T|}))\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "So suppose that $w_{i}$ has some neighbours in $T$ . Enumerate the neighbours as: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathcal{N}^{T}(w_{i})=\\{y_{1},\\ldots,y_{r}\\}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Define the Lipschitz function: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathrm{WMean}_{T}(a_{1},\\ldots,a_{r},b_{1},\\ldots,b_{r}):=\\frac{\\sum_{j=0}^{r}a_{j}h(b_{j})}{\\sum_{j=0}^{r}h(b_{j})}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Note that whenever $\\mathcal{N}_{\\ell}^{G}(\\bar{u})\\cong(T,\\bar{w})$ , letting $\\mathcal{N}^{G}(u_{i})=\\{v_{1},\\ldots,v_{r}\\}$ be the enumeration of the neighbourhood of $u_{i}$ given by the isomorphism, we have that: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\lVert\\pi(\\bar{u})\\rVert=\\mathrm{WMean}_{T}(\\lVert\\rho(\\bar{u},v_{1})\\rVert,\\dots,\\lVert\\rho(\\bar{u},v_{r})\\rVert,\\lVert\\eta(\\bar{u},v_{1})\\rVert,\\dots,\\lVert\\eta(\\bar{u},v_{r})\\rVert)\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "For any $y_{j}\\in\\mathcal{N}(w_{i})$ , let: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad T_{j}:=\\mathcal{N}_{\\ell-1}^{T}(\\bar{w},y_{j})}\\\\ &{T_{j}^{\\rho}:=\\mathcal{N}_{\\mathrm{Reach}(\\rho)}^{T}(\\bar{w},y_{j})}\\\\ &{T_{j}^{\\eta}:=\\mathcal{N}_{\\mathrm{Reach}(\\eta)}^{T}(\\bar{w},y_{j})}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Further, for any $\\bar{\\pmb{a}}\\in([0,1]^{d})^{|T|}$ let $\\Bar{\\pmb{a}}_{j}^{\\rho}$ and $\\bar{\\pmb{a}}_{j}^{\\eta}$ be the tuple of elements of $\\bar{\\pmb{a}}$ corresponding to the nodes of $T_{j}^{\\rho}$ and $T_{j}^{\\eta}$ respectively. ", "page_idx": 30}, {"type": "text", "text": "Let: ", "page_idx": 30}, {"type": "equation", "text": "$$\ne_{\\pi}^{T}(\\bar{\\pmb{a}}):=\\mathrm{WMean}_{T}\\left(e_{\\rho}^{T_{1}^{\\rho}}(\\bar{\\pmb{a}}_{1}^{\\rho}),\\dots,e_{\\rho}^{T_{r}^{\\rho}}(\\bar{\\pmb{a}}_{r}^{\\rho}),e_{\\eta}^{T_{1}^{\\eta}}(\\bar{\\pmb{a}}_{1}^{\\eta}),\\dots,e_{\\eta}^{T_{r}^{\\eta}}(\\bar{\\pmb{a}}_{r}^{\\eta}),\\right)\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Take $\\epsilon,\\theta>0$ . Applying the induction hypothesis to the term $\\rho(\\bar{u},v)$ and to $\\eta(\\bar{u},v)$ , which have Reach at most $\\ell-1$ , and using the fact that $\\boldsymbol{\\mathcal{N}}^{T}(\\boldsymbol{w_{i}})$ is finite, there is $N$ such that for all $n\\geq N$ with probability at least $1-\\theta$ , for every $y_{j}\\in\\mathcal{N}^{T}(w_{i})$ and every $(k+1)$ -tuple of nodes $(\\bar{u},v)$ such that $\\mathcal{N}_{\\ell-1}^{G}(\\bar{u},v)\\cong(T_{j},\\bar{w},y_{j})$ we have that both: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\left\\|e_{\\rho}^{T_{j}^{\\rho}}(H_{G}(s_{1}),\\ldots,H_{G}(s_{|T_{j}^{\\rho}|}))-\\mathbb{I}\\rho(\\bar{u},v)\\mathbb{I}\\right\\|<\\epsilon\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "and: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\left\\|e_{\\eta}^{T_{j}^{\\eta}}(H_{G}(s_{1}),\\dots,H_{G}(s_{|T_{j}^{\\eta}|}))-\\mathbb{I}\\eta(\\bar{u},v)\\mathbb{I}\\right\\|<\\epsilon\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Under this event, by equation $(\\boxed{\\varDelta})$ and the Lipschitzness of $\\mathrm{WMean}_{T}$ we have: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\left\\|e_{\\pi}^{T}(H_{G}(s_{1}),\\dots,H_{G}(s_{|T|}))-\\|\\pi(\\bar{u})\\|\\right\\|<L_{T}\\epsilon\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where $L_{T}$ is the Lipschitz constant of $\\mathrm{WMean}_{T}$ ", "page_idx": 30}, {"type": "text", "text": "\u00b7 Inductive step for the GCN aggregator $\\pi=\\operatorname{GCN}_{v\\in{\\mathcal N}(u_{i})}\\rho.$ ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "This step is very similar to the previous, where we again use the fact that the value of a local aggregator is determined by the values of the terms it is aggregating in the local neighbourhood. The only difference is that we now have a different Lipschitz function: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathrm{GCN}_{T}(a_{1},\\ldots,a_{r}):=\\sum_{j=1}^{r}\\frac{1}{\\left|\\mathcal{N}^{T}(u_{i})\\right|\\left|\\mathcal{N}^{T}(y_{j})\\right|}a_{j}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "The rest of the proof is as before. ", "page_idx": 30}, {"type": "text", "text": "\u00b7 Inductive step for global weighted mean $\\begin{array}{r}{\\pi=\\sum_{v}\\rho\\star h(\\eta)}\\end{array}$ ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Note that $\\ell=0$ . Let $\\ell^{\\prime}=\\operatorname*{max}(\\operatorname{Reach}(\\rho),\\operatorname{Reach}(\\eta))$ . Take a $k$ -rooted graph $(T,\\bar{w})$ . For any tuple $\\bar{u}$ and $(k+1)$ -rooted graph $(T^{\\prime},\\bar{w},y)$ extending $(T,\\bar{w})$ let: ", "page_idx": 30}, {"type": "equation", "text": "$$\n[T^{\\prime}(\\bar{u})]\\!\\!]:=|\\{v\\mid\\!\\mathcal{N}_{\\ell^{\\prime}}^{G}(\\bar{u},v)\\cong(T^{\\prime},\\bar{w},y)\\}|\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "By the parameterized Weak Local Convergence result, Theorem D.4, for every such $T^{\\prime}$ extending there is a relative neighborhood weight $\\bar{q}_{T|T^{\\prime}}\\in[0,1]$ such that for all $\\epsilon,\\theta>0$ there is $N\\in\\mathbb{N}$ such that for all $n\\geq N$ with probability at least $1-\\theta$ , for every $k$ -tuple $\\bar{u}$ such that $\\mathcal{N}_{\\ell}(\\bar{u})=T$ wehavethat: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\left|\\frac{[T^{\\prime}(\\bar{u})]}{n}-q_{T|T^{\\prime}}\\right|<\\epsilon\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "and moreover the relative neighborhood weights sum to one: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\sum_{T^{\\prime}\\in\\mathsf{E x t}(T)}q_{T|T^{\\prime}}=1\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Consider any $(T^{\\prime},\\bar{w},y)\\in\\mathsf{E x t}(T)$ In order to define the controller, we need to identify the nodes in the canonical enumeration corresponding to $\\mathcal{N}_{\\ell^{\\prime}-1}^{T}(\\bar{u})$ . Let: ", "page_idx": 30}, {"type": "equation", "text": "$$\nr_{T}:=|\\mathcal{N}_{\\ell^{\\prime}-1}^{T}(\\bar{u})|\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Given tuples of $\\mathbb{R}^{d}$ -vectors $\\bar{\\pmb{a}}=(\\pmb{a}_{1},\\dots,\\pmb{a}_{r_{T}})$ and $\\bar{\\pmb{b}}=(\\pmb{b}_{r_{T}+1},\\dots,\\pmb{b}_{|T^{\\prime}|})$ , let $\\mathrm{Arrange}_{T^{\\prime}}(\\bar{\\boldsymbol{a}},\\bar{\\boldsymbol{b}})$ be the $\\left|T^{\\prime}\\right|$ -tuple of vectors obtained by assigning the $\\pmb{a}_{i}$ 's to the nodes in $\\mathcal{N}_{\\ell^{\\prime}-1}^{T}(\\bar{u})$ and the $b_{i}$ to the remaining nodes in $T^{\\prime}$ , according to the canonical enumeration of $T^{\\prime}$ ", "page_idx": 30}, {"type": "text", "text": "Define the controller: ", "page_idx": 30}, {"type": "equation", "text": "$$\ne_{\\pi}^{T}(\\bar{\\pmb{a}}):=\\frac{f_{\\pi}^{T}(\\bar{\\pmb{a}})}{g_{\\pi}^{T}(\\bar{\\pmb{a}})}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where: ", "page_idx": 31}, {"type": "equation", "text": "$$\nf_{\\pi}^{T}(\\bar{\\pmb{a}}):=\\sum_{T^{\\prime}\\in\\mathsf{E x t}(T)}\\frac{\\mathbb{E}}{\\bar{b}\\sim\\mathbb{F}}\\left[e_{\\rho}^{T^{\\prime}}(\\mathrm{Arrange}_{T^{\\prime}}(\\bar{\\pmb{a}},\\bar{\\pmb{b}}))\\cdot h(e_{\\eta}^{T^{\\prime}}(\\mathrm{Arrange}_{T^{\\prime}}(\\bar{\\pmb{a}},\\bar{\\pmb{b}})))\\right]\\cdot q_{T|T^{\\prime}}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "and: ", "page_idx": 31}, {"type": "equation", "text": "$$\ng_{\\pi}^{T}\\big(a_{1},\\dots,a_{k}\\big):=\\sum_{T^{\\prime}\\in\\mathsf{E x t}(T)}\\frac{\\mathbb{E}}{b\\!\\sim\\!\\mathbb{F}}\\left[h\\big(e_{\\eta}^{T^{\\prime}}(\\mathrm{Arrange}_{T^{\\prime}}(\\bar{a},\\bar{b}))\\big)\\right]\\cdot q_{T|T^{\\prime}}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where in both cases the expectation is taken over $b_{i}$ 's sampled independently from node feature distribution $\\mathbb{F}$ ", "page_idx": 31}, {"type": "text", "text": "Since: ", "page_idx": 31}, {"type": "text", "text": "and: ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{e_{\\rho}^{T^{\\prime}}(\\pmb{a}_{1},\\dots,\\pmb{a}_{|T^{\\prime}|})h(e_{\\eta}^{T^{\\prime}}(\\pmb{a}_{1},\\dots,\\pmb{a}_{|T^{\\prime}|}))}\\\\ &{\\qquad\\qquad\\qquad\\quad h(e_{\\eta}^{T^{\\prime}}(\\pmb{a}_{1},\\dots,\\pmb{a}_{|T^{\\prime}|}))}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "are bounded, and $\\sum_{T^{\\prime}\\in\\mathsf{E}\\times\\mathsf{t}(T)}q_{T|T^{\\prime}}$ convergestsuerabltla controller is well-defined. ", "page_idx": 31}, {"type": "text", "text": "This completes the definition of the controller. We now present the proof that it is correct. ", "page_idx": 31}, {"type": "text", "text": "Take $\\epsilon,\\theta>0$ . Take a finite $S\\subseteq\\mathsf{E x t}(T)$ such that: ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\sum_{T^{\\prime}\\in S}q_{T|T^{\\prime}}>1-\\epsilon\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "and each $q_{T|T^{\\prime}}>0$ for $T^{\\prime}\\in S$ ", "page_idx": 31}, {"type": "text", "text": "The guarantee given when we applied the parameterized version of Weak Local Convergence, Theorem D.4, is that there is $N_{1}$ such that for all $n\\geq N_{1}$ with probability at least $1-\\theta$ , for all $T^{\\prime}\\in S$ and every $k$ -tuple $\\bar{u}$ such that $\\mathcal{N}_{\\ell}(\\bar{u})=T$ we have that: ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\left|\\frac{[T^{\\prime}(\\bar{u})]}{n}-q_{T|T^{\\prime}}\\right|<\\epsilon\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Define the function: ", "page_idx": 31}, {"type": "equation", "text": "$$\nf_{\\pi}^{\\circ}(\\bar{a},\\bar{b}):=e_{\\rho}^{T^{\\prime}}(\\mathrm{Arrange}_{T^{\\prime}}(\\bar{a},\\bar{b}))\\cdot h(e_{\\eta}^{T^{\\prime}}(\\mathrm{Arrange}_{T^{\\prime}}(\\bar{a},\\bar{b})))\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "and similarly: ", "page_idx": 31}, {"type": "equation", "text": "$$\ng_{\\pi}^{\\circ}(\\bar{\\pmb{a}},\\bar{\\pmb{b}}):=h(e_{\\eta}^{T^{\\prime}}(\\mathrm{Arrange}_{T^{\\prime}}(\\bar{\\pmb{a}},\\bar{\\pmb{b}})))\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Note that these are just the integrands used in $f_{\\pi}^{T}(\\bar{\\pmb{a}})$ and $g_{\\pi}^{T}(\\bar{\\pmb{a}})$ ", "page_idx": 31}, {"type": "text", "text": "Now, for any $(k+1)$ -tuple of nodes $(\\bar{u},v)$ let $\\bar{\\pmb{a}}^{\\bar{u}}$ be the tuple of node features of $\\mathcal{N}_{\\ell^{\\prime}-1}^{G}(\\bar{u})$ and $\\bar{\\pmb{b}}^{(\\bar{u},v)}$ be the tuple of node features of the remaining nodes in $\\mathcal{N}_{\\ell^{\\prime}}^{G}(\\bar{u},v)$ , ordered as in the canonical enumeration of $\\mathcal{N}_{\\ell^{\\prime}}^{G}(\\bar{u},v)$ ", "page_idx": 31}, {"type": "text", "text": "Note that $f_{\\pi}^{\\circ}$ is bounded, say by $\\Lambda$ Then for every $\\gamma>0$ , by Hoeffding's Inequality (Theorem B.4) for any tuple $\\bar{u}$ such that $\\mathcal{N}_{\\ell}(\\bar{u})=T$ and ${T}^{\\prime}\\in S$ we have that: ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\left\\lVert\\sum_{\\mathcal{N}_{\\ell^{\\prime}}^{G}(\\bar{u},v)\\geq\\alpha^{\\prime}}f_{\\pi}^{\\circ}(\\bar{a}^{\\bar{u}},\\bar{b}^{(\\bar{u},v)})-|[T^{\\prime}(\\bar{u})]|\\cdot\\frac{\\mathbb{E}}{\\bar{b}\\sim\\mathbb{F}}\\left[f_{\\pi}^{\\circ}(\\bar{a}^{\\bar{u}},\\bar{b})\\right]\\right\\rVert\\geq|[T^{\\prime}(\\bar{u})]|\\gamma\\right)}\\\\ {\\leq2\\exp\\left(-\\frac{2\\left\\lVert T^{\\prime}(\\bar{u})\\right\\rVert\\gamma^{2}}{\\Lambda^{2}}\\right)}\\\\ {\\leq2\\exp\\left(-\\frac{2n(q_{T}^{\\prime}-\\epsilon)\\gamma^{2}}{\\Lambda^{2}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where in the last inequality we use equation $\\left(\\ddag\\right)$ . Hence, taking a union bound, there is $N_{2}$ such that for all $n\\geq N_{2}$ with probability at least $1-\\theta$ , for all $\\bar{u}$ such that $\\mathcal{N}_{\\ell}(\\bar{u})=T$ and $T^{\\prime}\\in\\mathsf{E x t}(T)$ we have: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\left\\|\\sum_{\\mathcal{N}_{\\ell^{\\prime}}^{G}(\\bar{u},v)\\cong T^{\\prime}}f_{\\pi}^{\\circ}(\\bar{a}^{\\bar{u}},\\bar{b}^{(\\bar{u},v)})-|\\mathbb{I}T^{\\prime}(\\bar{u})\\mathbb{I}|\\cdot\\underbrace{\\mathbb{E}}_{\\bar{b}\\sim\\mathbb{F}}\\left[f_{\\pi}^{\\circ}(\\bar{a}^{\\bar{u}},\\bar{b})\\right]\\right\\|<|\\mathbb{I}T^{\\prime}(\\bar{u})]|\\gamma\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "If we divide both sides of the inequality through by. $n$ , take a $\\gamma$ below 1, and apply equation $\\left(\\ddag\\right)$ again, we conclude that there is $N_{4}$ such that for all $n\\geq N_{4}$ with probability at least $1-\\theta$ , for all $\\bar{u}$ such that $\\mathcal{N}_{\\ell}(\\bar{u})=T$ and $T^{\\prime}\\in\\mathsf{E x t}(T)$ we have: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\left\\|\\frac{1}{n}\\sum_{\\stackrel{N_{\\ell}^{G}(\\bar{u},v)\\cong T^{\\prime}}{N_{\\ell^{\\prime}}^{G}(\\bar{u},v)\\cong T^{\\prime}}}f_{\\pi}^{\\circ}(\\bar{a}^{\\bar{u}},\\bar{b}^{(\\bar{u},v)})-q_{T|T^{\\prime}}\\underset{\\bar{b}\\sim\\mathbb{R}}{\\mathbb{E}}\\left[f_{\\pi}^{\\circ}(\\bar{a}^{\\bar{u}},\\bar{b})\\right]\\right\\|<\\epsilon\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Similarly, there is $N_{5}$ such that for all $n\\geq N_{5}$ with probability at least $1-\\theta$ , for all $\\bar{u}$ such that $\\mathcal{N}_{\\ell}(\\bar{u})=T$ and $T^{\\prime}\\in\\mathsf{E x t}(T)$ we have: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\left\\|\\frac{1}{n}\\sum_{\\stackrel{N_{\\ell}^{G}(\\bar{u},v)\\cong T^{\\prime}}{N_{\\ell^{\\prime}}^{G}(\\bar{u},v)\\cong T^{\\prime}}}g_{\\pi}^{\\circ}(\\bar{a}^{\\bar{u}},\\bar{b}^{(\\bar{u},v)})-q_{T|T^{\\prime}}\\underset{\\bar{b}\\sim\\mathbb{R}}{\\mathbb{E}}\\left[g_{\\pi}^{\\circ}(\\bar{a}^{\\bar{u}},\\bar{b})\\right]\\right\\|<\\epsilon\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "By the induction hypothesis, there is $N_{6}$ such that for all $n\\geq N_{6}$ with probability at least $1-\\theta$ for every $T^{\\prime}\\in S$ and every $(k+1)$ -tuple of nodes $(\\bar{u},v)$ such that $\\mathcal{N}_{\\ell^{\\prime}}^{G}(\\bar{u},v)\\cong T^{\\prime}$ we have that both: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|e_{\\rho}^{T^{\\prime}}(H_{G}(s_{1}),\\dots,H_{G}(s_{|T^{\\prime}|}))-\\mathbb{I}\\rho(\\bar{u},v)\\mathbb{I}\\right\\|<\\epsilon}\\\\ &{\\left\\|e_{\\eta}^{T^{\\prime}}(H_{G}(s_{1}),\\dots,H_{G}(s_{|T^{\\prime}|}))-\\mathbb{I}\\rho(\\bar{u},v)\\mathbb{I}\\right\\|<\\epsilon}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "and: ", "page_idx": 32}, {"type": "text", "text": "Let $N:=\\operatorname*{max}(N_{1},N_{2},N_{3},N_{4},N_{5},N_{6})$ and take $n\\geq N$ . Then for such $n$ the probability that these six events happen is at least $1-6\\theta$ . We will condition on this. ", "page_idx": 32}, {"type": "text", "text": "Take any $k$ -tuple of nodes $\\bar{u}$ such that $\\mathcal{N}_{\\ell}(\\bar{u})=T$ . It suffices to show, using the definition of the interpretation of the weighted mean operator, that: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\left\\|\\frac{\\sum_{v}\\[\\rho(\\bar{u},v)]h(\\mathbb{I}\\eta(\\bar{u},v)\\mathbb{I}))}{\\sum_{v}h(\\mathbb{I}\\eta(\\bar{u},v)\\mathbb{I})}-\\frac{f_{\\pi}^{T}(H_{G}(u_{1}),\\ldots,H_{G}(u_{k}))}{g_{\\pi}^{T}(H_{G}(u_{1}),\\ldots,H_{G}(u_{k}))}\\right\\|<\\upsilon\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "for some $\\iota$ which we can make arbitrarily small by choosing $\\epsilon$ small enough. ", "page_idx": 32}, {"type": "text", "text": "As above it suffices to find $\\zeta,\\xi>0$ which we can make arbitrarily small and constants $\\nu,\\Omega>0$ such that: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\displaystyle\\frac{1}{n}\\sum_{v}[p(\\bar{u},v)]h([\\eta(\\bar{u},v)])-f_{\\pi}^{T}(H_{G}(s_{1}),\\ldots,H_{G}(s_{\\mathcal{T}}))\\right|\\right|<\\varsigma}\\\\ &{\\quad\\left|\\displaystyle\\frac{1}{n}\\sum_{v}h([\\eta(\\bar{u},v)])-g_{\\pi}^{T}(H_{G}(s_{1}),\\ldots,H_{G}(s_{\\mathcal{T}}))\\right|\\right|<\\xi}\\\\ &{\\qquad\\qquad\\left\\lVert\\displaystyle\\sum_{v}\\sum[p(\\bar{u},v)]h([\\eta(\\bar{u},v)])\\right\\rVert<\\Omega n}\\\\ &{\\qquad\\qquad\\left\\lVert\\displaystyle\\sum_{v}h([\\eta(\\bar{u},v)])\\right\\rVert<\\Omega n}\\\\ &{\\qquad\\qquad\\forall i\\le d:\\left[\\displaystyle\\sum_{v}h([\\eta(\\bar{u},v)])\\right]_{i}>\\nu n}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Equation (8), equation (9) and equation (10) follow as before. We show equation (7), the proof of equation (6) being similar. ", "page_idx": 33}, {"type": "text", "text": "By equation $(\\dagger_{\\eta})$ we have that for every $v$ ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\left\\|h(\\mathbb{[}\\eta(\\bar{u},v)\\mathbb{]})-g_{\\pi}^{\\circ}(\\bar{{a}}^{\\bar{u}},\\bar{{b}}^{(\\bar{u},v)})\\right\\|<\\epsilon\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Hence: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\left\\|\\frac{1}{n}\\sum_{v}h(\\mathbb{[}\\eta(\\bar{u},v)\\mathbb{]})-\\frac{1}{n}\\sum_{v}g_{\\pi}^{\\circ}\\big(\\bar{{\\boldsymbol{a}}}^{\\bar{u}},\\bar{{\\boldsymbol{b}}}^{(\\bar{u},v)}\\big)\\right\\|<\\epsilon\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Now: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\frac{1}{n}\\sum_{v}g_{\\pi}^{\\circ}(\\bar{\\pmb{a}}^{\\bar{\\pi}},\\bar{\\pmb{b}}^{(\\bar{u},v)})=\\frac{1}{n}\\sum_{T^{\\prime}\\in\\mathsf{E x t}(T)}\\sum_{\\underset{N_{\\ell^{\\prime}}^{G}(\\bar{u},v)\\cong T^{\\prime}}{v}}g_{\\pi}^{\\circ}(\\bar{\\pmb{a}}^{\\bar{u}},\\bar{\\pmb{b}}^{(\\bar{u},v)})\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Letting $\\Lambda$ be a bound on the norm of $g_{\\pi}^{\\circ}(\\bar{a}^{\\bar{u}},\\bar{b}^{(\\bar{u},v)})$ , by equation $\\left(\\bigcirc\\right)$ we have that: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\left\\lVert\\frac{1}{n}\\sum_{v}g_{\\pi}^{\\circ}(\\bar{\\pmb{a}}^{\\bar{u}},\\bar{\\pmb{b}}^{(\\bar{u},v)})-\\frac{1}{n}\\sum_{T^{\\prime}\\in S}\\sum_{N_{\\ell^{\\prime}}^{G}(\\bar{u},v)\\cong T^{\\prime}}g_{\\pi}^{\\circ}(\\bar{\\pmb{a}}^{\\bar{u}},\\bar{\\pmb{b}}^{(\\bar{u},v)})\\right\\rVert<\\epsilon\\Lambda\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "By equation $(\\triangle_{g})$ we have that: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\left\\lVert\\frac{1}{n}\\sum_{T^{\\prime}\\in S}\\sum_{N_{\\ell^{\\prime}}^{G}(\\bar{u},v)\\geq T^{\\prime}}g_{\\pi}^{\\circ}(\\bar{a}^{\\bar{u}},\\bar{b}^{(\\bar{u},v)})-\\frac{1}{n}\\sum_{T^{\\prime}\\in S}q_{T|T^{\\prime}}\\operatorname{\\mathbb{E}}_{\\bar{b}\\sim\\mathbb{F}}\\left[g_{\\pi}^{\\circ}(\\bar{a}^{\\bar{u}},\\bar{b})\\right]\\right\\rVert\\leq\\epsilon\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Finally, by equation $\\left(\\bigcirc\\right)$ again we have that: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\left\\|\\frac{1}{n}\\sum_{T^{\\prime}\\in S}q_{T|T^{\\prime}}\\underset{\\bar{\\pmb{b}}\\sim\\mathbb{F}}{\\mathbb{E}}\\left[g_{\\pi}^{\\circ}(\\bar{\\pmb{a}}^{\\bar{\\boldsymbol{u}}},\\bar{\\pmb{b}})\\right]-g_{\\pi}^{T}(\\bar{\\pmb{a}}^{\\bar{\\boldsymbol{u}}})\\right\\|<\\epsilon\\Lambda\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "This completes the proof of the inductive construction of the controllers, thus proving Theorem 5.6. \u53e3 ", "page_idx": 33}, {"type": "text", "text": "Application to prove the final theorem, Theorem 5.1 for the sparse Erdos-Renyi and BarabasiAlbert cases.  To complete the proof, we note that the term $\\tau$ has no free variables, and as a subterm of itself has reach O. The controller $e_{\\tau}^{\\mathcal{B}}$ of is a constant $_{\\textit{z}}$ , and hence by the induction hypothesis applied to it, for every $\\epsilon,\\theta>0$ there is $N\\in\\mathbb N$ such that for all $n\\geq N$ with probability at least $1-\\theta$ we have that: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\|[\\tau]-z\\|<\\epsilon\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "E Proof for the stochastic block model ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "We now prove the convergence result for the stochastic block model. The proof follows the same structure as the density case in Theorem 5.1, except that the notion of graph type is augmented slightly. We therefore only indicate the differences in the proof. ", "page_idx": 33}, {"type": "text", "text": "For this is it helpful to be able to remember the community to which each node belongs. Given any graph $G$ generated by the stochastic block model and node $v$ ,let $C(v)\\,\\in\\,\\{1,\\dots,\\bar{m}\\}$ be the community to which $v$ belongs. ", "page_idx": 33}, {"type": "text", "text": "To prove the result, we first need that the random work positional encodings converge, as in Lemma C.8. In fact they converge to 0. ", "page_idx": 33}, {"type": "text", "text": "Lemma E.1. Let $n_{1},...,n_{m}\\colon\\mathbb{N}\\to\\mathbb{N}$ and $_{P}$ be as in Theorem 5.1. Then for every $k\\,\\in\\,\\mathbb{N}$ and $\\epsilon,\\theta>0$ there is $N\\in\\mathbb{N}$ such that for all $n\\geq N$ with probability at least $1-\\theta_{i}$ for all nodes $v$ we have that: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\|r w_{k}(v)\\|<\\epsilon\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Proof. For each $j\\in\\{1,\\ldots,m\\}$ , let $\\frac{n_{j}}{n}$ converge to $q_{j}$ . Let: ", "page_idx": 34}, {"type": "equation", "text": "$$\nr_{j}:=q_{1}P_{1,j}+\\cdot\\cdot\\cdot+q_{m}P_{m,j}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Note that $n r_{j}$ is the expected degree of a node in community $j$ . By Hoeffding's Inequality (Theorem B.4) and a union bound, there is $N$ such that for all $n\\geq N$ with probability at least $1-\\theta$ , for all $v$ we have that: ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\left|\\frac{d(v)}{n}-r_{C(v)}\\right|<\\epsilon\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Take $n\\geq N$ and condition on this event. Take any node $v$ . When $r_{C(v)}=0$ , the node $v$ has no neighbours, and so $\\mathbf{rw}_{k}(v)=\\mathbf{0}$ ", "page_idx": 34}, {"type": "text", "text": "So assume $r_{C(v)}>0$ .Let $j=C(v)$ : Then as in the proof of Lemma C.8 we can show that the proportion of random walks of length $k$ starting at $v$ is at most: ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\frac{(\\tilde{p}+\\epsilon)^{k-1}}{(\\tilde{p}-\\epsilon)^{k}}n^{-1}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "which converges to O as $n\\to\\infty$ ", "page_idx": 34}, {"type": "text", "text": "We now follow the structure of the proof for the Erdos-R\u00e9nyi dense cases. This time we augment our vocabulary for atomic types with a predicate $P_{j}$ for each community $j$ , so that $P_{j}(v)$ holds if and only if $v$ belongs to community $j$ ", "page_idx": 34}, {"type": "text", "text": "With this we define the community atomic type of a tuple of nodes $\\bar{u}$ in graph, notation $\\mathrm{ComTp}(\\bar{u})$ to be the atomic formulas satisfied by $\\bar{u}$ in the language augmented with the $P_{j}$ predicates. For $k\\in\\mathbb{N}$ let ${\\mathsf{C o m T p}}_{k}$ be the set of all complete community atomic types with $k$ free variables. ", "page_idx": 34}, {"type": "text", "text": "For each $j\\in\\{1,\\ldots,m\\}$ , let $\\frac{n_{j}}{n}$ converge to $q_{j}$ . For any type $t(\\bar{u})$ and $t^{\\prime}(\\bar{u},v)\\in\\mathsf{E x t}(t)$ , let: ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\alpha(t,t^{\\prime}):=q_{C(v)}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Further, for any type $t(\\bar{u})$ , free variable $u_{i}$ in $t$ and $t^{\\prime}(\\bar{u},v)\\in\\mathsf{E x t}_{u_{i}}(t)$ , let: ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\alpha_{u_{i}}(t,t^{\\prime}):=P_{C(u_{i}),C(v)}q_{C(v)}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "For any term $\\pi$ with $k$ free variables, the feature-type controller: ", "page_idx": 34}, {"type": "equation", "text": "$$\ne_{\\pi}\\colon([0,1]^{d})^{k}\\times{\\mathsf{C o m T p}}_{k}\\to[0,1]^{d}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "is defined exactly as as in the proof of the density case of Theorem 5.1, using the extension proportions $\\alpha(t,t^{\\prime})$ and $\\alpha_{u_{i}}(t,t^{\\prime})$ just defined. ", "page_idx": 34}, {"type": "text", "text": "We show by induction that every $\\epsilon,\\delta,\\theta>0$ and $\\wedge$ -desc $\\psi(\\bar{u})$ , there is $N\\in\\mathbb N$ such that for all $n\\geq N$ with probability at least $1-\\theta$ in the space of graphs of size $n$ , out of all the tuples $\\bar{u}$ such that $\\psi(\\bar{u})$ at least $1-\\delta$ satisfy: ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\|e_{\\pi}(H_{G}(\\bar{u}),\\mathrm{GrTp}(\\bar{u}))-[\\pi(\\bar{u})]\\|<\\epsilon\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "We then proceed as in the proof of the non-sparse Erdos-R\u00e9nyi cases of Theorem 5.1. The only difference is that when showing equation $(\\triangle_{f})$ and equation $(\\triangle_{g})$ we use the fact that the expected proportion of type extensions $t^{\\bar{\\prime}}(\\bar{u},v)$ of a type $t(\\bar{u})$ at a node $u_{j}$ is $\\alpha(t,t^{\\prime})$ ", "page_idx": 34}, {"type": "text", "text": "F  Additional experiments ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "In this section we provide additional experiments using MeanGNN, GCN [30], GAT [49], and $\\mathrm{GPS+RW}$ with random walks of length up to 5, over the distributions $\\mathrm{ER}(n,p(n)~=~0.1)$ $\\begin{array}{r}{\\mathrm{ER}(n,p(n)\\ =\\ \\frac{\\log n}{n})}\\end{array}$ \uff0ci $\\begin{array}{r}{\\mathrm{ER}(n,p(n)~=~\\frac{1}{50n})}\\end{array}$ $\\mathrm{BA}(n,m\\;=\\;5)$   \nSBM of i0 communities with equal size, where an edge between nodes within the same community is included with probability 0.7 and an edge between nodes of different communities is included with probability 0.1. ", "page_idx": 34}, {"type": "text", "text": "Setup. Our setup is carefully designed to eliminate confounding factors: ", "page_idx": 34}, {"type": "image", "img_path": "Dn68qdfTry/tmp/c8daa989b5a21f8f4d35472594cfd016c4cb4a2235cf45e46ea52de2ea1c3773.jpg", "img_caption": [], "img_footnote": [], "page_idx": 35}, {"type": "image", "img_path": "", "img_caption": ["Figure 6: The 5 class probabilities (in different colours) of a MEANGNN, GCN, GAT and $\\mathrm{GPS+RW}$ model initialization over the $\\mathrm{ER}(n,p(n)=0.1)$ graph distributions, as we draw increasingly larger graphs. "], "img_footnote": [], "page_idx": 35}, {"type": "text", "text": "\u00b7 We consider 5 models with the same architecture, each having randomly initialized weights, utilizing a ReLU non-linearity, and applying a softmax function to their outputs. Each model uses a hidden dimension of 128, 3 layers and an output dimension of 5. \u00b7 We draw graphs of sizes up to 10,000, where we take 100 samples of each graph size. Node features are independently drawn from $U[0,1]$ and the initial feature dimension is 128. ", "page_idx": 35}, {"type": "text", "text": "Further details are available in the experiments repository at https://github.com/ benfinkelshtein/GNN-Asymptotically-Constant. ", "page_idx": 35}, {"type": "text", "text": "Much like in Section 6, the convergence of class probabilities is apparent across all models and graph distributions, in accordance with our main theorems (Q1). We again observe that attention-based models such as GAT and $\\mathrm{GPS+RW}$ exhibit delayed convergence and greater standard deviation in comparison to MeanGNN and GCN, which further strengthening our previous conclusions. ", "page_idx": 35}, {"type": "text", "text": "G Acknowledgements ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "This research was funded in whole or in part by EPSRC grant EP/T022124/1. For the purpose of Open Access, the authors have applied a CC BY public copyright license to any Author Accepted Manuscript (AAM) version arising from this submission. ", "page_idx": 35}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: We thoroughly investigate the phenomenon of a.a.s. convergence of GNNs, both from a theoretical and experimental point of view, as claimed in the abstract. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "image", "img_path": "Dn68qdfTry/tmp/d10ed87431797e48456db95ae25cfed7f497d617580b35bb11b36a8b0e8016a0.jpg", "img_caption": ["Figure 7: The 5 class probabilities (in different colours) of a MEANGNN, GCN, GAT and $\\mathrm{GPS+RW}$ model initialization over the $\\begin{array}{r}{\\mathrm{ER}(n,p(n)=\\frac{\\log n}{n})}\\end{array}$ graph distributions, as we draw increasingly larger graphs. "], "img_footnote": [], "page_idx": 36}, {"type": "image", "img_path": "Dn68qdfTry/tmp/8df4afc78b4eaf046126116b2ca86bbb2da583843506aa67e21ff5ddbb2dcb60.jpg", "img_caption": ["Figure 8: The 5 class probabilities (in different colours) of a MEANGNN, GCN, GAT and $\\mathrm{GPS+RW}$ model initialization over the $\\begin{array}{r}{\\mathrm{ER}(n,p(n)=\\frac{1}{50n})}\\end{array}$ graph distributions, as we draw increasingly larger graphs. "], "img_footnote": [], "page_idx": 36}, {"type": "image", "img_path": "Dn68qdfTry/tmp/cbc6783409e2240c9f9c8592279a306eabe46252f04fc7416419c9eada95f24b.jpg", "img_caption": ["Figure 9: The 5 class probabilities (in different colours) of a MEANGNN, GCN, GAT and $\\mathrm{GPS+RW}$ model initialization over the SBM graph distributions, as we draw increasingly larger graphs. "], "img_footnote": [], "page_idx": 37}, {"type": "image", "img_path": "Dn68qdfTry/tmp/059cb6236a3841b8063da01050e71d3372c40a296a0205531f75cb33771e2866.jpg", "img_caption": ["Figure 10: The 5 class probabilities (in different colours) of a MEANGNN, GCN, GAT and $\\mathrm{GPS+RW}$ model initialization over the $\\mathrm{BA}(n,m\\,=\\,5)$ graph distributions, as we draw increasingly larger graphs. "], "img_footnote": [], "page_idx": 37}, {"type": "image", "img_path": "Dn68qdfTry/tmp/54f72cb80051bbca93f19eb3932d63c727aa0072c5d84ade9193750dfbf5113f.jpg", "img_caption": ["Figure 11: A three-layer GCN with hidden dimension 128 is trained on the ENZYMES dataset with one class removed so that there are five output classes. This model is then run on graphs drawn from $\\mathrm{ER}(n,p(n)=0.1)$ for increasingsizes $n$ , and the mean output probabilities are recorded, along with standarddeviation "], "img_footnote": [], "page_idx": 38}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and refect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 38}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer:[Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: We discuss limitations in the discussion section: in particular we note that a.a.s. convergence does not apply universally to every GNN, and it does not apply for arbitrary instantiations of popular random graph models (like Erdos Renyi), regardless of the controlling parameters. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\"\u2019 section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications wouldbe.   \n\u00b7 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational effciency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 38}, {"type": "text", "text": "", "page_idx": 39}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Justification: full proofs are provided in the appendix to the paper. Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 39}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Justification: we detail the experimental setings and datasets in the paper, and provide a link to a github with information about how to reproduce the experiments. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). ", "page_idx": 39}, {"type": "text", "text": "(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 40}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 40}, {"type": "text", "text": "Justification: all of the necessary information is available in the GitHub repository. Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please see the NeurIPS code and data submission guidelines (https : //nips . cc/public/ guides/CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips. cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 40}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: we provide all the necessary information on the datasets and the models used (our submission does not deal with training). ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.   \n\u00b7 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 40}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: we provide envelope error regions for Figure 3. For the figures plotting standard deviations across graph sizes, we judged that it would be clearer to simply plot all datapoints, rather than take the mean (otherwise we would need to plot the standard deviation of standard deviations). ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 40}, {"type": "text", "text": "", "page_idx": 41}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 41}, {"type": "text", "text": "Justification: we provide information on the resources we used, and for replication these could be utilized. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it intothepaper). ", "page_idx": 41}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Justification: we have reviewed the ethics code and are confident that our paper conforms to it. Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 41}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 41}, {"type": "text", "text": "Justification: this is a theoretically-oriented paper on convergence properties of graph neural networks. There is no direct path to negative applications. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u00b7 The answer NA means that there is no societal impact of the work performed. \u00b7 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 41}, {"type": "text", "text": "\u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u00b7 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 42}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 42}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 42}, {"type": "text", "text": "Justification: we do not provide new datasets or models in this work. Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u00b7 The answer NA means that the paper poses no such risks.   \n\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 42}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 42}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Justification: we cite the public datasets which we use. Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not use existing assets.   \n\u00b7 The authors should cite the original paper that produced the code package or dataset.   \n\u00b7 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset's creators. ", "page_idx": 42}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 43}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 43}, {"type": "text", "text": "Justification: this theoretically-oriented paper does not provide new assets. Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not release new assets.   \n\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u00b7 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 43}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 43}, {"type": "text", "text": "Answer: [NA] . ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Justification: there are no crowdsourcing experiments in our submission. Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 43}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 43}, {"type": "text", "text": "Answer: [NA] .   \nJustification: no human subjects were used. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 43}]