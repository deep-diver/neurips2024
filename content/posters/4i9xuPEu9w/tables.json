[{"figure_path": "4i9xuPEu9w/tables/tables_4_1.jpg", "caption": "Table 7: p-values of different methods (each has 10 random trials) against BECAUSE in various environments. Under the significance level 0.05, we mark all the baseline results that are significantly lower than BECAUSE as green, and the rest as red. We can see that BECAUSE significantly outperforms 10 baselines in 18 tasks in 91.1% of the experiments (164 out of total 180 pairs of experiments).", "description": "This table presents the statistical significance tests comparing the performance of the proposed BECAUSE method against 10 baseline methods across 18 different tasks.  A p-value is calculated for each comparison, indicating the probability of observing the results if there were no true difference between BECAUSE and the baseline.  P-values below 0.05 are considered statistically significant and highlighted in green, indicating that BECAUSE outperforms the baseline.", "section": "4.2 Experiment Results Analysis"}, {"figure_path": "4i9xuPEu9w/tables/tables_7_1.jpg", "caption": "Table 1: Comparison of visual RL performance.", "description": "This table compares the performance of ICIL, IFactor, and BECAUSE on visual RL tasks in the Unlock environment.  It shows the average success rate for each method across different data quality levels (random, medium, expert) and in-distribution (I) and out-of-distribution (O) settings.  BECAUSE consistently demonstrates significantly higher success rates than the baselines, highlighting its superior generalization capabilities in visual scenarios.", "section": "4 Experiment Results"}, {"figure_path": "4i9xuPEu9w/tables/tables_8_1.jpg", "caption": "Table 2: The ablation studies between BECAUSE and its variants. We report the overall Success rate (%) over 9 in-distribution (I) and 9 out-of-distribution (O) tasks, respectively. Bold is the best.", "description": "This table presents the results of ablation studies comparing the performance of the main proposed method, BECAUSE, against three variants: Optimism, Linear, and Full.  The \"Overall-I\" column shows the average success rate across nine in-distribution tasks (tasks where the testing environment matches the training environment), while \"Overall-O\" represents the average success rate across nine out-of-distribution tasks (where the testing environment differs from the training one).  The bolded values indicate the best performance for each scenario. This demonstrates the importance of each component in the BECAUSE framework for achieving superior performance.", "section": "Ablation Studies"}, {"figure_path": "4i9xuPEu9w/tables/tables_16_1.jpg", "caption": "Table 3: Notations used in this paper and their corresponding meanings.", "description": "This table lists notations used in the paper and their corresponding meanings.  It provides a key to understanding the mathematical symbols and abbreviations used throughout the paper, clarifying the meaning of variables, functions, sets, and other mathematical objects. The table is essential for anyone attempting to reproduce the results or understand the algorithms presented in the paper.", "section": "A.1 Notation Summary"}, {"figure_path": "4i9xuPEu9w/tables/tables_17_1.jpg", "caption": "Table 7: p-values of different methods (each has 10 random trials) against BECAUSE in various environments. Under the significance level 0.05, we mark all the baseline results that are significantly lower than BECAUSE as green, and the rest as red. We can see that BECAUSE significantly outperforms 10 baselines in 18 tasks in 91.1% of the experiments (164 out of total 180 pairs of experiments).", "description": "This table presents the statistical significance tests comparing the proposed BECAUSE method against ten baseline methods across eighteen tasks.  The p-values indicate whether the performance difference between BECAUSE and each baseline is statistically significant (p < 0.05). Green cells highlight statistically significant outperformance by BECAUSE, while red cells indicate no significant difference or better performance by the baseline.", "section": "4.2 Experiment Results Analysis"}, {"figure_path": "4i9xuPEu9w/tables/tables_29_1.jpg", "caption": "Table 5: The comparison results of the p-value and the W\u2081 distance (\u00d710\u207b\u2074) between MOPO and BECAUSE. Bold means the better.", "description": "This table compares the performance of the proposed BECAUSE method against the baseline MOPO method in terms of two metrics: the p-value from Mann-Whitney U test and the Wasserstein-1 distance (W\u2081).  Both metrics assess the difference in the distribution of model loss between successful and unsuccessful trajectory samples. Lower p-values indicate a more significant difference (better performance for BECAUSE), and higher W\u2081 distance indicates a larger difference between the distributions (again, better for BECAUSE).  The results are presented for three different scenarios (Unlock-Expert, Unlock-Medium, Unlock-Random) with varying data quality and spurious correlations.", "section": "4.3 Ablation Studies"}, {"figure_path": "4i9xuPEu9w/tables/tables_29_2.jpg", "caption": "Table 6: Success rate (%) for 18 tasks in three different environments. We evaluate the mean and 95% confidence interval given by the t-test of the best performance among 10 random seeds, as well as the p-value between the overall performance. Bold is the best.", "description": "This table presents the average success rate of different offline reinforcement learning algorithms across 18 tasks categorized into three environments (Lift, Unlock, Crash).  Each environment has three variations (Random, Medium, Expert) reflecting different data qualities, resulting in 18 tasks.  The table shows the mean and 95% confidence interval of the success rates for each algorithm and task, calculated across 10 random seeds.  Bold values indicate the best performing algorithm for each task.  The p-values are provided to indicate the statistical significance of the differences in performance compared to the best algorithm.", "section": "4 Experiment Results"}, {"figure_path": "4i9xuPEu9w/tables/tables_30_1.jpg", "caption": "Table 7: p-values of different methods (each has 10 random trials) against BECAUSE in various environments. Under the significance level 0.05, we mark all the baseline results that are significantly lower than BECAUSE as green, and the rest as red. We can see that BECAUSE significantly outperforms 10 baselines in 18 tasks in 91.1% of the experiments (164 out of total 180 pairs of experiments).", "description": "This table presents the statistical significance tests comparing the performance of the proposed BECAUSE method against 10 baseline methods across 18 different tasks.  Each task is evaluated using 10 different random seeds, resulting in 180 total comparisons. A p-value is calculated for each comparison, indicating whether the performance of BECAUSE is significantly better than the baseline method. The table visually highlights the significant differences (p<0.05) using color-coding.", "section": "4.2 Experiment Results Analysis"}, {"figure_path": "4i9xuPEu9w/tables/tables_30_2.jpg", "caption": "Table 6: Success rate (%) for 18 tasks in three different environments. We evaluate the mean and 95% confidence interval given by the t-test of the best performance among 10 random seeds, as well as the p-value between the overall performance. Bold is the best.", "description": "This table presents the average success rates for 18 different reinforcement learning tasks across three environments (Lift, Unlock, Crash), categorized by data quality (random, medium, expert).  The results are averaged over 10 trials with 95% confidence intervals and p-values to compare the performance of the BECAUSE algorithm against baselines.  Bold values indicate the best-performing algorithm for each task.", "section": "4.2 Experiment Results Analysis"}, {"figure_path": "4i9xuPEu9w/tables/tables_31_1.jpg", "caption": "Table 7: p-values of different methods (each has 10 random trials) against BECAUSE in various environments. Under the significance level 0.05, we mark all the baseline results that are significantly lower than BECAUSE as green, and the rest as red. We can see that BECAUSE significantly outperforms 10 baselines in 18 tasks in 91.1% of the experiments (164 out of total 180 pairs of experiments).", "description": "This table presents the p-values resulting from statistical significance tests comparing BECAUSE against 10 baseline methods across 18 different tasks.  A p-value less than 0.05 indicates a statistically significant difference, with green highlighting indicating that BECAUSE outperformed the baseline. The table demonstrates BECAUSE's superior performance across a wide range of scenarios.", "section": "4.2 Experiment Results Analysis"}, {"figure_path": "4i9xuPEu9w/tables/tables_32_1.jpg", "caption": "Table 10: Environment configurations used in experiments", "description": "This table summarizes the key parameters and characteristics of the three different reinforcement learning environments (Lift, Unlock, Crash) used in the experiments described in the paper.  It shows the maximum number of steps allowed per episode, the dimensionality of the state and action spaces, the type of actions (hybrid, discrete, or hybrid), and the intrinsic rank of the state and action spaces.  This information provides context for understanding the complexity and nature of the tasks faced by the reinforcement learning agents.", "section": "4.1 Experiment Setting"}, {"figure_path": "4i9xuPEu9w/tables/tables_32_2.jpg", "caption": "Table 11: Bahavior policies used to collect offline data in different environments.", "description": "This table shows the behavior policies used to generate offline datasets for three different environments: Lift, Unlock, and Crash.  For each environment, three types of behavior policies were used: Random, Medium, and Expert.  The table lists the number of episodes collected for each policy type and the resulting success rate.  The \"Additional Description\" column provides qualitative details on the characteristics of each policy type. This information is crucial for understanding the quality and characteristics of the data used to train the reinforcement learning models in the paper.", "section": "Additional Baseline Information"}, {"figure_path": "4i9xuPEu9w/tables/tables_33_1.jpg", "caption": "Table 12: Hyper-parameters of models used in experiments of BECAUSE and baselines (Part I)", "description": "This table lists the hyperparameters used for training the BECAUSE model and several baseline models across three different reinforcement learning environments: Lift, Unlock, and Crash.  The hyperparameters are categorized by model (BECAUSE, MOPO, CDL, GNN) and parameter type (learning rate, data size, batch size, planning horizon, reward discount, regularization parameters, network architecture specifics etc.).  The table shows that different models required different hyperparameter settings for optimal performance in each environment.", "section": "C.6 Additional Baseline Information"}, {"figure_path": "4i9xuPEu9w/tables/tables_34_1.jpg", "caption": "Table 12: Hyper-parameters of models used in experiments of BECAUSE and baselines (Part I)", "description": "This table lists the hyperparameters used for the BECAUSE model and several baseline models in the experiments described in the paper.  It includes parameters related to training, the planning process, and other model-specific settings. The table is divided into sections for each model, making it easy to compare the different configurations used.", "section": "C.6 Additional Baseline Information"}]