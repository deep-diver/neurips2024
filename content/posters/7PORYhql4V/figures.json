[{"figure_path": "7PORYhql4V/figures/figures_1_1.jpg", "caption": "Figure 1: A synthetic illustration of the distribution of the directional gradients of stochastically optimized models of the same input data. The subfigures demonstrate (a) an intuitive, stochastic scenario, where the distributions of different model families are not closely dependent, and (b) the converging distribution trend introduced by our hypothesis. Different colors represent different model families, and points represent different optimized models.", "description": "This figure provides a synthetic illustration to visualize the hypothesis of the paper regarding the distribution of directional gradients (input salience) of stochastically optimized models.  Panel (a) shows a scenario where distributions for different model families are independent, while panel (b) shows the converging trend hypothesized in the paper as model capacity increases.  Different colors represent different model families, and each point represents a different optimized model.", "section": "2 The Convergence of Input Saliency"}, {"figure_path": "7PORYhql4V/figures/figures_2_1.jpg", "caption": "Figure 2: The individual similarity pind(f(1), f(2)) = Ex\u2208x[CosSim(\u2207\u00e6 f(1)(x), \u2207\u00e6 f(2)(x))], where f(1) \u2208 F(k1), f(2) \u2208 F(k2). CIFAR-10/100 and CNN & ResNets are tested here.", "description": "This figure shows the individual similarity between input salience of two models f(1) and f(2) which belong to model families with different capacities (k1 and k2). The similarity is measured using cosine similarity of input gradients over the test dataset. The figure presents results for CIFAR-10 and CIFAR-100 datasets, with both CNN and ResNet model architectures.", "section": "2.1 Salience Similarities"}, {"figure_path": "7PORYhql4V/figures/figures_2_2.jpg", "caption": "Figure 2: The individual similarity pind(f(1), f(2)) = Ex\u2208x[CosSim(\u2207\u00e6 f(1)(x), \u2207\u00e6 f(2)(x))], where f(1) \u2208 F(k1), f(2) \u2208 F(k2). CIFAR-10/100 and CNN & ResNets are tested here.", "description": "This figure displays the individual similarity between input salience of two models with varying capacities (k1 and k2).  The similarity is measured using cosine similarity of input gradients across the test set.  Results are shown for CIFAR-10 and CIFAR-100 datasets, using both CNN and ResNet model architectures.", "section": "2.1 Salience Similarities"}, {"figure_path": "7PORYhql4V/figures/figures_5_1.jpg", "caption": "Figure 4: (a) presents an illustration of H1 and H2. Blue and green caps represent u\u2081 \u2208 Gk\u2081(x) (green) and u\u2082 \u2208 Gk\u2082(x) (blue) regions 2. H1: larger ks lead to smaller spherical variances; H2: the mean directions are extremely similar. (b) illustrates (left) the decomposition of u to the mean direction and the orthogonal direction; and (right) the marginalization of the distribution from u to t.", "description": "This figure provides a visualization of the two hypotheses (H1 and H2) proposed in the paper regarding the distribution of input salience in deep neural networks (DNNs).  Panel (a) shows how the distributions of input gradient directions for different model families (colors) converge towards a common mean direction as model capacity (k) increases. This illustrates both H1 (increasing capacity leads to smaller spherical variance) and H2 (mean directions align across families). Panel (b) breaks down the mathematical representation of the process. The left side shows how a gradient vector (u) can be decomposed into a component along the mean direction (t\u03bc(x)) and a component orthogonal to it (\u221a1\u2212t\u00b2\u03bc(x)\u22a5). The right side illustrates the marginalization process focusing on the distribution of t, the projection of u onto the mean direction.", "section": "The Directional Distribution of Gradients"}, {"figure_path": "7PORYhql4V/figures/figures_6_1.jpg", "caption": "Figure 1: A synthetic illustration of the distribution of the directional gradients of stochastically optimized models of the same input data. The subfigures demonstrate (a) an intuitive, stochastic scenario, where the distributions of different model families are not closely dependent, and (b) the converging distribution trend introduced by our hypothesis. Different colors represent different model families, and points represent different optimized models.", "description": "This figure provides a synthetic visualization to illustrate the hypothesis of the paper regarding the distribution of input salience maps of stochastically trained models.  (a) shows a random distribution where model families (represented by colors) are independent. (b) illustrates the hypothesized convergent trend where model families align as capacity increases, showing a shared population mean direction.", "section": "2 The Convergence of Input Saliency"}, {"figure_path": "7PORYhql4V/figures/figures_6_2.jpg", "caption": "Figure 2: The individual similarity pind(f(1), f(2)) = Ex\u2208x[CosSim(\u2207\u00e6 f(1)(x), \u2207\u00e6 f(2)(x))], where f(1) \u2208 F(k1), f(2) \u2208 F(k2). CIFAR-10/100 and CNN & ResNets are tested here.", "description": "This figure shows the individual similarity between input salience maps of two stochastically optimized models with varying capacities (k1 and k2).  The similarity is measured using the cosine similarity of the input gradients. Experiments are performed on CIFAR-10 and CIFAR-100 datasets, using both CNN and ResNet model architectures.  The heatmaps illustrate how similarity changes depending on model capacity.", "section": "2.1 Salience Similarities"}, {"figure_path": "7PORYhql4V/figures/figures_7_1.jpg", "caption": "Figure 8: (left) The illustration of the frequency of the mixture Tk, where k \u2208 {10, 20, 40, 80, 160}. Specifically, the black histogram represents the distribution Porigin. The dashed curves are the approximated PDF pk obtained by KDE. The results are generated using CNNSmall and CIFAR-10. (right) The illustration of log pk, which is linearly related to log \u03c8k.", "description": "This figure shows the empirical results that support Hypothesis I and Hypothesis II. The left panel shows the estimated probability density functions (PDFs) of the marginal distribution of the cosine similarity between input saliency maps (denoted as t) for different model widths (k). The black histogram represents the PDF of uniformly distributed data on a hypersphere. The colored curves represent the estimated PDFs of t for varying k, which are approximated using kernel density estimation (KDE). The right panel shows the relationship between log(pk) and log(\u03c8k), which demonstrates the relationship between the estimated PDF of t and the shape function of the Saw distribution.", "section": "The Directional Distribution of Gradients"}, {"figure_path": "7PORYhql4V/figures/figures_7_2.jpg", "caption": "Figure 7: The marginal distribution of t of the first test sample of CIFAR-10. Red dashed lines partition the range of t every 10 percent of the frequency.", "description": "This figure shows the marginal distribution of t, where t is the cosine similarity between the gradient direction of a model and the mean gradient direction.  The distribution is shown for the first test sample of the CIFAR-10 dataset. Red dashed lines divide the distribution into deciles (10% increments) to show the density in different regions of the distribution.", "section": "2.3 The Directional Distribution of Gradients"}, {"figure_path": "7PORYhql4V/figures/figures_8_1.jpg", "caption": "Figure 2: The individual similarity pind(f(1), f(2)) = Ex\u2208x[CosSim(\u2207\u00e6 f(1)(x), \u2207\u00e6 f(2)(x))], where f(1) \u2208 F(k1), f(2) \u2208 F(k2). CIFAR-10/100 and CNN & ResNets are tested here.", "description": "This figure shows the individual similarity between input salience of two models, f(1) and f(2), with varying capacities (k1 and k2).  The similarity is measured using the cosine similarity of their input gradients across the entire test set. The results are shown for CIFAR-10 and CIFAR-100 datasets, using both CNN and ResNet architectures. The figure aims to visually demonstrate the increasing trend of similarity as model capacities (k1 and k2) increase.", "section": "2.1 Salience Similarities"}, {"figure_path": "7PORYhql4V/figures/figures_8_2.jpg", "caption": "Figure 11: The comparison between the single-model attack from the largest model (red), the single-model attack from the very same capacity (green) and the attack by the mean direction (blue).", "description": "This figure compares the effectiveness of three different black-box attack strategies. The first uses the gradients from the largest model in the same model family as the target model (red). The second uses the gradients from a model with the same capacity as the target model (green). The third utilizes the mean gradient direction across all model families (blue). The results are presented as a prediction decay ratio (\u03b1) plotted against the width of the target model. This helps visualize how effectively each attack strategy influences the performance of the target model.", "section": "4.2 Black-Box Attack via Saliency Similarity"}, {"figure_path": "7PORYhql4V/figures/figures_14_1.jpg", "caption": "Figure 1: A synthetic illustration of the distribution of the directional gradients of stochastically optimized models of the same input data. The subfigures demonstrate (a) an intuitive, stochastic scenario, where the distributions of different model families are not closely dependent, and (b) the converging distribution trend introduced by our hypothesis. Different colors represent different model families, and points represent different optimized models.", "description": "This figure illustrates the difference between a random distribution of model gradients (a) and a converging distribution (b), which is a key hypothesis of the paper.  The converging trend shows that models with increasing capacity tend to align in terms of their input salience, even if trained stochastically.", "section": "2 The Convergence of Input Saliency"}, {"figure_path": "7PORYhql4V/figures/figures_15_1.jpg", "caption": "Figure 2: The individual similarity pind(f(1), f(2)) = Ex\u2208x[CosSim(\u2207\u00e6 f(1)(x), \u2207\u00e6 f(2)(x))], where f(1) \u2208 F(k1), f(2) \u2208 F(k2). CIFAR-10/100 and CNN & ResNets are tested here.", "description": "This figure shows the individual similarity between input salience of two models f(1) and f(2) from different families F(k1) and F(k2) with varying capacities k1 and k2.  The similarity is measured using cosine similarity of input gradients.  The experiments were conducted on CIFAR-10 and CIFAR-100 datasets using CNN and ResNet model architectures.", "section": "2 The Convergence of Input Saliency"}, {"figure_path": "7PORYhql4V/figures/figures_16_1.jpg", "caption": "Figure 12: The expected similarity p(k1,k2) between models of varying widths k1,k2. Here we include CNNSmall, CNNLarge, ResNetSmall, and ResNetLarge as F. The values of k1, k2 determine the widths in each layer. Here the datasets are CIFAR-10 (top), CIFAR-100 (middle) and tinyImagenet (bottom).", "description": "This figure shows heatmaps representing the expected cosine similarity between input salience maps of two models with different widths (k1 and k2).  The models tested belong to four different families (CNNSmall, CNNLarge, ResNetSmall, ResNetLarge), and the experiment is repeated across three different datasets (CIFAR-10, CIFAR-100, TinyImagenet). Each heatmap cell represents the expected similarity between models from families with widths k1 and k2.  The color intensity indicates the degree of similarity (higher intensity means higher similarity). This figure visually demonstrates the hypothesis that the similarity between models increases as the model width (capacity) increases, and this behavior is consistent across different model architectures and datasets.", "section": "2.1 Salience Similarities"}, {"figure_path": "7PORYhql4V/figures/figures_17_1.jpg", "caption": "Figure 1: A synthetic illustration of the distribution of the directional gradients of stochastically optimized models of the same input data. The subfigures demonstrate (a) an intuitive, stochastic scenario, where the distributions of different model families are not closely dependent, and (b) the converging distribution trend introduced by our hypothesis. Different colors represent different model families, and points represent different optimized models.", "description": "This figure illustrates two scenarios of the directional gradient distribution of stochastically optimized models.  (a) shows a random distribution where different model families (represented by different colors) are not closely related. (b) shows a convergent distribution, where model families align more as model capacity increases, supporting the hypothesis of the paper.", "section": "2 The Convergence of Input Saliency"}, {"figure_path": "7PORYhql4V/figures/figures_17_2.jpg", "caption": "Figure 16: The # of trainable parameters of models vs. the width parameter k for each architecture.", "description": "This figure shows the number of trainable parameters for different model architectures (CNN Small, CNN Large, ResNet Small, ResNet Large) as a function of the width parameter k.  It illustrates how the model size dramatically increases with increasing k, especially for the larger ResNet models.", "section": "B Experiment Details"}, {"figure_path": "7PORYhql4V/figures/figures_19_1.jpg", "caption": "Figure 1: A synthetic illustration of the distribution of the directional gradients of stochastically optimized models of the same input data. The subfigures demonstrate (a) an intuitive, stochastic scenario, where the distributions of different model families are not closely dependent, and (b) the converging distribution trend introduced by our hypothesis. Different colors represent different model families, and points represent different optimized models.", "description": "This figure illustrates two scenarios of the distribution of directional gradients of stochastically optimized models. The left subfigure shows an intuitive scenario where distributions of different model families are independent, while the right shows the converging distribution trend hypothesized in the paper. Each color represents a different model family, and each point represents a different optimized model.", "section": "2 The Convergence of Input Saliency"}, {"figure_path": "7PORYhql4V/figures/figures_20_1.jpg", "caption": "Figure 1: A synthetic illustration of the distribution of the directional gradients of stochastically optimized models of the same input data. The subfigures demonstrate (a) an intuitive, stochastic scenario, where the distributions of different model families are not closely dependent, and (b) the converging distribution trend introduced by our hypothesis. Different colors represent different model families, and points represent different optimized models.", "description": "This figure provides a synthetic illustration to visualize the hypothesis of the paper regarding the distribution of input salience maps. (a) shows a random distribution where different model families (different colors) are not closely related, and (b) illustrates a convergent distribution where different model families tend to align in terms of their population mean directions as model capacity increases. This illustrates the paper's main hypothesis about the universal convergence trend of input salience.", "section": "2 The Convergence of Input Saliency"}, {"figure_path": "7PORYhql4V/figures/figures_20_2.jpg", "caption": "Figure 1: A synthetic illustration of the distribution of the directional gradients of stochastically optimized models of the same input data. The subfigures demonstrate (a) an intuitive, stochastic scenario, where the distributions of different model families are not closely dependent, and (b) the converging distribution trend introduced by our hypothesis. Different colors represent different model families, and points represent different optimized models.", "description": "This figure uses synthetic data to illustrate two scenarios of the distribution of directional gradients from stochastically optimized models.  (a) shows a typical stochastic scenario where models from different families (represented by different colors) have independent distributions. (b) illustrates the authors' hypothesis that distributions converge towards their shared population mean as model capacity increases.", "section": "2 The Convergence of Input Saliency"}, {"figure_path": "7PORYhql4V/figures/figures_20_3.jpg", "caption": "Figure 11: The comparison between the single-model attack from the largest model (red), the single-model attack from the very same capacity (green) and the attack by the mean direction (blue).", "description": "This figure compares three different attack strategies on deep neural networks (DNNs):\n\n1.  **Single-model attack from the largest model (red):**  This uses the gradients from the largest model (highest capacity) in the model family to generate adversarial examples.\n2.  **Single-model attack from the same capacity (green):** This uses gradients from a model with the same capacity as the target model to generate adversarial examples. This is a control to isolate the effect of the capacity differences on the attack.\n3.  **Attack by the mean direction (blue):** This attack leverages the average of gradient directions across multiple models of the same capacity. This explores if averaging gradient directions improves the attack's transferability, and aims to test the impact of the hypothesis that gradient directions across models converge as model capacity increases.\n\nThe x-axis represents the capacity of the target model being attacked, and the y-axis represents the success rate of the attack (lower values indicate more successful attacks).\n\nThe figure aims to demonstrate the effectiveness of using the mean gradient direction for black-box attacks, showing that it significantly outperforms attacks generated from individual models.  This supports the paper's hypothesis about the convergence of gradient directions across model families with increasing capacity.", "section": "4 Applications of Hypotheses"}, {"figure_path": "7PORYhql4V/figures/figures_20_4.jpg", "caption": "Figure 18: (a) and (b) illustrate the cosine similarity between (a) CNNs and (b) ResNets with different batch sizes in {64, 128, 256, 512}. (c) shows the loss and (d) shows the accuracy of trained models.", "description": "This figure shows the cosine similarity between CNNs and ResNets with different batch sizes.  The cosine similarity is a measure of the similarity between the input salience maps of two models. As can be seen, the cosine similarity increases as the batch size increases. The figure also shows the training loss and accuracy for the models with different batch sizes. The loss decreases as the batch size increases, while the accuracy increases.", "section": "3.2 The Decreasing Spherical Variance with k (H1)"}, {"figure_path": "7PORYhql4V/figures/figures_21_1.jpg", "caption": "Figure 21: The cosine similarity between Vision Transformers (ViTs) on CIFAR-10. The capacity is controlled by k \u2208 {10, 20, 40, 80}, where the embedding dimension is 4k, separated to k/2 heads. The left subfigure shows the mean of the similarity. The right subfigure shows the similarity of the population mean.", "description": "This figure shows the results of experiments on Vision Transformers (ViTs) trained on CIFAR-10 dataset.  The model capacity is varied by changing the embedding dimension (4k), which is separated into k/2 heads. The figure consists of two heatmaps. The left heatmap displays the average cosine similarity between pairs of models with different capacities (k1 and k2). The right heatmap shows the cosine similarity between the mean salience directions of models with different capacities.", "section": "2.2 The Spherical Distribution of the Salience"}, {"figure_path": "7PORYhql4V/figures/figures_23_1.jpg", "caption": "Figure 22: The relation between the probability P(Z > p) and the cosine similarity value p.", "description": "This figure shows the cumulative distribution function (CDF) of the cosine similarity between two independent, high-dimensional Gaussian vectors for different dimensions (d = 3, 192, 3072).  It demonstrates how the probability of observing a high cosine similarity decreases dramatically as the dimensionality increases.  In high-dimensional spaces, the cosine similarity is highly concentrated around zero, indicating a low probability of randomly sampled vectors having large cosine similarity. This result is important because the cosine similarity is directly related to the study's measure of input salience similarities.", "section": "D Uniform Distributions on the Hypersphere"}, {"figure_path": "7PORYhql4V/figures/figures_24_1.jpg", "caption": "Figure 2: The individual similarity pind(f(1), f(2)) = Ex\u2208x[CosSim(\u2207\u00e6 f(1)(x), \u2207\u00e6 f(2)(x))], where f(1) \u2208 F(k1), f(2) \u2208 F(k2). CIFAR-10/100 and CNN & ResNets are tested here.", "description": "This figure shows the individual similarity between input salience of two stochastically optimized models, f(1) and f(2), which belong to model families F(k1) and F(k2) respectively, with varying capacities k1 and k2.  The similarity is calculated as the cosine similarity of input gradients averaged over the entire test set. The experiment is performed using CIFAR-10 and CIFAR-100 datasets and two different model architectures: CNN and ResNet. The figure aims to illustrate how the similarity of input salience between two models changes with respect to their capacities.", "section": "2 The Convergence of Input Saliency"}, {"figure_path": "7PORYhql4V/figures/figures_24_2.jpg", "caption": "Figure 24: The illustration of the relation between the expected testing loss Ex[L] and the marginal expectation Ex[t]. Both (a) CIFAR-100 and (b) TinyImagenet results are shown as supplementary to Figure 9. Models are from (i) single models with varying structure; and (ii) deep ensembles with varying members. Each color represents a model family.", "description": "This figure shows the correlation between the expected testing loss and the marginal expectation of t for both single models and deep ensembles, using CIFAR-100 and TinyImagenet datasets.  The plots demonstrate that as the marginal expectation of t (a measure of gradient alignment) increases, the testing loss generally decreases, indicating a relationship between gradient alignment and model performance. Different colors and shapes represent different model families and the capacity of the models. This figure supports the authors' hypothesis about the convergence trend of input salience and its relationship to model performance.", "section": "4 Applications of Hypotheses"}, {"figure_path": "7PORYhql4V/figures/figures_25_1.jpg", "caption": "Figure 25: The results of single model black-box attack. The value of each entry is \u03b1(k\u2081, k\u2082) for different model capacities, where k\u2081 is the width parameter of the source model and k\u2082 is the width parameter of the target model.", "description": "This figure shows the results of a single model black-box attack where the adversarial samples are generated from the gradients of source models. The y-axis shows the width of the source models, while the x-axis shows the width of the target models. The color of each entry represents the attack success rate, with darker colors indicating higher success rates.", "section": "4.2 Black-Box Attack via Saliency Similarity"}, {"figure_path": "7PORYhql4V/figures/figures_25_2.jpg", "caption": "Figure 11: The comparison between the single-model attack from the largest model (red), the single-model attack from the very same capacity (green) and the attack by the mean direction (blue).", "description": "This figure compares three different black-box attack strategies on various model architectures (CNNs and ResNets) using CIFAR-10 datasets. The attacks target models of different capacities (width).  The attack strategies are: (1) using the gradient from the largest trained model (red), (2) using gradients from models of the same capacity as the target model (green), and (3) using the mean gradient direction across models of different capacities (blue). The y-axis represents the attack success rate, and the x-axis represents the capacity of the target model. The results show that using the mean gradient direction consistently outperforms the other two attack strategies, highlighting its effectiveness in the black-box setting.", "section": "4.2 Black-Box Attack via Saliency Similarity"}]