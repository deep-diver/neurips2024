[{"figure_path": "Q5RYn6jagC/tables/tables_8_1.jpg", "caption": "Table 1: Visual analogy results: GPT-4v.", "description": "This table presents the results of a visual analogy experiment using the GPT-4v model.  It shows the model's accuracy and 95% confidence intervals for four different tasks: the main analogy task, relation decoding, full feature decoding, and single feature decoding. The results are broken down by two experimental conditions: Unified (all objects presented in one image) and Decomposed (objects presented across multiple images).  The comparison highlights how the task's visual complexity affects the model's performance.", "section": "5 Visual Analogy"}, {"figure_path": "Q5RYn6jagC/tables/tables_8_2.jpg", "caption": "Table 2: Visual analogy results: GPT-40.", "description": "This table presents the results of a visual analogy experiment using the GPT-40 model.  It shows the accuracy and 95% confidence intervals for four different tasks: Analogy (solving the main analogy problem), Relation decoding (identifying relations between objects), Full feature decoding (identifying all features of objects), and Single feature decoding (identifying a single feature of an object). The results are shown separately for two conditions: Unified (all objects presented in one image) and Decomposed (objects presented across multiple images to reduce potential interference).", "section": "5 Visual Analogy"}, {"figure_path": "Q5RYn6jagC/tables/tables_8_3.jpg", "caption": "Table 3: Visual analogy results: Claude Sonnet 3.5.", "description": "This table presents the results of a visual analogy experiment using the Claude Sonnet 3.5 model.  It shows the model's accuracy in solving analogy tasks under two conditions: unified and decomposed. The unified condition presents all the visual information in a single image, while the decomposed condition breaks this information into multiple images.  The results are presented for the overall analogy task and for sub-tasks focused on relation decoding, full feature decoding, and single feature decoding.  95% confidence intervals are provided for all accuracy scores.", "section": "5 Visual Analogy"}, {"figure_path": "Q5RYn6jagC/tables/tables_8_4.jpg", "caption": "Table 4: Visual analogy results: Gemini Ultra 1.5.", "description": "This table presents the results of a visual analogy task performed by the Gemini Ultra 1.5 language model.  It shows the model's accuracy (and 95% confidence intervals) for four different subtasks: the main analogy task, relation decoding, full feature decoding, and single feature decoding.  The results are separated into two conditions: unified (source and target pairs in a single image) and decomposed (source and target pairs in separate images). The table highlights the performance differences between the two conditions, indicating how the decomposition of the task affects the model's ability to solve different aspects of the analogy problem.", "section": "5 Results"}]