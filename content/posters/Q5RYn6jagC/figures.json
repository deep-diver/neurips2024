[{"figure_path": "Q5RYn6jagC/figures/figures_2_1.jpg", "caption": "Figure 1: Visual search tasks and results. Example trials for the 2D (top) and 3D (bottom) variants of the disjunctive (left/red column) and conjunctive (middle/blue column) search conditions. Performance for 2D and 3D task variants are plotted on the right. Results reflect aggregate performance for all four VLMs (GPT-4v, GPT-40, Gemini Ultra 1.5, and Claude Sonnet 3.5; see Supplementary Figure 5 for separate model results). Error bars denote 95% binomial confidence intervals.", "description": "This figure displays example trials and results of a visual search experiment conducted on four different vision-language models (VLMs).  The experiment used two types of search tasks: disjunctive (easy, target 'pops out') and conjunctive (harder, requires focused attention). Both 2D and 3D versions of each task were tested. The plots show how the accuracy of each VLM decreases as the number of distractor objects increases, particularly in the conjunctive search condition.  The results are compared across 2D and 3D datasets, with aggregate performance of the four models displayed.", "section": "2 Visual Search"}, {"figure_path": "Q5RYn6jagC/figures/figures_4_1.jpg", "caption": "Figure 2: Numerical estimation tasks and results. Top left: Examples of images generated by text-to-image (T2I) models for different numbers and categories of objects. Top right: Performance of T2I models as a function of the number and category of objects. Results reflect an aggregate of four models (Stable Diffusion Ultra, DALL-E 3, Google Parti, and Google Muse). Bottom left: Examples of images (featuring either 2D or 3D objects) used to evaluate numerosity estimation. Feature entropy was varied in four conditions (low entropy, high entropy, and two medium entropy conditions). Bottom middle: Numerosity estimation results for four multimodal language models (GPT-4v, GPT-40, Gemini Ultra 1.5, Claude Sonnet 3.5; see Supplementary Figure 6 for results with Llava-1.5). Bottom right: Numerosity estimation results plotted as a function of the number of objects in an image, aggregated across all four models (see Supplementary Figure 8 for individual model results). Error bars for all plots reflect 95% binomial confidence intervals.", "description": "This figure presents results from numerical estimation tasks performed on both text-to-image and multimodal language models.  The top shows examples of generated images and the overall performance of text-to-image models in relation to the number and category of objects. The bottom shows examples of the stimuli used in the numerosity estimation task, the performance of multimodal models in 2D and 3D settings, and the overall performance of the models in relation to the number of objects and feature entropy.", "section": "3 Numerical Estimation"}, {"figure_path": "Q5RYn6jagC/figures/figures_6_1.jpg", "caption": "Figure 3: Scene description task and results. A) Example image used in 2D scene description task, illustrating the concept of feature triplets: sets of three objects where one pair of objects shares a feature, and another pair shares a different feature. This example contains three feature triplets, demarcated by the dashed lines. 3D scenes were also investigated. B) Scene description results for text-to-image (T2I) models) as a function of the number of feature triplets. C) 2D scene description results for multimodal language models as a function of the number of feature triplets. Left panel illustrates the results aggregated across four models (GPT-4v, GPT-40, Gemini Ultra 1.5, and Claude Sonnet 3.5). Right panel illustrates the results aggregated across scenes with different numbers of objects. D) 3D scene description results. Error bars represent the standard error of the mean.", "description": "This figure demonstrates the results of a novel scene description task designed to test the impact of representational interference on Vision Language Models (VLMs).  The task involves describing scenes with varying numbers of objects and feature triplets (sets of three objects where pairs share features).  Panels (B-D) show how the number of errors in scene descriptions increases with both the number of objects and feature triplets, supporting the hypothesis that representational interference due to the binding problem hinders VLMs\u2019 performance.", "section": "Scene Description"}, {"figure_path": "Q5RYn6jagC/figures/figures_7_1.jpg", "caption": "Figure 4: Visual analogy task. The Unified and Decomposed conditions present the same object pairs, but in the Decomposed condition it is broken up across three images. The correct target pair must share both relations (shape and color) with the source pair, so the correct answer in this example is Target Pair 2 because it satisfies both the 'same shape' and 'different color' relations.", "description": "This figure illustrates the visual analogy task used in the experiment.  It shows two versions of the task: a unified condition where all pairs are shown in a single image, and a decomposed condition where the source pair and target pairs are shown in separate images. The goal is to identify the target pair that shares the same relationships (in this case, shape and color) as the source pair. The decomposed condition aims to reduce the potential interference from multiple objects in a single image.", "section": "5 Visual Analogy"}, {"figure_path": "Q5RYn6jagC/figures/figures_12_1.jpg", "caption": "Figure 1: Visual search tasks and results. Example trials for the 2D (top) and 3D (bottom) variants of the disjunctive (left/red column) and conjunctive (middle/blue column) search conditions. Performance for 2D and 3D task variants are plotted on the right. Results reflect aggregate performance for all four VLMs (GPT-4v, GPT-40, Gemini Ultra 1.5, and Claude Sonnet 3.5; see Supplementary Figure 5 for separate model results). Error bars denote 95% binomial confidence intervals.", "description": "This figure shows example trials and results for 2D and 3D visual search tasks.  The left columns show examples of disjunctive search (where the target object differs from distractors by a single feature), and the middle columns show examples of conjunctive search (where the target shares features with distractors). The right-hand side presents the results for four different vision-language models on these search tasks, showing the relationship between accuracy and the number of objects presented.  The figure demonstrates how the performance of these models on the conjunctive search task is negatively impacted by the increasing number of objects, similar to human performance, suggesting that the models struggle to manage interference between objects.", "section": "2 Visual Search"}, {"figure_path": "Q5RYn6jagC/figures/figures_12_2.jpg", "caption": "Figure 2: Numerical estimation tasks and results. Top left: Examples of images generated by text-to-image (T2I) models for different numbers and categories of objects. Top right: Performance of T2I models as a function of the number and category of objects. Results reflect an aggregate of four models (Stable Diffusion Ultra, DALL-E 3, Google Parti, and Google Muse). Bottom left: Examples of images (featuring either 2D or 3D objects) used to evaluate numerosity estimation. Feature entropy was varied in four conditions (low entropy, high entropy, and two medium entropy conditions). Bottom middle: Numerosity estimation results for four multimodal language models (GPT-4v, GPT-40, Gemini Ultra 1.5, Claude Sonnet 3.5; see Supplementary Figure 6 for results with Llava-1.5). Bottom right: Numerosity estimation results plotted as a function of the number of objects in an image, aggregated across all four models (see Supplementary Figure 8 for individual model results). Error bars for all plots reflect 95% binomial confidence intervals.", "description": "This figure presents results from numerical estimation tasks using both text-to-image (T2I) and multimodal language models. The top section shows examples of T2I generated images and model performance based on object count and category. The bottom section displays results from multimodal language models tested with varied image complexity (feature entropy) and object counts. Both sections highlight accuracy decreases as object number increases and demonstrate human-like capacity constraints.", "section": "3 Numerical Estimation"}, {"figure_path": "Q5RYn6jagC/figures/figures_13_1.jpg", "caption": "Figure 7: Visual search results with additional control experiment. Results for Claude Sonnet 3.5 on 2D visual search tasks, including disjunctive and conjunctive conditions, and an additional disjunctive search condition ('Disjunctive Search Control') in which target and distractor colors were varied between trials.", "description": "The figure displays the performance of the Claude Sonnet 3.5 model on three different visual search tasks: disjunctive search, conjunctive search, and a control disjunctive search. The control condition was designed to isolate the effect of target-distractor color similarity by varying the colors of targets and distractors between trials. The x-axis represents the number of objects present in the search array, while the y-axis represents the model's accuracy. The results show that the model performed well in disjunctive searches but had difficulty in conjunctive searches, especially as the number of objects increased. The control condition further demonstrates the importance of feature similarity in affecting search performance. ", "section": "2 Visual Search"}, {"figure_path": "Q5RYn6jagC/figures/figures_13_2.jpg", "caption": "Figure 2: Numerical estimation tasks and results. Top left: Examples of images generated by text-to-image (T2I) models for different numbers and categories of objects. Top right: Performance of T2I models as a function of the number and category of objects. Results reflect an aggregate of four models (Stable Diffusion Ultra, DALL-E 3, Google Parti, and Google Muse). Bottom left: Examples of images (featuring either 2D or 3D objects) used to evaluate numerosity estimation. Feature entropy was varied in four conditions (low entropy, high entropy, and two medium entropy conditions). Bottom middle: Numerosity estimation results for four multimodal language models (GPT-4v, GPT-40, Gemini Ultra 1.5, Claude Sonnet 3.5; see Supplementary Figure 6 for results with Llava-1.5). Bottom right: Numerosity estimation results plotted as a function of the number of objects in an image, aggregated across all four models (see Supplementary Figure 8 for individual model results). Error bars for all plots reflect 95% binomial confidence intervals.", "description": "This figure displays the results of numerical estimation experiments performed on both text-to-image and multimodal language models.  The top section shows examples of images generated by T2I models and their performance in estimating object counts, showing a decline in accuracy as the number of objects increases. The bottom section presents similar experiments performed on multimodal language models, again showing a decrease in accuracy with more objects.  The impact of feature variability (entropy) is also explored, demonstrating that higher entropy (more visual distinctions between objects) leads to better performance. The results highlight the limitation of both model types in tasks involving numerosity estimations and their similarities to human limitations.", "section": "3 Numerical Estimation"}, {"figure_path": "Q5RYn6jagC/figures/figures_18_1.jpg", "caption": "Figure 3: Scene description task and results. A) Example image used in 2D scene description task, illustrating the concept of feature triplets: sets of three objects where one pair of objects shares a feature, and another pair shares a different feature. This example contains three feature triplets, demarcated by the dashed lines. 3D scenes were also investigated. B) Scene description results for text-to-image (T2I) models) as a function of the number of feature triplets. C) 2D scene description results for multimodal language models as a function of the number of feature triplets. Left panel illustrates the results aggregated across four models (GPT-4v, GPT-40, Gemini Ultra 1.5, and Claude Sonnet 3.5). Right panel illustrates the results aggregated across scenes with different numbers of objects. D) 3D scene description results. Error bars represent the standard error of the mean.", "description": "This figure shows the results of a novel scene description task designed to test the impact of representational interference on vision-language models (VLMs). The task systematically varies the likelihood of interference by changing the number of feature triplets in the scene.  The results demonstrate that performance decreases as the number of feature triplets and objects increase, supporting the hypothesis that representational interference, a manifestation of the binding problem, underlies the limitations of VLMs in multi-object scene description.", "section": "4 Scene Description"}]