[{"type": "text", "text": "Pretrained Optimization Model for Zero-Shot Black Box Optimization ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Xiaobin Li Kai Wu\u2217 Xidian University Xidian University 22171214784@stu.xidian.edu.cn kwu@xidian.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Yujian Betterrest Li Xiaoyu Zhang Handing Wang Xidian University Xidian University Xidian University bebetterest@outlook.com xiaoyuzhang@xidian.edu.cn hdwang@xidian.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Jing Liu Xidian University neouma@mail.xidian.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Zero-shot optimization involves optimizing a target task that was not seen during training, aiming to provide the optimal solution without or with minimal adjustments to the optimizer. It is crucial to ensure reliable and robust performance in various applications. Current optimizers often struggle with zero-shot optimization and require intricate hyperparameter tuning to adapt to new tasks. To address this, we propose a Pretrained Optimization Model (POM) that leverages knowledge gained from optimizing diverse tasks, offering efficient solutions to zero-shot optimization through direct application or fine-tuning with few-shot samples. Evaluation on the BBOB benchmark and two robot control tasks demonstrates that POM outperforms state-of-the-art black-box optimization methods, especially for high-dimensional tasks. Fine-tuning POM with a small number of samples and budget yields significant performance improvements. Moreover, POM demonstrates robust generalization across diverse task distributions, dimensions, population sizes, and optimization horizons. For code implementation, see https://github.com/ninja-wm/POM/. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Black box optimization, including tasks like hyperparameter optimization (HPO) [1], neuroevolution [2\u20134], neural architecture search (NAS) [5], and algorithm selection [6], is very important. In these scenarios, the algorithm can evaluate $f(\\mathbf{x})$ for any solution $\\mathbf{x}$ ; however, access to additional information about $f$ , such as the Hessian and gradients, is unavailable. ", "page_idx": 0}, {"type": "text", "text": "Addressing diverse BBO problems necessitates the tailored design of specific algorithms to achieve satisfactory performance. Crafting these algorithms typically demands substantial expertise. Therefore, it is crucial to ensure reliable and robust performance of the optimizer in various applications, called zero-shot optimization. Zero-shot optimization involves optimizing a target task that was not seen during training, aiming to provide the optimal solution without or with minimal adjustments to the optimizer. ", "page_idx": 0}, {"type": "text", "text": "The studies [7\u20139] employed Transformer or diffusion models to pretrain model-based optimizers using offilne datasets. While effective, these methods primarily fit optimization trajectories of other BBO algorithms to a specific task, potentially requiring retraining for new tasks, limiting their ability to zero-shot optimization. Subsequently, [10, 11] introduced two learned optimization frameworks for meta-learning evolution strategy (ES) and genetic algorithm (GA). However, the performance of these two methods on zero-shot optimization is weaker than that of CMA-ES [12] (see Section 4.2). ", "page_idx": 1}, {"type": "text", "text": "To address zero-shot optimization, especially for continuous optimization, we introduce a populationbased Pretrained Optimization Model, called POM. Leveraging multiple individuals, populationbased optimizers gain a better understanding of the fitness landscape. The core of the optimizer is how to design optimization strategies that sample better solutions. Inspired by the solutionproducing mechanism of evolutionary computation, we design powerful POM blocks to form a general optimization strategy representation framework. Drawing inspiration from [13], we introduce an end-to-end gradient-based training method for POM, termed MetaGBT (Meta Gradient-Based Training), ensuring stable and rapid training for POM. Pretraining POM on a set of training functions with MetaGBT ensures good optimization strategy. Our contributions can be summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 Excellent ability to solve zero-shot BBO. We develop a efficient POM for zero-shot BBO, demonstrating a substantial performance advantage over state-of-the-art black-box optimizers. ", "page_idx": 1}, {"type": "text", "text": "\u2022 Excellent ability to solve few-shot BBO. Few-shot optimization is the existence of a small budget of function evaluations for the target task to tune the optimizer for better performance. More than $30\\%$ performance improvement can be obtained with 25 random function evaluations. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Heuristic Population-based BBO Algorithms. Numerous metaheuristic population-based algorithms, such as genetic algorithms [14], evolution strategies [15\u201317], particle swarm optimization [18, 19], and differential evolution [20, 21], have been devised to address optimization problems. Notably, CMA-ES [12] and L-SHADE [22] stand out as state-of-the-art methods for BBO. However, these approaches rely on manually designed components, exhibiting inefficiency and fragility when confronted with new tasks. In contrast, the proposed POM can autonomously acquire optimization strategies from problem instances, mitigating the aforementioned limitations. ", "page_idx": 1}, {"type": "text", "text": "Pretrained Population-based BBO Algorithms. Pre-training BBO algorithms can be categorized into two types within the meta-learning framework. The first type frames meta-learning BBO algorithms as a bi-level optimization problem [23]. For instance, [24] leverages meta-learning to infer population-based black-box optimizers that automatically adapt to specific task classes. LES [25] designs a self-attention-based search strategy for discovering effective update rules for evolution strategies through meta-learning. Subsequent works like LGA [10] utilize this framework to discover the update rules of Gaussian genetic algorithms via Open-ES [26]. The second type models the meta-learning of a BBO algorithm as a reinforcement learning problem. [27] meta-learn a policy that adjusts the mutation step-size parameters of CMA-ES [12]. Category one faces the curse of dimensionality, where an escalating number of model parameters leads to skyrocketing training difficulty, impeding the development of intricate strategies. In contrast, category two, which models meta-learning optimizers as reinforcement learning tasks, grapples with training instability. POM, employing a gradient-based end-to-end training approach, successfully bypasses the curse of dimensionality, ensuring stable training. ", "page_idx": 1}, {"type": "text", "text": "LLM for Optimization. In line with POMs, various optimization approaches leveraging Large Language Models (LLMs) have emerged to address diverse problem domains, including NP-hard problems [28, 29], algorithm evolution [30\u201333], reward design [34], and Neural Architecture Search (NAS) [35, 36]. Notably, LLMs play a role in sampling new solutions. However, their optimization strategies depend on externally introduced natural selection mechanisms and are less effective in numerical optimization scenarios [37]. LLaMoCo [38] and EoH [39] use LLM to generate code to solve optimization problems, but the performance of LLaMoCo depends on carefully designed instructions and prompts, and EoH has expensive evaluation costs. TNPs [40], ExPT [41] and LICO [42] use transformer structures to solve the BBO problem and have achieved good results. However, TNPs requires contextual information of the target problem, and neither ExPT nor LICO can be directly used to solve tasks with different dimensions from the training task. These methods lack the universal applicability as pretrained BBO models due to a deficiency in generating capabilities across tasks. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "All the above methods cannot be the zero-shot optimizer. The first two categories need to adjust the hyperParameters when optimizing the new tasks, while the latter must fine-tune the instructions to achieve satisfactory results. ", "page_idx": 2}, {"type": "text", "text": "3 Pretrained Optimization Model ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Problem Definition ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "A black-box optimization problem can be transformed as a minimization problem, and constraints may exist for corresponding solutions: min $f(\\mathbf{x}),s.t.\\ x_{i}\\,\\in\\,[l_{i},u_{i}]$ , where $\\mathbf{x}=\\left(x_{1},x_{2},\\cdot\\cdot\\cdot\\,,x_{d}\\right)$ x   \nrepresents the solution of optimization problem $f$ , the lower and upper bounds $\\mathbf{l}=(l_{1},l_{2},\\cdot\\cdot\\cdot\\,,l_{d})$ and $\\mathbf{u}\\,=\\,\\left(u_{1},u_{2},\\cdot\\cdot\\cdot\\,,u_{d}\\right)$ , and $d$ is the dimension of $\\mathbf{x}$ . For more background information on evolutionary algorithms, see Appendix A. ", "page_idx": 2}, {"type": "text", "text": "Definition 1 Zero-shot Optimization. Zero-shot optimization refers to an optimizer that is applied directly to solve a continuous black-box optimization problem $f$ without any tuning. This means that the optimizer does not require any contextual information about $f$ and can be directly used to handle problems of any dimensionality. ", "page_idx": 2}, {"type": "text", "text": "Definition 2 Few-shot Optimization. Alternatively, it is permissible to fine-tune the optimizer using a small portion of the function evaluation budget for the objective task, and then use the fine-tuned optimizer to solve $f$ . ", "page_idx": 2}, {"type": "text", "text": "3.2 Classic Population Optimization Algorithm ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we use Differential Evolution (DE) as an example to review classic evolutionary algorithms. DE [20, 43] is a prominent family within evolutionary algorithms (EAs), known for its advantageous properties such as rapid convergence and robust performance [44, 45]. The optimization strategy of DE primarily involves mutation and crossover operations. ", "page_idx": 2}, {"type": "text", "text": "The classic DE/rand/1 crossover operator is illustrated in Eq. (1) (additional examples are listed in Appendix A.2). Each mutation strategy can be viewed as a specific instance of Eq. (2); Further details are provided in Appendix A.2. Additionally, we represent the mutation strategy in a matrix form, as shown in Eq. (3). The matrix S evolves with the generation index $t$ , indicating that the mutation strategy adapts across different generations. Consequently, we propose a module to enhance the performance of the mutation operation, which leverages the information from the population of the tth generation to generate $\\mathbf{S}^{t}$ . This serves as the motivation for our design of the LMM. ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbf{v}_{i}^{t}=\\mathbf{x}_{r1}^{t}+F\\cdot\\left(\\mathbf{x}_{r2}^{t}-\\mathbf{x}_{r3}^{t}\\right)\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "In the crossover phase at step $t$ , DE uses a fixed crossover probability $c r_{i}^{t}\\in[0,1]$ for each individual $\\mathbf{x}_{i}^{t}$ in the population, as shown in Eq. (9). The crossover strategy for the entire population can then be expressed as a vector $\\mathbf{cr}^{t}=(c r_{1}^{t},c r_{2}^{t},\\cdot\\cdot\\cdot\\ ,c r_{N}^{t})$ . Our goal is to design a module that adaptively generates $\\mathbf{cr}^{t}$ using the information from the population. This approach allows for the automatic design of the crossover strategy by controlling the parameter $c r$ . This serves as the motivation for our design of LCM. ", "page_idx": 2}, {"type": "text", "text": "3.3 Design of POM ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "A population consists of $n$ individuals, denoted as $\\mathbf{X}=\\left\\{\\mathbf{x}_{1},\\mathbf{x}_{2},\\cdot\\cdot\\cdot{\\mathbf{\\Omega}},\\mathbf{x}_{n}\\right\\}$ . In this paper, $\\mathbf{X}$ is also treated as $\\mathbf{X}=[\\mathbf{x}_{1},\\mathbf{x}_{2},\\cdots\\,,\\mathbf{x}_{n}]^{T}$ to support matrix operations. We feed POM an initial random population $\\mathbf{X}^{0}$ at step 0, specify the evolution generation $T$ for it, and hope that it can generate a population ${\\bf X}^{T}$ close to the global optimum at step $T$ , as shown in $\\mathbf{X}^{T}=\\mathbf{\\dot{\\mathit{POM}}}(\\mathbf{X}^{0},T|\\mathbf{\\dot{\\theta}})$ , where $\\theta\\in\\Omega$ is the parameters of POM, where $\\Omega$ stands for the strategy space. The goal of training POM is to find an optimal $\\theta$ in $\\Omega$ . As shown in Fig. 1, POM consists of LMM, LCM and SM. ", "page_idx": 2}, {"type": "text", "text": "LMM LMM generates candidate solutions $\\mathbf{v}_{i}^{t}$ for individual $\\mathbf{x}_{i}^{t}$ through Eq. (2), which enables the population information to be fully utilized in the process of generating candidate solutions $\\mathbf{v}_{i}^{t}$ . ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{v}_{i}^{t}=\\sum_{j}^{N}w_{i,j}\\mathbf{x}_{j}^{t}\\quad(\\forall w_{i,j}\\in\\mathbb{R},w_{i,i}\\neq0)\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Further, we organize Eq. (2) into a matrix form, as shown in Eq. (3). ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{V}^{t}=\\mathbf{S}^{t}\\times\\mathbf{X}^{t}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "$\\mathbf{X}^{t}\\in\\mathbb{R}^{N\\times d}$ is the population in generation $t$ and $\\mathbf{S}^{t}\\in\\mathbb{R}^{N\\times N}$ . S evolves with each change in $t$ signifying a mutation strategy that adapts across generations. Consequently, it is imperative to devise a module that leverages information from the population at generation $t$ to generate $\\mathbf{\\bar{S}}^{t}$ . Any mutation operator of differential evolution, such as the classic DE/rand/1 mutation operator, can be converted into Equation (3) in the specific case of $\\mathbf{S}$ (see Appendix A.2 for details). At the same time, the crossover operation of GAs can also be generalized into the form of Equation (3) [46]. ", "page_idx": 3}, {"type": "text", "text": "The function of LMM is designed based on Multi-head self-attention (MSA) [47], as shown as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{S}^{t}=L M M(\\mathbf{H}^{t}|\\theta_{1})\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\theta_{1}=\\left\\{\\mathbf{W}_{m1},\\mathbf{W}_{m2},\\mathbf{W}_{m3},\\mathbf{b}_{m1},\\mathbf{b}_{m2},\\mathbf{b}_{m3}\\right\\}$ denotes the trainable parameters within LMM, while $\\mathbf{H}^{t}=[\\mathbf{h}_{1}^{t},\\mathbf{h}_{2}^{t},\\cdot\\cdot\\cdot\\mathbf{\\Omega},\\mathbf{h}_{N}^{t}]$ serves as LMM\u2019s input, encapsulating population information. Each $\\mathbf{h}_{i}^{t}$ incorporates details about $\\mathbf{x}_{i}^{t}$ , encompassing: 1) $\\hat{f}_{i}^{t}$ : the normalized ftiness $f(\\mathbf{x}_{i}^{t})$ of $\\mathbf{x}_{i}^{t}$ ; 2) $\\hat{r}_{i}^{t}$ : the centralized ranking of $\\mathbf{x}_{i}^{t}$ . The method for calculating $\\hat{f}_{i}^{t}$ is: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\hat{f}_{i}^{t}=\\frac{f(\\mathbf{x}_{i}^{t})-\\mu^{t}}{\\sigma^{t}}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mu^{t}$ and $\\sigma^{t}$ denote the mean and standard deviation, respectively, of individual fitness values within the population at time $t$ . We build $\\hat{r}_{i}^{t}$ as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\hat{r}_{i}^{t}=(\\frac{r a n k(\\mathbf{x}_{i}^{t},\\mathbf{X}^{t})}{N}-0.5)\\times2\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where rank yields the ranking of $\\mathbf{x}_{i}^{t}$ within the population $\\mathbf{X}^{t}$ , with values ranging from 1 to $N$ Thus, LMM utilizes information on the relative fitness of individuals to dynamically generate the strategy $\\hat{\\mathbf{S}}^{t}$ . $\\hat{r}_{i}^{t}$ serves as position encoding, explicitly offering the ranking information of individuals. Equation (7) details the computation of $\\hat{\\mathbf{S}}^{t}$ . ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{\\mathbf{H}}^{t}=T a n h(\\mathbf{H}^{t}\\times\\mathbf{W}_{m1}+\\mathbf{b}_{m1}),\\;\\;\\;\\mathbf{Q}^{t}=T a n h(\\hat{\\mathbf{H}}^{t}\\times\\mathbf{W}_{m2}+\\mathbf{b}_{m2})}\\\\ &{\\mathbf{K}^{t}=T a n h(\\hat{\\mathbf{H}}^{t}\\times\\mathbf{W}_{m3}+\\mathbf{b}_{m3}),\\;\\;\\;\\hat{\\mathbf{S}}^{t}=T a n h(\\frac{\\mathbf{Q}^{t}\\times(\\mathbf{K}^{t})^{T}}{\\sqrt{(d_{m})}})}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where Tanh is an activation function. $\\mathbf{W}_{m1}\\in\\mathbb{R}^{2\\times d_{m}}$ and ${\\bf W}_{m2}$ , $\\mathbf{W}_{m;3}\\in\\mathbb{R}^{d_{m}\\times d_{m}}$ . ${\\bf b}_{m1}$ , ${\\bf b}_{m2}$ , and ${\\bf b}_{m3}$ are vector with dimension $d_{m}$ . $\\hat{\\mathbf{H}}^{t}\\in\\mathbb{R}^{N\\times d_{m}}$ , $\\mathbf{Q}^{t},\\mathbf{K}^{t}\\in\\mathbb{R}^{N\\times d_{m}}$ , and $\\hat{\\mathbf{S}}^{t}\\in\\mathbb{R}^{N\\times N}$ . ", "page_idx": 3}, {"type": "text", "text": "The topological structure of the population significantly influences their information exchange [48]. When all individuals engage in information exchange, the algorithm\u2019s convergence may suffer, diversity could diminish, and susceptibility to local optima increases. To address this, we introduce a mask operation during both training and testing phases, where the probability of setting each element in $\\hat{\\mathbf{S}}^{t}$ to 0 is $r_{m a s k}$ . This operation enhances POM\u2019s ability to learn efficient and robust strategies, as validated in our experiments. Consequently, $\\mathbf{S}^{t}$ is derived using Eq. (8). ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{S}^{t}=m a s k(\\hat{\\mathbf{S}}^{t}|r_{m a s k})\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Finally, we get ${\\bf V}^{t}$ via Eq. (3). ", "page_idx": 3}, {"type": "text", "text": "LCM For each individual $\\mathbf{x}_{i}^{t}$ at step $t$ , a crossover probability $c r_{i}^{t}\\in[0,1]$ is established. Consequently, the population\u2019s crossover strategy is encapsulated in the vector $\\mathbf{cr}^{t}=(c r_{1}^{t},c r_{2}^{t},\\cdot\\cdot\\cdot\\ ,c r_{N}^{t})$ . The crossover operation, as depicted in Eq. (9), can be elucidated as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{u}_{i,k}^{t}=\\left\\{\\begin{array}{l l}{\\mathbf{v}_{i,k}^{t},\\quad\\mathrm{if}\\quad r a n d(0,1)\\leq c r_{i}^{t}}\\\\ {\\mathbf{x}_{i,k}^{t},\\quad\\mathrm{otherwise}}\\end{array}\\right.\\quad\\forall i\\in[1,N]\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The module design should facilitate the adaptive generation of $\\mathbf{cr}^{t}$ by leveraging population information. Executing the crossover operation with $\\mathbf{cr}^{t}$ yields $\\mathbf{U}^{t}=[\\mathbf{u}_{1}^{t},\\dot{\\mathbf{u}}_{2}^{t},\\cdot\\cdot\\cdot\\mathbf{\\Omega},\\mathbf{\\bar{u}}_{N}^{t}]$ . ", "page_idx": 4}, {"type": "text", "text": "LCM is designed based on FFN [47], as shown in Eq. (10), ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{cr}^{t}=L C M(\\mathbf{Z}^{t}|\\theta_{2})\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\theta_{2}=\\{\\mathbf{W}_{c1},\\mathbf{b}_{c1},\\mathbf{W}_{c2},\\mathbf{b}_{c2},\\tau\\}$ is the parameter of LCM and $\\mathbf{Z}^{t}\\in\\mathbb{R}^{N\\times3}$ is the population information used by LCM. Here, $\\mathbf{Z}^{\\vec{t}}\\,=\\,[\\mathbf{z}_{1}^{t},\\mathbf{\\bar{z}}_{2}^{t},\\cdots\\,,\\mathbf{z}_{N}^{t}]$ . $\\mathbf{z}_{i}^{t}$ represents the relevant information of individual $\\mathbf{x}_{i}^{t}$ and $\\mathbf{X}^{t}$ . For example, it can include the ranking information of $\\mathbf{x}_{i}^{t}$ , the fitness information of $\\mathbf{\\dot{x}}_{i}^{t}$ , the Euclidean distance between $\\mathbf{x}_{i}^{t}$ and $\\mathbf{V}_{i}^{t}$ , and the distribution information of individuals within the population (such as the fitness distribution, the distance between pairs of individuals), etc. In this paper, $\\mathbf{z}_{i}^{t}$ includes the following information as a case study: 1) $\\hat{f}_{i}^{t}$ : the normalized fitness $f(\\mathbf{x}_{i}^{t})$ of $\\mathbf{x}_{i}^{t};2)\\,\\hat{r}_{i}^{t}$ : the centralized ranking of $\\mathbf{x}_{i}^{t}$ ; 3) $s i m_{i}^{t}$ : the cosine similarity between $\\boldsymbol{x}_{i}^{t}$ and $v_{i}^{t}$ . ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{h}^{t}=T a n h(\\mathbf{Z}^{t}\\times\\mathbf{W}_{\\mathbf{c}\\mathbf{1}}+\\mathbf{b}_{\\mathbf{c}\\mathbf{1}}),\\;\\;\\hat{\\mathbf{h}}^{\\mathbf{t}}=l a y e r n o r m(\\mathbf{h}^{t}|\\tau),\\;\\;\\mathbf{cr}^{t}=S i g m o i d(\\hat{\\mathbf{h}}^{\\mathbf{t}}\\times\\mathbf{W}_{\\mathbf{c}\\mathbf{2}}+\\mathbf{b}_{\\mathbf{c}\\mathbf{2}})\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where the activation function Sigmoid maps inputs to the range $(0,1)$ . $\\mathbf{W_{c1}}\\in\\mathbb{R}^{3\\times d_{c}}$ , $\\mathbf{W_{c2}}\\in\\mathbb{R}^{d_{c}\\times1}$ , $\\tau$ is the learnable parameters of layernorm [49]. ${\\bf b_{c1}}$ and ${\\bf b_{c2}}$ are vectors with dimensions $d_{c}$ and 1, respectively. ", "page_idx": 4}, {"type": "text", "text": "Although we derive $\\mathbf{cr}^{t}$ from Eq. (11) as in Eq. (9), the discrete nature of the crossover operator renders it non-differentiable, impeding gradient-based training of the $L C M$ module. To address this limitation, we introduce the gumbel_softmax method [50], providing an efficient gradient estimator that replaces non-differentiable samples from a categorical distribution with differentiable samples from a novel Gumbel-Softmax distribution. ", "page_idx": 4}, {"type": "text", "text": "Eq. (12) shows how to perform crossover operations between $\\mathbf{x}_{i}^{t}$ and $\\mathbf{v}_{i}^{t}$ in $L C M\\left(\\forall i\\in\\left[1,N\\right]\\right)$ . ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{r}_{i}^{t}=r a n d(d),~~\\mathbf{c}\\mathbf{v}_{i}^{t}=g u m b e l\\lnot s o f t m a x(c a t(\\mathbf{r}_{i}^{t},t i l e(c r_{i}^{t},d))),}\\\\ &{\\mathbf{u}_{i}^{t}=\\mathbf{c}\\mathbf{v}_{i,0}^{t}\\cdot\\mathbf{x}_{i}^{t}+\\mathbf{c}\\mathbf{v}_{i,1}^{t}\\cdot\\mathbf{v}_{i}^{t},~~\\mathbf{U}^{t}=[\\mathbf{u}_{1}^{t},\\mathbf{u}_{2}^{t},\\cdot\\cdot\\cdot\\mathbf{\\nabla},\\mathbf{u}_{N}^{t}]}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "First, the rand function samples uniformly from the range $[0,1]$ to obtain a vector $\\mathbf{r}_{i}^{t}$ . Then get $c r_{i}^{t}$ from $\\mathbf{cr}^{t}$ according to the index. The tile function expands $c r_{i}^{t}$ into a $d$ -dimensional vector: $[\\boldsymbol{c}\\boldsymbol{\\dot{r}}_{i}^{t},\\boldsymbol{c}\\boldsymbol{r}_{i}^{t},\\cdots,\\boldsymbol{c}\\boldsymbol{r}_{i}^{t}]$ . The cat function concatenates them into a matrix as shown below: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\left[\\begin{array}{l l l l}{r_{i,1}^{t}}&{r_{i,2}^{t}}&{\\cdots}&{r_{i,d}^{t}}\\\\ {c r_{i}^{t}}&{c r_{i}^{t}}&{\\cdots}&{c r_{i}^{t}}\\end{array}\\right]\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Here, gumbel_softmax is executed column-wise. For any column, the larger element becomes 1 after gumbel_softmax and 0 otherwise. Therefore, $\\mathbf{cv}_{i}^{t}\\in\\mathbb{R}^{2\\times d}$ may be a matrix like this: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{cv}_{i}^{t}=\\left[\\!\\!1\\quad\\mathbf{0}\\quad\\mathbf{0}\\quad\\mathbf{0}\\quad1\\quad1\\quad\\cdot\\cdot\\quad1\\quad1\\right]\n$$", "text_format": "latex", "page_idx": 4}, {"type": "image", "img_path": "fWQhXdeuSG/tmp/6a1db278e1550c793ebb69b6c1da112deb38821acf533c63f189d6775979e573.jpg", "img_caption": [], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Figure 1: In the figure, $\\mathbf{X}^{0}$ is the initial random population. (a) The overall architecture of the POM. (b) POM training process. Here $T$ is the size of the inner loop iteration step during training, and the training function should be differentiable. (c) POM testing process. Here, $T$ is the number of iterations of the testing process and $f$ is the target task. $f$ does not have to be differentiable. Here we directly apply the trained POM to solve $f$ without requiring gradient information. ", "page_idx": 4}, {"type": "text", "text": "Overall Framework We design LMM and LCM to achieve the generation of sample strategy (that is, generate $\\mathbf{S}^{t}$ ) and crossover strategy (that is, generate $\\mathbf{cr}^{t}$ ), respectively. The overall architecture of POM is shown in Fig. 1. The parameters that need to be trained in $P O M$ are $\\theta=\\{\\theta_{1},\\theta_{2}\\}$ . At time step $t$ , the population is $\\mathbf{X}^{t}$ . Initially, we amalgamate the information from $\\mathbf{X}^{t}$ to construct descriptive representations of the population, $\\mathbf{H}^{t}$ and ${\\bf Z}^{t}$ . LMM adaptively generates $\\mathbf{S}^{t}$ based on $\\mathbf{H}^{t}$ ", "page_idx": 4}, {"type": "text", "text": "The multiplication of $\\mathbf{X}^{t}$ and $\\mathbf{S}^{t}$ yields ${\\bf V}^{t}$ (see Eq. (3)). Next, $L C M$ adaptively generates $\\mathbf{cr}^{t}$ based on its input ${\\bf Z}^{t}$ , and performs a crossover operation based on $\\mathbf{cr}^{t}$ to obtain ${\\bf U}^{t}$ . Finally, $S M$ [51], a $^{\\,l}$ -to-1 selection strategy is executed between $\\mathbf{U}^{t}$ and $\\mathbf{X}^{t}$ to produce the next-generation population $\\mathbf{X}^{t+1}$ . ", "page_idx": 5}, {"type": "text", "text": "${\\bf X}^{t+1}=S M({\\bf X}^{t},{\\bf U}^{t})=t i l e(l_{x>0}({\\bf M}_{F^{\\prime}}-{\\bf M}_{F}))\\odot{\\bf X}^{t}+t i l e(1-l_{x>0}({\\bf M}_{F^{\\prime}}-{\\bf M}_{F}))\\odot{\\bf U}^{t}$ (15) where $l_{x>0}(x)\\,=\\,1$ if $x\\,>\\,0$ and $l_{x>0}(x)\\,=\\,0$ if $x\\,<\\,0$ , and the tile copy function extends the indication matrix to a tensor with size $(N,d)$ , ${\\bf M}_{F}({\\bf M}_{F^{\\prime}})$ denotes the ftiness matrix of $\\mathbf{X}^{t}(\\mathbf{U}^{t})$ , and $\\odot$ indicates the pairwise multiplication between inputs. ", "page_idx": 5}, {"type": "table", "img_path": "fWQhXdeuSG/tmp/eb81f47a24b38f5d4ff37b56fcbd2e718eb87512ad59d742d9f8a5a2acf551c9.jpg", "table_caption": [], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "3.4 Tasks, Loss Function & MetaGBT ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "POM is meticulously crafted as a model amenable to end-to-end training based on gradients. While POM necessitates gradient information from the training task during the training phase, it exhibits the ability to tackle BBO problems in the testing phase without relying on any gradient information. To ensure the acquisition of an efficient, highly robust, and broadly generalizable optimization strategy, POM undergoes training on a diverse set of tasks. Training on these tasks sequentially poses the risk of domain overfitting, local optima entrapment, and diminished generalization performance. Consequently, we introduce a training methodology named MetaGBT. ", "page_idx": 5}, {"type": "text", "text": "Tasks. We form a training task set $T S=\\{f_{i}(\\mathbf{X}|\\omega^{j})\\}$ ,where $i\\in[1,5]$ and $j\\in[1,N]$ , comprising $4N$ tasks derived from Table 3 in appendix, where $\\omega_{i}$ denotes the task parameter influencing the function\u2019s landscape offset. Our selection of these functions for the training task is motivated by their diverse landscape features. The specific landscape features encompassed in $T S$ are detailed in Appendix B. ", "page_idx": 5}, {"type": "text", "text": "Loss Function. To avoid bias of different output scales in $T S$ , for any function $f_{i}$ in $T S$ , we design the normalized loss function $l_{i}({\\bf X}^{t},{\\bf X}^{t-1},f_{i}^{'},\\omega^{i},\\lambda)$ . In Equation (16), $l_{i}^{1}$ calculates the average fitness difference between the input and output of the POM, further normalized within [0, 1]. This encourages convergence of the algorithm. $l_{i}^{2}$ uses standard deviation to simulate the distribution of the output population, encouraging diversity in the output population. $s t d(\\mathbf{X}^{t},j)$ is the standard deviation of the jth dimension of the population. $\\lambda$ is a hyperparameter, and we find that setting it to 0.005 can make model training more stable. ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{X}^{t}=P O M(\\mathbf{X}^{t-1},1|\\theta)}\\\\ &{l_{i}^{1}=\\frac{\\frac{1}{|\\mathbf{X}^{t}|}\\displaystyle\\sum_{\\mathbf{x}\\in\\mathbf{X}^{t}}f_{i}(\\mathbf{x}|\\omega^{i})-\\frac{1}{|\\mathbf{X}^{t-1}|}\\displaystyle\\sum_{\\mathbf{x}\\in\\mathbf{X}^{t-1}}f_{i}(\\mathbf{x}|\\omega^{i})}{\\left|\\frac{1}{|\\mathbf{X}^{t-1}|}\\displaystyle\\sum_{\\mathbf{x}\\in\\mathbf{X}^{t-1}}f_{i}(\\mathbf{x}|\\omega^{i})\\right|},\\;\\;l_{i}^{2}=\\frac{j-1}{d}{d}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "MetaGBT. The pseudocode for MetaGBT is presented in Algorithm 1. Initially, we sample the POM parameter $\\theta$ from a standard normal distribution. The objective of MetaGBT is to iteratively update $\\theta$ to bring it closer to the global optimum $\\theta^{*}$ . In line 2, we sample a population for each task in TS. Lines 3, 4 and 5 involve the resampling of task parameters for all tasks in TS, thereby altering the task landscape, augmenting training complexity, and enhancing the learning of robust optimization strategies by POM. The final loss function (line 10) is determined by computing the average of the loss functions for all tasks. Subsequently, in line 12, we update $\\theta$ using a gradient-based optimizer, such as Adam [52]. The trained $P O M$ is then ready for application in solving an unknown BBO problem, as depicted in Algorithm 1. ", "page_idx": 6}, {"type": "image", "img_path": "fWQhXdeuSG/tmp/ee472ddb056eaefea0b2d682fee32488e1f11a38ab302ef74439ca5881d57ba3.jpg", "img_caption": [], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Figure 2: The critical difference diagram illustrates the performance ranking of seven algorithms across 24 BBOB problems with dimensions $d=$ 30, 100, employing Wilcoxon-Holm analysis [53] at a significance level of $p=0.05$ . Algorithm positions are indicative of their mean scores across multiple datasets, with higher scores signifying a method consistently outperforming competitors. Thick horizontal lines denote scenarios where there is no statistically significant difference in algorithm performance. ", "page_idx": 6}, {"type": "image", "img_path": "fWQhXdeuSG/tmp/430167b362db335d67b9a058aacaa1262c7b27c69946f883b766e95283edfceb.jpg", "img_caption": ["Figure 3: Experimental results are presented for the Bipedal Walker (a) and Enduro (b), with the vertical axis denoted as $R$ , representing the strategy score. The score corresponds to the total reward acquired by the agent during interactions with the environment. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "4.1 Experimental Setup ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We test the performance of POM on the widely used BBO benchmark and two complex real-world problems (see Appendix C). Selected methods include DE (DE/rand/1/bin) [54] and ES $(\\mu,\\lambda)$ -ES) as population-based baselines, L-SHADE [22] and CMA-ES [12] as state-of-the-art population-based BBO methods, and LES [25] and LGA [10] as state-of-the-art POMs. POM is trained on $T S$ with $T=100$ , $n=100$ , and $d=10$ . Detailed parameters for all compared methods are provided in Appendix E. Please refer to Appendix $\\mathrm{D}$ for the reasons for choosing these algorithms. ", "page_idx": 6}, {"type": "text", "text": "4.2 Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "BBOB [55]. We evaluate the generalization ability of POM across 24 BBOB functions with dimensions $d=30$ and $d=100$ , where optimal solutions are located at 0. Figure 2 presents the critical difference diagram comparing all algorithms (refer to Appendix Tables 4 and 6, and Figures 11, 12 and 13 for detailed results). POM significantly outperforms all methods, showcasing its efficacy across varying dimensions. Despite being trained solely on TF1-TF4 with $d=10$ , POM excels in higher dimensions $(d=\\{30,100,500\\})$ , with its performance advantage becoming more pronounced with increasing dimensionality. Particularly on complex problems F21-F24, where global structure is weak, POM lags behind LSHADE but surpasses other methods, attributed to its adaptability through fine-tuning. TurBO [56] is the Bayesian optimization algorithm with the best performance on BBOB [57]. Under little budget conditions, the performance of POM outperforms that of TurBO in most cases (see Appendix $\\mathrm{G}$ for details). ", "page_idx": 6}, {"type": "text", "text": "Bipedal Walker [58]. The Bipedal Walker task involves optimizing a fully connected neural network with $d\\,=\\,874$ parameters over $k\\,=\\,800$ time steps to enhance robot locomotion control. In Fig. 3(a), LSHADE shows ineffectiveness, while CMA-ES, LSHADE, and LGA suffer from premature convergence. Conversely, POM achieves stable and swift convergence, ultimately attaining the highest score. ", "page_idx": 6}, {"type": "text", "text": "Enduro [58]. Enduro task entails controlling a strategy with $d=4149$ parameters across $k=500$ steps, posing greater difficulty than Bipedal Walker. As depicted in Fig. 3(b), LGA and LES exhibit premature convergence and limited exploration. While CMA-ES initially converges slightly faster than POM, the latter maintains a superior balance between exploration and exploitation, outperforming LSHADE. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "image", "img_path": "fWQhXdeuSG/tmp/6cb281c68ec8af5269e11686d8afad66140908eade31fa1cd201d9e726e3e38e.jpg", "img_caption": [], "img_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "fWQhXdeuSG/tmp/857bf21f5a11cd2ab93cadd214c2168a775c3df649aea91b5ac03c4b92173204.jpg", "img_caption": [], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Figure 4: (a) Results of ablation study. The metric used to evaluate performance is the optimal value of the function found, with smaller values being better. Here, $d=30$ . (b) Results of POMs with different sizes on BBOB tests $d=100)$ ). ", "page_idx": 7}, {"type": "image", "img_path": "fWQhXdeuSG/tmp/2693727ce925ed10eb36f56064e000ecbaca8f5e2b0dafc6ffeb67958dd95774.jpg", "img_caption": [], "img_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "fWQhXdeuSG/tmp/21d20bf7c40f76b4f2d110ce92d7b93baf9cb4347ae87d056539c3248f29b605.jpg", "img_caption": [], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Figure 5: The impact of training dataset size on the performance of POM. $d=100$ . 1 means that the training set only contains $T F1$ , and 2 means that the training set only contains $T F1$ and $T F2$ , and so on. ", "page_idx": 7}, {"type": "image", "img_path": "fWQhXdeuSG/tmp/207031e46f332fc0d6c56d8b522291ff274949c5e41ab23a3877344a631322e2.jpg", "img_caption": ["Figure 6: Experimental results of fine-tuning tests. RFI = performance improvement ormance of base P OM "], "img_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "fWQhXdeuSG/tmp/aa664fed18be92fa292692465f30ced921ae93f6847f860d55591072fb3b7378.jpg", "img_caption": ["Figure 7: (a) Time cost of POM. (b) Testing cost of baselines and POM. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "4.3 Analysis ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Ablation Study The ablation study results for the designed modules are presented in Fig. 4 (a) (refer to Appendix Table 7 for additional details). Configurations include UNTRAINED, representing an untrained POM with randomly initialized parameters; NO LMM, where the LMM is excluded, and a simple DE/rand/1/bin mutation operator is employed; NO LCM, indicating the absence of the learnable crossover operation, using only binomial crossover; and NO MASK, signifying the omission of the mask operation described in Eq. (8). ", "page_idx": 7}, {"type": "text", "text": "While UNTRAINED yields optimal results for F9 and F16, as an untrained POM is inherently an optimization strategy, the adaptability of trained POM surpasses the baselines in most scenarios. In simpler tasks with $d=30$ , UNTRAINED underperforms, demonstrating the advantage of trained POM on more complex tasks. Notably, NO LMM and NO LCM excel on F5, F11, and F19, respectively. This could be attributed to potential overfitting of POM to the relatively simple training set. The exclusion of mask operation (NO MASK) significantly diminishes POM\u2019s performance, highlighting the importance of the mask for global information sharing and population interaction, crucial for maintaining diversity. All modules contribute to POM\u2019s overall performance, with the negative impact on POM\u2019s performance ranked as follows: $N O\\:M A S K>N O\\:L M M>U N T R A I N E D>N O$ LCM. ", "page_idx": 7}, {"type": "text", "text": "Fine-tuning Test We evaluate the fine-tuned POM\u2019s performance on $T F6\u2013T F8$ as detailed in Appendix Table 3. We replace LCM with a standard transformer encoder to obtain more stable experimental results. For $\\mathbf{x}_{i}$ and $\\mathbf{v}_{i}$ , we normalize their features and then concatenate them by dimension to obtain $\\mathbf{xv}_{i}\\in\\mathbb{R}^{d\\times2}$ . $\\mathbf{xv}_{i}$ and the normalized [ftiness, ranking] information of $\\mathbf{x}_{i}$ in the parent population are concatenated to obtain $\\mathbf{xvf}_{i}\\in\\mathbb{R}^{(d+1)\\times2}$ . Based on this input, the transformer encoder will generate $\\mathbf{cv}_{i}$ in Eq. (12). Different numbers of $\\omega$ are generated as fine-tuning samples for $T F6\u2013T F8$ , and Algorithm 1 is used to fine-tune POM for each function. The base POM is initially trained on $T F1\u2013T F5$ . We calculate the relative performance improvement (RFI) achieved by the fine-tuned POM compared to the base POM, with results displayed in Figure 6. Experimental results indicate that fine-tuning POM leads to significant performance improvements even with a small sample size. The method for obtaining fine-tuning samples is not restricted; for black-box tasks, a surrogate model can be constructed to facilitate fine-tuning. ", "page_idx": 7}, {"type": "image", "img_path": "fWQhXdeuSG/tmp/cd5cc2453f571ef440f0b86aa3efc2108cc9f0e128d016537db7debe0b3dc853.jpg", "img_caption": ["Figure 8: Displayed are visualized outcomes of LMM $S^{t}$ in BBOB with $d=100$ using $n=10$ for clarity. Blank squares in the matrix denote masked portions from Eq. (8). Steps 1, 50, and 100 correspond to the 1st, 50th, and 100th generations in population evolution. The horizontal and vertical axes denote individual rankings, with 1 as the best and 10 as the worst in the population. Each row illustrates the weight assigned to other individuals when executing mutation operations for the respective individual. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "fWQhXdeuSG/tmp/62471eaf3f40c0806a5b5398732ebae588f4a0cc43d9d703b5f5f036b95e6280.jpg", "img_caption": ["Figure 9: Visual analysis results of LCM on BBOB F1, F11, and F24 with $d=100$ , employing $n\\,=\\,100$ , are presented. \"Rank\" signifies an individual\u2019s position, with rank 5 representing the fifth-ranked individual in the population. Subgraphs depict the evolution of the probability that an individual will undergo crossover across three tasks as the population progresses. For example, (a) illustrates the crossover probability change for the top-ranked individual on F1, F11, and F24 with the number of generations. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Size of Training Dataset Any complex problem can be simulated by a polynomial composed of simple basic function terms. To ensure that the optimization strategy learned by POM has robust generalization ability and performance, we should train POM on a set of basic functions. ", "page_idx": 8}, {"type": "text", "text": "First, we tested the impact of increasing the number of basic functions in the training set on model performance. Next, we examined the effect of introducing complex functions into the training set. Functions $T F1-T F5$ are basic simple terms. For example, $T F1$ is an absolute value term, and $T F5$ is a square summation term. Functions $T F6-T F8$ are composite terms composed of several basic functions. For instance, $T F6$ includes both a cumulative multiplier term and a cosine term. The test results are shown in Figure 5 (a) and (b) (see Appendix Table 8 for details), respectively. ", "page_idx": 8}, {"type": "text", "text": "Experimental results indicate that increasing the number of basic functions leads to an overall improvement in POM performance, whereas the introduction of composite terms results in a significant performance decline. This aligns with our hypothesis. ", "page_idx": 8}, {"type": "text", "text": "Scale of POM We explore the performance of POM at different scales, which is shown in Fig. 4 (b) (refer to Appendix Table 9 for additional details). We increase POM\u2019s parameter count by perturbing the hidden layers of each module $(d_{m},d_{c})$ . Six models are constructed in ascending order of parameter count, labeled as VS (very small), $S$ (small), $M$ (medium), $L$ (large), $V L$ (very large), and $X L$ (extra large) (details in the Appendix Table 2). $X L$ achieves the best performance, while VS and $M$ also perform well. S exhibits the worst performance, and $V L$ performs worse than $L$ . Two core factors contribute to this phenomenon: the number of parameters and training. We observe a complex relationship between the number of parameters and training difficulty. VS, with the fewest parameters, is the easiest to train and performs well on BBOB. Conversely, $X L$ , with a large number of parameters, exhibits the strongest capability to represent strategies, resulting in the best performance. The performance of $X L$ aligns with our expectations. We obtain the following principles: 1) Larger models can have stronger capabilities but are more challenging to train; 2) Training difficulty and model scale do not exhibit a simple linear relationship, warranting further research; 3) Larger models require more functions for effective training. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Time Budget We assess the training and test time efficiency of POM across various architectures on BBOB ( $d=10)$ ) and BBOB $d=100)$ ) respectively, as illustrated in Figure 7. POM demonstrates remarkable efficiency in tackling BBO problems, with negligible training costs relative to its exceptional generalization ability and high performance. ", "page_idx": 9}, {"type": "text", "text": "4.4 Visualization Analysis ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "LMM Learning Analysis Figure 8 displays $S^{t}$ for an in-depth analysis of the LMM strategy (refer to Appendix Figure 15-20 for additional details). Key observations and conclusions include: 1) Generally, superior individuals receive higher weights during LMM, showcasing POM\u2019s ability to balance exploration and exploitation as the population converges. 2) Across diverse function problems, POM dynamically generates optimization strategies, highlighting its adaptability and contributing to robust generalization. 3) Disadvantaged individuals exhibit a more uniform weight distribution, potentially aiding in their escape from local optima and enhancing algorithm convergence. ", "page_idx": 9}, {"type": "text", "text": "LCM Learning Analysis We visually examine the LCM strategy, presenting the results in Fig. 9 (refer to Appendix Figure 21-26 for additional details). LCM displays the capacity to adaptively generate diverse strategies for individuals across different ranks in the population, revealing distinct patterns among tasks and rankings. Notably, top-ranking individuals within the top 20, such as those ranked 1st, 5th, and 18th, exhibit a flexible crossover strategy. The dynamic adjustment of crossover probability with population evolution aids in preserving dominant genes and facilitating escape from local optima. Conversely, lower-ranking individuals show an increasing overall probability of crossover, promoting exploration of disadvantaged individuals and enhancing the algorithm\u2019s exploration capability. LCM proficiently generates adaptive crossover strategies across tasks, individuals, and convergence stages, significantly boosting both convergence and exploration capabilities. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We present POM, a novel Pretrained Optimization Model designed to address the inefficiencies of existing methods in zero-shot optimization. Evaluation on BBOB and robot control tasks demonstrates POM\u2019s superiority over other black-box optimizers, particularly in high-dimensional scenarios. Additionally, POM excels in solving few-shot optimization problems. Future research avenues include designing enhanced loss functions to optimize POM for both population convergence and diversity, thereby improving overall algorithm performance. In addition, the limitations of model scale and time performance deserve further study (see Appendix I for details). ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was supported in part by the National Natural Science Foundation of China under Grant 62206205 and 62471371, in part by the Young Talent Fund of Association for Science and Technology in Shaanxi, China under Grant 20230129, in part by the Guangdong High-level Innovation Research Institution Project under Grant 2021B0909050008, and in part by the Guangzhou Key Research and Development Program under Grant 202206030003. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Frank Hutter, Lars Kotthoff, and Joaquin Vanschoren. Automated machine learning: methods, systems, challenges. Springer Nature, 2019. ", "page_idx": 9}, {"type": "text", "text": "[2] Felipe Petroski Such, Vashisht Madhavan, Edoardo Conti, Joel Lehman, Kenneth O Stanley, and Jeff Clune. Deep neuroevolution: Genetic algorithms are a competitive alternative for training deep neural networks for reinforcement learning. arXiv preprint arXiv:1712.06567, 2017.   \n[3] Jiacheng Chen, Zeyuan Ma, Hongshu Guo, Yining Ma, Jie Zhang, and Yue-Jiao Gong. SYMBOL: Generating flexible black-box optimizers through symbolic equation learning. In The Twelfth International Conference on Learning Representations, 2024. URL https: //openreview.net/forum?id $\\cdot$ vLJcd43U7a. [4] Zeyuan Ma, Hongshu Guo, Jiacheng Chen, Zhenrui Li, Guojun Peng, Yue-Jiao Gong, Yining Ma, and Zhiguang Cao. Metabox: A benchmark platform for meta-black-box optimization with reinforcement learning. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages 10775\u201310795. Curran Associates, Inc., 2023.   \n[5] Qing Ye, Yanan Sun, Jixin Zhang, and Jiancheng Lv. A distributed framework for ea-based nas. IEEE Transactions on Parallel and Distributed Systems, 32(7):1753\u20131764, 2021. doi: 10.1109/TPDS.2020.3046774.   \n[6] Hongshu Guo, Yining Ma, Zeyuan Ma, Jiacheng Chen, Xinglin Zhang, Zhiguang Cao, Jun Zhang, and Yue-Jiao Gong. Deep reinforcement learning for dynamic algorithm selection: A proof-of-principle study on differential evolution. IEEE Transactions on Systems, Man, and Cybernetics: Systems, pages 1\u201313, 2024. doi: 10.1109/TSMC.2024.3374889.   \n[7] Yutian Chen, Xingyou Song, Chansoo Lee, Zi Wang, Richard Zhang, David Dohan, Kazuya Kawakami, Greg Kochanski, Arnaud Doucet, Marc\u2019aurelio Ranzato, et al. Towards learning universal hyperparameter optimizers with transformers. Advances in Neural Information Processing Systems, 35:32053\u201332068, 2022.   \n[8] Siddarth Krishnamoorthy, Satvik Mehul Mashkaria, and Aditya Grover. Generative pretraining for black-box optimization. arXiv preprint arXiv:2206.10786, 2022. [9] Siddarth Krishnamoorthy, Satvik Mehul Mashkaria, and Aditya Grover. Diffusion models for black-box optimization. arXiv preprint arXiv:2306.07180, 2023.   \n[10] Robert Lange, Tom Schaul, Yutian Chen, Chris Lu, Tom Zahavy, Valentin Dalibard, and Sebastian Flennerhag. Discovering attention-based genetic algorithms via meta-black-box optimization. In Proceedings of the Genetic and Evolutionary Computation Conference, pages 929\u2013937, 2023.   \n[11] Robert Tjarko Lange, Tom Schaul, Yutian Chen, Tom Zahavy, Valentin Dalibard, Chris Lu, Satinder Singh, and Sebastian Flennerhag. Discovering evolution strategies via meta-black-box optimization. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=mFDU0fP3EQH.   \n[12] Nikolaus Hansen. The cma evolution strategy: A tutorial. arXiv preprint arXiv:1604.00772, 2016.   \n[13] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In International conference on machine learning, pages 1126\u20131135. PMLR, 2017.   \n[14] John H Holland. Genetic algorithms. Scientific american, 267(1):66\u201373, 1992.   \n[15] Nikolaus Hansen and Andreas Ostermeier. Completely derandomized self-adaptation in evolution strategies. Evolutionary Computation, 9(2):159\u2013195, 2001.   \n[16] Nikolaus Hansen, Sibylle D M\u00fcller, and Petros Koumoutsakos. Reducing the time complexity of the derandomized evolution strategy with covariance matrix adaptation (cma-es). Evolutionary computation, 11(1):1\u201318, 2003.   \n[17] Raymond Ros and Nikolaus Hansen. A simple modification in cma-es achieving linear time and space complexity. In International Conference on Parallel Problem Solving from Nature, pages 296\u2013305. Springer, 2008.   \n[18] James Kennedy and Russell Eberhart. Particle swarm optimization. In Proceedings of ICNN\u201995- International Conference on Neural Networks, volume 4, pages 1942\u20131948. IEEE, 1995.   \n[19] Yue-Jiao Gong, Jing-Jing Li, Yicong Zhou, Yun Li, Henry Shu-Hung Chung, Yu-Hui Shi, and Jun Zhang. Genetic learning particle swarm optimization. IEEE Transactions on Cybernetics, 46(10):2277\u20132290, 2015.   \n[20] Rainer Storn and Kenneth Price. Differential evolution\u2013a simple and efficient heuristic for global optimization over continuous spaces. Journal of global optimization, 11:341\u2013359, 1997.   \n[21] Vladimir Stanovov, Shakhnaz Akhmedova, and Eugene Semenkin. Nl-shade-lbc algorithm with linear parameter adaptation bias change for cec 2022 numerical optimization. In 2022 IEEE Congress on Evolutionary Computation (CEC), pages 01\u201308. IEEE, 2022.   \n[22] Ryoji Tanabe and Alex S. Fukunaga. Improving the search performance of shade using linear population size reduction. In 2014 IEEE Congress on Evolutionary Computation (CEC), pages 1658\u20131665, 2014. doi: 10.1109/CEC.2014.6900380.   \n[23] Risheng Liu, Jiaxin Gao, Jin Zhang, Deyu Meng, and Zhouchen Lin. Investigating bi-level optimization for learning and vision from a unified perspective: A survey and beyond. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(12):10045\u201310067, 2022. doi: 10.1109/TPAMI.2021.3132674.   \n[24] Hugo Siqueira Gomes, Benjamin L\u00e9ger, and Christian Gagn\u00e9. Meta learning black-box population-based optimizers. arXiv preprint arXiv:2103.03526, 2021.   \n[25] Robert Lange, Tom Schaul, Yutian Chen, Tom Zahavy, Valentin Dalibard, Chris Lu, Satinder Singh, and Sebastian Flennerhag. Discovering evolution strategies via meta-black-box optimization. In Proceedings of the Companion Conference on Genetic and Evolutionary Computation, pages 29\u201330, 2023.   \n[26] Xingwen Zhang, Jeff Clune, and Kenneth O Stanley. On the relationship between the openai evolution strategy and stochastic gradient descent. arXiv preprint arXiv:1712.06564, 2017.   \n[27] Gresa Shala, Andr\u00e9 Biedenkapp, Noor Awad, Steven Adriaensen, Marius Lindauer, and Frank Hutter. Learning step-size adaptation in cma-es. In International Conference on Parallel Problem Solving from Nature, pages 691\u2013706. Springer, 2020.   \n[28] Bernardino Romera-Paredes, Mohammadamin Barekatain, Alexander Novikov, Matej Balog, M Pawan Kumar, Emilien Dupont, Francisco JR Ruiz, Jordan S Ellenberg, Pengming Wang, Omar Fawzi, et al. Mathematical discoveries from program search with large language models. Nature, pages 1\u20133, 2023.   \n[29] Elliot Meyerson, Mark J Nelson, Herbie Bradley, Arash Moradi, Amy K Hoover, and Joel Lehman. Language model crossover: Variation through few-shot prompting. arXiv preprint arXiv:2302.12170, 2023.   \n[30] Fei Liu, Xialiang Tong, Mingxuan Yuan, and Qingfu Zhang. Algorithm evolution using large language model. arXiv preprint arXiv:2311.15249, 2023.   \n[31] Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V Le, Denny Zhou, and Xinyun Chen. Large language models as optimizers. arXiv preprint arXiv:2309.03409, 2023.   \n[32] Joel Lehman, Jonathan Gordon, Shawn Jain, Kamal Ndousse, Cathy Yeh, and Kenneth O Stanley. Evolution through large models. In Handbook of Evolutionary Machine Learning, pages 331\u2013366. Springer, 2023.   \n[33] Fei Liu, Xialiang Tong, Mingxuan Yuan, Xi Lin, Fu Luo, Zhenkun Wang, Zhichao Lu, and Qingfu Zhang. Evolution of heuristics: Towards efficient automatic algorithm design using large language mode, 2024.   \n[34] Yecheng Jason Ma, William Liang, Guanzhi Wang, De-An Huang, Osbert Bastani, Dinesh Jayaraman, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Eureka: Human-level reward design via coding large language models. arXiv preprint arXiv:2310.12931, 2023.   \n[35] Angelica Chen, David M Dohan, and David R So. Evoprompting: Language models for code-level neural architecture search. arXiv preprint arXiv:2302.14838, 2023.   \n[36] Muhammad U Nasir, Sam Earle, Julian Togelius, Steven James, and Christopher Cleghorn. Llmatic: Neural architecture search via large language models and quality-diversity optimization. arXiv preprint arXiv:2306.01102, 2023.   \n[37] Beichen Huang, Xingyu Wu, Yu Zhou, Jibin Wu, Liang Feng, Ran Cheng, and Kay Chen Tan. Exploring the true potential: Evaluating the black-box optimization capability of large language models. arXiv preprint arXiv:2404.06290, 2024.   \n[38] Zeyuan Ma, Hongshu Guo, Jiacheng Chen, Guojun Peng, Zhiguang Cao, Yining Ma, and Yue-Jiao Gong. Llamoco: Instruction tuning of large language models for optimization code generation, 2024. URL https://arxiv.org/abs/2403.01131.   \n[39] Fei Liu, Xialiang Tong, Mingxuan Yuan, Xi Lin, Fu Luo, Zhenkun Wang, Zhichao Lu, and Qingfu Zhang. Evolution of heuristics: Towards efficient automatic algorithm design using large language model, 2024. URL https://arxiv.org/abs/2401.02051.   \n[40] Tung Nguyen and Aditya Grover. Transformer neural processes: Uncertainty-aware meta learning via sequence modeling, 2023. URL https://arxiv.org/abs/2207.04179.   \n[41] Tung Nguyen, Sudhanshu Agrawal, and Aditya Grover. Expt: synthetic pretraining for few-shot experimental design. Advances in Neural Information Processing Systems, 36, 2024.   \n[42] Tung Nguyen and Aditya Grover. Lico: Large language models for in-context molecular optimization. arXiv preprint arXiv:2406.18851, 2024.   \n[43] Radha Thangaraj, Millie Pant, and Ajith Abraham. A simple adaptive differential evolution algorithm. In 2009 world congress on nature & biologically inspired computing (nabic), pages 457\u2013462. IEEE, 2009.   \n[44] Swagatam Das, Sankha Subhra Mullick, and Ponnuthurai N Suganthan. Recent advances in differential evolution\u2013an updated survey. Swarm and evolutionary computation, 27:1\u201330, 2016.   \n[45] Ferrante Neri and Ville Tirronen. Recent advances in differential evolution: a survey and experimental analysis. Artificial intelligence review, 33:61\u2013106, 2010.   \n[46] Jiangning Zhang, Chao Xu, Jian Li, Wenzhou Chen, Yabiao Wang, Ying Tai, Shuo Chen, Chengjie Wang, Feiyue Huang, and Yong Liu. Analogous to evolutionary algorithm: Designing a unified sequence model. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, volume 34, pages 26674\u201326688. Curran Associates, Inc., 2021.   \n[47] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.   \n[48] Yang Yu, Zhenyu Lei, Yirui Wang, Tengfei Zhang, Chen Peng, and Shangce Gao. Improving dendritic neuron model with dynamic scale-free network-based differential evolution. IEEE/CAA Journal of automatica sinica, 9(1):99\u2013110, 2021.   \n[49] Jingjing Xu, Xu Sun, Zhiyuan Zhang, Guangxiang Zhao, and Junyang Lin. Understanding and improving layer normalization. Advances in Neural Information Processing Systems, 32, 2019.   \n[50] Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. arXiv preprint arXiv:1611.01144, 2016.   \n[51] Kai Wu, Penghui Liu, and Jing Liu. Decn: Automated evolutionary algorithms via evolution inspired deep convolution network, 2023.   \n[52] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.   \n[53] Hassan Ismail Fawaz, Germain Forestier, Jonathan Weber, Lhassane Idoumghar, and PierreAlain Muller. Deep learning for time series classification: a review. Data Mining and Knowledge Discovery, 33(4):917\u2013963, 2019.   \n[54] Swagatam Das and Ponnuthurai Nagaratnam Suganthan. Differential evolution: A survey of the state-of-the-art. IEEE transactions on evolutionary computation, 15(1):4\u201331, 2010.   \n[55] Nikolaus Hansen, Anne Auger, Raymond Ros, Olaf Mersmann, Tea Tu\u0161ar, and Dimo Brockhoff. Coco: A platform for comparing continuous optimizers in a black-box setting. Optimization Methods and Software, 36(1):114\u2013144, 2021.   \n[56] David Eriksson, Michael Pearce, Jacob Gardner, Ryan D Turner, and Matthias Poloczek. Scalable global optimization via local bayesian optimization. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch\u00e9-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019.   \n[57] Maria Laura Santoni, Elena Raponi, Renato De Leone, and Carola Doerr. Comparison of high-dimensional bayesian optimization algorithms on bbob. arXiv preprint arXiv:2303.00890, 2023.   \n[58] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym, 2016.   \n[59] Steffen Finck, Nikolaus Hansen, Raymond Ros, and Anne Auger. Real-parameter black-box optimization benchmarking 2009: Presentation of the noiseless functions. Technical report, Citeseer, 2010.   \n[60] Jazzbin. Geatpy: The genetic and evolutionary algorithm toolbox with high performance in python, 2020. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Preliminaries ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.1 Genetic Algorithms ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The crossover, mutation, and selection operators form the basic framework of GAs. GA starts with a randomly generated initial population. Then, genetic operations such as crossover and mutation will be carried out. After the fitness evaluation of all individuals in the population, a selection operation is performed to identify fitter individuals to undergo reproduction to generate offspring. Such an evolutionary process will be repeated until specific predefined stopping criteria are satisfied. ", "page_idx": 14}, {"type": "text", "text": "Crossover The crossover operator generates a new individual $\\mathbf{x}_{i}^{c}\\in\\mathbb{R}^{d}$ by Eq. (17), and $c r$ is the probability of the crossover operator. ", "page_idx": 14}, {"type": "equation", "text": "$$\nx_{k}^{c}={\\binom{x_{k}^{i}}{x_{k}^{j}}}\\quad{\\underset{o t h e r w i s e}{r a n d(0,1)}}<c r\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $k\\in[1,\\cdots,d]$ , $i$ and $j\\in[1,2,\\dots,n]$ $(i\\neq j)$ ). $d$ represents the dimension of the problem. $\\boldsymbol{x}_{k}^{i}$ and $\\v x_{k}^{j}$ represent the $k$ -th element of $\\mathbf{x}^{i}$ and $\\mathbf{x}^{j}$ respectively. This operator is commonly conducted on $n$ individuals; $n$ represents the population size. After an expression expansion, we reformulate Eq. (17) as $\\textstyle\\sum_{i=1}^{n}\\mathbf{x}_{i}\\mathbf{W}_{i}^{c}$ [46]. $\\mathbf x_{i}\\in\\mathbb R^{d}$ represents the ith individual in $\\mathbf{X}$ , where $\\mathbf{X}=\\{\\mathbf{x}_{1},\\mathbf{x}_{2},\\cdots,\\mathbf{x}_{n}\\}$ is a population. ${\\bf W}_{i}^{c}\\in\\mathbb{R}^{d\\times d}$ is the diagonal matrix. If $\\mathbf{W}_{i}^{c}$ is full of zeros, the $i$ th individual has no contribution. ", "page_idx": 14}, {"type": "text", "text": "Mutation The mutation operator brings about random changes in the population. Specifically, an individual $\\mathbf{x}_{i}$ in the population goes through the mutation operator to form the new individual $\\mathbf{x}_{i}^{m}$ , formulated as follows: ", "page_idx": 14}, {"type": "equation", "text": "$$\nx_{k}^{m}=\\bigg\\{r a n d(l_{k},u_{k})\\quad r a n d(0,1)<m r\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $m r$ is the probability of mutation operator and $k\\in[1,\\cdots,d]$ . $\\boldsymbol{x}_{k}^{m}$ and $\\v x_{k}^{c}$ represent the $k$ -th element of $\\mathbf{x}^{m}$ and $\\mathbf{x}^{c}$ respectively. Similarly, Equation (18) can be reformulated as $\\mathbf{x}_{i}^{c}\\mathbf{W}_{i}^{m}$ , where $\\mathbf{W}_{i}^{m}\\in\\mathbb{R}^{d\\times d}$ is the diagonal matrix. ", "page_idx": 14}, {"type": "text", "text": "Selection We introduce the binary tournament mating selection operator in Eq. (19). The selection operator survives individuals of higher quality for the next generation until the number of individuals is chosen. As shown in Eq. (19), ", "page_idx": 14}, {"type": "equation", "text": "$$\np_{i}=\\left\\{1\\right.\\;\\;\\;f(\\mathbf{x}_{i})<f(\\mathbf{x}_{k})\\;,\\;\\;(\\mathbf{x}_{i},\\mathbf{x}_{k})\\in\\mathbf{X}\\cup\\mathbf{X}^{m}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $p_{i}$ reflects the probability that $\\mathbf{x}_{i}$ is selected for the next generation, and $\\begin{array}{r l}{\\mathbf{X}^{m}}&{{}=}\\end{array}$ $\\{\\mathbf{x}_{1}^{m},\\mathbf{x}_{2}^{m},\\cdot\\cdot\\cdot\\mathbf{\\delta},\\mathbf{x}_{n}^{m}\\}$ . ", "page_idx": 14}, {"type": "text", "text": "A.2 Mutation Strategy in DE ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The core components of the optimization model include modules that generate solutions and modules that select solutions. GA and DE basically include crossover modules, mutation modules and selection modules. The evolutionary strategy represented by CMA-ES needs to sample a population from a certain distribution (such as Gaussian distribution), and further select individuals to update this distribution. In this paper, we design parameterized trainable LMM and LCM as modules for generating solutions. The function of LMM is to generate a candidate population, and LCM further performs crossover between the candidate population and the original population to obtain the offspring population. ", "page_idx": 14}, {"type": "text", "text": "We list some classic DE mutation strategies. ", "page_idx": 14}, {"type": "text", "text": "\u2022 DE/rand/1 ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbf{v}_{i}^{t}=\\mathbf{x}_{r1}^{t}+F\\cdot\\left(\\mathbf{x}_{r2}^{t}-\\mathbf{x}_{r3}^{t}\\right)\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "\u2022 DE/rand/2 ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbf{v}_{i}^{t}=\\mathbf{x}_{r1}^{t}+F\\cdot(\\mathbf{x}_{r2}^{t}-\\mathbf{x}_{r3}^{t}+\\mathbf{x}_{r4}^{t}-\\mathbf{x}_{r5}^{t})\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "\u2022 DE/best/1 ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbf{v}_{i}^{t}=\\mathbf{x}_{b e s t}^{t}+\\boldsymbol{F}\\cdot(\\mathbf{x}_{r1}^{t}-\\mathbf{x}_{r2}^{t})\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "\u2022 DE/current-to-rand/1 ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbf{v}_{i}^{t}=(1-F)\\mathbf{x}_{i}^{t}+F\\cdot(\\mathbf{x}_{r1}^{t}-\\mathbf{x}_{r2}^{t}+\\mathbf{x}_{r3}^{t})\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "\u2022 DE/current-to-best/1 ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbf{v}_{i}^{t}=(1-F)\\mathbf{x}_{i}^{t}+F\\cdot\\mathbf{x}_{b e s t}^{t}+F\\cdot(\\mathbf{x}_{r1}^{t}-\\mathbf{x}_{r2}^{t})\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "\u2022 DE/current-to-pbest/1 ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbf{v}_{i}^{t}=(1-F)\\mathbf{x}_{i}^{t}+F\\cdot\\mathbf{x}_{p b e s t}^{t}+F\\cdot(\\mathbf{x}_{r1}^{t}-\\mathbf{x}_{r2}^{t})\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The integer index $r1$ (and similarly, $r2$ and $r3$ ) is randomly selected from the range $[0,N]$ . pbest is randomly selected from the indices of the best $p$ individuals. $x_{b e s t}^{t}$ is the individual with the best fitness in the population at generation $t$ . ", "page_idx": 15}, {"type": "text", "text": "The generalized form of the mutation strategy is ", "page_idx": 15}, {"type": "equation", "text": "$$\n{\\bf v}_{i}^{t}=\\sum_{j}^{N}w_{i,j}{\\bf x}_{j}\\quad(\\forall w_{i,j}\\in\\mathbb{R})\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "For example, when $w_{i,q}=1,w_{i,k}=-w_{i,j}\\neq0$ , and $w_{i,l}=0\\,(\\forall l\\not\\in\\{q,j,k\\},q\\not=k,k\\not=j,q\\not=j)$ , it becomes DE/rand1/1. If individuals of the population has been sorted from good to bad by ftiness, when $w_{i,0}=1$ , $w_{i,k}=-w_{i,j}\\neq0$ , and $w_{i,l}=\\bar{0}\\,(\\forall l\\not\\in\\{0,j,k\\},k\\not=j)$ , it becomes DE/best/1. ", "page_idx": 15}, {"type": "text", "text": "B Landscape Features of TF1-TF8", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The landscape features included in $T S$ are shown as follows: ", "page_idx": 15}, {"type": "text", "text": "\u2022 TF1: Unimodal   \n\u2022 TF2: Separable   \n\u2022 TF3: Unimodal, Separable   \n\u2022 TF4: Unimodal, Separable   \n\u2022 TF5: Multimodal, Non-separable, Having a very narrow valley from local optimum to global optimum, Ill-conditioned   \n\u2022 TF6: Multi-modal, Non-separable, Rotated   \n\u2022 TF7: Multimodal, Separable, Asymmetrical, Local optima\u2019s number is huge   \n\u2022 TF8: Multi-modal, Non-separable, Asymmetrical ", "page_idx": 15}, {"type": "text", "text": "C Test Set ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "C.1 BBOB ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "BBOB [59, 55] is a widely researched and recognized collection of benchmark test problems to evaluate the performance of optimization algorithms. The dataset consists of a series of highdimensional continuous optimization functions, including single-peak, multi-peak, rotated, and distorted functions, as well as some functions with specific properties such as Lipschitz continuity and second-order differentiability. ", "page_idx": 15}, {"type": "text", "text": "C.2 Robot Control Tasks ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We test the performance of POM on two complex robot control tasks. ", "page_idx": 15}, {"type": "text", "text": "C.2.1 Bipedal Walker ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "The continuous control task Bipedal Walker [58], implemented within the Box2D physics engine, has been designed to test the ability of walking agents to navigate varying terrain by controlling their joints and maintaining balance. The challenge requires the agent to learn efficient walking strategies that enable it to traverse the intended path without falling or deviating from its trajectory. The robot\u2019s state comprises a range of variables, including the hull angle speed, angular velocity, horizontal speed, vertical speed, joint positions and angular speeds, legs contact with the ground, and lidar rangefinder measurements. The robot\u2019s actions involve determining motor speed values in the range of [-1, 1] for each of the four joints at the hips and knees. The performance of the agent is evaluated through a reward system, whereby it receives points for moving forward, with a maximum of ${300+}$ points awarded upon successfully reaching the end of the designated course. However, the penalty of -100 points is imposed if the robot loses balance and falls. Furthermore, applying motor torque incurs a small cost in terms of points. The score accrued by the agent serves as a measure of its optimal performance. The Bipedal Walker task represents a challenging and dynamic environment that effectively evaluates the walking and balance control abilities of agents. As such, it provides a valuable benchmark for testing and comparing different reinforcement learning algorithms for robotic locomotion. ", "page_idx": 16}, {"type": "text", "text": "C.2.2 Enduro ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Enduro [58] is one of the classic reinforcement learning environments provided by OpenAI Gym. It is a driving racing game based on the Atari 2600 game. In this environment, your goal is to drive as far as possible by controlling the car. The Enduro game is set on an endless highway where you need to avoid other vehicles and overtake as many other vehicles as possible within a limited time. You can avoid collisions with other vehicles by moving your car left and right, and be careful to control your speed to avoid accidents. The game rewards you based on how far you drive, so your goal is to learn a good driving strategy to maximize the distance traveled. ", "page_idx": 16}, {"type": "text", "text": "In these two test tasks, the agent interacts with the environment for $k$ time steps, and the reward at the $i$ -th step is $r_{i}$ . We evaluate strategy performance as follows: ", "page_idx": 16}, {"type": "equation", "text": "$$\nR=\\sum_{i=0}^{k}r_{i}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "In these two tasks we conduct 10 sets of experiments, each set of experiments consists of 5 independent runs. We finally take the best results of each set of experiments to calculate the mean and standard deviation. ", "page_idx": 16}, {"type": "text", "text": "D Baselines ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Our core is the population-based pre-training BBO algorithm, so we do not compare with nonpopulation methods such as Bayesian optimization methods. Moreover, Bayesian optimization methods are difficult to deal with continuous optimization problems of more than 100 dimensions. We do not use LLM-based approaches [28\u201332, 34\u201336] as baselines because they can only be used for a specific type of task. ", "page_idx": 17}, {"type": "text", "text": "Heuristic Population-based BBO Algorithm. DE(DE/rand/1/bin) [54], $\\mathrm{ES}((\\mu,\\lambda)$ -ES), L-SHADE [22], and CMA-ES [12], where DE [54] and ES are implemented based on Geatpy [60], CMA-ES and IPOP-CMA-ES are implemented by cmaes2, and L-SHADE is implemented by pyade3. The reasons for choosing these baselines are the following: ", "page_idx": 17}, {"type": "text", "text": "\u2022 DE(DE/rand/1/bin): A classic numerical optimization algorithm.   \n\u2022 ES( $(\\mu,\\lambda)$ -ES): A classic variant of the evolution strategy.   \n\u2022 CMA-ES: CMA-ES is often considered the state-of-the-art method for continuous domain optimization under challenging settings (e.g., ill-conditioned, non-convex, non-continuous, multimodal).   \n\u2022 L-SHADE: The state-of-the-art variant of DE. ", "page_idx": 17}, {"type": "text", "text": "Pretrained BBO Algorithm. We chose three state-of-the-art meta-learn BBO algorithms for comparison with POM. ", "page_idx": 17}, {"type": "text", "text": "\u2022 LES [25]: A recently proposed learnable ES. It uses a data-driven approach to discover new ES with strong generalization performance and search efficiency.   \n\u2022 LGA [10]: A recently proposed learnable GA that discovers new GA in a data-driven manner. The learned algorithm can be applied to unseen optimization problems, search dimensions, and evaluation budgets.   \n\u2022 We train POM on $T S$ . During training, the maximum number of evolution generations is 100, $n=100$ and the problem dimension is set to 10. ", "page_idx": 17}, {"type": "text", "text": "E Parameters and Training Dataset ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "The primary control parameters of CMA-ES and L-SHADE are automatically adjusted. For LGA and LES, we utilized the optimal parameters provided by the authors without modifications. Other hyperparameters were tuned using grid search to identify the optimal combinations, and multiple experiments were conducted accordingly. Detailed parameter settings are presented in Table 1. Each experiment reports the mean and standard deviation of the results from various sets of experiments, with a consistent population size of 100 across all trials. All experiments are performed on a device with GeForce RTX 3090 24G GPU, Intel Xeon Gold 6126 CPU and 64G RAM. ", "page_idx": 18}, {"type": "table", "img_path": "fWQhXdeuSG/tmp/54cdb1ee4d8abff22b412012193b68edc0c3ca5c4fcfcc11635ae42c0ff1e3ac.jpg", "table_caption": ["Table 1: Detailed parameter settings for all baselines. "], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "fWQhXdeuSG/tmp/7c8f07693ecd2cad9560720056fa2f8c3a086c3533a93b80d8e0fd76bbf748db.jpg", "table_caption": ["Table 2: POM parameters of different architectures and architecture settings. "], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "fWQhXdeuSG/tmp/fbb9123fbdaa506c8c88774f93d9f4ce9727b9fb322d972ed7428e6e38d4182d.jpg", "table_caption": ["Table 3: Additional Training Functions. $z_{i}=x_{i}-\\omega_{i}$ . "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "F Additional Experimental results on BBOB ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "F.1 BBOB Test ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Table 4: BBOB RESULT. POM is trained on TF1-TF5 with $d\\!=\\!10$ . The best results are indicated in bold, and the suboptimal results are underlined. ", "page_idx": 19}, {"type": "table", "img_path": "fWQhXdeuSG/tmp/6765c920a443d132b28e38d878377f28ebd498522507da81562cdd32737c8517.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "F.2 BBOB Test With Optimal Solution Disturbed ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We further test the performance of the algorithm on the BBOB. Here, the optimal solution of each function is randomly disturbed, that is, $\\mathbf{x}^{*}=\\mathbf{x}_{o p t}+\\mathbf{z}$ , where $\\mathbf{x}^{*}$ represents the optimal solution after disturbing, $\\mathbf{x}_{o p t}$ represents the original optimal solution, $\\mathbf{x}_{o p t}$ is a vector obtained by random sampling and $\\textbf{z}\\in\\bar{[-1,1]}^{d}$ . The results are displayed in Table 5 and Figure 10. We found that the performance of POM can still dominate other algorithms when the function optimal solution is disturbed. ", "page_idx": 20}, {"type": "image", "img_path": "fWQhXdeuSG/tmp/bee722bcf15a3b6069e75d41e1ca28260e9d7f4d7b944fbda87d035d09628a74.jpg", "img_caption": [], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "Figure 10: Critical difference diagram of 3 algorithms on 24 BBOB problems with $d=100$ . The locations of the optimal solutions are in the range of $[-1,1]$ . ", "page_idx": 20}, {"type": "text", "text": "Table 5: Additional Experimental results on BBOB ( $d=100,$ ). The best results are indicated in bold, and the suboptimal results are underlined. ", "page_idx": 20}, {"type": "table", "img_path": "fWQhXdeuSG/tmp/009b54205f29c1d8e26311503c63f58a0795b153450ab0f4ecf0d16b97a74419.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "fWQhXdeuSG/tmp/29ea25b08535fc514843c9fe325e2c2840e63dd37db46cc97151bc8b7f2e9934.jpg", "img_caption": ["Figure 11: Critical difference diagram of 7 algorithms on 24 BBOB problems with $d=500$ . "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "Table 6: Additional Experimental results on BBOB $d=500)$ ). The best results are indicated in bold, and the suboptimal results are underlined. ", "page_idx": 21}, {"type": "table", "img_path": "fWQhXdeuSG/tmp/6619399da0d40576c507afe7d0ce8aca2db9ee4fefd1221ad6a1adae538f6642.jpg", "table_caption": [], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "G Compare with TurBO ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We compared POM and Bayesian optimization algorithms, finding that Bayesian optimization converges very slowly on high-dimensional problems. TurBO [56], noted for its fast convergence and strong performance [57], was used as a benchmark. Although TurBO requires substantial time for 10,000 evaluations, POM completes the same task in under one second. Therefore, we plotted the convergence curves of TurBO and POM with only 3,100 evaluations. As shown in Figure 14, POM demonstrates significant performance advantages over TurBO in most cases. ", "page_idx": 21}, {"type": "image", "img_path": "fWQhXdeuSG/tmp/9d4c9bbd95388153487715dac676497b56dc9bb3cc92935221cc64d2738f74bf.jpg", "img_caption": ["Figure 12: The log convergence curves of POM and other baselines. It shows the convergence curve of these algorithms on functions in BBOB with $d=30$ . "], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "fWQhXdeuSG/tmp/5599ce15a8a5f0516b7afc991c93bb758c1d0e8ec1e22f0184da8ee62fe6ec3a.jpg", "img_caption": ["Figure 13: The log convergence curves of POM and other baselines. It shows the convergence curve of these algorithms on the functions in BBOB with $d=100$ . "], "img_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "fWQhXdeuSG/tmp/c2d906dacfbed0d9273961a03a4759758fc77141bfc33bc5cbcdbc3f3ea7d9e4.jpg", "img_caption": ["Figure 14: The log convergence curves of POM and TurBO. It shows the convergence curve of these algorithms on functions in BBOB with $d=100$ . "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "H Results of Analysis Study ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Table 7: Results of ablation experiments. The best results are indicated in bold, and the suboptimal results are underlined. Here $d=30$ . ", "page_idx": 25}, {"type": "table", "img_path": "fWQhXdeuSG/tmp/982714c09aa9951309bfbeb312e110762b35ccb71a8554ed2b25262e1e3ca47b.jpg", "table_caption": [], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "Table 8: Results of Training Dataset Experiments. The best results are indicated in bold, and the suboptimal results are underlined. ", "page_idx": 25}, {"type": "table", "img_path": "fWQhXdeuSG/tmp/cde124356fb5e77fe07f0da640fcc794fdc7de993516c99aa4f43fe9e9a75cc1.jpg", "table_caption": [], "table_footnote": [], "page_idx": 25}, {"type": "image", "img_path": "fWQhXdeuSG/tmp/0fd0004a23591ce2c619ac08468c8c27c63369b9d5455ea0c579bc782e19ff6c.jpg", "img_caption": ["Figure 15: Visualized results of mutation strategy $S^{t}$ on BBOB (F1-F4) with $d=100$ . Here, $n=10$ for the sake of clarity. The blank squares in the matrix indicate the masked parts in Eq. (8). Steps 1, 50 and 100 correspond to the 1st, 50th and 100th generations in the population evolution process. The horizontal and vertical axes show the ranking of individuals, with 1 being the best and 10 being the worst in the population. Each row represents the weight assigned to other individuals when performing mutation operations for the corresponding individual. "], "img_footnote": [], "page_idx": 26}, {"type": "image", "img_path": "fWQhXdeuSG/tmp/8b9dacf9d3b1e2be2053910b34f82cbbad461ce1240267f06177f30cf3ae0601.jpg", "img_caption": ["Figure 16: Visualized results of mutation strategy $S^{t}$ on BBOB (F5-F8) with $d=100$ "], "img_footnote": [], "page_idx": 27}, {"type": "image", "img_path": "fWQhXdeuSG/tmp/8ce4c1b28a079619c8e4cc7035dbd9cfbc24b9b8c43615abbef23b838bfd0846.jpg", "img_caption": ["Figure 17: Visualized results of mutation strategy $S^{t}$ on BBOB (F9-F12) with $d=100$ . "], "img_footnote": [], "page_idx": 28}, {"type": "image", "img_path": "fWQhXdeuSG/tmp/dcff97e10cb372eb8e7984532f55e33065b98928f92212ff5e5ae616d0ea9839.jpg", "img_caption": ["Figure 18: Visualized results of mutation strategy $S^{t}$ on BBOB (F13-F16) with $d=100$ . "], "img_footnote": [], "page_idx": 29}, {"type": "image", "img_path": "fWQhXdeuSG/tmp/a1cd09f483d30b0b9b11e6af5e6b3a30fe4326ac0ad4df9cd9609c7e9dc01d40.jpg", "img_caption": ["Figure 19: Visualized results of mutation strategy $S^{t}$ on BBOB (F17-F20) with $d=100$ . "], "img_footnote": [], "page_idx": 30}, {"type": "image", "img_path": "fWQhXdeuSG/tmp/cead65580694274cfccaf18ff207b45c9eed79b87352049c725e367464e70c03.jpg", "img_caption": ["Figure 20: Visualized results of mutation strategy $S^{t}$ on BBOB (F21-F24) with $d=100$ . "], "img_footnote": [], "page_idx": 31}, {"type": "text", "text": "Table 9: Results of POMs of different sizes on BBOB tests $d=100)$ ). The best results are indicated in bold, and the suboptimal results are underlined. ", "page_idx": 32}, {"type": "table", "img_path": "fWQhXdeuSG/tmp/389cc2d568e5d09abfedafa56b608ff6e42faf28741ca4e91540b99d237c73b2.jpg", "table_caption": [], "table_footnote": [], "page_idx": 32}, {"type": "image", "img_path": "fWQhXdeuSG/tmp/2b86b4a728b38f63864a337eaa2d4ccbac760cdc99750958dee045fbbeed4733.jpg", "img_caption": [""], "img_footnote": [], "page_idx": 32}, {"type": "text", "text": "Figure 21: Results of a visual analysis of LCM on BBOB with $d=100$ . Here, $n=100$ . This is the crossover strategy of the individual ranked No. 1. Rank denotes the ranking of an individual. A subgraph illustrates the change in the probability of an individual crossing three tasks as the population evolves. ", "page_idx": 32}, {"type": "image", "img_path": "fWQhXdeuSG/tmp/df72894766240f3c6448e319905455c32e2be032e96b1442b094743f2c29c99a.jpg", "img_caption": ["(e) F13 & F14 & F15 (f) F16 & F17 & F18 (g) F19 & F20 & F21 (h) F22 & F23 & F24 "], "img_footnote": [], "page_idx": 33}, {"type": "text", "text": "Figure 22: Results of a visual analysis of LCM on BBOB with $d=100$ . Here, $n=100$ . This is the crossover strategy of the individual ranked No. 5. Rank denotes the ranking of an individual. A subgraph illustrates the change in the probability of an individual crossing three tasks as the population evolves. ", "page_idx": 33}, {"type": "image", "img_path": "fWQhXdeuSG/tmp/8ce923557e97181f4ca33b9c109237195ee7d66a4f330055cc4592bcd681d596.jpg", "img_caption": ["(e) F13 & F14 & F15 (f) F16 & F17 & F18 (g) F19 & F20 & F21 (h) F22 & F23 & F24 "], "img_footnote": [], "page_idx": 33}, {"type": "text", "text": "Figure 23: Results of a visual analysis of LCM on BBOB with $d=100$ . Here, $n=100$ . This is the crossover strategy of the individual ranked No. 18. Rank denotes the ranking of an individual. A subgraph illustrates the change in the probability of an individual crossing three tasks as the population evolves. ", "page_idx": 33}, {"type": "image", "img_path": "fWQhXdeuSG/tmp/ec0d4f2714dae803a0e8c146dba0114f7fba5baa5207fe8fc2c110bc9affebf8.jpg", "img_caption": ["(e) F13 & F14 & F15 (f) F16 & F17 & F18 (g) F19 & F20 & F21 (h) F22 & F23 & F24 "], "img_footnote": [], "page_idx": 34}, {"type": "text", "text": "Figure 24: Results of a visual analysis of LCM on BBOB with $d=100$ . Here, $n=100$ . This is the crossover strategy of the individual ranked No. 51. Rank denotes the ranking of an individual. A subgraph illustrates the change in the probability of an individual crossing three tasks as the population evolves. ", "page_idx": 34}, {"type": "image", "img_path": "fWQhXdeuSG/tmp/fb4f4452b864f6350b6085944e54321f13cb7fed18e4c725b80a388e34264f73.jpg", "img_caption": ["Figure 25: Results of a visual analysis of LCM on BBOB with $d=100$ . Here, $n=100$ . This is the crossover strategy of the individual ranked No. 75. Rank denotes the ranking of an individual. A subgraph illustrates the change in the probability of an individual crossing three tasks as the population evolves. ", ""], "img_footnote": [], "page_idx": 34}, {"type": "image", "img_path": "fWQhXdeuSG/tmp/2733e8f1f9c7eb104eaa4d1fe4100c6b6128bfd1b2800d67eab506cd96e96dc2.jpg", "img_caption": ["Figure 26: Results of a visual analysis of LCM on BBOB with $d=100$ . Here, $n=100$ . This is the crossover strategy of the individual ranked No. 100. Rank denotes the ranking of an individual. A subgraph illustrates the change in the probability of an individual crossing three tasks as the population evolves. "], "img_footnote": [], "page_idx": 35}, {"type": "text", "text": "I Limitations ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "\u2022 Model size: In the experiment, we found that the relationship between the model size and the performance of POM is not a strict linear relationship. Although the larger the model, the more difficult it is to train, there is still no very quantitative design criterion between model size, training data volume and training difficulty.   \n\u2022 Time performance: We introduced an operation similar to the attention mechanism, whose time complexity is $O(n^{2})$ , which makes POM require a lot of time cost when processing large-scale populations. How to reduce and improve the time efficiency of POM is also worthy of further study. ", "page_idx": 35}, {"type": "text", "text": "J Potential Impact ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here. ", "page_idx": 35}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: Yes, we accurately reflect the contribution and scope of the paper. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 36}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Justification: We discussed the limitations of work in the experimental analysis part. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 36}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: Yes, we have a complete experimental proof. Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 37}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Justification: We give all the details required to reproduce the main experimental results, see section 4.1 for details. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 37}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: We provide source code, and the data sets used are public data sets. Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 38}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: We provide all the details (see section 3.3 for details). Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 38}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Justification: Yes, we provide statistical experimental results. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 38}, {"type": "text", "text": "", "page_idx": 39}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: Yes, we provide device resources information and time analysis (see section 4). ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 39}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: Our research is in line with Neurips Code of Ethics. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 39}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: See the appendix J for specific content. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 39}, {"type": "text", "text": "", "page_idx": 40}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 40}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 40}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: We respect the relevant original author and the open source agreement. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 40}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 41}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 41}, {"type": "text", "text": "Justification: We provide anonymous code link ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 41}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 41}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 41}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 41}]