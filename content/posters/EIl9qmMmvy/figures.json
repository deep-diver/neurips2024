[{"figure_path": "EIl9qmMmvy/figures/figures_1_1.jpg", "caption": "Figure 1: Illustration of the guide-then-select paradigm", "description": "This figure illustrates the difference between the guide-then-select paradigm used in Diffusion-DICE and the guide-only or select-only approaches used in previous diffusion-based offline RL methods.  The guide-then-select approach uses in-sample actions for training, minimizing error exploitation in the value function. In contrast, guide-only methods use predictions of actions' values to guide toward high-return actions, and select-only methods sample many actions and choose the best one according to the value function, both introducing potential error.", "section": "3 Diffusion-DICE"}, {"figure_path": "EIl9qmMmvy/figures/figures_6_1.jpg", "caption": "Figure 2: Toycase of a 2-D bandit problem. The action in the offline dataset follows a bivariate standard normal distribution constrained within an annular region. The ground truth reward has two peaks extending from the center outward. We use a diffusion model to fit the behavior policy and a reward model R to fit the ground truth reward R. Both and R fit in-distribution data well while making error in out-of-distribution regions. Diffusion-DICE could generate correct optimal actions in the outer circle while other methods tend to exploit error information from R and only generate overestimated, sub-optimal actions.", "description": "This figure shows a visualization of a 2D bandit problem used to illustrate the differences between Diffusion-DICE and other offline RL algorithms.  The dataset actions are sampled from a bivariate normal distribution, confined within a ring.  The true reward function has two peaks in the outer ring. The figure displays the dataset action distribution, a diffusion model fit to the behavior policy, the true and learned reward functions, and the actions sampled by Diffusion-DICE, QGPO, and IDQL.  Diffusion-DICE successfully generates actions in the high-reward outer ring, while the others are misled by errors in the learned reward function and generate suboptimal actions in the low-reward inner region. This highlights Diffusion-DICE's ability to avoid error exploitation.", "section": "3 Diffusion-DICE"}, {"figure_path": "EIl9qmMmvy/figures/figures_8_1.jpg", "caption": "Figure 2: Toycase of a 2-D bandit problem. The action in the offline dataset follows a bivariate standard normal distribution constrained within an annular region. The ground truth reward has two peaks extending from the center outward. We use a diffusion model to fit the behavior policy and a reward model R to fit the ground truth reward R. Both and R fit in-distribution data well while making error in out-of-distribution regions. Diffusion-DICE could generate correct optimal actions in the outer circle while other methods tend to exploit error information from R and only generate overestimated, sub-optimal actions.", "description": "This figure illustrates a 2D bandit problem used to compare Diffusion-DICE against other offline RL methods.  The offline dataset consists of actions sampled from a bivariate normal distribution constrained to an annulus. The true reward function has two peaks in the outer regions of the annulus.  The figure shows that while the diffusion model and reward model accurately represent the data within the annulus, they misrepresent the data outside the annulus.  Diffusion-DICE correctly identifies the optimal actions (in the outer ring), while other methods are misled by errors in the reward model and propose suboptimal actions.", "section": "3 Diffusion-DICE"}, {"figure_path": "EIl9qmMmvy/figures/figures_25_1.jpg", "caption": "Figure 2: Toycase of a 2-D bandit problem. The action in the offline dataset follows a bivariate standard normal distribution constrained within an annular region. The ground truth reward has two peaks extending from the center outward. We use a diffusion model to fit the behavior policy and a reward model R to fit the ground truth reward R. Both and R fit in-distribution data well while making error in out-of-distribution regions. Diffusion-DICE could generate correct optimal actions in the outer circle while other methods tend to expolit error information from R and only generate overestimated, sub-optimal actions.", "description": "This figure shows a 2D bandit problem where the offline dataset's actions follow a bivariate standard normal distribution within a ring shape.  The true reward function has two peaks outside the ring.  The figure compares the performance of Diffusion-DICE, QGPO, and IDQL in learning this reward function.  It demonstrates that Diffusion-DICE successfully avoids out-of-distribution errors which hinder the other two methods.", "section": "3 Diffusion-DICE"}, {"figure_path": "EIl9qmMmvy/figures/figures_27_1.jpg", "caption": "Figure 2: Toycase of a 2-D bandit problem. The action in the offline dataset follows a bivariate standard normal distribution constrained within an annular region. The ground truth reward has two peaks extending from the center outward. We use a diffusion model to fit the behavior policy and a reward model R to fit the ground truth reward R. Both and R fit in-distribution data well while making error in out-of-distribution regions. Diffusion-DICE could generate correct optimal actions in the outer circle while other methods tend to exploit error information from R and only generate overestimated, sub-optimal actions.", "description": "This figure shows a comparison of different offline RL methods on a 2D bandit problem. The dataset is generated from a bivariate standard normal distribution constrained to an annulus. The reward function has two peaks, one in the inner and one in the outer ring.  The figure compares the learned reward function and action distributions of three methods: Diffusion-DICE, QGPO (guide-only), and IDQL (select-only), illustrating how Diffusion-DICE successfully avoids the out-of-distribution action exploitation that plagues the other methods, leading to better performance.", "section": "3 Diffusion-DICE"}, {"figure_path": "EIl9qmMmvy/figures/figures_27_2.jpg", "caption": "Figure 2: Toycase of a 2-D bandit problem. The action in the offline dataset follows a bivariate standard normal distribution constrained within an annular region. The ground truth reward has two peaks extending from the center outward. We use a diffusion model to fit the behavior policy and a reward model R to fit the ground truth reward R. Both and R fit in-distribution data well while making error in out-of-distribution regions. Diffusion-DICE could generate correct optimal actions in the outer circle while other methods tend to expolit error information from R and only generate overestimated, sub-optimal actions.", "description": "This figure illustrates a 2D bandit problem used to compare Diffusion-DICE against other offline RL methods.  The offline dataset's actions follow a bivariate normal distribution within a ring shape. The true reward function has two peaks radiating outwards.  The plots show the dataset's action distribution, the learned diffusion model's behavior policy, and the true vs. learned reward functions. The comparison highlights how Diffusion-DICE avoids errors from out-of-distribution data to select the optimal actions, unlike other methods.", "section": "3 Diffusion-DICE"}, {"figure_path": "EIl9qmMmvy/figures/figures_28_1.jpg", "caption": "Figure 2: Toycase of a 2-D bandit problem. The action in the offline dataset follows a bivariate standard normal distribution constrained within an annular region. The ground truth reward has two peaks extending from the center outward. We use a diffusion model to fit the behavior policy and a reward model R to fit the ground truth reward R. Both and R fit in-distribution data well while making error in out-of-distribution regions. Diffusion-DICE could generate correct optimal actions in the outer circle while other methods tend to expolit error information from R and only generate overestimated, sub-optimal actions.", "description": "This figure shows a 2D bandit problem where actions are sampled from a bivariate standard normal distribution within a ring. The reward function has two peaks outside the ring.  The figure compares the learned reward function and action distributions of several offline RL methods against the ground truth.  It highlights how Diffusion-DICE avoids out-of-distribution errors and successfully generates optimal actions, while others fail due to error exploitation.", "section": "3 Diffusion-DICE"}]