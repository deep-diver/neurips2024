{"references": [{"fullname_first_author": "J. Fu", "paper_title": "D4RL: Datasets for deep data-driven reinforcement learning", "publication_date": "2020-MM-DD", "reason": "This paper provides the benchmark datasets used for evaluating the proposed method and comparing it with other state-of-the-art methods."}, {"fullname_first_author": "O. Nachum", "paper_title": "Dualdice: Behavior-agnostic estimation of discounted stationary distribution corrections", "publication_date": "2019-MM-DD", "reason": "This paper introduces the DICE method, which is the foundation of the proposed method, Diffusion-DICE."}, {"fullname_first_author": "O. Nachum", "paper_title": "Reinforcement learning via Fenchel-Rockafellar duality", "publication_date": "2020-MM-DD", "reason": "This paper provides the theoretical foundation for the DICE method, which is crucial for understanding the proposed method."}, {"fullname_first_author": "L. Chen", "paper_title": "Decision transformer: Reinforcement learning via sequence modeling", "publication_date": "2021-MM-DD", "reason": "This paper introduces the decision transformer model, which is closely related to the diffusion models used in the proposed method."}, {"fullname_first_author": "J. Ho", "paper_title": "Denoising diffusion probabilistic models", "publication_date": "2020-MM-DD", "reason": "This paper introduces denoising diffusion probabilistic models, which are used in the proposed method to generate actions."}]}