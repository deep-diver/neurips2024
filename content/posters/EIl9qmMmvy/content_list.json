[{"type": "text", "text": "Diffusion-DICE: In-Sample Diffusion Guidance for Offline Reinforcement Learning ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Liyuan Mao\u2217 Haoran Xu\u2217 Shanghai Jiao Tong University UT Austin Xianyuan Zhan Weinan Zhang\u2020 Amy Zhang\u2020 Tsinghua University Shanghai Jiao Tong University UT Austin ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "One important property of DIstribution Correction Estimation (DICE) methods is that the solution is the optimal stationary distribution ratio between the optimized and data collection policy. In this work, we show that DICE-based methods can be viewed as a transformation from the behavior distribution to the optimal policy distribution. Based on this, we propose a novel approach, Diffusion-DICE, that directly performs this transformation using diffusion models. We find that the optimal policy\u2019s score function can be decomposed into two terms: the behavior policy\u2019s score function and the gradient of a guidance term which depends on the optimal distribution ratio. The first term can be obtained from a diffusion model trained on the dataset and we propose an in-sample learning objective to learn the second term. Due to the multi-modality contained in the optimal policy distribution, the transformation in Diffusion-DICE may guide towards those localoptimal modes. We thus generate a few candidate actions and carefully select from them to approach global-optimum. Different from all other diffusion-based offline RL methods, the guide-then-select paradigm in Diffusion-DICE only uses in-sample actions for training and brings minimal error exploitation in the value function. We use a didatic toycase example to show how previous diffusion-based methods fail to generate optimal actions due to leveraging these errors and how Diffusion-DICE successfully avoids that. We then conduct extensive experiments on benchmark datasets to show the strong performance of Diffusion-DICE. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We study the problem of offline reinforcement learning (RL), where the goal is to learn effective policies solely from offline data, without any additional online interactions. Offline RL is quite useful for scenarios where arbitrary exploration with untrained policies is costly or dangerous, but sufficient prior data is available, such as robotics [18] or industrial control [56]. Most previous model-free offline RL methods add a pessimism term to off-policy RL algorithms [50, 9, 2], this pessimism term acts as behavior regularization to avoid extrapolation errors caused by querying the $Q$ -function about values of potential out-of-distribution (OOD) actions produced by the policy [25]. However, explicitly adding the regularization requires careful tuning of the regularization weight because otherwise, the policy will still output actions that are not seen in the dataset. DIstribution Correction Estimation (DICE) methods [35, 41, 34] provide an implicit way of doing so. By applying convex duality, DICE-based methods solve for the optimal stationary distribution ratio between the optimized and data collection policy in an in-sample manner without querying unseen actions [55]. ", "page_idx": 0}, {"type": "text", "text": "In this paper, we extend the analysis of DICE-based methods: we show that the optimal distribution ratio in DICE-based methods can be extended to a transformation from the behavior distribution to the optimal policy distribution. This different view motivates the use of deep generative models, e.g. diffusion models [42, 15, 43], to first fit the behavior distribution using their strong expressivity of fitting multi-modal distributions and then directly perform this transformation during sampling. We find that the optimal policy\u2019s score function can be decomposed into two terms: the behavior policy\u2019s score function and the gradient of a guidance term which depends on the optimal distribution ratio learned by DICE. The first term can be easily obtained from the diffusion model trained on the offilne dataset. While it is usually intractable to find a closed-form solution of the second term, we make a subtle mathematical transformation and show its equivalence to solving a convex optimization problem. In this manner, both of these terms can be learned by only dataset samples, providing accurate guidance towards in-distribution while high-value data points. Due to the multi-modality contained in the optimal policy distribution, the transformation may guide towards those local-optimal modes due to stochasticity. We thus generate a few candidate actions and use the value function to select the max from them to go towards the global optimum. ", "page_idx": 1}, {"type": "text", "text": "We term our method Diffusion-DICE, the guidethen-select procedure in Diffusion-DICE differs from all previous diffusion-based offilne RL methods [49, 4, 14, 32], which are either only guide-based or only select-based. Guide-based methods [32] use predicted values of actions generated by the diffusion behavior policy to guide toward high-return actions. Select-based methods [4, 14] bypass the need for guidance but require sampling a large number of actions from the diffusion behavior policy and using the value function to select the optimal one. All these methods need to query the value function ", "page_idx": 1}, {"type": "image", "img_path": "EIl9qmMmvy/tmp/e7c2dd9d0e1e32940c7baf826fc967fa099724e3df599a32be4f75a942b7c67b.jpg", "img_caption": ["Figure 1: Illustration of the guide-then-select paradigm "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "of actions sampled from the diffusion behavior policy, which may produce potential OOD actions and bring overestimation errors in the guiding or selecting process. Our method, however, brings minimal error in the guide-step by using accurate in-sample guidance to generate in-distribution actions with high values, and by doing so, in the select step only a few candidate actions are needed to find the optimal one, which further reduces error exploitation. We use an illustrative toy case to demonstrate the error exploitation in previous methods and how Diffusion-DICE successfully alleviates that. We also verify the effectiveness of Diffusion-DICE in benchmark D4RL offline datasets [8]. Note that Diffusion-DICE also provides a replacement for the Gaussian policy extraction part used in current DICE methods, successfully unleashing the power of DICE-based methods. Diffusion-DICE surpasses both diffusion-based and DICE-based strong baselines, reaching SOTA performance in D4RL benchmarks [8]. We also conduct ablation experiments and validate the superiority of the guide-then-select learning procedure. ", "page_idx": 1}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We consider the RL problem presented as a Markov decision process [45], which is specified by a tuple $\\mathcal{M}=\\langle\\boldsymbol{S},\\boldsymbol{\\mathcal{A}},\\mathcal{\\bar{P}},d_{0},r,\\gamma\\rangle$ . Here $\\boldsymbol{S}$ and $\\boldsymbol{\\mathcal{A}}$ are state and action space, $\\mathcal{P}(s^{\\prime}|s,a)$ and $d_{0}$ denote transition dynamics and initial state distribution, $r(s,a)$ and $\\gamma$ represent reward function and discount factor, respectively. The goal of RL is to find a policy $\\pi(a|s)$ which maximizes expected return $\\begin{array}{r}{\\mathbb{E}[\\sum_{t=0}^{\\infty}\\gamma^{\\textit{\\qot{t}}}\\cdot r(s_{t},\\dot{a}_{t})]}\\end{array}$ . Another equivalent LP form of expected return is $\\mathbb{E}_{(s,a)\\sim d^{\\pi}}[r(s,a)]$ , where $\\begin{array}{r}{d^{\\pi}(s,a):=(1-\\gamma)\\sum_{t=0}^{\\infty}\\gamma^{t}P r(s_{t}=s,a_{t}=a)}\\end{array}$ is the normalized discounted stationary distribution of $\\pi$ [36]. For simpl icity, we refer to $d^{\\pi}(s,a)$ as the stationary distribution of $\\pi$ . Offilne RL considers the setting where interaction with the environment is prohibited, and one need to learn the optimal $\\pi$ from a static dataset $\\mathcal{D}=\\{s_{i},a_{i},r_{i},s_{i}^{\\prime}\\}_{i=1}^{N}$ . We denote the empirical behavior policy of $\\mathcal{D}$ as $\\pi^{\\mathcal{D}}$ . ", "page_idx": 1}, {"type": "text", "text": "2.1 Distribution Correction Estimation ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "DICE methods [41] incorporate the LP form of expected return $\\mathcal{I}(\\pi)\\,=\\,\\mathbb{E}_{(s,a)\\sim d^{\\pi}}[r(s,a)]$ with a regularizer $\\begin{array}{r}{D_{f}(d^{\\pi}||d^{D})\\,=\\,\\mathbb{E}_{(s,a)\\sim d^{D}}[f\\bigl(\\frac{d^{\\pi}(s,a)}{d^{D}(s,a)}\\bigr)]}\\end{array}$ , where $D_{f}$ is the $f$ -divergence induced by a ", "page_idx": 1}, {"type": "text", "text": "convex function $f$ [3]. More specifically, DICE methods try to find an optimal policy $\\pi^{*}$ that satisfies: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\pi^{*}=\\arg\\operatorname*{max}_{\\pi}\\mathbb{E}_{(s,a)\\sim d^{\\pi}}[r(s,a)]-\\alpha D_{f}(d^{\\pi}||d^{\\mathcal{D}}).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "This objective is generally intractable due to the dependency on $d^{\\pi}(s,a)$ , especially under the offilne setting. However, by imposing the Bellman-flow constraint [33] $\\begin{array}{r}{\\sum_{a\\in A}d(s,a)=(1-\\gamma)d_{0}(s)+}\\end{array}$ $\\gamma\\sum_{(s^{\\prime},a^{\\prime})}d(s^{\\prime},a^{\\prime})p(s|s^{\\prime},a^{\\prime})$ on states and applying Lagrangian duality and convex conjugate, its dual problem has the following tractable form: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{V}\\,(1-\\gamma)\\mathbb{E}_{s\\sim d_{0}}[V(s)]+\\alpha\\mathbb{E}_{(s,a)\\sim d^{\\mathcal{D}}}[f^{*}([\\mathcal{T}V(s,a)-V(s)]\\,/\\alpha)].\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Here $f^{*}$ is a variant of $f$ \u2019s convex conjugate and $\\mathcal{T}V(s,a)=r(s,a)\\!+\\!\\gamma\\mathbb{E}_{s^{\\prime}\\sim p(\\cdot|s,a)}[V(s^{\\prime})]$ represents the Bellman operator on $V$ . In practice, one often uses a prevalent semi-gradient technique in RL that estimates $\\tau V(s,a)$ with $Q(s,a)$ and replaces the initial state distribution $d_{0}$ with dataset distribution $d^{\\mathcal{D}}$ to stabilize learning [41, 34]. In addition, because $\\mathcal{D}$ usually cannot cover all possible $s^{\\prime}$ for a specific $(s,a)$ , DICE methods only use a single sample of the next state $s^{\\prime}$ . The update of $Q(s,a)$ and $V(s)$ in DICE methods are as follows and we refer to a detailed derivation in Appendix A: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{V}{\\operatorname*{min}}~\\mathbb{E}_{(s,a)\\sim d^{\\mathcal{D}}}\\left[(1-\\gamma)V(s)+\\alpha f^{*}\\big(\\left[Q(s,a)-V(s)\\right]/\\alpha\\big)\\right]}\\\\ &{\\underset{Q}{\\operatorname*{min}}~\\mathbb{E}_{(s,a,s^{\\prime})\\sim d^{\\mathcal{D}}}\\left[\\big(r(s,a)+\\gamma V(s^{\\prime})-Q(s,a)\\big)^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Note that learning objectives of DICE-methods can be calculated solely with a $(s,a,s^{\\prime})$ sample from $\\mathcal{D}$ , which is totally in-sample. One important property of DICE methods is that $Q^{*}$ and $V^{*}$ have a relationship with the optimal stationary distribution ratio $w^{*}(s,a)$ as ", "page_idx": 2}, {"type": "equation", "text": "$$\nw^{\\ast}(s,a):=\\frac{d^{\\ast}(s,a)}{d^{D}(s,a)}=\\operatorname*{max}\\big(0,(f^{\\prime})^{-1}\\big(Q^{\\ast}(s,a)-V^{\\ast}(s)\\big)\\big),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $d^{\\ast}$ is the stationary distribution of $\\pi^{*}$ . To get $\\pi^{*}$ from $w^{*}$ , previous policy extraction methods in DICE methods include weighted behavior cloning [34], information projection [27] or policy gradient [37]. All these methods parametrize an unimodal Gaussian policy in order to compute $\\log\\pi(a|s)$ [13], which greatly limits its expressivity. ", "page_idx": 2}, {"type": "text", "text": "2.2 Diffusion Models in Offline Reinforcement Learning ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Diffusion models [42, 15, 43] are generative models based on a Markovian noising and denoising process. Given a random variable $x_{0}$ and its corresponding probability distribution $q_{0}(x_{0})$ , the diffusion model defines a forward process that gradually adds Gaussian noise to the samples from $x_{0}$ to $x_{T}(T>0)$ . Kingma et al. [20] shows there exists a stochastic process that has the same transition distribution $q_{t0}(x_{t}|x_{0})$ and Song et al. [43] shows that under some conditions, this process has an equivalent reverse process from $T$ to 0. The forward process and the equivalent reverse process can be characterized as follows, where $\\bar{\\bf w}_{t}$ is a standard Wiener process in the reverse time. ", "page_idx": 2}, {"type": "equation", "text": "$$\nq_{t0}(x_{t}|x_{0})=\\mathcal{N}(x_{t}|\\alpha_{t}x_{0},\\sigma_{t}^{2}\\mathbf{I})\\quad d x_{t}=[f(t)x_{t}-g^{2}(t)\\nabla_{x_{t}}\\log q_{t}(x_{t})]d t+g(t)d\\overline{{\\mathbf{w}}}_{t},\\;x_{T}\\sim q_{T}(x_{T}).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Here we slightly abuse the subscript $t$ to denote the diffusion timestep. $\\alpha_{t},\\,\\sigma_{t}$ are the noise schedule and $f(t),\\,g(t)$ can be derived from $\\alpha_{t}$ , $\\sigma_{t}$ [31]. For simplicity, we denote $q_{t0}(x_{t}|x_{0})$ and $p_{0t}(\\boldsymbol{x}_{0}|\\boldsymbol{x}_{t})$ as $q(x_{t}|x_{0})$ and $p(x_{0}|x_{t})$ , respectively. To sample from $q_{0}(x_{0})$ by following the reverse stochastic differential equation (SDE), the score function $\\nabla_{x_{t}}\\log{q_{t}(x_{t})}$ is required. Typically, diffusion models use denoising score matching to train a neural network $\\epsilon_{\\theta}(x_{t},t)$ that estimates the score function [46, 15, 43], by minimizing $\\mathbb{E}_{t\\sim\\mathcal{U}(0,T),x_{0}\\sim q_{0}(x_{0}),\\epsilon\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{I})}[\\left|\\left|\\epsilon_{\\theta}(x_{t},t)-\\epsilon\\right|\\right|^{2}]$ , where $x_{t}=\\alpha_{t}x_{0}+\\sigma_{t}\\epsilon$ As we mainly focus on diffusion policy in RL, this objective is usually impractical because $q_{0}(x_{0})$ is expected to be the optimal policy $\\pi^{*}(a|s)$ . A more detailed discussion is given in Appendix A. ", "page_idx": 2}, {"type": "text", "text": "To make diffusion models compatible with RL, there are generally two approaches: guide-based and select-based. Guide-based methods [32, 17] incorporate the behavior policy\u2019s score function with an additional guidance term. Specifically, they learn a time-dependent guidance term $\\mathcal{I}_{t}$ and use it to drift the generated actions towards high-value regions. The learning objective of ${\\mathcal{I}}_{t}$ can be generally formalized with $\\mathcal{L}(\\mathcal{J}_{t}(a_{t}),w(s,\\{a_{0}^{i}\\}_{i=1}^{K})))$ , where $w(s,\\{a_{0}^{i}\\}_{i=1}^{K})$ are critic-computed values on $K$ diffusion behavior samples. $\\mathcal{L}$ can be a contrastive objective [32] or mean-square-error objective [17]. ", "page_idx": 2}, {"type": "text", "text": "After training, the augmented score function $\\nabla_{a_{t}}\\log\\pi_{t}(a_{t}|s)=\\nabla_{a_{t}}\\log\\pi_{t}^{\\mathcal{D}}(a_{t}|s)+\\nabla_{a_{t}}\\mathcal{I}_{t}(a_{t})$ is used to characterize the learned policy. ", "page_idx": 3}, {"type": "text", "text": "Select-based methods [4, 14] utilize the observation that for some RL algorithms, the actor induced through critic learning manifests as a reweighted behavior policy. To sample from the optimized policy, these methods first sample multiple candidates $\\{a_{0}^{i}\\}_{i=1}^{N}$ from the diffusion behavior policy and then resample from them using critic-computed values $\\bar{w}(s,\\{a_{0}^{i}\\}_{i=1}^{N})$ . More precisely, the sampling procedure follows the categorical distribution Pr[a = aj0|s] = iNw=1( sw,a(0s,)ai0). ", "page_idx": 3}, {"type": "text", "text": "Error exploitation As we can see, both guide-based and select-based methods need the information of $w(s,\\{a_{0}^{i}\\}_{i=1}^{N})$ to get the optimal action. However, this term may bring two sources of errors. One is the diffusion model\u2019s approximation error in modeling complicated policy distribution, and the other is the critic\u2019s error in evaluating unseen actions. Although trained on offilne data, the diffusion model may still generate OOD actions (especially with frequent sampling) and the learned critic can make erroneous predictions on these OOD actions, causing the evaluated value on these actions to be over-estimated due to the learning nature of value functions [11, 25]. As a result, the generation of high-quality actions in existing methods is greatly affected due to this error exploitation, which we will also show empirically in the next section. ", "page_idx": 3}, {"type": "text", "text": "3 Diffusion-DICE ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we introduce our method, Diffusion-DICE. We start from an extension of DICE-based method that considers DICE as a transformation from the behavior distribution to the optimal policy distribution, which motivates us to use diffusion models to perform such transformation. We then propose a guide-then-select procedure to achieve the best action, i.e., we propose in-sample guidance learning for accurate policy transformation and use the critic to do optimal action selection to boost the performance. We also propose a piecewise $f$ -divergence to stabilize the gradient during learning. We give an illustration of the guide-then-select paradigm and use a toycase to showcase the error exploitation in previous methods and how Diffusion-DICE successfully alleviates that. ", "page_idx": 3}, {"type": "text", "text": "3.1 An Optimal Policy Transformation View of DICE ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "As mentioned before, DIstribution Correction Estimation (DICE) methods, an important line of work in offline RL and $\\mathrm{IL}$ provides us with an elegant way to estimate the optimal stationary distribution ratio between $d^{*}(s,a))$ and $d^{D}(s,a)$ [28, 41, 34]. We show that this ratio also directly indicates a proportional relationship between the optimal in-support policy $\\pi^{*}(a|s)$ and the behavior policy $\\pi^{\\hat{\\mathcal{D}}}(a|s)$ . This proportional relationship enables us to transform $\\pi^{\\mathcal{D}}$ into $\\pi^{*}$ . ", "page_idx": 3}, {"type": "text", "text": "Our key observation is that the definition of $d^{\\pi}(s,a)$ inherently reveals a bijection between $\\pi(a|s)$ and $d^{\\pi}\\bar{(}s,a)$ . Given a relationship between $d^{*}(s,a)$ and $d^{D}(s,\\bar{a})$ , we can use this bijection to derive a relationship between $\\pi^{*}(a|s)$ and $\\pi^{\\mathcal{D}}(a|s)$ . We formalize the bijection and derived relationship as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\pi(a|s)=\\frac{d^{\\pi}(s,a)}{\\int_{a\\in A}d^{\\pi}(s,a)d a},\\quad\\pi^{*}(a|s)=\\frac{\\frac{d^{*}(s,a)}{d^{p}(s,a)}}{\\int_{a\\in A}\\frac{d^{*}(s,a)}{d^{p}(s,a)}\\pi^{\\mathcal{D}}(a|s)d a}\\pi^{\\mathcal{D}}(a|s).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The proof is given in Appendix B. Since the denominator in the second equation involves an integral over $a\\in{\\mathcal{A}}$ and is unrelated to the chosen action, the relationship indicates $\\pi^{*}(a|s)\\;\\propto$ ${\\frac{d^{*}(s,a)}{d^{D}(s,a)}}\\pi^{D}(a|s)$ $d^{D}(s,a)$ $d^{*}(s,a)$ to the transformation between $\\pi^{\\mathcal{D}}(a|s)$ and $\\pi^{*}(a|s)$ . This transformation motivates the use of deep generative models, e.g. diffusion models, to first fit the behavior distribution using their strong expressiveness and then directly perform this transformation during sampling. More specifically, the score function of the optimal policy and the behavior policy satisfy the following relationship: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\nabla_{a_{t}}\\log\\pi_{t}^{*}(a_{t}|s)=\\nabla_{a_{t}}\\log\\pi_{t}^{\\mathcal{D}}(a_{t}|s)+\\nabla_{a_{t}}\\log\\mathbb{E}_{a_{0}\\sim\\pi^{\\mathcal{D}}(a_{0}|a_{t},s)}[\\frac{d^{*}\\bigl(s,a_{0}\\bigr)}{d^{\\mathcal{D}}(s,a_{0})}].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The proof is given in Appendix B. Intuitively, $\\nabla_{a_{t}}\\log\\pi_{t}^{\\mathcal{D}}(a_{t}|s)$ tells us how to generate actions from $\\pi^{\\mathcal{D}}$ and $\\nabla_{a_{t}}\\log\\mathbb{E}_{\\pi^{\\mathcal{D}}(a_{0}|a_{t},s)}[w^{*}(s,a_{0})]$ tells us how to transform from $\\pi^{\\mathcal{D}}$ to in-support $\\pi^{*}$ during the reverse diffusion process. As mentioned before, when representing $\\pi^{*}$ with a diffusion model, all we need is its score function $\\nabla_{a_{t}}\\log\\pi_{t}^{*}(a_{t}|s)$ . Equivalently, we focus on the right-hand side of Eq.(6). The first term is just the score function of $\\pi^{\\mathcal{D}}$ , which is fairly easy to obtain from the offline dataset [14, 4]. To perform the transformation, we still require the second term. Fortunately, DICE allows us to acquire the inner ratio term in an in-sample manner directly from the offilne dataset, and we will show in the next section how to exactly compute the whole second term using offline dataset. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "3.2 In-sample Guidance Learning for Accurate Policy Transformation ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Although DICE provides us with the optimal stationary distribution ratio, which is the cornerstone of the transformation, the second term on the right-hand side of Eq.(6) is still intractable due to the conditional expectation. Our key observation is that for arbitrary non-negative function $f(x)$ , the optimizer of the following convex problem is unique and takes the desired form of log-expectation. ", "page_idx": 4}, {"type": "text", "text": "Lemma 1. Given a random variable $X$ and its corresponding distribution $P(X)$ , for any nonnegative function $f(x)$ , the following problem is convex and its optimizer is given by $y^{\\ast}\\;=$ $\\log\\mathbb{E}_{x\\sim P(X)}[f(x)]$ , ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{y}\\mathbb{E}_{x\\sim P(X)}[f(x)\\cdot e^{-y}+y].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "It is evident that the optimizer can be used to derive the second term in Eq.(6). In fact, we show the second term can be obtained directly from the offilne dataset, if we optimize the following objective. Here $g_{\\theta}$ is a guidance network parameterized by $\\theta$ and we denote $\\frac{d^{*}(s,a)}{d^{D}(s,a)}$ as $w^{*}(s,a)$ for shorthand. ", "page_idx": 4}, {"type": "text", "text": "Theorem 1. $\\nabla_{a_{t}}\\log\\mathbb{E}_{\\pi^{\\mathcal{D}}(a_{0}|a_{t},s)}[w^{*}(s,a_{0})]$ can be obtained by solving the following optimization problem: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\theta}\\mathbb{E}_{t\\sim\\mathcal{U}(0,T)}\\mathbb{E}_{a\\sim\\pi^{\\mathcal{D}}(a|s)}\\mathbb{E}_{a_{t}\\sim p(a_{t}|a_{0})}\\left[w^{*}(s,a)e^{-g_{\\theta}(s,a_{t},t)}+g_{\\theta}(s,a_{t},t)\\right],\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "as the optimal solution $\\theta^{*}$ satisfies $\\nabla_{a_{t}}g_{\\theta^{*}}(s,a_{t},t)=\\nabla_{a_{t}}\\log\\mathbb{E}_{\\pi^{\\mathcal{D}}(a_{0}|a_{t},s)}[w(s,a_{0})].$ ", "page_idx": 4}, {"type": "text", "text": "This objective has two appealing properties. Firstly, compared to guide-based and select-based methods, we don\u2019t need to use multiple actions generated by the behavior diffusion model, we only need one sample from $\\pi^{\\mathcal{D}}(a|s)$ to estimate the second expectation, enabling an unbiased estimator of this objective using only offilne dataset possible. Secondly, because $w(s,a)$ must be non-negative to ensure a valid transformation, this objective is convex with respect to $g_{\\theta}(s,a_{t},t)$ . This indicates a guarantee of convergence to $g_{\\theta^{*}}\\left(s,a_{t},t\\right)$ under mild assumptions (see proof in Appendix B). ", "page_idx": 4}, {"type": "text", "text": "Directly inducing $\\nabla_{a_{t}}\\log\\mathbb{E}_{\\pi^{\\mathcal{D}}(a_{0}|a_{t},s)}[w^{*}(s,a_{0})]$ from the dataset is crucial in the offline setting. Firstly, it avoids the evaluation of OOD actions and solely depends on reliable value predictions of in-sample actions. Consequently, the gradient exploits minimal error in the critic, enabling it to more accurately transform $\\pi^{\\mathcal{D}}$ into in-support $\\pi^{*}$ . Moreover, because all values of $w^{*}(s,a)$ used in our method are based on in-sample actions, applying this gradient in the reverse process will guide the samples towards in-sample actions with high $w^{*}(s,a)$ value. ", "page_idx": 4}, {"type": "text", "text": "We refer to this method as In-sample Guidance Learning (IGL). Note that previous methods either have a biased estimate of the gradient [17, 6], or have to rely on value predictions for diffusion generated actions, which could be OOD [32]. We refer to Appendix C for an in-depth discussion of IGL against other guidance methods. ", "page_idx": 4}, {"type": "text", "text": "Stablizing gradient using piecewise $f$ -divergence. In practice, there exists one issue when computing the guidance term. Note that in Eq.(4), $w^{*}(s,a)$ could become zero, depends on the choice of $f$ . Given a specific $a_{t}$ , if $w^{*}(s,a_{0})=0$ for actions under $\\pi^{\\mathcal{D}}(a_{0}|a_{t},s)$ , the second gradient term will not be well-defined due to taking the logarithm of 0. In practice, this can result in an unstable gradient. To avoid such issue, $(\\bar{f}^{\\prime})^{-1}(x)^{\\prime}$ should be positive for any given $x$ . Also, to facilitate the optimization process, $f^{*}(x)$ should possess a closed-form solution and good numerical properties. Unfortunately, none of the commonly used $f$ -divergence can accommodate both. However, considering the following two $f$ -divergences: ", "page_idx": 4}, {"type": "table", "img_path": "EIl9qmMmvy/tmp/dae3f7d6f94bec1e230cd6c2663d191bf6173d6ed9b316bc8ca70b05ae109937.jpg", "table_caption": [], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "It is obvious that Reverse KL divergence possesses the positive property but exhibits numerical instability given a large $x$ due to the exponential function in $f^{*}(x)$ , while Pearson $\\chi^{2}$ divergence avoids the instability in the exponential function, its $(f^{\\prime})^{-1}(x)$ value is negative when $x<-2$ . ", "page_idx": 5}, {"type": "text", "text": "To take advantage of both divergences while avoiding their drawbacks, we propose the following piecewise $f$ -divergence that has properties similar to Pearson $\\chi^{2}$ when $x$ is large while has properties similar to Reverse KL when $x$ is small: ", "page_idx": 5}, {"type": "equation", "text": "$$\nf(x)={\\left\\{\\!\\!\\begin{array}{l l}{(x-1)^{2}}&{x\\geq1,}\\\\ {x\\log x-x+1}&{0\\leq x<1.}\\end{array}\\!\\!\\right.}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "In Appendix B, we prove that $f(x)$ is a well-defined $f$ -divergence. We also prove that $f(x)$ has the following proposition: ", "page_idx": 5}, {"type": "text", "text": "Proposition 1. Given the piecewise $f$ -divergence in Eq.(8), $(f^{\\prime})^{-1}(x)$ and $f^{*}(x)$ has the following formulation: ", "page_idx": 5}, {"type": "equation", "text": "$$\n(f^{\\prime})^{-1}(x)={\\left\\{\\frac{x}{2}+1\\quad x\\geq0}\\qquad f^{*}(x)={\\left\\{\\frac{x^{2}}{4}+x\\quad x\\geq0\\right.}\\qquad}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "We can see that given any $x$ , the exponential function in $(f^{\\prime})^{-1}(x)$ ensures a strictly positive value. Meanwhile, $f^{*}(x)$ is at most a quadratic polynomial, which ensures numerical stability in practice. Note that $f(x)$ enables a stable policy transformation and can also be applied to other DICE-based methods when the optimal stationary distribution ratio needs to be positive. ", "page_idx": 5}, {"type": "text", "text": "3.3 Boost Performance with Optimal Action Selection ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "After transforming $\\pi^{\\mathcal{D}}$ into in-support $\\pi^{*}$ , we now introduce the select-step to boost the performance during evaluation. One issue is the multi-modality in the optimal policy, this partially arises from the policy constraint used to regularize its output [9, 25, 55, 34]. Under policy constraint, the regularized optimal policy is unlikely to be deterministic and may be multi-modal. ", "page_idx": 5}, {"type": "text", "text": "More specifically, in our method, as we strictly follow the score function $\\nabla_{a_{t}}\\log\\pi_{t}^{*}(a_{t}|s)$ of insupport optimal policy $\\pi^{*}$ to generate high-value actions during the reverse diffusion process, any mode that has non-zero probability under $\\pi^{*}(a|s)$ could be generated, which lead to a sub-optimal choice. Similar issues have been discussed in previous works [14, 13]. In Diffusion-DICE, it\u2019s natural to leverage the optimal critic $Q^{*}$ derived from DICE in Eq.(3) to identify the best action. Given a specific state $s$ , we first follow $\\nabla_{a_{t}}\\log\\pi_{t}^{*}(a_{t}|s)$ in the reverse diffusion process to generate a few actions from $\\pi^{*}(a|s)$ . Then we evaluate these actions with $Q^{*}$ and select the action with the highest $Q^{*}(s,a)$ value as the policy\u2019s output as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\pi(s)=a^{*}\\triangleq\\underset{a\\in\\{a_{0}^{1},\\ldots,a_{0}^{K}\\sim\\pi^{*}(a|s)\\}}{\\arg\\operatorname*{max}}Q^{*}(s,a).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Different from previous guide-only or selectonly methods, we base the select stage on the guide stage. As mentioned before, using IGL accurately guides the generated candidate actions towards in-support actions with high value. With the assistance of the guide stage, we can sample candidates from $\\pi^{*}(a|s)$ , which has a high probability of being high-quality. This means only a small number of candidates are required to be sampled, which reduces the probability of sampling OOD actions. This leads to minimal error exploitation while still attaining high returns. We give an illustration of the guide-then-select paradigm in Figure 1. ", "page_idx": 5}, {"type": "text", "text": "Algorithm 1 Diffusion-DICE   \n1: Initialize value function $Q_{\\phi_{1}}$ , $V_{\\phi_{2}}$ , diffusion behavior model $\\epsilon_{\\psi}$ , guidance network $g_{\\theta}$   \n2: // Training   \n3: Pretrain the diffusion behavior model by minimizing $\\mathbb{E}_{\\mathcal{U}(t),(s,a)\\sim\\mathcal{D},\\epsilon\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{I})}\\big[\\|\\epsilon_{\\psi}\\big(\\alpha_{t}a+\\sigma_{t}\\epsilon,s,t\\big)-\\epsilon\\|^{2}\\big]$   \n4: for $t=1,2,\\cdots\\,,N$ do   \n5: Sample transitions $(s,a,r,s^{\\prime})\\sim\\mathcal{D}$   \n6: Update $Q_{\\phi_{1}}$ and $V_{\\phi_{2}}$ by minimizing Eq.(3)   \n7: Update $g_{\\theta}$ by minimizing Eq.(7)   \n8: end for   \n9: // Evaluation   \n10: Sample $\\{a_{0}^{i}\\}_{i=1}^{K}$ from $\\pi^{*}$ following Eq.(5), with $\\begin{array}{r}{\\nabla_{a_{t}}\\log\\pi_{t}^{*}(a_{t}|s)=\\frac{\\epsilon_{\\psi}(a_{t},s,t)}{-\\sigma_{t}}+\\nabla_{a_{t}}g_{\\theta}(s,a_{t},t)}\\end{array}$   \n11: Select action according to Eq.(10) ", "page_idx": 5}, {"type": "text", "text": "this guideDiffusion-DICE and present its pseudo-code in Algorithm 1. During the training stage, DiffusionDICE estimates the optimal stationary distribution ratio using DICE with our piecewise $f$ -divergence and calculates the guidance with IGL, both in an in-sample manner. During the testing stage, ", "page_idx": 5}, {"type": "image", "img_path": "EIl9qmMmvy/tmp/123568b9c51c4dc985027736f059ebed2dfab8952d8dba1fd4633ceae750937f.jpg", "img_caption": ["Figure 2: Toycase of a 2-D bandit problem. The action in the offilne dataset follows a bivariate standard normal distribution constrained within an annular region. The ground truth reward has two peaks extending from the center outward. We use a diffusion model $\\hat{\\pi}^{\\mathcal{D}}$ to fit the behavior policy and a reward model $\\hat{R}$ to fit the ground truth reward $R$ . Both $\\hat{\\pi}^{\\mathcal{D}}$ and $\\hat{R}$ fit in-distribution data well while making error in out-of-distribution regions. Diffusion-DICE could generate correct optimal actions in the outer circle while other methods tend to expolit error information from $\\hat{R}$ and only generate overestimated, sub-optimal actions. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Diffusion-DICE selects the action from $\\pi^{*}(a|s)$ with the highest value. By selecting from a small number of action candidates from $\\pi^{*}(a|s)$ , minimal error of the action evaluation model will be exploited. Besides serving as a diffusion-based offilne RL algorithm, Diffusion-DICE also introduces a novel policy extraction method from other DICE-based algorithms. By leveraging expressive generative models, Diffusion-DICE can capture the multi-modality in $\\pi^{*}$ and better draw upon the knowledge from ddD((ss,,aa)) and \u03c0D. ", "page_idx": 6}, {"type": "text", "text": "Toycase validation. We use a toycase to validate that the guide-then-select paradigm used in Diffusion-DICE indeed brings minimal error exploitation that other diffusion-based methods suffer from. Here we aim to solve a 2-D bandit problem given a fixed action dataset. The action space is continuous and actions in the offilne dataset follow a bivariate standard normal distribution constrained within an annular region. We show the dataset distribution $\\pi^{\\mathcal{D}}$ , the learned diffusion behavior policy $\\hat{\\pi}^{\\mathcal{D}}$ , the ground truth reward $R$ and the predicted reward $\\hat{R}$ in Figure 2. Note that the true optimal reward occurs on the outer circle. However, due to limited data coverage, the learned reward function exploits overestimation error on unseen regions, e.g., actions inside the inner circle have erroneous high values. What\u2019s worse, due to fitting errors, the diffusion behavior policy may generate such overestimated actions. We take Diffusion-DICE with a guide-based method, QGPO [32], and a select-based method, IDQL [14] for comparison. ", "page_idx": 6}, {"type": "text", "text": "More specifically, we visualize the sample generation process in different methods. For guide-based method QGPO, as it utilizes actions generated by $\\hat{\\pi}^{\\mathcal{D}}$ and the value of $\\hat{R}$ on them to obtain guidance, the guidance exploits overestimation errors of OOD actions in the center and drifts the generated actions towards them. Note that adding a select step in QGPO will not help as the guide-step is already incorrect. IDQL, because it is free of guidance, requires a large number of generated candidate actions to ensure the coverage of optimal action. IDQL utilizes $\\hat{R}$ to select from those suboptimal or out-of-distribution actions, which however, are the sources of error exploitation in $\\hat{R}$ . For Diffusion-DICE, because the first guide-step accurately guides the generated actions towards in-support while high-value regions (i.e., actions around the outer circle), in the select step, the learned value function $\\hat{R}$ could make a correct evaluation and successfully select the true best actions in the outer circle. ", "page_idx": 6}, {"type": "table", "img_path": "EIl9qmMmvy/tmp/2f03b52a39ca349535182209feff5e3691a604d268b244a71385027ae185bd2d.jpg", "table_caption": ["Table 1: Evaluation results on D4RL benchmark. We report the average normalized scores at the end of training with standard deviation across 5 random seeds. Diffusion-DICE (D-DICE) demonstrates superior performance compared to all baseline algorithms in 13 out of 15 tasks, especially on more challenging tasks. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we present empirical evaluations of Diffusion-DICE. To validate Diffusion-DICE\u2019s ability to transform the behavior policy into the optimal policy while maintaining minimal error exploitation, we conduct experiments on two fronts. On one hand, we evaluate Diffusion-DICE on the D4RL offline RL benchmark and compare it with other strong diffusion-based and DICE-based methods. On the other hand, we use alternative criteria to demonstrate that the guide-then-select paradigm results in minimal error exploitation. Experimental details are shown in Appendix D. ", "page_idx": 7}, {"type": "text", "text": "4.1 D4RL Benchmark Datasets: ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We first evaluate Diffusion-DICE on the D4RL benchmark [8] and compare it with several related algorithms. For the evaluation tasks, we select MuJoCo locomotion tasks and AntMaze navigation tasks. While MuJoCo locomotion tasks are popular in offilne RL, AntMaze navigation tasks are more challenging due to their stronger need for trajectory stitching. For baseline algorithms, we selected state-of-the-art methods not only from traditional methods that use Gaussian-policy (including DICEbased methods) but also from diffusion-based methods. Gaussian-policy-based baseline includes CQL [26], in-sample based methods IQL [24], SQL [55] and DICE-based method O-DICE [34]. Notably, O-DICE is a recently proposed DICE-based algorithm that stands out among various DICE-based offline RL methods. Diffusion-policy-based baseline includes Diffusion-QL [49] SfBC [4], QGPO [32] and IDQL [14]. We also compare Diffusion-DICE with its Gaussian-policy counterpart to show the benefit of using Diffusion-policy in Appendix D. While SfBC and IDQL simply sample from behavior policy candidates and select according to the action evaluation model, Diffusion-QL and QGPO will guide generated actions towards high-value ones. It is worth noting that Diffusion-QL will also resample from generated actions after the guidance. ", "page_idx": 7}, {"type": "text", "text": "The results show that Diffusion-DICE outperforms all other baseline algorithms, including previous SOTA diffusion-based methods, especially on MuJoCo medium, medium-replay datasets, and AntMaze datasets. The consistently better performance compared with Diffusion-QL and QGPO demonstrates the essentiality of in-sample guidance learning. Compared with SfBC and IDQL, Diffusion-DICE also shows superior performance, even with fewer action candidates. This is because the guide stage provides the in-support optimal policy for the select stage to sample from, which underscores the necessity of sampling carefully from an in-support optimal action distribution, rather than from the behavior distribution. We refer to the comparison of candidate numbers under different environments in Appendix D. Furthermore, the substantial performance gap between Diffusion-DICE and ", "page_idx": 7}, {"type": "text", "text": "O-DICE reflects the multi-modality of DICE\u2019s optimal policy and firmly positions Diffusion-DICE as a superior policy extraction method for other DICE-based algorithms. ", "page_idx": 8}, {"type": "text", "text": "4.2 Further Experiments on Error Exploitation ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We then continue to verify that the guide-then-select paradigm used in Diffusion-DICE indeed exploits minimal error. This is obvious for the guide stage because we only leverage in-sample actions for both critic training and guidance learning. For the select stage, additional evidence is needed, as Diffusion-DICE also selects actions from generated candidates. Our key observation is that if the action value remains high during evaluation but results in a low return trajectory, these action values must suffer from overestimation error. ", "page_idx": 8}, {"type": "text", "text": "Based on this, we choose to compare the average state-action value and the average return between actions selected by our guide-then-select paradigm and simply select from the behavior policy. Given identical critic and behavior policy, we run Diffusion-DICE and its guidancefree variant for 10 episodes, comparing their average $Q(s,a)$ values and their normalized scores. The results are shown in Figure 3. We also compare the learning curves of their average $Q(s,\\bar{a})$ values over time in Appendix D. ", "page_idx": 8}, {"type": "image", "img_path": "EIl9qmMmvy/tmp/99f3da9b42498d6dee94646e3c0fdd16f34b3b8b4d66fe81d4a588f494a09556.jpg", "img_caption": ["Figure 3: Actions generated by the guide-then-select paradigm result in better performance while have less overestimation error. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "It\u2019s clear from the results that our guide-then-select paradigm achieves better performance while having less overestimated $Q(s,a)$ values. This result validates the minimal error exploitation in Diffusion-DICE. ", "page_idx": 8}, {"type": "text", "text": "5 Related Work ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Offilne RL To tackle the distributional shift problem, most model-free offilne RL methods augment existing off-policy RL methods with a behavior regularization term. Behavior regularization can appear explicitly as divergence penalties [50, 25, 51, 9], implicitly through weighted behavior cloning [48, 38, 52], or more directly through careful parameterization of the policy [11, 58]. Another way to apply action-level regularization is via modification of value learning objective to incorporate some form of regularization, to encourage staying near the behavioral distribution and being pessimistic about OOD state-action pairs [26, 24, 55, 47]. There are also several works incorporating action-level regularization through the use of uncertainty [2] or distance function [29]. ", "page_idx": 8}, {"type": "text", "text": "All these methods are based on the actor-critic framework and use unimodal Gaussian policy. However, several works indicate their limited ability to model multi-modal policy distribution [49, 14, 4], thereby leading to suboptimal performance. To remedy this, it\u2019s natural to employ powerful generative models to represent the policy. DT [5] uses transformer as the policy, Diffusion-QL [49], SfBC [4], QGPO [32] and IDQL [14] leverage diffusion models to represent policy. Other generative models like CVAE and consistency model have also been used as policy in offilne RL [58, 54, 7]. Another line of methods utilizes diffusion models for trajectory-level planning by generating high-return trajectories and taking the corresponding action of the current state [17, 1]. While generative models are widely used in offline RL, few methods consider the existing errors in these models and whether the generation process exploits these errors. ", "page_idx": 8}, {"type": "text", "text": "DICE-based methods The core of DICE-based methods revolves around the ratio of the stationary distribution between two policies. This ratio can serve as the estimation target in off-policy evaluation [36, 57] or as part of a state-action level constraint term in RL [37, 41, 34]. In offline IL [53], it can connect the optimal stationary distribution of a regularized MDP with the dataset distribution [22, 12, 19]. In constrained RL, this ratio can induce the discounted sum of cost without an additional function approximator [28]. Under the imperfect rewards setting, this ratio can reflect the gap between the given rewards and the underlying perfect rewards [30]. All of these DICE methods consider this ratio as a transformation from one stationary distribution to another. In this paper, however, we extend this ratio as a transformation from the behavior policy to the optimal policy and propose a novel way of doing so by using diffusion models, which also serve as a replacement for the Gaussian-based policy extraction in DICE. ", "page_idx": 8}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we propose a new diffusion-based offline RL algorithm, Diffusion-DICE. DiffusionDICE uses a guide-then-select paradigm to select the best in-support actions while achieving minimal error exploitation in the value function. Diffusion-DICE also serves as a replacement for the Gaussian-based policy extraction part in current DICE methods, successfully unleashing the power of DICE-based methods. Through toycase illustration and extensive experiments, we show that Diffusion-DICE outperforms prior SOTA methods on a variety of datasets, especially those with multi-modal complex behavior distribution. One limitation of Diffusion-DICE is the sampling process of diffusion models is costly and slow. Another limitation is the training of diffusion models may suffer in low-data regimes. One future work is using more advanced generative models [44, 40] as a better choice. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgement ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "HX thanks Harshit Sikchi for insightful discussions. The SJTU team is partially supported by Shanghai Municipal Science and Technology Major Project (2021SHZDZX0102) and National Natural Science Foundation of China (62322603, 62076161). HX and AZ are supported by NSF 2340651. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] A. Ajay, Y. Du, A. Gupta, J. Tenenbaum, T. Jaakkola, and P. Agrawal. Is conditional generative modeling all you need for decision-making? arXiv preprint arXiv:2211.15657, 2022. [2] G. An, S. Moon, J.-H. Kim, and H. O. Song. Uncertainty-based offilne reinforcement learning with diversified q-ensemble. Proc. of NeurIPS, 2021.   \n[3] S. Boyd, S. P. Boyd, and L. Vandenberghe. Convex optimization. Cambridge university press, 2004. [4] H. Chen, C. Lu, C. Ying, H. Su, and J. Zhu. Offline reinforcement learning via high-fidelity generative behavior modeling. arXiv preprint arXiv:2209.14548, 2022. [5] L. Chen, K. Lu, A. Rajeswaran, K. Lee, A. Grover, M. Laskin, P. Abbeel, A. Srinivas, and I. Mordatch. Decision transformer: Reinforcement learning via sequence modeling. Proc. of NeurIPS, 2021. [6] H. Chung, J. Kim, M. T. Mccann, M. L. Klasky, and J. C. Ye. Diffusion posterior sampling for general noisy inverse problems. arXiv preprint arXiv:2209.14687, 2022.   \n[7] Z. Ding and C. Jin. Consistency models as a rich and efficient policy class for reinforcement learning. arXiv preprint arXiv:2309.16984, 2023. [8] J. Fu, A. Kumar, O. Nachum, G. Tucker, and S. Levine. D4rl: Datasets for deep data-driven reinforcement learning. ArXiv preprint, 2020.   \n[9] S. Fujimoto and S. S. Gu. A minimalist approach to offline reinforcement learning. Advances in neural information processing systems, 34:20132\u201320145, 2021.   \n[10] S. Fujimoto, H. van Hoof, and D. Meger. Addressing function approximation error in actor-critic methods. In Proc. of ICML, pages 1582\u20131591, 2018.   \n[11] S. Fujimoto, D. Meger, and D. Precup. Off-policy deep reinforcement learning without exploration. In Proc. of ICML, pages 2052\u20132062, 2019.   \n[12] D. Garg, S. Chakraborty, C. Cundy, J. Song, and S. Ermon. Iq-learn: Inverse soft-q learning for imitation. Advances in Neural Information Processing Systems, 34:4028\u20134039, 2021.   \n[13] T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In International conference on machine learning, pages 1861\u20131870. PMLR, 2018.   \n[14] P. Hansen-Estruch, I. Kostrikov, M. Janner, J. G. Kuba, and S. Levine. Idql: Implicit q-learning as an actor-critic method with diffusion policies. arXiv preprint arXiv:2304.10573, 2023.   \n[15] J. Ho, A. Jain, and P. Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:6840\u20136851, 2020.   \n[16] J. Ho, T. Salimans, A. Gritsenko, W. Chan, M. Norouzi, and D. J. Fleet. Video diffusion models. Advances in Neural Information Processing Systems, 35:8633\u20138646, 2022.   \n[17] M. Janner, Y. Du, J. B. Tenenbaum, and S. Levine. Planning with diffusion for flexible behavior synthesis. arXiv preprint arXiv:2205.09991, 2022.   \n[18] D. Kalashnikov, J. Varley, Y. Chebotar, B. Swanson, R. Jonschkowski, C. Finn, S. Levine, and K. Hausman. Mt-opt: Continuous multi-task robotic reinforcement learning at scale. ArXiv preprint, 2021.   \n[19] G.-H. Kim, S. Seo, J. Lee, W. Jeon, H. Hwang, H. Yang, and K.-E. Kim. Demodice: Offline imitation learning with supplementary imperfect demonstrations. In Proc. of ICLR, 2021.   \n[20] D. Kingma, T. Salimans, B. Poole, and J. Ho. Variational diffusion models. Advances in neural information processing systems, 34:21696\u201321707, 2021.   \n[21] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. In Proc. of ICLR, 2015.   \n[22] I. Kostrikov, O. Nachum, and J. Tompson. Imitation learning via off-policy distribution matching. In Proc. of ICLR, 2020.   \n[23] I. Kostrikov, A. Nair, and S. Levine. Offline reinforcement learning with implicit q-learning. ArXiv preprint, 2021.   \n[24] I. Kostrikov, A. Nair, and S. Levine. Offline reinforcement learning with implicit q-learning. arXiv preprint arXiv:2110.06169, 2021.   \n[25] A. Kumar, J. Fu, M. Soh, G. Tucker, and S. Levine. Stabilizing off-policy q-learning via bootstrapping error reduction. Advances in neural information processing systems, 32, 2019.   \n[26] A. Kumar, A. Zhou, G. Tucker, and S. Levine. Conservative q-learning for offilne reinforcement learning. Advances in Neural Information Processing Systems, 33:1179\u20131191, 2020.   \n[27] J. Lee, W. Jeon, B. Lee, J. Pineau, and K.-E. Kim. Optidice: Offline policy optimization via stationary distribution correction estimation. In International Conference on Machine Learning, pages 6120\u20136130. PMLR, 2021.   \n[28] J. Lee, C. Paduraru, D. J. Mankowitz, N. Heess, D. Precup, K.-E. Kim, and A. Guez. Coptidice: Offline constrained reinforcement learning via stationary distribution correction estimation. arXiv preprint arXiv:2204.08957, 2022.   \n[29] J. Li, X. Zhan, H. Xu, X. Zhu, J. Liu, and Y.-Q. Zhang. When data geometry meets deep function: Generalizing offilne reinforcement learning. In The Eleventh International Conference on Learning Representations, 2022.   \n[30] J. Li, X. Hu, H. Xu, J. Liu, X. Zhan, Q.-S. Jia, and Y.-Q. Zhang. Mind the gap: Offline policy optimization for imperfect rewards. arXiv preprint arXiv:2302.01667, 2023.   \n[31] C. Lu, Y. Zhou, F. Bao, J. Chen, C. Li, and J. Zhu. Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps. Advances in Neural Information Processing Systems, 35:5775\u20135787, 2022.   \n[32] C. Lu, H. Chen, J. Chen, H. Su, C. Li, and J. Zhu. Contrastive energy prediction for exact energy-guided diffusion sampling in offilne reinforcement learning. In International Conference on Machine Learning, pages 22825\u201322855. PMLR, 2023.   \n[33] A. S. Manne. Linear programming and sequential decisions. Management Science, 6(3): 259\u2013267, 1960.   \n[34] L. Mao, H. Xu, W. Zhang, and X. Zhan. Odice: Revealing the mystery of distribution correction estimation via orthogonal-gradient update. arXiv preprint arXiv:2402.00348, 2024.   \n[35] O. Nachum and B. Dai. Reinforcement learning via fenchel-rockafellar duality. arXiv preprint arXiv:2001.01866, 2020.   \n[36] O. Nachum, Y. Chow, B. Dai, and L. Li. Dualdice: Behavior-agnostic estimation of discounted stationary distribution corrections. Advances in neural information processing systems, 32, 2019.   \n[37] O. Nachum, B. Dai, I. Kostrikov, Y. Chow, L. Li, and D. Schuurmans. Algaedice: Policy gradient from arbitrary experience. arXiv preprint arXiv:1912.02074, 2019.   \n[38] A. Nair, M. Dalal, A. Gupta, and S. Levine. Accelerating online reinforcement learning with offline datasets. ArXiv preprint, 2020.   \n[39] A. R\u00e9nyi. On measures of entropy and information. In Proceedings of the fourth Berkeley symposium on mathematical statistics and probability, volume 1: contributions to the theory of statistics, volume 4, pages 547\u2013562. University of California Press, 1961.   \n[40] A. Shocher, A. Dravid, Y. Gandelsman, I. Mosseri, M. Rubinstein, and A. A. Efros. Idempotent generative network. arXiv preprint arXiv:2311.01462, 2023.   \n[41] H. Sikchi, Q. Zheng, A. Zhang, and S. Niekum. Dual rl: Unification and new methods for reinforcement and imitation learning. arXiv preprint arXiv:2302.08560, 2023.   \n[42] J. Sohl-Dickstein, E. Weiss, N. Maheswaranathan, and S. Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International conference on machine learning, pages 2256\u20132265. PMLR, 2015.   \n[43] Y. Song, J. Sohl-Dickstein, D. P. Kingma, A. Kumar, S. Ermon, and B. Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020.   \n[44] Y. Song, P. Dhariwal, M. Chen, and I. Sutskever. Consistency models. arXiv preprint arXiv:2303.01469, 2023.   \n[45] R. S. Sutton, A. G. Barto, et al. Introduction to reinforcement learning. MIT press Cambridge, 1998.   \n[46] P. Vincent. A connection between score matching and denoising autoencoders. Neural Comput., 23(7):1661\u20131674, 2011. doi: 10.1162/NECO\\_A\\_00142. URL https://doi.org/10.1162/ NECO_a_00142.   \n[47] X. Wang, H. Xu, Y. Zheng, and X. Zhan. Offilne multi-agent reinforcement learning with implicit global-to-local value regularization. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \n[48] Z. Wang, A. Novikov, K. Zolna, J. Merel, J. T. Springenberg, S. E. Reed, B. Shahriari, N. Y. Siegel, \u00c7. G\u00fcl\u00e7ehre, N. Heess, and N. de Freitas. Critic regularized regression. In Proc. of NeurIPS, 2020.   \n[49] Z. Wang, J. J. Hunt, and M. Zhou. Diffusion policies as an expressive policy class for offline reinforcement learning. arXiv preprint arXiv:2208.06193, 2022.   \n[50] Y. Wu, G. Tucker, and O. Nachum. Behavior regularized offilne reinforcement learning. ArXiv preprint, 2019.   \n[51] H. Xu, X. Zhan, J. Li, and H. Yin. Offilne reinforcement learning with soft behavior regularization. ArXiv preprint, 2021.   \n[52] H. Xu, L. Jiang, L. Jianxiong, and X. Zhan. A policy-guided imitation approach for offline reinforcement learning. In Advances in Neural Information Processing Systems, volume 35, pages 4085\u20134098, 2022.   \n[53] H. Xu, X. Zhan, H. Yin, and H. Qin. Discriminator-weighted offline imitation learning from suboptimal demonstrations. In International Conference on Machine Learning, pages 24725\u2013 24742. PMLR, 2022.   \n[54] H. Xu, X. Zhan, and X. Zhu. Constraints penalized q-learning for safe offline reinforcement learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 8753\u20138760, 2022.   \n[55] H. Xu, L. Jiang, J. Li, Z. Yang, Z. Wang, V. W. K. Chan, and X. Zhan. Offline rl with no ood actions: In-sample learning via implicit value regularization. arXiv preprint arXiv:2303.15810, 2023.   \n[56] X. Zhan, H. Xu, Y. Zhang, X. Zhu, H. Yin, and Y. Zheng. Deepthermal: Combustion optimization for thermal power generating units using offline reinforcement learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 4680\u20134688, 2022.   \n[57] R. Zhang, B. Dai, L. Li, and D. Schuurmans. Gendice: Generalized offline estimation of stationary values. In International Conference on Learning Representations, 2019.   \n[58] W. Zhou, S. Bajracharya, and D. Held. Latent action space for offline reinforcement learning. In Conference on Robot Learning, 2020.   \n[59] Z. Zhu, M. Liu, L. Mao, B. Kang, M. Xu, Y. Yu, S. Ermon, and W. Zhang. Madiff: Offline multi-agent learning with diffusion models. arXiv preprint arXiv:2305.17330, 2023. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 13}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 13}, {"type": "text", "text": "Justification: Our main claims can be reflected by Section 3 and Section 4. ", "page_idx": 13}, {"type": "text", "text": "Guidelines: ", "page_idx": 13}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 13}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 13}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Justification: Limitations are discussed in Section 6 ", "page_idx": 13}, {"type": "text", "text": "Guidelines: ", "page_idx": 13}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 13}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 13}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 13}, {"type": "text", "text": "Justification: Seed Appendix B ", "page_idx": 14}, {"type": "text", "text": "Guidelines: ", "page_idx": 14}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 14}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 14}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 14}, {"type": "text", "text": "Justification: Experimental details are given in Appendix D. ", "page_idx": 14}, {"type": "text", "text": "Guidelines: ", "page_idx": 14}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 14}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 14}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 15}, {"type": "text", "text": "Justification: Code provided in the project link. Guidelines: ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 15}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 15}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 15}, {"type": "text", "text": "Justification: Experimental settings are given in Section 4 and Appendix D. Details are given in Appendix D. ", "page_idx": 15}, {"type": "text", "text": "Guidelines: ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 15}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 15}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Justification: We use sufficient random seeds and provide standard deviation of the normalized scores in Section 4. ", "page_idx": 15}, {"type": "text", "text": "Guidelines: ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: Details of computation resources are given in Appendix E. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 16}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: This paper doesn\u2019t involve human subjects or participants and the datasets in Section 4 are common open-sourced datasets. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 16}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Justification: Potential positive impacts and negative impacts are discussed in Appendix F. Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 17}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 17}, {"type": "text", "text": "Justification: ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 17}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: All assets we use are properly credited, and the corresponding license are given in Appendix D. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 17}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 18}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 18}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 18}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 18}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 18}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 18}, {"type": "text", "text": "Answer: [NA] Justification: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 18}, {"type": "text", "text": "A A More Detailed Discussion of DICE and Diffusion Model in RL ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Derivation of learning objectives in DICE DICE algorithms consider the following regularized RL problem as a convex programming problems with Bellman-flow constraints and apply FenchelRockfeller duality or Lagrangian duality to solve it. The regularization term aims at imposing state-action level constraints.[35, 27, 34]. ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\pi}\\mathbb{E}_{(s,a)\\sim d^{\\pi}}[r(s,a)]-\\alpha D_{f}(d^{\\pi}(s,a)\\|d^{2}(s,a))\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Here $D_{f}(d^{\\pi}(s,a)\\vert\\vert d^{\\mathcal{D}}(s,a))$ is the $f$ -divergence which is defined with $\\begin{array}{r l r}{D_{f}(P\\|Q)}&{{}=}&{}\\end{array}$ $\\mathbb{E}_{\\omega\\in Q}\\left[f\\left(\\frac{P(\\omega)}{Q(\\omega)}\\right)\\right].$ . Directly solving $\\pi^{*}$ is impossible because it\u2019s intractable to calculate $d^{\\pi}(s,a)$ However, one can change the optimization variable from $\\pi$ to because of the bijection existing between them. Then with the assistance of Bellman-flow constraints, we can obtain an optimization problem with respect to $d$ : ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\underset{d\\geq0}{\\operatorname*{max}}\\mathbb{E}_{(s,a)\\sim d}[r(s,a)]-\\alpha D_{f}(d(s,a)\\|d^{D}(s,a))}\\\\ &{\\mathrm{~s.t.~}\\displaystyle\\sum_{a\\in A}d(s,a)=(1-\\gamma)d_{0}(s)+\\gamma\\displaystyle\\sum_{(s^{\\prime},a^{\\prime})}d(s^{\\prime},a^{\\prime})p(s|s^{\\prime},a^{\\prime}),\\forall s\\in\\mathcal{S}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Note that the feasible region has to be $\\{d:\\forall s\\,\\in\\,{\\mathcal{S}},a\\,\\in\\,{\\mathcal{A}},d(s,a)\\,\\geq\\,0\\}$ because $d$ should be non-negative to ensure a valid corresponding policy. After applying Lagrangian duality, we can get the following optimization target following [27]: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\operatorname*{min}_{V(s)}\\operatorname*{max}_{\\alpha\\geq0}\\mathbb{E}_{(s,a)\\sim d}[r(s,a)]-\\alpha D_{f}(d(s,a)\\|d^{D}(s,a))}\\\\ &{\\qquad+\\displaystyle\\sum_{s}V(s)\\Big((1-\\gamma)d_{0}(s)+\\gamma\\displaystyle\\sum_{(s^{\\prime},a^{\\prime})}d(s^{\\prime},a^{\\prime})p(s|s^{\\prime},a^{\\prime})-\\displaystyle\\sum_{a}d(s,a)\\Big)}\\\\ &{=\\displaystyle\\operatorname*{min}_{V(s)}\\operatorname*{max}_{\\alpha\\geq0}(1-\\gamma)\\mathbb{E}_{d_{0}(s)}[V(s)]}\\\\ &{\\qquad\\quad+\\mathbb{E}_{s,a\\sim d^{D}}\\bigg[\\omega(s,a)\\Big(r(s,a)+\\gamma\\displaystyle\\sum_{s^{\\prime}}p(s^{\\prime}|s,a)V(s^{\\prime})-V(s)\\Big)\\bigg]-\\alpha\\mathbb{E}_{s,a\\sim d^{D}}\\bigg[f(\\omega(s,a))\\bigg].}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Here we denote $\\omega(s,a)$ as $\\frac{d(s,a)}{d^{D}(s,a)}$ for simplicity. By incorporating the non-negative constraint of $d$ and again solving the constraint problem with Lagrangian duality, we can derive the optimal solution $w^{*}(s,a)$ for the inner problem and thus reduce the bilevel optimization problem to the following optimization problem: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{V(s)}(1-\\gamma)\\mathbb{E}_{d_{0}(s)}[V(s)]+\\alpha\\mathbb{E}_{s,a\\sim d^{\\mathcal{D}}}\\left[f^{*}\\big(\\frac{r(s,a)+\\gamma\\sum_{s^{\\prime}}p(s^{\\prime}|s,a)V(s^{\\prime})-V(s)}{\\alpha}\\big)\\right]\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Here $f^{*}$ is a variant of $f$ \u2019s convex conjugate. Note that in the offilne RL setting, the inner summation $\\begin{array}{r}{\\sum_{s^{\\prime}}p(s^{\\prime}|s,a)V(s^{\\prime})}\\end{array}$ is usually intractable because of limited data samples. To handle this issue and increase training stability, DICE methods usually use additional network $Q(s,a)$ to fit $r(s,a)+$ $\\begin{array}{r}{\\gamma\\sum_{s^{\\prime}}p(s^{\\prime}|s,a)\\bar{V}(s^{\\prime})}\\end{array}$ , by optimizing the following MSE objective: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{Q}\\mathbb{E}_{(s,a,s^{\\prime})\\sim d^{\\mathcal{D}}}\\left[\\left(r(s,a)+\\gamma V(s^{\\prime})-Q(s,a)\\right)^{2}\\right]\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "And because of doing so, the optimization objective in 14 can be replaced with: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{V}\\mathbb{E}_{s\\sim d^{0}}[(1-\\gamma)V(s)]+\\mathbb{E}_{(s,a)\\sim d^{\\mathcal{D}}}\\big[\\alpha f^{*}\\big(\\left[Q(s,a)-V(s)\\right]/\\alpha\\big)\\big]\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Also note that to increase the diversity of samples, one often extends the distribution of initial state $d_{0}$ to $d^{\\mathcal{D}}$ by treating every state in a trajectory as initial state [22]. ", "page_idx": 19}, {"type": "text", "text": "Discussion of diffusion policy and other methods in offline RL In offline RL, as discussed before, most diffusion-based algorithms utilize diffusion models to represent policy $\\pi$ . This policy can either be the behavior policy [14, 4], the optimal policy [49, 32]. When representing policy with diffusion model, the ultimate goal is to model the optimal policy $\\pi^{*}$ with diffusion model. Different from other tasks in image generation or video generation [43, 16], RL tasks will not provide data samples under $\\pi^{*}$ , while the agent has to learn such optimal policy. This makes directly leveraging the diffusion model to fti the optimal policy impossible. However, besides these methods that are based on diffusion policy, some methods utilize a diffusion model to fit the entire trajectory\u2019s distribution [17, 1]. These methods try to generate the optimal trajectory based on the current state. After generating the entire trajectory, these methods simply take the action corresponding to the current state. It\u2019s worth noting that although these methods avoid modeling $\\pi^{*}$ , extra effort is required to generate high-quality trajectories. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "B Additional Proofs ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Optimal policy transformation with stationary distribution ratio: Given that $\\pi^{\\mathcal{D}}$ is the behavior policy, dD(s,a) and $\\pi^{*}$ are the optimal stationary distribution correction and its corresponding optimal policy. $\\pi^{*}$ and $\\pi^{\\mathcal{D}}$ satisfy the following proposition: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\pi^{*}(a|s)=\\frac{\\frac{d^{*}(s,a)}{d^{D}(s,a)}}{\\int_{a\\in A}\\frac{d^{*}(s,a)}{d^{D}(s,a)}\\pi^{D}(a|s)d a}\\pi^{D}(a|s)\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. Using the definition between $d^{\\pi}(s)$ and $d^{\\pi}(s,a)$ , we have for any $\\pi$ $,d^{\\pi}(s,a)=d^{\\pi}(s)\\!\\cdot\\!\\pi(a|s)$ . Then we have: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\frac{d^{*}(s,a)}{d^{D}(s,a)}}{\\int_{a\\in A}\\frac{d^{*}(s,a)}{d^{D}(s,a)}\\pi^{D}(a|s)d a}\\pi^{D}(a|s)=\\!\\frac{\\frac{d^{*}(s,a)}{d^{D}(s,a)}}{\\frac{\\int_{a\\in A}d^{*}(s,a)d a}{d^{D}(s)}}\\pi^{D}(a|s)}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad=\\!\\frac{d^{*}(s,a)}{d^{D}(s,a)}\\cdot\\frac{d^{D}(s)}{d^{*}(s)}\\pi^{D}(a|s)}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad=\\!\\pi^{*}(a|s)}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "pVoelriicfyi $\\pi^{\\mathcal{D}}(a|s)$ sEaqti.s(f6y):: $\\begin{array}{r}{\\nabla_{a_{t}}\\log\\pi_{t}^{*}(a_{t}|s)=\\nabla_{a_{t}}\\log\\pi_{t}^{\\mathcal{D}}(a_{t}|s)+\\nabla_{a_{t}}\\log\\mathbb{E}_{a_{0}\\sim\\pi^{\\mathcal{D}}(a_{0}|a_{t},s)}[\\frac{d^{*}(s,a_{0})}{d^{\\mathcal{D}}(s,a_{0})}]}\\end{array}$ $\\pi^{*}(a|s)$ r We mainly follow the proof in Lu et al. [32] to verify this equation. Given the relationship between $\\pi^{*}(a|s)$ and $\\pi^{*}(a|s)$ in Eq.(18) we have: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\pi_{t}^{*}}(a_{\\alpha}|s)=\\int p(a_{\\alpha}|a_{\\alpha})\\pi_{0}^{*}(a_{\\alpha}|s)d a_{\\alpha}}\\\\ &{\\phantom{=}\\int p(a_{\\alpha}|a_{\\alpha},s)^{\\alpha}\\pi^{*}(a_{\\alpha}|s)d a_{\\alpha}}\\\\ &{\\phantom{=}\\int p(a_{\\alpha}|a_{\\alpha},s)\\frac{\\frac{d^{2}}{d}(s)\\,}{\\int_{a_{\\alpha}\\in A}d\\pi^{*}(s)\\,\\sin^{2}(\\pi(a_{\\alpha}|s))d a}=\\pi^{\\gamma}(a_{\\alpha}|s)d a_{\\alpha}}\\\\ &{\\phantom{=\\int p(a_{\\alpha}|a_{\\alpha},s)\\pi^{*}(a_{\\alpha}|s)\\pi^{*}(a_{\\alpha}|s)\\pi^{*}(a_{\\alpha}|s)d a}}\\\\ &{\\phantom{=\\int p(a_{\\alpha}|a_{\\alpha},s)\\pi^{*}(a_{\\alpha}|s)\\frac{\\frac{d^{2}}{d}(s)\\,\\sin^{2}(\\pi(a_{\\alpha}|s))d a}{\\int_{a_{\\alpha}\\in A}\\frac{d^{2}}{d\\pi(a_{\\alpha}|s)}\\pi^{*}(a_{\\alpha}|s)d a}d a_{\\alpha}}\\\\ &{\\phantom{=\\int p(a_{\\alpha}|a_{\\alpha},s)\\pi^{*}(a_{\\alpha}|s)\\frac{\\frac{d^{2}}{d}(s)\\,}{\\int_{a_{\\alpha}\\in A}\\frac{d^{2}}{d\\pi(a_{\\alpha}|s)}\\pi^{*}(a_{\\alpha}|s)d a}d a_{\\alpha}}\\\\ &{\\phantom{=\\int p(a_{\\alpha}|a_{\\alpha},s)\\pi^{*}(a_{\\alpha}|s)\\frac{\\frac{d^{2}}{d}(s,a_{\\alpha}|s)}{\\int_{a_{\\alpha}\\in A}\\frac{d^{2}}{d\\pi(a_{\\alpha}|s)}\\pi^{*}(a_{\\alpha}|s)d a}d a_{\\alpha}}\\\\ &{\\phantom{=\\int\\pi^{\\gamma}(a_{\\alpha}|a_{\\alpha},s)\\pi^{*}(a_{\\alpha}|s)\\pi^{*}(a_{\\alpha}|s)d a}=\\pi^{\\gamma}(a_{\\alpha}|s)d a_{\\alpha}}\\\\ &{\\phantom{=\\int\\pi^{\\gamma}(a_{\\alpha}|a_{\\alpha},s)\\pi^{*}(a_{\\alpha}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Note that the second equation holds because $p(a_{t}|a_{0})$ represents the forward transition probability in the diffusion process, which is unrelated to $s$ and $\\pi_{0}^{*}$ is exactly the optimal policy $\\pi^{*}$ . The ", "page_idx": 20}, {"type": "text", "text": "forth equation holds for the similar reasons. We exchange the condition variable to derive the final relationship between $\\pi_{t}^{*}(a|s)$ and $\\pi_{t}^{*}(a|s)$ . Having this relationship, we can directly calculate their score functions as follows: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{a_{t}}\\log\\pi_{t}^{*}(a_{t}|s)=\\nabla_{a_{t}}\\log\\pi_{t}^{\\mathcal{D}}(a_{t}|s)+\\nabla_{a_{t}}\\log\\mathbb{E}_{a_{0}\\sim\\pi^{\\mathcal{D}}(a_{0}|a_{t},s)}[\\frac{\\frac{d^{*}(s,a_{0})}{d^{*}(s,a_{0})}}{\\int_{a\\in A}\\frac{d^{*}(s,a)}{d^{*}(s,a_{0})}\\pi^{\\mathcal{D}}(a|s)d a}]}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad=\\nabla_{a_{t}}\\log\\pi_{t}^{\\mathcal{D}}(a_{t}|s)+\\nabla_{a_{t}}\\log\\frac{\\mathbb{E}_{a_{0}\\sim\\pi^{\\mathcal{D}}(a_{0}|a_{t},s)}[\\frac{d^{*}(s,a_{0})}{d^{\\mathcal{D}}(s,a_{0})}]}{\\int_{a\\in A}\\frac{d^{*}(s,a)}{d^{\\mathcal{D}}(s,a)}\\pi^{\\mathcal{D}}(a|s)d a}}\\\\ &{\\quad\\quad\\quad\\quad\\quad=\\nabla_{a_{t}}\\log\\pi_{t}^{\\mathcal{D}}(a_{t}|s)+\\nabla_{a_{t}}\\log\\mathbb{E}_{a_{0}\\sim\\pi^{\\mathcal{D}}(a_{0}|a_{t},s)}[\\frac{d^{*}(s,a_{0})}{d^{\\mathcal{D}}(s,a_{0})}]}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Note that the second equation holds because the integral a\u2208AddD\u2217((ss,,aa))\u03c0 $\\begin{array}{r}{\\int_{a\\in\\mathcal{A}}\\frac{d^{*}(s,a)}{d^{D}(s,a)}\\pi^{D}(a|s)d a}\\end{array}$ is unrelevant to $a_{0}$ , $a_{t}$ and because of so, its gradient with respect to $a_{t}$ becomes 0. ", "page_idx": 21}, {"type": "text", "text": "Lemma 1. Given a random variable $X$ and its corresponding distribution $P(X)$ , for any non-negative function $f(x)$ , the following problem is convex and its optimizer is given by $y^{*}=\\log\\mathbb{E}_{x\\sim P(X)}[f(x)]$ . ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{y}\\mathbb{E}_{x\\sim P(X)}[f(x)\\cdot e^{-y}+y]\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof. Because $y$ is the constant optimizer, this problem can be reformulated as: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{y}\\mathbb{E}_{x\\sim P(X)}[f(x)]\\cdot e^{-y}+y\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "We first verify that this problem is actually a convex problem with respect to $y$ . Because $f(x)$ is a non-negative function, we can take the second derivative of this objective and get: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{E}_{x\\sim P(X)}[f(x)]\\cdot e^{-y}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Because $f(x)$ is a non-negative function, the second derivative is also non-negative, which means this problem is a convex problem with respect to $y$ . For a convex problem, the optimizer can be induced from the first-order condition: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{y^{*}}\\big(\\mathbb{E}_{x\\sim P(X)}[f(x)]\\cdot e^{-y^{*}}+y^{*}\\big)=0}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad y^{*}=\\log\\mathbb{E}_{x\\sim P(X)}[f(x)]}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Theorem 1. $\\nabla_{a_{t}}\\log\\mathbb{E}_{\\pi^{\\mathcal{D}}(a_{0}|a_{t},s)}[w(s,a_{0})]$ can be obtained by solving the following optimization problem: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\theta}\\mathbb{E}_{t\\sim\\mathcal{U}(0,T)}\\mathbb{E}_{\\pi^{\\mathcal{D}}(a_{0}|s)}\\mathbb{E}_{p(a_{t}|a_{0})}\\Big[w\\big(s,a_{0}\\big)e^{-g_{\\theta}(s,a_{t},t)}+g_{\\theta}\\big(s,a_{t},t\\big)\\Big]\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "The optimal solution $\\theta^{*}$ satisfies $\\nabla_{a_{t}}g_{\\theta^{*}}(s,a_{t},t)=\\nabla_{a_{t}}\\log\\mathbb{E}_{\\pi^{\\mathcal{D}}(a_{0}|a_{t},s)}[w(s,a_{0})],$ . ", "page_idx": 21}, {"type": "text", "text": "Proof. Because the forward diffusion process simply adds noise to $a_{0}$ and is unrelated to $s$ , taking $s$ as a condition doesn\u2019t affect $p(a_{t}|a_{0})$ . This means $\\bar{p(a_{t}|a_{0},t)}=p(a_{t}|a_{0})$ . By changing the conditional variables from $(a_{0})^{(1:K)}$ to $(a_{t})^{(1:K)}$ we have: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}_{t\\sim\\mathcal{U}(0,T)}\\mathbb{E}_{\\pi^{\\mathcal{P}}(a_{0}|s)}\\mathbb{E}_{p(a_{t}|a_{0})}\\Big[w\\big(s,a_{0}\\big)e^{-g_{\\theta}(s,a_{t},t)}+g_{\\theta}\\big(s,a_{t},t\\big)\\Big]}\\\\ &{=\\mathbb{E}_{t\\sim\\mathcal{U}(0,T)}\\mathbb{E}_{\\pi^{\\mathcal{P}}(a_{0}|s)}\\mathbb{E}_{p(a_{t}|a_{0},s)}\\Big[w\\big(s,a_{0}\\big)e^{-g_{\\theta}(s,a_{t},t)}+g_{\\theta}\\big(s,a_{t},t\\big)\\Big]}\\\\ &{=\\mathbb{E}_{t\\sim\\mathcal{U}(0,T)}\\mathbb{E}_{\\pi^{\\mathcal{P}}(a_{t},a_{0}|s)}\\Big[w\\big(s,a_{0}\\big)e^{-g_{\\theta}(s,a_{t},t)}+g_{\\theta}\\big(s,a_{t},t\\big)\\Big]}\\\\ &{=\\mathbb{E}_{t\\sim\\mathcal{U}(0,T)}\\mathbb{E}_{\\pi^{\\mathcal{P}}(a_{t}|s)}\\mathbb{E}_{\\pi^{\\mathcal{P}}(a_{0}|a_{t},s)}\\Big[w\\big(s,a_{0}\\big)e^{-g_{\\theta}(s,a_{t},t)}+g_{\\theta}\\big(s,a_{t},t\\big)\\Big]}\\\\ &{=\\mathbb{E}_{t\\sim\\mathcal{U}(0,T)}\\mathbb{E}_{\\pi^{\\mathcal{P}}(a_{t}|s)}\\Big[\\mathbb{E}_{\\pi^{\\mathcal{P}}(a_{0}|a_{t},s)}\\Big[w\\big(s,a_{0}\\big)\\Big]e^{-g_{\\theta}(s,a_{t},t)}+g_{\\theta}\\big(s,a_{t},t\\big)\\Big]}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "We first take the first-order derivative with respect to each $g_{\\theta}(s,a_{t},t)$ for this objective and solve its saddle point: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\nabla_{g_{\\theta}(s,a_{t},t)}\\mathbb{E}_{t\\sim\\mathcal{U}(0,T)}\\mathbb{E}_{\\pi^{\\mathcal{D}}(a_{t}\\mid s)}\\left[\\mathbb{E}_{\\pi^{\\mathcal{D}}(a_{0}\\mid a_{t},s)}\\left[w(s,a_{0})\\right]e^{-g_{\\theta}(s,a_{t},t)}+g_{\\theta}(s,a_{t},t)\\right]}\\\\ &{=\\!\\!\\frac{1}{T}\\cdot\\pi^{\\mathcal{D}}(a_{t}\\mid s)\\cdot\\Big(-\\mathbb{E}_{\\pi^{\\mathcal{D}}(a_{0}\\mid a_{t},s)}\\left[w(s,a_{0})\\right]e^{-g_{\\theta}(s,a_{t},t)}+1\\Big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Because any possible intermediate action $a_{t}$ has a positive value of $\\pi^{\\mathcal{D}}(a_{t}|s)$ , the saddle point $g_{\\theta^{*}}\\left(s,a_{t},t\\right)$ must satisfy: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{-\\mathbb{E}_{\\pi^{\\mathcal{D}}(a_{0}|a_{t},s)}\\big[w(s,a_{0})\\big]e^{-g_{\\theta^{*}}(s,a_{t},t)}+1=0}\\\\ &{\\qquad\\qquad\\qquad\\qquad g_{\\theta^{*}}(s,a_{t},t)=\\log\\mathbb{E}_{\\pi^{\\mathcal{D}}(a_{0}|a_{t},s)}[w(s,a_{0})]}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "and then $\\nabla_{a_{t}}g_{\\theta^{*}}(s,a_{t},t)=\\nabla_{a_{t}}\\log\\mathbb{E}_{\\pi^{\\mathcal{D}}(a_{0}|a_{t},s)}[w(s,a_{0})]$ ", "page_idx": 22}, {"type": "text", "text": "We then take the second-order derivative with respect to each $g_{\\theta}(s,a_{t},t)$ for this objective: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\nabla_{g_{\\theta}(s,a_{t},t)}^{2}\\mathbb{E}_{t\\sim\\mathcal{U}(0,T)}\\mathbb{E}_{\\pi^{\\mathcal{D}}(a_{t}\\mid s)}\\left[\\mathbb{E}_{\\pi^{\\mathcal{D}}(a_{0}\\mid a_{t},s)}\\left[w(s,a_{0})\\right]e^{-g_{\\theta}(s,a_{t},t)}+g_{\\theta}(s,a_{t},t)\\right]}\\\\ &{=\\!\\!\\frac{1}{T}\\cdot\\pi^{\\mathcal{D}}(a_{t}\\mid s)\\cdot\\mathbb{E}_{\\pi^{\\mathcal{D}}(a_{0}\\mid a_{t},s)}\\left[w(s,a_{0})\\right]e^{-g_{\\theta}(s,a_{t},t)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Because $w(s,a_{0})$ has to be non-negative for any $a_{0}$ to ensure a valid transformation from $\\pi^{\\mathcal{D}}$ to $\\pi^{*}$ , it\u2019s obvious that the second-order derivative is always non-negative. This means this objective is convex with respect to $g_{\\theta}(s,a_{t},t)$ and has convergence guarantee given unlimited model capacity. ", "page_idx": 22}, {"type": "text", "text": "Lemma 2. The following $f_{p}(\\boldsymbol{x})$ is a well-defined $f$ -divergence: ", "page_idx": 22}, {"type": "equation", "text": "$$\nf_{p}(x)={\\binom{(x-1)^{2}}{x\\log x-x+1}}\\quad x\\geq1\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof. By the definition of $f$ -divergence in R\u00e9nyi [39], $f_{p}(\\boldsymbol{x})$ should be a convex function from $[0,+\\infty)~\\rightarrow~(-\\infty,+\\infty]$ . $f_{p}(\\boldsymbol{x})$ should be finite for all $x~>~0$ and $f_{p}(1)\\ =\\ 0$ , $f_{p}(0)\\;=\\;$ $\\textstyle\\operatorname*{lim}_{x\\to0^{+}}f_{p}(x)$ . ", "page_idx": 22}, {"type": "text", "text": "We first verify that $f_{p}(\\boldsymbol{x})$ is convex. By taking first-order and second-order derivatives on $f_{p}(\\boldsymbol{x})$ we have: ", "page_idx": 22}, {"type": "equation", "text": "$$\nf_{p}^{\\prime}(x)={\\binom{2(x-1)}{\\log x}}\\quad x\\geq1\\quad\\quad\\;f_{p}^{\\prime\\prime}(x)={\\binom{2}{\\frac{1}{x}}}\\quad x\\geq1\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "It\u2019s obvious that $f_{p}^{\\prime}(x)$ is continuous and $f_{p}^{\\prime\\prime}(x)$ is strictly positive. According to the second-order condition for convex functions, $f_{p}(\\boldsymbol{x})$ is convex. ", "page_idx": 22}, {"type": "text", "text": "We then move on to verify the rest of the properties. It\u2019s obvious that given a specific $x>0$ , $f_{p}(\\boldsymbol{x})$ is finite and $f_{p}(1)=0$ . In mathematics, the value of $x\\log x$ when $x=0$ is defined by its right limit, which means $\\begin{array}{r}{f_{p}(0)=\\operatorname*{lim}_{x\\to0^{+}}f_{p}(x)=1}\\end{array}$ . \u53e3 ", "page_idx": 22}, {"type": "text", "text": "Proposition 1: Given the piece-wise $f-$ divergence in eq(30), $(f_{p}^{\\prime})^{-1}(x)$ and $f_{p}^{*}(x)$ has the following formulation: ", "page_idx": 22}, {"type": "equation", "text": "$$\n(f_{p}^{\\prime})^{-1}(x)=\\left\\{\\!\\!{\\frac{x}{2}}+1\\!\\!\\!\\begin{array}{l l}{{x\\geq0}}\\\\ {{e^{x}}}&{{x<0}}\\end{array}}\\right.\\,\\,\\,f_{p}^{*}(x)=\\left\\{\\!\\!{\\frac{x^{2}}{4}}+x\\!\\!\\!\\begin{array}{l l}{{x\\geq0}}\\\\ {{e^{x}-1}}&{{x<0}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof. We begin with the proof of $(f_{p}^{\\prime})^{-1}(x)$ . Given Lemma 2, the form of $(f_{p}^{\\prime})^{-1}(x)$ directly comes from the continuity and monotonic increasing property of $f_{p}^{\\prime}(x)$ . One can easily verify $(f_{p}^{\\prime})^{-1}(x)$ \u2019s form by calculating the reverse function of $f_{p}^{\\prime}(\\boldsymbol{x})$ . ", "page_idx": 22}, {"type": "text", "text": "We then continue with the proof of $f_{p}^{*}(x)$ . By the definition of Fenchel conjugate we have: ", "page_idx": 22}, {"type": "equation", "text": "$$\nf_{p}^{*}(x)=\\operatorname*{sup}_{z\\in[0,+\\infty)}x\\cdot z-f_{p}(z)\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Because here $f_{p}(\\boldsymbol{x})$ is a piecewise function, the supremum could be achieved in either $[0,1)$ or $\\lbrack1,+\\infty)$ . This means we should first take supremum over two intervals respectively and then choose the greater value as $f_{p}^{*}(x)$ . More specifically, we perform the following decomposition: ", "page_idx": 23}, {"type": "equation", "text": "$$\nf_{p}^{*}(x)=\\operatorname*{max}\\Big(\\operatorname*{sup}_{z\\in[0,1)}x\\cdot z-f_{p}(z),\\operatorname*{sup}_{z\\in[1,+\\infty)}x\\cdot z-f_{p}(z)\\Big)\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "When constrained to either $[0,1)$ or $\\lbrack1,+\\infty)$ , $f_{p}(z)$ is no longer piecewise, and the inner supremum becomes the Fenchel conjugate, albeit under a narrower interval. To calculate Fenchel conjugate under a limited interval, one should observe that for any given $x$ , the derivative of the inner objective satisfies: ", "page_idx": 23}, {"type": "equation", "text": "$$\n{\\frac{d(x\\cdot z-f_{p}(z))}{d z}}=x-f_{p}^{\\prime}(z)\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "By Lemma 2, we have that $f_{p}^{\\prime}(z)$ is strictly increasing. This means d(x\u00b7z\u2212fp(z)) has only three possible behaviors on any interval: always positive, always negative, or transitioning from positive to negative with a zero crossing within the interval. Then for any interval $[a,b]$ $;b],\\operatorname*{sup}_{z\\in[a,b]}x\\cdot z-f_{p}(z)$ has the following formulation. Note that similar results can be extended to open intervals. ", "page_idx": 23}, {"type": "equation", "text": "$$\nf_{p}^{*}(x)={\\left\\{\\begin{array}{l l}{x\\cdot(f_{p}^{\\prime})^{-1}(x)-f_{p}((f_{p}^{\\prime})^{-1}(x))}&{{\\mathrm{if}}\\quad(f_{p}^{\\prime})^{-1}(x)\\in[a,b]}\\\\ {\\operatorname*{max}\\left(x\\cdot a-f_{p}(a),x\\cdot b-f_{p}(b)\\right)}&{{\\mathrm{else}}}\\end{array}\\right.}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Leveraging this formulation, one can easily derive the close form solution of 34. In fact, $f_{p}^{*}(x)$ has the following formulation: ", "page_idx": 23}, {"type": "equation", "text": "$$\nf_{p}^{*}(x)=\\operatorname*{max}\\Big(\\left\\{\\begin{array}{l l l}{\\frac{x^{2}}{4}+x}&{x>0}\\\\ {x}&{x\\leq0}\\end{array},\\left\\{\\!\\!\\begin{array}{l l}{x}&{x\\geq0}\\\\ {e^{x}-1}&{x<0}\\end{array}\\right.\\Big)\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "The final form of $f_{p}^{*}(x)$ could be derived by directly taking the maximum. ", "page_idx": 23}, {"type": "text", "text": "C In-depth Discussion of In-sample Guidance Learning and Other Guidance Methods ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Difference between IGL and other inexact guidance methods One important property of IGL is that it can induce the exact optimal policy, rather than a policy similar to the optimal policy, if the optimal policy can be represented as a weighted behavior policy. Some of the previous methods use inexact guidance terms and thus have no guarantee of the induced policy [17, 16, 6]. Among them, Janner et al. [17] uses a mean-square-error (MSE) objective to train the guidance model. Ho et al. [16], Chung et al. [6] leverage training-free guidance by reusing the pre-trained diffusion model in the data prediction formulation to characterize intermediate guidance. ", "page_idx": 23}, {"type": "text", "text": "Difference between IGL and Contrastive Energy Prediction (CEP) Besides the inexact guidance methods mentioned above, Contrastive Energy Prediction (CEP) [32], is a guidance learning method that could induce exactly the desired policy if it can be represented as a weighted behavior policy. However, CEP is still prone to exploit overestimation error in the critic. We provide additional explanations for the differences between Contrastive Energy Prediction and In-sample Guidance Learning, as well as the reasons for these differences. Intuitively, the difference mainly stems from CEP\u2019s inherent property as a contrastive loss. ", "page_idx": 23}, {"type": "text", "text": "In order to obtain $\\nabla_{a_{t}}\\log\\mathbb{E}_{p(a_{0}|a_{t},s)}[w(s,a_{0})]$ , CEP will first train a model $g_{\\theta}(s,a_{t},t)$ to fit $\\log\\mathbb{E}_{p(a_{0}|a_{t},s)}[w(s,a_{0})]$ and then take the gradient of it. Its optimization objective has the following contrastive form: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{E}_{P(t)}\\mathbb{E}_{\\pi^{\\mathcal{D}}\\big((a_{0})^{(1;K)}|s\\big)}\\mathbb{E}_{p\\big((a_{t})^{(1;K)}|(a_{0})^{(1;K)},s\\big)}\\Big[-\\sum_{i=1}^{K}w\\big(s,(a_{0})^{(i)}\\big)\\log\\frac{e^{g_{\\theta}\\big(s,(a_{t})^{(i)},t\\big)}}{\\sum_{j=1}^{K}e^{g_{\\theta}\\big(s,(a_{t})^{(j)},t\\big)}}\\Big]\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Note that here $K$ must satisfy $K~>~1$ to ensure the optimal condition is $g_{\\theta^{*}}(s,a_{t},t)\\;\\;=\\;\\;$ $\\nabla_{a_{t}}\\log\\mathbb{E}_{p(a_{0}|a_{t},s)}[w(s,a_{0})]$ . This means under offilne setting, additional fake actions are required on every state in the dataset. CEP first uses diffusion model to pre-train a behavior policy $\\hat{\\pi}^{\\mathcal{D}}$ from the dataset and then generates multiple fake actions $(a_{0})^{(1:K)}\\sim\\hat{\\pi}^{{\\mathcal D}}(a_{0}|s)$ for each state in the dataset. The second expectation in the objective will be calculated using these fake actions. It\u2019s clear that when $\\hat{\\pi}^{\\mathcal{D}}$ mistakenly generates OOD actions, CEP still uses action evaluation model to predict $w(s,a_{0}^{O O D})$ on these OOD actions. The introduction of $w(s,a_{0}^{O O D})$ can make $\\nabla_{a_{t}}\\log\\mathbb{E}_{p(a_{0}|a_{t},s)}[w(s,a_{0})]$ prefer these $a_{0}^{O O D}$ . This is because $\\nabla_{a_{t}}\\log\\mathbb{E}_{p(a_{0}|a_{t},s)}[w(s,a_{0})]$ will guide the samples towards actions with high $w(s,a_{0})$ value. These high $w(s,a_{0})$ value actions can be OOD actions whose $w(s,a_{0})$ is overestimated. The catastrophic result is that CEP may guide action samples towards OOD actions with erroneously high $w(s,a_{0})$ value rather than real high-quality actions. ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "Finally and most importantly, we will explain from a probabilistic perspective why the contrastive loss in CEP can\u2019t be estimated with one sample from $\\pi^{\\mathcal{D}}(a|s)$ . The derivation of the optimality condition also depends on changing the conditional variables from $(a_{0})^{(1:K)}$ to $(a_{t})^{(1:K)}$ : ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{E}_{P(t)}\\mathbb{E}_{\\pi^{p}\\big((a_{0})^{(1;K)}|s\\big)}\\mathbb{E}_{p\\big((a_{t})^{(1;K)}|s\\big)}\\Big[-\\sum_{i=1}^{K}w\\big(s,(a_{0})^{(i)}\\big)\\log\\frac{e^{g\\theta\\big(s,(a_{t})^{(i)},t\\big)}}{\\sum_{j=1}^{K}e^{g\\theta\\big(s,(a_{t})^{(j)},t\\big)}}\\Big]}\\\\ {\\displaystyle=\\mathbb{E}_{P(t)}\\mathbb{E}_{p((a_{0})^{(1;K)},(a_{t})^{(1;K)}|s)}\\Big[-\\sum_{i=1}^{K}w\\big(s,(a_{0})^{(i)}\\big)\\log\\frac{e^{g\\theta\\big(s,(a_{t})^{(i)},t\\big)}}{\\sum_{j=1}^{K}e^{g\\theta\\big(s,(a_{t})^{(i)},t\\big)}}\\Big]}\\\\ {\\displaystyle=\\mathbb{E}_{P(t)}\\mathbb{E}_{p((a_{t})^{(1;K)}|s)}\\mathbb{E}_{p\\big((a_{0})^{(1;K)}|(a_{t})^{(1;K)},s\\big)}\\Big[-\\sum_{i=1}^{K}w\\big(s,(a_{0})^{(i)}\\big)\\log\\frac{e^{g\\theta\\big(s,(a_{t})^{(i)},t\\big)}}{\\sum_{j=1}^{K}e^{g\\theta\\big(s,(a_{t})^{(j)},t\\big)}}\\Big]}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "To derive the optimality condition that $g_{\\theta}\\big(s,(a_{t})^{(i)},t\\big)=\\log\\mathbb{E}_{p\\big((a_{0})^{(i)}|(a_{t})^{(i)},s\\big)}[w\\big(s,(a_{0})^{(i)}\\big)]$ for each $i$ , one have to decompose the joint probability $p\\big((a_{0})^{(1:K)}|(a_{t})^{(1:K)},s\\big)$ into the product of each $(a_{0})^{i}$ \u2019s conditional probability: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\mathbb{E}_{P(t)}\\mathbb{E}_{p((a_{t})^{(1:K)}|s)}\\mathbb{E}_{\\Pi_{i=1}^{K}p\\big((a_{0})^{(i)}|(a_{t})^{(1:K)},s\\big)}\\Big[-\\displaystyle\\sum_{i=1}^{K}w\\big(s,(a_{0})^{(i)}\\big)\\log\\frac{e^{g\\theta\\big(s,(a_{t})^{(i)},t\\big)}}{\\displaystyle\\sum_{j=1}^{K}e^{g\\theta\\big(s,(a_{t})^{(j)},t\\big)}}\\Big]}\\\\ &{=\\mathbb{E}_{P(t)}\\mathbb{E}_{p((a_{t})^{(1:K)}|s)}\\Big[-\\displaystyle\\sum_{i=1}^{K}\\mathbb{E}_{p\\big((a_{0})^{(i)}|(a_{t})^{(i)},s\\big)}w\\big(s,(a_{0})^{(i)}\\big)\\log\\frac{e^{g\\theta\\big(s,(a_{t})^{(i)},t\\big)}}{\\displaystyle\\sum_{j=1}^{K}e^{g\\theta\\big(s,(a_{t})^{(j)},t\\big)}}\\Big]}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Note that here we absorb the expectation over $\\Pi_{i=1}^{K}p\\big((a_{0})^{(i)}|(a_{t})^{(1:K)},s\\big)$ into the sum over $i$ and replace it with $p\\big((a_{0})^{(i)}|(a_{t})^{(i)},s\\big)$ . This is because each reverse process $p\\big((a_{0})^{(i)}|(a_{t})^{(i)},s\\big)$ is independent of other $(a_{t})^{(j\\neq i)}$ . ", "page_idx": 24}, {"type": "text", "text": "The requirement of decomposing joint probability $p\\big((a_{0})^{(1:K)}|(a_{t})^{(1:K)},s\\big)$ means that $(a_{0})^{(1:K)}$ should be mutually independent from each other, once $(a_{t})^{(1:K)}$ and $s$ are determined. Obviously, if we only have one sample of $a_{0}$ , $(a_{0})^{(1:K)}$ must be identical and not independent. On the other hand, because the objective of In-sample Guidance Learning doesn\u2019t contain expectation over any joint distribution, it can be estimated with only one sample from $\\pi^{\\mathcal{D}}(a|s)$ . ", "page_idx": 24}, {"type": "text", "text": "D Experimental Details ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Additional details for the practical algorithm In this part, we further provide the implementation details based on the given pseudo-code. Firstly, the score network $\\epsilon_{\\theta}$ we used is based on a U-net architecture, which is fairly common in diffusion-based RL algorithms [1, 59, 32]. For the DICE part, we use the double Q-learning trick [10] for the training of Q. For sampling, we use DPM-solver [31] to accelerate the sampling of the diffusion model, while the score function is given by IGL. The implementation of our score model and sampling process are based on DPM-solver, which uses MIT license. Diffusion-DICE contains 2 hyper-parameters in total, which are $\\alpha$ in the DICE part and $K$ during sampling. ", "page_idx": 24}, {"type": "text", "text": "Toy case experimental details As shown in Fig. 2, the behavior policy follows a standard Gaussian distribution constrained in an annular region. We sample 1500 times from the ground truth behavior policy to construct the offline dataset. The reward model $\\hat{R}$ is represented with a 3-layer MLP and because we don\u2019t have to model complicated policy distribution, the score network $\\epsilon_{\\theta}$ in the toy case is simply multi-layer MLP. For better visualization, we sample 500 candidates during the guide stage. For QGPO, because it doesn\u2019t have the select stage, the sample distribution remains the same. For the other 2 methods, we select actions from candidates generated either from diffusion behavior policy (IDQL), or from guided policy (Diffusion-DICE) for 64 times. The reverse diffusion trajectories contain 16 action outputs. Because these 3 methods stand for different paradigms, we tuned different hyper-parameter sets for all of them to achieve the best performance. ", "page_idx": 25}, {"type": "text", "text": "D4RL experimental details For all tasks, we ran Diffusion-DICE for $10^{6}$ steps and reported the final performance. In MuJoCo locomotion tasks, we computed the average mean returns over 10 evaluations across 5 different seeds. For AntMaze tasks, we calculated the average over 50 evaluations, also across 5 seeds. Because the inference of diffusion model is relatively slow, we conduct the evaluation every $4\\cdot10^{4}$ training steps. Following previous research, we standardized the returns by dividing the difference in returns between the best and worst trajectories in MuJoCo tasks. In AntMaze tasks, we subtracted 1 from the rewards following previous methods [23, 34]. Note that for the D4RL benchmark and its corresponding datasets, Apache-2.0 license is used. ", "page_idx": 25}, {"type": "text", "text": "For the network, we use a 3-layer MLP with 256 hidden units to represent $Q$ and $V$ . For the guidance network $g_{\\boldsymbol{\\theta}}$ , we use a slightly more complicated 4-layer MLP with 256 hidden units. Note that the time embedding in the guidance network is the Gaussian Fourier projection. We use Adam optimizer [21] to update all the networks, with a learning rate of $3\\cdot10^{-4}$ . The target network of $Q$ used for double Q-learning trick is soft updated with a weight of $5\\cdot10^{-3}$ . ", "page_idx": 25}, {"type": "text", "text": "For the baseline algorithms, we report the performance of IDQL under any number of hyperparameters. Other baseline methods\u2019 results are sourced from their original papers. For all tasks, we list the chosen hyper-parameters in Table 2. As mentioned before, Diffusion-DICE requires significantly fewer action candidates during the select stage and we present the comparison of $K$ between Diffusion-DICE and IDQL in Table 3. ", "page_idx": 25}, {"type": "text", "text": "To further demonstrate the superior performance of Diffusion-DICE compared to traditional DICE algorithms, we conduct ablation studies by replacing the policy in Diffusion-DICE with a simple Gaussian policy, while preserving the piecewise $f$ -divergence. The results are given in Figure 4. ", "page_idx": 25}, {"type": "image", "img_path": "EIl9qmMmvy/tmp/b1f47c13cad16833a54caa02a416f3bf673a9f6dcffe134c5fb6c3eb1965c30e.jpg", "img_caption": ["Figure 4: Although using piecewise $f$ -divergence, the Gaussian policy in traditional DICE algorithm still results in inferior performance than using diffusion policy. "], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "We also perform hyper-parameter study for candidate number $K$ . To compare the performance of Diffusion-DICE under different $K$ , we select some of the datasets in D4RL benchmark and test its performance following the same protocol before. Because the optimal $K$ for MuJoCo locomotion tasks and AntMaze navigation tasks have significant differences in magnitude, we chose to test 5 neighboring orders of magnitude around the optimal K value. The results are given in Table 4. ", "page_idx": 25}, {"type": "text", "text": "Comparison of average $Q(s,a)$ curves For a detailed comparison between the guide-then-select paradigm and the select-from-behavior paradigm regarding error exploitation, we present their average $Q(s,a)$ curves normalized returns in Fig. 5. It\u2019s evident from the results that the average $Q(s,a)$ curves of the select-from-behavior paradigm almost remain above the guide-then-select paradigm, while the latter exhibits better performance. ", "page_idx": 25}, {"type": "text", "text": "Learning curves on D4RL benchmark We present the learning curves on D4RL benchmark in Figure 6 and Figure 7. ", "page_idx": 25}, {"type": "table", "img_path": "EIl9qmMmvy/tmp/777ecdca8ce072ce487377923c4742976bb0b38b977e494f7ec7de6d0c08b9b2.jpg", "table_caption": ["Table 2: The chosen $\\alpha$ and $K$ in Diffusion-DICE. "], "table_footnote": [], "page_idx": 26}, {"type": "table", "img_path": "EIl9qmMmvy/tmp/d169c5d2b18fea143ed8ba6b8c7c45f378cfbd97083f6555f9550f320d472d89.jpg", "table_caption": ["Table 3: Comparison of $K$ between Diffusion-DICE and IDQL "], "table_footnote": [], "page_idx": 26}, {"type": "table", "img_path": "EIl9qmMmvy/tmp/fc682996ecf682271c1da09377ddf5c29796657d9bf6df5779b081c68b0cec4f.jpg", "table_caption": ["Table 4: Hyper-parameter Study for $K$ "], "table_footnote": [], "page_idx": 26}, {"type": "table", "img_path": "EIl9qmMmvy/tmp/7996a33088a68d8e9bc1c20e3ee119e1e56c3475db6f8d09c819bb1945a919c0.jpg", "table_caption": [], "table_footnote": [], "page_idx": 26}, {"type": "image", "img_path": "EIl9qmMmvy/tmp/caa4932319c99e41cbda72840f0efce1d74bc9812202bb8e437d6f8fda4d8db8.jpg", "img_caption": ["Figure 5: $Q(s,a)$ curves for guide-then-select paradigm and select-from-behavior paradigm. "], "img_footnote": [], "page_idx": 27}, {"type": "image", "img_path": "EIl9qmMmvy/tmp/295296c4c10ce1a48c0c704a0747c2f2ada1992a53a3f4ec6f36cde304dfda7c.jpg", "img_caption": ["Figure 6: Learning curves of Diffusion-DICE on D4RL MuJoCo locomotion datasets. "], "img_footnote": [], "page_idx": 27}, {"type": "image", "img_path": "EIl9qmMmvy/tmp/8897c0baa49acc972a2c78e8f601c77f0aaeec17148c9770aee7009e82cc399e.jpg", "img_caption": ["Figure 7: Learning curves of Diffusion-DICE on D4RL AntMaze navigation datasets. "], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "E Experiments Compute Resources ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "For all the experiments, we evaluate Diffusion-DICE on either NVIDIA RTX 3080Ti GPUs or NVIDIA RTX 4090 GPUs. The training process of Diffusion-DICE takes about 6 hours on NVIDIA RTX 3080Ti or 4 hours on NVIDIA RTX 4090. The evaluation process of Diffusion-DICE takes about 3 hours on NVIDIA RTX 3080Ti or 2 hours on NVIDIA RTX 4090. Because we parallelize training and testing process, a complete experiment takes about 8 hours on NVIDIA RTX 3080Ti or 4 hours on NVIDIA RTX 4090. ", "page_idx": 28}, {"type": "text", "text": "F Broader Impacts ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Besides the positive social impacts that Diffusion-DICE can help solve various practical offline RL tasks, for example, robotics, health care, industrial control tasks, some potential negative impacts also exists. Part of the Diffusion-DICE algorithm, the In-sample Guidance Learning, can also be applied to other generative tasks like image generation. This could be used to generate fake images, which could possibly mislead the public after being spread online. ", "page_idx": 28}]