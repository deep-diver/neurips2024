[{"figure_path": "95VyH4VxN9/figures/figures_1_1.jpg", "caption": "Figure 1: How SAD enables autonomous driving from vision to planning: The system processes inputs from six cameras across multiple frames. The perception module encodes feature information related to the present input frame (T = n), the prediction module predicts feature information of the next frame using sequential information (T = n + 1), and the model output generates a steering and acceleration plan. This process creates a bird's eye view (BEV) and trajectory plan for navigation.", "description": "This figure illustrates the Spiking Autonomous Driving (SAD) model's workflow. It takes input from six cameras, processes this information through three main modules (perception, prediction, and planning), and outputs a steering angle and speed for navigation.  The perception module creates a bird's-eye view representation of the environment.  The prediction module forecasts future states based on sequential data, and the planning module uses these predictions to generate safe trajectories.", "section": "1 Introduction"}, {"figure_path": "95VyH4VxN9/figures/figures_2_1.jpg", "caption": "Figure 2: Overview of SAD. The multi-view features from the perception encoder, including a spiking ResNet with inverted bottleneck and spiking DeepLab head, are fed into a prediction module using spiking neurons. The perception decoder then generates lane divider, pedestrian, vehicle and drivable area predictions. Finally, the planning module models the scene and generates future predictions to inform rule-based command decisions for turning, stopping, and goal-directed navigation.", "description": "This figure shows a detailed overview of the Spiking Autonomous Driving (SAD) model's architecture, illustrating the flow of information and processing steps across the three main modules: perception, prediction, and planning.  The perception module takes multi-view camera data, processes it using spiking neural networks (SNNs), such as a ResNet, to create a bird's eye view (BEV) representation of the scene. This representation is then passed to the prediction module, which uses spiking neurons to predict future states.  Finally, the planning module uses the predictions and scene understanding to generate safe and efficient driving commands.", "section": "3 Method"}, {"figure_path": "95VyH4VxN9/figures/figures_3_1.jpg", "caption": "Figure 3: The perception module. The encoder takes multi-camera input data, passes it through a spiking ResNet with inverted bottleneck to generate feature representations, each of which has its own depth estimation. These are fused and passed to the decoder, which generates predictions for lane dividers, pedestrians, vehicles and drivable areas.", "description": "This figure illustrates the architecture of the perception module in the Spiking Autonomous Driving (SAD) model.  The encoder processes data from multiple cameras, using a spiking ResNet with an inverted bottleneck to extract features and depth estimations. These are combined, and then the decoder uses this information to generate a bird's eye view representation of the scene, predicting the location of lane dividers, pedestrians, vehicles, and drivable areas.", "section": "3 Method"}, {"figure_path": "95VyH4VxN9/figures/figures_5_1.jpg", "caption": "Figure 4: Dual pathway modeling for prediction. Neuron a captures future multi-modality by incorporating uncertainty distribution. Neuron b compensates for information gaps using past variations.", "description": "This figure illustrates the dual-pathway prediction model used in the paper.  It shows how the model uses two parallel streams of LIF neurons to predict future states. One pathway (Neuron a) incorporates uncertainty about future states using a Gaussian distribution. The other pathway (Neuron b) uses historical information to compensate for missing data and provide a more robust prediction. The outputs from both pathways are then fused together to generate the final prediction.", "section": "3.3 Prediction: Fusing Parallel Spike Streams"}, {"figure_path": "95VyH4VxN9/figures/figures_8_1.jpg", "caption": "Figure 5: Qualitative Results of the SAD Model on the nuScenes Dataset. (a) displays six camera view inputs utilized by the model. (b) illustrates the planning result of the ANN model, and (c) presents the planning results of our SAD model. The comparison shows that our SAD model can achieve performance comparable to that of the ANN model and successfully generate a safe trajectory.", "description": "This figure shows a qualitative comparison of the Spiking Autonomous Driving (SAD) model and a traditional Artificial Neural Network (ANN) model on the nuScenes dataset.  It showcases the input from six cameras (a), the planning output from an ANN (b), and the planning output from the SAD model (c). The comparison highlights the SAD model's ability to generate a safe and comparable trajectory to the ANN model, despite using spiking neural networks.", "section": "4.2 Visualization"}, {"figure_path": "95VyH4VxN9/figures/figures_16_1.jpg", "caption": "Figure 6: MS-ResNet and SEW-ResNet.", "description": "This figure compares the architectures of MS-ResNet and SEW-ResNet.  Both are residual learning methods for Spiking Neural Networks (SNNs) designed to overcome the degradation problem in deep SNNs.  MS-ResNet creates shortcut connections between membrane potentials of different layers, while SEW-ResNet uses shortcut connections between the output spikes of different layers.  The figure illustrates the distinct ways these architectures handle the identity mapping for residual connections, showing the spatial and temporal forward propagation paths within each.", "section": "3.2 Perception: Distinct Temporal Strategies for Encoder and Decoder"}, {"figure_path": "95VyH4VxN9/figures/figures_19_1.jpg", "caption": "Figure 5: Qualitative Results of the SAD Model on the nuScenes Dataset. (a) displays six camera view inputs utilized by the model. (b) illustrates the planning result of the ANN model, and (c) presents the planning results of our SAD model. The comparison shows that our SAD model can achieve performance comparable to that of the ANN model and successfully generate a safe trajectory.", "description": "This figure displays a comparison of the planning results between the proposed Spiking Autonomous Driving (SAD) model and a traditional Artificial Neural Network (ANN) model.  Subfigure (a) shows the six camera view inputs used by both models. Subfigure (b) displays the results from an ANN model, visualizing the planned trajectory, drivable areas, lane markings, vehicles, and pedestrians. Subfigure (c) displays the corresponding results generated by the SAD model.  The comparison highlights that SAD, despite using spiking neurons, performs comparably to the ANN and produces a safe and viable trajectory.", "section": "4.2 Visualization"}]