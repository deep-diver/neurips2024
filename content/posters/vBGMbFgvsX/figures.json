[{"figure_path": "vBGMbFgvsX/figures/figures_6_1.jpg", "caption": "Figure 1: (a) The vertical axis represents the interquartile mean (IQM) [26] of normalized task return across 29 tasks. HEPO outperforms the policy trained solely with a heavily engineered heuristic reward function (H-only) and other methods, demonstrating that HEPO makes better use of heuristic rewards for learning. (b) The horizontal axis shows the probability that algorithm X outperforms the policy trained solely with heuristic rewards (H-only). HEPO achieves a 62% probability of improvement over the heuristic policy on average, with the lower bound of the confidence interval above 50%, indicating statistically significant improvements over the heuristic policy.", "description": "This figure compares the performance of HEPO against several baselines across 29 tasks.  Subfigure (a) shows the interquartile mean (IQM) of the normalized task return for each algorithm, demonstrating that HEPO significantly outperforms the other methods, particularly the \"H-only\" (heuristic-only) baseline. Subfigure (b) shows the probability that each algorithm outperforms the H-only baseline, with HEPO showing a probability of improvement greater than 50%, indicating statistical significance.", "section": "4.1 Benchmark results"}, {"figure_path": "vBGMbFgvsX/figures/figures_6_2.jpg", "caption": "Figure 1: (a) The vertical axis represents the interquartile mean (IQM) [26] of normalized task return across 29 tasks. HEPO outperforms the policy trained solely with a heavily engineered heuristic reward function (H-only) and other methods, demonstrating that HEPO makes better use of heuristic rewards for learning. (b) The horizontal axis shows the probability that algorithm X outperforms the policy trained solely with heuristic rewards (H-only). HEPO achieves a 62% probability of improvement over the heuristic policy on average, with the lower bound of the confidence interval above 50%, indicating statistically significant improvements over the heuristic policy.", "description": "This figure compares the performance of different reinforcement learning algorithms on 29 robotic tasks.  Panel (a) shows the interquartile mean (IQM) of normalized task return, illustrating that HEPO significantly outperforms algorithms trained solely on heuristic rewards or those that attempt to balance heuristic and task rewards.  Panel (b) displays the probability that each algorithm outperforms the heuristic-only approach, highlighting HEPO's superior performance with a probability exceeding 50%.  The results strongly suggest that HEPO's method for incorporating heuristic rewards is more effective than existing methods.", "section": "4.1 Benchmark results"}, {"figure_path": "vBGMbFgvsX/figures/figures_7_1.jpg", "caption": "Figure 1: (a) The vertical axis represents the interquartile mean (IQM) [26] of normalized task return across 29 tasks. HEPO outperforms the policy trained solely with a heavily engineered heuristic reward function (H-only) and other methods, demonstrating that HEPO makes better use of heuristic rewards for learning. (b) The horizontal axis shows the probability that algorithm X outperforms the policy trained solely with heuristic rewards (H-only). HEPO achieves a 62% probability of improvement over the heuristic policy on average, with the lower bound of the confidence interval above 50%, indicating statistically significant improvements over the heuristic policy.", "description": "This figure presents a comparison of the performance of different RL algorithms, including the proposed HEPO method, on various robotic tasks.  Panel (a) shows the interquartile mean (IQM) of normalized task returns, demonstrating HEPO's superior performance compared to other methods. Panel (b) displays the probability of improvement, indicating HEPO significantly outperforms policies trained only on heuristic rewards.  This showcases HEPO's effectiveness in utilizing heuristic information while prioritizing the task objective, even with limited data.", "section": "4.1 Benchmark results"}, {"figure_path": "vBGMbFgvsX/figures/figures_8_1.jpg", "caption": "Figure 3: (a) We show that using the policies trained with heuristic rewards (J-only) is better than using the policies trained with task rewards (J-only) when training HEPO. (b) HEPO(Joint) that collects trajectories using both policies leads to better performance than HEPO(Alternating) that alternates between two policies to collect trajectories. See Section 4.3 for details", "description": "This figure presents ablation study results comparing two key design choices in the HEPO algorithm: (a) the reference policy used in the constraint J(\u03c0) \u2265 J(\u03c0ref) and (b) the strategy for collecting trajectories (Joint vs. Alternating).  Subfigure (a) shows that using the heuristic policy as the reference policy yields better performance than using the task-only policy in the constraint. Subfigure (b) demonstrates that concurrently collecting trajectories using both the enhanced policy and the heuristic policy (Joint) leads to better performance than alternating between policies for trajectory collection.", "section": "4.3 Ablation Studies"}, {"figure_path": "vBGMbFgvsX/figures/figures_16_1.jpg", "caption": "Figure 1: (a) The vertical axis represents the interquartile mean (IQM) [26] of normalized task return across 29 tasks. HEPO outperforms the policy trained solely with a heavily engineered heuristic reward function (H-only) and other methods, demonstrating that HEPO makes better use of heuristic rewards for learning. (b) The horizontal axis shows the probability that algorithm X outperforms the policy trained solely with heuristic rewards (H-only). HEPO achieves a 62% probability of improvement over the heuristic policy on average, with the lower bound of the confidence interval above 50%, indicating statistically significant improvements over the heuristic policy.", "description": "This figure compares the performance of HEPO against other baseline methods across 29 benchmark tasks. The interquartile mean (IQM) of normalized task return shows that HEPO outperforms other methods, particularly the H-only baseline which uses only a heuristic reward.  The probability of improvement metric further confirms that HEPO significantly outperforms the H-only baseline, indicating the effectiveness of its policy improvement constraint.", "section": "4.1 Benchmark results"}, {"figure_path": "vBGMbFgvsX/figures/figures_17_1.jpg", "caption": "Figure 1: (a) The vertical axis represents the interquartile mean (IQM) [26] of normalized task return across 29 tasks. HEPO outperforms the policy trained solely with a heavily engineered heuristic reward function (H-only) and other methods, demonstrating that HEPO makes better use of heuristic rewards for learning. (b) The horizontal axis shows the probability that algorithm X outperforms the policy trained solely with heuristic rewards (H-only). HEPO achieves a 62% probability of improvement over the heuristic policy on average, with the lower bound of the confidence interval above 50%, indicating statistically significant improvements over the heuristic policy.", "description": "This figure shows a comparison of different reinforcement learning algorithms on 29 robotic tasks.  The algorithms are compared based on their normalized task return (a measure of how well they perform on the task relative to a baseline and a random policy) and the probability that they outperform a heuristic-only policy. HEPO significantly outperforms other methods including classic methods that ensure optimal policy invariance which shows theoretical guarantees but fails in practice.  HEPO consistently improves performance compared to a baseline that uses only heuristic rewards, indicating its robustness and effectiveness in leveraging heuristic rewards to improve task performance.", "section": "4.1 Benchmark results"}, {"figure_path": "vBGMbFgvsX/figures/figures_18_1.jpg", "caption": "Figure 1: (a) The vertical axis represents the interquartile mean (IQM) [26] of normalized task return across 29 tasks. HEPO outperforms the policy trained solely with a heavily engineered heuristic reward function (H-only) and other methods, demonstrating that HEPO makes better use of heuristic rewards for learning. (b) The horizontal axis shows the probability that algorithm X outperforms the policy trained solely with heuristic rewards (H-only). HEPO achieves a 62% probability of improvement over the heuristic policy on average, with the lower bound of the confidence interval above 50%, indicating statistically significant improvements over the heuristic policy.", "description": "This figure displays the results of experiments comparing HEPO's performance to other baselines on 29 tasks. Subfigure (a) shows the interquartile mean (IQM) of normalized task return for all algorithms, demonstrating that HEPO achieves the best performance. Subfigure (b) shows that HEPO outperforms the H-only policy (trained solely on heuristic rewards) with a 62% probability of improvement and a lower bound of the confidence interval above 50%, indicating a statistically significant improvement.", "section": "4.1 Benchmark results"}, {"figure_path": "vBGMbFgvsX/figures/figures_19_1.jpg", "caption": "Figure 1: (a) The vertical axis represents the interquartile mean (IQM) [26] of normalized task return across 29 tasks. HEPO outperforms the policy trained solely with a heavily engineered heuristic reward function (H-only) and other methods, demonstrating that HEPO makes better use of heuristic rewards for learning. (b) The horizontal axis shows the probability that algorithm X outperforms the policy trained solely with heuristic rewards (H-only). HEPO achieves a 62% probability of improvement over the heuristic policy on average, with the lower bound of the confidence interval above 50%, indicating statistically significant improvements over the heuristic policy.", "description": "This figure presents a comparison of different reinforcement learning algorithms on 29 robotic tasks.  The algorithms are tested using both task and heuristic rewards. Panel (a) shows the interquartile mean (IQM) of normalized task return, demonstrating HEPO's superior performance compared to other algorithms, especially when using heavily engineered heuristic rewards. Panel (b) illustrates the probability of each algorithm outperforming the heuristic-only policy, highlighting that HEPO significantly surpasses the heuristic-only policy in most cases (62% probability, with confidence interval's lower bound above 50%).", "section": "4.1 Benchmark results"}, {"figure_path": "vBGMbFgvsX/figures/figures_20_1.jpg", "caption": "Figure 1: (a) The vertical axis represents the interquartile mean (IQM) [26] of normalized task return across 29 tasks. HEPO outperforms the policy trained solely with a heavily engineered heuristic reward function (H-only) and other methods, demonstrating that HEPO makes better use of heuristic rewards for learning. (b) The horizontal axis shows the probability that algorithm X outperforms the policy trained solely with heuristic rewards (H-only). HEPO achieves a 62% probability of improvement over the heuristic policy on average, with the lower bound of the confidence interval above 50%, indicating statistically significant improvements over the heuristic policy.", "description": "This figure presents a comparison of different reinforcement learning algorithms on 29 robotic tasks.  Figure 1(a) shows the Interquartile Mean (IQM) of the normalized task return for each algorithm, demonstrating HEPO's superior performance compared to methods using only heuristic rewards (H-only), heuristic rewards with optimal policy invariance (PBRS), or a mixture of task and heuristic rewards (J+H). Figure 1(b) further highlights HEPO's statistical significance, showing a greater than 50% probability of outperforming the H-only baseline across tasks.  This indicates that HEPO effectively leverages heuristic rewards without sacrificing task performance.", "section": "4.1 Benchmark results"}, {"figure_path": "vBGMbFgvsX/figures/figures_21_1.jpg", "caption": "Figure 1: (a) The vertical axis represents the interquartile mean (IQM) [26] of normalized task return across 29 tasks. HEPO outperforms the policy trained solely with a heavily engineered heuristic reward function (H-only) and other methods, demonstrating that HEPO makes better use of heuristic rewards for learning. (b) The horizontal axis shows the probability that algorithm X outperforms the policy trained solely with heuristic rewards (H-only). HEPO achieves a 62% probability of improvement over the heuristic policy on average, with the lower bound of the confidence interval above 50%, indicating statistically significant improvements over the heuristic policy.", "description": "This figure presents a comparison of different reinforcement learning methods for solving robotic tasks using heuristic rewards. The left panel shows the interquartile mean (IQM) of the normalized task return for several methods, highlighting HEPO's superior performance.  The right panel displays the probability of each method outperforming the heuristic-only approach, further demonstrating HEPO's significant improvement.", "section": "4.1 Benchmark results"}, {"figure_path": "vBGMbFgvsX/figures/figures_24_1.jpg", "caption": "Figure 1: (a) The vertical axis represents the interquartile mean (IQM) [26] of normalized task return across 29 tasks. HEPO outperforms the policy trained solely with a heavily engineered heuristic reward function (H-only) and other methods, demonstrating that HEPO makes better use of heuristic rewards for learning. (b) The horizontal axis shows the probability that algorithm X outperforms the policy trained solely with heuristic rewards (H-only). HEPO achieves a 62% probability of improvement over the heuristic policy on average, with the lower bound of the confidence interval above 50%, indicating statistically significant improvements over the heuristic policy.", "description": "This figure compares the performance of HEPO against other methods across 29 tasks from the ISAAC and BI-DEX benchmarks.  The left panel shows the interquartile mean (IQM) of normalized task return, demonstrating that HEPO significantly outperforms other methods, including one trained solely with heuristic rewards. The right panel shows the probability of HEPO outperforming the heuristic-only policy, further highlighting HEPO's superior performance and statistical significance (probability exceeding 50%).", "section": "4.1 Benchmark results"}, {"figure_path": "vBGMbFgvsX/figures/figures_25_1.jpg", "caption": "Figure 1: (a) The vertical axis represents the interquartile mean (IQM) [26] of normalized task return across 29 tasks. HEPO outperforms the policy trained solely with a heavily engineered heuristic reward function (H-only) and other methods, demonstrating that HEPO makes better use of heuristic rewards for learning. (b) The horizontal axis shows the probability that algorithm X outperforms the policy trained solely with heuristic rewards (H-only). HEPO achieves a 62% probability of improvement over the heuristic policy on average, with the lower bound of the confidence interval above 50%, indicating statistically significant improvements over the heuristic policy.", "description": "This figure compares the performance of HEPO against other methods in terms of normalized task return and probability of improvement over the heuristic-only policy.  The interquartile mean (IQM) of normalized task return is shown in (a), indicating HEPO's superior performance.  The probability of improvement is shown in (b), showing that HEPO outperforms the heuristic-only policy in over 62% of the tasks, achieving statistical significance.", "section": "4.1 Benchmark results"}, {"figure_path": "vBGMbFgvsX/figures/figures_26_1.jpg", "caption": "Figure 1: (a) The vertical axis represents the interquartile mean (IQM) [26] of normalized task return across 29 tasks. HEPO outperforms the policy trained solely with a heavily engineered heuristic reward function (H-only) and other methods, demonstrating that HEPO makes better use of heuristic rewards for learning. (b) The horizontal axis shows the probability that algorithm X outperforms the policy trained solely with heuristic rewards (H-only). HEPO achieves a 62% probability of improvement over the heuristic policy on average, with the lower bound of the confidence interval above 50%, indicating statistically significant improvements over the heuristic policy.", "description": "This figure presents a comparison of the performance of different reinforcement learning algorithms across 29 tasks.  Part (a) shows the interquartile mean (IQM) of normalized task return. HEPO consistently outperforms other methods, indicating effective use of heuristic rewards. Part (b) shows the probability of each algorithm outperforming a heuristic-only policy. HEPO significantly surpasses the heuristic-only policy, demonstrating substantial improvement.", "section": "4.1 Benchmark results"}, {"figure_path": "vBGMbFgvsX/figures/figures_27_1.jpg", "caption": "Figure 1: (a) The vertical axis represents the interquartile mean (IQM) [26] of normalized task return across 29 tasks. HEPO outperforms the policy trained solely with a heavily engineered heuristic reward function (H-only) and other methods, demonstrating that HEPO makes better use of heuristic rewards for learning. (b) The horizontal axis shows the probability that algorithm X outperforms the policy trained solely with heuristic rewards (H-only). HEPO achieves a 62% probability of improvement over the heuristic policy on average, with the lower bound of the confidence interval above 50%, indicating statistically significant improvements over the heuristic policy.", "description": "This figure displays a comparison of the performance of different reinforcement learning algorithms on 29 robotic tasks. The algorithms are evaluated based on two metrics: the interquartile mean (IQM) of the normalized task return and the probability of improvement over the heuristic policy.  The results show that HEPO outperforms other algorithms in both metrics, indicating a statistically significant improvement in task performance. This improvement is consistent across a wide range of tasks and regardless of the heuristic rewards quality.", "section": "4.1 Benchmark results"}, {"figure_path": "vBGMbFgvsX/figures/figures_28_1.jpg", "caption": "Figure 1: (a) The vertical axis represents the interquartile mean (IQM) [26] of normalized task return across 29 tasks. HEPO outperforms the policy trained solely with a heavily engineered heuristic reward function (H-only) and other methods, demonstrating that HEPO makes better use of heuristic rewards for learning. (b) The horizontal axis shows the probability that algorithm X outperforms the policy trained solely with heuristic rewards (H-only). HEPO achieves a 62% probability of improvement over the heuristic policy on average, with the lower bound of the confidence interval above 50%, indicating statistically significant improvements over the heuristic policy.", "description": "This figure presents a comparison of different reinforcement learning algorithms' performance in maximizing task rewards when trained with heuristic rewards.  Subfigure (a) shows the interquartile mean (IQM) of normalized task returns, indicating HEPO's superior performance compared to other methods, including those using only task rewards (J-only) or only heuristic rewards (H-only). Subfigure (b) displays the probability of improvement for each algorithm compared to H-only, revealing that HEPO has a 62% chance of outperforming the heuristic-only policy, statistically significant due to the lower confidence bound exceeding 50%. This demonstrates HEPO's effectiveness in leveraging heuristic rewards to enhance task performance in a finite-data setting.", "section": "4.1 Benchmark results"}, {"figure_path": "vBGMbFgvsX/figures/figures_29_1.jpg", "caption": "Figure 1: (a) The vertical axis represents the interquartile mean (IQM) [26] of normalized task return across 29 tasks. HEPO outperforms the policy trained solely with a heavily engineered heuristic reward function (H-only) and other methods, demonstrating that HEPO makes better use of heuristic rewards for learning. (b) The horizontal axis shows the probability that algorithm X outperforms the policy trained solely with heuristic rewards (H-only). HEPO achieves a 62% probability of improvement over the heuristic policy on average, with the lower bound of the confidence interval above 50%, indicating statistically significant improvements over the heuristic policy.", "description": "This figure shows the results of comparing HEPO against several baselines across 29 robotic tasks.  The interquartile mean (IQM) of the normalized task return for HEPO is significantly higher than for the other baselines. The probability of HEPO outperforming the heuristic-only policy is 62%, which is statistically significant.", "section": "4.1 Benchmark results"}, {"figure_path": "vBGMbFgvsX/figures/figures_30_1.jpg", "caption": "Figure 1: (a) The vertical axis represents the interquartile mean (IQM) [26] of normalized task return across 29 tasks. HEPO outperforms the policy trained solely with a heavily engineered heuristic reward function (H-only) and other methods, demonstrating that HEPO makes better use of heuristic rewards for learning. (b) The horizontal axis shows the probability that algorithm X outperforms the policy trained solely with heuristic rewards (H-only). HEPO achieves a 62% probability of improvement over the heuristic policy on average, with the lower bound of the confidence interval above 50%, indicating statistically significant improvements over the heuristic policy.", "description": "This figure presents a comparison of the performance of HEPO against other baseline methods on 29 robotic tasks.  The interquartile mean (IQM) of the normalized task return shows HEPO's superiority.  Additionally, HEPO demonstrates a greater than 50% probability of improving upon the heuristic-only policy, showcasing statistically significant performance gains.", "section": "4.1 Benchmark results"}, {"figure_path": "vBGMbFgvsX/figures/figures_31_1.jpg", "caption": "Figure 1: (a) The vertical axis represents the interquartile mean (IQM) [26] of normalized task return across 29 tasks. HEPO outperforms the policy trained solely with a heavily engineered heuristic reward function (H-only) and other methods, demonstrating that HEPO makes better use of heuristic rewards for learning. (b) The horizontal axis shows the probability that algorithm X outperforms the policy trained solely with heuristic rewards (H-only). HEPO achieves a 62% probability of improvement over the heuristic policy on average, with the lower bound of the confidence interval above 50%, indicating statistically significant improvements over the heuristic policy.", "description": "This figure shows the results of comparing HEPO with other methods. The left panel (a) presents the interquartile mean (IQM) of the normalized task return across 29 tasks, demonstrating that HEPO outperforms other methods, especially H-only (heuristic-only). The right panel (b) shows that HEPO has a high probability (62%) of outperforming H-only, indicating statistically significant improvement.", "section": "4.1 Benchmark results"}, {"figure_path": "vBGMbFgvsX/figures/figures_32_1.jpg", "caption": "Figure 1: (a) The vertical axis represents the interquartile mean (IQM) [26] of normalized task return across 29 tasks. HEPO outperforms the policy trained solely with a heavily engineered heuristic reward function (H-only) and other methods, demonstrating that HEPO makes better use of heuristic rewards for learning. (b) The horizontal axis shows the probability that algorithm X outperforms the policy trained solely with heuristic rewards (H-only). HEPO achieves a 62% probability of improvement over the heuristic policy on average, with the lower bound of the confidence interval above 50%, indicating statistically significant improvements over the heuristic policy.", "description": "This figure presents a comparison of the performance of different reinforcement learning algorithms, including the proposed HEPO method, across various robotic manipulation tasks.  Panel (a) shows the interquartile mean (IQM) of the normalized task return, indicating that HEPO consistently outperforms the other methods, particularly the H-only baseline which uses only heuristic rewards.  Panel (b) displays the probability of each algorithm outperforming the H-only baseline, highlighting HEPO's significantly higher success rate (62%).  The results demonstrate HEPO's effectiveness in leveraging heuristic rewards to improve task performance, even surpassing policies trained solely on engineered heuristics.", "section": "4.1 Benchmark results"}, {"figure_path": "vBGMbFgvsX/figures/figures_33_1.jpg", "caption": "Figure 1: (a) The vertical axis represents the interquartile mean (IQM) [26] of normalized task return across 29 tasks. HEPO outperforms the policy trained solely with a heavily engineered heuristic reward function (H-only) and other methods, demonstrating that HEPO makes better use of heuristic rewards for learning. (b) The horizontal axis shows the probability that algorithm X outperforms the policy trained solely with heuristic rewards (H-only). HEPO achieves a 62% probability of improvement over the heuristic policy on average, with the lower bound of the confidence interval above 50%, indicating statistically significant improvements over the heuristic policy.", "description": "This figure presents a comparison of different reinforcement learning algorithms in terms of their performance on 29 tasks. The performance is measured using the interquartile mean (IQM) of normalized task return and the probability of improvement over a baseline policy trained only with heuristic rewards. HEPO, the proposed algorithm, consistently outperforms the baseline and other algorithms in both measures, indicating its effectiveness in leveraging heuristic rewards for improved task performance.", "section": "4.1 Benchmark results"}, {"figure_path": "vBGMbFgvsX/figures/figures_34_1.jpg", "caption": "Figure 1: (a) The vertical axis represents the interquartile mean (IQM) [26] of normalized task return across 29 tasks. HEPO outperforms the policy trained solely with a heavily engineered heuristic reward function (H-only) and other methods, demonstrating that HEPO makes better use of heuristic rewards for learning. (b) The horizontal axis shows the probability that algorithm X outperforms the policy trained solely with heuristic rewards (H-only). HEPO achieves a 62% probability of improvement over the heuristic policy on average, with the lower bound of the confidence interval above 50%, indicating statistically significant improvements over the heuristic policy.", "description": "This figure displays a comparison of different reinforcement learning algorithms in terms of their performance on 29 tasks.  The interquartile mean (IQM) of normalized task returns shows that HEPO significantly outperforms other methods, particularly the H-only baseline.  Further, the probability of improvement metric highlights HEPO's ability to surpass heuristic-only policies with a statistically significant probability (greater than 50%).", "section": "4.1 Benchmark results"}, {"figure_path": "vBGMbFgvsX/figures/figures_36_1.jpg", "caption": "Figure 1: (a) The vertical axis represents the interquartile mean (IQM) [26] of normalized task return across 29 tasks. HEPO outperforms the policy trained solely with a heavily engineered heuristic reward function (H-only) and other methods, demonstrating that HEPO makes better use of heuristic rewards for learning. (b) The horizontal axis shows the probability that algorithm X outperforms the policy trained solely with heuristic rewards (H-only). HEPO achieves a 62% probability of improvement over the heuristic policy on average, with the lower bound of the confidence interval above 50%, indicating statistically significant improvements over the heuristic policy.", "description": "This figure presents a comparison of different reinforcement learning algorithms on 29 robotic tasks.  The interquartile mean (IQM) of normalized task return shows that HEPO significantly outperforms other algorithms, including one trained only with heuristic rewards.  Additionally, the probability of HEPO's performance exceeding that of the heuristic-only algorithm is 62%, with a lower confidence bound above 50%, indicating statistical significance.", "section": "4.1 Benchmark results"}, {"figure_path": "vBGMbFgvsX/figures/figures_37_1.jpg", "caption": "Figure 1: (a) The vertical axis represents the interquartile mean (IQM) [26] of normalized task return across 29 tasks. HEPO outperforms the policy trained solely with a heavily engineered heuristic reward function (H-only) and other methods, demonstrating that HEPO makes better use of heuristic rewards for learning. (b) The horizontal axis shows the probability that algorithm X outperforms the policy trained solely with heuristic rewards (H-only). HEPO achieves a 62% probability of improvement over the heuristic policy on average, with the lower bound of the confidence interval above 50%, indicating statistically significant improvements over the heuristic policy.", "description": "This figure presents the results comparing HEPO with several baseline methods.  Subfigure (a) shows the interquartile mean (IQM) of normalized task return for each method. HEPO consistently achieves higher returns compared to baselines (especially H-only, which uses only heuristic rewards). Subfigure (b) displays the probability of improvement; HEPO significantly outperforms H-only in most of the tasks. This indicates that HEPO's constraint for policy improvement is effective and robust even with finite data and potentially poor heuristic rewards.", "section": "4.1 Benchmark results"}, {"figure_path": "vBGMbFgvsX/figures/figures_38_1.jpg", "caption": "Figure 1: (a) The vertical axis represents the interquartile mean (IQM) [26] of normalized task return across 29 tasks. HEPO outperforms the policy trained solely with a heavily engineered heuristic reward function (H-only) and other methods, demonstrating that HEPO makes better use of heuristic rewards for learning. (b) The horizontal axis shows the probability that algorithm X outperforms the policy trained solely with heuristic rewards (H-only). HEPO achieves a 62% probability of improvement over the heuristic policy on average, with the lower bound of the confidence interval above 50%, indicating statistically significant improvements over the heuristic policy.", "description": "This figure displays a comparison of the performance of different reinforcement learning algorithms, including HEPO, on 29 tasks.  Panel (a) uses interquartile mean (IQM) of normalized task return to show HEPO's superior performance compared to baselines (H-only, J-only, J+H, HuRL, PBRS, EIPO). Panel (b) shows the probability that each algorithm outperforms the H-only baseline, demonstrating that HEPO significantly outperforms the heuristic policy.", "section": "4.1 Benchmark results"}, {"figure_path": "vBGMbFgvsX/figures/figures_39_1.jpg", "caption": "Figure 1: (a) The vertical axis represents the interquartile mean (IQM) [26] of normalized task return across 29 tasks. HEPO outperforms the policy trained solely with a heavily engineered heuristic reward function (H-only) and other methods, demonstrating that HEPO makes better use of heuristic rewards for learning. (b) The horizontal axis shows the probability that algorithm X outperforms the policy trained solely with heuristic rewards (H-only). HEPO achieves a 62% probability of improvement over the heuristic policy on average, with the lower bound of the confidence interval above 50%, indicating statistically significant improvements over the heuristic policy.", "description": "This figure presents a comparison of different reinforcement learning algorithms on 29 tasks using two metrics: interquartile mean (IQM) of normalized return and probability of improvement over the heuristic-only policy. The results show that HEPO consistently outperforms other methods, demonstrating its effectiveness in leveraging heuristic rewards to enhance task performance.", "section": "4.1 Benchmark results"}, {"figure_path": "vBGMbFgvsX/figures/figures_40_1.jpg", "caption": "Figure 1: (a) The vertical axis represents the interquartile mean (IQM) [26] of normalized task return across 29 tasks. HEPO outperforms the policy trained solely with a heavily engineered heuristic reward function (H-only) and other methods, demonstrating that HEPO makes better use of heuristic rewards for learning. (b) The horizontal axis shows the probability that algorithm X outperforms the policy trained solely with heuristic rewards (H-only). HEPO achieves a 62% probability of improvement over the heuristic policy on average, with the lower bound of the confidence interval above 50%, indicating statistically significant improvements over the heuristic policy.", "description": "This figure displays a comparison of different reinforcement learning algorithms' performance on 29 robotic tasks.  Part (a) shows the interquartile mean (IQM) of normalized task return, indicating HEPO's superior performance compared to other methods, including those trained with only heuristic rewards or a mixture of task and heuristic rewards.  Part (b) presents the probability of each algorithm outperforming the heuristic-only policy, highlighting HEPO's statistically significant (over 50%) improvement.", "section": "4.1 Benchmark results"}, {"figure_path": "vBGMbFgvsX/figures/figures_42_1.jpg", "caption": "Figure 1: (a) The vertical axis represents the interquartile mean (IQM) [26] of normalized task return across 29 tasks. HEPO outperforms the policy trained solely with a heavily engineered heuristic reward function (H-only) and other methods, demonstrating that HEPO makes better use of heuristic rewards for learning. (b) The horizontal axis shows the probability that algorithm X outperforms the policy trained solely with heuristic rewards (H-only). HEPO achieves a 62% probability of improvement over the heuristic policy on average, with the lower bound of the confidence interval above 50%, indicating statistically significant improvements over the heuristic policy.", "description": "This figure shows the results of experiments comparing HEPO against several baselines across 29 tasks.  Part (a) displays the interquartile mean (IQM) of normalized task return, showing HEPO significantly outperforms other methods, particularly those using only heuristic rewards. Part (b) illustrates the probability of improvement, indicating that HEPO outperforms the heuristic-only policy in over 60% of the tasks, a statistically significant result.", "section": "4.1 Benchmark results"}, {"figure_path": "vBGMbFgvsX/figures/figures_44_1.jpg", "caption": "Figure 1: (a) The vertical axis represents the interquartile mean (IQM) [26] of normalized task return across 29 tasks. HEPO outperforms the policy trained solely with a heavily engineered heuristic reward function (H-only) and other methods, demonstrating that HEPO makes better use of heuristic rewards for learning. (b) The horizontal axis shows the probability that algorithm X outperforms the policy trained solely with heuristic rewards (H-only). HEPO achieves a 62% probability of improvement over the heuristic policy on average, with the lower bound of the confidence interval above 50%, indicating statistically significant improvements over the heuristic policy.", "description": "This figure shows the comparison of HEPO with other methods in terms of the Interquartile Mean (IQM) of normalized task return and the probability of improvement over the heuristic policy on 29 tasks. HEPO significantly outperforms other methods in both metrics, indicating its effectiveness in utilizing heuristic rewards for improved task performance.", "section": "4.1 Benchmark results"}, {"figure_path": "vBGMbFgvsX/figures/figures_47_1.jpg", "caption": "Figure 1: (a) The vertical axis represents the interquartile mean (IQM) [26] of normalized task return across 29 tasks. HEPO outperforms the policy trained solely with a heavily engineered heuristic reward function (H-only) and other methods, demonstrating that HEPO makes better use of heuristic rewards for learning. (b) The horizontal axis shows the probability that algorithm X outperforms the policy trained solely with heuristic rewards (H-only). HEPO achieves a 62% probability of improvement over the heuristic policy on average, with the lower bound of the confidence interval above 50%, indicating statistically significant improvements over the heuristic policy.", "description": "This figure displays a comparison of different reinforcement learning algorithms on 29 tasks.  The first subfigure (a) shows the Interquartile Mean (IQM) of the normalized task return for each algorithm; HEPO significantly outperforms all other methods.  The second subfigure (b) shows the probability that each algorithm outperforms the heuristic-only policy; HEPO shows a statistically significant improvement (above 50%).", "section": "4.1 Benchmark results"}, {"figure_path": "vBGMbFgvsX/figures/figures_48_1.jpg", "caption": "Figure 1: (a) The vertical axis represents the interquartile mean (IQM) [26] of normalized task return across 29 tasks. HEPO outperforms the policy trained solely with a heavily engineered heuristic reward function (H-only) and other methods, demonstrating that HEPO makes better use of heuristic rewards for learning. (b) The horizontal axis shows the probability that algorithm X outperforms the policy trained solely with heuristic rewards (H-only). HEPO achieves a 62% probability of improvement over the heuristic policy on average, with the lower bound of the confidence interval above 50%, indicating statistically significant improvements over the heuristic policy.", "description": "This figure presents a comparison of different reinforcement learning algorithms on 29 robotic tasks.  The interquartile mean (IQM) of normalized task returns shows that HEPO significantly outperforms other methods, including those using only heuristic rewards or a mixture of task and heuristic rewards.  The probability of improvement plot further demonstrates that HEPO has a statistically significant advantage over the heuristic-only policy. This indicates HEPO's effectiveness in utilizing heuristic rewards to improve task performance.", "section": "4.1 Benchmark results"}]