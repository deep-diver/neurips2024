{"references": [{"fullname_first_author": "Richard S Sutton", "paper_title": "Reinforcement learning: An introduction", "publication_date": "2018-00-00", "reason": "This is a foundational textbook in reinforcement learning, providing the theoretical basis for many concepts used in the paper."}, {"fullname_first_author": "Andrew Y Ng", "paper_title": "Policy invariance under reward transformations: Theory and application to reward shaping", "publication_date": "1999-00-00", "reason": "This paper introduces the concept of optimal policy invariance, a key idea that the current work builds upon and challenges."}, {"fullname_first_author": "John Schulman", "paper_title": "Proximal policy optimization algorithms", "publication_date": "2017-00-00", "reason": "The algorithm used in the experiments, Proximal Policy Optimization (PPO), is described in this paper, a widely used RL algorithm."}, {"fullname_first_author": "John Schulman", "paper_title": "Trust region policy optimization", "publication_date": "2015-00-00", "reason": "This paper introduces Trust Region Policy Optimization (TRPO), a predecessor to PPO, also used for comparison."}, {"fullname_first_author": "Eric Chen", "paper_title": "Redeeming intrinsic rewards via constrained optimization", "publication_date": "2022-00-00", "reason": "This closely related work, Extrinsic-Intrinsic Policy Optimization (EIPO), is compared with the proposed method, HEPO, showing their similarities and differences."}]}