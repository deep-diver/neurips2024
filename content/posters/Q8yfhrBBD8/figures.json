[{"figure_path": "Q8yfhrBBD8/figures/figures_1_1.jpg", "caption": "Figure 1: Overview of Bridge-IF. Bridge-IF consists of an expressive structure encoder supervised by native sequences for proposing a discrete, deterministic prior, and a Markov bridge model for learning the dependency between the distribution of prior sequences and the distribution of native sequences. During the inference stage, Bridge-IF progressively refines the prior sequence.", "description": "The figure illustrates the Bridge-IF model, which uses an expressive structure encoder to create a discrete prior sequence from a given protein structure.  A Markov bridge model then learns the probabilistic relationship between this prior and the native sequences. During inference, the model iteratively refines the prior sequence, eventually generating a plausible protein sequence that folds into the desired structure. The diagram visually shows this process, starting with a structure (s), encoding it to a prior sequence (x), and then using a Markov bridge process (with intermediate steps represented by Zt) to arrive at a final native-like sequence (y).", "section": "Methods"}, {"figure_path": "Q8yfhrBBD8/figures/figures_5_1.jpg", "caption": "Figure 2: Model architecture of Bridge-IF.", "description": "The figure illustrates the architecture of Bridge-IF, a model for inverse protein folding. It shows how a structure encoder processes structural information, and how this information, along with timestep embeddings, is used to modulate a Transformer block. This block then generates a protein sequence by progressively refining an initial sequence. The figure highlights which parts of the model are trainable and which parts are frozen, emphasizing the use of pre-trained weights to ensure efficient training.", "section": "4.3 Network architecture design space"}, {"figure_path": "Q8yfhrBBD8/figures/figures_7_1.jpg", "caption": "Figure 3: Performance comparison w.r.t. model scales of PLMs using ESM-2 series on CATH 4.3.", "description": "This figure shows the performance comparison of Bridge-IF and LM-Design models with varying scales of ESM-2 PLMs on the CATH 4.3 dataset.  The x-axis represents the total number of model parameters (in millions), while the y-axis shows the recovery rate (%).  The graph illustrates how the performance of both models improves with larger model sizes, demonstrating a scaling law in logarithmic scale. Bridge-IF consistently outperforms LM-Design across all model sizes.  The figure highlights the scaling behavior of the two models, indicating that increasing model size improves performance in inverse protein folding.", "section": "5.2 Inverse folding"}, {"figure_path": "Q8yfhrBBD8/figures/figures_8_1.jpg", "caption": "Figure 4: Folding comparison of our designed sequences (in blue) and the native sequences (in nude).", "description": "This figure presents a visual comparison of protein structures. It showcases three examples where the predicted protein structures generated by the Bridge-IF model are compared with their corresponding native structures.  Each example includes the protein ID (PDB ID), the recovery rate, and the TM-score (TM-score measures structural similarity). The predicted structures are shown in light blue, while the native structures are in beige. The visual comparison allows assessing the accuracy and quality of the protein structures generated by the Bridge-IF model.", "section": "5.3 Foldability"}]