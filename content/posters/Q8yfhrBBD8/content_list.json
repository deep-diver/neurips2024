[{"type": "text", "text": "Bridge-IF: Learning Inverse Protein Folding with Markov Bridges ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yiheng ${\\mathbf{Z}}{\\mathbf{h}}{\\mathbf{u}}^{1}$ , Jialu $\\mathbf{W}\\mathbf{u}^{2}$ , Qiuyi $\\mathbf{Li}^{3}$ , Jiahuan $\\mathbf{Yan}^{1}$ , Mingze $\\mathbf{Yin^{4}}$ , Wei $\\mathbf{W}\\mathbf{u}^{5}$ , Mingyang $\\mathbf{Li}^{3}$ , Jieping $\\mathbf{Y}\\mathbf{e}^{3}$ , Zheng Wang3,\u2217 Jian $\\mathbf{W}\\mathbf{u}^{1,4,6*}$ ", "page_idx": 0}, {"type": "text", "text": "1College of Computer Science & Technology and Liangzhu Laboratory, Zhejiang University 2College of Pharmaceutical Sciences, Zhejiang University 3Alibaba Cloud Computing 4School of Public Health, Zhejiang University ", "page_idx": 0}, {"type": "text", "text": "5School of Artificial Intelligence and Data Science, University of Science and Technology of China 6The Second Affiliated Hospital Zhejiang University School of Medicine {zhuyiheng2020, jialuwu, jyansir, yinmingze, wujian2000} $@$ zju.edu.cn {liqiuyi.lqy, sangheng.lmy, yejieping.ye, wz388779} $@$ alibaba-inc.com urara@mail.ustc.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Inverse protein folding is a fundamental task in computational protein design, which aims to design protein sequences that fold into the desired backbone structures. While the development of machine learning algorithms for this task has seen significant success, the prevailing approaches, which predominantly employ a discriminative formulation, frequently encounter the error accumulation issue and often fail to capture the extensive variety of plausible sequences. To fill these gaps, we propose Bridge-IF, a generative diffusion bridge model for inverse folding, which is designed to learn the probabilistic dependency between the distributions of backbone structures and protein sequences. Specifically, we harness an expressive structure encoder to propose a discrete, informative prior derived from structures, and establish a Markov bridge to connect this prior with native sequences. During the inference stage, Bridge-IF progressively refines the prior sequence, culminating in a more plausible design. Moreover, we introduce a reparameterization perspective on Markov bridge models, from which we derive a simplified loss function that facilitates more effective training. We also modulate protein language models (PLMs) with structural conditions to precisely approximate the Markov bridge process, thereby significantly enhancing generation performance while maintaining parameter-efficient training. Extensive experiments on well-established benchmarks demonstrate that Bridge-IF predominantly surpasses existing baselines in sequence recovery and excels in the design of plausible proteins with high foldability. The code is available at https://github.com/violet-sto/Bridge-IF. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Proteins are 3D folded linear chains of amino acids that execute the myriad of biological processes fundamental to life, such as catalysing metabolic reactions, mediating immune responses, and responding to stimuli [23]. Designing protein sequences that fold into desired 3D structures, known as inverse protein folding, is a crucial task with great potential for applications in protein engineering [29, 66, 5]. Beyond long-established physics-based methods like Rosetta [2], the considerable promise of leveraging geometric deep learning for protein structure modeling has given rise to an ongoing paradigm. This paradigm is centered on deciphering the principles of protein design directly from data and on predicting sequences corresponding to specific structures [25, 27, 6, 22]. ", "page_idx": 0}, {"type": "image", "img_path": "Q8yfhrBBD8/tmp/81de5a7559b743b894b6f8905a97160ba4cf7e798b85eceb5eec6219e3d6660f.jpg", "img_caption": ["Figure 1: Overview of Bridge-IF. Bridge-IF consists of an expressive structure encoder supervised by native sequences for proposing a discrete, deterministic prior, and a Markov bridge model for learning the dependency between the distribution of prior sequences and the distribution of native sequences. During the inference stage, Bridge-IF progressively refines the prior sequence. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Despite substantial advancements, most existing approaches follow a discriminative formulation for learning inverse folding [58], consequently encountering two principal obstacles: (i) Error accumulation issue. For instance, Transformer-based autoregressive models are constrained by their inherent sequential generation process and exposure bias, which prevents them from correcting preceding erroneous predictions. (ii) One-to-many mapping nature of the inverse folding problem. A multitude of distinct amino acid sequences possess the capability to fold into an identical protein backbone structure, a phenomenon exemplified by homologous proteins. Discriminative models are incapable of capturing the one-to-many mapping from the protein structure to non-unique sequences, thereby facing difficulties in covering the broad spectrum of plausible solutions [58]. ", "page_idx": 1}, {"type": "text", "text": "Recent studies have advanced the iterative refinement strategy to optimize the previously generated results, aiming to reduce prediction errors [64, 14, 42]. These approaches employ a refinement module to identify and correct inaccurately predicted amino acids. However, as the number of refinement iterations grows, managing the intermediate stages effectively becomes more challenging, potentially hindering sustained performance gains. ", "page_idx": 1}, {"type": "text", "text": "Diffusion-based generative models [45, 17], particularly their discrete extensions [3], which offer a structured iterative refinement process with probabilistic interpretation, appear to be a promising solution. GraDe-IF [58] is a pioneer in investigating diffusion models for inverse folding, leveraging the backbone structure to guide the denoising process on the amino acid residues. However, as diffusion models are designed to learn a single intractable data distribution, the prior distribution utilized by GraDe-IF is restricted to a simple noise distribution (i.e., a uniform distribution across all residue types), which has little or no information about the distribution of native sequences. It remains unclear whether this default formulation best suits conditional generative problems such as inverse protein folding, where the backbone structures provide significantly more information than random noise. Thus, an exciting research question naturally arises: Can we propose a more strong and informative prior based on desired backbone structures to enhance the quality of samples and accelerate the inference process? ", "page_idx": 1}, {"type": "text", "text": "In this work, we propose Bridge-IF, a novel generative diffusion bridge model for inverse folding. Its core design is aimed at generating protein sequences from a structure-aware prior. As shown in Figure 1, we leverage an expressive structure encoder supervised by native sequences to propose a discrete, deterministic prior based on desired structures, and build a Markov bridge [10, 24] between it and the native sequence. By approximating the reference Markov bridge process, Bridge-IF learns to progressively refine the prior sequence, resulting in a more plausible design. Furthermore, we present a fresh reparameterization perspective on Markov bridge models and derive a simplified loss function that yields enhanced training effectiveness. Inspired by significant advances in protein language models (PLMs) for understanding proteins [9, 34], we innovatively integrate conditions, including timestep and structures, into PLMs to accurately approximate the Markov bridge process. This approach notably improves generation performance while ensuring parameter-efficient training. Empirically, we demonstrate that Bridge-IF outperforms state-of-the-art baselines on several standard benchmarks and excels in the design of plausible proteins with high foldability. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "To summarise, the main contributions of this work are as follows: ", "page_idx": 2}, {"type": "text", "text": "\u2022 We introduce Bridge-IF, the first generative diffusion bridge model based on Markov bridges for inverse folding. We also offer a reparameterization perspective and derive a simplified loss function to facilitate effective training. \u2022 We innovatively adapt PLMs to effectively capture both timestep and structural information while ensuring the modified architecture is compatible with pre-trained weights. \u2022 Experiments verify that Bridge-IF achieves state-of-the-art performance on standard benchmarks. ", "page_idx": 2}, {"type": "text", "text": "2 Related work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 Inverse protein folding ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Recently, AI algorithms have spurred a revolution in modeling protein folding [28, 34]. Meanwhile, the inverse problem of protein folding, which aims to infer an amino acid sequence that will fold into the desired structure, is gaining increasing attention [6]. By representing protein backbone structures as a $k$ -NN graph, geometric deep learning has achieved remarkable progress in learning inverse folding [25, 6, 22], surpassing traditional physics-based approaches [2], and even facilitating the design of a range of experimentally validated proteins [6, 56]. Modern deep learning-based inverse folding approaches typically comprise a structure encoder and a sequence decoder. Depending on their decoding strategies, these approaches can be classified into three categories: autoregressive models, one-shot models, and iterative models. Most methods adopt the autoregressive decoding scheme to generate amino acid sequences [25, 6, 22]. Given that autoregressive models tend to have low inference speed, some researchers have investigated one-shot methods that facilitate the parallel generation of multiple tokens [12, 38]. Since directly predicting highly plausible sequences is challenging, some works have shifted their attention to iterative refinement [64, 14, 26, 42, 58]. For instance, LM-Design [64] and KW-design [14] utilize the pre-trained knowledge from PLMs to reconstruct a native sequence from a corrupted version. The Potts model-based ChromaDesign [26] and CarbonDesign [42] employ iterative sampling techniques, including Markov chain Monte Carlo, to design protein sequences. GraDe-IF [58] further leverages the principles of discrete denoising diffusion probabilistic models [3], demonstrating a strong capacity to encompass diverse plausible solutions. In this work, we present the first generative diffusion bridge model for inverse folding. ", "page_idx": 2}, {"type": "text", "text": "2.2 Diffusion models ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Diffusion-based generative models [45, 17] have showcased remarkable successes in a wide range of applications, ranging from image synthesis [8], audio synthesis [31], to video generation [18]. Generally, the essential idea behind these models is to define a forward diffusion process that gradually transforms the data into a simple prior distribution and learn a reverse denoising process to gradually recover original data samples from the prior distribution. While most existing methods are designed for modeling continuous data, a few efforts have extended diffusion models to discrete data domains [3, 33, 52, 36]. Recently, diffusion models have also found utility in scientific discovery [55], particularly in protein design [56, 59, 1, 15, 58]. ", "page_idx": 2}, {"type": "text", "text": "2.3 Schr\u00f6dinger bridge problem ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The Schr\u00f6dinger bridge (SB) problem is a classical entropy-regularized optimal transport problem [43, 32, 4]. Given a data distribution, a prior distribution, and a reference stochastic process between them, solving the SB problem amounts to finding the closest process to the reference in terms of Kullback-Leibler divergence on path spaces. This concept exhibits fundamental similarities to diffusion models [47], particularly in the field of unconditional generative modeling [49, 54, 7, 44], where the prior distribution assumes the form of Gaussian noise. Notably, SB formalism offers a general framework for approximating the reference stochastic process by training on coupled samples from two continuous distributions [19, 46, 35, 65]. The recently proposed Markov bridge [10, 24] has broadened the scope of the SB, enabling it to model categorical distributions. In this work, we present the first diffusion bridge model for inverse protein folding. ", "page_idx": 3}, {"type": "text", "text": "3 Background ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "3.1 Problem formulation and notation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Generally, a protein can be represented as a pair of amino acid sequence and structure $(y,s)$ , where $\\pmb{y}=[y_{1},\\dot{y}_{2},\\dot{.}\\dot{.}\\dot{.}\\dot{,}y_{n}]$ denotes its sequence of $n$ residues with $y_{i}\\in\\bar{\\{1,2,\\ldots,20\\}}$ indicating the type of the $i$ -th residue, and $\\pmb{s}=[s_{1},s_{2},\\bar{.}...,s_{n}]\\in\\mathbb{R}^{n\\times4\\times3}$ denotes its structure with $s_{i}$ representing the Cartesian coordinates of the $i$ -th residue\u2019s backbone atoms (i.e., N, $\\mathbf{C}{\\mathrm{-}}\\alpha$ , and $\\mathbf{C}$ , with $\\mathrm{o}$ optionally). The inverse protein folding problem aims to automatically identify the protein sequence $\\textit{\\textbf{y}}$ that can fold into the given structure $\\pmb{s}$ . Given that homologous proteins invariably exhibit similar structures, the solution for a given structure is not unique [16]. Hence, an ideal model, parameterized by $\\theta$ , should be capable of learning the underlying mapping from protein backbone structures to their corresponding sequence distributions $p_{\\theta}(\\pmb{y}\\vert\\pmb{s})$ . ", "page_idx": 3}, {"type": "text", "text": "3.2 Markov bridge models ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Markov bridge model [24] is a general framework for learning the probabilistic dependency between two intractable discrete-valued distributions $p_{\\mathcal{X}}$ and $p_{\\mathcal{Y}}$ . For a pair of samples $\\bar{(x,y)}\\sim p\\dot{x},y(x,y)$ , it defines a Markov process pinned to fixed start and end points $z_{\\mathrm{0}}\\,=\\,x$ and $z_{T}=\\pmb{y}$ through a sequence of random variables $(z_{t})_{t=0}^{T}$ that satisfies the Markov property, ", "page_idx": 3}, {"type": "equation", "text": "$$\np(z_{t}|z_{0},z_{1},\\ldots,z_{t-1},\\pmb{y})=p(z_{t}|z_{t-1},\\pmb{y}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "To pin the process at the end point $z_{T}=y$ , we have an additional requirement, ", "page_idx": 3}, {"type": "equation", "text": "$$\np(z_{T}=y|z_{T-1},y)=1.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Assuming that both $p_{\\mathcal{X}}$ and $p_{\\mathcal{Y}}$ are categorical distributions with a finite sample space $\\{1,\\ldots,K\\}$ , we can represent data points as one-hot vectors: $\\mathbf{\\boldsymbol{x}},\\mathbf{\\boldsymbol{y}},z_{t}\\in\\{0,1\\}^{K}$ , and define the transition probabilities (Equation 1) as follows, ", "page_idx": 3}, {"type": "equation", "text": "$$\np(z_{t+1}|z_{t},\\boldsymbol{y})=\\mathrm{Cat}\\left(z_{t+1};Q_{t}z_{t}\\right),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\operatorname{Cat}(\\cdot\\,;p)$ is a categorical distribution with probabilities given by $\\pmb{p}$ , and $Q_{t}$ is a transition matrix parameterized as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{{\\pmb Q}_{t}:={\\pmb Q}_{t}({\\pmb y})=\\beta_{t}{\\pmb I}_{K}+(1-\\beta_{t}){\\pmb y}{\\pmb1}_{K}^{\\top},}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\beta_{t}$ is a schedule parameter transitioning from $\\beta_{0}=1$ to $\\beta_{T-1}=0$ . It is easy to see that $\\mathscr{z}_{t}$ can be efficiently sampled from $p(z_{t+1}|z_{0},z_{T})=\\mathrm{Cat}\\left(z_{t+1};\\overline{{Q}}_{t}z_{0}\\right)$ with a cumulative product matrix $\\overline{{\\pmb{Q}}}_{t}=Q_{t}\\pmb{Q}_{t-1}...\\pmb{Q}_{0}=\\overline{{\\beta}}_{t}\\pmb{I}_{K}+(1-\\overline{{\\beta}}_{t})\\pmb{y}\\pmb{1}_{K}^{\\top}$ , where $\\begin{array}{r}{\\overline{{\\beta}}_{t}=\\prod_{s=0}^{t}\\beta_{s}}\\end{array}$ . ", "page_idx": 3}, {"type": "text", "text": "Training Using the finite set of coupled samples $\\{(\\pmb{x}_{i},\\pmb{y}_{i})\\}_{i=1}^{D}\\sim p_{\\mathcal{X},\\mathcal{Y}}$ , Markov bridge model learns to sample $\\textit{\\textbf{y}}$ when only $\\textbf{\\em x}$ is available by approximating $\\textit{\\textbf{y}}$ with a neural network $\\varphi_{\\theta}$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\hat{\\pmb y}=\\varphi_{\\theta}(\\pmb z_{t},t),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "and defining an approximated transition kernel, ", "page_idx": 3}, {"type": "equation", "text": "$$\nq_{\\theta}(z_{t+1}|z_{t})=\\mathrm{Cat}\\left(z_{t+1};Q_{t}(\\hat{\\pmb{y}})z_{t}\\right).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "$\\varphi_{\\theta}$ is trained by optimizing the variational bound on negative log-likelihood $\\log q_{\\theta}(\\pmb{y}|\\pmb{x})$ , which has the following closed-form expression, ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{-\\log q_{\\theta}({\\pmb y}|{\\pmb x})\\leq T\\cdot\\mathbb{E}_{t\\sim\\mathcal{U}(0,\\dots,T-1)}\\underbrace{\\mathbb{E}_{{\\pmb z}_{t}\\sim p({\\pmb z}_{t}|{\\pmb x},{\\pmb y})}D_{\\mathrm{KL}}\\left(p({\\pmb z}_{t+1}|{\\pmb z}_{t},{\\pmb y})\\|q_{\\theta}({\\pmb z}_{t+1}|{\\pmb z}_{t})\\right)}_{{\\mathcal{L}}_{t}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Sampling To sample a data point ${\\textbf{\\em y}}\\equiv\\ z_{T}$ starting from a given $z_{0}\\;\\equiv\\;{\\pmb x}\\;\\sim\\;p_{\\mathcal{X}}({\\pmb x})$ , one can iteratively predict $\\pmb{\\hat{y}}=\\varphi_{\\theta}(\\pmb{z}_{t},\\pmb{\\bar{t}})$ and then derive $z_{t+1}\\sim q_{\\theta}(z_{t+1}|z_{t})=\\mathrm{Cat}\\left(z_{t+1};\\dot{Q}_{t}(\\hat{\\pmb{y}})z_{t}\\right)$ for $t=0,\\dots,T-1$ . ", "page_idx": 4}, {"type": "text", "text": "4 Methods ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we introduce Bridge-IF, a Markov bridge-based model for inverse protein folding. Figure 1 shows an overview of our proposed Bridge-IF. Due to space limitation, we present the detailed algorithm in Appendix A. To begin, we describe how to extend Markov bridge techniques to facilitate the inverse protein folding task. Next, we propose a simplified training objective. Finally, we elucidate how to modulate pre-trained PLMs with structural conditions to approximate the Markov bridge process. ", "page_idx": 4}, {"type": "text", "text": "4.1 Overview of Bridge-IF ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We frame the inverse protein folding problem as a generative problem of modeling a stochastic process between the distributions of backbone structures $p_{\\cal{S}}(s)$ and protein sequences $p_{\\mathcal{Y}}(y)$ . As previously discussed, diffusion bridge models, with their general properties of an unrestricted prior form, serves as an ideal substitution for diffusion models in the presence of a well-defined informative prior. Regrettably, to the best of our knowledge, no existing method can directly model the dependency between two distinct types of distributions: specifically, the continuous source distribution of backbone structures and the discrete target distribution of protein sequences. ", "page_idx": 4}, {"type": "text", "text": "To reconcile the differences between source and target distributions and streamline the modeling process, we propose introducing a discrete proposal distribution to serve as a deterministic prior. We parameterize the proposal distribution using a structure encoder $\\mathcal{E}:\\mathcal{S}\\rightarrow\\mathcal{X}$ that is supervised by ground-truth target sequences. Recent advancements have demonstrated that an expressive encoder is capable of directly predicting pretty good protein sequences in a one-shot manner [12]. This approach enables us to utilize structural information more effectively, rather than simply employing it to guide the denoising process as in previous diffusion-based methods like GraDe-IF [58]. In this work, we will take the discriminative model PiFold [12] as the structure encoder to produce a clean and deterministic prior $\\pmb{x}=\\mathcal{E}(\\pmb{s})$ . Upon this deterministic mapping from structure to sequence, we simplify the originally complex problem of modeling $p(s,y)$ into the more tractable problem of modeling $p(x,y)$ . Then, we build a Markov bridge [10, 24] between the prior sequence and the native sequence to model the stochastic process, leading to a data-to-data process. As depicted in the lower half of Figure 1, each sampling step progressively refines the prior sequence, which contains significant information about the target sequence, ultimately resulting in a more precise prediction. ", "page_idx": 4}, {"type": "text", "text": "Recall that the Markov bridge models are typically trained by optimizing the variational bound on negative log-likelihood $\\operatorname{og}q_{\\theta}(\\pmb{y}|\\pmb{x})$ (Equation 7), which is analytically complicated and hard to optimize in practice [63, 62]. Therefore, we here propose a reparameterization perspective on Markov bridge models, deriving a simplified loss function for easier optimization (\u00a74.2). ", "page_idx": 4}, {"type": "text", "text": "We build Markov bridges in the sequence space, treating the sequence representation as a set of independent categorical random variables. To model the Markov bridge process, $Q_{t}$ is applied separately to each residue within a protein sequence. Motivated by the impressive advancements in PLMs for understanding and generating proteins [9, 34, 37, 60], we advocate for employing PLMs to approximate the Markov bridge process. This approach capitalizes on the emergent evolutionary knowledge of proteins, learned from an extensive dataset of protein sequences. Additionally, we utilize the latent structural features extracted by the structure encoder to prompt PLMs, thereby guiding the generation of structurally coherent proteins. Formally, the final state of the Markov bridge process is approximated by $\\hat{\\pmb{y}}=\\varphi_{\\theta}(\\pmb{z}_{t},\\pmb{s},t)$ , foregoing the use of Equation 5. We investigate the integration of conditional information, such as timestep and structure, into PLMs, focusing on preserving their emergent knowledge and achieving parameter-efficient training (\u00a74.3). ", "page_idx": 4}, {"type": "text", "text": "4.2 Reparameterized Markov bridge models ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Inspired by the similarities between Markov bridge models [24] and discrete diffusion models [3, 63], we propose a reparameterization of the Markov bridge model characterized in $\\S3.2$ to enable more effective training. With the reparameterization trick, we introduce a latent binary random variable ", "page_idx": 4}, {"type": "text", "text": "$v_{t}\\sim\\mathrm{Bernoulli}(\\overline{{\\beta}}_{t-1})$ to indicate whether $\\scriptstyle z_{t}$ has been transformed from $\\relax z_{\\mathrm{0}}$ to $z_{T}$ . Thus $\\scriptstyle z_{t}$ can be sampled from $p(\\tilde{z}_{t}|\\tilde{v_{t}},z_{0},\\pmb{y})=v_{t}z_{0}+(1-v_{t})\\pmb{y}$ . Accordingly, $p(z_{t+1}|z_{t},\\boldsymbol{y})$ can be equivalently written as: ", "page_idx": 5}, {"type": "equation", "text": "$$\np(z_{t+1}|v_{t},z_{t},\\pmb{y})=\\left\\{{z}_{t}\\begin{array}{l l}{\\mathrm{~if~}v_{t}=0}\\\\ {(1-\\beta_{t})\\pmb{y}+\\beta_{t}\\pmb{z}_{t}}&{\\mathrm{~if~}v_{t}=1}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Using the teacher-forcing approach, we can similarly define the approximation process as ", "page_idx": 5}, {"type": "equation", "text": "$$\nq_{\\theta}(z_{t+1}|v_{t},z_{t})=\\left\\{\\!\\!\\begin{array}{l l}{z_{t}}&{\\mathrm{if~}v_{t}=0}\\\\ {(1-\\beta_{t})\\varphi_{\\theta}(z_{t},t)+\\beta_{t}z_{t}}&{\\mathrm{if~}v_{t}=1}\\end{array}\\!\\!\\right.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Proposition 4.1. The loss objective $\\textstyle{\\mathcal{L}}_{t}(\\theta)$ for sequence $x$ at the $t$ -th step can be reduced to the form ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{t}(\\theta)=\\lambda_{t}\\mathbb{E}_{p(z_{t}|x,y)}[-v_{t}y^{T}\\log\\varphi_{\\theta}(z_{t},t)],\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\lambda_{t}=1-\\beta_{t}$ . ", "page_idx": 5}, {"type": "text", "text": "The full derivation is provided in C. This derived expression of $\\mathcal{L}_{t}(\\theta)$ formulates the training loss as a re-weighted standard multi-class cross-entropy loss function, which is computed over tokens that have not been transformed to the ground truth ${\\pmb y}=z_{T}$ . Following Ho et al. [17], we set $\\lambda_{t}$ to a constant 1 in practice. Compared to the simpler cross-entropy loss calculated across all tokens, this new formulation places greater weight on tokens that require refinement. On the other hand, it is conceptually simpler than the original training loss (Equation 7), which requires calculating the complicated KL divergence between two categorical distributions $D_{\\mathrm{KL}}[p(z_{t+1}\\bar{|}z_{t},\\pmb{y})\\|q_{\\theta}(z_{t+1}\\bar{|}z_{t})]$ . ", "page_idx": 5}, {"type": "text", "text": "4.3 Network architecture design space ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We adopt pre-trained PLMs as the base network to approximate the final state of the Markov bridge process. Typically, PLMs exclusively take protein sequences as input during the pre-training stage, making it non-trivial to integrate timestep and structural conditions into the PLMs. Hence, we innovatively tailor the Transformer blocks [50] to effectively capture timestep and structural information, as depicted in Figure 2. To facilitate efficient training, the architecture of our model is delicately designed for compatibility with the pre-trained weights. Our exposition emphasizes fundamental principles and the corresponding modifications to the base network. ", "page_idx": 5}, {"type": "text", "text": "4.3.1 AdaLN-Bias ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Inspired by DiT [41], we explore replacing standard layer norm layers in transformer blocks with adaptive layer norm (adaLN) to modulate the normalization\u2019s output based on both the timestep of the Markov bridge process and the backbone structure. The key idea is to regress the dimension-wise scale and shift parameters $\\gamma$ and $\\beta$ of the layer norm from the sum of the timestep embedding and the pooled structure representation. In our situation, meaningful pretrained parameters $\\gamma$ and $\\beta$ are readily accessible. Upon commencing the fine-tuning stage, it is crucial that these parameters are close to the pre-trained values to preserve the effectiveness of the original model, since a poor initialization could significantly deteriorate performance. For simplicity, we propose to predict bias $\\Delta\\gamma$ and $\\Delta\\beta$ on the frozen original scalars and initialize the multi-layer perception (MLP) to output the zero-vector for all $\\Delta\\gamma$ and $\\Delta\\beta$ . We term the proposed variant of adaLN as adaLN-Bias. ", "page_idx": 5}, {"type": "image", "img_path": "Q8yfhrBBD8/tmp/d580fe5dd9995b50bcc092e5e2012d43235425184a858b7b46f25852039eb739.jpg", "img_caption": ["Figure 2: Model architecture of Bridge-IF. "], "img_footnote": [], "page_idx": 5}, {"type": "table", "img_path": "Q8yfhrBBD8/tmp/6b518f78a4aad174d33d7a0d9cbd5e674f5bb7afba269ac7e03ddff14873a409.jpg", "table_caption": ["Table 1: Results comparison on the CATH dataset. Benchmarked results are quoted from Hsu et al. [22], Zheng et al. [64], Yi et al. [58], Gao et al. [14]. $\\dagger$ : \u201cSingle-chain\u201d in Hsu et al. [22] is defined differently. The best and suboptimal results are labeled with bold and underline. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "4.3.2 Structural adapter ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Considering that the pooled structure representation might only retain coarse-grained information, the network could consequently lack a detailed understanding of the structure input and necessitate information derived from original structural features to compensate. We incorporate a multi-head cross-attention module to the transformer block, enabling the network to flexibly interact with the structural features extracted from the structure encoder [64]. To facilitate pre-trained weights, we further integrate it into a bottleneck adapter layer [21] with residual connection, preserving the input for the subsequent layers. ", "page_idx": 6}, {"type": "text", "text": "We stress that we freeze all pre-trained parameters of the base network during training. ", "page_idx": 6}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we first demonstrate the effectiveness of our Bridge-IF on the standard CATH benchmark [40]. Next, we assess Bridge-IF for its applicability in de novo protein design. Moreover, we conduct several ablation studies to empirically justify the key design choices. Further results pertaining to the design of multi-chain protein complexes can be found in Appendix B.1. ", "page_idx": 6}, {"type": "text", "text": "5.1 Experimental protocol ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Training setup We conduct experiments on both CATH v4.2 and CATH v4.3, where proteins are categorized based on the CATH hierarchical classification of protein structure, to ensure a comprehensive analysis. Following the standard data splitting provided by Ingraham et al. [25], CATH v4.2 dataset consists of 18,024 proteins for training, 608 proteins for validation, and 1,120 proteins for testing. Following the standard data splitting provided by Hsu et al. [22], CATH v4.3 dataset consists of 16,153 proteins for training, 1,457 proteins for validation, and 1,797 proteins for testing. For a fair comparison with iterative models [64, 14], we use pre-trained PiFold [12] to propose the prior distribution. We use the cosine schedule [39] with number of timestep $T=25$ . The model is trained up to 50 epochs by default on an NVIDIA 3090. We used the same training settings as ProteinMPNN [6], where the batch size was set to approximately 6000 residues, and Adam optimizer [30] with noam learning rate scheduler [51] was used. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Baselines We compare Bridge-IF with several state-of-the-art baselines, categorized into three groups: (1) autoregressive models, including StructGNN [25], GraphTrans [25], GCA [48], GVP [27], AlphaDesign [11], ESM-IF [22], and ProteinMPNN [6]; (2) the one-shot model, PiFold [12]; (3) iterative models, including LM-Design [64], KW-Design [14], and diffusion-based GraDe-IF [58]. ", "page_idx": 7}, {"type": "text", "text": "Evaluation We evaluate the generative quality using perplexity and recovery rate. Following previous studies [25, 22], we report perplexity and median recovery rate on three settings, namely short proteins (length $\\leq100_{\\circ}$ ), single-chain proteins (labeled with 1 chain in CATH), and all proteins. ", "page_idx": 7}, {"type": "text", "text": "5.2 Inverse folding ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "The performance of Bridge-IF, compared to competitive baselines, is summarized in Table 1. BridgeIF demonstrates superior performance over previous methods. We highlight the following: (1) Iterative models comprehensively surpass the previously dominant autoregressive and one-shot methods. (2) Our Bridge-IF outperforms LM-Design and KW-Design with the same pre-trained PLMs, supporting our hypothesis that the iterative refinement process should be modeled in a probabilistic framework. (3) Compared with diffusion-based GraDe-IF, our Bridge-IF achieves better performance with fewer diffusion steps (25 vs. 500), demonstrating that our bridge-based formulation can better leverage the structural prior. ", "page_idx": 7}, {"type": "text", "text": "Following Zheng et al. [64], we also study the impact of the scale of PLMs on CATH v4.3. We use ESM-2 series, with parameters ranging from 8M to 3B. As depicted in Figure 3, the performance of Bridge-IF improves with model scaling, exhibiting a distinct scaling law in logarithmic scale. Using ESM-2 at the same scale, we observe that Bridge-IF consistently obtains greater enhancements relative to LMDesign. Besides, Bridge-IF does not exhibit any performance degradation, even when the smallest model (i.e, ESM-2 8M) is employed. Remarkably, the largest ESM2-3B-based variant of Bridge-IF attains a record-setting recovery rate of $61.27\\%$ on CATH v4.3. ", "page_idx": 7}, {"type": "image", "img_path": "Q8yfhrBBD8/tmp/8e3049b3bf9bdc2e6215aaf2cdd10b9399723c9c6d28ab9e75ab36f318f906a8.jpg", "img_caption": ["Figure 3: Performance comparison w.r.t. model scales of pLMs using ESM-2 series on CATH 4.3. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "5.3 Foldability ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "While perplexity and recovery rate serve as effective proxy metrics, it is imperative to recognize that these measurements may not accurately reflect the foldability of the designed protein sequences in real-world scenarios [58, 13, 53]. Given that wet-lab assessment is extremely costly, we leverage the in silico structure prediction model ESMFold [34], to evaluate whether our designs can adhere to the structure condition. Here we assess the agreement of the native structures with the predicted structures using the TM-score [61], and follow the evaluation configurations as in Wang et al. [53]. Specifically, we use the small, high-quality test set of 82 samples curated by Wang et al. [53] and randomly generate 100 sequences for each structure. ", "page_idx": 7}, {"type": "table", "img_path": "Q8yfhrBBD8/tmp/cc770fb3d0517924424e3682c19faa4e5ecf87b0b40c3bde16a7ef2a38b27e58.jpg", "table_caption": ["Table 2: Numerical comparison on foldability and recovery rate. Benchmarked results are quoted from Wang et al. [53]. The best and suboptimal results are labeled with bold and underline. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "We report the TM-score and recovery metrics in Table 2. We observe that our Bridge-IF stands out as the leading model, exhibiting both high foldability and a high recovery rate. Notably, the predicted structures of our redesigned sequences align more closely with the given structures than do the native sequences, implying better structural validity of our redesigns. Another interesting finding is that PiFold and LM-Design achieve high recovery via a discriminative formulation but fall short on TM-score, indicating the limitation of structure-agnostic metrics. In contrast, probabilistic models Bridge-IF and ProteinMPNN,2 perform exceptionally well on foldability. These results support our hypothesis that inverse protein folding should be modeled in a probabilistic framework considering the absence of a unique native sequence for a given backbone structure. Figure 4 showcases several instances where the folded structures of sequences designed by Bridge-IF are compared with reference crystal structures. ", "page_idx": 7}, {"type": "image", "img_path": "Q8yfhrBBD8/tmp/23d563ee70cdbaaad748dac26c209963531e078b88fa74b71d599b7dd6323f6b.jpg", "img_caption": ["Figure 4: Folding comparison of our designed sequences (in blue) and the native sequences (in nude). "], "img_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "Q8yfhrBBD8/tmp/d1e0b822980b85b375c76e06fbb8501cfe199424783f620bb0a50f6c60992433.jpg", "table_caption": ["Table 3: Ablation studies of key design choices on CATH v4.2. \"w/ AdaLN-Bias\" replaces the vanilla AdaLN with AdaLN-Bias. \"w/ SCE\" replaces the variational lower bound loss with simplified cross-entropy loss. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "5.4 De novo protein design ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Thus far, our experiments have been limited to accurate experimentally-determined structures. However, in real-world applications like de novo protein design, inverse folding models are commonly used to design sequences for novel structures generated by backbone generation models [56, 26]. Consequently, we next evaluate Bridge-IF for its potential in such a scenario. The experimental methodology is detailed as follows: we sample 10 backbones at every length $[100,105,\\ldots,500]$ in intervals of 5 using Chroma [26]. For each de novo structure, we employ inverse folding models to design 8 sequences. Subsequently, these sequences are folded using ESMFold to identify the sequence with the highest TM-score (scTM). We compare Bridge-IF with ProteinMPNN [6], which is widely used in de novo protein design [59, 57]. Our results show that Bridge-IF surpasses ProteinMPNN in terms of scTM (0.73 vs. 0.69) and designability (0.85 vs. 0.80), using $\\mathrm{scTM}>0.5$ as the criterion. ", "page_idx": 8}, {"type": "text", "text": "5.5 Ablation Studies ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We conduct ablation experiments on CATH v4.2 to verify the impact of key design choices, and present the results in Table 3. ", "page_idx": 8}, {"type": "text", "text": "5.5.1 Prior ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We investigate two training strategies distinguished by their prior: 1) the structure encoder and the PLM are jointly trained; 2) the structure encoder is first pre-trained and remains frozen during the subsequent training of the PLM. We noted that the structure encoder is trained with an equivalent objective in both strategies. The latter consistently yields higher-quality protein sequences. Hence, it has been established as our default configuration. ", "page_idx": 9}, {"type": "text", "text": "5.5.2 Training objective ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We find that the proposed simplified cross-entropy loss works better than the variational lower bound loss [24], demonstrating that the inferior performance of the vanilla Markov bridge model may stem from a harder optimization. ", "page_idx": 9}, {"type": "text", "text": "5.5.3 Network architecture ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We observe that the performance of Bridge-IF further increases $(57.92\\%\\rightarrow58.59\\%)$ when we replace the vanilla AdaLN with the proposed variant AdaLN-Bias. We highlight the use of AdaLN-Bias to enhance compatibility with pre-trained parameters when modulating a pre-trained Transformer model with additional conditions. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we introduce Bridge-IF, the first diffusion bridge model based on the Markov bridge process for inverse protein folding. Bridge-IF can gradually generate high-quality protein sequences from a deterministic prior. Bridge-IF achieves state-of-the-art performance in sequence recovery and foldability. Future work will focus on investigating more advanced structural encoders [38] and pre-training Bridge-IF using more protein structure data predicted by AlphaFold2 [28] to further enhance performance. We also intend to apply Bridge-IF to guide protein engineering aimed at designing novel functional proteins. One potential limitation of the proposed Bridge-IF is its lack of validation through wet-lab experiments in practical applications. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This research was partially supported by National Natural Science Foundation of China under grants No.12326612, Zhejiang Key R&D Program of China under grant No. 2023C03053 and No. 2024SSYS0026, Alibaba Research Intern Program. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Sarah Alamdari, Nitya Thakkar, Rianne van den Berg, Alex Xijie Lu, Nicolo Fusi, Ava Pardis Amini, and Kevin K Yang. Protein generation with evolutionary diffusion: sequence is all you need. bioRxiv, pages 2023\u201309, 2023.   \n[2] Rebecca F Alford, Andrew Leaver-Fay, Jeliazko R Jeliazkov, Matthew J O\u2019Meara, Frank P DiMaio, Hahnbeom Park, Maxim V Shapovalov, P Douglas Renfrew, Vikram K Mulligan, Kalli Kappel, et al. The rosetta all-atom energy function for macromolecular modeling and design. Journal of chemical theory and computation, 13(6):3031\u20133048, 2017.   \n[3] Jacob Austin, Daniel D Johnson, Jonathan Ho, Daniel Tarlow, and Rianne Van Den Berg. Structured denoising diffusion models in discrete state-spaces. Advances in Neural Information Processing Systems, 34:17981\u201317993, 2021.   \n[4] Yongxin Chen, Tryphon T Georgiou, and Michele Pavon. Optimal transport in systems and control. Annual Review of Control, Robotics, and Autonomous Systems, 4:89\u2013113, 2021.   \n[5] Alexander E Chu, Tianyu Lu, and Po-Ssu Huang. Sparks of function by de novo protein design. Nature Biotechnology, 42(2):203\u2013215, 2024.   \n[6] Justas Dauparas, Ivan Anishchenko, Nathaniel Bennett, Hua Bai, Robert J Ragotte, Lukas F Milles, Basile IM Wicky, Alexis Courbet, Rob J de Haas, Neville Bethel, et al. Robust deep learning\u2013based protein sequence design using proteinmpnn. Science, 378(6615):49\u201356, 2022.   \n[7] Valentin De Bortoli, James Thornton, Jeremy Heng, and Arnaud Doucet. Diffusion schr\u00f6dinger bridge with applications to score-based generative modeling. Advances in Neural Information Processing Systems, 34:17695\u201317709, 2021.   \n[8] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:8780\u20138794, 2021.   \n[9] Ahmed Elnaggar, Michael Heinzinger, Christian Dallago, Ghalia Rehawi, Yu Wang, Llion Jones, Tom Gibbs, Tamas Feher, Christoph Angerer, Martin Steinegger, et al. Prottrans: Toward understanding the language of life through self-supervised learning. IEEE transactions on pattern analysis and machine intelligence, 44(10):7112\u20137127, 2021.   \n[10] Pat Fitzsimmons, Jim Pitman, and Marc Yor. Markovian bridges: construction, palm interpretation, and splicing. In Seminar on Stochastic Processes, 1992, pages 101\u2013134. Springer, 1992.   \n[11] Zhangyang Gao, Cheng Tan, and Stan Z Li. Alphadesign: A graph protein design method and benchmark on alphafolddb. arXiv preprint arXiv:2202.01079, 2022.   \n[12] Zhangyang Gao, Cheng Tan, and Stan Z. Li. Pifold: Toward effective and efficient protein inverse folding. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id $\\cdot$ oMsN9TYwJ0j.   \n[13] Zhangyang Gao, Cheng Tan, Yijie Zhang, Xingran Chen, Lirong Wu, and Stan Z. Li. Proteininvbench: Benchmarking protein inverse folding on diverse tasks, models, and metrics. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023. URL https://openreview.net/forum?id $\\equiv$ bqXduvuW5E.   \n[14] Zhangyang Gao, Cheng Tan, Xingran Chen, Yijie Zhang, Jun Xia, Siyuan Li, and Stan Z. Li. KW-design: Pushing the limit of protein design via knowledge refinement. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview. net/forum?id=mpqMVWgqjn.   \n[15] Nate Gruver, Samuel Don Stanton, Nathan C. Frey, Tim G. J. Rudner, Isidro Hotzel, Julien Lafrance-Vanasse, Arvind Rajpal, Kyunghyun Cho, and Andrew Gordon Wilson. Protein design with guided discrete diffusion. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id $\\mathbf{\\mu=}$ MfiK69Ga6p.   \n[16] Tymor Hamamsy, James T Morton, Robert Blackwell, Daniel Berenberg, Nicholas Carriero, Vladimir Gligorijevic, Charlie EM Strauss, Julia Koehler Leman, Kyunghyun Cho, and Richard Bonneau. Protein remote homology detection and structural alignment using deep learning. Nature biotechnology, pages 1\u201311, 2023.   \n[17] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:6840\u20136851, 2020.   \n[18] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J Fleet. Video diffusion models. Advances in Neural Information Processing Systems, 35:8633\u20138646, 2022.   \n[19] Lars Holdijk, Yuanqi Du, Priyank Jaini, Ferry Hooft, Bernd Ensing, and Max Welling. Path integral stochastic optimal control for sampling transition paths. In ICML 2022 2nd AI for Science Workshop, 2022.   \n[20] Emiel Hoogeboom, Alexey A. Gritsenko, Jasmijn Bastings, Ben Poole, Rianne van den Berg, and Tim Salimans. Autoregressive diffusion models. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=Lm8T39vLDTE.   \n[21] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for nlp. In International conference on machine learning, pages 2790\u20132799. PMLR, 2019.   \n[22] Chloe Hsu, Robert Verkuil, Jason Liu, Zeming Lin, Brian Hie, Tom Sercu, Adam Lerer, and Alexander Rives. Learning inverse folding from millions of predicted structures. In International conference on machine learning, pages 8946\u20138970. PMLR, 2022.   \n[23] Po-Ssu Huang, Scott E Boyken, and David Baker. The coming of age of de novo protein design. Nature, 537(7620):320\u2013327, 2016.   \n[24] Ilia Igashov, Arne Schneuing, Marwin Segler, Michael M. Bronstein, and Bruno Correia. Retrobridge: Modeling retrosynthesis with markov bridges. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum? id=770DetV8He.   \n[25] John Ingraham, Vikas Garg, Regina Barzilay, and Tommi Jaakkola. Generative models for graph-based protein design. Advances in neural information processing systems, 32, 2019.   \n[26] John B Ingraham, Max Baranov, Zak Costello, Karl W Barber, Wujie Wang, Ahmed Ismail, Vincent Frappier, Dana M Lord, Christopher $\\mathrm{Ng}.$ -Thow-Hing, Erik R Van Vlack, et al. Illuminating protein space with a programmable generative model. Nature, 623(7989):1070\u20131078, 2023.   \n[27] Bowen Jing, Stephan Eismann, Patricia Suriana, Raphael John Lamarre Townshend, and Ron Dror. Learning from protein structure with geometric vector perceptrons. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum? id=1YLJDvSx6J4.   \n[28] John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin \u017d\u00eddek, Anna Potapenko, et al. Highly accurate protein structure prediction with alphafold. Nature, 596(7873):583\u2013589, 2021.   \n[29] Hamed Khakzad, Ilia Igashov, Arne Schneuing, Casper Goverde, Michael Bronstein, and Bruno Correia. A new age in protein design empowered by deep learning. Cell Systems, 14(11): 925\u2013939, 2023.   \n[30] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.   \n[31] Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. Diffwave: A versatile diffusion model for audio synthesis. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id $=\\mathtt{a}$ -xFK8Ymz5J.   \n[32] Christian L\u00e9onard. A survey of the schr\u00f6dinger problem and some of its connections with optimal transport. Discrete & Continuous Dynamical Systems-A, 34(4):1533\u20131574, 2014.   \n[33] Xiang Li, John Thickstun, Ishaan Gulrajani, Percy S Liang, and Tatsunori B Hashimoto. Diffusion-lm improves controllable text generation. Advances in Neural Information Processing Systems, 35:4328\u20134343, 2022.   \n[34] Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin, Robert Verkuil, Ori Kabeli, Yaniv Shmueli, et al. Evolutionary-scale prediction of atomic-level protein structure with a language model. Science, 379(6637):1123\u20131130, 2023.   \n[35] Guan-Horng Liu, Arash Vahdat, De-An Huang, Evangelos A Theodorou, Weili Nie, and Anima Anandkumar. I2sb: image-to-image schr\u00f6dinger bridge. In Proceedings of the 40th International Conference on Machine Learning, pages 22042\u201322062, 2023.   \n[36] Aaron Lou, Chenlin Meng, and Stefano Ermon. Discrete diffusion modeling by estimating the ratios of the data distribution. In Forty-first International Conference on Machine Learning, 2024. URL https://openreview.net/forum?id=CNicRIVIPA.   \n[37] Ali Madani, Ben Krause, Eric R Greene, Subu Subramanian, Benjamin P Mohr, James M Holton, Jose Luis Olmos, Caiming Xiong, Zachary Z Sun, Richard Socher, et al. Large language models generate functional protein sequences across diverse families. Nature Biotechnology, 41(8): 1099\u20131106, 2023.   \n[38] Weian Mao, Muzhi Zhu, Zheng Sun, Shuaike Shen, Lin Yuanbo Wu, Hao Chen, and Chunhua Shen. De novo protein design using geometric vector field networks. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum? id $\\equiv$ 9UIGyJJpay.   \n[39] Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In International conference on machine learning, pages 8162\u20138171. PMLR, 2021.   \n[40] Christine A Orengo, Alex D Michie, Susan Jones, David T Jones, Mark B Swindells, and Janet M Thornton. Cath\u2013a hierarchic classification of protein domain structures. Structure, 5 (8):1093\u20131109, 1997.   \n[41] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 4195\u20134205, 2023.   \n[42] Milong Ren, Chungong Yu, Dongbo Bu, and Haicang Zhang. Accurate and robust protein sequence design with carbondesign. Nature Machine Intelligence, 6(5):536\u2013547, 2024.   \n[43] Erwin Schr\u00f6dinger. Sur la th\u00e9orie relativiste de l\u2019\u00e9lectron et l\u2019interpr\u00e9tation de la m\u00e9canique quantique. In Annales de l\u2019institut Henri Poincar\u00e9, volume 2, pages 269\u2013310, 1932.   \n[44] Yuyang Shi, Valentin De Bortoli, Andrew Campbell, and Arnaud Doucet. Diffusion schr\u00f6dinger bridge matching. Advances in Neural Information Processing Systems, 36, 2024.   \n[45] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International conference on machine learning, pages 2256\u20132265. PMLR, 2015.   \n[46] Vignesh Ram Somnath, Matteo Pariset, Ya-Ping Hsieh, Maria Rodriguez Martinez, Andreas Krause, and Charlotte Bunne. Aligned diffusion schr\u00f6dinger bridges. In Uncertainty in Artificial Intelligence, pages 1985\u20131995. PMLR, 2023.   \n[47] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In International Conference on Learning Representations, 2021. URL https://openreview. net/forum?id=PxTIG12RRHS.   \n[48] Cheng Tan, Zhangyang Gao, Jun Xia, Bozhen Hu, and Stan Z Li. Global-context aware generative protein design. In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 1\u20135. IEEE, 2023.   \n[49] Francisco Vargas, Pierre Thodoroff, Austen Lamacraft, and Neil Lawrence. Solving schr\u00f6dinger bridges via maximum likelihood. Entropy, 23(9):1134, 2021.   \n[50] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141 ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper_files/paper/2017/file/ 3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf.   \n[51] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.   \n[52] Clement Vignac, Igor Krawczuk, Antoine Siraudin, Bohan Wang, Volkan Cevher, and Pascal Frossard. Digress: Discrete denoising diffusion for graph generation. arXiv preprint arXiv:2209.14734, 2022.   \n[53] Chuanrui Wang, Bozitao Zhong, Zuobai Zhang, Narendra Chaudhary, Sanchit Misra, and Jian Tang. Pdb-struct: A comprehensive benchmark for structure-based protein design. In NeurIPS 2023 Workshop on New Frontiers of AI for Drug Discovery and Development, 2023.   \n[54] Gefei Wang, Yuling Jiao, Qian Xu, Yang Wang, and Can Yang. Deep generative learning via schr\u00f6dinger bridge. In International conference on machine learning, pages 10794\u201310804. PMLR, 2021.   \n[55] Hanchen Wang, Tianfan Fu, Yuanqi Du, Wenhao Gao, Kexin Huang, Ziming Liu, Payal Chandak, Shengchao Liu, Peter Van Katwyk, Andreea Deac, et al. Scientific discovery in the age of artificial intelligence. Nature, 620(7972):47\u201360, 2023.   \n[56] Joseph L Watson, David Juergens, Nathaniel R Bennett, Brian L Trippe, Jason Yim, Helen E Eisenach, Woody Ahern, Andrew J Borst, Robert J Ragotte, Lukas F Milles, et al. De novo design of protein structure and function with rfdiffusion. Nature, 620(7976):1089\u20131100, 2023.   \n[57] Kevin E Wu, Kevin K Yang, Rianne van den Berg, Sarah Alamdari, James Y Zou, Alex X Lu, and Ava P Amini. Protein structure generation via folding diffusion. Nature communications, 15(1):1059, 2024.   \n[58] Kai Yi, Bingxin Zhou, Yiqing Shen, Pietro Lio, and Yu Guang Wang. Graph denoising diffusion for inverse protein folding. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id $\\overrightharpoon{}$ u4YXKKG5dX.   \n[59] Jason Yim, Brian L Trippe, Valentin De Bortoli, Emile Mathieu, Arnaud Doucet, Regina Barzilay, and Tommi Jaakkola. Se (3) diffusion model with application to protein backbone generation. In International Conference on Machine Learning, pages 40001\u201340039. PMLR, 2023.   \n[60] Mingze Yin, Hanjing Zhou, Yiheng Zhu, Miao Lin, Yixuan Wu, Jialu Wu, Hongxia Xu, ChangYu Hsieh, Tingjun Hou, Jintai Chen, et al. Multi-modal clip-informed protein editing. arXiv preprint arXiv:2407.19296, 2024.   \n[61] Yang Zhang and Jeffrey Skolnick. Tm-align: a protein structure alignment algorithm based on the tm-score. Nucleic acids research, 33(7):2302\u20132309, 2005.   \n[62] Lingxiao Zhao, Xueying Ding, Lijun Yu, and Leman Akoglu. Improving and unifying discrete&continuous-time discrete denoising diffusion. arXiv preprint arXiv:2402.03701, 2024.   \n[63] Lin Zheng, Jianbo Yuan, Lei Yu, and Lingpeng Kong. A reparameterized discrete diffusion model for text generation. arXiv preprint arXiv:2302.05737, 2023.   \n[64] Zaixiang Zheng, Yifan Deng, Dongyu Xue, Yi Zhou, Fei Ye, and Quanquan Gu. Structureinformed language models are protein designers. In International Conference on Machine Learning, pages 42317\u201342338. PMLR, 2023.   \n[65] Linqi Zhou, Aaron Lou, Samar Khanna, and Stefano Ermon. Denoising diffusion bridge models. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id $=$ FKksTayvGo.   \n[66] Yiheng Zhu, Zitai Kong, Jialu Wu, Weize Liu, Yuqiang Han, Mingze Yin, Hongxia Xu, ChangYu Hsieh, and Tingjun Hou. Generative ai for controllable protein sequence design: A survey. arXiv preprint arXiv:2402.10516, 2024. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Algorithms ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The overall workflow of the training and sampling process are provided in Algorithm 1 and Algorithm 2. ", "page_idx": 14}, {"type": "text", "text": "Algorithm 1 Training of the Bridge-IF   \nInput: coupled sample $(\\pmb{s},\\pmb{y})\\sim p_{S,y}$ , structure encoder $\\mathcal{E}$ , neural network $\\varphi_{\\theta}$   \n$x\\,{\\bar{=}}\\,\\mathcal E(s)$ \u25b7Deterministic mapping from structure to sequence $t\\sim\\mathcal{U}(0,\\,.\\,.\\,.\\,,T-1)$ , $z_{t}\\sim\\mathrm{Cat}\\left(z_{t};\\overline{{\\pmb{Q}}}_{t-1}\\pmb{x}\\right)$ $\\triangleright$ Sample time step and intermediate state $\\hat{\\pmb y}\\leftarrow\\varphi_{\\theta}(\\pmb z_{t},t)$ $\\triangleright$ Output of $\\varphi_{\\theta}$ is a vector of probabilities $p(z_{t+1}|z_{t},\\pmb{y})\\leftarrow\\mathrm{Cat}\\left(z_{t+1};\\pmb{Q}_{t}(\\pmb{y})z_{t}\\right)$ $\\triangleright$ Reference transition distribution $q_{\\theta}(z_{t+1}|z_{t})\\gets\\mathrm{Cat}\\left(z_{t+1};Q_{t}(\\hat{\\boldsymbol{y}})z_{t}\\right)$ \u25b7Approximated transition distribution Minimize $D_{\\mathrm{KL}}\\left(p(z_{t+1}|z_{t},\\pmb{y})\\Vert q_{\\theta}(z_{t+1}|z_{t})\\right)$ ", "page_idx": 14}, {"type": "text", "text": "Algorithm 2 Sampling ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Input: starting point $s\\sim p s$ , structure encoder $\\mathcal{E}$ , neural network $\\varphi_{\\theta}$   \n$z_{0}\\gets\\mathcal{E}(s)$   \nfor $t$ in $0,...,T-1$ : y\u02c6 \u2190\u03c6\u03b8(zt, t) $\\triangleright$ Output of $\\varphi_{\\theta}$ is a vector of probabilities $q_{\\theta}(z_{t+1}|z_{t})\\gets\\mathrm{Cat}\\left(z_{t+1};Q_{t}(\\hat{y})z_{t}\\right)$ \u25b7Approximated transition distribution $\\boldsymbol{z}_{t+1}\\sim q_{\\theta}\\big(\\boldsymbol{z}_{t+1}|\\boldsymbol{z}_{t}\\big)$   \nReturn zT ", "page_idx": 14}, {"type": "text", "text": "B Additional results ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "B.1 Multi-chain protein complex design ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Studying protein sequence design for multichain assemble structures is crucial for drug design. Next, we assess the capabilities of designing multi-chain complexes using the PDB dataset curated by Dauparas et al. [6], where sequences were clustered at $30\\%$ identity, resulting in 25,361 clusters. Following the standard data splitting, we divided those clusters randomly into three groups for training (23,358), validation (1,464), ensuring that neither the chains from the target chain nor the chains from the biounits of the target chain would be present in the other two groups. ", "page_idx": 14}, {"type": "text", "text": "As shown in Table 4, Bridge-IF also achieves similar improvements when extending to the PDB dataset, further validating its effectiveness and generalizability. ", "page_idx": 14}, {"type": "table", "img_path": "Q8yfhrBBD8/tmp/74be16f4c34c38ce245c5159d830cea3c7c43bd73df88efdaa84b27fb9bae959.jpg", "table_caption": ["Table 4: Performance on multi-chain protein complex dataset (in median recovery). Results of the original ProteinMPNN and GVP-Transformer were obtained using publicly available checkpoints. "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "These results show that Bridge-IF can not only design single-chain proteins, which are mostly studied in previous works but also be used for designing multi-chain protein complexes. ", "page_idx": 14}, {"type": "text", "text": "C Derivations for the variational bound of reparameterized Markov bridge models ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We derive the variational bound on negative log-likelihood $\\log q_{\\theta}(\\pmb{y}|\\pmb{x})$ as discussed in Section 4.2. ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{-\\log q_{\\theta}(\\boldsymbol{y}|\\boldsymbol{x})=-\\log q_{\\theta}(\\boldsymbol{z}_{T}|\\boldsymbol{z}_{0})}\\\\ &{\\phantom{=}\\;-\\log\\int q_{\\theta}(\\boldsymbol{z}_{1:T},\\boldsymbol{v}_{1:T}|\\boldsymbol{z}_{0})\\;d\\boldsymbol{v}_{1:T}\\;d\\boldsymbol{z}_{1:T-1}}\\\\ &{\\phantom{=}=-\\log\\int\\frac{p\\left(\\boldsymbol{z}_{1:T},\\boldsymbol{v}_{1:T}|\\boldsymbol{z}_{0},\\boldsymbol{z}_{T}\\right)}{p\\left(\\boldsymbol{z}_{1:T},\\boldsymbol{v}_{1:T}|\\boldsymbol{z}_{0},\\boldsymbol{z}_{T}\\right)}q_{\\theta}(\\boldsymbol{z}_{1:T},\\boldsymbol{v}_{1:T}|\\boldsymbol{z}_{0})\\;d\\boldsymbol{v}_{1:T}\\;d\\boldsymbol{z}_{1:T-1}}\\\\ &{\\phantom{=}\\leq-\\int p(\\boldsymbol{z}_{1:T},\\boldsymbol{v}_{1:T}|\\boldsymbol{z}_{0},\\boldsymbol{z}_{T})\\log\\frac{q_{\\theta}(\\boldsymbol{z}_{1:T},\\boldsymbol{v}_{1:T}|\\boldsymbol{z}_{0})}{p\\left(\\boldsymbol{z}_{1:T},\\boldsymbol{v}_{1:T}|\\boldsymbol{z}_{0},\\boldsymbol{z}_{T}\\right)}\\;d\\boldsymbol{v}_{1:T}\\;d\\boldsymbol{z}_{1:T-1}}\\\\ &{\\phantom{=}=\\boldsymbol{T}\\cdot\\mathbb{E}_{t\\sim\\mathcal{U}(0,\\dots,T-1)}\\mathcal{L}_{t}(\\boldsymbol{\\theta})}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}_{t}(\\theta)}\\\\ &{\\mathrm{\\quad}=\\mathbb{E}_{p(z_{t}\\mid x,y)}\\left[\\mathbb{E}_{p(v_{t})}[\\mathrm{KL}(p(z_{t+1}\\vert v_{t},z_{t},z_{T})\\vert\\vert q_{\\theta}(z_{t+1}\\vert v_{t},z_{t}))]+\\mathrm{KL}\\left(p(v_{t})\\Vert q_{\\theta}(v_{t})\\right)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We adopt the simplifying assumption that $q_{\\theta}\\big(v_{t}\\big)=p\\big(v_{t}\\big)$ , then $\\mathcal{L}_{t}(\\theta)$ can be written as ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathcal{L}_{t}(\\theta)=\\mathbb{E}_{p(z_{t}|x,y)p(v_{t})}[\\mathrm{KL}(p(z_{t+1}|v_{t},z_{t},z_{T})||q_{\\theta}(z_{t+1}|v_{t},z_{t}))],\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "in which the KL divergence has the form ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathrm{KL}[p(z_{t+1}|v_{t},z_{t},z_{T})||q_{\\theta}(z_{t+1}|v_{t},z_{t})]=\\left\\{\\begin{array}{l l}{(1-\\beta_{t})\\mathrm{KL}(y||\\varphi_{\\theta}(z_{t},t))}&{{\\mathrm{if~}}v_{t}=1}\\\\ {\\mathrm{KL}(z_{t}||z_{t})=0}&{{\\mathrm{if~}}v_{t}=0}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Given that $\\mathrm{KL}(\\pmb{y}||\\varphi_{\\theta}(z_{t},t))=-\\pmb{y}^{T}\\log\\varphi_{\\theta}(z_{t},t)$ , we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{p(v_{t})}[{\\bf K L}\\left[(p(z_{t+1}|v_{t},z_{t},z_{T})||q_{\\theta}(z_{t+1}|v_{t},z_{t}))\\right]}\\\\ &{\\mathrm{~=~}-(1-\\beta_{t})v_{t}{\\bf y}^{T}\\log\\varphi_{\\theta}(z_{t},t)}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "D Broader impacts ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Inverse protein folding models, operating within the broader realm of bioinformatics and computational biology, have significant impacts across various scientific and practical domains. These models, by enabling the design or prediction of protein sequences that fold into specific three-dimensional structures, foster advancements in numerous fields. The broader impacts encompass several areas, including drug discovery, enzyme design, and synthetic biology. ", "page_idx": 15}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: The abstract and introduction clearly state the claims made, including the contributions made in the paper and important assumptions. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 16}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Justification: See conclusion. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 16}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: See $\\S4$ ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 17}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: See $\\S5$ Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 17}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 17}, {"type": "text", "text": "Answer: [No] ", "page_idx": 18}, {"type": "text", "text": "Justification: The code and pre-trained models will be made publicly available upon acceptance of the paper. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 18}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: See $\\S5$ ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 18}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: See $\\S5$ ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: See $\\S5$ . ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 19}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: The research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 19}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: See Appendix D. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 20}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: See $\\S5$ . ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets. ", "page_idx": 20}, {"type": "text", "text": "\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 20}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 21}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 21}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 21}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 21}]