[{"figure_path": "pVPyCgXv57/tables/tables_5_1.jpg", "caption": "Table 1: Classification results with off-the-shelf frozen pre-trained models. Reduction roughly represents the decreases in FLOPs.", "description": "This table presents the classification accuracy (Acc@1), GFLOPs (floating point operations), and images per second (im/s) for different vision transformer models (DeiT-S, DeiT-B, MAE-B, MAE-L) with varying token reduction rates (35% and 50%).  The results compare the performance of our proposed method (DTEM) against existing methods (EVIT and ToMe). It demonstrates the impact of DTEM on model efficiency while maintaining competitive accuracy.", "section": "4.1 Image Classification"}, {"figure_path": "pVPyCgXv57/tables/tables_6_1.jpg", "caption": "Table 1: Classification results with off-the-shelf frozen pre-trained models. Reduction roughly represents the decreases in FLOPs.", "description": "This table presents the classification accuracy (Acc@1), GFLOPs (floating point operations), and images per second (im/s) for different models on the ImageNet-1k dataset.  The models used are DeiT-S, DeiT-B, MAE-B, and MAE-L. The results are shown for different token reduction rates (35% and 50%). The table compares the performance of the proposed DTEM method with existing methods like EViT and ToMe, demonstrating the efficiency gains achieved by DTEM with minimal accuracy loss.  The reduction rate reflects the decrease in computational cost.", "section": "4.1 Image Classification"}, {"figure_path": "pVPyCgXv57/tables/tables_7_1.jpg", "caption": "Table 4: Image captioning evaluation results when token merging is applied. We report with caption evaluation metrics: BLEU-4 (B@4), CIDEr (C), METEOR (M) and SPICE (S). Reduction represents the decreases in FLOPs within the ViT encoder, and # indicates the number of tokens passed to language decoder.", "description": "This table presents the results of image captioning experiments using different token merging methods.  It shows the performance metrics (BLEU-4, CIDEr, METEOR, SPICE) achieved by different methods under various reduction rates (representing a decrease in FLOPs).  The '#' column indicates the number of tokens passed from the vision transformer to the language decoder.  The table compares the performance of ToMe and the proposed DTEM method, highlighting the improvements in efficiency and performance achieved by DTEM.", "section": "4.2 Image Captioning"}, {"figure_path": "pVPyCgXv57/tables/tables_7_2.jpg", "caption": "Table 5: Results on semantic segmentation when token merging is applied. The reduction ratio indicates the portion of merged tokens.", "description": "This table presents the results of semantic segmentation experiments using the Seg-S-Mask/16 model [29] with different token merging methods (ToMe [2] and the proposed DTEM).  The baseline (r=0) represents the performance without token merging. The reduction ratio indicates the percentage of tokens merged.  The table shows the GFLOPs (floating-point operations) and mIoU (mean Intersection over Union) for each method at different reduction ratios.  Lower GFLOPs indicate improved efficiency, while higher mIoU indicates better segmentation accuracy.", "section": "4.3 Semantic Segmentation"}, {"figure_path": "pVPyCgXv57/tables/tables_8_1.jpg", "caption": "Table 6: Ablation study on the impact of decoupled embedding. We successively add soft token merging and decoupled embedding module into ToMe. The number in parentheses indicates the reduction in FLOPs.", "description": "This table presents the results of an ablation study evaluating the impact of the proposed decoupled embedding module on the performance of token merging.  The study is performed on the DeiT-S and DeiT-B models.  It shows the impact of adding the soft token merging component and then further adding the decoupled embedding module, comparing the results to the baseline ToMe method. The accuracy is reported for two different reduction rates (-35% and -50%), representing the decrease in FLOPs.", "section": "4.4 Analysis"}, {"figure_path": "pVPyCgXv57/tables/tables_8_2.jpg", "caption": "Table 7: Kendall rank correlation coefficient changed through training. We report changes in the Kendall rank correlation between token similarities derived from two different features: self-attention keys and decoupled embedding.", "description": "This table presents the Kendall rank correlation coefficients between token similarities derived from self-attention keys and decoupled embeddings, before and after training.  The correlation is calculated separately for three sets of transformer blocks (1-4, 5-8, 9-12).  The results show a decrease in correlation after training, suggesting that the decoupled embedding learns a different feature representation for token merging, distinct from the self-attention features.", "section": "4.4 Analysis"}, {"figure_path": "pVPyCgXv57/tables/tables_13_1.jpg", "caption": "Table 1: Classification results with off-the-shelf frozen pre-trained models. Reduction roughly represents the decreases in FLOPs.", "description": "This table presents the classification accuracy (Acc@1), GFLOPs (floating point operations), and images per second (im/s) for different vision transformer (ViT) models using three different token reduction methods: EViT, ToMe, and DTEM.  The results are shown for two reduction rates (35% and 50%), indicating the computational savings achieved by each method. The table demonstrates the performance of DTEM compared to existing methods when using pre-trained models without further fine-tuning. ", "section": "4.1 Image Classification"}, {"figure_path": "pVPyCgXv57/tables/tables_13_2.jpg", "caption": "Table 1: Classification results with off-the-shelf frozen pre-trained models. Reduction roughly represents the decreases in FLOPs.", "description": "This table shows the classification accuracy (Acc@1), GFLOPs, and images per second (im/s) for different vision transformer models (DeiT-S, DeiT-B, MAE-B, MAE-L) with varying reduction rates (35% and 50%).  The results are presented for both the baseline models and models using the proposed DTEM method.  The reduction percentage reflects the decrease in FLOPs achieved using token reduction techniques.", "section": "4.1 Image Classification"}, {"figure_path": "pVPyCgXv57/tables/tables_13_3.jpg", "caption": "Table 4: Image captioning evaluation results when token merging is applied. We report with caption evaluation metrics: BLEU-4 (B@4), CIDEr (C), METEOR (M) and SPICE (S). Reduction represents the decreases in FLOPs within the ViT encoder, and # indicates the number of tokens passed to language decoder.", "description": "This table presents the results of image captioning experiments using different token merging methods.  It shows the performance of each method across various reduction rates (percentage decrease in FLOPs) on two different ViT models (GIT-B and GIT-L). The metrics used for evaluation are BLEU-4, CIDEr, METEOR, and SPICE, which are common in evaluating image captioning performance.  The '#' column indicates the number of tokens remaining after applying the token merging method, that is passed to the language decoder for caption generation.", "section": "4.2 Image Captioning"}, {"figure_path": "pVPyCgXv57/tables/tables_14_1.jpg", "caption": "Table 14: Image classification results with 100 epochs of end-to-end training.", "description": "This table presents the results of image classification experiments conducted using DeiT-T and DeiT-S models with end-to-end training for 100 epochs. The table compares the performance of the proposed DTEM method against the dTPS method for different reduction rates (35% and 50%).  The results show accuracy (Acc@1) achieved by each method under these conditions.  The purpose is to demonstrate the effectiveness of DTEM, even with extensive training, in maintaining or surpassing the performance of a comparable method.", "section": "4.1 Image Classification"}, {"figure_path": "pVPyCgXv57/tables/tables_14_2.jpg", "caption": "Table 14: Image classification results with 100 epochs of end-to-end training.", "description": "This table presents the results of image classification experiments using DeiT-S and DeiT-T models trained for 100 epochs with end-to-end training.  It compares the performance of DTEM and the dTPS method at different reduction rates (r), indicating the top-1 accuracy achieved.  The table highlights the improvements in accuracy that DTEM provides compared to dTPS.  The GFLOPs column shows the computational cost at different reduction rates.", "section": "4.1 Image Classification"}, {"figure_path": "pVPyCgXv57/tables/tables_14_3.jpg", "caption": "Table 2: Classification results with LV-ViT-S. * indicates the results with off-the-shelf frozen pretrained model.", "description": "This table presents the results of image classification experiments using the LV-ViT-S model.  It compares the performance of different token reduction methods, specifically ToMe and DTEM, when applied to a pre-trained LV-ViT-S model. The table shows the top-1 accuracy (Acc@1), GFLOPS (floating point operations per second), and images per second (im/s) for each method, with and without the use of pretrained models.", "section": "4.1 Image Classification"}, {"figure_path": "pVPyCgXv57/tables/tables_14_4.jpg", "caption": "Table 1: Classification results with off-the-shelf frozen pre-trained models. Reduction roughly represents the decreases in FLOPs.", "description": "This table presents the classification accuracy (Acc@1), GFLOPs, and images per second (im/s) for different ViT models (DeiT-S, DeiT-B, MAE-B, MAE-L) using three token reduction methods (EViT, ToMe, DTEM) at two different reduction levels (35% and 50%).  It demonstrates the performance of DTEM compared to existing methods while maintaining efficiency by reducing FLOPs.", "section": "4.1 Image Classification"}, {"figure_path": "pVPyCgXv57/tables/tables_15_1.jpg", "caption": "Table 4: Image captioning evaluation results when token merging is applied. We report with caption evaluation metrics: BLEU-4 (B@4), CIDEr (C), METEOR (M) and SPICE (S). Reduction represents the decreases in FLOPs within the ViT encoder, and # indicates the number of tokens passed to language decoder.", "description": "This table presents the results of image captioning experiments using two different token merging methods: ToMe and DTEM.  The evaluation metrics used are BLEU-4, CIDEr, METEOR, and SPICE.  The table shows the performance of each method at various reduction rates (representing decreased FLOPs in the ViT encoder), indicating the trade-off between computational efficiency and captioning quality.  The '# tokens' column shows how many tokens are passed to the language decoder after the token merging process.", "section": "4.2 Image Captioning"}]