{"references": [{"fullname_first_author": "A. Dosovitskiy", "paper_title": "An image is worth 16x16 words: Transformers for image recognition at scale", "publication_date": "2021-00-00", "reason": "This paper introduces Vision Transformers (ViTs), a foundational model for the field, which this paper builds upon and improves."}, {"fullname_first_author": "H. Touvron", "paper_title": "Training data-efficient image transformers & distillation through attention", "publication_date": "2021-07-18", "reason": "This paper introduces DeiT, a data-efficient training method for ViTs, which is directly compared against in this paper's experiments."}, {"fullname_first_author": "K. He", "paper_title": "Masked autoencoders are scalable vision learners", "publication_date": "2022-00-00", "reason": "This paper introduces Masked Autoencoders (MAE), a significant method for self-supervised learning of ViTs, which is used as a baseline for comparison in this paper."}, {"fullname_first_author": "D. Bolya", "paper_title": "Token merging: Your vit but faster", "publication_date": "2023-00-00", "reason": "This paper proposes ToMe, a key prior work that this paper directly addresses, improves upon, and compares against in its experiments."}, {"fullname_first_author": "C. Renggli", "paper_title": "Learning to merge tokens in vision transformers", "publication_date": "2022-02-12", "reason": "This paper is another highly relevant prior work which is directly compared to and improved upon by the proposed method."}]}