[{"figure_path": "3ivnixHy16/tables/tables_1_1.jpg", "caption": "Table 1: Statistics of existing preference datasets for text-to-video generative models. * denotes annotated by human while \u2020 denotes annotated by GPT-4 V.", "description": "This table presents a comparison of various existing preference datasets used for training text-to-video generative models.  It shows the number of prompts, videos, and preference choices in each dataset.  The asterisk (*) indicates datasets where human annotation was used to create the preference dataset, while the dagger (\u2020) indicates that GPT-4 V (a large language model from OpenAI) was used for annotation.  The table highlights the significantly larger scale of the VIDEOPREFER dataset compared to existing datasets.", "section": "1 Introduction"}, {"figure_path": "3ivnixHy16/tables/tables_4_1.jpg", "caption": "Table 1: Statistics of existing preference datasets for text-to-video generative models. * denotes annotated by human while \u2020 denotes annotated by GPT-4 V.", "description": "This table presents a comparison of various existing preference datasets used for training text-to-video generative models.  It shows the number of prompts, videos, and preference choices included in each dataset.  The table highlights the relative scarcity of large-scale preference datasets in the video domain compared to the image domain, and indicates whether the annotations were provided by humans or an AI model (GPT-4 Vision).  The dataset VIDEOPREFER is introduced as a significantly larger dataset compared to those previously available.", "section": "1 Introduction"}, {"figure_path": "3ivnixHy16/tables/tables_8_1.jpg", "caption": "Table 4: Ablation study for VIDEORM. The Aesthetic Classifier (simplified as Aesthetic) makes prediction without seeing the text prompt.", "description": "This table presents the results of an ablation study conducted on the VIDEORM model.  The study investigates the impact of different components and configurations of the model on its performance.  Specifically, it shows the pairwise preference prediction accuracy across three different human-crafted datasets (TVGE [56], VBench [15], T2VQA-DB [18]) for several variations of the VIDEORM model. The variations include changes to the temporal feature modeling method (VIDEORM+, VIDEORM\u2020), the backbone model (VIDEORMa, VIDEORM\u03b2), and  the original VIDEORM model. The 'Aesthetic' row represents a baseline model that makes predictions without considering text prompts.  The average accuracy across all three datasets is also provided for comparison.", "section": "4.3 Ablation Study"}]