[{"figure_path": "NLmAGkN6nn/tables/tables_7_1.jpg", "caption": "Table 1: Performance comparison on ImageNet 256x256. \u2018(W/A)\u2019 indicates that the precision of weights and activations are W and A bits, respectively.", "description": "This table presents a quantitative comparison of different post-training quantization (PTQ) methods on the ImageNet dataset with 256x256 image resolution.  It compares the performance of several methods including PTQ4DM, Q-Diffusion, PTQD, and RepQ*, against the full precision (FP) model. The comparison uses different quantization bit-widths (8-bit and 4-bit for both weights and activations) and varying numbers of timesteps (250, 100, and 50) during the image generation process.  The metrics used for evaluation are FID, SFID, IS, and Precision. Lower FID and SFID scores, and higher IS and Precision scores indicate better performance.", "section": "5.2 Quantization Performance"}, {"figure_path": "NLmAGkN6nn/tables/tables_7_2.jpg", "caption": "Table 1: Performance comparison on ImageNet 256x256. \u2018(W/A)\u2019 indicates that the precision of weights and activations are W and A bits, respectively.", "description": "This table presents a comparison of the performance of different post-training quantization (PTQ) methods on the ImageNet 256x256 dataset.  The methods are compared using FID, sFID, IS, and Precision metrics at various bit-widths (W/A) for weights and activations.  The results show the effectiveness of the proposed PTQ4DiT method in maintaining high performance compared to existing methods, even at lower bit-widths.", "section": "5.2 Quantization Performance"}, {"figure_path": "NLmAGkN6nn/tables/tables_8_1.jpg", "caption": "Table 1: Performance comparison on ImageNet 256x256. \u2018(W/A)\u2019 indicates that the precision of weights and activations are W and A bits, respectively.", "description": "This table compares the performance of different post-training quantization (PTQ) methods on the ImageNet 256x256 dataset. The methods are evaluated using various metrics (FID, sFID, IS, Precision) at different bit-widths (8-bit and 4-bit) for weights and activations and different numbers of sampling steps.  It allows for assessing the effectiveness of each quantization method in preserving image generation quality while reducing computational cost.", "section": "5.2 Quantization Performance"}]