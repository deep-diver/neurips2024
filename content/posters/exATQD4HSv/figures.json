[{"figure_path": "exATQD4HSv/figures/figures_5_1.jpg", "caption": "Figure 1: Schematic of training protocol and gradient flow. A: Before training, observations {xt} and nuisance artifacts {rt} are deconvolved. B: The deconvolved time series are used to generate a forcing signal dt-1 which is used for guiding cshPLRNN training. C: Latent states zt-r:t and nuisance artifacts rt are used to predict \u00eet through the decoder model. Gradients are computed on the squared error loss Lt, propagated from the decoder model back to the latent states (blue), and from the latent DS model backwards in time (orange).", "description": "This figure illustrates the training process of the convSSM model. It shows three stages: pre-training (deconvolution of observations and artifacts), training (teacher forcing using deconvolved time series to guide cshPLRNN), and gradient flow (backpropagation from decoder to latent states and then back in time through PLRNN). The figure uses color-coded arrows to represent the flow of gradients and data.", "section": "Results"}, {"figure_path": "exATQD4HSv/figures/figures_6_1.jpg", "caption": "Figure 2: Validations on Lorenz63 and ALN. A: Illustration of reconstruction performance as assessed by the geometrical agreement measure Dstsp. Average Dstsp values for the convSSM were Dstsp < 0.30 at noise level \u03c3 = .01 and Dstsp < 0.71 at noise level \u03c3 = .1, indicating successful reconstructions in the majority of cases. B: Example trajectory from the Lorenz63 system in latent space (top) and observation space (convolved with hrf0.2) (bottom). C: Probability density over maximal Amax values (orange) assessed on 1000 convSSMs trained on Lorenz63 time series of length 1000 (example shown in right panel). Black line denotes the known Amax \u2248 0.9056 of the Lorenz system. D: Comparison of standard SSM ('standard'), convSSM ('conv'), and convSSM trained without generalized teacher forcing ('conv (NoGTF)') on the ALN data set. Histograms over Dstsp assessed on the observed space (left panel) and latent space (right panel). E: Dstsp for convSSM evaluated on the full pseudo-empirical time series of typical empirically available length (T = 500; x-axis) vs. the long GT test set (T = 5,000; y-axis). F: Dstsp for convSSM evaluated on the observed time series (x-axis) vs. on the latent time series (y-axis).", "description": "This figure shows several validations and evaluations of the convSSM model on the Lorenz63 and ALN datasets. It includes visualizations of the model's reconstruction performance using geometrical agreement measure (Dstsp), probability density of maximal Lyapunov exponent (Amax), and histograms of Dstsp on different datasets. It also shows scatter plots comparing Dstsp values on short time series with long time series and latent space with observation space.", "section": "3 Results"}, {"figure_path": "exATQD4HSv/figures/figures_8_1.jpg", "caption": "Figure 2: Validations on Lorenz63 and ALN. A: Illustration of reconstruction performance as assessed by the geometrical agreement measure Dstsp. Average Dstsp values for the convSSM were Dstsp < 0.30 at noise level \u03c3 = .01 and Dstsp < 0.71 at noise level \u03c3 = .1, indicating successful reconstructions in the majority of cases. B: Example trajectory from the Lorenz63 system in latent space (top) and observation space (convolved with hrf0.2) (bottom). C: Probability density over maximal Amax values (orange) assessed on 1000 convSSMs trained on Lorenz63 time series of length 1000 (example shown in right panel). Black line denotes the known Amax \u2248 0.9056 of the Lorenz system. D: Comparison of standard SSM ('standard'), convSSM ('conv'), and convSSM trained without generalized teacher forcing ('conv (NoGTF)') on the ALN data set. Histograms over Dstsp assessed on the observed space (left panel) and latent space (right panel). E: Dstsp for convSSM evaluated on the full pseudo-empirical time series of typical empirically available length (T = 500; x-axis) vs. the long GT test set (T = 5,000; y-axis). F: Dstsp for convSSM evaluated on the observed time series (x-axis) vs. on the latent time series (y-axis).", "description": "This figure demonstrates the model's performance on the Lorenz63 and ALN datasets.  It shows that the convSSM model accurately reconstructs the dynamics of these systems, even with noisy or convolved data. Key performance metrics such as Dstsp and DPSE are compared across different models and conditions.  The figure highlights the convSSM's superior performance and scalability, especially when dealing with short time series.", "section": "3 Results"}, {"figure_path": "exATQD4HSv/figures/figures_16_1.jpg", "caption": "Figure 4: Training duration per epoch (y-axis) in seconds for different TRs (A), hidden dimensions L (B), latent dimensions M (C), time series length (D), and observation dimensions (E). Mean, standard error (SEM) and linear curve fits (gray dashed lines) are displayed. The per-epoch-runtime increases approximately linearly with dimensions L, M, and N; explained variance R\u00b2 = 0.989, R\u2081 = 0.993, and R = 0.996 for linear regressions with predictors 'L', 'M', and 'N', respectively. Experiments were performed on a standard notebook with Intel i5-8250U 1,60 GHz CPU and 8GB RAM.", "description": "This figure shows the scalability of the convSSM model.  The training time per epoch is plotted against different model parameters (TR, L, M, T, N). The results demonstrate that the training time increases approximately linearly with the model dimensionality and filter length, indicating good scalability.", "section": "3 Results"}, {"figure_path": "exATQD4HSv/figures/figures_23_1.jpg", "caption": "Figure 5: A: Agreement in DSR measures assessed on the observed (x-axis) vs. latent (y-axis) space of the short pseudo-empirical test set (top) and the full pseudo-empirical time series (bottom). Correlations between Dstsp (left), DPSE (middle), and PE10 (right) are displayed, respectively. B: Top: Agreement in DSR measures assessed on the pseudo-empirical test set (short) vs. GT test set (long). Bottom: Same for full pseudo-empirical time series (short) vs. GT test set (long). Correlations between Dstsp (left), DPSE (middle), and PE10 (right) are displayed, respectively. C: Correlations between DSR measures between pseudo-empirical test set and full pseudo-empirical time series, same order as in B.", "description": "This figure demonstrates the agreement between the different DSR measures (Dstsp, DPSE, and PE10) when evaluated on different subsets of the ALN dataset. Panel A shows the agreement between measures calculated on the short pseudo-empirical test set and the full pseudo-empirical time series. Panel B displays the correlation between the same measures using short pseudo-empirical time series and longer time series (GT test set). Panel C shows the agreement between measures on the pseudo-empirical test set and the full pseudo-empirical time series.", "section": "3 Results"}, {"figure_path": "exATQD4HSv/figures/figures_25_1.jpg", "caption": "Figure 6: A. Ground truth Lorenz trajectory sampled with noise (black), a good reconstruction with low Dstsp (orange) that accurately recovers the attractor, and a poor reconstruction with high Dstsp (green) that represents the attractor inaccurately, yielding an oscillatory (limit cycle) instead of a chaotic solution. B. Trajectories of systems in A. unfolded in time. The inaccurate reconstruction (top) achieves a lower prediction error (PE) than the accurate reconstruction (bottom), due to trajectory divergence in chaotic systems. This example illustrates that PEs are inadequate to capture the reconstruction of chaotic DS.", "description": "This figure shows a comparison between a good and a bad reconstruction of a Lorenz attractor. The ground truth data (black) is shown along with a good reconstruction (orange) and a bad reconstruction (green). The good reconstruction accurately captures the shape of the attractor, while the bad reconstruction produces a limit cycle instead of a chaotic attractor.  The bottom panel plots the corresponding time series for each of the reconstructions and the ground truth.  The figure highlights the fact that simple prediction error (PE) is not a suitable metric to evaluate the quality of dynamical system reconstruction, as the inaccurate model achieves a lower PE than the accurate one.", "section": "3.3 Validating performance measures on short time series"}, {"figure_path": "exATQD4HSv/figures/figures_26_1.jpg", "caption": "Figure 7: A: Full (solid) and residual (dashed) average (across dimensions) auto-correlation functions for the latent (left) and observed (right) time series. For the residual auto-correlation, the immediately preceding time step was regressed out. For the dotted curve, cshPLRNN(\u2248t\u22121) instead of zt\u22121 was regressed out. B: Same as A for the mutual information as a function of time lag.", "description": "This figure demonstrates the effectiveness of the proposed deconvolution method by comparing the autocorrelation and mutual information of the latent and observed time series, both with and without removing the linear dependencies between consecutive time points. It shows that the deconvolution effectively removes the temporal dependencies in the latent states, while the dependencies remain in the observed time series. The comparison of the residual time series using both latent states and cshPLRNN model output shows that the temporal dependencies are removed effectively by the proposed deconvolution and modeling method.", "section": "Further Details"}]