[{"type": "text", "text": "A scalable generative model for dynamical system reconstruction from neuroimaging data ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Eric Volkmann $^{1,2,*}$ , Alena Br\u00e4ndle $^{1,3,4,*}$ , Daniel Durstewitz $^{1,3,4,\\dagger}$ , Georgia Koppe3,5,6,\u2020 ", "page_idx": 0}, {"type": "text", "text": "1Department of Theoretical Neuroscience, Central Institute of Mental Health (CIMH), Medical Faculty Mannheim, Heidelberg University, Mannheim, Germany 2Institute for Machine Learning, Johannes Kepler University, Linz, Austria   \n3Interdisciplinary Center for Scientific Computing, Heidelberg University, Heidelberg, Germany 4Faculty of Physics and Astronomy, Heidelberg University, Heidelberg, Germany 5Hector Institute for AI in Psychiatry & Dept. for Psychiatry and Psychotherapy, CIMH   \n6Faculty of Mathematics and Computer Science, Heidelberg University, Heidelberg, Germany \u2217,\u2020 These authors contributed equally ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Data-driven inference of the generative dynamics underlying a set of observed time series is of growing interest in machine learning and the natural sciences. In neuroscience, such methods promise to alleviate the need to handcraft models based on biophysical principles and allow to automatize the inference of inter-individual differences in brain dynamics. Recent breakthroughs in training techniques for state space models (SSMs) specifically geared toward dynamical systems (DS) reconstruction (DSR) enable to recover the underlying system including its geometrical (attractor) and long-term statistical invariants from even short time series. These techniques are based on control-theoretic ideas, like modern variants of teacher forcing (TF), to ensure stable loss gradient propagation while training. However, as it currently stands, these techniques are not directly applicable to data modalities where current observations depend on an entire history of previous states due to a signal\u2019s filtering properties, as common in neuroscience (and physiology more generally). Prominent examples are the blood oxygenation level dependent (BOLD) signal in functional magnetic resonance imaging (fMRI) or $\\mathrm{{Ca^{2+}}}$ imaging data. Such types of signals render the SSM\u2019s decoder model non-invertible, a requirement for previous TF-based methods. Here, exploiting the recent success of control techniques for training SSMs, we propose a novel algorithm that solves this problem and scales exceptionally well with model dimensionality and fliter length. We demonstrate its efficiency in reconstructing dynamical systems, including their state space geometry and long-term temporal properties, from just short BOLD time series. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Models and theories based on dynamical systems (DS) concepts have a long tradition in computational neuroscience in accounting for physiological phenomena and computational processes of the brain [74, 53, 33, 23]. Constructing such models from first principles (biophysics) is time-consuming and hard, and utilizing them to account for inter-individual differences in brain dynamics, when model settings need to be personalized, is even more challenging. Yet, constructing valid models of the brain\u2019s functional dynamics is immensely important, not only for understanding the neurocomputational basis of inter-individual differences in cognitive and emotional style [23], but also when aiming at diagnosing or predicting brain dysfunction and clinical characteristics based on DS features [66], or for designing personalized therapies [54]. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "1.1 Dynamical models in neuroscience ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In computational neuroscience, multiple approaches to infer large-scale brain dynamics have been advanced over the past decades (e.g., [17, 46, 58, 32]). Pioneering work introduced latent linear DS, including Gaussian process latent variable models [64, 81], and extensions like Switching Linear DS (SLDS) [27]. Whereas linear DS models are very limited in their dynamical repertoire, SLDS offer a more flexible approach to modeling a larger range of dynamical phenomena by combining several linear (or affine) DS, with a switching mechanism that selects the active system at each moment [44, 43, 27]. These approaches have become common tools for inferring and visualizing neural trajectories within low-dimensional state spaces. ", "page_idx": 1}, {"type": "text", "text": "Arguably the most popular class of generative models in whole brain simulations relies on mean field neural population models, including neural mass [76, 5] and neural field models [35, 5]. These model the moments of the activity of cortical areas by averaging over properties (like firing rates) of neural (sub)populations, and are often biophysically motivated [17]. Many popular large-scale mean field modeling approaches are implemented in The Virtual Brain (TVB) environment [56, 58]. TVB incorporates biologically realistic brain connectivity into neural field models to generate simulations of large-scale brain activity. Alternatively, Dynamic Causal Modeling (DCM) describes a set of more statistically motivated and mostly linear techniques, primarily for the purpose of inferring the effective connectivity between brain regions based on invasive or non-invasive brain recordings (e.g., [16, 37, 46]). While many of these models may account for aspects of the dynamics, like patterns of functional connectivity and their task modulation, they are not, strictly, dynamical systems reconstruction (DSR) tools, entailing that they may miss important dynamical phenomena by being constrained through the biological assumptions and simplifications imposed. ", "page_idx": 1}, {"type": "text", "text": "In the analysis of functional magnetic resonance imaging (fMRI), only recently a shift in focus has introduced data-driven, deep-learning based methods for inferring generative models of system dynamics [40, 62, 63]. Models that implement dynamics either directly on the observed [62], or within an underlying latent [40, 63] space have been proposed, partly using available structural information and hierarchical inference approaches [63]. ", "page_idx": 1}, {"type": "text", "text": "1.2 Dynamical systems reconstruction (DSR) ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "A variety of Deep Learning (DL)-based models for approximating the generative dynamical processes underlying observed time series has been put forward in recent years ([8, 69, 49, 40, 60, 42, 6, 71, 31, 13, 80]; see also [24] for an overview). These include approaches which approximate a system\u2019s vector field, e.g., through a library of basis functions and penalized regression as in SINDy [8], or through deep neural networks and neural Ordinary Differential Equations (ODE) [14, 57, 39]. Alternatively, methods that approximate the associated flow (solution) operator directly have been suggested, often employing state space model (SSM)-type architectures which distinguish between an observation process and a latent process commonly instantiated through recurrent neural networks (RNNs; e.g., [40, 29, 70, 60, 7]). Recurrent SLDS (rSLDS) [44], an extension of SLDS, and Latent Factor Analysis via Dynamical Systems (LFADS) [48] also fall into this category. ", "page_idx": 1}, {"type": "text", "text": "In DSR we ask for models that are generative in the sense that \u2013 after training \u2013 they provide an executable surrogate model of the observed system, from which we can simulate samples that agree with their empirical counterparts in topological, geometrical and temporal characteristics (in contrast to [24]. This required agreement in long-term temporal and geometric properties is not automatically guaranteed for standard training of common RNNs or neural ODEs, which may yield good short-term predictions but may fail to recover the full system dynamics [34, 24, 52]. Recent breakthroughs in data-driven DSR build on insights from the field of chaos control and synchronization [51, 1, 68, 2], by guiding the training process through optimally chosen control signals \u2013 modern variations of classical teacher forcing (TF) \u2013 that prevent exploding gradients [47, 6, 31, 7, 39]. ", "page_idx": 1}, {"type": "text", "text": "Chaotic dynamics in particular, as typical for neural systems (e.g., [67, 22, 36, 26]), poses a severe problem here as trajectories and hence loss gradients inevitably diverge due to the presence of a positive Lyapunov exponent [47]. Recent amendments of TF protocols, including sparse TF (STF; [47]) and generalized TF (GTF; [31]), keep trajectories and gradients in check by \u2018weakly synchronizing\u2019 them with the observed signals. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "1.3 Specific contributions ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Nonlinear SSMs distinguish between an underlying latent process that governs the dynamics of a system\u2019s state, and an observation process (referred to as observation or decoder model), that links the system states to the actual measurements [20]. Invertible (or pseudo-invertible) decoder models play a crucial role in control-theoretic approaches, like STF or GTF, for training SSMs, in order to project observations into the model\u2019s latent space. This inversion is fairly straightforward when ones assumes that the current measurement depends solely on the present latent state, i.e, when $x_{t}=f(z_{t})$ , but not if it depends on a history of states $\\left\\{z_{t},z_{t-1},\\ldots,z_{t-\\tau}\\right\\}$ . In practice, unfortunately, this assumption is often violated due to a signal\u2019s flitering properties. For instance, blood oxygenation level dependent (BOLD) signals, as assessed via fMRI, are only an indirect measurement of neural activity, with the hemodynamic response function $(h r f)$ broadly smearing out the signal across time. Each measurement is therefore a function of a history of past neural (latent) states whose dynamics we wish to infer [28, 78]. Similar challenges arise in calcium imaging when spike times are to be inferred [79, 72], or, in fact, any other observation process where the actual measurement is a flitering of the process of interest. ", "page_idx": 2}, {"type": "text", "text": "Here we rectify this issue by developing a particularly efficient SSM approach which works for measurements that depend on longer histories of latent states, yet allows to take advantage of recent powerful training strategies for DSR [47, 31]. In particular, our contributions are threefold: First, we create and validate a novel SSM-based DSR algorithm for observation models which involve convolutions with latent state series, and demonstrate its scalability with SSM size, as well as convolution filter length. Second, we introduce an evaluation scheme for selecting DSR models on short empirical time series, by demonstrating that the used DSR measures assessed on short time series accurately predict a system\u2019s long-term temporal and geometric properties. This is of high practical relevance, as in many empirically relevant scenarios, like fMRI, we only have access to comparatively short time series. Finally, we show that the proposed models can reliably extract key DS features that, moreover, differentiate between subjects. ", "page_idx": 2}, {"type": "text", "text": "2 Convolution SSM model (convSSM) ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 Latent DSR model ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We start by defining our generative model used for DSR, a variant of a piecewise linear RNN (PLRNN). Its specific architecture has first been suggested in [21] and later expanded to increase the PLRNN\u2019s expressivity [6, 31]. While the present approach is generic and independent of the specific DSR architecture, here we use the so-called shallow PLRNN (shPLRNN; Appx. A.3) and the clipped shallow PLRNN (cshPLRNN; [31]). In the cshPLRNN, the temporal evolution of the (latent) system state $z_{t}\\in\\mathbb{R}^{M}$ is expressed as ", "page_idx": 2}, {"type": "equation", "text": "$$\nz_{t}={\\pmb{A}}z_{t-1}+{\\pmb{W}}_{1}\\left[\\phi\\left({\\pmb{W}}_{2}z_{t-1}+h_{2}\\right)-\\phi\\left({\\pmb{W}}_{2}z_{t-1}\\right)\\right]+h_{1}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\phi(\\cdot)=\\operatorname*{max}(0,\\cdot)$ is an elementwise ReLU activation function, $W_{1}\\in\\mathbb{R}^{M\\times L}$ , $W_{2}\\in\\mathbb{R}^{L\\times M}$ are connection weights, $A\\in\\mathbb{R}^{M\\times M}$ is a diagonal matrix of autoregressive weights, and $h_{1}\\in\\mathbb{R}^{M}$ and $h_{2}\\ \\in\\ \\mathbb{R}^{L}$ are bias vectors. Its trajectories $\\left\\{z_{t}\\right\\}\\,\\in\\,\\mathbb{R}^{M\\times T}$ will be bounded if the absolute eigenvalues of $\\pmb{A}$ are smaller than 1 [31]. The Markov property of the latent model is crucial to ensure it formally constitutes a DS with complete state space [50, 65]. Finally, Equation 1 can easily be extended to incorporate the effect of external inputs, such as experimental stimuli, by adding $C s_{t}$ (with $s_{t}\\in\\mathbb{R}^{K}$ representing an input vector and $\\boldsymbol{C}\\in\\mathbb{R}^{M\\times K}$ its effect on the latent dynamics). However, here we consider input-free data from resting state experiments. ", "page_idx": 2}, {"type": "text", "text": "2.2 Teacher forcing for invertible decoder models ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In [31], the latent state $z_{t}$ at each time point is assumed to be related to the actual observation $\\boldsymbol{x}_{t}\\in\\mathbb{R}^{N}$ by a linear (Gaussian) decoder model ", "page_idx": 2}, {"type": "equation", "text": "$$\nx_{t}=B z_{t}+\\eta_{t},\\ \\eta_{t}\\sim\\mathcal{N}(0,\\Gamma),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "referred to simply as \u2018standard SSM\u2019 in the following. Here, $B\\in\\mathbb{R}^{N\\times M}$ is a matrix of regression weights, and $\\eta_{t}$ describes Gaussian observation noise with diagonal covariance matrix $\\mathbf{T}$ . A conventional mean squared error (MSE) type loss function $\\begin{array}{r}{\\mathcal{L}=\\sum_{t}\\mathcal{L}_{t}=\\sum_{t}\\left\\|\\hat{x}_{t}-x_{t}\\right\\|_{2}^{2}}\\end{array}$ between the generated (predicted) $\\left\\{\\hat{x}_{t}\\right\\}$ and the observed $\\left\\{x_{t}\\right\\}$ sequence is commonly used to optimize parameters by stochastic gradient descent (SGD) with GTF [31]. Regularization terms to enforce a structure in latent space that helps to map slowly evolving processes may further be added to this loss [60]. ", "page_idx": 3}, {"type": "text", "text": "A fundamental issue in training such systems by SGD is the well-known \u2018exploding-and-vanishing gradients\u2019 problem (EVGP), preventing systems from capturing relevant time scales in the data. In fact, [47] proved that for chaotic systems gradient-based training techniques for RNNs will inevitably lead to diverging loss gradients (see also [26]). Successful DSR algorithms need to address this problem. Based on this connection between chaos and diverging gradients, Engelken [25] suggested regularizing the system\u2019s Lyapunov spectrum, thereby also biasing the dynamics toward certain (non-hyperbolic) solutions. A theoretically well founded approach that does not limit a system\u2019s dynamical expressivity, which we will adopt here, is GTF, proposed in [31]. GTF is designed to keep model generated trajectories on track and, theoretically, can completely abolish the EVGP without constraining model expressivity. The main idea is that during training the latent state $\\tilde{z}_{t}$ is computed as a linear interpolation between the PLRNN generated state $z_{t}=\\mathrm{PLRNN}(\\tilde{z}_{t-1})$ and a data-inferred state $d_{t}$ that serves as a control signal [19], i.e., ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\tilde{z}_{t}:=(1-\\alpha)\\cdot z_{t}+\\alpha\\cdot d_{t},\\ \\alpha\\in[0,1).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "There is a theoretically optimal choice for $\\alpha$ that can be approximated concurrently whilst training through a specifically designed annealing protocol [31], but more simply $\\alpha$ may just be determined by grid search (as done here). Control signals $d_{t}$ are obtained by inverting the decoder model (Equation 2). Since in general $M\\ne N$ , the matrix inverse of $B\\overset{\\cdot}{\\in}\\mathbb{R}^{N\\times M}$ does not exist and is approximated by the Moore-Penrose (pseudo-) inverse $B^{+}$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\nd_{t}=B^{+}x_{t}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "To keep the gradients on track, the interpolation is performed at each time step before applying the cshPLRNN mapping (Equation 1). These control signals are turned off during actual data generation by the model (i.e., in a test phase), where it runs completely autonomously. ", "page_idx": 3}, {"type": "text", "text": "2.3 Teacher forcing for decoder models with signal convolution ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Decoder model for convolved signals GTF (and similar techniques like STF; [47]) are powerful state-of-the-art (SOTA) tools for controlling gradients, especially in the context of DSR. However, they require a (pseudo-)invertible observation model for producing adequate control signals. Empirically, there are many situations where this requirement is not met. For instance, in BOLD time series the observed signal is a highly flitered and strongly smoothed version of the underlying neuronal process that we would like to recover [9, 10, 28]. This fairly complex hemodynamic process is often modeled by the hr $f$ [28]. ", "page_idx": 3}, {"type": "text", "text": "A decoder model that relates the neuronal processes given as latent time series $\\left\\{z_{t}\\right\\}$ to measured BOLD time series $\\left\\{x_{t}\\right\\}$ may be formulated as in [40], ", "page_idx": 3}, {"type": "equation", "text": "$$\nx_{t}=B\\left((h r f*z)_{t}\\right)+J r_{t}+\\eta_{t},\\ \\eta_{t}\\sim N(0,\\Gamma)\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "with regression coefficient matrices $B\\in\\mathbb{R}^{N\\times M}$ and $\\pmb{J}\\in\\mathbb{R}^{N\\times P}$ , nuisance variables $r_{t}\\in\\mathbb{R}^{P}$ (such as movement or respiratory artifacts) and a Gaussian observation noise term $\\eta_{t}$ (with usually diagonal covariance $\\mathbf{T}\\in\\mathrm{R}^{\\dot{N}\\times N};$ ). Here, $^*$ denotes the convolution operation and $z$ is a history of states $z_{t-\\tau:t}$ the length of which depends on the observed sampling rate, commonly referred to as time of repetition (TR). The discrete $h r f$ sequence is computed by evaluating the canonical $h r f$ at the observed TR [78]. We will denote the $h r f$ response for a given TR by $h r f_{T R}$ . ", "page_idx": 3}, {"type": "text", "text": "By incorporating the hr $f$ into the observation model, we disentangle the neural state and its dynamics \u2013 the processes of interest \u2013 from the neurovascular mechanics (or any filtering at the level of observed signals). We thereby eliminate the history dependence present in the observations, and thus help unfolding trajectories in latent space and satisfying the uniqueness assumptions required in reconstructing dynamical systems [50] (see also Appx. Figure 7). However, Equation 5 poses a major complication for applying TF techniques, as observations (and model outputs $\\hat{x}_{t}$ ) do not simply depend on the current state $z_{t}$ , but \u2013 due to the convolution \u2013 on a set of states across several previous time steps. We can thus not compute the control signal $d_{t}$ through straightforward decoder inversion anymore, but require a new type of inversion algorithm. ", "page_idx": 3}, {"type": "text", "text": "Wiener deconvolution Following [78], we use a Wiener fliter [75] to invert Equation 5. We briefly introduce this approach here in the context of our specific problem, and refer to Appx. A.4 for further details. Given an observed noisy signal $\\left\\{x_{t}\\right\\}$ , composed of the signal of interest $\\left\\{z_{t}\\right\\}$ convolved with a known impulse response hr $f$ plus some noise term $\\eta_{t}$ (distribution unknown, Wiener is optimal for Gaussian distribution), ", "page_idx": 4}, {"type": "equation", "text": "$$\nx_{t}=(h r f\\ast z)_{t}+\\eta_{t},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "the Wiener deconvolution provides the estimate $\\hat{z}_{t}$ of the unknown signal $z_{t}$ through least-MSE estimation. Defining $\\mathrm{Conv}^{-1}(\\cdot,h r f)$ as the Wiener deconvolution operator, we can write ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\{\\hat{z}_{t}\\}=\\operatorname{Conv}^{-1}(\\{x_{t}\\},h r f).}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Inversion of BOLD decoder model Using the notation introduced above, we can write the inversion to obtain control signals as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\{d_{t}\\}=\\operatorname{Conv}^{-1}\\big(\\{B^{+}(x_{t}-J r_{t})\\},h r f\\big),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\left\\{B^{+}(x_{t}-J r_{t})\\right\\}$ is the time series that needs to be deconvolved. Note that this approach is quite general and we can simply exchange the $h r f$ with alternative functions if we want to account for filtering in the original signal. As stated, since $_B$ and $_{J}$ are matrices of learnable parameters updated during training, we would need to perform this deconvolution step at every training epoch, which is computationally very costly. We therefore make use of the linearity of convolutions and separate the deconvolution step from the learnable parameters, rewriting Equation 8 as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\left\\{d_{t}\\right\\}=B^{+}\\left(\\operatorname{Conv}^{-1}(\\{x_{t}\\},h r f)-J\\cdot\\operatorname{Conv}^{-1}(\\{r_{t}\\},h r f)\\right).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "With $\\{{x_{t}^{\\mathrm{deconv}}}\\}$ and $\\left\\{r_{t}^{\\mathrm{deconv}}\\right\\}$ denoting the respective deconvolved time series, this can be written as ", "page_idx": 4}, {"type": "equation", "text": "$$\nd_{t}=B^{+}\\left(x_{t}^{\\mathrm{deconv}}-J\\cdot r_{t}^{\\mathrm{deconv}}\\right).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "This now is computationally much more efficient, as we need to perform the deconvolution only once prior to training. During training then, only the decoder model parameters need to be inferred to obtain the control signal. We will refer to the convolutional model for DSR (Equation 1 and Equation 5) trained with GTF and SGD as \u2018convSSM\u2019. The full inversion algorithm is provided in Algorithm 1 with additional information given in Appx. A.6. Key components of $\\mathrm{SGD+GTF}$ training are illustrated in Figure 1. ", "page_idx": 4}, {"type": "text", "text": "3 Results ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "3.1 Performance measures ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In DS theory in general, and DSR more specifically, we are mostly concerned with invariant properties of a system, such as attractor geometry and long-term temporal statistics [24]. In chaotic systems in particular, in which trajectories diverge exponentially fast with time, the mean squared prediction error (PE) is a useful statistic only on relatively short time scales [77, 40, 60] (see Appx. Figure 6). To evaluate our model\u2019s performance, in addition to short-term $n$ -step ahead PEs, $P E_{n}$ , we assess the following two established performance measures to capture the temporal and geometrical structure: ", "page_idx": 4}, {"type": "text", "text": "1. The deviation in power spectra between the (smoothed) empirical and model-generated power spectra, assessed in terms of the Hellinger distance and referred to as power spectrum error (PSE), $D_{P S E}$ , in the following [47], and   \n2. the Kullback Leibler divergence between the empirical and model-generated trajectories across state space, $D_{s t s p}$ , measuring the overlap in attractor geometries [40]. ", "page_idx": 4}, {"type": "text", "text": "To obtain a reference value for $D_{s t s p}$ , we further included two references in which we assessed $D_{s t s p}$ when all mass is centered on the expectation value (similar to a fixed point solution), and when the state space is populated by points drawn from a Gaussian with mean and variance equal to the data (similar to a fixed point solution plus measurement noise). ", "page_idx": 4}, {"type": "text", "text": "For comparability with experimental data, we evaluated performance on comparatively short time series obtained from the adaptive linear-nonlinear (ALN) cascade model and the LEMON data set. In these cases, performance metrics were assessed on 100 generated trajectories per model and then averaged. For more details, see Appx. A.7. ", "page_idx": 4}, {"type": "image", "img_path": "exATQD4HSv/tmp/360de27b2564e160d4c46fff788619401d95ae814edfc70c6f557c3b1dd9b607.jpg", "img_caption": ["Figure 1: Schematic of training protocol and gradient flow. A: Before training, observations $\\left\\{x_{t}\\right\\}$ and nuisance artifacts $\\left\\{r_{t}\\right\\}$ are deconvolved. B: The deconvolved time series are used to generate a forcing signal $d_{t-1}$ which is used for guiding cshPLRNN training. C: Latent states $z_{t-\\tau:t}$ and nuisance artifacts $r_{t}$ are used to predict $\\hat{x}_{t}$ through the decoder model. Gradients are computed on the squared error loss $\\mathcal{L}_{t}$ , propagated from the decoder model back to the latent states (blue), and from the latent DS model backwards in time (orange). "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "3.2 convSSM validation & scalability on Lorenz63 ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "As a well-established and popular benchmark for a chaotic system, we first performed numerical experiments with the famous Lorenz63 system. The Lorenz63 is a 3-dimensional system introduced in [45] to describe atmospheric convection, and exhibits chaotic behaviour in the chosen regime (see Appx. B.1). To mimic BOLD observations, we generated 100 standardized chaotic Lorenz63 trajectories, convolved them with $h r f_{T R}$ functions at different sampling rates $\\mathrm{TR}\\in\\lbrace0.2\\mathrm{s},0.5\\mathrm{s},1.2\\mathrm{s}\\rbrace$ (Figure 2B), and added Gaussian noise with standard deviation $\\sigma\\in\\{0.01,0.1\\}$ . This resulted in 6 benchmark settings with different levels of signal degradation by convolution and noise. Each data set was divided into a training and a test set of T = 5 \u00b7 104time steps each. ", "page_idx": 5}, {"type": "text", "text": "We trained 100 models on each of these 6 data sets. The following models were compared: the convSSM trained via SGD+GTF, the convSSM trained via SGD and no GTF, the standard SSM trained via $\\mathrm{SGD+GTF}_{\\mathrm{}}$ , and MINDy, a recently published method for DSR in fMRI [62]. convSSM and standard SSMs were trained with the shPLRNN, with $M=3$ , $L=50$ , and $\\alpha=.1$ (see Appx. Table 4 for all additional hyperparameters). The hidden dimension was selected such that the standard SSM (no-hr $f$ model) performed well [31]. We emphasize that the standard SSM has already been extensively benchmarked on a variety of simulated and real-world data sets and is considered to be a SOTA model in the field [6, 31]. The performance measures $D_{s t s p}$ , $D_{P S E}$ , and $P E_{20}$ were assessed on the test sets after training for 1, 000 epochs. We used the same hyperparameters for all networks (aside from TR) to show that performance increases can be solely attributed to the improved decoder model. Hyperparameters were chosen such that the shPLRNN achieved near perfect performance on unconvolved, noiseless trajectories from the Lorenz63 system. ", "page_idx": 5}, {"type": "text", "text": "The convSSM significantly outperformed all other methods, including the standard SSM in almost all cases, with the performance gap increasing with decreasing TR (see Appx. Table 2 for performance, and Figure 2A for example reconstructions, providing an intuition on how to interpret $D_{s t s p}$ ). The more heavily the signal was degraded by the convolution fliter, the larger was the performance gap in favor of the convSSM. ", "page_idx": 5}, {"type": "text", "text": "An important consideration especially for large-scale applications of such models to empirical data is how well they scale with model size and convolution filter length. To assess this, we collected trajectories of length $\\textit{T}=\\ 10^{5}$ from the chaotic Lorenz63 system, and studied training epoch times as a function of convSSM latent dimension $M\\;=\\;$ $\\{3,10,50,100,500\\}$ , hidden dimension $L\\ =\\ \\{10,50,100,500,1000\\}$ , $\\mathrm{{TR}\\it~=~\\{0.2,0.5,1.2,3\\}}$ , time series length $T~=~\\{500,1000,5000,10000,50000,100000\\}$ and observation dimension ", "page_idx": 5}, {"type": "image", "img_path": "exATQD4HSv/tmp/bd593b0526864eacada6782b0f3e305064a18b8d3a9488507504b8d4df09ba45.jpg", "img_caption": ["Figure 2: Validations on Lorenz63 and ALN. A: Illustration of reconstruction performance as assessed by the geometrical agreement measure $D_{s t s p}$ . Average $D_{s t s p}$ values for the convSSM were $D_{s t s p}<0.30$ at noise level $\\sigma=.01$ and $D_{s t s p}<0.71$ at noise level $\\sigma=.1$ , indicating successful reconstructions in the majority of cases. B: Example trajectory from the Lorenz63 system in latent space (top) and observation space (convolved with $h r f_{0.2})$ (bottom). C: Probability density over maximal $\\lambda_{m a x}$ values (orange) assessed on 1000 convSSMs trained on Lorenz63 time series of length 1000 (example shown in right panel). Black line denotes the known $\\lambda_{m a x}\\approx0.9056$ of the Lorenz system. D: Comparison of standard SSM (\u2019standard\u2019), convSSM (\u2019conv\u2019), and convSSM trained without generalized teacher forcing (\u2019conv (NoGTF)\u2019) on the ALN data set. Histograms over $D_{s t s p}$ assessed on the observed space (left panel) and latent space (right panel). E: $D_{s t s p}$ for convSSM evaluated on the full pseudo-empirical time series of typical empirically available length ( $T=500$ ; $x$ -axis) vs. the long GT test set $T=5$ , 000; $y$ -axis). F: $D_{s t s p}$ for convSSM evaluated on the observed time series $\\overrightharpoon{x}$ -axis) vs. on the latent time series $(y$ -axis). "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "$N=\\{10,30,50,100,500,1000\\}$ . Shorter/longer TRs directly implicate longer/shorter convolution fliters since the fliters assume a constant time interval. Results are displayed in Appx. Figure 4A. The runtime per epoch did not significantly depend on TR, which means that time series convolved with long impulse response functions can be trained in times comparable to short ones. The per-epochruntime increases approximately linearly with dimensions $L,\\,M$ , and $N$ (Appx. Figure 4 B,C,E), implying that models can be scaled up efficiently. ", "page_idx": 6}, {"type": "text", "text": "Finally, empirical data is often short, yet we want to reliably infer DS features that characterize the underlying dynamics. To demonstrate that our model can robustly reconstruct dynamics based on short time series, we inferred 1000 convSSMs on $n=100$ convolved Lorenz63 time series (TR $=0.5)$ ) of length $T=1000$ only (see Figure 2C). We then assessed the degree of chaoticity in the recovered trajectories by examining the trained models\u2019 maximum Lyapunov exponents, $\\lambda_{m a x}$ . $\\lambda_{m a x}$ measures how quickly trajectories starting from nearby points in a system\u2019s state space converge or diverge with time. If $\\lambda_{m a x}>0$ , trajectories will exponentially diverge and the system, if bounded, will exhibit chaos. We show that we can successfully recover $\\lambda_{m a x}$ (known for the Lorenz63 system; Figure 2C) even from models trained on these just short series. ", "page_idx": 6}, {"type": "text", "text": "3.3 Validating performance measures on short time series ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In empirical situations, we do not have access to the latent dynamics of the true system, of course, but we still rely on our reconstruction measures evaluated on the observed signals to yield results valid for the (unobserved) latent space. It is therefore a practically very relevant question whether a) the convSSM trained on such short time series would be able to accurately recover the underlying neural latent dynamics, and b) our measures $(D_{s t s p},D_{P S E})$ evaluated on such short time series, and directly on the observations, would yield results similar to what would be expected if much longer time series and access to the ground truth latent space were available. ", "page_idx": 7}, {"type": "text", "text": "To tackle these questions, we used a more realistic simulation model, the ALN model [3, 11] for simulating whole brain (neural) activity. 100 data sets of length $T=10,000$ were simulated from this model using neurolib [12], sampled at $0.1\\,\\mathrm{ms}$ , and flitered through Equation 5 (with $\\mathrm{TR}=0.1\\,\\mathrm{ms}$ ) to compute the corresponding BOLD time series. Subsequently, these time series were downsampled to a TR of 0.5 s to mimic an experimentally realistic scenario (see Appx. B.2 for details). Most hyperparameters were adopted from previous experiments (see Appx. Table 4). For the latent dimension, we chose $M\\,=\\,16$ to match the dimensions of the empirical LEMON data set, see subsection 3.4. ", "page_idx": 7}, {"type": "text", "text": "To mimic real fMRI experiments, we then pretended that only the first 500 time points are available for model estimation (called \u2018pseudo-empirical\u2019 here to distinguish it from the actual empirical LEMON data set). We trained 10 convSSM models on the first 375 time steps of each of these virtual experiments, treating the left out 125 time points as pseudo-empirical test set and call the last 5, 000 time points of the entire trajectory (i.e., time steps 5, 001-10, 000 of the full simulation set) the ground truth (GT) test set (which would not be accessible in a real experiment). DSR performance was assessed on both the observed $\\left\\{x_{t}\\right\\}$ and latent $\\left\\{z_{t}\\right\\}$ time series, evaluated for a) the short pseudo-empirical test set of length 125, b) the full pseudo-empirical time series (i.e., of length 500), and c) the GT test set of length 5000 (which also assesses dynamics on the limit set and does not contain transients anymore). ", "page_idx": 7}, {"type": "text", "text": "Figure 2D shows histograms over $D_{s t s p}$ (on the full pseudo-empirical time series) for the convSSM, the standard SSM, and the benchmark conditions. The convSSM significantly outperformed the standard SSM in latent space (rank-sum test $Z=11.50$ , $p\\leq.001\\$ ), demonstrating an improved recovery of the ground truth DS and indicating that the deconvolution acts as an inductive bias that forces the model to learn a latent space structured in agreement with our biophysical understanding of fMRI. Moreover, the proposed performance measures $(D_{s t s p},D_{P S E})$ successfully discriminated between good and poor reconstructions even on these short time series more typical for empirical data: For one, evaluating DSR on the observations was consistent with evaluating DSR directly on the latent dynamics space (Figure 2F and Appx. Figure 5). Second, DSR assessed on the pseudo-empirical time series (either full or only test set) was strongly correlated with performance assessed on the long GT test set (which, again, in empirical situations we do not have, Figure 2E and Appx. Figure 5). ", "page_idx": 7}, {"type": "text", "text": "3.4 Application to experimental fMRI data ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We finally tested convSSM on empirical data, for which we chose the LEMON study (\u2018Leipzig Study for Mind-Body-Emotion Interactions\u2019) as a publicly available data set. This data set was collected at the Max-Planck-Institute Leipzig [4] and consists of 227 healthy participants, each of whom completed a battery of tests, including a $15\\mathrm{min}\\;30\\mathrm{s}$ resting state fMRI (rsfMRI) session sampled at $\\mathrm{TR}=1.4s$ (thus comprising $T=652$ time points). We used the preprocessed rsfMRI data sets as provided, and selected 16 regions from which we extracted a subset of the available time series. These were subsequently smoothed, band-pass filtered, and standardized as in [40]. The time series were split $3:1$ into training $(T_{t r a i n}\\,=\\,489)$ and test $(T_{t e s t}\\,=\\,163)$ set, respectively. Data from participants with non-stable variance were discarded (i.e., non-stationary data, see Appx. B.3 for details), leaving $N=51$ participants for analysis. ", "page_idx": 7}, {"type": "text", "text": "We trained 20 models on data of each participant with latent dimension $M=N=16$ (i.e., equal to the observation dimension), $\\alpha=.1$ , and hidden dimension $L=50$ (where $L$ and $M$ refer to the dimensions of the connectivity matrices $W_{1}$ , $W_{2}$ in the cshPLRNN, Equation 1). Latent dimension and $\\alpha$ were determined via grid search, by inferring systems using a subset of the data and assessing the performance on the held-out set [6]. Otherwise the same hyperparameters as used in [31] for EEG data were applied (see Appx. Table 4 for all details). In addition to the model comparisons discussed earlier, we also compared our method to the performance of rSLDS [44] and LFADS [48] (see Appx. C.2 for details). On top of $D_{s t s t p}$ , $D_{P S E}$ , and $P E_{n}$ , we also assessed the trained models\u2019 maximum Lyapunov exponents, $\\lambda_{m a x}$ , analyzed how reliably these can be inferred, and whether they distinguish between subjects. Note that obtaining an estimate of the Lyapunov exponent is an advantage of the generative model, as empirical time series are often too short to compute it reliably. Also, since we do have access to the cshPLRNN\u2019s Jacobians, the computations can be performed analytically (although practically we need to evaluate these along model-generated trajectories, where here we used an algorithm proposed in [73]). ", "page_idx": 7}, {"type": "image", "img_path": "exATQD4HSv/tmp/5c0875adf374e97f60eb166d686dce9c9e3ec3ada009d4ee2df6c33869b1ff12.jpg", "img_caption": ["Figure 3: Results on empirical LEMON data set. A: Distribution over $D_{s t s p}$ for 1020 systems inferred with convSSM. B: Example of a good and C: poor reconstruction. D: Illustration of reconstruction performance as a function of $D_{s t s p}$ . E: Histogram over maximum Lyapunov exponents $\\lambda_{m a x}$ . F: Distribution over $\\lambda_{m a x}$ for 5 selected participants $n=100$ systems with 10 trajectories each). G: Within- as compared to between-subject variance in $\\lambda_{m a x}$ distribution after filtering models by DS performance measures (selecting the 20 best by $D_{s t s p}$ and 10 best by $D_{P S E}$ ). "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "The DSR results are shown in Table 1. We obtained successful reconstructions on average with a mean $D_{s t s p}$ of 2.73, better than all other models (Figure 3A and D). Interestingly, most recovered systems were characterized by a positive maximal Lyapunov exponent $\\lambda_{m a x}$ (Figure 3E), indicating the presence of chaotic attractors in these data (consistent with previous observations, [40, 36, 41]). Moreover, $\\lambda_{m a x}$ values could be inferred reliably (Figure 3F), and differentiated between individuals, as indicated by lower within- as compared to between-subject variation (Figure 3G, $T(50)\\;=$ $-11.53,p<.001$ . ", "page_idx": 8}, {"type": "table", "img_path": "exATQD4HSv/tmp/e9ed731e0727f7265099a26db87ee64e9b2ad82288a6e5745b9a42bf689bd03c.jpg", "table_caption": ["Table 1: DSR measures evaluated for the convSSM, standard SSM, convSSM trained without GTF, as well as MINDy [62], rSLDS [44] and LFADS [14], trained on the LEMON dataset. Model runs were excluded if the 1-step $\\mathrm{PE}>1$ on the training data. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "4 Conclusions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Methods for producing generative models of the underlying dynamics from time series observations is a rapidly expanding research field [24]. Current SOTA models for this purpose rely on control theoretically motivated training techniques like STF [47] and GTF [31], but these require some means to generate from the actual observations a TF signal in the model\u2019s latent space for guiding trajectory and gradient flows. This becomes difficult if the current observation depends on a whole series of latent states, as common if the actual measurements are some filtering of an underlying process of interest, such as in fMRI or $\\mathrm{{Ca^{2+}}}$ imaging. Here we provide a novel technique that efficiently deals with this problem, exploiting linearity of Wiener deconvolution. A hallmark of our technique is that it efficiently scales with model size and convolution length. ", "page_idx": 9}, {"type": "text", "text": "Another major contribution of this work is to numerically demonstrate that the short time series obtained in typical fMRI experiments are actually sufficient for proper model selection according to established DSR performance measures, and that these can indeed be properly evaluated in observation space and do not require access to the unobserved dynamics/ latent space. This is of major empirical relevance for many scientific scenarios, beyond fMRI, in which time series sampling is costly or restricted for technical reasons. Finally, using our DSR technique, we showed that experimental fMRI signals mostly exhibit properties of chaotic oscillators (consistent with [36]), and that these can be reliably inferred and differ between subjects. Taken together, these contributions pave the way for deploying data-driven fMRI DSR models at large scale to understand inter-individual differences in brain dynamics and explore the predictive value of nonlinear DS features for cognitive or clinical assessment. ", "page_idx": 9}, {"type": "text", "text": "We emphasize that the proposed framework is highly flexible due to its modular structure, and may be easily adapted to meet diverse requirements. First, the latent model can be replaced with any other differentiable and recursive dynamical model, such as e.g. LSTMs [59]. The GTF training framework would remain unchanged as the control signal and the latent state update (Equation 3) are not affected by such modifications [31]. Likewise, the observation model can easily be adapted to account for nonlinear effects of nuisance covariates, e.g. through basis expansions in these variables, or through learnable but regularized MLPs. While our model was designed as a scalable method to integrate biological prior knowledge on convolution filters like the hr $f$ , alternatively we can parameterize the filter weights within the observation model, making them learnable through BPTT, with filter length either as a hyperparameter, or by imposing a regularization that truncates filter length by driving coefficients to zero. To prevent confilcts between fliter adjustment and latent model, a viable strategy may be stage-wise learning as suggested in [40]. Once the fliter is adjusted, one may reduce the learning rate on the observation model, or even fix its parameters, to prioritize learning of the dynamics. Fixing the fliter parameters after an initial stage would have the advantage that subsequent training would enjoy the same speed benefits as in our suggested method. ", "page_idx": 9}, {"type": "text", "text": "We furthermore highlight that our framework could be adapted to accommodate noise in the latent process. For example, in Brenner et al. [7] the GTF procedure has been modified to work in the context of stochastic DSR models using variational inference. Instead of the multimodal encoder model in Brenner et al. [7], one may use the inversion in Equation 9 to generate a TF signal which steers a probabilistic latent DS model, i.e. controls its distributional mean, via Equation 3, and using the reparameterization trick [55, 38] for BPTT in latent space. However, although probabilistic frameworks are appealing, \u2018deterministic\u2019 BPTT has previously been shown to be (at least) comparable in terms of DSR performance, even for clearly noisy observations and latent processes [6], such that the benefits for DSR would need to be further examined. ", "page_idx": 9}, {"type": "text", "text": "Limitations Data-driven approaches such as the one proposed here lack detailed biophysical mechanisms and may thus not be as suited to address specific questions relating to pharmacological or receptor mechanisms beyond functional-dynamical implications. Moreover, currently open questions are how to best deal with non-stationarity in the data, how to efficiently combine data from many subjects, and how trained models generalize to out-of-domain data. ", "page_idx": 9}, {"type": "text", "text": "Software and Data ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Code for the convSSM is available at https://github.com/humml-lab/GTF-ConvSSM. ", "page_idx": 9}, {"type": "text", "text": "5 Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This work was supported by the German Research Foundation (DFG) within the collaborative research center TRR 265, subproject B08, granted to GK, TRR 265 subproject A06 granted to DD and GK, Germany\u2019s Excellence Strategy EXC 2181/1 \u2013 390900948 (STRUCTURES), and the Hector II foundation. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] H. D. I. Abarbanel. Predicting the future: Completing models of observed complex systems. Springer, 2013.   \n[2] H. D. I. Abarbanel. The statistical physics of data assimilation and machine learning. Cambridge University Press, 2022.   \n[3] M. Augustin, J. Ladenbauer, F. Baumann, and K. Obermayer. Low-dimensional spike rate models derived from networks of adaptive integrate-and-fire neurons: Comparison and implementation. PLoS Computational Biology, 13(6):e1005545, 2017.   \n[4] A. Babayan, M. Erbey, D. Kumral, J. D. Reinelt, A. M. Reiter, J. R\u00f6bbig, H. L. Schaare, M. Uhlig, A. Anwander, P.-L. Bazin, et al. A mind-brain-body dataset of MRI, EEG, cognition, emotion, and peripheral physiology in young and old adults. Scientific Data, 6(1):1\u201321, 2019.   \n[5] M. Breakspear. Dynamic models of large-scale brain activity. Nature Neuroscience, 20(3): 340\u2013352, 2017.   \n[6] M. Brenner, F. Hess, J. M. Mikhaeil, L. F. Bereska, Z. Monfared, P.-C. Kuo, and D. Durstewitz. Tractable dendritic RNNs for reconstructing nonlinear dynamical systems. In International Conference on Machine Learning, volume 162, pages 2292\u20132320. PMLR, 2022. [7] M. Brenner, G. Koppe, and D. Durstewitz. Multimodal teacher forcing for reconstructing nonlinear dynamical systems. AAAI Workshops, 2023.   \n[8] S. L. Brunton, J. L. Proctor, and J. N. Kutz. Discovering governing equations from data by sparse identification of nonlinear dynamical systems. Proceedings of the National Academy of Sciences USA, 113(15):3932\u20133937, 2016.   \n[9] R. L. Buckner. Event-related fMRI and the hemodynamic response. Human Brain Mapping, 6 (5-6):373\u2013377, 1998.   \n[10] R. B. Buxton, K. Uludag\u02d8, D. J. Dubowitz, and T. T. Liu. Modeling the hemodynamic response to brain activation. NeuroImage, 23:S220\u2013S233, 2004.   \n[11] C. Cakan and K. Obermayer. Biophysically grounded mean-field models of neural populations under electrical stimulation. PLoS Computational Biology, 16(4):e1007822, 2020.   \n[12] C. Cakan, N. Jajcay, and K. Obermayer. neurolib: A simulation framework for whole-brain neural mass modeling. Cognitive Computation, 15(4):1132\u20131152, 2023.   \n[13] J. Chen and K. Wu. Deep-OSG: Deep learning of operators in semigroup. Journal of Computational Physics, 493:112498, 2023.   \n[14] R. T. Q. Chen, Y. Rubanova, J. Bettencourt, and D. Duvenaud. Neural ordinary differential equations. In Advances in Neural Information Processing Systems, volume 31, pages 6571\u20136583, 2018.   \n[15] G. Datseris. DynamicalSystems.jl: A Julia software library for chaos and nonlinear dynamics. Journal of Open Source Software, 3(23):598, 2018.   \n[16] O. David and K. J. Friston. A neural mass model for MEG/EEG:: Coupling and neuronal dynamics. NeuroImage, 20(3):1743\u20131755, 2003.   \n[17] G. Deco, V. K. Jirsa, P. A. Robinson, M. Breakspear, and K. Friston. The dynamic brain: From spiking neurons to neural masses and cortical fields. PLoS Computational Biology, 4(8): e1000092, 2008.   \n[18] D. L. Donoho and I. M. Johnstone. Ideal spatial adaptation by wavelet shrinkage. Biometrika, 81(3):425\u2013455, 1994.   \n[19] K. Doya. Bifurcations in the learning of recurrent neural networks. In International Symposium on Circuits and Systems, volume 6, pages 2777\u20132780. IEEE, 1992.   \n[20] J. Durbin and S. J. Koopman. Time series analysis by state space methods, volume 38. OUP Oxford, 2012.   \n[21] D. Durstewitz. A state space approach for piecewise-linear recurrent neural networks for identifying computational dynamics from neural measurements. PLoS Computational Biology, 13(6):e1005542, 2017.   \n[22] D. Durstewitz and T. Gabriel. Dynamical basis of irregular spiking in NMDA-driven prefrontal cortex neurons. Cerebral Cortex, 17(4):894\u2013908, 2007.   \n[23] D. Durstewitz, Q. J. Huys, and G. Koppe. Psychiatric illnesses as disorders of network dynamics. Biological Psychiatry: Cognitive Neuroscience and Neuroimaging, 6(9):865\u2013876, 2021.   \n[24] D. Durstewitz, G. Koppe, and M. I. Thurm. Reconstructing computational system dynamics from neural data with recurrent neural networks. Nature Reviews Neuroscience, pages 1\u201318, 2023.   \n[25] R. Engelken. Gradient flossing: Improving gradient descent through dynamic control of Jacobians. In Advances in Neural Information Processing Systems, volume 36, pages 10412\u2013 10439, 2023.   \n[26] R. Engelken, F. Wolf, and L. F. Abbott. Lyapunov spectra of chaotic recurrent neural networks. Physical Review Research, 5(4):043044, 2023.   \n[27] Z. Ghahramani. Variational learning for switching state-space models. Neural Computation, 12 (4):831\u2013864, 2000.   \n[28] R. Henson and K. Friston. Chapter 14 - Convolution Models for fMRI. In Statistical Parametric Mapping, pages 178\u2013192. Academic Press, London, 2007.   \n[29] D. Hernandez, A. K. Moretti, Z. Wei, S. Saxena, J. Cunningham, and L. Paninski. Nonlinear evolution via spatially-dependent linear dynamics for electrophysiology and calcium data. Neurons, Behavior, Data analysis, and Theory, 3(3), 2020.   \n[30] J. R. Hershey and P. A. Olsen. Approximating the Kullback Leibler divergence between Gaussian mixture models. In International Conference on Acoustics, Speech and Signal Processing, volume 4, pages 317\u2013320. IEEE, 2007.   \n[31] F. Hess, Z. Monfared, M. Brenner, and D. Durstewitz. Generalized teacher forcing for learning chaotic dynamics. In International Conference on Machine Learning, volume 202, page 13017\u201313049. PMLR, 2023.   \n[32] R. Hindriks, M. H. Adhikari, Y. Murayama, M. Ganzetti, D. Mantini, N. K. Logothetis, and G. Deco. Can sliding-window correlations reveal dynamic functional connectivity in restingstate fMRI? NeuroImage, 127:242\u2013256, 2016.   \n[33] E. M. Izhikevich. Dynamical Systems in Neuroscience: The Geometry of Excitability and Bursting. MIT press, 2006.   \n[34] R. Jiang, P. Y. Lu, E. Orlova, and R. Willett. Training neural operators to preserve invariant measures of chaotic attractors. In Advances in Neural Information Processing Systems, volume 36, pages 27645\u201327669, 2023.   \n[35] V. K. Jirsa and H. Haken. Field theory of electromagnetic brain activity. Physical Review Letters, 77(5):960, 1996.   \n[36] S. Keilholz, E. Maltbie, X. Zhang, B. Yousef,i W.-J. Pan, N. Xu, M. Nezafati, T. J. LaGrow, and Y. Guo. Relationship between basic properties of BOLD fluctuations and calculated metrics of complexity in the human connectome project. Frontiers in Neuroscience, 14:550923, 2020.   \n[37] S. J. Kiebel, M. I. Garrido, R. J. Moran, and K. J. Friston. Dynamic causal modelling for EEG and MEG. Cognitive Neurodynamics, 2:121\u2013136, 2008.   \n[38] D. Kingma and M. Welling. Auto-encoding variational bayes. In International Conference on Learning Representations. ICLR, 2014.   \n[39] J.-H. Ko, H. Koh, N. Park, and W. Jhe. Homotopy-based training of NeuralODEs for accurate dynamics discovery. In Advances in Neural Information Processing Systems, volume 36, pages 64725\u201364752, 2023.   \n[40] G. Koppe, H. Toutounji, P. Kirsch, S. Lis, and D. Durstewitz. Identifying nonlinear dynamical systems via generative recurrent neural networks with applications to fMRI. PLoS Computational Biology, 15(8):e1007263, 2019.   \n[41] D. Kramer, P. L. Bommer, C. Tombolini, G. Koppe, and D. Durstewitz. Reconstructing nonlinear dynamical systems from multi-modal time series. In International Conference on Machine Learning, volume 162, page 11613\u201311633. PMLR, 2022.   \n[42] F. Lejarza and M. Baldea. Data-driven discovery of the governing equations of dynamical systems via moving horizon optimization. Scientific Reports, 12(1):11836, 2022.   \n[43] S. Linderman, M. Johnson, A. Miller, R. Adams, D. Blei, and L. Paninski. Bayesian learning and inference in recurrent switching linear dynamical systems. In Artificial Intelligence and Statistics, pages 914\u2013922. PMLR, 2017.   \n[44] S. W. Linderman, A. C. Miller, R. P. Adams, D. M. Blei, L. Paninski, and M. J. Johnson. Recurrent switching linear dynamical systems. In Advances in Neural Information Processing Systems, volume 33, pages 14867\u201314878, 2020.   \n[45] E. N. Lorenz. Deterministic nonperiodic flow. Journal of Atmospheric Sciences, 20(2):130\u2013141, 1963.   \n[46] A. C. Marreiros, S. J. Kiebel, and K. J. Friston. Dynamic causal modelling for fMRI: A two-state model. NeuroImage, 39(1):269\u2013278, 2008.   \n[47] J. Mikhaeil, Z. Monfared, and D. Durstewitz. On the difficulty of learning chaotic dynamics with RNNs. In Advances in Neural Information Processing Systems, volume 35, pages 11297\u201311312, 2022.   \n[48] C. Pandarinath, D. J. O\u2019Shea, J. Collins, R. Jozefowicz, S. D. Stavisky, J. C. Kao, E. M. Trautmann, M. T. Kaufman, S. I. Ryu, L. R. Hochberg, et al. Inferring single-trial neural population dynamics using sequential auto-encoders. Nature Methods, 15(10):805\u2013815, 2018.   \n[49] J. Pathak, B. Hunt, M. Girvan, Z. Lu, and E. Ott. Model-free prediction of large spatiotemporally chaotic systems from data: A reservoir computing approach. Physical Review Letters, 120(2): 024102, 2018.   \n[50] L. Perko. Differential equations and dynamical systems, volume 7. Springer Science & Business Media, 2013.   \n[51] A. Pikovsky, M. Rosenblum, and J. Kurths. Synchronization: A universal concept in nonlinear sciences. Cambridge Nonlinear Science Series. Cambridge University Press, 2001.   \n[52] J. A. Platt, S. G. Penny, T. A. Smith, T.-C. Chen, and H. D. Abarbanel. Constraining chaos: Enforcing dynamical invariants in the training of reservoir computers. Chaos: An Interdisciplinary Journal of Nonlinear Science, 33(10):103107, 2023.   \n[53] M. I. Rabinovich, P. Varona, A. I. Selverston, and H. D. Abarbanel. Dynamical principles in neuroscience. Reviews of Modern Physics, 78(4):1213, 2006.   \n[54] J. P. Ramirez-Mahaluf, A. Roxin, H. S. Mayberg, and A. Compte. A computational model of major depression: the role of glutamate dysfunction on cingulo-frontal network dynamics. Cerebral Cortex, 27(1):660\u2013679, 2017.   \n[55] D. J. Rezende, S. Mohamed, and D. Wierstra. Stochastic backpropagation and approximate inference in deep generative models. In International Conference on Machine Learning, volume 32, pages 1278\u20131286. PMLR, 2014.   \n[56] P. Ritter, M. Schirner, A. R. McIntosh, and V. K. Jirsa. The virtual brain integrates computational modeling and multimodal neuroimaging. Brain Connectivity, 3(2):121\u2013145, 2013.   \n[57] N. A. Roy, J. H. Bak, A. Akrami, C. Brody, and J. W. Pillow. Efficient inference for time-varying behavior during learning. In Advances in Neural Information Processing systems, volume 31, pages 5695\u20135705, 2018.   \n[58] P. Sanz-Leon, S. Knock, M. Woodman, L. Domide, J. Mersmann, A. McIntosh, and V. Jirsa. The Virtual Brain: a simulator of primate brain network dynamics. Frontiers in Neuroinformatics, 7: 10, 2013.   \n[59] J. Schmidhuber, S. Hochreiter, et al. Long short-term memory. Neural Computation, 9(8): 1735\u20131780, 1997.   \n[60] D. Schmidt, G. Koppe, Z. Monfared, M. Beutelspacher, and D. Durstewitz. Identifying nonlinear dynamical systems with multiple time scales and long-range dependencies. In International Conference on Learning Representations, 2021.   \n[61] A. R. Sedler and C. Pandarinath. lfads-torch: A modular and extensible implementation of latent factor analysis via dynamical systems. arXiv preprint arXiv:2309.01230, 2023.   \n[62] M. Singh, T. Braver, M. Cole, and S. Ching. Estimation and validation of individualized dynamic brain models with resting state fMRI. NeuroImage, 221:117046, 2020.   \n[63] V. Sip, M. Hashemi, T. Dickscheid, K. Amunts, S. Petkoski, and V. Jirsa. Characterization of regional differences in resting-state fMRI with a data-driven network model of brain dynamics. Science Advances, 9(11):eabq7547, 2023.   \n[64] A. C. Smith and E. N. Brown. Estimating a state-space model from point process observations. Neural computation, 15(5):965\u2013991, 2003.   \n[65] S. H. Strogatz. Nonlinear dynamics and chaos: With applications to physics, biology, chemistry, and engineering. CRC press, 2018.   \n[66] J. Thome, R. Steinbach, J. Grosskreutz, D. Durstewitz, and G. Koppe. Classification of amyotrophic lateral sclerosis by brain volume, connectivity, and network dynamics. Human Brain Mapping, 43(2):681\u2013699, 2022.   \n[67] C. van Vreeswijk and H. Sompolinsky. Chaos in neuronal networks with balanced excitatory and inhibitory activity. Science, 274(5293):1724\u20131726, 1996.   \n[68] P. Verzelli, C. Alippi, and L. Livi. Learn to synchronize, synchronize to learn. Chaos: An Interdisciplinary Journal of Nonlinear Science, 31(8), 2021.   \n[69] P. R. Vlachas, W. Byeon, Z. Y. Wan, T. P. Sapsis, and P. Koumoutsakos. Data-driven forecasting of high-dimensional chaotic systems with long short-term memory networks. Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences, 474(2213):20170844, 2018.   \n[70] P. R. Vlachas, J. Pathak, B. R. Hunt, T. P. Sapsis, M. Girvan, E. Ott, and P. Koumoutsakos. Backpropagation algorithms and reservoir computing in recurrent neural networks for the forecasting of complex spatiotemporal dynamics. Neural Networks, 126:191\u2013217, 2020.   \n[71] P. R. Vlachas, G. Arampatzis, C. Uhler, and P. Koumoutsakos. Multiscale simulations of complex systems by learning their effective dynamics. Nature Machine Intelligence, 4(4): 359\u2013366, 2022.   \n[72] J. T. Vogelstein, A. M. Packer, T. A. Machado, T. Sippy, B. Babadi, R. Yuste, and L. Paninski. Fast nonnegative deconvolution for spike train inference from population calcium imaging. Journal of Neurophysiology, 104(6):3691\u20133704, 2010.   \n[73] R. Vogt, M. Puelma Touzel, E. Shlizerman, and G. Lajoie. On Lyapunov exponents for RNNs: Understanding information propagation using dynamical systems tools. Frontiers in Applied Mathematics and Statistics, 8:818799, 2022.   \n[74] X.-J. Wang. Probabilistic decision making by slow reverberation in cortical circuits. Neuron, 36(5):955\u2013968, 2002.   \n[75] N. Wiener. Extrapolation, interpolation, and smoothing of stationary time series: With engineering applications. MIT Press, 1949.   \n[76] H. R. Wilson and J. D. Cowan. Excitatory and inhibitory interactions in localized populations of model neurons. Biophysical Journal, 12(1):1\u201324, 1972.   \n[77] S. N. Wood. Statistical inference for noisy nonlinear ecological dynamic systems. Nature, 466 (7310):1102\u20131104, 2010.   \n[78] G.-R. Wu, N. Colenbier, S. Van Den Bossche, K. Clauw, A. Johri, M. Tandon, and D. Marinazzo. rsHRF: A toolbox for resting-state HRF estimation and deconvolution. NeuroImage, 244: 118591, 2021.   \n[79] E. Yaksi and R. W. Friedrich. Reconstruction of firing rate changes across neuronal populations by temporally deconvolved $\\mathrm{{Ca^{2+}}}$ imaging. Nature Methods, 3(5):377\u2013383, 2006.   \n[80] L. Yang, X. Sun, B. Hamzi, H. Owhadi, and N. Xie. Learning dynamical systems from data: A simple cross-validation perspective, part V: Sparse kernel flows for 132 chaotic dynamical systems. arXiv preprint arXiv:2301.10321, 2023.   \n[81] B. M. Yu, J. P. Cunningham, G. Santhanam, S. Ryu, K. V. Shenoy, and M. Sahani. Gaussianprocess factor analysis for low-dimensional single-trial analysis of neural population activity. In Advances in Neural Information Processing Systems, volume 21, pages 1881\u20131888, 2008. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "A Further methodological details ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A.1 Deconvolution algorithm in convSSM ", "page_idx": 15}, {"type": "text", "text": "Algorithm 1 Deconvolution in convSSM Input: $\\mathcal{X}$ : measured time series as $(N\\times T)$ matrix Parameters: $\\psi$ : analyzing wavelet $\\tilde{\\sigma}_{m i n}$ : minimum noise level cutl, $c u t_{r}$ : edge cutoffs hr $f_{t}$ : kernel of hemodynamic response function Output: $\\chi_{d e c o n v}$ : deconvolved time series as $(N\\times T)$ matrix Initialize $\\ X_{d e c o n v}:={\\mathrm{zeros}}\\left(N,T\\right)$ for $i=1$ to $N$ do $x_{t}:=\\mathcal{X}[i,:]$ $\\tilde{\\sigma},\\tilde{z}_{t}:=\\boldsymbol{\\mathrm{VISUSHRINK}}(x_{t},\\psi)$ if $\\tilde{\\sigma}<\\tilde{\\sigma}_{m i n}$ then $\\tilde{\\sigma}:=\\tilde{\\sigma}_{m i n}$ end if Compute Fourier transforms ${\\mathcal{F}}\\{\\cdot\\}$ and expectation values of spectral densities $\\mathbb{E}[|{\\mathcal{F}}\\{\\cdot\\}|^{2}]$ $\\begin{array}{r l}&{X_{k}:=\\mathcal{F}\\left\\{x_{t}\\right\\}}\\\\ &{H R F_{k}:=\\mathcal{F}\\left\\{h r f_{t}\\right\\}}\\\\ &{N_{k}:=\\mathbb{E}\\bigl[|\\mathcal{F}\\left\\{\\eta_{t}\\right\\}|^{2}\\bigr]}\\\\ &{S_{k}:=\\mathbb{E}\\bigl[|\\mathcal{F}\\left\\{\\tilde{z}_{t}\\right\\}|^{2}\\bigr]}\\end{array}$ Compute and apply Wiener filter $\\begin{array}{r}{\\stackrel{\\cdot}{W_{k}}:=\\frac{H R\\dot{F_{k}^{*}}\\cdot\\dot{S_{k}}}{|H R F_{k}|^{2}\\cdot S_{k}+N_{k}}}\\\\ {\\tilde{Z}_{k}:=\\dot{W_{k}}\\cdot X_{k}\\qquad\\qquad\\qquad\\quad}\\end{array}$ Transform back to time domain $\\widetilde{\\boldsymbol{x}}_{t}:=\\mathcal{F}^{-1}\\left\\{\\widetilde{Z}_{k}\\right\\}$ Remove signal edges $\\tilde{x}_{t}[\\mathsf{b e g i n}+c u t_{l}:\\mathsf{e n d}]:=\\mathrm{NaN}$ $\\tilde{x}_{t}[\\mathrm{begin:end}-c u t_{r}]:=\\mathrm{NaN}$ $\\mathcal{X}_{d e c o n v}[i,:]:=\\tilde{x}_{t}$ end for ", "page_idx": 15}, {"type": "text", "text": "A.2 Scalability ", "text_level": 1, "page_idx": 16}, {"type": "image", "img_path": "exATQD4HSv/tmp/617690908437e38de30cbdfcd6b7b375ff541f48054fd86181cec7c9ff74b06a.jpg", "img_caption": [], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "Figure 4: Training duration per epoch (y-axis) in seconds for different TRs (A), hidden dimensions L (B), latent dimensions M (C), time series length (D), and observation dimensions (E). Mean, standard error (SEM) and linear curve ftis (gray dashed lines) are displayed. The per-epoch-runtime increases approximately linearly with dimensions $L$ , $M$ , and $N$ ; explained variance $R_{L}^{2}=0.989$ , $R_{M}^{2}=0.993$ and $R_{N}^{2}=0.996$ for linear regressions with predictors $L'$ , $M^{\\star}$ , and \u2019 $N^{\\bullet}$ , respectively. Experiments were performed on a standard notebook with Intel i5-8250U 1,60 GHz CPU and 8GB RAM. ", "page_idx": 16}, {"type": "text", "text": "A.3 PLRNNs ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "The simplest form of the PLRNN is given by ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{z_{t}=\\boldsymbol{A}z_{t-1}+\\boldsymbol{W}\\boldsymbol{\\Phi}(z_{t-1})+\\boldsymbol{h}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $z_{t}\\in\\mathbb{R}^{M}$ is a latent state vector at time $t$ , $\\Phi(\\cdot)=\\operatorname*{max}(0,\\cdot)$ is the ReLU activation function, $W\\in\\mathbb{R}^{M\\times M}$ is an off-diagonal matrix of connection weights and $A\\in\\mathbb{R}^{M\\times M}$ a diagonal matrix containing the autoregressive weights [21]. ", "page_idx": 16}, {"type": "text", "text": "Neurobiologically motivated, the entries of the latent state $z_{i,t}$ may be interpreted as membrane potentials, the diagonal elements in $\\pmb{A}$ as the neurons\u2019 individual membrane time constants, and the off-diagonal elements in $W$ as synaptic connections between neurons. The ReLU activation emulates that neurons only start spiking above a certain firing threshold. ", "page_idx": 16}, {"type": "text", "text": "By adding one hidden layer, we obtain the so-called shallow PLRNN ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{z_{t}=\\mathbf{A}z_{t-1}+W_{1}\\phi\\left(W_{2}z_{t-1}+h_{2}\\right)+h_{1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "with $z_{t}\\,\\in\\,\\mathbb{R}^{M}$ latent states at time $t$ , $\\phi(\\cdot)=\\mathrm{ReLU}(\\cdot)$ , diagonal matrix $A\\in\\mathbb{R}^{M\\times M}$ , rectangular connectivity matrices $W_{1}\\in\\mathbb{R}^{M\\times L}$ and $\\dot{W}_{2}\\in\\mathbb{R}^{L\\times\\dot{M}}$ and thresholds $h_{1}\\in\\mathbb{R}^{M}$ and $h_{2}\\in\\mathbb{R}^{L}$ . ", "page_idx": 16}, {"type": "text", "text": "A.4 Wiener filter ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "The Wiener deconvolution fliter is typically described in the frequency domain, and, for the setting in Equation 6 and Equation 7, is given by ", "page_idx": 16}, {"type": "equation", "text": "$$\nW_{k}=\\frac{H R F_{k}^{*}S_{k}}{|H R F_{k}|^{2}S_{k}+N_{k}}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and returns the estimate $\\hat{z}_{t}$ as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\hat{z}_{t}=\\mathcal{F}^{-1}(W_{k}X_{k})=\\big(\\mathrm{Conv}^{-1}(\\{x_{t^{\\prime}}\\},h r f)\\big)_{t}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "\u2022 where time series denoted with capital letters correspond to the Fourier transformation of time series denoted by lowercase letters, i.e., $\\{X_{k}\\}\\stackrel{=}{=}\\mathcal{F}(\\{x_{t}\\})$ ,   \n\u2022 $W_{k}$ is the Wiener filter,   \n\u2022 $S_{k}=\\mathbb{E}[|Z_{k}|^{2}]$ is the mean power spectral density of the original signal $z_{t}$ , ", "page_idx": 16}, {"type": "text", "text": "\u2022 $N_{k}=\\mathbb{E}[|H_{k}|^{2}]$ is the mean power spectral density of the noise $\\eta_{t}$ , \u2022 the superscript \u2217denotes complex conjugation, and \u2022 $\\mathcal{F}^{-1}$ is the inverse Fourier transform. ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "The noise spectrum $N_{k}$ is typically unknown in practice, but can be reliably estimated based on the median estimator on the finest scale wavelet coefficients of $x_{t}$ . As approximation to the power spectrum of the original signal, we use the denoised signal $\\tilde{x}_{t}$ which we obtain by applying the VISUSHRINK algorithm [18], Algorithm 2, to the observed signal $x_{t}$ . This approximation works well in practice in absence of knowledge about the true underlying signal. ", "page_idx": 17}, {"type": "text", "text": "Algorithm 2 VISUSHRINK algorithm ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Input: time series data $\\left\\{x_{t}\\right\\}$ of length $N$ ; analyzing wavelet $\\psi$ ", "page_idx": 17}, {"type": "text", "text": "1. Apply the discrete wavelet transformation (DWT) to the input data $\\left\\{x_{t}\\right\\}$ to obtain the wavelet coefficients $\\Theta_{t}^{0}$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\Theta_{t}=\\mathcal{W}_{\\psi}\\{x_{t}\\}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "2. Calculate the MAD of the fine scale coefficients $\\Theta_{t}^{0}$ , the estimate of the noise level $\\tilde{\\sigma}$ and the universal estimator $\\lambda_{U}$ . Let $\\bar{\\Theta}=\\mathrm{median}(\\Theta_{t}^{0})$ be the median of the fine scale coefficients and $N$ the length of the time series $\\left\\{x_{t}\\right\\}$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{MAD}=\\mathrm{median}(|\\Theta_{t}^{0}-\\bar{\\Theta}|)}\\\\ &{\\quad\\quad\\tilde{\\sigma}=\\frac{\\mathrm{MAD}}{0.6745}}\\\\ &{\\quad\\quad\\lambda_{U}=(2\\ln N)^{1/2}\\tilde{\\sigma}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "3. Apply hard thresholding to the fine scale wavelet coefficients ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\Theta_{t}^{1}=\\left\\{0,\\quad|\\Theta_{t}^{0}|<\\lambda_{U}\\right.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "4. Apply the inverse DWT to the tresholded coefficients $\\Theta_{t}^{1}$ to obtain an estimate of the denoised signal $\\left\\{\\tilde{x}_{t}\\right\\}$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\{\\tilde{x}_{t}\\}=\\mathcal{W}_{\\psi}^{-1}\\{\\Theta_{t}^{1}\\}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Output: estimate of the noise level $\\tilde{\\sigma}$ ; estimate of denoised signal $\\left\\{\\tilde{x}_{t}\\right\\}$ ", "page_idx": 17}, {"type": "text", "text": "A.5 Additional details on BOLD observation model ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "The discrete convolution of two time series $\\{f_{t}\\}$ and $\\left\\{g_{t}\\right\\}$ in its general definition is given by ", "page_idx": 17}, {"type": "equation", "text": "$$\n(f*g)_{t}=\\sum_{s=-\\infty}^{\\infty}f_{s}\\cdot g_{t-s}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "which in case of the decoder model given in Equation 5 translates to ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\left(h r f*z\\right)_{t}=\\sum_{s=0}^{\\tau}h r f_{s}\\cdot z_{t-s}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "due to the finite non-zero value of $\\tau$ , i.e., the maximum time difference at which a previous state $z_{t-\\tau}$ still influences the current state $z_{t}$ according to the $h r f$ . Furthermore, due to causality, only past states are allowed to influence the current state. ", "page_idx": 17}, {"type": "text", "text": "Since the convolution operation as well as matrix multiplication are linear, the order of convolution and matrix multiplication in Equation 5 can be inverted to yield Equation 9. This can be seen more easily if we consider a decoder model without noise ", "page_idx": 17}, {"type": "equation", "text": "$$\nx_{t}=B(h r f*z)_{t}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Due to the $h r f$ being the same for all latent dimensions (i.e., a scalar and not a function), we interchange the matrix multiplication with $_B$ and the convolution with the $h r f$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\cal B}(h r f*z)_{t}={\\cal B}\\sum_{s=0}^{\\tau}h r f_{s}\\cdot z_{t-s}}\\ ~~}\\\\ {{\\displaystyle({\\cal B}(h r f*z)_{t})_{i}=\\sum_{j=1}^{M}{\\cal B}_{i j}\\sum_{s=0}^{\\tau}h r f_{s}\\cdot z_{t-s,j}=\\sum_{s=0}^{\\tau}h r f_{s}\\sum_{j=1}^{M}{\\cal B}_{i j}\\cdot z_{t-s,j}=\\sum_{s=0}^{\\tau}h r f_{s}({\\cal B}\\cdot z_{t-s})_{i}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\nB(h r f*z)_{t}=(h r f*(B z))_{t}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Consequentially, we divide the observations into two parts ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{c}{x_{t}=\\left(h r f*x^{\\mathrm{deconv}}\\right)_{t}\\Leftrightarrow\\{x_{t}\\}=\\mathrm{Conv}\\big(\\{x_{t^{\\prime}}^{\\mathrm{deconv}}\\},h r f\\big)}\\\\ {x_{t}^{\\mathrm{deconv}}=B z_{t}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The second part, Equation 28, is of the same form as Equation 2 and permits the same inversion.   \nTherefore, by computing $\\left\\{{x_{t}^{\\mathrm{{deconv}}}}\\right\\}$ from $\\left\\{x_{t}\\right\\}$ once, we can perform GTF as in the standard SSM. ", "page_idx": 18}, {"type": "text", "text": "To include the nuisance artifacts $\\{r_{t}\\}$ , one has to also swap the order of the matrix multiplication and the convolution in the full decoder model (Equation 5). This poses a problem since the $\\{r_{t}\\}$ are not convolved. Our solution is deconvolving the $\\{r_{t}\\}$ time series as well ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{J r_{t}=\\left\\{h r f*\\left(J r^{\\mathrm{deconv}}\\right)\\right\\}_{t}}\\\\ {r_{s}^{\\mathrm{deconv}}=\\mathrm{Conv}^{-1}(\\{r_{t}\\},h r f)_{s}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We use this as a simple mathematical trick to incorporate the artifacts into the convolution ", "page_idx": 18}, {"type": "equation", "text": "$$\nx_{t}=B(h r f*z)_{t}+J r_{t}=(h r f*B z)_{t}+\\left(h r f*\\left(J r^{\\mathrm{deconv}}\\right)\\right)_{t}=\\left(h r f*\\left(B z+J r^{\\mathrm{deconv}}\\right)\\right)_{t}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Finally, we obtain the relation between the deconvolved time series and the latent states as ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{x_{t}^{\\mathrm{deconv}}=B z_{t}+J r_{t}^{\\mathrm{deconv}}}\\\\ &{~~~~~~z_{t}=B^{+}\\left(x_{t}^{\\mathrm{deconv}}-J r_{t}^{\\mathrm{deconv}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "A.6 Additional information on Algorithm 1 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In order to deal with numerical instabilities, additional hyperparameters were introduced. A too low noise level $\\tilde{\\sigma}$ determined by the VISUSHRINK Algorithm 2 can lead to high frequency artifacts, which can be dealt with by defining a lower noise level boundary $\\tilde{\\sigma}_{m i n}$ . Although this is unlikely to occur in empirical (noisy) data, the lower noise level boundary helps to study artificial noise free data. Since the convolution treats the finite signal as periodic, artifacts at the boundaries of the computed deconvolved signal $\\boldsymbol x_{t}^{\\mathrm{deconv}}$ may occur. With the hyperparameters cut_l, cut_r (corresponding to start and end of signal, respectively) one can therefore define absolute cutoff times, either by integer or by floating point values (if, e.g., a cutoff time is to be defined relative to the length of the $h r f$ ). ", "page_idx": 18}, {"type": "text", "text": "A.7 Performance measures ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "If the maximum Lyapunov exponent $\\lambda_{\\mathrm{max}}$ of a dynamical system is larger than 0, a necessary condition for chaos, nearby trajectories will diverge exponentially. This limits the applicability of $n$ -step ahead prediction errors (PEs), as conventionally used in machine learning, to evaluate model performance, as even small numerical errors will lead to exponentially growing PEs. In processes in which we can expect chaotic behavior (like neural recordings), we therefore need performance measures which are insensitive to a system\u2019s initial conditions and yet capture the most relevant dynamical properties. Here, we use two established measures [40, 47] to evaluate the DSR, the state space divergence, capturing geometric overlap of (ergodic) distributions in state space, and the power spectrum error, capturing agreement in long-term temporal properties. ", "page_idx": 18}, {"type": "text", "text": "On the ALN and LEMON benchmark, we computed these two measures as average over 100 trajectories, generated by perturbing the initial state with a small Gaussian noise term $\\scriptstyle\\left\\langle\\mu\\right.\\,=\\,0$ , $\\sigma=.01$ ). ", "page_idx": 18}, {"type": "text", "text": "A.7.1 Prediction Error $P E$ ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "The $n$ -step prediction error is given by ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathrm{PE}(n)=\\frac{1}{N(T-n)}\\sum_{t=1}^{T-n}\\|x_{t+n}-\\hat{x}_{t+n}\\|_{2}^{2},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "i.e. , the mean squared error between ground truth data $\\left\\{x_{t}\\right\\}$ and $n$ -step ahead predictions of the model $\\left\\{\\hat{x}_{t}\\right\\}$ . ", "page_idx": 19}, {"type": "text", "text": "A.7.2 State space divergence $D_{s t s p}$ ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Given an observed $N$ -dimensional time series $\\left\\{x_{t}\\right\\}$ of length $T$ and a time series $\\left\\{\\hat{x}_{t}\\right\\}$ with the same dimension/length generated by a model, $D_{s t s p}$ measures the geometrical overlap of orbits in state space [40]. ", "page_idx": 19}, {"type": "text", "text": "For low dimensional systems, $N\\leq6$ , the state space is segregated into $k^{N}$ bins where $k$ is the number of bins per dimension. Each bin is given an index $i$ and we count the number of times $n_{i}$ the time series visited bin $i$ . The relative frequency of visits is then obtained by dividing by time series length $T$ ", "page_idx": 19}, {"type": "equation", "text": "$$\np_{i}={\\frac{n_{i}}{T}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Combining these frequencies across all bins in space results in a probability distribution which approximates the state space distribution (the occupation measure) of the underlying dynamical system. The Kullback-Leibler divergence of these empirical distributions can then be computed to assess the overlap of both systems in their state space geometry, ", "page_idx": 19}, {"type": "equation", "text": "$$\nD_{s t s p}=\\sum_{i=1}^{k^{N}}p_{i}\\log\\left(\\frac{p_{i}}{q_{i}}\\right)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $p_{i}$ are the relative frequencies of the observed and $q_{i}$ of the predicted (generated) time series. Note that $D_{s t s p}$ is not a metric in the mathematical sense, but a divergence that assesses the (dis)agreement of probability distributions. ", "page_idx": 19}, {"type": "text", "text": "The complexity of this binning approach scales exponentially with the observation dimension $N$ and thus becomes intractable for larger $N$ . To compute $D_{s t s p}$ in higher-dimensional systems, [6] use Gaussian mixture models (GMMs) with centers (means) $x_{t}$ and diagonal covariance $\\textbf{\\Sigma}=$ $\\mathrm{diag}(\\sigma^{2},\\cdots,\\sigma^{2})$ , where $\\sigma$ is a hyperparameter. The GMMs along the trajectory points are given by ", "page_idx": 19}, {"type": "equation", "text": "$$\nf(y)=\\frac{1}{T}\\sum_{t=1}^{T}\\mathcal{N}(y;x_{t},\\Sigma).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Following [30], the Kullback-Leibler divergence of the two GMMs can be computed using a MonteCarlo sampling approach ", "page_idx": 19}, {"type": "equation", "text": "$$\nD_{s t s p}\\approx\\frac{1}{K}\\sum_{i=1}^{K}\\mathrm{log}\\bigg(\\frac{f_{o b s}(y_{i})}{f_{g e n}(y_{i})}\\bigg)=\\frac{1}{K}\\sum_{i=1}^{K}\\mathrm{log}\\bigg(\\frac{1/T\\sum_{t=1}^{T}\\mathcal{N}\\left(y_{i};x_{t};\\mathbf{\\vec{Z}}\\right)}{1/T\\sum_{t=1}^{T}\\mathcal{N}\\left(y_{i};\\hat{x}_{t};\\mathbf{\\vec{Z}}\\right)}\\bigg)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $K$ is the number of samples drawn, $f_{o b s}$ is the distribution of the observed time series, and $f_{g e n}$ is the distribution of the generated time series. The binning and GMM-based measures correlate strongly in low dimensions (whereas determining the correlation in high dimensions is challenging for the stated reasons). ", "page_idx": 19}, {"type": "text", "text": "A.7.3 Power spectrum error $D_{P S E}$ ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "The state space measure introduced above discards all temporal structure in the data. To include temporal information in the model evaluation, we compare the power spectra of observed and ", "page_idx": 19}, {"type": "text", "text": "generated time series. For each dimension $i\\in\\{1,\\cdots\\,,N\\}$ , the scalar time series $\\{x_{i,t}\\}$ is converted into a power spectrum density (PSD) $\\{S_{i,k}\\}$ . The components are computed from the Fourier transform of $\\{\\bar{x_{i,t}}\\}$ ", "page_idx": 20}, {"type": "equation", "text": "$$\nS_{i,k}=\\frac{\\left|\\widehat{x}_{i,k}\\right|}{T}=\\frac{\\left|\\mathcal{F}\\{x_{i,t}\\}_{k}\\right|}{T}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "The agreement in power spectra is then evaluated in terms of the Hellinger Distance (HD) as suggested by [47]. Since HD is a probability measure, the computed power spectra have to be normalized by ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\bar{S}_{i,k}=\\frac{S_{i,k}}{\\sum_{j=1}^{T}{S_{i,j}}}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Using the power spectra of the observed time series $p_{i,k}=\\bar{S}_{i,k}\\big(\\{x_{i,t}\\}\\big)$ and the generated time series $q_{i,k}\\overset{\\cdot}{=}\\bar{S}_{i,k}\\overset{\\cdot}{(\\{\\hat{x}_{i,t}\\})}$ , the HD is then assessed as ", "page_idx": 20}, {"type": "equation", "text": "$$\nD_{H,i}=\\sqrt{1-\\sum_{k=1}^{T}{\\sqrt{p_{i,k}q_{i,k}}}}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "The power spectrum error $(D_{P S E})$ is computed by averaging the HDs across all $N$ dimensions of the observed system: ", "page_idx": 20}, {"type": "equation", "text": "$$\nD_{P S E}=\\frac{1}{N}\\sum_{i=1}^{N}D_{H,i}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Before analysis, the power spectra usually have to be smoothed with a Gaussian kernel of width $\\sigma$ to reduce noise. $\\sigma$ can thus be considered a hyperparameter in the evaluation process and was set to $\\sigma=1$ in the current work as in [6]. ", "page_idx": 20}, {"type": "text", "text": "B Details on dynamical systems benchmarks ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "B.1 Lorenz63 system ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "The Lorenz63 introduced in [45] was designed to describe atmospheric convection based on three dynamic variables. A two-dimensional fluid layer is uniformly warmed from below and cooled from above. It is a continuous-time dynamical system given by the following set of differential equations: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle{\\frac{d x_{1}}{d t}=\\sigma(x_{2}-x_{1})}}\\\\ {\\displaystyle{\\frac{d x_{2}}{d t}=x_{1}(\\rho-x_{3})-x_{2}}}\\\\ {\\displaystyle{\\frac{d x_{3}}{d t}=x_{1}x_{2}-\\beta x_{3}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $x_{1}$ is proportional to the rate of convection, $x_{2}$ to the horizontal temperature variation, and $x_{3}$ to the vertical temperature variation. The constants $\\sigma,\\rho$ and $\\beta$ are system parameters proportional to the Prandtl number, Rayleigh number, and certain physical dimensions of the fluid-layer. For chaotic behavior, $\\sigma=10$ , $\\rho\\,=\\,28$ and $\\beta\\,=\\,{\\frac{8}{3}}$ are typical settings. These settings produce the so-called \"butterfly attractor\" characteristic of the Lorenz system. For each data set, we drew random initial conditions $\\pmb{x}_{0}\\sim\\mathcal{N}(0,1_{3\\times3})$ and simulated the system using the DynamicalSystems.jl Julia package, ([15]). $10^{5}$ time steps were saved and used as data set ( $1:1$ training vs. test split) after discarding the first $1,000$ time points to remove transients from the data. To create the benchmark data sets, these trajectories were then convolved with the $h r f_{T R}$ function. Gaussian white noise drawn from $\\mathcal{N}(0,\\sigma\\bar{1_{3\\times3}})$ was added to all data points. ", "page_idx": 20}, {"type": "text", "text": "Results on the Lorenz63 system are in Table 2. ", "page_idx": 20}, {"type": "table", "img_path": "exATQD4HSv/tmp/a3e847573c045f8a6f4981e6b42c3bfd7d239b7c4f570e7560ae8317db9b6c0a.jpg", "table_caption": ["Table 2: Quantitative comparison between standard SSM and convSSM on noisy Lorenz63 data (Nconverged is the number of converged models). "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "B.2 ALN model ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "The adaptive linear-nonlinear (ALN) cascade model is a population model of spiking neural networks. The dynamical variables of the ALN model describe the average firing rate and other macroscopic variables of a randomly connected, delay-coupled network of excitatory and inhibitory adaptive exponential integrate-and-fire neurons (AdEx) with non-linear synaptic currents [3]. ", "page_idx": 21}, {"type": "text", "text": "We used neurolib to create neural activity generated by an ALN model [12]. In neurolib, the firing rate of the excitatory subpopulation of every brain area is used to simulate the BOLD signal via the Balloon\u2013Windkessel model (for formula see [11]). As an alternative, we implemented the BOLD decoder model (Equation 5) as model linking the latent states $z_{t}$ (i.e., the latent neural activity corresponding to the excitatory firing rates) to the observed BOLD signal. In order to create interesting dynamics, certain values were altered from the authors\u2019 default settings, \u2019sigma_ou\u2019 $=0$ and $\\mathrm{\\Delta}^{\\circ}\\mathrm{b}^{\\circ}=5.0$ . Furthermore, only the first 16 dimensions of the structural connectivity matrix \u2019Cmat\u2019 and the delay matrix \u2019Dmat\u2019 (see explanatory notebook provided by [12]) were used for comparability with the empirical LEMON data set. ", "page_idx": 21}, {"type": "text", "text": "neurolib produces simulated (latent) neural activity with a sampling rate of $0.1\\mathrm{ms}$ . To stay in a comparable regime with the fMRI and Lorenz time series, we chose a sampling rate of 0.5s for the simulated (observed) data. To achieve this without loss of critical information, the neuronal activity as well as the BOLD time series were decimated using a 30 point finite impulse response (FIR) fliter with Hamming window. Furthermore, the neural activity time series was smoothed with a Gaussian kernel with standard deviation $\\sigma=1$ and length 5, and then standardized. ", "page_idx": 21}, {"type": "text", "text": "Outliers in DSR measures (Table 3) were removed using the interquartile range (IQR) method. The IQR method considers values as outliers if they are 1.5 IQRs above the third $(Q_{3})$ or below the first $(Q_{1})$ quantile (where $I Q R=Q_{3}-Q_{1})$ . ", "page_idx": 21}, {"type": "text", "text": "In Table 3 and corresponding Figure 2D, we present the quantitative comparison between the standard SSM, the convSSM, the convSSM without GTF, and two benchmark conditions. Figure 5 depicts correlations between measures computed on different subsets of the ALN dataset. ", "page_idx": 21}, {"type": "text", "text": "Table 3: DSR measures evaluated on the ALN data set for the convSSM, the standard SSM, and the convSSM trained without generalized teacher forcing by setting $\\alpha=0$ . Measures were evaluated on the ground truth latent space and the noisy observation space on the different created test sets. ", "page_idx": 22}, {"type": "table", "img_path": "exATQD4HSv/tmp/73ca811746642743125e4f889e31660772f2fa30ad1e4ba0644cafff5d1a8ef3.jpg", "table_caption": [], "table_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "exATQD4HSv/tmp/807177430ffb672dd6ad272cf8bb26a30e649ca545ab79740d86cebcbeef5293.jpg", "img_caption": ["Figure 5: A: Agreement in DSR measures assessed on the observed ( $\\scriptstyle{\\mathrm{~\\vec{~}{\\boldsymbol{x}}~}}$ -axis) vs. latent $_y$ -axis) space of the short pseudo-empirical test set (top) and the full pseudo-empirical time series (bottom). Correlations between $D_{s t s p}$ (left), $D_{P S E}$ (middle), and $P E_{10}$ (right) are displayed, respectively. B: Top: Agreement in DSR measures assessed on the pseudo-empirical test set (short) vs. GT test set (long). Bottom: Same for full pseudo-empirical time series (short) vs. GT test set (long). Correlations between $D_{s t s p}$ (left), $D_{P S E}$ (middle), and $P E_{10}$ (right) are displayed, respectively. C: Correlations between DSR measures between pseudo-empirical test set and full pseudo-empirical time series, same order as in B. "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "B.3 LEMON data set ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "For the LEMON data set, we observed non-stationarity in the data that partly resulted in performance degradation. We therefore only included participants with nearly constant variance over time. To remove non-stationary data sets, we assessed the moving average of the variance over time (window size $w\\,=\\,40$ time steps). We then discarded data sets in which the variance changed with time (assessed by computing the correlation with time, with threshold set to $\\vert r\\vert>.16)$ . ", "page_idx": 24}, {"type": "text", "text": "The LEMON dataset can be found at https://ftp.gwdg.de/pub/misc/MPI-Leipzig_ Mind-Brain-Body-LEMON/. ", "page_idx": 24}, {"type": "text", "text": "B.4 Hyperparameter settings for the different experiments ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Table 4: Hyperparameter settings for the experiments conducted. \u2018Varies\u2019 means the respective hyperparameter was varied in the experiment. ", "page_idx": 24}, {"type": "table", "img_path": "exATQD4HSv/tmp/3f67a4bfb66e3f507b3c62a9cb58f55f938af97905eb7ae5f05257c1579886ea.jpg", "table_caption": [], "table_footnote": ["All experiments were run on a system with a Xeon Gold 6248 CPU and 768 GB of RAM. "], "page_idx": 24}, {"type": "text", "text": "C Further Details ", "text_level": 1, "page_idx": 25}, {"type": "image", "img_path": "exATQD4HSv/tmp/1ee7e74b9a96080057008d52ccc471ba0a4a10b0e494b4b48935ce16bb8d991c.jpg", "img_caption": ["C.1 Illustration of reconstruction measures "], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "Figure 6: A. Ground truth Lorenz trajectory sampled with noise (black), a good reconstruction with low $D_{\\mathrm{stsp}}$ (orange) that accurately recovers the attractor, and a poor reconstruction with high $D_{\\mathrm{stsp}}$ (green) that represents the attractor inaccurately, yielding an oscillatory (limit cycle) instead of a chaotic solution. B. Trajectories of systems in A. unfolded in time. The inaccurate reconstruction (top) achieves a lower prediction error (PE) than the accurate reconstruction (bottom), due to trajectory divergence in chaotic systems. This example illustrates that PEs are inadequate to capture the reconstruction of chaotic DS. ", "page_idx": 25}, {"type": "text", "text": "C.2 Comparison methods ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "MINDy: For MINDy [62] we used the implementation at https://github.com/singhmf/MINDy. We trained models for each subject with the settings provided by the authors for fMRI data. To obtain trajectories for calculating $P E_{10}$ , $D_{P S E}$ , and $D_{s t s p}$ , we used the provided deconvolution function to obtain an initial condition in latent space. We iterated the model forward in time for the latent trajectory and then applied the authors\u2019 observation function to output a BOLD time series which is compared with the test data. ", "page_idx": 25}, {"type": "text", "text": "LFADS: For LFADS [48], we used the lfads-torch implementation at https://github.com/ arsedler9/lfads-torch, which provides an LFADS re-implementation in the deep learning library Pytorch [61]. The default hyperparameters provided are optimized for neural spiking data. We changed the observation model (in the framework this is referred to as reconstruction target) to a Gaussian, and changed the start and stop learning rates from $l r_{s t a r t}=4\\cdot10^{-3}$ , $l r_{e n d}=\\bar{1}\\cdot10^{-5}$ , to $l r_{s t a r t}=4\\cdot10^{-4}$ , $l r_{e n d}=1\\cdot10^{-6}$ , which improved the fit to our data. To obtain trajectories, we iterated the trained models forward in time with initial conditions from the test dataset using the provided model.predict_step function. ", "page_idx": 25}, {"type": "text", "text": "rSLDS: For rSLDS [44], we used the implementation at https://github.com/lindermanlab/ ssm. We trained the rSLDS with the Laplace-EM method with the Structured Mean-Field Posterior, as recommended by the authors, with diagonal_Gaussian dynamics and Gaussian_id emissions. For a fair comparison, we used the same number of latent dimensions as for the observations (same as for the cshPLRNN). We determined the rSLDS training parameters $\\alpha\\,=\\,0.9$ and $K\\,=\\,2$ via grid search over $\\alpha\\in[0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]$ and $K\\in[2,15]$ by inferring systems using a subset of the data and assessing the performance on the held-out set [6]. To generate trajectories for the model comparison, we first approximated the posterior of the test data and then sampled with model.sample using the approximated posteriors $x$ and $z$ and the test data as prefix input for the model. ", "page_idx": 25}, {"type": "text", "text": "C.3 History dependence ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "To illustrate the difference in \u201chistory dependence\" for latent time series $\\left\\{z_{t}\\right\\}$ and convolved counterparts $\\left\\{x_{t}\\right\\}$ , an illustrative example was created: A latent time series $\\left\\{z_{t}\\right\\}$ was produced stochastically ", "page_idx": 25}, {"type": "image", "img_path": "exATQD4HSv/tmp/cf121e6411c8245e4ba497a154544d8d498493696adcb1a29ae09ffb383aeb48.jpg", "img_caption": ["Figure 7: A: Full (solid) and residual (dashed) average (across dimensions) auto-correlation functions for the latent (left) and observed (right) time series. For the residual auto-correlation, the immediately preceding time step was regressed out. For the dotted curve, $\\mathrm{cshPLRNN}(z_{t-1})$ instead of $z_{t-1}$ was regressed out. B: Same as A for the mutual information as a function of time lag. "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "by using a cshPLRNN equipped with a process noise term $\\epsilon_{t}$ , i.e. ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{z_{t}=\\mathrm{cshPLRNN}(z_{t-1})+\\epsilon_{t},~\\epsilon_{t}\\sim\\mathcal{N}(0,\\mathrm{diag}(\\sigma,\\sigma,\\sigma)).}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "The cshPLRNN employed here was trained to become a surrogate model for the Lorenz63 system, and $\\epsilon_{t}$ is Gaussian white noise with standard deviation $\\sigma=0.1$ on each dimension. Note that there are no correlations between noise terms at different time points. A time series of length $T=10^{5}$ was simulated. The corresponding observation time series $\\left\\{x_{t}\\right\\}$ was created by convolving $\\left\\{z_{t}\\right\\}$ with the hemodynamic response function $h r f_{0.5}$ for $\\mathrm{TR}=0.5\\:\\mathrm{s}$ . ", "page_idx": 26}, {"type": "text", "text": "We then computed auto-correlation functions for the actual and a residual time series, where for the latter the linear effect of $z_{t-1}$ on $z_{t}$ (and, likewise, $x_{t-1}$ on $x_{t}$ ) was removed (similar to a partial auto-correlation). As Figure 7A shows, the autocorrelation of the residual time series drops much faster, instantaneously at first, for the latent states (left) as compared to the observed/convolved variables on the right, illustrating the convolution effect is removed in the model\u2019s latent space. It is not completely gone if only linear dependencies are removed \u2013 if the model forwarded-iterated states $\\mathrm{cshPLRNN}(\\Bar{z_{t-1}})$ are regressed out instead, the auto-correlation immediately drops to zero for the latent states (dotted lines in Figure 7, left), as it should by model definition. Likewise, the (nonlinear) mutual information (Figure 7B) shows there are no temporal dependencies left in the residual latent series, while still present in the residual observed series, \u2018empirically\u2019 confirming our approach does what it is supposed to do. ", "page_idx": 26}, {"type": "text", "text": "C.4 Acronyms ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "SSM: State space models   \nDS: Dynamical systems   \nDSR: Dynamical systems reconstruction   \nTF: Teacher forcing   \nBOLD: Blood oxygenation level dependent   \nfMRI: Functional magnetic resonance imaging   \nSLDS): Switching Linear   \nTVB: The Virtual Brain   \nDCM: Dynamic Causal Modeling   \nDL: Deep Learning   \nODE: Ordinary Differential Equation   \nRNN: Recurrent neural network   \nrSLDS: Recurrent SLDS   \nLFADS: Latent Factor Analysis via Dynamical   \nSystems   \nSTF: Sparse TF   \nGTF: Generalized TF   \nHRF: Hemodynamic response function   \nPLRNN: Piecewise linear RNN   \nshPLRNN: Shallow PLRNN   \ncshPLRNN: Clipped shallow PLRNN   \nMSE: Mean squared error   \nSGD: Stochastic gradient descent   \nEVGP: Exploding-and-vanishing gradients   \nproblem   \nSOTA: State of the art   \nTR: Time of repetition   \nPE: Prediction error   \n$D_{\\mathrm{PSE}}$ : Hellinger distance/Power spectrum er  \nror   \n$D_{\\mathrm{stsp}}$ : Kullback Leibler/ State space divergence   \nALN: Adaptive linear-nonlinear cascade model   \n$\\lambda_{\\operatorname*{max}}$ : Maximum Lyapunov exponent GT:   \nGround truth   \nLEMON: \"Leipzig Study for Mind-Body  \nEmotion Interactions\"   \nrsfMRI: Resting state fMRI EEG: Electroen  \ncephalography   \nLSTM: Long Short-Term Memory   \nMLP: Multi layer perceptron   \nBPTT: Backpropagation through time ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: The abstract accurately states the scope of our paper, the introduction and validation of a generative model for dynamical systems reconstruction for BOLD time series. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 28}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: We discuss the limitations of our approach at the end of section 4. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 28}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: Our paper does not include theoretical results. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 29}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We provide the data preprocessing steps and hyperparameters used to train our models, the settings to compute the benchmark datasets with open source libraries and we use a publicly available dataset. We provide code with an implementation of the convSSM framework. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 29}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: All data used in the paper is generated with open source libraries or taken from publicly available datasets. We provide public access to our model implementations and training data. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 30}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: All experimental settings are included either in the main manuscript or in the Appx. subsection B.4, as referenced in the manuscript. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 30}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We include error bars in all figures, where appropriate, report standard deviations in all tables, and use statistical tests to back up claims made in the paper. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 30}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 31}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We include the system specifications of the machines which were used to perform all experiments, see Figure 4 and subsection B.4. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 31}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: The dataset we use containing human subjects was fully anonimized and all subjects gave consent for the the data to be used. The data acquisition experiments were carried out in accordance with the Declaration of Helsinki and the experimental protocol was approved by the ethics committee at the medical faculty of the University of Leipzig (reference number 154/13-ff), see [4] for details. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 31}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 31}, {"type": "text", "text": "Answer: [No] ", "page_idx": 31}, {"type": "text", "text": "Justification: This paper presents a novel algorithm in the context of data-driven dynamical systems reconstruction, with applications to fields such as neuroscience and psychiatry. We believe the primary consequences of the intended applications, such as characterizing human cognition in health and disease, are positive. We therefore point to these applications. However, we cannot entirely rule out the exploitation of such methods for other, currently not known to us, potentially unethical, purposes that may arise in the future. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 32}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [No] ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Justification: We do not release data and we believe our models do not have a high risk for misuse. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 32}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: We cite the libraries used to create our benchmarks and the fMRI dataset used in our experimental section. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 33}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: Along with the manuscript, we upload code for training the convSSM model, as well as evaluation and plotting routines. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 33}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [No] ", "page_idx": 33}, {"type": "text", "text": "Justification: Full experimental details on acquisition of the human publicly available LEMON data set are given in [4], and the dataset is properly referenced in the paper. Here, we only include the information relevant to our analyses and replication. The data set consists of far more details, beyond the scope of the paper. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 33}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 33}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 34}, {"type": "text", "text": "Answer: [No] ", "page_idx": 34}, {"type": "text", "text": "Justification: In this work, we performed data analysis on an anonymized publicly available dataset of human subjects. The data acquisition was carried out in accordance with the Declaration of Helsinki and the data acquisition protocol was approved by the ethics committee at the medical faculty of the University of Leipzig (reference number 154/13- ff) [4]. An ethics approval in Germany requires study participants to be made aware of foreseeable risks. The data is specifically made available for research purposes. To the best of our knowledge, no additional risks were incurred by the analyses presented in this manuscript. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 34}]