[{"figure_path": "MDpIQ9hQ7H/figures/figures_3_1.jpg", "caption": "Figure 1: TTCT overview. TTCT consists of two training components: (1) the text-trajectory alignment component connects trajectory to text with multimodal architecture, and (2) the cost assignment component assigns a cost value to each state-action based on its impact on satisfying the constraint. When training RL policy, the text-trajectory alignment component is used to predict whether a trajectory violates a given constraint and the cost assignment component is used to assign non-violation cost.", "description": "This figure provides a high-level overview of the Trajectory-level Textual Constraints Translator (TTCT) framework.  It shows the two main components: the text-trajectory alignment component and the cost assignment component. The text-trajectory alignment component uses a multimodal architecture to connect trajectories and text, predicting whether a trajectory violates a given constraint. The cost assignment component assigns a cost to each state-action pair based on its impact on constraint satisfaction. During RL policy training, these components work together; the alignment component predicts violations, and the assignment component provides a cost signal for training the policy.", "section": "4 TTCT: Trajectory-level Textual Constraints Translator"}, {"figure_path": "MDpIQ9hQ7H/figures/figures_6_1.jpg", "caption": "Figure 2: (a) One layout in Hazard-World-Grid [46], where orange tiles are lava, blue tiles are water and green tiles are grass. Agents need to collect reward objects in the grid while avoiding violating our designed textual constraint for the entire episode. (b) Robot navigation task Safety Goal that is built in Safety-Gymnasium [19], where there are multiple types of objects in the environment. Agents need to reach the goal while avoiding violating our designed textual constraint for the entire episode. (c) LavaWall [5], a task has the same goal but different hazard objects compared to Hazard-World-Grid.", "description": "This figure shows three different environments used in the paper's experiments: Hazard-World-Grid, SafetyGoal, and LavaWall.  Each environment presents a unique challenge for the RL agent, requiring it to navigate while adhering to specified textual constraints.  The image provides a visual representation of the task layouts, highlighting obstacles (lava, water, hazards) and the agent's goal.", "section": "6 Experiments"}, {"figure_path": "MDpIQ9hQ7H/figures/figures_7_1.jpg", "caption": "Figure 3: Evaluation results of our proposed method TTCT. The blue bars are our proposed cost prediction (CP) mode performance and the orange bars are the ground-truth cost (GC) mode performance. The black dashed lines are PPO performance. (a) Results on Hazard-World-Grid task. (b) Results on SafetyGoal task.", "description": "This figure presents the comparison of the proposed TTCT method with other baselines on two tasks: Hazard-World-Grid and SafetyGoal.  The performance metrics shown are average reward and average cost. The blue bars represent the performance of TTCT using the predicted cost, the orange bars show the performance using ground truth cost, and the black dashed lines show the performance of the standard PPO algorithm (without constraints). The results demonstrate that TTCT achieves comparable or better performance compared to baselines on both tasks while maintaining fewer constraint violations.", "section": "6.2 Main Results and Analysis"}, {"figure_path": "MDpIQ9hQ7H/figures/figures_8_1.jpg", "caption": "Figure 4: Learning curve of our proposed method TTCT. Each column is an algorithm. The six figures on the left show the results of experiments on the Hazard-World-Grid task and the six figures on the right show the results of experiments on the SafetyGoal task. The solid line is the mean value, and the light shade represents the area within one standard deviation.", "description": "This figure presents the learning curves for different reinforcement learning algorithms applied to two different tasks: Hazard-World-Grid and SafetyGoal.  The algorithms were trained using either ground-truth cost functions (GC) or predicted costs from the proposed TTCT method (CP), along with a version of CP without the cost assignment component (CP w/o CA) for ablation. The plots show the average reward and cost over training epochs, providing insights into the learning progress and performance of the algorithms under different cost calculation methods.", "section": "6 Experiments"}, {"figure_path": "MDpIQ9hQ7H/figures/figures_8_2.jpg", "caption": "Figure 3: Evaluation results of our proposed method TTCT. The blue bars are our proposed cost prediction (CP) mode performance and the orange bars are the ground-truth cost (GC) mode performance. The black dashed lines are PPO performance. (a) Results on Hazard-World-Grid task. (b) Results on SafetyGoal task.", "description": "This figure presents the performance comparison between the proposed method (TTCT) and several baselines on two tasks: Hazard-World-Grid and SafetyGoal.  It shows the average episodic reward (Avg. R) and average episodic cost (Avg. C) for each method. The blue bars represent the performance using the predicted cost from TTCT, while the orange bars represent performance using the ground truth cost. The black dashed lines represent the performance of the PPO baseline, which does not use constraints.  The results demonstrate TTCT's effectiveness in achieving lower violation rates (Avg. C) while maintaining comparable rewards (Avg. R) to baselines using ground truth cost.", "section": "6.2 Main Results and Analysis"}, {"figure_path": "MDpIQ9hQ7H/figures/figures_8_3.jpg", "caption": "Figure 6: Results of Pareto frontiers. We compare the performance of 200 policies trained using cost prediction (CP) and 200 policies trained with ground-truth cost (GC). The \u2605 symbol represents the policy on the Pareto frontier. And we connect the Pareto-optimal policies with a curve.", "description": "This figure compares the performance of policies trained using cost prediction (CP) and ground truth cost (GC) in terms of reward and cost.  The Pareto frontier is plotted to showcase the optimal trade-offs between these two objectives.  Policies trained with CP are shown to be closer to the ideal point (high reward, low cost) on the Pareto frontier, indicating better overall performance.", "section": "6.4 Further Results"}, {"figure_path": "MDpIQ9hQ7H/figures/figures_9_1.jpg", "caption": "Figure 7: Zero-shot adaptation capability of TTCT on LavaWall task. The left figure shows the average reward and the right figure shows the average cost.", "description": "This figure presents the results of a zero-shot transfer experiment using the TTCT model. The model, trained on the Hazard-World-Grid environment, was directly applied to a new environment, LavaWall, without any fine-tuning.  The left subplot shows the average reward obtained over training epochs, while the right subplot displays the average cost (violation rate). This demonstrates the model's ability to generalize to unseen environments.", "section": "7 Conclusion and Future Work"}, {"figure_path": "MDpIQ9hQ7H/figures/figures_15_1.jpg", "caption": "Figure 1: TTCT overview. TTCT consists of two training components: (1) the text-trajectory alignment component connects trajectory to text with multimodal architecture, and (2) the cost assignment component assigns a cost value to each state-action based on its impact on satisfying the constraint. When training RL policy, the text-trajectory alignment component is used to predict whether a trajectory violates a given constraint and the cost assignment component is used to assign non-violation cost.", "description": "This figure provides a detailed overview of the Trajectory-level Textual Constraints Translator (TTCT) framework. It illustrates the two main components of TTCT: the text-trajectory alignment component and the cost assignment component.  The text-trajectory alignment component uses a multimodal architecture to connect trajectory and text data, enabling the prediction of whether a trajectory violates a given constraint.  The cost assignment component then assigns a cost value to each state-action pair based on its impact on constraint satisfaction.  This dual-component structure is crucial for both predicting constraint violations and addressing the issue of sparse costs in safe reinforcement learning.", "section": "4 TTCT: Trajectory-level Textual Constraints Translator"}, {"figure_path": "MDpIQ9hQ7H/figures/figures_17_1.jpg", "caption": "Figure 10: Heatmap of cosine similarity between trajectory and text embeddings.", "description": "This figure shows two heatmaps visualizing the cosine similarity between trajectory and text embeddings.  The left heatmap displays the cosine similarity after scaling, while the right heatmap shows the ground truth similarity.  The color intensity in each heatmap represents the strength of the similarity, with lighter colors indicating higher similarity and darker colors indicating lower similarity. This visualization is used to evaluate the performance of the text-trajectory alignment component in accurately capturing the relationship between trajectory and text representations.", "section": "B.1 Violations Prediction Capability of Text-Trajectory Alignment Component"}, {"figure_path": "MDpIQ9hQ7H/figures/figures_18_1.jpg", "caption": "Figure 11: ROC curve of text-trajectory alignment component. The x-axis represents the false positive rate, and the y-axis represents the true positive rate. The closer the AUC value is to 1, the better the performance of the model; conversely, the closer the AUC value is to 0, the worse the performance of the model.", "description": "This ROC curve demonstrates the performance of the text-trajectory alignment component in predicting whether a trajectory violates a given textual constraint.  The AUC (Area Under the Curve) of 0.98 indicates high accuracy in distinguishing between violating and non-violating trajectories.", "section": "B Additional Experiments"}, {"figure_path": "MDpIQ9hQ7H/figures/figures_18_2.jpg", "caption": "Figure 12: Case study of cost assignment component on three types of textual constraints. The first row of every case shows the textual constraint, the second row shows the trajectory of the agent in the environment and each square represents the object stepped on by the agent at that time step, the third row shows the assigned cost of the agent at each time step, and the fourth row shows the time steps. The red line indicates the final observation where the agent violates the textual constraint.", "description": "This figure presents a case study of the cost assignment component of the TTCT model. It showcases how the model assigns costs to state-action pairs based on three different types of textual constraints: quantitative, sequential, and mathematical. Each row represents a different type of constraint, with the agent's trajectory displayed in the second row, the cost assigned by the model in the third row and the time step at the bottom. The red line in the second row highlights the point in the trajectory where the textual constraint was violated. This case study helps to visualize and understand how the cost assignment component works in practice.", "section": "B.2 Case Study of Cost Assignment (CA) Component"}, {"figure_path": "MDpIQ9hQ7H/figures/figures_19_1.jpg", "caption": "Figure 4: Learning curve of our proposed method TTCT. Each column is an algorithm. The six figures on the left show the results of experiments on the Hazard-World-Grid task and the six figures on the right show the results of experiments on the SafetyGoal task. The solid line is the mean value, and the light shade represents the area within one standard deviation.", "description": "This figure displays the learning curves for the proposed TTCT method and several baseline algorithms across two different tasks: Hazard-World-Grid and SafetyGoal.  The plots show the average reward and cost over training epochs for each algorithm.  Shaded regions indicate the standard deviation, providing insight into the variability of the performance.  The figure helps to illustrate the effectiveness of the TTCT model in comparison to baselines.", "section": "6 Experiments"}, {"figure_path": "MDpIQ9hQ7H/figures/figures_19_2.jpg", "caption": "Figure 4: Learning curve of our proposed method TTCT. Each column is an algorithm. The six figures on the left show the results of experiments on the Hazard-World-Grid task and the six figures on the right show the results of experiments on the SafetyGoal task. The solid line is the mean value, and the light shade represents the area within one standard deviation.", "description": "This figure shows the learning curves for different RL algorithms (PPO, PPO-Lagrangian, CPPO-PID, FOCOPS) trained with both ground-truth cost (GC) and predicted cost (CP) using the TTCT method. The left half shows the results from the Hazard-World-Grid environment, and the right half from SafetyGoal. It displays average reward and cost over training epochs, illustrating the performance of TTCT in reducing cost while maintaining reward.", "section": "6 Experiments"}, {"figure_path": "MDpIQ9hQ7H/figures/figures_19_3.jpg", "caption": "Figure 4: Learning curve of our proposed method TTCT. Each column is an algorithm. The six figures on the left show the results of experiments on the Hazard-World-Grid task and the six figures on the right show the results of experiments on the SafetyGoal task. The solid line is the mean value, and the light shade represents the area within one standard deviation.", "description": "This figure shows the training curves for different reinforcement learning algorithms applied to two different tasks (Hazard-World-Grid and SafetyGoal).  For each algorithm, two versions are presented: one trained with ground-truth cost functions (GC) and another trained with the predicted costs generated by the proposed TTCT method (CP).  The figure visually compares the performance of the proposed method and baselines across both tasks, showing reward and violation rate over the course of training.  The shaded area represents standard deviation. This plot helps evaluate the effectiveness and stability of TTCT's cost prediction.", "section": "6 Experiments"}, {"figure_path": "MDpIQ9hQ7H/figures/figures_20_1.jpg", "caption": "Figure 14: Inference time of different trajectory lengths for Hazard-World-Grid on the V100-32G hardware device. Batch size is 64.", "description": "The figure shows the inference time of the TTCT model for different trajectory lengths on the Hazard-World-Grid environment.  The inference time is measured using a V100-32G GPU with a batch size of 64.  The x-axis represents the trajectory length, and the y-axis represents the inference time in seconds. The graph shows a roughly linear increase in inference time as trajectory length increases, indicating that longer trajectories require more processing time from the model. This information is relevant to understanding the computational efficiency and scalability of the TTCT approach.", "section": "B.4 Inference time"}, {"figure_path": "MDpIQ9hQ7H/figures/figures_20_2.jpg", "caption": "Figure 15: Empirical analyses on Hazard-World-Grid with varying text encoders. We choose three different models, transformer-25M [41], gpt2-137M [31], and bert-base-uncased-110M [7].", "description": "This figure displays the results of an empirical analysis conducted on the Hazard-World-Grid environment using three different text encoders: transformer-25M, gpt2-137M, and bert-base-uncased-110M.  The analysis likely involves training reinforcement learning agents with these encoders and evaluating their performance across various metrics.  The chart probably shows the average episodic reward and cost over training epochs for each encoder, allowing for a comparison of their effectiveness in achieving high rewards while minimizing costs.", "section": "B.5 Different Text Encoder"}, {"figure_path": "MDpIQ9hQ7H/figures/figures_21_1.jpg", "caption": "Figure 16: Evaluation results of different trajectory lengths for Hazard-World-Grid. Sets of trajectories with varying lengths shared the same set of textual constraints.", "description": "This figure shows the performance of the text-trajectory alignment component on the Hazard-World-Grid task with varying trajectory lengths.  The AUC (Area Under the Curve) is plotted against the length of the trajectories. The results show that the performance improves initially with increasing trajectory length, but then begins to decline after a certain point. This suggests that the model's ability to capture the relationships between the trajectory and the textual constraint is limited for very long trajectories. The limitations are thought to stem from the transformer's encoding capacity.", "section": "C Broader Impacts and Limitation"}]