{"references": [{"fullname_first_author": "Jonathan Ho", "paper_title": "Denoising diffusion probabilistic models", "publication_date": "2020-06-11", "reason": "This paper is foundational to diffusion models, introducing the core concept of denoising diffusion for image generation that underpins much of the work in this field."}, {"fullname_first_author": "Robin Rombach", "paper_title": "High-resolution image synthesis with latent diffusion models", "publication_date": "2022-06-18", "reason": "This paper introduced Stable Diffusion, a highly influential model demonstrating the power of latent diffusion models for high-quality image generation."}, {"fullname_first_author": "Alexey Dosovitskiy", "paper_title": "An image is worth 16x16 words: Transformers for image recognition at scale", "publication_date": "2020-10-26", "reason": "This paper introduced Vision Transformers (ViTs), a crucial development that brought the power of Transformers to computer vision tasks, setting the stage for the application of transformers to diffusion models."}, {"fullname_first_author": "William Peebles", "paper_title": "Scalable diffusion models with transformers", "publication_date": "2023-10-01", "reason": "This paper introduced Diffusion Transformers (DiTs), directly applying transformer architectures to diffusion models for image generation, which is central to the current work's context."}, {"fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-06-12", "reason": "This paper introduced the Transformer architecture, a fundamental building block for many modern deep learning models, including the DiTs that form the base of the presented research."}]}