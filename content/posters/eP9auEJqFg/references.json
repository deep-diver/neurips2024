{"references": [{"fullname_first_author": "Alessandro Achille", "paper_title": "Dynamics and reachability of learning tasks", "publication_date": "2019-10-26", "reason": "This paper provides the theoretical foundation for the RepNoise defense, modeling the transition probability of model weights during training and linking it to mutual information for representation learning."}, {"fullname_first_author": "Xiangyu Qi", "paper_title": "Fine-tuning aligned language models compromises safety, even when users do not intend to!", "publication_date": "2023-10-17", "reason": "This paper establishes the harmful fine-tuning attack (HFA) threat model that motivates the development of the RepNoise defense."}, {"fullname_first_author": "Domenic Rosati", "paper_title": "Immunization against harmful fine-tuning attacks", "publication_date": "2024-02-14", "reason": "This paper formalizes the criteria for a successful defense against HFAs, which the RepNoise defense aims to satisfy."}, {"fullname_first_author": "Samuel Gehman", "paper_title": "RealToxicityPrompts: Evaluating neural toxic degeneration in language models", "publication_date": "2020-09-23", "reason": "This paper provides the dataset used for one of the harmful fine-tuning attack scenarios in the evaluation, establishing a benchmark for toxic text generation tasks."}, {"fullname_first_author": "Hugo Touvron", "paper_title": "Llama 2: Open foundation and fine-tuned chat models", "publication_date": "2023-07-18", "reason": "This paper introduces the large language model (LLM) used as the base model in the experiments, providing the foundation for evaluating the effectiveness of the RepNoise defense."}]}