[{"figure_path": "eP9auEJqFg/tables/tables_4_1.jpg", "caption": "Table 1: Average harmfulness classifier scores before and after attacks performed using 1k and 10k samples of HarmfulQA from BeaverTails and learning rates \u2208 {3 \u00d7 10\u22125, 6 \u00d7 10\u22125,8 \u00d7 10\u22125}. Blue indicates lower harmfulness score than the base model.", "description": "This table presents the results of harmful fine-tuning attacks on the llama2-7b-chat model and several defence mechanisms.  The \"Base\" row shows the harmfulness score of the original model before any attacks.  The other rows show the harmfulness scores after attacks using 1k and 10k samples from the HarmfulQA dataset, with different learning rates and defence mechanisms applied.  Lower scores indicate that the defence mechanism was successful in reducing the harmfulness of the model after attack.", "section": "4.1 Resistance"}, {"figure_path": "eP9auEJqFg/tables/tables_5_1.jpg", "caption": "Table 2: Toxicity score from Perspective API when the model is requested to continue highly toxic prompts. RepNoise is able to defend against training models for toxic content generation.", "description": "This table presents the results of an experiment evaluating the effectiveness of different defense mechanisms against harmful fine-tuning attacks on a toxic content generation task.  The \"pre-attack\" column shows the initial toxicity scores before any attack. The remaining columns represent toxicity scores after attacks performed with different learning rates (3\u00d710\u22125, 6\u00d710\u22125, 8\u00d710\u22125), each using 1k and 10k samples from a harmful dataset. Each row indicates a different defense method (RepNoise, Security Vectors, Vaccine, Gradient Ascent, Adversarial loss), and the scores indicate the mean toxicity scores obtained from the Perspective API. Lower scores indicate better defence performance, showing that RepNoise demonstrates significant resistance compared to the base model and other defence strategies.", "section": "4.1 Resistance"}, {"figure_path": "eP9auEJqFg/tables/tables_5_2.jpg", "caption": "Table 3: Evaluation of RepNoise on common language model capability benchmarks.", "description": "This table shows the results of evaluating the RepNoise model on several common language model benchmarks including TruthfulQA, MMLU, Hellaswag, Winogrande, ARC, Ethics, and CrowS-Pairs.  The scores indicate that the RepNoise model does not significantly degrade the model's overall capabilities after the application of the RepNoise defence. This demonstrates that the RepNoise method preserves the general capabilities of the language model while mitigating the risk of harmful fine-tuning.", "section": "4.2 Stability"}, {"figure_path": "eP9auEJqFg/tables/tables_6_1.jpg", "caption": "Table 1: Average harmfulness classifier scores before and after attacks performed using 1k and 10k samples of HarmfulQA from BeaverTails and learning rates \u2208 {3 \u00d7 10\u22125, 6 \u00d7 10\u22125,8 \u00d7 10\u22125}. Blue indicates lower harmfulness score than the base model.", "description": "This table presents the average harmfulness classifier scores obtained before and after performing several attacks on the llama2-7b-chat model.  The attacks vary in the number of samples used (1k or 10k) from the HarmfulQA dataset and in the learning rate applied (3 \u00d7 10\u22125, 6 \u00d7 10\u22125, and 8 \u00d7 10\u22125).  Various defense mechanisms are compared, including random initialization, security vectors, additional safety training, and the proposed RepNoise method. Lower scores in blue indicate better defense performance than the base (unattacked) model. The results show that RepNoise consistently achieves lower harmfulness scores compared to other defense mechanisms across different attack strengths.", "section": "4.1 Resistance"}, {"figure_path": "eP9auEJqFg/tables/tables_6_2.jpg", "caption": "Table 1: Average harmfulness classifier scores before and after attacks performed using 1k and 10k samples of HarmfulQA from BeaverTails and learning rates \u2208 {3 \u00d7 10\u22125, 6 \u00d7 10\u22125,8 \u00d7 10\u22125}. Blue indicates lower harmfulness score than the base model.", "description": "This table shows the average harmfulness classifier scores for the base llama2-7b-chat model and several defense mechanisms before and after performing harmful fine-tuning attacks (HFAs) using 1k and 10k samples from the HarmfulQA dataset.  The attacks vary the learning rate used in the fine-tuning process.  Blue coloring highlights cases where a defense mechanism resulted in lower harmfulness scores than the base model.  This illustrates the effectiveness of the different defense mechanisms against HFAs of varying strength.", "section": "4.1 Resistance"}, {"figure_path": "eP9auEJqFg/tables/tables_7_1.jpg", "caption": "Table 6: Freezing earlier layers prevents effective defence indicating that the 'depth' of the defence is critical.", "description": "This table shows the results of an ablation study where different layers of the language model were frozen during the training process of RepNoise. The results show that freezing earlier layers in the model significantly reduces the effectiveness of RepNoise, highlighting the importance of \"depth\"\u2014the degree to which information about harmful representations is removed across all layers\u2014in achieving effective defence.", "section": "Mechanistic Analysis"}, {"figure_path": "eP9auEJqFg/tables/tables_21_1.jpg", "caption": "Table 1: Average harmfulness classifier scores before and after attacks performed using 1k and 10k samples of HarmfulQA from BeaverTails and learning rates \u2208 {3 \u00d7 10\u22125, 6 \u00d7 10\u22125,8 \u00d7 10\u22125}. Blue indicates lower harmfulness score than the base model.", "description": "This table presents the average harmfulness classifier scores for the base llama2-7b-chat model and several defense mechanisms before and after a harmful fine-tuning attack.  The attack uses 1,000 and 10,000 samples from the HarmfulQA dataset with three different learning rates. Lower scores indicate lower harmfulness, and blue highlighting shows when a defense mechanism performed better than the base model.", "section": "4.1 Resistance"}, {"figure_path": "eP9auEJqFg/tables/tables_22_1.jpg", "caption": "Table 1: Average harmfulness classifier scores before and after attacks performed using 1k and 10k samples of HarmfulQA from BeaverTails and learning rates \u2208 {3 \u00d7 10\u22125, 6 \u00d7 10\u22125,8 \u00d7 10\u22125}. Blue indicates lower harmfulness score than the base model.", "description": "This table shows the average harmfulness classifier scores for different defense mechanisms before and after performing harmful fine-tuning attacks. The attacks used 1,000 and 10,000 samples from the HarmfulQA dataset, with three different learning rates.  Lower scores indicate lower harmfulness.  The base model is compared to several defense mechanisms, including random initialization, additional safety training, gradient ascent, adversarial loss, and security vectors.", "section": "4.1 Resistance"}, {"figure_path": "eP9auEJqFg/tables/tables_25_1.jpg", "caption": "Table 9: Cross-domain generalization: Harmfulness scores after attacks with learning rates \u2208 {3 \u00d7 10\u22125, 6 \u00d7 10\u22125, 8 \u00d7 10\u22125} and immunization using RepNoise on different datasets.", "description": "This table presents the results of a cross-domain generalization experiment.  It shows the average harmfulness scores (as measured by a classifier) before and after attacks with various learning rates. The experiments involve training RepNoise on one dataset and testing it on another, to determine its ability to generalize across different domains. The table includes rows for different combinations of immunization and attack datasets (Decoding Trust and BeaverTails), showing the effect of training on one dataset and testing on another on the resulting harmfulness scores.", "section": "4.4 Generalization"}, {"figure_path": "eP9auEJqFg/tables/tables_26_1.jpg", "caption": "Table 1: Average harmfulness classifier scores before and after attacks performed using 1k and 10k samples of HarmfulQA from BeaverTails and learning rates \u2208 {3 \u00d7 10\u22125, 6 \u00d7 10\u22125,8 \u00d7 10\u22125}. Blue indicates lower harmfulness score than the base model.", "description": "This table shows the average harmfulness scores (using a harmfulness classifier) for the llama2-7b-chat model before and after several attacks (harmful fine-tuning attacks using 1k and 10k samples from the HarmfulQA dataset). It compares the base model to several defence mechanisms, including RepNoise, against these attacks. Lower scores indicate lower harmfulness. The attacks are performed with three different learning rates. The results show that RepNoise is the only defence method that consistently provides significant resistance against all attacks, even with 10k samples.", "section": "4.1 Resistance"}, {"figure_path": "eP9auEJqFg/tables/tables_27_1.jpg", "caption": "Table 1: Average harmfulness classifier scores before and after attacks performed using 1k and 10k samples of HarmfulQA from BeaverTails and learning rates \u2208 {3 \u00d7 10\u22125, 6 \u00d7 10\u22125,8 \u00d7 10\u22125}. Blue indicates lower harmfulness score than the base model.", "description": "This table presents the average harmfulness classifier scores for the base llama2-7b-chat model and several defense mechanisms before and after performing harmful fine-tuning attacks using 1k and 10k samples from the HarmfulQA dataset. The attacks were performed using three different learning rates. Lower scores indicate lower harmfulness.", "section": "4.1 Resistance"}, {"figure_path": "eP9auEJqFg/tables/tables_27_2.jpg", "caption": "Table 1: Average harmfulness classifier scores before and after attacks performed using 1k and 10k samples of HarmfulQA from BeaverTails and learning rates \u2208 {3 \u00d7 10\u22125, 6 \u00d7 10\u22125,8 \u00d7 10\u22125}. Blue indicates lower harmfulness score than the base model.", "description": "This table presents the average harmfulness classifier scores for the base llama-2-7b-chat model and several defense mechanisms before and after harmful fine-tuning attacks.  The attacks used 1,000 and 10,000 samples from the HarmfulQA dataset, with three different learning rates. Lower scores indicate better defense against the attacks.", "section": "4.1 Resistance"}, {"figure_path": "eP9auEJqFg/tables/tables_28_1.jpg", "caption": "Table 6: Freezing earlier layers prevents effective defence indicating that the 'depth' of the defence is critical.", "description": "This table shows the results of an ablation study where different layers of the language model were frozen during the training of the RepNoise defence. The results show that freezing earlier layers significantly reduces the effectiveness of the defence. This suggests that the effectiveness of RepNoise depends on its ability to remove information about harmful representations across all layers of the model, rather than just the final layers.  The 'depth' of the defence (how many layers are affected) appears critical for effectiveness. ", "section": "Mechanistic Analysis"}, {"figure_path": "eP9auEJqFg/tables/tables_29_1.jpg", "caption": "Table 1: Average harmfulness classifier scores before and after attacks performed using 1k and 10k samples of HarmfulQA from BeaverTails and learning rates \u2208 {3 \u00d7 10\u22125, 6 \u00d7 10\u22125,8 \u00d7 10\u22125}. Blue indicates lower harmfulness score than the base model.", "description": "This table presents the results of harmful fine-tuning attacks on the llama-2-7b-chat model with different defense mechanisms.  It shows the average harmfulness classifier scores before and after attacks using 1,000 and 10,000 samples from the HarmfulQA dataset, with three different learning rates. The base model's scores are compared against those of models with different defenses applied (Random, Security Vectors, Vaccine, Additional safety training, Gradient Ascent, Adversarial loss, and RepNoise), showing the effectiveness of each defense in mitigating harmful fine-tuning attacks.", "section": "4.1 Resistance"}, {"figure_path": "eP9auEJqFg/tables/tables_30_1.jpg", "caption": "Table 1: Average harmfulness classifier scores before and after attacks performed using 1k and 10k samples of HarmfulQA from BeaverTails and learning rates \u2208 {3 \u00d7 10\u22125, 6 \u00d7 10\u22125,8 \u00d7 10\u22125}. Blue indicates lower harmfulness score than the base model.", "description": "This table shows the average harmfulness classifier scores before and after a harmful fine-tuning attack was performed on the llama2-7b-chat model.  The attacks used 1k and 10k samples from the HarmfulQA dataset of BeaverTails, with three different learning rates.  The table compares the base model's performance to several defense mechanisms, including RepNoise. Lower scores indicate lower harmfulness.", "section": "4.1 Resistance"}, {"figure_path": "eP9auEJqFg/tables/tables_30_2.jpg", "caption": "Table 16: Learning rate study, even slightly larger learning rates result in ineffective defences. Results are reported on an 8 \u00d7 10\u22125 @ 10k sample attack.", "description": "This table presents the results of an ablation study on the impact of the learning rate used during the RepNoise defence.  It shows that even small increases in the learning rate from 2 \u00d7 10\u207b\u2075 to 4 \u00d7 10\u207b\u2075 significantly reduce the effectiveness of RepNoise against harmful fine-tuning attacks, highlighting the sensitivity of the defence to this hyperparameter. The attack used for this experiment was an 8 \u00d7 10\u207b\u2075 @ 10k sample attack.", "section": "4.1 Resistance"}, {"figure_path": "eP9auEJqFg/tables/tables_30_3.jpg", "caption": "Table 17: Our method is even quite sensitive to the random seed used", "description": "This table shows the average harmfulness classifier scores for different random seeds used during the RepNoise defense.  It demonstrates the sensitivity of RepNoise to the random seed, highlighting a limitation of the method.  The results indicate that even small changes to the random seed can significantly alter the effectiveness of the defense, underscoring the importance of considering this factor in future research and practical applications.", "section": "4.1 Resistance"}, {"figure_path": "eP9auEJqFg/tables/tables_31_1.jpg", "caption": "Table 1: Average harmfulness classifier scores before and after attacks performed using 1k and 10k samples of HarmfulQA from BeaverTails and learning rates \u2208 {3 \u00d7 10\u22125, 6 \u00d7 10\u22125,8 \u00d7 10\u22125}. Blue indicates lower harmfulness score than the base model.", "description": "This table presents the average harmfulness classifier scores (lower is better) for the base llama2-7b-chat model and several defense mechanisms before and after attacks using 1k and 10k samples from the HarmfulQA dataset.  The attacks varied the learning rate.  The results show the effectiveness of each defense mechanism against the attacks.", "section": "4.1 Resistance"}, {"figure_path": "eP9auEJqFg/tables/tables_31_2.jpg", "caption": "Table 1: Average harmfulness classifier scores before and after attacks performed using 1k and 10k samples of HarmfulQA from BeaverTails and learning rates \u2208 {3 \u00d7 10\u22125, 6 \u00d7 10\u22125,8 \u00d7 10\u22125}. Blue indicates lower harmfulness score than the base model.", "description": "This table presents the results of harmful fine-tuning attacks on the llama-2-7b-chat model and several defense mechanisms.  The \"harmfulness score\" is a metric measuring how harmful a model's responses are. The table shows the average harmfulness score before any attack (pre-attack), and after attacks with different strengths (1k and 10k samples from the HarmfulQA dataset, and three different learning rates).  The results are presented for the base model (llama2-7b-chat), and several defense methods. The table shows that the base model's harmfulness score increases significantly after the attacks, while the RepNoise defense method effectively reduces the harmfulness score, highlighting its effectiveness against harmful fine-tuning attacks.", "section": "4.1 Resistance"}, {"figure_path": "eP9auEJqFg/tables/tables_31_3.jpg", "caption": "Table 1: Average harmfulness classifier scores before and after attacks performed using 1k and 10k samples of HarmfulQA from BeaverTails and learning rates \u2208 {3 \u00d7 10\u22125, 6 \u00d7 10\u22125,8 \u00d7 10\u22125}. Blue indicates lower harmfulness score than the base model.", "description": "This table presents the average harmfulness classifier scores for the base llama-2-7b-chat model and several defense mechanisms before and after a harmful fine-tuning attack.  The attack used 1k and 10k samples from the HarmfulQA subset of the BeaverTails dataset with three different learning rates. Lower scores indicate that the defense mechanism was successful in mitigating the harmful fine-tuning attack.", "section": "4.1 Resistance"}, {"figure_path": "eP9auEJqFg/tables/tables_32_1.jpg", "caption": "Table 1: Average harmfulness classifier scores before and after attacks performed using 1k and 10k samples of HarmfulQA from BeaverTails and learning rates \u2208 {3 \u00d7 10\u22125, 6 \u00d7 10\u22125,8 \u00d7 10\u22125}. Blue indicates lower harmfulness score than the base model.", "description": "This table presents the results of harmful fine-tuning attacks on various defense mechanisms, including RepNoise.  The table shows the average harmfulness classifier scores before and after attacks. The attacks were performed using 1,000 and 10,000 samples from the HarmfulQA dataset, with three different learning rates.  The base model is compared against various defense methods, and the lower the harmfulness score after the attack, the more effective the defense is.", "section": "4.1 Resistance"}, {"figure_path": "eP9auEJqFg/tables/tables_33_1.jpg", "caption": "Table 1: Average harmfulness classifier scores before and after attacks performed using 1k and 10k samples of HarmfulQA from BeaverTails and learning rates \u2208 {3 \u00d7 10\u22125, 6 \u00d7 10\u22125,8 \u00d7 10\u22125}. Blue indicates lower harmfulness score than the base model.", "description": "This table presents the average harmfulness classifier scores for the base llama-2-7b-chat model and several defense mechanisms before and after a harmful fine-tuning attack using 1k and 10k samples from the HarmfulQA dataset of BeaverTails.  The attacks were performed with three different learning rates (3e-5, 6e-5, 8e-5). Lower scores indicate better defense performance against the harmful attacks.", "section": "4.1 Resistance"}]