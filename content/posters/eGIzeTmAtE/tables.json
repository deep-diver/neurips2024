[{"figure_path": "eGIzeTmAtE/tables/tables_6_1.jpg", "caption": "Table 1: Quantitative evaluation of ColJailBreak and baselines in jailbreaking two commercial T2I models across two metrics: CLIP Scores and ASR. The best results are highlighted with bold values(w/ Ext and w/ Own represent evaluation on external dataset and evaluation on our dataset respectively. The symbol \"/\" indicates that the CLIP Scores value is not calculated because the generated image does not contain inappropriate content.)", "description": "This table presents a quantitative comparison of three methods (MMA-Diffusion, QF-Attack, and ColJailBreak) for jailbreaking two commercial text-to-image (T2I) models (GPT-4 and DALL-E 2).  The comparison is made across four categories of unsafe content (violence, harassment, self-harm, and nudity), using two evaluation metrics: CLIP Scores (measuring the semantic similarity between generated images and unsafe prompts) and Attack Success Rate (ASR, representing the percentage of successful jailbreaks).  The results show ColJailBreak's superior performance in generating unsafe content and bypassing safety filters compared to the baselines, with improvements in both CLIP Scores and ASR across all unsafe content categories.", "section": "5.2 Comparison Results"}, {"figure_path": "eGIzeTmAtE/tables/tables_6_2.jpg", "caption": "Table 2: Quantitative evaluation of ColJailBreak and baselines in jailbreaking T2I models with removal-based defense mechanisms via the metric of ASR. The best results are highlighted with bold values(w/ Ext and w/ Own represent evaluation on external dataset and evaluation on our dataset respectively.)", "description": "This table presents a quantitative analysis of the ColJailBreak method and two baseline methods (MMA-Diffusion and QF-Attack) in the context of jailbreaking text-to-image (T2I) models.  The evaluation specifically focuses on the models' resilience to removal-based defense mechanisms. The results are categorized by four types of unsafe content (Violence, Harassment, Self-harm, and Nudity) and are further divided based on whether external datasets or the authors' own dataset were used for testing.  The primary metric used for evaluation is the Attack Success Rate (ASR), reflecting the percentage of successful jailbreaking attempts.  The table helps to illustrate ColJailBreak's effectiveness against various types of defenses and datasets.", "section": "5.2 Comparison Results"}, {"figure_path": "eGIzeTmAtE/tables/tables_15_1.jpg", "caption": "Table 1: Quantitative evaluation of ColJailBreak and baselines in jailbreaking two commercial T2I models across two metrics: CLIP Scores and ASR. The best results are highlighted with bold values(w/ Ext and w/ Own represent evaluation on external dataset and evaluation on our dataset respectively. The symbol \"/\" indicates that the CLIP Scores value is not calculated because the generated image does not contain inappropriate content.)", "description": "This table presents a quantitative comparison of the proposed ColJailBreak method against two baseline methods (MMA-Diffusion and QF-Attack) for jailbreaking two commercial text-to-image (T2I) models: GPT-4 and DALL-E 2. The evaluation is performed across four categories of unsafe content (violence, harassment, self-harm, and nudity) using two metrics: CLIP Scores (measuring the semantic similarity between generated images and unsafe prompts) and Attack Success Rate (ASR, the percentage of successful jailbreaks).  The results show ColJailBreak's superior performance in generating unsafe content compared to the baselines, indicated by significantly higher CLIP scores and ASR values across all categories and models.  The table also distinguishes between results obtained using an external dataset and a dataset curated by the authors.", "section": "5.2 Comparison Results"}]