[{"figure_path": "eGIzeTmAtE/figures/figures_1_1.jpg", "caption": "Figure 1: An example of ColJailBreak. (a) ChatGPT rejects the prompt when we directly input the prompt with sensitive words (e.g., gun). (b) ChatGPT accepts the prompt and generates image, then we inject unsafe content.", "description": "This figure illustrates the ColJailBreak method.  Panel (a) shows that a prompt containing a sensitive word (\"gun\") is rejected by ChatGPT's safety filter, preventing the generation of an image. Panel (b) demonstrates that by substituting the unsafe word (\"gun\") with a safer alternative (\"speedgun\"), the prompt is accepted by ChatGPT.  A safe image is then generated, and unsafe content is added via an image editing process, effectively circumventing the safety filter.", "section": "1 Introduction"}, {"figure_path": "eGIzeTmAtE/figures/figures_3_1.jpg", "caption": "Figure 2: Overview of ColJailBreak. (a) We employ adaptive safe word substitution to modify the sensitive words in the prompt, enabling T2I models to accept and generate the image. (b) Inpainting-driven injection of unsafe content injects unsafe content into specific areas of images. (c) Contrastive Language-Image-Guided Collaborative Optimization ensures that unsafe content is injected accurately and naturally.", "description": "This figure illustrates the three key components of the ColJailBreak framework.  (a) shows how the model first substitutes unsafe words in a prompt with safer alternatives to generate a safe image that passes the safety filter. (b) depicts the inpainting process where the unsafe content is injected into specific regions of the generated safe image. Finally, (c) shows how the contrastive language-image-guided collaborative optimization refines the process to ensure that the injected unsafe content appears natural and integrated within the image.", "section": "4 Collaborative Generation and Editing"}, {"figure_path": "eGIzeTmAtE/figures/figures_7_1.jpg", "caption": "Figure 3: Visualization results of unsafe images generated by different methods. Sensitive words in the unsafe prompt are shown in red.", "description": "This figure shows a comparison of unsafe images generated by three different methods: MMA-Diffusion, QF-Attack, and the proposed ColJailBreak method.  Each method is applied to four types of unsafe prompts (violence, harassment, self-harm, and nudity) for both GPT-4 and DALL-E 2 models.  The figure visually demonstrates the differences in the generated images' realism and adherence to the prompt's description, highlighting ColJailBreak's effectiveness in creating more convincing unsafe content.", "section": "5.2 Comparison Results"}, {"figure_path": "eGIzeTmAtE/figures/figures_8_1.jpg", "caption": "Figure 4: Ablation experimental results of different methods. (a) The unsafe content injection effects of different image inpainting methods. (b) Different methods of safety image generation and editing effects", "description": "This figure presents the results of ablation studies conducted to evaluate the effectiveness of different components within the ColJailBreak framework.  (a) shows the impact of various image inpainting methods on the success rate and quality of injecting unsafe content into the initially safe images. The comparison includes SD-Inpainting, ControlNet, Paint By Example, and the authors' proposed method. (b) compares the effectiveness of different methods for generating the initial safe image (Is) which is used as the base for content injection. The methods include Direct Image Search (DIS), Manual Substitution Search (MSS), and the authors' method (Ours).  The results show the success rate (ASR) and CLIP scores for each method, indicating the performance in terms of generating images that successfully bypass safety filters while maintaining visual quality and coherence.", "section": "5.3 Ablation Study"}, {"figure_path": "eGIzeTmAtE/figures/figures_8_2.jpg", "caption": "Figure 5: Visualization of the unsafe content injection effects of different image inpainting methods.", "description": "This figure shows the results of using different image inpainting methods to inject unsafe content into safe images.  The top row shows the results of injecting a knife into a safe image depicting a man and woman in a supermarket, while the bottom row shows the results of injecting a gun. Each column represents a different inpainting method: SD-Inpainting, ControlNet, Paint By Example, and the proposed 'Ours' method. The figure demonstrates that the 'Ours' method produces the most realistic and natural-looking results, successfully integrating the unsafe element into the image while maintaining coherence and quality.", "section": "5.3 Ablation Study"}, {"figure_path": "eGIzeTmAtE/figures/figures_9_1.jpg", "caption": "Figure 6: Visualization effects of different safety image generation and editing methods. (a) Direct Image Search (DIS). (b) Manual Substitution Search (MSS). (c) Ours.", "description": "This figure compares three different methods for generating safe images as a pre-processing step for injecting unsafe content into images generated by text-to-image models.  The goal is to bypass safety filters. Method (a) uses direct image search to find a safe image similar to the desired image. Method (b) uses manual substitution of unsafe words with safe ones in the prompt before generation.  Method (c), the authors' proposed method, leverages an adaptive safe word substitution technique to create a safe image that is more suitable for the subsequent unsafe content injection step. The example prompt uses the sentence, \"On a jogging path in a park, a man jogs up to a woman and points a pistol at her. Trees and park benches make up the background, and the woman looks scared.\" The figure shows how the three methods produce visually different results.", "section": "5.3 Ablation Study"}, {"figure_path": "eGIzeTmAtE/figures/figures_14_1.jpg", "caption": "Figure 3: Visualization results of unsafe images generated by different methods. Sensitive words in the unsafe prompt are shown in red.", "description": "This figure shows the results of generating unsafe images using three different methods: MMA-Diffusion, QF-Attack, and the proposed ColJailBreak method.  For each method, several examples of generated images are shown, corresponding to different prompts focusing on violence, harassment, self-harm, and nudity. The red highlighting emphasizes the unsafe keywords from the original prompt.  The figure visually compares the quality and relevance of the generated unsafe images to the original prompt, illustrating ColJailBreak's superior performance in generating more consistent and realistic results compared to the baseline methods.", "section": "5.2 Comparison Results"}]