[{"figure_path": "yUqUBGioBG/figures/figures_3_1.jpg", "caption": "Figure 1: Illustration of the parametric model. Classes of each type are best separated along specific axes: classes of type a1 along the red axis (z(1)) and classes of type a2 along the green axis (z(2)). On axis z(0) both types can be separated but not as effectively as on their respective optimal axes.", "description": "This figure illustrates the parametric model used in the paper to study class distribution shifts in zero-shot learning.  It shows a simplified representation of data points from two classes (a1 and a2), each characterized by an attribute. The data points are represented in a three-dimensional space with axes z(0), z(1), and z(2).  The axes z(1) and z(2) represent features that allow for optimal separation of classes a1 and a2, respectively, while z(0) shows features shared between the two classes, offering weaker separation. This model helps to understand how class distribution shifts in an unknown attribute A can result in poor performance for zero-shot learning models, even if the conditional distribution of data given the class remains the same.", "section": "Parametric Model of Class Distribution Shifts in Zero-Shot Learning"}, {"figure_path": "yUqUBGioBG/figures/figures_4_1.jpg", "caption": "Figure 1: Illustration of the parametric model. Classes of each type are best separated along specific axes: classes of type a1 along the red axis (z(1)) and classes of type a2 along the green axis (z(2)). On axis z(0) both types can be separated but not as effectively as on their respective optimal axes.", "description": "This figure illustrates the parametric model used in the paper to demonstrate how class distribution shifts can affect zero-shot learning.  It shows data points from two different class types (a1 and a2) in a three-dimensional space. The axes z(0), z(1), and z(2) represent different features or dimensions.  Classes of type a1 are best separated along the z(1) axis (red), while classes of type a2 are best separated along the z(2) axis (green). The z(0) axis (black) provides some separation, but less effectively than the other two axes. This illustrates how learning representations that work well for the training data (where one type of class might be more prevalent), may not perform well when the class distribution shifts at test time.", "section": "3 Parametric Model of Class Distribution Shifts in Zero-Shot Learning"}, {"figure_path": "yUqUBGioBG/figures/figures_4_2.jpg", "caption": "Figure 2: Optimal weights. Top row: do is fixed, d\u2081 and d\u2082 vary. Middle and bottom rows: do, d\u2081, d\u2082 are fixed. Middle: v/v varies. Bottom: vo/v+ varies.", "description": "This figure displays the optimal weights obtained by minimizing the expected loss in a parametric model of class distribution shifts in zero-shot learning.  The plots show how the optimal weights (w*\u00b2, w\u2081\u00b2, w\u2082\u00b2) change depending on several factors: (Top row) varying the number of dimensions d\u2081 and d\u2082 that allow good separation for classes of different types while keeping the number of shared dimensions (d\u2080) constant. (Middle and Bottom rows) Varying the variance ratios (v\u2080/v\u207b and v\u2080/v\u207a respectively) while maintaining constant number of dimensions (d\u2080, d\u2081, d\u2082). The x-axis represents the proportion (1-p) of type a2 classes in the data, while the y-axis represents the relative magnitude of the optimal weights.", "section": "3 Parametric Model of Class Distribution Shifts in Zero-Shot Learning"}, {"figure_path": "yUqUBGioBG/figures/figures_5_1.jpg", "caption": "Figure 3: Illustration of the proposed hierarchical sampling. Top: N = 6 classes, with 2 minority-type classes D, F (in purple). Middle: synthetic environments formed by sampling small (k = 3) class subsets; in 1/5 of the environments, minority-type classes become the majority constituting 2/3 of the classes. Bottom: sampling r = 1 positive and r = 1 negative pairs for each class in the environment.", "description": "This figure illustrates the hierarchical sampling method used to create diverse synthetic environments for training.  It starts with a set of classes, some of which are in the minority. Subsets of these classes are then randomly sampled to form the environments. The composition of these environments varies, with some having a higher proportion of minority classes than others, simulating real-world class distribution shifts. Finally, pairs of data points are sampled within and between classes to create training examples for each environment.", "section": "4.1 Synthetic Environments"}, {"figure_path": "yUqUBGioBG/figures/figures_7_1.jpg", "caption": "Figure 4: Average AUC over 10 simulation repetitions for majority attribute proportion p = 0.9 in training (and 0.1 in test). Solid lines: distribution-shift. Dashed lines: in-distribution. Our method improves robustness for shifts, without compromising training distribution results.", "description": "This figure shows the average AUC (Area Under the Curve) across 10 simulation runs, comparing different methods for handling class distribution shifts in zero-shot learning.  The x-axis represents the number of data points (pairs) used for training. The y-axis shows the AUC.  Solid lines depict the performance under a class distribution shift (from 0.9 in training to 0.1 in testing), while dashed lines show the in-distribution performance (no shift, both training and testing at 0.9). The figure demonstrates that the proposed VarAUC method outperforms other methods in terms of robustness to the shift, maintaining comparable performance to the other methods when there is no shift. ", "section": "Empirical Results"}, {"figure_path": "yUqUBGioBG/figures/figures_8_1.jpg", "caption": "Figure 5: Average feature importance for p = 0.9, 10 repetitions. Our VarAUC penalty favors shared features (blocks 1 and 3), while deprioritizing majority features (block 2). All methods assign low weight to noise features (block 4).", "description": "The figure shows the average feature importance across ten repetitions of the simulation for a majority attribute proportion of 0.9.  The VarAUC method prioritizes features that are useful for separating classes of both types (shared features), while other methods prioritize features primarily useful for the majority class in the training data.  Noise features receive low weight from all methods.", "section": "5.1 Simulations: Revisiting the Parametric Model"}, {"figure_path": "yUqUBGioBG/figures/figures_8_2.jpg", "caption": "Figure 6: Average percentage changes of our method compared to ERM across 10 repetitions are shown for the ETHEC (top) and CelebA (bottom) datasets. Error bars represent \u00b1 one std-dev.", "description": "This figure compares the performance of the proposed method against the ERM baseline on two real-world datasets: ETHEC (species recognition) and CelebA (face recognition).  The y-axis represents the percentage change in AUC, with positive values indicating improvement over ERM.  The x-axis shows different methods including the proposed method with different penalties. The top panel displays the results for ETHEC, and the bottom for CelebA.  Error bars depict the standard deviation across ten repetitions of the experiments.", "section": "5.2 Experiments on Real Data"}, {"figure_path": "yUqUBGioBG/figures/figures_23_1.jpg", "caption": "Figure 7: Additional simulation results. Top row: Additional dimensions of the representation. Middle row: additional rations of the attribute variances. Bottom row: unbalanced sets of positive and negative examples. Bars show mean AUC values on the test set across 5 repetitions of the experiment, whiskers show \u00b1 standard deviation.", "description": "This figure displays the results of additional simulations conducted to further investigate the impact of various factors on the performance of the proposed algorithm.  The top row explores the effect of increasing the dimensionality of the representation space. The middle row examines the effect of varying the ratio of attribute variances between training classes. The bottom row assesses the impact of having an imbalanced number of positive and negative examples during training.  Across all rows, the algorithm's performance in terms of AUC (Area Under the Curve) is compared across different methods and under various conditions.", "section": "Additional Simulation Results"}, {"figure_path": "yUqUBGioBG/figures/figures_25_1.jpg", "caption": "Figure 8: Additional Simulation Results. Top row: p = 0.05, Bottom row: p = 0.3. Left: Average AUC progress over 10 repetitions of the simulation. Solid lines correspond to performance on test data (distribution shift scenario), dashed lines show performance on data sampled from the same distribution as training data (in-distribution scenario). Right: Average feature importance results over 10 repetitions.", "description": "This figure presents simulation results for different proportions of the majority attribute in training data (p = 0.05 and p = 0.3).  The left panels show the average AUC over 10 simulation runs, comparing the performance of various methods on both in-distribution data (same distribution as training) and out-of-distribution data (shifted distribution). Dashed lines represent in-distribution results, and solid lines represent out-of-distribution results. The right panels show the average feature importance, indicating which features each method prioritizes.  The figure demonstrates the performance and feature weighting behavior of the different methods under varying levels of distribution shift.", "section": "Additional Empirical Results"}, {"figure_path": "yUqUBGioBG/figures/figures_26_1.jpg", "caption": "Figure 9: Analysis of Loss Differences. Histograms of differences between ERM and our algorithm with VarAUC penalty are shown for two experiments in separate sub-figures: (a) CelebA dataset, (b) ETHEC dataset. The top rows show differences for negative pairs (y = 0), bottom ones show differences for positive pairs (y = 1). In each sub-figure the left column corresponds to the minority type and right one to the majority. A dotted black line marks a difference of 0. Positive values correspond to higher losses for ERM.", "description": "This figure presents histograms visualizing the differences in loss between the ERM baseline and the proposed algorithm (with VarAUC penalty) across four groups: minority negative pairs, minority positive pairs, majority negative pairs, and majority positive pairs.  The histograms are separated by dataset (CelebA and ETHEC) and show whether the ERM method resulted in higher or lower loss compared to the proposed approach.", "section": "D.3 Analysis of Loss Values"}, {"figure_path": "yUqUBGioBG/figures/figures_27_1.jpg", "caption": "Figure 10: Sample Images from the CelebA Dataset. Top: a random sample of the training data with 95% non-blond people. Bottom: a random sample of the test data with 95% blond people.", "description": "This figure shows example images from the CelebA dataset used in the paper's experiments.  The top row displays a sample of the training data, which is predominantly composed of individuals without blond hair (95%). The bottom row shows a sample of the test data, which is predominantly composed of individuals with blond hair (95%). This illustrates the class distribution shift used to evaluate the robustness of the proposed zero-shot learning approach.", "section": "E Datasets"}, {"figure_path": "yUqUBGioBG/figures/figures_27_2.jpg", "caption": "Figure 1: Illustration of the parametric model. Classes of each type are best separated along specific axes: classes of type a1 along the red axis (z(1)) and classes of type a2 along the green axis (z(2)). On axis z(0) both types can be separated but not as effectively as on their respective optimal axes.", "description": "This figure illustrates the parametric model used in the paper to demonstrate the effect of class distribution shifts in zero-shot learning. The model assumes that data points are sampled from a Gaussian distribution, with the mean determined by the class and an attribute A indicating the type of class. The figure shows how the optimal separation between classes of different types depends on the relative proportions of types in the training data, illustrating how the shift in class distribution can lead to poor performance in zero-shot learning settings. ", "section": "3 Parametric Model of Class Distribution Shifts in Zero-Shot Learning"}]