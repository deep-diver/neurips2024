[{"figure_path": "4M9f8VMt2C/figures/figures_1_1.jpg", "caption": "Figure 1: Our automatic factuality evaluator, SAFE, uses a large language model to rate the factuality of a long-form response to a given prompt using Google Search. We empirically demonstrate that SAFE outperforms human annotators while being more than 20 times cheaper (Section 4).", "description": "The figure illustrates the SAFE (Search-Augmented Factuality Evaluator) process.  SAFE uses an LLM to assess the factuality of a long-form response by breaking it down into individual facts.  Each fact is checked for self-containment and relevance to the prompt.  Then, Google Search is used to verify the accuracy of relevant facts.  The output shows the number of facts categorized as supported, not supported, and irrelevant.", "section": "3 SAFE: LLM agents as factuality autoraters"}, {"figure_path": "4M9f8VMt2C/figures/figures_2_1.jpg", "caption": "Figure 2: We present LongFact, a prompt set designed to probe a model's factuality when generating long-form responses. Left: LongFact consists of 38 topics (listed in Appendix B.2) categorized into four supercategories. Right: compared to existing factuality datasets, LongFact (a) tests long-form responses rather than short answers and (b) covers a broad range of topics.", "description": "This figure demonstrates the composition and breadth of the LongFact dataset. The left panel shows a pie chart illustrating the distribution of 38 topics across four supercategories: Social Sciences, STEM, Humanities, and Other.  The right panel is a table comparing LongFact with existing factuality benchmarks, highlighting its unique focus on long-form responses and diverse topic coverage, unlike datasets that primarily assess short-answer factuality or only cover a limited range of topics.", "section": "2 LongFact: Using LLMs to generate a multi-topic benchmark for long-form factuality"}, {"figure_path": "4M9f8VMt2C/figures/figures_3_1.jpg", "caption": "Figure 3: SAFE is able to accurately rate factual claims that require multi-step evaluation by leveraging an LLM to iteratively issue Google Search queries and reason with search results.", "description": "This figure illustrates the SAFE (Search-Augmented Factuality Evaluator) process.  It shows how an LLM (large language model) acts as an agent to assess the factuality of individual claims within a long-form response. The process involves the LLM breaking down a response into individual facts, generating relevant search queries for each fact using Google Search, and then reasoning about whether the search results support or refute the facts. The figure provides a visual representation of this iterative process, showing the LLM's reasoning steps and the final determination (supported or not supported).", "section": "3 SAFE: LLM agents as factuality autoraters"}, {"figure_path": "4M9f8VMt2C/figures/figures_4_1.jpg", "caption": "Figure 4: On 16,011 individual facts, SAFE annotations agree with 72.0% of human annotations.", "description": "This figure shows the results of comparing SAFE's annotations with human annotations on a dataset of 16,011 individual facts.  The large majority (72%) of SAFE's annotations agreed with the human annotations, demonstrating a high level of agreement between the automated method and human evaluation. The remaining 28% represent disagreements, indicating areas where further refinement of the SAFE method might be needed.  This figure highlights the accuracy and effectiveness of SAFE in automating the factuality evaluation task.", "section": "4 LLM agents can be better factuality annotators than humans"}, {"figure_path": "4M9f8VMt2C/figures/figures_4_2.jpg", "caption": "Figure 5: SAFE outperforms human annotators while being more than 20\u00d7 cheaper. Left: on a randomly-sampled subset of 100 individual facts where the annotation from SAFE and humans disagreed, SAFE annotations were correct 76% of the time (we use \u201cresearcher + full internet access\u201d labels as the ground truth). Right: rating a single model response costs $4.00 using human annotations (Min et al., 2023), compared to $0.19 using SAFE configured with GPT-3.5-Turbo and Serper API.", "description": "This figure demonstrates the superior performance of SAFE over human annotators. The left panel shows that in a subset of 100 cases where SAFE disagreed with human annotations, SAFE's annotations were correct 76% of the time, in comparison to only 19% accuracy for human annotators. The right panel highlights the cost-effectiveness of SAFE, indicating that it is more than 20 times cheaper than human annotations.", "section": "LLM agents can be better factuality annotators than humans"}, {"figure_path": "4M9f8VMt2C/figures/figures_5_1.jpg", "caption": "Figure 6: Long-form factuality performance of thirteen large language models across four model families (Gemini, GPT, Claude, and PaLM-2), sorted by F\u2081@K in descending order. F\u2081@K averaged over SAFE (Section 3) evaluation results on model responses to the same set of 250 random prompts from LongFact-Objects. F\u2081@K was calculated with K = 64 (the median number of facts among all model responses) and K = 178 (the maximum number of facts among all model responses). The ranking of models remains relatively stable at sufficiently-large K values (more discussion in Appendix E.3). Raw metrics are shown in Table 2.", "description": "This figure presents a benchmark of thirteen large language models from four families (Gemini, GPT, Claude, and PaLM-2) on their long-form factuality performance.  The models are ranked in descending order based on the F\u2081@K metric, calculated using two different values of K (64 and 178), representing the median and maximum number of facts across all model responses, respectively.  The results are obtained using the Search-Augmented Factuality Evaluator (SAFE) method and show a general trend that larger language models tend to achieve better long-form factuality. The ranking stability at larger K values is further discussed in Appendix E.3.", "section": "6 Larger LLMs are more factual"}, {"figure_path": "4M9f8VMt2C/figures/figures_15_1.jpg", "caption": "Figure 7: Causes of SAFE returning an incorrect rating on the 100 disagreement examples with \u201cresearcher + full internet access\u201d ground-truth labels from Section 4. \"Reasoning error\": error in the language model's reasoning in either rating a claim as relevant/irrelevant to the prompt or rating a claim as supported/not supported by a set of search results. \"Google Search\": the necessary information to rate the fact was not found in the results from the model's Google Search queries. \"Revision error\": the model did not correctly revise a response to be self-contained.", "description": "The figure is a pie chart that shows the percentage of errors in SAFE that are caused by three main factors: reasoning errors, Google Search failures, and revision errors. Reasoning errors account for the largest proportion of errors (50%), followed by Google Search failures (30%) and revision errors (20%).", "section": "A Frequently asked questions"}, {"figure_path": "4M9f8VMt2C/figures/figures_16_1.jpg", "caption": "Figure 8: Presumed causes of crowdsourced human annotators giving an incorrect rating on the 100 disagreement examples with \u201cresearcher + full internet access\u201d ground-truth labels from Section 4.", "description": "This figure shows the presumed causes of errors made by human annotators when evaluating the 100 disagreement cases in Section 4 of the paper.  The causes are categorized as follows: Label confusion (5), Information not given (8), Information missed (30), Misread response (16), and Technicality/reasoning (22). The largest source of error appears to stem from the annotators either misreading the responses or missing information contained in them.  It also shows that a significant portion of annotation errors were caused by annotators confusing the labels themselves.", "section": "4 LLM agents can be better factuality annotators than humans"}, {"figure_path": "4M9f8VMt2C/figures/figures_17_1.jpg", "caption": "Figure 9: While a model's average precision does not obtain statistically-significant correlation with its Chatbot Arena ELO, its average F\u2081@64 does achieve statistically-significant correlation with its ELO. Lines are lines of best fit using np.polyfit. ELO scores recorded on March 18, 2024.", "description": "This figure shows the correlation between a model's performance on the LongFact benchmark and its ELO rating in the LMSys Chatbot Arena.  The blue dots represent the correlation between precision (the percentage of supported facts in a response) and ELO, while the brown dots represent the correlation between F1@64 (a metric that combines precision and recall, considering human-preferred response length) and ELO.  The figure demonstrates that F1@64, which includes recall, has a stronger, statistically significant correlation with human evaluation (as measured by ELO) than precision alone, suggesting that incorporating recall in evaluating long-form factuality better aligns with human judgment.", "section": "Frequently asked questions"}, {"figure_path": "4M9f8VMt2C/figures/figures_18_1.jpg", "caption": "Figure 10: Adding a postamble to LongFact-Objects that asks the model to provide as many specific details as possible lowers the factual precision of responses (F\u2081@1 is lower when the postamble is included) and encourages models to provide more facts in their responses (F\u2081 is higher at large K when the postamble is included).", "description": "This figure shows the impact of adding a postamble to the LongFact-Objects prompts on the performance of language models.  The postamble instructed models to provide more detailed responses. The results indicate that while the factual precision (accuracy of individual facts) decreased slightly, adding the postamble significantly increased the recall (number of supported facts retrieved) at larger values of K (the number of facts considered for recall). This demonstrates a trade-off between precision and recall when more facts are solicited from models.  In simpler terms, when models were asked for longer, more detailed answers, they provided more total facts, but had a slightly lower accuracy on the facts given.", "section": "A Frequently asked questions"}, {"figure_path": "4M9f8VMt2C/figures/figures_18_2.jpg", "caption": "Figure 10: Adding a postamble to LongFact-Objects that asks the model to provide as many specific details as possible lowers the factual precision of responses (F\u2081@1 is lower when the postamble is included) and encourages models to provide more facts in their responses (F\u2081 is higher at large K when the postamble is included).", "description": "This figure shows the impact of adding a postamble to the LongFact-Objects prompts on the performance of language models.  The postamble instructs the model to provide as many specific details as possible. The results show that adding the postamble slightly reduces precision (accuracy of individual facts) at K=1 (measuring only precision). However, at higher values of K, the postamble improves the F1 score (harmonic mean of precision and recall), indicating an increase in recall (proportion of relevant facts retrieved). This suggests that the postamble, while slightly reducing accuracy for concise answers, encourages models to generate longer responses with more factual information.", "section": "A Frequently asked questions"}, {"figure_path": "4M9f8VMt2C/figures/figures_19_1.jpg", "caption": "Figure 1: Our automatic factuality evaluator, SAFE, uses a large language model to rate the factuality of a long-form response to a given prompt using Google Search. We empirically demonstrate that SAFE outperforms human annotators while being more than 20 times cheaper (Section 4).", "description": "The figure illustrates the SAFE (Search-Augmented Factuality Evaluator) process.  SAFE uses an LLM to break down a long-form response into individual facts. It then checks the relevance of each fact and assesses its accuracy using Google Search.  The evaluation results demonstrate that SAFE is both more accurate and significantly cheaper than using human annotators for this task.", "section": "1 Introduction"}, {"figure_path": "4M9f8VMt2C/figures/figures_32_1.jpg", "caption": "Figure 1: Our automatic factuality evaluator, SAFE, uses a large language model to rate the factuality of a long-form response to a given prompt using Google Search. We empirically demonstrate that SAFE outperforms human annotators while being more than 20 times cheaper (Section 4).", "description": "The figure illustrates the SAFE (Search-Augmented Factuality Evaluator) process.  SAFE uses an LLM to break down a long-form response into individual facts.  It then uses the LLM to formulate search queries for each fact, using Google Search to determine if the fact is supported by the search results. The figure shows an example of a prompt and response being processed by SAFE, along with the individual facts extracted, Google Search queries generated, and the final output indicating the number of supported and unsupported facts.", "section": "Search-Augmented Factuality Evaluator (SAFE)"}, {"figure_path": "4M9f8VMt2C/figures/figures_42_1.jpg", "caption": "Figure 12: Allowing the language model in SAFE to issue at least five search queries achieves the highest correlation with human annotators for the number of supported and not-supported facts in a response. Further increases do not improve performance and are therefore unnecessary. We primarily focus on the correlation score for the number of supported and not-supported facts in responses because as we found in Appendix A.4, human annotators often mislabeled both supported and not-supported facts as \u201cirrelevant.\u201d All tested SAFE configurations use three as the number of search results returned for each search query issued.", "description": "This figure shows the correlation between the number of search queries used in SAFE and the number of supported and not-supported facts. The results indicate that using at least five search queries yields the best correlation with human annotations, while increasing the number of queries beyond five does not lead to significant improvement.", "section": "C.8 Hyperparameter ablation"}, {"figure_path": "4M9f8VMt2C/figures/figures_42_2.jpg", "caption": "Figure 12: Allowing the language model in SAFE to issue at least five search queries achieves the highest correlation with human annotators for the number of supported and not-supported facts in a response. Further increases do not improve performance and are therefore unnecessary. We primarily focus on the correlation score for the number of supported and not-supported facts in responses because as we found in Appendix A.4, human annotators often mislabeled both supported and not-supported facts as \u201cirrelevant.\u201d All tested SAFE configurations use three as the number of search results returned for each search query issued.", "description": "This figure displays the correlation between the number of search queries used in the SAFE model and the accuracy of the model's annotations. The results indicate that using at least five search queries yields the highest correlation with human annotations, and increasing the number of queries beyond five does not significantly improve the model's accuracy. The focus is primarily on supported and not-supported facts since Appendix A.4 revealed that human annotators frequently mislabeled these facts as irrelevant.  All tests kept the number of returned search results per query constant at 3.", "section": "C.8 Hyperparameter ablation"}, {"figure_path": "4M9f8VMt2C/figures/figures_46_1.jpg", "caption": "Figure 14: Longer responses produced by GPT-4-Turbo for long-form factuality prompts have slightly-lower factual precision.", "description": "This figure shows the relationship between the number of supported facts and precision for GPT-4-Turbo's responses to long-form factuality prompts.  The x-axis represents the number of supported facts in a response, and the y-axis represents the precision (percentage of supported facts out of all facts). The plot shows that as the number of supported facts increases, the precision tends to slightly decrease.  This indicates a trade-off between recall (number of supported facts) and precision (accuracy of facts) when generating longer responses.  Longer responses, while having higher recall, may sacrifice a bit of precision.", "section": "D.4 Factual precision-recall curves"}, {"figure_path": "4M9f8VMt2C/figures/figures_53_1.jpg", "caption": "Figure 15: Claude-3 models achieve higher factual precision on LongFact-Objects than Claude models from previous generations.", "description": "The figure shows a bar chart comparing the factual precision of different Claude language models on the LongFact-Objects dataset.  It demonstrates that the newer Claude-3 models (Claude-3-Haiku, Claude-3-Sonnet, and Claude-3-Opus) significantly outperform earlier generations (Claude-Instant, Claude-2.0, and Claude-2.1) in terms of factual precision.  This suggests that improvements in model architecture or training techniques have led to better factual accuracy in the newer models.", "section": "E.2 Scaling large language models improves long-form factuality"}, {"figure_path": "4M9f8VMt2C/figures/figures_53_2.jpg", "caption": "Figure 6: Long-form factuality performance of thirteen large language models across four model families (Gemini, GPT, Claude, and PaLM-2), sorted by F\u2081@K in descending order. F\u2081@K averaged over SAFE (Section 3) evaluation results on model responses to the same set of 250 random prompts from LongFact-Objects. F\u2081@K was calculated with K = 64 (the median number of facts among all model responses) and K = 178 (the maximum number of facts among all model responses). The ranking of models remains relatively stable at sufficiently-large K values (more discussion in Appendix E.3). Raw metrics are shown in Table 2.", "description": "This figure presents the long-form factuality performance of 13 large language models from 4 different families (Gemini, GPT, Claude, and PaLM-2).  The models are ranked in descending order based on their F1@K score, a metric that combines precision and recall of factual information in long-form responses.  Two different values of K (64 and 178) are used, representing the median and maximum number of facts in the responses, respectively. The chart visually demonstrates how the larger models generally achieve better long-form factuality, although some newer models from different families (like Claude-3) match or even surpass the performance of GPT-4.", "section": "6 Larger LLMs are more factual"}, {"figure_path": "4M9f8VMt2C/figures/figures_54_1.jpg", "caption": "Figure 6: Long-form factuality performance of thirteen large language models across four model families (Gemini, GPT, Claude, and PaLM-2), sorted by F\u2081@K in descending order. F\u2081@K averaged over SAFE (Section 3) evaluation results on model responses to the same set of 250 random prompts from LongFact-Objects. F\u2081@K was calculated with K = 64 (the median number of facts among all model responses) and K = 178 (the maximum number of facts among all model responses). The ranking of models remains relatively stable at sufficiently-large K values (more discussion in Appendix E.3). Raw metrics are shown in Table 2.", "description": "This figure presents the results of a benchmark evaluating thirteen large language models' performance on long-form factuality using two different metrics (F\u2081@64 and F\u2081@178).  The models are categorized into four families (Gemini, GPT, Claude, and PaLM-2). The F\u2081 score is a combined measure of precision (percentage of supported facts) and recall (percentage of provided facts relative to a user-preferred length), calculated using the SAFE evaluation method.  The figure shows that larger language models generally perform better, though the relationships between model families and their performance are complex.  The stability of model rankings at higher K values suggests the results are relatively robust to variations in human-preferred response length.", "section": "6 Larger LLMs are more factual"}, {"figure_path": "4M9f8VMt2C/figures/figures_54_2.jpg", "caption": "Figure 18: PaLM-2-L-IT-RLHF achieves higher F\u2081@K than PaLM-2-L-IT at all K. The difference in F\u2081@K is more significant at larger values of K, indicating that RLHF can increase factual recall.", "description": "This figure shows the performance comparison of two PaLM-2 language models, one with RLHF (Reinforcement Learning from Human Feedback) and one without.  The x-axis represents the value of K, which is a hyperparameter determining the number of supported facts required for full recall in the F\u2081@K metric. The y-axis shows the F\u2081@K score (a metric combining precision and recall).  The results show that the RLHF model consistently outperforms the non-RLHF model across all K values. The difference in performance is more pronounced for larger values of K, suggesting that RLHF improves the model's ability to generate responses with more factual information (higher recall).", "section": "E.4 RLHF improves long-form factuality"}]