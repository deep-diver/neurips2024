[{"figure_path": "4M9f8VMt2C/tables/tables_6_1.jpg", "caption": "Table 2: Full benchmarking results on the same 250 random prompts from LongFact-Objects for the thirteen language models evaluated in Section 6. Metrics are averaged over all prompts. Raw metrics are obtained from evaluating responses using SAFE (see Section 3). S, NS, and I denote the number of supported, not-supported, and irrelevant facts, respectively. Aggregated metrics are precision (Prec), recall (RK), and F\u2081@K at K = 64 and K = 178 (see Section 5). The bold numbers are the best of each metric.", "description": "This table presents a comprehensive benchmark of thirteen large language models across four model families (Gemini, GPT, Claude, and PaLM-2) on the LongFact-Objects dataset.  It shows both raw metrics (number of supported, not-supported, and irrelevant facts in model responses) and aggregated metrics (precision, recall at k=64 and k=178, and F1 scores at k=64 and k=178) to evaluate long-form factuality.  The models are ranked by their performance on these metrics, highlighting the best-performing models for each metric.", "section": "6 Larger LLMs are more factual"}, {"figure_path": "4M9f8VMt2C/tables/tables_7_1.jpg", "caption": "Table 2: Full benchmarking results on the same 250 random prompts from LongFact-Objects for the thirteen language models evaluated in Section 6. Metrics are averaged over all prompts. Raw metrics are obtained from evaluating responses using SAFE (see Section 3). S, NS, and I denote the number of supported, not-supported, and irrelevant facts, respectively. Aggregated metrics are precision (Prec), recall (RK), and F\u2081@K at K = 64 and K = 178 (see Section 5). The bold numbers are the best of each metric.", "description": "This table presents a comprehensive benchmark of thirteen large language models across four model families (Gemini, GPT, Claude, and PaLM-2) on a subset of LongFact-Objects prompts.  The raw metrics (S, NS, I) represent the number of supported, not-supported and irrelevant facts found by SAFE in the model's responses. Aggregated metrics include precision, recall at K=64 (median number of facts) and K=178 (maximum number of facts), and the F1 score calculated at both K values.  The best performing models for each metric are highlighted in bold.", "section": "6 Larger LLMs are more factual"}, {"figure_path": "4M9f8VMt2C/tables/tables_20_1.jpg", "caption": "Table 2: Full benchmarking results on the same 250 random prompts from LongFact-Objects for the thirteen language models evaluated in Section 6. Metrics are averaged over all prompts. Raw metrics are obtained from evaluating responses using SAFE (see Section 3). S, NS, and I denote the number of supported, not-supported, and irrelevant facts, respectively. Aggregated metrics are precision (Prec), recall (RK), and F\u2081@K at K = 64 and K = 178 (see Section 5). The bold numbers are the best of each metric.", "description": "This table presents the results of benchmarking thirteen large language models on a subset of LongFact-Objects prompts.  It shows the performance of each model in terms of raw metrics (number of supported, not-supported and irrelevant facts) and aggregated metrics (precision, recall at K=64 and K=178, and F1 score at K=64 and K=178).  The raw metrics are obtained using the Search-Augmented Factuality Evaluator (SAFE) method described in the paper. K represents the median and maximum number of facts found in human-preferred responses, respectively. The bold numbers highlight the best performing models for each metric.", "section": "6 Larger LLMs are more factual"}, {"figure_path": "4M9f8VMt2C/tables/tables_21_1.jpg", "caption": "Table 2: Full benchmarking results on the same 250 random prompts from LongFact-Objects for the thirteen language models evaluated in Section 6. Metrics are averaged over all prompts. Raw metrics are obtained from evaluating responses using SAFE (see Section 3). S, NS, and I denote the number of supported, not-supported, and irrelevant facts, respectively. Aggregated metrics are precision (Prec), recall (RK), and F\u2081@K at K = 64 and K = 178 (see Section 5). The bold numbers are the best of each metric.", "description": "This table presents the results of benchmarking thirteen large language models across four model families (Gemini, GPT, Claude, and PaLM-2) on a subset of 250 prompts from the LongFact-Objects dataset.  The models' responses were evaluated using the Search-Augmented Factuality Evaluator (SAFE).  The table shows both raw metrics (number of supported, not-supported, and irrelevant facts) and aggregated metrics (precision, recall, and F1 scores at K=64 and K=178, where K represents the human-preferred number of facts).  The F1 score combines precision and recall to give an overall measure of factuality, and the bold numbers highlight the best performance for each metric.", "section": "6 Larger LLMs are more factual"}, {"figure_path": "4M9f8VMt2C/tables/tables_22_1.jpg", "caption": "Table 2: Full benchmarking results on the same 250 random prompts from LongFact-Objects for the thirteen language models evaluated in Section 6. Metrics are averaged over all prompts. Raw metrics are obtained from evaluating responses using SAFE (see Section 3). S, NS, and I denote the number of supported, not-supported, and irrelevant facts, respectively. Aggregated metrics are precision (Prec), recall (RK), and F\u2081@K at K = 64 and K = 178 (see Section 5). The bold numbers are the best of each metric.", "description": "This table presents a comprehensive benchmark of thirteen large language models across four model families (Gemini, GPT, Claude, and PaLM-2) on a subset of 250 prompts from the LongFact-Objects dataset.  For each model, it shows raw metrics (number of supported, not-supported, and irrelevant facts) calculated using the SAFE evaluation method, along with aggregated metrics such as precision, recall at two different recall thresholds (K=64 and K=178), and the F1 score (combining precision and recall) at those thresholds. The bold numbers highlight the best performance for each metric, offering a direct comparison of models based on their long-form factuality performance.", "section": "6 Larger LLMs are more factual"}, {"figure_path": "4M9f8VMt2C/tables/tables_23_1.jpg", "caption": "Table 2: Full benchmarking results on the same 250 random prompts from LongFact-Objects for the thirteen language models evaluated in Section 6. Metrics are averaged over all prompts. Raw metrics are obtained from evaluating responses using SAFE (see Section 3). S, NS, and I denote the number of supported, not-supported, and irrelevant facts, respectively. Aggregated metrics are precision (Prec), recall (RK), and F\u2081@K at K = 64 and K = 178 (see Section 5). The bold numbers are the best of each metric.", "description": "This table presents a comprehensive benchmark of thirteen large language models across four families (Gemini, GPT, Claude, and PaLM-2) on a subset of prompts from the LongFact-Objects dataset.  It shows both raw metrics (number of supported, not-supported, and irrelevant facts) and aggregated metrics (precision, recall, and F1@K for K=64 and K=178) derived from the SAFE evaluation method. The F1@K score balances precision and recall, considering both the factual accuracy and the length of the response. The table helps assess the long-form factuality performance of various models, indicating which achieve better precision, recall, or an overall F1 score.", "section": "6 Larger LLMs are more factual"}, {"figure_path": "4M9f8VMt2C/tables/tables_24_1.jpg", "caption": "Table 2: Full benchmarking results on the same 250 random prompts from LongFact-Objects for the thirteen language models evaluated in Section 6. Metrics are averaged over all prompts. Raw metrics are obtained from evaluating responses using SAFE (see Section 3). S, NS, and I denote the number of supported, not-supported, and irrelevant facts, respectively. Aggregated metrics are precision (Prec), recall (RK), and F\u2081@K at K = 64 and K = 178 (see Section 5). The bold numbers are the best of each metric.", "description": "This table presents a comprehensive benchmark of thirteen large language models across four model families (Gemini, GPT, Claude, and PaLM-2) on the LongFact-Objects dataset.  It shows the raw metrics (number of supported, not-supported, and irrelevant facts) and aggregated metrics (precision, recall, and F1 scores at K=64 and K=178) calculated using the SAFE evaluation method.  The F1 score balances the precision (accuracy of the facts) and recall (completeness of the factual information provided, considering a human-preferred length) of the model's responses. The bold numbers indicate the best performance for each metric.", "section": "6 Larger LLMs are more factual"}, {"figure_path": "4M9f8VMt2C/tables/tables_25_1.jpg", "caption": "Table 2: Full benchmarking results on the same 250 random prompts from LongFact-Objects for the thirteen language models evaluated in Section 6. Metrics are averaged over all prompts. Raw metrics are obtained from evaluating responses using SAFE (see Section 3). S, NS, and I denote the number of supported, not-supported, and irrelevant facts, respectively. Aggregated metrics are precision (Prec), recall (RK), and F\u2081@K at K = 64 and K = 178 (see Section 5). The bold numbers are the best of each metric.", "description": "This table presents a comprehensive benchmark of thirteen large language models across four model families (Gemini, GPT, Claude, and PaLM-2) on a subset of 250 prompts from the LongFact-Objects dataset.  It provides both raw metrics (number of supported, not-supported, and irrelevant facts) and aggregated metrics (precision, recall at K=64 and K=178, and F1 scores at K=64 and K=178) for each model's performance. The bold numbers highlight the best performance for each metric across all models.", "section": "6 Larger LLMs are more factual"}, {"figure_path": "4M9f8VMt2C/tables/tables_26_1.jpg", "caption": "Table 2: Full benchmarking results on the same 250 random prompts from LongFact-Objects for the thirteen language models evaluated in Section 6. Metrics are averaged over all prompts. Raw metrics are obtained from evaluating responses using SAFE (see Section 3). S, NS, and I denote the number of supported, not-supported, and irrelevant facts, respectively. Aggregated metrics are precision (Prec), recall (RK), and F\u2081@K at K = 64 and K = 178 (see Section 5). The bold numbers are the best of each metric.", "description": "This table presents the results of benchmarking thirteen large language models on a subset of LongFact-Objects prompts.  The models are evaluated using the SAFE method, and the results are aggregated using different metrics. The table displays both the raw metrics and the aggregated metrics, showing performance in terms of supported, not-supported, and irrelevant facts for each model.  The best-performing model for each metric is highlighted.", "section": "6 Larger LLMs are more factual"}, {"figure_path": "4M9f8VMt2C/tables/tables_33_1.jpg", "caption": "Table 2: Full benchmarking results on the same 250 random prompts from LongFact-Objects for the thirteen language models evaluated in Section 6. Metrics are averaged over all prompts. Raw metrics are obtained from evaluating responses using SAFE (see Section 3). S, NS, and I denote the number of supported, not-supported, and irrelevant facts, respectively. Aggregated metrics are precision (Prec), recall (RK), and F\u2081@K at K = 64 and K = 178 (see Section 5). The bold numbers are the best of each metric.", "description": "This table presents a comprehensive evaluation of thirteen large language models across four families (Gemini, GPT, Claude, and PaLM-2) using the LongFact-Objects dataset.  It shows both raw metrics (number of supported, not-supported, and irrelevant facts in model responses) and aggregated metrics (precision, recall at K=64 and K=178, and F1 scores at K=64 and K=178). The raw metrics provide a detailed breakdown of the model's factual accuracy, while the aggregated metrics offer a summarized view of the overall long-form factuality performance.  Bold numbers highlight the best performance for each metric.", "section": "6 Larger LLMs are more factual"}, {"figure_path": "4M9f8VMt2C/tables/tables_34_1.jpg", "caption": "Table 2: Full benchmarking results on the same 250 random prompts from LongFact-Objects for the thirteen language models evaluated in Section 6. Metrics are averaged over all prompts. Raw metrics are obtained from evaluating responses using SAFE (see Section 3). S, NS, and I denote the number of supported, not-supported, and irrelevant facts, respectively. Aggregated metrics are precision (Prec), recall (RK), and F\u2081@K at K = 64 and K = 178 (see Section 5). The bold numbers are the best of each metric.", "description": "This table presents a comprehensive benchmark of thirteen large language models across four model families (Gemini, GPT, Claude, and PaLM-2) on a subset of 250 prompts from the LongFact-Objects dataset.  For each model and each prompt, the evaluation metrics from SAFE (Search-Augmented Factuality Evaluator) are calculated. Raw metrics include the number of supported facts (S), not-supported facts (NS), and irrelevant facts (I).  Aggregated metrics show precision (Prec), recall at K=64 (R64) and K=178 (R178), and the F1 score at K=64 (F1@64) and K=178 (F1@178). The F1 score, incorporating both precision and recall, measures the long-form factuality, and different K values represent different lengths of ideal responses.  The bold numbers highlight the best performance for each metric.", "section": "6 Larger LLMs are more factual"}, {"figure_path": "4M9f8VMt2C/tables/tables_35_1.jpg", "caption": "Table 2: Full benchmarking results on the same 250 random prompts from LongFact-Objects for the thirteen language models evaluated in Section 6. Metrics are averaged over all prompts. Raw metrics are obtained from evaluating responses using SAFE (see Section 3). S, NS, and I denote the number of supported, not-supported, and irrelevant facts, respectively. Aggregated metrics are precision (Prec), recall (RK), and F\u2081@K at K = 64 and K = 178 (see Section 5). The bold numbers are the best of each metric.", "description": "This table presents a comprehensive evaluation of thirteen large language models across four model families (Gemini, GPT, Claude, and PaLM-2) on a subset of 250 prompts from the LongFact-Objects benchmark.  It shows both raw metrics (number of supported, not-supported, and irrelevant facts) and aggregated metrics (precision, recall, and F1 scores at two different recall thresholds (K=64 and K=178)). The F1 score incorporates both precision and recall to provide a more holistic measure of long-form factuality.  Bold numbers highlight the best-performing model for each metric.", "section": "6 Larger LLMs are more factual"}, {"figure_path": "4M9f8VMt2C/tables/tables_36_1.jpg", "caption": "Table 2: Full benchmarking results on the same 250 random prompts from LongFact-Objects for the thirteen language models evaluated in Section 6. Metrics are averaged over all prompts. Raw metrics are obtained from evaluating responses using SAFE (see Section 3). S, NS, and I denote the number of supported, not-supported, and irrelevant facts, respectively. Aggregated metrics are precision (Prec), recall (RK), and F\u2081@K at K = 64 and K = 178 (see Section 5). The bold numbers are the best of each metric.", "description": "This table presents a comprehensive benchmark of thirteen large language models across four model families (Gemini, GPT, Claude, and PaLM-2) on a subset of LongFact-Objects prompts.  It shows raw metrics (number of supported, not-supported, and irrelevant facts) and aggregated metrics (precision, recall, and F1@K, calculated at K=64 and K=178, representing median and maximum number of facts respectively).  The F1@K score balances precision and recall in evaluating long-form factuality, combining accuracy and completeness of information provided.  Bold values highlight the best performance for each metric across all models.", "section": "6 Larger LLMs are more factual"}, {"figure_path": "4M9f8VMt2C/tables/tables_37_1.jpg", "caption": "Table 2: Full benchmarking results on the same 250 random prompts from LongFact-Objects for the thirteen language models evaluated in Section 6. Metrics are averaged over all prompts. Raw metrics are obtained from evaluating responses using SAFE (see Section 3). S, NS, and I denote the number of supported, not-supported, and irrelevant facts, respectively. Aggregated metrics are precision (Prec), recall (RK), and F\u2081@K at K = 64 and K = 178 (see Section 5). The bold numbers are the best of each metric.", "description": "This table presents a comprehensive evaluation of thirteen large language models across four families (Gemini, GPT, Claude, and PaLM-2) on the LongFact-Objects benchmark.  It shows both raw metrics (number of supported, not-supported, and irrelevant facts) and aggregated metrics (precision, recall at K=64 and K=178, and F1 score at K=64 and K=178). The raw metrics reflect the individual fact-level assessments from the SAFE evaluation method.  Aggregated metrics provide a holistic view of the models' performance considering both the accuracy (precision) and completeness (recall) of their long-form responses. The bolded numbers highlight the top performers for each metric.", "section": "6 Larger LLMs are more factual"}, {"figure_path": "4M9f8VMt2C/tables/tables_38_1.jpg", "caption": "Table 2: Full benchmarking results on the same 250 random prompts from LongFact-Objects for the thirteen language models evaluated in Section 6. Metrics are averaged over all prompts. Raw metrics are obtained from evaluating responses using SAFE (see Section 3). S, NS, and I denote the number of supported, not-supported, and irrelevant facts, respectively. Aggregated metrics are precision (Prec), recall (RK), and F\u2081@K at K = 64 and K = 178 (see Section 5). The bold numbers are the best of each metric.", "description": "This table presents a comprehensive benchmark of thirteen large language models across four model families (Gemini, GPT, Claude, and PaLM-2) on a subset of 250 prompts from the LongFact-Objects dataset.  For each model, it shows raw metrics (number of supported, not-supported, and irrelevant facts) obtained using the SAFE evaluation method.  It further provides aggregated metrics, including precision, recall at different response lengths (K=64 and K=178), and the F1 score (F1@K), reflecting the balance between precision and recall.  The bold numbers highlight the best performance across each metric.", "section": "6 Larger LLMs are more factual"}, {"figure_path": "4M9f8VMt2C/tables/tables_39_1.jpg", "caption": "Table 2: Full benchmarking results on the same 250 random prompts from LongFact-Objects for the thirteen language models evaluated in Section 6. Metrics are averaged over all prompts. Raw metrics are obtained from evaluating responses using SAFE (see Section 3). S, NS, and I denote the number of supported, not-supported, and irrelevant facts, respectively. Aggregated metrics are precision (Prec), recall (RK), and F\u2081@K at K = 64 and K = 178 (see Section 5). The bold numbers are the best of each metric.", "description": "This table presents a comprehensive benchmark of thirteen large language models across four model families (Gemini, GPT, Claude, and PaLM-2) on a subset of 250 prompts from LongFact-Objects.  It details raw metrics (number of supported, not-supported, and irrelevant facts) and aggregated metrics (precision, recall, and F1@K at K=64 and K=178).  The F1@K score combines precision and recall, incorporating a user's preferred response length.  Bold numbers highlight the best performance for each metric.", "section": "6 Larger LLMs are more factual"}, {"figure_path": "4M9f8VMt2C/tables/tables_40_1.jpg", "caption": "Table 2: Full benchmarking results on the same 250 random prompts from LongFact-Objects for the thirteen language models evaluated in Section 6. Metrics are averaged over all prompts. Raw metrics are obtained from evaluating responses using SAFE (see Section 3). S, NS, and I denote the number of supported, not-supported, and irrelevant facts, respectively. Aggregated metrics are precision (Prec), recall (RK), and F\u2081@K at K = 64 and K = 178 (see Section 5). The bold numbers are the best of each metric.", "description": "This table presents a comprehensive benchmark of thirteen large language models across four model families (Gemini, GPT, Claude, and PaLM-2) on the LongFact-Objects dataset.  It shows the raw metrics (number of supported, not-supported, and irrelevant facts) and aggregated metrics (precision, recall at K=64 and K=178, and F1 score at K=64 and K=178) for each model's performance on 250 randomly selected prompts.  The bold numbers indicate the best performance for each metric.", "section": "6 Larger LLMs are more factual"}, {"figure_path": "4M9f8VMt2C/tables/tables_47_1.jpg", "caption": "Table 2: Full benchmarking results on the same 250 random prompts from LongFact-Objects for the thirteen language models evaluated in Section 6. Metrics are averaged over all prompts. Raw metrics are obtained from evaluating responses using SAFE (see Section 3). S, NS, and I denote the number of supported, not-supported, and irrelevant facts, respectively. Aggregated metrics are precision (Prec), recall (RK), and F\u2081@K at K = 64 and K = 178 (see Section 5). The bold numbers are the best of each metric.", "description": "This table presents a comprehensive benchmark of thirteen large language models across four model families (Gemini, GPT, Claude, and PaLM-2) on a subset of LongFact-Objects prompts.  It shows both raw metrics (number of supported, not-supported, and irrelevant facts) and aggregated metrics (precision, recall at K=64 and K=178, and F1 score at K=64 and K=178) for each model's performance.  The bold numbers highlight the top performers for each metric.", "section": "6 Larger LLMs are more factual"}, {"figure_path": "4M9f8VMt2C/tables/tables_48_1.jpg", "caption": "Table 2: Full benchmarking results on the same 250 random prompts from LongFact-Objects for the thirteen language models evaluated in Section 6. Metrics are averaged over all prompts. Raw metrics are obtained from evaluating responses using SAFE (see Section 3). S, NS, and I denote the number of supported, not-supported, and irrelevant facts, respectively. Aggregated metrics are precision (Prec), recall (RK), and F\u2081@K at K = 64 and K = 178 (see Section 5). The bold numbers are the best of each metric.", "description": "This table presents a comprehensive benchmark of thirteen large language models across four families (Gemini, GPT, Claude, and PaLM-2) on a subset of LongFact-Objects prompts.  It shows both raw metrics (number of supported, not-supported, and irrelevant facts) and aggregated metrics (precision, recall, and F\u2081@K, calculated at median and maximum numbers of facts).  The F\u2081@K score combines precision and recall, considering the number of supported facts relative to a chosen threshold (K).  The results indicate the long-form factuality performance of each model. Bold numbers highlight the best performance for each metric.", "section": "6 Larger LLMs are more factual"}, {"figure_path": "4M9f8VMt2C/tables/tables_49_1.jpg", "caption": "Table 2: Full benchmarking results on the same 250 random prompts from LongFact-Objects for the thirteen language models evaluated in Section 6. Metrics are averaged over all prompts. Raw metrics are obtained from evaluating responses using SAFE (see Section 3). S, NS, and I denote the number of supported, not-supported, and irrelevant facts, respectively. Aggregated metrics are precision (Prec), recall (RK), and F\u2081@K at K = 64 and K = 178 (see Section 5). The bold numbers are the best of each metric.", "description": "This table presents a comprehensive benchmark of thirteen large language models across four model families (Gemini, GPT, Claude, and PaLM-2) on a subset of 250 prompts from LongFact-Objects.  For each model and prompt, it shows raw metrics (number of supported, not-supported, and irrelevant facts), and aggregated metrics (precision, recall at K=64 and K=178, and F1-score at K=64 and K=178). The metrics are averaged over all prompts, allowing for a comparison of model performance in terms of factual accuracy and completeness of responses.  Bold numbers highlight the best performance for each metric.", "section": "6 Larger LLMs are more factual"}, {"figure_path": "4M9f8VMt2C/tables/tables_50_1.jpg", "caption": "Table 2: Full benchmarking results on the same 250 random prompts from LongFact-Objects for the thirteen language models evaluated in Section 6. Metrics are averaged over all prompts. Raw metrics are obtained from evaluating responses using SAFE (see Section 3). S, NS, and I denote the number of supported, not-supported, and irrelevant facts, respectively. Aggregated metrics are precision (Prec), recall (RK), and F\u2081@K at K = 64 and K = 178 (see Section 5). The bold numbers are the best of each metric.", "description": "This table presents a comprehensive benchmark of thirteen large language models across four model families (Gemini, GPT, Claude, and PaLM-2) on the LongFact-Objects dataset.  It shows the performance of each model in terms of raw metrics (number of supported, not-supported, and irrelevant facts) and aggregated metrics (precision, recall at K=64 and K=178, and F1 scores at K=64 and K=178). The raw metrics are obtained using the Search-Augmented Factuality Evaluator (SAFE) method.  The table helps to compare the long-form factuality performance of different models and model families.", "section": "6 Larger LLMs are more factual"}, {"figure_path": "4M9f8VMt2C/tables/tables_51_1.jpg", "caption": "Table 2: Full benchmarking results on the same 250 random prompts from LongFact-Objects for the thirteen language models evaluated in Section 6. Metrics are averaged over all prompts. Raw metrics are obtained from evaluating responses using SAFE (see Section 3). S, NS, and I denote the number of supported, not-supported, and irrelevant facts, respectively. Aggregated metrics are precision (Prec), recall (RK), and F\u2081@K at K = 64 and K = 178 (see Section 5). The bold numbers are the best of each metric.", "description": "This table presents a comprehensive benchmark of thirteen large language models across four model families (Gemini, GPT, Claude, and PaLM-2) on the LongFact-Objects dataset.  For each model, it shows raw metrics (number of supported, not-supported, and irrelevant facts), and aggregated metrics (precision, recall at K=64 and K=178, and F1 scores at K=64 and K=178).  The F1 scores combine precision and recall to provide a holistic measure of long-form factuality.  Bold numbers highlight the best performance for each metric.", "section": "6 Larger LLMs are more factual"}, {"figure_path": "4M9f8VMt2C/tables/tables_52_1.jpg", "caption": "Table 2: Full benchmarking results on the same 250 random prompts from LongFact-Objects for the thirteen language models evaluated in Section 6. Metrics are averaged over all prompts. Raw metrics are obtained from evaluating responses using SAFE (see Section 3). S, NS, and I denote the number of supported, not-supported, and irrelevant facts, respectively. Aggregated metrics are precision (Prec), recall (RK), and F\u2081@K at K = 64 and K = 178 (see Section 5). The bold numbers are the best of each metric.", "description": "This table presents a comprehensive benchmark of thirteen large language models across four families (Gemini, GPT, Claude, and PaLM-2) on a subset of 250 prompts from the LongFact-Objects dataset.  The evaluation uses the Search-Augmented Factuality Evaluator (SAFE) method described in the paper.  For each model, the table shows raw metrics (number of supported, not-supported, and irrelevant facts) and aggregated metrics calculated using the F1@K metric (precision, recall at K=64 and K=178).  The F1@K metric balances precision and recall by considering human preferred response length (K). The best performing model for each metric is highlighted in bold.", "section": "6 Larger LLMs are more factual"}, {"figure_path": "4M9f8VMt2C/tables/tables_55_1.jpg", "caption": "Table 2: Full benchmarking results on the same 250 random prompts from LongFact-Objects for the thirteen language models evaluated in Section 6. Metrics are averaged over all prompts. Raw metrics are obtained from evaluating responses using SAFE (see Section 3). S, NS, and I denote the number of supported, not-supported, and irrelevant facts, respectively. Aggregated metrics are precision (Prec), recall (RK), and F\u2081@K at K = 64 and K = 178 (see Section 5). The bold numbers are the best of each metric.", "description": "This table presents a comprehensive benchmark of thirteen large language models across four model families (Gemini, GPT, Claude, and PaLM-2) on a subset of 250 prompts from the LongFact-Objects dataset.  For each model and prompt, it shows raw metrics (number of supported facts (S), not-supported facts (NS), and irrelevant facts (I)), and aggregated metrics (precision (Prec), recall at K=64 (R64) and K=178 (R178), and F1 score at K=64 and K=178 (F1@64, F1@178)).  The K values (64 and 178) represent the median and maximum number of facts across all model responses, respectively. The bold numbers highlight the best performance for each metric.", "section": "6 Larger LLMs are more factual"}]