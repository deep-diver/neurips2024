[{"figure_path": "t3BhmwAzhv/figures/figures_1_1.jpg", "caption": "Figure 1: An example of using object identifiers during the conversation.", "description": "This figure shows a 3D scene with three objects detected and assigned unique identifiers: <OBJ013>, <OBJ023>, and <OBJ032>.  The example demonstrates how the model uses these identifiers to answer a question about the objects in the scene. The first question uses a more complex, descriptive phrasing to locate the trash bin, while the second question uses the object identifier for conciseness and accuracy.", "section": "Reference 3D scene using object identifiers"}, {"figure_path": "t3BhmwAzhv/figures/figures_3_1.jpg", "caption": "Figure 2: Overall model architecture. The model processes a 3D scene's point cloud input by decomposing it into object proposals via a pre-trained detector. Subsequently, the 3D and 2D encoders are employed to extract object-centric representations. After projection layers, they are combined with object identifiers to form the scene embeddings as a sequence of object-level embeddings, which are then fed into the LLM. The assigned unique identifiers enable efficient object referencing in subsequent interactions.", "description": "The figure illustrates the architecture of the Chat-Scene model.  It starts with a 3D scene's point cloud as input.  A detector identifies individual objects within the scene, assigning each a unique object identifier. These objects are then processed by both 3D and 2D encoders to extract features, which are projected into a space compatible with the language model. The object identifiers and encoded features are combined and fed into the language model (LLM) to generate responses that can efficiently reference objects in the 3D scene.", "section": "3.2 Model Architecture"}, {"figure_path": "t3BhmwAzhv/figures/figures_5_1.jpg", "caption": "Figure 2: Overall model architecture. The model processes a 3D scene's point cloud input by decomposing it into object proposals via a pre-trained detector. Subsequently, the 3D and 2D encoders are employed to extract object-centric representations. After projection layers, they are combined with object identifiers to form the scene embeddings as a sequence of object-level embeddings, which are then fed into the LLM. The assigned unique identifiers enable efficient object referencing in subsequent interactions.", "description": "This figure illustrates the architecture of the proposed model, which processes a 3D scene's point cloud.  First, it uses a pre-trained detector to break the scene into object proposals.  Then, 3D and 2D encoders extract object-centric representations from the point cloud and multi-view images, respectively. These representations are projected into the language model's embedding space and combined with unique object identifiers, creating object-level embeddings. Finally, these embeddings are fed into a Large Language Model (LLM) for interaction and object referencing.", "section": "3.2 Model Architecture"}, {"figure_path": "t3BhmwAzhv/figures/figures_8_1.jpg", "caption": "Figure 4: Visualization results of video grounding for video input. \"GT\" denotes the projected 2D masks derived from the ground-truth 3D point cloud mask.", "description": "This figure visualizes the results of video grounding. It shows a comparison between the ground truth (GT) and the model's predictions (\"Ours\") for localizing objects in video frames.  The GT masks are projections of the ground truth 3D point cloud masks onto the 2D video frames. The model's predictions are shown as red boxes, while the GT masks are shown as blue boxes. Two examples are provided showing the model's performance at locating a TV and a blue rectangular bin.", "section": "4.4 Experiments with 2D Video Input"}, {"figure_path": "t3BhmwAzhv/figures/figures_16_1.jpg", "caption": "Figure 5: Visualization results of 3D question answering on ScanQA [2].", "description": "This figure visualizes four examples of 3D question answering on the ScanQA dataset. Each example shows a 3D scene with a question and the model's answer.  The green checkmarks indicate correct answers, while the red 'x' indicates an incorrect answer. The examples demonstrate the model's ability to answer various types of questions related to object properties, location, and counting, highlighting both the strengths and weaknesses of the model's 3D scene understanding capabilities.", "section": "C.1 3D Question Answering"}, {"figure_path": "t3BhmwAzhv/figures/figures_17_1.jpg", "caption": "Figure 2: Overall model architecture. The model processes a 3D scene's point cloud input by decomposing it into object proposals via a pre-trained detector. Subsequently, the 3D and 2D encoders are employed to extract object-centric representations. After projection layers, they are combined with object identifiers to form the scene embeddings as a sequence of object-level embeddings, which are then fed into the LLM. The assigned unique identifiers enable efficient object referencing in subsequent interactions.", "description": "This figure illustrates the architecture of the Chat-Scene model.  It shows how a 3D point cloud is processed: first, a detector identifies objects; then, 3D and 2D encoders extract features for each object.  These features, combined with unique object identifier tokens, are fed into a language model (LLM) as a sequence of object-level embeddings. This allows for efficient referencing and grounding of objects within the scene.", "section": "3.2 Model Architecture"}, {"figure_path": "t3BhmwAzhv/figures/figures_17_2.jpg", "caption": "Figure 2: Overall model architecture. The model processes a 3D scene's point cloud input by decomposing it into object proposals via a pre-trained detector. Subsequently, the 3D and 2D encoders are employed to extract object-centric representations. After projection layers, they are combined with object identifiers to form the scene embeddings as a sequence of object-level embeddings, which are then fed into the LLM. The assigned unique identifiers enable efficient object referencing in subsequent interactions.", "description": "This figure illustrates the architecture of the Chat-Scene model.  The model takes a 3D point cloud as input, which is first processed by an object detector to identify individual objects. Each object is then encoded using both 3D and 2D encoders, which capture different aspects of the object's appearance and spatial relationships.  These object-centric representations are projected into the embedding space of a language model and concatenated with unique object identifier tokens. Finally, these combined embeddings are fed into a language model (LLM) for downstream tasks. The use of object identifiers enables efficient referencing and grounding of objects during interaction with the LLM.", "section": "3.2 Model Architecture"}, {"figure_path": "t3BhmwAzhv/figures/figures_19_1.jpg", "caption": "Figure 2: Overall model architecture. The model processes a 3D scene's point cloud input by decomposing it into object proposals via a pre-trained detector. Subsequently, the 3D and 2D encoders are employed to extract object-centric representations. After projection layers, they are combined with object identifiers to form the scene embeddings as a sequence of object-level embeddings, which are then fed into the LLM. The assigned unique identifiers enable efficient object referencing in subsequent interactions.", "description": "This figure illustrates the architecture of the Chat-Scene model, which processes 3D scene point cloud data.  The process begins with object detection and proposal generation using a pre-trained detector.  Object-centric representations are extracted from both 3D (using a 3D encoder) and 2D (multi-view images, using a 2D encoder) sources. These object representations are projected into the language model's embedding space and combined with unique object identifiers.  The resulting sequence of object-level embeddings is then input into the large language model (LLM) for scene understanding and interaction. The use of object identifiers allows for more efficient object referencing during interactions with the model.", "section": "3.2 Model Architecture"}]