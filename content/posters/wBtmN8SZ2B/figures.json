[{"figure_path": "wBtmN8SZ2B/figures/figures_2_1.jpg", "caption": "Figure 1: (left) An unweighted label tree with two coarse nodes: F, G. F contains two fine classes A, B and G contains three fine classes C, D, E. We cannot embed this in l2 exactly (right).", "description": "This figure demonstrates the limitations of using Euclidean space to represent hierarchical relationships. The left panel shows a simple tree structure with two coarse nodes (F and G) and five fine-grained classes (A, B, C, D, and E) as leaf nodes. The right panel illustrates the impossibility of embedding this tree structure in Euclidean space without introducing distortions in the distances between nodes, which is exemplified by the contradictions to the triangle inequality and the Pythagorean theorem. This inability stems from the bounded nature of Euclidean space in capturing hierarchical structures, providing motivation for the use of hyperbolic space as proposed in the paper.", "section": "2.2 l2-CPCC"}, {"figure_path": "wBtmN8SZ2B/figures/figures_3_1.jpg", "caption": "Figure 2: Using l2-CPCC for structured representation on CIFAR10. CIFAR10 hierarchy (left) has a three level structure with 13 vertices. For a 512-dimensional embedding, we apply l2-CPCC either for the full tree (middle) or the leaf nodes only (right) and plot the ground truth tree metric against pairwise Euclidean centroid distances of the learnt representation. The optimal train CPCC is 1.", "description": "This figure shows the limitations of using Euclidean distance in structured representation learning.  CIFAR-10's class hierarchy is shown on the left. The middle and right panels compare the ground truth tree metric (x-axis) with the pairwise Euclidean centroid distances in the learned feature representation (y-axis) for 512-dimensional embeddings.  The middle panel shows results using the l2-CPCC method applied to the full tree, while the right panel shows results using the method only on the leaf nodes. The optimal CPCC is 1, representing perfect correspondence between the tree structure and the learned feature representation. The figure demonstrates the significant distortion introduced by using Euclidean distances in this context; applying l2-CPCC on only the leaf nodes leads to a lower CPCC.", "section": "Quality of Hierarchical Information"}, {"figure_path": "wBtmN8SZ2B/figures/figures_3_2.jpg", "caption": "Figure 3: Lines on different models for 2-dimensional hyperbolic space.", "description": "This figure compares three different models for representing 2-dimensional hyperbolic space: the hyperboloid model, the Klein model, and the Poincar\u00e9 ball model.  Each model provides a different geometric interpretation of hyperbolic space. The figure shows how lines appear differently in each model, illustrating the distinct geometric properties of each representation.  Understanding these different models is crucial for applying hyperbolic geometry to machine learning tasks, as it allows for representing hierarchical relationships with minimal distortion. The Poincar\u00e9 ball model, in particular, is widely used due to its convenient properties for embedding tree-like structures.", "section": "3.1 Hyperbolic Geometry"}, {"figure_path": "wBtmN8SZ2B/figures/figures_5_1.jpg", "caption": "Figure 4: Evaluation of distortion vs feature dimensions for HypStructure.", "description": "The figure shows the Gromov's hyperbolicity (\u03b4rel) for different embedding dimensions (16, 32, 64, 128, 256, 512) using three different methods: Flat, l2-CPCC, and HypStructure.  Lower \u03b4rel values indicate higher tree-likeness, with 0 representing a perfect tree metric space.  The plot demonstrates that HypStructure consistently achieves lower \u03b4rel values across all dimensions, indicating that it learns more tree-like representations than the other methods, particularly beneficial in lower dimensional settings.", "section": "4.1 Quality of Hierarchical Information"}, {"figure_path": "wBtmN8SZ2B/figures/figures_6_1.jpg", "caption": "Figure 6: Left: Hyperbolic UMAP visualization of CIFAR10's HypStructure representation on Poincar\u00e9 disk. Middle and Right: t-SNE visualization of learnt representations on CIFAR100.", "description": "This figure shows visualizations of learned representations using different techniques. The left panel displays a hyperbolic UMAP visualization of CIFAR10 features learned with HypStructure, highlighting the hierarchical structure in the Poincar\u00e9 disk. The middle and right panels show Euclidean t-SNE visualizations of CIFAR100 features, comparing the representations learned with the standard method (Flat) against those learned with HypStructure. The visualizations demonstrate HypStructure's ability to capture hierarchical relationships between classes, resulting in more compact and semantically meaningful clusters.", "section": "4.2 Classification"}, {"figure_path": "wBtmN8SZ2B/figures/figures_6_2.jpg", "caption": "Figure 7: Left: OOD detection score across various datasets on the CIFAR100 ID dataset. Right: Hyperbolic UMAP of the CIFAR100 (ID) test vs SVHN (OOD) test features learnt from HypStructure with a clear separation in the Poincar\u00e9 disk.", "description": "The figure shows the results of out-of-distribution (OOD) detection experiments.  The left panel presents the area under the receiver operating characteristic curve (AUROC) for various OOD datasets using CIFAR100 as the in-distribution (ID) dataset. The right panel visualizes the learned features from the HypStructure model using UMAP in the Poincar\u00e9 disk. It highlights the clear separation between the ID and OOD data points, indicating that HypStructure effectively learns features that are useful for OOD detection.", "section": "4.3 OOD Detection"}, {"figure_path": "wBtmN8SZ2B/figures/figures_7_1.jpg", "caption": "Figure 8: CIFAR100 as in-distribution dataset. Left (a): Hierarchical block pattern of K. Middle (b): Top 100 eigenvalues of K for different representation. Right (c): OOD detection for CIFAR100 vs. SVHN with the top k-th principal component.", "description": "This figure shows the eigenspectrum analysis of structured representations. The left panel (a) visualizes the hierarchical block pattern of the covariance matrix K. The middle panel (b) plots the top 100 eigenvalues of K for different representation methods (Flat, l2-CPCC, and HypStructure). The right panel (c) illustrates the OOD detection performance when using the top k principal components for CIFAR100 vs. SVHN.", "section": "5 Eigenspectrum Analysis of Structured Representations"}, {"figure_path": "wBtmN8SZ2B/figures/figures_19_1.jpg", "caption": "Figure 9: Subtree corresponds to the shown submatrix of K.", "description": "This figure shows a subtree corresponding to a submatrix of K.  The figure helps to visualize the hierarchical block structure of matrix K, a key element in proving Theorem 5.1, which analyzes the eigenspectrum of structured representations with a balanced label tree. The structure visually explains how the matrix's entries relate to the hierarchy within the tree. ", "section": "A Details of Eigenspectrum Analysis"}, {"figure_path": "wBtmN8SZ2B/figures/figures_26_1.jpg", "caption": "Figure 10: Example of a \u03b4-slim triangle, where each side of \u25b3ABC is the geodesic distance of two points in the metric space.", "description": "This figure illustrates the concept of a \u03b4-slim triangle in a metric space.  A \u03b4-slim triangle is one where each side of the triangle lies within a distance \u03b4 of the union of the other two sides. The left panel shows a geodesic triangle (a triangle whose sides are geodesics, or shortest paths) in a curved space.  The sides of the triangle are not straight lines; rather they curve. The dotted lines represent a distance \u03b4 from each side of the triangle.  The shaded region depicts the union of these dotted regions. Observe that all three sides of the triangle are contained within this region. The right panel shows a comparison; it depicts a triangle embedded in a tree-like structure where the sides are simply the lengths of the branches of the tree.  This emphasizes that in spaces with high curvature, triangles can look more tree-like, hence the concept of \u03b4-slimness.", "section": "3.1 Hyperbolic Geometry"}, {"figure_path": "wBtmN8SZ2B/figures/figures_28_1.jpg", "caption": "Figure 6: Left: Hyperbolic UMAP visualization of CIFAR10's HypStructure representation on Poincar\u00e9 disk. Middle and Right: t-SNE visualization of learnt representations on CIFAR100.", "description": "This figure visualizes the learned representations from the HypStructure model and two baseline models on CIFAR10 and CIFAR100 datasets. The left panel shows a Hyperbolic UMAP visualization of CIFAR10 features on a Poincar\u00e9 disk, illustrating the spatial arrangement of features learned by HypStructure. The middle and right panels display Euclidean t-SNE visualizations of CIFAR100 features learned by a flat model and HypStructure, respectively.  These visualizations help to demonstrate the impact of HypStructure on the organization and separation of features in the feature space, highlighting improved cluster formation and separation of semantically related classes with HypStructure.", "section": "4.2 Classification"}, {"figure_path": "wBtmN8SZ2B/figures/figures_28_2.jpg", "caption": "Figure 12: Hyperbolic UMAP Visualizations on CIFAR100 and ImageNet100.", "description": "The figure shows hyperbolic UMAP visualizations of learned representations on CIFAR100 and ImageNet100 datasets using the HypStructure method. It demonstrates how the learned features are organized in the hyperbolic space, exhibiting a hierarchical structure that reflects the underlying label hierarchy.", "section": "C.4 Visualization of Learned Features"}, {"figure_path": "wBtmN8SZ2B/figures/figures_29_1.jpg", "caption": "Figure 7: Left: OOD detection score across various datasets on the CIFAR100 ID dataset. Right: Hyperbolic UMAP of the CIFAR100 (ID) test vs SVHN (OOD) test features learnt from HypStructure with a clear separation in the Poincar\u00e9 disk.", "description": "The figure shows the AUROC (Area Under the Receiver Operating Characteristic curve) scores for out-of-distribution (OOD) detection on various datasets using CIFAR100 as the in-distribution dataset.  The left panel displays a bar chart showing the AUROC for several methods.  The right panel displays a UMAP visualization of the learned features in the hyperbolic space.  This visualization shows a clear separation between the in-distribution (CIFAR100) and out-of-distribution (SVHN) samples, illustrating the effectiveness of HypStructure in improving OOD detection performance.", "section": "4.3 OOD Detection"}, {"figure_path": "wBtmN8SZ2B/figures/figures_29_2.jpg", "caption": "Figure 14: Hyperbolic UMAP Visualizations on CIFAR100 using HypStructure without embedding the internal nodes and a hyperbolic centering loss (left), and with embedding the internal nodes along with a centering loss (right).", "description": "This figure compares two UMAP visualizations of CIFAR100 features learned with HypStructure.  The left panel shows the results when only leaf nodes are used in the hyperbolic CPCC loss and there's no centering loss. The right panel shows results when all internal nodes are included in the loss, and a centering loss is also used.  The visualizations highlight the difference in how the hierarchical structure is captured by HypStructure with and without these components.", "section": "C.5 Effect of Centering Loss and Embedding Internal Node"}, {"figure_path": "wBtmN8SZ2B/figures/figures_30_1.jpg", "caption": "Figure 15: Hyperbolic UMAP Visualizations on ImageNet100 using HypStructure without embedding the internal nodes and a hyperbolic centering loss (left), and with embedding the internal nodes along with a centering loss (right).", "description": "This figure shows the results of applying UMAP to visualize the learned features from HypStructure on the ImageNet100 dataset.  The left panel shows the visualization when only leaf nodes are considered in the CPCC loss, while the right panel shows the visualization when all internal nodes are included in the CPCC loss. The inclusion of internal nodes and the centering loss lead to a more representative and organized visualization of the hierarchical relationships in the data.", "section": "C.5 Effect of Centering Loss and Embedding Internal Node"}, {"figure_path": "wBtmN8SZ2B/figures/figures_30_2.jpg", "caption": "Figure 6: Left: Hyperbolic UMAP visualization of CIFAR10\u2019s HypStructure representation on Poincar\u00e9 disk. Middle and Right: t-SNE visualization of learnt representations on CIFAR100.", "description": "This figure visualizes the learned representations of CIFAR-10 and CIFAR-100 datasets using different techniques. The left panel shows a hyperbolic UMAP visualization of CIFAR-10 features on a Poincar\u00e9 disk, highlighting the spatial arrangement of data points according to their hierarchical structure. The middle and right panels display t-SNE visualizations of CIFAR-100 representations, comparing the results obtained using the proposed HypStructure method with a standard flat representation. These visualizations demonstrate the effectiveness of HypStructure in capturing the hierarchical relationships between classes and producing more interpretable and structured representations.", "section": "4.1 Quality of Hierarchical Information"}]