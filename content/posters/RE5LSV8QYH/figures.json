[{"figure_path": "RE5LSV8QYH/figures/figures_3_1.jpg", "caption": "Figure 1: \u0399\u03bc.", "description": "This figure shows the components of the information profile vector I\u03bc for three variables, X, Y, Z.  The components represent different information-theoretic quantities:  conditional entropy (H(Y|X,Z), H(Y|X), H(Z|X), H(Z|Y)), mutual information (I(X;Y|Z), I(X;Z|Y), I(Y;Z|X), I(X;Y), I(X;Z), I(Y;Z)), and interaction information I(X;Y;Z).  These quantities measure various aspects of independence and dependence between the variables. The figure is referenced in the context of explaining how an arbitrary conjunction of conditional (in)dependencies can be expressed as a constraint I\u03bc \u00b7 v \u2265 0, with the appropriate choice of coefficient vector v.", "section": "4 QIM-Compatibility and Information Theory"}, {"figure_path": "RE5LSV8QYH/figures/figures_7_1.jpg", "caption": "Figure 1: \u0399\u03bc.", "description": "This figure shows the information theoretic quantities involved in the information profile of a distribution \u03bc, such as conditional entropy, mutual information, and conditional mutual information. The quantities are represented as areas in a Venn diagram, illustrating how these quantities interact and relate to one another.", "section": "4 QIM-Compatibility and Information Theory"}, {"figure_path": "RE5LSV8QYH/figures/figures_7_2.jpg", "caption": "Figure 1: \u0399\u03bc.", "description": "This figure illustrates the information profile of \u03bc for three variables, X, Y, Z, in the form of a Venn diagram.  Each area in the Venn diagram represents a component of the information profile, which are components of a coefficient vector used in conjunction with the qualitative PDG scoring function (IDef). These components are: the conditional entropy of Y given X and Z (H(Y|X,Z)), the conditional entropy of Z given X and Y (H(Z|X,Y)), the mutual information between X and (Y,Z) (I(X; YZ)), the interaction information between X, Y, and Z (I(X; Y; Z)), the conditional mutual information of Y and Z given X (I(Y;Z|X)), the entropy of X (H(X)), the entropy of Y (H(Y)), the entropy of Z (H(Z)), the joint entropy of X and Y (H(XY)), the joint entropy of Y and Z (H(YZ)), and the joint entropy of X and Z (H(XZ)).  The values are shown in the areas of the Venn diagram and used to express an arbitrary conjunction of (conditional) independencies as a constraint I\u03bc \u00b7 v \u2265 0.", "section": "4 QIM-Compatibility and Information Theory"}, {"figure_path": "RE5LSV8QYH/figures/figures_8_1.jpg", "caption": "Figure 1: \u0399\u03bc.  For three variables, the components of this vector are illustrated in Figure 1 (right). It is not hard to see that an arbitrary conjunction of (conditional) (in)dependencies can be expressed as a constraint I\u03bc \u00b7 v \u2265 0, for some appropriate choice of vector v.", "description": "This figure illustrates the information profile of \u03bc for three variables X, Y, and Z. It shows how various information-theoretic quantities, such as conditional entropy and mutual information, can be represented as components of a vector. The figure highlights the connection between these information-theoretic quantities and the concept of (in)dependence, demonstrating that an arbitrary conjunction of (conditional) (in)dependencies can be expressed as a constraint on the information profile.", "section": "4 QIM-Compatibility and Information Theory"}, {"figure_path": "RE5LSV8QYH/figures/figures_29_1.jpg", "caption": "Figure 1: \u0399\u03bc.", "description": "This figure shows a vector representation of the information profile of a probability distribution \u03bc over three variables X, Y, and Z.  The components of the vector are conditional entropies (H(Y|X,Z), H(Y|X), H(Y|Z)), mutual informations (I(X;Y|Z), I(X;Y;Z), I(X;Z|Y), I(Y;Z|X)), and the joint entropy H(X,Y,Z).  These quantities capture the dependencies among the variables, reflecting how far the distribution is from functional dependence or conditional independence.", "section": "4 QIM-Compatibility and Information Theory"}, {"figure_path": "RE5LSV8QYH/figures/figures_31_1.jpg", "caption": "Figure 2: Illustrations of the structural deficiency IDefA underneath drawn underneath their associated hypergraphs {G}. Each circle represents a variable; an area in the intersection of circles {Ci} but outside of circles {Dk} corresponds to information that is shared between all Ci's, but not in any Dk. Variation of a candidate distribution \u03bc in a green area makes its qualitative fit better (according to IDef), while variation in a red area makes its qualitative fit worse; grey is neutral. Only the boxed structures in blue, whose IDef can be seen as measuring distance to a particular set of (conditional) independencies, are expressible as BNs.", "description": "This figure illustrates how the structural deficiency IDefA, which is a measure of how far a distribution is from being compatible with a hypergraph, varies across different hypergraph structures.  The circles represent variables, and the intersections and overlaps visualize how information is shared between them.  Green areas indicate that changing the distribution in those areas improves the fit, while red areas indicate that changing it worsens the fit. Grey areas are neutral.  Only the boxed blue structures are expressible as Bayesian networks.", "section": "C.2 Structural Deficiency: More Motivation, and Examples"}]