[{"figure_path": "W2qGSMl2Uu/tables/tables_6_1.jpg", "caption": "Table 1: The quantitative results obtained from the proposed method ContextGS and other competitors. Baseline methods, namely 3DGS [15] and Scaffold-GS [20], are included for reference. The intermediary approaches are specifically designed for 3DGS compression. Our methodology showcases two results representing varying size and fidelity tradeoffs, achieved through adjustment of \u03bbe. Highlighted in red and yellow cells are the best and second-best results, respectively. Size measurements are expressed in megabytes (MB).", "description": "This table presents a quantitative comparison of the proposed ContextGS method against several baseline and state-of-the-art 3DGS compression techniques.  It shows PSNR, SSIM, LPIPS, and model size (in MB) for various datasets, highlighting the superior performance of ContextGS in terms of both compression ratio and rendering quality.  Two variants of ContextGS are shown, representing a trade-off between compression and fidelity.", "section": "5 Experiments"}, {"figure_path": "W2qGSMl2Uu/tables/tables_7_1.jpg", "caption": "Table 2: The ablation study of each component we proposed measured on BungeeNerf [32] dataset. \u201cHP\u201d and \u201cCM\u201d represent the hyperprior and anchor level context model respectively. Ours w/o HP w/o CM can be roughly regarded as a Scaffold-GS [20] model with entropy coding and masking loss [18].", "description": "This table presents an ablation study evaluating the impact of different components of the proposed ContextGS model on the BungeeNerf dataset.  It compares the performance (size, PSNR, SSIM, LPIPS) of the full model against versions without the hyperprior (HP), without the context model (CM), and without both.  The baseline, \"Ours w/o HP w/o CM\", is essentially a Scaffold-GS model enhanced with entropy coding and masking loss.", "section": "5.3 Ablation studies and discussions"}, {"figure_path": "W2qGSMl2Uu/tables/tables_8_1.jpg", "caption": "Table 3: The ablation study of our method w/ and w/o reusing anchors from coarser levels, i.e., anchor forward technique, measured on BungeeNerf [32] dataset.", "description": "This table presents the ablation study of the proposed method by removing the anchor level division and anchor reusing (forwarding) components.  It shows the impact of these components on the model's size (in MB), Peak Signal-to-Noise Ratio (PSNR), Structural Similarity Index (SSIM), and Learned Perceptual Image Patch Similarity (LPIPS) metrics, evaluated using the BungeeNerf dataset.  The results highlight the contribution of each component to the overall performance.", "section": "5.3 Ablation studies and discussions"}, {"figure_path": "W2qGSMl2Uu/tables/tables_8_2.jpg", "caption": "Table 4: The storage cost of each component and rending qualities of our method and baselines evaluated on the scene rome in BungeeNerf [32] dataset. \"w/ APC\" represents using anchor position coding, i.e., using the hyperprior features to code the anchor positions. (The encoding/decoding time is measured on an RTX3090.)", "description": "This table presents a quantitative comparison of the proposed method (ContextGS) against baselines on the \"rome\" scene from the BungeeNerf dataset. It breaks down the storage cost into various components (hyperprior, position, features, scaling, offset, mask, and MLPs), providing the total storage cost in megabytes (MB) for each method. It also includes fidelity metrics (PSNR, SSIM) and encoding/decoding speeds in seconds (s) measured using an RTX3090 GPU.", "section": "5.2 Comparison with baselines"}, {"figure_path": "W2qGSMl2Uu/tables/tables_11_1.jpg", "caption": "Table 1: The quantitative results obtained from the proposed method ContextGS and other competitors. Baseline methods, namely 3DGS [15] and Scaffold-GS [20], are included for reference. The intermediary approaches are specifically designed for 3DGS compression. Our methodology showcases two results representing varying size and fidelity tradeoffs, achieved through adjustment of Ae. Highlighted in red and yellow cells are the best and second-best results, respectively. Size measurements are expressed in megabytes (MB).", "description": "This table presents a quantitative comparison of the proposed ContextGS method against several baseline and state-of-the-art 3DGS compression techniques across multiple datasets.  Metrics include PSNR, SSIM, LPIPS, and model size (in MB).  Two versions of ContextGS are shown, representing different compression ratios (low-rate and high-rate).  The best and second-best results for each metric are highlighted.", "section": "5 Experiments"}, {"figure_path": "W2qGSMl2Uu/tables/tables_12_1.jpg", "caption": "Table 1: The quantitative results obtained from the proposed method ContextGS and other competitors. Baseline methods, namely 3DGS [15] and Scaffold-GS [20], are included for reference. The intermediary approaches are specifically designed for 3DGS compression. Our methodology showcases two results representing varying size and fidelity tradeoffs, achieved through adjustment of \u03bbe. Highlighted in red and yellow cells are the best and second-best results, respectively. Size measurements are expressed in megabytes (MB).", "description": "This table presents a quantitative comparison of the proposed ContextGS method against several baseline and competing 3DGS compression techniques across multiple datasets.  It shows PSNR, SSIM, LPIPS scores, and model size (in MB). Two versions of ContextGS are shown, representing a tradeoff between compression ratio and fidelity.", "section": "5 Experiments"}, {"figure_path": "W2qGSMl2Uu/tables/tables_12_2.jpg", "caption": "Table 7: Per-scene results of our method on DeepBlending [14] dataset.", "description": "This table presents a quantitative evaluation of the proposed ContextGS method on the DeepBlending dataset. It shows the performance of the model at different compression ratios (low-rate and high-rate).  The metrics used for evaluation are PSNR (Peak Signal-to-Noise Ratio), SSIM (Structural Similarity Index), and LPIPS (Learned Perceptual Image Patch Similarity).  The results are broken down for two different scenes (drjohnson and playroom) and an average of the two is provided for each compression setting.  This helps assess the impact of the compression rate on visual quality.", "section": "5.2 Comparison with baselines"}, {"figure_path": "W2qGSMl2Uu/tables/tables_12_3.jpg", "caption": "Table 8: Per-scene results of our method on Tank &Template [16] dataset.", "description": "This table presents a quantitative evaluation of the proposed ContextGS method on the Tanks & Temples dataset [16].  It shows the performance for two different compression ratios (low-rate and high-rate), each with results broken down by scene (train and truck) and overall average. The metrics reported are the size of the compressed model (Size, in MB), Peak Signal-to-Noise Ratio (PSNR, in dB), Structural Similarity Index (SSIM), and Learned Perceptual Image Patch Similarity (LPIPS).  This table allows for comparison of compression efficiency against perceptual quality across varying compression levels.", "section": "5.2 Comparison with baselines"}, {"figure_path": "W2qGSMl2Uu/tables/tables_13_1.jpg", "caption": "Table 1: The quantitative results obtained from the proposed method ContextGS and other competitors. Baseline methods, namely 3DGS [15] and Scaffold-GS [20], are included for reference. The intermediary approaches are specifically designed for 3DGS compression. Our methodology showcases two results representing varying size and fidelity tradeoffs, achieved through adjustment of Ae. Highlighted in red and yellow cells are the best and second-best results, respectively. Size measurements are expressed in megabytes (MB).", "description": "This table presents a quantitative comparison of the proposed ContextGS method against several baseline and state-of-the-art 3DGS compression techniques across multiple datasets.  It compares PSNR, SSIM, LPIPS, and model size (in MB).  Two versions of the ContextGS model are shown, representing different size/fidelity trade-offs.  The best and second-best results for each metric are highlighted.", "section": "5 Experiments"}]