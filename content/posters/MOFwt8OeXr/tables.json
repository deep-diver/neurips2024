[{"figure_path": "MOFwt8OeXr/tables/tables_18_1.jpg", "caption": "Table 1: The hyper-parameters for CP3ER.", "description": "This table lists the hyperparameters used in the CP3ER algorithm.  It includes parameters related to the replay buffer, training process, network architecture, and regularization.  Note that some parameters, such as the learning rate and feature dimension, have different settings for different task difficulty levels (hard vs. medium).", "section": "B Implementation Details"}, {"figure_path": "MOFwt8OeXr/tables/tables_21_1.jpg", "caption": "Table 2: Comparison of CP3ER and other methods on state-based RL tasks in DeepMind control suite.", "description": "This table compares the performance of CP3ER against other state-of-the-art methods (TD3, SAC, PPO, MPO, DMPO, D4PG, DreamerV3, CPQL) on six state-based reinforcement learning tasks from the DeepMind control suite.  The metrics used for comparison are the average scores achieved on each task.  It demonstrates the relative performance of CP3ER compared to existing methods.", "section": "C.4 Results on State-based Tasks"}, {"figure_path": "MOFwt8OeXr/tables/tables_21_2.jpg", "caption": "Table 3: Comparison of CP3ER with diffusion/consistency based RL methods.", "description": "This table compares the performance of CP3ER against two other methods, Diffusion-QL and Consistency-AC, on three online reinforcement learning tasks: Halfcheetah-m, Hopper-m, and Walker2d-m.  The results are presented as average scores \u00b1 standard deviation. CP3ER demonstrates significantly higher average scores on all three tasks compared to the other methods, indicating its superior performance in online reinforcement learning.", "section": "D Limitations"}]