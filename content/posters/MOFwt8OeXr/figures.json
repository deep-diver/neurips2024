[{"figure_path": "MOFwt8OeXr/figures/figures_3_1.jpg", "caption": "Figure 10: The dormant ratios of the policy under the online and offline training. All results are averaged over 4 random seeds, and the shaded region stands for standard deviation across different random seeds.", "description": "This figure displays the dormant ratios of the policy network during both online and offline training.  The dormant ratio is a metric that reflects the network's expressive ability; higher ratios indicate more inactive neurons and thus reduced expressiveness.  The graph shows the dormant ratio plotted against the number of gradient steps in the training process.  Separate lines are shown for online and offline training, with shaded regions representing the standard deviation across four different random seeds. This visualizes the impact of non-stationary data distribution (in online RL) on the stability and expressiveness of the consistency policy network, showing whether the non-stationary data significantly affects training.", "section": "More Results on Dormant Ratios"}, {"figure_path": "MOFwt8OeXr/figures/figures_4_1.jpg", "caption": "Figure 2: The dormant ratios of the policy networks with different losses and observations.", "description": "This figure visualizes the dormant ratios of policy networks trained under various conditions.  The top row (a) and (b) shows dormant ratios for online Halfcheetah and Walker2d tasks using consistency loss and Q-loss, respectively, with different random seeds. The bottom row (c) and (d) shows a comparison between using state (low-dimensional) and image (high-dimensional) data as input for the same visual RL tasks, again comparing the consistency loss and Q-loss methods.", "section": "4 Is Consistency-AC Applicable to Visual RL?"}, {"figure_path": "MOFwt8OeXr/figures/figures_5_1.jpg", "caption": "Figure 3: (a) The framework of CP3ER, where PPE is the abbreviation of prioritized proximal experience. (b) The sampling weights \u03b2 with different \u03b1.", "description": "This figure shows the framework of the proposed CP3ER method, which uses prioritized proximal experience (PPE) for sample-efficient policy regularization and a consistency policy in an actor-critic framework.  Panel (a) illustrates the overall architecture, highlighting the integration of PPE and consistency models into DrQ-v2 for sample-efficient training. Panel (b) shows how the sampling weights \u03b2 vary with different values of \u03b1, demonstrating the effect of the hyperparameter \u03b1 on the sampling strategy.", "section": "5 Consistency Policy with Prioritized Proximal Experience Regularization"}, {"figure_path": "MOFwt8OeXr/figures/figures_6_1.jpg", "caption": "Figure 4: Results on medium-level tasks in DeepMind control suite with 5 random seeds.", "description": "The figure shows the results of CP3ER and other baseline methods on 8 medium-level tasks in the DeepMind control suite.  The left panel displays learning curves showing the IQM normalized score versus the number of training steps (in millions). Error bars represent the standard deviation across five different random seeds.  The right panel provides a comparison of the mean, interquartile mean (IQM), median, and optimal gap (difference between the best performance and the mean performance) across the methods, highlighting CP3ER's superior performance and stability. ", "section": "6.1.1 Does CP3ER have performance advantages compared to current SOTA methods?"}, {"figure_path": "MOFwt8OeXr/figures/figures_7_1.jpg", "caption": "Figure 4: Results on medium-level tasks in DeepMind control suite with 5 random seeds.", "description": "This figure presents the results of the CP3ER method compared to other state-of-the-art methods on 8 medium-level tasks from the DeepMind control suite.  The left panel shows the learning curves (IQM normalized score vs. number of steps in millions), indicating CP3ER's superior sample efficiency. The right panel provides a detailed comparison of the mean, interquartile mean (IQM), median, and optimal gap across all methods, highlighting CP3ER's overall superior performance and reduced variance, suggesting greater training stability.", "section": "6.1.1 Does CP3ER have performance advantages compared to current SOTA methods?"}, {"figure_path": "MOFwt8OeXr/figures/figures_7_2.jpg", "caption": "Figure 4: Results on medium-level tasks in DeepMind control suite with 5 random seeds.", "description": "This figure displays the results of the CP3ER method compared to DrQ-v2, ALIX, and TACO on eight medium-level tasks from the DeepMind control suite. The left panel shows the learning curves, illustrating the normalized score (IQM) against the number of steps (in millions). The right panel presents a comparison of the mean, interquartile mean (IQM), median, and optimal gap for each method across the tasks, highlighting CP3ER's superior performance and stability. The shaded regions represent standard deviations across five different random seeds.", "section": "6.1.1 Does CP3ER have performance advantages compared to current SOTA methods?"}, {"figure_path": "MOFwt8OeXr/figures/figures_7_3.jpg", "caption": "Figure 7: Results on the toy example. Left part is action distributions during training, while right is returns of different policies.", "description": "This figure shows the action distributions and returns of different policies during training on a 1D continuous bandit problem.  The three policies compared are MaxEnt GP (maximum entropy Gaussian policy), Consistency-AC (consistency policy trained with the actor-critic framework and Q-loss), and MaxEnt CP (maximum entropy consistency policy). The left panel shows that MaxEnt GP maintains a relatively broad action distribution, reflecting good exploration. Consistency-AC shows a sharp peak at the optimal action, indicating quick convergence to a local optimum with limited exploration. MaxEnt CP, with policy regularization, shows a broader distribution than Consistency-AC while still centering around the optimal action, achieving a better balance between exploration and exploitation. The right panel shows that MaxEnt GP explores more widely, initially having lower returns but eventually reaching a higher level than Consistency-AC, which quickly converges to a local optimum but remains at a lower return level. MaxEnt CP performs similarly to MaxEnt GP, showing a slower rise but eventually achieving comparable performance. The figure highlights the effect of policy regularization in stabilizing and improving the performance of the consistency policy.", "section": "6.2.1 Can policy regularization improve the behavior of the policy during training?"}, {"figure_path": "MOFwt8OeXr/figures/figures_8_1.jpg", "caption": "Figure 8: Dormant ratios of the policy networks on different tasks with 5 random seeds.", "description": "The figure compares the dormant ratios (a measure of neural network inactivity) of consistency policy trained with and without entropy regularization across three different tasks: Acrobot-swingup, Reacher-hard, and Dog-stand.  Each line represents the average dormant ratio across 5 different random seeds. The shaded areas indicate standard deviations for each model. The results suggest that entropy regularization helps maintain a lower dormant ratio and therefore a more active and expressive policy network.", "section": "6.2.2 What is the impact of different modules on the performance?"}, {"figure_path": "MOFwt8OeXr/figures/figures_8_2.jpg", "caption": "Figure 2: The dormant ratios of the policy networks with different losses and observations.", "description": "This figure compares the dormant ratios of policy networks trained with different loss functions (consistency loss and Q-loss) and using different input types (state and image). The dormant ratio is a metric that indicates the proportion of inactive neurons in a neural network, with lower values representing more active and expressive networks. The results show that the Q-loss leads to a significant increase in the dormant ratio, particularly in visual RL with high-dimensional state space, suggesting reduced expressive ability and instability. The study highlights the impact of loss functions and input types on the performance of consistency policy in online RL.", "section": "4 Is Consistency-AC Applicable to Visual RL?"}, {"figure_path": "MOFwt8OeXr/figures/figures_15_1.jpg", "caption": "Figure 10: The dormant ratios of the policy under the online and offline training. All results are averaged over 4 random seeds, and the shaded region stands for standard deviation across different random seeds.", "description": "This figure displays the dormant ratios of a policy network under two training scenarios: online and offline.  The online scenario reflects the non-stationary data distribution typical of reinforcement learning, while the offline scenario uses a pre-collected dataset.  Four random seeds were used for each training type. The shaded region shows the standard deviation for each data point, indicating the variability of the results.", "section": "A More Results on Dormant Ratios"}, {"figure_path": "MOFwt8OeXr/figures/figures_15_2.jpg", "caption": "Figure 2: The dormant ratios of the policy networks with different losses and observations.", "description": "This figure shows the dormant ratios of policy networks trained with different loss functions (consistency loss and Q-loss) and using different observations (state and image). The dormant ratio is a metric to quantify the expression ability of a neural network. A higher dormant ratio indicates that fewer neurons are active, implying reduced network capacity and expressiveness. The results indicate that using Q-loss in the actor-critic framework leads to a higher dormant ratio, especially in visual RL, suggesting that Q-loss destabilizes policy training by limiting policy network's expression ability. The non-stationary distribution in online RL does not significantly affect the consistency policy training.", "section": "4 Is Consistency-AC Applicable to Visual RL?"}, {"figure_path": "MOFwt8OeXr/figures/figures_16_1.jpg", "caption": "Figure 8: Dormant ratios of the policy networks on different tasks with 5 random seeds.", "description": "The figure shows the dormant ratios of the policy networks when training with Consistency-AC and CP3ER on three different tasks (Acrobot-swingup, Reacher-hard, and Dog-stand).  The results demonstrate that CP3ER maintains lower dormant ratios than Consistency-AC, indicating improved stability and preventing the network from falling into local optima.  The shaded regions represent standard deviations across five different random seeds, highlighting the robustness of CP3ER.", "section": "6.2.2 What is the impact of different modules on the performance?"}, {"figure_path": "MOFwt8OeXr/figures/figures_16_2.jpg", "caption": "Figure 3: (a) The framework of CP3ER, where PPE is the abbreviation of prioritized proximal experience. (b) The sampling weights \u03b2 with different \u03b1.", "description": "This figure illustrates the architecture of the proposed Consistency Policy with Prioritized Proximal Experience Regularization (CP3ER) method and the sampling weight function.  (a) shows the overall framework, highlighting the prioritized proximal experience (PPE) sampling from the replay buffer, image augmentation, and the actor-critic network with a consistency policy and a mixture of Gaussian critic. (b) shows how the sampling weight \u03b2 varies with different values of \u03b1, demonstrating the prioritized sampling strategy.", "section": "5 Consistency Policy with Prioritized Proximal Experience Regularization"}, {"figure_path": "MOFwt8OeXr/figures/figures_18_1.jpg", "caption": "Figure 2: The dormant ratios of the policy networks with different losses and observations.", "description": "This figure shows the dormant ratios of policy networks trained with different loss functions (consistency loss and Q-loss) and using different observations (state and image) for two online RL tasks: Halfcheetah and Walker2d.  It demonstrates that the Q-loss in the actor-critic framework leads to a rapid increase in the dormant ratio, especially significant in visual RL (image-based) settings with high-dimensional state space, indicating that the Q-loss destabilizes the policy training by reducing the policy network's expressive ability. The figure also shows that the non-stationary distribution of online RL data does not significantly affect consistency model training.", "section": "4 Is Consistency-AC Applicable to Visual RL?"}, {"figure_path": "MOFwt8OeXr/figures/figures_19_1.jpg", "caption": "Figure 15: Performance profiles and probabilities of improvement of different methods.", "description": "This figure shows the performance profiles and probabilities of improvement for different methods on the medium-level tasks. The performance profiles plot the fraction of runs with a score greater than a certain threshold (\u03c4) across different time steps. The probability of improvement calculates the probability that a method performs better than another method. CP3ER shows better performance compared to other baselines in both measures.", "section": "6.1.1 Does CP3ER have performance advantages compared to current SOTA methods?"}, {"figure_path": "MOFwt8OeXr/figures/figures_19_2.jpg", "caption": "Figure 2: The dormant ratios of the policy networks with different losses and observations.", "description": "The figure shows the dormant ratios of policy networks trained with different loss functions (consistency loss and Q-loss) and using different observations (state and image) in online RL tasks. The dormant ratio is a measure of the proportion of inactive neurons in a neural network, indicating its expressive ability.  The results illustrate that the Q-loss in the actor-critic framework leads to a sharp increase in the dormant ratio, significantly impairing the policy's ability to learn complex behaviors, especially in visual RL with high-dimensional state spaces. In contrast, the consistency loss shows a more stable dormant ratio, indicating better training stability.", "section": "4 Is Consistency-AC Applicable to Visual RL?"}, {"figure_path": "MOFwt8OeXr/figures/figures_20_1.jpg", "caption": "Figure 17: Performance profiles and probabilities of improvement of different methods.", "description": "This figure compares the performance of CP3ER against four baseline algorithms (DrQ-v2, ALIX, TACO, and DrM) across different checkpoints.  The left side shows performance profiles, illustrating the cumulative fraction of runs achieving a certain normalized success rate. The right side shows the probability of improvement, indicating the likelihood of CP3ER outperforming each baseline algorithm.", "section": "6.1.1 Does CP3ER have performance advantages compared to current SOTA methods?"}, {"figure_path": "MOFwt8OeXr/figures/figures_20_2.jpg", "caption": "Figure 2: The dormant ratios of the policy networks with different losses and observations.", "description": "The figure shows the dormant ratios of policy networks trained with different loss functions (consistency loss and Q-loss) and using different observations (state and image) in online RL. The results demonstrate the instability of the policy training in visual RL using Q-loss due to the high-dimensional state space and actor-critic framework, which leads to a sharp increase in the dormant ratio and a loss of complex expression ability.  The plots show the dormant ratios across gradient steps for different tasks, highlighting the impact of the Q-loss on the consistency policy training in visual RL. ", "section": "4 Is Consistency-AC Applicable to Visual RL?"}, {"figure_path": "MOFwt8OeXr/figures/figures_20_3.jpg", "caption": "Figure 15: Performance profiles and probabilities of improvement of different methods.", "description": "The figure shows the performance profiles and probabilities of improvement for different reinforcement learning methods on a set of tasks. The performance profiles show the fraction of runs that achieved a normalized score greater than a given threshold, while the probabilities of improvement show the probability that one method outperformed another. The results indicate that the CP3ER method shows superior performance compared to other methods across different thresholds and tasks.", "section": "6.2.1 Can policy regularization improve the behavior of the policy during training?"}]