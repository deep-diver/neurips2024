[{"figure_path": "NhucGZtikE/figures/figures_2_1.jpg", "caption": "Figure 1: Illustration of the telescoping model of a trained neural network. Unlike the more standard framing of a neural network in terms of an iteratively learned set of parameters, the telescoping model takes a functional perspective on training a neural network in which an arbitrary test example's initially random prediction, fe (x), is additively updated by a linearized adjustment A ft (x) at each step t as in Eq. (5).", "description": "This figure illustrates the telescoping model of deep learning.  It shows how a neural network's prediction for a given input starts randomly and is iteratively refined through additive updates (Aft(x)). Each update is a linear approximation of the change in prediction based on the gradient of the loss function with respect to the parameters.  The final prediction (f~\u03b8T(x)) is the sum of the initial prediction and all these incremental linear adjustments, offering a functional view of the training process, unlike traditional parameter-centric models. This process is visually represented by a sequence of cylinders, each representing a training step.", "section": "3 A Telescoping Model of Deep Learning"}, {"figure_path": "NhucGZtikE/figures/figures_3_1.jpg", "caption": "Figure 2: Approximation error of the telescoping (fo\u2081 (x), red) and the linear model (foin (x), gray).", "description": "This figure compares the approximation errors of two models: the telescoping model (fo\u2081 (x)) and a linear model (foin (x)). The telescoping model is an iterative approximation of a trained neural network, while the linear model is a first-order approximation around the initialization point. The plot shows that using smaller learning rates reduces the approximation errors for both models.  However, the telescoping model consistently yields a much better approximation than the linear model.  The optimizer used also affects approximation quality; AdamW (KB14, LH17), which naturally makes larger updates due to rescaling, necessitates smaller learning rates to obtain the same approximation quality as SGD.", "section": "3 A Telescoping Model of Deep Learning"}, {"figure_path": "NhucGZtikE/figures/figures_4_1.jpg", "caption": "Figure 3: Double descent in MSE (top) and effective parameters p (bottom) on CIFAR-10.", "description": "This figure shows the results of a double descent experiment on the CIFAR-10 dataset.  The top panel displays the mean squared error (MSE) on both the training and testing sets as a function of the number of hidden neurons in a single-hidden-layer ReLU network. The bottom panel shows the corresponding effective parameters (p) calculated using the telescoping model, separately for the training and testing sets.  The figure illustrates the non-monotonic relationship between model size and test error that is characteristic of double descent, and demonstrates how the telescoping model's complexity metric can quantify this phenomenon by tracking the divergence between training and test complexity.", "section": "4.1 Case study 1: Exploring surprising generalization curves and benign overfitting"}, {"figure_path": "NhucGZtikE/figures/figures_5_1.jpg", "caption": "Figure 4: Grokking in mean squared error on a polynomial regression task (1, replicated from [KBGP24]) and in misclassification error on MNIST using a network with large initialization (2, replicated from [LMT22]) (top), against effective parameters (bottom). Column (3) shows test results on MNIST with standard initialization (with and without sigmoid activation) where time to generalization is quick and grokking does not occur.", "description": "This figure displays the results of three grokking experiments. The top row shows the mean squared error (MSE) for a polynomial regression task and the misclassification error for MNIST datasets with large and standard initializations. The bottom row shows the corresponding effective parameters (p) for each case. The experiments highlight the relationship between grokking, model complexity, and different initialization strategies. The use of a sigmoid activation function is also explored in one of the MNIST experiments, which influences the learning dynamics and complexity.", "section": "4 A Closer Look at Deep Learning Phenomena Through a Telescoping Lens"}, {"figure_path": "NhucGZtikE/figures/figures_6_1.jpg", "caption": "Figure 5: Neural Networks vs GBTs: Relative performance (top) and behavior of kernels (bottom) with increasing test data irregularity using the houses dataset.", "description": "This figure compares the performance of neural networks and gradient boosted trees (GBTs) on the \"houses\" dataset as the proportion of irregular examples in the test set increases.  The top panel shows the relative increase in mean squared error (MSE) difference between the two models. The bottom panel shows the normalized maximum kernel weight norm for both models. This visualization helps illustrate how the behavior of the models' kernels contributes to their performance differences, particularly in scenarios with more irregular data.", "section": "Case study 2: Understanding differences between gradient boosting and neural networks"}, {"figure_path": "NhucGZtikE/figures/figures_8_1.jpg", "caption": "Figure 6: Linear mode connectivity and gradient changes by t'. (1) Decrease in accuracy when using averaged weights \u03b1\u03b8\u03c4 + (1 \u2212 a) for randomly initialized (orange) and pre-trained ResNet-20 (green). (2) & (3) Changes in model gradients by layer for a randomly initialized (2) and pretrained (3) model.", "description": "This figure shows the results of an experiment investigating linear mode connectivity (LMC) and how it relates to gradient stabilization.  The experiment uses a ResNet-20 model, comparing randomly initialized and pre-trained versions. Panel (1) shows the decrease in accuracy when averaging weights at different checkpoints during training. Panels (2) and (3) show the change in model gradients across layers for both randomly initialized and pre-trained models.  The results suggest that gradient stabilization correlates with LMC.", "section": "4.3 Case study 3: Towards understanding the success of weight averaging"}, {"figure_path": "NhucGZtikE/figures/figures_24_1.jpg", "caption": "Figure 2: Approximation error of the telescoping (fo\u2081 (x), red) and the linear model (foin (x), gray).", "description": "This figure shows the approximation error of two models: the telescoping model and a simple linear model.  The x-axis represents the learning rate, and the y-axis represents the approximation error.  The figure shows that for small learning rates, the error of the telescoping model is negligible.  This suggests that the telescoping model is a good approximation of the true neural network for small learning rates. The figure also shows that the telescoping model is a better approximation than the linear model, especially for larger learning rates. The different colored lines represent different optimizers used.", "section": "3 A Telescoping Model of Deep Learning"}, {"figure_path": "NhucGZtikE/figures/figures_25_1.jpg", "caption": "Figure 8: Test accuracy of the telescoping (fo\u2081 (x), red, top row) and the model linearized around the initialization (foin(x), blue, bottom row) against accuracy of the actual neural network (gray) by optimization step for different optimization strategies and other design choices. While the telescoping model visibly matches the accuracy of the actual neural network, the linear approximation around the initialization leads to substantial differences in accuracy later in training.", "description": "This figure compares the accuracy of three different models over the course of training: the telescoping model, a linear approximation, and the actual neural network.  It demonstrates that the telescoping model closely tracks the accuracy of the real neural network, while the linear approximation significantly diverges as training progresses, highlighting the accuracy and utility of the telescoping model for analyzing deep learning phenomena.", "section": "4.1 Case study 1: Exploring surprising generalization curves and benign overfitting"}, {"figure_path": "NhucGZtikE/figures/figures_25_2.jpg", "caption": "Figure 9: Double descent experiments using MNIST, distinguishing 3-vs-5, with 20% added label noise during training (left) and no added label noise (right). Without label noise, there is no double descent in error on this task; when label noise is added we observe the prototypical double descent shape in test error.", "description": "This figure shows the results of double descent experiments on the MNIST dataset.  The left panel displays the results when 20% label noise is added during training, while the right panel shows the results without added noise. The x-axis represents the number of hidden neurons in the model, and the y-axis shows the mean squared error (MSE).  The key observation is that the characteristic double descent curve (initial improvement, then a dip, then another rise) only emerges when label noise is present.  Without noise, test error decreases monotonically with increased model size.", "section": "4.1 Case study 1: Exploring surprising generalization curves and benign overfitting"}, {"figure_path": "NhucGZtikE/figures/figures_26_1.jpg", "caption": "Figure 3: Double descent in MSE (top) and effective parameters p (bottom) on CIFAR-10.", "description": "This figure shows the double descent phenomenon observed in a neural network trained on the CIFAR-10 dataset.  The top panel displays the mean squared error (MSE) on the test set as a function of the number of hidden neurons in a single-hidden-layer ReLU network. The bottom panel shows the effective number of parameters (p) used by the model, calculated using a new metric introduced in the paper, as a function of the number of hidden neurons.  The plot demonstrates that test error initially decreases with increasing model size, then increases (overfitting), and finally decreases again as the model size significantly surpasses the amount of training data, exhibiting the double descent phenomenon. The effective parameters on the training set monotonically increase with model size, while test-time effective parameters show non-monotonic behavior, decreasing beyond the interpolation threshold. This divergence in effective parameters between training and test sets is linked to the non-monotonic generalization curve.", "section": "4.1 Case study 1: Exploring surprising generalization curves and benign overfitting"}, {"figure_path": "NhucGZtikE/figures/figures_27_1.jpg", "caption": "Figure 11: Grokking in mean squared error (top) on a polynomial regression task (replicated from [KBGP24]) against effective parameters (bottom) with different task alignment parameters \u20ac.", "description": "This figure shows the results of a polynomial regression experiment.  The top panels show mean squared error (MSE) over optimization steps for three different levels of task alignment (epsilon). The bottom panels display the effective number of parameters used during training (green) and testing (blue) for the same three task alignment levels. The plots illustrate the grokking phenomenon, where improvements in test performance occur after perfect training performance is already achieved.", "section": "4.1 Case study 1: Exploring surprising generalization curves and benign overfitting"}, {"figure_path": "NhucGZtikE/figures/figures_27_2.jpg", "caption": "Figure 12: Grokking in misclassification error on MNIST using a network with large initialization (replicated from [LMT22]) (top), against effective parameters (bottom) with different initialization scales a.", "description": "This figure shows the results of replicating an experiment from the paper [LMT22] on the MNIST dataset.  Three experiments are shown, each with a different initialization scale ('a'). The top row displays the misclassification error (test and train) over training steps.  The bottom row shows the effective number of parameters (test and train) over training steps. The results illustrate the grokking phenomenon, where test performance improves significantly after training accuracy reaches near perfect.", "section": "4.1 Case study 1: Exploring surprising generalization curves and benign overfitting"}, {"figure_path": "NhucGZtikE/figures/figures_27_3.jpg", "caption": "Figure 13: No grokking in misclassification error on MNIST (top), against effective parameters (bottom) using a network with standard initialization (a = 1) with and without sigmoid activation.", "description": "This figure displays the results of an experiment testing for the grokking phenomenon on MNIST using a network with standard initialization and comparing the results with and without using a sigmoid activation function. The top panel shows the misclassification error on the MNIST dataset, and the bottom panel shows the number of effective parameters used. The results show no grokking, with both train and test error decreasing smoothly and almost identically during training.  The low number of effective parameters supports the absence of grokking. The experiment highlights the role of initialization and activation functions in influencing the learning dynamics and generalization behavior. ", "section": "4.1 Case study 1: Exploring surprising generalization curves and benign overfitting"}, {"figure_path": "NhucGZtikE/figures/figures_28_1.jpg", "caption": "Figure 14: Neural Networks vs GBTs: Relative performance (top) and behavior of kernels (bottom) with increasing test data irregularity for three additional datasets.", "description": "This figure compares the performance of neural networks and gradient boosted trees (GBTs) on three additional tabular datasets from the work of [GOV22] as the proportion of irregular test examples increases. The top row shows the relative increase in the mean squared error (MSE) difference between neural networks and GBTs.  The bottom row displays the normalized maximum kernel weight norm for both neural networks and GBTs.  The figure demonstrates how the performance gap between the two models changes with increasing data irregularity, and how that change relates to the behavior of their respective kernels.", "section": "4.2 Case study 2: Understanding differences between gradient boosting and neural networks"}]