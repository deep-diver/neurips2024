[{"figure_path": "VrVx83BkQX/figures/figures_1_1.jpg", "caption": "Figure 1: Safe RLHF [16] respectively fits reward and safety models to reward and safety datasets with human preferences, and then leverages PPO-Lagrangian to optimize an LM policy and a Lagrangian multiplier to balance helpfulness and harmlessness. In contrast, SACPO first aligns an LM policy with the reward metric and then realigns the resulting reward-aligned policy with the safety metric (or vice versa). In this process, we can use simple RL-free algorithms (e.g., DPO, KTO) for each step, which leads to simplicity, stability, and flexibility.", "description": "This figure compares the Safe RLHF and SACPO methods for language model policy optimization. Safe RLHF uses separate reward and safety models trained on human preference data, then uses PPO-Lagrangian to optimize the language model policy while balancing helpfulness and harmlessness.  In contrast, SACPO takes a step-wise approach, first aligning the model with a reward metric using RL-free algorithms like DPO or KTO, and then aligning it with a safety metric using the same type of algorithms. This step-wise approach offers advantages in simplicity, stability, and flexibility.", "section": "1 Introduction"}, {"figure_path": "VrVx83BkQX/figures/figures_8_1.jpg", "caption": "Figure 2: Win rate against the SFT model. H and S are abbreviations for helpfulness and safety (i.e., harmlessness), respectively. Crosses represent SFT and Safe RLHF, and blue circles represent models aligned with a single metric. (a) DPO (H) \u2192 DPO (S), DPO (H) \u2192 KTO (S), and KTO (H) \u2192 DPO (S). (b) DPO (S) \u2192 DPO (H). (c) P-SACPO based on linear model merging. In (a) and (b), the numbers indicate \u03b2/\u03bb. In (c), the numbers for the red triangles represent \u03b2/\u03bb, while those for the green and purple squares represent q.", "description": "This figure shows the win rates of different language models against a Supervised Fine-Tuning (SFT) model in terms of helpfulness and harmlessness.  It compares several models, including Safe RLHF and various versions of SACPO (using different algorithms and alignment orders) and P-SACPO (model merging).  The plots visualize the trade-offs between helpfulness and harmlessness achieved by each model, showing how different approaches balance these two metrics.  Different parameter settings (\u03b2/\u03bb and q) are explored to show their influence.", "section": "7.2 Experimental Results"}, {"figure_path": "VrVx83BkQX/figures/figures_26_1.jpg", "caption": "Figure 2: Win rate against the SFT model. H and S are abbreviations for helpfulness and safety (i.e., harmlessness), respectively. Crosses represent SFT and Safe RLHF, and blue circles represent models aligned with a single metric. (a) DPO (H) \u2192 DPO (S), DPO (H) \u2192 KTO (S), and KTO (H) \u2192 DPO (S). (b) DPO (S) \u2192 DPO (H). (c) P-SACPO based on linear model merging. In (a) and (b), the numbers indicate \u03b2/\u03bb. In (c), the numbers for the red triangles represent \u03b2/\u03bb, while those for the green and purple squares represent q.", "description": "This figure shows the win rates of different language models against a supervised fine-tuning (SFT) model, in terms of helpfulness and harmlessness.  It compares various methods, including Safe RLHF and different versions of the proposed SACPO algorithm. Each point represents a model trained with a specific configuration of hyperparameters. The figure helps visualize the trade-off between helpfulness and harmlessness achieved by different alignment strategies.", "section": "7.2 Experimental Results"}, {"figure_path": "VrVx83BkQX/figures/figures_27_1.jpg", "caption": "Figure 2: Win rate against the SFT model. H and S are abbreviations for helpfulness and safety (i.e., harmlessness), respectively. Crosses represent SFT and Safe RLHF, and blue circles represent models aligned with a single metric. (a) DPO (H) \u2192 DPO (S), DPO (H) \u2192 KTO (S), and KTO (H) \u2192 DPO (S). (b) DPO (S) \u2192 DPO (H). (c) P-SACPO based on linear model merging. In (a) and (b), the numbers indicate \u03b2/\u03bb. In (c), the numbers for the red triangles represent \u03b2/\u03bb, while those for the green and purple squares represent q.", "description": "This figure displays the win rates of different language models against a supervised fine-tuned (SFT) model.  It compares SACPO variants (stepwise alignment with different algorithms and orders) to Safe RLHF and a single-metric alignment baseline.  The x-axis represents helpfulness, and the y-axis represents harmlessness. Different colors represent different models and configurations, with numbers indicating hyperparameter settings (\u03b2/\u03bb or q).", "section": "7.2 Experimental Results"}, {"figure_path": "VrVx83BkQX/figures/figures_27_2.jpg", "caption": "Figure 2: Win rate against the SFT model. H and S are abbreviations for helpfulness and safety (i.e., harmlessness), respectively. Crosses represent SFT and Safe RLHF, and blue circles represent models aligned with a single metric. (a) DPO (H) \u2192 DPO (S), DPO (H) \u2192 KTO (S), and KTO (H) \u2192 DPO (S). (b) DPO (S) \u2192 DPO (H). (c) P-SACPO based on linear model merging. In (a) and (b), the numbers indicate \u03b2/\u03bb. In (c), the numbers for the red triangles represent \u03b2/\u03bb, while those for the green and purple squares represent q.", "description": "This figure displays the win rates of various LLMs against the Supervised Fine-Tuning (SFT) model in terms of helpfulness and harmlessness.  The results show the effectiveness of SACPO, comparing different alignment orders and algorithms (DPO and KTO), and its practical variant, P-SACPO (using model merging). The different colored shapes represent the various models, and numbers in the plots represent the hyperparameters \u03b2/\u03bb and q.", "section": "7.2 Experimental Results"}, {"figure_path": "VrVx83BkQX/figures/figures_29_1.jpg", "caption": "Figure 2: Win rate against the SFT model. H and S are abbreviations for helpfulness and safety (i.e., harmlessness), respectively. Crosses represent SFT and Safe RLHF, and blue circles represent models aligned with a single metric. (a) DPO (H) \u2192 DPO (S), DPO (H) \u2192 KTO (S), and KTO (H) \u2192 DPO (S). (b) DPO (S) \u2192 DPO (H). (c) P-SACPO based on linear model merging. In (a) and (b), the numbers indicate \u03b2/\u03bb. In (c), the numbers for the red triangles represent \u03b2/\u03bb, while those for the green and purple squares represent q.", "description": "This figure shows the win rates of different language models against a baseline model (SFT) in terms of helpfulness and harmlessness.  It compares various SACPO model configurations (stepwise alignment of reward and safety models using different algorithms) and a baseline Safe RLHF method. Different parameters (\u03b2/\u03bb, q) are evaluated across various configurations. The results indicate that certain stepwise alignment strategies and model merging improve performance compared to the baseline.", "section": "7.2 Experimental Results"}]