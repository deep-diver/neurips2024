[{"type": "text", "text": "Active learning of neural population dynamics using two-photon holographic optogenetics ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Andrew Wagenmaker\u2217 Lu Mi\u2217 Marton Rozsa University of California, Berkeley Georgia Tech Allen Institute for Neural Dynamics ", "page_idx": 0}, {"type": "text", "text": "Matthew S. Bull Karel Svoboda Allen Institute for Brain Science Allen Institute for Neural Dynamics ", "page_idx": 0}, {"type": "text", "text": "Kayvon Daie\u2020 Matthew D. Golub\u2020 Kevin Jamieson\u2020 Allen Institute for Neural Dynamics University of Washington University of Washington ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Recent advances in techniques for monitoring and perturbing neural populations have greatly enhanced our ability to study circuits in the brain. In particular, two-photon holographic optogenetics now enables precise photostimulation of experimenter-specified groups of individual neurons, while simultaneous twophoton calcium imaging enables the measurement of ongoing and induced activity across the neural population. Despite the enormous space of potential photostimulation patterns and the time-consuming nature of photostimulation experiments, very little algorithmic work has been done to determine the most effective photostimulation patterns for identifying the neural population dynamics. Here, we develop methods to efficiently select which neurons to stimulate such that the resulting neural responses will best inform a dynamical model of the neural population activity. Using neural population responses to photostimulation in mouse motor cortex, we demonstrate the efficacy of a low-rank linear dynamical systems model, and develop an active learning procedure which takes advantage of low-rank structure to determine informative photostimulation patterns. We demonstrate our approach on both real and synthetic data, obtaining in some cases as much as a two-fold reduction in the amount of data required to reach a given predictive power. Our active stimulation design method is based on a novel active learning procedure for low-rank regression, which may be of independent interest. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Neural population dynamics describe how the activities across a population of neurons evolve over time due to local recurrent connectivity and inputs to the population from other neurons or brain areas. Identifying these population dynamics can provide critical insight into the computations performed by a neural population [1]. Dynamical systems models have enabled neuroscientists to generate and test a multitude of hypotheses about how specific neural populations support the neural computations that underlie, for example, motor control [2\u20134], motor timing [5, 6], decision making [7\u201310], working memory [11], social behavior [12], and learning [13\u201316]. ", "page_idx": 0}, {"type": "text", "text": "The traditional approach to data-driven modeling of a neural population typically involves two separate stages. First, neural population activity is recorded while an animal performs a task of interest. Then, a dynamical systems model is fit to the recorded neural responses [17\u201333]. This approach suffers from two key limitations. First, any inferred structure is purely correlational, and cannot be interpreted with any notion of causality. Second, the experimenter has limited control over how the neural population dynamics are sampled, which can lead to inefficient data collection\u2014oversampling in some parts of neural activity space while altogether missing others. Given constraints on time and resources in neurophysiological experiments, there is a strong need for techniques that minimize the amount of experimental data required to identify the neural population dynamics. ", "page_idx": 1}, {"type": "text", "text": "We seek to overcome these limitations by actively designing the causal circuit perturbations that will be most informative to learning a dynamical model of the neural population response. For circuit perturbations, we employ two-photon holographic photostimulation (Figure 1), which provides temporally precise, cellular-resolution optogenetic control over the activity of ensembles of neurons [34\u201341]. When paired with two-photon calcium imaging, photostimulation protocols can provide insight into network connectivity by enabling the measurement of the causal influence that each perturbed neuron exerts on all other recorded neurons [36, 39, 42\u201346]. This platform enables targeted excitation of the neural population dynamics, thus providing the experimenter with unprecedented control over the data collected for informing a model of the neural population dynamics. ", "page_idx": 1}, {"type": "text", "text": "Here, we develop active learning techniques for designing photostimulation patterns that allow for efficient estimation of low-rank neural population dynamics and the underlying network connectivity. First, we introduce a low-rank autoregressive model that captures low-dimensional structure in neural population dynamics and allows inference of the causal interactions between recorded neurons. We then propose an active learning procedure which chooses photostimulations to target this low-dimensional structure, and demonstrate it in two settings: estimating the underlying causal interactions when using the learned autoregressive model as a simulator of the true dynamics, and adaptively selecting which samples to observe from our dataset of neural population activity recorded via two-photon calcium imaging of mouse motor cortex in response to two-photon holographic photostimulation. In both cases, we show that our active approach obtains substantially more accurate estimates with fewer measurements compared to passive baselines. Our methodology is based on a novel analysis of nuclear-norm regression with non-isotropic inputs. To the best of our knowledge, this is the first approach to demonstrate significant gains applying active learning to low-rank matrix estimation problems, and thus we believe this may be of independent interest. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Modeling Neural Responses to Stimulation. Many studies have applied direct electrical or optical stimulation to neural populations to probe the dynamical properties of neural circuits and their relation to circuit function [4, 10, 26, 47\u201349]. However, these stimulation techniques lack the spatial specificity needed to precisely probe the causal influence of individuals neurons on the population dynamics, and these experimental designs were passive in that the stimulation protocols were specified prior to data collection (with [44, 48] as notable exceptions). Other work has explored a related but separate problem of minimizing off-target effects when photostimulating individual neurons [41]. ", "page_idx": 1}, {"type": "text", "text": "Low-Rank Matrix Recovery. Low-rank matrix recovery has been intensively researched over the last decade and a half [50\u201352]. However, existing analyses rely critically on the assumption that the set of measurements taken are highly symmetric and satisfy some notion of the restricted isometry property (RIP) or incoherence. The matrix recovery problem of our setting departs from the classical literature in several ways. First, the set of feasible measurements we can take is constrained by the physical limits of the photostimulation system. Second, as we aim to adapt and actively learn these matrix coefficients, we should expect that our resulting set of measurements should be highly skewed by design. Motivated by this, we develop, to the best of our knowledge, the first bounds on low-rank estimation using the nuclear norm heuristic that gives a quantification of the estimation error in terms of the precise individual measurements taken (i.e., in contrast to a more global property like RIP). ", "page_idx": 1}, {"type": "text", "text": "Active Learning and Low-Rank Estimation. The active learning literature is vast, and a full survey is beyond the scope of this work. We focus in particular on active learning for dynamical systems, and problems with low-rank structure. The estimation of dynamical systems\u2014the system identification problem\u2014is central to many areas of engineering and science [53]. The problem of actively designing inputs to effectively estimate the parameters of a dynamical system has been studied extensively for decades [54\u201361]. More recently, a variety of provably efficient approaches have been developed for both linear [62, 63] and nonlinear [64, 65] systems. Other related work has considered active learning for latent variable models [66], which are often effective models of neural dynamics. As compared to these works, a key feature of our setting is the low-rank structure present in the data, which to our knowledge has not been previously studied within the active system identification literature. ", "page_idx": 1}, {"type": "image", "img_path": "nLQeE8QGGe/tmp/559f30c5f01f1f30f6a27c09af49cb5200b128e25de63a91f4ce172a315581a7.jpg", "img_caption": ["Figure 1: (a) Two-photon imaging and holographic photostimulation platform (left) and a representative image frame (right). Purple circles indicate neurons photostimulated immediately before frame acquisition. Red and blue indicate increases and decreases of firing activity, respectively, relative to before photostimulation. (b) Example time series photostimulation inputs (top) and neural responses (bottom) from 100 randomly selected neurons (out of $\\bar{d}=663$ recorded neurons identified in the FoV). (c) Neural responses $y_{t}$ occupy a low-dimensional subspace. Singular values from a representative dataset\u2019s demeaned neural activity data matrix (blue) indicate substantially more data variance residing in a few dozen dimensions (out of the full $d=663$ dimensional neural activity space) than is expected by chance (orange, singular values when removing low-dimensional structure by shuffilng time indices independently for each neuron; note clipped horizontal axis). "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Beyond dynamical systems, some attention has been devoted to active learning with low-rank structure, in particular works on low-rank bandits [67\u201370]. While the setting considered in these works is somewhat different\u2014they aim to solve a bandit problem, while we are interested in regression\u2014they similarly seek to develop active learning approaches which make efficient use of low-rank structure. Also related is the work of [71], which shows that in the related sparse estimation setting, there does not exist more than a logarithmic gain to being adaptive. The results of this work are minimax, however\u2014only applying to certain \u201chard\u201d problems\u2014and do not address the matrix recovery problem. ", "page_idx": 2}, {"type": "text", "text": "3 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Dataset Details. Neural population activity was recorded in mouse motor cortex using two-photon calcium imaging at $20\\mathrm{Hz}$ of a $1\\mathrm{mm}{\\times}1\\mathrm{mm}$ field of view (FoV) containing 500-700 neurons. Each recording spanned approximately 25 minutes and 2000 photostimulation trials. In each trial, a $150\\mathrm{ms}$ photostimulus was delivered and was followed by a $600\\mathrm{ms}$ response period before the next trial began. Each photostimulus targeted a group of 10-20 randomly selected neurons, and a total of 100 unique photostimulation groups were defined for each experiment $\\approx20$ trials per group). We evaluate our techniques on four such datasets. ", "page_idx": 2}, {"type": "text", "text": "3.1 Fitting Low-Rank Dynamical Models ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We first seek to develop effective dynamical models of the neural activity in our photostimulation datasets. Obtaining such models will provide insight into which photostimuli are most informative, and gives us a means to evaluate the effectiveness of our active learning methods. We consider three classes of models: autoregressive (AR) models, low-rank AR models, and nonlinear RNN models. Results from fitting these models are shown in Figure 2. We describe the model details next. ", "page_idx": 2}, {"type": "text", "text": "At discrete time $t\\in\\mathbb{N}$ , we denote the true neural activity across the $d$ imaged neurons as $\\boldsymbol{x}_{t}\\in\\mathbb{R}^{d}$ , the noisy, measured activity as $y_{t}\\ \\in\\ \\mathbb{R}^{d}$ , and the photostimulus intensity applied across those same $d$ neurons as $u_{t}\\in\\mathbb{R}^{d}$ . Applying stimulus $u_{t}$ influences the measured neural activity at the next timestep $y_{t+1}$ . However, just the snapshot $y_{t}$ may not capture the full true state of the neural population, which may include not just the current neural activity, but potentially also multiple orders of temporal derivatives. To capture these effects, we consider an AR- $k$ model defined as: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{x_{t+1}=\\sum_{s=0}^{k-1}(A_{s}x_{t-s}+B_{s}u_{t-s})+v,\\quad y_{t}=x_{t}+w_{t}\\quad\\mathrm{with}\\quad w_{t}\\sim\\mathcal{N}(0,\\sigma^{2}I_{d}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "image", "img_path": "nLQeE8QGGe/tmp/d6c3b603ead9eb013c648a2c64bb248b49e4f2219aa096d48769c4a1e8796263.jpg", "img_caption": ["Figure 2: Example data and cross-validated model predictions. (a) Roll-out predictions of the activity of an example neuron $i$ using low-rank AR- $k$ models ( $[k=4,$ ) and GRU networks for 22 example data segments (3.3s per segment; segments separated by brief horizontal spaces). Each model\u2019s predictions are seeded with the first $k=4$ timesteps $(200\\mathrm{ms})$ of activity from $d=663$ neurons and are then unrolled to predict the activity across all $d$ neurons over the next 66 timesteps, given the full 70-timestep sequence of photostimulation to all $d$ neurons. Most responses of neuron $i$ are tied to \u201cdirect\u201d photostimulation of neuron $i$ (pink, first row of panels). Several \u201cindirect responses\u201d are tied to stimulation of other neurons $j\\neq i$ that influence neuron $i$ through the population dynamics. To avoid showing all indirect stimuli (to $d-1$ neurons), only select indirect stimuli are shown (green, second row of panels). (b) Receiver operator characteristic (ROC) curve of true-positive rate and false-positive rate for response detection are calculated on indirect responses only (left) and all direct and indirect responses (right). (c) Area under ROC curve (AUROC) and (d) mean square error (MSE) for all predictions. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "where $A_{s}\\in\\mathbb{R}^{d\\times d}$ and $B_{s}\\in\\mathbb{R}^{d\\times d}$ describe the coupling between neurons and stimulus at the time lag of $s$ timesteps, $s=0,\\ldots,k-1$ , and offset $v\\in\\dot{\\mathbb{R}}^{d}$ accounts for baseline neural activity. Given input-observation pairs $\\{(u_{t},y_{t})\\}_{t}$ , the coefficients $\\{(A_{s},B_{s})_{s=0}^{k-1},v\\}$ of (3.1) can be fit using least squares. Despite its simplicity, this linear model reproduces the recorded neural activity remarkably well (see \u201cfull rank\u201d model of Figure 2). ", "page_idx": 3}, {"type": "text", "text": "Neural population dynamics are frequently reported as residing in a subspace of lower dimension than the total number of recorded neurons [2, 7, 17, 72\u201377]. The population dynamics in our datasets are consistent with such low-dimensional structure, as indicated by the singular value spectrum in Figure 1(c). Inspired by this observation, we introduce a set of low-rank dynamical models, where each matrix of $\\{(A_{s},B_{s})_{s=0}^{k-1}\\}$ is re-defined as diagonal plus low-rank. Explicitly, we parameterize $A_{s}=$ $D_{A_{s}}+U_{A_{s}}V_{A_{s}}^{\\top}$ and $\\vec{B_{s}}=D_{B_{s}}+U_{B_{s}}V_{B_{s}}^{\\top}$ , where $D\\in\\mathbb{R}^{d\\times d}$ with $D_{i j}=0$ for all $i\\neq j$ , $U\\in\\mathbb{R}^{d\\times r}$ , and $V\\in\\mathbb{R}^{d\\times r}$ for predefined rank $r$ . The diagonal matrices account for substantial autocorrelation in each neuron\u2019s activity $(D_{A_{s}})$ and for the reliable response of each neuron to direct photostimulation $(D_{B_{s}})$ , whereas the low-rank matrices $(U V^{\\top})$ confer coupling between neurons. To fit these parameters, we optimize the following objective function with gradient descent over all parameters: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\underset{A_{s},B_{s}\\in\\mathbb{R}^{d\\times d},s=0,\\ldots,k-1,v\\in\\mathbb{R}^{d}}{\\mathrm{minimize}}\\sum_{t=1}^{T}\\left(y_{t+1}-\\sum_{s=0}^{k-1}A_{s}y_{t-s}-\\sum_{s=0}^{k-1}B_{s}u_{t-s}-v\\right)^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Figure 2 shows that these low-rank models perform comparably to the full rank versions in terms of predictive performance; indeed the rank $r=35$ model appears almost indistinguishable from the full rank model. From a statistical perspective, low-rank models have far fewer degrees of freedom, and hence require less data to fit. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "To assess whether more expressive nonlinear models could be advantageous, we also fit a gated recurrent unit (GRU) network model, adapted from [22], as shown in Figure 2. Interestingly, the GRU model did not perform as well as the AR- $k$ models, potentially due to the complexities of hyperparameter tuning. Therefore, we focus on linear models in the analysis that follows. Additional details on model fitting are provided in Appendix B.2. ", "page_idx": 4}, {"type": "text", "text": "3.2 The Causal Connectivity Matrix ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "While we require dynamical models to predict the temporal evolution of the neural population activity, we are also interested in inferring how the activity of one recorded neuron causally influences the activity of the other recorded neurons. To address this need, we define a causal connectivity matrix, $H\\,\\in\\,\\dot{\\mathbb{R}}^{d\\times d}$ , to be the mapping such that $H u\\in\\mathbb{R}^{d}$ quantifies the total response (across time) of each neuron to a single-timestep photostimulus $u$ . That is, $\\textstyle\\sum_{t=1}^{\\infty}x_{t}=H u$ , where $x_{1},x_{2},x_{3},\\ldots$ are the neural activities generated by the population dynamic s if $u_{0}=u$ , $u_{t\\geq1}=0$ , and $x_{t\\leq0}=x_{\\infty}$ is the steady state or resting state of the system subject to no photostimulation. If the dynamics are linear, or more specifically follow (3.1), such a matrix $H$ is guaranteed to exist, and can be formed by simply rolling out (3.1) with the appropriate initializations. While $H$ is not explicitly constrained to be low-rank, if it is obtained from a low-rank AR- $k$ model, it too will exhibit low-rank structure. ", "page_idx": 4}, {"type": "text", "text": "In our experimental paradigm, photostimulation acts as a causal perturbation to the population dynamics, and as such, our statistical framework is able to capture causal interactions, as opposed to merely correlative interactions. This is in contrast to the majority of work on neural population dynamics, which involves fitting dynamical models to passively obtained data. Due to the lack of causal manipulations in these studies, one cannot distinguish whether statistical relationships arise between neurons due to correlation (e.g., due to a shared upstream influence) versus causation (e.g., neuron $i$ directly influences neuron $j],$ ). Such correlative relationships are typically referred to as \u201cfunctional connectivity\u201d; we instead use the term \u201ccausal connectivity\u201d to convey the additional causal interpretability afforded in our setting. ", "page_idx": 4}, {"type": "text", "text": "To fti $H$ , we could first fti $\\{\\widehat{A}_{s},\\widehat{B}_{s}\\}_{s=0}^{k-1}$ and then use these as plug-in estimates for their true values to compute . Alternatively, we take a more direct approach inspired by the definition of itself. By inspecting the raw data of Figure 2(a) and observing the rate at which each stimulated neuron returns to baseline activity, it is clear that the system mixes (i.e., forgets the past) quickly. This suggests that the total response due to input $u$ asymptotes after some finite number of timesteps $\\tau$ . Thus, we can apply some photostimulus $\\dot{u}\\in\\mathbb{R}^{d}$ at time $t=0$ and then measure the total response $\\textstyle z=\\sum_{t=1}^{\\tau}y_{t}$ , where $y_{t}\\in\\mathbb{R}^{d}$ is the noisy measurement of the true neural response $x_{t}$ . If we repeat this for many pairs $\\{(u_{n},z_{n})\\}_{n}$ then we can approximate $H$ as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widehat{H}:=\\arg\\operatorname*{min}_{H^{\\prime}}\\sum_{n}\\|z_{n}-H^{\\prime}u_{n}\\|_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "In this work we adopt this latter approach. Since we believe $H$ to be low rank, this amounts to a low-rank matrix recovery problem with matrix-vector observations. In the next section, we will describe how to adaptively choose $\\{u_{n}\\}_{n}$ to estimate $H$ using as few (stimulus, response) pairs as possible. Subsequently in Section 5, we will demonstrate that actively designing inputs to accelerate the learning of $H$ effectively accelerates the learning of the full dynamics as well. ", "page_idx": 4}, {"type": "text", "text": "4 Active Learning of Low-Rank Matrices ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In the previous section, we saw that estimating the causal connectivity matrix $H$ induced by the neural population dynamics amounts to low-rank matrix recovery, where we apply some photostimulus $\\bar{u}\\in\\mathbb{R}^{d}$ and observe the neural population response $z\\approx H u$ plus noise. In this section we seek to understand how we should choose the photostimuli to estimate the causal connectivity as quickly as possible. To this end, in Section 4.1 we present novel results characterizing the estimation error of the nuclear norm regression estimator, and in Section 4.2 present an algorithm motivated by these results which seeks to actively estimate low-rank matrices. These results will directly motivate a procedure for designing photostimulation inputs. ", "page_idx": 4}, {"type": "text", "text": "To demonstrate the generality of our results, in Section 4.1 we consider a general matrix regression setting. In particular, let $\\Theta_{\\star}\\in\\mathbb{R}^{d_{1}\\times d_{2}}$ be a rank $r$ (potentially non-square) matrix, $\\varphi_{n}\\in\\overline{{\\mathbb{R}^{d_{1}\\times d_{2}}}}$ some input matrix, and assume scalar observations: ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{z_{n}=\\langle\\Theta_{\\star},\\varphi_{n}\\rangle+\\eta_{n},\\quad\\eta_{n}\\sim\\mathcal{N}(0,1),}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\langle\\Theta_{\\star},\\varphi_{n}\\rangle\\,=\\,\\mathrm{tr}(\\Theta_{\\star}^{\\top}\\varphi_{n})$ for $\\operatorname{tr}(\\cdot)$ the trace of a matrix. Note that the setting considered in Section 3.2 is a special case of this observation model with $\\Theta_{\\star}\\leftarrow H$ and, for each input stimulation $u$ , measuring the response of (4.1) to $d$ inputs $\\varphi_{j}$ of the form $\\varphi_{j}\\equiv{\\mathbf e}_{j}u^{\\top}$ for $j=1,\\ldots,d$ . ", "page_idx": 5}, {"type": "text", "text": "Matrix Notation. We let $\\|\\cdot\\|_{\\mathrm{F}},\\|\\cdot\\|_{\\mathrm{op}},\\|\\cdot\\|_{*}$ denote the Frobenius, operator, and nuclear norm of a matrix, respectively. $\\dagger$ denotes the pseudo-inverse of a matrix. $\\operatorname{vec}(\\cdot)$ denotes the vectorization of a matrix, and $\\mathrm{\\mat(\\cdot)}$ the inverse of the vectorization. We also let $\\triangle_{\\mathcal{U}}$ denote the simplex\u2014the set of distributions\u2014over a set $\\boldsymbol{\\mathcal{U}}$ . ", "page_idx": 5}, {"type": "text", "text": "4.1 Constrained Nuclear Norm Estimator under Non-Isotropic Measurements ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We are interested in understanding how we can effectively take into account the low-rank structure of $\\Theta_{\\star}$ , if our goal is to estimate $\\Theta_{\\star}$ from the observations of (4.1). To this end, we consider the following nuclear-norm constrained least-squares estimator for $\\Theta_{\\star}$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widehat{\\Theta}=\\underset{\\Theta\\in\\mathcal{K}}{\\arg\\operatorname*{min}}\\,\\|\\Phi(\\Theta)-z\\|_{2}^{2}:=\\sum_{n=1}^{N}(\\langle\\varphi_{n},\\Theta\\rangle-z_{n})^{2}\\quad\\mathrm{for}\\quad K:=\\{\\Theta:\\|\\Theta\\|_{*}\\leq\\|\\Theta_{*}\\|_{*}\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where here we let $\\Phi(\\Theta_{\\star})\\,\\in\\,\\mathbb{R}^{N}$ denote the vector where the $n$ th element is $\\langle\\varphi_{n},\\Theta_{\\star}\\rangle$ , and $z=$ $\\Phi(\\Theta_{\\star})+\\eta$ the vector of observations, for $\\eta$ the vector with elements $\\eta_{n}$ . Define $\\Theta_{\\star}=U\\Sigma V^{\\top}$ as the skinny SVD such that $U\\,\\in\\,\\mathbb{R}^{d_{1}\\times r}$ , $V\\,\\in\\,\\mathbb{R}^{d_{2}\\times r}$ , and consider the linear projection operators $P_{\\perp},P_{\\|}:\\dot{\\mathbb{R}}^{d_{1}\\times d_{2}}\\rightarrow\\mathbb{R}^{d_{1}\\times d_{2}}$ defined as: ", "page_idx": 5}, {"type": "equation", "text": "$$\nP_{\\bot}(M):=(I-U U^{\\top})M(I-V V^{\\top})\\quad\\mathrm{and}\\quad P_{\\|}(M):=M-P_{\\bot}(M),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "for any $\\boldsymbol{M}\\,\\in\\,\\mathbb{R}^{d_{1}\\times d_{2}}$ . We call $P_{\\parallel}$ the projection onto the tangent space of $\\Theta_{\\star}$ . Note that the dimension of the range of $P_{\\parallel}$ is equal to just $r(d_{1}+d_{2})-r^{2}\\ll d_{1}d_{2}$ . We are now ready to state our main result on the estimation error of $\\widehat{\\Theta}$ , for $\\widehat{\\Theta}$ as defined in (4.2). ", "page_idx": 5}, {"type": "text", "text": "Theorem 1. Define $\\mu:=\\|(\\Phi^{*}\\Phi)^{1/2}((P_{\\|}\\Phi^{*}\\Phi P_{\\|})^{\\dag})^{1/2}\\|_{\\mathrm{op}}$ . Then with probability at least $1-2\\delta$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\widehat\\Theta-\\Theta_{\\star}\\|_{\\mathrm{F}}\\leq\\frac{4}{1-\\mu}\\sqrt{\\mathrm{tr}\\big((P_{\\|}\\Phi^{*}\\Phi P_{\\|})^{\\dagger}\\big)+2\\|(P_{\\|}\\Phi^{*}\\Phi P_{\\|})^{\\dagger}\\|_{\\mathrm{op}}\\log\\frac{d_{1}d_{2}}{\\delta}}\\qquad\\qquad}\\\\ {+\\,4\\|P_{\\bot}(\\Phi^{*}\\Phi)^{1/2}P_{\\bot}\\|_{\\mathrm{op}}\\|(P_{\\|}\\Phi^{*}\\Phi P_{\\|})^{\\dagger}\\|_{\\mathrm{op}}\\big(\\sqrt{d_{1}}+\\sqrt{d_{2}}+\\sqrt{2\\log\\frac{1}{\\delta}}\\big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where here $\\begin{array}{r}{\\Phi^{*}\\Phi(M):=\\sum_{n}\\varphi_{n}\\langle\\varphi_{n},M\\rangle}\\end{array}$ and $\\operatorname{tr}(\\cdot)$ describes the sum of the eigenvalues of the linear operator $(P_{\\|}\\Phi^{\\ast}\\Phi P_{\\|})^{\\dagger}:\\mathbb{R}^{d_{1}\\times d_{2}}\\rightarrow\\mathbb{R}^{d_{1}\\times d_{2}}$ . ", "page_idx": 5}, {"type": "text", "text": "Theorem 1 provides a precise bound on the estimation error of the nuclear norm estimator under arbitrary inputs $\\{\\varphi_{n}\\}_{n}$ . To the best of our knowledge, this is the first such characterization of this estimator. This characterization is particularly essential in active learning problems, such as the problem considered here, where it is critical that we understand precisely how the estimation error scales with different inputs, in order to determine which inputs will most effectively reduce the estimation error. As the observation model of Section 3.2 is a special case of the setting considered in (4.1) with $\\Theta_{\\star}\\leftarrow H$ , Theorem 1 provides a quantification of how quickly we can estimate the causal connectivity matrix given some set of inputs; we expand on the implications of this connection in Section 4.2. ", "page_idx": 5}, {"type": "text", "text": "Theorem 1 states that the estimation error of the estimator (4.2) scales (predominantly) with the strength of our inputs $\\varphi_{n}$ in the tangent space of $\\Theta_{\\star}$ . Indeed, if $\\left[w_{1},\\dots,w_{d_{1}}\\right]$ and $\\left[v_{1},\\ldots,v_{d_{2}}\\right]$ are the left and right singular vectors of the full SVD of $\\Theta_{\\star}$ , and $L\\in\\mathbb{R}^{d_{1}d_{2}\\times r(d_{1}+d_{2})-r^{2}}$ is a matrix with orthonormal columns vec $\\big(w_{i}v_{j}^{\\top}\\big)$ for $(i,j):\\{i\\leq r\\}\\cup\\{j\\leq r\\}$ , then ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{tr}\\big((P_{\\parallel}\\Phi^{*}\\Phi P_{\\parallel})^{\\dagger}\\big)=\\mathrm{tr}\\big(\\big(L^{\\top}\\!\\sum_{n=1}^{N}\\!\\mathrm{vec}(\\varphi_{n})\\mathrm{vec}(\\varphi_{n})^{\\top}L\\big)^{\\dagger}\\big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "so we see that the estimation error depends only on the scaling of $\\begin{array}{r}{\\sum_{n=1}^{N}\\mathrm{vec}(\\varphi_{n})\\mathrm{vec}(\\varphi_{n})^{\\top}}\\end{array}$ in the space spanned by $\\mathrm{vec}(u_{i}v_{j}^{\\top})$ for $i\\,\\le\\,r$ or $j\\le r$ \u2014the tangent space to $\\Theta_{\\star}$ . As an example of how this scales, assume that for $n\\;=\\;1,\\ldots,N$ the entries of each $\\varphi_{n}$ are IID ${\\mathcal{N}}(0,1)$ and ", "page_idx": 5}, {"type": "text", "text": "$N\\geq r(d_{1}+d_{2})-r^{2}$ . Then $\\mu\\approx0$ , $\\begin{array}{r}{\\mathrm{tr}\\big((P_{\\|}\\Phi^{*}\\Phi P_{\\|})^{\\dagger}\\big)\\approx\\frac{r(d_{1}+d_{2})-r^{2}}{N}}\\end{array}$ , $\\begin{array}{r}{\\|(P_{\\|}\\Phi^{*}\\Phi P_{\\|})^{\\dagger}\\|_{\\mathrm{op}}\\approx\\frac{1}{N}}\\end{array}$ , and $\\|P_{\\perp}(\\Phi^{*}\\Phi)^{1/2}P_{\\perp}\\|_{\\mathrm{op}}\\approx\\sqrt{N}$ . This translates to a bound of $\\begin{array}{r}{\\|\\widehat\\Theta-\\Theta_{\\star}\\|_{\\mathrm{F}}^{2}\\,\\le\\,\\frac{r(d_{1}+d_{2})-r^{2}+\\log(1/\\delta)}{N}}\\end{array}$ . Critically, we see that this does not scale with the total number of parameters, $d_{1}d_{2}$ , but instead with $r(d_{1}+d_{2})$ , which could be much smaller. The following result, due to [78], provides a lower bound on the estimation error of any unbiased estimator, and shows that the rate obtained by Theorem 1 is essentially unimprovable. ", "page_idx": 6}, {"type": "text", "text": "Theorem 2 (Corollary 1 of [78]). For unbiased estimator $\\begin{array}{r}{\\widehat{\\Theta},\\mathbb{E}[\\|\\widehat{\\Theta}-\\Theta_{\\star}\\|_{\\mathrm{F}}^{2}]\\ge\\mathrm{tr}\\big((P_{\\|}\\Phi^{*}\\Phi P_{\\|})^{\\dagger}\\big).}\\end{array}$ . ", "page_idx": 6}, {"type": "text", "text": "4.2 Active Learning for Low-Rank Matrix Estimation ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Given the above characterization, we turn now to the active learning problem: how can we best choose our inputs $\\varphi_{n}$ to speed up estimation error of $\\Theta_{\\star}$ ? For simplicity, rather than the general matrix regression setting of (4.1), we consider here the vector regression case, as this is the setting of interest in learning the causal connectivity. In particular, assume that we play some $u_{n}\\in\\mathbb{R}^{d_{2}}$ and observe $z_{n}=\\Theta_{\\star}u_{n}+\\eta_{n}$ , for $\\eta_{n}\\sim\\mathcal{N}(0,\\dot{I}_{d_{1}})$ . A single vector observation corresponds to observing $d_{1}$ observations from (4.1), the responses to the matrix inputs $\\varphi_{j}\\equiv{\\mathbf e}_{j}u_{n}^{\\top}$ for $j=1,\\ldots,d_{1}$ . Assume that $\\Theta_{\\star}$ is rank $r$ and let $V_{0}:=[v_{1},\\ldots,v_{r}]$ denote the first $r$ right singular vectors of the full SVD of $\\Theta_{\\star}$ . Then we have that: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{tr}\\big((P_{\\parallel}\\Phi^{*}\\Phi P_{\\parallel})^{\\dagger}\\big)=(d_{1}-r)\\cdot\\mathrm{tr}\\big((V_{0}^{\\top}\\Sigma_{N}V_{0})^{\\dagger}\\big)+r\\cdot\\mathrm{tr}\\big((\\Sigma_{N})^{\\dagger}\\big),\\quad\\Sigma_{N}:=\\sum_{n=1}^{N}u_{n}u_{n}^{\\top}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "This calculation, combined with Theorem 1, shows that the estimation error of $\\Theta_{\\star}$ scales with a weighting of two terms: one quantifying the amount of input energy we put into directions spanned by the top$r$ right singular vectors, and one that quantifies the amount of input energy played isotropically (that is, in all directions). Note, however, that the input energy played in directions $V_{0}$ is weighted by a factor of $d_{1}-r\\approx d_{1}$ , much larger weight than the weight of $r$ given to the term quantifying the isotropic input energy. This suggests that, to minimize the estimation error of $\\Theta_{\\star}$ , we should focus a large portion of our sampling budget to target the directions spanned by the top- $^r$ right singular vectors of $\\Theta_{\\star}$ . ", "page_idx": 6}, {"type": "text", "text": "This strategy admits a transparent intuition. If $\\Theta_{\\star}$ is rank- $^r$ and some vector $u$ is orthogonal to the top$r$ right singular vectors of $\\Theta_{\\star}$ , then $\\Theta_{\\star}u=0$ . Thus, if we know what subspace the top- ${\\bf\\nabla}r$ right singular vectors of $\\Theta_{\\star}$ span, playing $u$ orthogonal to this subspace gives us no additional information about $\\Theta_{\\star}$ ; in this case we should instead play $u$ aligned with this subspace. This is precisely what the first term in (4.3) quantifies, while the second term reflects the fact that we must also estimate the subspace spanned by the top- ${\\bf\\nabla}r$ right singular vectors of $\\Theta_{\\star}$ , for which playing inputs isotropically is optimal. ", "page_idx": 6}, {"type": "text", "text": "In general, as we do not know $\\Theta_{\\star}$ , we do not know $V_{0}$ , and so cannot directly compute inputs minimizing (4.3). To circumvent this, we consider the following iterative procedure, which alternates between obtaining an estimate of $\\Theta_{\\star},{\\widehat{\\Theta}}$ , and then playing the inputs that would minimize the estimation error\u2014minimize (4.3)\u2014if $\\widehat{\\Theta}$ were the true parameter. We present this procedure in Algorithm 1. ", "page_idx": 6}, {"type": "text", "text": "Algorithm 1 Active Estimation of Low-Rank Matrices   \n1: input: horizon $N$ , feasible inputs $\\boldsymbol{\\mathcal{U}}$ , rank $r$ , feasible set $\\kappa$   \n2: $\\widehat{\\Theta}_{1}\\gets I,\\mathcal{D}\\gets\\emptyset$   \n3:  for $\\ell=1,2,3,\\ldots,\\lceil\\log_{2}N\\rceil$ do   \n4: Let $\\widehat{V}_{0}$ denote the top- $^r$ right singular vectors of $\\widehat{\\Theta}_{\\ell}$ and $\\begin{array}{r}{\\pmb{\\Lambda}(\\lambda):=\\sum_{u\\in\\mathcal{U}}\\lambda_{u}u u^{\\top}}\\end{array}$ , solve: $\\begin{array}{r}{\\lambda_{\\ell}^{V}\\gets\\arg\\operatorname*{min}_{\\lambda\\in\\triangle_{\\mathcal{U}}}\\mathrm{tr}\\big((\\widehat{V}_{0}^{\\top}\\mathbf{A}(\\lambda)\\widehat{V}_{0})^{\\dagger}\\big),\\quad\\lambda_{\\ell}^{\\mathrm{unif}}\\gets\\arg\\operatorname*{min}_{\\lambda\\in\\triangle_{\\mathcal{U}}}\\mathrm{tr}\\big(\\mathbf{A}(\\lambda)^{\\dagger}\\big)}\\end{array}$ 5: For $2^{\\ell}$ steps, play input $\\begin{array}{r}{u_{n}\\sim\\frac{1}{2}\\lambda_{\\ell}^{V}+\\frac{1}{2}\\lambda_{\\ell}^{\\mathrm{unif}}}\\end{array}$ , add observations to $\\mathfrak{D}$   \n6: Update estimate of $\\begin{array}{r}{\\Theta_{\\star}\\!:\\widehat\\Theta_{\\ell+1}\\leftarrow\\arg\\operatorname*{min}_{\\Theta\\in\\mathcal{K}}\\sum_{(u,z)\\in\\mathfrak{D}}\\|z-\\Theta u\\|_{\\mathrm{F}}^{2}}\\end{array}$   \n7: return $\\widehat{\\Theta}_{\\ell+1}$ . ", "page_idx": 6}, {"type": "text", "text": "At every iteration $\\ell$ , Algorithm 1 computes two distributions over inputs: $\\lambda_{\\ell}^{V}$ , which targets the top- $^r$ right singular vectors of our current estimate of $\\Theta_{\\star}$ , and $\\lambda_{\\ell}^{\\mathrm{unif}}$ , which plays inputs isotropically, covering all directions. Rather than playing these distributions according to the precise weighting given in (4.3), we instead found it most effective to mix them at an equal rate. As we do not initially know which directions are spanned by the top- $^r$ right singular vectors of $\\Theta_{\\star},\\lambda_{\\ell}^{V}$ is not guaranteed to target the correct directions, especially in early iterations. $\\lambda_{\\ell}^{\\mathrm{unif}}$ plays inputs in every direction, however, and thus, even if $\\lambda_{\\ell}^{V}$ is not aligned to the top- $^r$ right singular vectors of $\\Theta_{\\star}$ , will ensure sufficient energy is still being played in the correct directions to allow for learning. Given this, we increase the weight of playing $\\bar{\\lambda}_{\\ell}^{\\mathrm{unif}}$ relative to that prescribed by (4.3). ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Note that the computation of the optimal inputs is a form of $A$ -optimal experiment design [79], which in general can be efficiently solved by, for example, the Frank-Wolfe algorithm [80]. Furthermore, efficient procedures for solving nuclear-norm regression problems exist, allowing us to estimate $\\widehat{\\Theta}_{\\ell+1}$ on line 6 efficiently [81]. We remark that Algorithm 1 takes as input $r$ , the rank of $\\Theta_{\\star}$ , and $\\kappa$ ,  which requires knowledge of $\\left\\|\\Theta_{\\star}\\right\\|_{*}$ . In general, when these quantities are unknown, they can be chosen via standard cross-validation procedures. ", "page_idx": 7}, {"type": "text", "text": "We emphasize again that the setting considered here corresponds precisely to the setting considered in Section 3.2 with $\\Theta_{\\star}\\leftarrow H$ , $u_{n}$ the input stimulation patterns, and $z_{n}$ the observed neural response to input $u_{n}$ . As such, if the causal connectivity $H$ is low rank, Algorithm 1 and the preceding results provide a methodology to select input stimuli to most efficiently estimate $H$ . In the following section, we will apply this to our photostimulation datasets. ", "page_idx": 7}, {"type": "text", "text": "5 Active Learning for Estimating Neural Population Dynamics ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We return now to the problem of photostimulus design for learning neural population dynamics, and seek to apply the insights of Section 4 to this setting. We present two sets of experiments. In Section 5.1 we use real data to fit a model of the population dynamics, treat this fitted model as a simulator for the true dynamics, and then demonstrate that we can learn the causal connectivity matrix $H$ of this simulator faster using active inputs versus passive inputs. Then, in Section 5.2 we split our real data into $750\\mathrm{ms}$ long trials of (stimulus, response) pairs (see Section 3) and demonstrate that our active learning algorithm is able to improve the performance of learning dynamical models on real data by adaptively selecting which trials to observe, training a model on the observed trials, and evaluating on a hold-out set of unseen trials. Here we find that our approach is able to learn an accurate model of the dynamics more quickly than non-adaptive approaches. ", "page_idx": 7}, {"type": "text", "text": "5.1 Active Learning on Data-Driven Neural Population Dynamics Simulator ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In Section 3.1, we demonstrated that photostimulation data can be effectively reconstructed using an AR- $k$ dynamics model. Given the effectiveness of these models at ftiting our data, in this section we treat them as a simulated representation of our true dynamics, allowing us to query them arbitrarily as a stand-in for the ground truth dynamics, and seek to determine whether carefully choosing the photostimulation pattern allows for efficient estimation of the causal connectivity matrix $H$ . ", "page_idx": 7}, {"type": "text", "text": "Experiment Details. To obtain models of the population dynamics to use for simulation, we fit an AR- $k$ model to each dataset as described in Section 3.1. In all cases we use an AR- $k$ model with order $k=4$ . We do one run of the experiments using low-rank model parameters $U V^{\\top}$ with rank $r=15$ , and then repeat the experiments using $r=35$ . In each case, we simulate $N=10000$ trials, where each trial corresponds to applying a photostimulus and observing the response for $\\tau=15$ timesteps, simulating our true data generation process. To simulate measurement noise and other trial-to-trial variability in neural responses, we corrupt the observations with Gaussian random noise. Motivated by the empirically observed fast decay of population dynamics in our datasets, we reset the initial state of the simulator at each new trial. ", "page_idx": 7}, {"type": "text", "text": "In practice, both the magnitude of the stimuli and number of neurons stimulated at each timestep are constrained by the photostimulation platform. To reflect this limitation in our simulator, we constrain our inputs to lie in $[0,1]$ , and also impose a sparsity penalty. Precisely, we choose the input set $\\boldsymbol{\\mathcal{U}}$ in Algorithm 1 to be ${\\bar{\\mathcal{U}}}:=\\{u\\in[0,1]^{d}~:~\\|u\\|_{1}\\leq\\gamma\\}$ , for some value $\\gamma>0$ (which we set to $\\gamma=30$ ). While this does not explicitly constrain inputs to be sparse, it can be efficiently optimized over, and we found in practice that the optimal inputs within this constraint set are in general at least $2\\gamma$ -sparse. As baseline methods, we consider the following: ", "page_idx": 7}, {"type": "text", "text": "\u2022 Random Stimulation: At each trial $n$ , choose $\\gamma$ neurons at random, and set corresponding elements of $u_{n}$ to 1. ", "page_idx": 7}, {"type": "text", "text": "\u2022 Uniform Stimulation: Compute $\\lambda^{\\mathrm{unif}}$ as in Algorithm 1 and play inputs $u_{n}\\sim\\lambda^{\\mathrm{unif}}$ for all $n$ . ", "page_idx": 7}, {"type": "image", "img_path": "nLQeE8QGGe/tmp/2c7d937028abb72759867c4737f9fed43459fa045cda644a165c2501c3b8fcb5.jpg", "img_caption": ["Figure 3: Performance of active stimulation design on estimating learned dynamics model. For each mouse dataset, we fti a low-rank AR- $k$ model as described in Section 3.1 (for ranks of 15 and 35, and $k=4$ ). Treating this as a simulator of the true dynamics, we compare our active stimulation design procedure (Active, Algorithm 1) to randomly choosing groups of neurons to excite (Random), and uniformly allocating stimulation across all neurons (Uniform), and plot how effectively each is able to estimate the connectivity of the simulator dynamics. For each figure and method we average over 20 trials, and plot the mean performance with error bars denoting 1 standard error (note that the error bars are barely visible as the standard deviation is very small). "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Our goal is to estimate the causal connectivity matrix $H$ induced by our learned dynamics (see Section 3.2). In practice, we are most interested in estimating the off-diagonal elements of $H$ , as these correspond to causal interactions between different neurons. To this end, we consider the error metric $\\frac{\\|M\\odot(H\\!-\\!\\widehat{H})\\|_{\\mathrm{F}}}{\\|M\\odot H\\|_{\\mathrm{F}}}$ , for $\\widehat{H}$ our estimate of $H$ , $M$ a matrix with all entries 1 except its diagonal, which is 0, and $\\odot$ element-wise multiplication. ", "page_idx": 8}, {"type": "text", "text": "Experiment Results. We present our results in Figure 3. As can be seen, across all learned simulators and rank levels, our active learning approach yields a non-trivial gain over both baseline approaches. In particular, on Mouse 1 and both datasets for Mouse 3, we observe a gain of between $1.5-2\\times$ over baselines\u2014that is, to achieve a given estimation error, our approach requires between $1.5-2\\times$ fewer samples than baseline methods. This demonstrates the effectiveness of our active learning procedure for estimating low-rank matrices\u2014our method is able to exploit the low-rank structure present in the underlying dynamics to speed up estimation, as compared to methods which do not take into account this structure. Furthermore, it shows that on a realistic simulation of neural population dynamics, we can effectively design stimuli to speed up the estimation of the dynamics. ", "page_idx": 8}, {"type": "text", "text": "5.2 Active Ranking of Real Data Observations ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "As described in Section 3, each of our datasets consist of roughly 2000 (stimulus, response) trials. In an online photostimulation experiment, we would choose the photostimulus actively for each trial. Here we seek to simulate this process using real experimental data, but offline, by choosing the ordering of the trials available in our pre-collected datasets. This serves as a testbed for active learning procedures: if we can more efficiently learn models in this offline setting, that is a strong indication that we should also see gains in online experiments. Indeed, those gains may be even greater online because in our offline setting we are severely restricted to choosing from only 100 candidate stimulation patterns. Thus, we interpret the results in this section as a lower bound on the performance we might expect online. ", "page_idx": 8}, {"type": "text", "text": "To validate this approach, we randomly choose 20 (out of the 100 total) unique photostimulation patterns and set aside a test set containing all 20 repeated trials of those photostimuli. This creates an $80\\%/20\\%$ train-test split of non-overlapping stimulus patterns. For $\\mathfrak{D}_{\\mathrm{train}}$ and $\\mathfrak{D}_{\\mathrm{test}}$ our train and test datasets, respectively, we consider the following query model: ", "page_idx": 8}, {"type": "image", "img_path": "nLQeE8QGGe/tmp/054f24f1d997a43cc607f53423caadf2896d16e06336edbb8ef1e99da85d5de4.jpg", "img_caption": ["Figure 4: Performance of active learning estimating photostimulation response on held-out trials. Each mouse dataset is split into trials corresponding to a stimulus-response pair, and we consider how these trials might be ordered to obtain more effective estimates with fewer training data trials, simulating the active learning process. Our approach (Active) is motivated by the low-rank excitation criteria of Algorithm 1 (see Appendix B.4 for more details) and we compare with randomly choosing which trial to observe next (Random). We plot the accuracy of the learned model in predicting neural responses on held-out test trials. We consider 20 different train-test splits (with 20 trials per split), and include plots of average performance across these splits, as well as splits where Active has the largest and smallest improvement over Random. We plot error bars denoting 1 standard error (note again that the error bars are barely visible as the standard deviation is very small). "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "We fit a dynamics model to the current set of observed trials, as described in Section 3.1, and use this model to predict the response of the true system on the held-out test inputs, computing the mean-squared error of these predictions as our metric. We apply a variant of Algorithm 1, described in more detail in Appendix B.4, and adapted to the query model above. In particular, to apply Algorithm 1 to learning a full dynamical system, we choose our inputs to target the right singular vectors of $B_{s}$ in (3.1). As a baseline method, we consider the procedure which randomly chooses an unobserved segment from $\\mathfrak{D}_{\\mathrm{train}}$ at each iteration. ", "page_idx": 9}, {"type": "text", "text": "We run the above experiment for 20 different randomly generated train-test splits on each dataset, and present our results in Figure 4, providing the results for the average performance over the train-test splits, as well as the best- and worst-case splits for active learning performance. As these results illustrate, though active learning does not give a substantial gain in all cases, in many cases it is able to give a gain of up to a factor of $2\\times$ in the number of samples required over the random baseline, and in the worst case, matches the baseline performance. This further confirms that taking into account low-rank structure when choosing which measurements to take can improve estimation rates, and, we believe, is a strong indicator that our active learning procedure would speed up estimation of neural population dynamics in online settings. ", "page_idx": 9}, {"type": "text", "text": "6 Discussion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we have developed a principled approach to active learning of photostimulation inputs for the identification of neural population dynamics and connectivity. We discuss three limitations of our approach, which each suggest potential future directions. First, we have considered active learning of the causal connectivity matrix and minimization of prediction error, both uniformly across all recorded neurons. Future work may focus on more specific scenarios, such as targeting particular dimensions of the neural activity space or changes in connectivity due to learning. Second, while we found that linear dynamics fti our data remarkably well, this may not always be the case. Does our methodology effectively scale to nonlinear dynamics? Finally, our real-data experiments were performed offilne. Future work may explore running our algorithm online during closed-loop photostimulation experiments. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This work was supported by NSF DMR award 2308979 to the University of Washington Materials Science Research Center (AW & KJ), the Shanahan Foundation Fellowship (LM & MSB), the Paul G. Allen Foundation (MR, KS, KD & MDG), NIH award R00-MH121533 (MDG), NSF CCF award 2007036 (KJ), and NSF CAREER award 2141511 (KJ). ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Saurabh Vyas, Matthew D Golub, David Sussillo, and Krishna V Shenoy. Computation through neural population dynamics. Annual Review of Neuroscience, 43:249\u2013275, 2020. [2] Mark M Churchland, John P Cunningham, Matthew T Kaufman, Justin D Foster, Paul Nuyujukian, Stephen I Ryu, and Krishna V Shenoy. Neural population dynamics during reaching. Nature, 487(7405):51\u201356, 2012.   \n[3] David Sussillo, Mark M Churchland, Matthew T Kaufman, and Krishna V Shenoy. A neural network that finds a naturalistic solution for the production of muscle activity. Nature Neuroscience, 18(7):1025\u20131033, 2015. [4] Nuo Li, Kayvon Daie, Karel Svoboda, and Shaul Druckmann. Robust neuronal dynamics in premotor cortex during motor planning. Nature, 532(7600):459\u2013464, 2016. [5] Evan D Remington, Devika Narain, Eghbal A Hosseini, and Mehrdad Jazayeri. Flexible sensorimotor computations through rapid reconfiguration of cortical dynamics. Neuron, 98(5): 1005\u20131019, 2018.   \n[6] Hidehiko K Inagaki, Susu Chen, Margreet C Ridder, Pankaj Sah, Nuo Li, Zidan Yang, Hana Hasanbegovic, Zhenyu Gao, Charles R Gerfen, and Karel Svoboda. A midbrain-thalamus-cortex circuit reorganizes cortical dynamics to initiate movement. Cell, 185(6):1065\u20131081, 2022. [7] Valerio Mante, David Sussillo, Krishna V Shenoy, and William T Newsome. Context-dependent computation by recurrent dynamics in prefrontal cortex. Nature, 503(7474):78\u201384, 2013.   \n[8] Federico Carnevale, Victor de Lafuente, Ranulfo Romo, Omri Barak, and N\u00e9stor Parga. Dynamic control of response criterion in premotor cortex during perceptual detection under temporal uncertainty. Neuron, 86(4):1067\u20131077, 2015. [9] Krithika Mohan, Ou Zhu, and David J Freedman. Interaction between neuronal encoding and population dynamics during categorization task switching in parietal cortex. Neuron, 109(4): 700\u2013712, 2021.   \n[10] Arseny Finkelstein, Lorenzo Fontolan, Michael N Economo, Nuo Li, Sandro Romani, and Karel Svoboda. Attractor dynamics gate cortical information flow during decision-making. Nature Neuroscience, 24(6):843\u2013850, 2021.   \n[11] Warasinee Chaisangmongkon, Sruthi K Swaminathan, David J Freedman, and Xiao-Jing Wang. Computing by robust transience: how the fronto-parietal network performs sequential, categorybased decisions. Neuron, 93(6):1504\u20131517, 2017.   \n[12] Aditya Nair, Tomomi Karigo, Bin Yang, Surya Ganguli, Mark J Schnitzer, Scott W Linderman, David J Anderson, and Ann Kennedy. An approximate line attractor in the hypothalamus encodes an aggressive state. Cell, 186(1):178\u2013193, 2023.   \n[13] Saurabh Vyas, Nir Even-Chen, Sergey D Stavisky, Stephen I Ryu, Paul Nuyujukian, and Krishna V Shenoy. Neural population dynamics underlying motor learning transfer. Neuron, 97 (5):1177\u20131186, 2018.   \n[14] Britton A Sauerbrei, Jian-Zhong Guo, Jeremy D Cohen, Matteo Mischiati, Wendy Guo, Mayank Kabra, Nakul Verma, Brett Mensh, Kristin Branson, and Adam W Hantman. Cortical pattern generation during dexterous movement is input-driven. Nature, 577(7790):386\u2013391, 2020.   \n[15] Xulu Sun, Daniel J O\u2019Shea, Matthew D Golub, Eric M Trautmann, Saurabh Vyas, Stephen I Ryu, and Krishna V Shenoy. Cortical preparatory activity indexes learned motor memories. Nature, 602(7896):274\u2013279, 2022.   \n[16] Emily R Oby, Alan D Degenhart, Erinn M Grigsby, Asma Motiwala, Nicole T McClain, Patrick J Marino, Byron Yu, and Aaron P Batista. Dynamical constraints on neural population activity. bioRxiv, pages 2024\u201301, 2024.   \n[17] Byron M Yu, John P Cunningham, Gopal Santhanam, Stephen I Ryu, Krishna V Shenoy, and Maneesh Sahani. Gaussian-process factor analysis for low-dimensional single-trial analysis of neural population activity. Journal of Neurophysiology, 102(1):614\u2013635, 2009.   \n[18] Jakob H Macke, Lars Buesing, John P Cunningham, Byron M Yu, Krishna V Shenoy, and Maneesh Sahani. Empirical models of spiking in neural populations. Advances in Neural Information Processing Systems, 24, 2011.   \n[19] Evan W Archer, Urs Koster, Jonathan W Pillow, and Jakob H Macke. Low-dimensional models of neural population activity in sensory cortical circuits. Advances in Neural Information Processing Systems, 27, 2014.   \n[20] Scott Linderman, Ryan P Adams, and Jonathan W Pillow. Bayesian latent structure discovery from multi-neuron recordings. Advances in Neural Information Processing Systems, 29, 2016.   \n[21] Yuanjun Gao, Evan W Archer, Liam Paninski, and John P Cunningham. Linear dynamical neural population models through nonlinear embeddings. Advances in Neural Information Processing Systems, 29, 2016.   \n[22] Chethan Pandarinath, Daniel J O\u2019Shea, Jasmine Collins, Rafal Jozefowicz, Sergey D Stavisky, Jonathan C Kao, Eric M Trautmann, Matthew T Kaufman, Stephen I Ryu, Leigh R Hochberg, Jaimie M Henderson, Krishna V Shenoy, Larry F Abbott, and David Sussillo. Inferring singletrial neural population dynamics using sequential auto-encoders. Nature Methods, 15(10): 805\u2013815, 2018.   \n[23] Joshua Glaser, Matthew Whiteway, John P Cunningham, Liam Paninski, and Scott Linderman. Recurrent switching dynamical systems models for multiple interacting neural populations. Advances in Neural Information Processing Systems, 33:14867\u201314878, 2020.   \n[24] Timothy D Kim, Thomas Z Luo, Jonathan W Pillow, and Carlos D Brody. Inferring latent dynamics underlying neural population activity via neural differential equations. In International Conference on Machine Learning, pages 5551\u20135561. PMLR, 2021.   \n[25] Orren Karniol-Tambour, David M Zoltowski, E Mika Diamanti, Lucas Pinto, David W Tank, Carlos D Brody, and Jonathan W Pillow. Modeling communication and switching nonlinear dynamics in multi-region neural activity. bioRxiv, pages 2022\u201309, 2022.   \n[26] Daniel J O\u2019Shea, Lea Duncker, Werapong Goo, Xulu Sun, Saurabh Vyas, Eric M Trautmann, Ilka Diester, Charu Ramakrishnan, Karl Deisseroth, Maneesh Sahani, et al. Direct neural perturbations reveal a dynamical mechanism for robust computation. bioRxiv, pages 2022\u201312, 2022.   \n[27] Mohammad Reza Keshtkaran, Andrew R Sedler, Raeed H Chowdhury, Raghav Tandon, Diya Basrai, Sarah L Nguyen, Hansem Sohn, Mehrdad Jazayeri, Lee E Miller, and Chethan Pandarinath. A large-scale neural network training framework for generalized estimation of single-trial population dynamics. Nature Methods, 19(12):1572\u20131577, 2022.   \n[28] Adrian Valente, Jonathan W. Pillow, and Srdjan Ostojic. Extracting computational mechanisms from neural data using low-rank RNNs. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022.   \n[29] Arthur Pellegrino, N Alex Cayco Gajic, and Angus Chadwick. Low tensor rank learning of neural dynamics. Advances in Neural Information Processing Systems, 36:11674\u201311702, 2023.   \n[30] Daniel Durstewitz, Georgia Koppe, and Max Ingo Thurm. Reconstructing computational system dynamics from neural data with recurrent neural networks. Nature Reviews Neuroscience, 24 (11):693\u2013710, 2023.   \n[31] Hyun Dong Lee, Andrew Warrington, Joshua Glaser, and Scott Linderman. Switching autoregressive low-rank tensor models. Advances in Neural Information Processing Systems, 36: 57976\u201358010, 2023.   \n[32] Aniruddh R Galgali, Maneesh Sahani, and Valerio Mante. Residual dynamics resolves recurrent contributions to neural computation. Nature Neuroscience, 26(2):326\u2013338, 2023.   \n[33] Omid G Sani, Bijan Pesaran, and Maryam M Shanechi. Dissociative and prioritized modeling of behaviorally relevant neural dynamics using recurrent neural networks. Nature Neuroscience, pages 1\u201313, 2024.   \n[34] Adam M Packer, Lloyd E Russell, Henry WP Dalgleish, and Michael H\u00e4usser. Simultaneous all-optical manipulation and recording of neural circuit activity with cellular resolution in vivo. Nature methods, 12(2):140\u2013146, 2015.   \n[35] Zihui Zhang, Lloyd E Russell, Adam M Packer, Oliver M Gauld, and Michael H\u00e4usser. Closedloop all-optical interrogation of neural circuits in vivo. Nature methods, 15(12):1037\u20131040, 2018.   \n[36] Selmaan N Chettih and Christopher D Harvey. Single-neuron perturbations reveal featurespecific competition in v1. Nature, 567(7748):334\u2013340, 2019.   \n[37] James H Marshel, Yoon Seok Kim, Timothy A Machado, Sean Quirin, Brandon Benson, Jonathan Kadmon, Cephra Raja, Adelaida Chibukhchyan, Charu Ramakrishnan, Masatoshi Inoue, et al. Cortical layer\u2013specific critical dynamics triggering perception. Science, 365(6453): eaaw5202, 2019.   \n[38] Luis Carrillo-Reid, Shuting Han, Weijian Yang, Alejandro Akrouh, and Rafael Yuste. Controlling visually guided behavior by holographic recalling of cortical ensembles. Cell, 178(2): 447\u2013457, 2019.   \n[39] Kayvon Daie, Karel Svoboda, and Shaul Druckmann. Targeted photostimulation uncovers circuit motifs supporting short-term memory. Nature Neuroscience, 24(2):259\u2013265, 2021.   \n[40] Hillel Adesnik and Lamiae Abdeladim. Probing neural codes with two-photon holographic optogenetics. Nature Neuroscience, 24(10):1356\u20131366, 2021.   \n[41] Marcus Triplett, Marta Gajowa, Hillel Adesnik, and Liam Paninski. Bayesian target optimisation for high-precision holographic optogenetics. Advances in Neural Information Processing Systems, 36, 2024.   \n[42] Christopher A Baker, Yishai M Elyada, Andres Parra, and M McLean Bolton. Cellular resolution circuit mapping with temporal-focused excitation of soma-targeted channelrhodopsin. Elife, 5: e14193, 2016.   \n[43] Laurence Aitchison, Lloyd Russell, Adam M Packer, Jinyao Yan, Philippe Castonguay, Michael Hausser, and Srinivas C Turaga. Model-based bayesian inference of neural activity and connectivity from all-optical interrogation of a neural circuit. Advances in Neural Information Processing Systems, 30, 2017.   \n[44] Anne Draelos and John Pearson. Online neural connectivity estimation with noisy group testing. Advances in Neural Information Processing Systems, 33:7437\u20137448, 2020.   \n[45] Travis A Hage, Alice Bosma-Moody, Christopher A Baker, Megan B Kratz, Luke Campagnola, Tim Jarsky, Hongkui Zeng, and Gabe J Murphy. Distribution and strength of interlaminar synaptic connectivity in mouse primary visual cortex revealed by two-photon optogenetic stimulation. BioRxiv, pages 2019\u201312, 2019.   \n[46] Arseny Finkelstein, Kayvon Daie, Marton Rozsa, Ran Darshan, and Karel Svoboda. Connectivity underlying motor cortex activity during naturalistic goal-directed behavior. bioRxiv, pages 2023\u201311, 2023.   \n[47] Mark M Churchland and Krishna V Shenoy. Delay of movement caused by disruption of cortical preparatory activity. Journal of neurophysiology, 97(1):348\u2013359, 2007.   \n[48] Nishal Shah, Sasidhar Madugula, Pawel Hottowy, Alexander Sher, Alan Litke, Liam Paninski, and EJ Chichilnisky. Efficient characterization of electrically evoked responses for neural interfaces. Advances in Neural Information Processing Systems, 32, 2019.   \n[49] Yuxiao Yang, Shaoyu Qiao, Omid G Sani, J Isaac Sedillo, Breonna Ferrentino, Bijan Pesaran, and Maryam M Shanechi. Modelling and prediction of the dynamic responses of large-scale brain networks during direct electrical stimulation. Nature biomedical engineering, 5(4): 324\u2013345, 2021.   \n[50] Emmanuel Candes and Benjamin Recht. Exact matrix completion via convex optimization. Communications of the ACM, 55(6):111\u2013119, 2012.   \n[51] Roman Vershynin. High-dimensional probability: An introduction with applications in data science, volume 47. Cambridge university press, 2018.   \n[52] Martin J Wainwright. High-dimensional statistics: A non-asymptotic viewpoint, volume 48. Cambridge university press, 2019.   \n[53] Lennart Ljung. System identification. Springer, 1998.   \n[54] Raman Mehra. Optimal input signals for parameter estimation in dynamic systems\u2013survey and new results. IEEE Transactions on Automatic Control, 19(6):753\u2013768, 1974.   \n[55] L\u00e1szl\u00f3 Gerencs\u00e9r and H\u00e5kan Hjalmarsson. Adaptive input design in system identification. In Proceedings of the 44th IEEE Conference on Decision and Control, pages 4988\u20134993. IEEE, 2005.   \n[56] Dimitrios Katselis, Cristian R Rojas, H\u00e5kan Hjalmarsson, and Mats Bengtsson. Applicationoriented finite sample experiment design: A semidefinite relaxation approach. IFAC Proceedings Volumes, 45(16):1635\u20131640, 2012.   \n[57] Ian R Manchester. Input design for system identification via convex relaxation. In 49th IEEE Conference on Decision and Control (CDC), pages 2041\u20132046. IEEE, 2010.   \n[58] Cristian R Rojas, James S Welsh, Graham C Goodwin, and Arie Feuer. Robust optimal experiment design for system identification. Automatica, 43(6):993\u20131008, 2007.   \n[59] Graham Clifford Goodwin and Robert L Payne. Dynamic system identification: experiment design and data analysis. Academic press, 1977.   \n[60] Kristian Lindqvist and H\u00e5kan Hjalmarsson. Identification for control: Adaptive input design using convex optimization. In Proceedings of the 40th IEEE Conference on Decision and Control (Cat. No. 01CH37228), volume 5, pages 4326\u20134331. IEEE, 2001.   \n[61] L\u00e1szl\u00f3 Gerencs\u00e9r, Jonas M\u00e5rtensson, and H\u00e5kan Hjalmarsson. Adaptive input design for arx systems. In 2007 European Control Conference (ECC), pages 5707\u20135714. IEEE, 2007.   \n[62] Andrew Wagenmaker and Kevin Jamieson. Active learning for identification of linear dynamical systems. In Conference on Learning Theory, pages 3487\u20133582. PMLR, 2020.   \n[63] Andrew J Wagenmaker, Max Simchowitz, and Kevin Jamieson. Task-optimal exploration in linear dynamical systems. In International Conference on Machine Learning, pages 10641\u2013 10652. PMLR, 2021.   \n[64] Horia Mania, Michael I Jordan, and Benjamin Recht. Active learning for nonlinear system identification with guarantees. J. Mach. Learn. Res., 23:32\u20131, 2022.   \n[65] Andrew Wagenmaker, Guanya Shi, and Kevin G Jamieson. Optimal exploration for model-based rl in nonlinear systems. Advances in Neural Information Processing Systems, 36, 2024.   \n[66] Aditi Jha, Zoe C Ashwood, and Jonathan W Pillow. Active learning for discrete latent variable models. Neural Computation, 36(3):437\u2013474, 2024.   \n[67] Sumeet Katariya, Branislav Kveton, Csaba Szepesvari, Claire Vernade, and Zheng Wen. Stochastic rank-1 bandits. In Artificial Intelligence and Statistics, pages 392\u2013401. PMLR, 2017.   \n[68] Kwang-Sung Jun, Rebecca Willett, Stephen Wright, and Robert Nowak. Bilinear bandits with low-rank structure. In International Conference on Machine Learning, pages 3163\u20133172. PMLR, 2019.   \n[69] Yangyi Lu, Amirhossein Meisami, and Ambuj Tewari. Low-rank generalized linear bandit problems. In International Conference on Artificial Intelligence and Statistics, pages 460\u2013468. PMLR, 2021.   \n[70] Yue Kang, Cho-Jui Hsieh, and Thomas Chun Man Lee. Efficient frameworks for generalized low-rank matrix bandit problems. Advances in Neural Information Processing Systems, 35: 19971\u201319983, 2022.   \n[71] Ery Arias-Castro, Emmanuel J Candes, and Mark A Davenport. On the fundamental limits of adaptive sensing. IEEE Transactions on Information Theory, 59(1):472\u2013481, 2012.   \n[72] John P Cunningham and Byron M Yu. Dimensionality reduction for large-scale neural recordings. Nature Neuroscience, 17(11):1500\u20131509, 2014.   \n[73] Patrick T Sadtler, Kristin M Quick, Matthew D Golub, Steven M Chase, Stephen I Ryu, Elizabeth C Tyler-Kabara, Byron M Yu, and Aaron P Batista. Neural constraints on learning. Nature, 512:423\u2013426, 2014.   \n[74] Saul Kato, Harris S Kaplan, Tina Schr\u00f6del, Susanne Skora, Theodore H Lindsay, Eviatar Yemini, Shawn Lockery, and Manuel Zimmer. Global brain dynamics embed the motor command sequence of caenorhabditis elegans. Cell, 163(3):656\u2013669, 2015.   \n[75] Matthew D Golub, Patrick T Sadtler, Emily R Oby, Kristin M Quick, Stephen I Ryu, Elizabeth C Tyler-Kabara, Aaron P Batista, Steven M Chase, and Byron M Yu. Learning by neural reassociation. Nature Neuroscience, 21(4):607\u2013616, 2018.   \n[76] Juan A Gallego, Matthew G Perich, Stephanie N Naufel, Christian Ethier, Sara A Solla, and Lee E Miller. Cortical population activity within a preserved neural manifold underlies multiple motor behaviors. Nature Communications, 9(1):4233, 2018.   \n[77] Rishidev Chaudhuri, Berk Ger\u00e7ek, Biraj Pandey, Adrien Peyrache, and Ila Fiete. The intrinsic attractor manifold and population dynamics of a canonical cognitive circuit across waking and sleep. Nature Neuroscience, 22(9):1512\u20131520, 2019.   \n[78] Gongguo Tang and Arye Nehorai. Lower bounds on the mean-squared error of low-rank matrix reconstruction. IEEE Transactions on Signal Processing, 59(10):4559\u20134571, 2011.   \n[79] Friedrich Pukelsheim. Optimal design of experiments. SIAM, 2006.   \n[80] Marguerite Frank and Philip Wolfe. An algorithm for quadratic programming. Naval research logistics quarterly, 3(1-2):95\u2013110, 1956.   \n[81] Maryam Fazel. Matrix rank minimization with applications. PhD thesis, PhD thesis, Stanford University, 2002.   \n[82] Marius Pachitariu, Carsen Stringer, Sylvia Schr\u00f6der, Mario Dipoppa, L Federico Rossi, Matteo Carandini, and Kenneth D Harris. Suite2p: beyond 10,000 neurons with standard two-photon microscopy. BioRxiv, page 061507, 2016.   \n[83] Diederik P Kingma. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "A Proof of Theorem 1 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "As $\\widehat{\\Theta}$ is a minimizer, we have that $\\|\\Phi(\\widehat{\\Theta})-z\\|_{2}^{2}\\leq\\|\\Phi(\\Theta_{\\star})-z\\|_{2}^{2}$ . Consequently, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\Phi(\\Theta_{\\star})-z\\|_{2}^{2}<\\operatorname*{min}_{\\Theta_{\\star}+\\Delta\\in\\mathcal{K}:\\|\\Delta\\|_{\\mathrm{F}}\\geq\\rho}\\|\\Phi(\\Theta_{\\star}+\\Delta)-z\\|_{2}^{2}\\implies\\widehat{\\Theta}\\not\\in\\left\\{\\Theta_{\\star}+\\Delta\\in\\mathcal{K}:\\|\\Delta\\|_{\\mathrm{F}}\\geq\\rho\\right\\}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\implies\\|\\widehat{\\Theta}-\\Theta_{\\star}\\|_{\\mathrm{F}}<\\rho.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Thus, our strategy will attempt to find a minimum $\\rho\\,>\\,0$ that makes the first expression true. Equivalently, we show that the following quantity is non-positive ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\Phi(\\Theta_{\\star})-z\\|_{2}^{2}-\\underset{\\Theta_{\\star}+\\Delta\\in\\mathcal{K}:\\|\\Delta\\|_{\\mathrm{F}}\\geq\\rho}{\\operatorname*{min}}\\|\\Phi(\\Theta_{\\star}+\\Delta)-z\\|_{2}^{2}}\\\\ &{\\qquad\\qquad\\qquad=-\\Big(\\underset{\\Theta_{\\star}+\\Delta\\in\\mathcal{K}:\\|\\Delta\\|_{\\mathrm{F}}\\geq\\rho}{\\operatorname*{min}}2\\langle\\Phi(\\Delta),\\Phi(\\Theta_{\\star})-z\\rangle+\\|\\Phi(\\Delta)\\|_{2}^{2}\\Big)}\\\\ &{\\underset{M+\\Delta\\in\\mathcal{K}:\\|\\Delta\\|_{\\mathrm{F}}\\geq\\rho}{\\operatorname*{max}}2\\langle\\Phi(\\Delta),\\eta\\rangle-\\|\\Phi(\\Delta)\\|_{2}^{2}}\\\\ &{\\underset{t\\geq\\rho}{\\operatorname*{max}}\\underset{M+\\Delta\\in\\mathcal{K}:\\|\\Delta\\|_{\\mathrm{F}}=t}{\\operatorname*{max}}2\\langle\\Phi(\\Delta),\\eta\\rangle-\\|\\Phi(\\Delta)\\|_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Intuitively, when $t$ is large, the quadratic term will dominate the inner product making the entire expression non-positive. We will employ a geometric fact about the nuclear norm ball. ", "page_idx": 15}, {"type": "text", "text": "Lemma A.1. Let $\\mathcal{K}=\\{\\Theta:\\|\\Theta\\|_{*}\\leq\\|\\Theta_{\\star}\\|_{*}\\}$ . If $\\Theta_{\\star}+\\Delta\\in\\mathcal{K}$ then $\\begin{array}{r}{\\|P_{\\perp}(\\Delta)\\|_{*}\\leq-\\langle\\Delta,U V^{\\top}\\rangle\\leq}\\end{array}$ $\\|P_{\\|}(\\Delta)\\|_{\\mathrm{F}}$ . ", "page_idx": 15}, {"type": "text", "text": "Proof. If $\\|\\Theta_{\\star}+\\Delta\\|_{*}>\\|\\Theta_{\\star}\\|_{*}$ then $\\Theta_{\\star}+\\Delta\\not\\in K$ . By the convexity of the nuclear norm ball, we have that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\|\\Theta_{\\star}+\\Delta\\|_{*}\\geq\\|\\Theta_{\\star}\\|_{*}+\\langle\\Delta,U V^{\\top}\\rangle+\\langle\\Delta,W\\rangle}&{\\quad\\forall W:W=P_{\\perp}(W)\\,,\\|W\\|_{2}\\le1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Consequently, as the dual norm to $\\|\\cdot\\|_{2}$ is the nuclear norm $\\|\\cdot\\|_{*}$ we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\Theta_{\\star}+\\Delta\\|_{*}\\geq\\|\\Theta_{\\star}\\|_{*}+\\langle\\Delta,U V^{\\top}\\rangle+\\|P_{\\perp}(\\Delta)\\|_{*}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Thus, if $\\langle\\Delta,U V^{\\top}\\rangle+\\|P_{\\bot}(\\Delta)\\|_{*}>0$ then $\\Theta_{\\star}+\\Delta\\notin\\mathcal{K}$ . Consequently, if $\\Theta_{\\star}+\\Delta\\in\\mathcal{K}$ then $\\langle\\Delta,U V^{\\top}\\rangle+\\|P_{\\bot}(\\Delta)\\|_{*}\\leq0$ . \u53e3 ", "page_idx": 15}, {"type": "text", "text": "Recalling that $\\|M\\|_{\\mathrm{F}}\\leq\\|M\\|_{*}$ for any matrix $M$ , an interesting consequence of the above lemma is that $\\|P_{\\perp}(\\Delta)\\|_{\\mathrm{F}}\\leq\\|P_{\\|}(\\Delta)\\|_{\\mathrm{F}}$ which implies $\\|P_{\\|}(\\Delta)\\|_{\\mathrm{F}}^{2}\\leq\\|\\bar{\\Delta}\\|_{\\mathrm{F}}^{2}\\leq2\\|P_{\\|}(\\Delta)\\|_{\\mathrm{F}}^{2}$ . That is, the total error is dominated by the error in the tanget space of $\\Theta_{\\star}$ . ", "page_idx": 15}, {"type": "text", "text": "Applying this lemma, we have that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\substack{d+\\Delta\\in K:||\\Delta||_{\\mathrm{F}}=t}}{\\operatorname*{max}}2\\langle\\Phi(\\Delta),\\eta\\rangle-||\\Phi(\\Delta)||_{2}^{2}\\leq\\underset{\\Delta:||P_{\\perp}(\\Delta)||_{\\mathrm{F}},||\\Delta||_{\\mathrm{F}},||\\Delta||_{\\mathrm{F}}=t}{\\operatorname*{max}}2\\langle\\Phi(\\Delta),\\eta\\rangle-||\\Phi(\\Delta)||_{2}^{2}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\underset{\\Delta:||P_{\\perp}(\\Delta)||_{\\mathrm{F}},||\\Delta||_{\\mathrm{F}}=t}{\\operatorname*{max}}2\\langle\\Phi(P_{\\|}(\\Delta))+\\Phi(P_{\\perp}(\\Delta)),\\eta\\rangle-||\\Phi(\\Delta)||_{\\mathrm{F}},||\\Delta||_{\\mathrm{F}}=t}\\\\ &{\\leq\\underset{\\Delta:||P_{\\perp}(\\Delta)||_{\\mathrm{F}}(\\Delta)||_{\\mathrm{F}},||\\Delta||_{\\mathrm{F}},||\\Delta||_{\\mathrm{F}}=t}{\\operatorname*{max}}2\\langle\\Phi(P_{\\|}(\\Delta)),\\eta\\rangle-||\\Phi(\\Delta)||_{2}^{2}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\underset{\\Delta:||P_{\\perp}(\\Delta)||_{\\mathrm{F}},||\\Delta||_{\\mathrm{F}}=t}{\\operatorname*{max}}2\\langle\\Phi(P_{\\perp}(\\Delta)),\\eta\\rangle}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where the equality uses the fact that $\\Delta=P_{\\parallel}(\\Delta)+P_{\\perp}(\\Delta)$ and the linearity of $\\Phi$ . For any $\\boldsymbol{v}\\in\\mathbb{R}^{N}$ we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\langle\\Phi(\\Delta),v\\rangle=\\langle\\Delta,\\sum_{n=1}^{N}X_{n}v_{n}\\rangle=:\\langle\\Delta,\\Phi^{*}(v)\\rangle\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\Phi^{*}$ denotes the adjoint of $\\Phi$ . We also recognize that the operator $\\Phi^{*}\\Phi:\\mathbb{R}^{d_{1}\\times d_{2}}\\rightarrow\\mathbb{R}^{d_{1}\\times d_{2}}$ is also linear, defined as $\\Phi^{*}\\Phi(M)\\;=\\;\\Phi^{*}(\\{\\langle X_{n},M\\rangle\\}_{n})\\;=\\;\\sum_{n=1}^{N}X_{n}\\langle X_{n},M\\rangle$ . Consequently $(\\Phi^{*}\\Phi)^{1/2}$ is well-defined and is the same operator as $\\Phi^{*}\\Phi$ after taking the square root of its eigenvalues. ", "page_idx": 15}, {"type": "text", "text": "The next three lemmas bound the two terms of above. Combining them yields the result of the theorem. ", "page_idx": 15}, {"type": "text", "text": "Lemma A.2. With probability at least $1-\\delta$ we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\substack{\\mathbf{1}:\\|P_{\\perp}(\\Delta)\\|_{*}\\leq\\|P_{\\mathbb{I}}(\\Delta)\\|_{\\mathrm{F}},\\|\\Delta\\|_{\\mathrm{F}}=t}}2\\langle\\Phi(P_{\\perp}(\\Delta)),\\eta\\rangle\\leq2t\\|P_{\\perp}(\\Phi^{*}\\Phi)^{1/2}P_{\\perp}\\|_{\\mathrm{op}}(\\sqrt{d_{1}}+\\sqrt{d_{2}}+\\sqrt{2}\\log(1/\\delta))\\|_{\\mathrm{F}},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof. Computing this term amounts to bounding a Gaussian width. Begin by recognizing that by the non-expansive property of projections, $\\|P_{\\|}(\\Delta)\\|_{\\mathrm{F}}\\leq\\|\\Delta\\|_{\\mathrm{F}}\\leq t$ which results in the simplification: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{\\Delta:\\vert\\boldsymbol{\\mathrm{P}}_{\\perp}(\\Delta)\\vert\\vert_{*}\\leq\\vert\\boldsymbol{\\mathrm{J}}\\vert\\vert_{\\mathbb{P}_{\\!\\mathrm{l}}(\\Delta)\\vert\\vert_{\\mathrm{F}},\\vert\\vert\\Delta\\vert\\vert\\mathrm{F}=t}}{\\operatorname*{max}}2\\langle\\Phi(\\boldsymbol{P}_{\\perp}(\\Delta)),\\boldsymbol{\\eta}\\rangle\\leq\\underset{\\Delta:\\vert\\boldsymbol{\\mathrm{P}}_{\\perp}(\\Delta)\\vert\\vert_{*}\\leq t}{\\operatorname*{max}}2\\langle\\Phi(\\boldsymbol{P}_{\\perp}(\\Delta)),\\boldsymbol{\\eta}\\rangle}&{}\\\\ {=\\underset{\\Delta:\\vert\\boldsymbol{\\mathrm{P}}_{\\perp}(\\Delta)\\vert\\vert_{*}\\leq t}{\\operatorname*{max}}2\\langle\\sum_{n=1}^{N}\\eta_{n}\\boldsymbol{X}_{n},\\boldsymbol{P}_{\\perp}(\\Delta)\\rangle}&{}\\\\ {=\\underset{\\Delta:\\vert\\boldsymbol{\\mathrm{P}}_{\\perp}(\\Delta)\\vert\\vert_{*}\\leq t}{\\operatorname*{max}}2\\langle\\boldsymbol{P}_{\\perp}(\\sum_{n=1}^{N}\\eta_{n}\\boldsymbol{X}_{n}),\\boldsymbol{P}_{\\perp}(\\Delta)\\rangle}&{}\\\\ {\\leq2t\\vert\\vert\\boldsymbol{\\mathrm{P}}_{\\perp}(\\sum_{n=1}^{N}\\eta_{n}\\boldsymbol{X}_{n})\\vert\\vert_{\\mathrm{op}}}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "using the fact that the operator norm and nuclear norm are dual to each other. Note that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\sum_{n=1}^{N}\\eta_{n}X_{n}=\\operatorname*{mat}(\\sum_{n=1}^{N}\\eta_{n}\\mathrm{vec}(X_{i}))=\\operatorname*{mat}((\\sum_{n=1}^{N}\\mathrm{vec}(X_{n})\\mathrm{vec}(X_{n})^{\\top})^{1/2}\\mathrm{vec}(\\eta^{\\prime}))=:(\\Phi^{*}\\Phi)^{1/2}(\\eta)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\eta^{\\prime}\\in\\mathbb{R}^{d_{1}\\times d_{2}}$ with $\\eta_{i,j}^{\\prime}\\sim\\mathcal{N}(0,1)$ . We then observe that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\|P_{\\perp}(\\sum_{n=1}^{N}\\eta_{n}X_{n})P_{\\perp}\\|_{\\mathrm{op}}=\\|P_{\\perp}(\\mathrm{mat}((\\sum_{n=1}^{N}\\mathrm{vec}(X_{n})\\mathrm{vec}(X_{n})^{\\top})^{1/2}\\mathrm{vec}(\\eta^{\\prime})))P_{\\perp}\\|_{\\mathrm{op}}}}\\\\ &{\\leq\\|P_{\\perp}(\\Phi^{*}\\Phi)^{1/2}P_{\\perp}\\|_{\\mathrm{op}}\\|\\eta^{\\prime}\\|_{\\mathrm{op}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "which completes the proof of the first claim. Recognizing that $\\|\\eta^{\\prime}\\|_{\\mathrm{op}}$ \u221ais just t\u221ahe largest singular value of a Gaussian matrix, we find that $\\begin{array}{r}{\\mathbb{E}[\\operatorname*{sup}_{\\|u\\|_{2}\\leq1,\\|v\\|_{2}\\leq1}\\langle u v^{\\top},\\eta\\rangle]\\leq\\sqrt{d_{1}}+\\sqrt{d_{2}}}\\end{array}$ by Exercise 5.14 of [52]. Applying a sub-Gaussian tail bound completes the proof. \u53e3 ", "page_idx": 16}, {"type": "text", "text": "Lemma A.3. Define $\\mu:=\\|(\\Phi^{*}\\Phi)^{1/2}((P_{\\|}\\Phi^{*}\\Phi P_{\\|})^{\\dag})^{1/2}\\|_{\\mathrm{op}}$ . Then ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\Delta\\colon\\lVert P_{\\perp}(\\Delta)\\rVert\\,\\mathrm{\\nabla_{\\b{*}}^{2}}\\rVert_{\\mathrm{F}},\\lVert\\Delta\\rVert_{\\mathrm{F}}\\rVert_{\\mathrm{F}}\\mathrm{e}^{2\\langle\\Phi(P_{\\parallel}(\\Delta)),\\eta\\rangle}-\\lVert\\Phi(\\Delta)\\rVert_{2}^{2}}\\quad}&{}\\\\ &{\\leq\\mathrm{\\normalfont~\\displaystyle~\\operatorname*{max}_{\\Delta\\colon\\lVert P_{\\parallel}(\\Delta)\\rVert_{\\mathrm{F}}\\geq t/\\sqrt2}\\,2\\langle\\Phi(P_{\\parallel}(\\Delta)),\\eta\\rangle-(1-\\mu)\\lVert\\Phi(P_{\\parallel}(\\Delta))\\rVert_{2}^{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof. Recognizing that $\\Phi(\\Delta)\\in\\mathbb{R}^{N}$ we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\Phi(\\Delta)\\|_{2}^{2}=\\|\\Phi(P_{\\|}(\\Delta)+P_{\\perp}(\\Delta))\\|_{2}^{2}}\\\\ &{\\qquad\\qquad=\\|\\Phi(P_{\\|}(\\Delta))+\\Phi(P_{\\perp}(\\Delta))\\|_{2}^{2}}\\\\ &{\\qquad\\qquad=\\|\\Phi(P_{\\|}(\\Delta))\\|_{2}^{2}+\\|\\Phi(P_{\\perp}(\\Delta))\\|_{2}^{2}+2\\langle\\Phi(P_{\\|}(\\Delta)),\\Phi(P_{\\perp}(\\Delta))\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "To aid in readability, we make a number of notational modifications. First, we drop parentheses so that $\\Phi(P_{\\|}(\\Delta))$ is just notated as $\\Phi P_{\\|}\\boldsymbol{\\Delta}$ . Second, we define $M^{\\dag/2}:=(M^{\\dag})^{1/2}$ where $M^{\\dagger}$ is the pseudoinverse. If $\\Phi^{*}\\Phi$ is invertible restricted to the range of $P_{\\parallel}$ , then $P_{\\|}\\Delta=(P_{\\|}\\Phi^{*}\\Phi P_{\\|})^{\\dagger/2}(\\Phi^{*}\\Phi)^{1/2}P_{\\|}\\Delta$ for all $\\Delta$ . Thus, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|\\langle\\Phi(P_{\\|}(\\Delta)),\\Phi(P_{\\bot}(\\Delta))\\rangle|=|\\langle\\Phi P_{\\|}\\Delta,\\Phi P_{\\bot}\\Delta\\rangle|}\\\\ &{\\quad\\quad\\quad\\quad=|\\langle P_{\\|}\\Delta,(\\Phi^{*}\\Phi)P_{\\bot}\\Delta\\rangle|}\\\\ &{\\quad\\quad\\quad=|\\langle(\\Phi^{*}\\Phi)^{1/2}P_{\\|}\\Delta,(\\Phi^{*}\\Phi)^{1/2}P_{\\bot}\\Delta\\rangle|}\\\\ &{\\quad\\quad\\quad=|\\langle(\\Phi^{*}\\Phi)^{1/2}P_{\\|}(P_{\\|}\\Phi^{*}\\Phi P_{\\|})^{\\dagger/2}(\\Phi^{*}\\Phi)^{1/2}P_{\\|}\\Delta,(\\Phi^{*}\\Phi)^{1/2}P_{\\bot}\\Delta\\rangle|}\\\\ &{\\quad\\quad\\quad\\leq\\|(\\Phi^{*}\\Phi)^{1/2}(P_{\\|}\\Phi^{*}\\Phi P_{\\|})^{\\dagger/2}(\\Phi^{*}\\Phi)^{1/2}P_{\\|}\\Delta\\|_{\\mathrm{F}}\\,\\|(\\Phi^{*}\\Phi)^{1/2}P_{\\bot}\\Delta\\|_{\\mathrm{F}}}\\\\ &{\\quad\\quad\\quad\\leq\\|(\\Phi^{*}\\Phi)^{1/2}(P_{\\|}\\Phi^{*}\\Phi P_{\\|})^{\\dagger/2}\\|_{\\mathrm{op}}\\,\\|(\\Phi^{*}\\Phi)^{1/2}P_{\\|}\\Delta\\|_{\\mathrm{F}}\\,\\|(\\Phi^{*}\\Phi)^{1/2}P_{\\bot}\\Delta\\|_{\\mathrm{F}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the last two lines follow from Cauchy-Schwartz. Observe that for $\\mu<1$ we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{c}{a^{2}+b^{2}-2a b\\mu=(1-\\mu)a^{2}+(1-\\mu)b^{2}+\\mu a^{2}+\\mu b^{2}-2a b\\mu}\\\\ {=(1-\\mu)a^{2}+(1-\\mu)b^{2}+\\mu(a-b)^{2}}\\\\ {\\geq(1-\\mu)a^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Thus, if $\\mu:=\\|(\\Phi^{*}\\Phi)^{1/2}(P_{\\|}\\Phi^{*}\\Phi P_{\\|})^{\\dag/2}\\|_{\\mathrm{op}}$ then ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\Phi(\\Delta)\\|_{2}^{2}\\geq\\|\\Phi(P_{\\|}(\\Delta))\\|_{2}^{2}+\\|\\Phi(P_{\\perp}(\\Delta))\\|_{2}^{2}-2|\\langle\\Phi(P_{\\|}(\\Delta)),\\Phi(P_{\\perp}(\\Delta))\\rangle|}\\\\ &{\\qquad\\qquad\\geq\\|\\Phi(P_{\\|}(\\Delta))\\|_{2}^{2}+\\|\\Phi(P_{\\perp}(\\Delta))\\|_{2}^{2}-2\\mu\\|(\\Phi^{*}\\Phi)^{1/2}P_{\\|}\\Delta\\|_{\\mathrm{F}}\\,\\|(\\Phi^{*}\\Phi)^{1/2}P_{\\perp}\\Delta\\|_{\\mathrm{F}}}\\\\ &{\\qquad\\qquad\\geq(1-\\mu)\\|\\Phi(P_{\\|}(\\Delta))\\|_{2}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Thus, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\Delta\\!:\\!\\|P_{\\bot}(\\Delta)\\|\\!\\|_{*}\\!\\leq\\!\\!\\|P_{\\|}(\\Delta)\\|\\!\\|_{\\mathbb{V}}\\!,\\!\\|\\Delta\\|_{\\mathbb{F}}\\!=\\!t^{2}\\big\\langle\\Phi(P_{\\|}(\\Delta)),\\eta\\big\\rangle-\\|\\Phi(\\Delta)\\|_{2}^{2}}}\\\\ &{\\leq_{\\Delta:\\|P_{\\bot}(\\Delta)\\|_{*}\\leq\\!\\!\\|P_{\\|}(\\Delta)\\|_{\\mathbb{F}},\\|\\Delta\\|_{\\mathbb{F}}\\!=\\!t^{2}\\big\\langle\\Phi(P_{\\|}(\\Delta)),\\eta\\big\\rangle-(1-\\mu)\\|\\Phi(P_{\\|}(\\Delta))\\|_{2}^{2}}}\\\\ &{\\leq_{\\Delta:\\|P_{\\bot}(\\Delta)\\|_{\\mathbb{F}}\\leq\\|P_{\\|}(\\Delta)\\|_{\\mathbb{F}},\\|\\Delta\\|_{\\mathbb{F}}\\!=\\!t^{2}\\big\\langle\\Phi(P_{\\|}(\\Delta)),\\eta\\big\\rangle-(1-\\mu)\\|\\Phi(P_{\\|}(\\Delta))\\|_{2}^{2}}}\\\\ &{\\leq_{\\Delta:\\|P_{\\|}(\\Delta)\\|_{\\mathbb{F}}\\geq t/\\sqrt{2}}2\\big\\langle\\Phi(P_{\\|}(\\Delta)),\\eta\\big\\rangle-(1-\\mu)\\|\\Phi(P_{\\|}(\\Delta))\\|_{2}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where we\u2019ve used the facts that $\\|\\cdot\\|_{*}\\,\\leq\\,\\|\\,\\cdot\\,\\|_{\\mathrm{F}}$ and $t^{2}\\,=\\,\\|\\Delta\\|_{\\mathrm{F}}^{2}\\,=\\,\\|P_{\\|}(\\Delta)\\|_{\\mathrm{F}}^{2}+\\|P_{\\perp}(\\Delta)\\|_{\\mathrm{F}}^{2}\\,\\le$ $2\\|P_{\\|}(\\Delta)\\|_{\\mathrm{F}}^{2}$ . ", "page_idx": 17}, {"type": "text", "text": "Lemma A.4. Let $K=\\operatorname*{min}\\{\\log_{2}(N),d_{1}d_{2}\\}$ . Then for any $\\alpha>0$ , $i f$ ", "page_idx": 17}, {"type": "equation", "text": "$$\nt\\geq\\frac{1}{1-\\mu}\\sqrt{16\\mathrm{tr}\\Big((P_{\\parallel}\\Phi^{*}\\Phi P_{\\parallel})^{\\dagger}\\Big)+32\\Vert(P_{\\parallel}\\Phi^{*}\\Phi P_{\\parallel})^{\\dagger}\\Vert_{\\mathrm{op}}\\log(K/\\delta)}+\\frac{2\\alpha\\Vert(P_{\\parallel}\\Phi^{*}\\Phi P_{\\parallel})^{\\dagger}\\Vert_{\\mathrm{op}}}{1-\\mu}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "then with probability at least $1-\\delta$ we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\alpha t+\\operatorname*{max}_{\\substack{\\Delta:\\|P_{\\parallel}(\\Delta)\\|_{\\mathrm{F}}\\geq t/\\sqrt{2}}}2\\langle\\Phi(P_{\\parallel}(\\Delta)),\\eta\\rangle-(1-\\mu)\\|\\Phi(P_{\\parallel}(\\Delta))\\|_{2}^{2}\\leq0.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof. The linear operator $\\Phi P_{\\parallel}:\\mathbb{R}^{d_{1}\\times d_{2}}\\to\\mathbb{R}^{N}$ can be decomposed as $\\begin{array}{r}{\\Phi P_{\\|}=\\sum_{n=1}^{N}\\beta_{n}w_{n}\\psi_{n}}\\end{array}$ where $\\{w_{n}\\}_{n}$ are orthonormal on $\\mathbb{R}^{T}$ , $\\{\\psi_{n}\\}_{n}$ are orthonormal linear operators on $\\mathbb{R}^{d_{1}\\times d_{2}}$ , and $\\beta_{n}\\geq0$ are decreasing. For $k=0,1,\\ldots,\\operatorname*{min}\\{\\log_{2}(N),d_{1}d_{2}\\}-1$ let $W_{k}=[w_{2^{k}},\\ldots,w_{2^{k+1}-1}]$ so that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\beta_{2^{k}}=\\operatorname*{max}_{\\|\\Delta\\|_{\\mathrm{F}}=1,\\|u\\|_{2}=1}u^{\\top}W_{k}^{\\top}\\Phi P_{\\|}(\\Delta)\\ge\\operatorname*{min}_{\\|\\Delta\\|_{\\mathrm{F}}=1,\\|u\\|_{2}=1}u^{\\top}W_{k}^{\\top}\\Phi P_{\\|}(\\Delta)\\ge\\beta_{2^{k+1}}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Then ", "text_level": 1, "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{max}_{\\substack{\\lambda\\mid P_{1}(\\Delta)\\mid P\\geq t/\\sqrt{2}}}2\\langle\\Phi(P_{1}(\\Delta)),\\eta\\rangle-(1-\\mu)\\|\\Phi(P_{1}(\\Delta))\\|_{2}^{2}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad}\\\\ &{\\quad=\\underset{\\Delta\\mid P_{1}(\\Delta)\\mid P_{1}\\geq t/\\sqrt{2}}{\\operatorname*{max}}\\underset{k=0}{\\underline{{K}}}2\\langle W_{k}W_{k}^{-}\\Phi(P_{1}(\\Delta)),\\eta\\rangle-(1-\\mu)\\|W_{k}W_{k}^{-}\\Phi(P_{1}(\\Delta))\\|_{2}^{2}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad}\\\\ &{\\quad\\quad\\quad=\\underset{\\Delta\\mid P_{1}(\\Delta)\\mid P\\geq t/\\sqrt{2}}{\\operatorname*{max}}\\underset{k=0}{\\underline{{K}}}2\\langle W_{k}^{-}\\Phi(P_{1}(\\Delta)),W_{k}^{-}\\eta\\rangle-(1-\\mu)\\|W_{k}^{\\top}\\Phi(P_{1}(\\Delta))\\|_{2}^{2}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad}\\\\ &{\\quad\\quad\\quad\\quad\\leq\\underset{k=0}{\\overset{K}{\\sum}}\\underset{k=0}{\\operatorname*{max}}\\underset{k\\in\\mathcal{V}}{\\operatorname*{max}}\\underset{\\Tilde{()}}{\\operatorname*{max}}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad2\\lVert W_{k}^{\\top}\\Phi(P_{1}(\\Delta))\\rVert_{2}\\,\\lVert W_{k}^{\\top}\\eta\\rVert_{2}-(1-\\mu)\\lVert W_{k}^{\\top}\\Phi(P_{1}(\\Delta))\\rVert_{2}^{2}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad}\\\\ &{\\quad\\quad\\leq\\underset{k=0}{\\overset{K}{\\sum}}t\\sqrt{2}\\beta_{2^{k+1}}\\,\\lVert W_{k}^{\\top}\\eta\\rVert_{2}-\\frac{t^{2}(1-\\mu)}{2}\\beta_{2^{k+1}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the first inequality holds by Cauchy-Schwartz, and the second holds for all $t\\_>$ $\\operatorname*{max}_{k=0,...,K}\\frac{\\|W_{k}^{\\top}\\eta\\|_{2}^{-}2\\sqrt{2}}{(1-\\mu)\\beta_{2^{k+1}}}$ . Moreover, $\\begin{array}{r}{t\\sqrt{2}\\beta_{2^{k+1}}\\,\\|W_{k}^{\\top}\\eta\\|_{2}-\\frac{t^{2}(1-\\mu)}{2}\\beta_{2^{k+1}}^{2}\\le-\\alpha t}\\end{array}$ if ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\geq\\operatorname*{max}_{k=0,\\ldots,K}\\frac{\\|W_{k}^{\\top}\\eta\\|_{2}2\\sqrt{2}}{(1-\\mu)\\beta_{2^{k+1}}}+\\operatorname*{max}_{k=0,\\ldots,K}\\frac{2\\alpha}{(1-\\mu)\\beta_{2^{k+1}}^{2}}=\\sqrt{\\operatorname*{max}_{k=0,\\ldots,K}\\frac{8\\|W_{k}^{\\top}\\eta\\|_{2}^{2}}{(1-\\mu)^{2}\\beta_{2^{k+1}}^{2}}}+\\operatorname*{max}_{k=0,\\ldots,K}\\frac{2\\beta_{2^{k+1}}}{(1-\\mu)\\beta_{2^{k+1}}^{2}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Note that for $K=\\operatorname*{min}\\{\\log_{2}(N),d_{1}d_{2}\\}$ we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathsf{P}(\\cup_{k=1}^{K}\\{\\|W_{k}^{\\top}\\eta\\|_{2}\\ge\\sqrt{2^{k}}+\\sqrt{2\\log(K/\\delta)}\\})\\le\\mathbf{P}(\\cup_{k=1}^{K}\\{\\|W_{k}^{\\top}\\eta\\|_{2}\\ge\\mathbb{E}[\\|W_{k}^{\\top}\\eta\\|_{2}]+\\sqrt{2\\log(K/\\delta)}\\})}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "On this good event, we have that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{k=0,\\ldots,K}{\\operatorname*{max}}\\frac{\\|W_{k}^{\\top}\\eta\\|_{2}^{2}}{\\beta_{2^{k+1}}^{2}}\\le\\underset{k=0,\\ldots,K}{\\operatorname*{max}}\\frac{(\\sqrt{2^{k}}+\\sqrt{2\\log(K/\\delta)})^{2}}{\\beta_{2^{k+1}}^{2}}}\\\\ &{\\phantom{\\sum_{k=0,\\ldots,K}}\\frac{2^{k+1}}{\\beta_{2^{k+1}}^{2}}+\\frac{4\\log(K/\\delta)}{\\beta_{2^{k+1}}^{2}}}\\\\ &{\\phantom{\\sum_{k=0,\\ldots,K}}\\le2\\underset{n=1}{\\overset{N}{\\sum}}\\frac{1}{\\beta_{n}^{2}}+\\underset{n=1,\\ldots,N}{\\operatorname*{max}}\\frac{4\\log(K/\\delta)}{\\beta_{n}^{2}}}\\\\ &{\\phantom{\\sum_{k=0,\\ldots,K}}=2\\mathrm{tr}\\Big((P_{\\parallel}\\Phi^{*}\\Phi P_{\\parallel})^{\\dagger}\\Big)+4\\|(P_{\\parallel}\\Phi^{*}\\Phi P_{\\parallel})^{\\dagger}\\|_{\\infty}\\log(K/\\delta)}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where we use the fact that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\sum_{n=1}^{N}\\frac{1}{\\beta_{n}^{2}}=\\sum_{k=0}^{K}\\sum_{n=2^{k}}^{2^{k+1}-1}\\frac{1}{\\beta_{n}^{2}}\\ge\\sum_{k=0}^{K-1}\\frac{2^{k}}{\\beta_{2^{k+1}}^{2}}\\ge\\operatorname*{max}_{k=0,\\ldots,K-1}\\frac{2^{k}}{\\beta_{2^{k+1}}^{2}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "B Additional Details on Experiments ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "B.1 Further Details on Dataset ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "The photostimulation data were collected from transgenic reporter mice Ai229, which express Crerecombinase-dependent cytosolic GCaMP6m and soma-targeted ChRmine, crossed with the Vglut1- cre mouse line. Imaging and photostimulation experiments were performed on a Bergamo (Thorlabs) microscope equipped with a 16x (0.8 NA) Nikon objective. Post-hoc motion correction and neuron segmentation were performed with the Suite2p package [82] (https://github.com/MouseLand/suite2p). ", "page_idx": 18}, {"type": "text", "text": "B.2 Further Details on Experiment of Section 3.1 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We split each of our photostimulation datasets into non-overlapping training and test datasets. All models were trained exclusively using the training dataset and were then evaluated (as shown in Figure 2) using the test dataset. To build our test datasets, we randomly chose 5 (out of the 100 total) unique photostimulation patterns and then included all 70-timestep windows about each of the 20 instances of those 5 unique photostimuli. The resulting test set amounted to ${\\sim}20\\%$ of each dataset. In Figure 2, all models were evaluated using these 70-timestep test sequences of the form $\\{y_{t},u_{t}\\}_{t=1}^{70}$ , where $y_{t}\\in R^{d}$ is the recorded neural activity and $u_{t}\\in R^{d}$ is the photostimulation delivered at time $t$ . $\\{y_{t}\\}_{t=5}^{7\\tilde{0}}$ e.valuation on a given test window, all models were provided $\\{y_{t}\\}_{t=1}^{4}$ and $\\{u_{t}\\}_{t=1}^{70}$ to predict ", "page_idx": 18}, {"type": "text", "text": "Autoregressive-k models: We fti the full-rank AR- $k$ models to training datasets via linear regression by expressing ", "page_idx": 18}, {"type": "equation", "text": "$$\ny_{t+1}=\\sum_{s=0}^{k-1}A_{s}y_{t-s}+\\sum_{s=0}^{k-1}B_{s}u_{t-s}+v\n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\nY={\\left[\\begin{array}{l}{y_{1}}\\\\ {y_{2}}\\\\ {\\vdots}\\\\ {y_{T+1}}\\end{array}\\right]}\\quad X={\\left[\\begin{array}{l l l l l l l l l l l l l}{y_{0}}&{y_{-1}}&{\\dotsc\\cdot}&{y_{1-k}}&{}&{u_{0}}&{u_{-1}}&{\\dotsc\\cdot}&{u_{1-k}}&{}&{1}\\\\ {y_{1}}&{\\ y_{0}}&{\\dotsc\\cdot}&{y_{2-k}}&{}&{u_{1}}&{u_{0}}&{\\dotsc\\cdot}&{u_{2-k}}&{}&{1}\\\\ {\\vdots}&{}&{}&{}&{\\vdots}&{}&{\\vdots}&{}&{}&{\\vdots}&{}\\\\ {y_{T+1}}&{y_{T-1}}&{\\dotsc}&{y_{T+1-k}}&{u_{T}}&{u_{T-1}}&{\\dotsc}&{u_{T+1-k}}&{}&{1}\\end{array}\\right]}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "equation", "text": "$$\nW={\\left[\\begin{array}{l}{A_{0}}\\\\ {A_{1}}\\\\ {\\vdots}\\\\ {A_{k-1}}\\\\ {B_{0}}\\\\ {B_{1}}\\\\ {\\vdots}\\\\ {B_{k-1}}\\\\ {v}\\end{array}\\right]}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and the closed-form solution is ${\\widehat{W}}=(X^{T}X)^{-1}X^{T}Y$ . For the low-rank AR- $k$ models, we fit all parameters via gradient descent using Adam [83] over 100 training epochs with a learning rate of 0.01. Gradient descent was implemented in PyTorch and ran on a single NVIDIA Tesla T4 GPU. ", "page_idx": 19}, {"type": "text", "text": "During evaluation of the AR- $k$ models, for each test window we first computed $\\widehat{y}_{k+1}$ given $\\{y_{t},u_{t}\\}_{t=1}^{k}$ using (B.1). Then for all subsequent predictions, on the right-hand side o f  (B.1) we replaced all instances of $y_{t}$ with $\\widehat{y}_{t}$ for $t>k$ . In this manner, each entire roll-out prediction of $\\{y_{t}\\}_{t=k+1}^{70}$ used all photostimulation inputs $\\{u_{t}\\}_{t=1}^{70}$ , but only the first $k$ timesteps of neural activity $\\{y_{t}\\}_{t=1}^{k}$ . All AR- $k$ models in this paper used $k=4$ . ", "page_idx": 19}, {"type": "text", "text": "Gated recurrent unit (GRU) networks: GRU networks were loosely based on the sequential variational autoencoders of [22]. Each model consisted of an encoder GRU network that encodes $k=4$ initial timesteps of recorded neural activity into a bottlenecked initial state for a decoder GRU network. The decoder then unrolls an entire predicted timeseries of recorded neural activity given (as input) all photostimulation that was delivered over that time period. Model fitting proceeded by optimizing the evidence lower bound (ELBO) with respect to all model parameters. Both encoder and decoder GRUs had 512 hidden units. We used Adam optimization with a learning rate of 0.001 over 4000 training epochs of batch size 100. Models were implemented with PyTorch, and optimized on a single NVIDIA Tesla T4 GPU. ", "page_idx": 19}, {"type": "text", "text": "Evaluation metrics: We evaluated all models using roll-out predictions on held-out test windows. We quantified performance with mean squared error between recorded and predicted neural activity for each neuron. We also performed thresholded response detection, whereby detections were defined as timesteps at which a given neuron\u2019s measured calcium fluorescence exceeded a predefined threshold. To calculate a receiver operator characteristic (ROC) curve, we enumerated a range of thresholds, normalized by the standard deviation of each neuron\u2019s empirical activity distribution, and performed threshold detection separately on the real and model-predicted neural activity traces. We then compute the overall false-positive rate and true-positive rate at each threshold level to trace out an ROC curve. We calculate area under the ROC curve (AUROC) to quantify the accuracy of each model. ", "page_idx": 19}, {"type": "text", "text": "Longer roll-out evaluations: To assess AR- $k$ models\u2019 ability to predict over longer time horizons, we implemented another train-test strategy, where the first $80\\%$ of timesteps in a recording are used for training, and the last $20\\%$ of timesteps (6736 steps) are used for testing. During the test phase, we use the same procedure described above, providing only the $k=4$ initial timesteps of neural activity and then unrolling predictions over the remainder of this long test window. We report these results in Figure 5. ", "page_idx": 19}, {"type": "image", "img_path": "nLQeE8QGGe/tmp/8e1d7c31067f95c6b1c730bb673bea9b5513c773eedca280e63e4323dd4f23ce.jpg", "img_caption": ["Figure 5: Longer roll-out evaluations. Same format as in Figure 2. ", "Table 1: Nuclear-Norm Constraint Settings for Results of Section 5.1 "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "B.3 Further Details on Experiment of Section 5.1 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "To fti the $H$ parameter in this experiment, we generate observations as described in Section 5.1. We then estimate $H$ as: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\widehat{H}\\gets\\underset{H\\in\\mathcal{K}}{\\arg\\operatorname*{min}}\\sum_{(u,z)\\in\\mathfrak{D}}\\|z-H u\\|_{\\mathrm{F}}^{2}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "for $u$ our input, and $\\textstyle z=\\sum_{t=1}^{\\tau}x_{t}$ the observed response, where here $x_{t}$ are the observations generated from playing input $u$ , and $\\tau=15$ . ", "page_idx": 20}, {"type": "text", "text": "As $\\kappa$ is defined with respect to the nuclear norm of the true parameter, which we do not assume is known, we run each method with a range of possible values for the nuclear-norm constraint, and plot the performance of each method for the constraint value that has minimum error. We state the value of the nuclear-norm constraint used for each plot below: ", "page_idx": 20}, {"type": "table", "img_path": "nLQeE8QGGe/tmp/d6483c4a3f4a6594c97d025c927999de86384de68669191dcfa97524134aacc9.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "To choose the input rank of Algorithm 1, we ran our experiment with several different ranks and provide results for the best-performing rank. We found, however, that results are typically robust to the setting of the rank parameter of Algorithm 1, and our choice of $r$ did not significantly impact performance. Furthermore, we believe this could effectively be chosen adaptively. We state our chosen values of $r$ below. ", "page_idx": 20}, {"type": "text", "text": "For all experiments, we add observation noise distributed as $\\mathcal{N}(0,0.4\\cdot I)$ to $\\textstyle z=\\sum_{t=1}^{\\tau}x_{t}$ . ", "page_idx": 20}, {"type": "table", "img_path": "nLQeE8QGGe/tmp/f3b8f9fb8c276b764fc993b1e0b1c2468a4e1b9cdb098a417dd96db3b0a086cc.jpg", "table_caption": ["Table 2: Input Rank $r$ for Results of Section 5.1 "], "table_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "nLQeE8QGGe/tmp/d4a2909202fc0d92c151ec5d379ef311900d4c5cdf79410611406be04f717e4c.jpg", "img_caption": ["Figure 6: Causal connectivity matrix for Mouse 3 FoV B with different levels of estimation error (corresponding to Figure 3). "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "To ground the estimation error values shown in Figure 3, in Figure 6 will illustrate the causal connectivity matrix for Mouse 3 FoV B with different levels of estimation error. ", "page_idx": 21}, {"type": "text", "text": "B.4 Further Details on Experiment of Section 5.2 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "For this experiment, on the data $\\mathfrak{D}$ we observed thus far, we fit the AR- $k$ model described in Section 3.1 with $k=1$ . We found that for this experiment, simply using the least squares estimator with no low-rank penalty produced the best results. We use the same estimation method for both our method and the baseline method. ", "page_idx": 21}, {"type": "text", "text": "Given a input response trajectory in the test set, $(x_{1},\\ldots,x_{15})$ , with input $u$ , to compute the test MSE, we provide our learned dynamics model with the initial state $x_{1}$ and input $u$ , and then roll this out for 15 timesteps to generate predictions $\\widehat{x}_{2},\\ldots,\\widehat{x}_{15}$ . Precisely, if $\\widehat{A}$ and $\\bar{\\widehat B}$ are our estimated parameters, we let ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\widehat{x}_{2}=\\widehat{A}x_{1}+\\widehat{B}u,}\\\\ {\\widehat{x}_{t+1}=\\widehat{A}\\widehat{x}_{t},\\quad t\\geq1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "We the compute the MSE on this segment as: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\frac{1}{14}\\sum_{t=2}^{15}\\|\\widehat{\\boldsymbol{x}}_{t}-\\boldsymbol{x}_{t}\\|_{2}^{2}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "It is not immediately obvious how to apply Algorithm 1 to this setting, since we must choose each trajectory sequentially, and once we have observed a trajectory it can no longer be chosen again. Rather than solving the optimization of Algorithm 1 to find the best inputs, we instead seek to ", "page_idx": 21}, {"type": "image", "img_path": "nLQeE8QGGe/tmp/df378b4de01fe1bea315e15c9e1f5a8a9389d6d0642cd739c81b7488b95ac9d8.jpg", "img_caption": ["(a) Neuron 0, $\\mathrm{MSE}=0.175$ (b) Neuron 0, $\\mathrm{MSE}\\,{=}\\,0.150$ (c) Neuron 0, $\\mathrm{MSE}=0.140$ (d) Neuron 0, $\\mathrm{MSE}\\!=\\!0.138$ "], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "nLQeE8QGGe/tmp/fd95e66d89b97ed4e0127bae0e00668d42f3fdf5914b8fabe4c228da96129d2f.jpg", "img_caption": ["(e) Neuron 3, $\\mathrm{MSE}=0.175$ (f) Neuron 3, MSE $=0.150$ (g) Neuron 3, $\\mathrm{MSE}=0.140$ (h) Neuron 3, MSE = 0.138 "], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "nLQeE8QGGe/tmp/96fbd626b55c1a514a5e5efb2721161f652646a32ea19668c61a425af432686c.jpg", "img_caption": ["(i) Neuron 95, MSE $=(\\mathrm{j})$ ) Neuron 95, MSE $=$ (k) Neuron 95, MSE $=$ (l) Neuron 95, MSE $=$ 0.175 0.150 0.140 0.138 "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "Figure 7: Estimated neural activity vs true neural activity on heldout trials for Mouse 2, Neurons 0, 3, and 95, at different levels of overall MSE on heldout trials (corresponding to Figure 4). ", "page_idx": 22}, {"type": "text", "text": "iteratively choose the next input that would maximize \u201cinformation gain\u201d in some sense. In particular, note that applying the Frank-Wolfe algorithm [80] to the objective, if we have inputs $\\boldsymbol{\\mathcal{U}}$ available: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\lambda\\in\\triangle_{\\mathcal{U}}}\\mathrm{tr}((V^{\\top}\\mathbf{A}(\\lambda)V)^{-1}),\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "the update is given by: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l}{u_{i+1}=\\displaystyle\\operatorname*{min}_{u\\in\\mathcal{U}}u^{\\top}V(V^{\\top}\\mathbf{A}(\\lambda_{i})V)^{-2}V^{\\top}u}\\\\ {\\lambda_{i+1}\\leftarrow(1-\\gamma_{i})\\lambda_{i}+\\gamma_{i}\\mathbb{I}\\{u=u_{i+1}\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "for learning rate $\\gamma_{i}$ . ", "page_idx": 22}, {"type": "text", "text": "In this experiment, we simply choose $u_{n}$ as above, with $\\boldsymbol{\\mathcal{U}}$ the set of remaining active inputs in $\\mathfrak{D}_{\\mathrm{train}}$ , and $\\mathbf{\\boldsymbol{\\Lambda}}(\\lambda_{n})$ replaced with $\\dot{\\sum}_{s=1}^{n-1}\\,u_{s}u_{s}^{\\top}$ . This therefore approximates the solution to the experiment design of Algorithm 1, an d has the advantage of being very computationally efficient. Furthermore, we set $V$ to be the right singular vectors of $\\widehat{B}$ . We believe this is reasonable in dynamical system settings with fast decay. ", "page_idx": 22}, {"type": "text", "text": "The primary hyperparameter for this experiment is the choice of $r$ , the rank of $V$ . As in the previous section, we did not find the results particularly sensitive to setting of $r$ . For each dataset, we ran with $r\\in[25,50,75,100,125,150]$ , and include results for the best-performing setting. ", "page_idx": 22}, {"type": "text", "text": "For both sets of experiments in Section 5, we ran on 56 Intel(R) Xeon(R) CPU E5-2690 v4 $@$ 2.60GHz CPUs. ", "page_idx": 22}, {"type": "text", "text": "To ground the MSE values shown in Figure 4, in Figure 7 we plot the predictions from the estimated model at different MSE values on heldout trials for Mouse 2. ", "page_idx": 22}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We validate all claims with theoretical results or experiments. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 23}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: Please see our discussion section. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 23}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: Proofs are given in the supplemental for stated theorems (or citations given to works where they proved). ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 24}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: To the extent possible, we provide as much information as we can on all results to ensure they are reproducible. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 24}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 25}, {"type": "text", "text": "Answer: [No] ", "page_idx": 25}, {"type": "text", "text": "Justification: We have not yet released our code but plan to in the future. We also hope to release the data we used in the future. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 25}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: To the extent possible, we state all hyperparameters. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 25}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: Where possible, we state confidence intervals in our results. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: Computational resources are listed in the supplemental. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 26}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: Our work does not raise any ethical concerns, and conforms with all ethical guidelines. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 26}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: This work studies fundamental concepts in machine learning and neuroscience, and we do not believe it has any immediate societal consequences. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 26}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 27}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: We are not releasing high-risk models or data with this work Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 27}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: We have credited all those involved with the collection of the data we utilize. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: No new assets are introduced in this work. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 28}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: No human subjects were involved in this work. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 28}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: No human subjects were involved with this work. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 28}]