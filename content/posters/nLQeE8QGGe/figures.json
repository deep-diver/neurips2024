[{"figure_path": "nLQeE8QGGe/figures/figures_2_1.jpg", "caption": "Figure 1: (a) Two-photon imaging and holographic photostimulation platform (left) and a representative image frame (right). Purple circles indicate neurons photostimulated immediately before frame acquisition. Red and blue indicate increases and decreases of firing activity, respectively, relative to before photostimulation. (b) Example time series photostimulation inputs (top) and neural responses (bottom) from 100 randomly selected neurons (out of d = 663 recorded neurons identified in the FoV). (c) Neural responses yt occupy a low-dimensional subspace. Singular values from a representative dataset's demeaned neural activity data matrix (blue) indicate substantially more data variance residing in a few dozen dimensions (out of the full d = 663 dimensional neural activity space) than is expected by chance (orange, singular values when removing low-dimensional structure by shuffling time indices independently for each neuron; note clipped horizontal axis).", "description": "This figure demonstrates the experimental setup and data. (a) shows the two-photon imaging and holographic photostimulation platform used to record neural activity and apply targeted photostimulation. (b) shows example time series data: photostimulation inputs (top) and the corresponding neural responses (bottom) from a subset of the recorded neurons. (c) shows the singular value decomposition of the neural activity data, illustrating that the data lies in a low-dimensional subspace.", "section": "3 Preliminaries"}, {"figure_path": "nLQeE8QGGe/figures/figures_3_1.jpg", "caption": "Figure 2: Example data and cross-validated model predictions. (a) Roll-out predictions of the activity of an example neuron i using low-rank AR-k models (k = 4) and GRU networks for 22 example data segments (3.3s per segment; segments separated by brief horizontal spaces). Each model\u2019s predictions are seeded with the first k = 4 timesteps (200ms) of activity from d = 663 neurons and are then unrolled to predict the activity across all d neurons over the next 66 timesteps, given the full 70-timestep sequence of photostimulation to all d neurons. Most responses of neuron i are tied to \u201cdirect\u201d photostimulation of neuron i (pink, first row of panels). Several \u201cindirect responses\u201d are tied to stimulation of other neurons j \u2260 i that influence neuron i through the population dynamics. To avoid showing all indirect stimuli (to d \u2013 1 neurons), only select indirect stimuli are shown (green, second row of panels). (b) Receiver operator characteristic (ROC) curve of true-positive rate and false-positive rate for response detection are calculated on indirect responses only (left) and all direct and indirect responses (right). (c) Area under ROC curve (AUROC) and (d) mean square error (MSE) for all predictions.", "description": "This figure demonstrates the performance of low-rank autoregressive models and GRU networks in predicting neural activity.  It shows example predictions compared to actual recordings, ROC curves assessing the models\u2019 ability to identify neural responses (both direct and indirect), and a comparison of AUROC and MSE values across models with varying ranks.", "section": "3.1 Fitting Low-Rank Dynamical Models"}, {"figure_path": "nLQeE8QGGe/figures/figures_8_1.jpg", "caption": "Figure 3: Performance of active stimulation design on estimating learned dynamics model. For each mouse dataset, we fit a low-rank AR-k model as described in Section 3.1 (for ranks of 15 and 35, and k = 4). Treating this as a simulator of the true dynamics, we compare our active stimulation design procedure (Active, Algorithm 1) to randomly choosing groups of neurons to excite (Random), and uniformly allocating stimulation across all neurons (Uniform), and plot how effectively each is able to estimate the connectivity of the simulator dynamics. For each figure and method we average over 20 trials, and plot the mean performance with error bars denoting 1 standard error (note that the error bars are barely visible as the standard deviation is very small).", "description": "This figure compares three different methods for estimating the connectivity matrix of a learned dynamical system: Active (the authors' proposed method), Random (randomly choosing neurons to stimulate), and Uniform (uniformly stimulating all neurons). The results show that the Active method outperforms the other two methods in terms of estimation accuracy. The performance is averaged over 20 trials, with error bars indicating the standard error.", "section": "5.1 Active Learning on Data-Driven Neural Population Dynamics Simulator"}, {"figure_path": "nLQeE8QGGe/figures/figures_9_1.jpg", "caption": "Figure 3: Performance of active stimulation design on estimating learned dynamics model. For each mouse dataset, we fit a low-rank AR-k model as described in Section 3.1 (for ranks of 15 and 35, and k = 4). Treating this as a simulator of the true dynamics, we compare our active stimulation design procedure (Active, Algorithm 1) to randomly choosing groups of neurons to excite (Random), and uniformly allocating stimulation across all neurons (Uniform), and plot how effectively each is able to estimate the connectivity of the simulator dynamics. For each figure and method we average over 20 trials, and plot the mean performance with error bars denoting 1 standard error (note that the error bars are barely visible as the standard deviation is very small).", "description": "This figure compares three different methods for estimating the connectivity matrix of a learned neural population dynamics model: active stimulation design, random stimulation, and uniform stimulation.  The results show that the active learning method outperforms the other two, achieving a given estimation error with significantly fewer trials.  The results are shown for four different mouse datasets, with separate plots for low-rank models of rank 15 and 35.", "section": "5.1 Active Learning on Data-Driven Neural Population Dynamics Simulator"}, {"figure_path": "nLQeE8QGGe/figures/figures_20_1.jpg", "caption": "Figure 2: Example data and cross-validated model predictions. (a) Roll-out predictions of the activity of an example neuron i using low-rank AR-k models (k = 4) and GRU networks for 22 example data segments (3.3s per segment; segments separated by brief horizontal spaces). Each model's predictions are seeded with the first k = 4 timesteps (200ms) of activity from d = 663 neurons and are then unrolled to predict the activity across all d neurons over the next 66 timesteps, given the full 70-timestep sequence of photostimulation to all d neurons. Most responses of neuron i are tied to \"direct\" photostimulation of neuron i (pink, first row of panels). Several \u201cindirect responses\u201d are tied to stimulation of other neurons j \u2260 i that influence neuron i through the population dynamics. To avoid showing all indirect stimuli (to d \u2013 1 neurons), only select indirect stimuli are shown (green, second row of panels). (b) Receiver operator characteristic (ROC) curve of true-positive rate and false-positive rate for response detection are calculated on indirect responses only (left) and all direct and indirect responses (right). (c) Area under ROC curve (AUROC) and (d) mean square error (MSE) for all predictions.", "description": "This figure demonstrates the predictive performance of low-rank autoregressive (AR) models and gated recurrent unit (GRU) networks on neural population activity data. It shows roll-out predictions of neural activity, receiver operating characteristic (ROC) curves, area under the ROC curve (AUROC) and mean squared error (MSE) for different model ranks, illustrating the effectiveness of low-rank models in capturing the dynamics of neural populations.  The figure also highlights the distinction between direct and indirect stimulation effects on neural activity.", "section": "3.1 Fitting Low-Rank Dynamical Models"}, {"figure_path": "nLQeE8QGGe/figures/figures_21_1.jpg", "caption": "Figure 2: Example data and cross-validated model predictions. (a) Roll-out predictions of the activity of an example neuron i using low-rank AR-k models (k = 4) and GRU networks for 22 example data segments (3.3s per segment; segments separated by brief horizontal spaces). Each model's predictions are seeded with the first k = 4 timesteps (200ms) of activity from d = 663 neurons and are then unrolled to predict the activity across all d neurons over the next 66 timesteps, given the full 70-timestep sequence of photostimulation to all d neurons. Most responses of neuron i are tied to \"direct\" photostimulation of neuron i (pink, first row of panels). Several \u201cindirect responses\u201d are tied to stimulation of other neurons j \u2260 i that influence neuron i through the population dynamics. To avoid showing all indirect stimuli (to d \u2013 1 neurons), only select indirect stimuli are shown (green, second row of panels). (b) Receiver operator characteristic (ROC) curve of true-positive rate and false-positive rate for response detection are calculated on indirect responses only (left) and all direct and indirect responses (right). (c) Area under ROC curve (AUROC) and (d) mean square error (MSE) for all predictions.", "description": "This figure demonstrates the predictive performance of low-rank Autoregressive (AR) models and Gated Recurrent Unit (GRU) networks on neural population activity data. It shows example roll-out predictions of neural activity, receiver operating characteristic (ROC) curves for response detection, and mean square error (MSE) values. The results suggest that low-rank AR models provide a good balance between predictive accuracy and model complexity.", "section": "3.1 Fitting Low-Rank Dynamical Models"}, {"figure_path": "nLQeE8QGGe/figures/figures_22_1.jpg", "caption": "Figure 7: Estimated neural activity vs true neural activity on heldout trials for Mouse 2, Neurons 0, 3, and 95, at different levels of overall MSE on heldout trials (corresponding to Figure 4).", "description": "This figure compares the estimated and true neural activity for three neurons (0, 3, and 95) in Mouse 2 across different levels of mean squared error (MSE) on held-out trials. The MSE values represent the model's predictive accuracy on unseen data, with lower MSE indicating better performance. Each subplot shows the comparison for a specific neuron and MSE level, illustrating how well the model's predictions match the actual recorded neural activity.", "section": "5.2 Active Ranking of Real Data Observations"}, {"figure_path": "nLQeE8QGGe/figures/figures_22_2.jpg", "caption": "Figure 7: Estimated neural activity vs true neural activity on heldout trials for Mouse 2, Neurons 0, 3, and 95, at different levels of overall MSE on heldout trials (corresponding to Figure 4).", "description": "This figure compares the estimated neural activity with the true neural activity for three different neurons (0, 3, and 95) in Mouse 2 at four different levels of mean squared error (MSE) on held-out trials. Each subplot shows the true and estimated activity over time, illustrating the model's performance at different accuracy levels.  The overall MSE values reflect how well the model predicts the neural activity on unseen data.", "section": "5.2 Active Ranking of Real Data Observations"}, {"figure_path": "nLQeE8QGGe/figures/figures_22_3.jpg", "caption": "Figure 7: Estimated neural activity vs true neural activity on heldout trials for Mouse 2, Neurons 0, 3, and 95, at different levels of overall MSE on heldout trials (corresponding to Figure 4).", "description": "This figure shows the comparison between estimated and true neural activity for three neurons (0, 3, and 95) in Mouse 2 at various levels of mean squared error (MSE). Each subplot represents a neuron and displays the true activity (blue) and the estimated activity (orange) over time. The MSE value for each subplot indicates the prediction accuracy of the model.  This visualization helps illustrate the model's ability to predict neural activity with varying degrees of accuracy based on the overall MSE of the model, providing a detailed evaluation of the model's performance.", "section": "5.2 Active Ranking of Real Data Observations"}]