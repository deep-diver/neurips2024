[{"figure_path": "b1ZNTgThgw/figures/figures_1_1.jpg", "caption": "Figure 1: (a) Test-time Prompt Tuning learns and optimizes prompts from a continuous flow of unlabelled test samples during the inference stage. (b) Most existing test-time prompt tuning methods such as TPT [7] and DiffTPT [8] tend to 'forget' historical knowledge learnt from previous test samples when the prompts are continuously updated with the test data flow. They learn effective prompts at early tuning stage, but the learnt prompts degrade gradually along the tuning process. This phenomenon becomes more apparent when the domain of test samples changes continuously. The curves are derived from 100 runs over 3 different domains [16, 17]. In each run, the order of the 3 domains as well as the samples within each domain is randomly shuffled to simulate continuously changing test domains.", "description": "This figure illustrates the concept of test-time prompt tuning (TPT).  Subfigure (a) shows the basic TPT process: prompts are learned online using unlabeled test samples, which are fed into a pre-trained vision model. The model's predictions are then used to self-supervise the prompt tuning process. Subfigure (b) shows a comparison of different prompt tuning methods.  It highlights that existing methods (TPT, DiffTPT) suffer from performance degradation, especially as the domain of test samples changes. The proposed HisTPT method is designed to address this issue of knowledge forgetting.", "section": "1 Introduction"}, {"figure_path": "b1ZNTgThgw/figures/figures_4_1.jpg", "caption": "Figure 2: Overview of the proposed HisTPT. HisTPT features three types of knowledge banks, namely, local knowledge bank, hard-sample knowledge bank, and global knowledge bank, which learn and memorize up-to-date, difficult and representative knowledge, respectively, from previous test samples (e.g., Xn-2 and Xn-1) and their learnt text tokens (e.g., tn-2 and tn-1) along the test-time prompt tuning process. For the current test sample Xn, HisTPT regularizes its prediction by retrieving the memorized knowledge via an adaptive knowledge retrieval mechanism, enabling prompt optimization for Xn with the self-supervised loss Lself.", "description": "This figure illustrates the overall architecture of the proposed HisTPT model.  It shows how the model uses three knowledge banks (local, hard-sample, and global) to store historical information from previous test samples. The adaptive knowledge retrieval mechanism uses this stored information to regularize the prediction of new test samples and optimize the prompts. The online update process is also shown, illustrating the continuous learning process of the model.", "section": "3.2 Historical Test-time Prompt Tuning"}, {"figure_path": "b1ZNTgThgw/figures/figures_7_1.jpg", "caption": "Figure 3: HisTPT with multiple optimization steps.", "description": "This figure shows the mean mIoU over 6 semantic segmentation datasets using SEEM-Tiny as the model backbone.  The x-axis represents the number of optimization steps, and the y-axis shows the mean intersection over union (mIoU) achieved. The plot demonstrates that increasing the number of optimization steps improves the model's performance, but the gains diminish after 6-8 steps.  This suggests a balance point between computational cost and performance gain exists when choosing the number of optimization steps for HisTPT.", "section": "4 Experiments"}, {"figure_path": "b1ZNTgThgw/figures/figures_17_1.jpg", "caption": "Figure 1: (a) Test-time Prompt Tuning learns and optimizes prompts from a continuous flow of unlabelled test samples during the inference stage. (b) Most existing test-time prompt tuning methods such as TPT [7] and DiffTPT [8] tend to 'forget' historical knowledge learnt from previous test samples when the prompts are continuously updated with the test data flow. They learn effective prompts at early tuning stage, but the learnt prompts degrade gradually along the tuning process. This phenomenon becomes more apparent when the domain of test samples changes continuously. The curves are derived from 100 runs over 3 different domains [16, 17]. In each run, the order of the 3 domains as well as the samples within each domain is randomly shuffled to simulate continuously changing test domains.", "description": "This figure shows the performance of existing test-time prompt tuning methods (TPT and DiffTPT) along with the proposed method (HisTPT).  The left panel (a) illustrates the general concept of test-time prompt tuning, where prompts are learned from a continuous stream of unlabeled test data.  The right panel (b) shows a performance comparison. Existing methods exhibit performance degradation when the domain of test samples changes continuously, demonstrating knowledge forgetting. HisTPT is shown to maintain better performance in this scenario.", "section": "1 Introduction"}, {"figure_path": "b1ZNTgThgw/figures/figures_17_2.jpg", "caption": "Figure 2: Overview of the proposed HisTPT. HisTPT features three types of knowledge banks, namely, local knowledge bank, hard-sample knowledge bank, and global knowledge bank, which learn and memorize up-to-date, difficult and representative knowledge, respectively, from previous test samples (e.g., Xn-2 and Xn-1) and their learnt text tokens (e.g., tn-2 and tn-1) along the test-time prompt tuning process. For the current test sample Xn, HisTPT regularizes its prediction by retrieving the memorized knowledge via an adaptive knowledge retrieval mechanism, enabling prompt optimization for Xn with the self-supervised loss Lself.", "description": "This figure illustrates the architecture of the proposed HisTPT method. It shows how three knowledge banks (local, hard-sample, and global) work together to memorize useful knowledge from previous test samples and use it to regularize the prediction of the current test sample. An adaptive knowledge retrieval mechanism is used to select the most relevant memorized knowledge for each test sample.", "section": "3.2 Historical Test-time Prompt Tuning"}, {"figure_path": "b1ZNTgThgw/figures/figures_18_1.jpg", "caption": "Figure 6: Qualitative comparison of HisTPT with the baseline model (SEEM-Tiny) [3] and TPT [7] over semantic segmentation task on Cityscapes.", "description": "This figure shows a qualitative comparison of the semantic segmentation results obtained using three different methods: SEEM-Tiny, TPT, and HisTPT.  For each of five example images, the original image is shown alongside the segmentation masks produced by each method and the ground truth segmentation.  The purpose of the figure is to visually demonstrate the improved performance of HisTPT compared to the baseline and a previous state-of-the-art method.", "section": "I Qualitative Results"}]