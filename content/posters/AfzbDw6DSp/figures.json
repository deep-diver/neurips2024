[{"figure_path": "AfzbDw6DSp/figures/figures_1_1.jpg", "caption": "Figure 1: The graph encoding scheme employed in our theoretical and empirical analysis that presents a graph reasoning task (e.g., connectivity) as a tokenized input to a standard transformer model.", "description": "This figure illustrates how a graph reasoning problem is presented to a transformer model.  The input consists of three parts: vertex tokens representing the nodes in the graph, edge tokens representing the connections between the nodes, and task tokens specifying the question to be answered. This encoding scheme is used in both the theoretical analysis and empirical evaluations presented in the paper. The example shows a graph with eight nodes and a query about the connectivity of two specific nodes.", "section": "1 Introduction"}, {"figure_path": "AfzbDw6DSp/figures/figures_3_1.jpg", "caption": "Figure 2: A summary of the theoretical hierarchy of Section 3 that visualizes which type of graph reasoning tasks can be solved in which transformer scaling regime (Depth1 (D1), LogDepth (LD), LogDepth Wide (LDW) and LogDepthPause (LDP)).", "description": "This figure presents a hierarchical representation of graph reasoning tasks based on their complexity for transformer models.  The hierarchy categorizes tasks into three levels of difficulty: Retrieval, Parallelizable, and Search.  Each level is associated with a specific transformer scaling regime defined by depth (L), embedding dimension (m), and the number of pause tokens (N').  Retrieval tasks are the simplest and can be solved by single-layer transformers. Parallelizable tasks require logarithmic depth transformers, while Search tasks demand even larger, logarithmic-depth and wide transformers.  The figure also provides example tasks for each category and their corresponding complexity.", "section": "Hardness taxonomy of transformer graph reasoning tasks"}, {"figure_path": "AfzbDw6DSp/figures/figures_3_2.jpg", "caption": "Figure 2: A summary of the theoretical hierarchy of Section 3 that visualizes which type of graph reasoning tasks can be solved in which transformer scaling regime (Depth1 (D1), LogDepth (LD), LogDepth Wide (LDW) and LogDepthPause (LDP)).", "description": "This figure presents a hierarchy that categorizes graph reasoning tasks into three difficulty levels based on their solvability using transformers with different parameter scaling regimes.  The three classes are: Retrieval tasks (easily solved with single-layer transformers), Parallelizable tasks (solved efficiently with logarithmic-depth transformers), and Search tasks (requiring significantly larger, logarithmic-depth transformers). The figure illustrates which types of tasks fall into each class and what transformer configurations are necessary to solve them.", "section": "Hardness taxonomy of transformer graph reasoning tasks"}, {"figure_path": "AfzbDw6DSp/figures/figures_7_1.jpg", "caption": "Figure 3: Accuracy of a variety of trained transformers and GNNs on the connectivity task.", "description": "This figure compares the accuracy of various trained transformer models and Graph Neural Networks (GNNs) on the graph connectivity task.  It shows how accuracy varies with the number of training examples used for each model.  The results demonstrate that transformers, particularly larger fine-tuned models, excel at this global graph reasoning task, surpassing GNNs, especially when trained on larger datasets.  GNNs, however, perform better with smaller training datasets, suggesting a difference in sample efficiency.", "section": "4 Empirical graph reasoning capabilities"}, {"figure_path": "AfzbDw6DSp/figures/figures_23_1.jpg", "caption": "Figure 1: The graph encoding scheme employed in our theoretical and empirical analysis that presents a graph reasoning task (e.g., connectivity) as a tokenized input to a standard transformer model.", "description": "The figure illustrates how a graph reasoning task is encoded as input for a standard transformer model.  A graph G is represented using vertex tokens, edge tokens, and task tokens. Vertex tokens represent the nodes in the graph, edge tokens represent the edges connecting the nodes, and task tokens specify the question to be answered (e.g., 'Are v2 and v4 connected?'). This tokenized input is then fed into a transformer model, which processes it through multiple layers of self-attention and MLPs to produce a final output (e.g., 'Yes'). This encoding scheme is used in both the theoretical and empirical analyses presented in the paper.", "section": "1 Introduction"}, {"figure_path": "AfzbDw6DSp/figures/figures_37_1.jpg", "caption": "Figure 5: The constant diameter graph construction for r = 3, A = (1, 0, 1) and B = (1,1,0). The source 1 and sink 11 are connected which is equivalent to DISJ(A, B) = 1 by construction.", "description": "This figure illustrates a constant diameter graph construction used to demonstrate the hardness of solving the graph connectivity problem by single-layer transformers.  The graph is designed such that connectivity between the source node (1) and sink node (11) directly encodes the solution to the set disjointness problem (DISJ).  The edges are partitioned into three groups: fixed edges, edges determined by Alice's input, and edges determined by Bob's input.  The connectivity problem is solvable if and only if the set disjointness problem is solvable, thus demonstrating the equivalence of the two problems under this specific graph construction.", "section": "C.2 Negative results for single-layer transformers"}, {"figure_path": "AfzbDw6DSp/figures/figures_38_1.jpg", "caption": "Figure 5: The constant diameter graph construction for r = 3, A = (1, 0, 1) and B = (1, 1, 0). The source 1 and sink 11 are connected which is equivalent to DISJ(A, B) = 1 by construction.", "description": "This figure shows a constant diameter graph construction used to encode a set disjointness problem as a graph connectivity problem. The graph consists of a source node, a sink node, and two paths connecting them, one path encoding Alice's input (A) and the other encoding Bob's input (B). An edge exists between nodes if and only if the corresponding bits in Alice's and Bob's inputs are both 1. Thus, the graph is connected if and only if Alice's and Bob's inputs have at least one bit in common (i.e., DISJ(A, B) = 1).", "section": "C.2 Negative results for single-layer transformers"}, {"figure_path": "AfzbDw6DSp/figures/figures_40_1.jpg", "caption": "Figure 7: Histogram of minimum cycle lengths for cycle check instances.", "description": "This histogram shows the distribution of minimum cycle lengths in the GraphQA cycle check dataset.  The majority of graphs (around 800 out of 1000) contain cycles of length 3.  A much smaller number of graphs have cycles of length 4, 5, or 6, and a small number of graphs have no cycles at all.", "section": "4 Empirical graph reasoning capabilities"}, {"figure_path": "AfzbDw6DSp/figures/figures_42_1.jpg", "caption": "Figure 3: Accuracy of a variety of trained transformers and GNNs on the connectivity task.", "description": "This figure shows the accuracy of various trained transformer models and Graph Neural Networks (GNNs) on the graph connectivity task.  The results are presented for two different numbers of training examples (1,000 and 100,000) in order to demonstrate the impact of training data size.  It demonstrates that transformers outperform GNNs when trained with sufficient data, particularly in solving tasks requiring the analysis of long-range dependencies within the graph. Conversely, in a low-data regime, GNNs can outperform transformers, highlighting a contrast in sample efficiency between the two model architectures.", "section": "4 Empirical graph reasoning capabilities"}]