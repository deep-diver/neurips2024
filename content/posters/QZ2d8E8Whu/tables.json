[{"figure_path": "QZ2d8E8Whu/tables/tables_5_1.jpg", "caption": "Table 1: The performance of LLMDFA in the overall detection and the three phases when using different LLMs. P, R, and F1 indicate the precision, recall, and F1 score, respectively.", "description": "This table presents the performance results of LLMDFA across different phases (extraction, summarization, validation) and overall detection for three bug types (DBZ, XSS, OSCI) using four different LLMs (gpt-3.5, gpt-4, gemini-1.0, claude-3).  It shows the precision, recall, and F1-score for each LLM and phase, providing a comprehensive view of LLMDFA's performance.", "section": "4 Evaluation"}, {"figure_path": "QZ2d8E8Whu/tables/tables_8_1.jpg", "caption": "Table 2: The statistics of TaintBench, including the total number of program lines, functions, and source-sink pairs, along with the maximal and average number per application.", "description": "This table presents a statistical overview of the TaintBench dataset, a collection of real-world Android malware applications used for evaluating dataflow analysis techniques.  It shows the total number of lines of code, functions, and source-sink pairs across all applications in the dataset.  It also provides the maximum and average values for each metric, offering insights into the size and complexity variation within the dataset.", "section": "4.1 Dataset"}, {"figure_path": "QZ2d8E8Whu/tables/tables_8_2.jpg", "caption": "Table 4: The performance of LLMDFA upon Juliet Test Suite of C/C++ version", "description": "This table presents the performance of LLMDFA (a novel LLM-powered dataflow analysis framework) on the Juliet Test Suite for C/C++ code.  It shows the precision, recall, and F1 score achieved by LLMDFA in detecting three types of bugs: Divide-by-Zero (DBZ), Absolute Path Traversal (APT), and OS Command Injection (OSCI).  The results demonstrate LLMDFA's effectiveness across different bug types in C/C++ code.", "section": "4. Evaluation"}, {"figure_path": "QZ2d8E8Whu/tables/tables_8_3.jpg", "caption": "Table 3: The performance results upon TaintBench. A1~A3 are LLMDFA with gpt-3.5, end-to-end analysis with gpt-3.5, and CodeFuseQuery, respectively.", "description": "This table presents the performance comparison results of three different dataflow analysis methods on the TaintBench dataset.  A1 represents the performance of LLMDFA using the gpt-3.5 language model, A2 shows the results of a baseline end-to-end analysis method also using gpt-3.5, and A3 displays the results of the CodeFuseQuery method. The metrics used for comparison include precision, recall, and F1 score, providing a comprehensive evaluation of the effectiveness of each method in detecting dataflow-related issues within real-world Android applications.", "section": "4.2 Performance of LLMDFA"}, {"figure_path": "QZ2d8E8Whu/tables/tables_8_4.jpg", "caption": "Table 5: The performance of LLMDFA upon the real-world benchmark SecBench.js", "description": "This table presents the performance of LLMDFA on the SecBench.js benchmark, a real-world dataset of JavaScript vulnerabilities.  The results show the precision, recall, and F1-score achieved by LLMDFA in detecting three types of vulnerabilities: command injection, tainted paths, and code injection.  The overall performance is also included.", "section": "4.5 Evaluation upon Real-world Programs"}, {"figure_path": "QZ2d8E8Whu/tables/tables_15_1.jpg", "caption": "Table 1: The performance of LLMDFA in the overall detection and the three phases when using different LLMs. P, R, and F1 indicate the precision, recall, and F1 score, respectively.", "description": "This table presents the performance of LLMDFA, a large language model-powered dataflow analysis framework, in detecting three types of bugs (DBZ, XSS, OSCI) using four different LLMs (gpt-4, gpt-3.5, gemini-1.0, claude-3).  It breaks down the performance into three phases: source/sink extraction, dataflow summarization, and path feasibility validation, showing the precision, recall, and F1-score for each phase and the overall detection. This allows for a detailed analysis of LLMDFA's performance across different LLMs and stages of the dataflow analysis process.", "section": "4 Evaluation"}, {"figure_path": "QZ2d8E8Whu/tables/tables_16_1.jpg", "caption": "Table 1: The performance of LLMDFA in the overall detection and the three phases when using different LLMs. P, R, and F1 indicate the precision, recall, and F1 score, respectively.", "description": "This table presents the performance of LLMDFA, a large language model-powered dataflow analysis framework, across different phases (source/sink extraction, dataflow summarization, path feasibility validation) and overall detection.  It shows the precision, recall, and F1 score achieved using four different LLMs (gpt-4, gpt-3.5-turbo-0125, gemini-1.0, and claude-3) for three types of bugs: Divide-by-Zero (DBZ), Cross-Site-Scripting (XSS), and OS Command Injection (OSCI).  The results demonstrate LLMDFA's effectiveness across various LLMs and bug types.", "section": "4 Evaluation"}, {"figure_path": "QZ2d8E8Whu/tables/tables_16_2.jpg", "caption": "Table 1: The performance of LLMDFA in the overall detection and the three phases when using different LLMs. P, R, and F1 indicate the precision, recall, and F1 score, respectively.", "description": "This table presents the performance of the LLMDFA model in detecting three types of bugs (DBZ, XSS, and OSCI) using four different LLMs (gpt-3.5, gpt-4, gemini-1.0, and claude-3).  The performance is broken down into three phases: source/sink extraction, dataflow summarization, and path feasibility validation.  For each phase and the overall detection, the precision, recall, and F1-score are reported for each LLM.", "section": "4 Evaluation"}, {"figure_path": "QZ2d8E8Whu/tables/tables_20_1.jpg", "caption": "Table 1: The performance of LLMDFA in the overall detection and the three phases when using different LLMs. P, R, and F1 indicate the precision, recall, and F1 score, respectively.", "description": "This table presents the performance of the LLMDFA model in detecting three types of bugs (DBZ, XSS, OSCI) using four different LLMs (gpt-4, gpt-3.5, gemini-1.0, claude-3).  It breaks down the performance into three phases: source/sink extraction, dataflow summarization, and path feasibility validation. For each phase and the overall detection, the precision, recall, and F1-score are reported for each LLM. This allows for a comparison of the model's performance across different LLMs and phases of the analysis.", "section": "4 Evaluation"}, {"figure_path": "QZ2d8E8Whu/tables/tables_20_2.jpg", "caption": "Table 1: The performance of LLMDFA in the overall detection and the three phases when using different LLMs. P, R, and F1 indicate the precision, recall, and F1 score, respectively.", "description": "This table presents the performance of LLMDFA in detecting three types of bugs (DBZ, XSS, and OSCI) using four different LLMs.  It breaks down the performance into three phases: source/sink extraction, dataflow summarization, and path feasibility validation. For each phase and the overall detection, the precision, recall, and F1-score are provided for each LLM, offering a comprehensive view of the model's accuracy and reliability in different aspects of the dataflow analysis process.", "section": "4 Evaluation"}, {"figure_path": "QZ2d8E8Whu/tables/tables_20_3.jpg", "caption": "Table 1: The performance of LLMDFA in the overall detection and the three phases when using different LLMs. P, R, and F1 indicate the precision, recall, and F1 score, respectively.", "description": "This table presents the performance of LLMDFA, a large language model-powered dataflow analysis framework, across different phases of its operation (extraction, summarization, validation) and overall bug detection.  The performance is evaluated using various LLMs (gpt-4, gpt-3.5, gemini-1.0, claude-3) for three types of bugs (DBZ, XSS, OSCI) in the Juliet Test Suite benchmark. The metrics used for evaluation are precision, recall, and F1 score.", "section": "4 Evaluation"}, {"figure_path": "QZ2d8E8Whu/tables/tables_21_1.jpg", "caption": "Table 1: The performance of LLMDFA in the overall detection and the three phases when using different LLMs. P, R, and F1 indicate the precision, recall, and F1 score, respectively.", "description": "This table presents the performance of LLMDFA (a large language model-powered dataflow analysis framework) across different phases (source/sink extraction, dataflow summarization, path feasibility validation, and overall detection) and various LLMs (gpt-4, gpt-3.5, gemini-1.0, and claude-3).  The metrics used are precision (P), recall (R), and F1 score (F1), which provide a comprehensive evaluation of the method's accuracy and effectiveness for different bug types (DBZ, XSS, OSCI).", "section": "4 Evaluation"}, {"figure_path": "QZ2d8E8Whu/tables/tables_21_2.jpg", "caption": "Table 1: The performance of LLMDFA in the overall detection and the three phases when using different LLMs. P, R, and F1 indicate the precision, recall, and F1 score, respectively.", "description": "This table presents the performance of the LLMDFA model in detecting bugs and across its three phases (source/sink extraction, dataflow summarization, and path feasibility validation). It shows precision, recall, and F1-score for each phase and overall detection.  The results are shown for four different LLMs: gpt-4, gpt-3.5-turbo-0125, gemini-1.0, and claude-3.", "section": "4 Evaluation"}]