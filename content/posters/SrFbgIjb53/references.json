{"references": [{"fullname_first_author": "Hugo Touvron", "paper_title": "Llama: Open and efficient foundation language models", "publication_date": "2023-02-13", "reason": "This paper introduces Llama, a foundational large language model that is central to the experiments conducted in the MoGU paper."}, {"fullname_first_author": "Long Ouyang", "paper_title": "Training language models to follow instructions with human feedback", "publication_date": "2022-12-01", "reason": "This paper describes RLHF, a crucial technique for aligning LLMs with human values, which is a relevant background for the safety considerations in the MoGU paper."}, {"fullname_first_author": "Andy Zou", "paper_title": "Universal and transferable adversarial attacks on aligned language models", "publication_date": "2023-07-15", "reason": "This paper presents various jailbreak attack strategies that serve as the basis for the malicious instructions used in evaluating MoGU's safety performance."}, {"fullname_first_author": "Deep Ganguli", "paper_title": "Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned", "publication_date": "2022-09-07", "reason": "This paper details the methodology of red-teaming LLMs, which is the core evaluation method used in the MoGU paper to assess its safety."}, {"fullname_first_author": "Edward J Hu", "paper_title": "LoRA: Low-rank adaptation of large language models", "publication_date": "2021-06-09", "reason": "This paper introduces LoRA, the parameter-efficient fine-tuning technique utilized in MoGU to create the usable and safe variants of the LLMs."}]}