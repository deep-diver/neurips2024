[{"type": "text", "text": "One Sample Fits All: Approximating All Probabilistic Values Simultaneously and Efficiently ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Weida Li ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "School of Computing National University of Singapore vidaslee@gmail.com ", "page_idx": 0}, {"type": "text", "text": "Yaoliang Yu   \nSchool of Computer Science   \nUniversity of Waterloo   \nVector Institute   \nyaoliang.yu@uwaterloo.ca ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The concept of probabilistic values, such as Beta Shapley values and weighted Banzhaf values, has gained recent attention in applications like feature attribution and data valuation. However, exact computation of these values is often exponentially expensive, necessitating approximation techniques. Prior research has shown that the choice of probabilistic values significantly impacts downstream performance, with no universally superior option. Consequently, one may have to approximate multiple candidates and select the best-performing one. Although there have been many efforts to develop efficient estimators, none are intended to approximate all probabilistic values both simultaneously and efficiently. In this work, we embark on the first exploration of achieving this goal. Adhering to the principle of maximum sample reuse and avoiding amplifying factors, we propose a one-sample-ftis-all framework parameterized by a sampling vector to approximate intermediate terms that can be converted to any probabilistic value. Leveraging the concept of $(\\epsilon,\\delta)$ -approximation, we theoretically identify a key formula that effectively determines the convergence rate of our framework. By optimizing the sampling vector using this formula, we obtain i) a one-for-all estimator that achieves the currently best time complexity for all probabilistic values on average, and ii) a faster generic estimator with the sampling vector optimally tuned for each probabilistic value. Particularly, our one-for-all estimator achieves the fastest convergence rate on Beta Shapley values, including the well-known Shapley value, both theoretically and empirically. Finally, we establish a connection between probabilistic values and the least square regression used in (regularized) datamodels, showing that our one-for-all estimator can solve a family of datamodels simultaneously. Our code is available at https://github.com/watml/one-for-all. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The problem of attribution is central in many aspects of machine learning (Rozemberczki et al. 2022). Examples include data valuation (Ghorbani and Zou 2019), feature attribution (Lundberg and Lee 2017), multi-agent reinforcement learning (Wang et al. 2022), data attribution (Ilyas et al. 2022), and the list goes on. One popular methodology is to leverage the concept of probabilistic values, which is uniquely characterized by the axioms of linearity, null, monotonicity and symmetry in cooperative game theory (Weber 1988). Recent studies demonstrate that downstream performance employing this concept relies on the choice of probabilistic values, and the best one varies (Kwon and Zou 2022b; Li and $\\mathrm{Yu}\\,2023$ ). Therefore, practitioners may resort to approximating multiple candidates of probabilistic values and then select the best-performing one (Kwon and Zou 2022b). ", "page_idx": 0}, {"type": "text", "text": "In general, probabilistic values can only be approximated as they require exponentially many utility evaluations to compute exactly. Thus far, there has been a line of works devoted to designing efficient estimators for the Shapley value (e.g., Covert and Lee 2021; Jia et al. 2019; Kolpaczki et al. 2024; Zhang et al. 2023), while Li and Yu (2023) and Wang and Jia (2023b) proposed efficient estimators specific to weighted Banzhaf values. Despite recent progress in research on generic estimators for approximating any probabilistic value (Li and Yu 2024; Lin et al. 2022), none of them can approximate all probabilistic values simultaneously and efficiently. Overall, there is a strong demand for efficient one-for-all estimators, the possibilities of which will be explored in this work. ", "page_idx": 1}, {"type": "text", "text": "To sum up, we propose a One-sample-Fits-All (OFA) framework parameterized by a sampling vector to approximate intermediate terms that can be converted to any probabilistic value. Particularly, our framework i) adheres to the principle of maximum sample reuse and ii) does not include amplifying factors in the conversion. These two properties are considered indispensable as we observe that i) the empirical fastest estimators designed for the Shapley value or weighted Banzhaf values all follow she principle of maximum sample reuse and ii) amplifying factors could deteriorate the convergence rates of estimators. Then, using the concept of $(\\epsilon,\\delta)$ -approximation, i.e., $P(\\|\\hat{\\phi}-\\phi\\|_{2}\\ge\\epsilon\\bar{\\epsilon})\\leq\\delta$ where $\\phi$ refers to some probabilistic value and $\\hat{\\phi}$ is its estimate, we theoretically identify a formula from our framework that effectively determines the corresponding convergence rate, through which the sampling vector can be optimized. Specifically, we deduce i) an efficient one-for-all estimator (OFA-A) while optimizing the formula for all probabilistic values on Average and ii) a faster generic estimator (OFA-S) while the optimization is done for each Specific probabilistic value. The results of our convergence analysis are summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "(i) Our OFA-A achieves the convergence rate $O(n\\log n)$ for all probabilistic values on average. Notably, $O(n\\log n)$ is the currently-known best time complexity for some probabilistic values.   \n(ii) For Beta Shapley values parameterized by $\\alpha,\\beta\\,\\geq\\,1$ (Kwon and Zou 2022a), our OFA-A estimator requires $O(n\\log n)$ utility evaluations to achieve an $(\\epsilon,\\delta)$ -approximation simultaneously. Note that $\\alpha=\\beta=1$ corresponds to the commonly-used Shapley value (Shapley 1953). For the Shapley value, the previous best convergence rate is $O(n\\bar{(}\\log n)^{2})$ ,1 achieved by the group testing estimator (Wang and Jia $2023\\mathrm{a}$ , Theorem 6); however, we note that in our experiments the previous best-performing estimator is the complement estimator (Zhang et al. 2023), whose convergence rate is unknown. For Beta Shapley values with $(\\alpha=1,\\beta>1)$ ) or $(\\alpha>1,\\beta=1)$ ), the previous best estimator requires $O(n(\\log n)^{3})$ ) utility evaluations instead (Li and $\\mathrm{Yu}\\,2024$ , Proposition 4 and Remark 3). ", "page_idx": 1}, {"type": "text", "text": "(iii) For weighted Banzhaf values parameterized by $0<w<1$ , the time complexity of our OFA-A is $O(n^{{\\frac{3}{2}}}\\log{n})$ , not rivaling the previous best convergence rate $O(n\\log n)$ achieved by the estimator exclusive to weighted Banzhaf values (Li and $\\mathrm{Yu}\\,2023$ , Proposition 2). Nevertheless, our OFA-S achieves the convergence rate of $O(n\\log n)$ for both Beta Shapley values (with $\\alpha,\\beta\\geq1;$ ) and weighted Banzhaf values. ", "page_idx": 1}, {"type": "text", "text": "In our experiments, the empirical convergence rates align well with the theoretical ones derived using the concept of $(\\epsilon,\\delta)$ -approximation. Additionally, we establish a connection between probabilistic values and the least square regressions employed in datamodels (Ilyas et al. 2022), demonstrating that our OFA-A estimator can solve a family of datamodels simultaneously if it is the distances between feature coordinates that matter. This condition is met while using datamodels to detect similar training examples to a given target. Furthermore, we also identify a group of regularized datamodels that our OFA-A estimator can solve simultaneously without this condition. ", "page_idx": 1}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Let $n$ be the number of players and $[n]:=\\{1,2,...,n\\}$ be the set of all players. In data valuation (feature attribution, respectively), $n$ refers to the number of training data (features, respectively). For simplicity, we write $S\\backslash i$ and $S\\cup i$ instead of $S\\backslash\\{i\\}$ and $S\\cup\\{i\\}$ , respectively. Meanwhile, (lowercase) $s$ denotes the cardinality of the set (uppercase) $S$ . Then, each probabilistic value can be written as ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\phi_{i}=\\phi_{i}(U)=\\sum_{S\\subseteq[n]\\setminus i}p_{s+1}[U(S\\cup i)-U(S)]\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $U\\ :\\ 2^{[n]}\\ \\ \\to\\ \\mathbb{R}$ is a utility function and $\\textbf{p}\\in\\mathbb{R}^{n}$ is a non-negative vector such that $\\begin{array}{r}{\\sum_{s=1}^{n}{\\binom{n-1}{s-1}}p_{s}\\;=\\;1}\\end{array}$ . Take data valuation as an example, $U(S)$ may measure the performance of models trained on $S\\subseteq[n]$ , with which $\\phi_{i}(U)$ can be interpreted as the contribution of the $i$ -th data point to the performance of models trained on $[n]$ . ", "page_idx": 2}, {"type": "text", "text": "If there exists a (Borel) probability measure $\\mu$ on the closed interval [0, 1] such that $\\begin{array}{r}{p_{s}=\\int_{0}^{1}w^{s-1}(1-}\\end{array}$ $w)^{n-s}\\mathrm{d}\\mu(w)$ , then the resulting probabilistic value is referred to as a semi-value (Dubey et al. 1981). If $\\mu$ represents a Dirac delta distribution $\\delta_{a}$ , the corresponding probabilistic value is referred to as the weighted Banzhaf value parameterized by $a$ , or WB- $a$ . For Beta Shapley values, denoted by Beta $(\\alpha,{\\bar{\\beta}})$ , $\\begin{array}{r}{\\mu(A)=\\int_{A}w^{\\beta-\\hat{1}}(1-w)^{\\alpha-1}\\mathrm{d}w}\\end{array}$ . In practice, the considered range of $\\alpha$ or $\\beta$ is $[1,\\infty)$ (Kwon and Zou 2022a,b). Particularly, Beta $(1,1)$ , whose $\\mu$ is the uniform distribution (over $[0,1].$ ), corresponds to the Shapley value. ", "page_idx": 2}, {"type": "text", "text": "We will use the standard notion of $(\\epsilon,\\delta)$ -approximation to analyze a (randomized) estimate $\\hat{\\phi}$ of some probabilistic value $\\phi$ . ", "page_idx": 2}, {"type": "text", "text": "Definition 1. We say a (randomized) estimate $\\hat{\\phi}$ achieves an $(\\epsilon,\\delta)$ -approximation of $\\phi$ if $P(\\|\\hat{\\phi}-$ $\\phi\\lVert_{2}\\geq\\epsilon\\right)\\leq\\delta$ . ", "page_idx": 2}, {"type": "text", "text": "For instance, Wang and Jia (2023b, Theorem 4.9) proved that their proposed estimator requires $\\begin{array}{r}{O\\big(\\frac{n}{\\epsilon^{2}}\\log\\frac{n}{\\delta}\\big)}\\end{array}$ utility evaluations to achieve an $(\\epsilon,\\delta)$ -approximation for WB-0.5, provided that $\\|U\\|_{\\infty}\\,\\leq^{\\prime}1$ . When $\\epsilon$ and $\\delta$ are considered fixed constants, we then simply say the estimator converges at $O(n\\log n)$ rate. ", "page_idx": 2}, {"type": "text", "text": "3 Motivations ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "One-For-All Estimators In this paper, an estimator is referred to as one-for-all if it is capable of sampling subsets Once to approximate All probabilistic values. ", "page_idx": 2}, {"type": "text", "text": "Though existing estimators are not designed to approximate all probabilistic values simultaneously, some of them can be easily modified for this end by using the weighted sampling technique. Take the sampling lift (SL) estimator (Moehle et al. 2022) as an example, its approximation is based on ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\phi_{i}=\\mathbb{E}_{S\\subseteq[n]\\setminus i}[U(S\\cup i)-U(S)]\\;\\mathrm{where}\\;P(S)=p_{s+1}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "If we fix the probability of sampling $S$ to be the one, denoted by $\\mathbf{q}\\in\\mathbb{R}^{n}$ , for the Shapley value, there is ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\phi_{i}=\\mathbb{E}_{S\\subseteq[n]\\setminus i}^{\\mathrm{Shap}}\\left[{\\frac{p_{s+1}}{q_{s+1}}}(U(S\\cup i)-U(S))\\right],\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "which is the weighted sampling lift (WSL) estimator employed by Kwon and Zou (2022a). Therefore, we can store the accumulated results $\\{U(S\\cup i)-U(S{\\bar{)}}\\}$ separately for each subset size of $S$ so that they can be reweighted to be any probabilistic value. ", "page_idx": 2}, {"type": "text", "text": "The Effect of Amplifying Factors However, the scalars $\\{\\frac{p_{s+1}}{q_{s+1}}\\}$ potentially introduce a nonnegligible factor into the theoretical convergence rate. To demonstrate, we take the WSL estimator as an example. In this case, $\\begin{array}{r}{\\hat{\\phi}_{i}=\\frac{1}{T}\\sum_{t=1}^{T}\\bar{X}_{t}}\\end{array}$ where $\\{X_{t}\\}_{t=1}^{T}$ are i.i.d. random variable such that $\\begin{array}{r}{P(X_{t}=\\frac{p_{s+1}}{q_{s+1}}(U(S\\cup i)-U(S)))^{.}=q_{s+1}}\\end{array}$ and thus $\\mathbb{E}[X_{t}]=\\phi_{i}$ . Assume that $\\|U\\|_{\\infty}\\leq1$ , by the Hoeffding\u2019s inequality, $\\begin{array}{r}{P(|\\hat{\\phi}_{i}-\\phi_{i}|\\geq\\epsilon)\\leq2\\exp\\left(-\\frac{T\\epsilon^{2}}{8C^{2}}\\right)}\\end{array}$ where C = max1\u2264k\u2264npqkk . By solving $\\begin{array}{r}{2\\exp\\left(-\\frac{T\\epsilon^{2}}{8C^{2}}\\right)\\le\\delta}\\end{array}$ , we eventually obtain $\\begin{array}{r}{T\\ge\\frac{8C^{2}}{\\epsilon^{2}}\\log\\frac{2}{\\delta}}\\end{array}$ and therefore the convergence rate of $\\hat{\\phi}_{i}$ is $\\begin{array}{r}{O(\\frac{C^{2}}{\\epsilon^{2}}\\log\\frac{2}{\\delta})}\\end{array}$ . Consequently, if $C\\to\\infty$ as $n\\to\\infty$ , this theoretical convergence rate deteriorates asymptotically. For the Banzhaf value, $\\begin{array}{r}{p_{k}\\,=\\,\\frac{1}{2^{n-1}}}\\end{array}$ ; s\u221aince $\\begin{array}{r}{q_{k}\\,=\\,\\frac{(k-1)!(n-k)!}{n!}}\\end{array}$ (k\u22121)!(n\u2212k)!, if k = n $\\textstyle k\\,=\\,{\\frac{n+1}{2}}$ , there is $\\begin{array}{r}{\\frac{p_{k}}{q_{k}}\\in\\Theta(n^{\\frac{1}{2}})}\\end{array}$ by the Stirling\u2019s approximation $d!\\simeq\\sqrt{d}\\left(\\frac{d}{e}\\right)^{d}$ . Therefore, $C^{2}$ introduces a factor of $\\Theta(n)$ into the theoretical convergence rate, though the derived formula may not be tight. If we switch the roles of $\\mathbf{p}$ and $\\mathbf{q}$ , the introduced factor $C^{2}$ is as worst as $\\Theta(2^{2n})$ . To generalize this idea, a formula is said to contain an amplifying factor if it involves $\\gamma\\cdot U(S)$ such that $\\gamma\\to\\infty$ as $n\\to\\infty$ . ", "page_idx": 2}, {"type": "text", "text": "Regarding this, we notice that Kwon and Zou (2022b) resort to a one-for-all estimator based on ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\phi_{i}=\\sum_{s=1}^{n}m_{s}\\cdot\\mathbb{E}_{R\\subseteq[n]\\setminus i}[U(R\\cup i)-U(R)]\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\begin{array}{r}{m_{s}={\\binom{n-1}{s-1}}p_{s}}\\end{array}$ and each expectation is taken over the corresponding uniform distribution. We refer to this estimator as weightedSHAP in this work. As can be verified, Eq. (2) does not contain any amplifying factors, i.e., each $m_{s}$ does not grow as $n\\to\\infty$ . ", "page_idx": 3}, {"type": "text", "text": "The Principle of Maximum Sample Reuse However, estimators designed according to Eqs. (1) and (2) are not expected to be efficient as it does not obey the principle of maximum sample reuse. Precisely, an estimator adheres to the principle of maximum sample reuse if each sampled subset is used to update all estimates $\\{\\hat{\\phi}_{i}\\}_{i\\in[n]}$ . As analyzed by Zhang et al. (2023, Section 4.2), estimators based on sampled marginal contributions $\\{U(S\\cup i)-U(S)\\}$ are impossible to meet the principle of maximum sample reuse. By contrast, we observe that the SHAP-IQ estimator proposed by Fumagalli et al. (2024) can also be adopted for this end, which employs the formula ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\phi_{i}=p_{n}(U([n])-U(\\emptyset))+2H\\mathbb{E}_{\\emptyset\\subseteq S\\subseteq[n]}[((n-s)m_{s}\\mathbb{1}_{i\\in S}-s m_{s+1}\\mathbb{1}_{i\\not\\in S})(U(S)-U(\\emptyset))]\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $m_{s}={\\binom{n-1}{s-1}}p_{s}$ , $\\begin{array}{r}{H=\\sum_{j=1}^{n-1}\\frac{1}{j}}\\end{array}$ , and $P(S)\\propto\\left(_{s-1}^{n-2}\\right)^{-1}$ . In particular, SHAP-IQ is equal to the unbiased KernelSHAP estimator (Covert and Lee 2021) for the Shapley value; see (Fumagalli et al. 2024, Theorem 4.5). Although SHAP-IQ follows the principle of maximum sample reuse, it is apparent that Eq. (3) contains an amplifying factor $H\\in\\Theta(\\log n)$ . Meanwhile, there is another line of research in quest of efficient estimators for the Shapley value by reducing the variance via the stratified sampling technique (Burgess and Chapman 2021; Castro et al. 2017; Maleki et al. 2013; Wu et al. 2023). However, such a technique also does not verify the principle of maximum sample reuse. ", "page_idx": 3}, {"type": "text", "text": "Empirical Evidence For convenience, we formally define the two aforementioned desirable properties for estimators to possess as P1: The underlying formula contains no amplifying factors and P2: Each sampled subset is used to update all the estimates $\\{\\hat{\\phi}_{i}\\}_{i=1}^{n}$ . In Figure 1, we provide some experiment results while setting $n=24$ to support our aforementioned informal analysis. Precisely, we implement six one-for-all estimators by combining the weighted sampling technique and the previous estimators. Some of our observations are: ", "page_idx": 3}, {"type": "text", "text": "(i) On WB-0.5, weightedSHAP, which satisfies P1 but not P2, is empirically not comparable to MSR-Banzhaf that possesses both P1 and P2. This observation supports the necessity of $\\mathbf{P}2$ .   \n(ii) On WB-0.5, SHAP-IQ sticks to P2 but not P1. It is clear that SHAP-IQ also performs significantly worse than MSR-Banzhaf, which highlights the role of $\\mathbf{P1}$ .   \n(iii) The sudden rises of relative differences stem from the existence of significantly large amplifying factors. For WSL-Banzhaf on Beta $(1,1)$ , the scalar for $U(i)-U(\\emptyset)$ is as large as $\\frac{2^{n}}{n}$ ! ", "page_idx": 3}, {"type": "text", "text": "In Table 1, we summarize the previous estimators in terms of $\\mathbf{P1}$ and $\\mathbf{P}2$ , and defer the technical details to Appendix D. Notably, the complement estimator is empirically the best for the Shapley value, while it is MSR for weighted Banzhaf values; both of them follow $\\mathbf{P1}$ and P2. ", "page_idx": 3}, {"type": "text", "text": "Table 1: A scope of \u201call\u201d indicates that the estimator can approximate any probabilistic value, whereas \u201cweighted Banzhaf\u201d suggests that the estimator can only approximate weighted Banzhaf values. P1 refers to the property that the underlying formula does not contain any amplifying factors for all probabilistic values in its scope, while P2 means whether each sampled subset is used to update all the estimates $\\{\\hat{\\phi}_{i}\\}_{i=1}^{n}$ . For AME, the range of $\\gamma$ in $\\gamma\\cdot U(S)$ could be $(0,\\infty)$ , independent of $n$ Originally, AME only applies to a subfamily of semi-values, but we extend it for all semi-values in Appendix D. ", "page_idx": 3}, {"type": "table", "img_path": "AUg9D2VjcF/tmp/76fd326a265fe6166d26776138856a4d89b37ad65f5d8b558b7fe6886a11214d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 3}, {"type": "image", "img_path": "AUg9D2VjcF/tmp/ed154569f9d6d962a43b31c87ec3d7d65094a33a05e1f569052d9ae5f177e9ce.jpg", "img_caption": ["Figure 1: Comparison of ten one-for-all estimators. Beta $(\\alpha,\\beta)$ denotes Beta Shapley values, whereas WB- $a$ refers to weighted Banzhaf values. Our OFA-S estimator is equal to the OFA-A estimator for the Shapley value. The suffix \u201cShapley\u201d indicates that there is no reweighting for the Shapley value, while \u201cBanzhaf\u201d stands for the Banzhaf value. The permutation estimator is originally proposed for the Shapley value. The utility function $U$ is the cross-entropy loss of LeNet trained on 24 data from FMNIST. All the results are averaged using 30 random seeds. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "4 Main Results ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The framework we propose is built upon ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\phi_{i}=\\sum_{s=1}^{n}m_{s}\\cdot\\left(\\underset{r=s}{\\mathbb{E}}[U(R)]-\\underset{r=s-1}{\\mathbb{E}}[U(R)]\\right)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\begin{array}{r}{m_{s}={\\binom{n-1}{s-1}}p_{s}}\\end{array}$ and each expectation is taken over the corresponding uniform distribution. For simplicity, we write $\\phi_{i,s}^{+}=\\mathbb{E}_{i\\in R,r=s}[U(R)]$ and $\\phi_{i,s-1}^{-}=\\mathbb{E}_{i\\notin R,r=s-1}[U(R)]$ . Clearly, there is no amplifying factors in Eq. (4). Meanwhile, the structure of Eq. (4) suits the principle of maximum sample reuse. Since $\\{\\phi_{i,k}^{\\mp}\\}_{k=1,n-1,n}$ and $\\{\\phi_{i,k}^{-}\\}_{k=0,1,n-1}$ can be calculated exactly using $2n+2$ utility evaluations of $U$ , our focus is to efficiently approximate $\\{\\phi_{i,s}^{+},\\phi_{i,s}^{-}\\}_{2\\leq s\\leq n-2}$ . The proposed framework is presented in Algorithm 1; $q_{j}$ is the probability of drawing a subset of $[n]$ with size $j+1$ . To facilitate the choice of the sampling vector $\\mathbf{q}\\in\\mathbb{R}^{n-3}$ appearing in Algorithm 1, our first step is to theoretically ascertain a key formula that effectively determines the convergence rate of Algorithm 1. ", "page_idx": 4}, {"type": "text", "text": "Theorem 1. Assume $i$ ) $\\|U\\|_{\\infty}\\leq u$ and ii) $0<\\epsilon\\leq\\sqrt{2D(\\mathbf{m},\\mathbf{q})\\gamma(\\mathbf{q})^{2}u^{2}}$ . For $\\hat{\\phi}$ in Algorithm $^{l}$ , it requires $\\begin{array}{r}{\\frac{4n u^{2}D(\\mathbf{m,q})}{\\epsilon^{2}}\\log\\frac{8n^{2}}{\\delta}}\\end{array}$ evaluations of $U$ to achieve $P(\\|\\hat{\\phi}-\\phi\\|_{2}\\ge\\epsilon)\\leq\\delta$ where ", "page_idx": 4}, {"type": "equation", "text": "$$\nD(\\mathbf{m},\\mathbf{q})=\\sum_{s=2}^{n-2}{\\frac{n}{q_{s-1}}}\\left({\\frac{m_{s}^{2}}{s}}+{\\frac{m_{s+1}^{2}}{n-s}}\\right)\\,a n d\\,\\gamma(\\mathbf{q})=\\operatorname*{min}_{2\\leq s\\leq n-2}\\operatorname*{min}\\left({\\frac{q_{s-1}\\cdot s}{n}},\\,{\\frac{q_{s-1}\\cdot(n-s)}{n}}\\right).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We remark that $D(\\mathbf{m},\\mathbf{q})$ is jointly convex in $\\mathbf{m}$ and $\\mathbf{q}$ . The second assumption in Theorem 1 can be removed if we pre-allocate the number of sampled subsets for each $\\phi_{i,s}^{+}$ or $\\phi_{i,s}^{-}$ and draw subsets in a predetermined order; see the proof in Appendix A for details. Precisely, let $T_{i,s}^{+}$ be the number of subsets for estimating $\\phi_{i,s}^{+}$ , and define $T_{i,s}^{-}$ similarly; then the pre-allocated numbers are $\\begin{array}{r}{T_{i,s}^{+}\\approx\\frac{s\\cdot q_{s-1}}{n}T}\\end{array}$ and $\\begin{array}{r}{T_{i,s}^{-}\\approx\\frac{(n-s)q_{s-1}}{n}T}\\end{array}$ , which are the expected values of $T_{i,s}^{+}$ and $T_{i,s}^{+}$ while using Algorithm 1; $T$ refers to the total number of sampled subsets. By Theorem 1, the convergence rate of Algorithm 1 is $O(D(\\mathbf{m},\\mathbf{q})\\cdot n\\log n)$ , and thus achieving the currently best convergence rate $O(n\\log{n})$ requires $D(\\mathbf{m},\\mathbf{q})\\in O(1)$ . ", "page_idx": 4}, {"type": "text", "text": "$\\mathbf{q}\\in\\mathbb{R}^{n-3}$ ", "page_idx": 5}, {"type": "image", "img_path": "AUg9D2VjcF/tmp/28298bef167216de50c62c36c0496be6b93433111b5940a21078e4ed675a7f00.jpg", "img_caption": [], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "4.1 A One-For-All Estimator ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "To obtain our one-for-all estimator, our goal is to find a $\\mathbf{q}^{\\mathrm{OFA-A}}$ such that $D(\\mathbf{m},\\mathbf{q}^{\\mathrm{OFA-A}})\\in O(1)$ for as many $\\mathbf{m}$ as possible. To this end, we define $\\mathbf{q}^{\\mathrm{OFA-A}}$ to be the uniquely optimal solution to ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\underset{\\mathbf{q}\\in\\mathbb{R}^{n-3}}{\\mathrm{argmin}}\\,\\overline{{D}}(\\mathbf{q})=\\int_{\\mathbf{m}\\in\\Delta}D(\\mathbf{m},\\mathbf{q})\\mathrm{d}\\nu(\\mathbf{m})\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\Delta=\\left\\{\\mathbf{m}\\in\\mathbb{R}^{n}\\mid m_{s}\\geq0\\right.$ and $\\textstyle\\sum_{s=1}^{n}m_{s}=1\\}$ and $\\nu$ is the uniform distribution on $\\Delta$ . In our work, our OFA-A estimator refers to  the use of $\\mathbf{q}^{\\mathrm{OFA-A}}$ in Algorithm 1. ", "page_idx": 5}, {"type": "text", "text": "Proposition 1. $\\begin{array}{r}{\\mathbf{q}_{s-1}^{O F\\!A-\\!A}\\propto\\frac{1}{\\sqrt{(s)(n-s)}}}\\end{array}$ and $\\overline{{D}}(\\mathbf{q}^{O F A-A})\\in O(1)$ . In other words, our OFA-A estimator achieves the convergence rate of $O(n\\log n)$ simultaneously for all probabilistic values on average. ", "page_idx": 5}, {"type": "text", "text": "Our next proposition provides a condition on $\\mu$ for semi-values such that our OFA-A estimator achieves the convergence rate of $O(n\\log n)$ . In other words, we explicitly identify a subfamily of semi-values for which our OFA-A estimator achieves the currently best time complexity simultaneously. ", "page_idx": 5}, {"type": "text", "text": "Proposition 2. Our OFA-A estimator achieves the convergence rate of $O(n\\log n)$ simultaneously for all semi-values whose probability density functions exist and are bounded. Particularly, Beta Shapley values with $\\alpha,\\beta\\geq1$ all satisfy this condition. ", "page_idx": 5}, {"type": "text", "text": "To our knowledge, the previous theoretically-fastest estimator for the Shapley value is demonstrated by Wang and Jia (2023a, Theorem 6) as $O(\\dot{n(\\log n)^{2}})$ . By contrast, our OFA-A estimator achieves the convergence rate of $O(n\\log n)$ . Meanwhile, it also surpasses the previous best time complexity for Beta Shapley values with $(\\alpha=1,\\beta>1)$ or $(\\alpha>1,\\beta=1)$ , which is $O(n(\\log n)^{3})$ (Li and $\\mathrm{Yu}\\,2024$ , Proposition 4 and Remark 3). Remarkably, our OFA-A estimator enjoys this fastest convergence rate simultaneously for a broad subfamily of probabilistic values . ", "page_idx": 5}, {"type": "text", "text": "Proposition 3. If $'p_{s}=a^{s-1}(1-a)^{n-s}$ with $0<a<1$ , which corresponds to the weighted Banzhaf value parameterized by $w$ , then $D(\\mathbf{m},\\mathbf{q}^{O F\\!A-\\!A})\\in O(n^{\\frac{1}{2}})$ . In other words, the OFA estimator achieves the convergence rate of $O(n^{{\\frac{3}{2}}}\\log{n})$ simultaneously for all WB- $^{a}$ with $0<a<1$ . ", "page_idx": 5}, {"type": "text", "text": "The previous best convergence rate for weighted Banzhaf values is $O(n\\log n)$ (Li and $\\mathrm{Yu}~2023$ , Proposition 2), ours is slower by a factor of $n^{{\\frac{1}{2}}}$ . Nevertheless, we will demonstrate that our generic estimator, which is expected to be faster than our OFA-A estimator, achieves the best convergence rate for all weighted Banzhaf values. ", "page_idx": 5}, {"type": "text", "text": "4.2 A Faster Generic Estimator ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Our faster generic estimator (OFA-S) is obtained via optimizing $\\mathbf{q}$ for each Specific m. Precisely, for each $\\mathbf{m}$ , we have ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathsf{q}_{s-1}^{\\mathrm{OFA\\!\\cdot\\!S}}\\propto\\sqrt{\\frac{m_{s}^{2}}{s}+\\frac{m_{s+1}^{2}}{n-s}}\\:\\mathrm{~where~}\\:\\mathsf{q}^{\\mathrm{oFA\\!\\cdot\\!S}}=\\underset{\\mathsf{q}\\in\\mathbb{R}^{n-3}}{\\operatorname{argmin}}\\:D(\\mathbf{m},\\mathbf{q})\\:\\mathrm{~s.t.~}\\:\\sum_{j=1}^{n-3}q_{j}=1,\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "which can be obtained using the Cauchy-Schwarz inequality. For the Shapley value, ${\\bf q}^{\\mathrm{OFA-S}}={\\bf q}^{\\mathrm{OFA-A}}$ .   \nOur next proposition specifies a sufficient condition for semi-values such that $D(\\mathbf{m},\\mathbf{q}^{\\mathrm{OFA-S}})\\in O(1)$ . ", "page_idx": 6}, {"type": "text", "text": "Proposition 4. For semi-values, $D(\\mathbf{m},\\mathbf{q}^{O F\\!A-S})\\in O(1)\\,\\,i f\\,i)$ $\\mu$ has a bounded probability density function or ii) (0,1)w(11\u2212w) $\\begin{array}{r}{\\int_{(0,1)}\\frac{1}{w(1-w)}\\mathrm{d}\\mu(w)<\\infty}\\end{array}$ . Particularly, this condition covers all weighted Banzhaf values and Beta Shapley values with $\\alpha,\\beta\\geq1$ . ", "page_idx": 6}, {"type": "text", "text": "All in all, we demonstrate that by sticking to the principle of maximum sample reuse and avoiding any amplifying factors, we are able to establish a generic estimator that achieves the currently best convergence rate for any previously-studied semi-value. ", "page_idx": 6}, {"type": "text", "text": "4.3 A Connection between Probabilistic Values and Datamodels ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "A datamodel, proposed by Ilyas et al. (2022), is to learn an easy-to-interpret surrogate to represent a model output distribution related to a specific test example. In this circumstance, the set of players $[n]$ is identified with all the available training data. Precisely, the feature coordinates $\\pmb{\\theta}^{*}\\in\\mathbb{R}^{n}$ imputed to every data point in $[n]$ is defined to be the uniquely optimal solution (together with a bias $b^{*}\\in\\mathbb{R}$ ) to the optimization problem ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\operatorname{argmin}_{\\theta\\in\\mathbb{R}^{n},b\\in\\mathbb{R}}\\sum_{S\\subseteq[n]}\\eta_{s+1}\\left(U(S)-b-\\sum_{i\\in S}\\theta_{i}\\right)^{2}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\pmb{\\eta}\\in\\mathbb{R}^{n+1}$ is non-negative and $\\textstyle\\sum_{s=0}^{n}\\eta_{s+1}>0$ . The weight vector $\\eta$ can be scaled such that the objective in the problem (6) can be treated as an expectation, and thus the objective can be approximated through sampling a sufficient number of subsets, upon which an estimate of $(\\pmb\\theta^{*},b^{*})$ can be obtained. We show below that $\\theta^{*}$ to a family of such least square regressions can be cast as some probabilistic values if it is the pairwise differences $\\theta_{j}^{\\ast}-\\theta_{k}^{\\ast}$ (for every $\\bar{j},k\\in[n])$ that matter. ", "page_idx": 6}, {"type": "text", "text": "Theorem 2. Let $(b^{*},\\theta^{*})$ be the uniquely optimal solution to the problem (6) where $\\eta_{s}=p_{s-1}+p_{s}$ for $2\\leq s\\leq n$ . Then, there is ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\theta_{j}^{*}-\\theta_{k}^{*}=\\phi_{j}-\\phi_{k}\\ f o r\\,e\\nu e r y\\ j,k\\in[n].\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "In other words, $\\theta^{*}\\,=\\,\\phi\\,+\\,c$ for some constant $^{c1}$ ; $\\textbf{1}\\in\\mathbb{R}^{n}$ is the all-one vector. When using datamodels to detect similar training examples to a given target, what matters is the relative order of components in $\\theta^{*}$ . Meanwhile, Ilyas et al. (2022) showed that the corresponding performance depends on the choice of the weight vector $\\eta$ . Therefore, our OFA-A estimator serves as a sufficient proxy for a range of $\\{\\pmb\\theta^{*}\\}$ and would facilitate the fine-tuning of $\\eta$ when using datamodels to detect similar training examples. ", "page_idx": 6}, {"type": "text", "text": "When $\\theta^{*}$ Can Be Recovered From $\\phi$ Interestingly, for specific choices of $\\mathbf{p}\\in\\mathbb{R}^{n}$ and $\\pmb{\\eta}\\in\\mathbb{R}^{n+1}$ , it holds that $\\pmb\\theta=\\phi$ . Theorem 2 can be seen as an extension to the previous result stated in the below. ", "page_idx": 6}, {"type": "text", "text": "Proposition 5 (Marichal and Mathonet 2011, Proposition 4). Suppose $0\\,<\\,a\\,<\\,1$ is given, if $p_{j}\\,\\stackrel{{\\displaystyle>}}{=}a^{j-1}(1-\\dot{a})^{n-j}$ for $1\\le j\\le n$ and $\\eta_{k}=a^{k-2}(1-a)^{n-k}$ for $1\\leq k\\leq n+1$ , which leads to $\\eta_{s}=p_{s-1}+p_{s}$ for $2\\leq s\\leq n$ , there is ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\pmb\\theta^{*}=\\phi.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "It is worth pointing out that $\\phi$ in Proposition 5 is exactly the weighted Banzhaf value parameterized by $a$ , i.e., WB- $a$ . Even more, under the same setting, we can even solve a group of datamodels with $\\ell_{1}$ or $\\ell_{2}$ regularization simultaneously. ", "page_idx": 6}, {"type": "text", "text": "Corollary 1. Under the setting of Proposition $^{5}$ , let $\\pmb{\\theta}^{*}$ be the uniquely optimal solution to ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\operatorname*{argmin}_{\\theta\\in\\mathbb{R}^{n},b\\in\\mathbb{R}}\\left(\\sum_{S\\subseteq[n]}\\eta_{s+1}\\left(U(S)-b-\\sum_{i\\in S}\\theta_{i}\\right)^{2}\\right)+\\frac{\\lambda}{a(1-a)}\\mathcal{R}(\\theta)\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\lambda>0$ , the following are true about the relation between $\\pmb{\\theta}^{*}$ and $\\phi$ : ", "page_idx": 7}, {"type": "text", "text": "1. If $\\mathcal{R}(\\pmb{\\theta})=\\|\\pmb{\\theta}\\|_{2}^{2}$ , then ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\pmb{\\theta}^{*}=\\left(1+\\frac{\\lambda}{a(1-a)}\\right)^{-1}\\pmb{\\phi}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "2. If $\\mathcal{R}(\\pmb{\\theta})=\\|\\pmb{\\theta}\\|_{1}$ , then ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\theta^{*}=\\mathrm{sign}(\\phi)\\cdot\\operatorname*{max}\\left(0,|\\phi|-\\frac{\\lambda}{2a(1-a)}\\right).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "All operations are element-wise. ", "page_idx": 7}, {"type": "text", "text": "This corollary is immediate by combining Proposition 5, and Theorem 2.2 by Saunshi et al. (2022). We comment that replacing $x_{i}$ by $2x_{i}-1$ , i.e., mapping 0 and 1 into $-1$ and 1, respectively, in $\\phi_{\\{i\\}}(x)$ used by Saunshi et al. (2022) yields $\\boldsymbol{v}_{\\{i\\}}(\\boldsymbol{x})$ used by Marichal and Mathonet (2011). A remarkable implication of the combination of Corollary 1 and our proposed OFA-A estimator is that we can solve a group of regularized datamodels covered by the problem (7) simultaneously! For example, the coefficient $\\lambda$ can be finetuned by running Algorithm 1 just once. ", "page_idx": 7}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we are to verify i) the simultaneous efficiency of our OFA-A estimator and ii) the faster convergence rate of our OFA-S estimator compared with the considered baselines and our OFA-A estimator. Particularly, if $D(\\mathbf{m},\\mathbf{q})$ is effective in determining the convergence rate of Algorithm 1, our OFA-S estimator is expected to be faster than our OFA-A estimator. All the experiments are conducted using CPUs. ", "page_idx": 7}, {"type": "text", "text": "We use two types of utility functions for this end: i) following the experiment settings of (Li and $\\mathrm{Yu}~2024\\$ ), $\\bar{U}(S)$ is set to be the cross-entropy of LeNets trained on $S$ on the classification datasets FMNIST, MNIST and iris; to obtain the exact values, the number of training data $n$ is set to be 24; ii) U is defined to be the sum of unanimity (SOU) games, i.e., U(S) = jd=1 \u03b1j1Sj\u2286S where each $\\emptyset\\,\\subsetneq\\,S_{j}\\,\\subsetneq\\,[n]$ is randomly sampled, for which each semi-value can be computed by $\\begin{array}{r}{\\phi_{i}\\,=\\,\\sum_{j=1}^{d}\\alpha_{j}\\int_{[0,1]}w^{s_{j}-1}\\mathrm{d}\\mu(w)}\\end{array}$ ; specifically, we set $n\\,\\in\\,\\{64,128,256\\}$ with $d\\,=\\,n^{2}$ , which implies that the implemented SOU games require $n^{2}$ utility evaluations to compute semi-values exactly. The random seed inside each utility function is fixed as 2024, and thus each $U$ is deterministic. ", "page_idx": 7}, {"type": "text", "text": "For the simplicity of presenting our empirical results, we use the area under the convergence curve (AUCC) to assess the convergence quality of estimators, and thus the smaller the better. For $n=24$ , the value of each player is approximated using $20,000$ utility evaluations, and we compute the AUCCs as110 0 j10=01\u2225\u03d5\u02c6(20\u22250\u03d5j)\u22252\u2212\u03d5\u22252 where \u03d5\u02c6(200j) refers to the estimate using 200j utility evaluations for each player. For $n\\in\\{64,128,256\\}$ , the value of each player is approximated using $2,000$ utility evaluations, and the corresponding AUCCs are calculated as1100 j=1 \u2225 100 \u2225\u03d5\u02c6(20j)\u2212\u03d5\u22252. All the AUCCs are reported with standard deviation using 30 different random seeds from $\\left\\lbrace0,1,2,\\ldots,29\\right\\rbrace$ . ", "page_idx": 7}, {"type": "text", "text": "Verification of Our OFA-A Estimator For our OFA-A estimator where we substitute $\\mathbf{q}^{\\mathrm{OFA-A}}$ , which is defined in Proposition 1, into Algorithm 1, we choose the baselines according to Figure 1. The selected baselines include WSL-Shapley (Kwon and Zou 2022a), SHAP-IQ (Fumagalli et al. 2024), weightedSHAP (Kwon and Zou 2022b) and permutation-Shapley (Castro et al. 2009). The corresponding results are reported in Figure 2. Overall, our OFA-A estimator performs the best on all the employed 18 probabilistic values, which verify the simultaneous efficiency of our OFA-A estimator. ", "page_idx": 7}, {"type": "image", "img_path": "AUg9D2VjcF/tmp/068c1604eedb55e78cdcecdeb635536f38d9bb23e52c8979f249f9ce45875dd6.jpg", "img_caption": ["Figure 2: Comparison of one-for-all estimators using six utility functions. All the AUCCs are reported with standard deviation using 30 random seeds. Smaller AUCC indicates faster convergence rate. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Verification of Our OFA-S Estimator Next, we verify the faster convergence rate of our OFAS estimator, using $\\mathbf{q}^{\\mathrm{OFA-S}}$ as defined in Eq. (5). The baselines we employ include: (unbiased) kernelSHAP (Covert and Lee 2021; Lundberg and Lee 2017), GELS and GELS-Shapley (Li and Yu 2024), ARM (Kolpaczki et al. 2024; Li and $\\mathrm{Yu}\\;2024\\$ ), complement (Zhang et al. 2023), group testing (Jia et al. 2019; Wang and Jia 2023a), AME (Lin et al. 2022), MSR (Wang and Jia 2023b) and sampling lift (Moehle et al. 2022). Note that not all the baselines are designed for all the probabilistic values we employ. For example, the complement estimator only works for $\\mathrm{{Beta}}(1,1)$ , i.e., the Shapley value. According to (Li and Yu 2024, Remark 9), we implement the paired sampling technique for (unbiased) kernelSHAP and group testing. The corresponding results are presented in Figure 3. ", "page_idx": 8}, {"type": "text", "text": "First, our OFA-S estimator is indeed faster than our OFA-A estimator, which aligns exactly with our theory; in other words, it implies that our proposed $D(\\mathbf{m},\\mathbf{p})$ indeed determines the convergence rate of our Algorithm 1. Second, our OFA-S estimator always performs the best except on the SOU games which require only $n^{2}$ utility evaluation to get the exact values; by contrast, the utility function defined using the classification datasets require $2^{n}$ utility evaluations instead. Third, our proposed estimator is consistently the fastest on the commonly-used Beta $(1,1)$ , i.e., the Shapley value; note that $\\mathbf{q}^{\\mathrm{OFA-A}}=\\mathbf{q}^{\\mathrm{OFA-S}}$ for the Shapley value; therefore, our proposed estimator achieves the currently best convergence rate both empirically and theoretically. ", "page_idx": 8}, {"type": "image", "img_path": "AUg9D2VjcF/tmp/82005880f4a1e0230abf16a844aa6d41dd4c113c5ece72f74e543bd65290a50d.jpg", "img_caption": ["Figure 3: Comparison of twelve estimators using six utility functions. The dashed lines correspond to the improved AME estimator developed in Appendix D. All the AUCCs are reported with standard deviation using 30 random seeds. Smaller AUCC indicates faster convergence rate. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we propose a framework, termed OFA, that i) adheres to the principle of maximum sample reuse and ii) contains no amplifying factors for the goal of optimizing all probabilistic values simultaneously and efficiently. Particularly, our OFA framework is parameterized by a sampling vector $\\mathbf{q}\\in\\mathbb{R}^{\\bar{n}-3}$ . To gain insights, we theoretically develop a key formula $D(\\mathbf{m},\\mathbf{\\bar{q}})$ concerning this framework that effectively determines the corresponding convergence rate. By optimizing q in $D(\\mathbf{m},\\mathbf{q})$ for all probabilistic values on average, we obtain our one-for-all estimator that can theoretically approximate all probabilistic values simultaneously with the currently best convergence rate $O(n\\log{n})$ on average. Meanwhile, we propose a faster generic estimator by optimizing q for each specific probabilistic value, and we demonstrate that our generic estimate enjoys the best convergence rate for all previously-studied probabilistic values. All of our theoretical findings are verified in our experiments. Finally, we establish a connection between probabilistic values and the least square regressions used in datamodels, showing that our OFA-A estimator is capable of solving a family of (regularized) datamodels simultaneously. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "We thank the reviewers and the area chair for thoughtful comments that have improved our final presentation. YY gratefully acknowledges NSERC and CIFAR for funding support. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Burgess, M. A. and A. C. Chapman (2021). \u201cApproximating the Shapley Value Using Stratified Empirical Bernstein Sampling\u201d. In: IJCAI, pp. 73\u201381.   \nCastro, J., D. G\u00f3mez, E. Molina, and J. Tejada (2017). \u201cImproving Polynomial Estimation of the Shapley Value by Stratified Random Sampling with Optimum Allocation\u201d. Computers & Operations Research, vol. 82, pp. 180\u2013188.   \nCastro, J., D. G\u00f3mez, and J. Tejada (2009). \u201cPolynomial Calculation of the Shapley Value Based on Sampling\u201d. Computers & Operations Research, vol. 36, no. 5, pp. 1726\u20131730.   \nCovert, I. and S.-I. Lee (2021). \u201cImproving KernelSHAP: Practical Shapley Value Estimation Using Linear Regression\u201d. In: International Conference on Artificial Intelligence and Statistics, pp. 3457\u20133465.   \nDubey, P., A. Neyman, and R. J. Weber (1981). \u201cValue Theory without Efficiency\u201d. Mathematics of Operations Research, vol. 6, no. 1, pp. 122\u2013128.   \nFumagalli, F., M. Muschalik, P. Kolpaczki, E. H\u00fcllermeier, and B. Hammer (2024). \u201cSHAP-IQ: Unified Approximation of Any-Order Shapley Interactions\u201d. In: Advances in Neural Information Processing Systems. Vol. 36.   \nGhorbani, A. and J. Y. Zou (2019). \u201cData Shapley: Equitable Valuation of Data for Machine Learning\u201d. In: International Conference on Machine Learning, pp. 2242\u20132251.   \nHammer, P. L. and R. Holzman (1992). \u201cApproximations of Pseudo-Boolean Functions; Applications to Game Theory\u201d. Zeitschrift f\u00fcr Operations Research, vol. 36, no. 1, pp. 3\u201321.   \nIlyas, A., S. M. Park, L. Engstrom, G. Leclerc, and A. Madry (2022). \u201cDatamodels: Predicting Predictions from Training Data\u201d. In: Proceedings of the 39th International Conference on Machine Learning.   \nJia, R. et al. (2019). \u201cTowards Efficient Data Valuation Based on the Shapley Value\u201d. In: The 22nd International Conference on Artificial Intelligence and Statistics, pp. 1167\u20131176.   \nKolpaczki, P., V. Bengs, M. Muschalik, and E. H\u00fcllermeier (2024). \u201cApproximating the Shapley Value without Marginal Contributions\u201d. In: Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 38. 12, pp. 13246\u201313255.   \nKwon, Y. and J. Y. Zou (2022a). \u201cBeta Shapley: a Unified and Noise-reduced Data Valuation Framework for Machine Learning\u201d. In: International Conference on Artificial Intelligence and Statistics, pp. 8780\u20138802. (2022b). \u201cWeightedSHAP: Analyzing and Improving Shapley Based Feature Attributions\u201d. In: Advances in Neural Information Processing Systems. Vol. 35, pp. 34363\u201334376.   \nLi, W. and Y. Yu (2023). \u201cRobust Data Valuation with Weighted Banzhaf Values\u201d. In: Advances in Neural Information Processing Systems. Vol. 36.   \n\u2013 (2024). \u201cFaster Approximation of Probabilistic and Distributional Values via Least Squares\u201d. In: The Twelfth International Conference on Learning Representations.   \nLin, J., A. Zhang, M. L\u00e9cuyer, J. Li, A. Panda, and S. Sen (2022). \u201cMeasuring the Effect of Training Data on Deep Learning Predictions via Randomized Experiments\u201d. In: International Conference on Machine Learning, pp. 13468\u201313504.   \nLundberg, S. M. and S.-I. Lee (2017). \u201cA Unified Approach to Interpreting Model Predictions\u201d. In: Advances in Neural Information Processing Systems. Vol. 30.   \nMaleki, S., L. Tran-Thanh, G. Hines, T. Rahwan, and A. Rogers (2013). \u201cBounding the Estimation Error of Sampling-Based Shapley Value Approximation\u201d. arXiv preprint arXiv:1306.4265.   \nMarichal, J.-L. and P. Mathonet (2011). \u201cWeighted Banzhaf Power and Interaction Indexes Through Weighted Approximations of Games\u201d. European Journal of Operational Research, vol. 211, no. 2, pp. 352\u2013358.   \nMoehle, N., S. Boyd, and A. Ang (2022). \u201cPortfolio Performance Attribution via Shapley Value\u201d. Journal Of Investment Management, vol. 20, no. 3, pp. 33\u201352.   \nMusco, C. and R. T. Witter (2024). \u201cProvably Accurate Shapley Value Estimation via Leverage Score Sampling\u201d. arXiv preprint arXiv:2410.01917.   \nRozemberczki, B., L. Watson, P. Bayer, H.-T. Yang, O. Kiss, S. Nilsson, and R. Sarkar (2022). \u201cThe Shapley Value in Machine Learning\u201d. In: The 31st International Joint Conference on Artificial Intelligence and the 25th European Conference on Artificial Intelligence, pp. 5572\u20135579.   \nRuiz, L. M., F. Valenciano, and J. M. Zarzuelo (1998). \u201cThe Family of Least Square Values for Transferable Utility Games\u201d. Games and Economic Behavior, vol. 24, no. 1-2, pp. 109\u2013130.   \nSaunshi, N., A. Gupta, M. Braverman, and S. Arora (2022). \u201cUnderstanding Influence Functions and Datamodels via Harmonic Analysis\u201d. In: The Eleventh International Conference on Learning Representations.   \nShapley, L. S. (1953). \u201cA Value for N-Person Games\u201d. Annals of Mathematics Studies, vol. 28, pp. 307\u2013317.   \nWang, J. T. and R. Jia (2023a). \u201cA Note on \u2018Towards Efficient Data Valuation Based on the Shapley Value\u2019\u201d. arXiv preprint arXiv:2302.11431.   \n\u2013 (2023b). \u201cData Banzhaf: A Robust Data Valuation Framework for Machine Learning\u201d. In: International Conference on Artificial Intelligence and Statistics, pp. 6388\u20136421.   \nWang, J., Y. Zhang, Y. Gu, and T.-K. Kim (2022). \u201cSHAQ: Incorporating Shapley Value Theory into Multi-Agent Q-Learning\u201d. In: Advances in Neural Information Processing Systems. Vol. 35, pp. 5941\u20135954.   \nWeber, R. J. (1988). \u201cProbabilistic Values for Games\u201d. In: The Shapley Value. Essays in Honor of Lloyd S. Shapley, pp. 101\u2013119.   \nWu, M., R. Jia, C. Lin, W. Huang, and X. Chang (2023). \u201cVariance Reduced Shapley Value Estimation for Trustworthy Data Valuation\u201d. Computers & Operations Research, vol. 159, p. 106305.   \nZhang, J., Q. Sun, J. Liu, L. Xiong, J. Pei, and K. Ren (2023). \u201cEfficient Sampling Approaches to Shapley Value Approximation\u201d. Proceedings of the ACM on Management of Data, vol. 1, no. 1, pp. 1\u201324. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Proof of Theorem 1 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Theorem 1. Assume $i$ ) $\\|U\\|_{\\infty}\\leq u$ and ii) $0<\\epsilon\\leq\\sqrt{2D(\\mathbf{m},\\mathbf{q})\\gamma(\\mathbf{q})^{2}u^{2}}$ . For $\\hat{\\phi}$ in Algorithm $^{\\,l}$ , it requires $\\begin{array}{r}{\\frac{4n u^{2}D(\\mathbf{m,q})}{\\epsilon^{2}}\\log\\frac{8n^{2}}{\\delta}}\\end{array}$ evaluations of $U$ to achieve $P(\\|\\hat{\\phi}-\\phi\\|_{2}\\ge\\epsilon)\\leq\\delta$ where ", "page_idx": 12}, {"type": "equation", "text": "$$\nD(\\mathbf{m},\\mathbf{q})=\\sum_{s=2}^{n-2}{\\frac{n}{q_{s-1}}}\\left({\\frac{m_{s}^{2}}{s}}+{\\frac{m_{s+1}^{2}}{n-s}}\\right)\\,a n d\\,\\gamma(\\mathbf{q})=\\operatorname*{min}_{2\\leq s\\leq n-2}\\operatorname*{min}\\left({\\frac{q_{s-1}\\cdot s}{n}},\\,{\\frac{q_{s-1}\\cdot(n-s)}{n}}\\right).\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Proof. Following Algorithm 1, let $\\{S_{t}\\}_{t=1}^{T}$ be $T$ independent random subsets. Define ", "page_idx": 12}, {"type": "equation", "text": "$$\nT_{i,s}^{+}=\\sum_{t=1}^{T}[\\![i\\in S_{t},|S_{t}|=s]\\!]\\mathrm{~and~}T_{i,s}^{-}=\\sum_{t=1}^{T}[\\![i\\notin S_{t},|S_{t}|=s]\\!]\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where $s=2,3,\\ldots,n-2$ . Then, we have ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\hat{\\phi}_{i,s}^{+}=\\frac{1}{T_{i,s}^{+}}\\sum_{i=1}^{T}[i\\in S_{t},|S_{t}|=s]\\cdot U(S_{t})\\mathrm{~and~}\\hat{\\phi}_{i,s}^{-}=\\frac{1}{T_{i,s}^{-}}\\sum_{i=1}^{T}[i\\notin S_{t},|S_{t}|=s]\\cdot U(S_{t}).\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Define $\\begin{array}{r}{r_{i,s}^{+}=\\frac{T_{i,s}^{+}}{T}}\\end{array}$ and $\\begin{array}{r}{r_{i,s}^{-}=\\frac{T_{i,s}^{-}}{T}}\\end{array}$ T iT,s . In particular, both i \u2208St, |St| = s and i \u0338\u2208St, |St| = s are Bernoulli random variables with ", "page_idx": 12}, {"type": "text", "text": "$\\Xi[r_{i,s}^{+}]=q_{s-1}{\\binom{n-1}{s-1}}{\\binom{n}{s}}^{-1}={\\frac{q_{s-1}\\cdot s}{n}}\\;\\mathrm{and}\\;\\mathbb{E}[r_{i,s}^{-}]=q_{s-1}{\\binom{n-1}{s}}{\\binom{n}{s}}^{-1}={\\frac{q_{s-1}\\cdot(n-s)}{n}}.$ Additionally, R and \u03c4 are defined to be vectors in R2n\u22126 such that R2k\u22121 = ri+,k+1, R2k = ri\u2212,k+1, $\\tau_{2k-1}\\,=\\,{\\textstyle\\frac{q_{k}\\cdot(k+1)}{n}}$ alaitnyd, $\\begin{array}{r}{\\tau_{2k}\\,=\\,\\frac{q_{k}\\cdot(n-k-1)}{n}}\\end{array}$ for $k\\,\\in\\,[n-3]$ . Note that $\\boldsymbol{R}$ is a random vector. By ", "page_idx": 12}, {"type": "equation", "text": "$$\nP(|R_{j}-\\tau_{j}|\\geq\\omega)\\leq2\\exp\\left(-2T\\omega^{2}\\right)\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where $\\omega>0$ , and thus ", "page_idx": 12}, {"type": "equation", "text": "$$\nP(\\|\\pmb{R}-\\pmb{\\tau}\\|_{\\infty}\\geq\\omega)\\leq P(\\bigcup_{j\\in[2n-6]}|\\cal R_{j}-\\tau_{j}|\\geq\\omega)\\leq(4n-12)\\exp\\left(-2T\\omega^{2}\\right).\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Denote the event $\\{\\sum_{s=2}^{n-2}[m_{s}(\\hat{\\phi}_{i,s}^{+}-\\phi_{i,s}^{+})+m_{s+1}(\\phi_{i,s}^{-}-\\hat{\\phi}_{i,s}^{-})]\\ge\\epsilon\\}$ by $E_{i}$ where $\\epsilon>0$ . Let $\\mathcal{C}$ be the set that contains all possible configurations $\\mathbf{C}\\,\\in\\,\\{0,1\\}^{(2n-6)\\times T}$ such that $\\begin{array}{r}{\\frac{\\mathbf{C1}_{T}}{T}=R}\\end{array}$ and $\\mathbf{1}_{T}^{\\top}\\mathbf{C}\\,=\\,\\mathbf{1}_{2n-6}^{\\top}$ , i.e., $C_{j,k}\\,=\\,0$ indicates that the $k$ -th subset is sampled from $\\{R\\,\\subseteq\\,[n]\\,\\mid\\,r\\,=$ $\\bar{(j+3)/2}$ and $i\\in R\\}$ if $j$ is odd and $\\{R\\subseteq[n]\\mid r=(j+2)/2$ and $i\\not\\in R\\}$ otherwise. Then, ", "page_idx": 12}, {"type": "equation", "text": "$$\nP(E_{i})=\\sum_{\\mathbf{C}\\in\\mathcal{C}}P(E_{i}\\cap\\mathbf{C})=\\sum_{\\mathbf{C}\\in\\mathcal{C}}P(E_{i}\\mid\\mathbf{C})\\cdot P(\\mathbf{C}).\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Observe that $\\mathcal{C}$ can be divided into two separate groups $\\mathcal{C}_{<\\omega}$ and $\\ensuremath{\\mathcal{C}}_{\\!\\geq\\omega}$ such that ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\sum_{\\mathbf{C}_{<\\omega}\\in\\mathcal{C}_{<\\omega}}P(\\mathbf{C}_{<\\omega})=P(\\|R-\\tau\\|_{\\infty}<\\omega)\\;\\mathrm{and}\\;\\sum_{\\mathbf{C}_{\\geq\\omega}\\in\\mathcal{C}_{\\geq\\omega}}P(\\mathbf{C}_{\\geq\\omega})=P(\\|R-\\tau\\|_{\\infty}\\geq\\omega).\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Therefore, ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{P(E_{i})=\\displaystyle\\sum_{\\mathbf{C}_{<\\omega}\\in\\mathcal{C}_{<\\omega}}P(E_{i}\\mid\\mathbf{C}_{<\\omega})\\cdot P(\\mathbf{C}_{<\\omega})+\\displaystyle\\sum_{\\mathbf{C}_{\\geq\\omega}\\in\\mathcal{C}_{\\geq\\omega}}P(E_{i}\\mid\\mathbf{C}_{\\geq\\omega})\\cdot P(\\mathbf{C}_{\\geq\\omega})}\\\\ &{\\quad\\leq\\displaystyle\\sum_{\\mathbf{C}_{<\\omega}\\in\\mathcal{C}_{<\\omega}}P(E_{i}\\mid\\mathbf{C}_{<\\omega})\\cdot P(\\mathbf{C}_{<\\omega})+(4n-12)\\exp\\left(-2T\\omega^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "For simplicity, we write PC<\u03c9(Ei) instead of P(Ei | C<\u03c9). Additionally, we assume \u03c9 < \u03b3(2q) so that neither $T_{i,s}^{+}$ nor $T_{i,s}^{-}$ is zero when conditioned on any ${\\bf C}_{<\\omega}$ . By the Chernoff bound, for any ", "page_idx": 12}, {"type": "text", "text": "$\\lambda>0$ , there is ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{P_{\\mathbf{C}_{<\\omega}}(E_{i})\\leq\\mathbb{E}_{\\mathbf{C}_{<\\omega}}\\left[\\exp\\left(\\displaystyle\\lambda\\sum_{s=2}^{n-2}\\left(m_{s}(\\hat{\\phi}_{i,s}^{+}-\\phi_{i,s}^{+})+m_{s+1}(\\phi_{i,s}^{-}-\\hat{\\phi}_{i,s}^{-})\\right)\\right)\\right]\\cdot e^{-\\lambda\\epsilon}}\\\\ &{\\qquad\\qquad=e^{-\\lambda\\epsilon}\\displaystyle\\prod_{s=2}^{n-2}\\mathbb{E}_{\\mathbf{C}_{<\\omega}}\\left[\\exp\\left(\\lambda m_{s}(\\hat{\\phi}_{i,s}^{+}-\\phi_{i,s}^{+})\\right)\\right]\\displaystyle\\prod_{s=2}^{n-2}\\mathbb{E}_{\\mathbf{C}_{<\\omega}}\\left[\\exp\\left(\\lambda m_{s+1}(\\phi_{i,s}^{-}-\\hat{\\phi}_{i,s}^{-})\\right)\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where the equality is due to the independence that stems from the independence of random subsets and that the configuration is fixed. Moreover, ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathbf{C}_{<\\omega}}\\left[\\exp\\left(\\lambda m_{s}(\\hat{\\phi}_{i,s}^{+}-\\phi_{i,s}^{+})\\right)\\right]=\\mathbb{E}_{\\mathbf{C}_{<\\omega}}\\left[\\exp\\left(\\lambda m_{s}\\frac{1}{T_{i,s}^{+}}\\sum_{j=1}^{T_{i,s}^{+}}(U(S_{i,s,j}^{+})-\\phi_{i,s}^{+})\\right)\\right]\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\{S_{i,s,j}^{+}\\}_{1\\leq j\\leq T_{i,s}^{+}}$ is obtained by ordering $\\{S_{t}\\ |\\ |S_{t}|=s$ and $i\\in S_{t}\\}$ . In a similar fashion, we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathbf{C}\\sim\\omega}\\left[\\exp\\left(\\lambda m_{s+1}(\\phi_{i,s}^{-}-\\hat{\\phi}_{i,s}^{-})\\right)\\right]=\\prod_{j=1}^{T_{i,s}^{-}}\\mathbb{E}_{\\mathbf{C}\\sim\\omega}\\left[\\exp\\left(\\frac{\\lambda m_{s+1}}{T_{i,s}^{-}}(\\phi_{i,s}^{-}-U(S_{i,s,j}^{-}))\\right)\\right]\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "By Hoeffding\u2019s lemma, ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\mathbf{C}_{<\\omega}}\\left[\\exp\\left(\\displaystyle\\frac{\\lambda m_{s}}{T_{i,s}^{+}}(U(S_{i,s,j}^{+})-\\phi_{i,s}^{+})\\right)\\right]\\leq\\exp\\left(\\displaystyle\\frac{\\lambda^{2}m_{s}^{2}u^{2}}{2T_{i,s}^{+}\\cdot T_{i,s}^{+}}\\right),}\\\\ &{\\mathbb{E}_{\\mathbf{C}_{<\\omega}}\\left[\\exp\\left(\\displaystyle\\frac{\\lambda m_{s+1}}{T_{i,s}^{-}}(\\phi_{i,s}^{-}-U(S_{i,s,j}^{-}))\\right)\\right]\\leq\\exp\\left(\\displaystyle\\frac{\\lambda^{2}m_{s+1}^{2}u^{2}}{2T_{i,s}^{-}\\cdot T_{i,s}^{-}}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "which leads to ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\prod_{j=1}^{T_{i,s}^{+}}\\mathbb{E}_{\\mathbf{C}<\\omega}\\left[\\exp\\left(\\frac{\\lambda m_{s}}{T_{i,s}^{+}}(U(S_{i,s,j}^{+})-\\phi_{i,s}^{+})\\right)\\right]\\leq\\exp\\left(\\frac{\\lambda^{2}m_{s}^{2}u^{2}}{2T_{i,s}^{+}}\\right),}\\\\ &{\\displaystyle\\prod_{j=1}^{T_{i,s}^{-}}\\mathbb{E}_{\\mathbf{C}<\\omega}\\left[\\exp\\left(\\frac{\\lambda m_{s+1}}{T_{i,s}^{-}}(\\phi_{i,s}^{-}-U(S_{i,s,j}^{-}))\\right)\\right]\\leq\\exp\\left(\\frac{\\lambda^{2}m_{s+1}^{2}u^{2}}{2T_{i,s}^{-}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Therefore, ", "page_idx": 13}, {"type": "equation", "text": "$$\nP_{\\mathbf{C}_{<\\omega}}(E_{i})\\leq\\exp\\left(\\frac{\\lambda^{2}u^{2}}{2T}\\hat{D}-\\lambda\\epsilon\\right)\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where D\u02c6 = sn=\u221222 $\\begin{array}{r}{\\hat{D}=\\sum_{s=2}^{n-2}\\left(\\frac{T}{T_{i,s}^{+}}m_{s}^{2}+\\frac{T}{T_{i,s}^{-}}m_{s+1}^{2}\\right)}\\end{array}$ . Next, we aim to show that $|\\hat{D}-D(\\mathbf{m},\\mathbf{q})|\\leq D(\\mathbf{m},\\mathbf{q})$ Observe that ", "page_idx": 13}, {"type": "equation", "text": "$$\n|\\hat{D}-D({\\bf m,q})|\\leq\\sum_{s=2}^{n-2}\\left(\\left|\\frac{1}{r_{2s-3}}-\\frac{1}{\\tau_{2s-3}}\\right|m_{s}^{2}-\\left|\\frac{1}{r_{2s-2}}-\\frac{1}{\\tau_{2s-2}}\\right|m_{s+1}^{2}\\right),\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "and since $|r_{j}-\\tau_{j}|<\\omega$ , ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\left|{\\frac{1}{r_{j}}}-{\\frac{1}{\\tau_{j}}}\\right|\\leq{\\frac{\\omega}{(\\tau_{j}-\\omega)\\tau_{j}}}={\\frac{1}{\\tau_{j}-\\omega}}-{\\frac{1}{\\tau_{j}}}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Since $\\gamma(\\mathbf{q})\\leq\\tau_{j}$ and $\\omega\\leq{\\frac{\\gamma(\\mathbf{q})}{2}}$ , ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\frac{1}{\\tau_{j}-\\omega}=\\frac{\\tau_{j}}{\\tau_{j}-\\omega}\\cdot\\frac{1}{\\tau_{j}}=\\frac{1}{1-\\frac{\\omega}{\\tau_{j}}}\\cdot\\frac{1}{\\tau_{j}}\\leq\\frac{2}{\\tau_{j}}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "As a result, we have $|\\hat{D}-D(\\mathbf{m},\\mathbf{q})|\\leq D(\\mathbf{m},\\mathbf{q})$ , and thus ", "page_idx": 14}, {"type": "equation", "text": "$$\nP_{\\mathbf{C}_{<\\omega}}(E_{i})\\leq\\exp\\left(\\frac{\\lambda^{2}u^{2}}{T}D(\\mathbf{m},\\mathbf{q})-\\lambda\\epsilon\\right).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Combining Eqs. (8) and (9) yields ", "page_idx": 14}, {"type": "equation", "text": "$$\nP(E_{i})\\leq\\exp\\left(\\frac{\\lambda^{2}u^{2}}{T}D({\\bf m},{\\bf q})-\\lambda\\epsilon\\right)+(4n-12)\\exp(-2T\\omega^{2}).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Choosing $\\lambda>0$ that minimizes the upper bound yields ", "page_idx": 14}, {"type": "equation", "text": "$$\nP(E_{i})\\leq\\exp\\left(-\\frac{T\\epsilon^{2}}{4u^{2}D({\\bf m},{\\bf q})}\\right)+(4n-12)\\exp(-2T\\omega^{2}).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Solving the equation \u22124u2DT (\u03f5m,q) $\\begin{array}{r}{-\\frac{T\\epsilon^{2}}{4u^{2}D(\\mathbf{m,q})}=-2T\\omega^{2}}\\end{array}$ yields $\\begin{array}{r}{\\omega=\\sqrt{\\frac{\\epsilon^{2}}{8D(\\mathbf{m},\\mathbf{q})u^{2}}}}\\end{array}$ 8D(m\u03f5,q)u2 , which gives ", "page_idx": 14}, {"type": "equation", "text": "$$\n-2T\\omega^{2}=-\\frac{T\\epsilon^{2}}{4D({\\bf m,q})u^{2}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Particularly, to meet the assumption $\\omega\\ \\leq\\ {\\frac{\\gamma(\\mathbf{q})}{2}}$ , we have to have $\\epsilon\\,\\leq\\,\\sqrt{2D({\\bf m},{\\bf q})\\gamma({\\bf q})^{2}u^{2}}$ . To conclude, provided that $\\epsilon\\leq\\sqrt{2D({\\bf m},{\\bf q})\\gamma({\\bf q})^{2}u^{2}}$ , we have ", "page_idx": 14}, {"type": "equation", "text": "$$\nP(\\sum_{s=2}^{n-2}\\Big(m_{s}(\\widehat{\\phi}_{i,s}^{+}-\\phi_{i,s}^{+})+m_{s+1}(\\phi_{i,s}^{-}-\\widehat{\\phi}_{i,s}^{-})\\Big)\\geq\\epsilon)\\leq4n\\exp(-\\frac{T\\epsilon^{2}}{4D(\\mathbf{m},\\mathbf{q})u^{2}}).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Similarly, there is ", "page_idx": 14}, {"type": "equation", "text": "$$\nP(\\sum_{s=2}^{n-2}\\Big(m_{s}(\\phi_{i,s}^{+}-\\hat{\\phi}_{i,s}^{+})+m_{s+1}(\\hat{\\phi}_{i,s}^{-}-\\phi_{i,s}^{-})\\Big)\\geq\\epsilon)\\leq4n\\exp(-\\frac{T\\epsilon^{2}}{4D(\\mathbf{m},\\mathbf{q})u^{2}}),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "and thus ", "page_idx": 14}, {"type": "equation", "text": "$$\nP(|\\hat{\\phi}_{i}-\\phi_{i}|\\geq\\epsilon)\\leq8n\\exp(-\\frac{T\\epsilon^{2}}{4D({\\bf m},{\\bf q})u^{2}}).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Eventually, we have ", "page_idx": 14}, {"type": "equation", "text": "$$\nP(\\|\\hat{\\phi}-\\phi\\|_{2}\\ge\\epsilon)\\le P(\\bigcup_{i\\in[n]}|\\hat{\\phi}_{i}-\\phi_{i}|\\ge\\frac{\\epsilon}{\\sqrt{n}})\\le8n^{2}\\exp(-\\frac{T\\epsilon^{2}}{4n D({\\bf m},{\\bf q})u^{2}}).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Solving $\\begin{array}{r}{\\delta\\,\\geq\\,8n^{2}\\exp(-{\\frac{T\\epsilon^{2}}{4n D(\\mathbf{m},\\mathbf{q})u^{2}}})}\\end{array}$ yields $\\begin{array}{r}{T\\,\\geq\\,\\frac{4n D(\\mathbf{m},\\mathbf{q})u^{2}}{\\epsilon^{2}}\\log\\frac{8n^{2}}{\\delta}}\\end{array}$ . Note the assumption $\\epsilon\\,\\leq$ $\\sqrt{2D({\\bf m},{\\bf q})\\gamma({\\bf q})^{2}u^{2}}$ can be removed if the configuration is fixed with $\\begin{array}{r}{T_{i,s}^{+}\\approx\\frac{s\\cdot q_{s-1}}{n}T}\\end{array}$ and $T_{i,s}^{-}\\approx$ $\\frac{(n\\!-\\!s)q_{s-1}}{n}T$ \u53e3 ", "page_idx": 14}, {"type": "text", "text": "B Proofs of Propositions ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Proposition 1. $\\begin{array}{r}{\\mathbf{q}_{s-1}^{O F\\!A-\\!A}\\propto\\frac{1}{\\sqrt{(s)(n-s)}}}\\end{array}$ and $\\overline{{D}}(\\mathbf{q}^{O F A-A})\\in O(1)$ . In other words, our OFA-A estimator achieves the convergence rate of $O(n\\log n)$ simultaneously for all probabilistic values on average. ", "page_idx": 14}, {"type": "text", "text": "Proof. Let $\\begin{array}{r}{\\Lambda=\\{\\mathbf{x}\\in\\mathbb{R}^{n-1}\\mid0\\leq\\sum_{j=1}^{n-1}x_{j}\\leq L_{n}\\}}\\end{array}$ where $L_{n}=n^{\\frac{1}{2(n-1)}}$ , and a smooth homeomorphism $f:\\Lambda\\to\\Delta$ is defined by letting ", "page_idx": 14}, {"type": "equation", "text": "$$\nf(\\mathbf{x})=\\frac{1}{L_{n}}(x_{1},x_{2},\\cdot\\cdot\\cdot\\cdot,x_{n-1},L_{n}-\\sum_{j=1}^{n-1}x_{j})^{\\top}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "In other words, both $f$ and $f^{-1}$ are $C^{\\infty}$ . Since the volume of $\\Delta$ is (nn\u22121)!, there is ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{(n-1)!}{n^{\\frac{1}{2}}}\\int_{\\mathbf{x}\\in\\Lambda}D(f(\\mathbf{x}),\\mathbf{q})\\sqrt{\\operatorname*{det}\\left(D f(\\mathbf{x})^{\\top}D f(\\mathbf{x})\\right)}\\mathrm{d}\\mathbf{x}=\\int_{\\mathbf{m}\\in\\Delta}D(\\mathbf{m},\\mathbf{q})\\mathrm{d}\\nu(\\mathbf{m}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Note that $\\sqrt{\\operatorname*{det}\\left(D f(\\mathbf{x})^{\\top}D f(\\mathbf{x})\\right)}=1$ for every $\\mathbf{x}\\in\\Lambda$ . With $\\begin{array}{r}{\\overline{{\\Lambda}}=\\left\\{\\mathbf{y}\\in\\mathbb{R}^{n-1}\\mid0\\leq\\sum_{j=1}^{n-1}y_{j}\\leq\\right.}\\end{array}$ $1\\}$ , we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{(n-1)!}{n^{\\frac{1}{2}}}\\int_{\\mathbf{x}\\in\\Lambda}D(f(\\mathbf{x}),\\mathbf{q})\\mathrm{d}\\mathbf{x}=(n-1)!\\int_{\\mathbf{y}\\in\\overline{{\\Lambda}}}D(f(L_{n}\\mathbf{y}),\\mathbf{q})\\mathrm{d}\\mathbf{y}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "For simplicity, assume that $n=4$ , notice that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\int_{y\\in\\overline{{\\Lambda}}}y_{n-1}^{2}\\mathrm{d}\\mathbf{y}=\\int_{0}^{1}\\mathrm{d}y_{1}\\int_{0}^{1-y_{1}}\\mathrm{d}y_{2}\\int_{0}^{1-y_{1}-y_{2}}y_{3}^{2}\\mathrm{d}y_{3}=\\frac{1}{3\\cdot4\\cdot5}=\\frac{1}{\\prod_{k=1}^{n-1}(2+k)}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Therefore, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\int_{\\mathbf{y}\\in\\overline{{\\Lambda}}}D\\big(f\\big(L_{n}\\mathbf{y}\\big),\\mathbf{q}\\big)\\mathrm{d}\\mathbf{y}=\\displaystyle\\sum_{s=2}^{n-2}\\frac{n}{q_{s-1}}\\int_{y\\in\\overline{{\\Lambda}}}\\left(\\frac{y_{s}^{2}}{s}+\\frac{y_{s+1}^{2}}{n-s}\\right)\\mathrm{d}\\mathbf{y}}\\\\ &{\\displaystyle\\qquad\\qquad=\\frac{1}{\\prod_{k=1}^{n-1}(2+k)}\\sum_{s=2}^{n-2}\\frac{n}{q_{s-1}}\\left(\\frac{1}{s}+\\frac{1}{n-s}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "which leads to ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\overline{{D}}({\\bf q})=\\frac{(n-1)!}{\\prod_{k=1}^{n-1}(2+k)}\\sum_{s=2}^{n-2}\\frac{n}{q_{s-1}}\\left(\\frac{1}{s}+\\frac{1}{n-s}\\right).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Since $\\overline{{D}}(\\mathbf{q})$ is convex in q, $\\mathbf{q}^{\\mathrm{OFA-A}}$ can be directly obtained using the KKT conditions, which is ", "page_idx": 15}, {"type": "equation", "text": "$$\nq_{s-1}^{\\mathrm{OFA-A}}=\\frac{\\sqrt{\\frac{n}{s}+\\frac{n}{n-s}}}{\\sum_{s=2}^{n-2}\\sqrt{\\frac{n}{s}+\\frac{n}{n-s}}}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Therefore, we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\overline{{D}}(\\mathbf{q}^{\\mathrm{OFA-A}})=\\frac{(n-1)!}{\\prod_{k=1}^{n-1}(2+k)}\\left(\\sum_{s=2}^{n-2}\\sqrt{\\frac{n}{s}+\\frac{n}{n-s}}\\right)^{2}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Since limn\u2192\u221e $\\begin{array}{r}{\\operatorname*{lim}_{n\\to\\infty}\\frac{(n-1)!(n-1)^{2}}{\\prod_{k=1}^{n-1}(2+k)}=2\\Gamma(3)}\\end{array}$ , when $n$ is sufficiently large, there is ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\overline{{3}}(\\mathbf{q}^{\\mathrm{OFA-A}})\\approx\\frac{1}{n^{2}}\\left(\\sum_{s=2}^{n-2}\\sqrt{\\frac{n}{s}+\\frac{n}{n-s}}\\right)^{2}=\\left(\\frac{1}{n}\\sum_{s=2}^{n-2}\\sqrt{\\frac{1}{\\frac{s}{n}(1-\\frac{s}{n})}}\\right)^{2}<\\left(\\int_{0}^{1}\\frac{1}{x(1-x)}\\mathrm{d}x\\right)^{2}=\\pi^{2}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proposition 2. Our OFA-A estimator achieves the convergence rate of $O(n\\log n)$ simultaneously for all semi-values whose probability density functions exist and are bounded. Particularly, Beta Shapley values with $\\alpha,\\beta\\geq1$ all satisfy this condition. ", "page_idx": 15}, {"type": "text", "text": "Proof. Let $\\phi$ be a semi-value such that $\\begin{array}{r c l}{{p_{s}}}&{{=}}&{{\\int_{0}^{1}w^{s-1}(1\\,-\\,w)^{n-s}\\mathrm{d}\\mu(w)\\ =\\ \\int_{0}^{1}w^{s-1}(1\\,-\\,w)^{n-s}\\mathrm{d}\\mu(w)}}\\end{array}$ $w)^{n-s}p_{\\mu}(w)\\mathrm{d}w$ such that $p_{\\mu}(w)\\leq B$ for every $w\\in[0,1]$ . Particularly, we have ", "page_idx": 15}, {"type": "equation", "text": "$$\nn_{s}={\\binom{n-1}{s-1}}p_{s}\\leq B\\cdot{\\binom{n-1}{s-1}}\\int_{0}^{1}w^{s-1}(1-w)^{n-s}\\mathrm{d}w=B\\cdot{\\binom{n-1}{s-1}}{\\frac{(s-1)!(n-s)!}{n!}}={\\frac{B}{n}}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Therefore, ", "page_idx": 15}, {"type": "equation", "text": "$$\nD(\\mathbf{m},\\mathbf{q}^{\\mathrm{OFA-A}})\\leq\\frac{B^{2}}{n}\\sum_{s=2}^{n-2}\\frac{1}{q_{s-1}^{\\mathrm{OFA-A}}}\\left(\\frac{1}{s}+\\frac{1}{n-s}\\right)=B^{2}\\left(\\sum_{s=2}^{n-2}\\frac{1}{\\sqrt{s(n-s)}}\\right)^{2}<B^{2}\\pi^{2}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proposition 3. If $p_{s}=a^{s-1}(1-a)^{n-s}$ with $0<a<1$ , which corresponds to the weighted Banzhaf value parameterized by $w$ , then $D(\\mathbf{m},\\mathbf{q}^{O F\\!A-\\!A})\\in O(n^{\\frac{1}{2}})$ . In other words, the OFA estimator achieves the convergence rate of $O(n^{{\\frac{3}{2}}}\\log{n})$ simultaneously for all WB- $^{a}$ with $0<a<1$ . ", "page_idx": 16}, {"type": "text", "text": "Proof. With $q_{s-1}^{\\mathrm{OFA-A}}\\propto\\frac{1}{\\sqrt{s(n-s)}}$ \u221as(n\u2212s), we have ", "page_idx": 16}, {"type": "equation", "text": "$$\nD(\\mathbf{m},\\mathbf{q}^{\\mathrm{OFA-A}})=C\\cdot n\\cdot\\sum_{s=2}^{n-2}\\left(\\sqrt{\\frac{n-s}{s}}m_{s}^{2}+\\sqrt{\\frac{s}{n-s}}m_{s+1}^{2}\\right)\\mathrm{~where~}C=\\sum_{s=2}^{n-2}\\frac{1}{\\sqrt{s(n-s)}}<\\pi\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Then, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\displaystyle{D({\\bf m},{\\bf q}^{\\mathrm{OFA\\cdotA}})}\\strut}}\\\\ {{\\displaystyle{=C\\sum_{s=2}^{n-2}n\\left(\\sqrt{\\frac{n-s}{s}}\\binom{n-1}{s-1}^{2}\\left(a^{s-1}(1-a)^{n-s}\\right)^{2}+\\sqrt{\\frac{s}{n-s}}\\binom{n-1}{s}^{2}\\left(a^{s}(1-a)^{n-s-1}\\right)^{2}\\right).}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Specifically, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\sqrt{\\frac{n-s}{s}}{\\binom{n-1}{s-1}}^{2}=\\sqrt{\\frac{s}{n-s}}\\frac{(n-1)!^{2}}{(s-1)!s!(n-s-1)!(n-s)!}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "equation", "text": "$\\sqrt{\\frac{s}{n-s}}{\\binom{n-1}{s}}^{2}=\\sqrt{\\frac{n-s}{s}}\\frac{(n-1)!^{2}}{(s-1)!s!(n-s-1)!(n-s)!},$ ", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and thus ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{n\\cdot\\left(\\sqrt{\\displaystyle{\\frac{n-s}{s}}}\\binom{n-1}{s-1}\\right)^{2}\\left(a^{s-1}(1-a)^{n-s}\\right)^{2}+\\sqrt{\\displaystyle{\\frac{s}{n-s}}}\\binom{n-1}{s}^{2}\\left(a^{s}(1-a)^{n-s-1}\\right)^{2}\\right)}}\\\\ {{=n\\cdot\\left(a^{s-1}(1-a)^{n-s-1}\\right)^{2}\\displaystyle{\\frac{(n-1)!^{2}}{(s-1)!s!(n-s-1)!(n-s)!}}\\left(\\sqrt{\\displaystyle{\\frac{s}{n-s}}}(1-a)^{2}+\\sqrt{\\displaystyle{\\frac{n-s}{s}}}a^{2}\\right)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Since ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\sqrt{\\frac{s}{n-s}}(1-a)^{2}+\\sqrt{\\frac{n-s}{s}}a^{2}\\leq\\sqrt{\\frac{s}{n-s}}+\\sqrt{\\frac{n-s}{s}}=\\frac{n}{\\sqrt{s(n-s)}},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "there is ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{n\\cdot\\left(\\sqrt{\\cfrac{n-s}{s}}\\binom{n-1}{s-1}^{2}\\left(a^{s-1}(1-a)^{n-s}\\right)^{2}+\\sqrt{\\cfrac{s}{n-s}}\\binom{n-1}{s}^{2}\\left(a^{s}(1-a)^{n-s-1}\\right)^{2}\\right)}\\\\ &{\\qquad\\qquad\\leq\\sqrt{s(n-s)}\\frac{\\left(\\binom{n}{s}a^{s}(1-a)^{n-s}\\right)^{2}}{a^{2}(1-a)^{2}}\\leq n\\cdot\\frac{\\left(\\binom{n}{s}a^{s}(1-a)^{n-s}\\right)^{2}}{a^{2}(1-a)^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Using the identity $\\begin{array}{r}{\\sum_{j=0}^{m}\\binom{m}{j}^{2}(x+y)^{2j}(x-y)^{2(m-j)}=\\sum_{j=0}^{m}\\binom{2j}{j}\\binom{2(m-j)}{m-j}x^{2j}y^{2(m-j)}}\\end{array}$ , there is ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\displaystyle\\sum_{s=2}^{n-2}\\left({\\binom{n}{s}}a^{s}(1-a)^{n-s}\\right)^{2}=\\displaystyle\\sum_{s=0}^{n}{\\binom{2s}{s}}{\\binom{2(n-s)}{n-s}}\\frac{1}{2^{2s}}\\left(\\frac{2a-1}{2}\\right)^{2(n-s)}}}\\\\ {{=\\displaystyle{\\binom{2n}{n}}\\left(\\frac{2a-1}{2}\\right)^{2n}+\\displaystyle\\sum_{s=1}^{n-1}{\\binom{2s}{s}}{\\binom{2(n-s)}{n-s}}\\frac{1}{2^{2s}}\\left(\\frac{2a-1}{2}\\right)^{2(n-s)}+\\binom{2n}{n}\\frac{1}{2^{2n}}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "For every $k\\geq1$ , $\\begin{array}{r}{\\binom{2k}{k}\\approx\\frac{2^{2k}}{\\sqrt{k}}}\\end{array}$ using the Stirling\u2019s approximation, and thus ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\binom{2n}{n}\\left(\\frac{2a-1}{2}\\right)^{2n}\\approx\\displaystyle\\frac{z^{n}}{\\sqrt{n}},\\quad\\binom{2n}{n}\\frac{1}{2^{2n}}\\approx\\frac{1}{\\sqrt{n}}}}\\\\ {{\\displaystyle\\sum_{s=1}^{n-1}\\binom{2s}{s}\\binom{2(n-s)}{n-s}\\frac{1}{2^{2s}}\\left(\\frac{2a-1}{2}\\right)^{2(n-s)}\\approx\\displaystyle\\sum_{s=1}^{n-1}\\frac{1}{\\sqrt{s(n-s)}}z^{n-s}\\leq\\frac{\\sum_{j=1}^{n-1}z^{j}}{\\sqrt{n-1}},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $z\\,=\\,(2a\\,-\\,1)^{2}\\,<\\,1$ . Therefore, we obtain $\\begin{array}{r}{\\sum_{s=2}^{n-2}\\left(\\binom{n}{s}a^{s}(1-a)^{n-s}\\right)^{2}\\le O(n^{-\\frac{1}{2}})}\\end{array}$ , which eventually leads to ", "page_idx": 17}, {"type": "equation", "text": "$$\nD(\\mathbf{m},\\mathbf{q}^{\\mathrm{0FA-A}})\\leq\\frac{n}{a^{2}(1-a)^{2}}\\sum_{s=2}^{n-2}\\left(\\binom{n}{s}a^{s}(1-a)^{n-s}\\right)^{2}\\leq O(n^{\\frac{1}{2}}).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proposition 4. For semi-values, $D(\\mathbf{m},\\mathbf{q}^{O F\\!A-S})\\in O(1)\\,\\,i f\\,i)$ $\\mu$ has a bounded probability density function or ii) (0,1)w(11\u2212w) $\\begin{array}{r}{\\int_{(0,1)}\\frac{1}{w(1-w)}\\mathrm{d}\\mu(w)<\\infty}\\end{array}$ . Particularly, this condition covers all weighted Banzhaf values and Beta Shapley values with $\\alpha,\\beta\\geq1$ . ", "page_idx": 17}, {"type": "text", "text": "Proof. If $\\mu(\\{0\\})\\neq0$ $\\mathbf{\\dot{\\rho}}\\mu(\\{1\\})\\neq0$ , respectively), its induced marginal contributions all reside in $\\phi_{i,1}^{+}$ and $\\phi_{i,0}^{-}\\;(\\phi_{i,n}^{+}$ and $\\phi_{i,n-1}^{-}$ , respectively), which is computed exactly using Algorithm 1. Therefore, W.L.O.G., we assume that $\\mu((0,1))=1$ . ", "page_idx": 17}, {"type": "text", "text": "Suffice it to show that if $\\begin{array}{r}{\\int_{0}^{1}\\frac{1}{w(1-w)}\\mathrm{d}\\mu(w)<\\infty}\\end{array}$ , there is ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\sum_{s=2}^{n-2}\\sqrt{\\frac{n}{s}m_{s}^{2}+\\frac{n}{n-s}m_{s+1}^{2}}\\in{\\cal O}(1).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Specifically, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\bf D}({\\bf m},{\\bf q}^{\\mathrm{0FA}\\cdot S})=\\sum_{s=2}^{n-2}\\sqrt{\\frac{n}{s}m_{s}^{2}+\\frac{n}{n-s}m_{s+1}^{2}}\\le\\sum_{s=2}^{n-2}\\left(\\sqrt{\\frac{n}{s}}m_{s}+\\sqrt{\\frac{n}{n-s}}m_{s+1}\\right)\\nonumber}}\\\\ {{\\displaystyle=\\int_{0}^{1}\\sum_{s=2}^{n-2}\\left(\\sqrt{\\frac{s}{n}}\\binom{n}{s}w^{s-1}(1-w)^{n-s}+\\sqrt{\\frac{n-s}{n}}\\binom{n}{s}w^{s}(1-w)^{n-s-1}\\right)\\mathrm{d}\\mu(w).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Since $\\begin{array}{r}{\\sqrt{\\frac{s}{n}}(1-w)+\\sqrt{\\frac{n-s}{n}}w\\leq2}\\end{array}$ , we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\int_{0}^{1}\\sum_{s=2}^{n-2}\\left(\\sqrt{\\frac{s}{n}}\\binom{n}{s}w^{s-1}(1-w)^{n-s}+\\sqrt{\\frac{n-s}{n}}\\binom{n}{s}w^{s}(1-w)^{n-s-1}\\right)\\mathrm{d}\\mu(w)}\\\\ {\\displaystyle\\qquad\\le\\int_{0}^{1}\\sum_{s=2}^{n-2}\\frac{2\\binom{n}{s}w^{s}(1-w)^{n-s}}{w(1-w)}\\mathrm{d}\\mu(w)\\le2\\int_{0}^{1}\\frac{1}{w(1-w)}\\mathrm{d}\\mu(w)\\in O(1).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "C Proof of Theorem 2 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "To prove this theorem, we first state useful definitions and lemmas. ", "page_idx": 17}, {"type": "text", "text": "Definition 2 (Semi Inner Product). Let $\\nu$ is a real linear space. $A$ semi inner product $\\langle\\cdot,\\cdot\\rangle$ on $\\nu$ satisfies, for every $x,y,z\\in\\nu$ and every $\\alpha\\in\\mathbb{R},$ , i) $\\langle x,y\\rangle\\,=\\,\\langle y,x\\rangle$ , ii) $\\langle\\alpha x,y\\rangle\\,=\\,\\alpha\\langle x,y\\rangle$ , iii) $\\langle x+y,z\\rangle=\\langle x,z\\rangle+\\langle y,z\\rangle$ , and $i\\nu$ ) $\\langle x,x\\rangle\\geq0$ . In addition, we write $\\|x\\|={\\sqrt{\\langle x,x\\rangle}}$ for every $x\\in\\mathcal{V}$ . ", "page_idx": 17}, {"type": "text", "text": "Lemma 1. Let a semi inner product on a linear space $\\mathcal{V}$ be given, and ${\\mathcal{A}}\\subseteq{\\mathcal{V}}$ is some affine space. For the following optimization problem ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\underset{x\\in{\\cal{A}}}{\\operatorname{argmin}}\\,\\|x-p\\|^{2}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $p\\in\\mathcal{V}$ , $x^{*}$ is optimal if and only if ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\left\\langle x^{*}-p,y-x^{*}\\right\\rangle=0,\\,\\forall y\\in{\\mathcal{A}}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof. Suppose $x^{*}$ verifies Eq. (10), for every $y\\in A$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\|y-p\\|^{2}=\\|x^{*}-p\\|^{2}+\\|y-x^{*}\\|^{2}+2\\langle x^{*}-p,y-x^{*}\\rangle\\geq\\|x^{*}-p\\|^{2}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Next, suppose $x^{*}$ is optimal, and for the sake of contradiction, assume that there is some $y\\in A$ such that $\\langle x^{\\ast}-p,y-x^{\\ast}\\rangle\\neq0$ . Write $z=y-x^{*}$ , for $t\\in\\mathbb R$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\|x^{*}+t z-p\\|^{2}=\\|x^{*}-p\\|^{2}+t^{2}\\|z\\|^{2}+2t\\langle x^{*}-p,z\\rangle.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Since $\\langle x^{*}-p,z\\rangle\\neq0.$ , there exists some $t_{o}\\in\\mathbb{R}$ such that $t^{2}\\|z\\|^{2}+2t\\langle x^{*}-p,z\\rangle<0$ , and thus $\\|x^{*}+t_{o}z-p\\|^{2}<\\|x^{*}-p\\|^{2}$ , a contradiction. \u53e3 ", "page_idx": 18}, {"type": "text", "text": "Definition 3 (Projection Induced by a Semi Inner Product). Given a semi inner product on a linear space $\\nu$ , the set of all optimal solutions to the problem ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\underset{x\\in{\\cal{A}}}{\\operatorname{argmin}}\\,\\|x-p\\|^{2},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $A\\subseteq\\ \\nu$ is an affine space and $\\textit{p}\\in\\textit{\\V}$ , is denoted by $\\mathrm{Proj}_{A}(\\{p\\})$ . To account for the possibility that there are multiple optimal solutions, we extend the definition by letting ${\\mathrm{Proj}}_{A}(S)=$ $\\begin{array}{r}{\\bigcup_{p\\in S}\\operatorname{Proj}_{A}(\\{p\\})}\\end{array}$ . ", "page_idx": 18}, {"type": "text", "text": "Lemma 2. Let $\\nu$ be a linear space with a semi inner product. Suppose there are two affine spaces $B\\subseteq A,$ , for every $p\\in\\mathcal{V}$ , there is ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname{Proj}_{\\mathcal{B}}(\\operatorname{Proj}_{\\mathcal{A}}(\\{p\\}))\\subseteq\\operatorname{Proj}_{\\mathcal{B}}(\\{p\\}).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof. We rephrase Lemma 1 to ease the proof. For each affine space ${\\mathcal{A}}\\subseteq{\\mathcal{V}}$ , define $\\mathcal{L}_{\\mathcal{A}}=\\mathcal{A}-q$ for some $q\\in{\\mathcal{A}}$ . Note that the resulting $\\mathcal{L}_{\\mathcal{A}}$ is independent of the choice of $q\\in{\\mathcal{A}}$ and it is a subspace in $\\mathcal{V}$ . Therefore, Eq. (10) is equivalent to ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\left\\langle x^{*}-p,z\\right\\rangle=0,\\,\\forall z\\in{\\mathcal{L}}_{A}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Suppose $x\\in\\mathrm{Proj}_{B}(\\mathrm{Proj}_{A}(\\{p\\}))$ , by Lemma 1, there exists $y\\in\\mathrm{Proj}_{A}(\\{p\\})$ such that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\langle y-p,a\\rangle=0,\\;\\forall a\\in\\mathcal{L}_{A}\\;\\;\\mathrm{and}\\;\\;\\langle x-y,b-x\\rangle=0,\\;\\forall b\\in\\mathcal{B}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Therefore, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\langle x-p,b-x\\rangle=\\langle x-y,b-x\\rangle+\\langle y-p,b-x\\rangle=0+0.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "$\\langle y-p,b-x\\rangle=0$ is due to that $b-x\\in\\mathcal{L}_{B}\\subseteq\\mathcal{L}_{A}$ . ", "page_idx": 18}, {"type": "text", "text": "Lemma 3 (Ruiz et al. 1998, Theorem 12). Let $\\mathbf{v}^{*}$ be the uniquely optimal solution to ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{argmin}_{{\\mathbf{v}}\\in\\mathbb{R}^{n}}\\sum_{S\\subseteq[n]}\\eta_{s+1}\\left(U(S)-U(\\emptyset)-\\sum_{i\\in S}v_{i}\\right)^{2}\\ s.t.\\ \\sum_{i\\in[n]}v_{i}=U([n])-U(\\emptyset)\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\eta_{s}=p_{s-1}+p_{s}$ for $2\\leq s\\leq n$ . Then, there is ", "page_idx": 18}, {"type": "equation", "text": "$$\nv_{i}^{*}-v_{j}^{*}=\\phi_{i}-\\phi_{j}\\;\\,f o r\\,e\\nu e r y\\;i,j\\in[n].\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Recall that the problem (6) is ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{argmin}_{\\theta\\in\\mathbb{R}^{n},b\\in\\mathbb{R}}\\sum_{S\\subseteq[n]}\\eta_{s+1}\\left(U(S)-b-\\sum_{i\\in S}\\theta_{i}\\right)^{2},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and our goal is to prove that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\theta_{i}^{*}-\\theta_{j}^{*}=v_{i}^{*}-v_{j}^{*}\\;\\;\\mathrm{for}\\;\\mathrm{every}\\;\\;i,j\\in[n],\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "which together with Lemma 3 is sufficient to complete our proof. ", "page_idx": 18}, {"type": "text", "text": "Theorem 2. Let $(b^{*},\\theta^{*})$ be the uniquely optimal solution to the problem (6) where $\\eta_{s}=p_{s-1}+p_{s}$ for $2\\leq s\\leq n$ . Then, there is ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\theta_{j}^{*}-\\theta_{k}^{*}=\\phi_{j}-\\phi_{k}\\ f o r\\,e\\nu e r y\\ j,k\\in[n].\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof. The first part of our proof was inspired by (Hammer and Holzman 1992, Lemma 2.9). Let $\\mathcal{G}\\;=\\;\\{U\\;:\\;2^{[n]}\\;\\}\\;\\rightarrow\\;\\mathbb{R}\\}$ , $\\textstyle A\\mathcal{G}\\;=\\;\\{U\\;\\in\\;\\mathcal{G}\\;\\mid\\;U(S)\\;=\\;a_{0}\\,+\\,\\sum_{i\\in S}a_{i}$ for every $S\\ \\subseteq\\ [n]\\}$ and $A_{U}=\\{g\\in A\\mathcal{G}\\mid U([n])=g([n])$ and $U(\\varnothing)=g(\\varnothing)\\}$ . Note that $\\mathcal{G}$ is a linear space and the other two are affine spaces with $A_{U}\\subseteq A{\\mathcal{G}}$ . For clarity, each game in $_{A\\mathcal{G}}$ is written as $[a_{0},\\mathbf{a}]$ where $\\mathbf{a}\\in\\mathbb{R}^{n}$ . ", "page_idx": 19}, {"type": "text", "text": "A semi inner product on $\\mathcal{G}$ can be defined by letting $\\begin{array}{r}{\\langle g_{1},g_{2}\\rangle=\\sum_{S\\subseteq[n]}\\eta_{s+1}\\cdot g_{1}(S)g_{2}(S)}\\end{array}$ for every $g_{1},g_{2}\\in\\mathcal{G}$ . Then, $[b^{*},\\theta^{*}]$ is the projection of $U$ onto $_{A\\mathcal{G}}$ , whereas $[U(\\emptyset),\\mathbf{v}^{*}]$ is the projection of $U$ onto $A_{U}$ where $\\mathbf{v}^{*}$ is the uniquely optimal solution to the problem (11). ", "page_idx": 19}, {"type": "text", "text": "By Lemma 2, there is $\\operatorname{Proj}_{A_{t r}}(\\operatorname{Proj}_{A\\mathcal{G}}(\\{U\\}))\\ \\subseteq\\ \\operatorname{Proj}_{A_{t r}}(\\{U\\})$ . Moreover, the uniqueness in problem (11) implies that $\\mathrm{Proj}_{A_{U}}(\\{U\\})=\\{[U(\\emptyset),\\mathbf{v}^{*}]\\}$ , and thus ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathrm{Proj}_{A_{U}}(\\mathrm{Proj}_{A\\mathcal{G}}(\\{U\\}))=\\mathrm{Proj}_{A_{U}}(\\{U\\})=\\{[U(\\emptyset),{\\bf v}^{*}]\\}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Since $[b^{*},\\pmb{\\theta}^{*}]\\in\\mathrm{Proj}_{A\\mathcal{G}}(\\{U\\})$ , the equality $\\operatorname{Proj}_{A_{U}}(\\left\\{[b^{*},\\pmb{\\theta}^{*}]\\right\\})=\\left\\{[U(\\emptyset),\\mathbf{v}^{*}]\\right\\}$ means that $[U_{\\varnothing},\\mathbf{v}^{*}]$ is the uniquely optimal solution to the problem ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{argmin}_{[U(\\emptyset),\\mathbf{v}]\\in A_{U}}\\sum_{S\\subseteq[n]}\\eta_{s+1}\\left([U(\\emptyset),\\mathbf{v}](S)-[b^{*},\\pmb{\\theta}^{*}](S)\\right)^{2}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Pick $i,j\\in[n]$ such that $i\\neq j$ , and define an additive game $\\mathbf{e}^{i}\\in\\mathcal{A}\\mathcal{G}$ by letting $\\mathbf{e}^{i}(S)=1$ if $i\\in S$ and 0 otherwise, $\\mathbf{e}^{j}$ is defined similarly. Consider the problem ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\underset{t\\in\\mathbb{R}}{\\mathrm{argmin}}\\sum_{S\\subseteq[n]}\\eta_{s+1}\\left([U(\\emptyset),{\\mathbf{v}}^{*}](S)-[b^{*},\\theta^{*}](S)+t({\\mathbf{e}}^{i}(S)-{\\mathbf{e}}^{j}(S))\\right)^{2}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Note that $\\left[U(\\varnothing),\\mathbf{v}^{*}\\right]+t(\\mathbf{e}^{i}-\\mathbf{e}^{j})\\in\\mathcal{A}_{U}$ for every $t\\in\\mathbb R$ , and the uniqueness to the problem (12) suggests that $t^{*}=0$ is the uniquely optimal solution to the problem (13). Removing all constant terms in the problem (13) yields an equivalent problem ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{t\\in\\mathbb{R}}{\\mathrm{argmin}}\\quad\\displaystyle\\sum_{S\\subseteq[n]:~i\\in S,j\\notin S}\\eta_{s+1}\\left([U(\\emptyset),\\mathbf{v}^{*}](S)-[b^{*},\\theta^{*}](S)+t\\right)^{2}}&{}\\\\ {\\quad}&{+\\displaystyle\\sum_{S\\subseteq[n]:~i\\notin S,j\\in S}\\eta_{s+1}\\left([U(\\emptyset),\\mathbf{v}^{*}](S)-[b^{*},\\theta^{*}](S)-t\\right)^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Write $g=[U(\\emptyset),\\mathbf{v}^{*}]-[b^{*},\\pmb{\\theta}^{*}]$ , since this problem is convex, letting the derivative equal 0 leads to ", "page_idx": 19}, {"type": "equation", "text": "$$\nt^{*}=\\frac{\\sum_{S\\subseteq[n]:\\;i\\notin S,j\\in S}\\eta_{s+1}\\cdot g(S)-\\sum_{S\\subseteq[n]:\\;i\\in S,j\\notin S}\\eta_{s+1}\\cdot g(S)}{2\\sum_{S:\\;i\\in S,j\\notin S}\\eta_{s+1}}=0.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Write $g=[g_{0},\\mathbf{g}]$ where $g_{0}=U(\\varnothing)-b^{*}$ and $\\mathbf{g}=\\mathbf{v}^{*}-\\pmb{\\theta}^{*}$ , there is ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\sum_{S\\subseteq[n]:\\ i\\in S,j\\notin S}\\eta_{s+1}\\cdot g(S)=\\alpha(g_{0}+g_{i})+\\beta\\sum_{1\\leq k\\leq n:\\ k\\neq i,j}g_{k}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Similarly, we have $\\begin{array}{r}{\\sum_{S\\subseteq[n]:~i\\notin S,j\\in S}\\eta_{s+1}\\cdot g(S)=\\alpha(g_{0}+g_{j})+\\beta\\sum_{1\\leq k\\leq n:~k\\neq i,j}g_{k};}\\end{array}$ , and therefore ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\alpha(g_{j}-g_{i})=0.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Since $\\alpha>0$ , we eventually get $g_{i}=g_{j}$ . In other words, $v_{i}^{*}-\\theta_{i}^{*}=v_{j}^{*}-\\theta_{j}^{*}$ . Because $i$ and $j$ are chosen arbitrarily, our proof is completed. ", "page_idx": 19}, {"type": "text", "text": "To be self-contained, we also prove that the problem (6) has only one optimal solution provided that $\\eta_{s}=p_{s-1}+p_{s}$ for $2\\leq s\\leq n$ . W.L.O.G., assume $\\begin{array}{r}{\\sum_{S\\subseteq[n]}\\eta_{s+1}=1}\\end{array}$ . By letting the derivative of the ", "page_idx": 19}, {"type": "text", "text": "problem (6) equal 0, we have $\\mathbf{Ax}=\\mathbf{b}$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbf{A}=\\left(\\begin{array}{l l l l l}{1}&{\\kappa}&{\\kappa}&{\\cdots}&{\\kappa}\\\\ {\\kappa}&{\\kappa}&{\\tau}&{\\cdots}&{\\tau}\\\\ {\\kappa}&{\\tau}&{\\kappa}&{\\ddots}&{\\vdots}\\\\ {\\vdots}&{\\vdots}&{\\ddots}&{\\ddots}&{\\tau}\\\\ {\\kappa}&{\\tau}&{\\cdots}&{\\tau}&{\\kappa}\\end{array}\\right),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "equation", "text": "$$\n\\kappa=\\sum_{s=1}^{n}{\\binom{n-1}{s-1}}\\eta_{s+1},\\quad\\tau=\\sum_{s=2}^{n}{\\binom{n-2}{s-2}}\\eta_{s+1},\\quad b_{1}=\\sum_{S\\subseteq[n]}\\eta_{s+1}U(S),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "$b_{j+1}=\\sum_{S\\subseteq[n]:\\ j\\in S}\\eta_{s+1}U(S)$ for every $j\\in[n],\\quad x_{1}=b$ and $x_{j+1}=\\theta_{j}$ for every $j\\in[n]$ . ", "page_idx": 20}, {"type": "text", "text": "Left multiplying $\\mathbf{A}$ with some row operation matrix $\\mathbf{R}$ gives ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbf{RA}=\\left(\\begin{array}{c c c c c}{1}&{\\kappa}&{\\kappa}&{\\cdots}&{\\kappa}\\\\ {0}&{\\kappa-\\kappa^{2}}&{\\tau-\\kappa^{2}}&{\\cdots}&{\\tau-\\kappa^{2}}\\\\ {0}&{\\tau-\\kappa^{2}}&{\\kappa-\\kappa^{2}}&{\\ddots}&{\\vdots}\\\\ {\\vdots}&{\\vdots}&{\\ddots}&{\\ddots}&{\\ddots}\\\\ {0}&{\\tau-\\kappa^{2}}&{\\cdots}&{\\tau-\\kappa^{2}}&{\\kappa-\\kappa^{2}}\\end{array}\\right).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "It is sufficient to prove that the bottom-right $n\\times n$ submatrix of RA is invertible. Suffice it to show $\\kappa-\\tau\\neq0$ and $\\kappa+(n-1)\\tau-n\\kappa^{2}\\neq0$ . Using $\\textstyle{\\binom{n}{s}}={\\binom{n-1}{s}}+{\\binom{n-1}{s-1}}$ , we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\kappa-\\tau=\\sum_{s=1}^{n-1}{\\binom{n-2}{s-1}}\\eta_{s+1}>0.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Using $n{\\binom{n-1}{s-1}}=s{\\binom{n}{s}}$ , we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\kappa+(n-1)\\tau=\\sum_{s=1}^{n}s\\binom{n-1}{s-1}\\eta_{s+1}=\\frac{1}{n}\\sum_{s=1}^{n}s^{2}\\binom{n}{s}\\eta_{s+1},}\\\\ {\\displaystyle n\\cdot\\kappa^{2}=n\\cdot\\left(\\sum_{s=1}^{n}\\binom{n-1}{s-1}\\eta_{s+1}\\right)^{2}=\\frac{1}{n}\\left(\\sum_{s=1}^{n}s\\binom{n}{s}\\eta_{s+1}\\right)^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Let $\\gamma=1-\\eta_{1}$ and $\\zeta_{s}=\\eta_{s+1}/\\gamma$ for every $s\\in[n]$ , there is ", "page_idx": 20}, {"type": "equation", "text": "$$\nn\\cdot\\kappa^{2}=\\frac{\\gamma^{2}}{n}\\left(\\sum_{s=1}^{n}s\\binom{n}{s}\\zeta_{s}\\right)^{2}=\\frac{\\gamma^{2}}{n}\\mathbb{E}[s]^{2}\\leq\\frac{\\gamma^{2}}{n}\\mathbb{E}[s^{2}]=\\gamma(\\kappa+(n-1)\\tau)\\leq\\kappa+(n-1)\\tau.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "If $\\eta_{1}>0$ , the last inequality is strict as $\\gamma<1$ . Otherwise, the first inequality is strict as $\\mathrm{Var}[s]=$ $\\mathbb{E}[\\mathring{s}^{2}]-\\mathbb{E}[s]^{2}>0$ . ", "page_idx": 20}, {"type": "text", "text": "D Overview of Estimators ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Recall that each probabilistic value is defined to be, for every $i\\in[n]$ , ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\phi_{i}=\\phi_{i}(U)=\\sum_{S\\subseteq[n]\\setminus i}p_{s+1}(U(S\\cup i)-U(S))\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\mathbf{p}\\in\\mathbb{R}^{n}$ is a non-negative vector with $\\begin{array}{r}{\\sum_{s=1}^{n}{\\binom{n-1}{s-1}}p_{s}=1.}\\end{array}$ . If $\\begin{array}{r}{p_{s}=\\int_{0}^{1}w^{s-1}(1-w)^{n-s}\\mathrm{d}\\mu(w)}\\end{array}$ for some probability measure $\\mu$ on the closed interval $[0,1]$ , the induced $\\phi$ is referred to as a semi-value. ", "page_idx": 20}, {"type": "text", "text": "The Sampling Lift Estimator (Moehle et al. 2022) The sampling lift estimator is based on ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "The sampling procedure is: i) sample a subset size $s~\\in~[n]$ with $\\begin{array}{r}{P(s)~=~\\binom{n-1}{s-1}p_{s}}\\end{array}$ , and then ii) sample a subset $S$ uniformly from $\\{R\\ \\subseteq\\ [n]\\backslash i\\ \\mid\\ r\\ =\\ s\\,-\\,1\\}$ . For semi-values such that $\\begin{array}{r}{p_{s}=\\int_{0}^{\\bar{1}}w^{s-1}(1-w)^{n-s}\\mathrm{d}\\mu(w)}\\end{array}$ where $\\mu$ is a probability measure on the closed interval $[0,1]$ , there is an alternative: i) sample a $w\\in[0,1]$ according to $\\mu$ , and then sample a subset $S\\subseteq[n]\\backslash i$ by incorporating each player in $[n]\\backslash i$ with probability $w$ . With a sequence of sampled subsets $\\bar{\\{S_{j}\\}}_{j=1}^{T}$ , the $i$ -th estimate is $\\begin{array}{r}{\\hat{\\phi}_{i}=\\frac{1}{T}\\sum_{j=1}^{T}(U(S_{j}\\cup i)-U(S_{j}))}\\end{array}$ . ", "page_idx": 21}, {"type": "text", "text": "The Weighted Sampling Lift Estimator (Kwon and Zou 2022a) The formula it is built upon is ", "text_level": 1, "page_idx": 21}, {"type": "equation", "text": "$$\n\\phi_{i}=\\mathbb{E}_{S\\subseteq[n]\\setminus i}^{\\mathrm{Shap}}\\left[\\frac{p_{s+1}}{q_{s+1}}(U(S\\cup i)-U(S))\\right]\\mathrm{\\\\where\\\\}P(S)=q_{s+1}=\\frac{s!(n-1-s)!}{n!}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Note that setting $\\mathbf p=\\mathbf q$ in Eq. (14) leads to the Shapley value. The sampling procedure is: i) sample a $w$ uniformly from $[0,1]$ , and then ii) sample a subset $S\\subseteq[n]\\backslash i$ by incorporating each player in [n]\\i with probability w. Then, the i-th estimate is \u03d5\u02c6i = T1 jT=1qssjj  ++11 $\\begin{array}{r}{\\hat{\\phi}_{i}=\\frac{1}{T}\\sum_{j=1}^{T}\\frac{p_{s_{j}+1}}{q_{s_{j}+1}}\\left(U(S_{j}\\cup i)-U(S_{j})\\right)}\\end{array}$ ", "page_idx": 21}, {"type": "text", "text": "The KernelSHAP Estimator (Lundberg and Lee 2017) This estimator is specific to the Shapley value. It employs the fact that the Shapley value \u03d5iShapis the uniquely optimal solution to ", "text_level": 1, "page_idx": 21}, {"type": "equation", "text": "$$\n\\underset{\\phi\\in\\mathbb{R}^{n}}{\\operatorname{argmin}}\\,\\sum_{\\emptyset\\subseteq S\\subseteq[n]}\\binom{n-2}{s-1}^{-1}\\left(U(S)-U(\\emptyset)-\\sum_{i\\in S}\\phi_{i}\\right)^{2}\\;\\textnormal{s.t}\\;\\sum_{i\\in[n]}\\phi_{i}=U([n])-U(\\emptyset).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Note that the weights can be scaled so that the objective is an expectation. A sequence of subsets $\\{S_{j}\\}_{j=1}^{T}$ where $\\emptyset\\ \\subseteq\\ S_{j}\\ \\subsetneq\\ [n]$ is sampled according to $P(S)\\;\\stackrel{^{.}}{\\propto}\\;\\binom{n-2}{s-1}^{-1}$ . Then, we have an approximate problem as ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname*{argmin}_{\\phi\\in\\mathbb{R}^{n}}\\frac{1}{T}\\sum_{j=1}^{T}\\left(U(S_{j})-U(\\emptyset)-\\sum_{i\\in S_{j}}\\phi_{i}\\right)^{2}\\;\\mathrm{~s.t.~}\\sum_{i\\in[n]}\\phi_{i}=U([n])-U(\\emptyset),\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "the uniquely optimal solution of which is treated as the estimates, i.e., ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{\\phi}^{\\mathrm{Shap}}=\\hat{\\bf A}^{-1}\\left(\\hat{\\bf b}-{\\bf1}_{n}\\frac{{\\bf1}_{n}^{\\top}\\hat{\\bf A}^{-1}\\hat{\\bf b}-U([n])+U(\\emptyset)}{{\\bf1}_{n}^{\\top}\\hat{\\bf A}^{-1}{\\bf1}_{n}}\\right)\\quad\\quad}\\\\ {\\mathrm{where~}\\hat{\\bf A}=\\displaystyle\\frac{1}{T}\\sum_{j=1}^{T}{\\bf1}_{S_{j}}{\\bf1}_{S_{j}}^{\\top}\\,\\mathrm{~and~}\\hat{\\bf b}=\\displaystyle\\frac{1}{T}\\sum_{j=1}^{T}(U(S_{j})-U(\\emptyset))\\cdot{\\bf1}_{S_{j}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Specifically, $\\mathbf{1}_{S_{j}}\\in\\{0,1\\}^{n}$ such that its $i$ -th entry is 1 if and only if $i\\in S_{j}$ . ", "page_idx": 21}, {"type": "text", "text": "The Unbiased KernelSHAP Estimator (Covert and Lee 2021) The uniquely optimal solution $\\phi^{\\mathrm{Shap}}$ to the problem (15) is ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\phi^{\\mathrm{Shap}}=\\mathbf{A}^{-1}\\left(\\mathbf{b}-\\mathbf{1}_{n}\\frac{\\mathbf{1}_{n}^{\\top}\\mathbf{A}^{-1}\\mathbf{b}-U([n])+U(\\emptyset)}{\\mathbf{1}_{n}^{\\top}\\mathbf{A}^{-1}\\mathbf{1}_{n}}\\right)}\\\\ &{\\mathrm{where~}\\;\\mathbf{A}=\\mathbb{E}[\\mathbf{1}_{S}\\mathbf{1}_{S}^{\\top}]\\;\\;\\mathrm{and}\\;\\mathbf{b}=\\mathbb{E}[(U(S)-U(\\emptyset))\\cdot\\mathbf{1}_{n}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "This estimator employs the fact that Aij = 12 if i = j andn(n1\u22121)  nsn\u2212=\u2212121sn\u22121\u22121s otherwise. In other words, the estimates of this estimator is ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\hat{\\phi}^{\\mathrm{Shap}}=\\mathbf{A}^{-1}\\left(\\hat{\\mathbf{b}}-\\mathbf{1}_{n}\\frac{\\mathbf{1}_{n}^{\\top}\\mathbf{A}^{-1}\\hat{\\mathbf{b}}-U([n])+U(\\varnothing)}{\\mathbf{1}_{n}^{\\top}\\mathbf{A}^{-1}\\mathbf{1}_{n}}\\right)\\ \\mathrm{~where~}\\hat{\\mathbf{b}}=\\frac{1}{T}\\sum_{j=1}^{T}(U(S_{j})-U(\\varnothing))\\cdot\\mathbf{1}_{S_{j}}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Particularly $\\{S_{j}\\}_{j=1}^{T}$ where $\\varnothing\\subsetneq S_{j}\\subsetneq[n]$ are sampled using $P(S)\\propto\\left(_{s-1}^{n-2}\\right)^{-1}$ . Recently, Fumagalli et al. (2024) proved that Eq. (16) can be simplified as ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\hat{\\phi}_{i}^{\\mathrm{Shap}}=\\frac{U([n])-U(\\emptyset)}{n}+\\frac{2\\sum_{s=1}^{n-1}\\frac{1}{s}}{T}\\sum_{j=1}^{T}U(S_{j})\\left(\\mathbb{1}_{i\\in S_{j}}-\\frac{s_{j}}{n}.\\right)\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "The ARM Estimator (Kolpaczki et al. 2024) This estimator is designed according to ", "text_level": 1, "page_idx": 22}, {"type": "equation", "text": "$$\n\\phi_{i}=\\mathbb{E}_{S\\sim P^{+}|i\\in S}[U(S)]-\\mathbb{E}_{S\\sim P^{-}|i\\notin S}[U(S)]\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $P^{+}(S)\\,\\propto\\,p_{s}$ for every $\\varnothing\\ \\subsetneq\\ S\\ \\subseteq\\ [n]$ and $\\underline{{P}}^{-}(S)\\,\\propto\\,p_{s+1}$ for every $S\\subsetneq[n]$ (Li and Yu 2024, Proposition 8). A sequence of subsets $\\{S_{j}\\}_{j=1}^{T}$ are sampled using $P^{+}$ and $P^{-}$ alternatively, i.e., $\\{S_{2k-1}\\}_{k=1}^{\\frac{T}{2}}$ are sampled independently according to $P^{+}$ , whereas $\\{S_{2k}\\}_{k=1}^{\\frac{T}{2}}$ are sampled independently using $P^{-}$ . Then, the -th estimate is ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\hat{\\phi}_{i}=\\frac{1}{T_{i}^{+}}\\sum_{k=1}^{\\frac{T}{2}}U(S_{2k-1})\\mathbb{1}_{i\\in S_{2k-1}}-\\frac{1}{T_{i}^{-}}\\sum_{k=1}^{\\frac{T}{2}}U(S_{2k})\\mathbb{1}_{i\\notin S_{2k}}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $\\begin{array}{r}{T_{i}^{+}=\\sum_{k=1}^{\\frac{T}{2}}\\mathbb{1}_{i\\in S_{2k-1}}}\\end{array}$ and $\\begin{array}{r}{T_{i}^{-}=\\sum_{k=1}^{\\frac{T}{2}}\\mathbb{1}_{i\\notin S_{2k}}}\\end{array}$ . ", "page_idx": 22}, {"type": "text", "text": "The AME Estimator (Lin et al. 2022) This estimator is restricted to a sub-family of semi-values that satisfy $\\begin{array}{r}{\\int_{0}^{1}\\frac{1}{w(1-w)}\\mathrm{d}\\mu(w)<\\infty}\\end{array}$ . For such a semi-value $\\phi$ , it can be cast as a uniquely optimal solution to ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\underset{\\mathbf{v}\\in\\mathbb{R}^{n}}{\\mathrm{argmin}}\\,\\mathbb{E}[(Y-{X^{\\top}}\\mathbf{v})^{2}]}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $X\\in\\mathbb{R}^{n}$ and $Y$ are random variables. The sampling procedure is: i) sample a $w\\in(0,1)$ using $\\mu$ , ii) sample a subset $S$ by incorporating each player with probability $w$ , and then iii) $Y=U(S)$ and $\\boldsymbol{X}=\\boldsymbol{X}(\\boldsymbol{S})$ such that $\\textstyle X_{i}\\,=\\,{\\frac{1}{w\\cdot C}}$ if $i\\in S$ and $-{\\frac{1}{(1-w)C}}$ otherwise where $\\begin{array}{r}{C=\\int_{0}^{1}\\frac{1}{w(1-w)}\\mathrm{d}\\mu(w)}\\end{array}$ With a sequence of subsets $\\{(w_{j},S_{j})\\}_{j=1}^{T}$ , the uniquely optimal solution to the approximate problem ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\underset{\\mathbf{v}\\in\\mathbb{R}^{n}}{\\operatorname{argmin}}\\,\\frac{1}{T}\\sum_{j=1}^{T}\\left(U(S_{j})-X(S_{j})^{\\top}\\mathbf{v}\\right)^{2}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "is taken as the induced estimates, which is ${\\hat{\\phi}}=(\\mathbf{A}^{\\top}\\mathbf{A})^{-1}\\mathbf{A}^{\\top}\\mathbf{b}$ where the $j$ -th row of $\\mathbf{A}$ is $X(S_{j})^{\\top}$ and $b_{j}=U(S_{j})$ . ", "page_idx": 22}, {"type": "text", "text": "One Way to Improve the AME Estimator The limitation of the AME estimator is that it only applies to semi-values that satisfy $\\begin{array}{r}{\\int_{0}^{1}\\frac{1}{w(1-w)}\\mathrm{d}\\mu(w)<\\infty}\\end{array}$ . Meanwhile, another potential drawback is its need to compute the inverse of $\\mathbf{A}^{\\top}\\mathbf{A}$ , though it can be circumvented by solving the approximate problem using gradients. In this work, we make a small improvement to the AME estimator by extending its applicability to all semi-values, removing $(\\mathbf{A}^{\\top}\\mathbf{\\dot{A}})^{-1}$ in the approximate formula and providing a more direct analysis of its convergence rate in terms of $(\\epsilon,\\delta)$ -approximation. ", "page_idx": 22}, {"type": "text", "text": "Our improvement begins with the observation that $\\mathbb{E}[X X^{\\top}]=\\mathbf{I}$ , suggesting that $\\left(\\frac{1}{T}\\mathbf{A}^{\\top}\\mathbf{A}\\right)^{-1}\\rightarrow\\mathbf{I}$ by the law of large numbers and thus $(\\mathbf{A}^{\\top}\\mathbf{A})^{-1}$ is redundant. Its removal leads to a simplified formula: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\hat{\\phi}_{i}=\\frac{1}{T}\\sum_{j=1}^{T}\\left(\\frac{\\left[i\\in S_{j}\\right]}{w_{j}}U(S_{j})-\\frac{\\left[i\\notin S_{j}\\right]}{1-w_{j}}U(S_{j})\\right).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We comment that the following proposition is complementary to (Lin et al. 2022, Proposition 3.3) that claims a similar result. ", "page_idx": 22}, {"type": "text", "text": "Proposition 6. Assume that $i$ ) $\\|U\\|_{\\infty}\\leq u$ and $i i$ ) $\\mu([A,B])=1$ for some $0<A<B<1,$ , the improved AME estimator requires 2nu\u03f522C2 $\\begin{array}{r}{\\frac{2n u^{2}C^{2}}{\\epsilon^{2}}\\log{\\frac{2n}{\\delta}}}\\end{array}$ utility evaluations of $U$ to achieve $P(\\|\\hat{\\phi}-\\phi\\|_{2}\\ge$ $\\epsilon)\\leq\\delta$ where $\\begin{array}{r}{C=\\frac{1}{\\operatorname*{min}(A,1-B)}}\\end{array}$ . ", "page_idx": 22}, {"type": "text", "text": "Proof. Notice that $\\begin{array}{r}{\\hat{\\phi}_{i}=\\frac{1}{T}\\sum_{j=1}^{T}Z_{j}}\\end{array}$ where $\\{Z_{j}\\}_{j=1}^{T}$ are i.i.d. random variables with $\\mathbb{E}[Z_{j}]=\\phi_{i}$ . By the Hoeffding\u2019s inequality, there is ", "page_idx": 23}, {"type": "equation", "text": "$$\nP(|\\hat{\\phi}_{i}-\\phi_{i}|\\geq\\epsilon)\\leq2\\exp\\left(-\\frac{T\\epsilon^{2}}{2u^{2}C^{2}}\\right)\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $\\begin{array}{r}{C=\\frac{1}{\\operatorname*{min}(A,1-B)}}\\end{array}$ min(A,1\u2212B). Then, ", "page_idx": 23}, {"type": "equation", "text": "$$\nP(\\|\\hat{\\phi}-\\phi\\|_{2}\\ge\\epsilon)\\le P(\\bigcup_{1\\le i\\le n}|\\hat{\\phi}_{i}-\\phi_{i}|\\ge\\frac{\\epsilon}{\\sqrt{n}})\\le2n\\exp\\left(-\\frac{T\\epsilon^{2}}{2n u^{2}C^{2}}\\right).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "$\\begin{array}{r}{2n\\exp\\left(-\\frac{T\\epsilon^{2}}{2n u^{2}C^{2}}\\right)\\leq\\delta}\\end{array}$ leads to $\\begin{array}{r}{T\\ge\\frac{2n u^{2}C^{2}}{\\epsilon^{2}}\\log\\frac{2n}{\\delta}}\\end{array}$ . ", "page_idx": 23}, {"type": "text", "text": "Remark 1. Notice that the improved AME estimator requires that $\\mu(\\{0,1\\})\\,=\\,0$ . Nevertheless, semi-values are additively decomposable on $\\mu$ and the part related to $\\mu(\\{0,1\\})$ can be computed exactly in linear time. Therefore, it is fair to conclude that the improved AME estimator applies to all semi-values. ", "page_idx": 23}, {"type": "text", "text": "The MSR Estimator (Wang and Jia 2023b) The methodology of this estimator is limited to weighted Banzhaf values parameterized with $0\\,<\\,a\\,<\\,1$ (Wang and Jia 2023b, Appendix C.2). Precisely, $p_{s}=a^{s-1}(1\\!-\\!a)^{\\flat-s}$ . Each subset is sampled by incorporating each player with probability $a$ , and then the $i$ -th estimate is ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\hat{\\phi}_{i}=\\frac{1}{T_{i}^{+}}\\sum_{j=1}^{T}U(S_{j})\\mathbb{1}_{i\\in S_{j}}-\\frac{1}{T_{i}^{-}}\\sum_{j=1}^{T}U(S_{j})\\mathbb{1}_{i\\notin S_{j}}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $\\begin{array}{r}{T_{i}^{+}=\\sum_{j=1}^{T}\\mathbb{1}_{i\\in S_{j}}}\\end{array}$ and $\\begin{array}{r}{T_{i}^{-}=\\sum_{j=1}^{T}\\mathbb{1}_{i\\notin S_{j}}}\\end{array}$ . ", "page_idx": 23}, {"type": "text", "text": "The GELS Estimator (Li and Yu 2024) This estimator is established using the fact that $\\phi_{i}=$ $v_{i}^{*}-v_{n+1}^{*}$ where $\\mathbf{v}^{\\ast}\\in\\mathbb{R}^{n+1}$ is the uniquely optimal solution to ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\operatorname*{argmin}_{\\mathbf{v}\\in\\mathbb{R}^{n+1}}\\sum_{\\emptyset\\subseteq S\\subseteq[n+1]}p_{s}\\left(U(S\\cap[n])-\\sum_{i\\in S}v_{i}\\right)^{2}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "The subsets $\\{S_{j}\\}_{j=1}^{T}$ where $\\emptyset\\,\\subsetneq\\,S_{j}\\;\\subsetneq\\;[n+1]$ are sampled using $P(S)\\propto p_{s}$ , and then the $i$ -th estimate is ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\hat{\\phi}_{i}=\\Bigg(\\sum_{s=1}^{n}\\binom{n}{s-1}p_{s}\\Bigg)\\left(\\hat{v}_{i}-\\hat{v}_{n+1}\\right)\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $\\begin{array}{r}{\\hat{v}_{k}=\\frac{1}{T_{k}}\\sum_{j=1}^{T}U(S_{j}\\cap[n])\\mathbb{1}_{k\\in S_{j}}}\\end{array}$ and $\\begin{array}{r}{T_{k}=\\sum_{j=1}^{T}\\mathbb{1}_{k\\in S_{j}}}\\end{array}$ . ", "page_idx": 23}, {"type": "text", "text": "The Complement Estimator (Zhang et al. 2023) The complement estimator is specific to the Shapley value using the fact that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\phi_{i}^{\\mathrm{Shap}}=\\frac{1}{n}\\sum_{S\\subseteq[n]\\setminus i}\\binom{n-1}{s}^{-1}\\left(U(S\\cup i)-U([n]\\setminus(S\\cup i))\\right).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "The sequence of subsets $\\{S_{j}\\}_{j=1}^{T}$ is sampled using i) sample a subset size $s\\in[n]$ uniformly, and then sample a subset $S$ uniformly from $\\{R\\subseteq[n]\\mid r=s\\}$ . Then, the $i$ -th estimate is ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\hat{\\phi}_{i}^{\\mathrm{Shap}}=\\frac{1}{n}\\sum_{s=1}^{n}\\hat{\\phi}_{i,s}\\;\\;\\mathrm{where}\\;\\;\\hat{\\phi}_{i,s}=\\frac{1}{T_{i,s}}\\sum_{j=1}^{n}\\left(v_{j}\\mathbb{I}{i\\in S_{j},s_{j}=s}\\mathbb{I}-v_{j}\\mathbb{I}{i\\notin S_{j},n-s_{j}=s}\\mathbb{I}\\right)\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "The Group Testing Estimator (Jia et al. 2019) We introduce the improved version presented by Wang and Jia (2023a). Note that this estimator is specific to the Shapley value. A sequence of subsets $\\{S_{j}\\}_{j=1}^{T}$ are independently sampled according to: i) sample a subset size $s\\in[n]$ using $\\begin{array}{r}{P(s)\\propto\\frac{1}{s(n+1-s)}}\\end{array}$ , and then ii) sample a subset $S$ uniformly from $\\{R\\subseteq[n+1]\\mid r=s\\}$ . Then, the $i$ -th estimate is ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\hat{\\phi}_{i}^{\\mathrm{Shap}}=\\frac{2\\sum_{s=1}^{n}\\frac{1}{s}}{T}\\sum_{j=1}^{T}U(S_{j}\\cap[n])\\left(\\left[i\\in S_{j},n+1\\not\\in S_{j}\\right]-\\left[i\\not\\in S_{j},n+1\\in S_{j}\\right]\\right).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "The Permutation Estimator (Castro et al. 2009) This estimator is specific to the Shapley value, using the formula ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\phi_{i}^{\\mathrm{Shap}}=\\frac{1}{n!}\\sum_{\\pi\\in\\Pi}(U(\\mathcal{P}^{i}(\\pi)\\cup i)-U(\\mathcal{P}^{i}(\\pi)))\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $\\Pi$ contains all permutations of $[n]$ and $\\mathcal{P}^{i}(\\pi)$ is the subset that contains all players preceding $i$ in $\\pi$ . Thus, it samples a sequence of permutations $\\{\\pi_{j}\\}_{j=1}^{T}$ from $\\Pi$ uniformly with replacement, and then the $i$ -th estimate is $\\begin{array}{r}{\\hat{\\phi}_{i}^{\\mathrm{Shap}}=\\frac{1}{T}\\sum_{j=1}^{T}\\left(U(\\mathcal{P}^{i}(\\pi_{j})\\cup\\dot{i})-U(\\mathcal{P}^{i}(\\pi_{j}))\\right)}\\end{array}$ . ", "page_idx": 24}, {"type": "text", "text": "The WeightedSHAP Estimator (Kwon and Zou 2022b) As mentioned in the main paper, it is based on ", "text_level": 1, "page_idx": 24}, {"type": "equation", "text": "$$\n\\phi_{i}=\\sum_{s=1}^{n}m_{s}\\cdot\\mathbb{E}_{R\\subseteq[n]\\setminus i}[U(R\\cup i)-U(R)]\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $\\begin{array}{r c l}{m_{s}}&{=}&{\\binom{n-1}{s-1}p_{s}}\\end{array}$ . For each player $\\textit{i}\\in\\ [n]$ , it samples a sequence of permutations $\\{\\pi_{j}\\}_{j=1}^{T}$ of $[n]\\backslash i$ . Then, the corresponding estimate is $\\begin{array}{c l l}{\\hat{\\phi}_{i}}&{=}&{\\sum_{s=1}^{n}m_{s}\\hat{\\phi}_{i,s}}\\end{array}$ where $\\begin{array}{r l}{\\hat{\\phi}_{i,k}}&{{}=}\\end{array}$ $\\begin{array}{r}{\\frac{1}{T}\\sum_{j=1}^{T}\\left(U(\\ensuremath{\\mathcal{S}}^{k}(\\pi_{j})\\cup i)-U(\\ensuremath{\\mathcal{S}}^{k}(\\pi_{j}))\\right)}\\end{array}$ and $S^{k}(\\pi_{j})$ is the subset that contains the first $k-1$ players in \u03c0j. ", "page_idx": 24}, {"type": "text", "text": "The SHAP-IQ Estimator (Fumagalli et al. 2024) Recall that its underlying formula is ", "text_level": 1, "page_idx": 24}, {"type": "equation", "text": "$$\n\\phi_{i}=p_{n}\\cdot(U([n])-U(\\emptyset))+2H\\cdot\\mathbb{E}_{\\emptyset\\subset{S\\subseteq[n]}}[((n-s)m_{s}\\mathbb{I}_{i\\in S}-s m_{s+1}\\mathbb{I}_{i\\not\\in S})\\cdot(U(S)-U(\\emptyset))]\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $m_{s}\\,=\\,{\\binom{n-1}{s-1}}p_{s}$ , $\\begin{array}{r}{H\\,=\\,\\sum_{j=1}^{n-1}\\frac{1}{j}}\\end{array}$ , and $P(S)\\,\\propto\\,{\\binom{n-2}{s-1}}^{-1}$ . Therefore, a sequence of subsets $\\{S_{j}\\}_{j=1}^{T}$ where $\\varnothing\\subsetneq S_{j}\\subsetneq[n]$ is sampled using $P(S)\\propto\\binom{n-2}{s-1}^{-1}$ , and the $i$ -th estimate is ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\hat{\\phi}_{i}=p_{n}\\cdot(U([n])-U(\\varnothing))+\\frac{2H}{T}\\sum_{j=1}^{T}(U(S_{j})-U(\\varnothing))\\cdot\\left((n-s)m_{s}\\mathbb{I}_{i\\in S_{j}}-s m_{s+1}\\mathbb{I}_{i\\notin S_{j}}\\right).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: Our claimed theories are presented in Section 4 and are empirically verified in Section 5. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 25}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: Proposition 3 demonstrates that our OFA-A estimator does not rival the previously best estimator for weighted Banzhaf values in terms of convergence rate, which is a price to pay for using a fixed sampling scheme for all probabilistic values. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 25}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We have provided detailed proofs in the Appendices A, B and C. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 26}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: Our experiment settings are stated in Section 5, and our method is presented in Algorithm 1. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 26}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: The datasets we used are from open resources, and our code will be released on a github repo. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 27}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: Our experiment settings are stated in Section 5. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 27}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: Our experiment results in Section 5 are all reported with standard deviation using 30 random seeds. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: It is stated in Section 5. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 28}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We have complied with the NeurIPS Code of Ethics. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 28}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: Our work focuses on the convergence rate of estimators for probabilistic values that do not appear to have any significant societal impact. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 28}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 29}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: Our work focuses on the convergence rate of estimators for probabilistic values that do not appear to pose any risk for misuse. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 29}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: It is stated in Section 5. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: We do not introduce any new assets. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 30}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: Our work does not involve human subjects. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 30}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: Our work does not involve human subjects. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 31}]