[{"figure_path": "ceIO1w0PmT/tables/tables_6_1.jpg", "caption": "Table 1: Evaluation results (rewards) on short-horizon atom tasks. The text-conditioned VPT [2] (\"VPT (text)*\") is from Appendix I of its paper.", "description": "This table presents the performance comparison of different agents on four simple Minecraft tasks: chopping trees, mining stones, digging dirt, and collecting wheat seeds.  The \"rewards\" represent the success of each agent in completing these tasks. The table includes results for text-conditioned VPT, STEVE-I, GROOT, and OmniJARVIS, indicating their performance in completing the tasks based on the type of conditioning (language or video) used during training.", "section": "4.2 Main Results I: Short-horizon Atomic Tasks"}, {"figure_path": "ceIO1w0PmT/tables/tables_6_2.jpg", "caption": "Table 4: Success rate of different agents on long-horizon programmatic tasks.", "description": "This table presents the success rates of various agents on 30 long-horizon programmatic Minecraft tasks.  These tasks range in difficulty and require a chain of actions to complete, testing the agents' planning and execution capabilities. The agents are categorized by their action tokenization method: Native (directly producing actions), or Language (using language as an intermediate step). The results are broken down by task category (wooden, food, stone, iron, diamond), showing the average success rate for each agent across all tasks within each category. The table highlights the superior performance of OmniJARVIS, which uses a behavior tokenizer, compared to baselines that rely on native actions or language-based planning.", "section": "4.3 Main Results II: Long-horizon Programmatic Tasks"}, {"figure_path": "ceIO1w0PmT/tables/tables_7_1.jpg", "caption": "Table 4: Ablation experiments on OmniJARVIS with different behavior tokenizers, vision tokenizers, and training on different interactive datasets. The first line is training on the unconditional interactive dataset, i.e., without instructions on the trajectories. OmniJARVIS with VQ-GROOT [42, 10] shows no results because of training collapse.", "description": "This table presents the ablation study results of OmniJARVIS. The experiment investigates the impact of different behavior tokenizers (FSQ GROOT, GROOT, VQ GROOT), vision tokenizers (LLaVA, Captioner+, FUYU), and dataset formats (with/without instructions, captions, thoughts, and memory) on the model's performance. The loss (both training and evaluation) is used as a metric to evaluate the model's performance under different configurations. The result shows that using FSQ GROOT as the behavior tokenizer and LLaVA as the vision tokenizer, with all components in the dataset, achieves the best performance.", "section": "4.5 Insights and Analysis"}, {"figure_path": "ceIO1w0PmT/tables/tables_7_2.jpg", "caption": "Table 5: Ablation experiments on behavior tokenizer with different code vocabulary size.", "description": "This table presents the ablation study results on the behavior tokenizer using various codebook sizes (e8, e10, e14).  It shows the impact of different codebook configurations on training loss, evaluation loss, reconstruction Fr\u00e9chet Sequence Distance (FSD), sampling FSD and average rewards. The results demonstrate that increasing codebook size generally enhances the performance, but there might be some diminishing returns after a certain size.", "section": "4.5 Insights and Analysis"}, {"figure_path": "ceIO1w0PmT/tables/tables_13_1.jpg", "caption": "Table 4: Success rate of different agents on long-horizon programmatic tasks.", "description": "This table presents the success rates of various agents in completing long-horizon programmatic tasks in Minecraft.  These tasks require a chain of actions to obtain a final item, starting from an empty inventory. The tasks are categorized into five groups (Wooden, Food, Stone, Iron, Diamond) based on their difficulty.  The table compares the performance of OmniJARVIS (using both its own behavior tokenizer and a baseline FSQ GROOT tokenizer), and several other agents using either native behavior tokenizers or Language-based planners. The results show that OmniJARVIS significantly outperforms the other agents, highlighting its ability to effectively perform long-horizon planning and execution.", "section": "4.3 Main Results II: Long-horizon Programmatic Tasks"}, {"figure_path": "ceIO1w0PmT/tables/tables_14_1.jpg", "caption": "Table 1: Evaluation results (rewards) on short-horizon atom tasks. The text-conditioned VPT [2] (\"VPT (text)*\") is from Appendix I of its paper.", "description": "This table presents the evaluation results of different agents on four short-horizon atomic tasks in Minecraft.  The tasks are basic yet fundamental skills like chopping trees, mining stones, digging dirt, and collecting wheat seeds. The table compares OmniJARVIS's performance against several baselines (text-conditioned VPT, Open-world Control, STEVE-I, and video-instructed GROOT), showing the average reward of each agent on every task across 10 runs.  The results highlight OmniJARVIS's effectiveness in following straightforward instructions and achieving high average rewards with minimal standard deviation.", "section": "4.2 Main Results I: Short-horizon Atomic Tasks"}, {"figure_path": "ceIO1w0PmT/tables/tables_17_1.jpg", "caption": "Table 1: Evaluation results (rewards) on short-horizon atom tasks. The text-conditioned VPT [2] (\"VPT (text)*\") is from Appendix I of its paper.", "description": "This table presents the performance comparison of different agents on four simple Minecraft tasks: chopping trees, mining stones, digging dirt, and collecting wheat seeds.  The \"rewards\" represent the average scores achieved by each agent across multiple trials.  It compares OmniJARVIS against baselines such as text-conditioned VPT, and other agents from prior works.  The goal is to assess the effectiveness of OmniJARVIS on basic, short-horizon tasks.", "section": "4.2 Main Results I: Short-horizon Atomic Tasks"}]