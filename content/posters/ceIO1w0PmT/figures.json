[{"figure_path": "ceIO1w0PmT/figures/figures_1_1.jpg", "caption": "Figure 1: Illustration of multi-modal interaction data for decision-making. A canonical interaction sequence depicting the human decision-making process starts from a given task instruction and memory, followed by a series of sub-task completion which involves initial observations, chain-of-thought reasoning, and behavior trajectories. Our proposed VLA model OmniJARVIS jointly models the vision (observations), language (instructions, memories, thoughts), and actions (behavior trajectories) as unified autoregressive sequence prediction. A self-supervised behavior encoder (detailed in Section 2 and Figure 2) converts the actions into behavior tokens while the other modalities are tokenized following the practices of MLMs [31, 3, 1].", "description": "This figure illustrates the multimodal interaction data used for decision-making in the OmniJARVIS model.  It shows a sequence of human interaction steps, starting with an instruction and memory, progressing through observations, chain-of-thought reasoning, and finally resulting in a behavior trajectory.  The OmniJARVIS model uniquely unifies these different modalities (vision, language, action) into a single autoregressive sequence for prediction. A key component is a self-supervised behavior encoder that converts actions into discrete behavior tokens, which are then incorporated into the overall sequence.", "section": "1 Introduction"}, {"figure_path": "ceIO1w0PmT/figures/figures_2_1.jpg", "caption": "Figure 2: Self-supervised learning for behavior tokenizer of OmniJARVIS. We modify the VAE-based self-supervised learning of behavior trajectories in [10] to train the behavior tokenizer and de-tokenizer in Omni JARVIS. Specifically, we adopt the auto-encoding objective but replace the Gaussian latent with a discrete representation based on Finite Scalar Quantizer [34]. The encoder will then be used as the behavior tokenizer to produce discrete tokens from the actions (behavior trajectories) in multimodal interaction data, while the behavior tokens emitted by OmniJARVIS will be sent to the policy decoder to perform motor control.", "description": "This figure illustrates the self-supervised learning process for the behavior tokenizer in OmniJARVIS.  It adapts a VAE-based approach, replacing the continuous Gaussian latent representation with a discrete representation using Finite Scalar Quantizer. The encoder part of this autoencoder functions as the behavior tokenizer, converting action trajectories into discrete behavior tokens. These tokens are then used by a policy decoder (the decoder part of the autoencoder) for motor control, creating a closed loop for behavior modeling.", "section": "2 A Tokenizer for Behaviors"}, {"figure_path": "ceIO1w0PmT/figures/figures_3_1.jpg", "caption": "Figure 3: Architecture and Inference of OmniJARVIS. The main body of OmniJARVIS is a multimodal language model (MLM) augmented with additional behavior tokens. Given a task instruction, initial memory, and observation, OmniJARVIS will iteratively perform chain-of-thought reasoning and produce behavior tokens as a means of control via the decoder policy (behavior de-tokenizer). Every 128 steps, OmniJARVIS is forced to reason again and produce new behavior tokens with the latest observation. (Not shown above) OmniJARVIS can also make textual responses, e.g. answering questions.", "description": "This figure illustrates the architecture and inference process of OmniJARVIS.  OmniJARVIS uses a multimodal language model (MLM) enhanced with behavior tokens. Starting with a task instruction, memory, and observation, it iteratively reasons using chain-of-thought and generates behavior tokens (actions) via a decoder policy.  Every 128 steps, it updates its reasoning with the latest observations.  It can also produce textual responses, such as answers to questions. The figure highlights the iterative decision-making process.", "section": "3 Multimodal Interaction Data and OmniJARVIS"}, {"figure_path": "ceIO1w0PmT/figures/figures_7_1.jpg", "caption": "Figure 5: Scaling potential of OmniJARVIS. Its evaluation loss continues to drop with the growth of data and model parameters. The Pearson coefficients for the 2B, 7B, and 13B models are 0.9991, 0.9999, and 0.9989.", "description": "This figure shows the scaling potential of the OmniJARVIS model. The evaluation loss is plotted against the amount of training data (in millions of tokens) on a logarithmic scale. Three different model sizes (2B, 7B, and 13B parameters) are shown. As expected, the loss decreases as the amount of training data increases for all model sizes.  The near-linear relationship in the log-log plot indicates that OmniJARVIS follows a power law scaling behavior, a characteristic observed in many large language models. The Pearson correlation coefficients close to 1 confirm a strong relationship between training data size and performance for each model, with larger models showing slightly slower improvements.", "section": "4.6 Generalization and Scaling Potential of OmniJARVIS"}, {"figure_path": "ceIO1w0PmT/figures/figures_8_1.jpg", "caption": "Figure 6: Examples of behavior tokenization-detokeinzation. Left: the reference video to be tokenized by our FSQ-based behavior tokenizer (encoder). Right: the behavior of the policy decoder is conditioned on the behavior tokens. The policy decoder can reproduce the task being accomplished in the reference video.", "description": "This figure shows examples of the behavior tokenization and detokenization process. The left side displays a reference video fed into the behavior tokenizer (encoder) which uses Finite Scalar Quantization (FSQ). The right side presents the video generated by the policy decoder (which is also an imitation learning policy decoder) using the behavior tokens generated by the encoder as conditioning. The figure demonstrates that the policy decoder can successfully replicate the task shown in the reference video by using the discrete behavior tokens.", "section": "4.5 Insights and Analysis"}, {"figure_path": "ceIO1w0PmT/figures/figures_8_2.jpg", "caption": "Figure 7: OmniJARVIS plays Montezuma's Revenge and gets a reward of 3600.", "description": "This figure shows a sequence of game frames from the Atari game Montezuma's Revenge, showcasing the agent's performance.  The agent successfully navigates the game environment, achieving a final reward of 3600. Each frame is labeled with timestamps and other relevant metrics.  The figure demonstrates the ability of the OmniJARVIS model to generalize beyond Minecraft environments and perform complex tasks in a different game.", "section": "4.6 Generalization and Scaling Potential of OmniJARVIS"}, {"figure_path": "ceIO1w0PmT/figures/figures_9_1.jpg", "caption": "Figure 8: Comparative Framework of Vision-Language Action Models. (a) depicts a model where upon receiving a language instruction, actions are directly output based on the environmental state, facilitating immediate interaction with the environment at a unified frequency. Smaller models with <1B parameters like VPT [2] maintain higher frequencies (>20Hz), though their capability for complex reasoning tasks is limited. Larger models with >7B parameters such as RT-2 [6], offer enhanced performance but operate at significantly reduced frequencies (2-3Hz). (b) illustrates a common approach utilizing large vision-language models for planning, subsequently outputting language goals [46, 14, 4]. A language-conditioned policy then translates these language goals into actions at a real-time interaction rate of 20Hz, with high-level models re-planning at less than 1Hz. This hierarchical structure balances interaction frequency and performance, while it requires language as an intermediary and additional language labels. The training process of high-level vision-language models and language-conditioned policies are separate, thus performing poorly on tasks that can not be easily connected by language. (c) (ours) mirrors the hierarchical structure of (b) but differentiates by employing a self-supervised encoder-decoder policy [10] and FSQ quantization [34] as a behavior tokenizer. The upper-level vision-language models produce self-supervised behavior tokens, which are then conditioned by a policy decoder to output actions, facilitating environment interaction. The behavior tokens are injected into the training corpus of vision-language-action models, which enables end-to-end inference. This approach also eliminates the need for external language supervision and scales efficiently.", "description": "This figure compares three different architectures for Vision-Language-Action models.  (a) shows a simple, high-frequency model directly mapping instructions to actions based on the current state. (b) presents a hierarchical model using a large language model for planning and a separate controller for execution.  (c) illustrates OmniJARVIS, which uses a self-supervised behavior tokenizer to create behavior tokens that are jointly modeled with vision and language, enabling more efficient and seamless action generation.", "section": "5 Related Works"}, {"figure_path": "ceIO1w0PmT/figures/figures_13_1.jpg", "caption": "Figure 2: Self-supervised learning for behavior tokenizer of OmniJARVIS. We modify the VAE-based self-supervised learning of behavior trajectories in [10] to train the behavior tokenizer and de-tokenizer in Omni JARVIS. Specifically, we adopt the auto-encoding objective but replace the Gaussian latent with a discrete representation based on Finite Scalar Quantizer [34]. The encoder will then be used as the behavior tokenizer to produce discrete tokens from the actions (behavior trajectories) in multimodal interaction data, while the behavior tokens emitted by OmniJARVIS will be sent to the policy decoder to perform motor control.", "description": "This figure illustrates the self-supervised learning framework for the behavior tokenizer in OmniJARVIS.  It shows how a variational autoencoder (VAE) is modified to learn a discrete representation of behavior trajectories. The encoder part of the VAE acts as the behavior tokenizer, converting continuous action sequences into discrete behavior tokens. These tokens are then used by a policy decoder (the decoder part of the VAE) to generate control commands.  The use of a finite scalar quantizer ensures the discrete nature of the learned representation, making it compatible with other modalities in the model.", "section": "A Tokenizer for Behaviors"}, {"figure_path": "ceIO1w0PmT/figures/figures_23_1.jpg", "caption": "Figure 3: Architecture and Inference of OmniJARVIS. The main body of OmniJARVIS is a multimodal language model (MLM) augmented with additional behavior tokens. Given a task instruction, initial memory, and observation, OmniJARVIS will iteratively perform chain-of-thought reasoning and produce behavior tokens as a means of control via the decoder policy (behavior de-tokenizer). Every 128 steps, OmniJARVIS is forced to reason again and produce new behavior tokens with the latest observation. (Not shown above) OmniJARVIS can also make textual responses, e.g. answering questions.", "description": "OmniJARVIS architecture is based on a pretrained multimodal language model (MLM) enhanced with behavior tokens.  It receives a task, memory, and observation and uses chain-of-thought reasoning to generate behavior tokens that guide actions via a decoder. Every 128 steps, it updates its reasoning with the latest observations and can additionally provide textual responses.", "section": "3 Multimodal Interaction Data and OmniJARVIS"}, {"figure_path": "ceIO1w0PmT/figures/figures_24_1.jpg", "caption": "Figure 3: Architecture and Inference of OmniJARVIS. The main body of OmniJARVIS is a multimodal language model (MLM) augmented with additional behavior tokens. Given a task instruction, initial memory, and observation, OmniJARVIS will iteratively perform chain-of-thought reasoning and produce behavior tokens as a means of control via the decoder policy (behavior de-tokenizer). Every 128 steps, OmniJARVIS is forced to reason again and produce new behavior tokens with the latest observation. (Not shown above) OmniJARVIS can also make textual responses, e.g. answering questions.", "description": "This figure illustrates the architecture and inference process of OmniJARVIS. OmniJARVIS is a multimodal language model enhanced with behavior tokens. It takes task instructions, memory, and observations as input and iteratively performs chain-of-thought reasoning to generate behavior tokens. These tokens act as control signals for the decoder policy, which outputs actions. The process is repeated every 128 steps to incorporate the latest observations.  OmniJARVIS is also capable of generating textual responses, such as answers to questions.", "section": "3 Multimodal Interaction Data and OmniJARVIS"}]