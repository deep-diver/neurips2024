{"references": [{"fullname_first_author": "J.-B. Alayrac", "paper_title": "Flamingo: a visual language model for few-shot learning", "publication_date": "2022-04-26", "reason": "This paper introduces Flamingo, a multimodal model foundational to OmniJARVIS's architecture, enabling unified vision-language-action processing."}, {"fullname_first_author": "B. Baker", "paper_title": "Video pretraining (VPT): Learning to act by watching unlabeled online videos", "publication_date": "2022-06-27", "reason": "VPT provides the foundational dataset for training the behavior tokenizer, enabling OmniJARVIS to learn from human gameplay videos."}, {"fullname_first_author": "T. Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-12-01", "reason": "This paper demonstrates the effectiveness of large language models as few-shot learners, a key concept leveraged by OmniJARVIS."}, {"fullname_first_author": "L. Chen", "paper_title": "Decision transformer: Reinforcement learning via sequence modeling", "publication_date": "2021-12-01", "reason": "The Decision Transformer architecture is a crucial inspiration for OmniJARVIS's approach to long-horizon decision-making through autoregressive modeling."}, {"fullname_first_author": "S. Cai", "paper_title": "Groot: Learning to follow instructions by watching gameplay videos", "publication_date": "2023-10-26", "reason": "GROOT directly inspires OmniJARVIS's behavior tokenization and self-supervised learning approach, providing a critical component for action representation."}]}