[{"type": "text", "text": "Byzantine Robustness and Partial Participation Can Be Achieved at Once: Just Clip Gradient Differences ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Grigory Malinovsky\\* Peter Richtarik Samuel Horvath Eduard Gorbunov? KAUST! MBZUAI+ KAUST MBZUAI MBZUAI ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Distributed learning has emerged as a leading paradigm for training large machine learning models. However, in real-world scenarios, participants may be unreliable or malicious, posing a significant challenge to the integrity and accuracy of the trained models. Byzantine fault tolerance mechanisms have been proposed to address these issues, but they often assume full participation from all clients, which is not always practical due to the unavailability of some clients or communication constraints. In our work, we propose the first distributed method with client sampling and provable tolerance to Byzantine workers. The key idea behind the developed method is the use of gradient clipping to control stochastic gradient differences in recursive variance reduction. This allows us to bound the potential harm caused by Byzantine workers, even during iterations when all sampled clients are Byzantine. Furthermore, we incorporate communication compression into the method to enhance communication efficiency. Under general assumptions, we prove convergence rates for the proposed method that match the existing state-ofthe-art (SOTA) theoretical results. We also propose a heuristic on adjusting any Byzantine-robust method to a partial participation scenario via clipping. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Distributed optimization problems are a cornerstone of modern machine learning research. They naturally arise in scenarios where data is distributed across multiple clients; for instance, this is typical in Federated Learning (FL) (Konecny et al., 2016; Kairouz et al., 2021). Such problems require specialized algorithms adapted to the distributed setup. Additionally, the adoption of distributed optimization methods is motivated by the sheer computational complexity involved in training modern machine learning models. Many models deal with massive datasets and intricate architectures, rendering training infeasible on a single machine (Li, 2020). Distributed methods, by parallelizing computations across multiple machines, offer a pragmatic solution to accelerate training and address these computational challenges, thus pushing the boundaries of machine learning capabilities. ", "page_idx": 0}, {"type": "text", "text": "To make distributed training accessible to the broader community, collaborative learning approaches have been actively studied in recent years (Kijsipongse et al., 2018b; Ryabinin and Gusev, 2020; Atre et al., 2021; Diskin et al., 2021a). In such applications, there is a high risk of the occurrence of so-called Byzantine workers (Lamport et al., 1982; Su and Vaidya, 2016)\u2014participants who can violate the prescribed distributed algorithm/protocol either intentionally or simply because they are faulty. In general, such workers may even have access to some private data of certain participants and may collude to increase their impact on the training. Since the ultimate goal is to achieve robustness in the worst case, many papers in the field make no assumptions limiting the power of Byzantine workers. Clearly, in this scenario, standard distributed methods based on the averaging of received information (e.g., stochastic gradients) are not robust, even to a single Byzantine worker. Such a worker can send an arbitrarily large vector that can shift the method arbitrarily far from the solution. This aspect makes it non-trivial to design methods with provable robustness to Byzantines (Baruch et al., 2019; Xie et al., 2020a). Despite all the challenges, multiple methods are developed/analyzed in the literature (Alistarh et al., 2018; Allen-Zhu et al., 2021; Wu et al., 2020; Zhu and Ling, 2021; Karimireddy et al., 2021, 2022; Gorbunov et al., 2022, 2023; Allouah et al., 2023). ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "However, literally, all existing methods with provable Byzantine robustness require the full (or close to full) participation of clients or rely on extra assumptions. The requirement of full participation is impractical for modern distributed learning problems since they can have millions of clients (Bonawitz et al., 2017; Niu et al., 2020). In such scenarios, it is more natural to use sampling of clients to speed up the training. Moreover, some clients can be unavailable at certain moments, e.g., due to a poor connection, low battery, or simply because of the need to use the computing power for some other tasks. Although partial participation of clients is a natural attribute of largescale collaborative training, it is not studied under the presence of Byzantine workers. Moreover, this question is highly non-trivial: the existing methods can fail to converge if combined naively with partial participation since Byzantine can form a majority during particular rounds and, thus, destroy the whole training with just one round of communications. Therefore, the field requires the development of new distributed methods that are provably robust to Byzantine attacks and can work with partial participation even when Byzantine workers form a majority during some rounds. ", "page_idx": 1}, {"type": "text", "text": "Our Contributions We develop Byzantine-tolerant Variance-Reduced MARINA with Partial Participation (Byz-VR-MARINA-PP, Algorithm 1) - the first distributed method having Byzantine robustness and allowing partial participation of clients without strong additional assumptions. Our method uses variance reduction to handle Byzantine workers and clipping of stochastic gradient differences to bound the potential harm of Byzantine workers even when they form a majority during particular rounds of communication. To make the method even more communication efficient, we add communication compression. We prove the convergence of Byz-VR-MARINA-PP for general smooth non-convex functions and Polyak-Lojasiewicz functions. In the special case of full participation, our complexity bounds recover the ones for Byz-VR-MARINA (Gorbunov et al., 2023) that are the current SOTA convergence results. Moreover, we prove that in some cases, partial participation is theoretically beneficial for Byz-VR-MARINA-PP. We also propose a simplified version of Byz-VRMARINA-PP with better neighborhood term in the convergence bounds (Byz-VR-MARINA- $\\mathsf{P P+}$ Algorithm 3) and a heuristic on how to use clipping to adapt any Byzantine-robust method to the partial participation setup and illustrate its performance in experiments. ", "page_idx": 1}, {"type": "text", "text": "1.1 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Below, we overview closely related works. Additional discussion is deferred to Appendix A. ", "page_idx": 1}, {"type": "text", "text": "Byzantine robustness. The primary vulnerability of standard distributed methods to Byzantine attacks lies in the aggregation rule: even one worker can arbitrarily distort the average. Therefore, many papers on Byzantine robustness focus on the application of robust aggregation rules, such as the geometric median (Pillutla et al., 2022), coordinate-wise median, trimmed median (Yin et al., 2018), Krum (Blanchard et al., 2017), and Multi-Krum (Damaskinos et al., 2019). However, simply robustifying the aggregation rule is insufficient to achieve provable Byzantine robustness, as illustrated by Baruch et al. (2019) and Xie et al. (2020a), who design special Byzantine attacks that can bypass standard defenses. This implies that more significant algorithmic changes are required to achieve Byzantine robustness, a point also formally proven by Karimireddy et al. (2021), who demonstrate that permutation-invariant algorithms - i.e., algorithms independent of the order of stochastic gradients at each step - cannot provably converge to any predefined accuracy in the presence of Byzantines. ", "page_idx": 1}, {"type": "text", "text": "Wu et al. (2020) are the first who exploit variance reduction to tolerate Byzantine attacks. They propose and analyze the method called Byrd-SAGA, which uses SAGA-type (Defazio et al., 2014) gradient estimators on the good workers and geometric median for the aggregation. Gorbunov et al. (2023) develop another variance-reduced method called Byz-VR-MARINA, which is based on (conditionally biased) GeomSARAH/PAGE-type (Horvath et al., 2023; Li et al., 2021) gradient estimator and any robust aggregation in the sense of the definition from (Karimireddy et al., 2021, ", "page_idx": 1}, {"type": "text", "text": "2022), and derive the improved convergence guarantees that are the current SOTA in the literature.   \nThere are also many other approaches and we discuss some of them in Appendix A. ", "page_idx": 2}, {"type": "text", "text": "Partial participation and client sampling. In the context of Byzantine-robust learning, there exists several works that develop and analyze methods with partial participation (Data and Diggavi, 2021; El-Mhamdi et al., 2021; Boubouh et al., 2022; Allouah et al., 2024a). However, these works rely on the restrictive assumption that the number of participating clients at each round is larger than the number of Byzantine workers. In this case, Byzantines cannot form a majority, and standard methods can be applied without any changes. In contrast, our method converges in more challenging scenarios, e.g., Byz-VR-MARINA-PP provably converges even when the server samples one client, which can be Byzantine. If the number of participating clients is such that Byzantine clients can form majority, these methods have a certain probability of divergence and this probability grows with each communication round. We provide a more detailed discussion in Appendix A. ", "page_idx": 2}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we formally introduce the problem, main definition, and assumptions used in the analysis. That is, we consider finite-sum distributed optimization problem? ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\operatorname*{min}_{x\\in\\mathbb{R}^{d}}\\left\\{f(x):=\\frac{1}{G}\\sum_{i\\in\\mathcal{G}}f_{i}(x)\\right\\},\\quad f_{i}(x):=\\frac{1}{m}\\sum_{j=1}^{m}f_{i,j}(x)\\quad\\forall i\\in\\mathcal{G},}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\mathcal{G}$ is a set of regular clients of size $G:=|\\mathcal{G}|$ In the context of distributed learning, $f_{i}:\\ensuremath{\\mathbb{R}^{d}}\\to\\ensuremath{\\mathbb{R}}$ corresponds to the loss function on the data of client $i$ , and $f_{i,j}:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ is the loss computed on the $j$ -th sample from the dataset of client $i$ . Next, we assume that the set of all clients taking part in the training is $[n]=\\{1,2,\\ldots,n\\}$ and ${\\mathcal{G}}\\subseteq[n]$ . The remaining clients $B:=[n]\\setminus{\\mathcal{G}}$ are Byzantine ones. We assume that $B:=|\\boldsymbol{B}|:=\\delta_{\\mathrm{real}}n\\leq\\delta n$ , where $\\delta_{\\mathrm{real}}$ is an exact ratio of Byzantine workers and $\\delta$ is a known upper bound for $\\delta_{\\mathrm{real}}$ . We also assume that $0\\leq\\delta_{\\mathrm{real}}\\leq\\delta<{1}/{2}$ since otherwise Byzantine workers form a majority and problem (1) becomes impossible to solve in general. ", "page_idx": 2}, {"type": "text", "text": "Notation. We use a standard notation for the literature on distributed stochastic optimization. Everywhere in the text $\\Vert x\\Vert$ denotes a standard $\\ell_{2}$ -norm of $x\\,\\in\\,\\mathbb{R}^{d}$ \uff0c $\\langle a,b\\rangle$ refers to the standard inner product of vectors $a,b\\,\\in\\,\\mathbb{R}^{d}$ . The clipping operator is defined as follows: $\\mathsf{c l i p}_{\\lambda}(x):=$ $\\operatorname*{min}\\{\\bar{1},\\lambda/||x||\\}x$ for $x\\neq0$ and ${\\mathsf{c l i p}}_{\\lambda}(0):=0$ . Finally, $\\mathrm{Prob}\\{A\\}$ denotes the probability of event $A$ $\\mathbb{E}[\\xi]$ is the full expectation of random variable $\\xi,\\mathbb{E}[\\xi\\mid A]$ is the expectation of $\\xi$ conditioned on the event $A$ . We also sometimes use $\\mathbb{E}_{k}[\\xi]$ to denote an expectation of $\\xi$ W.r.t. the randomness coming from step $k$ ", "page_idx": 2}, {"type": "text", "text": "Robust aggregator. We follow the definition from (Gorbunov et al., 2023) of $(\\delta,c)$ -robustaggregation, which is a generalization of the definitions proposed by Karimireddy et al. (2021, 2022). ", "page_idx": 2}, {"type": "text", "text": "Definition 2.1 $\\mathit{\\check{\\Psi}}(\\delta,c)$ -Robust Aggregator). Assume that $\\{x_{1},x_{2},\\ldots,x_{n}\\}$ is such that there exists a subset ${\\mathcal{G}}\\subseteq[n]$ of size $|\\mathcal{G}|=G\\geq(1-\\delta)n$ for $\\delta\\,\\leq\\,\\delta_{\\mathrm{max}}\\,<\\,0.5$ and there exists $\\sigma\\geq0$ such that $\\begin{array}{r}{\\frac{1}{G(G-1)}\\sum_{i,l\\in\\mathcal{G}}\\mathbb{E}\\left[\\left\\|x_{i}-x_{l}\\right\\|^{2}\\right]\\;\\leq\\;\\sigma^{2}}\\end{array}$ where the expectation is taken w.rt. the randomness of $\\{x_{i}\\}_{i\\in\\mathcal{G}}$ . We say that the quantity $\\widehat{x}$ $(\\delta,c)$ Robust Aggregator $(\\delta,c)\\!-\\!\\mathrm{RAgg})$ and write ${\\widehat{x}}\\,=$ ${\\tt R A g g}\\left(x_{1},\\ldots,x_{n}\\right)$ for some $c>0$ , if the following inequality holds: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\|\\widehat{\\boldsymbol{x}}-\\bar{\\boldsymbol{x}}\\|^{2}\\right]\\le c\\delta\\sigma^{2},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\begin{array}{r}{\\bar{x}:=\\frac{1}{|\\mathcal G|}\\sum_{i\\in\\mathcal G}x_{i}}\\end{array}$ If additionally $\\widehat{x}$ is computed without the knowledge of $\\sigma^{2}$ , we say that $\\widehat{x}$ is $(\\delta,c)$ -Agnostic Robust Aggregator $(\\delta,c)\\!-\\!\\mathtt{A R A g g}$ and write ${\\widehat{x}}=\\mathsf{A R A g g}\\left(x_{1},\\ldots,x_{n}\\right)$ ", "page_idx": 2}, {"type": "text", "text": "One can interpret the definition as follows. Ideally, we would like to filter out all Byzantine workers and compute just an average $\\textstyle{\\bar{x}}$ over the set of good clients. However, this is impossible in general since we do not know apriori who are Byzantine workers. Instead of this, it is natural to expect that the aggregation rule approximates the ideal average up in a certain sense, e.g., in terms of the expected squared distance to $\\bar{x}$ . As Karimireddy et al. (2021) formally show, in terms of such criterion $\\widehat{(\\mathbb{E}}[\\Vert\\widehat{x}-$ $\\bar{x}\\|^{2}])$ , the definition of $(\\delta,c)$ -RAgg cannot be improved (up to the numerical constant). Moreover, standard aggregators such as Krum (Blanchard et al., 2017), geometric median, and coordinate-wise median do not satisfy Definition 2.1 (Karimireddy et al., 2021), though another popular standard aggregation rule called coordinate-wise trimmed mean (Yin et al., 2018) satisfies Definition 2.1 as shown by Allouah et al. (2023) through the more general definition of robust aggregation. To address this issue, Karimireddy et al. (2021) develop the aggregator called CenteredClip and prove that it fits the definition of $(\\delta,c)$ -RAgg. Karimireddy et al. (2022) propose a procedure called Bucketing that fixes Krum, geometric median, and coordinate-wise median, i.e., with Bucketing Krum, geometric, and coordinate-wise median become $(\\delta,c)$ -ARAgg, which is important for our algorithm since the variance of the vectors received from regular workers changes over time in our method. We notice here that $\\delta$ is a part of the input that should satisfy $\\delta_{\\mathrm{{real}}}\\le\\delta\\le\\delta_{\\mathrm{{max}}}$ ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Compression operators. In our work, we use standard unbiased compression operators with relatively bounded variance (Khirirat et al., 2018; Horvath et al., 2023). ", "page_idx": 3}, {"type": "text", "text": "Definition 2.2 (Unbiased compression). Stochastic mapping $\\mathcal{Q}:\\mathbb{R}^{d}\\,\\rightarrow\\,\\mathbb{R}^{d}$ is called unbiased compressor/compression operator if there exists $\\omega\\ \\geq\\ 0$ such that for any $x\\,\\in\\,\\mathbb{R}^{d}\\,\\,\\mathbb{E}[\\,\\mathcal{Q}(x)]\\,=$ $x$ $\\begin{array}{r l}{\\zeta,}&{{}\\mathbb{E}\\left[\\|\\mathcal{Q}(x)-x\\|^{2}\\right]\\ \\leq\\ \\omega\\|x\\|^{2}}\\end{array}$ . For the given unbiased compressor $\\mathcal{Q}(x)$ , one can define the expected density6 as $\\begin{array}{r}{\\zeta_{\\mathcal{Q}}:=\\operatorname*{sup}_{x\\in\\mathbb{R}^{d}}\\mathbb{E}\\left[\\Vert\\boldsymbol{\\mathcal{Q}}(x)\\Vert_{0}\\right]}\\end{array}$ , where $\\Vert y\\Vert_{0}$ is the number of non-zero components of $\\boldsymbol{y}\\in\\mathbb{R}^{d}$ ", "page_idx": 3}, {"type": "text", "text": "In this definition, parameter $\\omega$ reflects how lossy the compression operator is: the larger $\\omega$ the more lossy the compression. For example, this class of compression operators includes random sparsification (RandK) (Stich et al., 2018) and quantization (Goodall, 1951; Roberts, 1962; Alistarh et al., 2017). For RandK compression $\\begin{array}{r}{\\omega=\\frac{d}{K}\\!-\\!1,\\zeta_{\\mathcal{Q}}=K}\\end{array}$ and for $\\ell_{2}$ quantization $\\omega=\\sqrt{d}\\!-\\!1,\\zeta_{\\mathcal{Q}}=$ $\\sqrt{d}$ , see the proofs in (Beznosikov et al., 2020). ", "page_idx": 3}, {"type": "text", "text": "Assumptions. Up to a couple of assumptions that are specific to our work, we use the same assumptions as in (Gorbunov et al., 2023). We start with two new assumptions. ", "page_idx": 3}, {"type": "text", "text": "Assumption 2.3 (Bounded $\\mathtt{A R A g g}$ ). We assume that the server applies aggregation rule $\\boldsymbol{\\mathcal{A}}$ such that $\\boldsymbol{\\mathcal{A}}$ is $(\\delta,c)$ -ARAgg and there exists constant $F_{A}>0$ such that for any inputs $x_{1},\\ldots,x_{n}\\in\\mathbb{R}^{d}$ the norm of the aggregator is not greater than the maximal norm of the inputs: $\\left\\|A\\left(x_{1},.\\,.\\,.\\,,x_{n}\\right)\\right\\|\\leq$ $F_{A}\\operatorname*{max}_{i\\in[n]}\\left\\|x_{i}\\right\\|$ ", "page_idx": 3}, {"type": "text", "text": "The above assumption is satisfied for popular $(\\delta,c)$ -robust aggregation rules presented in the literature (Karimireddy et al., 2021, 2022). Therefore, this assumption is more a formality than a real limitation: it is needed to exclude some pathological examples of $\\bar{(\\delta,c)}$ -robust aggregation rules, e.g., for any $\\boldsymbol{\\mathcal{A}}$ that is $(\\delta,c)$ -RAgg one can construct unbounded $(\\delta,2c)$ -RAgg as ${\\overline{{A}}}=A+X$ ,where $X$ is a random sample from the Gaussian distribution $\\mathcal{N}(0,c\\delta\\sigma^{2})$ ", "page_idx": 3}, {"type": "text", "text": "Next, for part of our results, we also make the following assumption. ", "page_idx": 3}, {"type": "text", "text": "Assumption 2.4 (Bounded compressor (optional). We assume that workers use compression operator $\\mathcal{Q}$ satisfying Definition 2.2 and bounded as follows: $\\|\\b{\\mathcal{Q}}(x)\\|\\leq D_{Q}\\|x\\|\\quad\\forall x\\in\\mathbb{R}^{\\hat{d}}$ ", "page_idx": 3}, {"type": "text", "text": "For example, RandK and $\\ell_{2}$ quantizationmet this assmpton with $\\begin{array}{r}{D_{\\mathcal{Q}}\\,=\\,\\frac{d}{K}}\\end{array}$ and $D_{\\mathcal{Q}}\\;=\\;\\sqrt{d}$ respectively. In general, constant $D_{\\mathcal{Q}}$ can be large (proportional to ). However, in practice, one can use RandK with K = 100\\* and, thus, have moderate $D_{\\mathcal{Q}}=100$ .We also have the results without Assumption 2.4, but with worse dependence on some other parameters, see Section 4. ", "page_idx": 3}, {"type": "text", "text": "Next, we assume that good workers have $\\zeta^{2}$ -heterogeneous local loss functions. ", "page_idx": 3}, {"type": "text", "text": "Assumption 2.5 $\\zeta^{2}$ -heterogeneity). We assume that good clients have $\\zeta^{2}$ -heterogeneous local loss functions for some $\\zeta\\geq0$ i.e., $\\begin{array}{r}{\\frac{1}{G}\\overset{\\cdot}{\\sum}_{i\\in\\mathcal{G}}\\|\\nabla f_{i}(x)-\\bar{\\nabla^{f}}(x)\\|^{2}\\leq\\zeta^{2}\\quad\\forall x\\in\\mathbb{R}^{d}.}\\end{array}$ ", "page_idx": 3}, {"type": "text", "text": "The above assumption is quite standard for the literature on Byzantine robustness (Wu et al., 2020; Karimireddy et al., 2022; Gorbunov et al., 2023; Allouah et al., 2023). Moreover, some kind of a bound on the heterogeneity of good clients is necessary since otherwise Byzantine robustness cannot be achieved in general. In the appendix, all proofs are given under a more general version of ", "page_idx": 3}, {"type": "text", "text": "Algorithm 1 Byz-VR-MARINA-PP: Byzantine-tolerant VR-MARINA with Partial Participation ", "page_idx": 4}, {"type": "text", "text": "1: Input: vectors $x^{0},g^{0}\\in\\mathbb{R}^{d}$ , stepsize $\\gamma$ , mini-batch size $b$ , probability $p\\,\\in\\,(0,1]$ , number of iterations $K$ $(\\delta,c)$ -ARAgg, clients\u2032 sample size $1\\leq C\\leq\\widehat{C}\\leq n$ , cliping coefficients $\\{\\alpha_{k}\\}_{k\\ge1}$   \n2: for $k=0,1,\\ldots,K-1$ do   \n3: Get a sample from Bernoulli distribution with parameter $p$ $c_{k}\\sim\\mathsf{B e}(p)$   \n4: Sample the set of clients $S_{k}\\subseteq[n]$ $|S_{k}|=C$ $c_{k}=0$ ; otherwise $|S_{k}|=\\widehat{C}$   \n5: Broadcast $g^{k}$ \uff0c $c_{k}$ to all workers   \n6: for $i\\in\\mathcal G\\cap S_{k}$ in parallel do   \n7: $x^{k+1}=x^{k}-\\gamma g^{k}$ and $\\lambda_{k+1}=\\alpha_{k+1}\\|x^{k+1}-x^{k}\\|$ {Vfi(\u03b1k+1), $c_{k}=1$   \n8: Set g+1 $\\begin{array}{r}{g_{i}^{k+1}=\\left\\{g^{k}+\\mathsf{c l i p}_{\\lambda_{k+1}}\\left(\\mathcal{Q}\\left(\\widehat{\\Delta}_{i}(x^{k+1},x^{k})\\right)\\right),\\displaystyle{\\qquad\\qquad\\qquad\\qquad\\quad}\\right.}\\end{array}$ otherwise, Where $\\widehat{\\Delta}_{i}(x^{k+1},x^{k})$ is amini-batched estimatorof $\\nabla f_{i}(x^{k+1})-\\nabla f_{i}(x^{k}),\\,\\mathcal{Q}(\\cdot)$ for $i\\in$ $\\mathcal{G}\\cap S_{k}$ are computed independently   \n9: end for   \n10: $\\begin{array}{r l}&{\\mathrm{e}^{\\mathrm{\\footnotesize{num\\ro1}}}}\\\\ &{g^{k+1}=\\left\\{\\!\\!\\!\\begin{array}{l l}{\\mathrm{ARAgg}\\left(\\{g_{i}^{k+1}\\}_{i\\in S_{k}}\\right),}&{\\mathrm{if~}c_{k}=1,}\\\\ {g^{k}+\\mathrm{ARAgg}\\left(\\left\\{\\mathrm{c}1\\mathrm{i}{\\mathrm{p}}_{\\lambda_{k+1}}\\left(\\mathcal{Q}\\left(\\widehat{\\Delta}_{i}(x^{k+1},x^{k})\\right)\\right)\\right\\}_{i\\in S_{k}}\\right),}&{\\mathrm{otherwise}}\\end{array}\\!\\!\\right.}\\end{array}$ ", "page_idx": 4}, {"type": "text", "text": "Assumption 2.5, see Assumption D.5. Finally, the case of homogeneous data ( $\\zeta=0$ is alsoquite popular for collaborative learning (Diskin et al., 2021b; Kijsipongse et al., 2018a). ", "page_idx": 4}, {"type": "text", "text": "The following assumption is classical for the literature on non-convex optimization. ", "page_idx": 4}, {"type": "text", "text": "Assumption 2.6 (Smoothness (simplified)). We assume that for all $i\\in\\mathcal G$ and $j\\in[m]$ there exists $\\mathcal{L}\\geq0$ such that $f_{i,j}$ is $\\mathcal{L}$ -smooth, i.e., for all $x,y\\in\\mathbb{R}^{d}$ ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\|\\nabla f_{i,j}(x)-\\nabla f_{i,j}(y)\\|\\leq\\mathcal{L}\\|x-y\\|.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Moreover, we assume that $f$ is uniformly lower bounded by $f^{*}\\in\\mathbb{R}$ ,i.e., $f^{*}:=\\operatorname*{inf}_{x\\in\\mathbb{R}^{d}}f(x)$ ", "page_idx": 4}, {"type": "text", "text": "For the sake of simplicity, we do not differentiate between various notions of smoothness in the main text. However, our analysis takes into account the differences between smoothness constants, similarity of local functions, and sampling strategy (see Appendix D.1). ", "page_idx": 4}, {"type": "text", "text": "Finally, we also consider functions satisfying Polyak-Lojasiewicz $({\\mathsf{P E}})$ condition (Polyak,1963; Lojasiewicz, 1963). This assumption belongs to the class of assumptions on the structured nonconvexity that allows achieving linear convergence (Necoara et al., 2019). ", "page_idx": 4}, {"type": "text", "text": "Assumption 2.7 ( $\\mathrm{PE}$ condition (optional)). We assume that function $f$ satisfies Polyak-Lojasiewicz $({\\mathsf{P E}})$ condition with parameter $\\mu>0$ , i.e., for all $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ there exists $f^{*}:=\\operatorname*{inf}_{x\\in\\mathbb{R}^{d}}f(x)$ such that $\\|\\nabla f(x)\\|^{2}\\geq2\\mu\\left(f(\\bar{x})-f^{*}\\right)$ ", "page_idx": 4}, {"type": "text", "text": "3New Method: Byz-VR-MARINA-PP ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We propose a new method called Byzantine-tolerant Variance-Reduced MARINA with Partial Participation (Byz-VR-MARINA-PP, Algorithm 1). Our method extends Byz-VR-MARINA (Gorbunov et al., 2023) to the partial participation case via the proper usage of the clipping operator. To illustrate how Byz-VR-MARINA-PP works, we first consider a special case of full participation. ", "page_idx": 4}, {"type": "text", "text": "Special case: Byz-VR-MARINA. If all clients participate at each round $\\mathbf{\\boldsymbol{S}}_{k}\\equiv[n])$ and clipping is turned off $\\langle\\lambda_{k}\\equiv+\\infty\\rangle$ , then Byz-VR-MARINA-PP reduces to Byz-VR-MARINA that works as follows. Consider the case when no compression is applied $\\mathcal{Q}(x)=x)$ and $\\widehat{\\Delta}_{i}(x^{k+1},x^{k})=$ $\\nabla f_{i,j_{k}}(x^{k+1})-\\nabla f_{i,j_{k}}(x^{k})$ ,where $j_{k}$ is sampled uniformly at random from $[m]$ \uff0c $i\\,\\in\\,\\mathcal G$ .Then, regular workers compute GeomSARAH/PAGE gradient estimator at each step: for $i\\in\\mathcal G$ ", "page_idx": 4}, {"type": "equation", "text": "$$\ng_{i}^{k+1}=\\left\\{\\!\\!\\!\\begin{array}{l}{\\nabla f_{i}(x^{k+1}),\\quad\\mathrm{with~probability}\\;p,}\\\\ {g^{k}+\\nabla f_{i,j_{k}}(x^{k+1})-\\nabla f_{i,j_{k}}(x^{k}),\\quad\\mathrm{otherwise}}\\end{array}\\!\\!\\!\\right.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "With small probability $p$ good workers compute full gradients, and with larger probability $1-p$ they update their estimator via adding stochastic gradient difference. To balance the oracle cost of these two cases, one can choose $p\\sim1/m$ (for $b$ -size mini-batched estimator - $p\\sim b/m)$ 0. Such estimators are known to be optimal for finding stationary points in the stochastic first-order optimization (Fang et al., 2018; Arjevani et al., 2023). Next, good workers send $g_{i}^{k+1}$ or $\\nabla f_{i,j_{k}}(x^{k+1})-\\nabla f_{i,j_{k}}(x^{k})$ to the server who robustly aggregate the received vectors. Since estimators are conditionally biased, i.e., E[g $\\mathbb{E}[g_{i}^{k+1}\\mid x^{k+1},x^{k}]\\neq\\overset{\\cdot}{\\nabla}\\bar{f}_{i}(x^{k+1})$ the additional bias coming from the agregaion does not cause significant issues in the analysis or practice. Moreover, the variance of $\\{g_{i}^{k+1}\\}_{i\\in\\mathcal{G}}$ + Jieg w.r.t. the sampling of the stochastic gradients is proportional to $\\|x^{k+1}-x^{k}\\|^{2}\\to0$ with probability $1-p$ (due to Assumption D.3) that progressively limits the effect of Byzantine attacks. For a more detailed explanation of why recursive variance reduction works better than SAGA/SVRG-type variance reduction, we refer to (Gorbunov et al., 2023). Arbitrary sampling allows the improvement of the dependence on the smoothness constants. Unbiased communication compression also naturally fits the framework since it is applied to the stochastic gradient difference, meaning that the variance of $\\{g_{i}^{k+1}\\}_{i\\in\\mathcal{G}}$ w.r.t. the sampling of the stochastic gradients and compression remains proportional to $\\|{\\boldsymbol{x}}^{k+1}-{\\boldsymbol{x}}^{k}\\|^{2}$ with probability $1-p$ ", "page_idx": 5}, {"type": "text", "text": "New ingredients: client sampling and clipping. The algorithmic novelty of Byz-VR-MARINA-PP in comparison to Byz-VR-MARINA is twofold: with (typically large) probability $1-p$ only $C$ clients sampled uniformly at random from the set of all clients participate at each round, and clipping is applied to the compressed stochastic gradient differences. With a small probability $p$ , a larger number of clients $\\overbar{C}\\leq n$ takes part in the communication. The main role of clipping is to ensure that the method can withstand the attacks of Byzantines when they form a majority or, more precisely when there are more than $\\delta C$ Byzantine workers among the sampled ones. Indeed, without clipping (or some other algorithmic changes) such situations are critical for convergence: Byzantine workers can shift the method arbitrarily far from the solution, e.g., they can collectively send some vector with the arbitrarily large norm. In contrast, Byz-VR-MARINA-PP tolerates any attacks even when all sampled clients are Byzantine workers since the update remains bounded due to the clipping. Via choosing $\\lambda_{k+1}\\sim\\|x^{k+1}-x^{k}\\|$ we ensure that the norm of transmitted vectors decreases with the same rate as it does in Byz-VR-MARINA with full client participation. Finally, with probability $1-p$ regular workers can transmit just compressed vectors and leave the clipping operation to the server since Byzantines can ignore clipping operation. ", "page_idx": 5}, {"type": "text", "text": "4 Convergence Results ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We define $\\mathcal{G}_{C}^{k}=\\mathcal{G}\\cap S_{k}$ and $G_{C}^{k}=|\\mathcal{G}_{C}^{k}|$ and $\\begin{array}{r}{{\\binom{n}{k}}=\\frac{n!}{k!(n-k)!}}\\end{array}$ represents the binomil cefcient We also use the following probabilities: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{p_{G}:=\\mathrm{Prob}\\left\\{G_{C}^{k}\\geq\\left(1-\\delta\\right)C\\right\\}=\\sum_{\\lceil(1-\\delta)C\\rceil\\leq t\\leq C}\\frac{\\binom{G}{t}\\binom{n-G}{C-t}}{\\binom{n}{C}},}\\\\ &{\\mathcal{P}_{\\mathcal{G}_{C}^{k}}:=\\mathrm{Prob}\\left\\{i\\in\\mathcal{G}_{C}^{k}\\mid G_{C}^{k}\\geq\\left(1-\\delta\\right)C\\right\\}=\\frac{C}{n p_{G}}\\cdot\\sum_{\\lceil(1-\\delta)C\\rceil\\leq t\\leq C}\\frac{\\binom{G-1}{t-1}\\binom{n-G}{C-t}}{\\binom{n-1}{C-1}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "These probabilities naturally appear in the analysis and statements of the theorems. When $c_{k}=0$ then server samples $C$ clients, and two situations can appear: either $G_{C}^{k}$ is at least $\\left(1-\\delta\\right)C$ meaning that the aggregator can ensure robustness according to Definition 2.1 or $G_{C}^{k}<(1-\\delta)\\,{\\cal C}$ Probability $p_{G}$ is the probability of the first event, and the second event implies that the aggregation can be spoiled by Byzantine workers (but clipping bounds the \\*harm\"). Finally, we use $\\mathcal{P}_{\\mathcal{G}_{C}^{k}}$ in the computation of some conditional expectations when the first event occurs. The mentioned probabilities can be easily computed for some special cases. For example, if $C=1$ , then $p_{G}=G/n$ and $\\mathcal{P}_{\\mathcal{G}_{C}^{k}}={{1}/{G}}$ ; if $C=2$ then $p_{G}=G(G{-}1){\\big/}n(n{-}1)$ and $\\mathcal{P}_{\\mathcal{G}_{C}^{k}}={{}^{2}\\!/G}$ ; finally, if $C=n$ , then $p_{G}=1$ and $\\mathcal{P}_{\\mathcal{G}_{C}^{k}}=1$ ", "page_idx": 5}, {"type": "text", "text": "The next theorem is our main convergence result for general unbiased compression operators. ", "page_idx": 5}, {"type": "text", "text": "Theorem 4.1. Let Assumptions 2.3, 2.5, 2.6 hold, $\\begin{array}{r c l}{\\lambda_{k+1}}&{=}&{2{\\mathcal{L}}\\left\\|x^{k+1}-x^{k}\\right\\|}\\end{array}$ and $\\widehat{C}\\ \\geq$ $\\operatorname*{max}\\{1,\\delta_{r e a l}n/\\delta\\}$ Assumethat $0<\\gamma\\leq1/\\mathcal{L}(1{+}\\sqrt{A})$ ,whereconstant $A$ isdefinedas ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{A}&{=}&{\\displaystyle\\frac{32p_{G}G\\mathcal{P}_{\\mathcal{G}_{C}^{k}}}{p^{2}(1-\\delta)C}\\left(30\\omega+11\\right)(1+2c\\delta)+\\frac{16(1-p_{G})(1+4F_{A}^{2})}{p^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Thenfor all $K\\geq0$ the iterates produced by Byz-VR-MARINA-PP (Algorithm $^{\\,l}$ )satisfy ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[\\left\\|\\nabla f\\left(\\widehat{x}^{K}\\right)\\right\\|^{2}\\right]\\leq\\frac{2\\Phi^{0}}{\\gamma(K+1)}+\\frac{4\\widehat{D}\\zeta^{2}}{p},}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "$\\begin{array}{r}{\\widehat{D}=\\frac{2\\delta\\mathcal{P}_{\\mathcal{G}_{\\widehat{C}}^{k}}}{1-\\delta}\\left(\\frac{6c G}{\\widehat{C}}+p\\right)+\\widetilde{D},}\\end{array}$ $\\widetilde{D}=0$ ${\\widehat{C}}=n$ $\\begin{array}{r}{\\widetilde D=\\frac{\\mathcal P_{\\mathcal G_{\\widehat{C}}}k^{\\,}G}{(1-\\delta)\\widehat C}}\\end{array}$ ${\\widehat{C}}=n$ ${\\widehat{x}}^{K}$ is chosen uniformly at random from $x^{0},x^{1},\\ldots,x^{K}$ ,and $\\begin{array}{r}{\\Phi^{0}=f\\left(x^{0}\\right)-f^{*}+\\frac{2\\gamma}{p}\\left\\|g^{0}-\\nabla f\\left(x^{0}\\right)\\right\\|^{2}}\\end{array}$ $I f,$ in addition, Assumption 2.7 holds and $0\\,<\\,\\gamma\\,\\leq\\,^{1}\\!/\\!\\mathcal{L}(1\\!+\\!\\sqrt{2A})$ then for all $K\\ge0$ the iterates produced by Byz-VR-MARINA-PP (Algorithm $^{\\,l}$ )with $\\rho=\\operatorname*{min}\\left\\{\\gamma\\mu,\\frac{p}{8}\\right\\}$ satisfy ", "page_idx": 6}, {"type": "text", "text": "where ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\mathbb{E}\\left[f\\left(x^{K}\\right)-f\\left(x^{*}\\right)\\right]\\leq\\left(1-\\rho\\right)^{K}\\Phi^{0}+\\frac{4\\widehat{D}\\zeta^{2}\\gamma}{p\\rho},}\\\\ {=f\\left(x^{0}\\right)-f^{*}+\\frac{4\\gamma}{p}\\left\\Vert g^{0}-\\nabla f\\left(x^{0}\\right)\\right\\Vert^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The above theorem establishes similar guarantees to the current SOTA ones obtained for Byz-VRMARINA. That is, in the general non-convex case, we prove ${\\mathcal O}(^{1}\\!/\\!K)$ rate, which is optimal (Arjevani et al., 2023), and for $\\mathrm{PE}$ -functions we derive linear convergence result to the neighborhood depending on the heterogeneity. The size of this neighborhood matches the one derived for Byz-VR-MARINA by Gorbunov et al. (2023). However, since our result is obtained considering the challenging scenario of partial participation of clients, the maximal theoretically allowed stepsize in our analysis of Byz-VR-MARINA-PP is smaller than the one from (Gorbunov et al., 2023). ", "page_idx": 6}, {"type": "text", "text": "In particular, the second term in the constant $A$ appears due to the partial participation, and the whole expression for $A$ is proportional to $1/p^{2}$ . In contrast, a similar constant $A$ from the result for Byz-VR-MARINA is proportional to $^1\\!/\\!p$ , which can be noticeably smaller than $1/p^{2}$ . Indeed, to make the expected number of clients participating in the communication round equal to ${\\mathcal{O}}(C)$ , to make the expected number of stochastic oracle calls equal to ${\\mathcal{O}}(b)$ , and to make the expected number of transmitted components for each worker taking part in the communication round equal $\\mathcal{O}(\\zeta_{\\mathcal{Q}})$ \uff0c parameter $p$ should be chosen as $p=\\operatorname*{min}\\{C/n,b/m,\\zeta e/d\\}$ , where the latter term in the minimum often equals to $\\Theta\\big({}^{1}\\!/(\\omega\\!+\\!1)\\big)$ (Gorbunov et al., 2021). Therefore, in some scenarios, $p$ can be small. ", "page_idx": 6}, {"type": "text", "text": "Next, in the special case of full participation, wehave $C={\\widehat{C}}\\,=\\,n$ $p_{G}\\,=\\,\\mathcal{P}_{\\mathcal{G}_{C}^{k}}\\,=\\,1$ meaning that $A=\\Theta\\big((1{+}\\omega)(1{+}c\\delta)\\big/p^{2}\\big)$ for Byz-VR-MARINA-PP. In contrast, the corresponding constant for Byz-VR-MARINA is of the order $\\Theta\\big((1{+}\\omega)/p n+(1{+}\\omega)c\\delta\\big/p^{2}\\big)$ , which is strictly better than our bound. In this special case, we do not recover the result for Byz-VR-MARINA. ", "page_idx": 6}, {"type": "text", "text": "Such a complexity deterioration can be explained as follows: the presence of clipping introduces additional technical difficulties in the analysis, resulting in a reduced step size compared to Byz-VRMARINA,evenwhen $C={\\widehat{C}}=n$ . To achieve a more favorable convergence rate, particularly in scenarios of complete participation, we also establish the results under Assumption 2.4. ", "page_idx": 6}, {"type": "text", "text": "Theorem 4.2. Let Assumptions 2.3, 2.4, 2.5, 2.6 hold, $\\lambda_{k+1}\\:=\\:D_{Q}\\mathcal{L}\\:\\|x^{k+1}-x^{k}\\|.$ and $\\widehat{C}\\,\\geq$ $\\operatorname*{max}\\{1,\\delta_{r e a l}n/\\delta\\}$ Assumethat $0<\\gamma\\leq1/\\mathcal{L}(1{+}\\sqrt{A})$ , where constant $A$ equals ", "page_idx": 6}, {"type": "equation", "text": "$$\nA\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!=\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\frac{4p_{G}G\\mathcal{P}_{\\mathcal{G}_{C}^{k}}}{p(1-\\delta)C}\\left(\\frac{3\\omega+2}{(1-\\delta)C}+\\frac{8(5\\omega+4)c\\delta}{p}\\right)+\\frac{8(1-p_{G})(2+F_{A}^{2}D_{Q}^{2})}{p^{2}}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Thenforall $K\\geq0$ the iterates produced by Byz-VR-MARINA-PP (Algorithm $^{\\,l}$ )satisfy ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[\\left\\|\\nabla f\\left(\\widehat{x}^{K}\\right)\\right\\|^{2}\\right]\\leq\\frac{2\\Phi^{0}}{\\gamma(K+1)}+\\frac{2\\widehat{D}\\zeta^{2}}{p},}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Where D =1 $\\begin{array}{r}{\\widehat{D}=\\frac{2\\delta\\mathcal{P}_{\\mathcal{G}_{\\widehat{C}}^{k}}}{1-\\delta}\\left(\\frac{6c G}{\\widehat{C}}+p\\right)+\\widetilde{D}}\\end{array}$ where $\\widetilde{D}=0$ when ${\\widehat{C}}=n$ $\\begin{array}{r}{\\widetilde D=\\frac{\\mathcal P_{\\mathcal G_{\\widehat{C}}}k^{\\,}G}{(1-\\delta)\\widehat C}}\\end{array}$ when ${\\widehat{C}}=n$ and ${\\widehat{x}}^{K}$ is chosen uniformly atrandomfrom $x^{0},x^{1},\\ldots,x^{K}$ and $\\begin{array}{r}{\\Phi^{0}=f\\left(x^{0}\\right)-f^{*}+\\frac{\\gamma}{p}\\left\\Vert g^{0}-\\nabla f\\left(x^{0}\\right)\\right\\Vert^{2}}\\end{array}$ ", "page_idx": 6}, {"type": "text", "text": "If, in addition, Assumption 2.7 holds and $0\\,<\\,\\gamma\\,\\leq\\,^{1}\\!/\\!\\mathcal{L}(1\\!+\\!\\sqrt{2A})$ ,thenfor all $K\\ge0$ theiterates produced by Byz-VR-MARINA-PP (Algorithm $^{\\,l}$ )satisfywith $\\rho=\\operatorname*{min}\\left\\{\\gamma\\mu,{\\frac{p}{4}}\\right\\}$ ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\mathbb{E}\\left[f\\left(x^{K}\\right)-f\\left(x^{*}\\right)\\right]\\leq\\left(1-\\rho\\right)^{K}\\Phi^{0}+\\frac{2\\widehat{D}\\zeta^{2}\\gamma}{p\\rho},}\\\\ {\\Phi^{0}=f\\left(x^{0}\\right)-f^{*}+\\frac{2\\gamma}{p}\\left\\Vert g^{0}-\\nabla f\\left(x^{0}\\right)\\right\\Vert^{2}.}\\end{array}\n$$where ", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "With Assumption 2.4, vectors $\\{\\mathcal{Q}(\\widehat{\\Delta}_{i}(x^{k+1},x^{k}))\\}_{i\\in\\mathcal{G}_{C}^{k}}$ can be upper bounded by $D_{Q}{\\mathcal{L}}\\left\\|x^{k+1}-x^{k}\\right\\|$ Using this fact, one can take the clipping level sufficiently large such that it is turned off for the regular workers. This allows us to simplify the proof and remove $^1\\!/\\!p$ factor in front of the terms not proportional to $\\delta$ or to $1-p_{G}$ in the expression for $A$ that can make the stepsize larger. However, the second term in (7) can be larger than (4), since it depends on potentially large constant $D_{Q}$ Therefore, the rates of convergence from Theorems 4.1 and 4.2 cannot be compared directly. We also highlight that the clipping level from Theorem 4.2 is in general larger than the clipping level from Theorem 4.1 and, thus, it is expected that with full participation Theorem 4.2 gives better results than Theorem 4.1: the bias introduced due to the clipping becomes smaller with the increase of the clipping level. However, in the partial participation regime, the price for this is a decrease of the stepsize to compensate for the increased harm from Byzantine clients in situations when they form a majority. Further discussion of the technical challenges we overcame is deferred to Appendix E.3. ", "page_idx": 7}, {"type": "text", "text": "Nevertheless in the case of full partipation, we have $C\\,=\\,{\\widehat{C}}\\,=\\,n$ \uff0c $p_{G}\\,=\\,\\mathcal{P}_{\\mathcal{G}_{C}^{k}}\\,=\\,\\mathcal{P}_{\\mathcal{G}_{\\widehat{C}}^{k}}\\,=\\,1$ meaning that $A=\\Theta(^{(1+\\omega)}/p n+^{(1+\\omega)c\\delta}/p^{2})$ in Theorem 4.2. That is, in this case, we recover the result of Byz-VR-MARINA. More generally, if $p_{G}=1$ , which is equivalent to $C\\geq\\operatorname*{max}\\{1,\\delta_{\\mathrm{real}}n/\\delta\\}$ then $\\mathcal{P}_{\\mathcal{G}_{C}^{k}}\\,=\\,\\mathrm{Prob}\\{i\\,\\in\\,\\mathcal{G}_{C}^{k}\\}\\,=\\,\\operatorname*{min}\\{1,C/G\\}$ \uff0c $\\mathcal{P}_{\\mathcal{G}_{\\widehat{C}}^{k}}\\,=\\,\\mathrm{Prob}\\{i\\,\\in\\,\\mathcal{G}_{\\widehat{C}}^{k}\\}\\,=\\,\\operatorname*{min}\\{1,\\widehat{C}/G\\}$ and we have $A=\\Theta(^{(1+\\omega)}/p C+^{(1+\\omega)c\\delta}/p^{2})$ . Here, the first term in $A$ is $n/C$ worse than the corresponding term for Byz-VR-MARINA. However, the second term in $A$ matches the corresponding term for Byz-VR-MARINA. Moreover, this term is the main one if $c\\delta\\geq p/C$ , which is typically the case since parameter $p$ is often small $(p=\\operatorname*{min}\\{C/\\hat{C},b/m,\\zeta e/d\\})$ . In such cases, Byz-VR-MARINA-PP has the same rate of convergence as Byz-VR-MARINA while utilizing, on average, just $O(C)$ workers at each step in contrast to Byz-VR-MARINA that uses $n$ workers at each step. That is, in some cases, partial participationisprovablybeneficialforByz-VR-MARINA-PP. ", "page_idx": 7}, {"type": "text", "text": "Byz-VR-MAR $\\Vvdash\\Delta+$ : simplified version of Byz-VR-MARINA. In Appendix F, we propose a simplified version of Byz-VR-MARINA called Byz-VR-MARINA $^{+}$ (Algorithm 3). The only difference is related to Line 10 of the method: when $c_{k}=0$ , Byz-VR-MARINA $^+$ computes just the average of $\\left\\{\\mathsf{c l i p}_{\\lambda_{k+1}}\\left(\\mathcal{Q}\\left(\\widehat{\\Delta}_{i}(x^{k+1},x^{k})\\right)\\right)\\right\\}_{i\\in S_{k}}$ instead of robust agregation, while keeps using ARAgg when $c_{k}=1$ . Of course, when $c_{k}=0$ and at least one Byzantine worker is sampled, then the step can be useless, but the \u201charm\" of this step is bounded due to the clipping. However, in certain regimes (e.g., when $C$ is small enough and the number of Byzantine workers is much smaller than the number of regular workers), the probability of sampling only regular workers is larger than sampling at least one Byzantine worker when $c_{k}=0$ , meaning that with high enough probability the resulting estimator has no additional bias coming from the robust aggregation. We formally analyze Byz-VR-MARINA $^+$ and show that such a modification of the method leads to better theoretical results (especially when $C$ is small). In particular, in the settings of Theorem 4.2, we prove that Byz-VR-MARINA $^+$ exhibits the same $\\bar{\\mathcal{O}}(1/\\kappa)$ rate but converges to $\\mathcal{O}(1/p)$ smaller neighborhood when ${\\widehat{C}}=n$ i.e.,the neighborhood term for Byz-VR-MARINA $^+$ is optimal (Karimireddy et al., 2022; Allouah et al., 2024b). Moreover, our results for Byz-VR-MAR $\\Vvdash\\vartriangle{\\Delta+}$ allow larger stepsizes when $C$ is small enough. For further details and complete proofs, we refer to Appendix F. ", "page_idx": 7}, {"type": "text", "text": "Extensions without full-batch gradient computations. The proposed methods - Byz-VRMARINA and Byz-VR-MAR $\\Vvdash\\Delta+$ - have a common limitation related to the full-batch gradient computation with probability $p$ . Although this probability is typically small, even one full-gradient computation can be very expensive for certain problems. To address this issue, we propose the modifications of Byz-VR-MARINA and Byz-VR-MARINA+ without full-batch gradient computations at all (see Algorithms 4 and 5 in Appendix G). That is, these modifications differ from Byz-VR-MARINA and Byz-VR-MARINA $^+$ in Line 8 only: when $c_{k}\\,=\\,1$ , every good worker $i$ from $S_{k}$ computes and sends to the server $b^{\\prime}$ -size mini-batched stochastic gradient estimator $\\widetilde{\\nabla}f_{i}(x^{k+1})$ of $\\nabla f_{i}(x^{k+1})$ ", "page_idx": 7}, {"type": "image", "img_path": "G8aS48B9bm/tmp/6241e75e85180251e3458f1bd6d7492af8f59b33542ae385dc386af8c45e93cf.jpg", "img_caption": ["Figure 1: The optimality gap $f(x^{k})-f(x^{*})$ for 3 different scenarios. We use coordinate-wise mean with bucketing equal to 2 as an aggregation and shift-back as an attack. We use the $\\mathrm{a9a}$ dataset, where each worker accesses the full dataset with 15 good and 5 Byzantine workers. We do not use any compression. In each step, we sample $20\\%$ of clients uniformly at random to participate in the given round unless we specifically mention that we use full participation. Left: Linear convergence of Byz-VR-MARINA-PP with clipping versus non-convergence without clipping. Middle: Full versus partial participation, showing faster convergence with clipping. Right: Clipping multiplier $\\lambda$ sensitivity, demonstrating consistent linear convergence across varying $\\lambda$ values. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Under the additional assumption that the variance of $\\widetilde{\\nabla}f_{i}(x^{k+1})$ is uniformly bounded by $\\sigma^{2}/b^{\\prime}$ ,which is a standard assumption for variance-reduced methods without full-batch gradient computations (Fang et al., 2018; Cutkosky and Orabona, 2019; Li et al., 2021; Gorbunov et al., 2021), we prove that both methods converge similarly as in the case of the (periodical) full-batch gradient computations but to the neighborhood having an additional term proportional to $\\left(\\boldsymbol{c}\\delta+\\mathcal{P}_{\\mathcal{G}_{\\widehat{C}}^{k}}\\,\\boldsymbol{G}/\\widehat{\\boldsymbol{C}}^{2}\\right)\\,\\sigma^{2}/b^{\\prime}$ .For further details and complete proofs, we refer to Appendix G. ", "page_idx": 8}, {"type": "text", "text": "Heuristic extension of Byz-VR-MARINA-PP. In this short remark, we illustrate how the proposed clipping technique can be applied to a general class of Byzantine-robust methods to adapt them to the case of partial participation. Consider the methods having the following update rule: $x^{k+1}=$ $x^{k}-\\gamma\\cdot\\mathsf{A g g}(\\{g_{i}^{k}\\}_{i\\in[n]}^{^{\\bullet}})$ wWhere $\\{g_{i}^{k}\\}_{i\\in[n]}$ are thevectorsreceivedfromworkersat iteration $k$ and Agg is some aggregation rule. A vast majority of existing Byzantine-robust methods fit this scheme. In the case of partial participation of clients, we propose to modify the scheme as follows: ", "page_idx": 8}, {"type": "equation", "text": "$$\nx^{k+1}=x^{k}-\\gamma g^{k},\\quad{\\mathrm{where~}}g^{k}:=g^{k-1}+{\\tt A g g}\\left(\\left\\{{\\tt c l i p}_{\\lambda_{k}}(g_{i}^{k}-g^{k-1})\\right\\}_{i\\in S_{k}}\\right),\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $S_{k}\\subseteq[n]$ is a subset of clients participating in round $k$ and $\\{\\lambda_{k}\\}_{k\\ge0}$ is sequence of clipping parameters specified by the server. In particular, Byz-VR-MARINA-PP can be seen as an application of scheme (10) to Byz-VR-MARINA (up to a minor modification when $c_{k}=1$ inByz-VR-MARINA) With $\\lambda_{k+1}=\\lambda\\|x^{k+1}-x^{k}\\|$ We suggest to use $\\lambda_{k+1}=\\lambda\\|x^{k+1}-x^{k}\\|$ with tunable parameter $\\lambda>0$ for other methods as well. ", "page_idx": 8}, {"type": "text", "text": "5 Numerical Experiments ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Firstly, we showcase the benefits of employing clipping to remedy the presence of Byzantine workers and partial participation. For this task, we consider the standard logistic regression model with $\\ell_{2}$ -regularization, i.e., $f_{i,j}(x)\\,=\\,-y_{i,j}\\log(h(x,a_{i,j}))-(1-y_{i,j})\\log(1-\\bar{h}(x,a_{i,j}))+\\eta\\|x\\|^{2}.$ where $y_{i,j}\\;\\in\\;\\{0,1\\}$ is the label, $a_{i,j}\\,\\in\\,\\mathbb{R}^{d}$ represents the feature vector, $\\eta$ is the regularization parameter, and $h(x,a)=1/(1{+}e^{-{\\,a^{\\top}}x})$ . This objective is smooth, and for $\\eta>0$ , it is also strongly convex, satisfying the $\\mathrm{PE}$ -condition. We consider the a9a LIBSVM dataset (Chang and Lin, 2011) and set $\\eta=0.01$ . In the experiments, we focus on an important feature of Byz-VR-MARINA-PP: it has linear convergence for homogeneous datasets across clients even in the presence of Byzantine workers and partial participation, as shown in Theorems 4.1 and 4.2. ", "page_idx": 8}, {"type": "text", "text": "To demonstrate this experimentally, we consider the setup with 15 good workers and 5 Byzantines, each worker can access the entire dataset, and the server uses coordinate-wise median with bucketing as the aggregator (see also Appendix C). For the attack, we propose a new attack that we refer to as the shift-back attack, which acts in the following way. If Byzantine workers are in the majority in the current round $k$ , then each Byzantine worker sends $\\bar{x}^{0}-x^{k}$ . Otherwise, they follow protocol and act as benign workers. Further experimental details are deferred to Appendix $\\mathrm{H}$ ", "page_idx": 8}, {"type": "image", "img_path": "G8aS48B9bm/tmp/1cf1bba9b6a49b5625b66842a9c5d26d94a9cf5f5bac155bde97a090ce930479.jpg", "img_caption": ["Figure 2: Training loss of 2 aggregation rules (CM, RFA) under 2 attacks (BF, SHB) on the MNIST dataset under heterogeneous data split with 20 clients, 5 of which are malicious. Additional experiments on CIFAR10 are provided in Appendix H. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "We compare our Byz-VR-MARINA-PP with its version without clipping. We note that the setup that we consider is the most favorable in terms of minimized variance in terms of data and gradient heterogeneity. We show that even in this simplest setup, the method without clipping does not converge since there is no method that can withstand the Byzantine majority. Therefore, any more complex scenario would also fall short using our simple attack. On the other hand, we show that once clipping is applied, Byz-VR-MARINA-PP is able to converge linearly to the exact solution, complementing our theoretical results. ", "page_idx": 9}, {"type": "text", "text": "Figure 1 showcases these observations. On the left, we can see Byz-VR-MARINA-PP converges linearly to the optimal solution, while the version without clipping remains stuck at the starting point since Byzantines are always able to push the solution back to the origin since they can create the majority in some rounds. In the middle plot, we compare the full participation scenario in which all the clients participate in each round, which does not require clipping since, in each step, we are guaranteed that Byzantines are not in the majority, to partial participation with clipping. We can see, when we compare the total number of computations (measured in epochs), Byz-VR-MARINA-PP leads to faster convergence even though we need to employ clipping. Finally, in the right plot, we measure the sensitivity of clipping multiplier $\\lambda$ We can see that Byz-VR-MARINA-PP is not very sensitive to $\\lambda$ in terms of convergence, i.e., for all the values of $\\lambda$ , we still converge linearly. However, the suboptimal choice of $\\lambda$ leads to slower convergence. ", "page_idx": 9}, {"type": "text", "text": "Furthermore, we also realize that other attacks and more complicated experiments could potentially damage clipping more than methods not using clipping. Therefore, we provide additional experiments with neural networks and different attacks in heterogeneous settings. For our experimental setup, we follow (Karimireddy et al., 2021). However, when working with neural networks, the choice of standard variance reduction is not effective (Defazio and Bottou, 2019). Therefore, we use Byzantine Robust Momentum SGD (Karimireddy et al., 2021) as an underlying optimization method; see (10). ", "page_idx": 9}, {"type": "text", "text": "We consider the MNIST dataset (LeCun and Cortes, 1998) with heterogeneous splits with 20 clients, 5 of which are malicious. For the attacks, we consider A Little is Enough (ALIE) (Baruch et al., 2019), Bit Flipping (BF), and aforementioned Shift-Back (SHB). For the aggregations, we consider coordinate median (CM) (Chen et al., 2017) and robust federated averaging (RFA) (Pillutla et al., 2022) with bucketing. ", "page_idx": 9}, {"type": "text", "text": "From Figure 2, we can see that clipping does not lead to performance degradation. On the contrary, clipping performs on par or better than its variant without clipping. Furthermore, we can see that no robust aggregator is able to withstand the shift-back attack without clipping. ", "page_idx": 9}, {"type": "text", "text": "6  Conclusion and Future Work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work makes an important step in the direction of achieving Byzantine robustness under the partial participation of clients. However, some important questions remain open. First of all, it will be interesting to understand whether the derived bounds can be further improved in terms of the dependenceon $\\omega,m$ ,and $C$ . Next, it would be interesting to rigorously prove that our heuristic works for SGD with client momentum (Karimireddy et al., 202i, 2022) and other Byzantine-robust methods. Finally, studying other participation patterns (non-uniform sampling/arbitrary client participation) is also a very prominent direction for future research. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "The work of G. Malinovsky and P. Richtarik was supported by funding from King Abdullah University of Science and Technology (KAUST): i) KAUST Baseline Research Scheme, ii) Center of Excellence for Generative AI, under award number 5940, i) SDAIA-KAUST Center of Excellence in Artificial Intelligence and Data Science. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Abadi, M., Chu, A., Goodfellow, I., McMahan, H. B., Mironov, I., Talwar, K., and Zhang, L. (2016). Deep learning with differential privacy. In Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security, pages 308-318. (Cited on page 20)   \nAgarwal, A. and Duchi, J. C. (2011). Distributed delayed stochastic optimization. Advances in neural information processing systems, 24. (Cited on page 20)   \nAjalloeian, A. and Stich, S. U. (2020). On the convergence of SGD with biased gradients. arXiv preprint arXiv:2008.00051. (Cited on page 20)   \nAlistarh, D., Allen-Zhu, Z., and Li, J. (2018). Byzantine stochastic gradient descent. In Advances in Neural Information Processing Systems, pages 4618-4628. (Cited on pages 2 and 19)   \nAlistarh, D., Grubic, D., Li, J., Tomioka, R., and Vojnovic, M. (2017). QSGD: Communicationefficient SGD via gradient quantization and encoding. In Advances in Neural Information Processing Systems, volume 30. (Cited on pages 4 and 20)   \nAllen-Zhu, Z., Ebrahimian, F., Li, J., and Alistarh, D. (2021). Byzantine-resilient non-convex stochastic gradient descent. In International Conference on Learning Representations. (Cited on pages 2 and 19)   \nAllouah, Y., Farhadkhani, S., Guerraoui, R., Gupta, N, Pinot, R., Rizk, G, and Voitovych, S. (2024a). Tackling byzantine clients in federated learning. arXiv preprint arXiv:2402.12780. (Cited on page 3)   \nAllouah, Y., Farhadkhani, S., Guerraoui, R., Gupta, N., Pinot, R., and Stephan, J. (2023). Fixing by mixing: A recipe for optimal byzantine ml under heterogeneity. In International Conference on Artificial Intelligence and Statistics, pages 1232-1300. (Cited on pages 2, 4, and 19)   \nAllouah, Y., Guerraoui, R., Gupta, N., Pinot, R., and Rizk, G. (2024b). Robust distributed learning: tight error bounds and breakdown point under data heterogeneity. Advances in Neural Information Processing Systems, 36. (Cited on pages 8 and 63)   \nArjevani, Y., Carmon, Y., Duchi, J. C., Foster, D. J., Srebro, N., and Woodworth, B. (2023). Lower bounds for non-convex stochastic optimization. Mathematical Programming, 199(1-2):165-214. (Cited on pages 6 and 7)   \nAtre, M., Jha, B., and Rao, A. (2021). Distributed deep learning using volunteer computing-like paradigm. arXiv preprint arXiv:2103.08894. (Cited on page 1)   \nBaruch, G., Baruch, M., and Goldberg, Y. (2019). A little is enough: Circumventing defenses for distributed learning. In Advances in Neural Information Processing Systems, volume 32. (Cited on pages 2, 10, and 74)   \nBasu, D., Data, D., Karakus, C., and Diggavi, S. (2019). Qsparse-local-SGD: Distributed SGD with quantization, sparsification and local computations. In Advances in Neural Information Processing Systems, volume 32. (Cited on page 20)   \nBernstein, J., Zhao, J., Azizzadenesheli, K., and Anandkumar, A. (2019). signSGD with majority vote is communication effcient and fault tolerant. In International Conference on Learning Representations. (Cited on page 20)   \nBeznosikov, A., Horvath, S., Richtarik, P, and Safaryan, M. (2020). On biased compression for distributed learning. arXiv preprint arXiv:2002.12410. (Cited on pages 4 and 20)   \nBlanchard, P, El Mhamdi, E. M., Guerraoui, R., and Stainer, J. (2017). Machine learning with adversaries: Byzantine tolerant gradient descent. In Advances in Neural Information Processing Systems, volume 30. (Cited on pages 2 and 4)   \nBonawitz, K., Ivanov, V., Kreuter, B., Marcedone, A., McMahan, H. B., Patel, S., Ramage, D., Segal, A., and Seth, K. (2017). Practical secure agregation for privacy-preserving machine learning. In Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security, pages 1175-1191. (Cited on page 2)   \nBoubouh, K, Boussetta, A., Gupta, N., Maurer, A., and Pinot, R. (2022). Democratizing machine learning: Resilient distributed learning with heterogeneous participants. In 2022 41st International Symposium on Reliable Distributed Systems (SRDS), pages 94-120. IEEE. (Cited on page 3)   \nChang, C.-C. and Lin, C.-J. (2011). Libsvm: alibrary for support vector machines. ACM transactions on inteligent systems and technology (TIST), 2(3):1-27. (Cited on page 9)   \nChen, L., Wang, H., Charles, Z., and Papailiopoulos, D. (2018). Draco: Byzantine-resilient distributd training via redundant gradients. In International Conference on Machine Learning, pages 903-912. (Cited on page 19)   \nChen, X., Wu, S. Z., and Hong, M. (2020). Understanding gradient clipping in private SGD: A geometric erspective In Advances in Neural Information Processing Systems, volume 3, pages 13773-13782. (Cited on page 20)   \nChen, Y., Su, L., and Xu, J. (2017). Distributed statistical machine learning in adversarial settings: Byzantine gradient descent.Proceedings of the ACM on Measurement and Analysis of Computing Systems, 1(2):1-25. (Cited on pages 10 and 74)   \nCutkosky, A. and Orabona, F. (2019). Momentum-based variance reduction in non-convex SGD. In Advances in Neural Information Processing Systems, volume 32. (Cited on pages 9, 19, and 66)   \nDamaskinos, G., El-Mhamdi, E.-M., Guerraoui, R., Guirguis, A., and Rouault, S. (2019). Aggregathor: Byzantine machine learning via robust gradient aggregation. Proceedings of Machine Learning and Systems, 1:81-106. (Cited on page 2)   \nDamaskinos, G., Guerraoui, R., Patra, R., Taziki, M., et al. (2018). Asynchronous byzantine machine learning (the case of sgd). In International Conference on Machine Learning, pages 1145-1154. PMLR. (Cited on page 20)   \nData, D. and Diggavi, S. (2021). Byzantine-resilient high-dimensional SGD with local iterations on heterogeneous data. In International Conference on Machine Learning, pages 2478-2488. PMLR. (Cited on pages 3, 6, 19, and 20)   \nDefazio, A., Bach, F., and Lacoste-Julien, S. (2014). SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives. In Advances in Neural Information Processing Systems, volume 27. (Cited on pages 2 and 19)   \nDefazio, A andBottou,  209). n the ineffctivenes of variane edd optmizao for de learning. Advances in Neural Information Processing Systems, 32. (Cited on page 10)   \nDemidovich, Y., Malinovsky, G., Sokoloy, I., and Richtarik, P. (2023). A guide through the Zoo of biased SGD. In Advances in Neural Information Processing Systems, volume 36. (Cited on page 20)   \nDiskin, M., Bukhtiyarov, A., Ryabinin, M., Saulnier, L., Lhoest, Q., Sinitsin, A., Popov, D., Pyrkin, D., Kashirin, M, Borzunov, A., del Moral, A. V, Mazur, D., Kobelev, I., Jrnite, Y., Wolf, T, and Pekhimenko, G. (2021a). Distributed deep learning in open collaborations. In Advances in Neural Information Processing Systems, volume 34, pages 7879-7897. (Cited on page 1)   \nDiskin, M., Bukhtiyarov, A., Ryabinin, M, Saulnier, L., Sinitsin, A., Popov, D., Pyrkin, D. V, Kashirin, M, Borzunoy, A., Villanva del Moral, A, et al. 2021b). Distributed deep learning n open collaborations. InAdvances in Neural Information Processing Systems, volume 34, pages 7879-7897. (Cited on page 5)   \nEl-Mhamdi, E.M., Farhadkhani, S., Guerraoui, R, Guirguis, A, Hoang, L-N., and Rouault, S. 2021). Collaborative learing  th jungle dcenralzed,byzantne,heterogeousasychronous and nonconvex learning). Advances in neural information processing systems, 34:25044-25057. (Cited on page 3)   \nFang, C., Li, C. J., Lin, Z., and Zhang, T. (2018). Spider: Near-optimal non-convex optimization via stochastic path-integrated differential estimator InAdvances inNeural Information Procesing Systems, volume 31. (Cited on pages 6, 9, and 71)   \nFang, M., Liu, J, Gong, N. Z., and Bentley, E.S. (2022). AFLGuard: Byzantine-robust asynchronous federatedlearning In Proceedings of the 38th Annual Computer Security Applications Conference, pages 632-646. (Cited on page 20)   \nFatkhullin, I., Sokolov, I., Gorbunov, E., Li, Z., and Richtarik, P. (2021). EF21 with bells & whistles: Practical algorithmic extensions of modern error feedback. arXiv preprint arXiv:2110.03294. (Cited on page 20)   \nGhadimi, S. and Lan, G. (2013). Stochastic first-and zeroth-order methods for nonconvex stochastic programming. SIAM journal on optimization, 23(4):2341-2368. (Cited on page 66)   \nGhosh, A., Maity, R. K., Kadhe, S., Mazumdar, A., and Ramchandran, K. (2021). Communicationefficient and byzantine-robust distributed learning with error feedback. IEEE Journal on Selected Areas in Information Theory,2(3):942-953. (Cited on page 20)   \nGhosh, A., Maity, R. K., and Mazumdar, A. (2020). Distributed newton can communicate less and resist Byzantine workers. In Advances in Neural Information Processing Systems, volume 33, pages 18028-18038. (Cited on page 20)   \nGoodall, W. (1951). Televisionby pulse code modulation. Bell System Technical Journal, 0(1):3349. (Cited on page 4)   \nGorbunoy, E., Borzunov, A., Diskin, M., and Ryabinin, M. (2022). Secure distributed training at scale. In International Conference on Machine Learning. (arXiv preprint arXiv:2106.11257, 2021). (Cited on pages 2 and 19)   \nGorbunov, E., Burlachenko, K. P., Li, Z., and Richtarik, P. (2021). MARINA: Faster non-convex distributed leaming withcomressionIn Intenational Conference onMachineLearningags 3788-3798. (Cited on pages 7, 9, 20, 66, and 71)   \nGorbunov, E., Danilova, M., and Gasnikov, A. (2020). Stochastic optimization with heavy-tailed noise via accelerated gradient clipping. In Advances in Neural Information Processing Systems, volume 33, pages 15042-15053. Cited on pages 20 26,and 57)   \nGorbunov, E., Horvath, S., Richtarik, P, and Gidel, G. (2023). Variance reduction is an antidote to Byzantines: Betterates, weaker assumptions and communication compression as a cherry on the top. International Conference on Learning Representations. (Cited on pages 2, 3, 4, 5, 6, 7, 19, 20, 25, 26, and 74)   \nGower, R. M., Schmidt, M., Bach, F, and Richtarik, P. (2020). Variance-reduced methods for machine learning. Proceedings of the IEEE, 108(11):1968-1983. (Cited on page 19)   \nHaddadpour, F., Kamani, M. M., Mokhtari, A., and Mahdavi, M. (2021). Federated learning with compression: Unified analysis and sharp guarantees. In International Conference on Artifcial Intelligence and Statistics, pages 2350-2358. (Cited on page 20)   \nHe, K., Zhang, X., Ren, S., and Sun, J. (2016). Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770778. (Cited on page 74)   \nHe, L., Karimireddy, S. P., and Jaggi, M. (2022). Byzantine-robust decentralized learning via ClippedGossip. arXiv preprint arXiv:2202.01545. (Cited on page 19)   \nHorvath, S., Kovalev, D., Mishchenko, K., Stich, S., and Richtarik, P. (2023). Stochastic distributed learning with gradient quantization and variance reduction. Optimization Methods and Software, 38(1). (arXiv preprint arXiv:1904.05115, 2019). (Cited on pages 2, 4, 19, and 20)   \nIslamov, R., Qian, X., and Richtarik, P. (2021). Distributed second order methods with fast rates and compressed communication. In International Conference on Machine Learning, pages 4617-4628. (Cited on page 20)   \nJohnson, R. and Zhang, T. (2013). Accelerating stochastic gradient descent using predictive variance reduction. In Advances in Neural Information Processing Systems, volume 26. (Cited on page 19)   \nKairouz, P., McMahan, H. B., Avent, B., Bellet, A., Bennis, M., Bhagoji, A. N., Bonawitz, K., Charles, Z., Cormode, G., Cummings, R., et al. (2021). Advances and open problems in federated learning. Foundations and Trends in Machine Learning, 14(1-2):1-2i0. (Cited on page 1)   \nKarimireddy, S. P., He, L., and Jaggi, M. (2021). Learning from history for Byzantine robust optimization. In International Conference on Machine Learning, pages 5311-5319. (Cited on pages 2, 3, 4, 10, 19, 20, and 74)   \nKarimireddy, S. P, He, L., and Jaggi, M. (2022). Byzantine-robust learning on heterogeneous datasets via bucketing. International Conference on Learning Representations. (Cited on pages 2, 3, 4, 8, 10, 19. 23, and 26)   \nKhirirat, S., Feyzmahdavian, H. R., and Johansson, M. (2018). Distributed learning with compressed gradients. arXiv preprint arXiv: 1806.06573. (Cited on pages 4 and 20)   \nKijsipongse, E., Piyatumrong, A., et al. (2018a). A hybrid gpu cluster and volunteer computing platform for scalable deep learning. The Journal of Supercomputing, 74(7):3236-3263. (Cited on page 5)   \nKijsipongse, E., Piyatumrong, A., and U-ruekolan, S. (2018b). A hybrid gpu cluster and volunteer computing platform for scalable deep learning. The Journal of Supercomputing. (Cited on page 1)   \nKoneeny, J., MeMahan, H B., Yu, F, Richtarik, P, Suresh, A.T., and Bacon,D. (2016). Feeratd learning: strategies for improving communication efficiency. In NIPS Private Multi-Party Machine Learning Workshop. (Cited on page 1)   \nKrizhevsky, A., Hinton, G., et al. (2009). Learning multiple layers of features from tiny images. (Cited on page 74)   \nLampor, L., Shostak, R., and Pease, M. (1982). The Byzantine generals problem. ACM Transactions on Programming Languages and Systems, 4(3):382-401. (Cited on page 1)   \nLeCun, Y. and Cortes, C. (1998). The mnist database of handwritten digits. http://yann.lecun.com/exb/mnist/. (Cited on pages 10 and 74)   \nLi, C. 2020). Demystifying gpt3 language model: A tecnical overview. \"https: //lambdalabs. com/blog/demystifying-gpt-3\". (Cited on page 1)   \nLi, Z., Bao, H., Zhang, X., and Richtarik, P. (2021). PAGE: A simple and optimal probabilistic gradient estimator fornonconvex optimization. In International Conference on Machine Learning, pages 6286-6295. (Cited on pages 2, 9, 19, 26,66, and 71)   \nLi, Z, Kovalev, D., Qian, X., and Richtarik, P (2020). Acceleration for comresed gradient desnt indistributed and federated optimization. InInternational ConferenceonMachine Learning,pages 5895-5904. (Cited on page 20)   \nLojasiewicz, S. (1963). A topological property of real analytic subsets. Coll. du CNRS, Les quations aux deriv\u00e9es partielles, 1i7:87-89. (Cited on page 5)   \nLyu, L., Yu, H, Ma, X., Sun, L.,Zhao, J., Yang, Q., and Yu, P S. (2020). Privacy and robustness in federated leaing: Atacks and defenses. arXiv prerint arXiv:2012.06337. (Citedon page 19)   \nMai, V. V. and Johansson, M. (2021). Stability and convergence of stochastic gradient clipping: BeyondLiphitz continuityand smtheInteationalConferenceonMachineLeag pages 7325-7335. (Cited on page 20)   \nMishchenko, K., Gorbunov, E., Takac, M., and Richtarik, P. (2019). Distributed learning with compressed gradient differences. arXiv preprint arXiv: 1901.09269. (Cited on page 20)   \nNazin, A. V., Nemirovsky, A. S., Tsybakov, A. B., and Juditsky, A. B. (2019). Algorithms of robust stochastic optimization based on mirror descent method. Automation and Remote Control, 80:1607-1627. (Cited on page 20)   \nNecoara, I., Nesterov, Y., and Glineur, F. (2019). Linear convergence of first order methods for non-strongly convex optimization. Mathematical Programming, 175:69-107. (Cited on page 5)   \nNedic, A., Bertsekas, D. P., and Borkar, V. S. (2001). Distributed asynchronous incremental subgradient methods. Studies in Computational Mathematics, 8(C):381-407. (Cited on page 20)   \nNemirovski, A. S., Juditsky, A. B., Lan, G., and Shapiro, A. (2009). Robust stochastic approximation approach to stochastic programming. SIAM Journal on Optimization, 19(4):1574-1609. (Cited on page 66)   \nNguyen, L. M., Liu, J., Scheinberg, K., and Takac, M. (2017). Sarah: A novel method for machine learning problems using stochastic recursive gradient. In International Conference on Machine Learning, pages 2613-2621. (Cited on page 19)   \nNguyen, T. D., Ene, A., and Nguyen, H. L. (2023). Improved convergence in high probability of clipped gradient methods with heavy tails. arXiv preprint arXiv:2304.01119. (Cited on page 20)   \nNiu, C., Wu, F., Tang, S., Hua, L., Jia, R., Lv, C., Wu, Z., and Chen, G. (2020). Billion-scale federated learning on mobile clients: A submodel design with tunable privacy. In Proceedings of the 26th Annual International Conference on Mobile Computing and Networking, pages 1-14. (Cited on page 2)   \nPascanu, R., Mikolov, T., and Bengio, Y. (2013). On the difficulty of training recurrent neural networks. In International Conference on Machine Learning, pages 1310-1318. (Cited on page 20)   \nPillutla, K, Kakade, S. M., and Harchaoui, Z. (2022). Robust aggregation for federated learning. IEEE Transactions on Signal Processing, 70:1142-1154. (Cited on pages 2, 10, and 74)   \nPolyak, B. T. (1963). Gradient methods for the minimisation of functionals. USSR Computational Mathematics and Mathematical Physics, 3(4):864-878. (Cited on page 5)   \nQian, X., Richtarik, P., and Zhang, T. (2021). Error compensated distributed SGD can be accelerated. In Advances in Neural Information Processing Systems, volume 34. (Cited on page 20)   \nRajput, S., Wang, H., Charles, Z., and Papailiopoulos, D. (2019). Detox: A redundancy-based framework for faster and more robust gradient aggregation. In Advances in Neural Information Processing Systems, volume 32. (Cited on page 19)   \nRegatti, J., Chen, H., and Gupta, A. (2020). ByGARS: Byzantine SGD with arbitrary number of attackers. arXiv preprint arXiv:2006.13421. (Cited on page 19)   \nRichtarik, P, Sokolov, I, and Fatkhullin, I. (2021). EF21: A new, simpler, theoretically better, and practically faster error feedback. In Advances in Neural Information Processing Systems, volume 34. (Cited on pages 20 and 22)   \nRoberts, L. (1962). Picture coding using pseudo-random noise. IRE Transactions on Information Theory, 8(2):145-154. (Cited on page 4)   \nRodriguez-Barroso, N., Martinez-Camara, E., Luzon, M., Seco, G. G., Veganzones, M. A., and Herrera, F. (2020). Dynamic federated learning model for identifying adversarial clients. arXiv preprint arXiv:2007.i5030. (Cited on page 19)   \nRyabinin, M. and Gusev, A. (2020). Towards crowdsourced training of large neural networks using decentralized mixture-of-experts. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M. F, and Lin, H., editors, Advances in Neural Information Processing Systems, volume 33, pages 3659-3672. Curran Associates, Inc. (Cited on page 1)   \nSadiev, A., Danilova, M., Gorbunov, E., Horvath, S., Gidel, G., Dvurechensky, P., Gasnikov, A., and Richtarik, P. (2023). High-probability bounds for stochastic optimization and variational inequalities: the case of unbounded variance. arXiv preprint arXiv:2302.00999. (Cited on page 20)   \nSadiev, A., Malinovsky, G., Gorbunov, E., Sokolov, I., Khaled, A., Burlachenko, K., and Richtarik, P. (2022). Federated optimization algorithms with random reshuffling and gradient compression. arXiv preprint arXiv:2206.07021. (Cited on page 20)   \nSafaryan, M., Islamov, R., Qian, X., and Richtarik, P. (2022). FedNL: Making Newton-type methods applicable to federated learning. In International Conference on Machine Learning. (arXiv preprint arXiv:2106.02969, 2021). (Cited on page 20)   \nSchmidt, M, Le Roux, N, and Bach, F. (2017). Minimizing fnite sums with the stochastic average gradient. Mathematical Programming, 162(1):83-112. (Cited on page 19)   \nSeide, F., Fu, H., Droppo, J., Li, G., and Yu, D. (2014). 1-bit stochastic gradient descent and its application to data-parallel distributed training of speech dnns. In Fifteenth Annual Conference of the International Speech Communication Association. Citeseer. (Cited on page 20)   \nStich, S. U., Cordonnier, J.-B., and Jaggi, M. (2018). Sparsified SGD with memory. In Advances in Neural Information Processing Systems, volume 31. (Cited on pages and 20)   \nSu, L. and Vaidya, N. H. (2016). Fault-tolerant multi-agent optimization: optimal iterative distributed algorithms. In Proceedings of the 2016 ACM Symposium on Principles of Distributed Computing, pages 425-434. (Cited on page 1)   \nSzlendak, R., Tyurin, A., and Richtarik, P. (2022). Permutation compressors for provably faster distributed nonconvex optimization. In International Conference on Learning Representations. (Cited on page 25)   \nVaswani, S., Bach, F, and Schmidt, M. (2019). Fast and faster convergence of SGD for overparameterized models and an accelerated perceptron. In International Conference on Artificial Intelligence and Statistics, pages 1195-1204. (Cited on page 26)   \nVogels, T., Karimireddy, S. P., and Jaggi, M. (2019). Powersgd: Practical low-rank gradient compression for distributed optimization. In Advances in Neural Information Processing Systems, volume 32. (Cited on page 20)   \nWen, W., Xu, C., Yan, F., Wu, C., Wang, Y., Chen, Y., and Li, H. (2017). Terngrad: Ternary gradients to reduce communication in distributed deep learning. In Advances in Neural Information Processing Systems, volume 30. (Cited on page 20)   \nWu, Z., Ling, Q., Chen, T., and Giannakis, G. B. (2020). Federated variance-reduced stochastic gradient descent with robustness to byzantine attacks. IEEE Transactions on Signal Processing, 68:4583-4596. (Cited on pages 2 and 4)   \nXie, C., Koyejo, O., and Gupta, I. (2020a). Fall of empires: Breaking byzantine-tolerant sgd by inner product manipulation. In Uncertainty in Artificial Intelligence, pages 261-270. (Cited on page 2)   \nXie, C., Koyejo, S., and Gupta, 1. (2020b). Zeno $^{++}$ : Robust fully asynchronous sgd. In International Conference on Machine Learning, pages 10495-10503. PMLR. (Cited on page 20)   \nXu, X. and Lyu, L. (2020). Towards building a robust and fair federated learning system. arXiv preprint arXiv:2011.10464. (Cited on page 19)   \nYang, Y.-R. and Li, W.-J. (2023). Buffered asynchronous sgd for byzantine learning. Journal of Machine Learning Research, 24(204):1-62. (Cited on page 20)   \nYin, D., Chen, Y., Kannan, R., and Bartlett, P. (2018). Byzantine-robust distributed learning: Towards optimal statistical rates. In International Conference on Machine Learning, pages 5650-5659. (Cited on pages 2 and 4)   \nZhang, J., He, T., Sra, S., and Jadbabaie, A. (2020a). Why gradient clipping accelerates training: A theoretical justification for adaptivity. In International Conference on Learning Representations. (arXiv preprint arXiv: 1905.11881, 2019). (Cited on page 20)   \nZhang, J., Karimireddy, S. P., Veit, A., Kim, S., Reddi, S., Kumar, S., and Sra, S. (2020b). Why are adaptive methods good for attention models? In Advances in Neural Information Processing Systems, volume 33, pages 15383-15393. (Cited on pages 20 and 57)   \nZhu, H. and Ling, Q. (2021). Broadcast: Reducing both stochastic and compression noise to robustify communication-efficient federated learning. arXiv preprint arXiv:2104.06685. (Cited on pages 2 and 20) ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "Contents ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Introduction 1.1Related Work ", "page_idx": 16}, {"type": "text", "text": "2  Preliminaries 3 ", "page_idx": 16}, {"type": "text", "text": "3 New Method: Byz-VR-MARINA-PP 5 ", "page_idx": 16}, {"type": "text", "text": "4 Convergence Results 6 ", "page_idx": 16}, {"type": "text", "text": "5 Numerical Experiments 9 ", "page_idx": 16}, {"type": "text", "text": "6 Conclusion and Future Work 10 ", "page_idx": 16}, {"type": "text", "text": "A Extra Related Work 19 ", "page_idx": 16}, {"type": "text", "text": "B Useful Facts 22 ", "page_idx": 16}, {"type": "text", "text": "CJustification of Assumption 2.3 23 ", "page_idx": 16}, {"type": "text", "text": "DGeneral Analysis 25   \nD.1 Refined Assumptions 25   \nD.2 Technical Lemmas 26   \nD.3 Main Results 42   \nE  Analysis for Bounded Compressors 46   \nE.1  Technical Lemmas 46   \nE.2 Main Results 56   \nE.3On the Technical Non-Triviality of the Analysis 57   \nByz-VR-MARINA-PP $^+$ : Simplified Version of Byz-VR-MARINA-PP 58   \nF.1 Analysis for Bounded Compressors. . 58   \nF.2 Discussion of the Results . . 63 ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "G  Analysis without Full-Batch Gradient Computations 65 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "G.1 New Lemma . 66   \nG.2 Main Results for Byz-VR-MARINA without Full-Batch Gradient Computations . : 70   \nG.2.1General Results . : 70   \nG.2.2 Results for Bounded Compressors . . 71   \nG.3 Main Results for Byz-VR-MARINA $^+$ without Full-Batch Gradient Computations 72   \nG.3.1 Results for Bounded Compressors . . 72   \nH Experimental Details and Extra Experiments 74   \nH.1  Experimental Details 74 ", "page_idx": 16}, {"type": "text", "text": "A Extra Related Work", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Further Comparison with Data and Diggavi (2021).  As we mention in the main text, Data and Diggavi (2021) assume that $3B$ is smaller than $C$ . More precisely, Data and Diggavi (2021) assume that $B\\le\\epsilon C$ , where $\\epsilon\\le\\frac{1}{3}-\\epsilon^{\\prime}$ for some parameter $\\epsilon^{\\prime}>0$ that will be explained later. That is, the results from Data and Diggavi (2021) do not hold when $C$ is smaller than $3B$ , and, in particular, their algorithm cannot tolerate the situation when the server samples only Byzantine workers at some particular communication round. We also notice that when $C\\geq4B$ , then existing methods such as Byz-VR-MARINA (Gorbunov et al., 2023) or Client Momentum (Karimireddy et al., 2021, 2022) can be applied without any changes to get a provable convergence. ", "page_idx": 18}, {"type": "text", "text": "Next, Data and Diggavi (2021) derive the upper bounds for the expected squared distance to the solution (in the strongly convex case) and the averaged expected squared norm of the gradient (in the non-convex case), where the expectation is taken w.r.t. the sampling of stochastic gradients only and the bounds itself hold with probability at least $\\begin{array}{r}{1-\\frac{K}{H}\\exp\\left(-\\frac{\\epsilon^{\\prime2}\\bar{(1-\\epsilon)}C}{16}\\right)}\\end{array}$ -(1-)C), where H is the number of local steps. For simplicity consider the best-case scenario: ${\\cal H}=1$ (local steps deteriorate the results from Data and Diggavi (2021)). Then, the lower bound for this probability becomes negative when either $C$ is not large enough or when $K$ is large or when $\\epsilon$ is close to $\\mathbf{\\bar{\\frac{1}{3}}}$ , e.g., for $K=10^{6}$ \uff0c $\\epsilon=\\epsilon^{\\prime}={\\textstyle\\frac{1}{6}},C=5000$ this lower bound is smaller than $-720$ , meaning that in this case, the result does not guarantee convergence. In contrast, our results have classical convergence criteria, where the expectations are taken w.r.t. the all randomness. ", "page_idx": 18}, {"type": "text", "text": "Finally, the bounds from Data and Diggavi (2021) have non-reduceable terms even for homogeneous data case: these terms are proportional to $\\frac{\\sigma^{2}}{b}$ ,where $\\sigma^{2}$ is the upper bound for the variance of the stochastic estimator on regular clients and $b$ is the batchsize. In contrast, our results have only decreasing terms in the upper bounds when the data is homogeneous. ", "page_idx": 18}, {"type": "text", "text": "Byzantine robustness. There exist various approaches to achieving Byzantine robustness (Lyu et al., 2020). Alistarh et al. (2018); Allen-Zhu et al. (2021) rely on the concentration inequalities for the stochastic gradients with bounded noise to iteratively remove them from the training. Karimireddy et al. (2021) formalize the definition of robust aggregation and propose the first provably robust aggregation rule called CenteredClip and the first provably Byzantine robust method under bounded variance assumption for homogeneous problems, i.e., when all good workers share one dataset. In particular, the method from (Karimireddy et al., 2021) uses client momentum on the clients that helps to memorize previous steps for good workers and withstand time-coupled attacks. This approach is extended by He et al. (2022) to the setup of decentralized learning. Allouah et al. (2023) develop an alternative definition for robust aggregation and propose a new aggregation rule satisfying their definition. Karimireddy et al. (2022) generalize these results to the heterogeneous data case and derive lower bounds for the optimization error that one can achieve in the heterogeneous case. Based on the formalism from Karimireddy et al. (2021), Gorbunov et al. (2022) propose a server-free approach that uses random checks of computations and bans of peers. This trick allows the elimination of all Byzantine workers after a finite number of steps on average. There are also many other approaches, e.g., one can use redundant computations of the stochastic gradients (Chen et al., 2018; Rajput et al., 2019) or introduce reputation metrics (Rodriguez-Barros0 et al., 2020; Regatti et al., 2020; Xu and Lyu, 2020) to achieve some robustness, see also a recent survey by Lyu et al. (2020). ", "page_idx": 18}, {"type": "text", "text": "Variance reduction.  The literature on variance-reduced methods is very rich (Gower et al., 2020). The first variance-reduced methods are designed to fix the convergence of standard Stochastic Gradient Descent (SGD) and make it convergent to any predefined accuracy even with constant stepsizes. Such methods as SAG (Schmidt et al., 2017), SVRG (Johnson and Zhang, 2013), SAGA (Defazio et al., 2014) are developed mainly for (strongly) convex smooth optimization problems, while methods like SARAH (Nguyen et al., 2017), STORM (Cutkosky and Orabona, 2019), GeomSARAH (Horvath et al., 2023), PAGE (Li et al., 2021) are designed for general smooth non-convex problems. In this paper, we use GeomSARAH/PAGE-type variance reduction as the main building block of the method that makes the method robust to Byzantine attacks. ", "page_idx": 18}, {"type": "text", "text": "Partial participation and client sampling. In the context of Byzantine-robust learning, there exists one work that develops and analyzes the method with partial participation (Data and Diggavi, 2021). However, this work relies on the restrictive assumption that the number of participating clients at each round is at least three times larger than the number of Byzantine workers. In this case, Byzantines cannot form a majority, and standard methods can be applied without any changes. In contrast, our method converges in more challenging scenarios, e.g., Byz-VR-MARINA-PP provably converges even when the server samples one client, which can be Byzantine. The results from Data and Diggavi (2021) have some other noticeable limitations that we discuss in Appendix A. ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "Communication compression. The literature on communication compression can be roughly divided into two huge groups. The first group studies the methods with unbiased communication compression. Different compression operators in the application to Distributed SGD/GD are studied in (Alistarh et al., 2017; Wen et al., 2017; Khirirat et al., 2018). To improve the convergence rate by fixing the error coming from the compression Mishchenko et al. (2019) propose to apply compression to the special gradient differences. Multiple extensions and generalizations of mentioned techniques are proposed and analyzed in the literature, e.g., see (Horvath et al., 2023; Gorbunov et al., 2021; Li et al., 2020; Qian et al., 2021; Basu et al., 2019; Haddadpour et al., 2021; Sadiev et al., 2022; Islamov et al., 2021; Safaryan et al., 2022). ", "page_idx": 19}, {"type": "text", "text": "Another large part of the literature on compressed communication is devoted to biased compression operators (Ajalloeian and Stich, 2020; Demidovich et al., 2023). Typically, such compression operators require more algorithmic changes than unbiased compressors since naive combinations of biased compression with standard methods (e.g., Distributed GD) can diverge (Beznosikov et al., 2020). Error feedback is one of the most popular ways of utilizing biased compression operators in practice (Seide et al., 2014; Stich et al., 2018; Vogels et al., 2019), see also (Richtarik et al., 2021; Fatkhullin et al., 2021) for the modern version of error feedback with better theoretical guarantees for non-convex problems. ", "page_idx": 19}, {"type": "text", "text": "In the context of Byzantine robustness, methods with communication compression are also studied. The existing approaches are based on aggregation rules based on the norms of the updates (Ghosh et al., 2020, 2021), SignSGD and majority vote (Bernstein et al., 2019), SAGA-type variance reduction coupled with unbiased compression (Zhu and Ling, 2021), and GeomSARAH/PAGE-type variance reduction combined with unbiased compression (Gorbunov et al., 2023). ", "page_idx": 19}, {"type": "text", "text": "Gradient clipping. Gradient clipping has multiple useful properties and applications. Originally it was used by Pascanu et al. (2013) to reduce the effect of exploding gradients during the training of RNNs. Gradient clipping is also a popular tool for achieving provable differential privacy (Abadi et al., 2016; Chen et al., 2020), convergence under generalized notions of smoothness (Zhang et al., 2020a; Mai and Johansson, 2021) and better (high-probability) convergence under heavy-tailed noise assumption (Zhang et al., 2020b; Nazin et al., 2019; Gorbunov et al., 2020; Sadiev et al., 2023; Nguyen et al., 2023). In the context of Byzantine-robust learning, gradient clipping is also utilized to design provably robust aggregation (Karimireddy et al., 2021). Our work proposes a novel useful application of clipping, i.e., we utilize clipping to achieve Byzantine robustness with partial participation of clients. ", "page_idx": 19}, {"type": "text", "text": "Byzantine-robust asynchronous methods.Byzantine-robust asynchronous methods are also very relevant to the problem of partial participation in the Byzantine-robust learning. Indeed, the asynchronous methods like Asynchronous SGD (Agarwal and Duchi, 2011; Nedic et al., 2001) naturally have partial participation since whenever some worker finishes the computation (of the stochastic gradients), this worker immediately sends the update to the server and the server applies this update without waiting all other clients. However, without extra assumptions asynchronous methods cannot be tolerate Byzantine attacks: Byzantine clients could immediately send any vector to the server to guarantee that their update is received earlier than the updates from regular clients. Clearly, such a behavior of Byzantine workers leads to the divergence of the method unless the server has additional information that can be used for acceptance/rejection of the update or some other alternation of the communication protocol preventing the situations when some client updates the model too many times in a row is applied. ", "page_idx": 19}, {"type": "text", "text": "Therefore, the existing approaches addressing this important problem rely on extra assumptions. Damaskinos et al. (2018) propose to use Lipschitz filter and frequency filters in order to filter out Byzantine workers. Next, Xie et al. (2020b); Fang et al. (2022) use additional validation data on the server to decide whether to accept the update from workers. This assumption is restrictive for many FL applications when the data on clients is private and is not available on the server. Yang and Li (2023) propose so-called BASGD (and its momentum version) where the key idea is to split workers into the buffers and wait until each buffer gets at least one gradient update. In the case when the number of buffers is sufficiently large (at least $2B$ ,where $B$ is the number of Byzantine workers), the authors show that BASGD converges. However, this means that to make the step BASGD requires to collect sufficiently large number of gradients such that the good buffers form majority, which is closer to full participation than to the partial participation in the worst case. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "We emphasize that in our work we consider a different setup of synchronous communications with partial participation. The approaches discussed in the above paragraph cannot be directly applied to the problem considered in this paper without extra assumptions. ", "page_idx": 20}, {"type": "text", "text": "B Useful Facts ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "For all $a,b\\in\\mathbb{R}^{d}$ and $\\alpha>0,p\\in(0,1]$ the following relations hold: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{2\\langle a,b\\rangle=\\|a\\|^{2}+\\|b\\|^{2}-\\|a-b\\|^{2}}}\\\\ {{\\|a+b\\|^{2}\\leq(1+\\alpha)\\|a\\|^{2}+\\left(1+\\alpha^{-1}\\right)\\|b\\|^{2}}}\\\\ {{-\\|a-b\\|^{2}\\leq-\\displaystyle\\frac{1}{1+\\alpha}\\|a\\|^{2}+\\displaystyle\\frac{1}{\\alpha}\\|b\\|^{2},}}\\\\ {{\\left(1-p\\right)\\left(1+\\displaystyle\\frac{p}{2}\\right)\\leq1-\\displaystyle\\frac{p}{2},\\quad p\\geq0}}\\\\ {{\\left(1-p\\right)\\left(1+\\displaystyle\\frac{p}{2}\\right)\\left(1+\\displaystyle\\frac{p}{4}\\right)\\leq1-\\displaystyle\\frac{p}{4}\\quad p\\geq0.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Lemma B.1. (Lemma $^{5}$ from (Richtarik etal, 2021) Lt $a,b\\;>\\;0.$ $\\begin{array}{r}{0\\,\\leq\\,\\gamma\\,\\leq\\,\\frac{1}{\\sqrt{a}+b}}\\end{array}$ then $a\\gamma^{2}+b\\gamma\\leq1.$ Thebodisff $\\begin{array}{r}{\\frac{1}{\\sqrt{a}+b}\\leq\\operatorname*{min}\\left\\{\\frac{1}{\\sqrt{a}},\\frac{1}{b}\\right\\}\\leq\\frac{2}{\\sqrt{a}+b}}\\end{array}$ ", "page_idx": 21}, {"type": "text", "text": "C  Justification of Assumption 2.3 ", "text_level": 1, "page_idx": 22}, {"type": "table", "img_path": "G8aS48B9bm/tmp/8d06bdaba74da1c45276e0d3c9de9334f7d3ce7bf6d1a65c4797c9e35a460eea.jpg", "table_caption": [], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "Krum and Krum $\\circ$ Bucketing. Krum aggregation rule is defined as ", "text_level": 1, "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathrm{Krum}(x_{1},\\ldots,x_{n})=\\operatorname*{argmin}_{x_{i}\\in\\{x_{1},\\ldots,x_{n}\\}}\\sum_{j\\in S_{i}}\\|x_{j}-x_{i}\\|^{2},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $S_{i}~\\subset~\\{x_{1},\\ldots,x_{n}\\}$ isthesubsetof $n\\,-\\,B\\,-\\,2$ closestvectorsto $x_{i}$ .Bydefinition,   \n$\\mathrm{Krum}(x_{1},\\ldots,x_{n})\\;\\;\\in\\;\\;\\{x_{1},\\ldots,x_{n}\\}$ and, thus $\\|\\mathrm{Krum}(x_{1},\\ldots,x_{n})\\|\\ \\leq\\ \\operatorname*{max}_{i\\in[n]}\\|x_{i}\\|$ , i.e., As$F_{A}\\,=\\,1$ $y_{i}$ $\\begin{array}{r}{\\|y_{i}\\|\\;\\leq\\;\\frac{1}{s}\\sum_{k=s(i-1)+1}^{\\operatorname*{min}\\{s i,n\\}}\\|x_{\\pi(k)}\\|\\;\\leq\\;\\operatorname*{max}_{i\\in[n]}\\|x_{i}\\|}\\end{array}$   \n$\\begin{array}{r}{\\|\\mathrm{Krum}\\circ\\mathrm{Bucketing}(x_{1},\\ldots,x_{n})\\|\\leq\\operatorname*{max}_{i\\in[n]}\\|x_{i}\\|}\\end{array}$ ", "page_idx": 22}, {"type": "text", "text": "Geometric median (GM) and $\\mathbf{GM}\\circ$ Bucketing.   Geometric median is defined as follows: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbf{GM}(x_{1},\\ldots,x_{n})=\\operatorname{argmin}_{x\\in\\mathbb{R}^{d}}\\sum_{i=1}^{n}\\|x-x_{i}\\|.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "One can show that $\\begin{array}{r l r}{\\mathrm{GM}(x_{1},\\ldots,x_{n})}&{{}\\in\\quad\\mathrm{Conv}(x_{1},\\ldots,x_{n})}&{:=}&{\\{x\\quad\\in\\quad\\mathbb{R}^{d}\\quad|\\quad x\\quad=}\\end{array}$ $\\textstyle\\sum_{i=1}^{n}\\alpha_{i}x_{i}$ for some $\\alpha_{1},\\ldots,\\alpha_{n}\\geq1$ such that $\\textstyle\\sum_{i=1}^{n}\\alpha_{i}=1\\}$ geometricmedianelnstt convex hull of the inputs. Indeed, let $\\mathrm{{GM}}(x_{1},\\ldots,x_{n})=x={\\hat{x}}+{\\tilde{x}}$ where $\\hat{x}$ is the projection of $x$ on $\\operatorname{Conv}(x_{1},\\ldots,x_{n})$ and $\\tilde{x}=x-\\hat{x}$ . Then, the optimality condition implies that $\\langle\\hat{x}-x,y-\\hat{x}\\rangle\\geq0$ for all $y\\in\\operatorname{Conv}(x_{1},\\ldots,x_{n})$ In particular, for all $i\\in[n]$ we have $\\langle\\hat{x}-x,x_{i}-\\dot{\\hat{x}}\\rangle\\geq0$ . Since ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l l l}{\\langle\\hat{x}-x,x_{i}-\\hat{x}\\rangle}&{=}&{\\langle\\tilde{x},\\hat{x}-x_{i}\\rangle=\\displaystyle\\frac{1}{2}\\|\\tilde{x}+\\hat{x}-x_{i}\\|^{2}-\\displaystyle\\frac{1}{2}\\|\\tilde{x}\\|^{2}-\\displaystyle\\frac{1}{2}\\|\\hat{x}-x_{i}\\|^{2}}\\\\ &{=}&{\\displaystyle\\frac{1}{2}\\|x-x_{i}\\|^{2}-\\displaystyle\\frac{1}{2}\\|\\tilde{x}\\|^{2}-\\displaystyle\\frac{1}{2}\\|\\hat{x}-x_{i}\\|^{2}}\\\\ &{\\leq}&{\\displaystyle\\frac{1}{2}\\|x-x_{i}\\|^{2}-\\displaystyle\\frac{1}{2}\\|\\hat{x}-x_{i}\\|^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "we get that $\\|x-x_{i}\\|\\geq\\|\\hat{x}-x_{i}\\|$ for all $i\\;\\in\\;[n]$ and the equality holds if and only if $\\tilde{x}\\;=\\;0$ Therefore, argmin from (16) is achieved for $x$ such that $x={\\hat{x}}$ , meaning that ${\\mathrm{GM}}(x_{1},\\ldots,x_{n})\\in$ $\\operatorname{Conv}(x_{1},\\ldots,x_{n})$ . Therefore, there exist some coeffcients $\\alpha_{1},\\ldots,\\alpha_{n}\\geq0$ such that $\\textstyle\\sum_{i=1}^{n}\\alpha_{i}=1$ and $\\begin{array}{r}{\\mathrm{GM}(x_{1},\\ldots,x_{n})=\\sum_{i=1}^{n}\\alpha_{i}x_{i}}\\end{array}$ , implying that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\|\\mathbf{G}\\mathbf{M}(x_{1},\\ldots,x_{n})\\|\\leq\\sum_{i=1}^{n}\\alpha_{i}\\|x_{i}\\|\\leq\\operatorname*{max}_{i\\in[n]}\\|x_{i}\\|.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "That is, GM satisfies Assumption 2.3 with $F_{\\cal A}=1$ . Similarly to the case of Krum $\\circ$ Bucketing, we also have $\\begin{array}{r}{\\|\\mathrm{GM\\circBucketing}(x_{1},\\ldots,x_{n})\\|\\leq\\operatorname*{max}_{i\\in[n]}\\|x_{i}\\|.}\\end{array}$ ", "page_idx": 22}, {"type": "text", "text": "Coordinate-wise median (CM) and $\\mathbf{CM}\\circ$ Bucketing.  Coordinate-wise median (CM) is formally defined as ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbf{CM}(x_{1},\\ldots,x_{n})=\\underset{x\\in\\mathbb{R}^{d}}{\\mathrm{argmin}}\\sum_{i=1}^{n}\\|x-x_{i}\\|_{1},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $\\Vert\\cdot\\Vert_{1}$ denotes $\\ell_{1}$ -norm. This is equivalent to geometric median/median applied to vectors $x_{1},\\ldots,x_{n}$ component-wise. Therefore, from the above derivations for GM we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\|\\mathbf{C}\\mathbf{M}(x_{1},\\ldots,x_{n})\\|_{\\infty}}&{\\le}&{\\displaystyle\\operatorname*{max}_{i\\in[n]}\\|x_{i}\\|_{\\infty},}\\\\ {\\|\\mathbf{C}\\mathbf{M}\\circ\\mathbf{B}\\mathrm{ucketing}(x_{1},\\ldots,x_{n})\\|_{\\infty}}&{\\le}&{\\displaystyle\\operatorname*{max}_{i\\in[n]}\\|x_{i}\\|_{\\infty},}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $\\|\\cdot\\|_{\\infty}$ denotes $\\ell_{\\infty}$ -norm. Therefore, due to the standard relations between $\\ell_{2}.$ and $\\ell_{\\infty}$ -norms, i.e., $\\|a\\|_{\\infty}\\leq\\|a\\|\\leq\\sqrt{d}\\|a\\|_{\\infty}$ for any $a\\in\\mathbb{R}^{d}$ wehave ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\|\\mathbf{CM}(x_{1},\\ldots,x_{n})\\|}&{\\leq}&{\\sqrt{d}\\underset{i\\in[n]}{\\operatorname*{max}}\\,\\|x_{i}\\|,}\\\\ {\\|\\mathbf{CM}\\circ\\mathbf{Bucketing}(x_{1},\\ldots,x_{n})\\|}&{\\leq}&{\\sqrt{d}\\underset{i\\in[n]}{\\operatorname*{max}}\\,\\|x_{i}\\|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "i.e., Assumption 2.3 is satisfied with $F_{A}={\\sqrt{d}}$ ", "page_idx": 23}, {"type": "text", "text": "D General Analysis ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "D.1 Refined Assumptions ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "For simplicity, in the main part of our paper, we present simplified versions of our main results.   \nHowever, our analysis works under more general assumptions presented in this section. ", "page_idx": 24}, {"type": "text", "text": "Assumption on $\\widehat{C}$ . In all the results of this paper, we assume that $n\\geq\\hat{C}\\geq\\operatorname*{max}\\{1,\\delta_{\\mathrm{real}}n/\\delta\\}$ . This condition ensures that the robust aggregation makes sense when $c_{k}=1$ , i.e., at least $1-\\delta$ proportion of sampled workers are not Byzantine ones when $c_{k}=1$ ", "page_idx": 24}, {"type": "text", "text": "Refined smoothness. The following assumption is classical for the literature on non-convex optimization. ", "page_idx": 24}, {"type": "text", "text": "Assumption D.1 $L$ -smoothness). We assume that function $f:\\ensuremath{\\mathbb{R}^{d}}\\to\\ensuremath{\\mathbb{R}}$ is $\\mathrm{L}$ -smooth, i.e., for all $x,y\\in\\bar{\\mathbb{R}}^{d}$ wehave ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\|\\nabla f(x)-\\nabla f(y)\\|\\leq L\\|x-y\\|.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Moreover, we assume that $f$ is uniformly lower bounded by $f^{*}\\in\\mathbb{R}$ , i.e., $f^{*}:=\\operatorname*{inf}_{x\\in\\mathbb{R}^{d}}f(x)$ . In addition, we assume that $f_{i}$ is $L_{i}$ -smooth for all $i\\in\\mathcal G$ , i.e., for all $x,y\\in\\mathbb{R}^{d}$ ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\|\\nabla f_{i}(x)-\\nabla f_{i}(y)\\|\\leq L_{i}\\|x-y\\|.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "We notice here that (19) implies $L$ -smoothness of $f$ with $\\begin{array}{r}{L\\leq\\frac{1}{G}\\sum_{i\\in\\mathcal{G}}L_{i}}\\end{array}$ .e., smoothness constant of $f$ can be better than the averaged smoothness constant of the local loss functions on the regular clients. ", "page_idx": 24}, {"type": "text", "text": "Following Gorbunov et al. (2023), we consider refined assumptions on the smoothness. ", "page_idx": 24}, {"type": "text", "text": "Assumption D.2 (Global Hessian variance assumption (Szlendak et al., 2022). We assume that thereexists $L_{\\pm}\\ge0$ such that for all $x,y\\in\\mathbb{R}^{d}$ ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\frac{1}{G}\\sum_{i\\in\\mathcal{G}}\\|\\nabla f_{i}(x)-\\nabla f_{i}(y)\\|^{2}-\\|\\nabla f(x)-\\nabla f(y)\\|^{2}\\leq L_{\\pm}^{2}\\|x-y\\|^{2}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "We notice that (19) implies (20) with $L_{\\pm}\\le\\operatorname*{max}_{i\\in\\mathcal{G}}L_{i}$ . Szlendak et al. (2022) prove that $L_{\\pm}$ satisfies the following relation: $L_{\\mathrm{avg}}^{2}-L^{2}\\leq L_{\\pm}^{2}\\leq L_{\\mathrm{avg}}^{2}.$ where $\\begin{array}{r}{L_{\\mathrm{avg}}^{2}:=\\frac{1}{G}\\sum_{i\\in\\mathcal{G}}\\bar{L}_{i}^{2}}\\end{array}$ . In particular, it is possible that $L_{\\pm}=0$ even if the data on the good workers is heterogeneous. ", "page_idx": 24}, {"type": "text", "text": "Assumption D.3 (Local Hessian variance assumption (Gorbunov et al., 2023)). We assume that there exists $\\mathcal{L}_{\\pm}\\geq0$ such that for all $x,y\\in\\mathbb{R}^{d}$ ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\frac{1}{G}\\sum_{i\\in\\mathcal{G}}\\mathbb{E}\\left\\|\\widehat{\\Delta}_{i}(x,y)-\\Delta_{i}(x,y)\\right\\|^{2}\\leq\\frac{\\mathcal{L}_{\\pm}^{2}}{b}\\|x-y\\|^{2},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Where $\\Delta_{i}(x,y):=\\nabla f_{i}(x)-\\nabla f_{i}(y)$ and $\\widehat{\\Delta}_{i}(x,y)$ is an unbiased mini-batched estimator of $\\Delta_{i}(x,y)$ withbatchsize $b$ ", "page_idx": 24}, {"type": "text", "text": "This assumption incorporates considerations for the smoothness characteristics inherent in all functions $\\{f_{i,j}\\}_{i\\in\\mathcal{G},j\\in[m]}$ , the sampling policy, and the similarity among the functions $\\{f_{i,j}\\}_{i\\in\\mathcal{G},j\\in[m]}$ Gorbunov et al. (2023) have demonstrated that, asuming smoothness of $\\{f_{i,j}\\}_{i\\in\\mathcal{G},j\\in[m]}$ Assumption D.3 holds for various standard sampling strategies, including uniform and importance samplings. ", "page_idx": 24}, {"type": "text", "text": "For part of our results, we also need to assume smoothness of all $\\{f_{i,j}\\}_{i\\in\\mathcal{G},j\\in[m]}$ explicitly. ", "page_idx": 24}, {"type": "text", "text": "Assumption D.4 (Smoothness of $f_{i,j}$ (optional). We assume that for all $i\\in\\mathcal G$ and $j\\in[m]$ there exists $L_{i,j}\\ge0$ such that $f_{i,j}$ is $L_{i,j}$ -smooth, i.e., for all $x,y\\in\\mathbb{R}^{d}$ ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\|\\nabla f_{i,j}(x)-\\nabla f_{i,j}(y)\\|\\leq L_{i,j}\\|x-y\\|.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Refined heterogeneity. Instead of Assumption 2.5, we consider a more generalized one. ", "page_idx": 24}, {"type": "text", "text": "Assumption D.5 $[\\boldsymbol{B},\\zeta^{2})$ -heterogeneity). We assume that good clients have $\\left(B,\\zeta^{2}\\right)$ -heterogeneous local loss functions for some $B\\geq0,\\zeta\\geq0$ ,i.e., ", "page_idx": 24}, {"type": "equation", "text": "$$\n{\\frac{1}{G}}\\sum_{i\\in{\\mathcal{G}}}\\left\\|\\nabla f_{i}(x)-\\nabla f(x)\\right\\|^{2}\\leq B\\|\\nabla f(x)\\|^{2}+\\zeta^{2}\\quad\\forall x\\in\\mathbb{R}^{d}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "When $B=0$ , the above assumption recovers Assumption 2.5. However, it also covers some situations when the model is over-parameterized (Vaswani et al., 2019) and can hold with smaller values of $\\zeta^{2}$ This assumption is also used in (Karimireddy et al., 2022; Gorbunov et al., 2023). ", "page_idx": 25}, {"type": "text", "text": "D.2  Technical Lemmas ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Lemma D.6. Let $X$ be a random vector in $\\mathbb{R}^{d}$ and $\\widetilde{X}=\\mathfrak{c}\\wr i p_{\\lambda}(X)$ .Assume that $\\mathbb{E}[X]=x\\in\\mathbb{R}^{d}$ and $\\|x\\|\\leq\\lambda/2$ , then ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\|\\widetilde{X}-x\\right\\|^{2}\\right]\\leq10\\mathbb{E}\\left\\|X-x\\right\\|^{2}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Proof. The proof follows a similar procedure to that presented in Lemma F.5 from (Gorbunov et al., 2020). To commence the proof, we introduce two indicator random variables: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\chi=\\mathbb{I}_{\\{X:\\|X\\|>\\lambda\\}}=\\left\\{\\begin{array}{l l}{1,}&{\\mathrm{~if~}\\|X\\|>\\lambda,}\\\\ {0,}&{\\mathrm{~otherwise~}}\\end{array}\\right.,\\eta=\\mathbb{I}_{\\left\\{X:\\|X-x\\|>\\frac{\\lambda}{2}\\right\\}}=\\left\\{\\begin{array}{l l}{1,}&{\\mathrm{~if~}\\|X-x\\|>\\frac{\\lambda}{2}}\\\\ {0,}&{\\mathrm{~otherwise~}}\\end{array}\\right..\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Moreover, since $\\begin{array}{r}{\\|X\\|\\leq\\|x\\|+\\|X-x\\|\\overset{\\|x\\|\\leq\\lambda/2}{\\leq}\\frac{\\lambda}{2}+\\|X-x\\|}\\end{array}$ , we have $\\chi\\leq\\eta$ . Using that we get ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\widetilde{X}=\\operatorname*{min}\\left\\{1,\\frac{\\lambda}{\\|X\\|}\\right\\}X=\\chi\\frac{\\lambda}{\\|X\\|}X+(1-\\chi)X.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "By Markov's inequality. ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\eta]=\\mathbb{P}\\left\\{\\|X-x\\|>\\frac{\\lambda}{2}\\right\\}=\\mathbb{P}\\left\\{\\|X-x\\|^{2}>\\frac{\\lambda^{2}}{4}\\right\\}\\leq\\frac{4}{\\lambda^{2}}\\mathbb{E}\\left[\\|X-x\\|^{2}\\right].\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Using $\\begin{array}{r}{\\|\\widetilde{X}-x\\|\\leq\\|\\widetilde{X}\\|+\\|x\\|\\leq\\lambda+\\frac{\\lambda}{2}=\\frac{3\\lambda}{2}}\\end{array}$ we obtain ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\|\\tilde{X}-x\\|^{2}\\right]=\\mathbb{E}\\left[\\|\\tilde{X}-x\\|^{2}\\chi+\\|\\tilde{X}-x\\|^{2}(1-\\chi)\\right]}\\\\ &{\\qquad\\qquad\\qquad=\\mathbb{E}\\left[\\chi\\left\\|\\frac{\\lambda}{\\|X\\|}X-x\\right\\|^{2}+\\|X-x\\|^{2}(1-\\chi)\\right]}\\\\ &{\\qquad\\qquad\\qquad\\leq\\mathbb{E}\\left[\\chi\\left(\\left\\|\\frac{\\lambda}{\\|X\\|}X\\right\\|+\\|x\\|\\right)^{2}+\\|X-x\\|^{2}(1-\\chi)\\right]}\\\\ &{\\qquad\\qquad\\overset{\\|x\\|\\leq\\frac{\\lambda}{2}}{\\leq}\\left(\\mathbb{E}\\left[\\chi\\left(\\frac{3\\lambda}{2}\\right)^{2}+\\|X-x\\|^{2}\\right]\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where in the last inequality we applied $1-\\chi\\leq1$ . Using (23) and $\\chi\\leq\\eta$ we get ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\|\\widetilde{X}-x\\|^{2}\\right]\\leq\\frac{9\\lambda^{2}}{4}\\left(\\frac{2}{\\lambda}\\right)^{2}\\mathbb{E}\\left[\\|X-x\\|^{2}\\right]+\\mathbb{E}\\left[\\|X-x\\|^{2}\\right]}\\\\ &{\\qquad\\qquad\\qquad\\leq10\\mathbb{E}\\left[\\|X-x\\|^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Lemma D.7 (Lemma 2 from Li et al. (2021)). Assume that function $f$ is $L$ -smooth(Assumption $D.I$ \u4e00 and $x^{k+1}=\\dot{x}^{k}-\\gamma g^{k}$ Then ", "page_idx": 25}, {"type": "equation", "text": "$$\nf\\left(x^{k+1}\\right)\\leq f\\left(x^{k}\\right)-{\\frac{\\gamma}{2}}\\left\\|\\nabla f\\left(x^{k}\\right)\\right\\|^{2}-\\left({\\frac{1}{2\\gamma}}-{\\frac{L}{2}}\\right)\\left\\|x^{k+1}-x^{k}\\right\\|^{2}+{\\frac{\\gamma}{2}}\\left\\|g^{k}-\\nabla f\\left(x^{k}\\right)\\right\\|^{2}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Lemma D.8. Let Assumptions $D.I,D.2,D.3$ hold and the Compression Operator satisfy Definition 2.2. Let us define \"ideal\"estimator: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\overline{{g}}^{k+1}=\\left\\{\\begin{array}{l l}{\\frac{1}{G_{C}^{k}}\\displaystyle\\sum_{i\\in\\mathcal{G}_{C}^{k}}\\nabla f_{i}(x^{k+1}),}&{c_{n}=1,}\\\\ {g^{k}+\\nabla f\\left(x^{k+1}\\right)-\\nabla f\\left(x^{k}\\right),}&{c_{n}=0\\,a n d\\,G_{C}^{k}<(1-\\delta)C,}\\\\ {g^{k}+\\frac{1}{G_{C}^{k}}\\displaystyle\\sum_{i\\in\\mathcal{G}_{C}^{k}}c\\,l\\,\\dot{q}_{\\lambda}\\left(\\mathcal{Q}\\left(\\widehat{\\Delta}_{i}\\left(x^{k+1},x^{k}\\right)\\right)\\right),}&{c_{n}=0\\,a n d\\,G_{C}^{k}\\geq(1-\\delta)C.}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Thenforall $k\\geq0$ the iterates produced by Byz-VR-MARINA-PP (Algorithm 1) satisfy ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{A_{1}=\\mathbb{E}\\left[\\left\\Vert\\overline{{g}}^{k+1}-\\nabla f\\left(x^{k+1}\\right)\\right\\Vert^{2}\\right]}\\\\ &{\\quad\\leq\\left(1-p\\right)\\left(1+\\frac{p}{4}\\right)\\mathbb{E}\\left[\\left\\Vert g^{k}-\\nabla f(x^{k})\\right\\Vert^{2}\\right]+p\\frac{\\delta\\cdot\\mathcal{P}_{\\mathcal{G}_{C}^{k}}}{(1-\\delta)}\\mathbb{E}\\left[B\\left\\Vert\\nabla f(x)\\right\\Vert^{2}+\\zeta^{2}\\right]}\\\\ &{\\quad+\\left(1-p\\right)p_{G}\\left(1+\\frac{4}{p}\\right)\\frac{2\\cdot\\mathcal{P}_{\\mathcal{G}_{C}^{k}}n}{C}\\left(10\\omega L^{2}+(10\\omega+1)L_{\\pm}^{2}+\\frac{10(\\omega+1)\\mathcal{L}_{\\pm}^{2}}{b}\\right)\\mathbb{E}\\left[\\left\\Vert x^{k+1}-x^{k}\\right\\Vert^{2}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where $p_{G}=\\mathrm{Prob}\\left\\{G_{C}^{k}\\geq(1-\\delta)C\\right\\}$ and $\\mathcal{P}_{\\mathcal{G}_{C}^{k}}=\\operatorname{Prob}\\big\\{i\\in\\mathcal{G}_{C}^{k}\\ |\\ G_{C}^{k}\\ge\\left(1-\\delta\\right)C\\big\\}.$ ", "page_idx": 26}, {"type": "text", "text": "Proof. Let us examine the expected value of the squared difference between ideal estimator and full gradient: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\mathrm{1}_{1}=\\mathbb{E}\\left[\\left\\|\\overline{{g}}^{k+1}-\\nabla f\\left(x^{k+1}\\right)\\right\\|^{2}\\right]}}\\\\ &{=\\mathbb{E}\\left[\\mathbb{E}_{k}\\left[\\left\\|\\overline{{g}}^{k+1}-\\nabla f\\left(x^{k+1}\\right)\\right\\|^{2}\\right]\\right]}\\\\ &{=(1-p)\\,p_{G}\\mathbb{E}\\left[\\mathbb{E}_{k}\\left[\\left\\|g^{k}+\\frac{1}{G_{C}^{k}}\\sum_{i\\in\\mathcal{C}_{C}^{k}}\\mathfrak{c}_{\\lambda}\\mathrm{i}_{\\mathfrak{P}_{\\lambda}}\\left(\\mathcal{Q}\\left(\\widehat{\\Delta}_{i}\\left(x^{k+1},x^{k}\\right)\\right)\\right)-\\nabla f\\left(x^{k+1}\\right)\\right\\|^{2}\\right]\\mid\\left[3\\right]\\right]}\\\\ &{+\\left(1-p\\right)(1-p_{G})\\mathbb{E}\\left[\\mathbb{E}_{k}\\left[\\left\\|g^{k}-\\nabla f(x^{k})\\right\\|^{2}\\right]\\mid\\left[2\\right]\\right]+p\\mathbb{E}\\left[\\left\\|\\frac{1}{G_{C}^{k}}\\sum_{i\\in\\mathcal{C}_{C}^{k}}\\nabla f_{i}(x^{k+1})-\\nabla f(x^{k+1})\\right\\|^{2}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Using (12) and $\\nabla f\\left(x^{k}\\right)-\\nabla f\\left(x^{k}\\right)=0$ we obtain ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{B_{1}=\\mathbb{E}\\left[\\mathbb{E}_{k}\\left[\\left\\|\\int_{0}^{k}+\\frac{1}{G_{C}^{k}}\\sum_{i=0}^{C}\\mathrm{d}\\boldsymbol{\\Phi}_{k}\\left(Q\\left(\\widehat{\\Delta}_{i}\\left(\\mathbf{\\pi}^{k+1},\\mathbf{z}^{k}\\right)\\right)\\right)-\\nabla f\\left(x^{k+1}\\right)\\right\\|^{2}\\right]\\mid\\left\\{\\mathcal{B}\\right\\}\\right]}\\\\ &{=\\mathbb{E}\\left[\\mathbb{E}_{k}\\left[\\left\\|\\int_{s}^{k}+\\frac{1}{G_{C}^{k}}\\sum_{i=0}^{C}\\mathrm{d}\\boldsymbol{\\Phi}_{k}\\left(Q\\left(\\widehat{\\Delta}_{i}\\left(\\mathbf{\\pi}^{k+1},\\mathbf{z}^{k}\\right)\\right)\\right)-\\nabla f\\left(x^{k+1}\\right)+\\nabla f\\left(x^{k}\\right)-\\nabla f\\left(x^{k}\\right)\\right\\|^{2}\\right]\\mid\\left\\{\\mathcal{B}\\right\\}\\right]}\\\\ &{\\overset{((2))}{\\leq}\\left(1+\\frac{\\rho_{1}}{\\rho}\\right)\\mathbb{E}\\left[\\left\\|\\mathbf{g}^{k}-\\nabla f\\left(x^{k}\\right)\\right\\|^{2}\\right]}\\\\ &{\\phantom{2p c}+\\left(1+\\frac{\\rho_{1}}{\\rho}\\right)\\mathbb{E}\\left[\\mathbb{E}_{k}\\left[\\left\\|\\frac{1}{G_{C}^{k}}\\sum_{i=0}^{C}\\mathrm{d}\\boldsymbol{\\Phi}_{k}\\left(Q\\left(\\widehat{\\Delta}_{i}\\left(\\mathbf{\\pi}^{k+1},\\mathbf{z}^{k}\\right)\\right)\\right)-\\left(\\nabla f\\left(x^{k+1}\\right)-\\nabla f\\left(x^{k}\\right)\\right)\\right\\|^{2}\\right]\\mid\\left\\{\\mathcal{B}\\right\\}\\right]}\\\\ &{=\\left(1+\\frac{\\rho_{1}}{\\delta}\\right)\\mathbb{E}\\left[\\left\\|\\mathbf{g}^{k}-\\nabla f\\left(x^{k}\\right)\\right\\|^{2}\\right]}\\\\ &{\\phantom{2p c}+\\left(1+\\frac{4}{\\rho_{1}}\\right)\\mathbb{E}\\left[\\frac{\\left\\|1\\right\\|^{2}}{G_{C}^{k}}\\sum_{i=0}^{C}\\mathrm{d}\\boldsymbol{\\Phi}_{k}\\left(Q\\left(\\widehat{\\Delta}_{i}\\left(\\mathbf{\\pi}^{k+1},\\mathbf{z}^{k}\\right)\\right)\\right)-\\Delta\\left(x^{k+1},\\mathbf{z}^{k}\\right)\\right\\|^{2 \n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Let us consider the last part of the inequality: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{B_{1}^{\\prime}=\\mathbb{E}\\left[\\mathbb{E}_{k}\\left[\\left\\|\\frac{1}{G_{C}^{k}}\\sum_{i\\in\\mathcal{G}_{C}^{k}}\\mathfrak{c l i p}_{\\lambda}\\left(\\mathcal{Q}\\left(\\widehat{\\Delta}_{i}\\left(x^{k+1},x^{k}\\right)\\right)\\right)-\\Delta\\left(x^{k+1},x^{k}\\right)\\right\\|^{2}\\right]\\mid[3]\\right]}\\\\ &{\\quad=\\mathbb{E}\\left[\\mathbb{E}_{S_{k}}\\left[\\mathbb{E}_{k}\\left[\\left\\|\\frac{1}{G_{C}^{k}}\\sum_{i\\in\\mathcal{G}_{C}^{k}}\\mathfrak{c l i p}_{\\lambda}\\left(\\mathcal{Q}\\left(\\widehat{\\Delta}_{i}\\left(x^{k+1},x^{k}\\right)\\right)\\right)-\\Delta\\left(x^{k+1},x^{k}\\right)\\right\\|^{2}\\right]\\mid[3]\\right]\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Note that $G_{C}^{k}\\geq(1-\\delta)C$ in this case: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{B_{1}^{\\prime}\\leq\\frac{1}{C(1-\\delta)}\\mathbb{E}\\left[\\mathbb{E}_{S_{k}}\\left[\\sum_{i\\in\\mathcal{G}_{C}^{k}}\\mathbb{E}_{k}\\left[\\left\\|\\mathbf{c}\\mathbf{1}\\mathbf{i}\\mathbf{i}\\mathbf{p}_{\\lambda}\\left(Q\\left(\\widehat{\\Delta}_{i}\\left(x^{k+1},x^{k}\\right)\\right)\\right)-\\Delta\\left(x^{k+1},x^{k}\\right)\\right\\|^{2}\\right]\\mid\\left[3\\right]\\right]\\right]}\\\\ &{\\ \\ \\leq\\frac{1}{C(1-\\delta)}\\mathbb{E}\\left[\\sum_{i\\in\\mathcal{G}_{C}^{k}}\\mathbb{E}_{S_{k}}\\left[\\mathbb{Z}_{Q_{C}^{k}}\\right]\\mathbb{E}_{k}\\left[\\left\\|\\mathbf{c}\\mathbf{1}\\mathbf{i}\\mathbf{p}_{\\lambda}\\left(Q\\left(\\widehat{\\Delta}_{i}\\left(x^{k+1},x^{k}\\right)\\right)\\right)-\\Delta\\left(x^{k+1},x^{k}\\right)\\right\\|^{2}\\right]\\mid\\left[3\\right]\\right]}\\\\ &{=\\frac{1}{C(1-\\delta)}\\mathbb{E}\\left[\\sum_{i\\in\\mathcal{G}_{C}^{k}}\\mathbb{P}_{Q_{C}^{k}}\\cdot\\mathbb{E}_{k}\\left[\\left\\|\\mathbf{c}\\mathbf{1}\\mathbf{i}\\mathbf{p}_{\\lambda}\\left(Q\\left(\\widehat{\\Delta}_{i}\\left(x^{k+1},x^{k}\\right)\\right)\\right)-\\Delta\\left(x^{k+1},x^{k}\\right)\\right\\|^{2}\\right]\\mid\\left[3\\right]\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where $\\mathcal{Z}_{\\mathcal{G}_{C}^{k}}$ is an indicator function for the event $\\left\\{i\\in\\mathcal{G}_{C}^{k}\\ |\\ G_{C}^{k}\\geq\\left(1-\\delta\\right)C\\right\\}$ and $\\begin{array}{r l}{\\mathcal{P}_{\\mathcal{G}_{C}^{k}}}&{{}=}\\end{array}$ Prob $\\left\\{i\\in\\mathcal{G}_{C}^{k}\\ |\\ G_{C}^{k}\\geq\\left(1-\\delta\\right)C\\right\\}$ is probability ofsuch event Note that $\\mathbb{E}_{S_{k}}\\left[\\mathcal{Z}_{\\mathcal{G}_{C}^{k}}\\right]=\\mathcal{P}_{\\mathcal{G}_{C}^{k}}$ In case of uniform sampling of clients we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\forall i\\in\\mathcal{G}}&{\\mathcal{P}_{\\mathcal{G}_{C}^{k}}=\\mathrm{Prob}\\left\\{i\\in\\mathcal{G}_{C}^{k}\\mid G_{C}^{k}\\geq\\left(1-\\delta\\right)C\\right\\}}\\\\ &{\\qquad=\\frac{C}{n p_{G}}\\cdot\\underset{\\left(1-\\delta\\right)C\\leq t\\leq C}{\\sum}\\left(\\left(\\begin{array}{l}{G}\\\\ {t}\\end{array}\\right)\\left(\\begin{array}{l}{n-G}\\\\ {C-t}\\end{array}\\right)\\left(\\left(\\begin{array}{l}{n}\\\\ {C}\\end{array}\\right)\\right)^{-1}\\right),}\\\\ &{p_{G}=\\underset{\\left(1-\\delta\\right)C\\leq t\\leq C}{\\sum}\\left(\\left(\\begin{array}{l}{G-1}\\\\ {t-1}\\end{array}\\right)\\left(\\begin{array}{l}{n-G}\\\\ {C-t}\\end{array}\\right)\\left(\\left(\\begin{array}{l}{n-1}\\\\ {C-1}\\end{array}\\right)\\right)^{-1}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Now we can continue with inequalities: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{B_{1}^{\\prime}\\leq\\frac{\\mathcal{P}_{\\mathcal{G}_{C}^{k}}}{C(1-\\delta)}\\mathbb{E}\\left[\\underset{i\\in\\mathcal{C}}{\\sum}\\mathbb{E}_{k}\\left[\\left\\|\\mathrm{c}\\mathrm{li}\\mathbf{ip}_{\\lambda}\\left(\\mathcal{Q}\\left(\\widehat{\\Delta}_{i}\\left(x^{k+1},x^{k}\\right)\\right)\\right)-\\Delta\\left(x^{k+1},x^{k}\\right)\\right\\|^{2}\\right]\\mid\\left[3\\right]\\right]}\\\\ &{\\quad\\leq\\frac{\\mathcal{P}_{\\mathcal{G}_{C}^{k}}}{C(1-\\delta)}\\mathbb{E}\\left[\\underset{i\\in\\mathcal{C}}{\\sum}\\mathbb{E}_{k}\\left[\\mathbb{E}_{\\mathcal{Q}}\\left[\\left\\|\\mathrm{c}\\mathrm{li}\\mathbf{ip}_{\\lambda}\\left(\\mathcal{Q}\\left(\\widehat{\\Delta}_{i}\\left(x^{k+1},x^{k}\\right)\\right)\\right)-\\Delta\\left(x^{k+1},x^{k}\\right)\\right\\|^{2}\\right]\\right]\\mid\\left[3\\right]\\right]}\\\\ &{\\quad\\overset{(12)}{\\leq}\\frac{\\mathcal{P}_{\\mathcal{G}_{C}^{k}}}{C(1-\\delta)}\\mathbb{E}\\left[\\underset{i\\in\\mathcal{C}}{\\sum}2\\mathbb{E}_{k}\\left[\\mathbb{E}_{\\mathcal{Q}}\\left[\\left\\|\\mathrm{c}\\mathrm{li}\\mathbf{ip}_{\\lambda}\\left(\\mathcal{Q}\\left(\\widehat{\\Delta}_{i}\\left(x^{k+1},x^{k}\\right)\\right)\\right)-\\Delta_{i}\\left(x^{k+1},x^{k}\\right)\\right\\|^{2}\\right]\\right]\\mid\\left[3\\right]\\right]}\\\\ &{\\quad+\\frac{\\mathcal{P}_{\\mathcal{G}_{C}^{k}}}{C(1-\\delta)}\\mathbb{E}\\left[\\underset{i\\in\\mathcal{C}}{\\sum}2\\mathbb{E}_{k}\\left[\\left\\|\\Delta_{i}\\left(x^{k+1},x^{k}\\right)-\\Delta\\left(x^{k+1},x^{k}\\right)\\right\\|^{2}\\right]\\mid\\left[3\\right]\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Using Lemma D.6 we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{P_{1}^{\\varepsilon}\\stackrel{\\mathrm{Lom}}{\\leq}\\frac{P_{\\phi_{\\varepsilon}^{\\star}}}{C(1-\\delta)}\\mathbb{E}\\left[\\sum_{\\ell\\in\\mathcal{E}_{n}^{\\star}}\\left[\\mathbb{E}_{Q}\\left[\\left\\|Q\\left(\\widehat{\\Delta}_{\\star}\\left(x^{k+1},x^{k}\\right)\\right)-\\Delta_{\\star}\\left(x^{k+1},x^{k}\\right)\\right\\|^{2}\\right]\\right]\\mid\\{3\\}\\right]}\\\\ &{+\\ \\frac{P_{Q_{\\varepsilon}^{\\star}}}{C(1-\\delta)}\\mathbb{E}\\left[\\sum_{\\ell\\in\\mathcal{E}_{n}^{\\star}}\\left[\\big\\|\\Delta_{\\star}\\left(x^{k+1},x^{k}\\right)-\\Delta\\left(x^{k+1},x^{k}\\right)\\right\\|^{2}\\right]\\mid\\{3\\}\\right]}\\\\ &{\\leq\\frac{2(2)^{n}\\mathcal{L}_{Q_{\\star}^{\\star}}}{C(1-\\delta)}\\mathbb{E}_{\\left\\{\\phi_{\\varepsilon}^{\\star}\\right\\}}\\mathbb{E}_{\\mathbf{R}}\\left[\\mathbb{E}_{Q}\\left[\\left\\|Q\\left(\\widehat{\\Delta}_{\\star}\\left(x^{k+1},x^{k}\\right)\\right)-\\Delta_{\\star}\\left(x^{k+1},x^{k}\\right)\\right\\|^{2}\\right]\\mid\\mid\\Delta\\right]}\\\\ &{+\\ \\frac{2\\cdot P_{Q_{\\star}^{\\star}}}{C(1-\\delta)}\\mathbb{E}_{\\left\\{\\sum_{\\ell\\in\\mathcal{E}_{n}^{\\star}}\\left[\\left\\|\\Delta_{\\star}\\left(x^{k+1},x^{k}\\right)-\\Delta\\left(x^{k+1},x^{k}\\right)\\right\\|^{2}\\right]\\mid\\Delta\\right\\}}}\\\\ &{\\leq\\frac{2\\cdot P_{Q_{\\star}^{\\star}}}{C(1-\\delta)}\\mathbb{E}_{\\left\\{\\sum_{\\ell\\in\\mathcal{E}_{n}^{\\star}}\\left[\\mathbb{E}_{Q}\\left[\\left\\|Q\\left(\\widehat{\\Delta}_{\\star}\\left(x^{k+1},x^{k}\\right)\\right)\\right\\|^{2}\\right]\\right]-\\sum_{\\ell\\in\\mathcal{E}_{n}^{\\star}}\\left[\\Delta_{\\star}\\left(x^{k+1},x^{k}\\right)\\right]^{2}\\mid\\left|\\Delta\\right|\\right\\}}}\\\\ &{+\\ \\frac{2\\cdot P_{Q_{\\star}^{\\star}}}{C(1-\\delta)}\\mathbb{E}_ \n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Applying Definition 2.2 of Unbiased Compressor we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{B_{1}^{t}\\leq\\frac{20\\cdot P_{\\mathcal{G}_{\\kappa}^{k}}}{C(1-\\delta)}\\mathbb{E}\\left[\\sum_{i\\in\\mathcal{G}}\\left(1+\\omega\\right)\\mathbb{E}_{k}\\left\\|\\widehat{\\Delta}_{i}\\left(x^{k+1},x^{k}\\right)\\right\\|^{2}-\\sum_{i\\in\\mathcal{G}}\\left\\|\\Delta_{i}\\left(x^{k+1},x^{k}\\right)\\right\\|^{2}\\right]\\left[3\\right]\\right]}\\\\ &{+\\ \\frac{2\\cdot P_{\\mathcal{G}_{\\kappa}^{k}}}{C(1-\\delta)}\\mathbb{E}\\left[\\sum_{i\\in\\mathcal{G}}\\left\\|\\Delta_{i}\\left(x^{k+1},x^{k}\\right)-\\Delta\\left(x^{k+1},x^{k}\\right)\\right\\|^{2}\\left\\|\\ \\{3\\right]}\\\\ &{\\leq\\frac{20\\cdot P_{\\mathcal{G}_{\\kappa}^{k}}}{C(1-\\delta)}\\mathbb{E}\\left[\\sum_{i\\in\\mathcal{G}}\\left(1+\\omega\\right)\\mathbb{E}_{k}\\left\\|\\widehat{\\Delta}_{i}\\left(x^{k+1},x^{k}\\right)-\\Delta_{i}\\left(x^{k+1},x^{k}\\right)\\right\\|^{2}\\right]}\\\\ &{+\\ \\frac{20\\cdot P_{\\mathcal{G}_{\\kappa}^{k}}}{C(1-\\delta)}\\mathbb{E}\\left[\\sum_{i\\in\\mathcal{G}}\\left(1+\\omega\\right)\\mathbb{E}_{k}\\left\\|\\Delta_{i}\\left(x^{k+1},x^{k}\\right)\\right\\|^{2}-\\sum_{i\\in\\mathcal{G}}\\mathbb{E}_{k}\\left\\|\\Delta_{i}\\left(x^{k+1},x^{k}\\right)\\right\\|^{2}\\left|\\ [3]\\right]}\\\\ &{+\\ \\frac{2\\cdot P_{\\mathcal{G}_{\\kappa}^{k}}}{C(1-\\delta)}\\mathbb{E}\\left[\\sum_{i\\in\\mathcal{G}}\\left\\|\\Delta_{i}\\left(x^{k+1},x^{k}\\right)-\\Delta\\left(x^{k+1},x^{k}\\right)\\right\\|^{2}\\left|\\ [3]\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Now we combine terms and have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{B_{1}^{t}\\leq\\frac{2\\cdot P_{0}\\varepsilon_{\\star}}{C(1-\\delta)}(1+\\omega)\\mathbb{E}\\left[\\sum_{\\ell=0}^{\\infty}\\mathbb{E}\\left[\\left\\|\\hat{\\Delta}_{i}\\left(x^{k+1},x^{k}\\right)-\\Delta_{i}\\left(x^{k+1},x^{k}\\right)\\right\\|^{2}\\right]\\mid\\left|3\\right|\\right]}\\\\ &{\\quad+\\frac{2\\cdot P_{0}\\varepsilon_{\\star}}{C(1-\\delta)}\\mathbb{E}\\left[\\sum_{\\ell=0}^{\\infty}\\left\\|\\Delta_{i}\\left(x^{k+1},x^{k}\\right)\\right\\|^{2}\\mid\\left|3\\right|\\right]}\\\\ &{\\quad+\\frac{2\\cdot P_{0}\\varepsilon_{\\star}}{C(1-\\delta)}\\mathbb{E}\\left[\\sum_{\\ell=0}^{\\infty}\\left\\|\\Delta_{i}\\left(x^{k+1},x^{k}\\right)-\\Delta\\left(x^{k+1},x^{k}\\right)\\right\\|^{2}\\mid\\left|3\\right|\\right]}\\\\ &{\\quad=\\frac{2\\cdot P_{0}\\varepsilon_{\\star}}{C(1-\\delta)}(1+\\omega)\\mathbb{E}\\left[\\sum_{\\ell=0}^{\\infty}\\mathbb{E}\\left[\\left\\|\\hat{\\Delta}_{i}\\left(x^{k+1},x^{k}\\right)-\\Delta_{i}\\left(x^{k+1},x^{k}\\right)\\right\\|^{2}\\right]\\mid\\left|3\\right|\\right]}\\\\ &{\\quad+\\frac{2\\cdot P_{0}\\varepsilon_{\\star}}{C(1-\\delta)}\\mathbb{E}\\left[\\sum_{\\ell=0}^{\\infty}\\left\\|\\Delta_{i}\\left(x^{k+1},x^{k}\\right)-\\Delta\\left(x^{k+1},x^{k}\\right)\\right\\|^{2}+\\left\\|\\Delta\\left(x^{k+1},x^{k}\\right)\\right\\|^{2}\\mid\\left|3\\right|\\right]}\\\\ &{\\quad+\\frac{2\\cdot P_{0}\\varepsilon_{\\star}}{C(1-\\delta)}\\mathbb{E}\\left[\\left\\|\\omega\\right\\|\\Delta_{i}\\left(x^{k+1},x^{k}\\right)-\\Delta\\left(x^{k+1},x^{k}\\right)\\right\\|^{2}\\mid\\left|3\\right|\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Rearranging terms leads to ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{B_{1}^{\\prime}\\leq\\displaystyle\\frac{20\\cdot\\mathcal{P}_{\\mathcal{G}_{C}^{k}}}{C(1-\\delta)}(1+\\omega)\\mathbb{E}\\left[\\sum_{i\\in\\mathcal{G}}\\mathbb{E}_{k}\\left[\\left\\|\\widehat{\\Delta}_{i}\\left(x^{k+1},x^{k}\\right)-\\Delta_{i}\\left(x^{k+1},x^{k}\\right)\\right\\|^{2}\\right]\\mid\\left[3\\right]\\right]}\\\\ &{\\quad+\\displaystyle\\frac{2\\cdot\\mathcal{P}_{\\mathcal{G}_{C}^{k}}}{C(1-\\delta)}(10\\omega+1)\\mathbb{E}\\left[\\sum_{i\\in\\mathcal{G}}\\left\\|\\Delta_{i}\\left(x^{k+1},x^{k}\\right)-\\Delta\\left(x^{k+1},x^{k}\\right)\\right\\|^{2}\\mid\\left[3\\right]\\right]}\\\\ &{\\quad+\\displaystyle\\frac{20\\cdot\\mathcal{P}_{\\mathcal{G}_{C}^{k}}}{C(1-\\delta)}\\omega\\mathbb{E}\\left[\\sum_{i\\in\\mathcal{G}}\\left\\|\\Delta\\left(x^{k+1},x^{k}\\right)\\right\\|^{2}\\mid\\left[3\\right]\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Now we apply Assumptions D.1, D.2, D.3: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{B_{1}^{\\prime}\\leq\\displaystyle\\frac{20\\cdot\\mathcal{P}_{\\mathcal{G}_{C}^{k}}}{C(1-\\delta)}(1+\\omega)\\mathbb{E}\\left[G\\displaystyle\\frac{\\mathcal{L}_{\\pm}^{2}}{b}\\|x^{k+1}-x^{k}\\|^{2}\\right]}\\\\ &{\\quad+\\displaystyle\\frac{2\\cdot\\mathcal{P}_{\\mathcal{G}_{C}^{k}}}{C(1-\\delta)}(10\\omega+1)\\mathbb{E}\\left[G L_{\\pm}^{2}\\|x^{k+1}-x^{k}\\|^{2}\\right]}\\\\ &{\\quad+\\displaystyle\\frac{20\\cdot\\mathcal{P}_{\\mathcal{G}_{C}^{k}}}{C(1-\\delta)}\\omega\\mathbb{E}\\left[G L^{2}\\left\\|x^{k+1}-x^{k}\\right\\|^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Finally, we have ", "page_idx": 29}, {"type": "equation", "text": "$$\nB_{1}^{\\prime}\\leq\\frac{2\\cdot\\mathcal{P}_{\\mathcal{G}_{C}^{k}}\\cdot G}{C(1-\\delta)}\\left(10\\omega L^{2}+(10\\omega+1)L_{\\pm}^{2}+\\frac{10(\\omega+1)\\mathcal{L}_{\\pm}^{2}}{b}\\right)\\mathbb{E}\\left[\\|x^{k+1}-x^{k}\\|^{2}\\right].\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Let us plug obtained results: ", "text_level": 1, "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\cal B}_{1}\\leq\\left(1+\\displaystyle\\frac{p}{4}\\right)\\mathbb{E}\\left[\\left\\|g^{k}-\\nabla f(x^{k})\\right\\|^{2}\\right]}\\\\ {~~~+\\left(1+\\displaystyle\\frac{4}{p}\\right)\\displaystyle\\frac{2\\cdot\\mathcal{P}_{\\mathcal{G}_{C}^{k}}\\cdot G}{C(1-\\delta)}\\left(10\\omega L^{2}+(10\\omega+1)L_{\\pm}^{2}+\\displaystyle\\frac{10(\\omega+1)\\mathcal{L}_{\\pm}^{2}}{b}\\right)\\mathbb{E}\\left[\\left\\|x^{k+1}-x^{k}\\right\\|^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Let us consider the term E Gk \u2211 Vf(xk+1)-Vf(xk+1) C ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\operatorname{g}\\left[\\left\\|\\frac{1}{G_{\\widehat{C}}^{k}}\\sum_{i\\in\\mathcal{G}_{\\widehat{C}}^{k}}\\nabla f_{i}(x^{k+1})-\\nabla f(x^{k+1})\\right\\|^{2}\\right]\\leq\\operatorname{\\mathbb{E}}\\left[\\frac{1}{G_{\\widehat{C}}^{k}}\\sum_{i\\in\\mathcal{G}_{\\widehat{C}}^{k}}\\left\\|\\nabla f_{i}(x^{k+1})-\\nabla f(x^{k+1})\\right\\|^{2}\\right]}}\\\\ &{\\leq\\frac{1}{(1-\\delta)\\widehat{C}}\\operatorname{\\mathbb{E}}\\left[\\displaystyle\\sum_{i\\in\\mathcal{G}_{\\widehat{C}}^{k}}\\left\\|\\nabla f_{i}(x^{k+1})-\\nabla f(x^{k+1})\\right\\|^{2}\\right]}\\\\ &{=\\frac{1}{(1-\\delta)\\widehat{C}}\\operatorname{\\mathbb{E}}\\left[\\displaystyle\\sum_{i\\in\\mathcal{G}}\\overline{{z}}_{\\widehat{C}_{\\widehat{C}}^{k}}\\left\\|\\nabla f_{i}(x^{k+1})-\\nabla f(x^{k+1})\\right\\|^{2}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Using definition of $\\mathcal{P}_{\\mathcal{G}_{C}^{k}}$ we get ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\Vert\\displaystyle\\frac{1}{G_{\\widehat{C}}^{k}}\\sum_{i\\in\\mathcal{G}_{\\widehat{C}}^{k}}\\nabla f_{i}(x^{k+1})-\\nabla f(x^{k+1})\\right\\Vert^{2}\\right]\\leq\\displaystyle\\frac{\\mathcal{P}_{\\mathcal{G}_{\\widehat{C}}^{k}}}{(1-\\delta)\\widehat{C}}\\mathbb{E}\\left[\\sum_{i\\in\\mathcal{G}}\\left\\Vert\\nabla f_{i}(x^{k+1})-\\nabla f(x^{k+1})\\right\\Vert^{2}\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\leq\\displaystyle\\frac{G\\cdot\\mathcal{P}_{\\mathcal{G}_{\\widehat{C}}^{k}}}{(1-\\delta)\\widehat{C}G}\\mathbb{E}\\left[\\sum_{i\\in\\mathcal{G}}\\left\\Vert\\nabla f_{i}(x^{k+1})-\\nabla f(x^{k+1})\\right\\Vert^{2}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Using Assumption D.5 we get ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\Vert\\displaystyle\\frac{1}{G_{\\widehat{C}}^{k}}\\sum_{i\\in\\mathcal{G}_{\\widehat{C}}^{k}}\\nabla f_{i}(x^{k+1})-\\nabla f(x^{k+1})\\right\\Vert^{2}\\right]\\leq\\displaystyle\\frac{G\\cdot\\mathcal{P}_{\\mathcal{G}_{\\widehat{C}}^{k}}}{(1-\\delta)\\widehat{C}}\\mathbb{E}\\left[B\\|\\nabla f(x)\\|^{2}+\\zeta^{2}\\right]}\\\\ &{\\qquad\\qquad\\leq\\displaystyle\\frac{\\delta_{\\mathrm{real}}n\\cdot\\mathcal{P}_{\\mathcal{G}_{\\widehat{C}}^{k}}}{(1-\\delta)\\frac{\\delta_{\\mathrm{real}}n}{\\delta}}\\mathbb{E}\\left[B\\|\\nabla f(x)\\|^{2}+\\zeta^{2}\\right]}\\\\ &{\\qquad\\qquad=\\displaystyle\\frac{\\delta\\cdot\\mathcal{P}_{\\mathcal{G}_{\\widehat{C}}^{k}}}{(1-\\delta)}\\mathbb{E}\\left[B\\|\\nabla f(x)\\|^{2}+\\zeta^{2}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Also, we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{A_{1}=\\mathbb{E}\\left[\\left\\Vert\\overline{{g}}^{k+1}-\\nabla f(x^{k+1})\\right\\Vert^{2}\\right]}\\\\ &{\\quad\\leq(1-p)p_{G}B_{1}+(1-p)(1-p_{G})\\mathbb{E}\\left[\\left\\Vert g^{k}-\\nabla f(x^{k})\\right\\Vert^{2}\\right]+p\\frac{\\delta\\cdot\\mathcal{P}_{\\mathcal{G}_{\\mathcal{C}}^{k}}}{(1-\\delta)}\\mathbb{E}\\left[B\\Vert\\nabla f(x)\\Vert^{2}+\\zeta^{2}\\right]}\\\\ &{\\quad\\leq(1-p)p_{G}\\left(1+\\frac{p}{4}\\right)\\mathbb{E}\\left[\\left\\Vert g^{k}-\\nabla f(x^{k})\\right\\Vert^{2}\\right]}\\\\ &{\\quad+(1-p)p_{G}\\left(1+\\frac{4}{p}\\right)\\frac{2\\cdot\\mathcal{P}_{\\mathcal{G}_{\\mathcal{C}}^{k}}\\cdot\\mathcal{C}}{C(1-\\delta)}\\left(10\\omega L^{2}+(10\\omega+1)L_{\\pm}^{2}+\\frac{10(\\omega+1)C_{\\pm}^{2}}{b}\\right)\\mathbb{E}\\left[\\Vert x^{k+1}-x^{k}\\Vert^{2}\\right]}\\\\ &{\\quad+\\left(1-p\\right)(1-p_{G})\\mathbb{E}\\left[\\left\\Vert g^{k}-\\nabla f(x^{k})\\right\\Vert^{2}\\right]+p\\frac{\\delta\\cdot\\mathcal{P}_{\\mathcal{G}_{\\mathcal{C}}^{k}}}{(1-\\delta)}\\mathbb{E}\\left[B\\Vert\\nabla f(x)\\Vert^{2}+\\zeta^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "To simplify the bound we use $\\textstyle{\\left(1+{\\frac{p}{4}}>1\\right)}$ and obtain ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{A_{1}\\leq(1-p)p_{G}\\left(1+\\frac{\\rho}{4}\\right)\\mathbb{E}\\left[\\|g^{k}-\\nabla f(x^{k})\\|^{2}\\right]+p_{(1-\\frac{\\rho}{\\delta})}^{\\frac{\\rho}{\\delta}-\\frac{\\rho}{\\delta}}\\mathbb{E}\\left[B\\|\\nabla f(x)\\|^{2}+\\zeta^{2}\\right]}\\\\ &{\\quad+(1-p)p_{G}\\left(1+\\frac{p}{4}\\right)^{2-\\frac{\\rho}{\\delta}}\\mathbb{E}\\left[(\\ln\\lambda^{2}+(\\ln\\lambda)L_{2}^{2}+\\frac{10(\\ln+1)C_{2}^{2}}{\\hbar})\\mathbb{E}\\left[\\|g^{k+1}-x^{k}\\|^{2}\\right]}\\\\ &{\\quad+(1-p)(1-p_{G})\\mathbb{E}\\left[\\|g^{k}-\\nabla f(x^{k})\\|^{2}\\right]}\\\\ &{\\quad\\leq(1-p)p_{G}\\left(1+\\frac{q}{\\delta}\\right)\\mathbb{E}\\left[\\|g^{k}-\\nabla f(x^{k})\\|^{2}\\right]}\\\\ &{\\quad+(1-p)p_{G}\\left(1+\\frac{q}{\\delta}\\right)^{2-\\frac{\\rho}{\\delta}}\\mathbb{E}\\left(\\ln\\lambda^{2}+(\\ln(1-1)L_{2}^{2}+\\frac{10(\\ln+1)C_{2}^{2}}{\\hbar})\\mathbb{E}\\left[\\|g^{k+1}-x^{k}\\|^{2}\\right]\\right.}\\\\ &{\\quad+(1-p)(1-p_{G})\\left(1+\\frac{q}{\\delta}\\right)\\mathbb{E}\\left[\\|g^{k}-\\nabla f(x^{k})\\|^{2}\\right]+p_{(1-\\frac{\\rho}{\\delta})}^{\\frac{\\rho}{\\delta}-\\frac{\\rho}{\\delta}}\\mathbb{E}\\left[B\\|\\nabla f(x)\\|^{2}+\\zeta^{2}\\right]}\\\\ &{\\quad\\leq(1-p)\\left(1+\\frac{p}{4}\\right)\\mathbb{E}\\left[\\|g^{k}-\\nabla f(x^{k})\\|^{2}\\right]+p_{(1-\\frac{\\rho}{\\delta})}^{\\frac{\\rho}{\\delta}-\\frac{\\rho}{\\delta}}\\mathbb{E}\\left[B\\|\\nabla f(x)\\|^{2}+\\zeta^{2}\\right]}\\\\ &{\\quad+(1-p)p_{G}\\left(1+\\frac{p}{4}\\right)^{2-\\frac{\\rho}{\\delta}}\\mathbb{E}\\\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Lemma D.9. Let us define \"ideal\" estimator: ", "text_level": 1, "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\overline{{g}}^{k+1}=\\left\\{\\begin{array}{l l}{\\frac{1}{G_{C}^{k}}\\displaystyle\\sum_{i\\in\\mathcal{G}_{C}^{k}}\\nabla f_{i}(x^{k+1}),}&{c_{n}=1,}\\\\ {g^{k}+\\nabla f\\left(x^{k+1}\\right)-\\nabla f\\left(x^{k}\\right),}&{c_{n}=0\\ a n d\\,G_{C}^{k}<(1-\\delta)C,}\\\\ {g^{k}+\\frac{1}{G_{C}^{k}}\\displaystyle\\sum_{i\\in\\mathcal{G}_{C}^{k}}c\\,l\\,\\dot{q}_{\\lambda}\\left(\\mathcal{Q}\\left(\\widehat{\\Delta}_{i}\\left(x^{k+1},x^{k}\\right)\\right)\\right),}&{c_{n}=0\\ a n d\\,G_{C}^{k}\\geq(1-\\delta)C.}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Also let us introducethenotation ", "page_idx": 30}, {"type": "text", "text": "$\\begin{array}{r}{A R A g g_{Q}^{k+1}=A R A g g\\left(c l\\,i p_{\\lambda_{k+1}}\\left(\\mathcal{Q}\\left(\\widehat{\\Delta}_{1}(x^{k+1},x^{k})\\right)\\right),\\ldots,c l\\,i p_{\\lambda_{k+1}}\\left(\\mathcal{Q}\\left(\\widehat{\\Delta}_{C}(x^{k+1},x^{k})\\right)\\right)\\right).}\\end{array}$ Thenfor all $k\\geq0$ the iterates produced by Byz-VR-MARINA-PP (Algorithm $^{\\,l}$ )satisfy ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{A_{2}=\\mathbb{E}\\left[\\left\\Vert g^{k+1}-\\overline{{g}}^{k+1}\\right\\Vert^{2}\\right]}\\\\ &{\\quad\\leq p\\mathbb{E}\\left[\\mathbb{E}_{k}\\left[\\left\\Vert A R A g g\\left(\\{g_{i}^{k+1}\\}_{i\\in S_{k}}\\right)-\\nabla f(x^{k+1})\\right\\Vert^{2}\\right]\\mid[1]\\right]}\\\\ &{\\quad+\\left(1-p\\right)p_{G}\\mathbb{E}\\left[\\mathbb{E}_{k}\\left[\\left\\Vert\\frac{1}{G_{C}^{k}}\\sum_{i\\in\\mathcal{G}_{C}^{k}}c l\\eta_{\\lambda}\\left(\\mathcal{Q}\\left(\\widehat{\\Delta}_{i}\\left(x^{k+1},x^{k}\\right)\\right)\\right)-A R A g g_{Q}^{k+1}\\right\\Vert^{2}\\mid[3]\\right]\\right]}\\\\ &{\\quad+\\left(1-p\\right)(1-p_{G})\\mathbb{E}\\left[\\mathbb{E}_{k}\\left[\\left\\Vert\\nabla f(x^{k+1})-\\nabla f(x^{k})-A R A g g_{Q}^{k+1}\\right\\Vert^{2}\\mid[2]\\right]\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where $p_{G}=\\mathrm{Prob}\\left\\{G_{C}^{k}\\geq(1-\\delta)C\\right\\}$ ", "page_idx": 31}, {"type": "text", "text": "Proof. Using conditional expectations we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{A_{2}=\\mathbb{E}\\left[\\mathbb{E}_{k}\\left[\\left\\Vert g^{k+1}-\\overline{{g}}^{k+1}\\right\\Vert^{2}\\right]\\right]}\\\\ &{\\quad=p\\mathbb{E}\\left[\\mathbb{E}_{k}\\left[\\left\\Vert\\mathsf{A R A g g}\\left(\\left\\{g_{i}^{k+1}\\right\\}_{i\\in S_{k}}\\right)-\\nabla f(x^{k+1})\\right\\Vert^{2}\\right]\\mid\\left[1\\right]\\right]}\\\\ &{\\quad+(1-p)p_{G}\\mathbb{E}\\left[\\mathbb{E}_{k}\\left[\\left\\Vert g^{k}+\\frac{1}{G_{C}^{k}}\\sum_{i\\in\\mathcal{E}_{C}^{k}}\\mathsf{c l i p}_{\\lambda}\\left(\\mathcal{Q}\\left(\\widehat{\\Delta}_{i}\\left(x^{k+1},x^{k}\\right)\\right)\\right)-\\left(g^{k}+\\mathsf{A R A g g}_{Q}^{k+1}\\right)\\right\\Vert^{2}\\right]\\mid\\left[3\\right]\\right]}\\\\ &{\\quad+\\left(1-p\\right)(1-p_{G})\\mathbb{E}\\left[\\mathbb{E}_{k}\\left[\\left\\Vert g^{k}+\\nabla f(x^{k+1})-\\nabla f(x^{k})-\\left(g^{k}+\\mathsf{A R A g g}_{Q}^{k+1}\\right)\\right\\Vert^{2}\\right]\\mid\\left[2\\right]\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "After simplification, we get the following bound: ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{A_{2}\\leq p\\mathbb{E}\\left[\\mathbb{E}_{k}\\left[\\left\\|\\mathsf{A R A g g}\\left(\\{g_{i}^{k+1}\\}_{i\\in S_{k}}\\right)-\\nabla f(x^{k+1})\\right\\|^{2}\\right]\\mid[1]\\right]}\\\\ &{\\quad+\\left(1-p\\right)p_{G}\\mathbb{E}\\left[\\mathbb{E}_{k}\\left[\\left\\|\\frac{1}{G_{C}^{k}}\\sum_{i\\in\\mathcal{G}_{C}^{k}}\\mathsf{c l i p}_{\\lambda}\\left(\\mathcal{Q}\\left(\\widehat{\\Delta}_{i}\\left(x^{k+1},x^{k}\\right)\\right)\\right)-\\mathsf{A R A g g}_{Q}^{k+1}\\right\\|^{2}\\mid[3]\\right]\\right]}\\\\ &{\\quad+\\left(1-p\\right)(1-p_{G})\\mathbb{E}\\left[\\mathbb{E}_{k}\\left[\\left\\|\\nabla f(x^{k+1})-\\nabla f(x^{k})-\\mathsf{A R A g g}_{Q}^{k+1}\\right\\|^{2}\\mid[2]\\right]\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Lemma D.10.Let Assumptions $D.I$ and D.5 hold and Aggregation Operator (ARAgg) satisfy Definition 2.1.Then for all $k\\ \\geq\\ 0$ the iterates produced by Byz-VR-MARINA-PP (Algorithm 1) satisfy ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{T_{1}=\\mathbb{E}\\left[\\mathbb{E}_{k}\\left[\\left\\|A R A g g\\left(\\{g_{i}^{k+1}\\}_{i\\in S_{k}}\\right)-\\nabla f(x^{k+1})\\right\\|^{2}\\right]\\mid[1]\\right]}\\\\ &{\\quad\\leq\\left(\\frac{8G\\mathcal{P}_{\\mathcal{G}_{\\widehat{C}}^{k}}c\\delta B}{(1-\\delta)\\widehat{C}}+2\\widetilde{B}\\right)\\mathbb{E}\\left[\\left\\|\\nabla f\\left(x^{k}\\right)\\right\\|^{2}+L^{2}\\left\\|x^{k+1}-x^{k}\\right\\|^{2}\\right]+\\frac{4G\\mathcal{P}_{\\mathcal{G}_{\\widehat{C}}^{k}}c\\delta\\zeta^{2}}{(1-\\delta)\\widehat{C}}+\\widetilde{\\zeta}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where $\\widetilde B:=0$ and $\\widetilde{\\zeta}^{2}:=0$ when ${\\widehat{C}}=n$ and $\\begin{array}{r}{\\widetilde{B}:=\\frac{\\mathcal{P}_{\\mathcal{G}_{\\widehat{C}}^{k}}G B}{(1-\\delta)\\widehat{C}}}\\end{array}$ and2 $\\begin{array}{r}{\\widetilde{\\zeta}^{2}:=\\frac{\\mathcal{P}_{\\mathcal{G}_{\\widehat{C}}^{k}}G\\zeta^{2}}{(1-\\delta)\\widehat{C}}}\\end{array}$ when ${\\widehat{C}}<n$ ", "page_idx": 31}, {"type": "text", "text": "Proof. Using the definition of aggregation operator, we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{T_{1}=\\mathbb{E}\\left[\\mathbb{E}_{k}\\left[\\left\\|\\mathbf{A}\\mathbf{RAgg}\\left(\\{g_{i}^{k+1}\\}_{i\\in S_{k}}\\right)-\\nabla f(x^{k+1})\\right\\|^{2}\\right]\\mid[1]\\right]}\\\\ &{\\ \\ \\stackrel{(12)}{\\leq}\\mathbb{E}\\left[\\mathbb{E}_{k}\\left[\\left\\|\\mathbf{A}\\mathbf{RAgg}\\left(\\{g_{i}^{k+1}\\}_{i\\in S_{k}}\\right)-\\frac{1}{G_{\\widehat{C}}^{k}}\\sum_{i\\in\\mathcal{G}_{\\widehat{C}}^{k}}\\nabla f_{i}(x^{k+1})\\right\\|^{2}\\right]\\mid[1]\\right]}\\\\ &{\\ \\ \\ \\ +\\mathbb{E}\\left[\\mathbb{E}_{k}\\left[\\left\\|\\frac{1}{G_{\\widehat{C}}^{k}}\\sum_{i\\in\\mathcal{G}_{\\widehat{C}}^{k}}\\nabla f_{i}(x^{k+1})-\\nabla f(x^{k+1})\\right\\|^{2}\\right]\\mid[1]\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "$\\frac{1}{G_{\\hat{\\cal{C}}}^{k}}\\sum_{i\\in\\mathcal{G}_{\\hat{\\cal{C}}}^{k}}\\nabla f_{i}(x^{k+1})=\\nabla f(x^{k+1})$ ${\\widehat{C}}=n$ term as ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\mathbb{E}_{k}\\left[\\left\\|\\frac{1}{C_{\\varepsilon}^{k}}\\sum_{i\\in\\mathcal{E}_{\\hat{\\mathcal{E}}_{\\hat{\\mathcal{E}}}}^{k}}\\nabla f_{i}(x^{k+1})-\\nabla f(x^{k+1})\\right\\|^{2}\\right]\\mid[1]\\right]}\\\\ &{\\leq\\left\\{\\begin{array}{l l}{0,}&{\\mathrm{if}\\;\\hat{C}=n}\\\\ {\\mathbb{E}\\left[\\frac{1}{C_{\\varepsilon}^{k}}\\sum_{i\\in\\mathcal{E}_{\\hat{\\mathcal{E}}_{\\mathcal{E}}}^{k}}\\mathbb{E}_{k}\\left[\\left\\|\\nabla f_{i}(x^{k+1})-\\nabla f(x^{k+1})\\right\\|^{2}\\right]\\mid[1]\\right],}&{\\mathrm{if}\\;\\hat{C}<n}\\\\ {\\vdots}&{\\mathrm{if}\\;\\hat{C}=n}\\end{array}\\right.}\\\\ &{\\leq\\left\\{\\begin{array}{l l}{0,}&{\\mathrm{if}\\;\\hat{C}=n}\\\\ {\\frac{\\mathcal{P}_{\\varepsilon}}{\\sqrt{1-\\delta_{\\varepsilon}^{k}}\\hat{C}}\\underset{i\\in\\mathcal{E}_{\\mathcal{E}}}{\\sum}\\mathbb{E}\\left[\\left\\|\\nabla f_{i}(x^{k+1})-\\nabla f(x^{k+1})\\right\\|^{2}\\right]\\;,}&{\\mathrm{if}\\;\\hat{C}<n}\\\\ {\\vdots}&{\\mathrm{if}\\;\\hat{C}=n}\\end{array}\\right.}\\\\ &{\\overset{(A\\setminus L)}{\\leq}\\left\\{\\begin{array}{l l}{0,}&{\\mathrm{if}\\;\\hat{C}=n}\\\\ {\\frac{\\mathcal{P}_{\\varepsilon}}{\\sqrt{1-\\delta_{\\varepsilon}^{k}}\\hat{C}}\\;\\big(B\\mathbb{E}\\left[\\|\\nabla f(x^{k+1})\\|^{2}\\right]+\\hat{\\mathcal{E}}^{2}\\big)\\;,}&{\\mathrm{if}\\;\\hat{C}<n}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widetilde B:=\\left\\{\\!\\!\\!\\begin{array}{l l}{0,}&{\\mathrm{if}\\;\\widehat C=n,}\\\\ {\\frac{\\mathcal{P}_{\\mathcal{G}_{\\widehat{C}}^{k}}G B}{(1-\\delta)\\widehat{C}},}&{\\mathrm{if}\\;\\widehat C<n,}\\end{array}\\right.\\quad\\mathrm{and}\\quad\\widetilde{\\zeta}^{2}:=\\left\\{\\!\\!\\!\\begin{array}{l l}{0,}&{\\mathrm{if}\\;\\widehat C=n,}\\\\ {\\frac{\\mathcal{P}_{\\mathcal{G}_{\\widehat{C}}^{k}}G\\zeta^{2}}{(1-\\delta)\\widehat{C}},}&{\\mathrm{if}\\;\\widehat C<n.}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Using the above bound, we continue the estimation of $T_{1}$ as follows: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\phantom{\\frac{1}{\\mu}}\\frac{\\partial\\int_{\\mathbb{Z}}\\bigg\\vert\\nabla\\cdot\\mathbf{u}^{\\theta}\\bigg\\vert^{2}-\\frac{\\theta}{2}\\bigg\\vert\\frac{\\partial\\theta}{\\partial\\theta}\\bigg\\vert\\nabla\\cdot\\mathbf{u}^{\\theta+1}-\\nabla f_{\\theta}(\\theta^{+})-\\nabla f_{\\theta}(\\theta^{+})\\bigg\\vert^{2}\\vert\\mathbf{\\theta}\\bigg\\vert}{\\partial\\theta}\\bigg\\vert}&{}\\\\ {+\\frac{\\theta}{2}\\mathbb{E}\\bigg\\vert\\|\\nabla f_{\\theta}(\\theta^{+})^{\\frac{1}{\\theta}}\\|^{2}+\\frac{\\theta}{2}\\|\\nabla^{2}f_{\\theta}(\\theta^{+})-\\nabla f_{\\theta}(\\theta^{+})\\|^{2}\\|\\theta\\|^{2}}&{}\\\\ {\\frac{\\theta}{\\sqrt{\\mu}}\\bigg\\vert\\frac{\\partial\\theta}{\\partial\\theta}\\bigg\\vert\\sqrt{\\frac{\\partial\\theta}{\\partial\\theta}}\\bigg\\vert-\\frac{\\theta}{2}\\frac{\\partial\\theta}{\\partial\\theta}\\bigg\\vert\\nabla\\big\\vert f_{\\theta}(\\theta^{+})-\\nabla f_{\\theta}(\\theta^{+})\\big\\vert^{2}\\vert\\mathbf{\\theta}\\bigg\\vert}&{\\bigg\\vert\\theta\\bigg\\vert}\\\\ {+\\frac{\\theta}{2}\\bigg\\vert\\frac{\\partial\\theta}{\\partial\\theta}\\bigg\\vert\\sqrt{\\frac{\\partial\\theta}{\\partial\\theta}}-\\frac{\\theta}{2}\\frac{\\partial\\theta}{\\partial\\theta}\\bigg\\vert\\nabla f_{\\theta}(\\theta^{+})-\\nabla f_{\\theta}(\\theta^{+})\\bigg\\vert^{2}\\vert\\mathbf{\\theta}\\bigg\\vert}&{}\\\\ {+\\frac{\\theta}{2}\\big\\vert\\nabla^{2}\\vert\\theta\\vert\\vert\\nabla f_{\\theta}(\\theta^{+})^{\\frac{1}{\\theta}}\\big\\vert\\nabla f_{\\theta}(\\theta^{+})-\\nabla f_{\\theta}(\\theta^{+})\\big\\vert^{2}\\vert\\mathbf{\\theta}\\bigg\\vert\\bigg\\vert}&{}\\\\ {-\\frac{\\theta}{2}\\bigg\\vert\\frac{\\partial\\theta}{\\partial\\theta}\\bigg\\vert\\nabla f_{\\theta}(\\theta^{+})-\\nabla f_{\\theta}(\\theta^{+})-\\nabla f_{\\theta}(\\theta^{+})\\bigg\\vert^{2}\\vert\\mathbf{\\theta}\\bigg\\vert\\bigg\\vert\\frac{\\partial\\theta}{\\partial\\theta}\\bigg\\vert\\sqrt{\\vert\\theta^{+}\\vert\\theta\\vert^{2}}+\\mathcal{D}}&{}\\\\ {\\frac{\\theta}{\\sqrt{\\mu}}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "which concludes the proof. ", "page_idx": 33}, {"type": "text", "text": "Lemma D.11. Let Assumptions D.1, D.2, D.3 hold and the Compression Operator satisfy Definition 2.2.Also let us introduce the notation ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{A R A g g_{Q}^{k+1}=A R A g g\\left(c l\\,i p_{\\lambda_{k+1}}\\left(\\mathcal{Q}\\left(\\widehat{\\Delta}_{1}(x^{k+1},x^{k})\\right)\\right),\\ldots,c l\\,i p_{\\lambda_{k+1}}\\left(\\mathcal{Q}\\left(\\widehat{\\Delta}_{C}(x^{k+1},x^{k})\\right)\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Thenfor all $k\\geq0$ the iterates produced by Byz-VR-MARINA-PP (Algorithm 1) satisfy ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{T_{2}=\\mathbb{E}\\left[\\mathbb{E}_{k}\\left[\\left\\|\\frac{1}{G_{C}^{k}}\\sum_{i\\in\\mathcal{G}_{C}^{k}}c^{\\i i}i\\pmb{p}_{\\lambda}\\left(\\mathcal{Q}\\left(\\widehat{\\Delta}_{i}\\left(x^{k+1},x^{k}\\right)\\right)\\right)-A R A g g_{Q}^{k+1}\\right\\|^{2}\\mid\\left[3\\right]\\right]\\right]}\\\\ &{\\quad\\leq\\frac{8G\\mathcal{P}\\mathcal{G}_{C}^{k}}{(1-\\delta)C}\\left(10(1+\\omega)\\frac{\\mathcal{L}_{\\pm}^{2}}{b}+(10\\omega+1)L_{\\pm}^{2}+10\\omega L^{2}\\right)c\\delta\\mathbb{E}\\left[\\|x^{k+1}-x^{k}\\|^{2}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where $\\mathcal{P}_{\\mathcal{G}_{C}^{k}}=\\operatorname{Prob}\\left\\{i\\in\\mathcal{G}_{C}^{k}\\mid G_{C}^{k}\\geq\\left(1-\\delta\\right)C\\right\\}$ ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{T_{2}=\\mathbb{E}\\left[\\mathbb{E}_{k}\\left[\\left\\|\\frac{1}{G_{C}^{k}}\\sum_{i\\in\\mathcal{G}_{C}^{k}}\\mathrm{clip}_{\\lambda}\\left(\\mathcal{Q}\\left(\\widehat{\\Delta}_{i}\\left(x^{k+1},x^{k}\\right)\\right)\\right)-\\mathrm{ARAgg}_{Q}^{k+1}\\right\\|^{2}\\mid\\left[3\\right]\\right]\\right]}\\\\ &{\\quad\\leq\\mathbb{E}\\left[\\frac{c\\delta}{D_{2}}\\sum_{\\stackrel{i,l\\in\\mathcal{S}_{C}^{k}}{i\\neq l}}\\mathbb{E}_{k}\\left[\\left\\|\\mathrm{clip}_{\\lambda}\\left(\\mathcal{Q}\\left(\\widehat{\\Delta}_{i}\\left(x^{k+1},x^{k}\\right)\\right)\\right)-\\mathrm{clip}_{\\lambda}\\left(\\mathcal{Q}\\left(\\widehat{\\Delta}_{l}\\left(x^{k+1},x^{k}\\right)\\right)\\right)\\right\\|^{2}\\mid\\left[3\\right]\\right]\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where $D_{2}=G_{C}^{k}(G_{C}^{k}-1)$ . Next, we consider pair-wise differences: ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{T_{2}^{\\varepsilon}(t,t)=\\frac{\\varepsilon}{2}\\Bigg[\\Bigg|\\frac{\\mathrm{d}}{\\mathrm{d}t}\\bigg(\\sigma\\Big(\\widehat{\\Delta}_{\\varepsilon}\\left(\\varepsilon^{+1},x^{\\varepsilon}\\right)\\Big)\\Big)-\\mathrm{culp}_{\\varepsilon}\\Big(Q\\Big(\\widehat{\\Delta}_{\\varepsilon}\\left(\\varepsilon^{+1},x^{\\varepsilon}\\right)\\Big)\\Big)\\Bigg|^{2}\\Bigg|\\mathbb{B}\\Bigg]}\\\\ &{\\quad\\stackrel{(a)}{\\leq}\\frac{1}{2}\\Bigg[\\mathrm{d}\\sigma_{\\varepsilon\\varepsilon}\\Big(Q\\Big(\\widehat{\\Delta}_{\\varepsilon}\\left(\\varepsilon^{+1},x^{\\varepsilon}\\right)\\Big)-\\widehat{n}_{\\varepsilon}\\Big(\\varepsilon^{+1},x^{\\varepsilon}\\Big)+\\delta_{\\varepsilon}\\Big(\\varepsilon^{+1},x^{\\varepsilon}\\Big)-\\sigma_{\\theta\\varepsilon}\\Big(Q\\Big(\\widehat{\\Delta}_{\\varepsilon}\\left(\\varepsilon^{+1},x^{\\varepsilon}\\right)\\Big)\\Big)\\Big|^{2}\\Bigg|\\ \\ \\mathrm{d}\\varepsilon}\\\\ &{\\quad+2\\varepsilon_{\\varepsilon}\\Big]\\Bigg[\\Delta_{\\varepsilon}\\Big(\\varepsilon^{+1},x^{\\varepsilon}\\Big)-\\Delta_{\\varepsilon}\\Big(\\varepsilon^{+1},x^{\\varepsilon}\\Big)\\Big]^{2}\\Bigg|\\ \\ |\\mathbb{B}\\Bigg|}\\\\ &{\\stackrel{(b)}{\\leq}\\frac{1}{2}\\Bigg\\{\\mathrm{d}\\varepsilon_{\\varepsilon}\\Big[\\bigg|\\widehat{\\Delta}_{\\varepsilon}\\Big(\\sigma\\Big(\\widehat{\\Delta}_{\\varepsilon}\\left(\\varepsilon^{+1},x^{\\varepsilon}\\right)\\Big)\\Big)-\\Delta_{\\varepsilon}\\Big(\\varepsilon^{+1},x^{\\varepsilon}\\Big)\\Big|^{2}\\Bigg|\\ \\ |\\widehat{\\Delta}\\Bigg|\\Bigg]}\\\\ &{\\quad+4\\varepsilon_{\\varepsilon}\\bigg[\\bigg|\\widehat{\\Delta}_{\\varepsilon}\\Big(\\varepsilon^{+1},x^{\\varepsilon}\\Big)-\\mathrm{culp}_{\\varepsilon}\\Big(Q\\Big(\\widehat{\\Delta}_{\\varepsilon}\\left(\\varepsilon^{+1},x^{\\varepsilon}\\right)\\Big)\\Big)\\bigg|^{2}\\Bigg|\\ \\ |\\widehat{\\Delta}\\Bigg|\\Bigg]}\\\\ &{\\quad+\\frac{2}{2}\\mathrm{d}\\varepsilon_{\\varepsilon}\\bigg[\\bigg|\\Delta_{\\varepsilon}\\Big(\\varepsilon^{+1},x^{\\varepsilon}\\Big)-\\Delta_{\\varepsilon}\\Big(\\varepsilon^{+1},x^{\\varepsilon}\\Big)\\Big|^{2}\\ |\\widehat{\\Delta}\\Big|\\Bigg]}\\\\ &{\\stackrel{(\n$$", "text_format": "latex", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\widetilde{T}_{2}=\\mathbb{E}\\left[\\frac{1}{\\widetilde{D}_{\\xi}(\\xi_{0}^{k+1},\\xi_{1}^{k})}\\sum_{i,j=1\\atop i\\neq j}^{T}\\mathbb{Z}_{\\xi_{i},\\xi_{j}}^{(i)}|\\xi_{i}|\\right]}\\\\ {\\le}&{\\mathbb{E}\\left[\\frac{1}{\\widetilde{D}_{\\xi}(\\xi_{0}^{k+1},\\xi_{1}^{k})}\\mathbb{H}\\left[\\left\\{1+|\\xi_{1}|+\\displaystyle\\left(\\alpha\\left(\\frac{\\alpha}{\\xi_{i}}+\\iota^{k+1},\\xi_{1}^{k}\\right)\\right)-\\Delta_{\\xi_{i}}(\\xi^{k+1},\\xi_{1}^{k})\\right\\}^{2}|\\xi_{1}|\\right]\\right]}\\\\ &{\\quad\\le\\mathbb{E}\\left[\\frac{1}{\\mathcal{D}_{\\xi}(\\xi_{0}^{k+1},\\xi_{0}^{k+1})}\\mathbb{E}\\left[\\left\\{\\left\\{1+|\\xi_{1}^{k+1},\\xi_{1}^{k}\\right\\}-\\mathrm{cin}_{\\mathbb{P}}\\left(\\left(\\widetilde{A}_{\\xi}\\left(\\xi^{k+1},\\xi_{1}^{k}\\right)\\right)\\right)|\\xi_{1}^{k}\\right\\}^{2}|\\xi_{1}|\\right]\\right]}\\\\ &{\\quad+\\mathbb{E}\\left[\\frac{1}{\\mathcal{D}_{\\xi}(\\xi_{0}^{k+1},\\xi_{1}^{k})}\\mathbb{E}\\left[\\left\\{\\left\\{\\Delta_{\\xi}(\\xi^{k+1},\\xi_{1}^{k})-\\Delta_{\\xi}(\\xi^{k+1},\\xi_{1}^{k})\\right\\}^{2}|\\xi_{1}|\\right\\}\\right]}\\\\ &{\\quad+\\mathbb{E}\\left[\\frac{1}{\\mathcal{D}_{\\xi}(\\xi_{0}^{k+1},\\xi_{1}^{k})}\\mathbb{E}\\left[\\left\\{\\left\\{1+|\\xi_{1}^{k+1},\\xi_{1}^{k}\\right\\}-\\Delta\\left(\\xi^{k+1},\\xi_{1}^{k}\\right)|\\xi_{1}^{k}\\right\\}^{2}|\\xi_{1}|\\right]\\right]}\\\\ &{\\quad+\\mathbb{E}\\left[\\frac{1}{\\mathcal{D}_{\\xi}(\\xi_{0}^{k+1},\\xi_{1}^{k+1})}\\mathbb{E}\\left[\\left\\\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Rearranging the terms, we obtain ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widehat{T}_{2}\\leq\\mathbb{E}\\left[\\frac{1}{D_{2}}\\sum_{\\stackrel{i,l\\in\\mathcal{I}_{C}^{k}}{i\\neq l}}8\\mathbb{E}_{k}\\left[\\left\\|\\mathbf{c}1\\mathbf{ip}_{\\lambda}\\left(\\mathcal{Q}\\left(\\widehat{\\Delta}_{i}\\left(x^{k+1},x^{k}\\right)\\right)\\right)-\\Delta_{i}\\left(x^{k+1},x^{k}\\right)\\right\\|^{2}\\mid\\left[3\\right]\\right]\\right]}\\\\ &{\\quad+\\mathbb{E}\\left[\\frac{1}{D_{2}}\\sum_{\\stackrel{i,l\\in\\mathcal{I}_{C}^{k}}{i\\neq l}}8\\mathbb{E}_{k}\\left[\\left\\|\\Delta_{i}\\left(x^{k+1},x^{k}\\right)-\\Delta\\left(x^{k+1},x^{k}\\right)\\right\\|^{2}\\mid\\left[3\\right]\\right]\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "It leads to ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widehat{T}_{2}\\leq\\mathbb{E}\\left[\\frac{1}{G_{C}^{k}}\\sum_{i\\in\\mathcal{C}_{c}^{k}}8\\mathbb{E}_{k}\\left[\\left\\|\\mathrm{cil}\\mathrm{p}_{\\lambda}\\left(Q\\left(\\widehat{\\Delta}_{i}\\left(x^{k+1},x^{k}\\right)\\right)\\right)-\\Delta_{i}\\left(x^{k+1},x^{k}\\right)\\right\\|^{2}\\mid\\left[3\\right]\\right]\\right]}\\\\ &{\\quad+\\mathbb{E}\\left[\\frac{1}{G_{C}^{k}}\\sum_{i\\in\\mathcal{C}_{c}^{k}}8\\mathbb{E}_{k}\\left[\\left\\|\\Delta_{i}\\left(x^{k+1},x^{k}\\right)-\\Delta\\left(x^{k+1},x^{k}\\right)\\right\\|^{2}\\mid\\left[3\\right]\\right]\\right]}\\\\ &{\\stackrel{\\mathrm{Lemm}\\;\\mathrm{D},6}{\\leq}\\mathbb{E}\\left[\\frac{1}{G_{C}^{k}}\\sum_{i\\in\\mathcal{C}_{c}^{k}}80\\mathbb{E}_{k}\\left[\\left\\|Q\\left(\\widehat{\\Delta}_{i}\\left(x^{k+1},x^{k}\\right)\\right)-\\Delta_{i}\\left(x^{k+1},x^{k}\\right)\\right\\|^{2}\\mid\\left[3\\right]\\right]\\right]}\\\\ &{\\quad+\\mathbb{E}\\left[\\frac{1}{G_{C}^{k}}\\sum_{i\\in\\mathcal{C}_{c}^{k}}8\\mathbb{E}_{k}\\left[\\left\\|\\Delta_{i}\\left(x^{k+1},x^{k}\\right)-\\Delta\\left(x^{k+1},x^{k}\\right)\\right\\|^{2}\\mid\\left[3\\right]\\right]\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widehat{T}_{2}\\leq\\mathbb{E}\\left[\\frac{1}{G_{C}^{k}}\\sum_{i\\in\\mathcal{G}_{C}^{k}}80\\mathbb{E}_{k}\\left[\\left\\|\\mathscr{Q}\\left(\\widehat{\\Delta}_{i}\\left(x^{k+1},x^{k}\\right)\\right)\\right\\|^{2}\\mid\\left[3\\right]\\right]\\right]}\\\\ &{\\quad-\\mathbb{E}\\left[\\frac{1}{G_{C}^{k}}\\sum_{i\\in\\mathcal{G}_{C}^{k}}80\\mathbb{E}_{k}\\left[\\left\\|\\Delta_{i}\\left(x^{k+1},x^{k}\\right)\\right\\|^{2}\\mid\\left[3\\right]\\right]\\right]}\\\\ &{\\quad+\\mathbb{E}\\left[\\frac{1}{G_{C}^{k}}\\sum_{i\\in\\mathcal{G}_{C}^{k}}8\\mathbb{E}_{k}\\left[\\left\\|\\Delta_{i}\\left(x^{k+1},x^{k}\\right)-\\Delta\\left(x^{k+1},x^{k}\\right)\\right\\|^{2}\\mid\\left[3\\right]\\right]\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Using properties of unbiased compressors (Definition 2.2) we have ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widehat{T}_{2}\\leq\\mathbb{E}\\left[\\frac{1}{G_{C}^{k}}\\sum_{i\\in\\mathcal{G}_{C}^{k}}80(1+\\omega)\\mathbb{E}_{k}\\left[\\left\\Vert\\widehat{\\Delta}_{i}\\left(x^{k+1},x^{k}\\right)\\right\\Vert^{2}\\mid\\left[3\\right]\\right]\\right]}\\\\ &{\\quad-\\mathbb{E}\\left[\\frac{1}{G_{C}^{k}}\\sum_{i\\in\\mathcal{G}_{C}^{k}}80\\mathbb{E}_{k}\\left[\\left\\Vert\\Delta_{i}\\left(x^{k+1},x^{k}\\right)\\right\\Vert^{2}\\mid\\left[3\\right]\\right]\\right]}\\\\ &{\\quad+\\mathbb{E}\\left[\\frac{1}{G_{C}^{k}}\\sum_{i\\in\\mathcal{G}_{C}^{k}}8\\mathbb{E}_{k}\\left[\\left\\Vert\\Delta_{i}\\left(x^{k+1},x^{k}\\right)-\\Delta\\left(x^{k+1},x^{k}\\right)\\right\\Vert^{2}\\mid\\left[3\\right]\\right]\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Also we have ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widehat{T}_{2}\\leq\\mathbb{E}\\left[\\frac{1}{G_{C}^{k}}\\sum_{i\\in\\mathcal{E}_{c}^{k}}80(1+\\omega)\\mathbb{E}_{k}\\left[\\left\\|\\widehat{\\Delta}_{i}\\left(x^{k+1},x^{k}\\right)-\\Delta_{i}\\left(x^{k+1},x^{k}\\right)\\right\\|^{2}\\right.\\right]\\left\\|3\\right]\\right]}\\\\ &{\\quad+\\mathbb{E}\\left[\\frac{1}{G_{C}^{k}}\\sum_{i\\in\\mathcal{E}_{c}^{k}}80(1+\\omega)\\mathbb{E}_{k}\\left[\\left\\|\\Delta_{i}\\left(x^{k+1},x^{k}\\right)\\right\\|^{2}\\left\\|\\ \\left[3\\right]\\right]\\right]}\\\\ &{\\quad-\\mathbb{E}\\left[\\frac{1}{G_{C}^{k}}\\sum_{i\\in\\mathcal{E}_{c}^{k}}80\\mathbb{E}_{k}\\left[\\left\\|\\Delta_{i}\\left(x^{k+1},x^{k}\\right)\\right\\|^{2}\\left\\|\\ \\left[3\\right]\\right]\\right]}\\\\ &{\\quad+\\mathbb{E}\\left[\\frac{1}{G_{C}^{k}}\\sum_{i\\in\\mathcal{E}_{c}^{k}}8\\mathbb{E}_{k}\\left[\\left\\|\\Delta_{i}\\left(x^{k+1},x^{k}\\right)-\\Delta\\left(x^{k+1},x^{k}\\right)\\right\\|^{2}\\left|\\ \\left[3\\right]\\right]\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Let us simplify the inequality: ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widehat{T}_{2}\\leq\\mathbb{E}\\left[\\frac{1}{G_{C}^{k}}\\sum_{i\\in\\mathcal{C}_{C}^{k}}80(1+\\omega)\\mathbb{E}_{k}\\left[\\left\\Vert\\widehat{\\Delta}_{i}\\left(x^{k+1},x^{k}\\right)-\\Delta_{i}\\left(x^{k+1},x^{k}\\right)\\right\\Vert^{2}\\mid\\left[3\\right]\\right]\\right]}\\\\ &{\\quad+\\mathbb{E}\\left[\\frac{1}{G_{C}^{k}}\\sum_{i\\in\\mathcal{C}_{C}^{k}}80\\omega\\mathbb{E}_{k}\\left[\\left\\Vert\\Delta_{i}\\left(x^{k+1},x^{k}\\right)\\right\\Vert^{2}\\mid\\left[3\\right]\\right]\\right]}\\\\ &{\\quad+\\mathbb{E}\\left[\\frac{1}{G_{C}^{k}}\\sum_{i\\in\\mathcal{C}_{C}^{k}}8\\mathbb{E}_{k}\\left[\\left\\Vert\\Delta_{i}\\left(x^{k+1},x^{k}\\right)-\\Delta\\left(x^{k+1},x^{k}\\right)\\right\\Vert^{2}\\mid\\left[3\\right]\\right]\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Using a variance decomposition once again, we get ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widehat{T}_{2}\\leq\\mathbb{E}\\left[\\frac{1}{G_{C}^{k}}\\sum_{i\\in\\mathcal{E}_{c}^{k}}80(1+\\omega)\\mathbb{E}_{k}\\left[\\left\\|\\widehat{\\Delta}_{i}\\left(x^{k+1},x^{k}\\right)-\\Delta_{i}\\left(x^{k+1},x^{k}\\right)\\right\\|^{2}\\mid\\left[3\\right]\\right]\\right]}\\\\ &{\\quad+\\mathbb{E}\\left[\\frac{1}{G_{C}^{k}}\\sum_{i\\in\\mathcal{E}_{c}^{k}}80\\mathbb{E}_{k}\\left[\\left\\|\\Delta_{i}\\left(x^{k+1},x^{k}\\right)-\\Delta\\left(x^{k+1},x^{k}\\right)\\right\\|^{2}\\mid3\\right]\\right]}\\\\ &{\\quad+\\mathbb{E}\\left[\\frac{1}{G_{C}^{k}}\\sum_{i\\in\\mathcal{E}_{c}^{k}}8\\mathbb{E}_{k}\\left[\\left\\|\\Delta_{i}\\left(x^{k+1},x^{k}\\right)-\\Delta\\left(x^{k+1},x^{k}\\right)\\right\\|^{2}\\mid\\left[3\\right]\\right]\\right]}\\\\ &{\\quad+\\mathbb{E}\\left[\\frac{1}{G_{C}^{k}}\\sum_{i\\in\\mathcal{E}_{c}^{k}}80\\mathbb{E}_{k}\\left[\\left\\|\\Delta\\left(x^{k+1},x^{k}\\right)-\\Delta\\left(x^{k+1},x^{k}\\right)\\right\\|^{2}\\mid\\left[3\\right]\\right]\\right]}\\\\ &{\\quad+\\mathbb{E}\\left[\\frac{1}{G_{C}^{k}}\\sum_{i\\in\\mathcal{E}_{c}^{k}}80\\mathbb{E}_{k}\\left[\\left\\|\\Delta\\left(x^{k+1},x^{k}\\right)\\right\\|^{2}\\mid\\left[3\\right]\\right]\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Using a similar argument to the one used in the previous lemma, we obtain ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widehat{T}_{2}\\leq\\mathbb{E}\\left[\\frac{\\mathcal{P}_{\\mathcal{G}_{c}^{k}}}{(1-\\delta)C}\\sum_{i\\in\\mathcal{G}}80(1+\\omega)\\mathbb{E}_{k}\\left[\\left\\|\\widehat{\\Delta}_{i}\\left(x^{k+1},x^{k}\\right)-\\Delta_{i}\\left(x^{k+1},x^{k}\\right)\\right\\|^{2}\\mid\\left[3\\right]\\right]\\right]}\\\\ &{\\quad+\\mathbb{E}\\left[\\frac{\\mathcal{P}_{\\mathcal{G}_{c}^{k}}}{(1-\\delta)C}\\sum_{i\\in\\mathcal{G}}80\\omega\\mathbb{E}_{k}\\left[\\left\\|\\Delta_{i}\\left(x^{k+1},x^{k}\\right)-\\Delta\\left(x^{k+1},x^{k}\\right)\\right\\|^{2}\\mid\\left[3\\right]\\right]\\right]}\\\\ &{\\quad+\\mathbb{E}\\left[\\frac{\\mathcal{P}_{\\mathcal{G}_{c}^{k}}}{(1-\\delta)C}\\sum_{i\\in\\mathcal{G}}8\\mathbb{E}_{k}\\left[\\left\\|\\Delta_{i}\\left(x^{k+1},x^{k}\\right)-\\Delta\\left(x^{k+1},x^{k}\\right)\\right\\|^{2}\\mid\\left[3\\right]\\right]\\right]}\\\\ &{\\quad+\\mathbb{E}\\left[\\frac{\\mathcal{P}_{\\mathcal{G}_{c}^{k}}}{(1-\\delta)C}\\sum_{i\\in\\mathcal{G}}80\\mathbb{E}_{k}\\left[\\left\\|\\Delta\\left(x^{k+1},x^{k}\\right)\\right\\|^{2}\\mid\\left[3\\right]\\right]\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Using Assumptions D.1, D.2, D.3: ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widehat{T}_{2}\\leq\\mathbb{E}\\left[\\frac{80(1+\\omega)G\\mathcal{P}_{\\mathcal{G}_{c}^{k}}\\mathcal{L}_{\\pm}^{2}}{(1-\\delta)C b}\\|x^{k+1}-x^{k}\\|^{2}\\right]+\\mathbb{E}\\left[\\frac{8(10\\omega+1)G\\mathcal{P}_{\\mathcal{G}_{c}^{k}}L_{\\pm}^{2}}{(1-\\delta)C}\\|x^{k+1}-x^{k}\\|^{2}\\right]}\\\\ &{\\quad+\\mathbb{E}\\left[\\frac{80G\\mathcal{P}_{\\mathcal{G}_{c}^{k}}\\omega L^{2}}{(1-\\delta)C}\\|x^{k+1}-x^{k}\\|^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Finally, we obtain ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{T_{2}=\\mathbb{E}\\left[\\mathbb{E}_{k}\\left[\\left\\|\\frac{1}{G_{C}^{k}}\\sum_{i\\in\\mathcal{G}_{C}^{k}}\\mathsf{c l i p}_{\\lambda}\\left(\\mathcal{Q}\\left(\\widehat{\\Delta}_{i}\\left(x^{k+1},x^{k}\\right)\\right)\\right)-\\mathsf{A R A g g}_{Q}^{k+1}\\right\\|^{2}\\mid\\left[3\\right]\\right]\\right]}\\\\ &{\\quad\\leq\\frac{8G\\mathcal{P}_{\\mathcal{G}_{C}^{k}}}{(1-\\delta)C}\\left(10(1+\\omega)\\frac{\\mathcal{L}_{\\pm}^{2}}{b}+(10\\omega+1)L_{\\pm}^{2}+10\\omega L^{2}\\right)c\\delta\\mathbb{E}\\left[\\|x^{k+1}-x^{k}\\|^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Lemma D.12. Let Assumptions 2.3 and $D.I$ hold. Also let us introduce the notation ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r}{A R A g g_{Q}^{k+1}=A R A g g\\left(c\\imath\\,i p_{\\lambda_{k+1}}\\left(\\mathcal{Q}\\left(\\widehat{\\Delta}_{1}(x^{k+1},x^{k})\\right)\\right),\\ldots,c\\imath\\,i p_{\\lambda_{k+1}}\\left(\\mathcal{Q}\\left(\\widehat{\\Delta}_{C}(x^{k+1},x^{k})\\right)\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Assume that $\\lambda_{k+1}=\\alpha_{\\lambda_{k+1}}\\|x^{k+1}-x^{k}\\|$ . Then for all $k\\,\\geq\\,0$ the iterates produced by Byz-VRMARINA-PP (Algorithm $^{\\,l}$ )satisfy ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{T_{3}=\\mathbb{E}\\left[\\mathbb{E}_{k}\\left[\\left\\Vert\\nabla f(x^{k+1})-\\nabla f(x^{k})-A R A g g_{Q}^{k+1}\\right\\Vert^{2}\\mid[2]\\right]\\right]}\\\\ &{\\quad\\leq2(L^{2}+F_{A}^{2}\\alpha_{\\lambda_{k+1}}^{2})\\mathbb{E}\\left[\\left\\Vert x^{k+1}-x^{k}\\right\\Vert^{2}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Proof. ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{T_{3}=\\mathbb{E}\\left[\\mathbb{E}_{k}\\left[\\left\\Vert\\nabla f(x^{k+1})-\\nabla f(x^{k})-\\mathtt{A R A g g}_{Q}^{k+1}\\right\\Vert^{2}\\mid[2]\\right]\\right]}\\\\ &{\\quad\\overset{(12)}{\\leq}\\mathbb{E}\\left[\\mathbb{E}_{k}\\left[2\\left\\Vert\\nabla f(x^{k+1})-\\nabla f(x^{k})\\right\\Vert^{2}+2\\left\\Vert\\mathtt{A R A g g}_{Q}^{k+1}\\right\\Vert^{2}\\mid[2]\\right]\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Using $L$ -smoothness and Assumption 2.3 we have ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{T_{3}\\overset{(12)}{\\leq}\\mathbb{E}\\left[\\mathbb{E}_{k}\\left[2L^{2}\\left\\|x^{k+1}-x^{k}\\right\\|^{2}+2F_{A}^{2}\\lambda_{k+1}^{2}\\mid[2]\\right]\\right]}\\\\ &{\\quad\\leq\\mathbb{E}\\left[\\mathbb{E}_{k}\\left[2L^{2}\\left\\|x^{k+1}-x^{k}\\right\\|^{2}+2F_{A}^{2}\\alpha_{\\lambda_{k+1}}^{2}\\|x^{k+1}-x^{k}\\|^{2}\\mid[2]\\right]\\right]}\\\\ &{\\quad\\leq2(L^{2}+F_{A}^{2}\\alpha_{\\lambda_{k+1}}^{2})\\mathbb{E}\\left[\\left\\|x^{k+1}-x^{k}\\right\\|^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Lemma D.13. Let Assumptions 2.3, D.1, D.2, D.3, D.5 hold and Compression Operator satisfy Definition 2.2.Also let us introduce thenotation ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r}{A R A g g_{Q}^{k+1}=A R A g g\\left(c l\\,i p_{\\lambda_{k+1}}\\left(\\mathcal{Q}\\left(\\widehat{\\Delta}_{1}(x^{k+1},x^{k})\\right)\\right),\\ldots,c l\\,i p_{\\lambda_{k+1}}\\left(\\mathcal{Q}\\left(\\widehat{\\Delta}_{C}(x^{k+1},x^{k})\\right)\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Thenfor all $k\\geq0$ the iterates produced by Byz-VR-MARINA-PP (Algorithm 1) satisfy ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\mathbb{E}\\left[\\left\\|g^{k+1}-\\nabla f\\left(x^{k+1}\\right)\\right\\|^{2}\\right]\\leq\\left(1-\\displaystyle\\frac{p}{4}\\right)\\mathbb{E}\\left[\\left\\|g^{k}-\\nabla f\\left(x^{k}\\right)\\right\\|^{2}\\right]}\\\\ {\\quad\\displaystyle+\\,\\widehat{B}\\mathbb{E}\\left[\\left\\|\\nabla f\\left(x^{k}\\right)\\right\\|^{2}\\right]+\\widehat{D}\\zeta^{2}+\\frac{p A}{4}\\|x^{k+1}-x^{k}\\|^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{A=\\displaystyle\\frac{4}{p}\\left(\\frac{80}{p}\\frac{p_{G}\\mathcal{P}_{\\mathcal{G}_{C}^{k}}n}{C}\\omega+24\\frac{G\\mathcal{P}_{\\mathcal{G}_{C}^{k}}c\\delta}{(1-\\delta)\\tilde{C}}B+6\\tilde{B}+\\frac{4}{p}(1-p_{G})+\\frac{160}{p}p_{G}\\frac{G\\mathcal{P}_{\\mathcal{G}_{C}^{k}}}{(1-\\delta)C}c\\delta\\omega\\right)L^{2}}\\\\ &{\\quad+\\displaystyle\\frac{4}{p}\\left(\\frac{8p_{G}\\mathcal{P}_{\\mathcal{G}_{C}^{k}}n}{C}\\left(10\\omega+1\\right)+\\frac{16}{p}p_{G}\\frac{G\\mathcal{P}_{\\mathcal{G}_{C}^{k}}}{(1-\\delta)C}c\\delta(10\\omega+1)\\right)L_{\\pm}^{2}}\\\\ &{\\quad+\\displaystyle\\frac{4}{p}\\left(\\frac{160}{p}p_{G}\\frac{G\\mathcal{P}_{\\mathcal{G}_{C}^{k}}}{(1-\\delta)C}(1+\\omega)c\\delta+\\frac{80}{p}p_{G}\\mathcal{P}_{\\mathcal{G}_{C}^{k}}(1+\\omega)\\frac{n}{C}\\right)\\frac{\\mathcal{L}_{\\pm}^{2}}{b}}\\\\ &{\\quad+\\displaystyle\\frac{4}{p}\\left(\\frac{4}{p}(1-p_{G})F_{\\mathcal{A}}^{2}\\alpha_{\\lambda_{k+1}}^{2}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "equation", "text": "$$\n\\widehat{B}=2\\frac{\\delta\\mathcal{P}_{\\mathcal{G}_{C}^{k}}}{1-\\delta}B\\left(\\frac{12c G}{\\widehat{C}}+p\\right)+6\\widetilde{B},\\quad\\widehat{D}=2\\frac{\\delta\\mathcal{P}_{\\mathcal{G}_{C}^{k}}}{1-\\delta}\\left(\\frac{6c G}{\\widehat{C}}+p\\right)+\\widetilde{D},\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "andwhere $\\widetilde{B}:=0$ and $\\tilde{D}:=0$ when ${\\widehat{C}}=n,$ and $\\begin{array}{r}{\\widetilde{B}:=\\frac{\\mathcal{P}_{\\mathcal{G}_{\\widehat{C}}^{k}}G B}{(1-\\delta)\\widehat{C}}}\\end{array}$ and $\\begin{array}{r}{\\widetilde{D}:=\\frac{\\mathcal{P}_{\\mathcal{G}_{\\widehat{C}}^{k}}G}{(1-\\delta)\\widehat{C}}}\\end{array}$ when ${\\widehat{C}}<n$ $p_{G}=\\mathrm{Prob}\\left\\{G_{C}^{k}\\geq(1-\\delta)C\\right\\}$ and $\\mathcal{P}_{\\mathcal{G}_{C}^{k}}=\\operatorname{Prob}\\left\\{i\\in\\mathcal{G}_{C}^{k}\\mid G_{C}^{k}\\geq\\left(1-\\delta\\right)C\\right\\}$ ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\Lambda_{0}=\\mathbb{E}\\left[\\left|\\hat{y}^{k+1}-\\nabla f\\left(x^{k+1}\\right)\\right|^{2}\\right]}\\\\ {\\ }&{\\le\\left(1+\\frac{p}{2}\\right)\\mathbb{E}\\left[\\left|\\hat{y}^{k+1}-\\nabla f\\left(x^{k+1}\\right)\\right|\\right]^{2}+\\left(1+\\frac{2}{p}\\right)\\mathbb{E}\\left[\\left|g^{k+1}-\\hat{y}^{k+1}\\right|^{2}\\right]}\\\\ {\\ }&{\\le\\left(1+\\frac{p}{2}\\right)\\lambda_{1}+\\left(1+\\frac{2}{p}\\right)\\lambda_{2}}\\\\ {\\ }&{\\le\\left(1+\\frac{p}{2}\\right)(1-p)\\left(1+\\frac{p}{2}\\right)\\mathbb{E}\\left[\\left|g^{k}-\\nabla f\\left(x^{k}\\right)\\right|^{2}\\right]}\\\\ {\\ }&{+\\left(1+\\frac{4}{p}\\right)(1-p)\\left(1+\\frac{4}{p}\\right)\\frac{\\sqrt{p}-p_{0}x}{p}\\left(\\ln\\left|g^{k}+(\\ln\\left|n+1)\\right|^{2}+\\frac{(\\ln(\\alpha+1)\\lambda_{3}^{2})}{\\Lambda}\\right)\\mathbb{E}\\left[\\left|x^{k+1}-x^{k}\\right|^{2}\\right.}\\\\ {\\ }&{\\left.+\\left(1+\\frac{p}{2}\\right)p\\left(\\frac{\\delta}{(1-\\delta)}\\mathbb{E}\\left[\\left|\\hat{y}^{k}(\\ell)\\right|^{2}+\\zeta^{2}\\right]\\right)}\\\\ {\\ }&{+\\left(1+\\frac{2}{p}\\right)p\\left(\\mathbb{E}\\left[\\left|\\hat{y}^{k+1}-\\nabla f\\left(x^{k+1}\\right),\\ldots,\\nabla f_{0}(x^{k+1})\\right|-\\nabla f(x^{k+1})\\right]^{2}\\right)\\left|1\\right|\\right]}\\\\ {\\ }&{+\\left(1+\\frac{2}{p}\\right)(1-p)\\log\\left[\\mathbb{E}\\left[\\left|\\frac{1}{\\sqrt{\\delta}}\\right|\\right]\\frac{1}{\\sqrt{\\delta}}\\mathrm{Eq}_{0}\\left(\\mathbb{E}\\left(\\left(\\frac{\\delta}{\\lambda}\\left(\\left(\\hat{y}^{k+1},x^{k}\\right)\\right)\\right)\\right)-\\mathrm{H}\\left(\\Delta\\xi_{0}^{k+1}\\right)^{2}\\right|^{2}\\right.\\right]}\\\\ {\\ }&{\\qquad\\left.+\\left(1+\\frac{2}{p}\\right)(1-p)(1-\n$$", "text_format": "latex", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{A_{0}\\stackrel{(1)}{\\leq}\\left(1-\\frac{p}{4}\\right)\\mathbb{E}\\left[\\|y^{k}-\\nabla f\\left(x^{k}\\right)\\|^{2}\\right]}}\\\\ &{+\\frac{8\\,P_{G_{k}^{k}}}{p}\\frac{p_{G_{k}^{k}}}{C}p_{G}\\left(10\\omega L^{2}+(10\\omega+1)L_{\\pm}^{2}+\\frac{10(\\omega+1)\\,C_{\\pm}^{2}}{b}\\right)\\mathbb{E}\\left[\\|x^{k+1}-x^{k}\\|^{2}\\right]}\\\\ &{+\\,2p\\left(\\frac{\\delta\\cdot\\mathcal{P}_{G_{k}^{k}}}{1-\\delta}\\mathbb{E}\\big[B\\|\\nabla f(x)\\|^{2}+\\zeta^{2}\\big]\\right)}\\\\ &{+\\left(p+2\\right)\\mathbb{E}\\bigg[\\mathbb{E}_{k}\\left[\\big\\|\\mathfrak{A}\\mathtt{M}g\\left(\\nabla f_{1}(x^{k+1}),\\dots,\\nabla f_{n}(x^{k+1})\\right)-\\nabla f(x^{k+1})\\big\\|^{2}\\right]\\,|[1]\\bigg]}\\\\ &{+\\,\\frac{2}{p}p\\varepsilon\\mathbb{E}\\left[\\mathbb{E}_{k}\\left[\\left\\|\\frac{1}{C_{\\mathcal{k}}^{k}\\,\\in\\mathbb{E}}\\mathrm{cirip}_{\\lambda}\\left(Q\\left(\\widehat{\\Delta}_{i}\\left(x^{k+1},x^{k}\\right)\\right)\\right)-\\mathfrak{A}\\mathtt{M}g_{Q_{q}^{k+1}}^{k+1}\\right\\|^{2}\\right]\\,|[3]\\right]}\\\\ &{+\\,\\frac{2}{p}(1-p\\varepsilon)\\mathbb{E}\\bigg[\\mathbb{E}_{k}\\left[\\left\\|\\nabla f(x^{k+1})-\\nabla f(x^{k})-\\mathrm{A}\\mathtt{M}g_{Q_{q}^{k+1}}^{k+1}\\right\\|^{2}\\,|\\Omega\\right]\\bigg]}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Now, we can apply Lemmas D.10, D.11, D.12: ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\Delta_{t}=}&{\\left[\\left\\{\\frac{\\partial}{\\partial\\tau}\\left(x^{4}-1\\right)^{2}\\right\\}^{2}\\left[\\left(x^{4}+1\\right)^{2}\\right]^{2}\\right]}\\\\ &{\\le\\left(1-\\frac{\\sigma}{2}\\right)\\left[\\left\\{1-\\frac{\\partial}{\\partial\\tau}\\left(x^{6}-1\\right)^{2}\\right\\}\\right]}\\\\ &{+\\frac{8\\sigma^{2}\\sigma^{2}\\sigma^{2}}{9}\\Bigg[\\left(10\\sigma^{2}\\left(1+(10\\sigma+1)L_{2}^{2}+\\frac{10\\sigma\\left(x^{4}-1\\right)Z_{0}^{2}}{8}\\right)\\mathbb{E}\\left[\\left\\{x^{4}-x^{2}-x^{2}\\right\\}^{2}\\right]\\right.}\\\\ &{\\quad\\left.+(\\mu+2)\\left(\\frac{8\\sigma^{2}\\sigma^{2}\\sigma^{4}}{1-\\delta_{0}\\left(x^{2}-1\\right)Z_{0}}+\\frac{21}{\\delta_{0}}\\right)\\mathbb{E}\\left[\\left\\{\\left\\{x^{2}(x^{4})\\right\\}\\right\\}^{2}\\left\\{x^{2}^{4}-x^{2}\\right\\}^{2}\\right]\\right.}\\\\ &{\\quad+\\left.4(\\mu+2)\\frac{\\partial^{2}\\sigma^{2}\\partial\\tau\\partial\\tau^{2}}{\\partial\\tau\\partial\\tau}+(\\mu+2)\\zeta^{2}\\sigma^{2}}\\\\ &{\\quad+\\frac{2}{2}\\rho\\rho\\rho\\nabla\\left\\{\\left[1-\\frac{\\sigma}{2}\\frac{\\partial}{\\partial\\tau}\\left(x^{4}-1\\right)Z_{0}^{2}\\right]\\sigma^{4}\\lambda_{1}^{4}}-x^{4}\\frac{1}{\\mu}\\right\\}}\\\\ &{\\quad+\\frac{2}{\\rho}\\rho\\nabla^{2}\\left[\\left\\{10\\sigma^{2}\\left(1-\\frac{\\sigma}{2}\\right)\\left(x^{2}-1\\right)Z_{0}^{2}\\right\\}\\sigma^{4}\\lambda_{1}^{4}+\\lambda_{0}\\right]}\\\\ &{\\quad+\\frac{2}{\\rho}\\rho\\nabla^{2}\\left[\\left\\{10\\sigma^{2}\\left(1-\\frac{\\sigma}{2}\\right)\\left(x^{4}-1\\right)Z_{0}^{2}\\right\\}\\sigma^{4}\\right]}\\\\ &{\\quad+\\frac{2}{\\rho}\\rho\\nabla^{2}\\left[\\left(1-\\frac{\\sigma}{2}\\right)^{2} \n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Finally, we have ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\|g^{k+1}-\\nabla f\\left(x^{k+1}\\right)\\right\\|^{2}\\right]\\leq\\left(1-\\frac{p}{4}\\right)\\mathbb{E}\\left[\\left\\|g^{k}-\\nabla f\\left(x^{k}\\right)\\right\\|^{2}\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+\\,\\hat{B}\\mathbb{E}\\left[\\left\\|\\nabla f\\left(x^{k}\\right)\\right\\|^{2}\\right]+\\hat{D}\\zeta^{2}+\\frac{p A}{4}\\mathbb{E}\\left[\\|x^{k+1}-x^{k}\\|^{2}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "where ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{A=\\frac{32p_{G}}{p^{2}}\\frac{\\mathcal{P}_{\\mathcal{G}_{C}^{k}}n}{C}\\left(10\\omega L^{2}+(10\\omega+1)L_{\\pm}^{2}+\\frac{10(\\omega+1)\\mathcal{L}_{\\pm}^{2}}{b}\\right)}\\\\ &{\\quad+\\,\\frac{8}{p^{2}}\\frac{G\\mathcal{P}_{\\mathcal{G}_{C}^{k}}}{(1-\\delta)C}p_{G}c\\delta\\left(80(1+\\omega)\\frac{\\mathcal{L}_{\\pm}^{2}}{b}+8(10\\omega+1)L_{\\pm}^{2}+80\\omega L^{2}\\right)}\\\\ &{\\quad+\\,\\frac{4}{p}\\left(\\frac{24G\\mathcal{P}_{\\mathcal{G}_{C}^{k}}c\\delta B}{(1-\\delta)\\widehat{C}}+6\\tilde{B}\\right)L^{2}+\\frac{16(1-p_{G})}{p^{2}}(L^{2}+F_{A}^{2}\\alpha_{\\lambda_{k+1}}^{2}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "and ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\widehat{B}=2\\frac{\\delta\\mathcal{P}_{\\mathcal{G}_{C}^{k}}}{1-\\delta}B\\left(\\frac{12c G}{\\widehat{C}}+p\\right)+6\\widetilde{B},\\quad\\widehat{D}=2\\frac{\\delta\\mathcal{P}_{\\mathcal{G}_{C}^{k}}}{1-\\delta}\\left(\\frac{6c G}{\\widehat{C}}+p\\right)+\\widetilde{D},\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "where $\\tilde{D}:=0$ When ${\\widehat{C}}=n$ and $\\begin{array}{r}{\\widetilde{D}:=\\frac{\\mathcal{P}_{\\mathcal{G}_{\\widehat{C}}^{k}}G}{(1-\\delta)\\widehat{C}}}\\end{array}$ when ${\\widehat{C}}<n$ .Once we simplify the equation, we obtain ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{A=\\displaystyle\\frac{4}{p}\\left(\\frac{80}{p}\\frac{p_{G}\\mathcal{P}_{\\mathcal{C}_{c}^{k}}n}{C}\\omega+24\\frac{G\\mathcal{P}_{\\mathcal{G}_{c}^{k}}c\\delta}{(1-\\delta)\\tilde{C}}B+6\\tilde{B}+\\frac{4}{p}(1-p_{G})+\\frac{160}{p}p_{G}\\frac{G\\mathcal{P}_{\\mathcal{G}_{c}^{k}}}{(1-\\delta)C}c\\delta\\omega\\right)L^{2}}\\\\ &{\\quad+\\displaystyle\\frac{4}{p}\\left(\\frac{8p_{G}\\mathcal{P}_{\\mathcal{G}_{c}^{k}}n}{C}\\left(10\\omega+1\\right)+\\frac{16}{p}p_{G}\\frac{G\\mathcal{P}_{\\mathcal{G}_{c}^{k}}}{(1-\\delta)C}c\\delta(10\\omega+1)\\right)L_{\\pm}^{2}}\\\\ &{\\quad+\\displaystyle\\frac{4}{p}\\left(\\frac{160}{p}p_{G}\\frac{G\\mathcal{P}_{\\mathcal{G}_{c}^{k}}}{(1-\\delta)C}(1+\\omega)c\\delta+\\frac{80}{p}p_{G}\\mathcal{P}_{\\mathcal{G}_{c}^{k}}(1+\\omega)\\frac{n}{C}\\right)\\frac{\\mathcal{L}_{\\pm}^{2}}{b}}\\\\ &{\\quad+\\displaystyle\\frac{4}{p}\\left(\\frac{4}{p}(1-p_{G})F_{\\mathcal{A}}^{2}\\alpha_{\\lambda_{k+1}}^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "D.3 Main Results ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Theorem D.14. Let Assumptions 2.3, D.1, D.2, D.3, D.5 hold. Setting $\\begin{array}{r l}{\\lambda_{k+1}}&{{}=}\\end{array}$ $2\\operatorname*{max}_{i\\in{\\mathcal{G}}}L_{i}\\left\\|x^{k+1}-x^{k}\\right\\|$ Assumethat ", "page_idx": 41}, {"type": "equation", "text": "$$\n0<\\gamma\\leq\\frac{1}{L+\\sqrt{A}},\\quad4\\widehat{B}<p,\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "where ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{A=\\displaystyle\\frac{4}{p}\\left(\\frac{80}{p}\\frac{p_{G}\\mathcal{P}_{\\mathcal{G}_{C}^{k}}n}{C}\\omega+24\\frac{G\\mathcal{P}_{\\mathcal{G}_{C}^{k}}c\\delta}{(1-\\delta)\\tilde{C}}B+6\\tilde{B}+\\frac{4}{p}(1-p_{G})+\\frac{160}{p}p_{G}\\frac{G\\mathcal{P}_{\\mathcal{G}_{C}^{k}}}{(1-\\delta)C}c\\delta\\omega\\right)L^{2}}\\\\ &{\\quad+\\displaystyle\\frac{4}{p}\\left(\\frac{8p_{G}\\mathcal{P}_{\\mathcal{G}_{C}^{k}}n}{C}\\left(10\\omega+1\\right)+\\frac{16}{p}p_{G}\\frac{G\\mathcal{P}_{\\mathcal{G}_{C}^{k}}}{(1-\\delta)C}c\\delta(10\\omega+1)\\right)L_{\\pm}^{2}}\\\\ &{\\quad+\\displaystyle\\frac{4}{p}\\left(\\frac{160}{p}p_{G}\\frac{G\\mathcal{P}_{\\mathcal{G}_{C}^{k}}}{(1-\\delta)C}(1+\\omega)c\\delta+\\frac{80}{p}p_{G}\\mathcal{P}_{\\mathcal{G}_{C}^{k}}(1+\\omega)\\frac{n}{C}\\right)\\frac{\\mathcal{L}_{\\pm}^{2}}{b}}\\\\ &{\\quad+\\displaystyle\\frac{4}{p}\\left(\\frac{4}{p}(1-p_{G})F_{\\mathcal{A}}^{2}\\alpha_{\\lambda_{k+1}}^{2}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "equation", "text": "$$\n\\widehat{B}=2\\frac{\\delta\\mathcal{P}_{\\mathcal{G}_{C}^{k}}}{1-\\delta}B\\left(\\frac{12c G}{\\widehat{C}}+p\\right)+6\\widetilde{B},\\quad\\widehat{D}=2\\frac{\\delta\\mathcal{P}_{\\mathcal{G}_{C}^{k}}}{1-\\delta}\\left(\\frac{6c G}{\\widehat{C}}+p\\right)+\\widetilde{D},\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "and where $\\widetilde{B}:=0$ and $\\tilde{D}:=0$ when ${\\widehat{C}}=n$ and $\\begin{array}{r}{\\widetilde{B}:=\\frac{\\mathcal{P}_{\\mathcal{G}_{\\widehat{C}}^{k}}G B}{(1-\\delta)\\widehat{C}}}\\end{array}$ and $\\begin{array}{r}{\\widetilde{D}:=\\frac{\\mathcal{P}_{\\mathcal{G}_{\\widehat{C}}^{k}}G}{(1-\\delta)\\widehat{C}}}\\end{array}$ when ${\\widehat{C}}<n$ and ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{P}_{\\mathcal{G}_{C}^{k}}=\\displaystyle\\frac{C}{n p_{G}}\\cdot\\sum_{\\scriptstyle(1-\\delta)C\\leq t\\leq C}\\left(\\left(\\begin{array}{l}{G-1}\\\\ {t-1}\\end{array}\\right)\\left(\\begin{array}{l}{n-G}\\\\ {C-t}\\end{array}\\right)\\left(\\left(\\begin{array}{l}{n}\\\\ {C}\\end{array}\\right)\\right)^{-1}\\right),}\\\\ &{\\quad p_{G}=\\mathrm{Prob}\\left\\{G_{C}^{k}\\geq\\left(1-\\delta\\right)C\\right\\}}\\\\ &{\\quad\\quad=\\displaystyle\\sum_{\\lceil(1-\\delta)C\\rceil\\leq t\\leq C}\\left(\\left(\\begin{array}{l}{G}\\\\ {t}\\end{array}\\right)\\left(\\begin{array}{l}{n-G}\\\\ {C-t}\\end{array}\\right)\\left(\\begin{array}{l}{n-1}\\\\ {C-1}\\end{array}\\right)^{-1}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Thenfor all $K\\geq0$ the iterates produced by Byz-VR-MARINA (Algorithm 1) satisfy ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\Vert\\nabla f\\left(\\widehat{x}^{K}\\right)\\right\\Vert^{2}\\right]\\leq\\frac{2\\Phi^{0}}{\\gamma\\left(1-\\frac{4\\widehat{B}}{p}\\right)\\left(K+1\\right)}+\\frac{4\\widehat{D}\\zeta^{2}}{p-4\\widehat{B}},\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "where $\\widehat{x}^{K}$ is chosen uniformly at random from $x^{0},x^{1},\\ldots,x^{K}$ \uff0cand $\\Phi^{0}\\ =\\ f\\left(x^{0}\\right)\\;-\\;f^{*}\\;+$ $\\begin{array}{r}{\\frac{2\\gamma}{p}\\left\\|g^{0}-\\nabla f\\left(x^{0}\\right)\\right\\|^{2}}\\end{array}$ ", "page_idx": 41}, {"type": "text", "text": "Proof of Theorem $D.I4$ For all $k\\,\\geq\\,0$ we introduce $\\begin{array}{r}{\\Phi^{k}\\,=\\,f\\left(x^{k}\\right)\\,-\\,f^{*}\\,+\\,\\frac{2\\gamma}{p}\\,\\big\\|g^{k}\\,-\\nabla f\\left(x^{k}\\right)\\big\\|^{2}.}\\end{array}$ Using the results of Lemmas D.13 and D.7, we derive ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}\\left\\{\\left\\{\\Phi^{k+1}\\Bigg\\}^{(k)}\\Bigg|\\mathcal{S}_{j}^{k}\\in\\mathcal{F}\\left(\\left\\{P^{k}\\right\\}\\right\\}^{2}-\\sqrt{-\\left(\\frac{k}{2}\\right)}\\,\\|\\,x^{k+1}-x^{k}\\|^{2}\\right)+\\frac{2}{3}\\left\\{\\Phi^{k}-\\nabla f\\left(x^{k}\\right)\\right\\}^{2}\\Bigg|}&{}\\\\ &{\\quad-\\frac{2}{3}\\mathbb{E}\\left\\{\\left\\|\\nabla f(x^{k})\\right\\|^{2}\\right\\}+\\frac{2\\,\\mathcal{F}_{j}^{k}}{3}\\mathbb{E}\\left\\{\\left\\|\\Phi^{k+1}-\\nabla f(x^{k})\\right\\|^{2}\\right\\}}\\\\ &{\\quad(x_{k}^{2})\\mathbb{E}\\left\\{\\left[f^{k}\\right]-f^{k}\\left(\\frac{1}{3}\\right)\\Big\\}\\Bigg|\\,\\|\\Phi^{k+1}-x^{k}\\|^{2}+\\frac{2}{3}\\Bigg\\}\\Bigg|^{2}}\\\\ &{\\quad-\\frac{2}{3}\\mathbb{E}\\left\\{\\left\\|\\nabla f(x^{k})\\right\\|^{2}\\right\\}+\\frac{2\\,\\mathcal{F}_{j}^{k}}{3}\\left(1-\\frac{k}{3}\\right)\\mathbb{E}\\left\\{\\left\\|\\Phi^{k}-\\nabla f(x^{k})\\right\\|^{2}\\right\\}}\\\\ &{\\quad+\\frac{2}{3}\\Bigg\\{\\bar{\\Phi}^{k}\\left[\\Phi^{k}\\right]\\left[\\nabla f(x^{k})\\right]^{2}\\right\\}+\\bar{\\Phi}^{k}\\!\\Bigg\\}+\\frac{\\bar{\\Phi}^{k}}{3}\\Bigg\\{\\Phi^{k+1}-x^{k}\\Bigg\\}^{2}}\\\\ &{\\quad-\\frac{2}{3}\\mathbb{E}\\left\\{\\left\\langle\\Phi^{k}\\right\\|^{2}\\right\\}-f^{k}\\left(\\left\\{1-\\frac{k}{2}\\right\\}+\\frac{\\bar{\\Phi}^{k}}{6}\\right)\\mathbb{E}\\left\\{\\left\\|\\Phi^{k}-\\nabla f(x^{k})\\right\\|^{2}\\right\\}+\\frac{2\\bar{\\Phi}^{k}(\\frac{1}{3})}{\\bar{\\Phi}^{k}}\\Bigg\\}}\\\\ &{\\quad+\\frac{1}{32}\\left(1-f^{k}-\\lambda^{2}\\right)\\mathbb{E}\\left\\{\\left\\|\\Phi^{k+1}-x^{k}\\right\\|^{2}\\right\\}-\\frac{2}{3}\\left(1-\\frac{k}{ \n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Using choice ofstepsize andsecond conditon: $\\begin{array}{r}{0<\\gamma\\leq\\frac{1}{L+\\sqrt{A}},4\\widehat{B}<p}\\end{array}$ and lemma B.1 we have ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\Phi^{k+1}\\right]\\leq\\mathbb{E}\\left[\\Phi^{k}\\right]+\\frac{2\\widehat{D}\\zeta^{2}\\gamma}{p}-\\frac{\\gamma}{2}\\left(1-\\frac{4\\widehat{B}}{p}\\right)\\mathbb{E}\\left[\\left\\|\\nabla f\\left(x^{k}\\right)\\right\\|^{2}\\right]\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Next, we have $\\begin{array}{r}{\\frac{\\gamma}{2}\\left(1-\\frac{4\\widehat{B}}{p}\\right)>0}\\end{array}$ and $\\Phi^{k+1}\\geq0$ Therefore, summing up the above inequality for $k=0,1,\\ldots,K$ and rearranging the terms, we get ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\frac{1}{K+1}\\displaystyle\\sum_{k=0}^{K}\\mathbb{E}\\left[\\left\\|\\nabla f\\left(x^{k}\\right)\\right\\|^{2}\\right]\\leq\\displaystyle\\frac{2}{\\gamma\\left(1-\\frac{4\\hat{B}}{p}\\right)(K+1)}\\displaystyle\\sum_{k=0}^{K}\\left(\\mathbb{E}\\left[\\Phi^{k}\\right]-\\mathbb{E}\\left[\\Phi^{k+1}\\right]\\right)}&{}\\\\ {\\displaystyle+\\,\\frac{4\\hat{D}\\zeta^{2}}{p-4\\hat{B}}}&{}\\\\ {\\displaystyle=\\frac{2\\left(\\mathbb{E}\\left[\\Phi^{0}\\right]-\\mathbb{E}\\left[\\Phi^{k+1}\\right]\\right)}{\\gamma\\left(1-\\frac{4\\hat{B}}{p}\\right)(K+1)}+\\frac{4\\hat{D}\\zeta^{2}}{p-4\\hat{B}}}&{}\\\\ {\\displaystyle}&{~\\leq\\displaystyle\\frac{2\\mathbb{E}\\left[\\Phi^{0}\\right]}{\\gamma\\left(1-\\frac{4\\hat{B}}{p}\\right)(K+1)}+\\frac{4\\hat{D}\\zeta^{2}}{p-4\\hat{B}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Theorem D.15. Let Assumptions 2.3, D.1, D.2, D.3, D.5, 2.7 hold.  Set $\\begin{array}{r l}{\\lambda_{k+1}}&{{}=}\\end{array}$ $\\operatorname*{max}_{i\\in{\\mathcal{G}}}L_{i}\\left\\|x^{k+1}-x^{k}\\right\\|$ Assumethat ", "page_idx": 42}, {"type": "equation", "text": "$$\n0<\\gamma\\leq{\\frac{1}{L+{\\sqrt{2A}}}},\\quad8\\widehat B<p\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "where ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{A=\\displaystyle\\frac{4}{p}\\left(\\frac{80}{p}\\frac{p_{G}\\mathcal{P}_{\\mathcal{C}_{c}^{k}}n}{C}\\omega+24\\frac{G\\mathcal{P}_{\\mathcal{G}_{c}^{k}}c\\delta}{(1-\\delta)\\tilde{C}}B+6\\tilde{B}+\\frac{4}{p}(1-p_{G})+\\frac{160}{p}p_{G}\\frac{G\\mathcal{P}_{\\mathcal{G}_{c}^{k}}}{(1-\\delta)C}c\\delta\\omega\\right)L^{2}}\\\\ &{\\quad+\\displaystyle\\frac{4}{p}\\left(\\frac{8p_{G}\\mathcal{P}_{\\mathcal{G}_{c}^{k}}n}{C}\\left(10\\omega+1\\right)+\\frac{16}{p}p_{G}\\frac{G\\mathcal{P}_{\\mathcal{G}_{c}^{k}}}{(1-\\delta)C}c\\delta(10\\omega+1)\\right)L_{\\pm}^{2}}\\\\ &{\\quad+\\displaystyle\\frac{4}{p}\\left(\\frac{160}{p}p_{G}\\frac{G\\mathcal{P}_{\\mathcal{G}_{c}^{k}}}{(1-\\delta)C}(1+\\omega)c\\delta+\\frac{80}{p}p_{G}\\mathcal{P}_{\\mathcal{G}_{c}^{k}}(1+\\omega)\\frac{n}{C}\\right)\\frac{\\mathcal{L}_{\\pm}^{2}}{b}}\\\\ &{\\quad+\\displaystyle\\frac{4}{p}\\left(\\frac{4}{p}(1-p_{G})F_{\\mathcal{A}}^{2}\\alpha_{\\lambda_{k+1}}^{2}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "equation", "text": "$$\n\\widehat{B}=2\\frac{\\delta\\mathcal{P}_{\\mathcal{G}_{\\widehat{C}}^{k}}}{1-\\delta}B\\left(\\frac{12c G}{\\widehat{C}}+p\\right)+6\\widetilde{B},\\quad\\widehat{D}=2\\frac{\\delta\\mathcal{P}_{\\mathcal{G}_{\\widehat{C}}^{k}}}{1-\\delta}\\left(\\frac{6c G}{\\widehat{C}}+p\\right)+\\widetilde{D},\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "andwhere $\\widetilde{B}:=0$ and $\\tilde{D}:=0$ when ${\\widehat{C}}=n$ and $\\begin{array}{r}{\\widetilde{B}:=\\frac{\\mathcal{P}_{\\mathcal{G}_{\\widehat{C}}^{k}}G B}{(1-\\delta)\\widehat{C}}}\\end{array}$ and $\\begin{array}{r}{\\widetilde{D}:=\\frac{\\mathcal{P}_{\\mathcal{G}_{\\widehat{C}}^{k}}G}{(1-\\delta)\\widehat{C}}}\\end{array}$ when ${\\widehat{C}}<n$ and ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{P}_{\\mathcal{G}_{C}^{k}}=\\displaystyle\\frac{C}{n p_{G}}\\cdot\\sum_{\\scriptstyle(1-\\delta)C\\leq t\\leq C}\\left(\\left(\\begin{array}{l}{G-1}\\\\ {t-1}\\end{array}\\right)\\left(\\begin{array}{l}{n-G}\\\\ {C-t}\\end{array}\\right)\\left(\\left(\\begin{array}{l}{n}\\\\ {C}\\end{array}\\right)\\right)^{-1}\\right),}\\\\ &{\\quad p_{G}=\\mathrm{Prob}\\left\\{G_{C}^{k}\\geq\\left(1-\\delta\\right)C\\right\\}}\\\\ &{\\quad\\quad=\\displaystyle\\sum_{\\lceil(1-\\delta)C\\rceil\\leq t\\leq C}\\left(\\left(\\begin{array}{l}{G}\\\\ {t}\\end{array}\\right)\\left(\\begin{array}{l}{n-G}\\\\ {C-t}\\end{array}\\right)\\left(\\begin{array}{l}{n-1}\\\\ {C-1}\\end{array}\\right)^{-1}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Thenfor all $K\\geq0$ the iterates produced by Byz-VR-MARINA (Algorithm $^{\\,l}$ )satisfy ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[f\\left(x^{K}\\right)-f\\left(x^{*}\\right)\\right]\\leq\\left(1-\\rho\\right)^{K}\\Phi^{0}+\\frac{4\\widehat{D}\\gamma\\zeta^{2}}{p\\rho},\n$$", "text_format": "latex", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\rho=\\operatorname*{min}\\left[\\gamma\\mu\\left(1-\\frac{8\\widehat{B}}{p}\\right),\\frac{p}{8}\\right]a n d\\,\\Phi^{0}=f\\left(x^{0}\\right)-f^{*}+\\frac{4\\gamma}{p}\\left\\Vert g^{0}-\\nabla f\\left(x^{0}\\right)\\right\\Vert^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Proof. For all $k\\geq0$ we introduce $\\begin{array}{r}{\\Phi^{k}=f\\left(x^{k}\\right)-f^{*}+\\frac{4\\gamma}{p}\\left\\|g^{k}-\\nabla f\\left(x^{k}\\right)\\right\\|^{2}}\\end{array}$ Using the results of Lemmas D.13 and D.7, we derive ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}\\left[\\Vert\\hat{\\mathbf{y}}^{k+1}\\right]}&{\\Vert_{\\mathbf{\\mathcal{G}}}^{(D,\\hat{\\mathbf{z}})}\\mathbb{E}\\left[f\\left(x^{k}\\right)-f^{\\ast}-\\left(\\frac{1}{2\\gamma}-\\frac{L}{2}\\right)\\Vert x^{k+1}-x^{k}\\Vert^{2}+\\frac{\\gamma}{2}\\left\\Vert g^{k}-\\nabla f\\left(x^{k}\\right)\\right\\Vert^{2}\\right]}\\\\ &{-\\frac{\\gamma}{2}\\mathbb{E}\\left[\\Vert\\nabla f\\left(x^{k}\\right)\\Vert^{2}\\right]+\\frac{4\\gamma}{\\gamma}\\mathbb{E}\\left[\\Vert\\phi^{k+1}-\\nabla f\\left(x^{k+1}\\right)\\Vert^{2}\\right]}\\\\ &{\\overset{{(D,1)}}{\\leq}\\frac{\\gamma}{2}\\mathbb{E}\\left[f\\left(x^{k}\\right)-f^{\\ast}-\\left(\\frac{1}{2\\gamma}-\\frac{L}{2}\\right)\\left\\Vert x^{k+1}-x^{k}\\right\\Vert^{2}+\\frac{\\gamma}{2}\\left\\Vert g^{k}-\\nabla f\\left(x^{k}\\right)\\right\\Vert^{2}\\right]}\\\\ &{-\\frac{\\gamma}{2}\\mathbb{E}\\left[\\Vert\\nabla f\\left(x^{k}\\right)\\Vert^{2}\\right]+\\frac{4\\gamma}{\\rho}\\left(1-\\frac{p}{4}\\right)\\mathbb{E}\\left[\\Vert\\phi^{k}-\\nabla f\\left(x^{k}\\right)\\Vert^{2}\\right]}\\\\ &{+\\frac{4\\gamma}{p}\\left(\\hat{B}\\mathbb{E}\\left[\\Vert\\nabla f\\left(x^{k}\\right)\\Vert^{2}\\right]+\\hat{D}\\zeta^{2}+\\frac{p\\lambda}{4}\\Vert x^{k+1}-x^{k}\\Vert^{2}\\right)}\\\\ &{=\\mathbb{E}\\left[f\\left(x^{k}\\right)-f^{\\ast}\\right]+\\frac{4\\gamma}{p}\\left(\\left(1-\\frac{p}{4}\\right)+\\frac{p}{8}\\right)\\mathbb{E}\\left[\\Vert\\phi^{k}-\\nabla f\\left(x^{k}\\right)\\Vert^{2}\\right]+\\frac{4\\hat{D}\\zeta^{2}\\gamma}{p}}\\\\ &{+\\frac{1}{2\\gamma}\\left(1-L\\gamma-24\\gamma^{2}\\right)\\mathbb{E}\\left[\\Vert x^{k+1}-x^{k}\\Vert^{2}\\right]-\\frac{\\gamma}{2}\\left(1-\\frac{8\\hat{B}}{p}\\right)\\mathbb{E}\\left[\\Vert\\nabla f\\left(x^{k}\\right)\\Vert^{2}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Using Assumption 2.7 we obtain ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\Phi^{k+1}\\right]\\leq\\mathbb{E}\\left[f\\left(x^{k}\\right)-f^{*}\\right]+\\left(1-\\frac{p}{8}\\right)\\frac{4\\gamma}{p}\\mathbb{E}\\left[\\left\\Vert g^{k}-\\nabla f\\left(x^{k}\\right)\\right\\Vert^{2}\\right]+\\frac{4\\widehat{D}\\zeta^{2}\\gamma}{p}}\\\\ &{\\quad\\quad\\quad+\\displaystyle\\frac{1}{2\\gamma}\\left(1-L\\gamma-2A\\gamma^{2}\\right)\\mathbb{E}\\left[\\left\\Vert x^{k+1}-x^{k}\\right\\Vert^{2}\\right]}\\\\ &{\\quad\\quad\\quad-\\gamma\\mu\\left(1-\\frac{8\\widehat{B}}{p}\\right)\\mathbb{E}\\left[f\\left(x^{k}\\right)-f^{*}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Finally, we have ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\Phi^{k+1}\\right]\\leq\\left(1-\\operatorname*{min}\\left[\\gamma\\mu\\left(1-\\frac{\\widehat{B}}{p}\\right),\\frac{p}{8}\\right]\\right)\\mathbb{E}\\left[\\Phi^{k}\\right]+\\frac{4\\widehat{D}\\zeta^{2}\\gamma}{p}.\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Unrolling the recurrence with $\\begin{array}{r}{\\rho=\\operatorname*{min}\\left[\\gamma\\mu\\left(1-\\frac{8\\widehat{B}}{p}\\right),\\frac p{8}\\right]}\\end{array}$ , we obtain ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\Phi^{k}\\right]\\leq\\left(1-\\rho\\right)^{K}\\mathbb{E}\\left[\\Phi^{0}\\right]+\\frac{4\\widehat{D}\\zeta^{2}\\gamma}{p}\\displaystyle\\sum_{k=0}^{K-1}\\left(1-\\rho\\right)^{k}}\\\\ &{\\qquad\\quad\\leq\\left(1-\\rho\\right)^{K}\\mathbb{E}\\left[\\Phi^{0}\\right]+\\frac{4\\widehat{D}\\zeta^{2}\\gamma}{p}\\displaystyle\\sum_{k=0}^{\\infty}\\left(1-\\rho\\right)^{k}}\\\\ &{\\qquad\\quad=\\left(1-\\rho\\right)^{K}\\mathbb{E}\\left[\\Phi^{0}\\right]+\\frac{4\\widehat{D}\\gamma\\zeta^{2}}{p\\rho}}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Taking into account $\\Phi^{k}\\geq f\\left(x^{k}\\right)-f\\left(x^{*}\\right)$ , we get the result. ", "page_idx": 44}, {"type": "text", "text": "E  Analysis for Bounded Compressors ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "E.1  Technical Lemmas ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Lemma E.1. Let Assumptions D.1, D.2, D.3 and 2.4 hold and the Compression Operator satisfy Definition 2.2.We set $\\lambda_{k+1}=D_{Q}\\operatorname*{max}_{i,j}L_{i,j}$ Let us define \"ideal\" estimator: ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\overline{{g}}^{k+1}=\\left\\{\\begin{array}{l l}{\\frac{1}{G_{\\widehat{C}}^{k}}\\sum_{i\\in{\\mathcal{G}}_{\\widehat{C}}^{k}}\\nabla f_{i}(x^{k+1}),}&{c_{n}=1,}\\\\ {g^{k}+\\nabla f\\left(x^{k+1}\\right)-\\nabla f\\left(x^{k}\\right),}&{c_{n}=0\\,a n d\\,G_{C}^{k}<(1-\\delta)C,}\\\\ {g^{k}+\\frac{1}{G_{\\widehat{C}}^{k}}\\sum_{i\\in{\\mathcal{G}}_{{c}}^{k}}c_{l}\\,i p_{\\lambda}\\left(Q\\left(\\widehat{\\Delta}_{i}\\left(x^{k+1},x^{k}\\right)\\right)\\right),}&{c_{n}=0\\,a n d\\,G_{C}^{k}\\geq(1-\\delta)C.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Thenfor all $k\\geq0$ the iterates produced by Byz-VR-MARINA-PP (Algorithm 1) satisfy ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{A_{1}=\\mathbb{E}\\left[\\left\\Vert\\overline{{g}}^{k+1}-\\nabla f\\left(x^{k+1}\\right)\\right\\Vert^{2}\\right]}\\\\ &{\\quad\\leq(1-p)\\mathbb{E}\\left[\\left\\Vert g^{k}-\\nabla f(x^{k})\\right\\Vert^{2}\\right]+p\\frac{\\delta\\mathcal{P}_{\\mathcal{G}_{\\widehat{C}}^{k}}}{(1-\\delta)}\\mathbb{E}\\left[B\\Vert\\nabla f(x)\\Vert^{2}+\\zeta^{2}\\right]}\\\\ &{\\quad+\\left(1-p\\right)p_{G}\\frac{\\mathcal{P}_{\\mathcal{G}_{C}^{k}}G}{C^{2}(1-\\delta)^{2}}\\left(\\omega L^{2}+(\\omega+1)L_{\\pm}^{2}+\\frac{(\\omega+1)\\mathcal{L}_{\\pm}^{2}}{b}\\right)\\mathbb{E}\\left[\\left\\Vert x^{k+1}-x^{k}\\right\\Vert^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Where $p_{G}=\\mathrm{Prob}\\left\\{G_{C}^{k}\\geq(1-\\delta)C\\right\\}$ and $\\mathcal{P}_{\\mathcal{G}_{C}^{k}}=\\operatorname{Prob}\\big\\{i\\in\\mathcal{G}_{C}^{k}\\ |\\ G_{C}^{k}\\ge\\left(1-\\delta\\right)C\\big\\}.$ ", "page_idx": 45}, {"type": "text", "text": "Proof. Similarly to general analysis, we start from conditional expectations: ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{A_{1}=\\mathbb{E}\\left[\\left\\|\\overline{{g}}^{k+1}-\\nabla f\\left(x^{k+1}\\right)\\right\\|^{2}\\right]}\\\\ &{\\quad=\\mathbb{E}\\left[\\mathbb{E}_{k}\\left[\\left\\|\\overline{{g}}^{k+1}-\\nabla f\\left(x^{k+1}\\right)\\right\\|^{2}\\right]\\right]}\\\\ &{\\quad=(1-p)p_{G}\\mathbb{E}\\left[\\mathbb{E}_{k}\\left[\\left\\|g^{k}+\\frac{1}{G_{C}^{k}}\\sum_{i\\in\\mathcal{E}_{C}^{k}}\\mathbb{c}\\mathrm{i}\\mathbf{i}\\mathbf{p}_{\\lambda}\\left(\\mathcal{Q}\\left(\\widehat{\\Delta}_{i}\\left(x^{k+1},x^{k}\\right)\\right)\\right)-\\nabla f\\left(x^{k+1}\\right)\\right\\|^{2}\\right]\\mid\\mathbb{B}\\right]}\\\\ &{\\quad+\\left(1-p\\right)(1-p_{G})\\mathbb{E}\\left[\\mathbb{E}_{k}\\left[\\left\\|g^{k}-\\nabla f(x^{k})\\right\\|^{2}\\right]\\mid\\mathbb{I}\\right]}\\\\ &{\\quad\\quad+\\,p\\mathbb{E}\\left[\\left\\|\\frac{1}{G_{C}^{k}}\\sum_{i\\in\\mathcal{E}_{C}^{k}}\\nabla f_{i}(x^{k+1})-\\nabla f(x^{k+1})\\right\\|^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Using (12) and $\\nabla f\\left(x^{k}\\right)-\\nabla f\\left(x^{k}\\right)=0$ we obtain ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\!\\!\\!\\!\\!\\!B_{1}=\\mathbb{E}\\left[\\mathbb{E}_{k}\\left[\\left\\|g^{k}+\\frac{1}{G_{C}^{k}}\\sum_{i\\in\\mathcal{G}_{C}^{k}}\\mathsf{c l i p}_{\\lambda}\\left(\\mathcal{Q}\\left(\\widehat{\\Delta}_{i}\\left(x^{k+1},x^{k}\\right)\\right)\\right)-\\nabla f\\left(x^{k+1}\\right)\\right\\|^{2}\\right]\\mid\\left[3\\right]\\right]}\\\\ &{\\quad=\\mathbb{E}\\left[\\mathbb{E}_{k}\\left[\\left\\|g^{k}+\\frac{1}{G_{C}^{k}}\\sum_{i\\in\\mathcal{G}_{C}^{k}}\\mathsf{c l i p}_{\\lambda}\\left(\\mathcal{Q}\\left(\\widehat{\\Delta}_{i}\\left(x^{k+1},x^{k}\\right)\\right)\\right)-\\nabla f\\left(x^{k+1}\\right)+\\nabla f\\left(x^{k}\\right)-\\nabla f\\left(x^{k}\\right)\\right\\|^{2}\\right]\\mid\\left[3\\right]\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Using $\\lambda_{k+1}\\;=\\;D_{Q}\\operatorname*{max}_{i,j}L_{i,j}\\|x^{k+1}\\,-\\,x^{k}\\|$ we can guarantee that clipping operator becomes identical since we have ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left\\|\\mathcal{Q}\\left(\\widehat{\\Delta}_{i}\\left(x^{k+1},x^{k}\\right)\\right)\\right\\|\\leq D_{Q}\\left\\|\\widehat{\\Delta}_{i}\\left(x^{k+1},x^{k}\\right)\\right\\|}&{}\\\\ {\\leq D_{Q}\\left\\|\\frac{1}{b}\\displaystyle\\sum_{j\\in m}\\nabla f_{i,j}(x^{k+1})-\\nabla f_{i,j}(x^{k})\\right\\|}&{}\\\\ {\\leq D_{Q}\\displaystyle\\frac{1}{b}\\displaystyle\\sum_{j\\in m}\\left\\|\\nabla f_{i,j}(x^{k+1})-\\nabla f_{i,j}(x^{k})\\right\\|}&{}\\\\ {\\leq D_{Q}\\displaystyle\\operatorname*{max}_{j\\in\\mathcal{N}}L_{i,j}\\left\\|x^{k+1}-x^{k}\\right\\|}&{}\\\\ {\\leq D_{Q}\\displaystyle\\operatorname*{max}_{i,j}L_{i,j}\\left\\|x^{k+1}-x^{k}\\right\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Therefore, we can continue as follows ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\!\\!\\!\\!\\!\\!\\!B_{1}=\\mathbb{E}\\left[\\mathbb{E}_{k}\\left[\\left\\|g^{k}+\\frac{1}{G_{C}^{k}}\\sum_{i\\in\\mathcal{G}_{C}^{k}}Q\\left(\\widehat{\\Delta}_{i}\\left(x^{k+1},x^{k}\\right)\\right)-\\nabla f\\left(x^{k+1}\\right)\\right\\|^{2}\\right]\\mid\\left[3\\right]\\right]}\\\\ &{\\quad=\\mathbb{E}\\left[\\mathbb{E}_{k}\\left[\\left\\|g^{k}+\\frac{1}{G_{C}^{k}}\\sum_{i\\in\\mathcal{G}_{C}^{k}}Q\\left(\\widehat{\\Delta}_{i}\\left(x^{k+1},x^{k}\\right)\\right)-\\nabla f\\left(x^{k+1}\\right)+\\nabla f\\left(x^{k}\\right)-\\nabla f\\left(x^{k}\\right)\\right\\|^{2}\\right]\\mid\\left[3\\right]\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Moreover, we can avoid application of Young's inequality and use variance decomposition instead: ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{B_{1}\\leq\\mathbb{E}\\left[\\left\\|g^{k}-\\nabla f\\left(x^{k}\\right)\\right\\|^{2}\\right]}}\\\\ &{+\\;\\mathbb{E}\\left[\\mathbb{E}_{k}\\left[\\left\\|\\frac{1}{G_{C}^{k}}\\sum_{i\\in\\mathcal{C}_{C}^{k}}\\mathcal{G}\\left(\\widehat{\\Delta}_{i}\\left(x^{k+1},x^{k}\\right)\\right)-\\left(\\nabla f(x^{k+1})-\\nabla f(x^{k})\\right)\\right\\|^{2}\\right]\\mid\\left[3\\right]\\right]}\\\\ &{\\leq\\mathbb{E}\\left[\\left\\|g^{k}-\\nabla f(x^{k})\\right\\|^{2}\\right]}\\\\ &{+\\;\\mathbb{E}\\left[\\mathbb{E}_{k}\\left[\\left\\|\\frac{1}{G_{C}^{k}}\\sum_{i\\in\\mathcal{C}_{C}^{k}}\\mathcal{G}\\left(\\widehat{\\Delta}_{i}\\left(x^{k+1},x^{k}\\right)\\right)-\\Delta\\left(x^{k+1},x^{k}\\right)\\right\\|^{2}\\right]\\mid\\left[3\\right]\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Let us consider the last part of the inequality. Note that $G_{C}^{k}\\geq(1-\\delta)C$ in this case and ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ensuremath{\\mathbb{B}}_{1}^{\\prime}=\\mathbb{E}\\left[\\ensuremath{\\mathbb{E}}_{k}\\left[\\left\\|\\frac{1}{G_{\\mathcal{C}_{i}}^{k}\\cup\\ensuremath{\\mathbb{S}}_{\\in\\mathcal{C}_{\\delta}}^{k}}\\mathcal{Q}\\left(\\widehat\\Delta_{i}\\left(x^{k+1},x^{k}\\right)\\right)-\\Delta\\left(x^{k+1},x^{k}\\right)\\right\\|^{2}\\right]\\mid[3]\\right]}\\\\ &{\\ \\ \\ =\\mathbb{E}\\left[\\ensuremath{\\mathbb{E}}_{S_{k}}\\left[\\mathbb{E}_{k}\\left[\\left\\|\\frac{1}{G_{\\mathcal{C}_{i}}^{k}}\\sum_{i\\in\\mathcal{C}_{\\delta}^{k}}{\\mathcal{Q}}\\left(\\widehat\\Delta_{i}\\left(x^{k+1},x^{k}\\right)\\right)-\\Delta\\left(x^{k+1},x^{k}\\right)\\right\\|^{2}\\right]\\mid[3]\\right]\\right]}\\\\ &{\\ \\ \\ \\leq\\frac{1}{C^{2}(1-\\delta)^{2}}\\mathbb{E}\\left[\\ensuremath{\\mathbb{E}}_{\\delta_{k}}\\left[\\sum_{i\\in\\mathcal{C}_{\\delta}^{k}}\\mathbb{E}_{k}\\left[\\left\\|\\mathcal{Q}\\left(\\widehat\\Delta_{i}\\left(x^{k+1},x^{k}\\right)\\right)-\\Delta\\left(x^{k+1},x^{k}\\right)\\right\\|^{2}\\right]\\mid1\\right]\\right]}\\\\ &{\\ \\ \\ \\leq\\frac{1}{C^{2}(1-\\delta)^{2}}\\mathbb{E}\\left[\\sum_{i\\in\\mathcal{C}_{\\delta}^{k}}\\mathbb{E}_{\\delta_{k}}\\left[\\left\\|\\mathcal{Q}\\left(\\widehat\\Delta_{i}\\left(x^{k+1},x^{k}\\right)\\right)-\\Delta\\left(x^{k+1},x^{k}\\right)\\right\\|^{2}\\right]\\mid[3]\\right]}\\\\ &{\\ \\ \\ =\\frac{1}{C^{2}(1-\\delta)^{2}}\\mathbb{E}\\left[\\sum_{i\\in\\mathcal{C}_{\\delta}^{k}}\\mathbb{E}_{k}\\left[\\big\\|\\mathcal{Q}\\left(\\widehat\\Delta_{i}\\left(x^{k+1},x^{k}\\right)\\right)-\\Delta\\left(x^{k+1},x^{k}\\right)\\right\\|^{2}\\right]\\mid[3]\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "where $\\mathcal{Z}_{\\mathcal{G}_{C}^{k}}$ is an indicator function for the event $\\left\\{i\\in\\mathcal{G}_{C}^{k}\\ |\\ G_{C}^{k}\\geq\\left(1-\\delta\\right)C\\right\\}$ and $\\begin{array}{r l}{\\mathcal{P}_{\\mathcal{G}_{C}^{k}}}&{{}=}\\end{array}$ Prob $\\left\\{i\\in\\mathcal{G}_{C}^{k}\\ |\\ G_{C}^{k}\\geq\\left(1-\\delta\\right)C\\right\\}$ is probabilityof such event. Note that $\\mathbb{E}_{S_{k}}\\left[\\mathcal{T}_{\\mathcal{G}_{C}^{k}}\\right]=\\mathcal{P}_{\\mathcal{G}_{C}^{k}}$ In the ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\forall i\\in\\mathcal{G}}&{\\mathcal{P}_{\\mathcal{G}_{C}^{k}}=\\mathrm{Prob}\\left\\{i\\in\\mathcal{G}_{C}^{k}\\mid G_{C}^{k}\\geq\\left(1-\\delta\\right)C\\right\\}}\\\\ &{\\qquad=\\cfrac{C}{n}\\cfrac{1}{p_{G}}\\cdot\\sum_{\\begin{array}{c}{(1-\\delta)C\\leq t\\leq C}\\\\ {(1-\\delta)C\\leq t\\leq C}\\end{array}}\\left(\\left(\\begin{array}{c}{G-1}\\\\ {t-1}\\end{array}\\right)\\left(\\begin{array}{c}{n-G}\\\\ {C-t}\\end{array}\\right)\\left(\\left(\\begin{array}{c}{n-1}\\\\ {C-1}\\end{array}\\right)\\right)^{-1}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Now, we can continue with inequalities: ", "text_level": 1, "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{B_{1}^{\\prime}\\leq\\frac{\\mathcal{P}_{\\mathcal{G}_{C}^{k}}}{C^{2}(1-\\delta)^{2}}\\mathbb{E}\\left[\\left\\|\\displaystyle\\sum_{i\\in\\mathcal{G}}\\mathbb{E}_{k}\\left[\\left\\|\\mathcal{Q}\\left(\\widehat{\\Delta}_{i}\\left(x^{k+1},x^{k}\\right)\\right)-\\Delta\\left(x^{k+1},x^{k}\\right)\\right\\|^{2}\\right]\\mid\\left\\{3\\right\\}\\right]}\\\\ &{\\quad\\leq\\frac{\\mathcal{P}_{\\mathcal{G}_{C}^{k}}}{C^{2}(1-\\delta)^{2}}\\mathbb{E}\\left[\\sum_{i\\in\\mathcal{G}}\\mathbb{E}_{k}\\left[\\mathbb{E}_{\\mathcal{G}}\\left[\\left\\|\\mathcal{Q}\\left(\\widehat{\\Delta}_{i}\\left(x^{k+1},x^{k}\\right)\\right)-\\Delta\\left(x^{k+1},x^{k}\\right)\\right\\|^{2}\\right]\\right]\\mid\\left\\{3\\right\\}\\right]}\\\\ &{\\quad\\leq\\frac{\\mathcal{P}_{\\mathcal{G}_{C}^{k}}}{C^{2}(1-\\delta)^{2}}\\mathbb{E}\\left[\\sum_{i\\in\\mathcal{G}}\\mathbb{E}_{k}\\left[\\mathbb{E}_{\\mathcal{Q}}\\left[\\left\\|\\mathcal{Q}\\left(\\widehat{\\Delta}_{i}\\left(x^{k+1},x^{k}\\right)\\right)-\\Delta_{i}\\left(x^{k+1},x^{k}\\right)\\right\\|^{2}\\right]\\right]\\mid\\left\\{3\\right\\}\\right]}\\\\ &{\\quad+\\frac{\\mathcal{P}_{\\mathcal{G}_{C}^{k}}}{C^{2}(1-\\delta)^{2}}\\mathbb{E}\\left[\\sum_{i\\in\\mathcal{G}}\\mathbb{E}_{k}\\left[\\left\\|\\Delta_{i}\\left(x^{k+1},x^{k}\\right)-\\Delta\\left(x^{k+1},x^{k}\\right)\\right\\|^{2}\\right]\\mid\\left\\{3\\right\\}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Using variance decomposition, we have ", "text_level": 1, "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{B_{1}^{\\prime}\\leq\\displaystyle\\frac{\\mathcal{P}_{\\mathcal{G}_{C}^{k}}}{C^{2}(1-\\delta)^{2}}\\mathbb{E}\\left[\\sum_{i\\in\\mathcal{G}}\\mathbb{E}_{k}\\left[\\mathbb{E}_{Q}\\left[\\left\\|\\mathcal{Q}\\left(\\widehat{\\Delta}_{i}\\left(x^{k+1},x^{k}\\right)\\right)\\right\\|^{2}\\right]\\right]-\\sum_{i\\in\\mathcal{G}}\\left\\|\\Delta_{i}\\left(x^{k+1},x^{k}\\right)\\right\\|^{2}\\mid\\left[3\\right]\\right]}\\\\ &{\\quad+\\displaystyle\\frac{\\mathcal{P}_{\\mathcal{G}_{C}^{k}}}{C^{2}(1-\\delta)^{2}}\\mathbb{E}\\left[\\sum_{i\\in\\mathcal{G}}\\mathbb{E}_{k}\\left[\\left\\|\\Delta_{i}\\left(x^{k+1},x^{k}\\right)-\\Delta\\left(x^{k+1},x^{k}\\right)\\right\\|^{2}\\right]\\mid\\left[3\\right]\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Applying the definition of unbiased compressor, we get ", "text_level": 1, "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{B_{1}^{\\prime}\\leq\\frac{\\mathcal{P}_{\\mathcal{G}_{\\hat{c}}}}{C^{2}(1-\\delta)^{2}}\\mathbb{E}\\left[\\underset{i\\in\\mathcal{C}}{\\sum}(1+\\omega)\\mathbb{E}_{k}\\left\\|\\widehat{\\Delta}_{i}\\left(x^{k+1},x^{k}\\right)\\right\\|^{2}-\\underset{i\\in\\mathcal{C}}{\\sum}\\left\\|\\Delta_{i}\\left(x^{k+1},x^{k}\\right)\\right\\|^{2}\\mid\\left[\\mathbb{B}\\right]\\right]}\\\\ &{\\quad+\\frac{\\mathcal{P}_{\\mathcal{G}_{\\hat{c}}}}{C^{2}(1-\\delta)^{2}}\\mathbb{E}\\left[\\underset{i\\in\\mathcal{C}}{\\sum}\\left\\|\\Delta_{i}\\left(x^{k+1},x^{k}\\right)-\\Delta\\left(x^{k+1},x^{k}\\right)\\right\\|^{2}\\mid\\left[\\mathbb{B}\\right]\\right]}\\\\ &{\\leq\\frac{\\mathcal{P}_{\\mathcal{G}_{\\hat{c}}}}{C^{2}(1-\\delta)^{2}}\\mathbb{E}\\left[\\underset{i\\in\\mathcal{C}}{\\sum}(1+\\omega)\\mathbb{E}_{k}\\left\\|\\widehat{\\Delta}_{i}\\left(x^{k+1},x^{k}\\right)-\\Delta_{i}\\left(x^{k+1},x^{k}\\right)\\right\\|^{2}\\right]}\\\\ &{\\quad+\\frac{\\mathcal{P}_{\\mathcal{G}_{\\hat{c}}}}{C^{2}(1-\\delta)^{2}}\\mathbb{E}\\left[\\underset{i\\in\\mathcal{C}}{\\sum}(1+\\omega)\\mathbb{E}_{k}\\left\\|\\Delta_{i}\\left(x^{k+1},x^{k}\\right)\\right\\|^{2}-\\underset{i\\in\\mathcal{C}}{\\sum}\\mathbb{E}_{k}\\left\\|\\Delta_{i}\\left(x^{k+1},x^{k}\\right)\\right\\|^{2}\\mid\\left[3\\right]\\right]}\\\\ &{\\quad+\\frac{\\mathcal{P}_{\\mathcal{G}_{\\hat{c}}}}{C^{2}(1-\\delta)^{2}}\\mathbb{E}\\left[\\underset{i\\in\\mathcal{C}}{\\sum}\\left\\|\\Delta_{i}\\left(x^{k+1},x^{k}\\right)-\\Delta\\left(x^{k+1},x^{k}\\right)\\right\\|^{2}\\mid\\left[\\mathbb{B}\\right]\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{B_{1}^{\\varepsilon}\\leq\\frac{P_{\\varepsilon_{0}}}{C^{2}(1-\\delta)^{2}}(1+\\omega)\\mathbb{E}\\left[\\sum_{i\\in\\mathcal{E}}\\left[\\left\\|\\widehat{\\Delta}_{i}\\left(x^{k+1},x^{k}\\right)-\\Delta_{i}\\left(x^{k+1},x^{k}\\right)\\right\\|^{2}\\right]\\mid\\mid3\\right]}\\\\ &{\\quad+\\frac{P_{\\varepsilon_{0}}}{C^{2}(1-\\delta)^{2}}\\approx\\left[\\sum_{i\\in\\mathcal{E}}\\left\\|\\Delta_{i}\\left(x^{k+1},x^{k}\\right)\\right\\|^{2}\\mid\\mid3\\right]}\\\\ &{\\quad+\\frac{P_{\\varepsilon_{0}}}{C^{2}(1-\\delta)^{2}}\\left[\\sum_{i\\in\\mathcal{E}}\\left\\|\\Delta_{i}\\left(x^{k+1},x^{k}\\right)-\\Delta\\left(x^{k+1},x^{k}\\right)\\right\\|^{2}\\mid3\\right]}\\\\ &{\\quad=\\frac{P_{\\varepsilon_{0}}}{C^{2}(1-\\delta)^{2}}(1+\\omega)\\mathbb{E}\\left[\\sum_{i\\in\\mathcal{E}}\\left\\|\\widehat{\\Delta}_{i}\\left(x^{k+1},x^{k}\\right)-\\Delta_{i}\\left(x^{k+1},x^{k}\\right)\\right\\|^{2}\\right]\\mid3\\right]}\\\\ &{\\quad+\\frac{P_{\\varepsilon_{0}}}{C^{2}(1-\\delta)^{2}}\\approx\\left[\\sum_{i\\in\\mathcal{E}}\\left\\|\\Delta_{i}\\left(x^{k+1},x^{k}\\right)-\\Delta\\left(x^{k+1},x^{k}\\right)\\right\\|^{2}+\\left\\|\\Delta\\left(x^{k+1},x^{k}\\right)\\right\\|^{2}\\mid3\\right]}\\\\ &{\\quad+\\frac{P_{\\varepsilon_{0}}}{C^{2}(1-\\delta)^{2}}\\left[\\sum_{i\\in\\mathcal{E}}\\left\\|\\Delta_{i}\\left(x^{k+1},x^{k}\\right)-\\Delta\\left(x^{k+1},x^{k}\\right)\\right\\|^{2}\\mid3\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Rearranging terms leads to ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle B_{1}^{\\prime}\\leq\\frac{\\mathcal{P}_{\\mathcal{G}_{C}^{k}}}{C^{2}(1-\\delta)^{2}}(1+\\omega)\\mathbb{E}\\left[\\sum_{i\\in\\mathcal{G}}\\mathbb{E}_{k}\\left[\\left\\|\\widehat{\\Delta}_{i}\\left(x^{k+1},x^{k}\\right)-\\Delta_{i}\\left(x^{k+1},x^{k}\\right)\\right\\|^{2}\\right]\\mid\\left[3\\right]\\right]}\\\\ &{\\quad+\\left.\\frac{\\mathcal{P}_{\\mathcal{G}_{C}^{k}}}{C^{2}(1-\\delta)^{2}}(\\omega+1)\\mathbb{E}\\left[\\sum_{i\\in\\mathcal{G}}\\left\\|\\Delta_{i}\\left(x^{k+1},x^{k}\\right)-\\Delta\\left(x^{k+1},x^{k}\\right)\\right\\|^{2}\\mid\\left[3\\right]\\right]}\\\\ &{\\quad+\\left.\\frac{\\mathcal{P}_{\\mathcal{G}_{C}^{k}}}{C^{2}(1-\\delta)^{2}}\\omega\\mathbb{E}\\left[\\sum_{i\\in\\mathcal{G}}\\left\\|\\Delta\\left(x^{k+1},x^{k}\\right)\\right\\|^{2}\\mid\\left[3\\right]\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Now we apply Assumptions D.1, D.2, D.3: ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{B_{1}^{\\prime}\\leq\\displaystyle\\frac{\\mathcal{P}_{\\mathcal{G}_{C}^{k}}}{C^{2}(1-\\delta)^{2}}(1+\\omega)\\mathbb{E}\\left[G\\displaystyle\\frac{\\mathcal{L}_{\\pm}^{2}}{b}\\|x^{k+1}-x^{k}\\|^{2}\\right]}\\\\ &{\\quad+\\displaystyle\\frac{\\mathcal{P}_{\\mathcal{G}_{C}^{k}}}{C^{2}(1-\\delta)^{2}}(\\omega+1)\\mathbb{E}\\left[G L_{\\pm}^{2}\\|x^{k+1}-x^{k}\\|^{2}\\right]+\\displaystyle\\frac{\\mathcal{P}_{\\mathcal{G}_{C}^{k}}}{C^{2}(1-\\delta)^{2}}\\omega\\mathbb{E}\\left[G L^{2}\\left\\|x^{k+1}-x^{k}\\right\\|^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Finally, we have ", "page_idx": 48}, {"type": "equation", "text": "$$\nB_{1}^{\\prime}\\leq\\frac{\\mathcal{P}_{\\mathcal{G}_{C}^{k}}\\cdot G}{C^{2}(1-\\delta)^{2}}\\left(\\omega L^{2}+(\\omega+1)L_{\\pm}^{2}+\\frac{(\\omega+1)\\mathcal{L}_{\\pm}^{2}}{b}\\right)\\mathbb{E}\\left[\\|x^{k+1}-x^{k}\\|^{2}\\right].\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Let us plug the obtained results in (27): ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{B_{1}\\leq\\mathbb{E}\\left[\\left\\|g^{k}-\\nabla f(x^{k})\\right\\|^{2}\\right]}\\\\ &{\\quad+\\,\\frac{\\mathcal{P}_{Q_{C}^{k}}\\cdot G}{C^{2}(1-\\delta)^{2}}\\left(\\omega L^{2}+(\\omega+1)L_{\\pm}^{2}+\\frac{(\\omega+1)\\mathcal{L}_{\\pm}^{2}}{b}\\right)\\mathbb{E}\\left[\\|x^{k+1}-x^{k}\\|^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Also, we have ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{A_{1}=\\mathbb{E}\\left[\\left\\Vert\\overline{{g}}^{k+1}-\\nabla f(x^{k+1})\\right\\Vert^{2}\\right]}\\\\ &{\\overset{(a)_{\\phi(2)}(\\leq)}{\\leq}(1-p)p_{G}B_{1}+(1-p)(1-p_{G})\\mathbb{E}\\left[\\left\\Vert g^{k}-\\nabla f(x^{k})\\right\\Vert^{2}\\right]}\\\\ &{\\quad+\\,p\\frac{\\delta\\cdot\\mathcal{P}_{G_{c}^{k}}}{(1-\\delta)}\\mathbb{E}\\left[B\\|\\nabla f(x)\\|^{2}+\\zeta^{2}\\right]}\\\\ &{\\leq(1-p)p_{G}\\mathbb{E}\\left[\\left\\Vert g^{k}-\\nabla f(x^{k})\\right\\Vert^{2}\\right]+p\\frac{\\delta\\cdot\\mathcal{P}_{G_{c}^{k}}}{(1-\\delta)}\\mathbb{E}\\left[B\\|\\nabla f(x)\\|^{2}+\\zeta^{2}\\right]}\\\\ &{\\quad+\\,(1-p)p_{G}\\frac{\\mathcal{P}_{G_{c}^{k}}}{\\zeta^{2}(1-\\delta)^{2}}\\left(\\omega L^{2}+(\\omega+1)L_{\\pm}^{2}+\\frac{(\\omega+1)L_{\\pm}^{2}}{b}\\right)\\mathbb{E}\\left[\\|x^{k+1}-x^{k}\\|^{2}\\right]}\\\\ &{\\quad+\\,(1-p)(1-p_{G})\\mathbb{E}\\left[\\left\\Vert g^{k}-\\nabla f(x^{k})\\right\\Vert^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Rearranging the terms, we get ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{l}{A_{1}\\leq(1-p)\\mathbb{E}\\left[\\left\\|g^{k}-\\nabla f(x^{k})\\right\\|^{2}\\right]+\\displaystyle\\frac{\\delta\\mathcal{P}_{\\mathcal{G}_{\\mathcal{C}}^{k}}}{(1-\\delta)}\\mathbb{E}\\left[B\\|\\nabla f(x)\\|^{2}+\\zeta^{2}\\right]}\\\\ {+\\left(1-p\\right)p_{G}\\displaystyle\\frac{\\mathcal{P}_{\\mathcal{G}_{\\mathcal{C}}^{k}}G}{C^{2}(1-\\delta)^{2}}\\left(\\omega L^{2}+(\\omega+1)L_{\\pm}^{2}+\\displaystyle\\frac{(\\omega+1)\\mathcal{L}_{\\pm}^{2}}{b}\\right)\\mathbb{E}\\left[\\|x^{k+1}-x^{k}\\|^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Lemma E.2. Let Assumptions D.1, D.2, D.3, D.4, 2.4 hold and the compression operator satisfy Definition 2.2.Also,let us introduce the notation ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r}{A R A g g_{Q}^{k+1}=A R A g g\\left(c l\\,i p_{\\lambda_{k+1}}\\left(\\mathcal{Q}\\left(\\widehat{\\Delta}_{1}(x^{k+1},x^{k})\\right)\\right),\\ldots,c l\\,i p_{\\lambda_{k+1}}\\left(\\mathcal{Q}\\left(\\widehat{\\Delta}_{C}(x^{k+1},x^{k})\\right)\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Thenfor all $k\\geq0$ the iterates produced by Byz-VR-MARINA-PP (Algorithm 1) satisfy ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{T_{2}=\\mathbb{E}\\left[\\mathbb{E}_{k}\\left[\\left\\|\\frac{1}{G_{C}^{k}}\\sum_{i\\in\\mathcal{G}_{C}^{k}}c l\\,i p_{\\lambda}\\left(\\mathcal{Q}\\left(\\widehat{\\Delta}_{i}\\left(x^{k+1},x^{k}\\right)\\right)\\right)-A R A g g_{Q}^{k+1}\\right\\|^{2}\\mid\\left[3\\right]\\right]\\right]}}\\\\ {{\\le4\\frac{G\\mathcal{P}_{\\mathcal{G}_{C}^{k}}}{C(1-\\delta)}c\\delta\\left((1+\\omega)\\frac{\\mathcal{L}_{\\pm}^{2}}{b}+(\\omega+1)L_{\\pm}^{2}+\\omega L^{2}\\right)\\mathbb{E}\\left[\\|x^{k+1}-x^{k}\\|^{2}\\right],}}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "where $\\mathcal{P}_{\\mathcal{G}_{C}^{k}}=\\operatorname{Prob}\\left\\{i\\in\\mathcal{G}_{C}^{k}\\mid G_{C}^{k}\\geq\\left(1-\\delta\\right)C\\right\\}$ ", "page_idx": 49}, {"type": "text", "text": "Proof. By definition of the robust aggregation, we have ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{T_{2}=\\mathbb{E}\\left[\\mathbb{E}_{k}\\left[\\left\\|\\frac{1}{G_{C}^{k}}\\sum_{i\\in\\mathcal{G}_{C}^{k}}\\mathrm{clip}_{\\lambda}\\left(Q\\left(\\widehat{\\Delta}_{i}\\left(x^{k+1},x^{k}\\right)\\right)\\right)-\\mathrm{ARAgg}_{Q}^{k+1}\\right\\|^{2}\\mid\\left[3\\right]\\right]\\right]}\\\\ &{\\quad\\leq\\mathbb{E}\\left[\\frac{c\\delta}{D_{2}}\\sum_{\\stackrel{i,l\\in\\mathcal{F}_{C}^{k}}{i\\neq l}}\\mathbb{E}_{k}\\left[\\left\\|\\mathrm{clip}_{\\lambda}\\left(Q\\left(\\widehat{\\Delta}_{i}\\left(x^{k+1},x^{k}\\right)\\right)\\right)-\\mathrm{clip}_{\\lambda}\\left(Q\\left(\\widehat{\\Delta}_{l}\\left(x^{k+1},x^{k}\\right)\\right)\\right)\\right\\|^{2}\\mid\\left[3\\right]\\right]\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "where $D_{2}=G_{C}^{k}(G_{C}^{k}-1)$ ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left\\|\\mathcal{Q}\\left(\\widehat{\\Delta}_{i}\\left(x^{k+1},x^{k}\\right)\\right)\\right\\|\\leq D_{Q}\\left\\|\\widehat{\\Delta}_{i}\\left(x^{k+1},x^{k}\\right)\\right\\|}&{}\\\\ {\\leq D_{Q}\\left\\|\\frac{1}{b}\\sum_{j\\in\\mathfrak{m}}\\nabla f_{i,j}(x^{k+1})-\\nabla f_{i,j}(x^{k})\\right\\|}&{}\\\\ {\\leq D_{Q}\\frac{1}{b}\\sum_{j\\in\\mathfrak{m}}\\left\\|\\nabla f_{i,j}(x^{k+1})-\\nabla f_{i,j}(x^{k})\\right\\|}&{}\\\\ {\\leq D_{Q}\\underset{j}{\\operatorname*{max}}L_{i,j}\\left\\|x^{k+1}-x^{k}\\right\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{T_{2}^{\\prime}(i,i)-\\mathbb{E}_{\\lambda}\\Bigg[\\Bigg|\\mathrm{Lip}_{\\lambda}\\Bigg(\\mathcal{G}\\stackrel{(\\lambda,(i+1,\\cdot)}{\\sim})\\Bigg)-\\mathrm{cym}_{\\lambda}\\Bigg(\\mathcal{G}\\stackrel{(\\lambda,(i+1,\\cdot)}{\\sim})\\Bigg)\\Bigg|^{2}\\Bigg|\\mathcal{B}_{\\lambda}^{\\top}\\Bigg]}\\\\ &{\\quad=\\mathbb{E}_{\\lambda}\\Bigg[\\Bigg|\\mathrm{Q}\\Bigg(\\widehat{\\Delta}_{(\\lambda}^{\\prime}(i+\\lambda,i^{\\prime}))-\\mathcal{A}\\Bigg(\\mathcal{Z}\\stackrel{(\\lambda,i^{\\prime})}{\\sim}\\mathcal{A}\\Bigg)\\Bigg|^{2}\\Bigg|\\mathcal{B}_{\\lambda}^{\\top}\\Bigg]}\\\\ &{\\quad=\\mathbb{E}_{\\lambda}\\Bigg[\\Bigg|\\mathrm{Q}\\Bigg(\\widehat{\\lambda}_{(\\lambda)}^{\\prime}\\Bigg(\\mathcal{A}^{\\star\\lambda,i^{\\prime}}\\Bigg)\\Bigg)-\\Delta_{(\\lambda}\\Big(\\mathcal{A}^{\\star\\lambda,i^{\\prime}}\\Bigg)\\Delta_{(\\lambda)}\\Bigg(\\mathcal{A}^{\\star\\lambda,i^{\\prime}}\\Bigg)\\Bigg|^{2}\\Bigg|\\mathcal{B}_{\\lambda}^{\\top}\\Bigg]}\\\\ &{\\quad+\\mathbb{E}_{\\lambda}\\Bigg[\\Bigg|\\mathrm{B}_{\\lambda}^{\\top}\\Bigg(\\mathcal{A}^{\\star\\lambda,i^{\\prime}}\\Bigg)-\\Delta_{(\\lambda}\\Big(\\mathcal{A}^{\\star\\lambda,i^{\\prime}}\\Bigg)\\Bigg|^{2}\\Bigg|\\mathcal{B}_{\\lambda}^{\\top}\\Bigg]}\\\\ &{\\quad\\stackrel{(\\lambda)}{\\le}\\mathbb{E}_{\\lambda}\\Bigg[\\Bigg|\\mathrm{Q}\\Bigg(\\widehat{\\lambda}_{(\\lambda)}^{\\prime}\\Bigg(\\mathcal{A}^{\\star\\lambda,i^{\\prime}}\\Bigg)\\Bigg)-\\Delta_{(\\lambda}\\Big(\\mathcal{A}^{\\star\\lambda,i^{\\prime}}\\Bigg)\\Bigg|^{2}\\ |\\mathcal{B}_{\\lambda}^{\\top}\\Bigg]}\\\\ &{\\quad+\\mathbb{E}_{\\lambda}\\Bigg[\\Bigg|\\mathrm{B}_{\\lambda}^{\\top}\\Bigg(\\mathcal{A}^{\\star\\lambda,i^{\\prime}}\\Bigg)-\\mathrm{Q}\\Bigg(\\widehat{\\lambda}_{(\\lambda)}^{\\prime}\\Bigg(\\mathcal{A}^{\\star\\lambda,i^{\\prime}}\\Bigg)\\Bigg|^{2}\\Bigg|\\mathcal{B}_{\\lambda}^{\\top}\\Bigg]}\\\\ &{\\quad+\\mathbb{E}_{\\\n$$", "text_format": "latex", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{T}_{2}=\\mathbb{E}\\left[\\frac{1}{\\tilde{\\mathcal{C}}_{\\mathcal{E}_{i}}(\\mathcal{E}_{t})-1}\\sum_{\\tilde{\\mathcal{C}}_{i}\\in\\mathcal{E}_{r}^{\\star}}\\mathcal{D}_{i}(i,\\tilde{\\mathcal{C}}_{i})\\right]}\\\\ &{\\lesssim\\mathbb{E}\\left[\\frac{1}{\\mathcal{D}_{t}}\\sum_{\\underline{{\\ell}},\\underline{{\\ell}}\\in\\mathcal{E}_{s}}\\mathbb{E}\\left[\\left\\|\\bar{\\mathcal{E}}_{t}\\left(\\tilde{\\Delta}_{t}\\left(x^{\\star+1},x^{\\star}\\right)-\\Delta_{t}\\left(x^{\\star+1},x^{\\star}\\right)\\right\\|^{2}\\mid\\mathcal{B}\\right)\\right]\\right.}\\\\ &{\\quad+\\mathbb{E}\\left[\\frac{1}{\\mathcal{D}_{t}}\\sum_{\\underline{{\\ell}},\\underline{{\\ell}}\\in\\mathcal{E}_{s}}\\mathbb{E}\\left[\\left\\|\\bar{\\mathcal{X}}_{t}\\left(x^{\\star+1},x^{\\star}\\right)-\\mathcal{B}\\left(\\bar{\\mathcal{X}}_{t}\\left(x^{\\star+1},x^{\\star}\\right)\\right)\\right\\|^{2}\\mid\\mathcal{B}\\right]\\right]}\\\\ &{\\quad+\\mathbb{E}\\left[\\frac{1}{\\mathcal{D}_{t}}\\sum_{\\underline{{\\ell}},\\underline{{\\ell}}\\in\\mathcal{E}_{s}}\\left[\\left\\|\\bar{\\mathcal{X}}_{t}\\left(x^{\\star+1},x^{\\star}\\right)-\\bar{\\mathcal{B}}\\left(\\bar{\\mathcal{X}}_{t}\\left(x^{\\star+1},x^{\\star}\\right)\\right)\\right\\|^{2}\\mid\\mathcal{B}\\right]\\right]}\\\\ &{\\quad+\\mathbb{E}\\left[\\frac{1}{\\mathcal{D}_{t}}\\sum_{\\underline{{\\ell}},\\underline{{\\ell}}\\in\\mathcal{E}_{s}}\\left[\\left\\|\\bar{\\mathcal{X}}_{t}\\left(x^{\\star+1},x^{\\star}\\right)-\\Delta_{t}\\left(x^{\\star+1},x^{\\star}\\right)\\right\\|^{2}\\mid\\mathcal{B}\\right]\\right]}\\\\ &{\\quad+\\mathbb{E}\\left[\\frac{1}{\\mathcal{D}_{t}}\\sum_{\\underline{{\\ell}},\\underline{{\\ell}}\\in\\mathcal{E}_{s}}\\left[\\left\\|\\bar{\\mathcal{X}}_{t}\\left(x^{\\star+1},x^{\\star}\\right)-\\Delta_{t}\\left(x^{ \n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "Rearranging the terms, we get ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widehat{T}_{2}\\leq\\mathbb{E}\\left[\\frac{4}{G_{C}^{k}}\\sum_{i\\in\\mathcal{G}_{C}^{k}}\\mathbb{E}_{k}\\left[\\left\\|\\mathcal{Q}\\left(\\widehat{\\Delta}_{i}\\left(x^{k+1},x^{k}\\right)\\right)-\\Delta_{i}\\left(x^{k+1},x^{k}\\right)\\right\\|^{2}\\mid[3]\\right]\\right]}\\\\ &{\\quad+\\,\\mathbb{E}\\left[\\frac{4}{G_{C}^{k}}\\sum_{i\\in\\mathcal{G}_{C}^{k}}\\mathbb{E}_{k}\\left[\\left\\|\\Delta_{i}\\left(x^{k+1},x^{k}\\right)-\\Delta\\left(x^{k+1},x^{k}\\right)\\right\\|^{2}\\mid[3]\\right]\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "Using variance decomposition, we get ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widehat{T}_{2}\\leq\\mathbb{E}\\left[\\frac{1}{G_{C}^{k}}\\sum_{i\\in\\mathcal{G}_{C}^{k}}4\\mathbb{E}_{k}\\left[\\left\\|\\mathscr{Q}\\left(\\widehat{\\Delta}_{i}\\left(x^{k+1},x^{k}\\right)\\right)\\right\\|^{2}\\mid\\left[3\\right]\\right]\\right]}\\\\ &{\\quad-\\mathbb{E}\\left[\\frac{1}{G_{C}^{k}}\\sum_{i\\in\\mathcal{G}_{C}^{k}}4\\mathbb{E}_{k}\\left[\\left\\|\\Delta_{i}\\left(x^{k+1},x^{k}\\right)\\right\\|^{2}\\mid\\left[3\\right]\\right]\\right]}\\\\ &{\\quad+\\mathbb{E}\\left[\\frac{1}{G_{C}^{k}}\\sum_{i\\in\\mathcal{G}_{C}^{k}}4\\mathbb{E}_{k}\\left[\\left\\|\\Delta_{i}\\left(x^{k+1},x^{k}\\right)-\\Delta\\left(x^{k+1},x^{k}\\right)\\right\\|^{2}\\mid\\left[3\\right]\\right]\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "Using the properties of unbiased compressors, we obtain ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widehat{I}_{2}\\leq\\mathbb{E}\\left[\\frac{1}{(\\widehat{\\alpha}_{C}^{2}+\\eta(1-\\delta))}\\mathbb{E}\\Bigg[\\left\\|\\widehat{\\Delta}_{t}\\left(x^{\\star}+\\varepsilon^{\\star}\\right),x^{\\star}\\right\\|^{2}\\left\\|\\mathcal{B}\\right\\right\\|\\right]}\\\\ &{\\quad-\\mathbb{E}\\left[\\frac{1}{(\\widehat{\\alpha}_{C}^{2}+\\eta(1-\\delta))}\\mathbb{E}\\Bigg[\\left\\|\\widehat{\\Delta}_{t}\\left(x^{\\star+\\varepsilon^{\\star}}\\right),x^{\\star}\\right\\|^{2}|\\mathcal{B}\\right]\\Bigg]}\\\\ &{\\quad+\\mathbb{E}\\left[\\frac{1}{(\\widehat{\\alpha}_{C}^{2}+\\eta(1-\\delta))}\\mathbb{E}\\Bigg[\\left\\|\\widehat{\\Delta}_{t}\\left(x^{\\star+\\eta(1-\\delta)},x^{\\star}\\right)-\\Delta\\left(x^{\\star+\\varepsilon^{\\star}},x^{\\star}\\right)\\right\\|^{2}|\\mathcal{B}\\right]\\Bigg]}\\\\ &{\\quad\\leq\\mathbb{E}\\left[\\frac{1}{(\\widehat{\\alpha}_{C}^{2}+\\eta(1-\\delta))}\\mathbb{E}\\Bigg[\\left\\|\\widehat{\\Delta}_{t}\\left(x^{\\star+\\eta(1-\\delta)},x^{\\star}\\right)-\\Delta\\left(x^{\\star+\\varepsilon^{\\star}},x^{\\star}\\right)\\right\\|^{2}|\\mathcal{B}\\right\\|\\right]}\\\\ &{\\quad+\\mathbb{E}\\left[\\frac{1}{(\\widehat{\\alpha}_{C}^{2}+\\eta(1-\\delta))}\\mathbb{E}\\Bigg[\\left\\|\\widehat{\\Delta}_{t}\\left(x^{\\star+\\eta(1-\\delta)},x^{\\star}\\right)\\right\\|^{2}|\\mathcal{B}\\right\\|\\right]}\\\\ &{\\quad-\\mathbb{E}\\left[\\frac{1}{(\\widehat{\\alpha}_{C}^{2}+\\eta(1-\\delta))}\\mathbb{E}\\Bigg[\\left\\|\\widehat{\\Delta}_{t}\\left(x^{\\star+\\eta(1-\\delta)},x^{\\star}\\right)\\right\\|^{2}|\\mathcal{B}\\right\\|\\right]}\\\\ &{\\quad-\\mathbb{E}\\left[\\frac{1}{(\\widehat{\\alpha}_{C}^{2}+\\eta(1-\\delta))}\\mathbb{E}\\Bigg[\\left\\|\\widehat{\\Delta}_{t}\\left(x^{\\star+\\eta(1-\\delta)},x^{\\star}\\right)\\right\\|^{2}|\\mathcal{B}\\right\\|\\right]}\\\\ &{\\quad+\\mathbb{E}\\Bigg[\\frac{1}{(\\\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "Let us simplify the inequality: ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widehat{T}_{2}\\leq\\mathbb{E}\\left[\\frac{1}{G_{C}^{k}}\\sum_{i\\in\\mathcal{G}_{C}^{k}}4(1+\\omega)\\mathbb{E}_{k}\\left[\\left\\Vert\\widehat{\\Delta}_{i}\\left(x^{k+1},x^{k}\\right)-\\Delta_{i}\\left(x^{k+1},x^{k}\\right)\\right\\Vert^{2}\\mid\\left[3\\right]\\right]\\right]}\\\\ &{\\quad+\\mathbb{E}\\left[\\frac{1}{G_{C}^{k}}\\sum_{i\\in\\mathcal{G}_{C}^{k}}4\\omega\\mathbb{E}_{k}\\left[\\left\\Vert\\Delta_{i}\\left(x^{k+1},x^{k}\\right)\\right\\Vert^{2}\\mid\\left[3\\right]\\right]\\right]}\\\\ &{\\quad+\\mathbb{E}\\left[\\frac{1}{G_{C}^{k}}\\sum_{i\\in\\mathcal{G}_{C}^{k}}4\\mathbb{E}_{k}\\left[\\left\\Vert\\Delta_{i}\\left(x^{k+1},x^{k}\\right)-\\Delta\\left(x^{k+1},x^{k}\\right)\\right\\Vert^{2}\\mid\\left[3\\right]\\right]\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "Using variance decomposition once again, we get ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widehat{T}_{2}\\leq\\mathbb{E}\\left[\\frac{1}{G_{C}^{k}}\\sum_{i\\in\\mathcal{E}_{c}^{k}}4(1+\\omega)\\mathbb{E}_{k}\\left[\\left\\|\\widehat{\\Delta}_{i}\\left(x^{k+1},x^{k}\\right)-\\Delta_{i}\\left(x^{k+1},x^{k}\\right)\\right\\|^{2}\\mid\\left[3\\right]\\right]\\right]}\\\\ &{\\quad+\\mathbb{E}\\left[\\frac{1}{G_{C}^{k}}\\sum_{i\\in\\mathcal{E}_{c}^{k}}4\\omega\\mathbb{E}_{k}\\left[\\left\\|\\Delta_{i}\\left(x^{k+1},x^{k}\\right)-\\Delta\\left(x^{k+1},x^{k}\\right)\\right\\|^{2}\\mid\\left[3\\right]\\right]\\right]}\\\\ &{\\quad+\\mathbb{E}\\left[\\frac{1}{G_{C}^{k}}\\sum_{i\\in\\mathcal{E}_{c}^{k}}4\\mathbb{E}_{k}\\left[\\left\\|\\Delta_{i}\\left(x^{k+1},x^{k}\\right)-\\Delta\\left(x^{k+1},x^{k}\\right)\\right\\|^{2}\\mid\\left[3\\right]\\right]\\right]}\\\\ &{\\quad+\\mathbb{E}\\left[\\frac{1}{G_{C}^{k}}\\sum_{i\\in\\mathcal{E}_{c}^{k}}4\\mathbb{E}_{k}\\left[\\left\\|\\Delta\\left(x^{k+1},x^{k}\\right)-\\Delta\\left(x^{k+1},x^{k}\\right)\\right\\|^{2}\\mid\\left[3\\right]\\right]\\right]}\\\\ &{\\quad+\\mathbb{E}\\left[\\frac{1}{G_{C}^{k}}\\sum_{i\\in\\mathcal{E}_{c}^{k}}4\\omega\\mathbb{E}_{k}\\left[\\left\\|\\Delta\\left(x^{k+1},x^{k}\\right)\\right\\|^{2}\\mid\\left[3\\right]\\right]\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "Then, we apply similar arguments to the ones used in deriving (28): ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widehat{T}_{2}\\leq\\mathbb{E}\\left[\\frac{\\mathcal{P}_{\\mathcal{G}_{c}^{k}}}{C(1-\\delta)}\\sum_{i\\in\\mathcal{G}}4(1+\\omega)\\mathbb{E}_{k}\\left[\\left\\|\\widehat{\\Delta}_{i}\\left(x^{k+1},x^{k}\\right)-\\Delta_{i}\\left(x^{k+1},x^{k}\\right)\\right\\|^{2}\\mid\\left[3\\right]\\right]\\right]}\\\\ &{\\quad+\\mathbb{E}\\left[\\frac{\\mathcal{P}_{\\mathcal{G}_{c}^{k}}}{C(1-\\delta)}\\sum_{i\\in\\mathcal{G}}4\\omega\\mathbb{E}_{k}\\left[\\left\\|\\Delta_{i}\\left(x^{k+1},x^{k}\\right)-\\Delta\\left(x^{k+1},x^{k}\\right)\\right\\|^{2}\\mid\\left[3\\right]\\right]\\right]}\\\\ &{\\quad+\\mathbb{E}\\left[\\frac{\\mathcal{P}_{\\mathcal{G}_{c}^{k}}}{C(1-\\delta)}\\sum_{i\\in\\mathcal{G}}4\\mathbb{E}_{k}\\left[\\left\\|\\Delta_{i}\\left(x^{k+1},x^{k}\\right)-\\Delta\\left(x^{k+1},x^{k}\\right)\\right\\|^{2}\\mid\\left[3\\right]\\right]\\right]}\\\\ &{\\quad+\\mathbb{E}\\left[\\frac{\\mathcal{P}_{\\mathcal{G}_{c}^{k}}}{C(1-\\delta)}\\sum_{i\\in\\mathcal{G}}4\\omega\\mathbb{E}_{k}\\left[\\left\\|\\Delta\\left(x^{k+1},x^{k}\\right)\\right\\|^{2}\\mid\\left[3\\right]\\right]\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "Using Assumptions D.1, D.2, D.3: ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widehat{T}_{2}\\leq\\mathbb{E}\\left[4(1+\\omega)\\displaystyle\\frac{G\\mathcal{P}_{\\mathcal{G}_{c}^{k}}}{C(1-\\delta)}\\displaystyle\\frac{\\mathcal{L}_{\\pm}^{2}}{b}\\|x^{k+1}-x^{k}\\|^{2}\\right]+\\mathbb{E}\\left[4(\\omega+1)\\displaystyle\\frac{G\\mathcal{P}_{\\mathcal{G}_{c}^{k}}}{C(1-\\delta)}\\omega L_{\\pm}^{2}\\|x^{k+1}-x^{k}\\|^{2}\\right]}\\\\ &{\\quad+\\mathbb{E}\\left[4\\displaystyle\\frac{G\\mathcal{P}_{\\mathcal{G}_{c}^{k}}}{C(1-\\delta)}\\omega L^{2}\\|x^{k+1}-x^{k}\\|^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "Finally, we obtain ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\begin{array}{l}{T_{2}=\\mathbb{E}\\left[\\mathbb{E}_{k}\\left[\\left\\|\\frac{1}{G_{C}^{k}}\\sum_{i\\in\\mathcal{G}_{C}^{k}}\\mathfrak{c}{\\mathrm{xinp}}_{\\lambda}\\left(\\mathcal{Q}\\left(\\widehat{\\Delta}_{i}\\left(x^{k+1},x^{k}\\right)\\right)\\right)-\\mathbb{A}\\mathbb{R}\\mathbb{A}\\mathrm{g}\\mathrm{g}_{Q}^{k+1}\\right\\|^{2}\\mid\\left[3\\right]\\right]\\right]}\\\\ {\\quad\\leq4\\frac{G\\mathcal{P}_{\\mathcal{G}_{C}^{k}}}{C(1-\\delta)}{\\mathfrak{c}}\\delta\\left((1+\\omega)\\frac{\\mathcal{L}_{\\pm}^{2}}{b}+(\\omega+1)L_{\\pm}^{2}+\\omega L^{2}\\right)\\mathbb{E}\\left[\\|x^{k+1}-x^{k}\\|^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "Lemma E.3. Let Assumptions 2.3, D.1, D.2, D.3, D.4, D.5, 2.4 hold and the compression operator satisfy Definition 2.2.We set $\\lambda_{k+1}=D_{Q}\\operatorname*{max}_{i,j}L_{i,j}\\|x^{k+1}-x^{k}\\|$ Also, let us introduce the notation $\\begin{array}{r}{A R A g g_{Q}^{k+1}=A R A g g\\left(c\\imath\\,i p_{\\lambda_{k+1}}\\left(\\mathcal{Q}\\left(\\widehat{\\Delta}_{1}(x^{k+1},x^{k})\\right)\\right),\\ldots,c\\imath\\,i p_{\\lambda_{k+1}}\\left(\\mathcal{Q}\\left(\\widehat{\\Delta}_{C}(x^{k+1},x^{k})\\right)\\right)\\right).}\\end{array}$ Thenforall $k\\geq0$ the iterates produced by Byz-VR-MARINA-PP (Algorithm 1) satisfy ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\mathbb{E}\\left[\\left\\|g^{k+1}-\\nabla f\\left(x^{k+1}\\right)\\right\\|^{2}\\right]\\leq\\left(1-\\displaystyle\\frac{p}{2}\\right)\\mathbb{E}\\left[\\left\\|g^{k}-\\nabla f\\left(x^{k}\\right)\\right\\|^{2}\\right]}\\\\ {\\quad\\displaystyle+\\,\\widehat{B}\\mathbb{E}\\left[\\left\\|\\nabla f\\left(x^{k}\\right)\\right\\|^{2}\\right]+\\widehat{D}\\zeta^{2}+\\frac{p A}{4}\\|x^{k+1}-x^{k}\\|^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "with ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{A=\\displaystyle\\frac{4}{p}\\left(\\frac{p_{G}\\mathcal{P}_{\\mathcal{G}_{C}^{k}}G}{C^{2}(1-\\delta)^{2}}\\omega+\\frac{8G\\mathcal{P}_{\\mathcal{G}_{C}^{k}}c\\delta}{(1-\\delta)\\widehat{C}}B+6\\widetilde{B}+\\frac{4}{p}(1-p_{G})+\\frac{8}{p}p_{G}\\frac{G\\mathcal{P}_{\\mathcal{G}_{C}^{k}}}{C(1-\\delta)}c\\delta\\omega\\right)L^{2}}\\\\ &{\\quad+\\displaystyle\\frac{4}{p}\\left(\\frac{p_{G}\\mathcal{P}_{\\mathcal{G}_{C}^{k}}G}{C^{2}(1-\\delta)^{2}}\\left(\\omega+1\\right)+\\frac{8}{p}p_{G}\\frac{G\\mathcal{P}_{\\mathcal{G}_{C}^{k}}}{C(1-\\delta)}c\\delta(\\omega+1)\\right)\\left(L_{\\pm}^{2}+\\frac{\\mathcal{L}_{\\pm}^{2}}{b}\\right)}\\\\ &{\\quad+\\displaystyle\\frac{16}{p^{2}}(1-p_{G})F_{\\mathcal{A}}^{2}\\left(D_{\\mathcal{Q}}\\operatorname*{max}_{i,j}\\cal L_{i,j}\\right)^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 53}, {"type": "equation", "text": "$$\n\\widehat{B}=2\\frac{\\delta\\mathcal{P}_{\\mathcal{G}_{\\widehat{C}}^{k}}}{1-\\delta}B\\left(\\frac{12c G}{\\widehat{C}}+p\\right)+6\\widetilde{B},\\quad\\widehat{D}=2\\frac{\\delta\\mathcal{P}_{\\mathcal{G}_{\\widehat{C}}^{k}}}{1-\\delta}\\left(\\frac{6c G}{\\widehat{C}}+p\\right)+\\widetilde{D},\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "Wwhere $\\tilde{B}:=0$ and $\\tilde{D}:=0$ when ${\\widehat{C}}\\,=\\,n$ and $\\begin{array}{r}{\\widetilde{B}\\;:=\\;\\frac{\\mathcal{P}_{\\mathcal{G}_{\\widehat{C}}^{k}}G B}{(1-\\delta)\\widehat{C}}}\\end{array}$ and $\\begin{array}{r}{\\tilde{D}\\,:=\\,\\frac{\\mathcal{P}_{\\mathcal{G}_{\\widehat{C}}^{k}}G}{(1-\\delta)\\widehat{C}}}\\end{array}$ when ${\\widehat{C}}\\,<\\,n.$$p_{G}=\\mathrm{Prob}\\left\\{G_{C}^{k}\\geq(1-\\delta)C\\right\\}$ and $\\mathcal{P}_{\\mathcal{G}_{C}^{k}}=\\operatorname{Prob}\\left\\{i\\in\\mathcal{G}_{C}^{k}\\mid G_{C}^{k}\\geq\\left(1-\\delta\\right)C\\right\\}$", "page_idx": 53}, {"type": "text", "text": "Proof. Let us combine bounds for $A_{1}$ and $A_{2}$ together: ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{A_{0}}&{=\\mathbb{E}\\left[\\left\\Vert\\phi^{k+1}-\\nabla f\\left(z^{k+1}\\right)\\right\\Vert^{2}\\right]}\\\\ &{\\leq\\left(1+\\frac{p}{2}\\right)\\mathbb{E}\\left[\\left\\Vert\\phi^{k+1}-\\nabla f\\left(x^{k+1}\\right)\\right\\Vert^{2}\\right]+\\left(1+\\frac{2}{p}\\right)\\mathbb{E}\\left[\\left\\Vert\\phi^{k+1}-\\bar{y}^{k+1}\\right\\Vert^{2}\\right]}\\\\ &{\\leq\\left(1+\\frac{p}{2}\\right){A_{1}}+\\left(1+\\frac{2}{p}\\right){A_{2}}}\\\\ &{\\leq\\left(1+\\frac{p}{2}\\right)\\left(1-p\\mathbb{E}\\left[\\left\\Vert\\phi^{k}-\\nabla f\\left(x^{k}\\right)\\right\\Vert^{2}\\right]+\\left(1+\\frac{p}{2}\\right)p\\frac{\\delta P_{\\phi_{\\phi_{\\phi}}}}{\\left(1-\\delta\\right)^{2}}\\mathbb{E}\\left[\\left\\Vert\\nabla f\\left(x^{k}\\right)\\right\\Vert^{2}+\\zeta^{2}\\right]\\right.}\\\\ &{\\left.+\\left(1+\\frac{2}{p}\\right)\\left(1-\\frac{\\delta\\sqrt{\\delta}}{\\phi_{\\phi}}\\right)^{2}\\left(1-\\delta^{2}\\left(\\frac{\\delta\\cdot\\zeta}{1}\\right)\\left(\\frac{4-\\delta\\cdot\\zeta}{1-\\delta\\cdot\\zeta}\\right)\\mathbb{E}\\left[\\left\\Vert\\phi^{k+1}-\\bar{x}^{k}\\right\\Vert^{2}\\right]\\right.}\\\\ &{\\left.+\\left(1+\\frac{2}{p}\\right)p\\mathbb{E}\\left[\\left\\Vert\\phi_{\\phi_{\\phi}}\\right\\Vert^{2}\\left\\Vert\\phi_{\\phi}\\left(x^{k+1},\\dots,\\nabla f_{\\rho}\\left(x^{k+\\delta}\\right)\\right)-\\nabla f\\left(x^{k+1}\\right)\\right\\Vert^{2}\\right]\\left\\Vert\\bar{\\mathbf{\\Phi}}\\right\\Vert}\\\\ &{+\\left(1+\\frac{2}{p}\\right)\\left(1-p\\right)\\kappa\\left\\Vert\\frac{1}{\\phi_{\\phi}}\\right\\Vert\\left[\\frac{1}{\\phi_{\\phi}}\\right]\\left\\Vert\\frac{1}{\\phi_{\\phi}}\\right\\Vert\\left(\\frac{2}{\\delta\\cdot\\zeta}\\right)\\operatorname*{dias}\\left(\\left(\\frac{\\delta\\cdot\\zeta}{\\delta\\cdot\\delta}\\right)\\left(1+\\frac{2}{\\delta\\cdot\\zeta}\\right)\\right)-18\\mathbf{E}_{\\phi}^{k+1}\\right\\Vert^{2}}\\\\ &{\\leq\\left(1+\\frac{2}{p \n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "Using Lemma E.2 and lemmas from General Analysis (Lemmas D.10 and D.12) we have ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{0=\\mathbb{E}\\left[\\left\\|{\\rho}^{k+1}-\\nabla f\\left(x^{k+1}\\right)\\right\\|^{2}\\right]}\\\\ &{\\leq\\left(1-\\frac{p}{2}\\right)\\mathbb{E}\\left[\\left\\|{\\rho}_{k}^{k}-\\nabla f\\left(x^{k}\\right)\\right\\|^{2}\\right]+2\\frac{\\delta\\mathcal{P}_{(\\frac{\\theta}{\\theta})}}{(1-\\delta)\\delta}\\mathbb{E}\\left[B\\Gamma f\\left(x\\right)\\right]^{2}+\\mathcal{C}^{2}\\right]}\\\\ &{\\quad+\\left(1-\\frac{p}{2}\\right)\\rho_{k}\\frac{\\mathcal{C}_{\\theta}}{C_{(1)}(1-\\delta)}\\int_{(x^{k})}^{\\infty}\\left(\\alpha f^{2}+(\\alpha+1)L_{\\frac{\\theta}{\\theta}}^{2}+\\left(\\frac{\\alpha+1}{b}\\right)\\mathcal{L}_{\\theta}^{2}\\right)\\mathbb{E}\\left[\\left\\|x^{k+1}-x^{k}\\right\\|^{2}\\right]}\\\\ &{\\quad+(p+2)\\left(\\frac{\\delta C_{\\theta}\\gamma_{0}\\phi\\delta}{(1-\\delta)\\delta}+2\\tilde{B}\\right)\\mathbb{E}\\left[\\left\\|{\\nabla f\\left(x^{k}\\right)\\right\\|^{2}+L^{2}}\\left\\|{\\mathbf{\\Phi}}^{2\\varepsilon+1}-x^{k}\\right\\|^{2}\\right]}\\\\ &{\\quad+4(p+2)\\frac{\\delta\\mathcal{C}_{\\theta}\\gamma_{0}\\phi}{(1-\\delta)\\delta}\\mathcal{C}^{2}+(p+2)\\tilde{C}^{2}}\\\\ &{\\quad+\\frac{2}{p}\\rho_{k}\\mathrm{E}\\left[\\left\\|{\\mathbf{\\Phi}}^{2}\\left(1+\\frac{\\delta}{\\theta}\\right)\\frac{\\mathcal{C}_{\\theta}}{C_{(1)}(1-\\delta)}\\phi\\frac{\\delta}{C_{\\theta}}\\right\\|_{1}^{2}k^{\\varepsilon+1}-x^{k}\\|^{2}\\right]}\\\\ &{\\quad+\\frac{2}{p}\\rho_{k}\\mathbb{E}\\left[\\left\\|{\\mathbf{\\Phi}}(1+\\frac{\\delta}{\\theta})\\frac{\\mathcal{C}_{\\theta}}{C_{(1)}(1-\\delta)}\\phi\\mathbf{\\Phi}^{2}\\right\\|_{1}^{2}k^{\\varepsilon+1}-x^{k}\\|^{2}\\right]}\\\\ &{\\quad+\\frac{2}{p}\\rho_{k}\\mathbb{E}\\left[\\left\\|{\\mathbf{\\Phi}}( \n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "Finally, we have ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\mathbb{E}\\left[\\left\\|g^{k+1}-\\nabla f\\left(x^{k+1}\\right)\\right\\|^{2}\\right]\\leq\\left(1-\\displaystyle\\frac{p}{2}\\right)\\mathbb{E}\\left[\\left\\|g^{k}-\\nabla f\\left(x^{k}\\right)\\right\\|^{2}\\right]}\\\\ {\\quad\\displaystyle+\\,\\widehat{B}\\mathbb{E}\\left[\\left\\|\\nabla f\\left(x^{k}\\right)\\right\\|^{2}\\right]+\\widehat{D}\\zeta^{2}+\\frac{p A}{4}\\|x^{k+1}-x^{k}\\|^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "where ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{A=\\displaystyle\\frac{4}{p}\\left(\\frac{p_{G}\\mathcal{P}_{\\mathcal{G}_{C}^{k}}G}{C^{2}(1-\\delta)^{2}}\\omega+\\frac{8G\\mathcal{P}_{\\mathcal{G}_{C}^{k}}c\\delta}{(1-\\delta)\\widehat{C}}B+6\\widetilde{B}+\\frac{4}{p}(1-p_{G})+\\frac{8}{p}p_{G}\\frac{G\\mathcal{P}_{\\mathcal{G}_{C}^{k}}}{C(1-\\delta)}c\\delta\\omega\\right)L^{2}}\\\\ &{\\quad+\\displaystyle\\frac{4}{p}\\left(\\frac{p_{G}\\mathcal{P}_{\\mathcal{G}_{C}^{k}}G}{C^{2}(1-\\delta)^{2}}\\left(\\omega+1\\right)+\\frac{8}{p}p_{G}\\frac{G\\mathcal{P}_{\\mathcal{G}_{C}^{k}}}{C(1-\\delta)}c\\delta(\\omega+1)\\right)\\left(L_{\\pm}^{2}+\\frac{\\mathcal{L}_{\\pm}^{2}}{b}\\right)}\\\\ &{\\quad+\\displaystyle\\frac{16}{p^{2}}(1-p_{G})F_{\\mathcal{A}}^{2}\\left(D_{\\mathcal{Q}}\\operatorname*{max}_{i,j}\\Sigma_{i,j}\\right)^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "and ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\widehat{B}=2\\frac{\\delta\\mathcal{P}_{\\mathcal{G}_{\\widehat{C}}^{k}}}{1-\\delta}B\\left(\\frac{12c G}{\\widehat{C}}+p\\right)+6\\widetilde{B},\\quad\\widehat{D}=2\\frac{\\delta\\mathcal{P}_{\\mathcal{G}_{\\widehat{C}}^{k}}}{1-\\delta}\\left(\\frac{6c G}{\\widehat{C}}+p\\right)+\\widetilde{D}.\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "E.2  Main Results ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "Theorem E.4, Let Assumptions 2.3, D.1, D.2, D.3, D.4, D.5, 2.4 hold. Setting $\\lambda_{k+1}~=$ $\\begin{array}{r l}{\\operatorname*{max}_{i,j}L_{i,j}\\left\\|x^{k+1}-x^{k}\\right\\|}&{{}}\\end{array}$ Assumethat ", "page_idx": 55}, {"type": "equation", "text": "$$\n0<\\gamma\\leq\\frac{1}{L+\\sqrt{A}},\\quad4\\widehat{B}<p,\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "where ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{A=\\frac{4}{p}\\left(\\frac{p\\alpha\\mathcal{P}_{\\mathcal{G}_{C}^{k}}G}{C^{2}(1-\\delta)^{2}}\\omega+\\frac{8G\\mathcal{P}_{\\mathcal{G}_{C}^{k}}\\delta}{(1-\\delta)\\tilde{C}}B+6\\tilde{B}+\\frac{4}{p}(1-p_{G})+\\frac{8}{p}p\\alpha\\frac{G\\mathcal{P}_{\\mathcal{G}_{C}^{k}}}{C(1-\\delta)}c\\delta\\omega\\right)L^{2}}\\\\ &{\\quad+\\frac{4}{p}\\left(\\frac{p\\alpha\\mathcal{P}_{\\mathcal{G}_{C}^{k}}G}{C^{2}(1-\\delta)^{2}}\\left(\\omega+1\\right)+\\frac{8}{p}p\\alpha\\frac{G\\mathcal{P}_{\\mathcal{G}_{C}^{k}}}{C(1-\\delta)}c\\delta(\\omega+1)\\right)\\left(L_{\\pm}^{2}+\\frac{\\mathcal{L}_{\\pm}^{2}}{b}\\right)}\\\\ &{\\quad+\\frac{16}{p^{2}}(1-p_{G})F_{\\mathcal{A}}^{2}\\left(D_{Q}\\operatorname*{max}_{i,j}\\right)^{2}}\\\\ &{\\quad\\quad\\widehat{B}=2\\frac{\\delta\\mathcal{P}_{\\mathcal{G}_{C}^{k}}}{1-\\delta}B\\left(\\frac{12c G}{\\tilde{C}}+p\\right)+6\\tilde{B},\\quad\\widehat{D}=2\\frac{\\delta\\mathcal{P}_{\\mathcal{G}_{C}^{k}}}{1-\\delta}\\left(\\frac{6c G}{\\widehat{C}}+p\\right)+\\tilde{D},}\\end{array}\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "where $\\widetilde B:=0$ and $\\tilde{D}:=0$ when ${\\widehat{C}}=n$ and $\\begin{array}{r}{\\widetilde{B}:=\\frac{\\mathcal{P}_{\\mathcal{G}_{\\widehat{C}}^{k}}G B}{(1-\\delta)\\widehat{C}}}\\end{array}$ and $\\begin{array}{r}{\\widetilde{D}:=\\frac{\\mathcal{P}_{\\mathcal{G}_{\\widehat{C}}^{k}}G}{(1-\\delta)\\widehat{C}}}\\end{array}$ when ${\\widehat{C}}<n$ and ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{P}_{\\mathcal{G}_{C}^{k}}=\\displaystyle\\frac{C}{n p_{G}}\\cdot\\sum_{\\scriptstyle(1-\\delta)C\\leq t\\leq C}\\left(\\left(\\begin{array}{l}{G-1}\\\\ {t-1}\\end{array}\\right)\\left(\\begin{array}{l}{n-G}\\\\ {C-t}\\end{array}\\right)\\left(\\left(\\begin{array}{l}{n}\\\\ {C}\\end{array}\\right)\\right)^{-1}\\right),}\\\\ &{\\quad p_{G}=\\mathrm{Prob}\\left\\{G_{C}^{k}\\geq\\left(1-\\delta\\right)C\\right\\}}\\\\ &{\\quad\\quad=\\displaystyle\\sum_{\\scriptstyle[\\left(1-\\delta\\right)C]\\leq t\\leq C}\\left(\\left(\\begin{array}{l}{G}\\\\ {t}\\end{array}\\right)\\left(\\begin{array}{l}{n-G}\\\\ {C-t}\\end{array}\\right)\\left(\\begin{array}{l}{n}\\\\ {C}\\end{array}\\right)^{-1}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "Thenfor all $K\\geq0$ the iterates produced by Byz-VR-MARINA (Algorithm $^{\\,l}$ )satisfy ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\Vert\\nabla f\\left(\\widehat{x}^{K}\\right)\\right\\Vert^{2}\\right]\\leq\\frac{2\\Phi^{0}}{\\gamma\\left(1-\\frac{4\\widehat{B}}{p}\\right)\\left(K+1\\right)}+\\frac{2\\widehat{D}\\zeta^{2}}{p-4\\widehat{B}},\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "Wwhere $\\widehat{x}^{K}$ ischosenuniformlyat randomfrom $x^{0},x^{1},\\ldots,x^{K}$ and $\\Phi^{0}\\ =\\ f\\left(x^{0}\\right)\\;-\\;f^{*}\\;+$ $\\begin{array}{r}{\\frac{\\gamma}{p}\\left\\|g^{0}-\\nabla f\\left(x^{0}\\right)\\right\\|^{2}}\\end{array}$ ", "page_idx": 55}, {"type": "text", "text": "Proof. The proof is analogous to the proof of Theorem D.14. ", "page_idx": 55}, {"type": "text", "text": "Theorem E.5. Let Assumptions 2.3, 2.4, D.1, D.2, D.3, D.4, D.5, 2.7 hold. Setting $\\lambda_{k+1}\\;=\\;$ $\\operatorname*{max}_{i,j}L_{i,j}\\left\\|x^{k+1}-x^{k}\\right\\|$ Assumethat ", "page_idx": 55}, {"type": "equation", "text": "$$\n0<\\gamma\\leq\\frac{1}{L+\\sqrt{2A}},\\quad8\\widehat{B}<p,\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "where ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{A=\\displaystyle\\frac{4}{p}\\left(\\frac{p_{G}\\mathcal{P}_{\\mathcal{G}_{C}^{k}}G}{C^{2}(1-\\delta)^{2}}\\omega+\\frac{8G\\mathcal{P}_{\\mathcal{G}_{C}^{k}}c\\delta}{(1-\\delta)\\widehat{C}}B+6\\widetilde{B}+\\frac{4}{p}(1-p_{G})+\\frac{8}{p}p_{G}\\frac{G\\mathcal{P}_{\\mathcal{G}_{C}^{k}}}{C(1-\\delta)}c\\delta\\omega\\right)L^{2}}\\\\ &{\\quad+\\displaystyle\\frac{4}{p}\\left(\\frac{p_{G}\\mathcal{P}_{\\mathcal{G}_{C}^{k}}G}{C^{2}(1-\\delta)^{2}}\\left(\\omega+1\\right)+\\frac{8}{p}p_{G}\\frac{G\\mathcal{P}_{\\mathcal{G}_{C}^{k}}}{C(1-\\delta)}c\\delta(\\omega+1)\\right)\\left(L_{\\pm}^{2}+\\frac{\\mathcal{L}_{\\pm}^{2}}{b}\\right)}\\\\ &{\\quad+\\displaystyle\\frac{16}{p^{2}}(1-p_{G})F_{\\mathcal{A}}^{2}\\left(D_{\\mathcal{Q}}\\operatorname*{max}_{i,j}\\Sigma_{i,j}\\right)^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 55}, {"type": "equation", "text": "$$\n\\widehat{B}=2\\frac{\\delta\\mathcal{P}_{\\mathcal{G}_{\\widehat{C}}^{k}}}{1-\\delta}B\\left(\\frac{12c G}{\\widehat{C}}+p\\right)+6\\widetilde{B},\\quad\\widehat{D}=2\\frac{\\delta\\mathcal{P}_{\\mathcal{G}_{\\widehat{C}}^{k}}}{1-\\delta}\\left(\\frac{6c G}{\\widehat{C}}+p\\right)+\\widetilde{D},\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "Wwhere $\\widetilde{B}:=0$ $\\tilde{D}:=0$ when ${\\widehat{C}}=n$ and $\\begin{array}{r}{\\widetilde{B}:=\\frac{\\mathcal{P}_{\\mathcal{G}_{\\widehat{C}}^{k}}G B}{(1-\\delta)\\widehat{C}}}\\end{array}$ $\\begin{array}{r}{\\tilde{D}:=\\frac{\\mathcal{P}_{\\mathcal{G}_{\\widehat{C}}^{k}}G}{(1-\\delta)\\widehat{C}}}\\end{array}$ when ${\\widehat{C}}<n,$ and where $p_{G}=\\mathrm{Prob}\\left\\{G_{C}^{k}\\geq(1-\\delta)C\\right\\}$ and $\\mathcal{P}_{\\mathcal{G}_{C}^{k}}=\\operatorname{Prob}\\left\\{i\\in\\mathcal{G}_{C}^{k}\\mid G_{C}^{k}\\geq\\left(1-\\delta\\right)C\\right\\}$ .Then for all $K\\geq0$ the iterates produced by Byz-VR-MARINA (Algorithm 1) satisfy ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\mathbb{E}\\left[f\\left(x^{K}\\right)-f\\left(x^{*}\\right)\\right]\\leq\\left(1-\\rho\\right)^{K}\\Phi^{0}+\\frac{2\\widehat{D}\\zeta^{2}}{p\\rho},}\\\\ {\\displaystyle\\,:=\\operatorname*{min}\\left[\\gamma\\mu\\left(1-\\frac{8\\widehat{B}}{p}\\right),\\frac{p}{4}\\right]a n d\\,\\Phi^{0}=f\\left(x^{0}\\right)-f^{*}+\\frac{2\\gamma}{p}\\left\\Vert g^{0}-\\nabla f\\left(x^{0}\\right)\\right\\Vert^{2}\\!.}\\end{array}\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "Proof. The proof is analogous to the proof of Theorem D.15. ", "page_idx": 56}, {"type": "text", "text": "E.3 On the Technical Non-Triviality of the Analysis ", "text_level": 1, "page_idx": 56}, {"type": "text", "text": "As we explain in the main part of the paper, the main reason why we propose to use clipping is to handle the situations when Byzantine workers form a majority during some communication rounds since the existing approaches are vulnerable to such scenarios. However, the introduction of the clipping does not come for free: if the clipping level is too small, clipping can create a noticeable bias to the updates. Because of this issue, existing works such as (Zhang et al., 2020b; Gorbunov et al., 2020) use non-trivial policies for the choice of the clipping level, and the analysis in these works differs significantly from the existing analysis for the methods without clipping. The analysis of Byz-VR-MARINA is based on the unbiasedness of vectors $\\mathcal{Q}(\\hat{\\Delta}_{i}(x^{k+1},x^{\\bar{k}}))$ , i.e., on the following iden $\\operatorname{tity}\\colon\\mathbb{E}[\\mathcal{Q}(\\hat{\\Delta}_{i}(x^{k+1},x^{k}))\\mid x^{k+1},x^{k}]=\\Delta_{i}(x^{k+1},x^{k})=\\nabla f_{i}(x^{k+1})-\\nabla f_{i}(x^{k}).$ Since $\\mathbb{E}[\\mathsf{c l i p}_{\\lambda_{k+1}}(\\mathcal{Q}(\\hat{\\Delta}_{i}(x^{k+1},x^{k})))\\mid x^{k+1},x^{k}]\\neq\\nabla f_{i}(x^{k+1})-\\nabla f_{i}(x^{k})$ in general, to analyze Byz-VR-MARINA-PP we also use a special choice of the clipping level: $\\lambda_{k+1}=\\alpha_{k+1}\\|x^{k+1}-x^{k}\\|$ To illustrate the main reasons for that, let us consider the case of uncompressed communication (Q(x) = a). In this setup, for large enough \u03b1+1 we have clip>1 $\\mathrm{clip}_{\\lambda_{k+1}}\\hat{\\Delta}_{i}(x^{\\bar{k+1}},x^{k})=\\hat{\\Delta}_{i}(x^{k+1},x^{k})$ for all $i\\in\\mathcal G$ (due to Assumption 2.6), which allows us using a similar proof to the one for Byz-VRMARINA when good workers form a majority in a round. Moreover, when Byzantine workers form a majority, our choice of the clipping level allows us to bound the second moment of the shift from the Byzantine workers as $\\sim\\|\\bar{x^{k+1}}-x^{k}\\|^{2}$ (see Lemmas D.9 and D.12), i.e., the second moment of the shift is of the same scale as the variance of $\\{g_{i}\\}_{i\\in\\mathcal{G}}$ , which goes to zero. Next, to properly analyze these two situations, we overcame another technical challenge related to the estimation of the conditional expectations and probabilities of corresponding events (see Lemmas D.9 and D.10 and formulas for $p_{G}$ and $\\mathcal{P}_{\\mathcal{G}_{C}^{k}}$ at the beginning of Section 4). In particular, the derivation of formula (24) is quite non-standard for stochastic optimization literature: there are two sources of stochasticity -- one comes from the sampling of clients, and the other one comes from the sampling of stochastic gradients and compression. This leads to the estimation of variance of the average of the random number of random vectors, which is novel on its own. In addition, when the compression operator is used, the analysis becomes even more involved since one cannot directly apply the main property of unbiased compression (Definition 2.2), and we use Lemma D.6 in the proof to address this issue. It is also worth mentioning that in contrast to Byz-VR-MARINA, our method does not require full participation even with a small probability $p$ . Instead, it is sufficient for Byz-VR-MARINA-PP to sample a large enough cohort of C clients with probability $p$ to ensure that Byzantine workers form a minority in such rounds. ", "page_idx": 56}, {"type": "text", "text": "In this section, we present a simplified version of Byz-VR-MARINA-PP called Byz-VR-MARINA$\\mathsf{P P+}$ (see Algorithm 3). The only difference between the two methods is in Line 10: Byz-VRMARINA $^+$ does not apply robust aggregation when $c_{k}=0$ and just averages the clipped vectors received from the set of clients $S_{k}$ . Nevertheless, when $c_{k}\\,=\\,1$ , i.e., a large cohort of clients is sampled, the method still uses robust aggregation. ", "page_idx": 57}, {"type": "text", "text": "Algorithm 3 Byz-VR-MARINA- $\\mathsf{P P+}$ : Simplified Byz-VR-MARINA-PP ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "1: Input: vectors $x^{0},g^{0}\\in\\mathbb{R}^{d}$ , stepsize $\\gamma$ , mini-batch size $b$ , probability $p\\,\\in\\,(0,1]$ , number of iterations $K$ $(\\delta,c)$ -ARAgg, clients sample size $1\\leq C\\leq\\widehat{C}\\leq n$ , liping coefcients $\\{\\alpha_{k}\\}_{k\\ge1}$   \n2: for $k=0,1,\\ldots,K-1$ do   \n3:  Get a sample from Bernoulli distribution with parameter $p$ $c_{k}\\sim\\mathsf{B e}(p)$   \n4: Sample the set of clients $S_{k}\\subseteq[n]$ $|S_{k}|=C$ \u8fd1 $c_{k}=0$ ; otherwise $|S_{k}|=\\widehat{C}$   \n5: Broadcast $g^{k}$ \uff0c $c_{k}$ to all workers   \n6: for $i\\in\\mathcal G\\cap S_{k}$ in parallel do   \n7: $x^{k+1}=x^{k}-\\gamma g^{k}$ and $\\lambda_{k+1}=\\alpha_{k+1}\\|x^{k+1}-x^{k}\\|$ $\\begin{array}{r}{g_{i}^{k+1}=\\left\\{\\underset{g^{k}+\\mathtt{c}}{\\nabla f}(x^{k+1}),\\right.}\\\\ {\\left.g^{k}+\\mathtt{c}1\\mathtt{i}\\mathtt{p}_{\\lambda_{k+1}}\\left(\\mathcal{Q}\\left(\\widehat{\\Delta}_{i}(x^{k+1},x^{k})\\right)\\right),\\right.}\\end{array}$ id $c_{k}=1$   \n8: Set g where $\\widehat{\\Delta}_{i}(x^{k+1},x^{k})$ is a mini-batched estimator of $\\nabla f_{i}(x^{k+1})-\\nabla f_{i}(x^{k}),\\,\\mathcal{Q}(\\cdot)$ for $i\\in$ $\\mathcal{G}\\cap S_{k}$ are computed independently   \n9: end for   \n10: $\\begin{array}{r l}&{\\mathbf{\\phi}^{\\mathrm{cnu\\:\\:no\\:}}}\\\\ &{g^{k+1}=\\left\\{\\begin{array}{l l}{\\displaystyle\\mathrm{ARAgg\\:}\\left(\\{g_{i}^{k+1}\\}_{i\\in S_{k}}\\right),}&{\\mathrm{if~}c_{k}=1,}\\\\ {g^{k}+\\frac{1}{C}\\displaystyle\\sum_{i\\in S_{k}}\\mathbf{clip}_{\\lambda_{k+1}}\\left(\\mathcal{Q}\\left(\\widehat{\\Delta}_{i}(x^{k+1},x^{k})\\right)\\right),}&{\\mathrm{otherwise}}\\end{array}\\right.}\\end{array}$ ", "page_idx": 57}, {"type": "text", "text": "11: end for ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "The key idea behind this modification can be explained as follows. For simplicity, let us assume that $C$ is small and $\\delta_{\\mathrm{real}}$ is also small. Then, for the communication rounds with $c_{k}=0$ , with a large probability, only good clients will be sampled. In this case, the method can use just an average of the received vectors and benefit from the lack of bias appearing due to the robust aggregation. Moreover, when $c_{k}=0$ and at least one of the sampled clients is Byzantine, the method will tolerate due to the clipping. That is, when $C$ is small, the method can potentially benefit from the lack of robust aggregation when $c_{k}=0$ . However, for the rounds with $c_{k}=1$ , in the worst case, ${\\widehat{C}}=n$ , meaning that all Byzantines workers are guaranteed to be sampled. To tolerate such situations, we keep the robust aggregation in the method when $c_{k}=1$ ", "page_idx": 57}, {"type": "text", "text": "F.1 Analysis for Bounded Compressors ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "For simplicity, we analyze Byz-VR-MARINA- $\\mathsf{P P+}$ for bounded compressors only. The analysis is very similar to the one we provide for Byz-VR-MARINA-PP, but several steps are significantly simpler. In particular, the central part in the analysis of Byz-VR-MARINA-PP is in deriving a good recursive inequality for $\\mathbb{E}[\\|g^{k^{\\ast}}-\\nabla f(x^{k})\\|^{2}]$ , which requires several quite technical steps. For Byz-VR-MARINA- $\\mathsf{P P+}$ , one can obtain a similar inequality much easier as shown in the next lemma. ", "page_idx": 57}, {"type": "text", "text": "Lemma F.1. Let Assumptions D.1, D.2, D.3, D.4, D.5, 2.4 hold and the compression operator satisfy Definition 2.2. Assume that $C\\leq G$ We set $\\lambda_{k+1}=D_{Q}\\operatorname*{max}_{i,j}L_{i,j}\\|x^{k+1}-x^{k}\\|$ Then for all $k\\geq0$ the iterates produced by Byz-VR-MARINA- $\\mathsf{P P+}$ (Algorithm 3) satisfy ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\mathbb{E}\\left[\\left\\|g^{k+1}-\\nabla f\\left(x^{k+1}\\right)\\right\\|^{2}\\right]\\leq\\left(1-\\displaystyle\\frac{p}{2}\\right)\\mathbb{E}\\left[\\left\\|g^{k}-\\nabla f\\left(x^{k}\\right)\\right\\|^{2}\\right]}\\\\ {\\quad\\displaystyle+\\,\\widehat{B}\\mathbb{E}\\left[\\left\\|\\nabla f\\left(x^{k}\\right)\\right\\|^{2}\\right]+\\widehat{D}\\zeta^{2}+\\frac{p A}{4}\\|x^{k+1}-x^{k}\\|^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "with ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{A=\\frac{4}{p}\\left(\\frac{8p B G\\mathcal{P}_{\\mathcal{G}_{\\widehat{C}}^{k}}c\\delta}{(1-\\delta)\\widehat{C}}+6p\\widetilde{B}+\\frac{(1-p)p_{\\mathcal{G}^{k}}^{k}\\omega}{C}+\\frac{6(1-p)(1-p_{\\mathcal{G}}^{k})}{p}\\right)L^{2}}}}\\\\ {{\\displaystyle{\\quad+\\,\\frac{4(1-p)p_{\\mathcal{G}}^{k}}{p}\\left(1+\\frac{\\omega}{C}\\right)L_{\\pm}^{2}+\\frac{4(1-p)p_{\\mathcal{G}}^{k}(1+\\omega)}{p C}\\frac{\\mathcal{L}_{\\pm}^{2}}{b}}}}\\\\ {{\\displaystyle{\\quad+\\,\\frac{24(1-p)(1-p_{\\mathcal{G}}^{k})}{p^{2}}\\left(D_{Q}\\operatorname*{max}_{i,j}\\right)^{2},}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 58}, {"type": "equation", "text": "$$\n\\widehat{B}=\\frac{8G\\mathcal{P}_{\\mathcal{G}_{\\widehat{C}}^{k}}c\\delta B p}{(1-\\delta)\\widehat{C}}+6p\\widetilde{B},\\quad\\widehat{D}=\\frac{4G\\mathcal{P}_{\\mathcal{G}_{\\widehat{C}}^{k}}c\\delta p}{(1-\\delta)\\widehat{C}}+p\\widetilde{D},\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "where $\\widetilde{B},\\,\\widetilde{D},\\,p_{\\mathcal{G}}^{k},\\,\\mathcal{P}_{\\mathcal{G}_{\\widehat{C}}^{k}}$ are defined in Lemma D.13. ", "page_idx": 58}, {"type": "text", "text": "Proof. From the update rule of $g^{k+1}$ , we have ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[\\left\\|g^{k+1}-\\nabla f(x^{k+1})\\right\\|^{2}\\right]=p\\,\\mathbb{E}\\left[\\left\\|\\mathsf{A R A g g}\\left(\\{g_{i}^{k+1}\\}_{i\\in S_{k}}\\right)-\\nabla f(x^{k+1})\\right\\|^{2}\\mid c_{k}=1\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 58}, {"type": "equation", "text": "$$\n+\\left(1-p\\right)\\mathbb{E}\\left[\\left\\Vert g^{k}+\\frac{1}{C}\\sum_{i\\in S_{k}}\\mathsf{c l i p}_{\\lambda_{k+1}}\\left(\\mathcal{Q}\\left(\\widehat{\\Delta}_{i}(x^{k+1},x^{k})\\right)\\right)-\\nabla f(x^{k+1})\\right\\Vert^{2}\\mid c_{k}=0\\right].\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "Next, we bound $T_{1}$ and $T_{2}$ separately. From Lemma D.10, we have ", "page_idx": 58}, {"type": "equation", "text": "$$\nT_{1}\\leq\\left(\\frac{8G\\mathcal{P}_{\\mathcal{G}_{\\widehat{C}}^{k}}c\\delta B}{(1-\\delta)\\widehat{C}}+2\\widetilde{B}\\right)\\mathbb{E}\\left[\\left\\Vert\\nabla f\\left(x^{k}\\right)\\right\\Vert^{2}+L^{2}\\left\\Vert x^{k+1}-x^{k}\\right\\Vert^{2}\\right]+\\frac{4G\\mathcal{P}_{\\mathcal{G}_{\\widehat{C}}^{k}}c\\delta\\zeta^{2}}{(1-\\delta)\\widehat{C}}+\\widetilde{\\zeta}^{2}.\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "Asfor $T_{2}$ , we consider two possible situations: either $S_{k}\\cap B=\\emptyset$ (no Byzantine workers are among sampled ones) or $S_{k}\\cap B\\neq\\emptyset$ (at least one Byzantine worker is sampled). Then, $T_{2}$ equals ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\Gamma_{2}=p_{Q}^{k}\\operatorname{\\mathbb{E}}\\left[\\left\\|g^{k}+{\\frac{1}{C}}\\sum_{i\\in S_{k}}\\operatorname{clip}_{\\lambda_{k+1}}\\left({\\mathcal{Q}}\\left({\\widehat{\\Delta}}_{i}(x^{k+1},x^{k})\\right)\\right)-\\nabla f(x^{k+1})\\right\\|^{2}\\mid c_{k}=0,S_{k}\\cap B={\\mathcal{Q}}\\right]\\right].\n$$", "text_format": "latex", "page_idx": 58}, {"type": "equation", "text": "$$\n+\\left(1-p_{Q}^{k}\\right)\\mathbb{E}\\left[\\left\\|g^{k}+\\frac{1}{C}\\sum_{i\\in S_{k}}c\\mathbf{1}\\mathbf{i}\\mathbf{p}_{\\lambda_{k+1}}\\left(\\mathcal{Q}\\left(\\widehat{\\Delta}_{i}(x^{k+1},x^{k})\\right)\\right)-\\nabla f(x^{k+1})\\right\\|^{2}\\mid c_{k}=0,S_{k}\\cap B\\neq\\emptyset,\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "where ", "page_idx": 58}, {"type": "equation", "text": "$$\np_{\\mathcal{G}}^{k}:=\\mathrm{Prob}\\{S_{k}\\cap B=\\emptyset\\mid c_{k}=0\\}=\\frac{{\\binom{G}{C}}}{{\\binom{n}{C}}}=\\frac{(G-C+1)(G-C+2)\\cdot\\ldots\\cdot(n-C)}{(G+1)(G+2)\\cdot\\ldots\\cdot n}.\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "The choice of the clipping level $\\lambda_{k+1}=D_{Q}\\operatorname*{max}_{i,j}L_{i,j}\\|x^{k+1}-x^{k}\\|$ and inequality (29) imply that $\\mathsf{c l i p}_{\\lambda_{k+1}}\\left(\\mathcal{Q}\\left(\\widehat{\\Delta}_{i}(x^{k+1},x^{k})\\right)\\right)$ for all $i\\in\\mathcal G$ . Therefore, for $\\widehat{T}_{2}$ ,we have ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widehat{T}_{2}=\\mathbb{E}\\left[\\left\\Vert g^{k}+\\frac{1}{C}\\sum_{i\\in S_{k}}Q\\left(\\widehat{\\Delta}_{i}(x^{k+1},x^{k})\\right)-\\nabla f(x^{k+1})\\right\\Vert^{2}\\mid c_{k}=0,S_{k}\\cap B=\\emptyset\\right]}\\\\ &{\\quad=\\mathbb{E}\\left[\\Vert g^{k}-\\nabla f(x^{k})\\Vert^{2}\\right]}\\\\ &{\\quad+\\mathbb{E}\\left[\\left\\Vert\\frac{1}{C}\\sum_{i\\in S_{k}}Q\\left(\\widehat{\\Delta}_{i}(x^{k+1},x^{k})\\right)-(\\nabla f(x^{k+1})-\\nabla f(x^{k}))\\right\\Vert^{2}\\mid c_{k}=0,S_{k}\\cap B=\\emptyset\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "where we use that $\\begin{array}{r c l}{\\mathbb{E}\\left[\\frac{1}{C}\\sum_{i\\in S_{k}}\\underline{{Q}}\\left(\\widehat{\\Delta}_{i}(x^{k+1},x^{k})\\right)\\mid c_{k}=0,S_{k}\\cap B=\\emptyset\\right]}&{=}&{\\nabla f(x^{k+1})\\ -\\ }\\end{array}$ $\\nabla f(x^{k})$ . Moreover, since $\\begin{array}{r l}{\\mathbb{E}\\left[\\frac{1}{C}\\sum_{i\\in S_{k}}\\mathcal{Q}\\left(\\widehat{\\Delta}_{i}(x^{k+1},x^{k})\\right)\\mid c_{k}=0,S_{k}\\cap B=\\emptyset,S_{k}\\right]}&{{}=}\\end{array}$ $\\begin{array}{r l}{\\frac{1}{C}\\sum_{i\\in S_{k}}(\\nabla f_{i}(x^{k+1})-\\nabla f_{i}(x^{k}))\\overset{\\cdot}{=}:\\frac{1}{C}\\sum_{i\\in S_{k}}\\Delta_{i}(x^{k+1},x^{k})}\\end{array}$ we can decompose the last term in the upper-bound for $\\widehat{T}_{2}$ as follows: ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\hat{T}_{2}=\\mathbb{E}\\left[\\|g^{k}-\\nabla f(x^{k})\\|^{2}\\right]}\\\\ {\\quad+\\mathbb{E}\\left[\\mathbb{E}\\left[\\left\\|\\frac{1}{C}\\sum_{i\\in S_{k}}\\left(\\mathcal{Q}\\left(\\widehat{\\Delta}_{i}(x^{k+1},x^{k})\\right)-\\Delta_{i}(x^{k+1},x^{k})\\right)\\right\\|^{2}\\mid S_{k}\\right]\\mid c_{k}=0,S_{k}\\cap B=\\mathcal{Q}\\right]}\\\\ {\\quad+\\mathbb{E}\\left[\\left\\|\\frac{1}{C}\\sum_{i\\in S_{k}}\\Delta_{i}(x^{k+1},x^{k})-(\\nabla f(x^{k+1})-\\nabla f(x^{k}))\\right\\|^{2}\\mid c_{k}=0,S_{k}\\cap B=\\mathcal{Q}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "Since the compression operator computations are independent on each client, we have ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\dot{E}_{j}=}&{\\mathbb{E}\\left[\\left\\|\\bar{\\mathbf{x}}^{k}-\\mathbf{Z}^{j}(x^{k})\\right\\|^{2}\\right]}\\\\ &{+\\frac{1}{C}\\mathbb{E}\\left[\\sum_{i=1}^{K}\\left[\\sum_{j=1}^{K}\\left[\\mathbb{I}\\left(\\Delta_{j}^{k+1,\\varepsilon}\\right)+\\Delta_{k}(x^{k+1,\\varepsilon})\\right]^{2}\\right]\\mid x\\right]\\mid=0,\\;S_{j}(x)\\mid t\\leq\\sigma\\right]}\\\\ &{+\\frac{C}{C}\\mathbb{E}\\left[\\left\\|\\sum_{i=1}^{K}\\sum_{j=1}^{K}\\left(x^{k+1,\\varepsilon}\\right)^{j}\\right]\\mid\\mid x\\right]=0,S_{j}(x)\\mid B=x\\right]-\\mathbb{E}\\left[\\big[\\big\\|\\nabla f(x^{k+1})-\\nabla f(x^{k})\\big\\|^{2}\\right]}\\\\ &{+\\mathbb{E}\\left[\\big\\|\\nabla f(x^{k})-\\nabla f(x^{k})\\big\\|^{2}\\right]}\\\\ &{+\\frac{C}{C}\\mathbb{E}\\left[\\sum_{i=1}^{K}\\bigg[\\sum_{j=1}^{K}\\left[\\mathbb{I}\\left(\\Delta_{j}^{k+1,\\varepsilon}\\right)+\\Delta_{k}(x^{k+1,\\varepsilon})\\right]^{2}\\right]\\mid x\\right]\\mid=0,\\;S_{j}(x)\\mid B=x\\right]}\\\\ &{+\\frac{C}{C}\\mathbb{E}\\left[\\bigg[\\sum_{i=1}^{K}\\bigg[\\sum_{j=1}^{K}\\bigg[\\Delta_{j}^{k+1,\\varepsilon}\\bigg]\\cdot\\Delta_{k}(x^{k+1,\\varepsilon})\\bigg]^{2}\\mid x\\bigg]\\mid=0,\\;S_{j}(x)\\mid B=x\\right]}\\\\ &{+\\frac{C}{C}\\mathbb{E}\\left[\\bigg[\\sum_{i=1}^{K}\\int_{0}^{\\infty}\\bigg[\\sum_{k=1}^{K}(-\\Delta_{k}(x^{k+1,\\varepsilon}))^{k}\\bigg]\\mid\\mid x\\mid=0,S_{j}(x)\\mid B=x\\right]}\\\\ &{+\\mathbb{E}\\left[\\bigg[\\sum_{i=1}^{K}\\sum_{j=1}^{K}(\\Delta_{j}^{k+1,\\varepsilon})\\cdot\\Delta_{k}^{j}\\bigg]^{2}\\mid x\\mid=0,S_{j}(x)\\mid B=x\\right]-\\mathbb{E}\\left[\\big[\\nabla f(x^{k+1,\\varepsilon})-\\nabla f(x^{k})\\big] \n$$$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left|E\\left[\\left|\\sum_{i=0}^{k}\\left|\\left(\\Delta_{j}(x^{i+1},x^{j})\\right)-\\Delta_{j}(x^{i+1},x^{j})\\right|^{2}\\right|\\;\\middle|x_{j}\\right]\\right]\\mid=0,\\;\\mathbb{E}\\left[\\left|\\sum_{i=1}^{k}\\left|\\sum_{l=1}^{l}\\alpha_{l}x\\right|\\right]}\\\\ &{\\quad+\\mathbb{E}\\left[\\left|\\sum_{i=1}^{k}\\left|\\sum_{l=1}^{l}\\left|\\sum_{l=k}^{l}\\left|\\left(\\Delta_{j}(x^{i+1},x^{j})\\right)\\right|^{2}\\right|x_{j}\\right]\\right|}\\\\ &{\\quad+\\mathbb{E}\\left[\\left|\\frac{1}{\\Delta x^{i+1}}\\sum_{i=1}^{k}\\alpha_{l}x^{i+1}\\right|^{2}\\right|^{2}\\;\\middle|x_{l}\\right|-\\alpha_{l}x\\right]\\mathbb{E}\\left[\\left|\\nabla_{l}(x^{i+1})-\\nabla_{l}(x^{i})\\right|^{2}\\right]}\\\\ &{\\quad+\\mathbb{E}\\left[\\left|\\frac{1}{\\Delta x^{i+1}}\\sum_{i=1}^{k}\\alpha_{l}x^{i+1}\\right|^{2}\\right|^{2}\\;\\middle|x_{l}\\right|-0,\\;\\mathbb{E}\\left[\\left|\\nabla_{l}(x^{i+1})-\\nabla_{l}(x^{i+1})\\right|^{2}\\right]}\\\\ &{\\quad+\\frac{2}{\\Delta x^{i}}\\mathbb{E}\\left[\\left|\\sum_{l=1}^{k}\\left|\\sum_{l=k}^{l}\\alpha_{l}x^{i+1}\\right|^{2}\\right|^{2}\\right|^{2}\\right|\\alpha\\right]-0,\\;\\mathbb{E}\\left[\\left|\\nabla_{l}(x^{i+1})-\\nabla_{l}(x^{i})\\right|^{2}\\right]}\\\\ &{\\quad+\\frac{2}{\\Delta x^{i}}\\mathbb{E}\\left[\\left|\\sum_{l=1}^{k}\\left|\\sum_{l=k}^{l}\\left|\\sum_{l=k}^{l}\\alpha_{l}x^{i+1}\\right|^{2}\\right|^{2}\\;\\middle|x_{l}\\right|=0,\\;\\mathbb{E}\\left[\\left|\\nabla_{l}(x^{i+1})-\\nabla_{l}(x^{i+1})\\right|^{2}\\right]}\\\\ &{\\quad+\\frac{2}{\\Delta x^{i}}\\mathbb{E}\\left[\\left\n$$", "text_format": "latex", "page_idx": 59}, {"type": "equation", "text": "", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "Using $\\begin{array}{r}{\\mathbb{E}\\left[\\left\\|\\widehat{\\Delta}_{i}(x^{k+1},x^{k})\\right\\|^{2}\\right]\\;=\\;\\mathbb{E}\\left[\\left\\|\\widehat{\\Delta}_{i}(x^{k+1},x^{k})-\\Delta_{i}(x^{k+1},x^{k})\\right\\|^{2}\\right]\\;+\\;\\mathbb{E}\\left[\\left\\|\\Delta_{i}(x^{k+1},x^{k})\\right\\|^{2}\\right],}\\end{array}$ we continue the derivation as follows: ", "page_idx": 60}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widehat{T}_{2}\\leq\\mathbb{E}\\left[\\|g^{k}-\\nabla f(x^{k})\\|^{2}\\right]+\\frac{1+\\omega}{C G}\\displaystyle\\sum_{i\\in\\mathcal{G}}\\mathbb{E}\\left[\\left\\|\\widehat{\\Delta}_{i}(x^{k+1},x^{k})-\\Delta_{i}(x^{k+1},x^{k})\\right\\|^{2}\\right]}\\\\ &{\\quad+\\left(1+\\frac{\\omega}{C}\\right)\\displaystyle\\frac{1}{G}\\sum_{i\\in\\mathcal{G}}\\mathbb{E}\\left[\\left\\|\\Delta_{i}(x^{k+1},x^{k})\\right\\|^{2}\\right]-\\mathbb{E}\\left[\\left\\|\\nabla f(x^{k+1})-\\nabla f(x^{k})\\right\\|^{2}\\right]}\\\\ &{\\overset{(21)}{\\leq}\\mathbb{E}\\left[\\|g^{k}-\\nabla f(x^{k})\\|^{2}\\right]+\\frac{(1+\\omega)C_{\\widehat{\\lambda}}^{2}}{b C}\\mathbb{E}\\left[\\|x^{k+1}-x^{k}\\|^{2}\\right]}\\\\ &{\\quad+\\left(1+\\frac{\\omega}{C}\\right)\\mathbb{E}\\left[\\displaystyle\\frac{1}{G}\\sum_{i\\in\\mathcal{G}}\\left\\|\\Delta_{i}(x^{k+1},x^{k})\\right\\|^{2}-\\left\\|\\nabla f(x^{k+1})-\\nabla f(x^{k})\\right\\|^{2}\\right]}\\\\ &{\\quad+\\frac{\\omega}{C}\\mathbb{E}\\left[\\|\\nabla f(x^{k+1})-\\nabla f(x^{k})\\|^{2}\\right]}\\\\ &{\\overset{(20)(10)}{\\leq}\\mathbb{E}\\left[\\|g^{k}-\\nabla f(x^{k})\\|^{2}\\right]+\\left(\\frac{\\omega}{C}L^{2}+\\left(1+\\frac{\\omega}{C}\\right)L_{\\widehat{\\lambda}}^{2}+\\frac{\\left(1+\\omega\\right)C_{\\widehat{\\lambda}}^{2}}{b C}\\right)\\mathbb{E}\\left[\\|x^{k+1}-x^{k}\\|^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "Next, we estimate $\\widetilde{T}_{2}$ using Young's inequality and the choice of the clipping level: ", "page_idx": 60}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widetilde{T}_{2}\\leq(1+\\beta)\\mathbb{E}\\left[\\left\\|g^{k}-\\nabla f(x^{k})\\right\\|^{2}\\right]+2(1+\\beta^{-1})\\mathbb{E}\\left[\\left\\|\\nabla f(x^{k+1})-\\nabla f(x^{k})\\right\\|^{2}\\right]}\\\\ &{\\quad+\\,2(1+\\beta^{-1})\\mathbb{E}\\left[\\left\\|\\frac{1}{C}\\sum_{i\\in S_{k}}\\mathbf{c}^{1}\\mathbf{i}\\mathbf{i}\\mathbf{p}_{\\lambda_{k+1}}\\left(\\mathcal{Q}\\left(\\widehat{\\Delta}_{i}(x^{k+1},x^{k})\\right)\\right)\\right\\|^{2}\\mid c_{k}=0,S_{k}\\cap B\\neq\\mathcal{Q}\\right]}\\\\ &{\\quad\\stackrel{(\\mathrm{B})}{\\leq}(1+\\beta)\\mathbb{E}\\left[\\left\\|g^{k}-\\nabla f(x^{k})\\right\\|^{2}\\right]+2(1+\\beta^{-1})\\left(L^{2}\\mathbb{E}\\left[\\|x^{k+1}-x^{k}\\|^{2}\\right]+\\mathbb{E}[\\lambda_{k+1}^{2}]\\right)}\\\\ &{\\quad=(1+\\beta)\\mathbb{E}\\left[\\left\\|g^{k}-\\nabla f(x^{k})\\right\\|^{2}\\right]+2(1+\\beta^{-1})\\left(L^{2}+D_{Q}^{2}\\underset{i,j}{\\operatorname*{max}}L_{i,j}^{2}\\right)\\mathbb{E}\\left[\\|x^{k+1}-x^{k}\\|^{2}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "where $\\beta>0$ will be specified later in the proof. Combining the derived upper bounds for $\\widehat{T}_{2}$ and $\\widetilde{T}_{2}$ weget ", "page_idx": 60}, {"type": "equation", "text": "$$\n\\begin{array}{l}{T_{2}\\leq\\left(p_{\\mathcal{G}}^{k}+(1-p_{\\mathcal{G}}^{k})(1+\\beta)\\right)\\mathbb{E}\\left[\\|g^{k}-\\nabla f(x^{k})\\|^{2}\\right]}\\\\ {\\quad+\\,p_{\\mathcal{G}}^{k}\\left(\\frac{\\omega}{C}L^{2}+\\left(1+\\frac{\\omega}{C}\\right)L_{\\pm}^{2}+\\frac{(1+\\omega)\\mathcal{L}_{\\pm}^{2}}{b C}\\right)\\mathbb{E}\\left[\\|x^{k+1}-x^{k}\\|^{2}\\right]}\\\\ {\\quad+\\,2(1-p_{\\mathcal{G}}^{k})(1+\\beta^{-1})\\left(L^{2}+D_{\\mathcal{G}}^{2}\\operatorname*{max}_{i,j}^{2}\\right)\\mathbb{E}\\left[\\|x^{k+1}-x^{k}\\|^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "Plugging the obtained bounds for $T_{1}$ and $T_{2}$ into (31), we obtain ", "page_idx": 60}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\mathbb{E}\\left[\\|g^{k+1}-\\nabla f(x^{k+1})\\|^{2}\\right]\\leq(1-p)\\left(p_{Q}^{k}+(1-p_{Q}^{k})(1+\\beta)\\right)\\mathbb{E}\\left[\\|g^{k}-\\nabla f(x^{k})\\|^{2}\\right]}\\\\ {\\quad+\\,p\\left(\\left(\\frac{8G\\mathcal{P}_{Q_{\\widehat{C}}^{k}}c\\delta B}{(1-\\delta)\\widehat{C}}+2\\widetilde{B}\\right)\\mathbb{E}\\left[\\left\\|\\nabla f\\left(x^{k}\\right)\\right\\|^{2}+L^{2}\\left\\|x^{k+1}-x^{k}\\right\\|^{2}\\right]+\\frac{4G\\mathcal{P}_{Q_{\\widehat{C}}^{k}}c\\delta\\zeta^{2}}{(1-\\delta)\\widehat{C}}+\\widetilde{\\zeta}^{2}\\right)}\\\\ {\\quad+\\,(1-p)p_{Q}^{k}\\left(\\frac{\\omega}{C}L^{2}+\\left(1+\\frac{\\omega}{C}\\right)L_{\\pm}^{2}+\\frac{(1+\\omega)\\mathcal{L}_{\\pm}^{2}}{b C}\\right)\\mathbb{E}\\left[\\|x^{k+1}-x^{k}\\|^{2}\\right]}\\\\ {\\quad+\\,2(1-p)(1-p_{\\mathcal{G}}^{k})(1+\\beta^{-1})\\left(L^{2}+D_{Q}^{2}\\operatorname*{max}_{i,j}^{L_{2}}\\right)\\mathbb{E}\\left[\\|x^{k+1}-x^{k}\\|^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "Taking ", "page_idx": 60}, {"type": "equation", "text": "$$\n\\beta:=\\left\\{\\begin{array}{l l}{\\frac{p}{2(1-p_{\\mathcal{G}}^{k})},}&{\\mathrm{if}\\:p_{\\mathcal{G}}^{k}<1,}\\\\ {1,}&{\\mathrm{if}\\:p_{\\mathcal{G}}^{k}=1,}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "we ensure that $\\begin{array}{r}{p_{\\mathcal{G}}^{k}+(1-p_{\\mathcal{G}}^{k})(1+\\beta)\\le1+\\frac{p}{2}}\\end{array}$ and $\\begin{array}{r l}{(1-p_{\\mathcal{G}}^{k})(1+\\beta^{-1})\\,\\le\\,\\frac{(1-p_{\\mathcal{G}}^{k})(p+2(1-p_{\\mathcal{G}}^{k}))}{p}\\,\\le\\,}&{{}}\\end{array}$ 3(1-) Using these inequalities and (1 - p)(1-)\u22641-, we simplify the upperbound for ", "page_idx": 60}, {"type": "text", "text": "$\\mathbb{E}\\left[\\|g^{k+1}-\\nabla f(x^{k+1})\\|^{2}\\right]$ as follows: ", "page_idx": 61}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\|g^{k+1}-\\nabla f(x^{k+1})\\|^{2}\\right]\\leq\\left(1-\\displaystyle\\frac{p}{2}\\right)\\mathbb{E}\\left[\\|g^{k}-\\nabla f(x^{k})\\|^{2}\\right]}\\\\ &{\\quad+\\,p\\left(\\left(\\frac{8G\\mathcal{P}_{\\mathcal{G}_{\\widehat{c}}^{k}}c\\delta B}{(1-\\delta)\\widehat{C}}+2\\widetilde{B}\\right)\\mathbb{E}\\left[\\left\\|\\nabla f\\left(x^{k}\\right)\\right\\|^{2}+L^{2}\\left\\|x^{k+1}-x^{k}\\right\\|^{2}\\right]+\\frac{4G\\mathcal{P}_{\\mathcal{G}_{\\widehat{c}}^{k}}c\\delta\\zeta^{2}}{(1-\\delta)\\widehat{C}}+\\widetilde{\\zeta}^{2}\\right)}\\\\ &{\\quad+\\left(1-p\\right)p_{\\mathcal{G}}^{k}\\left(\\frac{\\omega}{C}L^{2}+\\left(1+\\displaystyle\\frac{\\omega}{C}\\right)L_{\\pm}^{2}+\\frac{(1+\\omega)\\mathcal{L}_{\\pm}^{2}}{b C}\\right)\\mathbb{E}\\left[\\|x^{k+1}-x^{k}\\|^{2}\\right]}\\\\ &{\\quad+\\,\\displaystyle\\frac{6(1-p)(1-p_{\\mathcal{G}}^{k})}{p}\\left(L^{2}+D_{\\mathcal{G}}^{2}\\operatorname*{max}_{i,j}^{L}\\right)\\mathbb{E}\\left[\\|x^{k+1}-x^{k}\\|^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "Rearranging the terms, we get (35). ", "page_idx": 61}, {"type": "text", "text": "Then, similarly to the analysis of Byz-VR-MARINA, we get the following result ", "page_idx": 61}, {"type": "text", "text": "Theorem F.2, Let Assumptions D.1, D.2, D.3, D.4, D.5, 2.4 hold. Set\u5165k+1 $\\begin{array}{r l}{\\lambda_{k+1}}&{{}=}\\end{array}$ $\\begin{array}{r l}{\\operatorname*{max}_{i,j}L_{i,j}\\left\\|x^{k+1}-x^{k}\\right\\|}&{{}}\\end{array}$ Assumethat ", "page_idx": 61}, {"type": "equation", "text": "$$\n0<\\gamma\\leq\\frac{1}{L+\\sqrt{A}},\\quad4\\widehat{B}<p,\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "where ", "page_idx": 61}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{A=\\frac{4}{p}\\left(\\frac{8p B G\\mathcal{P}_{\\mathcal{G}_{\\mathcal{O}}^{k}}c\\delta}{(1-\\delta)\\widehat{C}}+6p\\widetilde{B}+\\frac{(1-p)p_{\\mathcal{G}^{k}}^{k}\\omega}{C}+\\frac{6(1-p)(1-p_{\\mathcal{G}}^{k})}{p}\\right)L^{2}}}}\\\\ {{\\displaystyle{\\quad+\\,\\frac{4(1-p)p_{\\mathcal{G}}^{k}}{p}\\left(1+\\frac{\\omega}{C}\\right)L_{\\pm}^{2}+\\frac{4(1-p)p_{\\mathcal{G}}^{k}(1+\\omega)}{p C}\\frac{\\mathcal{L}_{\\pm}^{2}}{b}}}}\\\\ {{\\displaystyle{\\quad+\\,\\frac{24(1-p)(1-p_{\\mathcal{G}}^{k})}{p^{2}}\\left(D_{Q}\\operatorname*{max}_{i,j}\\right)^{2},}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 61}, {"type": "equation", "text": "$$\n\\widehat{B}=\\frac{8G\\mathcal{P}_{\\mathcal{G}_{\\widehat{C}}^{k}}c\\delta B p}{(1-\\delta)\\widehat{C}}+6p\\widetilde{B},\\quad\\widehat{D}=\\frac{4G\\mathcal{P}_{\\mathcal{G}_{\\widehat{C}}^{k}}c\\delta p}{(1-\\delta)\\widehat{C}}+p\\widetilde{D},\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "and ", "page_idx": 61}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathcal{P}_{\\mathcal{G}_{C}^{k}}=\\frac{C}{n p_{G}}\\cdot\\sum_{\\substack{(1-\\delta)C\\leq t\\leq C}}\\left(\\left(\\begin{array}{l}{G-1}\\\\ {t-1}\\end{array}\\right)\\left(\\begin{array}{l}{n-G}\\\\ {C-t}\\end{array}\\right)\\left(\\left(\\begin{array}{l}{n}\\\\ {C}\\end{array}\\right)\\right)^{-1}\\right),}\\\\ {\\displaystyle p_{\\mathcal{G}}^{k}=\\mathrm{Prob}\\{S_{k}\\cap B=\\mathcal{O}\\mid c_{k}=0\\}=\\frac{(G-C+1)(G-C+2)\\cdot\\ldots\\cdot(n-C)}{(G+1)(G+2)\\cdot\\ldots\\cdot n}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "Then for all $K\\geq0$ the iterates produced by Byz-VR-MARINA $^+$ (Algorithm 3) satisfy ", "page_idx": 61}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\Vert\\nabla f\\left(\\widehat{x}^{K}\\right)\\right\\Vert^{2}\\right]\\leq\\frac{2\\Phi^{0}}{\\gamma\\left(1-\\frac{4\\widehat{B}}{p}\\right)\\left(K+1\\right)}+\\frac{2\\widehat{D}\\zeta^{2}}{p-4\\widehat{B}},\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "Where $\\widehat{x}^{K}$ ischosenuniformly atrandomfrom $x^{0},x^{1},\\ldots,x^{K}$ and $\\Phi^{0}\\ =\\ f\\left(x^{0}\\right)\\;-\\;f^{*}\\;+$ $\\begin{array}{r}{\\frac{\\gamma}{p}\\left\\|g^{0}-\\nabla f\\left(x^{0}\\right)\\right\\|^{2}}\\end{array}$ ", "page_idx": 61}, {"type": "text", "text": "Proof. The proof is analogous to the proof of Theorem D.14. ", "page_idx": 61}, {"type": "text", "text": "Theorem E3. Let Assumptions 2.4, D.1, D.2, D.3, D.4, D.5, 2.7 hold. Set $\\begin{array}{r l}{\\lambda_{k+1}}&{{}=}\\end{array}$ $\\begin{array}{r l}{\\operatorname*{max}_{i,j}L_{i,j}\\left\\|x^{k+1}-x^{k}\\right\\|}&{{}}\\end{array}$ Assumethat ", "page_idx": 61}, {"type": "equation", "text": "$$\n0<\\gamma\\leq\\operatorname*{min}\\left\\{\\frac{1}{L+\\sqrt{2A}}\\right\\},\\quad8\\widehat{B}<p,\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "where ", "page_idx": 62}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{A=\\frac{4}{p}\\left(\\frac{8p B G\\mathcal{P}_{\\mathcal{G}_{\\widehat{C}}^{k}}c\\delta}{(1-\\delta)\\widehat{C}}+6p\\widetilde{B}+\\frac{(1-p)p_{\\mathcal{G}^{k}}^{k}\\omega}{C}+\\frac{6(1-p)(1-p_{\\mathcal{G}}^{k})}{p}\\right)L^{2}}}}\\\\ {{\\displaystyle{\\quad+\\,\\frac{4(1-p)p_{\\mathcal{G}}^{k}}{p}\\left(1+\\frac{\\omega}{C}\\right)L_{\\pm}^{2}+\\frac{4(1-p)p_{\\mathcal{G}}^{k}(1+\\omega)}{p C}\\frac{\\mathcal{L}_{\\pm}^{2}}{b}}}}\\\\ {{\\displaystyle{\\quad+\\,\\frac{24(1-p)(1-p_{\\mathcal{G}}^{k})}{p^{2}}\\left(D_{Q}\\operatorname*{max}_{i,j}\\right)^{2},}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 62}, {"type": "equation", "text": "$$\n\\widehat{B}=\\frac{8G\\mathcal{P}_{\\mathcal{G}_{\\widehat{C}}^{k}}c\\delta B p}{(1-\\delta)\\widehat{C}}+6p\\widetilde{B},\\quad\\widehat{D}=\\frac{4G\\mathcal{P}_{\\mathcal{G}_{\\widehat{C}}^{k}}c\\delta p}{(1-\\delta)\\widehat{C}}+p\\widetilde{D},\n$$", "text_format": "latex", "page_idx": 62}, {"type": "text", "text": "and ", "page_idx": 62}, {"type": "equation", "text": "$$\n\\mathcal{P}_{\\mathcal{G}_{C}^{k}}=\\frac{C}{n p_{G}}\\cdot\\sum_{(1-\\delta)C\\leq t\\leq C}\\left(\\left(\\begin{array}{l}{G-1}\\\\ {t-1}\\end{array}\\right)\\left(\\begin{array}{l}{n-G}\\\\ {C-t}\\end{array}\\right)\\left(\\left(\\begin{array}{l}{n}\\\\ {C}\\end{array}\\right)\\right)^{-1}\\right),\n$$", "text_format": "latex", "page_idx": 62}, {"type": "equation", "text": "$$\np_{\\mathcal{G}}^{k}=\\mathrm{Prob}\\{S_{k}\\cap B=\\emptyset\\mid c_{k}=0\\}=\\frac{(G-C+1)(G-C+2)\\cdot\\ldots\\cdot(n-C)}{(G+1)(G+2)\\cdot\\ldots\\cdot n}.\n$$", "text_format": "latex", "page_idx": 62}, {"type": "text", "text": "Thenfor all $K\\geq0$ the iterates produced by Byz-VR-MARINA $^+$ (Algorithm $^3$ )satisfy ", "page_idx": 62}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\mathbb{E}\\left[f\\left(x^{K}\\right)-f\\left(x^{*}\\right)\\right]\\leq\\left(1-\\rho\\right)^{K}\\Phi^{0}+\\frac{2\\widehat{D}\\zeta^{2}}{p\\rho},}\\\\ {\\rho=\\operatorname*{min}\\left[\\gamma\\mu\\left(1-\\frac{8\\widehat{B}}{p}\\right),\\frac{p}{4}\\right]a n d\\,\\Phi^{0}=f\\left(x^{0}\\right)-f^{*}+\\frac{2\\gamma}{p}\\left\\Vert g^{0}-\\nabla f\\left(x^{0}\\right)\\right\\Vert^{2}.}\\end{array}\n$$where ", "text_format": "latex", "page_idx": 62}, {"type": "text", "text": "Proof. The proof is analogous to the proof of Theorem D.15 ", "page_idx": 62}, {"type": "text", "text": "F.2  Discussion of the Results ", "text_level": 1, "page_idx": 62}, {"type": "text", "text": "Improved neighborhood term and bound on $\\delta$ The key property of Byz-VR-MAR $\\Vvdash\\Delta+$ is its better neighborhood terms, and maximal allowed fraction of Byzantine workers $\\delta$ in comparison to Byz-VR-MARINA. To illustrate it, consider the non-PLsetting (the discussion for the $\\mathrm{PE}$ case is similar) For both algorithms, the neighborhood term in the convergence bounds quals $\\mathcal{O}\\left(\\frac{\\widehat{D}\\zeta^{2}}{p\\!-\\!4\\widehat{B}}\\right)$ but corresponding constants $\\widehat{B}$ and $\\widehat{D}$ are different: ", "page_idx": 62}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\widehat{B}=2\\frac{\\delta\\mathcal{P}_{\\mathcal{G}_{C}^{k}}}{1-\\delta}B\\left(\\frac{12c G}{\\widehat{C}}+p\\right)+6\\widetilde{B}}&{\\mathrm{and}}&{\\widehat{D}=2\\frac{\\delta\\mathcal{P}_{\\mathcal{G}_{C}^{k}}}{1-\\delta}\\left(\\frac{6c G}{\\widehat{C}}+p\\right)+\\widetilde{D}\\}&{\\mathrm{for}\\quad\\mathsf{B y z-\\nabla}\\mathsf{R}\\mathsf{1}\\mathsf{M}\\mathsf{A}}\\\\ {\\widehat{B}=\\frac{8G\\mathcal{P}_{\\mathcal{G}_{C}^{k}}c\\delta B p}{(1-\\delta)\\widehat{C}}+6p\\widetilde{B}}&{\\mathrm{and}}&{\\widehat{D}=\\frac{4G\\mathcal{P}_{\\mathcal{G}_{C}^{k}}c\\delta p}{(1-\\delta)\\widehat{C}}+p\\widetilde{D}\\quad\\mathrm{for}\\quad\\mathsf{B y z-\\nabla}\\mathsf{R}\\mathsf{1}\\mathsf{A}\\mathsf{R}\\mathsf{I}\\mathsf{N}\\mathsf{A}+.}\\end{array}\n$$", "text_format": "latex", "page_idx": 62}, {"type": "text", "text": "For the simplicity of the comparison, consider the case of ${\\widehat{C}}=n$ Then, $\\mathcal{P}_{\\mathcal{G}_{\\widehat{C}}^{k}}=1$ and ", "page_idx": 62}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\begin{array}{c c l l l}{\\widehat{B}=\\Theta\\left(c\\delta B\\right)}&{\\mathrm{and}}&{\\widehat{D}=\\Theta(c\\delta)}&{\\mathrm{for}}&{\\mathsf{B y z-V R-M A R I N A},}\\\\ {\\widehat{B}=\\Theta\\left(c\\delta B p\\right)}&{\\mathrm{and}}&{\\widehat{D}=\\Theta(c\\delta p)}&{\\mathrm{for}}&{\\mathsf{B y z-V R-M A R I N A},}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 62}, {"type": "text", "text": "implying that the neighborhood term for Byz-VR-MARINA $^+$ is $^1\\!/\\!p$ times smaller than the neighborhood term for Byz-VR-MARINA. Moreover, the restriction $4\\widehat{B}<p$ used in the analysis of both methods implies ", "page_idx": 62}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{c\\delta=\\mathcal{O}\\left(\\frac{1}{B p}\\right)}&{{}\\mathrm{for}}&{\\mathsf{B y z-V}\\mathsf{R-M A R l N A},}\\\\ {c\\delta=\\mathcal{O}\\left(\\frac{1}{B}\\right)}&{{}\\mathrm{for}}&{\\mathsf{B y z-V}\\mathsf{R-M A R l N A}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 62}, {"type": "text", "text": "i.e., the result for Byz-VR-MARINA $^+$ allows $\\left(1/{_{p}}\\right)$ -times more Byzantine workers when $B>0$ We emphasize that the neighborhood term and the bound on $\\delta$ in the results for Byz-VR-MARINA $^{+}$ cannot be improved up to the numerical factors (Allouah et al., 2024b). ", "page_idx": 62}, {"type": "text", "text": "Comparison of stepsizes when ${\\widehat{C}}\\,=\\,n$ and $C\\,=\\,1$ . For simplicity, to compare the stepsize restrictions for Byz-VR-MARINA and Byz-VR-MARINA+, we consider the case when C = n and $C=1$ . Moreover, let us assume that $b=1$ and let us ignore the differences between smoothness constants and replace them with their upper bound $\\mathcal{L}$ from Assumption 2.6. Then, for both methods, the results in the non-PLsetting (the discussion for the $\\mathrm{PE}$ case is similar) with $B\\,=\\,0$ hold for $0<\\gamma\\leq1/\\mathcal{L}(1{+}\\sqrt{A})$ ,where ", "page_idx": 63}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{A=\\Theta\\left(\\frac{1}{p}\\left(1+\\omega+\\frac{(1+\\omega)c\\delta}{p}\\right)+\\frac{\\delta_{\\mathrm{real}}(1+F_{A}^{2}D_{Q}^{2})}{p^{2}}\\right)\\quad\\mathrm{for}\\quad\\mathsf{B y z\\ \u2013\\nabla\\mathsf{R}-\\mathsf{M}\\mathsf{A}\\mathsf{R}\\mathsf{I N A}},}\\\\ &{A=\\Theta\\left(\\frac{1+\\omega}{p}+\\frac{\\delta_{\\mathrm{real}}D_{Q}^{2}}{p^{2}}\\right)\\quad\\mathrm{for}\\quad\\mathsf{B y z\\ \u2013\\nabla\\mathsf{R}\\cdot\\mathsf{M}\\mathsf{A}\\mathsf{R}\\mathsf{I N A}}+,}\\end{array}\n$$", "text_format": "latex", "page_idx": 63}, {"type": "text", "text": "where we use $p_{G}\\,=\\,G/_{n}\\,=\\,1\\,-\\,\\delta_{\\mathrm{real}},\\,\\mathcal{P}_{\\mathcal{G}_{C}^{k}}\\,=\\,^{1}/G,\\,p_{\\mathcal{G}}^{k}\\,=\\,G/_{n}\\,=\\,1\\,-\\,\\delta_{\\mathrm{real}}$ . That is, the result for Byz-VR-MAR $\\Vvdash\\Delta+$ allows to use larger stepsizes than in Byz-VR-MARINA (though the methods are equivalent when $C=1$ ). A similar comparison holds for small enough $C$ as well. Therefore, we recommend using Byz-VR-MARINA+ instead of Byz-VR-MARINA when $C$ is small. We also highlight that the result for Byz-VR-MARINA $^{+}$ does not require Assumption 2.3. ", "page_idx": 63}, {"type": "text", "text": "G Analysis without Full-Batch Gradient Computations ", "text_level": 1, "page_idx": 64}, {"type": "text", "text": "In this section, we consider versions of Byz-VR-MARINA-PP and Byz-VR-MARINA- $\\mathsf{P P+}$ that do not use full-batch gradient computations at all - see Algorithms 4 and 5. These variants of Byz-VR-MARINA-PP and Byz-VR-MARINA- $\\mathsf{P P+}$ use $b^{\\prime}$ -size mini-batched estimator $\\widetilde{\\nabla}f_{i}(x^{k+1})$ when $c_{k}=1$ for every $i\\in\\mathcal{G}\\cap S_{k}$ in line 8 and are identical to their original versions in all other steps/computations. This modification reduces the computation cost of iterations when $c_{k}\\,=\\,1$ making the methods more practical. ", "page_idx": 64}, {"type": "text", "text": "Algorithm 4 Byz-VR-MARINA-PP without full-batch gradient computations ", "text_level": 1, "page_idx": 64}, {"type": "text", "text": "1: Input: vectors $x^{0},g^{0}\\ \\in\\ \\mathbb R^{d}$ , stepsize $\\gamma$ , mini-batch size $b$ , mini-batch size $b^{\\prime}$ , probability $p\\in(0,1]$ , number of iterations $K$ $(\\delta,c)$ -ARAgg, clients\u2032 sample size $1\\leq C\\leq\\widehat{C}\\leq n$ clipping coefficients $\\{\\alpha_{k}\\}_{k\\ge1}$   \n2: for $k=0,1,\\ldots,\\bar{K}-1$ do   \n3: Get a sample from Bernoulli distribution with parameter $p$ $c_{k}\\sim\\mathsf{B e}(p)$   \n4: Sample the set of clients $S_{k}\\subseteq[n]$ $|S_{k}|=C$ $c_{k}=0$ ; otherwise $|S_{k}|=\\widehat{C}$   \n5: Broadcast $g^{k}$ $c_{k}$ to all workers   \n6: for $i\\in\\mathcal G\\cap S_{k}$ in parallel do   \n7: $\\begin{array}{r l}&{^{\\mathrm{+1}}=x^{k}-\\gamma g^{k}\\mathrm{~and~}\\lambda_{k+1}=\\alpha_{k+1}\\|x^{k+1}-x^{k}\\|}\\\\ &{g_{i}^{k+1}=\\left\\{\\overset{\\widetilde{\\nabla}}{\\nabla}f_{i}(x^{k+1}),\\right.\\quad}\\\\ &{g_{i}^{k}\\quad=\\left\\{\\overset{\\quad}{g^{k}}+\\mathtt{c l i p}_{\\lambda_{k+1}}\\left(\\mathcal{Q}\\left(\\widehat{\\Delta}_{i}(x^{k+1},x^{k})\\right)\\right),\\quad\\mathrm{otherwise},}\\end{array}$   \n8: Set gi where $\\widetilde{\\nabla}f_{i}(x^{k+1})$ is a $b^{\\prime}$ -size mini-batched estimator of $\\nabla f_{i}(x^{k+1})$ \uff0c $\\widehat{\\Delta}_{i}(x^{k+1},x^{k})$ is a $b$ size mini-batched estimator of $\\nabla f_{i}(x^{k+1})-\\nabla f_{i}(x^{k}),\\mathcal{Q}(\\cdot)$ for $i\\in\\mathcal G\\cap S_{k}$ are computed independently   \n9: end for   \n10: $\\begin{array}{r l}&{\\mathrm{e}^{\\mathrm{ans-wx}}}\\\\ &{g^{k+1}=\\left\\{\\!\\!\\!\\begin{array}{l r}{\\mathrm{ARAgg}\\left(\\{g_{i}^{k+1}\\}_{i\\in S_{k}}\\right),}&{\\mathrm{if~}c_{k}=1,}\\\\ {g^{k}+\\mathrm{ARAgg}\\left(\\left\\{\\mathrm{c1}\\mathrm{i}\\mathrm{p}_{\\lambda_{k+1}}\\left(\\mathcal{Q}\\left(\\widehat{\\Delta}_{i}(x^{k+1},x^{k})\\right)\\right)\\right\\}_{i\\in S_{k}}\\right),}&{\\mathrm{otherwise}}\\end{array}\\!\\!\\right.}\\\\ &{\\leq.}\\end{array}$ ", "page_idx": 64}, {"type": "text", "text": "11: end for ", "text_level": 1, "page_idx": 64}, {"type": "text", "text": "Algorithm 5 Byz-VR-MARINA-PP+: without full-batch gradient computations ", "text_level": 1, "page_idx": 64}, {"type": "text", "text": "1: Input: vectors $x^{0},g^{0}\\ \\in\\ \\mathbb R^{d}$ \uff0cstepsize $\\gamma$ ,mini-batch size $b$ ,mini-batch size $b^{\\prime}$ , probability $p\\in(0,1]$ , number of iterations $K$ $(\\delta,c)$ -ARAgg, clients sample size $1\\leq C\\leq\\widehat{C}\\leq n$ , clipping coefficients $\\{\\alpha_{k}\\}_{k\\ge1}$   \n2: for $k=0,1,\\ldots,K-1$ do   \n3: Get a sample from Bernoulli distribution with parameter $p$ $c_{k}\\sim\\mathsf{B e}(p)$   \n4: Sample the set of clients $S_{k}\\subseteq[n]$ $|S_{k}|=C$ if $c_{k}=0$ ; otherwise $|S_{k}|=\\widehat{C}$   \n5: Broadcast $g^{k}$ $c_{k}$ to all workers   \n6: for $i\\in\\mathcal G\\cap S_{k}$ in parallel do   \n7: $x^{k+1}=x^{k}-\\bar{\\gamma}g^{k}$ and $\\lambda_{k+1}=\\alpha_{k+1}\\|x^{k+1}-x^{k}\\|$ $g_{i}^{k+1}=\\left\\{\\widetilde{\\nabla}f_{i}(x^{k+1}),\\right.$ $c_{k}=1$   \n8: Set $\\begin{array}{r}{g_{i}^{\\star\\top\\bot}=\\Big\\{g^{k}+\\mathsf{c l i p}_{\\lambda_{k+1}}\\left(\\mathcal{Q}\\left(\\widehat{\\Delta}_{i}(x^{k+1},x^{k})\\right)\\right),}\\end{array}$ otherwise, where $\\widetilde{\\nabla}f_{i}(x^{k+1})$ is a $b^{\\prime}$ -size mini-batched estimator of $\\nabla f_{i}(x^{k+1})$ \uff0c $\\widehat{\\Delta}_{i}(x^{k+1},x^{k})$ is a $b$ -size mini-batched estimator of $\\nabla f_{i}(x^{k+1})-\\nabla f_{i}(x^{k}),\\mathcal{Q}(\\cdot)$ for $i\\in\\mathcal G\\cap S_{k}$ are computed independently   \n9: end for   \n10: $g^{k+1}=\\left\\{\\!\\!\\begin{array}{l l}{\\displaystyle\\mathtt{A R A g g}\\left(\\{g_{i}^{k+1}\\}_{i\\in S_{k}}\\right),}&{\\mathrm{if~}c_{k}=1,}\\\\ {g^{k}+\\frac{1}{C}\\displaystyle\\sum_{i\\in S_{k}}\\mathtt{c l i p}_{\\lambda_{k+1}}\\left(\\mathcal{Q}\\left(\\widehat{\\Delta}_{i}(x^{k+1},x^{k})\\right)\\right),}&{\\mathrm{otherwise}}\\end{array}\\!\\!\\right.$ ", "page_idx": 64}, {"type": "text", "text": "11: end for ", "text_level": 1, "page_idx": 64}, {"type": "text", "text": "However, our analysis of Byz-VR-MARINA-PP/Byz-VR-MARINA- $\\mathsf{P P+}$ without full-batch gradient computations requires the following additional assumption. ", "page_idx": 64}, {"type": "text", "text": "Assumption G.1. We assume that there exist $\\sigma\\geq0$ such that for all $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ and $i\\in[n]$ ", "page_idx": 65}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\Vert\\widetilde{\\nabla}f_{i}(x)-\\nabla f_{i}(x)\\Vert^{2}\\right]\\le\\frac{\\sigma^{2}}{b^{\\prime}},\n$$", "text_format": "latex", "page_idx": 65}, {"type": "text", "text": "where $\\widetilde{\\nabla}f_{i}(x)$ is an unbiased $b^{\\prime}$ -size mini-batched estimator of $\\nabla f_{i}(x)$ ", "page_idx": 65}, {"type": "text", "text": "In particular, when $\\begin{array}{r}{\\frac{1}{m}\\sum_{j=1}^{m}\\|\\nabla f_{i,j}(x)\\!-\\!\\nabla f_{i}(x)\\|^{2}\\leq\\sigma}\\end{array}$ which is a standard assumption for variancereduced methods without full-batch gradient computations (Cutkosky and Orabona, 2019; Li et al., 2021; Gorbunov et al., 2021), estimator $\\begin{array}{r}{\\widetilde{\\nabla}f_{i}(\\boldsymbol{x})=\\frac{1}{b^{\\prime}}\\sum_{j=1}^{b^{\\prime}}\\nabla f_{i,\\xi_{i}^{j}}(\\boldsymbol{x})}\\end{array}$ with $\\{\\xi_{i}^{j}\\}_{i\\in[n],j\\in[m]}$ being i.i.d. samples from the uniform distribution over $[m]$ satisfies (32). Assumption G.1 is also standard for general stochastic optimization (Nemirovski et al., 2009; Ghadimi and Lan, 2013). ", "page_idx": 65}, {"type": "text", "text": "G.1 New Lemma ", "text_level": 1, "page_idx": 65}, {"type": "text", "text": "The main change in the analysis is related to Lemma D.10 since it is the only lemma that relies on the full-batch gradient computation. Nevertheless, it can be easily generalized to the case of Algorithms 4 and 5, as shown in the next result. ", "page_idx": 65}, {"type": "text", "text": "Lemma G.2. Let Assumptions D.1, D.5, G.1 hold and Aggregation Operator $(A R A g g)$ satisfy Definition 2.1. Then for all $k\\geq0$ the iterates produced byByz-VR-MARINA-PP/Byz-VR-MARINA$\\mathsf{P P+}$ (Algorithms $^{4}$ and $^{5}$ ) satisfy ", "page_idx": 65}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{T_{1}=\\mathbb{E}\\left[\\mathbb{E}_{k}\\left[\\left\\|A R A g g\\left(\\{g_{i}^{k+1}\\}_{i\\in S_{k}}\\right)-\\nabla f(x^{k+1})\\right\\|^{2}\\right]\\mid[1]\\right]}\\\\ &{\\quad\\leq\\left(\\frac{8G\\mathcal{P}_{\\mathcal{G}_{\\widehat{C}}^{k}}c\\delta B}{(1-\\delta)\\widehat{C}}+2\\widetilde{B}\\right)\\mathbb{E}\\left[\\left\\|\\nabla f\\left(x^{k}\\right)\\right\\|^{2}+L^{2}\\left\\|x^{k+1}-x^{k}\\right\\|^{2}\\right]}\\\\ &{\\quad\\quad+\\frac{4G\\mathcal{P}_{\\mathcal{G}_{\\widehat{C}}^{k}}c\\delta B}{(1-\\delta)\\widehat{C}}\\zeta^{2}+\\widetilde{\\zeta}^{2}+\\left(\\frac{\\mathcal{P}_{\\mathcal{G}_{\\widehat{C}}^{k}}G}{(1-\\delta)^{2}\\widehat{C}^{2}}+4c\\delta\\right)\\frac{\\sigma^{2}}{b^{\\prime}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 65}, {"type": "text", "text": "where $\\widetilde B:=0$ and $\\widetilde{\\zeta}^{2}:=0$ when ${\\widehat{C}}=n$ and $\\begin{array}{r}{\\widetilde{B}:=\\frac{\\mathcal{P}_{\\mathcal{G}_{\\widehat{C}}^{k}}G B}{(1-\\delta)\\widehat{C}}}\\end{array}$ and2: $\\begin{array}{r}{\\widetilde{\\zeta}^{2}:=\\frac{\\mathcal{P}_{\\mathcal{G}_{\\widehat{C}}^{k}}G\\zeta^{2}}{(1-\\delta)\\widehat{C}}}\\end{array}$ when ${\\widehat{C}}<n$ ", "page_idx": 65}, {"type": "text", "text": "Proof. Using the definition of aggregation operator, we have ", "page_idx": 65}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{T_{1}=\\mathbb{E}\\left[\\mathbb{E}_{k}\\left[\\left\\|\\mathbf{A}\\mathbf{RAgg}\\left(\\{g_{i}^{k+1}\\}_{i\\in S_{k}}\\right)-\\nabla f(x^{k+1})\\right\\|^{2}\\right]\\mid[1]\\right]}\\\\ &{\\quad\\overset{(12)}{\\leq}\\mathbb{E}\\left[\\mathbb{E}_{k}\\left[\\left\\|\\mathbf{A}\\mathbf{RAgg}\\left(\\{g_{i}^{k+1}\\}_{i\\in S_{k}}\\right)-\\frac{1}{G_{\\widehat{C}}^{k}}\\sum_{i\\in\\mathcal{G}_{\\widehat{C}}^{k}}\\widetilde{\\mathbf{v}}f_{i}(x^{k+1})\\right\\|^{2}\\right]\\mid[1]\\right]}\\\\ &{\\quad\\quad+\\mathbb{E}\\left[\\mathbb{E}_{k}\\left[\\left\\|\\frac{1}{G_{\\widehat{C}}^{k}}\\sum_{i\\in\\mathcal{G}_{\\widehat{C}}^{k}}\\widetilde{\\mathbf{v}}f_{i}(x^{k+1})-\\nabla f(x^{k+1})\\right\\|^{2}\\right]\\mid[1]\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 65}, {"type": "text", "text": "To proceed, we estimate the second term in the right-hand side of the above inequality first. From variance decomposition, we have ", "page_idx": 65}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\mathbb{E}_{k}\\left[\\left\\|\\frac{1}{G_{\\widehat{C}}^{k}}\\sum_{i\\in\\mathcal{C}_{\\widehat{C}}^{k}}\\widetilde{\\nabla}f_{i}(x^{k+1})-\\nabla f(x^{k+1})\\right\\|^{2}\\right]\\mid[1]\\right]}\\\\ &{\\quad=\\mathbb{E}\\left[\\mathbb{E}_{k}\\left[\\left\\|\\frac{1}{G_{\\widehat{C}}^{k}}\\sum_{i\\in\\mathcal{C}_{\\widehat{C}}^{k}}(\\widetilde{\\nabla}f_{i}(x^{k+1})-\\nabla f_{i}(x^{k+1}))\\right\\|^{2}\\right]\\mid[1]\\right]}\\\\ &{\\quad\\quad+\\mathbb{E}\\left[\\mathbb{E}_{k}\\left[\\left\\|\\frac{1}{G_{\\widehat{C}}^{k}}\\sum_{i\\in\\mathcal{C}_{\\widehat{C}}^{k}}\\nabla f_{i}(x^{k+1})-\\nabla f(x^{k+1})\\right\\|^{2}\\right]\\mid[1]\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 65}, {"type": "text", "text": "The choice of $\\widehat{C}$ implies that $G_{\\widehat{C}}^{k}\\,\\geq\\,(1-\\delta)\\widehat{C}$ . Moreover, due to the independence of stochastic gradient computations on different workers, we have ", "page_idx": 66}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\mathbb{E}_{k}\\left[\\Bigg|\\frac{1}{G_{\\hat{c}}^{k}}\\sum_{i\\in\\mathcal{E}_{\\hat{c}}^{k}}(\\tilde{\\mathbf{v}}_{f i}(x^{k+1})-\\nabla f_{i}(x^{k+1}))\\Bigg|^{2}\\right]\\mid[1]\\right]}\\\\ &{\\quad\\le\\frac{1}{(1-\\delta)^{2}\\hat{C}^{2}}\\mathbb{E}\\left[\\mathbb{E}_{k}\\left[\\left\\|\\sum_{i\\in\\mathcal{E}_{\\hat{c}}^{k}}(\\tilde{\\mathbf{v}}_{f i}(x^{k+1})-\\nabla f_{i}(x^{k+1}))\\right\\|^{2}\\right]\\mid[1]\\right]}\\\\ &{\\quad=\\frac{1}{(1-\\delta)^{2}\\hat{C}^{2}}\\mathbb{E}\\left[\\displaystyle\\sum_{i\\in\\mathcal{E}_{\\hat{c}}^{k}}\\mathbb{E}_{k}\\left[\\Big\\|\\tilde{\\nabla}f_{i}(x^{k+1})-\\nabla f_{i}(x^{k+1})\\Big\\|^{2}\\right]\\mid[1]\\right]}\\\\ &{\\quad=\\frac{\\mathcal{P}_{\\sigma_{\\hat{c}}^{k}}}{(1-\\delta)^{2}\\hat{C}^{2}}\\sum_{i\\in\\mathcal{E}_{\\hat{c}}^{k}}\\mathbb{E}\\left[\\left\\|\\tilde{\\nabla}f_{i}(x^{k+1})-\\nabla f_{i}(x^{k+1})\\right\\|^{2}\\right]\\mid[1]\\right]}\\\\ &{\\quad=\\frac{\\mathcal{P}_{\\sigma_{\\hat{c}}^{k}}}{(1-\\delta)^{2}\\hat{C}^{2}}\\sum_{i\\in\\mathcal{E}_{\\hat{c}}^{k}}\\mathbb{E}\\left[\\left\\|\\tilde{\\nabla}f_{i}(x^{k+1})-\\nabla f_{i}(x^{k+1})\\right\\|^{2}\\right]^{\\otimes2}\\frac{\\mathcal{P}_{\\sigma_{\\hat{c}}^{k}}G\\sigma^{2}}{(1-\\delta)^{2}\\hat{C}^{2}\\hat{D}t},}\\end{array}\n$$", "text_format": "latex", "page_idx": 66}, {"type": "text", "text": "Next since $\\frac{1}{G_{\\hat{\\cal{C}}}^{k}}\\sum_{i\\in\\mathcal{G}_{\\hat{\\cal{C}}}^{k}}\\nabla f_{i}(x^{k+1})=\\nabla f(x^{k+1})$ with probability when ${\\widehat{C}}=n$ we can stimate the last term in (34) as ", "page_idx": 66}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\mathbb{E}_{k}\\left[\\left\\|\\frac{1}{C_{\\varepsilon}^{k}}\\sum_{i\\in\\mathcal{E}_{\\hat{\\mathcal{E}}_{\\mathcal{O}}}^{k}}\\nabla f_{i}(x^{k+1})-\\nabla f(x^{k+1})\\right\\|^{2}\\right]\\mid[1]\\right]}\\\\ &{\\leq\\left\\{\\begin{array}{l l}{0,}&{\\mathrm{if}\\;\\hat{C}=n}\\\\ {\\mathbb{E}\\left[\\frac{1}{C_{\\varepsilon}^{k}}\\sum_{i\\in\\mathcal{E}_{\\mathcal{O}}^{k}}\\mathbb{E}_{k}\\left[\\left\\|\\nabla f_{i}(x^{k+1})-\\nabla f(x^{k+1})\\right\\|^{2}\\right]\\mid[1]\\right],}&{\\mathrm{if}\\;\\hat{C}<n}\\\\ {\\vdots}&{\\mathrm{if}\\;\\hat{C}=n}\\end{array}\\right.}\\\\ &{\\leq\\left\\{\\begin{array}{l l}{0,}&{\\mathrm{if}\\;\\hat{C}=n}\\\\ {\\frac{\\mathcal{P}_{\\varepsilon}}{(1-\\delta)^{k}}\\sum_{i\\in\\mathcal{E}_{\\mathcal{O}}^{k}}\\mathbb{E}\\left[\\left\\|\\nabla f_{i}(x^{k+1})-\\nabla f(x^{k+1})\\right\\|^{2}\\right],}&{\\mathrm{if}\\;\\hat{C}<n}\\\\ {\\vdots}&{\\mathrm{if}\\;\\hat{C}=n}\\end{array}\\right.}\\\\ &{\\stackrel{(A\\setminus L)}{\\leq}\\left\\{\\begin{array}{l l}{0,}&{\\mathrm{if}\\;\\hat{C}=n}\\\\ {\\frac{\\mathcal{P}_{\\varepsilon}}{(1-\\delta)^{k}}\\sum_{i\\in\\mathcal{E}_{\\mathcal{O}}^{k}}\\left(B\\mathbb{E}\\left[\\Vert\\nabla f(x^{k+1})\\right\\|^{2}\\right]+\\zeta^{2}\\right),}&{\\mathrm{if}\\;\\hat{C}<n}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 66}, {"type": "text", "text": "where ", "page_idx": 66}, {"type": "equation", "text": "$$\n\\widetilde{B}:=\\left\\{\\!\\!\\begin{array}{l l}{0,}&{\\mathrm{if~}\\widehat{C}=n,}\\\\ {\\frac{\\mathcal{P}_{\\mathcal{G}_{\\widehat{C}}^{k}}G B}{(1-\\delta)\\widehat{C}},}&{\\mathrm{if~}\\widehat{C}<n,}\\end{array}\\right.\\quad\\mathrm{and}\\quad\\widetilde{\\zeta}^{2}:=\\left\\{\\!\\!\\begin{array}{l l}{0,}&{\\mathrm{if~}\\widehat{C}=n,}\\\\ {\\frac{\\mathcal{P}_{\\mathcal{G}_{\\widehat{C}}^{k}}G\\zeta^{2}}{(1-\\delta)\\widehat{C}},}&{\\mathrm{if~}\\widehat{C}<n.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 66}, {"type": "text", "text": "Plugging the derived bounds in (34), we get ", "page_idx": 66}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\lVert\\frac{1}{G_{\\hat{C}}^{k}}\\sum_{i\\in\\mathcal{G}_{\\hat{C}}^{k}}\\tilde{\\nabla}f_{i}(x^{k+1})-\\nabla f(x^{k+1})\\right\\rVert^{2}\\right]\\mid[1]\\right]\\leq\\frac{\\mathcal{P}_{\\mathcal{G}_{\\hat{C}}^{k}}G\\sigma^{2}}{(1-\\delta)^{2}\\hat{C}^{2}b^{\\prime}}~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~}\n$$", "text_format": "latex", "page_idx": 66}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{T_{1}^{\\mathrm{~O~G~2~}}\\mathbb{E}\\left[\\frac{c\\delta}{G_{\\delta}^{\\delta}}\\frac{\\bigtriangleup}{(G_{\\delta}^{*}-1)}\\sum_{i,j\\in\\mathcal{E}_{j}}\\mathbb{E}_{k}\\left[\\Big|\\tilde{\\nabla}f_{i}(s^{k+1})-\\tilde{\\nabla}f_{i}(s^{k+1})\\Big|^{2}\\;|[1]\\right]\\right]}\\\\ &{+\\frac{P_{0G}G^{2}}{(1-\\delta)^{2}\\tilde{C}G^{2}V}+\\frac{\\tilde{\\mu}_{\\mathrm{~E~}}\\big[\\big|\\nabla f_{i}(s^{k+1})\\big|^{2}\\big]+\\tilde{\\nabla}^{2}}{\\tilde{\\mu}_{\\mathrm{~E~}}\\big[\\big|\\nabla f_{i}(s^{k+1})\\big|^{2}\\big]+\\tilde{\\nabla}^{2}}}\\\\ &{=\\mathbb{E}\\left[\\frac{\\delta}{G_{\\delta}^{\\delta}(G_{\\delta}^{*}-1)}\\sum_{i,j\\in\\mathcal{E}_{j}}\\mathbb{E}_{k}\\left[\\big|\\nabla f_{i}(s^{k+1})-\\nabla f_{k}(s^{k+1})\\big|^{2}\\;|[1]\\right]\\right]}\\\\ &{+\\mathbb{E}\\left[\\frac{\\delta}{G_{\\delta}^{\\delta}(G_{\\delta}^{*}-1)}\\sum_{i,j\\in\\mathcal{E}_{j}}\\mathbb{E}_{k}\\left[\\Big|\\tilde{\\nabla}f_{i}(s^{k+1})-\\nabla f_{i}(s^{k+1})-\\tilde{\\nabla}f_{k}(s^{k+1})+\\nabla f_{k}(s^{k+1})\\Big|^{2}\\;|[1]\\right]}\\\\ &{+\\frac{P_{0G}G^{2}}{(1-\\delta)^{2}\\tilde{C}G^{2}V}+\\frac{\\big|\\nabla f_{i}(s^{k+1})\\big|^{2}}{\\tilde{\\mu}_{\\mathrm{~E~}}\\big[\\big|\\nabla f_{i}(s^{k+1})\\big|^{2}\\big]+\\tilde{\\nabla}^{2}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 67}, {"type": "text", "text": "wherein tlaqualt,wus ditleef $\\{\\widetilde{\\nabla}f_{i}(x^{k+1})\\}_{i\\in\\mathcal{G}_{\\widehat{C}}^{k}}$ for fixed $x^{k+1}$ . Next, using Young's inequality, we derive ", "page_idx": 68}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{S}^{\\perp}\\otimes\\frac{\\mathcal{S}^{\\perp}}{\\mathcal{S}^{\\perp}}\\otimes\\frac{\\mathcal{S}^{\\perp}}{\\mathcal{S}^{\\perp}}\\otimes\\frac{\\mathcal{S}^{\\perp}}{\\mathcal{S}^{\\perp}}\\otimes\\frac{\\mathcal{S}^{\\perp}}{\\mathcal{S}^{\\perp}}\\otimes\\frac{\\mathcal{S}^{\\perp}}{\\mathcal{S}^{\\perp}}\\otimes\\frac{\\mathcal{S}^{\\perp}}{\\mathcal{S}^{\\perp}}\\otimes\\frac{\\mathcal{S}^{\\perp}}{\\mathcal{S}^{\\perp}}\\otimes\\frac{\\mathcal{S}^{\\perp}}{\\mathcal{S}^{\\perp}}\\otimes\\frac{\\mathcal{S}^{\\perp}}{\\mathcal{S}^{\\perp}}}\\\\ {+}&{\\frac{P_{a}(\\Delta_{B}^{\\perp})^{2}}{16}+\\frac{P_{b}(\\Delta_{B}^{\\perp})^{2}}{16}+\\frac{P_{b}(\\Delta_{B}^{\\perp})^{2}}{16}+\\frac{Q^{2}}{16}}\\\\ {+}&{\\frac{P_{a}(\\Delta_{B}^{\\perp})^{2}}{16}+\\frac{P_{b}(\\Delta_{B}^{\\perp})^{2}}{16}+\\frac{P_{b}(\\Delta_{B}^{\\perp})^{2}}{16}+\\frac{P_{b}^{\\perp}(\\Delta_{B}^{\\perp})^{2}}{16}}\\\\ {+}&{\\frac{P_{a}(\\Delta_{B}^{\\perp})^{2}}{16}+\\frac{P_{b}^{\\perp}(\\Delta_{B}^{\\perp})^{2}}{16}+\\frac{P_{b}^{\\perp}(\\Delta_{B}^{\\perp})^{2}}{16}+\\frac{P_{b}^{\\perp}(\\Delta_{B}^{\\perp})^{2}}{16}+\\frac{P_{b}^{\\perp}(\\Delta_{B}^{\\perp})^{2}}{16}}\\\\ {\\overset{\\mathrm{S}}{\\cong}\\frac{\\mathcal{S}^{\\perp}}{\\mathcal{S}^{\\perp}}\\otimes\\frac{\\mathcal{S}^{\\perp}}{\\mathcal{S}^{\\perp}}\\otimes\\bigg[\\Bigg|\\mathcal{S}^{\\perp}\\otimes\\left(\\frac{\\mathcal{S}^{\\perp}}{\\mathcal{S}^{\\perp}}\\right)^{\\perp}+\\Bigg|\\mathcal{S}^{\\perp}_{\\perp}^{\\perp}\\otimes\\frac{\\mathcal{S}^{\\perp}}{\\mathcal{S\n$$", "text_format": "latex", "page_idx": 68}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{T_{\\frac{1}{2}}^{\\frac{\\theta}{2}}\\Bigg[\\frac{\\displaystyle\\frac{\\mathrm{d}}{\\displaystyle\\mathrm{d}t}}{\\displaystyle\\mathrm{d}t}\\sum_{s^{\\prime}\\in\\mathcal{T}_{s^{\\prime}}}\\frac{\\displaystyle\\mathrm{C}\\left[2\\mathrm{i}\\nabla f_{s}(s^{\\prime})-\\mathrm{v}f_{\\varepsilon}(s^{\\prime}+1)\\right]^{2}\\left[1\\right]}{\\displaystyle\\mathrm{d}s}\\Bigg]}&{{}}\\\\ {+\\frac{\\displaystyle\\mathrm{C}\\left[\\frac{\\displaystyle\\frac{\\mathrm{d}}{\\displaystyle\\mathrm{d}t}}{\\displaystyle\\mathrm{d}t}\\frac{\\displaystyle\\mathrm{d}}{\\displaystyle\\mathrm{d}t}-1\\right]_{\\{\\infty,\\infty\\}}}{\\displaystyle\\mathrm{C}\\left[\\frac{\\displaystyle\\frac{\\mathrm{d}}{\\displaystyle\\mathrm{d}t}}{\\displaystyle\\mathrm{d}t}\\sum_{s^{\\prime}\\in\\mathcal{T}_{s^{\\prime}}}\\frac{\\displaystyle\\mathrm{C}\\left[2\\mathrm{i}\\nabla f_{s}(s^{\\prime})-\\mathrm{v}f_{\\varepsilon}(s^{\\prime}+1)\\right]^{2}\\left[1\\right]}{\\displaystyle\\mathrm{d}s}\\right]}&{{}\\Bigg]}\\\\ {+\\frac{\\displaystyle\\mathrm{C}\\left[\\frac{\\displaystyle\\frac{\\mathrm{d}}{\\displaystyle\\mathrm{d}t}}{\\displaystyle\\mathrm{d}t}\\sum_{s^{\\prime}\\in\\mathcal{T}_{s^{\\prime}}}\\sum_{i=1}^{d}\\frac{\\displaystyle\\mathrm{C}\\left[\\mathrm{i}\\nabla f_{s}(s^{\\prime})-\\mathrm{v}f_{\\varepsilon}(s^{\\prime}+1)\\right]^{2}\\left[1\\right]}{\\displaystyle\\mathrm{d}s}\\right]}&{{}\\Bigg]}\\\\ {+\\frac{\\displaystyle\\mathrm{C}\\left[\\frac{\\displaystyle\\frac{\\mathrm{d}}{\\displaystyle\\mathrm{d}t}}{\\displaystyle\\mathrm{d}t}\\sum_{s^{\\prime}\\in\\mathcal{T}_{s^{\\prime}}}\\sum_{i=1}^{d}\\frac{\\displaystyle\\mathrm{C}\\left[2\\mathrm{i}\\nabla f_{s}(s^{\\prime})-\\mathrm{v}f_{\\varepsilon}(s^{\\prime}+1)\\right]^{2}\\left[1\\right]}{\\displaystyle\\mathrm{d}s}\\right]}&{{}\\Bigg]}\\\\ {+\\frac{\\displaystyle\\mathrm{C}\\left[\\frac{\\displaystyle\\frac{\\mathrm{d}}{\\displaystyle\\mathrm{d}t}}{\\displaystyle\\mathrm{d}t}\\sum_{s^{\\prime}\\in\\mathcal{T}_{s^{\\prime}}}+\\frac{\\displaystyle\\mathrm{C}\\left[\\frac{\\mathrm{d}}{\\displaystyle\\mathrm{d}t}\\left[1\\right]\n$$$$\n\\begin{array}{r l}&{\\left|-\\mathbb{E}\\left[\\widehat{\\omega}_{j}(\\widehat{x}_{j}-\\widehat{x}_{j+1}^{t})\\sum_{u,v\\in\\mathcal{V}_{j}}^{\\infty}\\mathbb{E}\\left[\\mathbb{E}\\left\\{F_{u}(\\nu^{t})-\\nabla F_{u}(\\nu^{t})\\right\\}^{t}\\mid\\mathcal{W}\\right]\\right|_{1}\\right]\\right|}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\quad}\\\\ &{\\qquad\\times\\left[\\frac{\\mathcal{S}_{j}}{\\mathcal{N}_{j}(\\mu^{t})}\\mathbb{E}_{u,v}^{t}-\\frac{1}{\\mathcal{N}_{j}(\\mu^{t})}\\sum_{u,v\\in\\mathcal{V}_{j}}^{\\infty}\\mathbb{E}\\left[\\mathbb{E}\\left\\{F_{u}(\\nu^{t})-\\nabla F_{u}(\\nu^{t})\\right\\}^{t}\\mid\\mathcal{W}\\right]\\right|_{1}\\right]}\\\\ &{\\qquad+\\frac{\\mathcal{S}_{j}}{\\mathcal{N}_{j}(\\mu^{t})}\\sum_{u,v\\in\\mathcal{V}_{j}}^{\\infty}\\mathbb{E}\\left[\\mathbb{E}\\left\\{F_{u}(\\nu^{t})+\\nabla F_{u}(\\nu^{t})\\right\\}^{t}+\\mathbb{E}\\left\\{\\mathbf{E}\\left\\}\\mid\\nabla F_{u}(\\nu^{t})\\right]}\\\\ &{\\qquad+\\frac{\\mathcal{S}_{j}}{\\mathcal{N}_{j}(\\mu^{t})}\\sum_{u,v\\in\\mathcal{V}_{j}}^{\\infty}\\mathbb{E}\\left[\\mathbb{E}\\left\\{F_{u}(\\nu^{t})-\\nabla F_{u}(\\nu^{t})\\right\\}^{t}\\mid\\mathcal{W}\\right]\\Bigg|_{1}\\right]+\\frac{\\mathcal{S}_{j}\\alpha^{2}}{\\mathcal{N}_{j}}}\\\\ &{\\qquad+\\frac{\\mathcal{S}_{j}}{\\mathcal{N}_{j}(\\mu^{t})}\\sum_{u,v\\in\\mathcal{V}_{j}}^{\\infty}\\mathbb{E}\\left[\\mathbb{E}\\left\\{F_{u}(\\nu^{t})+\\nabla F_{u}(\\nu^{t})\\right\\}^{t}+\\mathbb{E}\\left\\{\\mathbf{E}\\left\\}\\mid\\nabla F_{u}(\\nu^{t})\\right\\}^{t}}\\\\ &{\\qquad+\\frac{\\mathcal{S}_{j}}{\\mathcal{N}_{j}(\\mu^{t})}\\sum_{u,v\\in\\mathcal{V}_{j}}^{ \n$$", "text_format": "latex", "page_idx": 68}, {"type": "equation", "text": "", "text_format": "latex", "page_idx": 68}, {"type": "text", "text": "G.2 Main Results for Byz-VR-MARINA without Full-Batch Gradient Computations ", "text_level": 1, "page_idx": 69}, {"type": "text", "text": "G.2.1 General Results ", "text_level": 1, "page_idx": 69}, {"type": "text", "text": "All the lemmas derived in Appendix D.2 hold for Algorithm 4 as well except Lemma D.10, which can be replaced with Lemma G.2, and Lemma D.13 that has the following analog. ", "page_idx": 69}, {"type": "text", "text": "Lemma G.3. Let Assumptions 2.3, D.1, D.2, D.3, D.5, G.1 hold and Compression Operator satisfy Definition 2.2. Also, let us introduce the notation ", "page_idx": 69}, {"type": "equation", "text": "$$\n\\begin{array}{r}{A R A g g_{Q}^{k+1}=A R A g g\\left(c l\\,i p_{\\lambda_{k+1}}\\left(\\mathcal{Q}\\left(\\widehat{\\Delta}_{1}(x^{k+1},x^{k})\\right)\\right),\\ldots,c l\\,i p_{\\lambda_{k+1}}\\left(\\mathcal{Q}\\left(\\widehat{\\Delta}_{C}(x^{k+1},x^{k})\\right)\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 69}, {"type": "text", "text": "Thenforall $k\\,\\geq\\,0$ the iterates produced by Byz-VR-MARINA-PP without full-batch gradient computations (Algorithm 4) satisfy ", "page_idx": 69}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\|g^{k+1}-\\nabla f\\left(x^{k+1}\\right)\\right\\|^{2}\\right]\\leq\\left(1-\\frac{p}{4}\\right)\\mathbb{E}\\left[\\left\\|g^{k}-\\nabla f\\left(x^{k}\\right)\\right\\|^{2}\\right]+\\left(\\frac{3\\mathcal{P}_{\\mathcal{G}_{\\mathcal{O}}^{k}}G}{(1-\\delta)^{2}\\widehat{C}^{2}}+12c\\delta\\right)\\frac{\\sigma^{2}}{b^{\\prime}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+\\,\\widehat{B}\\mathbb{E}\\left[\\left\\|\\nabla f\\left(x^{k}\\right)\\right\\|^{2}\\right]+\\widehat{D}\\zeta^{2}+\\frac{p A}{4}\\|x^{k+1}-x^{k}\\|^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 69}, {"type": "text", "text": "where $A,\\widehat{B},\\widehat{D},p_{G},\\mathcal{P}_{\\mathcal{G}_{C}^{k}}$ are defined in Lemma D.13. ", "page_idx": 69}, {"type": "text", "text": "Proof. Up to the replacement of the bound from Lemma D.10 with the bound from Lemma G.2, the proof of the result is identical to the proof of Lemma D.13. \u53e3 ", "page_idx": 69}, {"type": "text", "text": "Theorem G.4. Let Assumptions 2.3, D.1, D.2, D.3, D.5, G.1 hold. Set $\\begin{array}{r l}{\\lambda_{k+1}}&{{}=}\\end{array}$ $2\\operatorname*{max}_{i\\in{\\mathcal{G}}}L_{i}\\left\\|x^{k+1}-x^{k}\\right\\|$ Assumethat ", "page_idx": 69}, {"type": "equation", "text": "$$\n0<\\gamma\\leq\\frac{1}{L+\\sqrt{A}},\\quad4\\widehat{B}<p,\n$$", "text_format": "latex", "page_idx": 69}, {"type": "text", "text": "Wwhere $A$ and $\\widehat{B}$ are defined in Theorem D.14. Then for all $K\\ge0$ the iterates produced by Byz-VRMARINA without full-batch gradient computations (Algorithm 4) satisfy ", "page_idx": 69}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\Vert\\nabla f\\left(\\widehat{x}^{K}\\right)\\right\\Vert^{2}\\right]\\leq\\frac{2\\Phi^{0}}{\\gamma\\left(1-\\frac{4\\widehat{B}}{p}\\right)(K+1)}+\\frac{4\\widehat{D}\\zeta^{2}}{p-4\\widehat{B}}+\\left(\\frac{12\\mathcal{P}_{\\mathcal{G}_{C}^{k}}G}{(1-\\delta)^{2}\\widehat{C}^{2}}+48c\\delta\\right)\\frac{\\sigma^{2}}{b^{\\prime}(p-4\\widehat{B})},\n$$", "text_format": "latex", "page_idx": 69}, {"type": "text", "text": "where $\\widehat{x}^{K}$ is chosen uniformly at random from $x^{0},x^{1},\\ldots,x^{K}$ \uff0cand $\\Phi^{0}\\ =\\ f\\left(x^{0}\\right)\\;-\\;f^{*}\\;+$ $\\begin{array}{r}{\\frac{2\\gamma}{p}\\left\\|g^{0}-\\nabla f\\left(x^{0}\\right)\\right\\|^{2}}\\end{array}$ ", "page_idx": 69}, {"type": "text", "text": "Proof. The proof is identical to the proof of Theorem D.14 up to the replacement of Lemma D.13 with Lemma G.3. \u53e3 ", "page_idx": 69}, {"type": "text", "text": "Theorem G.5. Let Assumptions 2.3, D.1, D.2, D.3, D.5, 2.7 hold. Set\u5165k+1 $\\begin{array}{r l}{\\lambda_{k+1}}&{{}=}\\end{array}$ $\\operatorname*{max}_{i\\in{\\mathcal{G}}}L_{i}\\left\\|x^{k+1}-x^{k}\\right\\|$ .Assumethat ", "page_idx": 69}, {"type": "equation", "text": "$$\n0<\\gamma\\leq\\operatorname*{min}\\left\\{{\\frac{1}{L+{\\sqrt{2A}}}}\\right\\},\\quad8{\\widehat{B}}<p\n$$", "text_format": "latex", "page_idx": 69}, {"type": "text", "text": "Where $A$ and $\\widehat{B}$ are defined in Theorem D.15. Then for all $K\\ge0$ the iterates produced by Byz-VRMARINA without full-batch gradient computations (Algorithm 4) satisfy ", "page_idx": 69}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[f\\left(x^{K}\\right)-f\\left(x^{*}\\right)\\right]\\leq\\left(1-\\rho\\right)^{K}\\Phi^{0}+\\frac{4\\widehat{D}\\gamma\\zeta^{2}}{p\\rho}+\\left(\\frac{12\\mathcal{P}_{\\mathcal{G}_{\\widehat{C}}^{k}}G}{(1-\\delta)^{2}\\widehat{C}^{2}}+48c\\delta\\right)\\frac{\\gamma\\sigma^{2}}{b^{\\prime}p\\rho},\n$$", "text_format": "latex", "page_idx": 69}, {"type": "text", "text": "where $\\begin{array}{r}{\\rho=\\operatorname*{min}\\left[\\gamma\\mu\\left(1-\\frac{8\\widehat{B}}{p}\\right),\\frac{p}{8}\\right]a n d\\,\\Phi^{0}=f\\left(x^{0}\\right)-f^{*}+\\frac{4\\gamma}{p}\\left\\Vert g^{0}-\\nabla f\\left(x^{0}\\right)\\right\\Vert^{2}.}\\end{array}$ Proof. The proof is identical to the proof of Theorem D.15 up to the replacement of Lemma D.13 with Lemma G.3. \u53e3 ", "page_idx": 69}, {"type": "text", "text": "", "page_idx": 70}, {"type": "text", "text": "In contrast to their counterparts for Byz-VR-MARINA-PP with (periodical) full-batch gradient $\\frac{\\sigma^{2}}{b^{\\prime}}$ smaller via the increase of $b^{\\prime}$ . A similar phenomenon appears in the analysis of the methods with recursive variance reduction even in Byzantine-free case (Fang et al., 2018; Li et al., 2021; Gorbunov et al., 2021), and to address it, $b^{\\prime}$ is typically chosen to be large. ", "page_idx": 70}, {"type": "text", "text": "G.2.2  Results for Bounded Compressors ", "text_level": 1, "page_idx": 70}, {"type": "text", "text": "Similarly to the previous section, we start with an adaptation of Lemma E.3 to the case without full-batch gradient computations. ", "page_idx": 70}, {"type": "text", "text": "Lemma G.6. Let Assumptions $2.3,\\,D.I,\\,D.2,\\,D.3,\\,D.4,\\,D.5,\\,G.I,\\,2.4\\,h o l$ dand thecompression operator satisfy Definition 2.2. We set $\\lambda_{k+1}=D_{Q}\\operatorname*{max}_{i,j}L_{i,j}\\|x^{k+1}-x^{k}\\|$ . Also, let us introduce thenotation ", "page_idx": 70}, {"type": "equation", "text": "$$\n\\begin{array}{r}{A R A g g_{Q}^{k+1}=A R A g g\\left(c\\imath\\,i p_{\\lambda_{k+1}}\\left(\\mathcal{Q}\\left(\\widehat{\\Delta}_{1}(x^{k+1},x^{k})\\right)\\right),\\ldots,c\\imath\\,i p_{\\lambda_{k+1}}\\left(\\mathcal{Q}\\left(\\widehat{\\Delta}_{C}(x^{k+1},x^{k})\\right)\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 70}, {"type": "text", "text": "Thenforall $k\\,\\geq\\,0$ the iterates produced by Byz-VR-MARINA-PP without full-batch gradient computations(Algorithm 4) satisfy ", "page_idx": 70}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\|g^{k+1}-\\nabla f\\left(x^{k+1}\\right)\\right\\|^{2}\\right]\\leq\\left(1-\\displaystyle\\frac{p}{2}\\right)\\mathbb{E}\\left[\\left\\|g^{k}-\\nabla f\\left(x^{k}\\right)\\right\\|^{2}\\right]+\\left(\\displaystyle\\frac{3\\mathcal{P}_{\\mathcal{G}_{\\widehat{C}}^{k}}G}{(1-\\delta)^{2}\\widehat{C}^{2}}+12c\\delta\\right)\\frac{\\sigma^{2}}{b^{\\prime}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+\\,\\widehat{B}\\mathbb{E}\\left[\\left\\|\\nabla f\\left(x^{k}\\right)\\right\\|^{2}\\right]+\\widehat{D}\\zeta^{2}+\\displaystyle\\frac{p A}{4}\\|x^{k+1}-x^{k}\\|^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 70}, {"type": "text", "text": "where $A,\\widehat{B},\\widehat{D},p_{G},\\mathcal{P}_{\\mathcal{G}_{C}^{k}}$ are defined in Lemma D.13. ", "page_idx": 70}, {"type": "text", "text": "Proof. Up to the replacement of the bound from Lemma D.10 with the bound from Lemma G.2, the proof of the result is identical to the proof of Lemma E.3. ", "page_idx": 70}, {"type": "text", "text": "Theorem G.7, Let Assumptions 2.3, D.1, D.2, D.3, D.4, D.5, G.1, 2.4 hold. Setting $\\lambda_{k+1}\\;=\\;$ $\\operatorname*{max}_{i,j}L_{i,j}\\left\\|x^{k+1}-x^{k}\\right\\|$ Assumethat ", "page_idx": 70}, {"type": "equation", "text": "$$\n0<\\gamma\\leq\\frac{1}{L+\\sqrt{A}},\\quad4\\widehat{B}<p,\n$$", "text_format": "latex", "page_idx": 70}, {"type": "text", "text": "where $A$ and $\\widehat{B}$ are defined in Theorem $E_{\\cdot}$ 4.Thenfor all $K\\ge0$ the iterates produced by Byz-VRMARINA without full-batch computations (Algorithm 4) satisfy ", "page_idx": 70}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\Vert\\nabla f\\left(\\widehat{x}^{K}\\right)\\right\\Vert^{2}\\right]\\leq\\frac{2\\Phi^{0}}{\\gamma\\left(1-\\frac{4\\widehat{B}}{p}\\right)(K+1)}+\\frac{2\\widehat{D}\\zeta^{2}}{p-4\\widehat{B}}+\\left(\\frac{6\\mathcal{P}_{\\mathcal{G}_{C}^{k}}G}{(1-\\delta)^{2}\\widehat{C}^{2}}+24c\\delta\\right)\\frac{\\sigma^{2}}{b^{\\prime}(p-4\\widehat{B})},\n$$", "text_format": "latex", "page_idx": 70}, {"type": "text", "text": "where $\\widehat{x}^{K}$ is chosen uniformly at random from $x^{0},x^{1},\\ldots,x^{K}$ \uff0cand $\\Phi^{0}\\ =\\ f\\left(x^{0}\\right)\\;-\\;f^{*}\\;+$ $\\begin{array}{r}{\\frac{\\gamma}{p}\\left\\|g^{0}-\\nabla f\\left(x^{0}\\right)\\right\\|^{2}}\\end{array}$ ", "page_idx": 70}, {"type": "text", "text": "Proof. The proof is identical to the proof of Theorem E.4 up to the replacement of Lemma E.3 with Lemma G.6. \u53e3 ", "page_idx": 70}, {"type": "text", "text": "Theorem G.8. Let Assumptions 2.3, D.1, D.2, D.3, $D$ .4, D.5, G.1, 2.4, 2.7 hold. Setting $\\lambda_{k+1}=$ $\\begin{array}{r l}{\\operatorname*{max}_{i,j}L_{i,j}\\left\\|x^{k+1}-x^{k}\\right\\|}&{{}}\\end{array}$ Assumethat ", "page_idx": 70}, {"type": "equation", "text": "$$\n0<\\gamma\\leq\\frac{1}{L+\\sqrt{2A}},\\quad8\\widehat{B}<p,\n$$", "text_format": "latex", "page_idx": 70}, {"type": "text", "text": "where $A$ and $\\widehat{B}$ are defined in Theorem E.5. Then for all $K\\ge0$ the iterates produced by Byz-VRMARINA without full-batch computations (Algorithm 4) satisfy ", "page_idx": 71}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[f\\left(x^{K}\\right)-f\\left(x^{*}\\right)\\right]\\leq\\left(1-\\rho\\right)^{K}\\Phi^{0}+\\frac{2\\widehat{D}\\zeta^{2}}{p\\rho}+\\left(\\frac{6\\mathcal{P}_{\\mathcal{G}_{\\widehat{C}}^{k}}G}{(1-\\delta)^{2}\\widehat{C}^{2}}+24c\\delta\\right)\\frac{\\gamma\\sigma^{2}}{b^{\\prime}p\\rho},}\\\\ &{=\\operatorname*{min}\\left[\\gamma\\mu\\left(1-\\frac{8\\widehat{B}}{p}\\right),\\frac{p}{4}\\right]a n d\\,\\Phi^{0}=f\\left(x^{0}\\right)-f^{*}+\\frac{2\\gamma}{p}\\left\\Vert g^{0}-\\nabla f\\left(x^{0}\\right)\\right\\Vert^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 71}, {"type": "text", "text": "Proof. The proof is identical to the proof of Theorem E.5 up to the replacement of Lemma E.3 with Lemma G.6. \u53e3 ", "page_idx": 71}, {"type": "text", "text": "G.3Main Results for Byz-VR-MARINA $^+$ without Full-Batch Gradient Computations ", "text_level": 1, "page_idx": 71}, {"type": "text", "text": "G.3.1 Results for Bounded Compressors ", "text_level": 1, "page_idx": 71}, {"type": "text", "text": "Similarly to the analysis of Byz-VR-MARINA without full-batch gradient computations, we start with the adaptation of Lemma F.1 to the no-full-batch gradient computations case. ", "page_idx": 71}, {"type": "text", "text": "Lemma G.9. Let Assumptions D.1, D.2, D.3, D.4, D.5, G.1, 2.4 hold and the compression operator satisfy Definition 2.2. Assume that $C\\leq G$ We set $\\lambda_{k+1}=D_{Q}\\operatorname*{max}_{i,j}L_{i,j}\\|x^{k+1}-x^{k}\\|$ .Then for all $k\\geq0$ the iterates produced byByz-VR-MARINA- $\\mathsf{P P+}$ without full-batch gradient computations (Algorithm 5) satisfy ", "page_idx": 71}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}\\left[\\left\\|g^{k+1}-\\nabla f\\left(x^{k+1}\\right)\\right\\|^{2}\\right]\\leq\\left(1-\\displaystyle\\frac{p}{2}\\right)\\mathbb{E}\\left[\\left\\|g^{k}-\\nabla f\\left(x^{k}\\right)\\right\\|^{2}\\right]+\\left(\\displaystyle\\frac{\\mathcal{P}g_{\\hat{C}}^{k}G}{(1-\\delta)^{2}\\hat{C}^{2}}+4c\\delta\\right)\\frac{p\\sigma^{2}}{b^{\\prime}}}&{{}}\\\\ {+\\,\\hat{B}\\mathbb{E}\\left[\\left\\|\\nabla f\\left(x^{k}\\right)\\right\\|^{2}\\right]+\\hat{D}\\zeta^{2}+\\displaystyle\\frac{p A}{4}\\|x^{k+1}-x^{k}\\|^{2},}&{{}\\quad(3-\\delta)\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 71}, {"type": "text", "text": "where $A,\\widehat{B},\\widehat{D},p_{G},\\mathcal{P}_{\\mathcal{G}_{C}^{k}}$ are defined in Lemma $F.l$ ", "page_idx": 71}, {"type": "text", "text": "Proof. Up to the replacement of the bound from Lemma D.10 with the bound from Lemma G.2, the proof of the result is identical to the proof of Lemma F.1. \u53e3 ", "page_idx": 71}, {"type": "text", "text": "Theorem G.10. Let Assumptions D.1, D.2, D.3, D.4, D.5, G.1, 2.4 hold. Set $\\begin{array}{r l}{\\lambda_{k+1}}&{{}=}\\end{array}$ $\\begin{array}{r l}{\\operatorname*{max}_{i,j}L_{i,j}\\left\\|x^{k+1}-x^{k}\\right\\|}&{{}}\\end{array}$ Assumethat ", "page_idx": 71}, {"type": "equation", "text": "$$\n0<\\gamma\\leq\\frac{1}{L+\\sqrt{A}},\\quad4\\widehat{B}<p,\n$$", "text_format": "latex", "page_idx": 71}, {"type": "text", "text": "where $A$ and $\\widehat{B}$ are defined in Theorem $F.2$ Then for all $K\\geq0$ the iterates produced by Byz-VRMARI $\\mathsf{N A+}$ without fuil-batch gradient computations (Algorithm 5\uff09 satisfy ", "page_idx": 71}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\Vert\\nabla f\\left(\\widehat{x}^{K}\\right)\\right\\Vert^{2}\\right]\\leq\\frac{2\\Phi^{0}}{\\gamma\\left(1-\\frac{4\\widehat{B}}{p}\\right)\\left(K+1\\right)}+\\frac{2\\widehat{D}\\zeta^{2}}{p-4\\widehat{B}}+\\left(\\frac{2\\mathcal{P}_{\\mathcal{G}_{\\widehat{C}}^{k}}G}{(1-\\delta)^{2}\\widehat{C}^{2}}+8c\\right)\\frac{p\\sigma^{2}}{b^{\\prime}(p-4\\widehat{B})},\n$$", "text_format": "latex", "page_idx": 71}, {"type": "text", "text": "where $\\widehat{x}^{K}$ is chosen uniformly at random from $x^{0},x^{1},\\ldots,x^{K}$ \uff0cand $\\Phi^{0}\\ =\\ f\\left(x^{0}\\right)\\;-\\;f^{*}\\;+$ $\\begin{array}{r}{\\frac{\\gamma}{p}\\left\\|g^{0}-\\nabla f\\left(x^{0}\\right)\\right\\|^{2}}\\end{array}$ ", "page_idx": 71}, {"type": "text", "text": "Proof. The proof is analogous to the proof of Theorem D.14. ", "page_idx": 71}, {"type": "text", "text": "Theorem G.11. Let Assumptions 2.4, D.1, D.2, D.3, D.4, D.5, G.1 2.7 hold. Set $\\lambda_{k+1}~=$ $\\operatorname*{max}_{i,j}L_{i,j}\\left\\|x^{k+1}-x^{k}\\right\\|$ Assumethat ", "page_idx": 71}, {"type": "equation", "text": "$$\n0<\\gamma\\leq\\operatorname*{min}\\left\\{\\frac{1}{L+\\sqrt{2A}}\\right\\},\\quad8\\widehat{B}<p,\n$$", "text_format": "latex", "page_idx": 71}, {"type": "text", "text": "where $A$ and $\\widehat{B}$ are defined in Theorem $F.3$ Then for all $K\\geq0$ the iterates produced by Byz-VRMARINA $^+$ without fuil-batch gradient computations (Algorithm $^{5}$ ) satisfy ", "page_idx": 71}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}\\left[f\\left(x^{K}\\right)-f\\left(x^{*}\\right)\\right]\\leq\\left(1-\\rho\\right)^{K}\\Phi^{0}+\\displaystyle\\frac{2\\widehat{D}\\zeta^{2}}{p\\rho}+\\left(\\frac{2\\mathcal{P}_{\\mathcal{G}_{\\widehat{C}}^{k}}G}{\\left(1-\\delta\\right)^{2}\\widehat{C}^{2}}+8c\\delta\\right)\\frac{\\gamma\\sigma^{2}}{b^{\\prime}\\rho},}\\\\ &{=\\operatorname*{min}\\left[\\gamma\\mu\\left(1-\\frac{8\\widehat{B}}{p}\\right),\\frac{p}{4}\\right]a n d\\,\\Phi^{0}=f\\left(x^{0}\\right)-f^{*}+\\frac{2\\gamma}{p}\\left\\Vert g^{0}-\\nabla f\\left(x^{0}\\right)\\right\\Vert^{2}.}\\end{array}\n$$where ", "text_format": "latex", "page_idx": 71}, {"type": "text", "text": "As in the case of Byz-VR-MARINA, the above upper bounds for Byz-VR-MARINA+ without fullbatch gradient computations have additional terms proportional to $\\frac{\\sigma^{2}}{b^{\\prime}}$ In contrast to the results for Byz-VR-MARINA+ without full-batch gradient computations, these terms for Byz-VR-MARINA $^+$ are $^1\\!/\\!p$ times smaller. ", "page_idx": 72}, {"type": "text", "text": "H Experimental Details and Extra Experiments ", "text_level": 1, "page_idx": 73}, {"type": "text", "text": "H.1 Experimental Details ", "text_level": 1, "page_idx": 73}, {"type": "text", "text": "For each experiment, we tune the step size using the following set of candidates $\\{0.1,0.01,0.001\\}$ The step size is fixed. We do not use learning rate warmup or decay. We use batches of size 32 for all methods. For partial participation, in each round, we sample $20\\%$ of clients uniformly at random. For $\\lambda_{k}=\\lambda\\|x^{\\hat{k}}-x^{k^{\\frac{2}{-1}}}\\|$ used for clipping, we select $\\lambda$ from $\\{0.1,1.,10.\\}$ . Each experiment is run with three varying random seeds, and we report the mean optimality gap with one standard error. The optimal value is obtained by running gradient descent (GD) on the complete dataset for 1000 epochs. Our implementation of attacks and robust aggregation schemes is based on the public implementation from (Gorbunov et al., 2023). ", "page_idx": 73}, {"type": "text", "text": "H.2 Extra Experiments ", "text_level": 1, "page_idx": 73}, {"type": "text", "text": "Below we provide the missing neural network experiments from the main paper. We consider the MNIST dataset (LeCun and Cortes, 1998) and CIFAR10 (Krizhevsky et al., 2009) (as in (Karimireddy et al., 2021)) with 20 clients, 5 of which are malicious, and 4 clients are sampled in each step. For the attacks, we consider A Little is Enough (ALIE) (Baruch et al., 2019) and the aforementioned Shift-Back (SHB). For the aggregations, we consider coordinate median (CM) (Chen et al., 2017) and robust federated averaging (RFA) (Pillutla et al., 2022) with bucketing. For the MNIST dataset, we use a simple neural network with two convolution layers followed by two fully connected. For CIFAR 10, we use ResNet18 (He et al., 2016) architecture with layer norm. One can note that the results are consistent with the ones provided in the main paper, i.e., clipping performs on par or better than its variant without clipping, and no robust aggregator is able to withstand the shift-back attack without clipping. Our implementation is available at https : //github . com/SamuelHorvath/VR_ Byzantine/tree/partial_participation. ", "page_idx": 73}, {"type": "image", "img_path": "G8aS48B9bm/tmp/ccba2fa9b359144b2349e4cd98ae7162a46258852e843f6b3e9a15e84ae4fd56.jpg", "img_caption": [], "img_footnote": [], "page_idx": 73}, {"type": "text", "text": "Figure 3: Training loss (top) and test accuracy (bottom) of 2 aggregation rules (CM, RFA) under 4 attacks (BF, LF, ALIE, SHB) on the MNIST dataset under heterogeneous data split with 20 clients, 5 of which are malicious, 4 clients sampled per round. ", "page_idx": 73}, {"type": "image", "img_path": "G8aS48B9bm/tmp/e3ad2e012bcc47bc2e78172bc692c36358943689a6c72ed417b0f7c773a1c079.jpg", "img_caption": ["Figure 4: Training loss (top) and test accuracy (bottom) of 2 aggregation rules (CM, RFA) under 4 attacks (BF, LF, ALIE, SHB) on the CIFAR10 dataset under heterogeneous data split with 20 clients, 5 of which are malicious, 4 clients sampled per round. "], "img_footnote": [], "page_idx": 73}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 74}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 74}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? ", "page_idx": 74}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 74}, {"type": "text", "text": "Justification: see Sections 1, 3, 4, 5 ", "page_idx": 74}, {"type": "text", "text": "Guidelines: ", "page_idx": 74}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 74}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 74}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 74}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 74}, {"type": "text", "text": "Justification: see Sections 2, 3, 4 ", "page_idx": 74}, {"type": "text", "text": "Guidelines: ", "page_idx": 74}, {"type": "text", "text": "\u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\"\u2019' section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u00b7 The authors should refect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should refect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 74}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 74}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 74}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 74}, {"type": "text", "text": "Justification: see Section 4 and the appendix ", "page_idx": 75}, {"type": "text", "text": "Guidelines: ", "page_idx": 75}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 75}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 75}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 75}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 75}, {"type": "text", "text": "Justification: see Section 5 Guidelines: ", "page_idx": 75}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b)  If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. () If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 75}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 75}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 75}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 76}, {"type": "text", "text": "Justification: see Section 5 Guidelines: ", "page_idx": 76}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please see the NeurIPS code and data submission guidelines (https: //nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https : //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 76}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 76}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 76}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 76}, {"type": "text", "text": "Justification: see Section 5 Guidelines: ", "page_idx": 76}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u00b7 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 76}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 76}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 76}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 76}, {"type": "text", "text": "Justification: see Section 5 Guidelines: ", "page_idx": 76}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The authors should answer \"'Yes' if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative errorrates).   \n\u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 76}, {"type": "text", "text": "", "page_idx": 77}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 77}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce theexperiments? ", "page_idx": 77}, {"type": "text", "text": "Answer: [No] ", "page_idx": 77}, {"type": "text", "text": "Justification: We do not provide specific information about the computer resources used for our experiments. However, none of our experiments require significant computational power. Each experiment can be run in less than 5 minutes on a Quadro RTX 6000 NVIDIA GPU. ", "page_idx": 77}, {"type": "text", "text": "Guidelines: ", "page_idx": 77}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 77}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 77}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines? ", "page_idx": 77}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 77}, {"type": "text", "text": "Justification:our workfollows theNeurIPS Code of Ethics ", "page_idx": 77}, {"type": "text", "text": "Guidelines: ", "page_idx": 77}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 77}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 77}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 77}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 77}, {"type": "text", "text": "Justification: theoretical paper ", "page_idx": 77}, {"type": "text", "text": "Guidelines: ", "page_idx": 77}, {"type": "text", "text": "\u00b7 The answer NA means that there is no societal impact of the work performed.   \n\u00b7 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u00b7 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 77}, {"type": "text", "text": "", "page_idx": 78}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 78}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 78}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 78}, {"type": "text", "text": "Justification: we do not train new models ", "page_idx": 78}, {"type": "text", "text": "Guidelines: ", "page_idx": 78}, {"type": "text", "text": "\u00b7 The answer NA means that the paper poses no such risks.   \n\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 78}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 78}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properlyrespected? ", "page_idx": 78}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 78}, {"type": "text", "text": "Justification: we provide citations when necessary ", "page_idx": 78}, {"type": "text", "text": "Guidelines: ", "page_idx": 78}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not use existing assets.   \n\u00b7 The authors should cite the original paper that produced the code package or dataset.   \n\u00b7 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode . com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 78}, {"type": "text", "text": "\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset's creators. ", "page_idx": 79}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 79}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 79}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 79}, {"type": "text", "text": "Justification: the paper does not release new assets ", "page_idx": 79}, {"type": "text", "text": "Guidelines: ", "page_idx": 79}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not release new assets.   \n\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose assetisused.   \n\u00b7 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 79}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 79}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 79}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 79}, {"type": "text", "text": "Justification: the paper does not involve crowdsourcing nor research with human subjects Guidelines: ", "page_idx": 79}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPs Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 79}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 79}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)were obtained? ", "page_idx": 79}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 79}, {"type": "text", "text": "Justification: the paper does not involve crowdsourcing nor research with human subjects Guidelines: ", "page_idx": 79}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 79}]