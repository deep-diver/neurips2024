[{"figure_path": "G8aS48B9bm/figures/figures_8_1.jpg", "caption": "Figure 1: The optimality gap f(xk) \u2212 f(x*) for 3 different scenarios. We use coordinate-wise mean with bucketing equal to 2 as an aggregation and shift-back as an attack. We use the a9a dataset, where each worker accesses the full dataset with 15 good and 5 Byzantine workers. We do not use any compression. In each step, we sample 20% of clients uniformly at random to participate in the given round unless we specifically mention that we use full participation. Left: Linear convergence of Byz-VR-MARINA-PP with clipping versus non-convergence without clipping. Middle: Full versus partial participation, showing faster convergence with clipping. Right: Clipping multiplier \u03bb sensitivity, demonstrating consistent linear convergence across varying \u03bb values.", "description": "This figure shows three scenarios comparing the performance of Byz-VR-MARINA-PP in the presence of Byzantine workers. The left panel compares the convergence with and without clipping, highlighting the importance of clipping for robustness. The middle panel contrasts full participation with partial participation (20% of clients), showing that partial participation with clipping can be faster. The right panel demonstrates the effect of varying the clipping multiplier, indicating consistent performance across a range of values.", "section": "Numerical Experiments"}, {"figure_path": "G8aS48B9bm/figures/figures_9_1.jpg", "caption": "Figure 1: The optimality gap f(xk) \u2212 f(x*) for 3 different scenarios. We use coordinate-wise mean with bucketing equal to 2 as an aggregation and shift-back as an attack. We use the a9a dataset, where each worker accesses the full dataset with 15 good and 5 Byzantine workers. We do not use any compression. In each step, we sample 20% of clients uniformly at random to participate in the given round unless we specifically mention that we use full participation. Left: Linear convergence of Byz-VR-MARINA-PP with clipping versus non-convergence without clipping. Middle: Full versus partial participation, showing faster convergence with clipping. Right: Clipping multiplier \u03bb sensitivity, demonstrating consistent linear convergence across varying \u03bb values.", "description": "This figure compares three scenarios to illustrate the performance of Byz-VR-MARINA-PP. The left panel shows the linear convergence with clipping against non-convergence without clipping in a full participation setting. The middle panel contrasts full participation with partial participation, highlighting faster convergence when clipping is employed. The right panel demonstrates the consistent linear convergence across varying clipping multipliers (\u03bb), showing robustness to parameter tuning.", "section": "Numerical Experiments"}, {"figure_path": "G8aS48B9bm/figures/figures_73_1.jpg", "caption": "Figure 1: The optimality gap f(xk) \u2212 f(x*) for 3 different scenarios. We use coordinate-wise mean with bucketing equal to 2 as an aggregation and shift-back as an attack. We use the a9a dataset, where each worker accesses the full dataset with 15 good and 5 Byzantine workers. We do not use any compression. In each step, we sample 20% of clients uniformly at random to participate in the given round unless we specifically mention that we use full participation. Left: Linear convergence of Byz-VR-MARINA-PP with clipping versus non-convergence without clipping. Middle: Full versus partial participation, showing faster convergence with clipping. Right: Clipping multiplier \u03bb sensitivity, demonstrating consistent linear convergence across varying \u03bb values.", "description": "This figure demonstrates the effectiveness of the proposed Byz-VR-MARINA-PP algorithm under different scenarios.  It shows three sets of experiments comparing the optimality gap (difference between current and optimal objective function values) over epochs of training. The left panel compares the algorithm's performance with and without clipping, showcasing linear convergence with clipping and failure to converge without. The middle panel shows that partial participation leads to faster convergence compared to full participation when using clipping. The right panel displays the algorithm's robustness to different clipping multiplier values, demonstrating consistent linear convergence.", "section": "Numerical Experiments"}, {"figure_path": "G8aS48B9bm/figures/figures_73_2.jpg", "caption": "Figure 1: The optimality gap f(xk) \u2212 f(x*) for 3 different scenarios. We use coordinate-wise mean with bucketing equal to 2 as an aggregation and shift-back as an attack. We use the a9a dataset, where each worker accesses the full dataset with 15 good and 5 Byzantine workers. We do not use any compression. In each step, we sample 20% of clients uniformly at random to participate in the given round unless we specifically mention that we use full participation. Left: Linear convergence of Byz-VR-MARINA-PP with clipping versus non-convergence without clipping. Middle: Full versus partial participation, showing faster convergence with clipping. Right: Clipping multiplier \u03bb sensitivity, demonstrating consistent linear convergence across varying \u03bb values.", "description": "This figure shows the optimality gap, which is the difference between the current function value and the optimal function value, in three different experimental scenarios. The first scenario compares the linear convergence rate of the Byz-VR-MARINA-PP algorithm with and without gradient clipping, showing the critical role of clipping for convergence when facing Byzantine workers. The second scenario contrasts the convergence speed under full participation versus partial participation, highlighting the benefit of partial participation when combined with clipping. The third scenario examines the sensitivity of convergence to the clipping multiplier (\u03bb), demonstrating consistent linear convergence across a range of \u03bb values.", "section": "Numerical Experiments"}]