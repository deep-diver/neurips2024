[{"figure_path": "XlAbMZu4Bo/tables/tables_1_1.jpg", "caption": "Table 1: Performance on standard academic benchmarks, compared to open-source base models. We reported model size, context length and total data tokens during model pretraining. \u2013 indicates that the number was not reported in the original paper.", "description": "This table compares the performance of MEGALODON-7B against other open-source large language models (LLMs) on a range of standard academic benchmarks.  It shows the model size, the number of training tokens used, and the maximum context length for each model.  Performance is measured on several tasks, reflecting various aspects of language understanding. The dashes indicate that some values were not available in the original papers.", "section": "4.2 Short-Context Evaluation on Academic Benchmarks"}, {"figure_path": "XlAbMZu4Bo/tables/tables_7_1.jpg", "caption": "Table 3: MT Bench. Comparison of Chat models. * LLAMA2-Chat utilizes RLHF.", "description": "This table compares the performance of several 7B parameter chat models on the MT-Bench benchmark.  The models include Vicuna, LLAMA2-Chat (which uses Reinforcement Learning from Human Feedback - RLHF), Mistral-Instruct, and MEGALODON. The MT-Bench score is a measure of the models' performance on a variety of tasks, with lower scores indicating better performance.  The table shows that MEGALODON performs comparably to LLAMA2-Chat, despite not using RLHF.", "section": "4.4 Instruction Finetuning"}, {"figure_path": "XlAbMZu4Bo/tables/tables_8_1.jpg", "caption": "Table 4: (ImageNet-1K) Top-1 accuracy.", "description": "This table presents the top-1 accuracy results on the ImageNet-1K dataset for several different image classification models.  The models compared include ResNet-152, ViT-B, DeiT-B, MEGA, and MEGALODON.  The table shows the number of parameters for each model and its corresponding top-1 accuracy.  The purpose is to benchmark the performance of MEGALODON against established and related models on a standard image classification task.", "section": "4.5 Evaluation on Medium-Scale Benchmarks"}, {"figure_path": "XlAbMZu4Bo/tables/tables_8_2.jpg", "caption": "Table 5: (PG-19) Word-level perplexity.", "description": "This table presents the word-level perplexity results on the PG-19 benchmark for several autoregressive language models, including the proposed MEGALODON model.  The table compares MEGALODON's performance against existing models with different parameter counts, showing its improved performance on this specific benchmark.", "section": "4.5 Evaluation on Medium-Scale Benchmarks"}, {"figure_path": "XlAbMZu4Bo/tables/tables_15_1.jpg", "caption": "Table 1: Performance on standard academic benchmarks, compared to open-source base models. We reported model size, context length and total data tokens during model pretraining. \u2013 indicates that the number was not reported in the original paper.", "description": "This table compares the performance of MEGALODON-7B against various other open-source language models on a range of standard academic benchmarks.  The benchmarks assess capabilities in different areas like commonsense reasoning, world knowledge, reading comprehension, and question answering.  The table also shows each model's size (in billions of parameters), the context length (maximum sequence length it can handle), the total number of training tokens, and performance scores for each benchmark. The '-' symbol shows when the original paper did not provide data for that entry. This allows for a direct comparison of MEGALODON's performance against similar-sized models and highlights its strengths and weaknesses.", "section": "4.2 Short-Context Evaluation on Academic Benchmarks"}, {"figure_path": "XlAbMZu4Bo/tables/tables_16_1.jpg", "caption": "Table 7: (SC-Raw) Accuracy.", "description": "This table presents the results of the raw speech classification experiments using the Speech Commands dataset.  The models are compared based on their accuracy and number of parameters.  The goal is to evaluate how well the different models classify raw audio without the use of traditional signal processing techniques. MEGALODON achieves the highest accuracy (98.14) among the compared models.", "section": "4.5 Evaluation on Medium-Scale Benchmarks"}, {"figure_path": "XlAbMZu4Bo/tables/tables_16_2.jpg", "caption": "Table 8: (WikiText-103) Word-level PPL.", "description": "This table compares the word-level perplexity (PPL) results of several autoregressive language models on the WikiText-103 dataset.  The models include a standard Transformer, Transformer-XL, S4, MEGA, and the proposed MEGALODON.  The table shows the number of parameters and the PPL achieved by each model, highlighting MEGALODON's improvement over existing models.", "section": "4.5 Evaluation on Medium-Scale Benchmarks"}]