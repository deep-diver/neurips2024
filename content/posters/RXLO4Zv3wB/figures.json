[{"figure_path": "RXLO4Zv3wB/figures/figures_1_1.jpg", "caption": "Figure 1: Example of Degradation Response Variations. We apply the same level of Gaussian Blur to different images from the LIVEitw [14] dataset. x and xa denote the original and degraded images, respectively. l(\u00b7, \u00b7) is the LPIPS metric [3] between x and xd, which measures the extent of changes in the feature space. The results demonstrate that images with different content and texture characteristics exhibit varying degrees of change.", "description": "This figure shows examples of how different images respond to the same Gaussian blur.  The top row shows original images, and the bottom row shows the same images after blurring. The numbers below each pair represent the LPIPS distance between the original and blurred versions.  LPIPS is a metric that measures perceptual similarity, so lower numbers indicate smaller changes in deep features; the images with simpler textures change less dramatically after blurring.", "section": "1 Introduction"}, {"figure_path": "RXLO4Zv3wB/figures/figures_1_2.jpg", "caption": "Figure 2: Distribution of DDR on the LIVEitw [14] dataset. \"Low\", \"optimal\", and \"high\" refer to different levels of handcrafted degradation applied in the pixel domain, while \"adaptive\" represents adaptively fusing text-driven degradation in the feature domain. The DDR with \"optimal\" and \"adaptive\" degradation achieve significantly better performance on the BIQA task.", "description": "This figure shows the distribution of Deep Degradation Response (DDR) values for images in the LIVEitw dataset under different degradation levels.  The x-axis represents the DDR value, and the y-axis represents the number of images with that DDR value. Four different degradation methods are compared: low, optimal, high levels of handcrafted degradation applied directly to the pixel domain of the image, and an adaptive method using text-driven feature-space degradation. The figure demonstrates that the adaptive method produces a DDR distribution similar to the optimal handcrafted method, suggesting its effectiveness in achieving optimal performance for Blind Image Quality Assessment (BIQA).", "section": "3 Deep Degradation Response as Flexible Image Descriptor"}, {"figure_path": "RXLO4Zv3wB/figures/figures_3_1.jpg", "caption": "Figure 3: The framework of our proposed DDR with two different degradation fusing methods. (a) Synthesizing degradation with a handcrafted process in the pixel domain. (b) Fusing text-driven degradation in the feature domain.", "description": "This figure illustrates the two methods used to compute the Deep Degradation Response (DDR). Method (a) uses handcrafted degradation in the pixel domain, applying degradation directly to the image and then extracting features. Method (b) uses text-driven feature degradation, leveraging a text encoder to generate a degradation representation from text prompts and then fusing this representation with the image features to create degraded features.  The DDR is then calculated as a distance metric between the original and degraded features.", "section": "3 Deep Degradation Response as Flexible Image Descriptor"}, {"figure_path": "RXLO4Zv3wB/figures/figures_4_1.jpg", "caption": "Figure 4: Images with high and low DDR to different degradation types. We measure DDR with five types of degradation by setting their corresponding prompt pair. We observe that image with lower DDR to a specific type of degradation is likely to obtain this degradation of a higher level.", "description": "This figure shows example images with high and low DDR values for five different degradation types (color, noise, blur, exposure, and content).  The DDR was calculated using a text-driven degradation method in the feature domain. Images with lower DDR values for a specific degradation type show a higher degree of that degradation, suggesting that DDR is correlated with the level of degradation present.", "section": "3.2 DDR as a Flexible Image Descriptor"}, {"figure_path": "RXLO4Zv3wB/figures/figures_7_1.jpg", "caption": "Figure 5: Qualitative result on RealBlur [52] dataset. The training of model is supervised by (1) reconstruction loss (PSNR) (2) reconstruction loss combined with feature domain loss, and (3) reconstruction loss combined with DDR. The red area is cropped from different results and enlarged for visual convenient. Appending DDR as an external self-supervised learning objective leads to result with more natural texture and less artifacts.", "description": "This figure shows a qualitative comparison of image deblurring results using different loss functions.  The top row shows an example image and the results obtained using PSNR loss alone, PSNR+LPIPS, PSNR+CTX, PSNR+PDL, PSNR+FDL, and PSNR+DDR (the proposed method). The bottom row shows zoomed-in portions of the same results, highlighting the difference in texture and artifacts.  The addition of DDR as a loss function leads to more natural-looking results with fewer artifacts.", "section": "4.3 Image Motion Deblurring"}, {"figure_path": "RXLO4Zv3wB/figures/figures_8_1.jpg", "caption": "Figure 6: Qualitative result on real-world SISR dataset [53, 54]. DDR leads to results with sharper texture.", "description": "This figure shows a qualitative comparison of single image super-resolution (SISR) results using different loss functions.  The leftmost image is the low-resolution (LR) input. The next three images show results using PSNR loss alone, PSNR loss combined with LPIPS perceptual loss, and PSNR loss combined with the proposed DDR loss, respectively. Red boxes highlight areas where the DDR loss produces noticeably sharper and more detailed textures compared to other methods.", "section": "4.4 Single Image Super Resolution"}, {"figure_path": "RXLO4Zv3wB/figures/figures_14_1.jpg", "caption": "Figure 7: SRCC between DDR and statistics of deep features. Deep features are extracted from different layers of pre-trained VGG [67] network.", "description": "This figure shows the Spearman Rank Correlation Coefficient (SRCC) between the Deep Degradation Response (DDR) and the statistics of deep features extracted from different layers of a pre-trained VGG network.  The statistics considered are the mean and standard deviation of the deep features.  The figure visualizes how the correlation between DDR and deep feature statistics changes depending on the layer of the network (relu1_2, relu2_2, relu3_3, relu4_3, and relu5_3) and the type of degradation used (color, noise, blur, content, and exposure).  The purpose is to show how the DDR correlates with different deep feature characteristics and how this relationship varies based on the type of image degradation.", "section": "A.2 Additional Experiment Results"}, {"figure_path": "RXLO4Zv3wB/figures/figures_16_1.jpg", "caption": "Figure 8: Qualitative result of image deblurring using the NAFNet [62] trained with GoPro [51] dataset. The red area is cropped from different results and enlarged for visual convenient.", "description": "This figure shows a qualitative comparison of image deblurring results using the NAFNet model trained on the GoPro dataset.  Four different loss functions were compared: PSNR (peak signal-to-noise ratio), LPIPS (Learned Perceptual Image Patch Similarity), CTX (Contextual Loss), PDL (Projected Distribution Loss), FDL (Focal Frequency Loss), and the proposed DDR (Deep Degradation Response) loss.  The results are displayed as image pairs, showing the blurred input image, ground truth, and outputs from each of the loss functions. A red box highlights a region of interest for easier comparison of the deblurring effects.", "section": "4.3 Image Motion Deblurring"}, {"figure_path": "RXLO4Zv3wB/figures/figures_17_1.jpg", "caption": "Figure 9: Qualitative result of image deblurring using the NAFNet [62] trained with realBlur [52] dataset. The red area is cropped from different results and enlarged for visual convenient.", "description": "This figure shows the qualitative results of image deblurring using the NAFNet model trained on the RealBlur dataset.  It compares the results obtained using different loss functions: PSNR (Peak Signal-to-Noise Ratio), LPIPS (Learned Perceptual Image Patch Similarity), CTX (Contextual Loss), PDL (Projected Distribution Loss), FDL (Focal Frequency Loss), and DDR (Deep Degradation Response). The red boxes highlight zoomed-in areas for a detailed comparison.  The image shows DDR loss function outperforms others for producing higher-quality results.", "section": "4.3 Image Motion Deblurring"}, {"figure_path": "RXLO4Zv3wB/figures/figures_17_2.jpg", "caption": "Figure 10: Qualitative result of SISR using the NAFNet [62] trained with real-world SISR [53, 54] dataset. The red area is cropped from different results and enlarged for visual convenient.", "description": "This figure shows a qualitative comparison of single image super-resolution (SISR) results obtained using different loss functions. The top row displays a close-up of a radial pattern image, and the bottom row shows a close-up of a textured image. For each image, four versions are presented: the low-resolution input (LR), results optimized using the PSNR loss function, results optimized using the LPIPS perceptual loss function, and results optimized using the proposed DDR loss function. The red boxes highlight the regions where the differences are most apparent. This figure visually demonstrates that using DDR as a loss function yields more natural and sharper results than using PSNR or LPIPS alone.", "section": "4.4 Single Image Super Resolution"}]