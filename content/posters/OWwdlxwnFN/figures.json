[{"figure_path": "OWwdlxwnFN/figures/figures_6_1.jpg", "caption": "Figure 1: Sample stimuli and corresponding reconstructions from models. The \"Spatial\" and \"Spatiotemporal\" column show results from the pre-trained inverse retinotopic mapping model, explained in Section 3.2.1. The \"End-to-end\" column shows reconstructions from the space-resolved model with a component that learns the neuron's receptive field explained in Section 3.2.1. \"Baseline\" shows the reconstructions of a model we implemented explained in Section 3.2.2.", "description": "This figure shows a comparison of image reconstruction results from four different decoding models: a spatial model, a spatiotemporal model, an end-to-end model, and a baseline model.  Each model's performance is evaluated on a set of sample images.  The results illustrate the models' abilities to reconstruct various image features, and highlights the improvements achieved by incorporating spatiotemporal information and a learned receptive field layer.", "section": "4.1 Stimulus reconstruction"}, {"figure_path": "OWwdlxwnFN/figures/figures_7_1.jpg", "caption": "Figure 2: Spatial occlusion analysis of spatial model as explained in Section 4.1.2. Title above column means included brain region.", "description": "This figure shows the results of a spatial occlusion analysis, where the model's input from different brain regions (V1, V4, IT) was selectively occluded. The columns represent the regions of interest, while the rows present different example stimuli. For each region, the neural responses were set to their baseline values (pre-stimulus onset) for the occlusion procedure.  The reconstructions demonstrate how the absence of input from a specific brain area impacts the final reconstructed images, highlighting the importance of each brain area's contribution to visual processing.", "section": "4.1 Stimulus reconstruction"}, {"figure_path": "OWwdlxwnFN/figures/figures_8_1.jpg", "caption": "Figure 3: Spatiotemporal occlusion analysis. Yellow indicates the active time window, with others occluded.", "description": "This figure demonstrates the results of a spatiotemporal occlusion analysis.  Multiple example stimuli are shown, with their corresponding reconstructions when different time windows of neural responses are included or excluded in the model. Each column represents a different set of time windows being occluded, with the first column showing the reconstruction using all time windows. The yellow highlighted regions indicate which time windows' data are used to reconstruct the image. This visualization is used to illustrate how different temporal components of neural activity contribute to the reconstruction of the visual stimuli.", "section": "4.1 Spatiotemporal occlusion analysis"}, {"figure_path": "OWwdlxwnFN/figures/figures_8_2.jpg", "caption": "Figure 4: Distribution of colorfulness metrics across V1, V4, and IT-constrained reconstructions, calculated using the Composite Colorfulness Score (CCS) based on RGB channel differences.", "description": "This histogram displays the distribution of colorfulness scores for image reconstructions derived from neural activity in three different brain regions (V1, V4, and IT). The colorfulness metric quantifies the perceptual quality of color in the images.  The means for each region are displayed, showing a clear trend of increasing colorfulness from IT to V1, likely reflecting the hierarchical processing of color information in the visual cortex.  The graph helps in visualizing the effect of restricting input to the reconstruction model to only a certain brain area.", "section": "Results and discussion"}, {"figure_path": "OWwdlxwnFN/figures/figures_11_1.jpg", "caption": "Figure 5: Overview of how the main reconstruction model is trained. A. The U-NET component is trained with a stack of 2D tensors (illustrated in grey) as input. These tensors are processed to produce reconstructions (depicted in yellow). The difference between the reconstructions and the target stimuli (represented in blue) are computed using the adversarial loss, feature loss, and pixel loss. B. Concurrently, the discriminator component undergoes its training phase. It evaluates the reconstructed outputs from the U-NET (labeled as 'fake images') alongside the original target images (labeled as 'real images'). This evaluation plays a critical role in calculating the Adversarial Loss, which is instrumental in guiding the parameter updates for the U-NET. This synergistic training approach ensures the progressive enhancement of the U-NET's ability to generate increasingly accurate and realistic reconstructed images.", "description": "This figure shows the training process of the main reconstruction model which is a U-NET. The U-NET processes a stack of 2D tensors to produce reconstructions. The differences between the reconstructions and target stimuli are used to compute loss functions (adversarial, feature, and pixel loss). The discriminator evaluates reconstructed outputs and target images. This process helps the U-NET generate more accurate images.", "section": "3.2 Models"}, {"figure_path": "OWwdlxwnFN/figures/figures_12_1.jpg", "caption": "Figure 6: A. The inverse receptive field layer produces for each brain response r\u2208 R an RF activation map (M) (also known as the embedding layer E) by using the learnable parameters (\u03bc\u03b1, \u03bcy, \u03c3) in conjunction with the width (W) and height (H) of the desired model inputs (X) with a 2D Gaussian function. B. Let R[H\u00d7W] be a matrix in RH\u00d7W such that each entry is r. R[H\u00d7W] is multiplied element-wise with its corresponding M, and then stacked based on its electrode number, resulting in 15 X in total (7 for V1, 4 for V4, and 4 for IT).", "description": "Figure 6 illustrates the process of the inverse receptive field layer in the homeomorphic decoder. Panel A shows the computation of the RF activation map (M) for each brain response r, using learnable parameters (\u00b5x, \u00b5y, \u03c3) and a 2D Gaussian function. Panel B depicts how the brain responses (R), which are represented as a matrix R[H\u00d7W], are combined with their corresponding activation maps (M) and stacked based on electrode number to generate the final input for the decoder. The combination of responses from V1, V4 and IT areas results in a 15-channel input (7 from V1, 4 from V4, and 4 from IT).", "section": "3.2 Models"}, {"figure_path": "OWwdlxwnFN/figures/figures_12_2.jpg", "caption": "Figure 7: The learned 2D Gaussian parameters as spatial receptive field maps for mapping the neuronal signals in visual space as input for the reconstruction model. The \"Visual field\" shows the learned mappings in 2D space. The plot adjacent shows the variations in size of these RFs as a function of distance from the foveal center, highlighting how the learned RFs expands with increased eccentricity.", "description": "This figure shows the learned receptive fields (RFs) of the model, visualizing how the model learned to map neuronal signals to spatial locations in the visual field. The left panel shows the spatial distribution of the receptive fields, while the right panel shows how the size of the receptive fields changes with distance from the center of the visual field (fovea). Different colors represent different brain regions (V1, V4, IT).", "section": "4.1.1 Model comparison"}, {"figure_path": "OWwdlxwnFN/figures/figures_13_1.jpg", "caption": "Figure 8: Relative correlation analysis of ROI-constrained reconstructions with AlexNet features. This figure shows the relative correlation coefficients between features from ROI-constrained reconstructions (V1, V4, IT) and corresponding AlexNet layers, normalized per brain region for fair comparison. Higher relative correlations are indicated by deeper colors and larger bars, marking the ROI reconstruction with the closest match to each AlexNet layer's processing characteristics.", "description": "This figure displays the relative correlation coefficients between features extracted from reconstructed images (using only V1, V4, IT, and all brain regions) and features from different layers of a pre-trained AlexNet model.  The relative correlations are normalized for each brain region, allowing direct comparison across regions and layers.  Higher correlations are indicated by deeper colors and larger bar heights, visually highlighting which ROI reconstruction best aligns with specific processing stages within AlexNet.", "section": "4.1.2 Spatial occlusion analysis"}, {"figure_path": "OWwdlxwnFN/figures/figures_13_2.jpg", "caption": "Figure 9: Temporal relative correlation analysis across AlexNet layers. This figure illustrates relative correlation coefficients across multiple time windows and AlexNet layers, with color and bar size representing the highest relative (not absolute) correlations per brain region. The x-axis is normalized, allowing direct comparison of relative contributions across time points.", "description": "This figure shows the relative correlation coefficients between features from reconstructions at different time windows and various AlexNet layers.  The color intensity and bar length represent the strength of the correlation, normalized per brain region for easier comparison. The x-axis is normalized, enabling a direct comparison of the relative contributions across different time points.", "section": "4.1.2 Spatiotemporal occlusion analysis"}, {"figure_path": "OWwdlxwnFN/figures/figures_14_1.jpg", "caption": "Figure 10: Correlation of features across time and brain, where i is varying timepoints of the initial timewindow for each ROI (V1: 0-27ms, V4: 33-60ms, IT: 66-93ms) after stimulus onset and +26 is the 26 shift of all of the three windows.", "description": "This figure shows the correlation of features across different time windows and brain regions (V1, V4, IT). It demonstrates how the relative contribution of each brain region changes over time in reconstructing visual stimuli.  The varying time windows after stimulus onset are represented (0-27ms for V1, 33-60ms for V4, 66-93ms for IT) and subsequent 26ms shifts.  The higher correlations are indicated by deeper colors and larger bars.", "section": "4.1.2 Spatial occlusion analysis"}, {"figure_path": "OWwdlxwnFN/figures/figures_14_2.jpg", "caption": "Figure 11: Training model with ablated components.", "description": "This figure shows the impact of removing different components from the training model on the quality of the reconstructed images. The columns represent different model variations: no ablation (full model), no discriminator, no L1 loss, and no VGG loss. Each row represents a different stimulus image, with the corresponding reconstructions shown in each column. Comparing the reconstructions across different columns helps visualize the role of each component in the model's performance.", "section": "4 Results and discussion"}, {"figure_path": "OWwdlxwnFN/figures/figures_15_1.jpg", "caption": "Figure 12: Training model on various brain regions.", "description": "This figure shows the results of training the model on different brain regions (V1, V4, IT, and V1+V4+IT).  Each column represents a different brain region used for training, and each row displays reconstructed images corresponding to a specific input stimulus from the THINGS dataset. By comparing the reconstructed images across different brain regions, the figure aims to demonstrate the impact of the source brain region on the quality and features of the reconstructed images, highlighting the region-specific contributions to image reconstruction.", "section": "4.1.2 Spatial occlusion analysis"}]