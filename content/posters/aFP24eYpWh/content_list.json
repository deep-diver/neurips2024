[{"type": "text", "text": "A Generative Model of Symmetry Transformations ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "James Urquhart Allingham ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "University of Cambridge jua23@cam.ac.uk ", "page_idx": 0}, {"type": "text", "text": "Bruno Kacper Mlodozeniec ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "University of Cambridge MPI for Intelligent Systems, T\u00fcbingen bkm28@cam.ac.uk ", "page_idx": 0}, {"type": "text", "text": "Shreyas Padhy University of Cambridge sp2058@cam.ac.uk ", "page_idx": 0}, {"type": "text", "text": "Javier Antor\u00e1n   \nUniversity of Cambridge   \n\u00c5ngstrom AI   \nja666@cam.ac.uk ", "page_idx": 0}, {"type": "text", "text": "David Krueger University of Cambridge david.scott.krueger@gmail.com ", "page_idx": 0}, {"type": "text", "text": "Richard E. Turner University of Cambridge ret26@cam.ac.uk ", "page_idx": 0}, {"type": "text", "text": "Eric Nalisnick University of Amsterdam e.t.nalisnick@uva.nl ", "page_idx": 0}, {"type": "text", "text": "Jos\u00e9 Miguel Hern\u00e1ndez-Lobato University of Cambridge jmh233@cam.ac.uk ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Correctly capturing the symmetry transformations of data can lead to efficient models with strong generalization capabilities, though methods incorporating symmetries often require prior knowledge. While recent advancements have been made in learning those symmetries directly from the dataset, most of this work has focused on the discriminative setting. In this paper, we take inspiration from group theoretic ideas to construct a generative model that explicitly aims to capture the data\u2019s approximate symmetries. This results in a model that, given a prespecified broad set of possible symmetries, learns to what extent, if at all, those symmetries are actually present. Our model can be seen as a generative process for data augmentation. We provide a simple algorithm for learning our generative model and empirically demonstrate its ability to capture symmetries under affine and color transformations, in an interpretable way. Combining our symmetry model with standard generative models results in higher marginal test-log-likelihoods and improved data efficiency. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "image", "img_path": "aFP24eYpWh/tmp/65fee6ba8133b397be006740dcd5cab9ca67873cdb7544f9998e14c999598204.jpg", "img_caption": ["Figure 1: Left: An example of a symmetry-aware generative process that we aim to model in this paper. A prototype \u02c6x $\\mathbb{(}\\boxed{\\bigcirc}$ is transformed by ${\\mathcal{T}}_{\\mathfrak{n}}$ into an observation $\\mathbf{x}$ ( , , ). The transformation\u2014 e.g., rotation\u2014is parameterized by $\\boldsymbol\\upeta$ \u2014e.g., an angle. Right: The corresponding orbit\u2014i.e., the set of all possible instances of $\\mathbf{x}$ that can result from applying $\\mathcal{T}_{\\mathfrak{n}}$ \u2014with a few elements shown. Under this generative process, the prototype is an arbitrary orbit element. Each element in the orbit has a probability $p\\left(\\mathbf{x}\\mid{\\hat{\\mathbf{x}}}\\right)$ induced by $p\\left(\\mathfrak{n}\\mid\\hat{\\mathbf{x}}\\right)$ . E.g., for handwritten $^3$ \u2019s, we expect digits in an upright orientation with some rotation around, say $\\pm40^{\\circ}$ , corresponding to natural variations in handwriting. "], "img_footnote": [], "page_idx": 0}, {"type": "text", "text": "Many physical phenomena exhibit symmetries; for example, many of the observable galaxies in the night sky share similar characteristics when accounting for their different rotations, velocities, and sizes. Hence, if we are to represent the world with generative models, they can be made more faithful and data-efficient by incorporating notions of symmetry. This has been well-understood for discriminative models for decades. Incorporating inductive biases such as invariance or equivariance to symmetry transformations dates back (at least) to ConvNets, which incorporate translation symmetries [LeCun et al., 1989]\u2014and can be extended to reflection and rotation [Cohen and Welling, 2016]\u2014and more recently, transformers, with permutation symmetries [Lee et al., 2019]. ", "page_idx": 1}, {"type": "text", "text": "In many cases, it is not known a priori which symmetries are present in the data. Learning symmetries in discriminative modeling is an active field of research [Nalisnick and Smyth, 2018, van der Wilk et al., 2018, Benton et al., 2020, Schw\u00f6bel et al., 2021, van der Ouderaa and van der Wilk, 2022, Rommel et al., 2022, Romero and Lohit, 2022, Immer et al., 2022, 2023, Miao et al., 2023, Mlodozeniec et al., 2023]. However, in these works\u2014which focus on invariant discriminative models\u2014the label is often assumed to be invariant, and thus, the symmetry information can be removed rather than explicitly modeled. On the other hand, a generative model must capture the factors of variation corresponding to the symmetry transformations of the data. Doing so can provide beneftis such as better representation learning\u2014by disentangling symmetry from other latent variables [Antor\u00e1n and Miguel, 2019]\u2014and data efficiency\u2014due to compactly encoding of factor(s) of variation corresponding to symmetries. Furthermore, learning about underlying symmetries in data could be used for scientific discovery. ", "page_idx": 1}, {"type": "text", "text": "We propose a generative model that explicitly encodes the (partial) symmetries in the data. Here, we are primarily interested in using this model to inspect the distribution over naturally occurring transformations for a given example $\\mathbf{x}$ , and resample new \u201cnaturally\u201d augmented versions of the example. Our contributions are ", "page_idx": 1}, {"type": "text", "text": "1. We propose a Symmetry-aware Generative Model (SGM). The SGM\u2019s latent representation is separated into an invariant component $\\hat{\\bf x}$ and an equivariant component $\\boldsymbol{\\mathfrak{\\eta}}$ . The latter, $\\boldsymbol{\\mathfrak{n}}$ , captures the symmetries in the data, while $\\hat{\\bf x}$ captures none. We recover $\\mathbf{x}$ by applying a parameterised transformation, $\\mathbf{x}=T_{\\mathfrak{n}}(\\hat{\\mathbf{x}})$ . We call $\\hat{\\bf x}$ a prototype since each $\\hat{\\bf x}$ can produce arbitrarily transformed observations; see Figure 1.   \n2. We propose a two-stage algorithm for learning our SGM: first learning $\\hat{\\bf x}$ using a selfsupervised approach and then learning $\\boldsymbol{\\mathfrak{\\eta}}$ via maximum likelihood. Importantly, this does not require modeling the distribution of prototypes $p\\left({\\hat{\\mathbf{x}}}\\right)$ , allowing the procedure to remain tractable even for complex data.   \n3. We verify experimentally that our SGM completely captures affine and color symmetries. A VAE\u2019s marginal test-log-likelihood can improved by using our SGM to incorporate symmetries. Additionally, unlike a standard VAE, explicitly modeling symmetries makes our VAE-SGM hybrid robust to deleting half of the dataset. ", "page_idx": 1}, {"type": "text", "text": "Notation. We use $a,\\,a$ , and $\\pmb{A}$ (i.e., lower, bold lower, and bold upper case) for scalars, vectors, and matrices, respectively. We distinguish between random variables such as x, \u03b7, A, and their realizations $\\pmb{x},\\pmb{\\eta},\\pmb{A}$ . Thus, for continuous a, $p\\left(\\mathbf{a}\\right)$ is a PDF that returns a density $p\\left(\\mathbf{a}=\\mathbf{a}\\right)=p\\left(a\\right)$ . We use $\\circ$ to represent function composition, e.g., $f_{1}\\circ f_{2}$ . ", "page_idx": 1}, {"type": "text", "text": "2 Symmetry-aware Generative Model (SGM) ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Consider a dataset of observations $\\{{\\pmb x}_{n}\\}_{n=1}^{N}$ on a space $\\mathcal{X}$ , and a collection $\\{\\tau_{\\eta}\\}$ of transformations $\\mathcal{T}_{\\eta}:\\mathcal{X}\\rightarrow\\mathcal{X}$ parameterised by transformation parameters $\\pmb{\\eta}\\in\\mathcal{H}\\subseteq\\mathbb{R}^{d_{\\eta}}$ . We assume $\\{\\tau_{\\eta}\\}_{\\eta\\in\\mathcal{H}}$ (abbreviated $\\{\\tau_{\\eta}\\},$ ) form a group. Loosely, our aim is to model the distribution over transformations present in the data. To do so, we model the distribution $p\\left(\\mathbf{x}\\right)$ by decomposing it into two disparate parts: (1) a distribution over prototypes and (2) a distribution over parameters controlling transformations to be applied to a prototype. Concretely, we specify our generative model as follows (also depicted in Figure 2): ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{\\mathbf{x}}\\sim p\\left(\\hat{\\mathbf{x}}\\right),}\\\\ &{\\eta\\sim p_{\\Psi}\\left(\\eta\\left|\\right.\\hat{\\mathbf{x}}\\right),}\\\\ &{\\mathbf{x}=\\mathcal{T}_{\\eta}\\left(\\hat{\\mathbf{x}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "image", "img_path": "aFP24eYpWh/tmp/cad183827d11ebdc44492d7005001e3a5dd7adbf6d9c3dc17ce376e194d65388.jpg", "img_caption": [], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "Figure 4: Self-supervised symmetry learning. We encourage $f_{\\boldsymbol{\\omega}}(\\mathbf{x})$ to be equivariant by mapping $\\textbf{\\em x}$ and a randomly transformed $\\textbf{\\em x}$ to the same $\\hat{\\pmb x}$ . Gray text shows examples for each variable in the graph. Note that $\\hat{\\pmb{x}}$ and $x_{\\mathrm{{rnd}}}$ may not appear in the dataset; see Figure 1. ", "page_idx": 2}, {"type": "text", "text": "That is, the SGM assumes that each observation $\\mathbf{x}$ is generated by applying a transformation ${\\mathcal{T}}_{\\mathfrak{n}}-$ parameterized by a latent variable $\\boldsymbol{\\mathfrak{n}}$ \u2014to a latent prototype $\\hat{\\bf x}$ . Since $\\hat{\\bf x}$ , by assumption, contains no information about the symmetries in the data, $p_{\\Psi}(\\mathfrak{n}\\,|\\,\\hat{\\mathbf{x}})$ must model the distribution over the transformations ${\\mathcal{T}}_{\\mathfrak{n}}$ present in the data. ", "page_idx": 2}, {"type": "text", "text": "Motivation. Why would we expect specifying $p\\left(\\mathbf{x}\\right)$ in this way to be useful? Firstly, our SGM allows us to query a distribution over naturally occurring transformations $p_{\\Psi}(\\mathfrak{n}\\,|\\,\\hat{\\mathbf{x}}=T_{\\eta}^{-1}(\\mathbf{x}))$ for any input $\\textbf{\\em x}$ , given the matching prototype $\\hat{\\pmb{x}}:=\\mathscr{T}_{\\pmb{\\eta}}^{-1}({\\pmb{x}})$ . Secondly, we expect our SGM to align with the true physical process of generating the data for many interesting datasets. As an illustrative example, when a person writes a digit, they first decide what kind of digit to write\u2014e.g., the prototype could be an upright $\\langle3\\rangle$ \u2014but when they put pen to paper, the digit ", "page_idx": 2}, {"type": "image", "img_path": "aFP24eYpWh/tmp/fa6c0b7b116409c8d1df42c668aae805a0dcfb4346b4798135a19c4cbc41c15f.jpg", "img_caption": [], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "Figure 2: SGM graphical model. The implicit edges denote that $\\hat{\\bf x}$ is fully specified by $\\boldsymbol{\\mathfrak{n}}$ and x\u2014 since $\\hat{\\mathbf{x}}=\\mathcal{T}_{\\mathfrak{n}}^{-1}(\\mathbf{x})$ \u2014and thus only $\\boldsymbol{\\mathfrak{n}}$ needs to be inferred given and observation $\\mathbf{x}$ . ", "page_idx": 2}, {"type": "text", "text": "they pictured is transformed due to various factors governing their handwriting1. Similarly, when a photographer captures an object, the photo is also a function of latent factors of variation, such as lighting, the lens, camera shake, etc. ", "page_idx": 2}, {"type": "text", "text": "What do we require of a prototype? \u02c6x can informally be considered a canonical/reference example with no transformation applied to it. More precisely, we require that for any orbit of an element $\\mathbf{x}$ \u2014defined as the set of elements in $\\mathcal{X}$ which $\\mathbf{x}$ can be mapped to by a transformation in $\\{\\mathcal{T}_{\\mathfrak{n}}\\}$ \u2014there is exactly one prototype in the orbit. Figure 1 depicts an example orbit\u2014a set { , , , $\\left|...\\right\\}$ of all rotated variants of a $\\varsigma_{3},$ \u2014with a unique prototype. ", "page_idx": 2}, {"type": "text", "text": "Why do we want a group? Having the transformations $\\{\\mathcal{T}_{\\mathfrak{n}}\\}$ be a group simplifies things, since $\\{\\mathcal{T}_{\\mathfrak{n}}\\}$ will then naturally partition the space $\\mathcal{X}$ into (disjoint) orbits. Within each orbit, every element can be transformed into one another with a transformation in $\\{\\mathcal{T}_{\\mathfrak{n}}\\}$ . As an example of such a partition, if our collection of transformations were horizontal shifts $T_{\\mathfrak{n}}:\\mathbf{x}\\mapsto\\mathbf{x}+(\\upeta,0)$ acting on a point $\\mathbf{x}\\in\\mathbb{R}^{2}$ , then the different orbits will correspond to all points on a given horizontal line; see Figure 3. Therefore, if we have chosen a unique prototype for each orbit and $\\{\\mathcal{T}_{\\mathfrak{n}}\\}$ forms a group, any two elements $\\mathbf{x},\\mathbf{x}^{\\prime}\\in\\mathcal{X}$ will have the same prototype if and only if they can be transformed into one another. ", "page_idx": 2}, {"type": "text", "text": "In Section 2.1, we describe a method for learning a transformation inference function $f_{\\omega}:\\mathcal{X}\\to\\mathcal{H}$ , with parameters $\\boldsymbol{\\omega}$ , that for $\\mathbf{x}\\in\\mathcal{X}$ returns transformation parameters $\\mathfrak{n}\\in\\mathcal{H}$ as $\\begin{array}{r}{\\boldsymbol{\\mathfrak{n}}=f_{\\omega}(\\mathbf{x})}\\end{array}$ . These map to a prototype that gen ", "page_idx": 2}, {"type": "table", "img_path": "aFP24eYpWh/tmp/1b79e073428c6dc89a20d807da69190b95acf207a482a74995c15f5ad4ac6838.jpg", "table_caption": [], "table_footnote": [], "page_idx": 2}, {"type": "text", "text": "Figure 3: Orbits due to horizontal shift transformations. Each point $(x_{1},x_{2})$ is transformed via $T_{\\eta}:(x_{1},x_{2})\\stackrel{.}{\\mapsto}(x_{1},x_{2}){+}(\\eta,0)$ . Thus, horizontal lines form disjoint orbits in which any point can be transformed into any other point on the same line but not on another line. For each line, we can choose an arbitrary prototype $(\\,\\bullet)$ from which all other points on the line can be reached via $\\mathcal{T}_{\\eta}$ . ", "page_idx": 2}, {"type": "text", "text": "${\\mathcal{T}}_{\\mathfrak{n}}(\\hat{\\mathbf{x}})^{2}$ . We then apply standard generative modeling tools to learn $p\\left({\\hat{\\mathbf{x}}},\\,{\\mathfrak{n}}\\right)=p\\left({\\hat{\\mathbf{x}}}\\right)p_{\\Psi}\\left({\\mathfrak{n}}\\mid{\\hat{\\mathbf{x}}}\\right)$ given the generated data pairs {x\u02c6n, \u03b7n}nN=1. ", "page_idx": 2}, {"type": "text", "text": "2.1 Learning ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We now discuss learning for the two NNs required by our model, $f_{\\boldsymbol{\\omega}}(\\mathbf{x})$ and $p_{\\Psi}(\\mathfrak{n}\\,|\\,\\hat{\\mathbf{x}})$ . In Appendix A, we connect our learning algorithm with MLL optimization using an ELBO. ", "page_idx": 3}, {"type": "text", "text": "Transformation inference function. For $\\mathcal{T}_{\\mathfrak{n}}^{-1}$ , with $\\boldsymbol{\\mathfrak{n}}$ given by $f_{\\omega}$ , to map $\\mathbf{x}$ to a prototype $\\hat{\\bf x}$ , it must, by definition, map all elements in any given orbit to the same element in that orbit. In other words, the output of $\\boldsymbol{\\mathcal{T}}_{f_{\\omega}(\\mathbf{x})}^{-1}(\\mathbf{\\boldsymbol{x}})$ should be invariant to transformations ${\\mathcal{T}}_{\\eta^{\\prime}}$ of $\\textbf{\\em x}$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{T}_{f_{\\omega}(\\pmb{x})}^{-1}(\\pmb{x})=\\mathcal{T}_{f_{\\omega}(\\mathcal{T}_{\\eta^{\\prime}}(\\pmb{x}))}^{-1}\\left(\\mathcal{T}_{\\eta^{\\prime}}(\\pmb{x})\\right),\\ \\forall\\pmb{\\eta}^{\\prime}\\in\\mathcal{H}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "To learn such a function, we optimize for this property directly. To this end, we sample transformation parameters $\\eta_{\\mathrm{rnd}}$ from some distribution over parameters $p(\\mathfrak{n}_{\\mathrm{rnd}})$ . This allows us to get random samples $x_{\\mathrm{rnd}}:=\\mathcal{T}_{\\eta_{\\mathrm{rnd}}}(x)\\in\\mathcal{X}$ in the orbit of any given element $\\pmb{x}\\in\\mathcal{X}$ . Since we want full (i.e., strict) invariance, $p\\left({\\mathfrak{n}}_{\\mathrm{rnd}}\\right)$ must have support on the entire orbit [van der Ouderaa and van der Wilk, 2022]. We then learn an equivariant via a self-supervised learning (SSL) scheme $f{\\omega}^{3}$ inspired by methods like BYOL [Grill et al., 2020] and, more directly, BINCE [Dubois et al., 2021]. For example, we could use the objective illustrated in Figure 4: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|\\mathcal{T}_{f_{\\omega}(x_{\\mathrm{md}})}^{-1}(x_{\\mathrm{md}})-\\mathcal{T}_{f_{\\omega}(x)}^{-1}(x)\\right\\|_{2}^{2},\\quad x_{\\mathrm{md}}=\\mathcal{T}_{\\eta_{\\mathrm{md}}}(x),\\eta_{\\mathrm{rnd}}\\sim p(\\eta_{\\mathrm{rnd}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Our actual objective differs slightly. Since ${\\mathcal{T}}_{\\eta^{\\prime}}({\\pmb x}^{\\prime})={\\mathcal{T}}_{\\eta^{\\prime\\prime}}({\\pmb x}^{\\prime\\prime})$ implies $\\pmb{x}^{\\prime}=\\mathcal{T}_{\\pmb{\\eta}^{\\prime}}^{-1}\\circ\\mathcal{T}_{\\pmb{\\eta}^{\\prime\\prime}}(\\pmb{x}^{\\prime\\prime})$ , we use ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|\\mathcal{T}_{f_{\\omega}(\\mathbf{x})}\\circ\\mathcal{T}_{f_{\\omega}(\\mathbf{x}_{\\mathrm{md}})}^{-1}(x_{\\mathrm{rnd}})-x\\right\\|_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "This change allows us to reduce the number of small discretization errors introduced with each transformation application by replacing repeated transformations with a single composed transformation; see Section 3.1 for further discussion. Our SSL loss is given in line 1 of Algorithm 1. ", "page_idx": 3}, {"type": "text", "text": "Generative model of transformations. Once we have a prototype inference function, we simply learn $\\bar{p_{\\psi}}(\\mathfrak{n}\\,|\\,\\hat{\\mathbf{x}})$ by maximum likelihood on the created data pairs $\\left\\{f_{\\omega}(\\mathbf{\\boldsymbol{x}}_{i}),T_{f_{\\omega}(\\mathbf{\\boldsymbol{x}}_{i})}^{-1}(\\mathbf{\\boldsymbol{x}}_{i})\\right\\}$ . This is shown in line 8 of Algorithm 1. While we need to specify the kinds of symmetry transformations ${\\mathcal{T}}_{\\mathfrak{n}}$ we expect to see in the data, by learning $p_{\\Psi}(\\mathbf{n}\\mid{\\hat{\\mathbf{x}}})$ the model can learn the degree to which those transformations are present in the data. Thus, we can specify several potential symmetry transformations and learn that some are absent in the data. Furthermore, the required prior knowledge (the support of $p\\left({\\mathfrak{n}}_{\\mathrm{rnd}}\\right))$ is small compared to what our SGM can learn (the shapes of the distributions for each of the present transformations). ", "page_idx": 3}, {"type": "text", "text": "Since we are primarily interested in using the model to (a) inspect the distribution over naturally occurring transformations for a given element $\\textbf{\\em x}$ , and ${\\bf(b)}$ resample ", "page_idx": 3}, {"type": "table", "img_path": "aFP24eYpWh/tmp/1610e0c37ebf0fd39dcb771c2b4c847c74382cebb7aa5b1876ff1be73d6c2523.jpg", "table_caption": [], "table_footnote": [], "page_idx": 3}, {"type": "text", "text": "new \u201cnaturally\u201d augmented versions of the element, we do not need to learn $p\\left({\\hat{\\mathbf{x}}}\\right)$ . We can do (a) by querying $p\\left({\\mathfrak{n}}\\mid{\\hat{\\mathbf{x}}}={\\hat{\\mathbf{x}}}\\right)$ for $\\hat{\\pmb{x}}:=\\mathscr{T}_{f_{\\eta}(\\pmb{x})}^{-1}(\\pmb{x})$ , and we can do (b) by sampling $\\pmb{\\eta}\\sim p\\left(\\pmb{\\eta}\\mid\\hat{\\pmb{x}}\\right)$ and transforming the $\\hat{\\pmb{x}}$ to get $\\pmb{x}:=\\mathcal{T}_{\\pmb{\\eta}}\\left(\\hat{\\pmb{x}}\\right)$ . Of course, if one wanted to sample new prototypes, one could fit $p_{\\theta}(\\hat{\\mathbf{x}})$ using, e.g., a VAE. Not learning $p\\left({\\hat{\\mathbf{x}}}\\right)$ greatly simplifies training for complicated datasets that would otherwise require a large generative model, an observation made by Dubois et al. [2021]. ", "page_idx": 3}, {"type": "table", "img_path": "aFP24eYpWh/tmp/3a5a571d838b1e3173c7a3edaed5bb1b8d210b5d1a7e2436ac98eed5e47f608d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 4}, {"type": "image", "img_path": "aFP24eYpWh/tmp/cb22014013d775cca4f294d01ba5182988d4201e8ed0863c10e8514afd7021da.jpg", "img_caption": ["(b) Simple $p_{\\Psi}(\\mathfrak{n}\\,|\\,\\hat{\\mathbf{x}})$ (c) Flexible $p_{\\Psi}(\\mathfrak{n}\\,|\\,\\hat{\\mathbf{x}})$"], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Figure 5: Idealised examples of simple and flexible learned distributions over angles $p_{\\Psi}(\\mathfrak{n}\\,|\\,\\hat{\\mathbf{x}})$ \u2014 \u2014given the true distribution $\\begin{array}{r}{p\\left(\\eta\\left|\\hat{\\mathbf{x}}\\right.\\right)=\\sum_{\\mathbf{x}\\in\\left\\{\\mathcal{S},\\dots,\\mathcal{S},\\dots,\\mathcal{S}\\right\\}}p\\left(\\left.\\eta\\left|\\hat{\\mathbf{x}},\\hat{\\mathbf{x}}\\right.\\right)-}\\end{array}$ . ", "page_idx": 4}, {"type": "text", "text": "3 Practical Considerations and Further Motivations ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Training our SGM, while simple, has potential pitfalls in practice. We discuss the key considerations in Section 3.1 and provide further recommendations in Appendix B. We then provide motivation for several of our modeling choices in Section 3.2. ", "page_idx": 4}, {"type": "text", "text": "3.1 Practical Considerations ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Working with transformations. Repeated application of transformations\u2014e.g., in Figure 4\u2014can introduce unwanted artifacts such as blurring. For many useful transformations, we can compose transformations before applying them. For affine transformations of images, for example, we can directly multiply affine-transformation matrices. More generally, if there is some representation of the transformation parameters $T({\\mathfrak{n}})$ where composition can be performed\u2014e.g., as matrix multiplication $\\begin{array}{r}{T_{\\eta_{2}}\\circ T_{\\eta_{1}}\\,=\\,T_{T\\left(\\eta_{2}\\right)T\\left(\\eta_{1}\\right)}^{\\prime}}\\end{array}$ T T\u2032 (\u03b72)T (\u03b71), in the case where T is a group representation\u2014then we recommend composing transformations in that space to minimize the number of applications. ", "page_idx": 4}, {"type": "text", "text": "Partial invertibility. In many common settings, transformations are not fully invertible. We encounter two such issues when working with affine transformations of images living in a finite, discrete coordinate space. Firstly, affine transformations are only approximately invertible in the discrete space due to the information loss when interpolating the transformed image onto a discrete grid. Thus, while only a single prototype $\\hat{\\bf x}$ exists for any $\\mathbf{x}$ , it may not be clear what the correct prototype is. Secondly, transformations can cause information loss due to the finite coordinate space (e.g., by shifting the contents of the image out-of-bounds4). If appropriate bounds are known a priori, we can prevent severe information loss by constraining $\\mathfrak{\\boldsymbol{n}}_{\\mathrm{min}}$ and $\\mathfrak{n}_{\\mathrm{max}}$ using tanh, scale, and shift bijectors. Alternatively, we can augment the SSL loss in Algorithm 1 with an invertibility loss ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{invertibility}}(\\omega)=\\mathrm{mse}\\left(x,\\mathcal{T}_{f_{\\omega}(\\mathbf{x})}^{-1}\\left(\\mathcal{T}_{f_{\\omega}(\\mathbf{x})}\\left(x\\right)\\right)\\right).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Learning $p_{\\Psi}(\\mathbf{n}\\mid{\\hat{\\mathbf{x}}})$ with imperfect inference. In practice, our transformation inference network $f_{\\boldsymbol{\\omega}}\\left(\\mathbf{x}\\right)$ will not be perfect; see Figure 10. Even after training, there may be small variations in the prototypes $\\hat{\\bf x}$ corresponding to different elements in the orbit of $\\mathbf{x}$ . To make $p_{\\psi}(\\eta_{\\times}\\mid\\hat{\\mathbf{x}})$ robust to these variations, we train it with prototypes corresponding to randomly transformed training data points. I.e., we modify the MLE objective in Algorithm 1 as $\\log p_{\\psi}(\\eta_{x}\\mid\\hat{x}^{\\prime})$ , where $\\begin{array}{r}{\\hat{\\pmb{x}}^{\\prime}=\\mathcal{T}_{f_{\\omega}(\\mathcal{T}_{\\eta_{\\mathrm{rnd}}}(\\pmb{x}))}^{-1}(\\mathcal{T}_{\\eta_{\\mathrm{rnd}}}(\\pmb{x}))}\\end{array}$ as in our SSL objective. Averaging the loss over multiple samples\u2014e.g., 5\u2014of $\\mathfrak{n}_{\\mathrm{rnd}}$ is beneficial. ", "page_idx": 4}, {"type": "text", "text": "3.2 Modelling Choices ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We now motivate some of the design choices for our SGM by means of illustrative examples. In each case, we assume that ${\\mathcal{T}}_{\\mathfrak{n}}$ is counter-clockwise rotation; thus, $\\boldsymbol\\upeta$ is the angle. ", "page_idx": 4}, {"type": "text", "text": "1. The distribution $p_{\\Psi}(\\mathfrak{n}\\,|\\,\\hat{\\mathbf{x}})$ is implemented as a normalising flow. Consider a dataset of \u20188\u2019s rotated in the range $-30^{\\circ}$ to $30^{\\circ}\\colon\\{\\diamond,\\,\\ldots,\\,8,\\,\\ldots,\\,\\ll\\}$ . Let us assume that the prototype is $\\cdot_{8},$ . Figure 5a shows $p\\left(\\mathfrak{n}\\mid\\mathbf{x},\\,\\hat{\\mathbf{x}}\\right)$ , an example of the true distribution for $\\boldsymbol\\upeta$ given $\\mathbf{x}$ and $\\hat{\\bf x}$ , for several observations, under the data generating process5. These distributions are composed of deltas because ", "page_idx": 4}, {"type": "table", "img_path": "aFP24eYpWh/tmp/070db9ed6726684f69c81bbee78f5b4b6d2f75a30458befac2193c613c97bdab.jpg", "table_caption": ["(a) Distribution for $\\boldsymbol\\upeta$ given $\\mathbf{x}$ and \u02c6x. "], "table_footnote": [], "page_idx": 5}, {"type": "image", "img_path": "aFP24eYpWh/tmp/c5c1be04175f39496739dfe7d0183d9da1baee827826747feac9eec44d11eb82.jpg", "img_caption": [], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Figure 6: Examples of learned distributions over angles $p_{\\Psi}(\\cdot),$ \u2014 \u2014with and without dependence on $\\hat{\\bf x}$ , given the true distribution $p\\left(\\cdot\\right)$ \u2014 . ", "page_idx": 5}, {"type": "text", "text": "only certain values of $\\boldsymbol\\upeta$ will transform $\\hat{\\bf x}$ into $\\mathbf{x}$ . Figures 5b and 5c compare idealised examples of the learned $p_{\\Psi}(\\mathfrak{n}\\,|\\,\\hat{\\mathbf{x}})$ \u2014given a simple uni-modal Gaussian family and a more flexible bi-modal mixtureof-Gaussian family\u2014with the aggregate true distribution $\\begin{array}{r}{p\\left(\\b{\\eta}\\mid\\hat{\\mathbf{x}}\\right)=\\sum_{\\mathbf{x}\\in\\{\\hat{\\mathbb{S}},\\dots,\\hat{\\mathbb{S}},\\dots,\\hat{\\mathbb{S}}\\}}p\\left(\\b{\\eta}\\mid\\mathbf{x},\\,\\hat{\\mathbf{x}}\\right)}\\end{array}$ Here, the simple uni-modal distribution is clearly worse than the bi-modal distribution due to the large amount of probability mass being wasted on angles with low density under the true data-generating process. Of course, one might argue that the bi-modal distribution is also not flexible enough. Furthermore, \u2018flexible enough\u2019 will be problem-specific. We solve this problem with normalizing flows, which can match a wide range of distributions. ", "page_idx": 5}, {"type": "text", "text": "2. The transformation parameters $\\boldsymbol\\upeta$ depend on the prototype \u02c6x. Consider a dataset of $^{\\bullet}2$ \u2019s and \u20188\u2019s rotated in the range $-30^{\\circ}$ to $30^{\\circ}\\colon\\{\\ge,\\dots,2,\\dots,\\ge,\\,\\mathbb{S},\\dots,8,\\dots,\\,\\vartheta\\,\\}$ , with prototypes $\\acute{\\bullet}$ and \u20188\u2019. Figure 6a shows $p\\left(\\mathfrak{n}\\,|\\,\\mathbf{x},\\,\\hat{\\mathbf{x}}\\right)$ , an example of a true distribution over $\\boldsymbol\\upeta$ , for several observations. Figures $^\\mathrm{6b}$ and $6\\mathrm{c}$ compares idealised examples of learned distributions over $\\boldsymbol\\upeta$ and $\\boldsymbol{\\upeta}\\mid\\hat{\\mathbf{x}}$ . Without dependence on $\\hat{\\bf x}$ , the model must place probability mass between $-150^{\\circ}$ and $150^{\\circ}$ , in order to capture the symmetries of the \u20188\u2019s, however this results invalid digits\u2014such as $\\{\\tau,\\tau,\\tau\\}$ \u2014which do not come from true data distribution. On the other hand, when $\\boldsymbol\\upeta$ depends on $\\hat{\\bf x}$ , the distribution conditioned on the prototype for the \u20182\u2019s only needs to place mass in $[-30^{\\circ},30^{\\circ}]$ . ", "page_idx": 5}, {"type": "text", "text": "3. The prototype $\\hat{\\bf x}$ is fully invariant to transformations of $\\mathbf{x}$ . Models such as CNNs are most useful when we know a priori which symmetries are present in the data. However, in many cases, this must be learned. In the case of handwritten digit recognition, we know that the model should be invariant to some amount of rotation since people naturally write with some variation in angle. But a model that is invariant to rotations in the full range $[-180^{\\circ},\\,180^{\\circ}]$ might be unable to distinguish between $\\surd6\\textdegree$ and \u20189\u2019. Thus, in the literature for learning invariances in the discriminative setting, it is common to learn partially invariant functions that capture some degree of invariance [van der Wilk et al., 2018, Benton et al., 2020, van der Ouderaa and van der Wilk, 2022]. However, as we will now show, this approach is unsuitable for our SGM, as it breaks our assumption that $\\hat{\\bf x}$ contains no information about the symmetries in the data. ", "page_idx": 5}, {"type": "table", "img_path": "aFP24eYpWh/tmp/2376d5ebb117d72f197fbd8c2a443323471b32caf627c7280dda8d00a17af2a1.jpg", "table_caption": ["(a) $p\\left(\\mathfrak{n}\\mid\\mathbf{x},\\,\\hat{\\mathbf{x}}\\right)$ with different levels of invariance. "], "table_footnote": [], "page_idx": 5}, {"type": "image", "img_path": "aFP24eYpWh/tmp/c4e8fe3fcb4bebd0924c597b0a87cdd7b543230f597e2dee8a89b9935a3c0d44.jpg", "img_caption": ["Figure 7: Examples of learned distributions over angles $p_{\\Psi}(\\mathfrak{n}\\,|\\,\\hat{\\mathbf{x}}).$ \u2014 / $\\nmid$ \u2014with different degrees of invariance in the prototype $\\hat{\\bf x}$ , given the true $p\\left(\\mathfrak{n}\\mid\\hat{\\mathbf{x}}\\right)$ \u2014 . "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Consider a dataset of \u20182\u2019s rotated in the range $-30^{\\circ}$ to $30^{\\circ}$ : $\\{2,\\ldots,2,\\ldots,2\\}$ . Figure 7a shows predicted prototypes and the corresponding distributions over $\\boldsymbol\\upeta$ for several observations. There are three cases: (a) a fully-invariant $\\hat{\\bf x}$ , i.e., there is a single prototype, ${\\bf(b)}$ a partially-invariant $\\hat{\\bf x}$ , for which there are two prototypes in this example, and (c) a non-invariant $\\hat{\\bf x}$ , which takes the partiallyinvariant case to the extreme and has as many prototypes as observations. In the partially-invariant and non-invariant cases, we can get multiple prototypes rather than a single unique prototype per orbit, which is invalid under the generative model of the data. As a result, $p_{\\Psi}(\\mathfrak{n}\\,|\\,\\hat{\\mathbf{x}})$ does not represent the distribution of naturally occurring transformations of $\\hat{\\bf x}$ in the data. This is illustrated in Figures 7b to 7d, which show idealized examples of the learned $p_{\\Psi}(\\mathfrak{n}\\,|\\,\\hat{\\mathbf{x}})$ in each case. While the distribution in Figure 7b matches the distribution of transformations in the dataset, in Figures 7c and 7d we see that the distributions corresponding to non-unique prototype do not. To illustrate why this is a problem, let us say we would like to probe the probability of a particular transformed variant ", "page_idx": 5}, {"type": "image", "img_path": "aFP24eYpWh/tmp/17c0aeb8cfd5cd8a117016bd521e298e01ccdd4bc5b6443e451b238a2e2836ea.jpg", "img_caption": ["(d) GalaxyMNIST under affine and color transformations "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Figure 8: Top: samples from the test set. Mid: prototypes for each test example. Bot: resampled versions of each test example given the prototype. Prototypes for examples from the same orbit (and in some cases from distinct but similar orbits) match (e.g., their size, position, rotation, etc. are similar). Resampled examples are usually indistinguishable from test examples. ", "page_idx": 6}, {"type": "text", "text": "of an observed example. For example, given an example of a digit $\\varsigma_{3},$ , we want to know the probability of observing , that digit rotated by $-90^{\\circ}$ . Assuming we can find a prototype $\\hat{\\pmb{x}}$ we would like $p\\left(\\mathfrak{n}\\,|\\,\\hat{\\mathbf{x}}=\\hat{\\mathbf{x}}\\right)$ to represent all naturally occurring augmentations. Unless $\\hat{\\bf x}$ is unique, this won\u2019t necessarily be the case, as illustrated in Figure 7. ", "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In Section 4.1, we explore our SGM\u2019s ability to learn symmetries. We show that it produces valid prototypes, and generates plausible samples from the data distribution, given those prototypes. Then, in Section 4.2, we leverage our SGM to improve data efficiency in deep generative models. ", "page_idx": 6}, {"type": "text", "text": "We conduct experiments using three datasets\u2014dSprites [Matthey et al., 2017], MNIST, and GalaxyMNIST [Walmsley et al., 2022]\u2014and two kinds of transformations\u2014affine and color. In Section 4.1, when working with MNIST under affine transformations, we add a small amount of rotation (in the range $[-15^{\\circ},15^{\\circ}])$ to the original data to make rotations in the figures easier to see. For MNIST under color transformations, we first convert the grey-scale images to color images using only the red channel. We then add a random hue rotation in the range $[0,0.6\\pi]$ and a random saturation multiplier in the range [0.6, 0.9]. In the case of dSprites, we carefully control the rotations, positions, and sizes of all of the sprites. For example, in the case of the heart sprites, we have removed the rotations and set the $y$ -positions to be bimodal in the top and bottom of the images. Further details about the dSprites setup, as well as all other experimental details, can be found in Appendix C. We focus on learning affine transformations (shifting, rotation, and scaling) as they are expressive while still being a group that is easy to work with. We also learn color transformations (hue, saturation, and value). See Appendix C.7 for details about how we parameterize ${\\mathcal{T}}_{\\mathfrak{n}}$ in both cases. ", "page_idx": 6}, {"type": "text", "text": "4.1 Learning Symmetries ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Exploring transformations and prototypes. Figure 8 shows that for both datasets and kinds of transformations we consider, our SGM produces close-to-invariant prototypes as well as realistic \u201cnatural\u201d examples that are almost indistinguishable from test examples. There are several illustrative examples which bear further discussion. The heart sprites in Figure 8a show that our SGM was able to learn the absence of a transformation (namely rotation) in the dataset. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "As expected, all of the prototypes for the sprites of the same shape are the same, since these shapes are in the same orbit as one another. This behaviour is also demonstrated for MNIST digits in Figures 19 and 20. The $\"6\",~\"8\"$ , and $\\mathbf{\\omega}^{\\bullet}\\mathbf{9}^{\\bullet}$ digits in Figure 8b demonstrate the ability of our SGM to learn bimodal distributions (on rotation in this case). The figure\u2019s third $^{\\prime}7^{\\prime}$ is interesting because our SGM interprets it as a $\\acute{\\bullet}$ . ", "page_idx": 7}, {"type": "text", "text": "Flexibility is important. In $\\eta$ , each dimension corresponds to a different transformation. We refer to $p_{\\Psi}(\\mathfrak{n}_{i}\\mid\\mathbf{x})$ as the marginal distribution of a single transformation parameter. Figure 9 shows these marginal learnt distributions for several digits from Figure 8b. We see that each of the parameters has its own range and shapes. For rotations, which are easy to reason about, we see distributions that make sense\u2014the round $\\surd0\\ '$ has an almost uniform distribution over rotations, and the \u20181\u2019 and one of the \u20189\u2019s are strongly bimodal as expected. The other $\\bullet\\,\\mathbf{\\Phi}^{\\bullet}$ , which does not look as much like an upside-down $\\surd6\\ '$ , has a much smaller $2^{\\mathrm{nd}}$ mode. The $\\acute{\\bullet}$ , which looks somewhat like an upside-down $\\acute{7}$ , is also bimodal. We see that prototypes of different sizes result in corresponding distributions over scaling parameters with different ranges. Figure 21 provides additional examples for MNIST with affine transformations, while Figure 22 provides the same for color transformations, and Figure 23 investigates the distributions for dSprites. These results provide experimental evidence of the need for flexibility in the generative model for $p_{\\Psi}(\\pmb{\\eta}\\mid\\mathbf{x})$ , as conjectured in Section 3.2. We also find significant dependencies between dimensions of $\\boldsymbol{\\mathfrak{\\eta}}$ (e.g., rotation and translation in dSprites). ", "page_idx": 7}, {"type": "image", "img_path": "aFP24eYpWh/tmp/fed2c00cb74cc005c1cab7476665ae4b9cdd38393abb1af197dbec9aac279d30.jpg", "img_caption": ["Figure 9: From left to right, test examples, their prototypes, and the corresponding marginal distributions $p_{\\Psi}(\\mathfrak{n}_{i}\\mid\\mathbf{x})$ over translation in $x$ , translation in $y$ , rotation, scaling in $x$ , and scaling in $y$ . "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Invariance of $f_{\\mathbf{\\omega}}_{\\mathbf{\\omega}}$ and the prototypes. In Figure 10, we investigate the imperfections of the inference network by considering an iterative procedure in which prototypes are treated as observed examples, allowing us to infer a chain of successive prototypes. We show several examples of such chains, as well as the average magnitude of the transformation parameters at each iteration, normalized by the maximum magnitude (at iteration 0). The first prototype $\\hat{\\mathbf{x}}_{1}$ is most different from the previous $\\hat{\\mathbf{x}}_{0}=\\mathbf{x}$ , with successive prototypes being similar visually and as measured by the magnitude of the inferred transformation parameters. However, the magnitude of the inferred parameters does not tend towards 0, rather plateauing at around $5\\%$ of the maximum. This highlights that, although simple NNs can learn to be approximately invariant, a natively invariant architecture has the potential to improve performance. ", "page_idx": 7}, {"type": "image", "img_path": "aFP24eYpWh/tmp/4882f8dceadbc81df64e3079ca5a53c225ae5d957d3202da471a76a506834642.jpg", "img_caption": ["Figure 10: Iterative prototype inference. Left: starting with a test example $\\mathbf{x}$ , we get a prototype $\\hat{\\mathbf{x}}_{1}$ , then treating prototype $\\hat{\\mathbf{x}}_{i}$ as an observed example we predict the next prototype $\\hat{\\mathbf{x}}_{i+1}$ . Right: The average magnitude of the transformation parameters as a function of iterations of this process. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "4.2 VAE Data Efficiency ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We use SGM to build data-efficient and robust generative models. In Figure 11, we compare a standard VAE to two VAE-SGM hybrid models\u2014\u201cAugVAE\u201d and \u201cInvVAE\u201d\u2014for different amounts of training data and added rotation of the MNIST digits. When adding rotation, each $\\textbf{\\em x}$ in the dataset set is always rotated by the same angle (sampled uniformly between $\\pm\\theta_{\\mathrm{{max}}}$ , the maximum added rotation angle). Thus, adding rotation here is not data augmentation. AugVAE is a VAE that uses our SGM to re-sample transformed examples $\\pmb{x}^{\\prime}\\,=\\,\\pmb{\\mathcal{T}}_{\\pmb{\\eta}\\mid\\hat{\\pmb{x}}}\\left(\\hat{\\pmb{x}}\\right)$ , introducing data augmentation at training time. InvVAE is a VAE that uses our SGM to convert each example $\\textbf{\\em x}$ to its prototype $\\hat{\\pmb x}$ at both train and test time. That is, the VAE in InvVAE sees only the invariant representation of each example. We also compare against a VAE trained with standard data augmentation6. We use test-set importance-weighted lower bound (IWLB) [Domke and Sheldon, 2018] of $p\\left(\\mathbf{x}\\right)$ , estimated with 300 samples of the VAE\u2019s latent variable ${\\bf z}$ , and $\\boldsymbol{\\mathfrak{n}}$ for InvVAE, to compare the models. Reconstruction error is provided in Appendix E. Further details\u2014e.g., hyperparameter sweeps\u2014are in Appendix C. ", "page_idx": 7}, {"type": "text", "text": "As expected, for the VAE ( ), as we decrease the amount of training data $(-\\rightarrow\\cdots)$ ) or increase the amount of randomly added rotation, performance degrades. This is because the VAE sees fewer training examples per-degree of rotation. On the other hand, the AugVAE ( ) is more data efficient. Its performance is unaffected by reducing the number of observations by three quarters. Furthermore, while the performance of AugVAE and the standard VAE are almost identical for small angles and large training sets, the drop in performance of AugVAE for larger random rotations is significantly smaller; AugVAE does not see less training examples per-degree of rotation. InvVAE ( ), which natively incorporates the inductive biases of our SGM and obtains a 500 nat larger likelihood than the other models. Its performance is almost perfectly robust to rotation in the dataset. Additionally, its metrics barely change $(<10\\%)$ ) when trained on half the data. Finally, while the VAE with data augmentation ( ) improves on the standard VAE for less training data, it is substantially worse in the presence of more data. This contrasts our AugVAEs, which are almost always better. This poor performance is because the augmentations are independent of the samples. Thus, highly rotated digits can be rotated ", "page_idx": 8}, {"type": "image", "img_path": "aFP24eYpWh/tmp/382135fea4e9d3131d67b19603ba931d458db56deb56a146b3925ed87d6eb2de.jpg", "img_caption": ["Maximum Added Rotation Angle (\u25e6) "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 11: Incorporating symmetries improves data efficiency. Importanceweighted lower bound (IWLB) (mean and std. err. over 3 random seeds) on rotated MNIST for a standard VAE (w. and w.o. data aug.) and two VAE variants that incorporate symmetries via our SGM. Improved data efficiency is demonstrated by better performance with less training data and less sensitivity to added rotation. ", "page_idx": 8}, {"type": "text", "text": "too much, smaller digits become too small, and digits near the image edges are moved out of frame.   \nThis highlights the importance of augmenting data in accordance with the true data distribution. ", "page_idx": 8}, {"type": "text", "text": "We further validate these results with the more complex GalaxyMNIST dataset and an enlarged set of both affine and color transformations. As with our rotated MNIST with affine transformation results, in Figure 12, we see that AugVAE ( ) outperforms the standard VAE ( ). Furthermore, we see that AugVAE is robust to training with only half of the dataset. Our SGM captures the true data distribution with only 3500 training examples. ", "page_idx": 8}, {"type": "text", "text": "5 Related Work ", "text_level": 1, "page_idx": 8}, {"type": "image", "img_path": "aFP24eYpWh/tmp/bb81b4db198e6d00bf406639ef23e30fc9a936b10f2b492260251f42da129880.jpg", "img_caption": ["Figure 12: GalaxyMNIST dataefficiency (3 seed mean & std. err.). "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Learning Lie groups. Rao and Ruderman [1998], Miao and Rao [2007], Keurti et al. [2023] learn Lie groups from sequences of transformed images in an unsupervised fashion. Hashimoto et al. [2017] learn to represent an image as a linear combination of transformed versions of its nearest neighbors. Dehmamy et al. [2021] use Lie algebras to define CNNs for automatic symmetry discovery. Yang et al. [2023] use a GAN-based approach to learn transformations of examples that leave the original data distribution unchanged, thereby fooling a discriminator. Falorsi et al. [2019] introduce a reparameterization trick for learning densities on arbitrary, but known, Lie groups. Chau et al. [2022] learn a generative model over Lie group transformations applied to prototypical images that are themselves composed of sparse combinations of learned dictionary elements. ", "page_idx": 8}, {"type": "text", "text": "Learning a prototype. Kaba et al. [2023] note that symmetry-based NNs are often contained in their architectures. Like us, they propose to learn \"canonicalization functions\" that produce prototypical representations of the data. Mondal et al. [2023] show that such canonicalization functions can be used to make large-pre-trained NNs equivariant and, when combined with dataset-dependent symmetry priors, do not degrade performance. Similarly, Kim et al. [2023] learn architecture-agnostic equivariant functions by averaging a non-equivariant function over a probabilistic prototypical input. Finally, while not explicitly trained to produce prototypes, spatial transformers learn to undo transformations such as translation, scaling, and rotations [Jaderberg et al., 2015]. ", "page_idx": 8}, {"type": "text", "text": "Data augmentations and symmetries. Prior work makes several connections between data augmentation and symmetries relevant to our findings. Bouchacourt et al. [2021b] show that invariances in the model tend to result from natural variations in the data rather than data augmentation or model architecture. This supports our approach of learning data augmentation from the data and our architecture-agnostic self-supervised invariance learning method. Balestriero et al. [2022], Miao et al. [2023], Bouchacourt et al. [2021b] show that learned symmetries (i.e., data augmentation) should be class-dependent, much like our transformations are prototype-dependent. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Symmetry-aware latent spaces. Encoding symmetries in latent space is well-studied. Higgins et al. [2018] posit that symmetry transformations that leave some parts of the world invariant are responsible for exploitable structure in any dataset. Thus, agents benefit from disentangled representations that separate out these transformations. Winter et al. [2022] split the latent space of an auto-encoder into invariant and equivariant partitions. However, they rely on geometric NN architectures, contrasting with our self-supervised learning approach. Furthermore, they do not learn a generative model\u2014they reconstruct the input exactly\u2014thus, they cannot sample new observations given a prototype. Xu et al. [2021] propose group equivariant subsampling layers that allow them to construct autoencoders with equivariant representations. Shu et al. [2018] propose an autoencoder whose representations are split such that the reconstruction of an observation is decomposed into a \u201ctemplate\u201d (much like our prototypes) and a spatial deformation (transformation). ", "page_idx": 9}, {"type": "text", "text": "In the generative setting, Louizos et al. [2016] construct a VAE with a latent space that is invariant to pre-specified sensitive attributes of the data. However, these sensitive attributes are observed rather than learned. Similarly, Aliee et al. [2023] construct a VAE with a partitioned latent space with a component that is invariance spurious factors of variation in the data. Bouchacourt et al. [2018], Hosoya [2019] learn VAE with two latent spaces\u2014a per-observation equivariant latent and an invariant latent shared across grouped examples. Other works have constructed rotation equivariant [Kuzina et al., 2022] and partitioned equivariant and invariant [Vadgama et al., 2022] latent spaces. Antor\u00e1n and Miguel [2019], Ilse et al. [2020] split the latent space of a VAE into domain, class, and residual variation components. The first of which can capture rotation symmetry in hand-written digits. Unlike us, they require class labels and auxiliary classifiers. Keller and Welling [2021] construct a VAE with a topographically organised latent space such that an approximate equivariance is learned from sequences of observations. In contrast to the works above, Bouchacourt et al. [2021a] argue that learning symmetries should not be achieved via a partitioned latent space but rather learning equivariant operators that are applied to the whole latent space. Finally, while Nalisnick and Smyth [2017] do not learn symmetries, their information lower bound objective is reminiscent of several works above\u2014and our own, see Appendix A\u2014in minimizing the mutual information between two quantities when learning a prior. ", "page_idx": 9}, {"type": "text", "text": "Self-supervised Equivariant Learning [Dangovski et al., 2022] generalize standard invariant SSL methods to produce representations that can be either insensitive (invariant) or sensitive (equivariant) to transformations in the data. Similarly, Eastwood et al. [2023] use a self-supervised learning approach to disentangle sources of variation in a dataset, thereby learning a representation that is equivariant to each of the sources while invariant to all others. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We have presented a Symmetry-aware Generative Model (SGM) and demonstrated that it is able to learn, in an unsupervised manner, a distribution over symmetries present in a dataset. This is done by modeling the observations as a random transformation of an invariant latent prototype. This is the first such model we are aware of. Building generative models that incorporate this understanding of symmetries significantly improves log-likelihoods and data sparsity robustness. This is exciting in the context of modern generative models, which are close to exhausting all of the data on the internet. We are also excited about the use of SGM for scientific discovery, given that the framework is ideal for probing for naturally occurring symmetries present in systems. For example, we could apply SGM to marginalize out the idiosyncrasies of different measuring equipment and observation geometry in radio astronomy data. Additionally, given the success of using our SGM for data augmentation when training VAEs, it would be interesting to apply it to data augmentation in discriminative settings and compare it with methods such as Benton et al. [2020], Miao et al. [2023]. ", "page_idx": 9}, {"type": "text", "text": "The main limitation of our SGM is that it requires specifying the super-set of possible symmetries. Future work might relax this requirement or explore how robust our SGM is to even larger sets. Furthermore, care must sometimes be taken when specifying the set of symmetries. For example, when rotating to images with \u201ccontent\u201d up to the boundaries of the image; see Appendix E.2. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "The authors would like to thank Taliesin Beynon for helpful discussions and Emile Mathieu for providing feedback on the paper. This work has been performed using resources provided by the Cambridge Tier-2 system operated by the University of Cambridge Research Computing Service (http://www.hpc.cam.ac.uk) funded by EPSRC Tier-2 capital grant EP/T022159/1. This work was also supported with Cloud TPUs from Google\u2019s TPU Research Cloud (TRC). JUA acknowledges funding from the EPSRC, the Michael E. Fisher Studentship in Machine Learning, and the Qualcomm Innovation Fellowship. JUA was also supported by an ELLIS mobility grant. SP acknowledges support from the Harding Distinguished Postgraduate Scholars Programme Leverage Scheme. JA acknowledges support from Microsoft Research, through its PhD Scholarship Programme, and from the EPSRC. JMH acknowledges support from a Turing AI Fellowship under grant EP/V023756/1. RET is supported by Google, Amazon, ARM, Improbable, EPSRC grant EP/T005386/1, and the EPSRC Probabilistic AI Hub (ProbAI, EP/Y028783/1). ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Hananeh Aliee, Ferdinand Kapl, Soroor Hediyeh-Zadeh, and Fabian J. Theis. Conditionally invariant representation learning for disentangling cellular heterogeneity. CoRR, abs/2307.00558, 2023. doi: 10.48550/arXiv.2307.00558. (Cited on p. 10.) ", "page_idx": 10}, {"type": "text", "text": "James Urquhart Allingham, Javier Antoran, Shreyas Padhy, Eric Nalisnick, and Jos\u00e9 Miguel Hern\u00e1ndez-Lobato. Learning generative models with invariance to symmetries. In NeurIPS 2022 Workshop on Symmetry and Geometry in Neural Representations, 2022. (Cited on p. 17.) ", "page_idx": 10}, {"type": "text", "text": "Javier Antor\u00e1n and Antonio Miguel. Disentangling and learning robust representations with natural clustering. In M. Arif Wani, Taghi M. Khoshgoftaar, Dingding Wang, Huanjing Wang, and Naeem Seliya, editors, 18th IEEE International Conference On Machine Learning And Applications, ICMLA 2019, Boca Raton, FL, USA, December 16-19, 2019, pages 694\u2013699. IEEE, 2019. doi: 10.1109/ICMLA.2019.00125. URL https://doi.org/10.1109/ICMLA.2019.00125. (Cited on pp. 2 and 10.) ", "page_idx": 10}, {"type": "text", "text": "Randall Balestriero, L\u00e9on Bottou, and Yann LeCun. The effects of regularization and data augmentation are class dependent. In NeurIPS, 2022. (Cited on p. 10.) ", "page_idx": 10}, {"type": "text", "text": "Gregory W. Benton, Marc Finzi, Pavel Izmailov, and Andrew Gordon Wilson. Learning invariances in neural networks from training data. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. (Cited on pp. 2, 6, 10, 17, and 24.) ", "page_idx": 10}, {"type": "text", "text": "Diane Bouchacourt, Ryota Tomioka, and Sebastian Nowozin. Multi-level variational autoencoder: Learning disentangled representations from grouped observations. In Proceedings of the ThirtySecond AAAI Conference on Artificial Intelligence, 2018. (Cited on p. 10.) ", "page_idx": 10}, {"type": "text", "text": "Diane Bouchacourt, Mark Ibrahim, and St\u00e9phane Deny. Addressing the topological defects of disentanglement via distributed operators. CoRR, abs/2102.05623, 2021a. (Cited on p. 10.) ", "page_idx": 10}, {"type": "text", "text": "Diane Bouchacourt, Mark Ibrahim, and Ari S. Morcos. Grounding inductive biases in natural images: invariance stems from variations in data. In Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pages 19566\u201319579, 2021b. (Cited on pp. 9 and 10.) ", "page_idx": 10}, {"type": "text", "text": "Ho Yin Chau, Frank Qiu, Yubei Chen, and Bruno A. Olshausen. Disentangling images with lie group transformations and sparse coding. In Sophia Sanborn, Christian Shewmake, Simone Azeglio, Arianna Di Bernardo, and Nina Miolane, editors, NeurIPS Workshop on Symmetry and Geometry in Neural Representations, 03 December 2022, New Orleans, Lousiana, USA, volume 197 of Proceedings of Machine Learning Research, pages 22\u201347. PMLR, 2022. URL https://proceedings.mlr.press/v197/chau23a.html. (Cited on p. 9.) ", "page_idx": 10}, {"type": "text", "text": "Taco Cohen and Max Welling. Group equivariant convolutional networks. In Proceedings of the 33nd International Conference on Machine Learning, ICML 2016, New York City, NY, USA, June 19-24, 2016, volume 48 of JMLR Workshop and Conference Proceedings, pages 2990\u20132999. JMLR.org, 2016. (Cited on p. 2.) ", "page_idx": 10}, {"type": "text", "text": "Rumen Dangovski, Li Jing, Charlotte Loh, Seungwook Han, Akash Srivastava, Brian Cheung, Pulkit Agrawal, and Marin Soljacic. Equivariant self-supervised learning: Encouraging equivariance in representations. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. URL https://openreview. net/forum?id=gKLAAfiytI. (Cited on p. 10.) ", "page_idx": 11}, {"type": "text", "text": "Nima Dehmamy, Robin Walters, Yanchen Liu, Dashun Wang, and Rose Yu. Automatic symmetry discovery with lie algebra convolutional network. In Marc\u2019Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan, editors, Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pages 2503\u20132515, 2021. URL https://proceedings.neurips.cc/paper/2021/hash/ 148148d62be67e0916a833931bd32b26-Abstract.html. (Cited on p. 9.) ", "page_idx": 11}, {"type": "text", "text": "Justin Domke and Daniel Sheldon. Importance weighting and variational inference. CoRR, abs/1808.09034, 2018. URL http://arxiv.org/abs/1808.09034. (Cited on p. 8.)   \nYann Dubois, Benjamin Bloem-Reddy, Karen Ullrich, and Chris J. Maddison. Lossy compression for lossless prediction. In Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pages 14014\u201314028, 2021. (Cited on pp. 4 and 17.)   \nConor Durkan, Artur Bekasov, Iain Murray, and George Papamakarios. Neural spline flows. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 7509\u20137520, 2019. (Cited on p. 20.)   \nCian Eastwood, Julius von K\u00fcgelgen, Linus Ericsson, Diane Bouchacourt, Pascal Vincent, Bernhard Sch\u00f6lkopf, and Mark Ibrahim. Self-supervised disentanglement by leveraging structure in data augmentations. CoRR, abs/2311.08815, 2023. doi: 10.48550/ARXIV.2311.08815. URL https: //doi.org/10.48550/arXiv.2311.08815. (Cited on p. 10.)   \nLuca Falorsi, Pim de Haan, Tim R. Davidson, and Patrick Forr\u00e9. Reparameterizing distributions on lie groups. In Kamalika Chaudhuri and Masashi Sugiyama, editors, The 22nd International Conference on Artificial Intelligence and Statistics, AISTATS 2019, 16-18 April 2019, Naha, Okinawa, Japan, volume 89 of Proceedings of Machine Learning Research, pages 3244\u20133253. PMLR, 2019. URL http://proceedings.mlr.press/v89/falorsi19a.html. (Cited on p. 9.)   \nJean-Bastien Grill, Florian Strub, Florent Altch\u00e9, Corentin Tallec, Pierre H. Richemond, Elena Buchatskaya, Carl Doersch, Bernardo \u00c1vila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, Bilal Piot, Koray Kavukcuoglu, R\u00e9mi Munos, and Michal Valko. Bootstrap your own latent - A new approach to self-supervised learning. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. (Cited on p. 4.)   \nTatsunori B. Hashimoto, Percy Liang, and John C. Duchi. Unsupervised transformation learning via convex relaxations. In Advances in Neural Information Processing Systems 30, 2017. (Cited on p. 9.)   \nIrina Higgins, David Amos, David Pfau, S\u00e9bastien Racani\u00e8re, Lo\u00efc Matthey, Danilo J. Rezende, and Alexander Lerchner. Towards a definition of disentangled representations. CoRR, abs/1812.02230, 2018. URL http://arxiv.org/abs/1812.02230. (Cited on p. 10.)   \nHaruo Hosoya. Group-based learning of disentangled representations with generalizability for novel contents. In Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI, 2019. (Cited on p. 10.)   \nMaximilian Ilse, Jakub M. Tomczak, Christos Louizos, and Max Welling. DIVA: domain invariant variational autoencoders. In International Conference on Medical Imaging with Deep Learning, MIDL 2020, 6-8 July 2020, Montr\u00e9al, QC, Canada, volume 121 of Proceedings of Machine Learning Research, pages 322\u2013348. PMLR, 2020. (Cited on p. 10.)   \nAlexander Immer, Tycho F. A. van der Ouderaa, Vincent Fortuin, Gunnar R\u00e4tsch, and Mark van der Wilk. Invariance learning in deep neural networks with differentiable laplace approximations. CoRR, abs/2202.10638, 2022. (Cited on pp. 2 and 17.)   \nAlexander Immer, Tycho F. A. van der Ouderaa, Mark van der Wilk, Gunnar R\u00e4tsch, and Bernhard Sch\u00f6lkopf. Stochastic marginal likelihood gradients using neural tangent kernels. In International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pages 14333\u201314352. PMLR, 2023. (Cited on p. 2.)   \nMax Jaderberg, Karen Simonyan, Andrew Zisserman, and Koray Kavukcuoglu. Spatial transformer networks. In Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada, pages 2017\u20132025, 2015. (Cited on pp. 9 and 24.)   \nS\u00e9kou-Oumar Kaba, Arnab Kumar Mondal, Yan Zhang, Yoshua Bengio, and Siamak Ravanbakhsh. Equivariance with learned canonicalization functions. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pages 15546\u201315566. PMLR, 2023. URL https://proceedings.mlr.press/v202/kaba23a.html. (Cited on p. 9.)   \nT. Anderson Keller and Max Welling. Topographic vaes learn equivariant capsules. In Marc\u2019Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan, editors, Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pages 28585\u201328597, 2021. URL https://proceedings.neurips.cc/paper/2021/hash/ f03704cb51f02f80b09bffba15751691-Abstract.html. (Cited on p. 10.)   \nHamza Keurti, Hsiao-Ru Pan, Michel Besserve, Benjamin F. Grewe, and Bernhard Sch\u00f6lkopf. Homomorphism autoencoder - learning group structured representations from observed transitions. 2023. (Cited on p. 9.)   \nJinwoo Kim, Dat Nguyen, Ayhan Suleymanzade, Hyeokjun An, and Seunghoon Hong. Learning probabilistic symmetrization for architecture agnostic equivariance. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. URL http://papers.nips.cc/paper_files/paper/2023/hash/ 3b5c7c9c5c7bd77eb73d0baec7a07165-Abstract-Conference.html. (Cited on p. 9.)   \nAnna Kuzina, Kumar Pratik, Fabio Valerio Massoli, and Arash Behboodi. Equivariant priors for compressed sensing with unknown orientation. In International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, volume 162 of Proceedings of Machine Learning Research, pages 11753\u201311771. PMLR, 2022. (Cited on p. 10.)   \nYann LeCun, Bernhard E. Boser, John S. Denker, Donnie Henderson, Richard E. Howard, Wayne E. Hubbard, and Lawrence D. Jackel. Backpropagation applied to handwritten zip code recognition. Neural Comput., 1(4):541\u2013551, 1989. doi: 10.1162/neco.1989.1.4.541. (Cited on p. 2.)   \nYann LeCun, Corinna Cortes, and CJ Burges. Mnist handwritten digit database. ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist, 2, 2010. (Cited on p. 20.)   \nJuho Lee, Yoonho Lee, Jungtaek Kim, Adam R. Kosiorek, Seungjin Choi, and Yee Whye Teh. Set transformer: A framework for attention-based permutation-invariant neural networks. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, volume 97 of Proceedings of Machine Learning Research, pages 3744\u20133753. PMLR, 2019. URL http://proceedings.mlr.press/v97/lee19d.html. (Cited on p. 2.)   \nChristos Louizos, Kevin Swersky, Yujia Li, Max Welling, and Richard S. Zemel. The variational fair autoencoder. In 4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings, 2016. (Cited on p. 10.)   \nKaitlin Maile, Dennis George Wilson, and Patrick Forr\u00e9. Equivariance-aware architectural optimization of neural networks. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. URL https://openreview.net/pdf?id $\\equiv$ a6rCdfABJXg. (Cited on p. 17.)   \nLoic Matthey, Irina Higgins, Demis Hassabis, and Alexander Lerchner. dsprites: Disentanglement testing sprites dataset. https://github.com/deepmind/dsprites-dataset/, 2017. (Cited on pp. 7 and 21.)   \nNing Miao, Tom Rainforth, Emile Mathieu, Yann Dubois, Yee Whye Teh, Adam Foster, and Hyunjik Kim. Learning instance-specific augmentations by capturing local invariances. In International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pages 24720\u201324736. PMLR, 2023. (Cited on pp. 2 and 10.)   \nXu Miao and Rajesh P. N. Rao. Learning the lie groups of visual invariance. Neural Computation, 19 (10):2665\u20132693, 2007. (Cited on p. 9.)   \nBruno Kacper Mlodozeniec, Matthias Reisser, and Christos Louizos. Hyperparameter optimization through neural network partitioning. In The Eleventh International Conference on Learning Representations, 2023. (Cited on p. 2.)   \nArnab Kumar Mondal, Siba Smarak Panigrahi, Oumar Kaba, Sai Mudumba, and Siamak Ravanbakhsh. Equivariant adaptation of large pretrained models. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. URL http://papers.nips.cc/paper_files/paper/2023/hash/ 9d5856318032ef3630cb580f4e24f823-Abstract-Conference.html. (Cited on p. 9.)   \nEric T. Nalisnick and Padhraic Smyth. Learning approximately objective priors. In Proceedings of the Thirty-Third Conference on Uncertainty in Artificial Intelligence, UAI 2017, Sydney, Australia, August 11-15, 2017. AUAI Press, 2017. (Cited on p. 10.)   \nEric T. Nalisnick and Padhraic Smyth. Learning priors for invariance. In International Conference on Artificial Intelligence and Statistics, AISTATS 2018, 9-11 April 2018, Playa Blanca, Lanzarote, Canary Islands, Spain, volume 84 of Proceedings of Machine Learning Research, pages 366\u2013375. PMLR, 2018. (Cited on p. 2.)   \nRajesh P. N. Rao and Daniel L. Ruderman. Learning lie groups for invariant visual perception. In Michael J. Kearns, Sara A. Solla, and David A. Cohn, editors, Advances in Neural Information Processing Systems 11, NIPS, 1998. (Cited on p. 9.)   \nDavid W. Romero and Suhas Lohit. Learning partial equivariances from data. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/ ec51d1fe4bbb754577da5e18eb54e6d1-Abstract-Conference.html. (Cited on p. 2.)   \nC\u00e9dric Rommel, Thomas Moreau, and Alexandre Gramfort. Deep invariant networks with differentiable augmentation layers. In NeurIPS, 2022. (Cited on p. 2.)   \nPola Elisabeth Schw\u00f6bel, Martin J\u00f8rgensen, Sebastian W. Ober, and Mark van der Wilk. Last layer marginal likelihood for invariance learning. CoRR, abs/2106.07512, 2021. (Cited on p. 2.)   \nZhixin Shu, Mihir Sahasrabudhe, Riza Alp G\u00fcler, Dimitris Samaras, Nikos Paragios, and Iasonas Kokkinos. Deforming autoencoders: Unsupervised disentangling of shape and appearance. In Vittorio Ferrari, Martial Hebert, Cristian Sminchisescu, and Yair Weiss, editors, Computer Vision - ECCV 2018 - 15th European Conference, Munich, Germany, September 8-14, 2018, Proceedings, Part X, volume 11214 of Lecture Notes in Computer Science, pages 664\u2013680. Springer, 2018. doi: 10.1007/978-3-030-01249-6\\_40. URL https://doi.org/10.1007/978-3-030-01249-6_ 40. (Cited on p. 10.)   \nSharvaree Vadgama, Jakub Mikolaj Tomczak, and Erik J Bekkers. Kendall shape-vae: Learning shapes in a generative framework. In NeurIPS 2022 Workshop on Symmetry and Geometry in Neural Representations, 2022. (Cited on p. 10.)   \nTycho F. A. van der Ouderaa and Mark van der Wilk. Learning invariant weights in neural networks. CoRR, abs/2202.12439, 2022. (Cited on pp. 2, 4, 6, and 17.)   \nMark van der Wilk, Matthias Bauer, S. T. John, and James Hensman. Learning invariances using the marginal likelihood. In Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montr\u00e9al, Canada, pages 9960\u20139970, 2018. (Cited on pp. 2 and 6.)   \nB. S. Veeling, J. Linmans, J. Winkens, T. Cohen, and M. Welling. Rotation equivariant cnns for digital pathology, September 2018. URL https://doi.org/10.1007/978-3-030-00934-2_ 24. (Cited on pp. 23 and 26.)   \nMike Walmsley, Chris Lintott, Tobias G\u00e9ron, Sandor Kruk, Coleman Krawczyk, Kyle W. Willett, Steven Bamford, Lee S. Kelvin, Lucy Fortson, Yarin Gal, William Keel, Karen L. Masters, Vihang Mehta, Brooke D. Simmons, Rebecca Smethurst, Lewis Smith, Elisabeth M. Baeten, and Christine Macmillan. Galaxy Zoo DECaLS: Detailed visual morphology measurements from volunteers and deep learning for 314 000 galaxies. 509(3):3966\u20133988, January 2022. (Cited on pp. 7 and 22.)   \nRobin Winter, Marco Bertolini, Tuan Le, Frank No\u00e9, and Djork-Arn\u00e9 Clevert. Unsupervised learning of group invariant and equivariant representations. In NeurIPS, 2022. (Cited on p. 10.)   \nJin Xu, Hyunjik Kim, Thomas Rainforth, and Yee Whye Teh. Group equivariant subsampling. In Marc\u2019Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan, editors, Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pages 5934\u20135946, 2021. URL https://proceedings.neurips.cc/paper/2021/hash/ 2ea6241cf767c279cf1e80a790df1885-Abstract.html. (Cited on p. 10.)   \nJianke Yang, Robin Walters, Nima Dehmamy, and Rose Yu. Generative adversarial symmetry discovery. In International Conference on Machine Learning, ICML, 2023. (Cited on pp. 9, 25, and 26.) ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "A Connections to MLL Optimization ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "As we will now show, Algorithm 1 has connections to marginal log-likelihood (MLL) maximization via VAE-like amortized inference. Given the graphical model in Figure 2, we can derive an Evidence Lower BOund (ELBO) for jointly learning the generative and inference parameters with gradients: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\log p\\left(\\mathbf{x}\\right)=\\log\\iint p\\left(\\mathbf{x},\\eta,\\hat{\\mathbf{x}}\\right)d\\mathbf{n}\\,d\\hat{\\mathbf{x}}}\\\\ &{\\qquad\\qquad=\\log\\iint p\\left(\\mathbf{x}\\left\\vert\\,\\mathfrak{n},\\hat{\\mathbf{x}}\\right\\vert p_{\\Psi}(\\mathbf{\\Delta}\\mathfrak{n}\\left\\vert\\,\\hat{\\mathbf{x}})p_{\\Phi}(\\hat{\\mathbf{x}})\\,d\\mathbf{n}\\,d\\hat{\\mathbf{x}}\\right.}\\\\ &{\\qquad\\qquad=\\log\\int\\!\\int p\\left(\\mathbf{x}\\left\\vert\\,\\mathfrak{n},\\hat{\\mathbf{x}}\\right\\vert p_{\\Psi}(\\mathbf{\\Delta}\\mathfrak{n}\\left\\vert\\,\\hat{\\mathbf{x}})p_{\\Phi}(\\hat{\\mathbf{x}})\\,\\frac{q_{\\Phi}(\\mathbf{n},\\hat{\\mathbf{x}}\\mid\\mathbf{x})}{q_{\\Phi}(\\mathbf{n},\\hat{\\mathbf{x}})\\,\\log\\left(\\mathfrak{n}\\right)\\,\\frac{\\hat{\\mathbf{x}}}{\\log\\left(\\mathfrak{n}\\right)}\\,d\\mathbf{n}\\,d\\hat{\\mathbf{x}}}}\\\\ &{\\qquad=\\log\\underset{q_{\\omega}(\\mathfrak{n},\\hat{\\mathbf{x}})\\,\\mathcal{S}}{\\underbrace{\\mathbb{E}}}\\left[\\frac{p\\left(\\mathbf{x}\\left\\vert\\hat{\\mathbf{x}},\\eta\\right)p_{\\Psi}(\\mathbf{n}\\left\\vert\\right)\\,\\log\\left(\\hat{\\mathbf{x}}\\right)}{q_{\\omega}(\\mathfrak{n},\\hat{\\mathbf{x}})\\,\\log\\left(\\mathfrak{n}\\right)}\\right]}\\\\ &{\\qquad\\stackrel{\\mathrm{~\\ge~}}{\\underbrace{\\underbrace{\\mathbb{E}}^{\\mathrm{\\footnotesize(\\mathfrak{n},\\hat{\\mathbf{x}}}\\right)\\,\\mathcal{S}}}}\\left[\\log p\\left(\\mathbf{x}\\left\\vert\\mathfrak{n},\\hat{\\mathbf{x}}\\right\\vert\\right)-\\underbrace{\\sum_{\\mathrm{KL}}\\left[q_{\\omega}\\left(\\mathfrak{n},\\hat{\\mathbf{x}}\\right\\vert\\right)\\left\\vert p_{\\Psi}(\\mathbf{n}\\left\\vert\\hat{\\mathbf{x}}\\right)p_{\\Theta}(\\hat{\\mathbf{x}})\\right]}_{\\mathrm{\\footnotesize{HCidvegrome}}}}\\\\ &{\\qquad=-\\mathcal{L}\\left(\\Theta,\\Psi,\\mathbf{\\Delta}\\mathfrak{w}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $p_{\\theta}(\\hat{\\mathbf{x}})$ is some generative model\u2014e.g., a VAE\u2014for prototypes, with parameters \u03b8, and $q_{\\pmb{\\omega}}\\left(\\eta,\\,\\hat{\\mathbf{x}}\\,|\\,\\mathbf{x}\\right)=q_{\\pmb{\\omega}}\\left(\\eta\\,|\\,\\mathbf{x}\\right)p\\left(\\hat{\\mathbf{x}}\\,|\\,\\mathbf{x},\\,\\eta\\right)$ . Now, we can show that the gradient of the likelihood term in the ELBO is approximated by the gradient of our SSL loss on line 1 of Algorithm 1: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\begin{array}{c}{\\nabla_{\\omega}\\underset{q_{\\omega}(\\mathfrak{n}\\mid\\mathbf{x})p(\\hat{\\mathbf{x}}\\mid\\mathbf{x},\\mathfrak{n})}{\\mathbb{E}}[\\log p(\\mathbf{x}\\mid\\hat{\\mathbf{x}},\\mathfrak{n})]}\\\\ {\\triangleright p(\\mathbf{x}\\mid\\hat{\\mathbf{x}},\\mathfrak{n})=\\delta(\\mathbf{x}-\\mathcal{T}_{\\mathfrak{n}}(\\hat{\\mathbf{x}}))=\\underset{\\sigma^{2}\\to0}{\\operatorname*{lim}}\\mathcal{N}\\big(\\mathbf{x}\\mid\\mathcal{T}_{\\mathfrak{n}}(\\hat{\\mathbf{x}}),\\sigma^{2}\\big);}\\\\ {\\approx\\nabla_{\\omega}\\underset{q_{\\omega}(\\mathfrak{n}\\mid\\mathbf{x})p(\\hat{\\mathbf{x}}\\mid\\mathbf{x},\\mathfrak{n})}{\\mathbb{E}}\\left[\\log\\mathcal{N}\\big(\\mathbf{x}\\mid\\mathcal{T}_{\\mathfrak{n}}(\\hat{\\mathbf{x}}),\\sigma^{2}\\big)\\right]}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "$\\triangleright$ take 1 sample, $\\eta\\sim q_{\\omega}\\left(\\upeta\\mid x\\right)$ : ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\approx\\nabla_{\\!\\omega}\\log\\mathcal{N}\\big(\\pmb{x}\\,\\big|\\,\\mathcal{T}_{\\pmb{\\eta}}(\\hat{\\pmb{x}}),\\upsigma^{2}\\big)\\,,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "$\\triangleright$ definition of Gaussian PDF: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\nabla_{\\omega}-0.5\\left\\|x-\\mathcal{T}_{\\eta}\\left(\\hat{x}\\right)\\right\\|_{2}^{2}/\\sigma^{2}-\\log{\\left(\\sqrt{2\\pi}\\sigma\\right)}}\\\\ {\\mathrm{~}>\\mathrm{drop~constant~term:~}}\\\\ {\\mathrm{~}}\\\\ &{=\\nabla_{\\omega}-0.5\\,\\mathsf{m s e}\\left(x,\\mathcal{T}_{\\eta}\\left(\\hat{x}\\right)\\right)/\\sigma^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The negative sign is due to the fact that the ELBO is maximized, whereas our SSL loss is minimized. The gradient of the $K L$ -divergence term w.r.t. $\\boldsymbol{\\Psi}$ is approximated by the gradient of our MLE loss on line 8 of Algorithm 1: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\nabla_{\\Psi}D_{\\mathrm{KL}}\\left[q_{\\omega}\\left(\\eta,\\,\\hat{\\mathbf{x}}\\,|\\,\\mathbf{x}\\right)\\,|\\right|p_{\\Psi}\\left(\\eta\\mid\\hat{\\mathbf{x}}\\right)p_{\\Theta}\\left(\\hat{\\mathbf{x}}\\right)\\right]\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "$\\triangleright$ definition of $D_{\\mathrm{KL}}$ ", "page_idx": 15}, {"type": "equation", "text": "$$\n=\\nabla_{\\Psi}\\underset{q_{\\omega}(\\boldsymbol{\\eta}\\,|\\,\\mathbf{x})p(\\hat{\\mathbf{x}}\\,|\\,\\mathbf{x},\\,\\boldsymbol{\\eta})}{\\mathbb{E}}\\left[\\log\\frac{q_{\\omega}(\\boldsymbol{\\eta}\\,|\\,\\mathbf{x})\\,p\\,(\\hat{\\mathbf{x}}\\,|\\,\\mathbf{x},\\,\\boldsymbol{\\eta})}{p_{\\Psi}\\left(\\boldsymbol{\\eta}\\,|\\,\\hat{\\mathbf{x}}\\right)\\,p_{\\Theta}\\left(\\hat{\\mathbf{x}}\\right)}\\right]\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "$\\triangleright$ drop constant terms and use $\\hat{\\mathbf{x}}=\\mathcal{T}_{\\mathfrak{n}}^{-1}(\\mathbf{x})$ ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\;}&{{}=\\nabla_{\\Psi}\\underset{q_{\\omega}(\\mathfrak{n}\\,|\\,\\mathbf{x})}{\\mathbb{E}}\\left[-\\log p_{\\Psi}\\left(\\mathfrak{n}\\,\\Big|\\,\\mathcal{T}_{\\mathfrak{n}}^{-1}(\\mathbf{x})\\right)\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "$\\triangleright$ take 1 sample, $\\eta_{x}\\sim q_{\\omega}\\left(\\upeta\\mid\\mathbf{x}\\right)$ : ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\approx\\nabla_{\\boldsymbol{\\Psi}}-\\log p_{\\boldsymbol{\\Psi}}\\left(\\eta_{\\boldsymbol{x}}\\left|\\,\\mathcal{T}_{\\eta_{\\boldsymbol{x}}}^{-1}(\\boldsymbol{x})\\right)\\,.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Note that the sampling approximations in both (15) and (21) also apply to VAE-like amortized inference algorithms. ", "page_idx": 15}, {"type": "text", "text": "While ELBO training and our algorithm share some similarities, some key differences exist. For instance, we do not learn the generative and inference models jointly. This disjoint training is equivalent ", "page_idx": 15}, {"type": "text", "text": "Figure 13: Failure of an invariant VAE encoder. Top: MNIST digits sampled from the test set. Mid: Prototypes produced by VAE who\u2019s encoder is made invariant using (22), where $\\eta\\sim\\mathcal{U}\\left(-\\eta_{\\mathrm{max}},\\eta_{\\mathrm{max}}\\right)$ and $\\dot{\\eta_{\\mathrm{max}}}\\bar{=}\\left(0.25,0.25,\\pi,0.25,0.25\\right)$ . Bot: Reconstructed digits. The model becomes stuck in a local optima where the prototypes and \u2018reconstructions\u2019 are all circles and rings of various sizes depending on the input image. The averaged latent code is free of (e.g.,) rotation information but has also lost almost all information that identifies each digit. ", "page_idx": 16}, {"type": "text", "text": "to ignoring the gradient $\\nabla_{\\mathbf{\\boldsymbol{\\omega}}}D_{\\mathrm{KL}}\\left[q_{\\mathbf{\\boldsymbol{\\omega}}}\\left(\\eta,\\,\\hat{\\mathbf{x}}\\,\\vert\\,\\mathbf{x}\\right)\\vert\\vert\\,p_{\\Psi}\\left(\\eta\\mid\\hat{\\mathbf{x}}\\right)p_{\\Theta}\\left(\\hat{\\mathbf{x}}\\right)\\right]$ when training $q_{\\omega}\\left(\\mathbf{n}\\mid\\mathbf{x}\\right)$ . This KL-divergence has two components: entropy $-\\mathbb{H}\\left[q_{\\pmb{\\omega}}\\right]$ and cross entropy $\\mathbb{H}\\left[q_{\\pmb{\\omega}},p_{\\pmb{\\psi}}p_{\\pmb{\\theta}}\\right]$ . Assuming that $p_{\\Psi}(\\mathfrak{n}\\,|\\,\\hat{\\mathbf{x}})$ is sufficiently flexible, the cross entropy term should not have a significant impact on $q_{\\omega}\\left(\\mathbf{n}\\mid\\mathbf{x}\\right)$ since $p_{\\Psi}$ is trained to match $q_{\\omega}$ . On the other hand, $q_{\\omega}\\left(\\mathfrak{n}\\left|\\,\\mathbf{x}\\right)$ should be close to a delta since there should be a single prototype for each $\\mathbf{x}$ . Thus, encouraging high variance with an entropy term might actually be harmful. Another difference is that we do not need to learn $p_{\\theta}(\\hat{\\mathbf{x}})$ , which has the benefti that we can learn the symmetries in a dataset without having to learn to generate the data itself, greatly simplifying training for the complicated dataset. Furthermore, actually evaluating the gradient of the likelihood term in (12) is challenging due to the fact that $p\\left(\\mathbf{x}\\mid{\\hat{\\mathbf{x}}},\\,\\mathfrak{n}\\right)$ is a delta. ", "page_idx": 16}, {"type": "text", "text": "Given all of these differences, it might be natural to question the utility of the comparison between our algorithm and maximization of (12). Perhaps the most useful connection to draw is that of Equations (18) and (21), which motivates our MLE learning objective for $p_{\\boldsymbol{\\omega}}(\\boldsymbol{\\mathsf{n}}\\mid\\hat{\\mathbf{x}})$ as being closely related to the process of learning a prior in an ELBO. ", "page_idx": 16}, {"type": "text", "text": "In an early version of this work [Allingham et al., 2022], we trained a variant of the SGM using an ELBO similar to (12), with the main difference being that $\\hat{\\bf x}$ was modeled using a VAE and invariance was incorporated into the VAE encoder. We constructed an invariant encoder $q_{\\Phi}\\left(\\mathbf{z}\\mid\\mathbf{x}\\right)$ from a non-invariant encoder $\\hat{q}_{\\Phi}\\left(\\mathbf{z}\\mid\\mathbf{x}\\right)$ : ", "page_idx": 16}, {"type": "equation", "text": "$$\nq_{\\Phi}(\\mathbf{z}\\,|\\,\\mathbf{x})\\equiv\\mathbb{E}_{\\mathbf{\\eta}}\\left[\\hat{q}_{\\Phi}\\left(\\mathbf{z}\\,|\\,\\mathbf{x}\\right)\\right],\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "following Benton et al. [2020], van der Ouderaa and van der Wilk [2022], Immer et al. [2022]. We found that this approach worked well for a single transformation (e.g., rotation) but that it quickly broke down as the space of transformations was expanded (e.g., to all affine transformations; see Figure 13). We hypothesize that the averaging of many latent codes makes it difficult to learn an invariant representation ${\\bf z}$ without throwing away almost all of the information in $\\mathbf{x}$ . This further motivates our SSL algorithm for learning invariant prototypes. A similar observation was also made by Dubois et al. [2021], who found that an SSL-based objective was superior to an ELBO-based method for learning invariant representations in the context of compression. ", "page_idx": 16}, {"type": "text", "text": "B Further Practical Considerations ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "This section elaborates on Section 3.1 and provides additional considerations. ", "page_idx": 16}, {"type": "text", "text": "Suitability of NN architectures. The architecture of $f_{\\mathbf{\\omega}}_{\\mathbf{\\omega}}$ must be compatible with learning an equivariant mapping from $\\mathbf{x}$ to $\\boldsymbol{\\mathfrak{\\eta}}$ . For example, a standard CNN requires many convolutional fliters to represent a function that is (approximately) equivariant to continuous rotations [Maile et al., 2023]. ", "page_idx": 16}, {"type": "text", "text": "$\\mathcal{X}$ -space vs. $\\mathcal{H}$ -space SSL objective. One might notice that it is possible to remove the $\\mathcal{T}_{\\mathfrak{n}}^{-1}$ operations from both paths of the SSL objective in Figure 4 and still have a valid objective (in $\\mathcal{H}_{\\mathrm{~\\,~}}$ -space rather than $\\mathcal{X}$ -space). However, the $\\mathcal{X}$ -space version is preferred since different parameters $\\eta_{1},\\eta_{2}$ can map to the same transformed element $\\begin{array}{r}{\\mathcal{T}_{\\eta_{1}}(\\pmb{x})=\\mathcal{T}_{\\eta_{2}}(\\pmb{x})}\\end{array}$ . E.g., consider rotations transformations applied to various shapes: for a square $\\mathcal{T}_{0^{\\circ}}\\equiv\\mathcal{T}_{90^{\\circ}}\\equiv\\mathcal{T}_{180^{\\circ}}\\equiv\\mathcal{T}_{270^{\\circ}}$ all map to the same transformed image, and an $\\mathcal{H}$ -space objective incorrectly penalizes differences of $\\pm n\\times90^{\\circ}$ in $\\eta$ values. ", "page_idx": 16}, {"type": "text", "text": "We compare rotation inference nets\u2014with hidden layers of dimensions [2048, 1024, 512, 256, 128] trained for 2k steps using the AdamW optimizer with a constant learning rate of $3\\times10^{-4}$ and a batch size of 256\u2014trained on fully rotated MNIST digits using both $\\mathcal{X}$ -space and $\\mathcal{H}$ -space SSL objectives: ", "page_idx": 17}, {"type": "table", "img_path": "aFP24eYpWh/tmp/b9601aba74f6c33a5548f2fbc1939b1843717718e03edda406f52b6a71337fe7.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "When using the $\\mathcal{H}$ -space objective, we see the distance in observation $(\\chi)$ space. ", "page_idx": 17}, {"type": "text", "text": "Learning $q_{\\bf{\\omega}\\bf{0}}(\\bf{\\omega})$ instead of $f_{\\mathbf{\\omega}}_{\\mathbf{\\omega}}$ . We found that learning $f_{\\mathbf{\\omega}}_{\\mathbf{\\omega}}$ probabilistically\u2014i.e., allowing for some uncertainty in the transformation during the training process by parameterizing a density over $\\mathcal{H}$ with $q_{\\omega}(\\boldsymbol{\\mathsf{n}}\\vert\\mathbf{x})$ and sampling $\\eta$ \u2014provides small improvements in performance. The distribution $q_{\\bf{\\omega}\\omega}({\\bf{\\sigma}}{\\bf{n}}|{\\bf{x}})$ quickly collapses to a delta. Thus, we hypothesize that the added noise from sampling acts as a regularizer that is helpful at the start of training. ", "page_idx": 17}, {"type": "text", "text": "Inference network blurring schedule. Occasionally, depending on the dataset, random seed, kind of transformations being applied, and other hyperparameters, training the inference network fails, and the prototype transformations would be $100\\%$ lossy\u2014i.e., they would result in completely empty images\u2014regardless of the strength of the invertibility loss. We found that we could prevent this from happening by adding a small amount of Gaussian blur to each example. Furthermore, we found that we only needed to add this blur for a small fraction of the initial training steps to prevent the model from falling into this degenerate local optima. ", "page_idx": 17}, {"type": "text", "text": "Averaging multiple samples for the SSL loss. Just as we found averaging the MLE loss over multiple samples to improve performance, so too is averaging the SSL loss. ", "page_idx": 17}, {"type": "text", "text": "We compare rotation inference nets\u2014with hidden layers of dimensions [2048, 1024, 512, 256, 128] trained for 2k steps using the AdamW optimizer with a cosine decayed with warmup learning rate schedule that starts at $1\\times10^{-4}$ , increases to $3\\times10^{-4}$ in 500 steps, and then decreases to $1\\times\\bar{1}0^{-7}$ , with a batch size of 256\u2014trained on fully rotated MNIST digits using the SSL objective averaged over 1, 3, 5, 10, and 30 samples: ", "page_idx": 17}, {"type": "table", "img_path": "aFP24eYpWh/tmp/2d45c84de2cd0c0467ee9f60fcb67bf974da28f462ca9479867ecad1ca96d15f.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "As the number of samples increases, x-mse decreases until saturating around 5 samples. Note that this relationship is not likely to be monotonically decreasing because there is random noise in each training run (i.e., due to random NN initialization, etc.). That said, we expect it will decrease on average as the number of samples increases. We find 5 samples to be a good trade-off between improved performance and increased compute. ", "page_idx": 17}, {"type": "text", "text": "Symmetric SSL loss. In our SSL loss, based on Figure 4, we are essentially comparing the prototypes given $\\textbf{\\em x}$ and $x_{\\mathrm{{rnd}}}$ (a randomly transformed version of $\\textbf{\\em x}$ ). An alternative is to compare the prototypes given $x_{\\mathrm{rndl}}$ and $x_{\\mathrm{{rnd}}2}$ , two randomly transformed versions of $\\textbf{\\em x}$ : ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|T_{f_{\\omega}(x_{m d1})}^{-1}(x_{\\mathrm{md1}})-T_{f_{\\omega}(x_{m d2})}^{-1}(x_{\\mathrm{md2}})\\right|\\right|_{2}^{2},\\ {x_{\\mathrm{md1}}}={\\mathcal{T}_{\\eta_{\\mathrm{md1}}}}(x),\\ {x_{\\mathrm{md2}}}={\\mathcal{T}_{\\eta_{\\mathrm{md2}}}}(x),\\ \\eta_{\\mathrm{md1}},\\eta_{\\mathrm{md2}}\\sim p(\\mathsf{\\eta}_{\\mathrm{md}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "As before, we modify this loss to allow us to compose transformations to get ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|\\mathcal{T}_{f_{\\omega}(\\mathbf{x}_{\\mathrm{md}2})}\\circ\\mathcal{T}_{f_{\\omega}(\\mathbf{x}_{\\mathrm{md}})}^{-1}(x_{\\mathrm{rnd}})-x_{\\mathrm{rnd}2}\\right\\|_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The motivation for using this \u2018symmetric\u2019 SSL loss is that it provides the inference network with additional data augmentation\u2014the inference network is now unlikely ever to see the $\\textbf{\\em x}$ twice. We find that while this works well for MNIST, it does not work well for dSprites. This is because the transformations in dSprites in dSprites are more lossy than those for MNIST. E.g., it is easier to shift a small sprite out of the frame of an image compared to a large digit. Thus, the symmetric loss results in a much higher variance when used with dSprites, which negatively impacts training. ", "page_idx": 18}, {"type": "text", "text": "Composing affine transformations of images. Care must be taken when composing affine transformations of images when implemented via a coordinate transformation (e.g., affine_grid & affine_sample in PyTorch, or scipy.map_coords in Jax). To compose two affine transformations parameterised by $\\pmb{\\eta}_{1}$ and $\\pmb{\\eta}_{2}$ , the affine matrices $T(\\mathfrak{n}_{1}),T(\\mathfrak{n}_{2})$ need to be right-multiplied with one another; in other words $\\mathcal{T}_{\\eta_{2}}\\circ\\mathcal{T}_{\\eta_{1}}=\\mathcal{T}_{T(\\mathfrak{n}_{1})T(\\mathfrak{n}_{2})}^{\\prime}$ T T\u2032 (\u03b7 )T (\u03b7 ). This is because, in these implementations of affine transformation of images, the affine transformation is applied to the pixel grid (i.e., the reference frame), rather than to the image itself. In effect, the resulting transformation as applied to the objects in the image is the opposite; if the reference frame moves to the right, the objects in the image move to the left, etc. More concretely, when the reference frame is affine-transformed by $\\tau$ , the image itself is affine-transformed by $\\mathcal{T}^{-1}$ . ", "page_idx": 18}, {"type": "text", "text": "Overfitting of the generative network. While we did not observe any overftiting of the inference network (likely due to the built-in \u2018data augmentation\u2019 of our SSL loss, and the general difficulty of learning a function with equivariance to arbitrary transformations), we did find that the generative network was prone to overfitting. We addressed this by using a validation set to optimize several relevant hyper-parameters (e.g., dropout rates, number of flow layers, number of training epochs, etc.); see Appendix C. ", "page_idx": 18}, {"type": "text", "text": "Learning $p_{\\Psi}(\\mathbf{n}\\,|\\,\\hat{\\mathbf{x}})$ with imperfect inference, continued. To encourage $p_{\\Psi}(\\mathbf{n}\\mid{\\hat{\\mathbf{x}}})$ produce the same distribution for the inconsistent prototypes produced by $q_{\\bf{\\omega}}({\\bf{\\uph}}|{\\bf{\\u x}})$ , we add a consistency loss to line 8 of Algorithm 1 the MLE objective: ", "page_idx": 18}, {"type": "equation", "text": "$$\n{\\cal L}_{\\mathrm{consistency}}(\\Psi)=\\frac{1}{N^{2}}\\sum_{i=1}^{N}\\sum_{j=1}^{N}|\\log p_{i}-\\log p_{j}|,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $p_{i}=p_{\\psi}(\\eta_{x}\\mid\\hat{x}_{i}^{\\prime})$ and $\\hat{\\pmb x}_{i}^{\\prime}$ is due to the $i^{\\mathrm{th}}\\;\\mathfrak{n}_{\\mathrm{rnd}}$ sample. ", "page_idx": 18}, {"type": "text", "text": "C Experimental Setup ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We use jax with flax for NNs, distrax for probability distributions, and optax for optimizers. We use ciclo with clu to manage our training loops, ml_collections to specify our configurations, and wandb to track our experiments. The code is available at https://github.com/ cambridge-mlg/sgm. ", "page_idx": 18}, {"type": "text", "text": "Unless otherwise specified, we use the following NN architectures and other hyperparameters for all of our experiments. We use the AdamW optimizer with weight decay of $1\\!\\times\\!10^{-4}$ , global norm gradient clipping, and a linear warm-up followed by a cosine decay as a learning rate schedule. The exact learning rates and schedules for each model are discussed below. We use a batch size of 512. ", "page_idx": 18}, {"type": "text", "text": "All of our MLPs use gelu activations and LayerNorm. In some cases, we use Dropout. The structure of each layer is ${\\mathrm{Dense}}\\rightarrow{\\mathrm{gelu}}\\rightarrow{\\mathrm{LayerNorm}}\\rightarrow{\\mathrm{Dropout.}}{\\mathrm{~W}}$ henever we learn or predict a scale parameter $\\sigma$ , it is constrained to be positive using a softplus operation. ", "page_idx": 18}, {"type": "text", "text": "Inference network. We use a MLP with hidden layers of dimension [2048, 1024, 512, 256]. The network outputs a mean $\\boldsymbol{\\mathfrak{\\eta}}$ prediction for each example and the uncertainty\u2014as mentioned in Appendix B\u2014is implemented as a homoscedastic scale parameter. We train for $60\\mathbf{k}$ steps. For each example, we average the loss over 5 random augmentations. In some settings\u2014also mentioned in Appendix B\u2014we add a small amount of blur to the images with a Gaussian filter of size 5 for the first $1\\%$ of training steps. The $\\sigma$ value for the filter was linearly decayed from their maximum to 0. The initial maximum value is specified below. ", "page_idx": 18}, {"type": "text", "text": "Generative network. Our generative model is a Neural Spline Flow [Durkan et al., 2019] with 6 bins in the range $[-3,3]$ . We use an MLP with hidden layers of dimension [1024, 512, 512] as a shared feature extractor. The base normal distribution\u2019s mean and scale parameters are predicted by another MLP, with hidden layers of dimension [256, 256], whose input is the shared feature representation. The parameters of the spline at each layer of the flow are predicted by MLPs with a single hidden layer of dimension 256, with a dropout rate of 0.1, whose input is a concatenation of the shared feature representation, and the (masked) outputs of the previous layer. For each example, we average the loss over 5 random augmentations. ", "page_idx": 19}, {"type": "text", "text": "C.1 MNIST under affine transformations ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We make use of the MNIST dataset [LeCun et al., 2010], which is available under the MIT license. ", "page_idx": 19}, {"type": "text", "text": "We split the MNIST training set by removing the last $10\\mathbf{k}$ examples and using them exclusively for validation and hyperparameter sweeps. ", "page_idx": 19}, {"type": "text", "text": "When randomly augmenting the inputs for our SSL (see Section 2.1 and Figure 4) and MLE (see Section 3.1) losses, we sample transformation parameters from $\\mathcal{U}(-\\eta_{\\mathrm{max}},\\,\\eta_{\\mathrm{max}})$ , where $\\eta_{\\mathrm{max}}~=$ $(0.25,0.25,\\pi,0.25,0.25)$ is the maximum ( $x$ -shift, $y$ -shift, rotation, $x_{\\mathrm{~\\,~}}$ -scale, $y$ -scale) applied to the images. All affine transformations are applied with bi-cubic interpolation. ", "page_idx": 19}, {"type": "text", "text": "Inference network. The invertibility loss Linvertibility (7) is multiplied by a factor of 0.1. For the VAE data-efficiency results in Figure 11, we performed the following hyperparameter grid search for each random seed and amount of training data: ", "page_idx": 19}, {"type": "text", "text": "\u2022 blur $\\sigma_{\\mathrm{init}}\\in[0,3]$ ,   \n\u2022 gradient clipping norm $\\in[3,10]$ ,   \n\u2022 learning rate $\\in[1\\!\\times\\!10^{-3},3\\!\\times\\!10^{-4},1\\!\\times\\!10^{-4}]$ ,   \n\u2022 initial learning rate multiplier $\\in[3\\!\\times\\!10^{-2},1\\!\\times\\!10^{-2}]$ , \u2022 final learning rate multiplier $\\in[1\\!\\times\\!10^{-3},3\\!\\times\\!10^{-4},]$ , and \u2022 warm-up steps $\\%\\in[0.05,0.1,0.2$ ]. ", "page_idx": 19}, {"type": "text", "text": "All of the other MNIST affine transformation results use a blur $\\sigma_{\\mathrm{init}}$ of 0, a gradient clipping norm of 10, a learning rate of $3{\\times}10^{-4}$ , an initial learning rate multiplier of $1\\!\\times\\!10^{\\bar{-2}}$ , a final learning rate multiplier of $1\\!\\times\\!10^{-3}$ , and a warm-up steps $\\%$ of 0.2, which are the best hyperparameters for $50\\mathrm{k}$ training examples with an arbitrarily chosen random seed. We use the \u2018symmetric\u2019 SLL loss discussed in Appendix B. ", "page_idx": 19}, {"type": "text", "text": "Generative network. We use an initial learning rate multiplier of 0.1, a gradient clipping norm of 2, and a warm-up steps $\\%$ of 0.2. For the VAE data-efficiency results in Figure 11, we performed the following hyperparameter grid search for each random seed and amount of training data: ", "page_idx": 19}, {"type": "text", "text": "\u2022 learning rate $\\in[3\\!\\times\\!10^{-3},3\\!\\times\\!10^{-4}]$ ,   \n\u2022 final learning rate multiplier $\\in[0.3,0.03]$ ,   \n\u2022 number of training steps $\\in[7.5\\mathbf{k},15\\mathbf{k},30\\mathbf{k},60\\mathbf{k}]$ ,   \n\u2022 number of flow layers $\\in[4,5,6]$ ,   \n\u2022 shared feature extractor dropout rate $\\in[0.05,0.1,0.2]$ , and \u2022 consistency loss multiplier $\\in[0,1]$ (whether or not to use (25)). ", "page_idx": 19}, {"type": "text", "text": "Note that we use the log-likelihood of the validation data under the generative model to select the best hyper-parameters. I.e., we do not use the total loss, which may or may not include the consistency term, since these losses are not directly comparable. We require a trained inference network when sweeping over the generative network hyperparameters. We use the inference network hyperparameters for the same (random seed, number of training examples) pair. All of the other MNIST affine transformation results use a learning rate of $3{\\times}10^{-3}$ , a final learning rate multiplier of 0.03, 60k training steps, 6 flow layers, a dropout rate of 0.2 in the shared feature extractor, and a consistency loss multiplier of 1, which are the best hyperparameters for $50\\mathrm{k}$ training examples. ", "page_idx": 19}, {"type": "text", "text": "C.2 MNIST under color transformations ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We follow the same setup as above for color transformation on the MNIST dataset, with the following exceptions. We do not use an invertibility loss when training the inference network. Instead, for both the inference and generative networks, we constrain the outputs to be in $[-\\eta_{\\mathrm{max}},\\,\\eta_{\\mathrm{max}}]+(0.5,0.,0.)$ , where $\\eta_{\\mathrm{max}}=(0.5,2.301,0.51)$ using with tanh and scale bijectors. We randomly augment the inputs by sampling transformation parameters from $\\mathcal{U}(-\\eta_{\\mathrm{max}}+(0.5,0.,0.)$ , $\\eta_{\\mathrm{max}}+(0.5,0.,0.))$ . ", "page_idx": 20}, {"type": "text", "text": "Inference network. We use a blur $\\sigma_{\\mathrm{init}}$ of 3, a gradient clipping norm of 2, a learning rate of $3{\\times}10^{-4}$ , an initial learning rate multiplier of $1\\!\\times\\!1\\bar{0}^{-2}$ , a final learning rate multiplier of $\\bar{1}\\!\\times\\!10^{-4}$ , and a warm-up steps $\\%$ of 0.1, which were chosen using the same grid sweep as MNIST with affine transformations. ", "page_idx": 20}, {"type": "text", "text": "Generative network. We use a learning rate of $3\\!\\times\\!10^{-3}$ , with an initial learning rate multiplier of $1\\!\\times\\!10^{-1}$ , a final learning rate multiplier of $3{\\times}10^{-2}$ , 15k training steps, 6 flow layers, and a dropout rate of 0.2 in the shared feature extractor. ", "page_idx": 20}, {"type": "text", "text": "C.3 dSprites under affine transformations ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We make use of the dSprites dataset [Matthey et al., 2017], which is available under the Apache 2.0 license. ", "page_idx": 20}, {"type": "text", "text": "For our dSprites experiments, we follow the same setup as for MNIST under affine transformations above, with the following exceptions. We do not use an invertibility loss when training the inference network. Instead, for both the inference and generative networks, we constrain their outputs to be in $[-\\eta_{\\mathrm{max}},\\,\\eta_{\\mathrm{max}}]$ , where $\\eta_{\\mathrm{max}}=(0.75,0.75,\\pi,0.75,0.75)$ using with tanh and scale bijectors. We do not use the \u2018symmetric\u2019 SSL loss discussed in Appendix B. ", "page_idx": 20}, {"type": "text", "text": "Inference network. We randomly augment the inputs by sampling transformation parameters from $\\mathcal{U}(-\\eta_{\\mathrm{max}},\\,\\eta_{\\mathrm{max}})$ , where $\\eta_{\\mathrm{max}}$ matches the constraints above. We use a blur $\\sigma_{\\mathrm{init}}$ of 3, a gradient clipping norm of 3, a learning rate of $1\\!\\times\\!10^{-3}$ , an initial learning rate multiplier of $3{\\times}10^{-{\\bar{2}}}$ , a final learning rate multiplier of $1\\!\\times\\!10^{-3}$ , and a warm-up steps $\\%$ of 0.05, which were chosen using the same grid sweep as MNIST with affine transformations. ", "page_idx": 20}, {"type": "text", "text": "Generative network. We randomly augment the inputs by sampling transformation parameters from $\\mathcal{U}(-\\eta_{\\mathrm{max}}\\times0.75$ , $\\eta_{\\mathrm{max}}\\times0.75)$ , where $\\eta_{\\mathrm{max}}$ matches the constraints above. We use a learning rate of $3\\!\\times\\!10^{-4}$ , a final learning rate multiplier of 0.3, 60k training steps, 6 flow layers, and a dropout rate of 0.05 in the shared feature extractor, which were chosen using the same grid sweep as MNIST with affine transformations. ", "page_idx": 20}, {"type": "text", "text": "Although we swept over the consistency loss multiplier, we accidentally always used a consistency loss multiplier of 1 in our experiments. This means that for some (random seed, amount of training data) pairs the performance of our generative network is slightly lower than it should be since the chosen hyperparameters may correspond to a consistency loss multiplier of 0. We include this detail for reproducibility but note that it does not change our findings in any material way. ", "page_idx": 20}, {"type": "text", "text": "C.3.1 dSprites Setup ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "The original dSprites dataset contains sprites with the following factors of variation [Matthey et al., 2017]. ", "page_idx": 20}, {"type": "text", "text": "\u2022 Color: white   \n\u2022 Shape: square, ellipse, heart   \n\u2022 Scale: 6 values linearly spaced in [0.5, 1]   \n\u2022 Orientation: 40 values linearly spaced in $[0,2\\pi]$   \n\u2022 X position: 32 values linearly spaced in $[0,1]$   \n\u2022 Y position: 32 values linearly spaced in $[0,1]$ ", "page_idx": 20}, {"type": "image", "img_path": "aFP24eYpWh/tmp/d6ca8af3cd547541578226ab489c712454d42465dac379b789ecd58cbdc85f6b.jpg", "img_caption": ["Figure 14: Latent factor distributions for our modified dSprites data loader. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "The dataset consists of sprites with the outer product of these factors, for a total of 737280 examples. We modified our data loader to resample the sprites proportional to the following distributions on the latent factors conditioned on the shape. ", "page_idx": 21}, {"type": "text", "text": "\u2013 Scale: TruncNorm $(\\mu=0.75,\\,\\sigma^{2}=0.2,\\,\\operatorname*{min}=0.55,\\,\\operatorname*{max}=1.0)$   \n\u2013 Orientation: $\\mathcal{U}(0.0,\\,2\\pi)$   \n\u2013 X position: $\\mathcal{U}(0.5,\\,0.95)$   \n\u2013 Y position: $\\mathcal{U}(0.5,\\,0.95)$ ", "page_idx": 21}, {"type": "text", "text": "\u2022 ellipse ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "\u2013 Scale: TruncNorm(0.65, 0.15, 0.5, 0.85) \u2013 Orientation: $\\mathcal{U}(0.0,\\,\\pi/2)$ \u2013 X position: TruncNorm(0.5, 0.25, 0.1, 0.9) \u2013 Y position: TruncNorm(0.5, 0.15, 0.35, 0.65) ", "page_idx": 21}, {"type": "text", "text": "\u2022 heart ", "text_level": 1, "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{-\\mathrm{\\Scale:}\\ \\mathcal{U}(0.9,1.0)}\\\\ &{-\\mathrm{\\Orientation:}\\ \\delta(0.0)}\\\\ &{-\\mathrm{\\Xposition:}\\ \\mathcal{U}(0.1,\\ 0.5)}\\\\ &{-\\mathrm{\\Yposition:}\\ 0.5\\cdot\\mathcal{U}(0.1,\\ 0.3)+0.5\\cdot\\mathcal{U}(0.7,\\ 0.9)}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "An example of the resulting empirical distributions over the latent factors is shown in Figure 14. The three shapes are sampled with equal proportions. ", "page_idx": 21}, {"type": "text", "text": "C.4 GalaxyMNIST under affine and color transformations ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We make use of the GalaxyMNIST dataset [Walmsley et al., 2022], which is available under the GPL-3.0 licence. ", "page_idx": 21}, {"type": "text", "text": "For our GalaxyMNIST experiments, we follow the same setup as for MNIST under affine transformations above, with the following exceptions. We do not use an invertibility loss when training the inference network. Instead, for both the inference and generative networks, we constrain their outputs to be in $[-\\eta_{\\mathrm{max}},\\,\\eta_{\\mathrm{max}}]\\,+\\,(0.,0.,0.,0.,0.,0.5,0.{\\dot{,}}0.)$ , where $\\begin{array}{r l}{\\pmb{\\eta}_{\\mathrm{max}}}&{{}=}\\end{array}$ $(0.75,0.75,\\pi,0.75,0.75,0.5,2.31,0.51)$ using with tanh and scale bijectors. This dataset contains $10\\mathbf{k}$ examples. We use the last 2k as our test set, and the previous 1k as a validation set. ", "page_idx": 21}, {"type": "text", "text": "Inference network. We use a MLP with hidden layers of dimension [1024, 1024, 512, 256]. We train for 10k steps. We randomly augment the inputs by sampling transformation parameters from $\\mathcal{U}(-\\eta_{\\mathrm{max}}+(0.,0.,0.,0.,0.,0.5,0.,0.)$ , $\\eta_{\\mathrm{max}}+(0.,0.,0.,0.,0.,0.5,0.,0.)$ ), where $\\eta_{\\mathrm{max}}$ matches the constraints above. For the VAE data-efficiency results in Figure 12, we performed the same hyperparameter grid search as above for each random seed and amount of training data. All of the other GalaxyMNIST results use a blur $\\sigma_{\\mathrm{init}}$ of 0, a gradient clipping norm of 10, a learning rate of $3{\\times}10^{-4}$ , an initial learning rate multiplier of $1\\!\\times\\!10^{-2}$ , a final learning rate multiplier of $3{\\times}\\bar{1}0^{-4}$ , and a warm-up steps $\\%$ of 0.2, which are the best hyperparameters for 7k training examples with an arbitrarily chosen random seed. We use the \u2018symmetric\u2019 SLL loss discussed in Appendix B. ", "page_idx": 22}, {"type": "text", "text": "Generative network. We randomly augment the inputs by sampling transformation parameters from $\\mathcal{U}(-\\eta_{\\mathrm{max}}\\times0.75+(0.,0.,0.,0.,0.,0.5,0.,0.)$ , $\\eta_{\\mathrm{max}}\\times0.75+(0.,0.,0.,0.,0.,0.5,0.,0.))$ ), where $\\eta_{\\mathrm{max}}$ matches the constraints above. For the VAE data-efficiency results in Figure 12, we perform the same hyperparameter grid search as above for each random seed and amount of training data, with the following changes.7 The sweep for number of training steps is [3.75k, 7.5k, 15k]. All of the other GalaxyMNIST results use a learning rate of $3\\!\\times\\!10^{-4}$ , a final learning rate multiplier of 0.03, $15\\mathbf{k}$ training steps, 4 flow layers, a dropout rate of 0.05 in the shared feature extractor, and a consistency loss multiplier of 1, which were chosen using the same grid sweep for an arbitrary random seed and $7\\mathbf{k}$ training examples. ", "page_idx": 22}, {"type": "text", "text": "C.5 PatchCamelyon under affine and color transformations ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We make use of the PatchCamelyon dataset [Veeling et al., 2018], which is available under the Creative Commons Zero v1.0 Universal license. ", "page_idx": 22}, {"type": "text", "text": "We resized the images from $96\\times96$ pixels to $64\\times64$ using bilinear interpolation. The dataset has dedicated train, test, and validation splits which we use without any modifications. ", "page_idx": 22}, {"type": "text", "text": "We follow the same setup as for GalaxyMNIST under affine and color transformations above, with the exceptions listed below. We only used a single random seed. ", "page_idx": 22}, {"type": "text", "text": "Inference network. We train for 20k steps. ", "page_idx": 22}, {"type": "text", "text": "Generative network. The sweep for number of training steps is [15k, 30k, 60k].8 ", "page_idx": 22}, {"type": "text", "text": "C.6 VAE, AugVAE, and InvVAE ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Our VAEs use a latent code size of 20. The prior is a normal distribution with learnable mean and scale, initialized to 0s and 1s, respectively. ", "page_idx": 22}, {"type": "text", "text": "Our VAE encoders are LeNet-style CNNs with convolutional feature extractors followed by an MLP with a single hidden layer of size 256. The convolutional feature extractors use gelu activations and LayerNorm. The structure is $\\mathtt{C o n v\\to g e l u\\to L a y e r N o r m}$ . All Conv layers use $3{\\times}3$ filters. The first two Conv have a stride of 2, while all others have a stride of 1. In between the convolutional layers and the MLP, there is a special dimensionality reduction Conv with only 3 fliters followed by a flatten. For each dimension of the latent code, the encoder predicts a mean $\\upmu$ and a scale $\\upsigma$ . The means and scales are initialized to 0s and 1s, respectively. ", "page_idx": 22}, {"type": "text", "text": "Our VAE decoders are inverted versions of our encoders. That is, we reverse the order of all of the Dense and Conv layers. The dimensionality reduction Conv layer and the flatten operation are replaced with the appropriate Dense layer and reshape operation. We replace all other Conv layers with ConvTransposed layers For each pixel of an image, the decoder predicts a mean $\\upmu$ . We learn a homoscedastic per-pixel scale \u03c3. The scales are initialized to 1. ", "page_idx": 22}, {"type": "text", "text": "We use an initial learning rate multiplier of $3{\\times}10^{-2}$ , and a final learning rate multiplier of $1\\!\\times\\!10^{-4}$ . We run the following grid sweep for each (random seed, number of training examples, maximum added rotation angle) triplet: ", "page_idx": 23}, {"type": "text", "text": "\u2022 learning rate $\\in[3{\\times}10^{-3},6{\\times}10^{-3},9{\\times}10^{-3}],$ , \u2022 convolutional filters $\\in[(64,128),(64,128,256)],$ \u2022 number of training steps $\\in[5\\mathrm{k},10\\mathrm{k},20\\mathrm{k}]$ , and \u2022 warm-up steps $\\%\\in[0.15,0.2]$ . ", "page_idx": 23}, {"type": "text", "text": "When running the sweep for AugVAE and InvVAE, we use the inference and generative network hyperparameters for the same (random seed, number of training examples) pair. ", "page_idx": 23}, {"type": "text", "text": "C.6.1 PatchCamelyon ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "For our PatchCamelyon experiments, we use only a single random seed and a slightly modified hyperparameter sweep: ", "page_idx": 23}, {"type": "text", "text": "\u2022 learning rate $\\in[3\\!\\times\\!10^{-3}$ , $6\\!\\times\\!10^{-3}$ ,   \n\u2022 convolutional filters $\\in[(64,128),(64,128,256),(128,256,512)],$ \u2022 number of dense hidden layers $\\in[1,2]$ ,   \n\u2022 number of training steps $\\in[20\\mathbf{k},30\\mathbf{k},40\\mathbf{k}]$ , and   \n\u2022 warm-up steps $\\%\\in[0.15]$ . ", "page_idx": 23}, {"type": "text", "text": "C.7 Parametrisations of Symmetry transformations ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We consider five affine transformations: shift in $x$ , shift in $y$ , rotation, scaling in $x$ , and scaling in $y$ . We represent these transformations using affine transformation matrices $\\begin{array}{r}{\\mathbf{\\check{A}}=\\exp\\left(\\sum_{i}\\eta_{i}\\mathbf{\\check{G}}_{i}\\right)}\\end{array}$ , where $G_{i}$ are generator matrices for rotation, translation, and scaling; see Benton et al. [2020]. The transformations are applied to an image by transforming the coordinates $(x,y)$ of each pixel, as in Jaderberg et al. [2015]: $[x^{\\prime}\\quad y^{\\prime}\\quad1]^{\\mathsf{T}}=\\pmb{\\dot{A}}\\cdot[x\\quad y\\quad1]^{\\mathsf{T}}$ . ", "page_idx": 23}, {"type": "text", "text": "To parameterize color transformations, we use an equivalent representation of color images in HueSaturation-Value (HSV) space, where each pixel is represented as a tuple $(h,s,v)\\in\\bar{\\{}[-\\pi,\\pi]\\times$ $[0,1]\\times[0,1]\\}$ . Intuitively, HSV space represents the color of each pixel in a conical space where the hue corresponds to the rotation angle around the cone\u2019s vertical axis, the saturation corresponds to the radial distance from the cone\u2019s center, and the value corresponds to the distance along the cone\u2019s vertical axis, with a value of 0 corresponding to the tip of the cone, and a value of 1 corresponding to the base of the cone. We color-transform an image by transforming each pixel as ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\left[\\!\\!\\begin{array}{l}{h^{\\prime}}\\\\ {s^{\\prime}}\\\\ {v^{\\prime}}\\end{array}\\!\\!\\right]=\\left[\\!\\!\\begin{array}{l l l}{(h+2\\pi\\eta_{h})}&{\\!\\!\\!\\mathrm{mod~}2\\pi}\\\\ {\\!\\!\\!\\operatorname*{max}(0,\\operatorname*{min}(s\\exp(\\eta_{s}),1))}\\\\ {\\!\\!\\!\\operatorname*{max}(0,\\operatorname*{min}(v\\exp(\\eta_{v}),1))}\\end{array}\\!\\!\\right].\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "We therefore obtain $\\eta=(\\eta_{h},\\eta_{s},\\eta_{v})\\in\\{[0,1]\\times\\mathbb{R}\\times\\mathbb{R}\\}$ . We choose this specific form of parametrizing the $\\eta$ parameters in order to gain the convenience of simply adding and subtracting in $\\eta$ space when carrying out color transform compositions and inverses. More concretely, with our chosen parametrization, we obtain the property that $\\mathcal{T}_{\\eta_{1}}\\circ\\mathcal{T}_{\\eta_{2}}=\\mathcal{T}_{\\eta_{1}+\\eta_{2}}$ . Therefore, we can easily perform compositions and inversions in $\\eta$ space for color transformations without resorting to matrix multiplications. In order to achieve this, we first consider hue, which is easy to parametrize in an additive fashion using a modulo operation due to the fact that hue is represented as a rotation angle in HSV space. On the other hand, saturation and value are discontinuous parameters that vary between 0 and 1, and cannot be directly modeled in an additive fashion, as they can\u2019t take values outside their range. Instead, we model them as multiplicative factors in $\\mathbb{R}^{+}$ , where we first exponentiate $\\eta_{s}$ and $\\eta_{v}$ to ensure the multiplicative factors are positive. We further clip the obtained values to ensure they are in the range $[0,1]$ . This parametrization allows us to effectively add parameters to compose them, as the multiplicative factors compose in exponent space. ", "page_idx": 23}, {"type": "text", "text": "In order to ensure that we can easily backpropagate through the clipping operation, we define a passthrough_clip function in Jax, where we define a custom gradient that doesn\u2019t zero out gradients even if the inputs to the function are out of bounds. We find that using the passthrough_clip operation is essential to training the model. ", "page_idx": 23}, {"type": "text", "text": "D Compute Requirements ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "The experiments for this paper were performed on a cluster equipped with NVIDIA A100 GPUs. All model training requires only a single such GPU. However, we used up to 64 GPUs at a time to run our hyper-parameter searches in parallel. Including exploratory experiments, all hyperparameter sweeps, discarded runs, etc., the total compute used for this paper is approximately 250 A100 GPU days. The total cost to reproduce the experiments in the paper is approximately 135 A100 GPU days. We break this cost down as follows. Note that the cost for different figures do not naively sum as hyper-parameter sweeps for some figures are reused for others, as discussed in Appendix C. ", "page_idx": 24}, {"type": "text", "text": "Figure 8a: 6 days Inference net sweeps: 4 days Generative net sweeps: 2 days   \nFigure 8b: 3 days Inference net sweeps: 2 days Generative net sweeps: 1 day   \nFigure 8c: 3 days Inference net sweeps: 2 days Generative net sweeps: 1 day   \nFigure 8d: 7 days Inference net sweeps: 6 days Generative net sweeps: 1 day   \nFigure 9: 3 days Inference net sweeps: 2 days Generative net sweeps: 1 day   \nFigure 10: 2 days Inference net sweeps: 2 days   \nFigure 11: 69 days Inference net sweeps: 30 days Generative net sweeps: 12 days VAE sweeps: 27 days   \nFigure 12: 53 days Inference net sweeps: 36 days Generative net sweeps: 8 days VAE sweeps: 9 days   \nE Additional Results   \nE.1 Comparisons to LieGAN ", "page_idx": 24}, {"type": "text", "text": "In this section, we compare the ability of our method to learn symmetries to LieGAN [Yang et al., 2023], which uses a generator-discriminator framework to automatically discover equivariances from a dataset using generative adversarial training. Similar to [Yang et al., 2023], we transform the MNIST dataset to have rotations in the range $[-45^{\\circ},45^{\\circ}]$ , which ensures the dataset contains SE(2) symmetry (rotations and translations). The dataset is processed and our method is trained as described in Section 4.1. For LieGAN, following the experimental design of [Yang et al., 2023], we set the number of generator channels to $c=1$ , and consider learnable 6-dimensional Lie matrices in the generator model. The discriminator model consists of a pre-trained LeNet5 feature extractor as the backbone, and the validator is a 3-layer MLP with 512 hidden units and ReLU activations. We train the GAN for 100 epochs with a batch size of 64, and obtain the Lie matrix below ", "page_idx": 24}, {"type": "equation", "text": "$$\nL=\\left[\\begin{array}{c c c}{{0.02}}&{{-0.34}}&{{0.28}}\\\\ {{0.33}}&{{0.08}}&{{-0.05}}\\\\ {{0}}&{{0}}&{{0}}\\end{array}\\right].\n$$", "text_format": "latex", "page_idx": 24}, {"type": "image", "img_path": "aFP24eYpWh/tmp/6b9d336e6a9fcfd624d6c5e20bdeb286711c70a06e6934e5c674f91633cf5b19.jpg", "img_caption": ["Figure 15: Learnt augmentation distribution for the MNIST dataset rotated in the range $[-45^{\\circ},45^{\\circ}]$ for our SGM model, and the LieGAN method. The columns correspond to distributions for translation in $x$ , translation in $y$ , rotation, scaling in $x$ , and scaling in $y$ . (Row 1-5) Our SGM learns accurate ranges of rotational invariance present in the training dataset of a width of $\\pi/2$ for most training examples, along with learning the natural invariances present in the training data for translations and scaling. Furthermore, for certain digits (i.e. 0), the SGM model accurately predicts a uniform distribution from $[-\\pi,\\pi]$ , signifying that rotationally invariant digits such as a 0 would not display a more narrow rotational invariance. (Row 6) On the other hand, the LieGAN model learns a single Lie matrix across the entire training dataset that encodes the maximum possible range of transformations, and predicts a uniform distribution between those ranges. It can be seen that LieGAN inaccurately predicts a large range for translations in $x$ , and does not recover the correct range of rotational invariances present in the training dataset. "], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "In Figure 15, we can see that LieGAN struggles to correctly recover the range of invariances present in the training dataset, especially for translations in $x$ . It is also unable to provide a fine-grained representation of invariances depending on specific examples or type of digits. We note that we re-implemented the rotated MNIST experiment from Yang et al. [2023], as the code for the image domain experiments was not open-source. Hence, the choice of using a pre-trained LeNet5 model for the discriminator, and the specific hyperparameter configurations, were informed decisions made by us based on ablations. However, our results appear to be inline with those presented by Yang et al. [2023]; concretely, we note that the results presented in their paper also display a mismatch between the invariances present in the dataset and those learned by LieGAN. For example, in their Figure 11, we see that the sampled digits are often rotated by significantly more than $45^{\\circ}$ . Furthermore, we see evidence of typical GAN mode collapse, with many very similar rotations for each digit. ", "page_idx": 25}, {"type": "text", "text": "E.2 PatchCamelyon \u2014 Boundary Effects ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "In this section, we provide a \u201cnegative\u201d result for our SGM when applied to the PatchCamelyon dataset [Veeling et al., 2018]. The examples in this dataset, unlike those used in Section 4, contain \u201ccontent\u201d up to the boundaries of the images. ", "page_idx": 25}, {"type": "text", "text": "Figure 16 shows examples of the prototypes and learned distributions for this dataset, with affine and color transformations. In particular, the allowed rotation was between $\\pm180^{\\circ}$ , while the actual dataset has only a rotational invariance of $\\pm n\\times90^{\\circ}$ . We see that in some cases the prototypes are rotated by close to $\\pm n\\times45^{\\circ}$ relative to the original images. In other cases, the rotation of the prototypes relative to the original images is closer to $\\pm n\\times90^{\\circ}$ . In the latter case, the learned distribution over rotation is close to the true distribution, but in the former case, the model learns a distribution that is closer to uniform. As a result, the resampled digits often display boundary effects that are not present in the original dataset. Otherwise, our SGM has learned reasonable distributions for translation, scaling, and HSV transformations. ", "page_idx": 25}, {"type": "image", "img_path": "aFP24eYpWh/tmp/c17e97675e31b0d0bfe58afa1807357f2166360cac7d6a67a1cc5d6a7a039039.jpg", "img_caption": ["Figure 17: VAE data-efficiency for PatchCamelyon. "], "img_footnote": [], "page_idx": 25}, {"type": "image", "img_path": "aFP24eYpWh/tmp/e5f2c4de486a5b6f7c953cb0ab9bade7abf1deb43bbac386f8b5f37be3775700.jpg", "img_caption": ["(a) Top: samples from the test set. Mid: prototypes for each test example. Bot: resampled versions of each test example given the prototype. "], "img_footnote": [], "page_idx": 26}, {"type": "image", "img_path": "aFP24eYpWh/tmp/883410cdd63199c48acee4240fff33e33d76d70c1fef97b1ae0e41ef790495a9.jpg", "img_caption": ["(b) From left to right, test examples, their prototypes, and the corresponding marginal distributions over translation in $x$ , translation in $y$ , rotation, scaling in $x$ , scaling in $y$ , hue, saturation, and value. ", "Figure 16: Prototypes and learned distributions for PatchCamelyon. "], "img_footnote": [], "page_idx": 26}, {"type": "image", "img_path": "aFP24eYpWh/tmp/9a998620c279a09462494b5d5651638bc3edee3f2bc2d7e9efbe596c85650da0.jpg", "img_caption": [], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "Figure 18: Incorporating symmetries improves data efficiency. Importance-weighted lower bound (IWLB) and reconstruction MSE (mean and std. err. over 3 random seeds) for rotated MNIST with a standard VAE (with and without standard data augmentations) and two VAE variants that incorporate symmetries via our SGM. Improved data efficiency is demonstrated by better performance with less training data, and reduced sensitivity to added rotations. ", "page_idx": 26}, {"type": "text", "text": "Figure 17 compares a standard VAE with AugVAE, an SGM-VAE hybrid model. We see that for small amounts of data, the VAE and AugVAE perform similarly. However, as the amount of training data increases, the VAE performs better. This is likely because the SGM has not learned the true distribution over rotations. ", "page_idx": 26}, {"type": "text", "text": "This \u201cnegative\u201d result highlights the importance of correctly choosing the prior transformation distributions in some settings. In this case, the performance of the SGM would have been improved by choosing a categorical distribution over rotations. ", "page_idx": 26}, {"type": "text", "text": "E.3 Additional Experiments ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "In this section, we provide additional plots to supplement those in Section 4. ", "page_idx": 26}, {"type": "text", "text": "Figure 18 extends the results in Figure 11 to by including an additional metric: reconstruction MSE.   \nOur findings with IWLB are consistent for this metric. ", "page_idx": 26}, {"type": "text", "text": "Figure 19 expands on Figure 8b in two ways. Firstly, it makes it clear that our inference network is able to provide the same or very similar prototype for observations in the same orbit. Secondly, it provides many more resampled examples of each digit, further demonstrating that our SGM has correctly captured the symmetries present in the dataset. Figure 20 expands on Figure 8c in the same way. ", "page_idx": 26}, {"type": "text", "text": "Figure 21 extends Figure 9 by including all of the digits shown in Figure 19. The conclusions are much the same as before. We see that the learned distributions all make sense, especially for the most easily interpretable transformation parameter, rotations. Again, we note that smaller and bigger prototypes have appropriately different scaling distributions. Figure 22 provides the learnt marginal distributions for the digits in Figure 20. Here, we manually controlled the distributions over hue and saturation when loading the dataset, so we know that the range of the hue distribution should be approximately $\\pi$ , while the range of the saturation distribution should be around 0.3. We see that this is indeed the case. We did not control the value of the images, so it is more difficult to interpret those. However, given that most (non-black) pixels are bright (i.e., close to 1) it makes sense that our SGM learns multiplicative values closer to 1. ", "page_idx": 27}, {"type": "text", "text": "Finally, Figure 23 extends our dSprites results in two ways. Firstly, it provides many more resampled sprites, which also serves to demonstrate further that our SGM has captured the symmetries correctly. Secondly, the figure includes empirical distributions of positions of each of the classes of digits, which we have carefully controlled as described in Appendix C.3.1. These empirical distributions for the dataset are compared with empirical distributions for our resampled sprites. We see that although the resampled densities don\u2019t match the original densities perfectly, their general shapes and ranges are correct. ", "page_idx": 27}, {"type": "image", "img_path": "aFP24eYpWh/tmp/edac59f3ff5bb851b9586edba7fd071806a20de2197944efebc634559e09e0d2.jpg", "img_caption": ["Figure 19: Columns from left to right: only rotation, only translations, translation $^+$ rotation $^+$ scaling. Each of the blocks in this figure follows the same format. Top: 7 examples from the same orbit. Mid: The corresponding prototypes. Bot: Resampled versions of the digits, given the prototypes. "], "img_footnote": [], "page_idx": 28}, {"type": "image", "img_path": "aFP24eYpWh/tmp/aa57400b05ded9a757c963f3682572320cfbca46fab4d5df02384f9c8d898074.jpg", "img_caption": ["Figure 20: Columns from left to right: only hue, only saturation, only value. Each of the blocks in this figure follows the same format. Top: 7 examples from the same orbit. Mid: The corresponding prototypes. Bot: Resampled versions of the digits, given the prototypes. "], "img_footnote": [], "page_idx": 29}, {"type": "image", "img_path": "aFP24eYpWh/tmp/051c5618035d569fbf8995631f5ec0c9f3fc4c864c5c912a513c23b7bc834c34.jpg", "img_caption": ["Figure 21: From left to right, test examples from MNIST, their prototypes, and the corresponding marginal distributions over translation in $x$ , translation in $y$ , rotation, scaling in $x$ , and scaling in $y$ . "], "img_footnote": [], "page_idx": 30}, {"type": "image", "img_path": "aFP24eYpWh/tmp/745f0270991efc4512be891c52992393619df9fa8484859224d7e4090bed20c9.jpg", "img_caption": ["Figure 22: From left to right, test examples from MNIST with added hue in the range 0 to $0.6\\pi$ , and saturation scaled by a factor in 0.6 to 0.9, their prototypes, and the corresponding marginal distributions over hue, saturation, and value. "], "img_footnote": [], "page_idx": 31}, {"type": "image", "img_path": "aFP24eYpWh/tmp/d15def8c6ec7fb4130155c5ea5b24f1f0202ab4e9d03219338a53f24330587d0.jpg", "img_caption": ["Figure 23: From left to right, samples from dSprites, the empirical distribution over the positions of the sprites, sprites resampled using our SGM, and the empirical distributions over the resampled sprites\u2019 positions. We see that the resampled sprites are visually very similar to the original sprites in terms of sizes, rotations, and positions. Furthermore, we see that the empirical distributions match in terms of ranges, although they are imperfect in density. "], "img_footnote": [], "page_idx": 32}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: In this paper we present a novel generative model of symmetry transformations. In our abstract and conclusion make two claims about this model: (1) it can accurately capture the symmetries in a dataset, and (2) when combined with a standard generative model we see improvements in data-efficiency. We believe that both of these claims reflect the paper\u2019s contributions well. In the introduction, we also discuss some aspirational goals for disentanglement and scientific discovery, however, we are clear that these are not the focus of the paper. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 33}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: Throughout the paper we provide footnotes to clarify the scope of our claims and point out their limitations (e.g., footnote 1 clarifies that our generative model does not always match the true generative process of the data). We also provide a detailed list of potential issues when using our method in practice. Furthermore, in our conclusion, we note that our method only learns approximate symmetries and requires a super-set of possible symmetries in the data to be specified. Finally, we provide some \u201cnegative results\u201d in Appendix E.2, which are also mentioned as a limitation in our conclusion. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. ", "page_idx": 33}, {"type": "text", "text": "\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 34}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: This paper contains no theoretical results. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 34}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: We provide a clear algorithm description (Algorithm 1), discussions of all of the practical issues encountered when implementing our method (Section 3.1 and Appendix B, and detailed experimental setup descriptions\u2014including dataset splits, model architectures, hyper-parameter settings and sweeps, transformation parameterisations, and a list of software libraries used\u2014(Appendix C). ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.   \n(b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.   \n(c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).   \n(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 34}, {"type": "text", "text": "", "page_idx": 35}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: We have provided a link to a GitHub repository. We have not given detailed instructions for reproducing the experiments, however, all of our configurations and training scripts are provided. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 35}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: We provide detailed experimental setup descriptions\u2014including dataset splits, model architectures, hyper-parameter settings and sweeps, transformation parameterisations, and a list of software libraries used\u2014in Appendix C. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 35}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: For all quantitative results, we report the mean and standard error over 3 random seeds. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 36}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: See Appendix D for estimates of the compute costs, in the form of A100 GPU days, for the whole project as well as each of the figures in the main text. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 36}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: We have read and acknowledged the NeurIPS Code of Ethics. We believe that our paper conforms with this code in every respect. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. ", "page_idx": 36}, {"type": "text", "text": "\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 37}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: Our work is foundational research that is not tied to any particular application for which we see a direct path to negative applications. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 37}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: Our work does not pose such risks. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 37}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: We cite and provide licenses for all of the datasets used in this paper. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 38}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: Our paper does not release any new assets. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 38}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: We did not make use of any crowdsourcing or human subjects. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 38}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 38}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 39}, {"type": "text", "text": "Justification: We did not make use of any crowdsourcing or human subjects. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 39}]