[{"figure_path": "aFP24eYpWh/figures/figures_0_1.jpg", "caption": "Figure 1: Left: An example of a symmetry-aware generative process that we aim to model in this paper. A prototype x ( ) is transformed by T\u03b7 into an observation x (,,). The transformation-e.g., rotation-is parameterized by \u03b7-e.g., an angle. Right: The corresponding orbit-i.e., the set of all possible instances of x that can result from applying T\u03b7-with a few elements shown. Under this generative process, the prototype is an arbitrary orbit element. Each element in the orbit has a probability p (xx) induced by p (\u03b7 | x). E.g., for handwritten '3's, we expect digits in an upright orientation with some rotation around, say \u00b140\u00b0, corresponding to natural variations in handwriting.", "description": "This figure illustrates the symmetry-aware generative model proposed in the paper. The left panel shows a generative process where a prototype x is transformed by a parameterized transformation T\u03b7 (where \u03b7 represents parameters like rotation angle) to produce an observation x.  The right panel shows the resulting orbit, which is the set of all possible transformed versions of x. The model learns to what extent these transformations are present in the data, offering a framework for data augmentation.", "section": "1 Introduction"}, {"figure_path": "aFP24eYpWh/figures/figures_2_1.jpg", "caption": "Figure 4: Self-supervised symmetry learning. We encourage f<sub>w</sub>(x) to be equivariant by mapping x and a randomly transformed x\u0303 to the same z. Gray text shows examples for each variable in the graph. Note that z and x\u0303<sub>rnd</sub> may not appear in the dataset; see Figure 1.", "description": "This figure illustrates the self-supervised learning process used to make the transformation inference function f<sub>w</sub>(x) equivariant.  Two inputs are processed: the original sample x, and a randomly transformed version x\u0303<sub>rnd</sub>. Both are passed through the function f<sub>w</sub>(x), which outputs transformation parameters that are then used to map the samples to prototypes. A mean squared error (MSE) loss encourages consistency, making the function equivariant to transformations.", "section": "Self-supervised symmetry learning"}, {"figure_path": "aFP24eYpWh/figures/figures_2_2.jpg", "caption": "Figure 2: SGM graphical model. The implicit edges denote that x is fully specified by \u03b7 and x\u2014since x = T\u03b7\u22121(x)\u2014and thus only \u03b7 needs to be inferred given and observation x.", "description": "This figure shows a graphical model representation of the Symmetry-aware Generative Model (SGM). The model has three latent variables: x (prototype), \u03b7 (equivariant component capturing symmetries), and x (observation).  The model shows how the prototype x and the transformation parameters \u03b7 combine to produce the observed data point x. The arrows indicate the direction of the generative process, while dashed lines represent inference steps. The model is designed such that the prototype x is invariant to transformations, while \u03b7 is equivariant to them.", "section": "2 Symmetry-aware Generative Model (SGM)"}, {"figure_path": "aFP24eYpWh/figures/figures_4_1.jpg", "caption": "Figure 1: Left: An example of a symmetry-aware generative process that we aim to model in this paper. A prototype x ( ) is transformed by T\u03b7 into an observation x (, , ). The transformation\u2014e.g., rotation\u2014is parameterized by \u03b7\u2014e.g., an angle. Right: The corresponding orbit\u2014i.e., the set of all possible instances of x that can result from applying T\u03b7\u2014with a few elements shown. Under this generative process, the prototype is an arbitrary orbit element. Each element in the orbit has a probability p (x|x) induced by p (\u03b7 | x). E.g., for handwritten '3's, we expect digits in an upright orientation with some rotation around, say \u00b140\u00b0, corresponding to natural variations in handwriting.", "description": "This figure illustrates the symmetry-aware generative model proposed in the paper.  The left panel shows a generative process where a prototype '3' is transformed (e.g., rotated) by a parameter \u03b7 to produce an observed '3'.  The right panel displays the set of all possible transformations of the prototype (its orbit). The model aims to learn the distribution of these transformations from data, enabling efficient data augmentation and improved model generalization.", "section": "1 Introduction"}, {"figure_path": "aFP24eYpWh/figures/figures_5_1.jpg", "caption": "Figure 1: Left: An example of a symmetry-aware generative process that we aim to model in this paper. A prototype x ( ) is transformed by \u03a4\u03b7 into an observation x (,,). The transformation-e.g., rotation-is parameterized by n-e.g., an angle. Right: The corresponding orbit-i.e., the set of all possible instances of x that can result from applying Tn-with a few elements shown. Under this generative process, the prototype is an arbitrary orbit element. Each element in the orbit has a probability p (xx) induced by p (\u03b7 | x). E.g., for handwritten '3's, we expect digits in an upright orientation with some rotation around, say \u00b140\u00b0, corresponding to natural variations in handwriting.", "description": "The figure shows an example of symmetry-aware generative process. On the left, a prototype is transformed into an observation via a transformation parameterized by \u03b7. On the right, the figure shows the corresponding orbit, which is the set of all possible instances of x that can be produced by applying the transformation.  The model assumes each observation is generated by applying a transformation to a latent prototype. The prototype itself is invariant to the transformation, capturing only non-symmetric properties of the data.", "section": "1 Introduction"}, {"figure_path": "aFP24eYpWh/figures/figures_5_2.jpg", "caption": "Figure 7: Examples of learned distributions over angles p\u03c8 (\u03b7 | x)\u2014with different degrees of invariance in the prototype x, given the true p(\u03b7|x).", "description": "This figure illustrates different scenarios of learned distributions over transformation parameters (\u03b7) given a prototype (x) in the context of symmetry-aware generative models.  The scenarios are:\n\n(a) FULL invariance: A single prototype represents all transformed versions of a data point.\n(b) PARTIAL invariance: A few prototypes represent transformed versions, reflecting some level of invariance.\n(c) NONE: Each transformed version has a unique prototype, indicating no learned invariance.\nThe figure showcases how the model's ability to capture symmetries is reflected in the distribution p\u03c8(\u03b7|x), demonstrating varying levels of invariance.", "section": "3.2 Modelling Choices"}, {"figure_path": "aFP24eYpWh/figures/figures_6_1.jpg", "caption": "Figure 8: Top: samples from the test set. Mid: prototypes for each test example. Bot: resampled versions of each test example given the prototype. Prototypes for examples from the same orbit (and in some cases from distinct but similar orbits) match (e.g., their size, position, rotation, etc. are similar). Resampled examples are usually indistinguishable from test examples.", "description": "This figure shows the results of applying the Symmetry-aware Generative Model (SGM) to four different datasets: dSprites, MNIST (under affine transformations), MNIST (under color transformations), and GalaxyMNIST.  The top row displays samples from the test set. The middle row displays the learned prototypes for each test example. The bottom row shows resampled versions of the test examples, generated by applying learned transformations to their corresponding prototypes.  The figure demonstrates that the SGM effectively learns prototypes that capture the underlying symmetries present in the data, producing resampled examples nearly indistinguishable from the originals.", "section": "Experiments"}, {"figure_path": "aFP24eYpWh/figures/figures_7_1.jpg", "caption": "Figure 8: Top: samples from the test set. Mid: prototypes for each test example. Bot: resampled versions of each test example given the prototype. Prototypes for examples from the same orbit (and in some cases from distinct but similar orbits) match (e.g., their size, position, rotation, etc. are similar). Resampled examples are usually indistinguishable from test examples.", "description": "This figure shows examples of prototypes and resampled examples generated by the Symmetry-aware Generative Model (SGM) on four datasets under affine and color transformations.  The top row displays original samples from the test set. The middle row shows the corresponding prototypes learned by the model. The bottom row displays new examples generated by applying learned transformations to the prototype.  The results demonstrate that the SGM successfully learns to capture the symmetries in the data by producing prototypes that are nearly invariant to transformations, and by generating new examples that are almost indistinguishable from the originals.", "section": "Experiments"}, {"figure_path": "aFP24eYpWh/figures/figures_7_2.jpg", "caption": "Figure 10: Iterative prototype inference. Left: starting with a test example x, we get a prototype x1, then treating prototype xi as an observed example we predict the next prototype xi+1. Right: The average magnitude of the transformation parameters as a function of iterations of this process.", "description": "This figure shows the results of an iterative prototype inference process. Starting with a test example, the model infers a prototype.  Then, treating this prototype as a new observation, the model infers another prototype and so on. The left panel shows several examples of this process. The right panel displays the average magnitude of the inferred transformation parameters across iterations. This shows how much the prototypes change across the iterations, demonstrating the model's ability to find an invariant representation.  The relatively small magnitude after a few iterations confirms the model's tendency toward invariant representations.", "section": "4.1 Learning Symmetries"}, {"figure_path": "aFP24eYpWh/figures/figures_8_1.jpg", "caption": "Figure 11: Incorporating symmetries improves data efficiency. Importance-weighted lower bound (IWLB) (mean and std. err. over 3 random seeds) on rotated MNIST for a standard VAE (w. and w.o. data aug.) and two VAE variants that incorporate symmetries via our SGM. Improved data efficiency is demonstrated by better performance with less training data and less sensitivity to added rotation.", "description": "This figure compares the performance of four different models on the rotated MNIST dataset: a standard VAE, a VAE with standard data augmentation, AugVAE (VAE with our SGM for data augmentation), and InvVAE (VAE with our SGM using only the invariant representation). The models are trained with different amounts of training data (12500, 25000, 37500, and 50000) and different amounts of added rotation (15\u00b0, 90\u00b0, and 180\u00b0).  The y-axis shows the IWLB, a metric that measures the performance of a generative model. The figure demonstrates that AugVAE and InvVAE are more data-efficient than the standard VAE, particularly when there is less training data or more rotation.  InvVAE shows the best performance in almost every scenario, highlighting the advantages of incorporating the symmetry information directly into the model.", "section": "4.2 VAE Data Efficiency"}, {"figure_path": "aFP24eYpWh/figures/figures_8_2.jpg", "caption": "Figure 11: Incorporating symmetries improves data efficiency. Importance-weighted lower bound (IWLB) (mean and std. err. over 3 random seeds) on rotated MNIST for a standard VAE (w. and w.o. data aug.) and two VAE variants that incorporate symmetries via our SGM. Improved data efficiency is demonstrated by better performance with less training data and less sensitivity to added rotation.", "description": "This figure compares the performance of a standard Variational Autoencoder (VAE) model with two variations incorporating the Symmetry-aware Generative Model (SGM) proposed in the paper.  The variations are AugVAE (data augmentation with SGM) and InvVAE (invariant representation with SGM). The comparison is made across varying amounts of training data and different levels of added rotation to the MNIST digits. The results show that incorporating the SGM improves data efficiency, especially in scenarios with limited training data or increased rotation.", "section": "4.2 VAE Data Efficiency"}, {"figure_path": "aFP24eYpWh/figures/figures_21_1.jpg", "caption": "Figure 1: Left: An example of a symmetry-aware generative process that we aim to model in this paper. A prototype x ( ) is transformed by \u03a4\u03b7 into an observation x (, ,). The transformation-e.g., rotation-is parameterized by n-e.g., an angle. Right: The corresponding orbit-i.e., the set of all possible instances of x that can result from applying Tn-with a few elements shown. Under this generative process, the prototype is an arbitrary orbit element. Each element in the orbit has a probability p (xx) induced by p (\u03b7 | x). E.g., for handwritten '3's, we expect digits in an upright orientation with some rotation around, say \u00b140\u00b0, corresponding to natural variations in handwriting.", "description": "This figure illustrates the core idea of the Symmetry-aware Generative Model (SGM). The left panel shows a generative process where a prototype '3' is transformed by a rotation parameter \u03b7 (angle) to generate an observed digit. The right panel shows the orbit, which is the set of all possible transformations of the prototype. The probability of each transformed digit is determined by the probability distribution of \u03b7 given the prototype.", "section": "1 Introduction"}, {"figure_path": "aFP24eYpWh/figures/figures_25_1.jpg", "caption": "Figure 15: Learnt augmentation distribution for the MNIST dataset rotated in the range [-45\u00b0, 45\u00b0] for our SGM model, and the LieGAN method. The columns correspond to distributions for translation in x, translation in y, rotation, scaling in x, and scaling in y. (Row 1-5) Our SGM learns accurate ranges of rotational invariance present in the training dataset of a width of \u03c0/2 for most training examples, along with learning the natural invariances present in the training data for translations and scaling. Furthermore, for certain digits (i.e. 0), the SGM model accurately predicts a uniform distribution from [-\u03c0, \u03c0], signifying that rotationally invariant digits such as a 0 would not display a more narrow rotational invariance. (Row 6) On the other hand, the LieGAN model learns a single Lie matrix across the entire training dataset that encodes the maximum possible range of transformations, and predicts a uniform distribution between those ranges. It can be seen that LieGAN inaccurately predicts a large range for translations in x, and does not recover the correct range of rotational invariances present in the training dataset.", "description": "This figure compares the learned augmentation distributions for MNIST data rotated in the range [-45\u00b0, 45\u00b0] using the proposed SGM and LieGAN.  The SGM accurately captures the ranges of rotational invariance, while LieGAN fails to precisely recover these ranges, especially for translations, highlighting the SGM's superior ability to learn and represent dataset-specific symmetries.", "section": "E.1 Comparisons to LieGAN"}, {"figure_path": "aFP24eYpWh/figures/figures_25_2.jpg", "caption": "Figure 11: Incorporating symmetries improves data efficiency. Importance-weighted lower bound (IWLB) (mean and std. err. over 3 random seeds) on rotated MNIST for a standard VAE (w. and w.o. data aug.) and two VAE variants that incorporate symmetries via our SGM. Improved data efficiency is demonstrated by better performance with less training data and less sensitivity to added rotation.", "description": "This figure compares the performance of four different models on rotated MNIST dataset: a standard VAE, a VAE with data augmentation, a VAE using the proposed SGM for data augmentation (AugVAE), and a VAE using the SGM to convert each example to its prototype before feeding into VAE (InvVAE). The performance is measured by the importance-weighted lower bound (IWLB), and the results are shown for different amounts of training data and different levels of added rotation. The AugVAE model shows improved data efficiency and robustness to added rotations, highlighting the benefits of incorporating symmetry information into generative models.", "section": "4.2 VAE Data Efficiency"}, {"figure_path": "aFP24eYpWh/figures/figures_26_1.jpg", "caption": "Figure 8: Top: samples from the test set. Mid: prototypes for each test example. Bot: resampled versions of each test example given the prototype. Prototypes for examples from the same orbit (and in some cases from distinct but similar orbits) match (e.g., their size, position, rotation, etc. are similar). Resampled examples are usually indistinguishable from test examples.", "description": "This figure shows the results of applying the Symmetry-aware Generative Model (SGM) to four different datasets: dSprites, MNIST, and GalaxyMNIST.  The top row displays samples from the test sets of each dataset. The middle row shows the prototypes generated by the SGM for each of the test examples, demonstrating that the model identifies invariant features despite variations in the original data.  The bottom row shows resampled versions of each test example generated from their corresponding prototype.  The similarity between the resampled images and the test images visually demonstrates the SGM's ability to learn and generate examples that capture the symmetries present in the data.", "section": "Experiments"}, {"figure_path": "aFP24eYpWh/figures/figures_26_2.jpg", "caption": "Figure 16: Prototypes and learned distributions for PatchCamelyon.", "description": "This figure shows the results for the PatchCamelyon dataset. The top row shows samples from the test set. The middle row shows the corresponding prototypes generated by the Symmetry-aware Generative Model (SGM). The bottom row displays the learned distributions over the transformation parameters (translation in x, translation in y, rotation, scaling in x, scaling in y, hue, saturation, and value) for each test example, given its prototype.  The figure demonstrates the SGM's ability to learn the underlying symmetries in the data and generate plausible samples.", "section": "Experiments"}, {"figure_path": "aFP24eYpWh/figures/figures_26_3.jpg", "caption": "Figure 8: Top: samples from the test set. Mid: prototypes for each test example. Bot: resampled versions of each test example given the prototype. Prototypes for examples from the same orbit (and in some cases from distinct but similar orbits) match (e.g., their size, position, rotation, etc. are similar). Resampled examples are usually indistinguishable from test examples.", "description": "This figure shows the results of applying the Symmetry-aware Generative Model (SGM) to four different datasets: dSprites, MNIST, and GalaxyMNIST under affine and color transformations.  The top row displays samples from the test set. The middle row shows the learned prototypes generated by the SGM for each test example. The bottom row presents resampled versions of each test example, created by the SGM using the learned prototype and applying various transformations. The figure demonstrates that the model learns to generate prototypes that capture the inherent symmetries within the data.  Prototypes from the same orbit (meaning they differ only by transformations like rotation or translation) are very similar, and the resampled examples are almost identical to the originals, demonstrating the ability of the SGM to capture and reproduce data symmetries.", "section": "Experiments"}, {"figure_path": "aFP24eYpWh/figures/figures_28_1.jpg", "caption": "Figure 8: Top: samples from the test set. Mid: prototypes for each test example. Bot: resampled versions of each test example given the prototype. Prototypes for examples from the same orbit (and in some cases from distinct but similar orbits) match (e.g., their size, position, rotation, etc. are similar). Resampled examples are usually indistinguishable from test examples.", "description": "This figure shows the results of applying the Symmetry-aware Generative Model (SGM) to four different datasets: dSprites, MNIST, and GalaxyMNIST. For each dataset, the top row shows examples from the test set; the middle row shows the prototypes generated by the SGM; and the bottom row shows resampled versions of the test examples, generated by applying transformations to the prototypes. The results demonstrate that the SGM is able to learn the symmetries present in the data and generate realistic samples that are nearly indistinguishable from the original data.", "section": "Experiments"}, {"figure_path": "aFP24eYpWh/figures/figures_29_1.jpg", "caption": "Figure 8: Top: samples from the test set. Mid: prototypes for each test example. Bot: resampled versions of each test example given the prototype. Prototypes for examples from the same orbit (and in some cases from distinct but similar orbits) match (e.g., their size, position, rotation, etc. are similar). Resampled examples are usually indistinguishable from test examples.", "description": "This figure shows the results of applying the Symmetry-aware Generative Model (SGM) to three different datasets: dSprites, MNIST, and GalaxyMNIST.  The top row displays examples from the test set. The middle row shows the learned prototypes generated by the SGM for each test example. The bottom row shows resampled versions of each test example generated using the corresponding prototype and the learned distribution of transformations. The results demonstrate that the SGM is able to learn accurate and representative prototypes that capture the essential characteristics of the data, generating resampled examples that are very similar to real examples.", "section": "Experiments"}, {"figure_path": "aFP24eYpWh/figures/figures_30_1.jpg", "caption": "Figure 8: Top: samples from the test set. Mid: prototypes for each test example. Bot: resampled versions of each test example given the prototype. Prototypes for examples from the same orbit (and in some cases from distinct but similar orbits) match (e.g., their size, position, rotation, etc. are similar). Resampled examples are usually indistinguishable from test examples.", "description": "This figure shows the results of applying the Symmetry-aware Generative Model (SGM) on four different datasets: dSprites, MNIST, and GalaxyMNIST under affine and color transformations. The top row displays samples from the test set.  The middle row shows the prototypes generated by the SGM for each test example.  The bottom row shows resampled versions of the test examples, generated using the corresponding prototypes.  The results demonstrate the SGM's ability to learn and generate realistic examples that closely resemble the original data, highlighting its capacity to capture underlying symmetries.", "section": "Experiments"}, {"figure_path": "aFP24eYpWh/figures/figures_31_1.jpg", "caption": "Figure 8: Top: samples from the test set. Mid: prototypes for each test example. Bot: resampled versions of each test example given the prototype. Prototypes for examples from the same orbit (and in some cases from distinct but similar orbits) match (e.g., their size, position, rotation, etc. are similar). Resampled examples are usually indistinguishable from test examples.", "description": "This figure shows the results of applying the Symmetry-aware Generative Model (SGM) to four different datasets (dSprites, MNIST, and GalaxyMNIST) under affine and color transformations. The top row shows samples from the test set of each dataset. The middle row shows the prototypes generated by the SGM for each test example. The bottom row shows resampled versions of the test examples, generated by applying transformations to the corresponding prototype.  The results demonstrate that the SGM is able to generate realistic and plausible samples from the data distribution, and that the prototypes capture the underlying symmetries in the data.", "section": "4 Experiments"}, {"figure_path": "aFP24eYpWh/figures/figures_32_1.jpg", "caption": "Figure 8: Top: samples from the test set. Mid: prototypes for each test example. Bot: resampled versions of each test example given the prototype. Prototypes for examples from the same orbit (and in some cases from distinct but similar orbits) match (e.g., their size, position, rotation, etc. are similar). Resampled examples are usually indistinguishable from test examples.", "description": "This figure shows the results of applying the Symmetry-aware Generative Model (SGM) to four different datasets (dSprites, MNIST, and GalaxyMNIST) with two different transformations (affine and color). The top row shows examples from the test set; the middle row shows the prototypes generated by the SGM for each test example; and the bottom row shows the resampled examples generated by the SGM, given the corresponding prototype. The figure demonstrates that the SGM is able to learn the symmetries present in the data and generate realistic resampled examples.", "section": "4.1 Learning Symmetries"}]