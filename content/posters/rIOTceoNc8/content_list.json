[{"type": "text", "text": "Graph Coarsening with Message-Passing Guarantees ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Antonin Joly Nicolas Keriven IRISA, Rennes, France CNRS, IRISA, Rennes, France antonin.joly@inria.fr nicolas.keriven@cnrs.fr ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Graph coarsening aims to reduce the size of a large graph while preserving some of its key properties, which has been used in many applications to reduce computational load and memory footprint. For instance, in graph machine learning, training Graph Neural Networks (GNNs) on coarsened graphs leads to drastic savings in time and memory. However, GNNs rely on the Message-Passing (MP) paradigm, and classical spectral preservation guarantees for graph coarsening do not directly lead to theoretical guarantees when performing naive message-passing on the coarsened graph. ", "page_idx": 0}, {"type": "text", "text": "In this work, we propose a new message-passing operation specific to coarsened graphs, which exhibit theoretical guarantees on the preservation of the propagated signal. Interestingly, and in a sharp departure from previous proposals, this operation on coarsened graphs is often oriented, even when the original graph is undirected. We conduct node classification tasks on synthetic and real data and observe improved results compared to performing naive message-passing on the coarsened graph. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In recent years, several applications in data science and machine learning have produced large-scale graph data [20, 5]. For instance, online social networks [13] or recommender systems [40] routinely produce graphs with millions of nodes or more. To handle such massive graphs, researchers have developed general-purpose graph reduction methods [4], such as graph coarsening [32, 7]. It consists in producing a small graph from a large graph while retaining some of its key properties, and starts to play an increasingly prominent role in machine learning applications [7]. ", "page_idx": 0}, {"type": "text", "text": "Graph Neural Networks. Machine Learning on graphs is now largely done by Graph Neural Networks (GNNs) [37, 27, 5]. GNNs are deep architectures on graph that rely on the MessagePassing paradigm [16]: at each layer, the representation $H_{i}^{l}\\,\\in\\,\\breve{\\mathbb{R}}^{d_{l}}$ of each node $1\\leq i\\leq N$ , is updated by aggregating and transforming the representations of its neighbours at the previous layer $\\{H_{j}^{l-1}\\}_{j\\in\\mathcal{N}(i)}$ , where $\\mathcal{N}(i)$ is the neighborhood of $i$ . In most examples, this aggregation can be represented as a multiplication of the node representation matrix $H^{l-1}\\in\\mathbb{R}^{N\\times d_{l-1}}$ by a propagation matrix $S\\in\\mathbb{R}^{N\\times N}$ related to the graph structure, followed by a fully connected transformation. That is, starting with initial node features $\\bar{H}^{0}$ , the GNN $\\Phi_{\\theta}$ outputs after $k$ layers: ", "page_idx": 0}, {"type": "equation", "text": "$$\nH^{l}=\\sigma\\left(S H^{l-1}\\theta_{l}\\right),\\quad\\Phi_{\\theta}(H^{0},S)=H^{k}\\,,\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "where $\\sigma$ is an activation function applied element-wise (often ReLU), $\\theta_{l}\\in\\mathbb{R}^{d_{l-1}\\times d_{l}}$ are learned parameters and $\\theta=\\{\\theta_{1},\\ldots,\\theta_{k}\\}$ . We emphasize here the dependency of the GNN on the propagation matrix $S$ . Classical choices include mean aggregation $\\bar{S_{\\mathrm{~}}}=\\bar{D^{-1}\\bar{A}}$ or the normalized adjacency $S=D^{-\\frac{1}{2}}A D^{-\\frac{1}{2}}$ , with $A$ the adjacency matrix of the graph and $D$ the diagonal matrix of degrees. When adding self-loops to $A$ , the latter corresponds for instance to the classical GCNconv layer [27]. ", "page_idx": 0}, {"type": "text", "text": "An interesting example is the Simplified Graph Convolution (SGC) model [42], which consists in removing all the non-linearity $\\pi=i d$ the identity function). Surprisingly, the authors of [42] have shown that SGC reaches quite good performances when compared to non-linear architectures and due to its simplicity, SGC has been extensively employed in theoretical analyses of GNNs [46, 26]. ", "page_idx": 1}, {"type": "text", "text": "Graph coarsening and GNNs. In this paper, we consider graph coarsening as a preprocessing step to downstream tasks [11, 21]: indeed, applying GNNs on coarsened graphs leads to drastic savings in time and memory, both during training and inference. Additionally, large graphs may be too big to fti on GPUs, and mini-batching graph nodes is known to be a difficult graph sampling problem [14], which may no longer be required on a coarsened graph. A primary question is then the following: is training a GNN on a coarsened graph provably close to training it on the original graph? To examine this, one must study the interaction between graph coarsening and message-passing. ", "page_idx": 1}, {"type": "text", "text": "There are many ways of measuring the quality of graph coarsening algorithms, following different criteria [10, 32, 7]. A classical objective is the preservation of spectral properties of the graph Laplacian, which gave birth to different algorithms [32, 8, 4, 24, 33]. Loukas [32] materializes this by the so-called Restricted Spectral Approximation (RSA, see Sec. 2) property: it roughly states that the frequency content of a certain subspace of graph signals is approximately preserved by the coarsening, or intuitively, that the coarsening is well-aligned with the low-frequencies of the Laplacian. Surprisingly, the RSA does not generally lead to guarantees on the message-passing process at the core of GNNs, even for very simple signals. That is, simply performing message-passing on the coarsened graph using $S_{c}$ , the naive propagation matrix corresponding to $S$ on the coarsened graph (e.g. normalized adjacency of the coarsened graph when $S$ is the normalized adjacency of the original one) does not guarantee that the outputs of the GNN on the coarsened graph and the original graph will be close, even with high-quality RSA. ", "page_idx": 1}, {"type": "text", "text": "Contribution. In this paper, we address this problem by defining a new propagation matrix $S_{c}^{\\mathrm{MP}}$ specific to coarsened graphs, which translate the RSA bound to message-passing guarantees: we show in Sec. 3.3 that training a GNN on the coarsened graph using $S_{c}^{\\mathrm{MP}}$ is provably close to training it on the original graph. The proposed matrix $S_{c}^{\\mathrm{MP}}$ can be computed for any given coarsening and is not specific to the coarsening algorithm used to produce it1, as long as it produces coarsenings with RSA guarantees. Interestingly, our proposed matrix $S_{c}^{\\mathrm{MP}}$ is not symmetric in general even when $S$ is, meaning that our guarantees are obtained by performing oriented message-passing on the coarsened graph, even when the original graph is undirected. To our knowledge, the only previous work to propose a new propagation matrix for coarsened graphs is [21], where the authors obtain guarantees for a specific GNN model (APPNP [28]), which is quite different from generic message-passing. ", "page_idx": 1}, {"type": "text", "text": "Related Work. Graph Coarsening originates from the multigrid-literature [36], and is part of a family of methods commonly referred to as graph reduction, which includes graph sampling [19], which consists in sampling nodes to extract a subgraph; graph sparsification [38, 1, 31], that focuses on eliminating edges; or more recently graph distillation [22, 45, 23], which extends some of these principles by authorizing additional informations inspired by dataset distillation [41]. ", "page_idx": 1}, {"type": "text", "text": "Some of the first coarsening algorithms were linked to the graph clustering community, e.g. [9] which used recursively the Graclus algorithm [10] algorithm itself built on Metis [25]. Linear algebra technics such as the Kron reduction were also employed [32] [12]. In [32], the author presents a greedy algorithm that recursively merge nodes by optimizing some cost, with the purpose of preserving spectral properties of the coarsened Laplacian. This is the approach we use in our experiments (Sec. 4). It was followed by several similar methods with the same spectral criterion [8, 4, 24, 33]. Since modern graph often includes node features, other approaches proposed to take them into account in the coarsening process, often by learning the coarsening with specific regularized loss [29, 34]. Closer to this work, [11] proposes an optimization process to explicitely preserve the propagated features, however with no theoretical guarantees and only one step of message-passing. While these works often seek to preserve a fixed number of node features as in e.g. [29]), the RSA guarantees [32] leveraged in this paper are uniform over a whole subspace: this stronger property is required to provide guarantees for GNNs with several layers. ", "page_idx": 1}, {"type": "text", "text": "Graph coarsening has been intertwined with GNNs in different ways. It can serve as graph pooling [44] within the GNN itself, with the aim of mimicking the pooling process in deep convolutional models on images. In the recent literature, the terms \u201ccoarsening\u201d and \u201cpooling\u201d tend to be a bit exchangeable. For instance, some procedures that were initially introduced as pooling could also be used as pre-processing step, such as Graclus [10], introduced by [9] as a pooling scheme, see also [17]. Graph pooling is often data-driven and fully differentiable, such as Diffpool [44], SAGPool [30], and DMoN [39]. Theoretical work on their ability to distinguish non homomorphic graphs after pooling have been conducted [3]. In return, GNNs can also be trained to produce data-driven coarsenings, e.g. GOREN [6] which proposes to learn new edge weights with a GNN. As mentioned before, in the framework we consider here, graph coarsening is a preprocessing step with the aim of saving time and memory during training and inference [21]. Here few works derive theoretical guarantees for GNNs and message-passing, beyond the APPNP architecture examined in [21]. To our knowledge, the proposed ScMP is the first to yield such guarantees. ", "page_idx": 2}, {"type": "text", "text": "Outline. We start with some preliminary material on graph coarsening and spectral preservation in Sec. 2. We then present our main contribution in Sec. 3: a new propagation matrix on coarsened graphs that leads to guarantees for message-passing. As is often the case in GNN theoretical analysis, our results mostly hold for the linear SGC model, however we still outline sufficient assumptions that would be required to apply our results to general GNNs, which represent a major path for future work. In Sec. 4 we test the proposed propagation matrix on real and synthetic data, and show how it leads to improved results compared to previous works. The code is available at https://gitlab.inria.fr/anjoly/mp-guarantees-graph-coarsening, and proofs are deferred to App. A. ", "page_idx": 2}, {"type": "text", "text": "Notations. For a matrix $Q\\in\\mathbb{R}^{n\\times N}$ , its pseudo-inverse $Q^{+}\\in\\mathbb{R}^{N\\times n}$ is obtained by replacing its nonzero singular values by their inverse and transposing. For a symmetric positive semi-definite (p.s.d.) matrix ${\\cal L}\\,\\in\\,\\mathbb{R}^{N\\times N}$ , we define $L^{\\frac{1}{2}}$ by replac\u221aing its eigenvalues by their square roots, and $L^{-\\frac{1}{2}}=(L^{+})^{\\frac{1}{2}}$ . For $\\boldsymbol{x}\\in\\mathbb{R}^{N}$ we denote by $\\Vert x\\Vert_{L}={\\sqrt{x^{\\top}L x}}$ the Mahalanobis semi-norm associated to $L$ . For a matrix $P\\,\\in\\,\\mathbb{R}^{N\\times N}$ , we denote by $\\|P\\|=\\operatorname*{max}_{\\|x\\|=1}\\left\\|P x\\right\\|$ the operator norm of $P$ , and $\\|P\\|_{L}\\,=\\,\\|L^{\\frac{1}{2}}P L^{-\\frac{1}{2}}\\|$ . For a subspace $R$ , we say that a matrix $P$ is $R$ -preserving if $x\\,\\in\\,R$ implies $P x\\,\\in\\,R$ . Finally, for a matrix $X\\,\\in\\,\\mathbb{R}^{N\\times d}$ , we denote its columns by $X_{:,i}$ , and define $\\begin{array}{r}{\\|\\bar{X^{}}\\|_{:,L}=\\sum_{i}\\|X_{:,i}\\|_{L}}\\end{array}$ . ", "page_idx": 2}, {"type": "text", "text": "2 Background on Graph Coarsening ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We mostly adopt the framework of Loukas [32], with some generalizations. A graph $G$ with $N$ nodes is described by its weighted adjacency matrix $A\\,\\in\\,\\mathbb{R}^{N\\times N}$ . We denote by $\\mathit{\\dot{L}}\\gets\\mathbb{R}^{N\\times N}$ a notion of symmetric p.s.d. Laplacian of the graph: classical choices include the combinatorial Laplacian $L=D-A$ with $D={\\bar{D}}(A):=\\operatorname{dia}\\!{\\bar{\\operatorname{g}}}({\\bar{A}}1_{n})$ the diagonal matrix of the degrees, or the symmetric normalized Laplacian $L=I_{N}-D^{-\\frac{1}{2}}A D^{-\\frac{1}{2}}$ . We denote by $\\lambda_{\\operatorname*{max}},\\lambda_{\\operatorname*{min}}$ respectively the largest and smallest non-zero eigenvalue of $L$ . ", "page_idx": 2}, {"type": "text", "text": "Coarsening matrix. A coarsening algorithm takes a graph $G$ with $N$ nodes, and produces a coarsened graph $G_{c}$ with $n<N$ nodes. Intuitively, nodes in $G$ are grouped in \u201csuper-nodes\u201d in $G_{c}$ (Fig. 1), with some weights to outline their relative importance. This mapping can be represented via a coarsening matrix Q \u2208Rn\u00d7N: ", "page_idx": 2}, {"type": "equation", "text": "$$\nQ={\\left\\{\\begin{array}{l l}{Q_{k i}>0}&{{\\mathrm{if~the~}}i{\\mathrm{-th~node~of~}}G{\\mathrm{~is~mapped~to~the~}}k{\\mathrm{-th~super-node~of~}}G_{c}}\\\\ {Q_{k i}=0}&{{\\mathrm{otherwise}}}\\end{array}\\right.}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The lifting matrix is the pseudo-inverse of the coarsening matrix $Q^{+}$ , and plays the role of inverse mapping from the coarsened graph to the original one. The coarsening ratio is defined as $r:$ $\\begin{array}{r}{r=1-\\frac{n}{N}}\\end{array}$ . That is, the higher $r$ is, the more coarsened the graph is. ", "page_idx": 2}, {"type": "text", "text": "A coarsening is said to be well-mapped if nodes in $G$ are mapped to a unique node in $G_{c}$ , that is, if $Q$ has exactly one non-zero value per column. Moreover, it is surjective if at least one node is mapped to each super node: $\\sum_{i}Q_{k i}>0$ for all $k$ . In this case, $Q\\bar{Q}^{\\top}$ is invertible diagonal and $Q^{+}=Q^{\\top}(Q Q^{\\top})^{-1}$ . Moreover, such a coarsening is said to be uniform when mapping weights are constant for each super-nodes and sum to one: $Q_{k i}=1/n_{k}$ for all $Q_{k i}>0$ , where $n_{k}$ is the number of nodes mapped to the super-node $k$ . In this case the lifting matrix is particularly simple: $Q^{+}\\in\\{0,1\\}^{N\\times n}$ (Fig. 1d). For simplicity, following the majority of the literature [32], in this paper we consider only well-mapped surjective coarsenings (but not necessarily uniform). ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Remark 1. Non-well-mapped coarsenings may appear in the literature, e.g. when learning the matrix $Q$ via a gradient-based optimization algorithm such as Diffpool [44]. However, these methods often include regularization penalties to favor well-mapped coarsenings. ", "page_idx": 3}, {"type": "text", "text": "Restricted Spectral Approximation. A large part of the graph coarsening literature measures the quality of a coarsening by quantifying the modification of the spectral properties of the graph, often represented by the graph Laplacian $L$ . In [32], this is done by establishing a near-isometry property for graph signals with respect to the norm $\\Vert\\cdot\\Vert_{L}$ , which can be interpreted as a measure of the smoothness of a signal across the graph edges. Given a signal $x\\in\\mathbb{R}^{N}$ over the nodes of $G$ , we define the coarsened signal $x_{c}\\in\\mathbb{R}^{n}$ and the re-lifted signal $\\tilde{x}\\in\\mathbb{R}^{N}$ by ", "page_idx": 3}, {"type": "equation", "text": "$$\nx_{c}=Q x,\\qquad\\tilde{x}=Q^{+}x_{c}=\\Pi x\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\Pi=Q^{+}Q$ . Loukas [32] then introduces the notion of Restricted Spectral Approximation (RSA) of a coarsening algorithm, which measures how much the projection $\\Pi$ is close to the identity for a class of signals. Since $\\Pi$ is at most of rank $n<N$ , this cannot be true for all signals, but only for a restricted subspace $\\mathcal{R}\\subset\\mathbb{R}^{N}$ . With this in mind, the RSA constant is defined as follows. ", "page_idx": 3}, {"type": "text", "text": "Definition 1 (Restricted Spectral Approximation constant). Consider a subspace $\\mathcal{R}\\,\\subset\\,\\mathbb{R}^{N}$ , $a$ Laplacian $L$ , a coarsening matrix $Q$ and its corresponding projection operator $\\Pi=Q^{+}Q$ . The RSA constant $\\epsilon_{L,Q,\\mathcal{R}}$ is defined as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\epsilon_{L,Q,\\mathcal{R}}=\\operatorname*{sup}_{\\substack{x\\in\\mathcal{R},\\|x\\|_{L}=1}}\\|x-\\Pi x\\|_{L}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "In other words, the RSA constant measures how much signals in $\\mathcal{R}$ are preserved by the coarseninglifting operation, with respect to the norm $\\Vert\\cdot\\Vert_{L}$ . Given some $\\mathcal{R}$ and Laplacian $L$ , the goal of a coarsening algorithm is then to produce a coarsening $Q$ with the smallest RSA constant possible. While the \u201cbest\u201d coarsening arg minQ $\\epsilon_{L,Q,\\mathcal{R}}$ is generally computationally unreachable, there are many possible heuristic algorithms, often based on greedy merging of nodes. In App. B.1, we give an example of such an algorithm, adapted from [32]. In practice, $\\mathcal{R}$ is often chosen as the subspace spanned by the first eigenvectors of $L$ ordered by increasing eigenvalue: intuitively, coarsening the graph and merging nodes is more likely to preserve the low-frequencies of the Laplacian. ", "page_idx": 3}, {"type": "text", "text": "While $\\epsilon_{L,Q,\\mathcal{R}}\\ll1$ is required to obtain meaningful guarantees, we remark that $\\epsilon_{L,Q,\\mathcal{R}}$ is not necessarily finite. Indeed, as $\\Vert\\cdot\\Vert_{L}$ may only be a semi-norm, its unit ball is not necessarily compact. It is nevertheless finite in the following case. ", "page_idx": 3}, {"type": "text", "text": "Proposition 1. When \u03a0 is ker $(L)$ -preserving, it holds that $\\epsilon_{L,Q,\\mathcal{R}}\\leq\\sqrt{\\lambda_{\\operatorname*{max}}/\\lambda_{\\operatorname*{min}}}.$ . ", "page_idx": 3}, {"type": "text", "text": "Hence, some examples where $\\epsilon_{L,Q,\\mathcal{R}}$ is finite include: ", "page_idx": 3}, {"type": "text", "text": "Example 1. For uniform coarsenings with $L\\,=\\,D\\,-\\,A$ and connected graph $G,\\;k e r(L)$ is the constant vector2, and $\\Pi$ is $\\ker(L)$ -preserving. This is the case examined by $[32]$ . ", "page_idx": 3}, {"type": "text", "text": "Example 2. For positive definite \u201cLaplacians\u201d, $\\ker(L)=\\{0\\}$ . This is a deceptively simple solution for which $\\Vert\\cdot\\Vert_{L}$ is a true norm. This can be obtained e.g. with ${\\cal L}=\\delta{\\cal I}_{N}+\\hat{\\cal L}$ for any p.s.d. Laplacian $\\hat{L}$ and small constant $\\delta>0$ . This leaves its eigenvectors unchanged and add $\\delta$ to its eigenvalues, and therefore does not alter the fundamental structure of the coarsening problem. ", "page_idx": 3}, {"type": "text", "text": "Given the adjacency matrix $A\\in\\mathbb{R}^{N\\times N}$ of $G$ , there are several possibilities to define an adjacency matrix $A_{c}$ for the graph $G_{c}$ [21, 29]. A natural choice that we make in this paper is ", "page_idx": 3}, {"type": "equation", "text": "$$\nA_{c}=(Q^{+})^{\\top}A Q^{+}\\,.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "In the case of a uniform coarsening, $(A_{c})_{k\\ell}$ equals the sum of edge weights for all edges in the original graph between all nodes mapped to the super-node $k$ and all nodes mapped to $\\ell$ . Moreover, we have the following property, derived from [32]. ", "page_idx": 3}, {"type": "image", "img_path": "rIOTceoNc8/tmp/1aa7404a5286f992037bf4cddce13a69a94ccb335c4515ead864f843c880e914.jpg", "img_caption": [], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Figure 1: Example of uniform coarsening. (a): original graph $G$ ; (b): coarsened adjacency matrix $A_{c}$ ; (c) representation of the proposed $S_{c}^{\\mathrm{MP}}$ when $S=A$ ; (d): corresponding matrices $Q,\\dot{Q}^{+},\\Pi$ . ", "page_idx": 4}, {"type": "text", "text": "Proposition 2. Assume that the coarsening $Q$ is uniform and consider combinatorial Laplacians $\\bar{L}=\\bar{D}(A)-A$ and $L_{c}=D(A_{c})-A_{c}$ . Then $L_{c}=\\stackrel{\\cdot}{(}Q^{+})^{\\top}L Q^{+}$ , and ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\forall x\\in\\mathcal{R},\\quad\\left(1-\\epsilon_{L,Q,\\mathcal{R}}\\right)\\|x\\|_{L}\\leq\\|x_{c}\\|_{L_{c}}\\leq(1+\\epsilon_{L,Q,\\mathcal{R}})\\|x\\|_{L}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "This draws a link between the RSA and a notion of near-isometric embedding for vectors in $\\mathcal{R}$ . Note that the proposition above is not necessarily true when considering normalized Laplacians, or non uniform coarsenings. In the next section, we propose a new propagation matrix on coarsened graphs and draw a link between the RSA constant $\\epsilon_{L,Q,\\mathcal{R}}$ and message-passing guarantees. ", "page_idx": 4}, {"type": "text", "text": "3 Message-Passing on coarsened graphs ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In the previous section, we have seen that coarsenings algorithms generally aim at preserving the spectral properties of the graph Laplacian, leading to small RSA constants $\\epsilon_{L,Q,\\mathcal{R}}$ . However, this generally does not directly translate to guarantees on the Message-Passing process that is at the core of GNNs, which as mentioned in the introduction is materialized by the matrix $S$ . In this section, we propose a new propagation matrix such that small RSA constants leads to preserved message-passing, which then leads to guarantees for training GNNs on coarsened graphs. ", "page_idx": 4}, {"type": "text", "text": "3.1 A new propagation matrix on coarsened graph ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Given a graph $G$ with a propagation matrix $S$ and a coarsened graph $G_{c}$ with a coarsening matrix $Q$ , our goal is to define a propagation matrix $S_{c}^{\\mathrm{MP}}\\in\\mathbb{R}^{n\\times n}$ such that one round of message-passing on the coarsened signal $x_{c}$ followed by lifting is close to performing message-passing in the original graph: $Q^{+}S_{c}^{\\mathrm{MP}}x_{c}^{-}\\approx S x$ . Assuming that the propagation matrix $S=f_{S}(A)$ is the output of a function $f_{S}$ of the graph\u2019s adjacency matrix, the most natural choice, often adopted in the literature [11], is therefore to simply take $S_{c}=f_{S}(A_{c})$ , where $A_{c}$ is the adjacency matrix of the coarsened graph defined in (4). However, this does not generally leads to the desired guarantees: indeed, considering for instance $S\\,=\\,A$ , we have in this case $Q^{\\mp}S_{c}x_{c}\\,=\\,Q^{+}(Q^{+})^{\\top}\\breve{A}\\Pi x$ , which involves the quite unnatural term $Q^{+}(Q^{+})^{\\top}$ . For other choices of normalized $S$ , the situation is even less clear. Some authors propose different variant of $S_{c}$ adapted to specific cases [21, 44] (see Sec. 4), but none offers generic message-passing guarantees. To address this, we propose a new propagation matrix: ", "page_idx": 4}, {"type": "equation", "text": "$$\nS_{c}^{\\mathrm{MP}}=Q S Q^{+}\\in\\mathbb{R}^{n\\times n}\\,.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "This expression is conceptually simple: it often amounts to some reweighting. For instance, when $S=A$ and in the case of uniform coarsening, we have $(S_{c}^{\\mathrm{MP}})_{k\\ell}=(A_{c}^{\\bar{}})_{k\\ell}/\\bar{n}_{k}$ (Fig. 1c). Despite this simplicity, we will see that under some mild hypotheses this choice indeed leads to preservation guarantees of message-passing for coarsenings with small RSA constants. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Orientation. An important remark is that, unlike all the examples in the literature, and unlike the adjacency matrix $A_{c}$ defined in (4), the proposed matrix $S_{c}^{\\mathrm{MP}}$ is generally asymmetric, even when $S$ is symmetric. This means that our guarantees are obtained by performing directed messagepassing on the coarsened graph, even when the original message-passing procedure was undirected. Conceptually, this is an important departure from previous works. However $S_{c}^{\\mathrm{MP}}$ becomes \u201cmore\u201d symmetric when $Q^{+}$ and $\\bar{Q^{T}}$ becomes more similar. This is for instance the case when $Q$ induces a balanced partition, where each supernodes has the same number of ancestors (which can be targeted by some pooling algorithms). On the contrary, the difference between $Q$ and $Q^{+}$ is more pronounced when supernodes are of very different sizes,which may happen for highly irregular graphs. ", "page_idx": 5}, {"type": "text", "text": "3.2 Message-Passing guarantees ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we show how the proposed propagation matrix (6) allows to transfer the spectral approximation guarantees to message-passing guarantees, under some hypotheses. First, we must make some technical assumptions relating to the kernel of the Laplacian. ", "page_idx": 5}, {"type": "text", "text": "Assumption 1. Assume that \u03a0 and $S$ are both ker $(L)$ -preserving. ", "page_idx": 5}, {"type": "text", "text": "Moreover, since spectral approximation pertains to a subspace $\\mathcal{R}$ , we must assume that this subspace is left unchanged by the application of $S$ . ", "page_idx": 5}, {"type": "text", "text": "Assumption 2. Assume that $S$ is $\\mathcal{R}$ -preserving. ", "page_idx": 5}, {"type": "text", "text": "As mentioned before, for Examples 1 and 2, the projection $\\Pi$ is $\\ker(L)$ -preserving. Moreover, $\\mathcal{R}$ is often chosen to be the subspace spanned by the low-frequency eigenvectors of $L$ and in this case, all matrices of the form $S\\,=\\,\\alpha I_{N}\\,+\\,\\beta L$ for some constant $\\alpha,\\beta$ are both $\\ker(L)$ -preserving and $\\mathcal{R}$ -preserving. Hence, for instance, a primary example in practice is to choose GCNconv [27] with $S=D(\\hat{A})^{-\\frac{1}{2}}\\hat{A}D(\\hat{A})^{-\\frac{1}{2}}$ with $\\hat{A}=A+I_{N}$ , and to compute a coarsening with a good RSA constant for the \u201cLaplacian\u201d $L=(1+\\delta)I_{N}-S$ with small $\\delta>0$ and $\\mathcal{R}$ spanned by eigenvectors of $L$ . In this case, Assumptions 1 and 2 are satisfied. This is the implementation we choose in our experiments. ", "page_idx": 5}, {"type": "text", "text": "We now state the main result of this section. ", "page_idx": 5}, {"type": "text", "text": "Theorem 1. Define $S_{c}^{\\mathrm{MP}}\\,a s$ (6). Under Assumption $^{\\,l}$ and 2, for all $x\\in\\mathcal{R}$ , ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|S x-Q^{+}S_{c}^{\\mathrm{MP}}x_{c}\\|_{L}\\leq\\epsilon_{L,Q,\\mathcal{R}}\\|x\\|_{L}\\left(C_{S}+C_{\\Pi}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $C_{S}:=\\|S\\|_{L}$ and $C_{\\Pi}:=\\|\\Pi S\\|_{L}$ . ", "page_idx": 5}, {"type": "text", "text": "Sketch of proof. The Theorem is proved in App. A. The proof is quite direct, and relies on the fact that, for this well-designed choice (6) of $S_{c}^{\\mathrm{MP}}$ , the lifted signal is precisely $Q^{+}S_{c}^{\\mathrm{MP}}x_{c}=\\Pi S\\Pi x$ . Then, bounding the error incurred by $\\Pi$ using the RSA, we show that this is indeed close to performing message-passing by $S$ in the original graph. \u53e3 ", "page_idx": 5}, {"type": "text", "text": "This theorem shows that the RSA error $\\epsilon_{L,Q,\\mathcal{R}}$ directly translates to an error bound between $S x$ and ${Q}^{+}{S}_{c}^{\\mathrm{MP}}{x}_{c}$ . As we will see in the next section, this leads to guarantees when training a GNN on the original graph and the coarsened graph. First, we discuss the two main multiplicative constant involved in Thm. 1. ", "page_idx": 5}, {"type": "text", "text": "Multiplicative constants. In full generality, for any matrix $M$ we have $\\begin{array}{r l}{\\|M\\|_{L}}&{{}\\leq}\\end{array}$ $\\|M\\|\\sqrt{\\lambda_{\\operatorname*{max}}/\\lambda_{\\operatorname*{min}}}$ . Moreover, when $M$ and $L$ commute, we have $\\|M\\|_{L}\\leq\\|M\\|$ . As mentioned before, choosing $S=\\alpha I_{N}+\\beta L$ for some constants $\\alpha,\\beta$ is a primary example to satisfy our assumptions. In this case $C_{S}=\\|S\\|_{L}\\leq\\|S\\|$ . Then, if $S$ is properly normalized, e.g. for the GCNconv [27] example outlined above, we have $\\|S\\|\\leq1$ . For combinatorial Laplacian $L=D-A$ however, we obtain $\\lVert S\\rVert\\leq|\\alpha|+|\\beta|N$ . We observed in our experiments that the combinatorial Laplacian generally yields poor results for GNNs. ", "page_idx": 5}, {"type": "text", "text": "For C\u03a0, in full generality we only have C\u03a0 \u2264CS\u2225\u03a0\u2225L \u2264CS \u03bb\u03bbmmianx , since $\\Pi$ is an orthogonal projector. However, in practice we generally observe that the exact value $C_{\\Pi}=\\|\\Pi S\\|_{L}$ is far better than this ratio of eigenvalues (e.g. we observe a ratio of roughly $C_{\\Pi}\\approx(1/10)\\cdot\\sqrt{\\lambda_{\\mathrm{max}}/\\lambda_{\\mathrm{min}}}$ in our experiments). Future work may examine more precise bounds in different contexts. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "3.3 GNN training on coarsened graph ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we instantiate our message-passing guarantees to GNN training on coarsened graph, with SGC as a primary example. To fix ideas, we consider a single large graph $G$ , and a node-level task such as node classification or regression. Given some node features $\\dot{X}\\in\\mathbb{R}^{N\\times d}$ , the goal is to minimize a loss function $J:\\mathbb{R}^{N}\\xrightarrow{\\bullet}\\mathbb{R}_{+}$ on the output of a GNN $\\Phi_{\\theta}(X,S)\\,\\in\\,\\mathbb{R}^{N}$ (assumed unidimensional for simplicity) with respect to the parameter $\\theta$ : ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\theta\\in\\Theta}R(\\theta){\\mathrm{~with~}}R(\\theta):=J(\\Phi_{\\theta}(X,S))\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\Theta$ is a set of parameters that we assume bounded. For instance, $J$ can be the cross-entropy between the output of the GNN and some labels on training nodes for classification, or the Mean Square Error for regression. The loss is generally minimized by first-order optimization methods on $\\theta$ , which requires multiple calls to the GNN on the graph $G$ . Roughly, the computational complexity of this approach is $O(\\bar{T}(N+E)D)$ , where $T$ is the number of iterations of the optimization algorithm, $D$ is the number of parameters in the GNN, and $E$ is the number of nonzero elements in $S$ . Instead, one may want to train on the coarsened graph $G_{c}$ , which can be done by minimizing instead3: ", "page_idx": 6}, {"type": "equation", "text": "$$\nR_{c}(\\theta):=J(Q^{+}\\Phi_{\\theta}(X_{c},S_{c}^{\\mathrm{MP}}))\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $X_{c}=Q X$ . That is, the GNN is applied on the coarsened graph, and the output is then lifted to compute the loss, which is then back-propagated to compute the gradient of $\\theta$ . The computational complexity then becomes $O(T(n+e)D+T N)$ , where $e\\leq E$ is the number of nonzeros in $S_{c}^{\\mathrm{MP}}$ , and the term $T N$ is due to the lifting. As this decorrelates $N$ and $D$ , it is in general much less costly. ", "page_idx": 6}, {"type": "text", "text": "We make the following two assumptions to state our result. Since our bounds are expressed in terms of $\\Vert\\cdot\\Vert_{L}$ , we must handle it with the following assumption. ", "page_idx": 6}, {"type": "text", "text": "Assumption 3. We assume that there is a constant $C_{J}$ such that ", "page_idx": 6}, {"type": "equation", "text": "$$\n|J(x)-J(x^{\\prime})|\\leq C_{J}\\|x-x^{\\prime}\\|_{L}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "For most loss functions, it is easy to show that $|J(x)-J(x^{\\prime})|\\lesssim\\|x-x^{\\prime}\\|$ , and when $L$ is positive definite (Example 2) then \u2225\u00b7 \u2225\u2264\u221a\u03bb1min . Otherwise, one must handle the kernel of $L$ , which may be done on a case-by-case basis of for an appropriate choice of $J$ . ", "page_idx": 6}, {"type": "text", "text": "The second assumption relates to the activation function. It is here mostly for technical completeness, as we do not have examples where it is satisfied beyond the identity $\\sigma=i d$ , which corresponds to the SGC architecture [42] often used in theoretical analyses [46, 26]. ", "page_idx": 6}, {"type": "text", "text": "Assumption 4. We assume that: ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "$i$ ) $\\sigma$ is $\\mathcal{R}$ -preserving, that is, for all $x\\in\\mathcal{R}$ , we have $\\sigma(x)\\in{\\mathcal{R}}$ . We discuss this constraint below.   \nii) $\\|\\sigma(x)-\\sigma(x^{\\prime})\\|_{L}\\leq C_{\\sigma}\\|x-x^{\\prime}\\|_{L}$ . Note that most activations are 1-Lipschitz w.r.t. the Euclidean norm, so this is satisfied when $L$ is positive-definite with $C_{\\sigma}=\\sqrt{\\lambda_{\\operatorname*{max}}/\\lambda_{\\operatorname*{min}}}$ .   \niii) $\\sigma$ and $Q^{+}$ commute: $\\sigma(Q^{+}y)=Q^{+}\\sigma(y)$ . This is satisfied for all uniform coarsenings, or when $\\sigma$ is 1-positively homogeneous: $\\sigma(\\lambda x)=\\lambda\\sigma(x)$ for nonnegative $\\lambda$ (e.g. ReLU). ", "page_idx": 6}, {"type": "text", "text": "Item i) above means that, when $\\mathcal{R}$ is spanned by low-frequency eigenvectors of the Laplacian, $\\sigma$ does not induce high frequencies. In other words, we want $\\sigma$ to preserve smooth signal. For now, the only example for which we can guarantee that Assumption 4 is satisfied is when $\\sigma=i d$ and the GNN is linear, which corresponds to the SGC architecture [42]. As is the case with many such analyses of GNNs, non-linear activations are a major path for future work. A possible study would be to consider random geometric graphs for which the eigenvectors of the Laplacian are close to explicit functions, e.g. spherical harmonics for dot-product graphs [2]. In this case, it may be possible to explicitely prove that Assumption 4 holds, but this is out-of-scope of this paper. ", "page_idx": 6}, {"type": "text", "text": "Our result on GNNs is the following. ", "page_idx": 6}, {"type": "text", "text": "Theorem 2. Under Assumptions 1-4: for all node features $X\\in\\mathbb{R}^{N\\times d}$ such that $X_{:,i}\\in\\mathcal{R}$ , denoting b $\\prime\\,\\theta^{\\star}=\\arg\\operatorname*{min}_{\\theta\\in\\Theta}R(\\theta)$ and $\\theta_{c}=\\arg\\operatorname*{min}_{\\theta\\in\\Theta}R_{c}(\\theta)$ , we have ", "page_idx": 7}, {"type": "equation", "text": "$$\nR(\\theta_{c})-R(\\theta^{\\star})\\leq C\\epsilon_{L,Q,\\mathcal{R}}\\|X\\|_{:,L}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "with C = 2CJC\u03c3kC\u0398(CS + C\u03a0) lk=1 C\u00afk\u03a0\u2212l where $\\bar{C}_{\\Pi}:=\\|\\Pi S\\Pi\\|_{L}$ and $C_{\\Theta}$ is a constant that depends on the parameter set $\\Theta$ . ", "page_idx": 7}, {"type": "text", "text": "The proof of Thm. 2 is given in App. A.3. In this proof, to apply the Theorem 1, we apply the RSA to each nodes features column. It relies on the assumption that each column of the nodes features $X_{:,i}$ belongs to the preserved space $\\mathcal{R}$ . This assumption seems reasonable for homophilic datasets (Cora, Citeseer) and large preserved space . The Theorem states that training a GNN that uses the proposed $S_{c}^{\\mathrm{MP}}$ on the coarsened graph by minimizing (9) yields a parameter $\\theta_{c}$ whose excess loss compared to the optimal $\\theta^{\\star}$ is bounded by the RSA constant. Hence, spectral approximation properties of the coarsening directly translates to guarantees on GNN training. The multiplicative constants $C_{S},C_{\\Pi}$ have been discussed in the previous section, and the same remarks apply to $\\bar{C}_{\\Pi}$ . ", "page_idx": 7}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Setup. We choose the propagation matrix from GCNconv [27], that is, $\\begin{array}{r c l}{S}&{=}&{f_{S}(A)}&{=}\\end{array}$ $D(\\hat{A})^{-\\frac{1}{2}}\\hat{A}D(\\hat{A})^{-\\frac{1}{2}}$ with $\\hat{A}=A\\!+\\!I_{N}$ . As detailed in the previous section, we take $L=(1{+}\\delta)I_{N}{-}S$ with $\\delta=0.001$ and $\\mathcal{R}$ as the $K$ first eigenvectors of $L$ $\\dot{K}=N/10$ in our experiments), ensuring that Assumptions 1 and 2 are satisfied. In our experiments, we observed that the combinatorial Laplacian $L=D-A$ gives quite poor results, as it corresponds to unusual propagation matrices $S=\\alpha I_{N}+\\beta L$ , and the constant $C_{S}=\\|S\\|_{L}$ is very large. Hence our focus on the normalized case. ", "page_idx": 7}, {"type": "text", "text": "On coarsened graphs, we compare five propagation matrices: ", "page_idx": 7}, {"type": "text", "text": "\u2022 $S_{c}^{\\mathrm{MP}}=Q S Q^{+}$ , our proposed matrix   \n\u2022 $S_{c}=f_{S}(A_{c})$ , the naive choice   \n\u2022 $S_{c}^{d i a g}=\\hat{D}^{\\prime}{}^{-1/2}(A_{c}+C)\\hat{D}^{\\prime}{}^{-1/2}$ , proposed in [21], where $C$ is the diagonal matrix of the $n_{k}$ and $\\hat{D^{\\prime}}$ the corresponding degrees. This yields theoretical guarantees for APPNP when $S$ is GCNconv; \u2022 $S_{c}^{d i f f}=Q S Q^{\\top}$ , which is roughly inspired by Diffpool [44];   \n\u2022 $S_{c}^{s y m}=(Q^{+})^{\\top}S Q^{+}$ , which is the lifting employed to compute $A_{c}$ (4). ", "page_idx": 7}, {"type": "text", "text": "Coarsening algorithm. Recall that the proposed $S_{c}^{\\mathrm{MP}}$ can be computed for any coarsening, and that the corresponding theoretical guarantees depend on the RSA constant $\\epsilon_{L,Q,\\mathcal{R}}$ . In our experiments, we adapt the algorithm from [32] to coarsen the graphs. It takes as input the graph $G$ and the coarsening ratio desired $r$ and output the propagation matrix $S_{c}^{\\mathrm{MP}}$ and the coarsening matrix $Q$ used for lifting. It is a greedy algorithm which successively merges edges by minimizing a certain cost. While originally designed for the combinatorial Laplacian, we simply adapt the cost to any Laplacian $L$ , see App. B.1. Note however that some mathematical justifications for this approach in [32] are no longer valid for normalized Laplacian, but we find in practice that it produces good RSA constants. ", "page_idx": 7}, {"type": "image", "img_path": "rIOTceoNc8/tmp/8b1417a1e603fa1b64a916f13c7687e362a08cb4fc9e98fffb7d5132d81afbfd.jpg", "img_caption": ["Figure 2: Message-Passing error for different propagation matrices. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "A major limit of this algorithm is its computational cost, which is quite high since it involves large matrix inversion and SVD computation. Hence we limit ourselves to middle-scale graphs like Cora [35] and Citeseer [15] and one larger graph with Reddit [18] in the following experiments. The design of more scalable coarsening algorithms with RSA guarantees is an important path for future work, but out-of-scope of this paper. ", "page_idx": 7}, {"type": "text", "text": "Message passing preservation guarantees To evaluate the effectiveness of the proposed propagation matrix, we first illustrate the theoretical message passing preservation guarantees (Thm. 1 and 2) on synthetic graphs, taken as random geometric graph, built by sampling 1000 nodes with coordinates in $[0,1]^{\\bar{2}}$ and connecting them if their distance is under a given threshold (details in App. B.3). For each choice of propagation matrix and different coarsening ratio, we compute numerically $\\|S^{k}x-Q^{+}(S_{c}^{\\mathrm{MP}})^{k}x_{c}\\|_{L}^{}$ for various signals $x\\in\\mathcal{R}$ . We perform $N_{p}=6$ message-passing steps to enhance the difference between propagation matrices. We evaluate and plot the upper bound defined by \u03f5L,Q,R(CS +C\u03a0) lk=1 C\u00afk\u03a0\u2212l (seen in the proof of Theorem 2 in App. A.3) in Fig. 2. We observe that our propagat ion matrix incurs a significantly lower error compared to other choices, and that as expected, this error is correlated to $\\epsilon_{L,Q,\\mathcal{R}}$ , which is not the case for other choices. More experiments can be found in App. B.4. ", "page_idx": 8}, {"type": "text", "text": "Node classification on real graphs. We then perform node classification experiments on real-world graphs, namely Cora [35] and Citeseer [15], using the public split from [43]. For simplicity, we restrict them to their largest connected component4, since using a connected graph is far more convenient for coarsening algorithms (details in App. B.3). The training procedure follows that of Sec. 3.3: the network is applied to the coarsened graph and coarsened node features, its output is lifted to the original graph with $\\bar{Q}^{+}$ , and the label of the original training graph nodes are used to compute the cross-entropy loss, which is then back-propagated to optimize the parameters $\\theta$ (pseudocode in App. B.2). Despite the lifting procedure, this results in faster training than using the entire graph (e.g., by approximately $30\\%$ for a coarsening ratio of $r\\,=\\,0.5$ when parallelized on GPU). For downstream tasks we introduce a novel metric to analyze a specific coarsening : \"Max acc possible\". It corresponds to the optimal prediction over the super-nodes of the coarsened graph (all the nodes coarsened in a super nodes has the same prediction, optimally the majority label of this cluster). It might be hard to achieve as the optimal assignment for the validation nodes or training nodes can be different. It allows comparing different coarsenings for classification task without training models on it. We test SGC [42] with $N_{p}=6$ and GCNconv [27] with $N_{p}=2$ on four different coarsening ratio: $r\\in\\{0.3,\\,0.5,\\,0.7\\}$ where $N_{p}$ is the number of propagation. Each classification results is averaged on 10 random training. ", "page_idx": 8}, {"type": "text", "text": "Results are reported in Table 1 and Table 2. We observe that the proposed propagation matrix $S_{c}^{\\mathrm{MP}}$ yields better results and is more stable, especially for high coarsening ratio. The benefits are more evident when applied to the SGC architecture [42], for which Assumption 4 of Thm. 2 is actually satisfied, than for GCN, for which ReLU is unlikely to satisfy Assumption 4. It is also interesting to notice that training on coarsened graphs sometimes achieve better results than on the original graph. This may be explained by the fact that, for homophilic graphs (connected nodes are more likely to have the same label), nodes with similar labels are more likely to be merged together during the coarsening, and thus become easier to predict for the model. The detailed hyper-parameters for each model and each dataset can be found in appendix B.5. ", "page_idx": 8}, {"type": "table", "img_path": "rIOTceoNc8/tmp/11343ee8b17c1afb1db5fd74aa5642c2a49304ed945d19a40325e07ac2eb30a7.jpg", "table_caption": ["Table 1: Accuracy in $\\%$ for node classification with SGC and different coarsening ratio "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Scaling to larger Datasets We performed experiments on the Reddit Dataset [18], which is approximately 100 times bigger than Cora or Citeseer. The Message-Passing error for different coarsening propagation matrices is reported in Table 3 with the node prediction results on two coarsening ratio $r=90\\%$ and $r=99\\bar{\\%}$ (their number of nodes,and edges can be found in App B.3), the details of the hyperparameters and coarsening procedure are in B.6. Our propagation matrix for coarsened graphs achieved a better Message-Passing error, close to the RSA-constant computed in the coarsened graph. It is consistent with the fact the Message-Passing error is bounded by Theorem 1 with our propagation matrix. Similarly, for the node prediction results, our propagation matrix $S_{c}^{\\mathrm{MP}}$ achieves good results with the SGC model, close to the maximum accuracy possible on the given coarsening. Our propagation matrix is still competitive with the GCNconv model and achieved better results on the biggest coarsening ratio. These experiments show the effectiveness of our method on large graphs for which coarsening as a preprocessing step is crucial: indeed, on most small-scale machines with single GPU, the Reddit dataset is too large to fit in memory and requires adapted strategies. ", "page_idx": 8}, {"type": "table", "img_path": "rIOTceoNc8/tmp/1e16ed6e39c57fcd8bf8b4620ccf9460534ef273331353d8ae2adcf36b9e0c3a.jpg", "table_caption": ["Table 2: Accuracy in $\\%$ for node classification with GCNconv and different coarsening ratio "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "table", "img_path": "rIOTceoNc8/tmp/9d0002fd4cf975c25fd0871d536be39d6a478ac14dd902173459e6a72343e08e.jpg", "table_caption": ["Table 3: Accuracy in $\\%$ for node classification on Reddit Dataset and Message passing errors "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we investigated the interactions between graph coarsening and Message-Passing for GNNs. Surprisingly, we found out that even for high-quality coarsenings with strong spectral preservation guarantees, naive (but natural) choices for the propagation matrix on coarsened graphs does not lead to guarantees with respect to message-passing on the original graph. We thus proposed a new message-passing matrix specific to coarsened graphs, which naturally translates spectral preservation to message-passing guarantees, for any coarsening, under some hypotheses relating to the structure of the Laplacian and the original propagation matrix. We then showed that such guarantees extend to GNN, and in particular to the SGC model, such that training on the coarsened graph is provably close to training on the original one. ", "page_idx": 9}, {"type": "text", "text": "There are many outlooks to this work. Concerning the coarsening procedure itself, which was not the focus of this paper, new coarsening algorithms could emerge from our theory, e.g. by instantiating an optimization problem with diverse regularization terms stemming from our theoretical bounds. The scalability of such coarsening algorithms is also an important topic for future work. From a theoretical point of view, a crucial point is the interaction between non-linear activation functions and the low-frequency vectors in a graph (Assumption 4). We focused on the SGC model here, but a more in-depth study of particular graph models (e.g. random geometric graphs) could shed light on this complex phenomenon, which we believe to be a major path for future work. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "The authors acknowledge the fundings of France 2030, PEPR IA, ANR-23-PEIA-0008 and ANR GrandMa ANR-21-CE23-0006. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Z. Allen-Zhu, Z. Liao, and L. Orecchia. Spectral sparsification and regret minimization beyond matrix multiplicative updates. In Proceedings of the forty-seventh annual ACM symposium on Theory of computing, pages 237\u2013245, 2015.   \n[2] E. Araya and Y. de Castro. Latent distance estimation for random geometric graphs. Advances in Neural Information Processing Systems (NeurIPS), 32, 2019. ISSN 10495258. URL http://arxiv.org/abs/1909.06841.   \n[3] F. M. Bianchi and V. Lachi. The expressive power of pooling in graph neural networks. Advances in neural information processing systems, 36, 2024.   \n[4] G. Bravo Hermsdorff and L. Gunderson. A unifying framework for spectrum-preserving graph sparsification and coarsening. Advances in Neural Information Processing Systems, 32, 2019.   \n[5] M. M. Bronstein, J. Bruna, T. Cohen, and P. Veli\u02c7ckovi\u00b4c. Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges. arXiv:2104.13478, 2021. URL http://arxiv.org/ abs/2104.13478.   \n[6] C. Cai, D. Wang, and Y. Wang. Graph coarsening with neural networks. In 9th International conference on Learning Representations, 2021.   \n[7] J. Chen, Y. Saad, and Z. Zhang. Graph coarsening: from scientific computing to machine learning, volume 79. Springer International Publishing, 2022. ISBN 4032402100282. doi: 10.1007/s40324-021-00282-x. URL https://doi.org/10.1007/s40324-021-00282-x.   \n[8] Y. Chen, R. Yao, Y. Yang, and J. Chen. A gromov-wasserstein geometric view of spectrumpreserving graph coarsening. In International Conference on Machine Learning, pages 5257\u2013 5281. PMLR, 2023.   \n[9] M. Defferrard, X. Bresson, and P. Vandergheynst. Convolutional neural networks on graphs with fast localized spectral filtering. Advances in neural information processing systems, 29, 2016.   \n[10] I. S. Dhillon, Y. Guan, and B. Kulis. Weighted graph cuts without eigenvectors a multilevel approach. IEEE transactions on pattern analysis and machine intelligence, 29(11):1944\u20131957, 2007.   \n[11] C. Dickens, E. Huang, A. Reganti, J. Zhu, K. Subbian, and D. Koutra. Graph coarsening via convolution matching for scalable graph neural network training. In Companion Proceedings of the ACM on Web Conference 2024, pages 1502\u20131510, 2024.   \n[12] F. Dorfler and F. Bullo. Kron reduction of graphs with applications to electrical networks. IEEE Transactions on Circuits and Systems I: Regular Papers, 60(1):150\u2013163, 2012.   \n[13] D. Ediger, K. Jiang, J. Riedy, D. A. Bader, C. Corley, R. Farber, and W. N. Reynolds. Massive social network analysis: Mining twitter for social good. Proceedings of the International Conference on Parallel Processing, pages 583\u2013593, 2010. ISSN 01903918. doi: 10.1109/ICPP. 2010.66.   \n[14] J. Gasteiger, C. Qian, and S. G\u00fcnnemann. Influence-based mini-batching for graph neural networks. 12 2022. URL http://arxiv.org/abs/2212.09083.   \n[15] C. L. Giles, K. D. Bollacker, and S. Lawrence. Citeseer: An automatic citation indexing system, 1998. URL www.neci.nj.nec.com.   \n[16] J. Gilmer, S. S. Schoenholz, P. F. Riley, O. Vinyals, and G. E. Dahl. Neural Message Passing for Quantum Chemistry. In International Conference on Machine Learning (ICML), pages 1\u201314, 2017. ISBN 978-1-4577-0079-8. doi: 10.1002/nme.2457. URL http://arxiv.org/abs/ 1704.01212.   \n[17] D. Grattarola, D. Zambon, F. M. Bianchi, and C. Alippi. Understanding pooling in graph neural networks. IEEE transactions on neural networks and learning systems, 35(2):2708\u20132718, 2022.   \n[18] W. Hamilton, Z. Ying, and J. Leskovec. Inductive representation learning on large graphs. Advances in neural information processing systems, 30, 2017.   \n[19] P. Hu and W. C. Lau. A Survey and Taxonomy of Graph Sampling. pages 1\u201334, 2013. URL http://arxiv.org/abs/1308.5865.   \n[20] W. Hu, M. Fey, M. Zitnik, Y. Dong, H. Ren, B. Liu, M. Catasta, and J. Leskovec. Open Graph Benchmark: Datasets for Machine Learning on Graphs. Neural Information Processing Systems (NeurIPS), (NeurIPS):1\u201334, 2020. URL http://arxiv.org/abs/2005.00687.   \n[21] Z. Huang, S. Zhang, C. Xi, T. Liu, and M. Zhou. Scaling up Graph Neural Networks Via Graph Coarsening, volume 1. Association for Computing Machinery, 2021. ISBN 9781450383325. doi: 10.1145/3447548.3467256.   \n[22] W. Jin, L. Zhao, S. Zhang, Y. Liu, J. Tang, and N. Shah. Graph condensation for graph neural networks. In International Conference on Learning Representations, 2021.   \n[23] W. Jin, X. Tang, H. Jiang, Z. Li, D. Zhang, J. Tang, and B. Yin. Condensing graphs via one-step gradient matching. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 720\u2013730, 2022.   \n[24] Y. Jin, A. Loukas, and J. JaJa. Graph coarsening with preserved spectral properties. In International Conference on Artificial Intelligence and Statistics, pages 4452\u20134462. PMLR, 2020.   \n[25] G. Karypis and V. Kumar. A fast and high quality multilevel scheme for partitioning irregular graphs. SIAM Journal on scientific Computing, 20(1):359\u2013392, 1998.   \n[26] N. Keriven. Not too little, not too much: a theoretical analysis of graph (over)smoothing. Advances in Neural Information Processing Systems (NeurIPS), 2022. URL http://arxiv. org/abs/2205.12156.   \n[27] T. N. Kipf and M. Welling. Semi-supervised classification with graph convolutional networks. In International Conference on Learning Representations, 2016.   \n[28] J. Klicpera, A. Bojchevski, and S. G\u00fcnnemann. Predict then propagate: Graph neural networks meet personalized PageRank. 7th International Conference on Learning Representations, ICLR 2019, pages 1\u201315, 2019.   \n[29] M. Kumar, A. Sharma, S. Saxena, and S. Kumar. Featured graph coarsening with similarity guarantees. In International Conference on Machine Learning, pages 17953\u201317975. PMLR, 2023.   \n[30] J. Lee, I. Lee, and J. Kang. Self-attention graph pooling. In International conference on machine learning, pages 3734\u20133743. PMLR, 2019.   \n[31] Y. T. Lee and H. Sun. Constructing linear-sized spectral sparsification in almost-linear time. SIAM Journal on Computing, 47(6):2315\u20132336, 2018.   \n[32] A. Loukas. Graph reduction with spectral and cut guarantees. Journal of Machine Learning Research, 20(116):1\u201342, 2019.   \n[33] A. Loukas and P. Vandergheynst. Spectrally approximating large graphs with smaller graphs. In International conference on machine learning, pages 3237\u20133246. PMLR, 2018.   \n[34] T. Ma and J. Chen. Unsupervised learning of graph hierarchical abstractions with differentiable coarsening and optimal transport. In Proceedings of the AAAI conference on artificial intelligence, volume 35, pages 8856\u20138864, 2021.   \n[35] A. K. Mccallum, K. Nigam, J. Rennie, and K. Seymore. Automating the construction of internet portals with machine learning, 2000. URL www.campsearch.com.   \n[36] J. W. Ruge and K. St\u00fcben. Algebraic multigrid. In Multigrid methods, pages 73\u2013130. SIAM, 1987.   \n[37] F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and G. Monfardini. The graph neural network model. IEEE transactions on neural networks, 20(1):61\u201380, 2008.   \n[38] D. A. Spielman and S.-H. Teng. Spectral sparsification of graphs. SIAM Journal on Computing, 40(4):981\u20131025, 2011.   \n[39] A. Tsitsulin, J. Palowitch, B. Perozzi, and E. M\u00fcller. Graph clustering with graph neural networks. Journal of Machine Learning Research, 24(127):1\u201321, 2023.   \n[40] J. Wang, P. Huang, H. Zhao, Z. Zhang, B. Zhao, and D. L. Lee. Billion-scale commodity embedding for E-commerce recommendation in alibaba. Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 839\u2013848, 2018. doi: 10.1145/3219819.3219869.   \n[41] T. Wang, J.-Y. Zhu, A. Torralba, and A. A. Efros. Dataset distillation. arXiv preprint arXiv:1811.10959, 2018.   \n[42] F. Wu, A. Souza, T. Zhang, C. Fifty, T. Yu, and K. Weinberger. Simplifying graph convolutional networks. In International conference on machine learning, pages 6861\u20136871. PMLR, 2019.   \n[43] Z. Yang, W. Cohen, and R. Salakhudinov. Revisiting semi-supervised learning with graph embeddings. In International conference on machine learning, pages 40\u201348. PMLR, 2016.   \n[44] Z. Ying, J. You, C. Morris, X. Ren, W. Hamilton, and J. Leskovec. Hierarchical graph representation learning with differentiable pooling. Advances in neural information processing systems, 31, 2018.   \n[45] X. Zheng, M. Zhang, C. Chen, Q. V. H. Nguyen, X. Zhu, and S. Pan. Structure-free graph condensation: From large-scale graphs to condensed graph-free data. Advances in Neural Information Processing Systems, 36, 2024.   \n[46] J. Zhu, R. A. Rossi, A. Rao, T. Mai, N. Lipka, N. K. Ahmed, and D. Koutra. Graph Neural Networks with Heterophily. 35th AAAI Conference on Artificial Intelligence, AAAI 2021, 12B: 11168\u201311176, 2021. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Proofs ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We start by a small useful lemma. ", "page_idx": 13}, {"type": "text", "text": "Lemma 1. For all $k e r(L)$ -preserving matrices $M$ , we have $\\|M x\\|_{L}\\leq\\|M\\|_{L}\\|x\\|_{L}$ . ", "page_idx": 13}, {"type": "text", "text": "Proof. Take the orthogonal decomposition $x=u+v$ where $u\\in\\ker(L)$ and $v\\in\\ker(L)^{\\perp}$ . Then, since $\\|x\\|_{L}=\\|v\\|_{L},\\bar{L^{-\\frac{1}{2}}L^{\\frac{1}{2}}v}=v$ and $\\|M u\\|_{L}=0$ , ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\|M x\\|_{L}\\leq\\|M v\\|_{L}=\\|L^{\\frac{1}{2}}M L^{-\\frac{1}{2}}L^{\\frac{1}{2}}v\\|\\leq\\|M\\|_{L}\\|v\\|_{L}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "A.1 Proof of Proposition 2 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Proof. The fact that $L_{c}=(Q^{+})L Q^{+}$ in this particular case is done by direct computation. It results that $\\|x_{c}\\|_{L_{c}}=\\|L^{\\frac{1}{2}}\\Pi x\\|$ and ", "page_idx": 13}, {"type": "equation", "text": "$$\n|\\|x\\|_{L}-\\|x_{c}\\|_{L_{c}}\\,|\\leq\\|L^{\\frac{1}{2}}x-L^{\\frac{1}{2}}\\Pi x\\|=\\|x-\\Pi x\\|_{L}\\leq\\epsilon_{L,Q,\\mathcal{R}}\\|x\\|_{L}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "by definition of $\\epsilon_{L,Q,\\mathcal{R}}$ . ", "page_idx": 13}, {"type": "text", "text": "A.2 Proof of Theorem 1 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Proof. Since $x\\in\\mathcal{R}$ and $S$ is $\\mathcal{R}$ -preserving, we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\|\\Pi^{\\perp}x\\|_{L}\\leq\\epsilon_{L,Q,\\mathcal{R}}\\|x\\|_{L}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\Pi^{\\perp}\\,=\\,I_{N}\\,-\\,\\Pi$ , and similarly for $S x$ . Moreover, under Assumption 1, both $\\Pi$ and $S$ are $\\ker(L)$ -preserving, such that $\\|\\Pi S x\\|_{L}\\leq\\|\\Pi S\\|_{L}\\|x\\|_{L}$ for all $x$ . Then ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\|S x-Q^{+}S_{c}^{\\mathrm{MP}}x_{c}\\|_{L}=\\|S x-\\Pi S\\Pi x\\|_{L}}&{}\\\\ {=\\|S x-\\Pi S x+\\Pi S x-\\Pi S\\Pi x\\|_{L}}\\\\ {=\\|\\Pi^{\\perp}S x+\\Pi S\\Pi^{\\perp}x\\|_{L}}\\\\ {\\leq\\|\\Pi^{\\perp}S x\\|_{L}+\\|\\Pi S\\Pi^{\\perp}x\\|_{L}}\\\\ {\\leq\\epsilon_{L,Q,\\mathcal{R}}\\|S x\\|_{L}+\\|\\Pi S\\|_{L}\\|\\Pi^{\\perp}x\\|_{L}}\\\\ {\\leq\\epsilon_{L,Q,\\mathcal{R}}\\|S x\\|_{L}+\\epsilon_{L,Q,\\mathcal{R}}\\|\\Pi S\\|_{L}\\|x\\|_{L}=\\epsilon_{L,Q,\\mathcal{R}}\\|x\\|_{L}\\left(C_{\\cal S}+C_{\\Pi}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "A.3 Proof of Theorem 2 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Recall that the GNN is such that $H^{0}=X$ , and ", "page_idx": 13}, {"type": "equation", "text": "$$\nH^{l}=\\sigma(S H^{l-1}\\theta_{l})\\in\\mathbb{R}^{N\\times d_{\\ell}},\\quad\\Phi_{\\theta}(X,S)=H^{k}\\in\\mathbb{R}^{N}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Similarly, for the GNN on coarsened graph we denote by $H_{c}^{0}=X_{c}$ and its layers ", "page_idx": 13}, {"type": "equation", "text": "$$\nH_{c}^{l}=\\sigma(S_{c}^{\\mathrm{MP}}H_{c}^{l-1}\\theta_{l})\\in\\mathbb{R}^{n\\times d_{\\ell}},\\quad\\Phi_{\\theta}(X_{c},S_{c}^{\\mathrm{MP}})=H_{c}^{k}\\in\\mathbb{R}^{N}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "For some set of parameters $\\theta$ of a GNN, we define ", "page_idx": 13}, {"type": "equation", "text": "$$\nC_{\\theta,l}=\\operatorname*{sup}_{i}\\sum_{j}|\\theta_{i j}^{l}|,\\qquad\\bar{C}_{\\theta,l}=\\prod_{p=1}^{l}C_{\\theta,p}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "We start with a small lemma. ", "page_idx": 13}, {"type": "text", "text": "Lemma 2. Define ", "page_idx": 13}, {"type": "equation", "text": "$$\nB_{l}=B_{l}(\\boldsymbol{X}):=\\sum_{i}\\|H_{:,i}^{l}\\|_{L}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Then we have ", "page_idx": 13}, {"type": "equation", "text": "$$\nB_{l}\\leq\\bar{C}_{\\theta,l}C_{S}^{l}C_{\\sigma}^{l}\\|X\\|_{:,L}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Proof. From assumption 4 we have $\\|\\sigma(x)\\|_{L}\\leq C_{\\sigma}\\|x\\|_{L}$ . Then, since $S$ is $\\ker(L)$ -preserving from Assumption 1, by Lemma 1 ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{i}\\|H_{\\cdot,i}^{l}\\|_{L}=\\sum_{i}\\|\\sigma(S H^{l-1}\\theta_{:,i}^{l})\\|_{L}\\leq C_{\\sigma}\\sum_{i}\\|S H^{l-1}\\theta_{:,i}^{l}\\|_{L}}}\\\\ &{}&{\\leq C_{\\sigma}(\\operatorname*{sup}_{j}\\sum_{i}|\\theta_{j i}^{l}|)\\sum_{j}\\|S H_{\\cdot,j}^{l-1}\\|_{L}\\leq C_{\\sigma}C_{\\theta,l}C_{S}B_{l-1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Since $B_{0}=\\|X\\|_{:,L}$ , we obtain the result ", "page_idx": 14}, {"type": "text", "text": "Proof. We start with classical risk bounding in machine learning ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{J(\\Phi_{{\\theta_{c}}}(X,S))-J(\\Phi_{\\theta^{*}}(X,S))=J(\\Phi_{{\\theta_{c}}}(X,S))-J(Q^{+}\\Phi_{\\theta_{c}}(X_{c},S_{c}^{\\mathrm{MP}}))}\\\\ &{\\phantom{J(\\Phi_{{\\theta_{c}}}(X,S))=}+J(Q^{+}\\Phi_{{\\theta_{c}}}(X_{c},S_{c}^{\\mathrm{MP}}))-J(Q^{+}\\Phi_{\\theta^{*}}(X_{c},S_{c}^{\\mathrm{MP}}))}\\\\ &{\\phantom{J(\\Phi_{{\\theta_{c}}}(X,S))=}+J(Q^{+}\\Phi_{\\theta^{*}}(X_{c},S_{c}^{\\mathrm{MP}}))-J(\\Phi_{\\theta^{*}}(X,S))}\\\\ &{\\phantom{J(\\Phi_{{\\theta_{c}}}(X,S))=}\\leq2\\operatorname*{sup}_{\\theta\\in\\Theta}\\lvert J(\\Phi_{\\theta}(X,S))-J(Q^{+}\\Phi_{\\theta}(X_{c},S_{c}^{\\mathrm{MP}}))\\rvert}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "since $\\theta_{c}$ minimizes $J(Q^{+}\\Phi_{\\theta}(X_{c},S_{c}^{\\mathrm{MP}}))$ . For all $\\theta$ , by Assumption 3, we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n|J(\\Phi_{\\theta}(X,S))-J(Q^{+}\\Phi_{\\theta}(X_{c},S_{c}^{\\mathrm{MP}}))|\\le C_{J}\\|\\Phi_{\\theta}(X,S)-Q^{+}\\Phi_{\\theta}(X_{c},S_{c}^{\\mathrm{MP}})\\|_{L^{2}}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We will prove a recurrence bound on ", "page_idx": 14}, {"type": "equation", "text": "$$\nE_{l}:=\\sum_{i}\\lVert H_{:,i}^{l}-Q^{+}(H_{c}^{l})_{:,i}\\rVert_{L}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "From Assumption 4, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{E_{l}=\\displaystyle\\sum_{i}\\|\\sigma(S H^{l-1}(\\theta_{l})_{:,i})-Q^{+}\\sigma(S_{c}^{\\mathrm{MP}}H_{c}^{l-1}(\\theta_{l})_{:,i})\\|_{L}}\\\\ &{\\quad=\\displaystyle\\sum_{i}\\|\\sigma(S H^{l-1}(\\theta_{l})_{:,i})-\\sigma(Q^{+}S_{c}^{\\mathrm{MP}}H_{c}^{l-1}(\\theta_{l})_{:,i})\\|_{L}}\\\\ &{\\quad\\leq C_{\\sigma}\\displaystyle\\sum_{i}\\|S H^{l-1}(\\theta_{l})_{:,i}-Q^{+}S_{c}^{\\mathrm{MP}}H_{c}^{l-1}(\\theta_{l})_{:,i}\\|_{L}}\\\\ &{\\quad\\leq C_{\\sigma}\\displaystyle\\sum_{j}\\left(\\sum_{i}|(\\theta_{l})_{j i}|\\right)\\|S H_{:,j}^{l-1}-Q^{+}S_{c}^{\\mathrm{MP}}(H_{c}^{l-1})_{:,j}\\|_{L}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We then write ", "page_idx": 14}, {"type": "equation", "text": "$$\nS H_{:,j}^{l-1}-Q^{+}S_{c}^{\\mathrm{MP}}(H_{c}^{l-1})_{:,j}\\|_{L}\\leq\\|S H_{:,j}^{l-1}-Q^{+}S_{c}^{\\mathrm{MP}}Q H_{:,j}^{l-1}\\|_{L}+\\|Q^{+}S_{c}^{\\mathrm{MP}}Q H_{:,j}^{l-1}-Q^{+}S_{c}^{\\mathrm{MP}}(H_{c}^{l-1})_{:,j}\\|_{L}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Then note that, since both $S$ and $\\sigma$ are $\\mathcal{R}$ -preserving, for all $l,i$ we have that $(H^{l})_{:,i}\\in\\mathcal{R}$ . We can thus apply Theorem 1 to the first term: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\|S H_{:,j}^{l-1}-Q^{+}S_{c}^{\\mathrm{MP}}Q H_{:,j}^{l-1}\\|_{L}\\leq\\epsilon_{L,Q,\\mathcal{R}}(C_{S}+C_{\\Pi})\\|H_{:,j}^{l-1}\\|_{L}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The second term is 0 when $l=1$ since $H_{c}^{0}=Q H^{0}$ . Otherwise, using $Q Q^{+}=I_{n}$ , and since under Assumption 1 both $S$ and $\\Pi$ are $\\ker(L)$ -preserving, applying Lemma 1: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\|Q^{+}S_{c}^{\\mathrm{MP}}Q H_{:,j}^{l-1}-Q^{+}S_{c}^{\\mathrm{MP}}(H_{c}^{l-1})_{:,j}\\|_{L}=\\|Q^{+}S_{c}^{\\mathrm{MP}}Q(H_{:,j}^{l-1}-Q^{+}(H_{c}^{l-1})_{:,j})\\|_{L}}&{}\\\\ {=\\|\\Pi S\\Pi(H_{:,j}^{l-1}-Q^{+}(H_{c}^{l-1})_{:,j})\\|_{L}}&{}\\\\ {\\leq\\|\\Pi S\\Pi\\|_{L}\\|H_{:,j}^{l-1}-Q^{+}(H_{c}^{l-1})_{:,j}\\|_{L}}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "At the end of the day, we obtain ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{E_{l}\\leq C_{\\sigma}C_{\\theta,l}(\\epsilon_{L,Q,\\mathcal{R}}(C_{S}+C_{\\Pi})B_{l-1}+\\bar{C}_{\\Pi}E_{l-1})}\\\\ &{\\quad\\leq C_{\\sigma}^{l}\\bar{C}_{\\theta,l}C_{S}^{l-1}(C_{S}+C_{\\Pi})\\epsilon_{L,Q,\\mathcal{R}}\\Vert X\\Vert_{\\cdot,L}+C_{\\sigma}C_{\\theta,l}\\bar{C}_{\\Pi}E_{l-1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "using Lemma 2, and $\\begin{array}{r}{E_{1}\\leq\\epsilon_{L,Q,\\mathcal{R}}C_{\\sigma}C_{\\theta,1}(C_{S_{-}}+C_{\\Pi})\\|\\boldsymbol{X}\\|_{:,L}}\\end{array}$ . We recognize a recursion of the form $u_{n}\\leq a_{n}c+b_{n}u_{n-1}$ , which leads to $\\begin{array}{r}{u_{n}\\leq\\sum_{p=2}^{n}a_{p}\\prod_{i=p+1}^{n}b_{i}+u_{1}\\prod_{i=2}^{n}b_{i}}\\end{array}$ , which results in: ", "page_idx": 15}, {"type": "equation", "text": "$$\nE_{k}\\leq\\epsilon_{L,Q,\\mathcal{R}}\\|X\\|_{:,L}C_{\\sigma}^{k}\\bar{C}_{\\theta,k},\\left(C_{S}+C_{\\Pi}\\right)\\sum_{l=1}^{k}\\bar{C}_{\\Pi}^{k-l}C_{S}^{l-1}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "This concludes the proof with $C_{\\Theta}=\\operatorname*{max}_{\\theta\\in\\Theta}\\bar{C}_{\\theta,k}$ . ", "page_idx": 15}, {"type": "text", "text": "B Coarsening algorithm and experimental details ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "B.1 Adaptation of Loukas Algorithm ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "You can find below the pseudo-code of Loukas algorithm. This algorithm works by greedy selection of contraction sets (see below) according to some cost, merging the corresponding nodes, and iterate. The main modification is to replace the combinatorial Laplacian in the Loukas code by any Laplacian $L=f_{L}(A)$ , and to update the adjacency matrix according to (4) at each iteration and recompute $L$ , instead of directly updating $L$ as in the combinatorial Laplacian case. Note that we also remove the diagonal of $A_{c}$ at each iteration, as we find that it produces better results. The output of the algorithm is the resulting coarsening $Q$ , as well as $S_{c}^{\\mathrm{MP}}=\\bar{Q}S Q^{+}$ for our application. ", "page_idx": 15}, {"type": "text", "text": "Algorithm 1 Loukas Algorithm ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Require: Adjacency matrix $A$ , Laplacian $L=f_{L}(A)$ , propagation matrix $S$ , a coarsening ratio $r$ , preserved space $\\mathcal{R}$ , maximum number of nodes merged at one coarsening step : $n_{e}$ 1: $n_{o b j}\\leftarrow\\mathrm{int}(N-N\\times r)$ the number of nodes wanted at the end of the algorithm.   \n2: compute cost matrix $B_{0}\\leftarrow V V^{T}L^{-1/2}$ with $V$ an orthonormal basis of $\\mathcal{R}$ 3: $Q\\leftarrow I_{N}$ 4: while $n\\ge n_{o b j}$ do 5: Make one coarsening STEP $l$ 6: Create candidate contraction sets. 7: For each contraction C, compute cost(C, Bl\u22121, Ll\u22121) =\u2225\u03a0CBl\u22121(BlT\u22121|CL|l\u2212\u221211Bl\u22121)\u22121/2\u2225LC 8: Sort the list of contraction set by the lowest score 9: Select the lowest scores non overlapping contraction set while the number of nodes merged is inferior to $\\operatorname*{min}(n-n_{o b j},n_{e})$   \n10: Compute $Q_{l},Q_{l}^{+}$ , uniform intermediary coarsening with contraction sets selected   \n11: 12: $\\begin{array}{r l}&{B_{l}\\gets\\mathcal{G}_{l}B_{l-1}}\\\\ &{Q\\gets Q_{l}Q}\\\\ &{A_{l}\\gets(Q_{l}^{+})^{\\top}A_{l-1}Q_{l}^{+}-\\mathrm{diag}((Q_{l}^{+})^{\\top}A_{l-1}Q_{l}^{+})1_{n})}\\\\ &{L_{l-1}=f_{L}(A_{l-1})}\\\\ &{n\\gets\\operatorname*{min}(n-n_{o b j},n_{e})}\\end{array}$   \n13:   \n14:   \n15:   \n16: end while   \n17: IF uniform coarsening THEN $Q\\gets\\mathrm{row-normalize}(Q_{l}Q)$   \n18: Compute $S_{c}^{\\mathrm{MP}}=Q S\\bar{Q}^{+}$   \n19: return $Q,S_{c}^{\\mathrm{MP}}$ ", "page_idx": 15}, {"type": "text", "text": "The terms $\\Pi_{\\mathcal{C}}$ and $L_{\\mathcal{C}}$ are some specific projection of the contraction set, their explicit definition can be find in Loukas work [32]. We did not modify them here and leave their eventual adaptation for future work. ", "page_idx": 15}, {"type": "text", "text": "Enforcing the iterative/greedy aspect In our adaptation we also add a parameter $n_{e}$ to limit the number of nodes contracted at each coarsening step. In one coarsening step, when a contraction set $\\mathcal{C}$ is selected, we merge $|{\\mathcal{C}}|$ nodes. In practice Loukas proposed in its implementation to force $n_{e}=\\infty$ and coarsen the graph in one single iteration. We observed empirically better results by diminishing $n_{e}$ and combining it with enforcing the uniform coarsening (Appendix B.4). ", "page_idx": 15}, {"type": "text", "text": "Candidate contraction Set. Candidate contractions sets come in two main flavors: they can be each two nodes linked by edges, or the neighborhood of each nodes (so-called \"variation edges\" and \"variation neighborhood\" versions). In practice, as the neighborhood are quite big in our graphs, it is not very convenient for small coarsening ratio and give generally poor results. We will use mainly the edges set as candidate contraction sets and adjust the parameter $n_{e}$ to control the greedy aspect of this algorithm. ", "page_idx": 16}, {"type": "text", "text": "Uniform Coarsening At each coarsening step, in Loukas algorithm $Q_{l}$ is uniform by construction. Nonetheless the product of uniform coarsening is not necessarily an uniform coarsening. Then, we propose an option to force the uniform distribution in the super-nodes in the Loukas algorithm by normalize the non zero values of each line of the final coarsening matrix $Q$ . We observe that uniform coarsening gives better results for $\\epsilon_{L,Q,\\mathcal{R}}$ , and works better for our message passing guarantees. See Appendix B.4. ", "page_idx": 16}, {"type": "text", "text": "B.2 Discussion on Training procedure ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "The pseudocode of our training procedure is detailed in Algo. 2. ", "page_idx": 16}, {"type": "table", "img_path": "rIOTceoNc8/tmp/9d8ea7fb95f614dec1c8fd7b5d28bc62c1a8116095361ba6169af2ec8691eb39.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "Note that it is different from the procedure of [21] which computes labels for the super-nodes (using the majority label in the coarsening cluster) and do not use the uplifting matrix $Q^{+}$ . We find this procedure to be less amenable to semi-supervised learning, as super-nodes may merge training and testing nodes, and prefer to uplift the output of the GNN in the original graph instead. Additionally, this preserves the theoretical guarantees of Sec. 3. Our procedure might be slightly lower but we find the uplifting operation to be of negligible cost compared to actual backpropagation. ", "page_idx": 16}, {"type": "text", "text": "B.3 Presentation of dataset ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Synthetic Dataset Random geometric graph is built by sampling nodes with coordinates in $[0,1]^{2}$ and connecting them if their distance is under a given threshold. For the experiment on illustrating the message passing preservation guarantees, we sample 1000 nodes with a threshold of 0.05 (fig 3 ). ", "page_idx": 16}, {"type": "table", "img_path": "rIOTceoNc8/tmp/6219a3098b582b40b7a84b2c8536088a431975176c58b7ae579ada933c4f97c0.jpg", "table_caption": ["Table 4: Characteristics of Cora and CiteSeer Datasets "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "Real World datasets We restrict the well known Cora and Citeseer to their principal connected component(PCC) as it more compatible with coarsening as preprocessing. Indeed, the loukas ", "page_idx": 16}, {"type": "image", "img_path": "rIOTceoNc8/tmp/11930c36315d518b35b67c1dc395c4ed4644d0f26712eaf84da70498ed2fa14b.jpg", "img_caption": ["Figure 3: Example of a random Geometric graph "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "algorithm tend to coarsen first the smallest connected components before going to the biggest which leads to poor results for small coarsening ratio. However working with this reduced graph make the comparison with other works more difficult as it is not the same training and evaluating dataset. ", "page_idx": 17}, {"type": "text", "text": "For the Reddit dataset ( 1 PCC) its characteristics and of its coarsened version as well of the Reddit and Cora coarsened dataset can be find in the table 5 ", "page_idx": 17}, {"type": "table", "img_path": "rIOTceoNc8/tmp/3da2c41620ec168cf44db2f09f32fcf5a2d4dcfd23f9e29a803fd173d8c1502a.jpg", "table_caption": ["Table 5: Characteristics of Reddit, Cora, Citeseer and its coarsen version "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "B.4 Discussion of hyperparameters and additional experiments ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In the following section, we will use two different view of the same plot, to focus on different parts. We use the log-log scale (fig 4a) to put the focus on low coarsening ratio and on the upper bound. We use the linear scale (fig 4b) to compare more precisely our propagation matrix with $\\dot{S}_{c}^{d i a g}$ and $S_{c}$ for higher coarsening ratio. ", "page_idx": 17}, {"type": "image", "img_path": "rIOTceoNc8/tmp/5fcfef1b9623a6e1ab14ddaf669cc64ddd4eca03413dc9afe99411b951e02704.jpg", "img_caption": ["Figure 4: Uniform coarsening with $n_{e}=5N/100$ and Normalized Laplacian "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "Uniform coarsening We observe that forcing uniform coarsening gives better $\\epsilon_{L,Q,\\mathcal{R}}$ and thus better message passing guarantees . It is shown in the figure 5 for $n_{e}=5N/100$ with N the number of Nodes of the graph (1000 here). ", "page_idx": 18}, {"type": "image", "img_path": "rIOTceoNc8/tmp/b66bf938fc779a7e335550ff9694bb75f7f218080dcc8bdd29ceb3aeca842299.jpg", "img_caption": ["Figure 5: Coarsening with $n_{e}=5N/100$ and Normalized Laplacian "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "Bounding $n_{e}$ . For high coarsening ratio, we observe limits of the variation edges defined as Loukas with $n_{e}\\,\\rightarrow\\,\\infty$ as it gives bigger $\\epsilon_{L,Q,\\mathcal{R}}$ and thus worse curve for our propagation matrix in the coarsened graph (fig 6). ", "page_idx": 18}, {"type": "text", "text": "B.5 Hyper-parameters for Table 1 and Table 2 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "For all experiments, we preserve $K$ eigenvectors of the normalized Laplacian defined as $L\\,=$ $I_{N}(1+\\delta\\bar{)}-S$ with $\\delta=0.001$ and $K\\bar{=}10\\%N$ where $N$ is the number of nodes in the original graph. We apply our adapted version of Loukas coarsening algorithm with $n_{e}=5\\%N$ for SGC Cora, SGC Citeseer and GCN citeseer and $n_{e}\\rightarrow\\infty$ for GCN Cora (variation edges as defined by Loukas). For SGC cora and SGC Citeseer we make 6 propagations as preprocessing for the features. For GCN Cora and Citeseer we use 2 convolationnal layer with a hidden dimension of 16. For all experiments we use an Adam Optimizer wit a learning rate of 0.05 and a weight decay of 0.01. ", "page_idx": 18}, {"type": "text", "text": "B.6 Hyper-parameters for Table 3 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "For the experiment on Reddit Dataset, we preserve $K$ eigenvectors of the normalized Laplacian defined as $L=I_{N}(1+\\delta)-S$ with $\\delta=0.001$ and $K=400$ eigenvectors to be computationally efficient ( $10\\%N$ being too big). We apply our adapted version of Loukas coarsening algorithm with ", "page_idx": 18}, {"type": "image", "img_path": "rIOTceoNc8/tmp/ddf75d094bee55748bc593ef9c99cead89da291be081919cc6d0554827727240.jpg", "img_caption": ["Figure 6: Uniform coarsening for Normalized Laplacian "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "$n_{e}=10\\%N$ for SGC Reddit and GCN Reddit. We computed 6 propagations for Reddit SGC and 2 for Reddit GCN. We keep the same hidden dimension as for Cora and Citeseer. For the reddit experiments, we use an Adam Optimizer wit a learning rate of 0.1 and a weight decay of 0.0. ", "page_idx": 19}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "i) Claims ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: This is a mostly theoretical paper. Theorems and their implications are described in abstract and introduction. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 20}, {"type": "text", "text": "ii) Limitations ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: This is a mostly theoretical paper. Hypotheses are illustrated by examples and limitations are discussed. For experiments, scalability is discussed. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 20}, {"type": "text", "text": "iii) Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: Proofs are provided in Appendix. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 21}, {"type": "text", "text": "iv) Experimental Result Reproducibility ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: Code is included as supplementary material, and can be run on any computer. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example i) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. ii) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. iii) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). iv) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 21}, {"type": "text", "text": "v) Open access to data and code ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: Code is included as supplementary material, and use only open-source Python libraries. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/ guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 22}, {"type": "text", "text": "vi) Experimental Setting/Details ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: All details are described in Appendix, and the code in supplementary material. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.   \n\u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 22}, {"type": "text", "text": "vii) Experiment Statistical Significance ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: Standard deviations are reported in Table results. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). ", "page_idx": 22}, {"type": "text", "text": "\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 23}, {"type": "text", "text": "viii) Experiments Compute Resources ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: This is small-scale code: it can be run on any computer in reasonable time. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 23}, {"type": "text", "text": "ix) Code Of Ethics ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: This is a mostly theoretical paper. The code use only open-source Python libraries. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 23}, {"type": "text", "text": "x) Broader Impacts", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: This is a mostly theoretical paper. We do not anticipate significant societal impact as a direct result of our work. Future algorithmic work on scalability of graph coarsening could include such discussion, but this is relatively out-of-topic for this paper. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "xi) Safeguards ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: This paper does not include such model. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 24}, {"type": "text", "text": "xii) Licenses for existing assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: The original paper for Cora and Citeseer is cited. Details are given in Appendix. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 24}, {"type": "text", "text": "xiii) New Assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: This paper does not release new assets. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that the paper does not release new assets. ", "page_idx": 24}, {"type": "text", "text": "\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 25}, {"type": "text", "text": "xiv) Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 25}, {"type": "text", "text": "xv) Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 25}]