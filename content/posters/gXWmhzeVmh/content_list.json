[{"type": "text", "text": "Rough Transformers: Lightweight Continuous-Time Sequence Modelling with Path Signatures ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Fernando Moreno-Pino1,\u2217 \u00c1lvaro Arroyo1,2,\u2217 Harrison Waldon1,\u2217 Xiaowen Dong1,2 \u00c1lvaro Cartea1,3 ", "page_idx": 0}, {"type": "text", "text": "1 Oxford-Man Institute, University of Oxford 2 Machine Learning Research Group, University of Oxford 3 Mathematical Institute, University of Oxford ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Time-series data in real-world settings typically exhibit long-range dependencies and are observed at non-uniform intervals. In these settings, traditional sequencebased recurrent models struggle. To overcome this, researchers often replace recurrent architectures with Neural ODE-based models to account for irregularly sampled data and use Transformer-based architectures to account for long-range dependencies. Despite the success of these two approaches, both incur very high computational costs for input sequences of even moderate length. To address this challenge, we introduce the Rough Transformer, a variation of the Transformer model that operates on continuous-time representations of input sequences and incurs significantly lower computational costs. In particular, we propose multi-view signature attention, which uses path signatures to augment vanilla attention and to capture both local and global (multi-scale) dependencies in the input data, while remaining robust to changes in the sequence length and sampling frequency and yielding improved spatial processing. We find that, on a variety of time-seriesrelated tasks, Rough Transformers consistently outperform their vanilla attention counterparts while obtaining the representational benefits of Neural ODE-based models, all at a fraction of the computational time and memory resources. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Real-world sequential data in areas such as healthcare [68], finance [38], and biology [30] often are irregularly sampled, of variable length, and exhibit long-range dependencies. Furthermore, these data, which may be drawn from financial limit order books [10] or EEG readings [89], are often sampled at high frequency, yielding long sequences of data. Hence, many popular machine learning models struggle to model real-world sequential data, due to input dimension inflexibility, memory constraints, and computational bottlenecks. Rather than treating these data as discrete sequences, effective theoretical models often assume data are generated from some underlying continuous-time process [56, 69]. Hence, there is an increased interest in developing machine learning methods that use continuous-time representations to analyze sequential data. ", "page_idx": 0}, {"type": "text", "text": "One recent approach to modelling continuous-time data involves the development of continuous-time analogues of standard deep learning models, such as Neural ODEs [14] and Neural CDEs [48], which extend ResNets [39] and RNNs [32], respectively, to continuous-time settings. Instead of processing discrete data directly, these models operate on a latent continuous-time representation of input sequences. This approach is successful in continuous-time modelling tasks where standard deep recurrent models fail. In particular, extensions of vanilla Neural ODEs to the time-series setting [73, 48] succeed in various domains such as adaptive uncertainty quantification [62], counterfactual inference [83], or generative modelling [9]. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In many practical settings, such as financial market volatility [22, 57] or heart rate fluctuations [37], continuous-time data also exhibit long-range dependencies. That is, data from the distant past may impact the system\u2019s current behavior. Deep recurrent models struggle in this setting due to vanishing gradients, whereas continuous-time analogues of these models have been shown to address this difficulty [49]. Several recent works [55, 61] also successfully extract long-range dependencies from sequential data with Transformers [90], which learn temporal dependencies of a tokenized representation of input sequences. Extracting such temporal dependencies requires a positional encoding of input data, because the attention mechanism is permutation invariant, which projects data into some latent space. The parallelizable nature of the Transformer allows for rapid training and evaluation on sequences of moderate length and it contributes to its success in fields such as natural language processing (NLP). ", "page_idx": 1}, {"type": "text", "text": "While the above approaches succeed in certain settings, several limitations hinder their wider applications. On the one hand, Neural ODEs and their analogues [48, 73] bear substantial computational costs when modelling long sequences of high dimension; see [60]. On the other hand, Transformers operate on discrete-time representations of input sequences, whose relative ordering is represented by the positional encoding. This representation may inhibit their expressivity in continuoustime data modelling tasks [95]. Moreover, Transformer-based models suffer from a number of difficulties, including (i) input sequences must be sampled at the same times, (ii) the sequence length must be fixed, and (iii) the computational cost scales quadratically in the length of the input sequence. These difficulties severely limit the application of Transformers to continuous-time data modelling. ", "page_idx": 1}, {"type": "text", "text": "Contributions 1) We introduce Rough Transformers, a variant of the Transformer architecture amenable to the processing of continuous-time signals, which can be easily integrated into existing code-bases. The Rough Transformer is built upon the path signature from Rough Path Theory [54]. We define a novel, multi-scale transformation which projects discrete input data to a continuous-time path and compresses the input data with minimal information loss. Moreover, this transformation is an efficient feature representation of continuous-time paths, because linear functionals of path signatures approximate continuous functions of paths arbitrarily well (see Theorem A.2 in Appendix A). ", "page_idx": 1}, {"type": "image", "img_path": "gXWmhzeVmh/tmp/bc4eb1e5cc757c9040ed71e71026e96a07780b433a82c735b78a30d37922533c.jpg", "img_caption": ["Figure 1: A representation of the multi-view signature. The continuous-time path is irregularly sampled at points marked with a red $x$ . The local and global signatures of a linear interpolation of these points are computed and concatenated to form the multi-view signature. The multi-view signature transform consists of $\\overline{{L}}$ multi-view signatures. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "2) We introduce the multi-view attention mechanism to extract both local and global dependencies of very long time-series efficiently. This mechanism operates directly on continuoustime representations of data without the need for expensive numerical solvers or constraints on the smoothness of the data stream. Moreover, the multi-view attention mechanism is provably robust to irregularly sampled data. ", "page_idx": 1}, {"type": "text", "text": "3) We carry out extensive experimentation on long and irregularly sampled time-series data. In particular, we show that Rough Transformers (i) improve the learning dynamics of the Transformer, making it more sample-efficient and allowing it to achieve better out-of-sample results, (ii) reduce the training cost by a factor of up to $25\\times$ when compared with vanilla Transformers and more when compared with Neural ODE based architectures, (iii) maintain similar performance when data are irregularly sampled, where traditional recurrent-based models suffer a substantial decrease in performance [73], and (iv) yield improved spatial processing, accounting for relationships between different temporal channels without having to pre-define a specific inter-channel relation structure. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "2 Background and Methodology ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Problem Formulation. In many real-world scenarios, sequential data are time-series sampled from some underlying continuous-time process, so datasets consist of long, irregularly sampled sequences of varied lengths. In these settings, the problem of sequence modelling is described as follows. Let $C(\\mathbb{R}^{+};\\mathbb{R}^{d})=\\{g:\\mathbb{R}^{+}\\to\\mathbb{R}^{d}\\,|\\,g\\mathrm{\\;continuous}\\},$ , and consider $\\widehat{X}\\in C(\\mathbb{R}^{\\bar{+}};\\mathbb{R}^{d})$ which we call a continuous-time path. A time-series of length $L$ with sampling t imes $\\mathcal{T}_{\\mathbf{X}}=\\{t_{i}\\}_{i=1}^{L}\\subset\\mathbb{R}^{+}$ is defined as $\\mathbf{X}=((t_{1},X_{1}),...,(t_{L},X_{L}))$ , where $X_{i}\\,=\\,\\widehat{X}(t_{i})\\,\\in\\,\\mathbb{R}^{d}$ . Now, define a continuous function on paths $f:C(\\mathbb{R}^{+};\\mathbb{R}^{d})\\to\\mathbb{R}^{k}$ . Next define a dat aset $\\mathcal{D}=\\left\\{(\\mathbf{X}^{i},f(\\widehat{X}^{i}))_{i=1}^{N}\\right\\}$ . We seek to approximate the function $f$ from the set $\\mathcal{D}$ for some downstream task. Importantly, we do not assume that $\\mathcal{T}_{\\mathbf{X}}=\\mathcal{T}_{\\mathbf{Y}}$ for all $\\mathbf{X},\\mathbf{Y}\\in\\mathcal{D}$ , so that $\\mathcal{D}$ may be irregularly sampled. ", "page_idx": 2}, {"type": "text", "text": "Sequence Modelling with Transformers. Transformers are used extensively as a baseline architecture to approximate functions of discrete-time sequential data and are successfully applied to settings when input sequences are fixed in length, relatively short, and sampled at regular intervals. First, the Transformer projects input time series $\\mathbf{X}\\in\\mathbb{R}^{L\\times d}$ to a high-dimensional space $\\mathbf{X}\\mapsto T(\\mathbf{X})\\in\\mathbb{R}^{L\\times d^{\\prime}}$ for $d^{\\prime}>>d$ using some linear positional encoding $T:\\mathbb{R}^{L\\times d}\\ \\rightarrow\\ \\mathbb{R}^{L\\times d^{\\prime}}$ . Next, a latent representation of the encoded sequence is learned by a multi-headed self-attention mechanism which splits $T(\\mathbf{X})$ into $H$ distinct query, key, and value sequences: $Q_{h}=T({\\bf X})W_{h}^{Q}$ , $K_{h}=T(\\mathbf{X})W_{h}^{K}$ $V_{h}\\,=\\,T({\\bf X})W_{h}^{V}$ , respectively, with $h=1,...,H$ and weight matrices $W_{h}^{Q},W_{h}^{K},W_{h}^{V}\\,\\in\\,\\mathbb{R}^{d^{\\prime}\\times d^{\\prime}}$ . The multi-head self-attention calculation for each head is given by ", "page_idx": 2}, {"type": "equation", "text": "$$\n{\\cal O}_{h}=\\mathrm{softmax}\\,\\left(\\frac{Q_{h}K_{h}^{\\sf T}}{\\sqrt{d_{k}}}\\right)V_{h}\\,,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "and the latent representation is projected to the output space $\\mathbb{R}^{k}$ using a multi-layer perceptron (MLP). ", "page_idx": 2}, {"type": "text", "text": "The input length $L$ of the MLP and the Transformer is fixed by assumption. To evaluate the Transformer on a time-series $\\mathbf{X}$ with $|\\mathcal{T}_{\\mathbf{X}}|\\neq n$ , one must perform some transformation (interpolation, extrapolation, etc.) which may degrade the performance of the model. Furthermore, the memory and time complexity of the Transformer is of order $O(L^{2}d)$ , which presents a substantial difficulty in modelling long sequences. ", "page_idx": 2}, {"type": "text", "text": "Rough Path Signatures. Broadly, the difficulties faced by the Transformer in modelling timeseries stem from time-series being sampled from underlying continuous-time objects, while the attention mechanism underpinning the Transformer is designed to model discrete sequences. To address these difficulties, Rough Transformers augment standard Transformers by lifting the input time-series to the space of continuous-time functions and performing the self-attention calculation in this infinite-dimensional space. To achieve this, we use the path signature from Rough Path Theory. ", "page_idx": 2}, {"type": "text", "text": "For a continuous-time path $\\widehat{X}\\in C_{b}^{1}(\\mathbb{R}^{+};\\mathbb{R}^{d})$ and times $s,t\\in\\mathbb{R}^{+}$ , the path signature of $\\widehat{X}$ from $s$ to $t$ , denoted $S(\\widehat{X})_{s,t}$ , is defined as follows. First, let ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{Z}_{d}=\\big\\{\\big(i_{1},...,i_{p}\\big):i_{j}\\in\\{1,...,n\\}\\,\\forall\\,j\\mathrm{~and}\\,p\\in\\mathbb{N}\\big\\}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "denote the set of all $d$ -multi-indices and ${\\mathcal{Z}}_{d}^{n}=\\{I\\in{\\mathcal{Z}}_{d}:|I|=n\\}$ . Next, set $S(\\widehat{X})_{s,t}^{0}:=1$ and for any $I\\in{\\mathcal{Z}}_{d}$ , define ", "page_idx": 2}, {"type": "equation", "text": "$$\nS(\\widehat{X})_{s,t}^{I}=\\int_{s<u_{1}<\\ldots<u_{p}<t}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\hat{X}^{i_{1}}(u_{1})\\cdot\\cdot\\cdot\\cdot\\,\\hat{\\widehat{X}}^{i_{p}}(u_{p})\\,d u_{1}\\ldots d u_{p}\\,,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where X = dXj/dt. Abusing notation, define level n of the signature as ", "page_idx": 2}, {"type": "equation", "text": "$$\nS^{n}(\\widehat{X})_{s,t}=\\left\\{S(\\widehat{X})_{s,t}^{I}:I\\in\\mathbb{Z}_{d}^{n}\\right\\}\\,.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "and define the signature as the infinite sequence ", "page_idx": 3}, {"type": "equation", "text": "$$\nS(\\widehat{X})_{s,t}^{n}=(S(\\widehat{X})_{s,t}^{0},S(\\widehat{X})_{s,t}^{1},...,S(\\widehat{X})_{s,t}^{n},...)\\,.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Finally, define the truncation of the signature $S(\\widehat{X})_{s,t}^{\\leq n}=(S(\\widehat{X})_{s,t}^{0},...,S(\\widehat{X})_{s,t}^{n})$ , where $S(\\widehat{X})_{s,t}^{n}$ can be interpreted as an element of the extended tensor algebra of $\\mathbb{R}^{d}$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\nT((\\mathbb{R}^{d}))=\\left\\{(a_{0},...,a_{n},...):a_{n}\\in\\mathbb{R}^{d\\otimes n}\\right\\}\\,.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Analogously, we say that $S(\\widehat{X})_{s,t}^{\\leq n}\\,\\in\\,T((\\mathbb{R}^{d})){\\leq}n$ . A central property of the signature is that is invariant with respect to time-reparameterization [54]. That is, let $\\gamma:[0,T]\\rightarrow[0,T]$ be surjective, continuous, and non-decreasing. Then we have ", "page_idx": 3}, {"type": "equation", "text": "$$\nS(\\widehat{X})_{0,T}=S(\\widehat{X}\\circ\\gamma)_{0,T}\\,,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "which will be crucial to demonstrate the Rough Transformer\u2019s robustness to irregularly sampled data. ", "page_idx": 3}, {"type": "text", "text": "In contrast to wavelets or Fourier transforms, which parameterize paths on a functional basis, the signature provides a basis for functions of continuous paths. Hence, the path signature is well-suited to sequence modelling tasks in which one seeks to learn a function of the underlying functional. For a more rigorous presentation of signatures and a description of additional properties, see Appendix A and Lyons et al. [54]. ", "page_idx": 3}, {"type": "text", "text": "3 Rough Transformers ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Now, we construct the Rough Transformer, a Transformer-based architecture that operates on continuous-time sequential data by means of the path signature. ", "page_idx": 3}, {"type": "text", "text": "Let $\\mathcal{D}$ be a dataset of irregularly sampled time-series. To project a discretized time-series $\\mathbf{X}\\in\\mathcal{D}$ to a continuous-time object, let X\u02dc denote the piecewise-linear interpolation of $\\mathbf{X}$ .2 Next, for $t_{k}\\in\\mathcal{T}$ , define the multi-view signature ", "page_idx": 3}, {"type": "equation", "text": "$$\nM({\\bf X})_{k}:=\\left(S(\\tilde{X})_{0,t_{k}},S(\\tilde{X})_{t_{k-1},t_{k}}\\right)\\,.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "In what follows, we refer to the components $\\left(S(\\tilde{X})_{0,t_{k}},S(\\tilde{X})_{t_{k-1},t_{k}}\\right)$ as global and local, respectively; see Figure 1. Intuitively, one can interpret the global component as an efficient representation of long-term information (see Theorem A.2 in Appendix A), and the local component as a type of convolutional fliter that is invariant to the sampling rate of the signal. Now, define the multi-view signature transform $M(\\mathbf{X})=(M(\\mathbf{X})_{1},...,M(\\mathbf{X})_{\\bar{L}})$ , and denote by $M(\\mathbf{X})^{\\leq n}$ the truncated signature for a truncation level $n$ . Next, define the multi-view attention mechanism, which uses the multi-view signature transform to extend the standard attention mechanism to the space of continuous functions [54]. First, fix a truncation level $n\\in\\mathbb N$ , and let $\\bar{d}\\in\\mathbb N$ be such that $M(\\bar{\\mathbf X_{k}})_{k}^{\\bar{\\le}n}\\in\\mathbb{R}^{\\bar{d}}$ . For $h=1,...,H$ let $W_{h}^{\\tilde{Q},\\tilde{K},\\tilde{V}}\\in\\mathbb{R}^{\\bar{d}\\times\\bar{d}^{\\prime}}$ for some $\\bar{d^{\\prime}}\\in\\mathbb{N}$ , and let ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\tilde{Q}_{h}=M({\\bf X})^{\\le n}W_{h}^{\\bar{Q}},\\quad\\tilde{K}_{h}=M({\\bf X})^{\\le n}W_{h}^{\\bar{K}},\\quad\\tilde{V}_{h}=M({\\bf X})^{\\le n}W_{h}^{\\bar{V}}\\,.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Then, the attention calculation is given by ", "page_idx": 3}, {"type": "equation", "text": "$$\n{\\cal O}_{h}=\\mathrm{softmax}\\,\\left(\\frac{\\tilde{Q}_{h}\\tilde{K}_{h}^{\\sf T}}{\\sqrt{\\bar{d}^{\\prime}}}\\right)\\tilde{V}_{h}\\,.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Notice that the attention calculation is similar to (1), however, we stress that the multi-view attention is built on continuous-time objects, the signatures, while the standard attention mechanism acts on discrete objects. The multi-view signature provides a compressed representation of the time series, minimizing the computational costs associated to quadratic scaling without excessive loss of representational capacity, see Appendix F. ", "page_idx": 3}, {"type": "image", "img_path": "gXWmhzeVmh/tmp/af6481a1be0ee69ad9ee147302267a0608d4c425acdd89f64ddd421bc7ab48db.jpg", "img_caption": ["Figure 2: Seconds per epoch for growing input length and for different model types on the sinusoidal dataset. Left: Log Scale. Middle: Regular Scale. Right: Log-log scale. When a line stops, it indicates an OOM error. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "3.1 Advantages of Rough Transformers ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Computational Efficiency. As demonstrated in Section 4, multi-view attention mechanism can substantially reduce the computational cost of vanilla Transformers. In particular, the attention calculation decreases from $O(L^{2}\\,d)$ in the vanilla case to $O(\\overline{{L}}^{2}\\,d)$ , where $\\overline{{L}}<<L$ with Rough Transformers. This enables both faster wall-clock training time and the ability to process long input sequences which would otherwise yield out-of-memory errors for the vanilla Transformer, see Figure 2. Moreover, the multi-view attention mechanism does not require backpropagation through the signature calculation and can be computed offilne. This is significantly more computationally efficient compared with the complexity of computing signatures batch-wise in every training step. Finally, the signature of piecewise-linear paths can be computed explicitly, see Appendix A, and there are a number of Python packages devoted to optimized signature calculation [46, 71]. ", "page_idx": 4}, {"type": "text", "text": "Variable Length and Irregular Sampling. The multi-view signature transform underpinning Rough Transformers is evaluated by constructing a continuous-time interpolation of input data and computing a series of iterated integrals of this interpolation. The bounds of these integrals are a fixed set of time points, meaning that the sequence length of the multi-view attention mechanism is fixed and independent of the sequence length of input samples. Furthermore, the following proposition shows that the output of the Rough Transformer for two (possibly irregular) samplings of the same path is similar. ", "page_idx": 4}, {"type": "text", "text": "Proposition 3.1. Let T be a Rough Transformer. Suppose $\\widehat{X}:[0,T]\\rightarrow\\mathbb{R}^{d}$ is a continuous-time process, and let $\\gamma:[0,T]\\,\\rightarrow\\,[0,T]$ denote a time-reparameterization. Suppose $\\mathbf{X}$ and $\\mathbf{X^{\\prime}}$ are samplings of $\\widehat{X}$ and ${\\widehat{X}}\\circ\\gamma,$ respectively. Then $\\mathbb{T}(\\mathbf{X})\\approx\\mathbb{T}(\\mathbf{X}^{\\prime})$ . ", "page_idx": 4}, {"type": "text", "text": "Proof. By (7), $S(\\widehat{X})_{s,t}\\,=\\,S(\\widehat{X}\\circ\\gamma)_{s,t}$ for all $s,t\\,\\in\\,[0,T]$ . Hence, one has $M(X^{1})\\approx M(X^{2})$ .   \nFinally, $\\mathbb{T}(X^{1})\\approx\\mathbb{T}(X^{2})$ because the attention mechanism and final MLP are both continuous. ", "page_idx": 4}, {"type": "text", "text": "Hence, the Rough Transformer is robust to irregular sampling. In many tasks, the sampling times convey important information about the time-series. In these settings, one may augment the input time-series with its sampling times, that is, write $X=((t_{0},X_{0}),...,(t_{L},X_{L}))$ . ", "page_idx": 4}, {"type": "text", "text": "Spatial Processing. While an interpolation of input data could be sampled to make vanilla Transformers independent of the length of the input sequence, important locality information could be lost, see Appendix F.2. Instead, Rough Transformers summarize spatial interactions between channels by means of the multi-view signature transform. One may notice that in (5), the dimension of the signature grows exponentially in the level of the signature $n$ . In particular, when $X_{i}\\,\\in\\,\\mathbb{R}^{d}$ , $\\begin{array}{r}{|S(\\bar{X})_{0,t}^{\\le_{n}}|\\,=\\,\\frac{\\bar{d}(d^{n}-1)}{d-1}\\,=\\,O(d^{n})}\\end{array}$ , so the multi-view attention calculation is of order $O(\\bar{L}^{2}d^{n})$ . In many practical time-series modelling problems, however, the value of $d$ is not very large. The signature terms also decay factorially in the signature level $n$ (see Proposition A.3 in Appendix A), so in practice, one may take the value of $n$ to be small without sacrificing performance. The majority of computational savings result from the reduction of the sequence length to $\\bar{L}$ , and in practice, we take $\\bar{L}<<L$ . ", "page_idx": 4}, {"type": "text", "text": "When the dimension $d$ is large, there are three possible remedies to maintain computational efficiency. First, instead of computing the signature in $\\bar{M}(X)_{k}=(S(X)_{0,t_{k}},S(\\tilde{X}_{t_{k-1},t_{k}}))$ , one may compute the log-signature, which is a compressed version of the signature [70]. When the dimension is large enough such that the log-signature is computationally infeasible, one may instead compute the univariate signatures of features coupled with the time channel. That is, consider $\\widehat{X}\\in C([0,T];\\mathbb{R}^{d})$ , with $\\widehat{X}(t)\\,=\\,(\\widehat{X}_{1}(t),...,\\widehat{X}_{d}(t))$ . Denote the time-added function $\\overline{{X}}_{i}(t):=\\,(t,\\widehat{X}_{i}(t))$ . Then we defin e the univa riate mult i-view signature ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\widehat{M}(\\widehat{X})_{k}=\\left(M(\\overline{{{X}}}_{1})_{k},...,M(\\overline{{{X}}}_{d})_{k}\\right)\\;.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The attention mechanism in this case is constructed as before. Fixing the maximum signature depth to be some value $n^{*}$ , one sees that the number of features in the univariate multi-view signature is approximately $2^{n^{*}}d$ . In practice we find that ${n^{*}}\\leq5$ provides sufficient performance, so the order of the attention calculation is $O(C\\,\\Bar{L}^{2}\\,d)$ for $C\\leq2^{n^{*}}$ . Finally, one may use randomized signatures to reduce dimension by using a Johnson-Lindenstrauss-type projection to a low-dimensional latent space and computing the signature in this space, as in [23, 21]. ", "page_idx": 5}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we present empirical results for the effectiveness of the Rough Transformer, hereafter denoted RFormer, on a variety of time-series-related tasks. Experimental and hyperparameter details regarding the implementation of the method are in Appendices C and D. We consider long multivariate time-series as our main experimental setting because we expect signatures to perform best in this scenario. Additional experimentation on long-range reasoning tasks on image-based datasets is left for future work, as these would likely require additional inductive biases. ", "page_idx": 5}, {"type": "text", "text": "To benchmark RFormer, we consider both discrete-time and continuous-time models. In particular, we include as main baselines traditional RNN models (GRU [17]), ODE-based methods designed for sequential data (Neural-CDE [48]), as well as ODE-based methods explicitly designed for long time-series (Neural-RDE [60]).3 Furthermore, we compare against a vanilla Transformer [90] which is the RFormer backbone. Finally, we present comparisons with a recent continuous-time Transformer model, ContiFormer [15], to highlight the computational efficiency gap between RFormer and similar continuous-time models. We note that the first two tasks focus on evaluating the performance improvement of RFormer over the Transformer baseline. For other long-range tasks, we include comparisons to recent state-space models [33, 65, 85]. In the irregular sampling regime, we benchmark against state-of-the-art models tailored to that setting [64, 82]. See Appendix B for additional discussion on related models and more details about our experimental choices. ", "page_idx": 5}, {"type": "text", "text": "4.1 Time Series Processing ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Frequency Classification. Our first experiment is based on a set of synthetically generated time series from continuous paths of the form ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\widehat{X}(t)=g(t)\\sin(\\omega\\,t+\\nu)+\\eta(t)\\,,\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $g(t)$ is a non-linear trend component, $\\nu$ and $\\eta$ are two noise terms, and $\\omega$ is the frequency. Here, the task of the model is to classify the time-series according to its frequency $\\omega$ . We consider 1000 samples in 100 classes with $\\omega$ evenly distributed from 10 to 500. Each time-series is regularly sampled with 2000 times-steps on the interval $[0,1]$ . ", "page_idx": 5}, {"type": "image", "img_path": "gXWmhzeVmh/tmp/7e52326547bbdf12d3a209d8959ee90df6cf2559a32fa59af287701063090be9.jpg", "img_caption": ["Figure 3: Test accuracy per epoch for the frequency classification task across three random seeds. Left: Sinusoidal dataset. Right: Long Sinusoidal dataset. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "This synthetic experiment is similar to others in recent work on time-series modelling [52, 93, 58]. We include an additional experiment in which we alter the signal in (12) so its frequency is $\\omega_{0}$ for $t<t_{0}$ and $\\omega_{1}$ afterward, where the task is to classify the sinusoid based on the first frequency. We call this dataset the \u201clong sinusoidal\" dataset. This extension of the original experiment aims to test the ability of the model to perform long-range reasoning effectively. Note that for this task, we also add ODE-RNN [73] to the previously mentioned baselines. ", "page_idx": 5}, {"type": "text", "text": "Figure 3 shows that the inclusion of both local and global information with the multi-view signature enhances the sample efficiency of the RFormer over the vanilla Transformer model, even though the attention mechanism is now operating on a much shorter sequence. When compared with other models, we see that GRU and ODE-RNN fail to capture the information in the signal, and are not able to obtain any meaningful performance improvement throughout the training period. This highlights the shortcomings of most RNN-based models when processing sequences of moderate length, which are very common in real-world applications. Both Neural-CDE and Neural-RDE capture some useful dependencies in the time series but fall short compared with both vanilla Transformer and RFormer. ", "page_idx": 6}, {"type": "text", "text": "HR dataset. Next, we consider the Heart Rate dataset from the TSR archive [87], originally sourced from Beth Israel Deaconess Medical Center (BIDMC). This dataset consists of time-series sampled from patient ECG readings, and each model is tasked to perform a regression by forecasting the patient\u2019s heart rate (HR) at ", "page_idx": 6}, {"type": "text", "text": "Table 1: Test RMSE (mean \u00b1 std) computed across five seeds on the Heart Rate (HR) dataset. ", "page_idx": 6}, {"type": "table", "img_path": "gXWmhzeVmh/tmp/3343ceaf72e9cdca2da8d7e0d25f03e761c4b0c5ba061a0caaadb6f8c1f58e0e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "the sample\u2019s conclusion. The data, sampled at $125\\mathrm{Hz}$ , consists of three-channel time-series (including time), each spanning 4000 time steps. We used the L2 loss metric to assess the performance. Table 1 shows the results, where $\\diamond$ denotes the results from Morrill et al. [60] and $\\dagger$ our reproduction. The sequences in the HR dataset are sufficiently short to remain within memory when running the Transformer model. The baseline Transformer model improves over GRU, and ODE-RNN, however, it is less competitive when compared with Neural-RDE, suggesting that the Transformer is not particularly well-suited for this type of task. However, the RFormer model improves over the baseline Transformer by $67\\%$ . Across all tasks, we see significant improvements in efficiency as a consequence of the signature computation. We elaborate on this in more detail in the following subsection. ", "page_idx": 6}, {"type": "text", "text": "Long Time Series Classification. We now evaluate the performance of RFormer on five long time series classification tasks from the UEA time series classification archive [5]. A summary of these datasets is provided in Table 13 in Appendix E. As previously done in [60], the original train and test datasets are merged and then randomly divided into new train, validation, and test sets, following a 70/15/15 split. The resulting performance metrics are summarized in Table 2.4 ", "page_idx": 6}, {"type": "text", "text": "In this setting, we see that RFormer generally matches or slightly outperforms the continuous-time and SSM baselines. Due to the scaling problems of ContiFormer with respect to sequence length, we were unable to run this baseline within GPU memory constraints in most cases, and thus no results are reported (see Appendix G.2 for efficiency comparisons between models). In contrast, RFormer can cheaply train on the same device (see Section 4.2 for details) due to its ability to take advantage of the parallel nature of GPU processing and compress the original time series. This is especially noticeable when compared to continuous-time models (Neural-CDE, Neural-RDE, LogCDE), which are sometimes orders of magnitude slower than our model and consistently report lower or similar results. Additional experimental details can be found in Appendix G, as well as some experiments on hyperparameter sensitivity. ", "page_idx": 6}, {"type": "table", "img_path": "gXWmhzeVmh/tmp/3148d50224d1bc52d4f75473e7389b4c6bee09eac9a4f3ad232470c35e834df3.jpg", "table_caption": ["Table 2: Classification performance on various long context temporal datasets from UCR TS archive. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "4.2 Training Efficiency ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Here, we focus on the computational gains of the model when compared with vanilla Transformers and methods that require numerical ODE solvers. ", "page_idx": 7}, {"type": "text", "text": "Attention-based architectures are highly parallelizable on modern GPUs, as opposed to traditional RNN models which require sequential updating. However, vanilla attention experiences a bottleneck in memory and time complexity as the sequence length $L$ grows. As covered above in Section 3, variations of the signature transform allow the model to operate on a reduced sequence length $\\bar{L}$ without increasing the dimensionality in a way that would become problematic for the model. This allows us to bypass the quadratic complexity of the model without resorting to sparsity techniques commonly used in the literature [28, 52]. ", "page_idx": 7}, {"type": "text", "text": "Tables 1-3 show that RFormer is competitive when modelling datasets with extremely long sequences without an explosion in the memory requirements. RFormer exploits the parallelism of the attention mechanism to significantly accelerate training time, as the length of the input sequence is decreased substantially. In particular, we observe speedups of $\\mathbf{1.4\\times}$ to $\\mathbf{26.11\\times}$ with respect to standard attention, and higher when compared with all methods requiring numerical solutions to ODEs. The computational efficiency gains of RFormer are attained due to the signature transform reducing the length of the timeseries with minimal information loss. The effectiveness of this transformation can be seen from the ablation study carried out in Appendix F. This contrasts with NRDEs [60], which augment NCDEs with local signatures of input data, ", "page_idx": 7}, {"type": "table", "img_path": "gXWmhzeVmh/tmp/6571f53c63f000b87fd0612c91f553861e071554820c24a70bf48eb66dfc8a38.jpg", "table_caption": ["Table 3: Seconds per epoch for all models considered. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "and find that smaller windows often perform better. Fur  \nthermore, NRDEs do not experience the same computational gains as RFormer because they must perform many costly ODE integration steps. ", "page_idx": 7}, {"type": "table", "img_path": "gXWmhzeVmh/tmp/19e144f951d60c2d0d77e18ccf255cf8e43311c29e90c3bb2f5ea11852d13067.jpg", "table_caption": ["Table 4: Dataset processing times for training, validation, and testing phases. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "In Figure 2, we showcase the improvements in computational efficiency of RFormer compared to vanilla Transformers [90], continuous-time Transformers [15], and other continuous-time RNNs [48, 60] when processing sequences from $L=100$ samples up to $L=10\\mathrm{K}$ . As seen, RFormer is significantly more efficient than its continuous-time and vanilla counterparts, even when performing the signature computation online, which involves computing the signatures for each batch during training, resulting in significant redundant computation. When signatures are precomputed just once before training, the computational time of each epoch remains constant across input all sequence lengths including $L\\,=\\,10\\mathrm{K}$ (see the exact signature computation times for different datasets in Table 4). We also stress the fact that RFormer also scales gracefully for extremely long sequences (up to $L=250\\mathrm{K}$ ) with both online and offline computation of the signatures, as shown in Appendix G. Finally, we highlight that ContiFormer has a sample complexity of $O(L^{2}d^{2}S)$ , where $S$ represents the normalized number of function evaluations of the numerical ODE solver, which makes ContiFormer orders of magnitude more computationally intensive when compared to RFormer and prevents the model from running on sequences longer than 500 points without running out of memory (see device details in Appendix C). ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "4.3 Irregular Time Series Classification ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "So far, we mainly focused on the efficiency and inductive bias afforded to the model through the use of signatures. However, a key element of RFormer is that it can naturally deal with irregularly sampled sequences without expensive numerical ODE solvers. This property follows from the fact that signatures are invariant to time reparameterization, see Proposition 3.1. In this subsection, we empirically test this property by training the model on the same datasets but randomly dropping a percentage of the data points. This test intends to find if the model is able to learn continuous-time representations of the original input time-series. The results can be found in Table 5. We find that ", "page_idx": 7}, {"type": "text", "text": "RFormer consistently results in the best performance, with a small performance drop when compared to the full dataset. Importantly, this property is achieved in conjunction with the efficiency gains afforded to the model and without the use of expensive numerical ODE solvers.5 Finally, we perform an additional set of experiments on the 15 univariate classification datasets from the UEA time series classification archive and compare our model with recent state-of-the-art models for irregular time series [64, 82]. Across the board, we find that our model is both faster and more accurate than the continuous-time benchmark despite having a discrete-time Transformer backbone, as shown in Figure 4. For more details and more exhaustive experimentation on random data drops, see Appendix G. ", "page_idx": 8}, {"type": "table", "img_path": "gXWmhzeVmh/tmp/63f66e4057d73da607674ed420627e876d1e13e0fdccde400c8413dcc16c391c.jpg", "table_caption": ["Table 5: Performance of all models under a random $50\\%$ drop in datapoints per epoch. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "5 Reasons for improved model performance ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this final section, we provide explanations for the superior inductive bias of the RFormer model compared to its vanilla Transformer counterpart, despite its lower computational cost. ", "page_idx": 8}, {"type": "text", "text": "5.1 Spatial Processing ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "First, we highlight that a key reason the model achieves significant compression benefits in the tasks considered is its ability to jointly account for temporal and spatial interactions through the self-attention mechanism and signature terms, respectively. In particular, we believe that for certain datasets, the relationships between different channels of the time series may hold more importance than the temporal information itself, which can often be redundant. This is exemplified in the Eigenworms dataset, which experiences a $20\\%$ performance drop when employing univariate signatures, but is able to achieve state-of-the-art performance with a $600\\times$ compression rate in the temporal dimension when signatures are applied across all channels, as shown in Figure 6. To this end, we draw parallels between the use of signatures and the field of temporal graph processing, where the use of the signature over all channels can be seen as a fully connected graph, capturing information from all channels, and the univariate signature would correspond to a graph with only self-connections between the nodes, as depicted in Figure 5. In our view, this hints towards the idea of using sparse graph learning techniques [19, 25] to reduce the explosion in signature terms while retaining the ability to perform effective spatial processing. ", "page_idx": 8}, {"type": "text", "text": "To empirically test these claims, we design a synthetic experiment using a 2-channel time series. Each channel contains a signal of the form $\\sin(\\omega_{i}t+\\nu_{i}),i=1,2$ , where $\\omega_{i}$ and $\\nu_{i}$ are randomly sampled from the interval $[0,2\\pi]$ . For half of the dataset, the last $1\\%$ of temporal samples in the second channel are set to match the frequency of the first channel. The task is to classify whether the samples in this final interval are of the same frequency. As shown in Figure 5, RFormer demonstrates greater sample efficiency and achieves higher test accuracy compared to its vanilla Transformer counterpart, highlighting the effectiveness of signatures in spatial processing. ", "page_idx": 8}, {"type": "image", "img_path": "gXWmhzeVmh/tmp/d85833a68b0178c26f533351678fedbc90986c8048a151a926f6b9e2917e55c0.jpg", "img_caption": ["Figure 4: Average performance of all models on the 15 univariate datasets from the UEA Time Series archive under different degrees of data drop. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "gXWmhzeVmh/tmp/0e0a54625ab730494997df2da1d559fc09cd27f87fc751316ff0b333bd92a1b9.jpg", "img_caption": ["Figure 5: Left: Graph connectivity structures for multivariate, univariate and sparse signature. Middle: Example samples for synthetic task. Right: Performance on spatial synthetic experiment. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "5.2 Sequence Coarsening as an Inductive Bias for Transformers ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In addition to the benefits of higher-order signature terms, we empirically observe that even using level-one signature terms resulted in performance improvements when compared to processing sequences without any transformation. We believe that the reduction in input signal length, achieved without significant information loss through the signature transform is another important factor in the improved inductive bias of RFormer. This finding aligns with the concurrent work of [6], which highlights some of the drawbacks of decoder-only Transformers for long sequences in terms of both oversquashing and representational collapse. ", "page_idx": 9}, {"type": "image", "img_path": "gXWmhzeVmh/tmp/8a39bc91f0d2f6a3109a937b5bbb20504f2d42596d97c781d288abf516f0324c.jpg", "img_caption": ["Figure 6: Left: Dirichlet energy as a function of window size for the Eigenworms dataset. Right: Original and hidden representation after signature layer for two examples in the EW dataset. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "To measure the degree of coarsening in the sequence, we find that interpreting the temporal sequence as a path graph and using ideas from the oversmoothing literature [78] serves as a good way to measure the similarity of the representations being fed to the Transformer. In particular, we compute the Dirichlet Energy [77], defined in this case as $\\begin{array}{r}{\\bar{E}({\\bf X})=\\frac{1}{N}\\sum_{i=1}^{N}||{\\bf X}_{i}-\\bar{{\\bf X}_{i-1}}||_{2}}\\end{array}$ of the temporal sequence resulting from taking increasing window sizes of the global signature. An example of this is shown in Figure 6 for the Eigenworms dataset, where we compared different numbers of windows (from 2 to 18k). Interestingly, we found that the \"elbow\" of the Dirichlet energy corresponded to 30 windows in this dataset, which we found empirically to be one of the the most performant settings. This hints at the idea of the Dirichlet energy being used for signature hyperparameter tuning as well. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We introduced the Rough Transformer, a variant of the original Transformer that allows the processing of discrete-time series as continuous-time signals through the use of multi-view signature attention. Empirical comparisons showed that Rough Transformers outperform vanilla Transformers and continuous-time models on a variety of time-series tasks and are robust to the sampling rate of the signal. Finally, we showed that RFormer provides significant speedups in training time compared to regular attention and ODE-based methods, without the need for major architectural modifications or sparsity constraints. ", "page_idx": 9}, {"type": "text", "text": "Impact Statement ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work is unlikely to result in any harmful societal repercussions. Its primary potential lies in its ability to enhance and advance existing data modelling and machine learning methods. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We thank Christopher Salvi, Antonio Orvieto, Yannick Limmer, and Benjamin Walker for discussions at different stages of the project. AA acknowledges support from the Rafael Del Pino Foundation. XD acknowledges support from the Oxford-Man Institute of Quantitative Finance. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] M. Arjovsky, A. Shah, and Y. Bengio. Unitary evolution recurrent neural networks. In International conference on machine learning, pages 1120\u20131128. PMLR, 2016.   \n[2] I. P. Arribas. Derivatives pricing using signature payoffs. arXiv preprint arXiv:1809.09466, 2018.   \n[3] A. Arroyo, B. Scalzo, L. Stankovic\u00b4, and D. P. Mandic. Dynamic portfolio cuts: A spectral approach to graph-theoretic diversification. In ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 5468\u20135472. IEEE, 2022.   \n[4] A. Arroyo, A. Cartea, F. Moreno-Pino, and S. Zohren. Deep attentive survival analysis in limit order books: Estimating fill probabilities with convolutional-transformers. Quantitative Finance, pages 1\u201323, 2024.   \n[5] A. Bagnall, H. A. Dau, J. Lines, M. Flynn, J. Large, A. Bostrom, P. Southam, and E. Keogh. The uea multivariate time series classification archive, 2018. arXiv preprint arXiv:1811.00075, 2018. [6] F. Barbero, A. Banino, S. Kapturowski, D. Kumaran, J. G. Ara\u00fajo, A. Vitvitskyi, R. Pascanu, and P. Veli\u02c7ckovi\u00b4c. Transformers need glasses! information over-squashing in language tasks. arXiv preprint arXiv:2406.04267, 2024.   \n[7] I. Beltagy, M. E. Peters, and A. Cohan. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020.   \n[8] M. Bilo\u0161, J. Sommer, S. S. Rangapuram, T. Januschowski, and S. G\u00fcnnemann. Neural flows: Efficient alternative to neural odes. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P. Liang, and J. W. Vaughan, editors, Advances in Neural Information Processing Systems, volume 34, pages 21325\u201321337. Curran Associates, Inc., 2021. URL https://proceedings.neurips.cc/ paper_files/paper/2021/file/b21f9f98829dea9a48fd8aaddc1f159d-Paper.pdf. [9] S. Calvo-Ordonez, J. Huang, L. Zhang, G. Yang, C.-B. Schonlieb, and A. I. Aviles-Rivero. Beyond u: Making diffusion models faster & lighter. arXiv preprint arXiv:2310.20092, 2023.   \n[10] \u00c1. Cartea, S. Jaimungal, and J. Penalva. Algorithmic and high-frequency trading. Cambridge University Press, 2015.   \n[11] \u00c1. Cartea, G. Duran-Martin, and L. S\u00e1nchez-Betancourt. Detecting toxic flow. arXiv preprint arXiv:2312.05827, 2023.   \n[12] B. Chang, M. Chen, E. Haber, and E. H. Chi. Antisymmetricrnn: A dynamical system view on recurrent neural networks. In International Conference on Learning Representations, 2018.   \n[13] P. Chang, G. Dur\u00e0n-Mart\u00edn, A. Y. Shestopaloff, M. Jones, and K. Murphy. Low-rank extended kalman filtering for online learning of neural networks from streaming data. arXiv preprint arXiv:2305.19535, 2023.   \n[14] R. T. Chen, Y. Rubanova, J. Bettencourt, and D. K. Duvenaud. Neural ordinary differential equations. Advances in neural information processing systems, 31, 2018.   \n[15] Y. Chen, K. Ren, Y. Wang, Y. Fang, W. Sun, and D. Li. Contiformer: Continuous-time transformer for irregular time series modeling. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \n[16] R. Child, S. Gray, A. Radford, and I. Sutskever. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509, 2019.   \n[17] K. Cho, B. Van Merri\u00ebnboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk, and Y. Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078, 2014.   \n[18] K. M. Choromanski, V. Likhosherstov, D. Dohan, X. Song, A. Gane, T. Sarlos, P. Hawkins, J. Q. Davis, A. Mohiuddin, L. Kaiser, et al. Rethinking attention with performers. In International Conference on Learning Representations, 2020.   \n[19] A. Cini, I. Marisca, D. Zambon, and C. Alippi. Taming local effects in graph-based spatiotemporal forecasting. Advances in Neural Information Processing Systems, 36, 2024.   \n[20] N. M. Cirone, A. Orvieto, B. Walker, C. Salvi, and T. Lyons. Theoretical foundations of deep selective state-space models. arXiv preprint arXiv:2402.19047, 2024.   \n[21] E. M. Compagnoni, A. Scampicchio, L. Biggio, A. Orvieto, T. Hofmann, and J. Teichmann. On the effectiveness of randomized signatures as reservoir for learning rough dynamics. In 2023 International Joint Conference on Neural Networks (IJCNN), pages 1\u20138. IEEE, 2023.   \n[22] F. Corsi. A simple approximate long-memory model of realized volatility. Journal of Financial Econometrics, 7(2):174\u2013196, 2009.   \n[23] C. Cuchiero, L. Gonon, L. Grigoryeva, J.-P. Ortega, and J. Teichmann. Discrete-time signatures and randomness in reservoir computing. IEEE Transactions on Neural Networks and Learning Systems, 33(11):6321\u20136330, 2021.   \n[24] T. Dao, D. Fu, S. Ermon, A. Rudra, and C. R\u00e9. Flashattention: Fast and memory-efficient exact attention with io-awareness. Advances in Neural Information Processing Systems, 35: 16344\u201316359, 2022.   \n[25] H. S. de Oc\u00e1riz Borde, A. Arroyo, and I. Posner. Projections of model spaces for latent graph inference. In ICLR 2023 Workshop on Physics for Machine Learning, 2023.   \n[26] E. Dupont, A. Doucet, and Y. W. Teh. Augmented neural odes. Advances in neural information processing systems, 32, 2019.   \n[27] N. B. Erichson, O. Azencot, A. Queiruga, L. Hodgkinson, and M. W. Mahoney. Lipschitz recurrent neural networks. In International Conference on Learning Representations, 2020.   \n[28] A. Feng, I. Li, Y. Jiang, and R. Ying. Diffuser: efficient transformers with multi-hop attention diffusion for long sequences. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 12772\u201312780, 2023.   \n[29] A. Fermanian. Embedding and learning with signatures. Computational Statistics & Data Analysis, 157:107148, 2021.   \n[30] C. Fleming, D. Sheldon, W. Fagan, P. Leimgruber, T. Mueller, D. Nandintsetseg, M. Noonan, K. Olson, E. Setyawan, A. Sianipar, et al. Correcting for missing and irregular data in homerange estimation. Ecological Applications, 28(4):1003\u20131010, 2018.   \n[31] E. Fons, A. Sztrajman, Y. El-Laham, A. Iosifidis, and S. Vyetrenko. Hypertime: Implicit neural representation for time series. arXiv preprint arXiv:2208.05836, 2022.   \n[32] K.-i. Funahashi and Y. Nakamura. Approximation of dynamical systems by continuous time recurrent neural networks. Neural networks, 6(6):801\u2013806, 1993.   \n[33] A. Gu and T. Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023.   \n[34] A. Gu, K. Goel, and C. Re. Efficiently modeling long sequences with structured state spaces. In International Conference on Learning Representations, 2021.   \n[35] B. Hambly and T. Lyons. Uniqueness for the signature of a path of bounded variation and the reduced path group. Annals of Mathematics, pages 109\u2013167, 2010.   \n[36] R. Hasani, M. Lechner, A. Amini, D. Rus, and R. Grosu. Liquid time-constant networks. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 7657\u20137666, 2021.   \n[37] J. M. Hausdorff and C.-K. Peng. Multiscaled randomness: A possible source of 1/f noise in biology. Physical review E, 54(2):2154, 1996.   \n[38] N. Hautsch. Modelling irregularly spaced financial data: theory and practice of dynamic duration models. Springer Science & Business Media, 2004.   \n[39] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013 778, 2016.   \n[40] M. Henaff, A. Szlam, and Y. LeCun. Recurrent orthogonal networks and long-memory tasks. In International Conference on Machine Learning, pages 2034\u20132042. PMLR, 2016.   \n[41] M. H\u00f6glund, E. Ferrucci, C. Hern\u00e1ndez, A. M. Gonzalez, C. Salvi, L. S\u00e1nchez-Betancourt, and Y. Zhang. A neural rde approach for continuous-time non-markovian stochastic control problems. In ICML Workshop on New Frontiers in Learning, Control, and Dynamical Systems, 2023.   \n[42] S. I. Holt, Z. Qian, and M. van der Schaar. Neural laplace: Learning diverse classes of differential equations in the laplace domain. In International Conference on Machine Learning, pages 8811\u20138832. PMLR, 2022.   \n[43] R. Huang and T. Polak. Lobster: Limit order book reconstruction system. Available at SSRN 1977207, 2011.   \n[44] A. Katharopoulos, A. Vyas, N. Pappas, and F. Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention. In International conference on machine learning, pages 5156\u20135165. PMLR, 2020.   \n[45] T. A. Keller, L. Muller, T. Sejnowski, and M. Welling. Traveling waves encode the recent past and enhance sequence learning. In The Twelfth International Conference on Learning Representations, 2023.   \n[46] P. Kidger and T. Lyons. Signatory: differentiable computations of the signature and logsignature transforms, on both cpu and gpu. arXiv preprint arXiv:2001.00706, 2020.   \n[47] P. Kidger, P. Bonnier, I. Perez Arribas, C. Salvi, and T. Lyons. Deep signature transforms. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch\u00e9-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper_files/paper/2019/file/ d2cdf047a6674cef251d56544a3cf029-Paper.pdf.   \n[48] P. Kidger, J. Morrill, J. Foster, and T. Lyons. Neural controlled differential equations for irregular time series. Advances in Neural Information Processing Systems, 33:6696\u20136707, 2020.   \n[49] M. Lechner and R. Hasani. Learning long-term dependencies in irregularly-sampled time series. arXiv preprint arXiv:2006.04418, 2020.   \n[50] M. Lemercier, C. Salvi, T. Cass, E. V. Bonilla, T. Damoulas, and T. J. Lyons. Siggpde: Scaling sparse gaussian processes on sequential data. In International Conference on Machine Learning, pages 6233\u20136242. PMLR, 2021.   \n[51] M. Lezcano-Casado and D. Mart\u0131nez-Rubio. Cheap orthogonal constraints in neural networks: A simple parametrization of the orthogonal and unitary group. In International Conference on Machine Learning, pages 3794\u20133803. PMLR, 2019.   \n[52] S. Li, X. Jin, Y. Xuan, X. Zhou, W. Chen, Y.-X. Wang, and X. Yan. Enhancing the locality and breaking the memory bottleneck of transformer on time series forecasting. Advances in neural information processing systems, 32, 2019.   \n[53] Z. Li, N. B. Kovachki, K. Azizzadenesheli, K. Bhattacharya, A. Stuart, A. Anandkumar, et al. Fourier neural operator for parametric partial differential equations. In International Conference on Learning Representations, 2020.   \n[54] T. J. Lyons, M. Caruana, and T. L\u00e9vy. Differential equations driven by rough paths. Springer, 2007.   \n[55] V. Melnychuk, D. Frauen, and S. Feuerriegel. Causal transformer for estimating counterfactual outcomes. In International Conference on Machine Learning, pages 15293\u201315329. PMLR, 2022.   \n[56] M. Morariu-Patrichi and M. S. Pakkanen. State-dependent hawkes processes and their application to limit order book modelling. Quantitative Finance, 22(3):563\u2013583, 2022.   \n[57] F. Moreno-Pino and S. Zohren. Deepvol: Volatility forecasting from high-frequency data with dilated causal convolutions. arXiv preprint arXiv:2210.04797, 2022.   \n[58] F. Moreno-Pino, P. M. Olmos, and A. Art\u00e9s-Rodr\u00edguez. Deep autoregressive models with spectral attention. Pattern Recognition, 133:109014, 2023.   \n[59] F. Moreno-Pino, \u00c1. Arroyo, H. Waldon, X. Dong, and \u00c1. Cartea. Rough transformers for continuous and efficient time-series modelling. arXiv preprint arXiv:2403.10288, 2024.   \n[60] J. Morrill, C. Salvi, P. Kidger, and J. Foster. Neural rough differential equations for long time series. In International Conference on Machine Learning, pages 7829\u20137838. PMLR, 2021.   \n[61] T. Nguyen and A. Grover. Transformer neural processes: Uncertainty-aware meta learning via sequence modeling. In International Conference on Machine Learning, pages 16569\u201316594. PMLR, 2022.   \n[62] A. Norcliffe, C. Bodnar, B. Day, J. Moss, and P. Li\u00f2. Neural ode processes. In International Conference on Learning Representations, 2020.   \n[63] A. Norcliffe, C. Bodnar, B. Day, N. Simidjievski, and P. Li\u00f2. On second order behaviour in augmented neural odes. Advances in neural information processing systems, 33:5911\u20135921, 2020.   \n[64] Y. Oh, D. Lim, and S. Kim. Stable neural stochastic differential equations in analyzing irregular time series data. In The Twelfth International Conference on Learning Representations.   \n[65] A. Orvieto, S. L. Smith, A. Gu, A. Fernando, C. Gulcehre, R. Pascanu, and S. De. Resurrecting recurrent neural networks for long sequences. In International Conference on Machine Learning, pages 26670\u201326698. PMLR, 2023.   \n[66] Y. Park, J. Choi, C. Yoon, M. Kang, et al. Learning pde solution operator for continuous modeling of time-series. arXiv preprint arXiv:2302.00854, 2023.   \n[67] I. Perez Arribas, G. M. Goodwin, J. R. Geddes, T. Lyons, and K. E. Saunders. A signature-based machine learning model for distinguishing bipolar disorder and borderline personality disorder. Translational psychiatry, 8(1):274, 2018.   \n[68] S. Perveen, M. Shahbaz, T. Saba, K. Keshavjee, A. Rehman, and A. Guergachi. Handling irregularly sampled longitudinal data and prognostic modeling of diabetes using machine learning technique. IEEE Access, 8:21875\u201321885, 2020.   \n[69] R. Ratcliff, P. L. Smith, S. D. Brown, and G. McKoon. Diffusion decision model: Current issues and history. Trends in cognitive sciences, 20(4):260\u2013281, 2016.   \n[70] J. Reizenstein. Calculation of iterated-integral signatures and log signatures. arXiv preprint arXiv:1712.02757, 2017.   \n[71] J. Reizenstein and B. Graham. The iisignature library: efficient calculation of iterated-integral signatures and log signatures. arXiv preprint arXiv:1802.08252, 2018.   \n[72] D. W. Romero, A. Kuzina, E. J. Bekkers, J. M. Tomczak, and M. Hoogendoorn. Ckconv: Continuous kernel convolution for sequential data. In International Conference on Learning Representations.   \n[73] Y. Rubanova, R. T. Chen, and D. K. Duvenaud. Latent ordinary differential equations for irregularly-sampled time series. Advances in neural information processing systems, 32, 2019.   \n[74] T. K. Rusch and S. Mishra. Coupled oscillatory recurrent neural network (cornn): An accurate and (gradient) stable architecture for learning long time dependencies. In International Conference on Learning Representations, 2020.   \n[75] T. K. Rusch and S. Mishra. Unicornn: A recurrent model for learning very long time dependencies. In International Conference on Machine Learning, pages 9168\u20139178. PMLR, 2021.   \n[76] T. K. Rusch, S. Mishra, N. B. Erichson, and M. W. Mahoney. Long expressive memory for sequence modeling. In International Conference on Learning Representations, 2021.   \n[77] T. K. Rusch, B. Chamberlain, J. Rowbottom, S. Mishra, and M. Bronstein. Graph-coupled oscillator networks. In International Conference on Machine Learning, pages 18888\u201318909. PMLR, 2022.   \n[78] T. K. Rusch, M. M. Bronstein, and S. Mishra. A survey on oversmoothing in graph neural networks. arXiv preprint arXiv:2303.10993, 2023.   \n[79] H. S\u00e1ez de Oc\u00e1riz Borde, A. Arroyo, I. Morales, I. Posner, and X. Dong. Neural latent geometry search: product manifold inference via gromov-hausdorff-informed bayesian optimization. Advances in Neural Information Processing Systems, 36, 2024.   \n[80] C. Salvi, M. Lemercier, C. Liu, B. Horvath, T. Damoulas, and T. Lyons. Higher order kernel mean embeddings to capture flitrations of stochastic processes. Advances in Neural Information Processing Systems, 34:16635\u201316647, 2021.   \n[81] B. Scalzo, A. Arroyo, L. Stankovic\u00b4, and D. P. Mandic. Nonstationary portfolios: Diversification in the spectral domain. In ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 5155\u20135159. IEEE, 2021.   \n[82] M. Schirmer, M. Eltayeb, S. Lessmann, and M. Rudolph. Modeling irregular time series with continuous recurrent units. In International conference on machine learning, pages 19388\u2013 19405. PMLR, 2022.   \n[83] N. Seedat, F. Imrie, A. Bellot, Z. Qian, and M. van der Schaar. Continuous-time modeling of counterfactual outcomes using neural controlled differential equations. In International Conference on Machine Learning, pages 19497\u201319521. PMLR, 2022.   \n[84] V. Sitzmann, J. Martel, A. Bergman, D. Lindell, and G. Wetzstein. Implicit neural representations with periodic activation functions. Advances in neural information processing systems, 33: 7462\u20137473, 2020.   \n[85] J. T. Smith, A. Warrington, and S. Linderman. Simplified state space layers for sequence modeling. In The Eleventh International Conference on Learning Representations.   \n[86] C. Tallec and Y. Ollivier. Can recurrent neural networks warp time? arXiv preprint arXiv:1804.11188, 2018.   \n[87] C. W. Tan, C. Bergmeir, F. Petitjean, and G. I. Webb. Monash university, uea, ucr time series extrinsic regression archive. arXiv preprint arXiv:2006.10996, 2020.   \n[88] A. Tong, T. Nguyen-Tang, D. Lee, T. M. Tran, and J. Choi. Sigformer: Signature transformers for deep hedging. In Proceedings of the Fourth ACM International Conference on AI in Finance, pages 124\u2013132, 2023.   \n[89] A. Vahid, M. M\u00fcckschel, S. Stober, A.-K. Stock, and C. Beste. Applying deep learning to singletrial eeg data provides evidence for complementary theories on action control. Communications biology, 3(1):112, 2020.   \n[90] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, \u0141. Kaiser, and I. Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.   \n[91] B. Walker, A. D. McLeod, T. Qin, Y. Cheng, H. Li, and T. Lyons. Log neural controlled differential equations: The lie brackets make a difference. arXiv preprint arXiv:2402.18512, 2024.   \n[92] S. Wang, B. Z. Li, M. Khabsa, H. Fang, and H. Ma. Linformer: Self-attention with linear complexity. arXiv preprint arXiv:2006.04768, 2020.   \n[93] J. Yoon, D. Jarrett, and M. Van der Schaar. Time-series generative adversarial networks. Advances in neural information processing systems, 32, 2019.   \n[94] M. Zaheer, G. Guruganesh, K. A. Dubey, J. Ainslie, C. Alberti, S. Ontanon, P. Pham, A. Ravula, Q. Wang, L. Yang, et al. Big bird: Transformers for longer sequences. Advances in neural information processing systems, 33:17283\u201317297, 2020.   \n[95] A. Zeng, M. Chen, L. Zhang, and Q. Xu. Are transformers effective for time series forecasting? In Proceedings of the AAAI conference on artificial intelligence, volume 37, pages 11121\u201311128, 2023. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "A Properties of Path Signatures ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "First, we recall that the path is uniquely determined by its signature, which motivates its use as a feature map. ", "page_idx": 16}, {"type": "text", "text": "Proposition A.1. Given a path $\\widehat{X}\\ :\\ [0,T]\\ \\rightarrow\\ \\mathbb{R}^{d}$ , then the map $P\\,:\\,[0,T]\\,\\,\\rightarrow\\,\\,\\mathbb{R}^{1+d}$ where $P(t)=(t,\\widehat{X}(t))$ is uniquely determined by it\u2019s signature $S(P)_{0,T}$ . ", "page_idx": 16}, {"type": "text", "text": "The proof can be found in Hambly and Lyons [35]. ", "page_idx": 16}, {"type": "text", "text": "For Rough Transformers, several features of path signatures are important. First, linear functionals on path signatures possess universal approximation properties for continuous functionals. ", "page_idx": 16}, {"type": "text", "text": "Theorem A.2. Fix $T>0$ , and let $K\\subset C_{b}^{1}([0,T];\\mathbb{R}^{d})$ . Let $f:K\\rightarrow\\mathbb{R}$ be continuous with respect to the sup-norm topology on $C_{b}^{1}([0,T];\\mathbb{R}^{d})$ . Then for any $\\epsilon>0$ , there exists a linear functional $\\ell$ such that ", "page_idx": 16}, {"type": "equation", "text": "$$\n|f(\\overline{{X}})-\\langle\\ell,S(\\overline{{X}})_{0,T}\\rangle|\\leq\\epsilon\\,,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "for any ${\\widehat{X}}\\in K$ , where $\\overline{{X}}$ denotes the time-added augmentation of $\\widehat{X}$ . ", "page_idx": 16}, {"type": "text", "text": "For a proof of A.2, see Arribas [2]. Even though Theorem A.2 guarantees that linear functionals are sufficient for universal approximation, linear models are not always sufficient in practice. This motivates the development of nonlinear models built upon the path signature which efficiently extract path behavior. ", "page_idx": 16}, {"type": "text", "text": "The second feature is that the terms of the path signature decay factorially, as described by the following proposition. ", "page_idx": 16}, {"type": "text", "text": "Proposition A.3. Given $\\widehat{X}\\in C_{b}^{1}([0,T];\\mathbb{R}^{d})$ , for any $s,t\\in[0,T]$ , we have that for any $I\\in{\\mathcal{T}}_{d}^{n}$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n|S(\\widehat{X})_{0,T}^{I}|=O\\left(1/n!\\right)\\,.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "For a proof of Proposition A.3, see [54]. Hence, the number of terms in the signature grows exponentially in the level of the signature, but the tail of the signature is well-behaved, so only a few levels in a truncated signature are necessary to adequately approximate continuous functionals. ", "page_idx": 16}, {"type": "text", "text": "A.1 Signatures of Piecewise Linear Paths. ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In the Rough Transformer, we use linear interpolation of input time-series to get a continuous-time representation of the data. As mentioned in Section 3, the signature computation in this case is particularly simple. ", "page_idx": 16}, {"type": "text", "text": "Suppose $\\widehat{X}_{k}:[t_{k},t_{k+1}]\\rightarrow\\mathbb{R}^{d}$ is a linear interpolation between two points $X_{k},X_{k+1}\\in\\mathbb{R}^{d}$ . That is, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\widehat{X}_{k}(t)=X_{k}+\\frac{t-t_{k}}{t_{k+1}-t_{k}}\\left(X_{k+1}-X_{k}\\right)\\,.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Then the signature of $\\widehat{X}_{k}$ is given explicitly by ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\Im(\\widehat{X}_{k})_{t_{k},t_{k+1}}=\\bigg(1,X_{k+1}-X_{k},\\frac{1}{2}(X_{k+1}-X_{k})^{\\otimes2},\\frac{1}{3!}(X_{k+1}-X_{k})^{\\otimes3},...,\\frac{1}{n!}(X_{k+1}-X_{k})^{\\otimes n},...\\bigg)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\otimes$ denotes the tensor product. Let $\\widehat{X}_{k}*\\widehat{X}_{k+1}$ denote the concatenation of ${\\widehat{X}}_{k}$ and $\\widehat{X}_{k+1}$ . That is, $\\widehat{X}_{k}*\\widehat{X}_{k+1}:[t_{k},t_{k+2}]\\rightarrow\\mathbb{R}^{d}$ is given by ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\widehat{X}_{k}*\\widehat{X}_{k+1}(t)=\\left\\{\\widehat{X}_{k}(t)\\begin{array}{r l}{t\\in[t_{k},t_{k+1}]}\\\\ {t\\in(t_{2},t_{k+2}]\\,.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The signature of the concatenation $\\widehat{X}_{k}*\\widehat{X}_{k+1}$ is given by Chen\u2019s relation, whose proof is in [54]. To state this result, we first note that $S(\\widehat{X})_{s,t}^{n}$ can be interpreted as an element of the extended tensor algebra of $\\mathbb{R}^{d}$ : ", "page_idx": 16}, {"type": "equation", "text": "$$\nT((\\mathbb{R}^{d}))=\\left\\{(a_{0},...,a_{n},...):a_{n}\\in\\mathbb{R}^{d\\otimes n}\\right\\}\\,.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proposition A.4 (Chen\u2019s Relation). The following identity holds: ", "page_idx": 17}, {"type": "equation", "text": "$$\nS(\\widehat{X}_{k}*\\widehat{X}_{k+1})_{t_{k},t_{k+2}}=S(\\widehat{X}_{k})_{t_{k},t_{k+1}}\\otimes S(\\widehat{X}_{k+1})_{t_{k+1},t_{k+2}}\\,,\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where for elements $A,B\\in T((\\mathbb{R}^{d}))$ with ${\\cal A}=(A_{0},A_{1},A_{2},...)$ and ${\\cal B}=(B_{0},B_{1},B_{2},\\ldots)$ the tensor product $\\otimes$ is defined ", "page_idx": 17}, {"type": "equation", "text": "$$\nA\\otimes B=\\left(\\sum_{j=0}^{k}A_{j}\\otimes B_{k-j}\\right)_{k\\geq0}\\,.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Let $\\mathbf{X}=(X_{0},...,X_{L})$ be a time-series. Then the linear interpolation $\\tilde{X}\\,:\\,[0,T]\\,\\rightarrow\\,\\mathbb{R}^{d}$ can be represented as the concatenation of a finite number of linear paths: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\tilde{X}=\\widehat{X}_{0}\\ast\\cdot\\cdot\\cdot\\ast\\widehat{X}_{L-1}\\,.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Hence, the signature is ", "page_idx": 17}, {"type": "equation", "text": "$$\nS(\\tilde{X})_{0,T}=S(\\widehat{X}_{0})_{0,t_{1}}\\otimes\\cdots\\otimes S(\\widehat{X}_{L-1})_{t_{L-1},T}\\,.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "B Related Work, Experimental Choices, and Impact Statement ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Continuous-time models. Since their introduction in [14], Neural ODEs were extended in various ways to facilitate modelling continuous time-series data [73, 63, 36, 42, 83]. While Neural ODEs and their extensions are successful in certain tasks they are burdened with a high computational cost, which makes them scale very poorly to long sequences in the time-series setting. Various authors propose methods and augmentations to vanilla Neural ODEs to decrease their computational overhead [26, 8]. Other approaches to augmenting deep learning methods to modelling continuous data include implicit neural representations [84, 31], continuous kernel convolutions [72], or Fourier neural operators [53, 66]. ", "page_idx": 17}, {"type": "text", "text": "Transformers. First proposed in [90], the Transformer has been exceptionally successful in discrete sequence modelling tasks such as natural language processing (NLP). Key to the success of the Transformer in NLP is the attention mechanism, which extracts long-range dependencies. There are a number of extensions to improve efficiency and decrease the computation cost of the attention mechanism [52, 92, 24, 44, 18]. ", "page_idx": 17}, {"type": "text", "text": "Signatures in machine learning. The path signature originates from theoretical stochastic analysis [54] and has since become a popular tool in machine learning. Path signatures are regarded as effective feature transformations for sequential data [67, 29, 47]. Additionally, signatures help mitigate the computational cost of Neural CDEs in long time-series [60] and non-Markovian stochastic control problems [41]. Other more recent works in this direction include [20, 91]. Approaches such as randomized signatures [23, 21] and the signature kernel [50, 80] have been developed to mitigate the curse of dimensionality inherent in path signature computations. Rough Transformers provide a first step towards incorporating path signatures for continuous-time sequence modelling using Transformers. 6 ", "page_idx": 17}, {"type": "text", "text": "We also note that contemporary work [88] employs a Transformer architecture with signature features for the task of deep hedging. However, our work differs in several key aspects. First, we introduce the multi-view attention mechanism, which uses signatures to extract both global and local information, which we found to be necessary in our experimentation, as Transformers are known to struggle in extracting local information (see Figure 7), whereas their work just uses a global signature. Moreover, their work computes the signature at every time step, strictly dilating input data. This is particularly problematic for long, multi-variate sequences, for reasons discussed above, and can actually negatively impact performance. Our work, however, compresses data using the multi-view signature transform, and we find that this compressed representation can actually improve performance. Finally, their work relies on the assumption that data is regularly sampled, as the signature is computed at every time step, in contrast to our work which is robust to irregular sampling. ", "page_idx": 17}, {"type": "text", "text": "Long-Range Sequence modelling. A highly relevant line of research related to enhancing recurrent neural networks\u2019 capability to capture long-term dependencies involves the development of various models. These include Unitary RNNs [1], Orthogonal RNNs [40], expRNNs [51], chronoLSTM [86], antisymmetric RNNs [12], Lipschitz RNNs [27], coRNNs [74], unicoRNNs [75], LEMs [76], waveRNN [45], Linear Recurrent Units [65], and Structured State Space Models [34, 33]. While we utilize many benchmarks and synthetic tasks from these works to test our model, it is important to note that our work is not intended to compete with the state-of-the-art in these tasks. Therefore, we do not directly compare our model with the models mentioned above. Instead, this paper seeks to show that the baseline Transformer architecture can benefit from the use of signatures by (i) becoming more computationally efficient, (ii) being invariant to the sampling rate of the signal, and (iii) having a good inductive bias for temporal and spatial processing. Furthermore, we highlight that RFormer brings alternative benefits, such as the ability to perform spatial processing effectively, which is a setting in which long-range sequence models typically struggle. ", "page_idx": 18}, {"type": "text", "text": "Efficient Attention Variants. There are several efficient self-attention variants that have emerged over the years, including Sparse Transformer [16], Longformer [7], Linear Transformers [44], BigBird [94], Performer [18], or Diffuser [28]. In our setting, we highlight that a central part of this paper is to showcase how signatures significantly reduce the computational requirements of vanilla attention and empirically demonstrate that this also results in improved learning dynamics and invariance to the sampling frequency of the signal. Given the large efficiency gains that we observed with this approach when employed on vanilla attention, we did not consider that further experimentation on other forms of \u201capproximate\u201d attention was needed. Since most variants of attention seek to make the operation more efficient through several approximations (e.g., linearization or sparsification techniques), we believe that a first attempt at showcasing the power of multi-view signatures on vanilla attention is already significant. However, other variants of attention (such as the ones outlined before) could be added on top of the signature representations to obtain even better efficiency gains. ", "page_idx": 18}, {"type": "text", "text": "Limitations and Future Work. While we found RFormer to be very performant in our experiments, much of this performance gain relies on heavy hyperparameter tuning, especially when it comes to the choice of window sizes and signature level. However, this could be handled using Neural Architecture Search (NAS) techniques, such as those employed in [79]. Furthermore, despite the computational gains we achieve for low-dimensional sequences, additional work would be required to scale this method to larger dimensions. We should also note that the experiments and results presented in this paper are constrained by the relatively small scale of the models studied. ", "page_idx": 18}, {"type": "text", "text": "C Experimental Details ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "All experiments are conducted on an NVIDIA GeForce RTX 3090 GPU with 24,564 MiB of memory, utilizing CUDA version 12.3. Hyperparameters used to produce the results in Table 2 are reported in Tables 6. The timings presented in all tables are obtained by executing each model independently for each dataset and averaging the resulting times across 100 epochs. ", "page_idx": 18}, {"type": "table", "img_path": "gXWmhzeVmh/tmp/f4a787984e734fdd75db528e5d6f3902b6d662718fb42a4d9c167d3d85f78da2.jpg", "table_caption": ["Table 6: Hyperparameters used for Table 2, where G and L refer to the Global and Local signature components, respectively. "], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "gXWmhzeVmh/tmp/0f16964f6b7ba3a329a67c3e5020e1c84a78e55534eaec91a2b37e620b2cf074.jpg", "table_caption": ["Table 7: Hyperparameters validation on remaining datasets. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "To prevent excessive growth in signature terms, we use the univariate signature in LOB datasets. As an alternative, one could employ randomized signatures [21] or low-rank approximations [11, 13] . ", "page_idx": 19}, {"type": "text", "text": "D Baselines Validation ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "This section collects the validation of Step and Depth for the Neural-RDE model. Optimal values are selected for evaluation on test-set. Early-stopping is used with the same criteria as [60]. ", "page_idx": 19}, {"type": "table", "img_path": "gXWmhzeVmh/tmp/dcb6c6c84437d7d91d791654c515df5a718d9136990b2c990fa7437b2c4b1593.jpg", "table_caption": ["Table 8: Validation accuracy on the sinusoidal dataset. "], "table_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "gXWmhzeVmh/tmp/b89faadbba482e494f636b3b757ddce08f971c67324eb2ad93c691073c098361.jpg", "table_caption": ["Table 9: Validation accuracy on the long sinusoidal dataset. "], "table_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "gXWmhzeVmh/tmp/e84fe1a99c34fc49f1620b30489964e038d51bda779fb69a685583ab7b0a85d7.jpg", "table_caption": ["Table 10: Validation accuracy on the EW dataset. "], "table_footnote": [], "page_idx": 20}, {"type": "table", "img_path": "gXWmhzeVmh/tmp/7cdad9cbc355ae5c7e451e6e12a0e2ef895473639cdb6d4c45efc1a172507383.jpg", "table_caption": ["Table 11: Validation loss on the HR dataset. "], "table_footnote": [], "page_idx": 20}, {"type": "table", "img_path": "gXWmhzeVmh/tmp/a3089682ba1cbbdb1f47787a17f478280b3635e38343529bcce0917a0871b594.jpg", "table_caption": ["Table 12: Validation loss on the LOB dataset (1K), included as an additional experiment in Appendix G.4. "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "E Long Temporal Datasets Details ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Table 13 summarises the long temporal modeling datasets from the UEA time series classification archive [5] used in Section 4. ", "page_idx": 20}, {"type": "table", "img_path": "gXWmhzeVmh/tmp/dca1fd28e17ac8bdf9b4f06f631185d320d5667b9e4fd302c56e921e5285b9dc.jpg", "table_caption": ["Table 13: Summary of datasets used in the long time-series classification task. "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "F Ablation Studies ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "F.1 Global and Local Signature Components ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In this section, we ablate the use of the multi-view signature transform over both global and local transformations of the input signal. The results for the sinusoidal datasets are shown in Figure 7. In most cases, the use of both local and global components improves the performance of RFormer. This choice, however, can be seen as a hyperparameter and will be dataset-dependent. ", "page_idx": 21}, {"type": "image", "img_path": "gXWmhzeVmh/tmp/c2ae51ca77ca22ad881768e8f718221fa6af26c112baed9385256f9aa690ff41.jpg", "img_caption": ["Figure 7: Ablation of local and local components of the multi-view signature for the sinusoidal datasets. Left: Sinusoidal dataset. Right: Long Sinusoidal dataset. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "F.2 Signature Level and Naive Downsampling ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "One of the main points of the paper is that the shorter representation of the time-series endowed by the signatures helps to significantly reduce the computational cost of the self-attention operation with minimal information loss (and with improved performance in many of the experiments). By equation (16), one sees that the first level of the signature of a linear function is the difference between its endpoints. Hence, using multi-view attention with signature level one operates on the increments of piecewise-linear interpolated data, which corresponds to naive downsampling. To test that higher levels of the signature provide improvements in performance, we compare the result of using the signature on the datasets tested in Table 14 below. ", "page_idx": 21}, {"type": "table", "img_path": "gXWmhzeVmh/tmp/640d854d402f53e4abb8988a83f7c1b4b41549f58311b6a0dd17eb3145f25881.jpg", "table_caption": ["Table 14: Comparative performance of different methods on datasets. "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "There is a significant performance gain in considering higher levels of the signature because one can capture the higher-order interactions between the different time-series. ", "page_idx": 21}, {"type": "text", "text": "G Additional Experiments and Comparisons ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "G.1 Random Drop Experiments ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Furthermore, we conduct a new set of experiments in which we dropped $30\\%$ and $70\\%$ of the dataset for RFormer. Note that even with a $70\\%$ drop rate in the EigenWorms dataset, the vanilla Transformer fails to run due to memory limitations. Therefore, to provide results for the Transformer model on the EigenWorms dataset, we conduct experiments with an $85\\%$ drop rate. This comparison highlights the performance gap between the vanilla Transformer and our proposed model under these conditions, with the RFormer model yielding superior results. All results are computed across five seeds and are summarized in the tables and figure below. ", "page_idx": 22}, {"type": "table", "img_path": "gXWmhzeVmh/tmp/ce1c6d5f449f4a4a16e344f47dff1573caf8be2956fb8d5fb4b7a852cadea7e8.jpg", "table_caption": ["Table 15: Performance of models under various data drop scenarios for EW dataset. "], "table_footnote": [], "page_idx": 22}, {"type": "table", "img_path": "gXWmhzeVmh/tmp/5751deb5c1c2e9c21f0f90f390dd6d9f95e802537514acdccadd4227f585f372.jpg", "table_caption": ["Table 16: Performance consistency of RFormer under data drop scenarios for HR dataset. "], "table_footnote": [], "page_idx": 22}, {"type": "table", "img_path": "gXWmhzeVmh/tmp/bd4338bca9fd8d676fc9eae04420482caecbe4d7ce0e07d8562c39c2f52347eb.jpg", "table_caption": ["Table 17: Epoch-wise performance under different data drop scenarios for the sinusoidal dataset. "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "Table 18: Epoch-wise performance under different data drop scenarios for the long sinusoidal dataset. ", "page_idx": 22}, {"type": "table", "img_path": "gXWmhzeVmh/tmp/b88d57625be45020caf37e621123a9a6390a8d13f74746e9aad2fc659150f792.jpg", "table_caption": [], "table_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "gXWmhzeVmh/tmp/270a7c6bf54385a3644a77715ce573bf7df9f0392c8a3f0c904eea85638a335c.jpg", "img_caption": ["Figure 8: Test accuracy per epoch for the frequency classification task across three random seeds for sinusoidal datasets with $50\\%$ random drop per epoch. Left: Sinusoidal dataset. Right: Long Sinusoidal dataset. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "Finally, Table 19 compares CRU and RFormer in an irregularly sampled synthetic data setting, featuring shorter sinusoids and fewer classes than the experiments in Section 4.1. Additionally, Table 20 presents the hyperparameter validation for CRU (see Table 21 for training time analysis). These experiments demonstrate that recurrent models perform well with short sequences. Note that despite RFormer\u2019s superior performance, our model is significantly faster than other continuous-time models, as shown in Appendix G.2, particularly in Table 21. ", "page_idx": 23}, {"type": "table", "img_path": "gXWmhzeVmh/tmp/4c0cbba299f97f28e0647c7c5cd2b3d213373b835de7453818e9b8bac3466087.jpg", "table_caption": ["Table 19: Comparison of RFormer and CRU (two best and simplest performing instances [Num.basis/Bandwidth $=20/3]$ ) at different random drop percentages. "], "table_footnote": [], "page_idx": 23}, {"type": "table", "img_path": "gXWmhzeVmh/tmp/fe0854c388d81bbda73c5b8833dd271584a1cbed13109891a54c54719ed53ee7.jpg", "table_caption": ["Table 20: CRU\u2019s hyperparameters $\\mathcal{L}=100\\,\\$ ) (latent state dimension (LSD), number of basis matrices (Num.basis), and their bandwidth). "], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "G.2 Additional Efficiency Experiments and Discussion ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We conduct additional experiments to compare the runtime of Rough Transformers with other models. In this experiment, we use the synthetic sinusoidal dataset considered in our paper and compute the runtime per epoch for varying sequence lengths. We demonstrate results for two variants of RFormer: \u201conline\u201d, which corresponds to computing the signatures of each batch during training (resulting in significant redundant computation), and \u201coffline\u201d, which corresponds to computing the signatures in one go at the beginning of training. We include a recent RNN-based model as a basis for comparison with high-performing RNN baselines. In addition to the models discussed in Section 4, we introduce Continuous Recurrent Units (CRU) [82] as a new baseline. See Table 21 for a summary of the results. ", "page_idx": 23}, {"type": "table", "img_path": "gXWmhzeVmh/tmp/1c5f85031b1e7c50a511f16d6f65b9836ffb47e4ed718fe7a0165c70c34fd785.jpg", "table_caption": ["Table 21: Seconds per epoch for growing input length and for different model types on the sinusoidal dataset. "], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "We remark that previous running times are obtained with a batch size of 10. Further, the ContiFormer model could be run for $L=1000$ if decreasing the batch size to 2 (which significantly affects the parallelization process), avoiding OOM issues and resulting in 4025 seconds/epoch, which is several orders of magnitude larger than RFormer. As an additional experiment, we tested the epoch time (S/E) of RFormer for extremely oversampled sinusoidal time series. We show our results in the table below. ", "page_idx": 24}, {"type": "table", "img_path": "gXWmhzeVmh/tmp/3f7ad3c48aba22afd41106565bdc5052f837cf0bd0e0ab3a5c2ccfe112583011.jpg", "table_caption": ["Table 22: Seconds per epoch for very large input length. "], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "Thus, the time needed to compute the signature is inconsequential when compared with the time required to train standard models on the full or even downsampled datasets, since this step has to be carried out only once. To put this into context with an example, we note that it takes 4s to compute the signature representations for the HR dataset (which is about half the time it takes for the Vanilla Transformer to go through one epoch) and results in a $26\\times$ increase in computational speed for RFormer when compared to the vanilla Transformer. ", "page_idx": 24}, {"type": "table", "img_path": "gXWmhzeVmh/tmp/2b470d55828dc2f9562645ee582ad45e7fa801e2be8e7c6c7b0a97a5d744f419.jpg", "table_caption": ["Table 23: Processing times for different sizes on the sinusoidal dataset. "], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "To showcase that this is the case for not only sequences of moderate length but also extremely long sequences, we also carry out the following experiment where we compute the signature representation for the sine dataset, with a progressively increasing number of datapoints. As seen in Table 23, this does not cause an explosion in computational time. ", "page_idx": 24}, {"type": "image", "img_path": "gXWmhzeVmh/tmp/2181a6d714867894371c87c9b4f775aba806a8a4670cbfea69bc56759569c778.jpg", "img_caption": ["Figure 9: Seconds per epoch for growing input length and for different model types on the sinusoidal dataset for extremely long lengths (up to 250k) Left: Log Scale. Middle: Regular Scale. Right: Log-log scale. When a line stops, it indicates an OOM error. "], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "G.3 Additional ContiFormer Comparisons ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Also, to provide some context of the performance of ContiFormer compared with our method (and not only results on complexity and training times), we run the model on the sinusoidal classification task for signals of length $L=100$ and $L=250$ . Due to the slow running time of the ContiFormer model, we did not consider sequence lengths of $L>250$ . We evaluate the ContiFormer model using one head. However, given the subpar results we obtain, we also test it with four heads, using the hyperparameters originally used in the paper for their irregularly sampled time series classification experiments. By contrast, all variations of RFormer tested in this paper for this experiment employ only one head, but reported significantly better results. ", "page_idx": 25}, {"type": "table", "img_path": "gXWmhzeVmh/tmp/9ca08ad01793e0566cb14174364697d7bc37a46a7e3e3897d40acda89ae00b77.jpg", "table_caption": ["Table 24: Model performance for $L=100$ . "], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "G.4 Additional Experiments ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Time-to-cancellation of limit orders. We use limit order book data provided by LOBSTER [43] and test each model\u2019s capacity to calculate the time-to-cancellation of 1000 limit orders of the AMZN ticker over a single trading day. We employ context windows of 1000 time-steps and 20000 time-steps in the past to make the prediction. Raw limit order book data consist of many uninformative samples [4], so one expects a longer context window would provide more information to make the prediction. However, this cannot be guaranteed given the non-stationarity and low SNR of financial environments [see 3, 4, 81]. Limit order book updates are not regularly sampled, even though they are obtained at a high sampling rate. Since this is a dataset with a higher number of features, we employ the univariate multi-view signature transform to avoid excessive growth with signature level. Table 25 shows that a longer context yields improved performance in this task, highlighting the importance of efficiently processing long time-series in practical applications. RNN-based models (GRU, ODE-RNN) achieve decent performance with both medium and long context windows, despite their problems with vanishing and exploding gradients. In this task, Neural-RDE (see Appendix D for hyperparameters validation details) performs poorly. This is likely because Neural-RDE computes the signature over all channels, which is ineffective in this context. The Transformer and RFormer models report the best performance. Also, the computational time and the performance of the model improve with univariate signatures in this context as well. ", "page_idx": 25}, {"type": "text", "text": "Finally, to assess the model\u2019s ability to understand the dynamics of the time series, we have carried out an additional experiment of forecasting the next-step intraday volatility of an Apple Limit Order Book. The results are shown in Table 26. ", "page_idx": 25}, {"type": "table", "img_path": "gXWmhzeVmh/tmp/33ea1ee0a82a98e86d077783c9e4be7042103f69793d49636c6c58dacaaa5b68.jpg", "table_caption": ["Table 25: Test RMSE (mean $\\pm$ std) and average seconds per epoch (S/E), computed across five seeds on the LOB dataset, on a scale of $10^{-2}$ . "], "table_footnote": [], "page_idx": 26}, {"type": "table", "img_path": "gXWmhzeVmh/tmp/aa4f3bf65355684c00bc2273ea9f63ba898625416717f9ddcd059a581704a2e1.jpg", "table_caption": ["Table 26: RMSE comparison between RFormer, Transformer, and NRDE. "], "table_footnote": [], "page_idx": 26}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: Yes, they are an accurate reflection of the paper\u2019s contributions and scope. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 27}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: Yes, the limitations ae discussed in Appendix B. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 27}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: The paper contains only one theoretical result, and a complete and correct proof is provided. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 28}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: All experimental details are in Appendix C. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 28}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: All datasets used are publicly available and the associated code can be found in the following anonymized repo: https://anonymous.4open.science/r/rformer_ submission-2546. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 29}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: All details are specified in Appendices C and D. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 29}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: All experiments are run with several seeds, and the standard deviation is reported alongside the average results. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: All experiments were conducted on an NVIDIA GeForce RTX 3090 GPU with 24,564 MiB of memory, as outlined in Appendix C. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 30}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We conform with the NeurIPS code of ethics. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 30}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: The societal impact statement is included in Appendix B. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 31}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: This paper does not involve the release of data or models that have a high risk for misuse. However, as mentioned in our impact statement included in Appendix B, we acknowledge potential misuses of our advancements in time series analysis and advocate for ethical application and regulatory oversight. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 31}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Justification: The owners of the datasets and packages used are acknowledged in the code and in the manuscript. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 31}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: This paper does not introduce new assets. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 32}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: This paper does not include any experiments involving crowdsourcing or human subjects. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 32}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: This paper does not include any research involving human subjects. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 32}]