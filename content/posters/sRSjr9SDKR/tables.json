[{"figure_path": "sRSjr9SDKR/tables/tables_7_1.jpg", "caption": "Table 1: Accuracy of the density represented as a flow (flow) compared to a factorized normal distribution (normal), both learned from preferential data, in three metrics: log-likelihood, Wasserstein distance, and the mean marginal total variation (MMTV). Averages over 20 repetitions (but excluding a few crashed runs), with standard deviations.", "description": "This table presents a comparison of the accuracy of representing a belief density using a normalizing flow versus a factorized normal distribution.  The comparison is made across three metrics: log-likelihood, Wasserstein distance, and mean marginal total variation (MMTV).  The results are averaged over 20 independent repetitions, excluding a small number of runs that experienced errors.", "section": "Experiments"}, {"figure_path": "sRSjr9SDKR/tables/tables_9_1.jpg", "caption": "Table 2: Wasserstein distances for varying k across different experiments", "description": "This table presents the Wasserstein distances for four different datasets (Onemoon2D, Gaussian6D, Funnel10D, Twogaussians10D) and four different values of k (k=2, k=3, k=5, k=10).  The Wasserstein distance is a measure of the distance between two probability distributions.  Lower values indicate a better fit between the estimated density and the ground truth density.  The results show that the Wasserstein distance decreases as k increases, indicating that using more alternatives in the pairwise comparisons leads to a better estimation of the underlying density.", "section": "Experiments"}, {"figure_path": "sRSjr9SDKR/tables/tables_16_1.jpg", "caption": "Table 1: Accuracy of the density represented as a flow (flow) compared to a factorized normal distribution (normal), both learned from preferential data, in three metrics: log-likelihood, Wasserstein distance, and the mean marginal total variation (MMTV). Averages over 20 repetitions (but excluding a few crashed runs), with standard deviations.", "description": "This table presents a comparison of the accuracy of representing a probability density using a normalizing flow versus a simpler factorized normal distribution.  The comparison is made using three metrics: log-likelihood (higher is better), Wasserstein distance (lower is better), and mean marginal total variation (MMTV, lower is better). The results are averages over 20 independent runs, excluding a few runs that crashed due to numerical instability. The table shows results on different synthetic datasets with varying dimensionality and complexity, demonstrating the relative performance of the two methods under different conditions.", "section": "Experiments"}, {"figure_path": "sRSjr9SDKR/tables/tables_18_1.jpg", "caption": "Table 1: Accuracy of the density represented as a flow (flow) compared to a factorized normal distribution (normal), both learned from preferential data, in three metrics: log-likelihood, Wasserstein distance, and the mean marginal total variation (MMTV). Averages over 20 repetitions (but excluding a few crashed runs), with standard deviations.", "description": "This table presents the performance comparison between the proposed normalizing flow method and a factorized normal distribution baseline in terms of three evaluation metrics. The metrics are computed for various synthetic datasets and the real-world Abalone dataset. The results highlight the superior performance of the normalizing flow method across all metrics and datasets.", "section": "Experiments"}, {"figure_path": "sRSjr9SDKR/tables/tables_25_1.jpg", "caption": "Table 1: Accuracy of the density represented as a flow (flow) compared to a factorized normal distribution (normal), both learned from preferential data, in three metrics: log-likelihood, Wasserstein distance, and the mean marginal total variation (MMTV). Averages over 20 repetitions (but excluding a few crashed runs), with standard deviations.", "description": "The table presents the performance comparison of two methods (flow-based and factorized normal distribution) in learning probability densities from preferential data.  The comparison uses three metrics: log-likelihood, Wasserstein distance, and mean marginal total variation (MMTV). Results are averaged over 20 runs (excluding those with errors), with standard deviations reported to show the variability.", "section": "Experiments"}, {"figure_path": "sRSjr9SDKR/tables/tables_27_1.jpg", "caption": "Table 1: Accuracy of the density represented as a flow (flow) compared to a factorized normal distribution (normal), both learned from preferential data, in three metrics: log-likelihood, Wasserstein distance, and the mean marginal total variation (MMTV). Averages over 20 repetitions (but excluding a few crashed runs), with standard deviations.", "description": "This table compares the performance of the proposed normalizing flow method against a simpler factorized normal distribution baseline for estimating probability densities from preferential data.  It shows the results for five synthetic datasets (Onemoon2D, Gaussian6D, Twogaussians10D, Twogaussians20D, Funnel10D) and a real-world dataset (Abalone). Three evaluation metrics are used: log-likelihood (higher is better), Wasserstein distance (lower is better), and mean marginal total variation (MMTV, lower is better). The results demonstrate that the flow model generally outperforms the simpler baseline, achieving significantly better log-likelihoods and lower distances, showcasing the advantage of more flexible models when dealing with limited data.", "section": "Experiments"}]