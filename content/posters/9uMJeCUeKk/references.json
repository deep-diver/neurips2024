{"references": [{"fullname_first_author": "Junnan Li", "paper_title": "BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation", "publication_date": "2022-07-15", "reason": "This paper introduces BLIP, a foundational model for unified vision-language understanding and generation, significantly impacting the field and directly relevant to the current research on image-to-text models."}, {"fullname_first_author": "Junnan Li", "paper_title": "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models", "publication_date": "2023-01-26", "reason": "BLIP-2 builds upon BLIP, improving practicality and performance, which is highly relevant to the context of advancing adversarial attacks against image-to-text models."}, {"fullname_first_author": "Stanislaw Antol", "paper_title": "VQA: Visual Question Answering", "publication_date": "2015-12-07", "reason": "This paper introduces the Visual Question Answering (VQA) task, a benchmark that has had a significant influence on vision-language research, and is relevant to the paper's exploration of adversarial attacks within this domain."}, {"fullname_first_author": "Wonjae Kim", "paper_title": "ViLT: Vision-and-Language Transformer without Convolution or Region Supervision", "publication_date": "2021-07-18", "reason": "ViLT provides a strong foundation for vision-language transformer models, the architecture that is often targeted by adversarial attacks, making it a critical reference."}, {"fullname_first_author": "Hongge Chen", "paper_title": "Attacking Visual Language Grounding with Adversarial Examples: A Case Study on Neural Image Captioning", "publication_date": "2018-07-01", "reason": "This paper is among the first to explore adversarial attacks on image captioning models, directly addressing the security concerns that the current research aims to improve."}]}