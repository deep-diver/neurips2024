[{"figure_path": "ZsS0megTsh/tables/tables_5_1.jpg", "caption": "Table 1: Cross-manipulation generalization. We report video-level AUC (%) on FF++, which contains four manipulation methods, i.e., Deepfakes (DF), FaceSwap (FS), Face2Face (F2F) and NeuralTextures (NT). * denotes results of our reproduction.", "description": "This table presents the results of a cross-manipulation generalization experiment.  The Area Under the Curve (AUC) metric, measuring the performance of various forgery detection methods, is reported for four different manipulation techniques (Deepfakes, FaceSwap, Face2Face, NeuralTextures) used in the FaceForensics++ dataset. The experiment uses a leave-one-out strategy, where each manipulation technique is tested on a model trained on the remaining three.  The table compares several state-of-the-art (SOTA) methods.  The results demonstrate how well the models generalize to unseen manipulation techniques.", "section": "4.2 Quantitative Comparisons"}, {"figure_path": "ZsS0megTsh/tables/tables_6_1.jpg", "caption": "Table 3: Cross-language generalization. AUC (%) scores on videos of different languages in the FF++.", "description": "This table presents the Area Under the Curve (AUC) scores achieved by the proposed method for face forgery detection across videos from different languages within the FaceForensics++ dataset.  The results demonstrate the model's cross-lingual generalization capabilities by showing consistent high AUC scores across various languages, indicating its robustness and ability to generalize beyond the language(s) it was primarily trained on.", "section": "4.2 Quantitative Comparisons"}, {"figure_path": "ZsS0megTsh/tables/tables_8_1.jpg", "caption": "Table 4: Effect of different models and time offset assumptions. We report the performance of models with different architectures and training datasets on FF++ and FakeAVCeleb.", "description": "This table shows the results of an ablation study on the proposed face forgery detection method, SpeechForensics. The study investigates the impact of different model architectures (BASE and LARGE versions of AVHUBERT and VATLM) and training datasets (LRS3 and LRS3+Vox2) on the model's performance, specifically measured by the Area Under the Curve (AUC) metric on the FaceForensics++ (FF++) and FakeAVCeleb datasets. It also explores the effect of two different time offset assumptions (Fixed and Dynamic) between audio and visual streams during the model's forgery detection process. This table helps to understand how different factors influence the model's generalization and robustness.", "section": "4 Experiments"}, {"figure_path": "ZsS0megTsh/tables/tables_13_1.jpg", "caption": "Table 5: Visual frontend architecture.The output size is of the form T \u00d7 H \u00d7 W, where T denotes the number of input frames, H denotes the height of frames and W denotes the width.", "description": "This table details the architecture of the visual frontend used in the proposed method. It shows the different stages (conv1, pool1, res1, res2, res3, res4, pool2), the filters used in each stage, and the output size of each stage. The output size is expressed in terms of T (number of input frames), H (height of frames), and W (width of frames).", "section": "A.1 Architecture Details"}, {"figure_path": "ZsS0megTsh/tables/tables_14_1.jpg", "caption": "Table 2: Cross-dataset generalization. Video-level AUC (%) on FakeAVCeleb and KoDF. We report the results of every categories of FakeAVCeleb, and the overall performance on it is reported in Overall. The average performance over two datasets is reported in Avg.", "description": "This table presents the Area Under the Curve (AUC) scores for the proposed SpeechForensics method and several state-of-the-art methods on two different datasets, FakeAVCeleb and KoDF.  It showcases the cross-dataset generalization capabilities of the approach.  For FakeAVCeleb, AUC scores are broken down by specific forgery methods (Faceswap, FSGAN, Wav2Lip, etc.) to assess performance across various manipulation techniques, with an overall average and comparison against KoDF.", "section": "4.2 Quantitative Comparisons"}, {"figure_path": "ZsS0megTsh/tables/tables_14_2.jpg", "caption": "Table 2: Cross-dataset generalization. Video-level AUC (%) on FakeAVCeleb and KoDF. We report the results of every categories of FakeAVCeleb, and the overall performance on it is reported in Overall. The average performance over two datasets is reported in Avg.", "description": "This table presents the results of a cross-dataset generalization experiment to evaluate the performance of the proposed method and other state-of-the-art methods on two unseen datasets: FakeAVCeleb and KoDF.  It breaks down the performance on FakeAVCeleb by the specific forgery method used, providing overall and average performance metrics across both datasets.", "section": "4.2 Quantitative Comparisons"}]