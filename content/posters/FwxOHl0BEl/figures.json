[{"figure_path": "FwxOHl0BEl/figures/figures_3_1.jpg", "caption": "Figure 1: Overview of the neural reparametrization method. Top: Architectures used for reparametrization. In linear reparametrization, X = ZT\u03a8slow. In the GNN case, we use the slow modes to construct a graph with adjacency A = \u03a8slow\u03a8slowT and use it in GCN layers to obtain X = gnn(Z). Left: Flowchart showing the key steps of the method. Right: Detailed algorithm for implementation.", "description": "This figure provides a comprehensive overview of the neural reparametrization method proposed in the paper.  It showcases three key aspects:  different architectures for reparametrization (linear and GNN-based), a flowchart illustrating the step-by-step process of the method, and a detailed algorithmic description. The linear approach uses a straightforward linear projection of latent variables (Z) onto slow modes (\u03a8slow), while the GNN approach leverages the slow modes to construct a graph, then uses graph convolutional networks (GCNs) to perform reparametrization. The flowchart visually guides the reader through the key computational steps (Hessian computation, slow mode extraction, GNN-based reparametrization, and optimization), enhancing understanding. The detailed algorithm provides a precise, code-like description of the procedure.", "section": "Neural Architectures for Reparametrization"}, {"figure_path": "FwxOHl0BEl/figures/figures_6_1.jpg", "caption": "Figure 2: Synthetic loop experiments. Example runs of the synthetic loop experiments with n = 400 nodes. On the left (Bond+LJ), the potential is the sum of a quadratic bond potential Ebond and a weak LJ (12,6) ELJ. The bonds form a line graph Abond connecting node i to i + 1, and a 10 weaker Aloop connecting node i to i + 10 via the LJ potential. To the right (Pure LJ) where the interactions are all LJ, but with a coupling matrix A = Abond + 0.1Aloop. In Bond+LJ, GD already finds good energies and the configuration is reasonably close to a loop, though flattened. Both linear CG reparametrization (CG Rep) and GNN also find a good layout. The pure LJ case is much more tricky. But in most runs, GD almost gets the layout, but some nodes remain far away. The CG Rep fails to bring all the pieces together. Only GNN succeeds in finding the correct layout.", "description": "This figure shows the results of synthetic loop experiments using three different methods: Gradient Descent (GD), Coarse-graining with Reparametrization (CG Rep), and Graph Neural Network (GNN).  Two scenarios are compared: one with both bond and Lennard-Jones (LJ) potentials (Bond+LJ), and one with only LJ potentials (Pure LJ). The GNN method consistently outperforms the others, especially in the more challenging Pure LJ scenario, where it is the only method that successfully forms the loop structure.", "section": "Experiments"}, {"figure_path": "FwxOHl0BEl/figures/figures_6_2.jpg", "caption": "Figure 3: Synthetic loop folding (n = 1000). Lower means better for both energy and time. In Bond+LJ (left), a quadratic potential \u2211i (rii+1 \u2013 1)\u00b2 attracts nodes i and i + 1. A weak LJ potential attracts nodes i and i + 10 to form loops. In LJ loop (right) both the backbone i, i + 1 and the 10x weaker loop are LJ. Orange crosses denote the baseline GD, green is GNN and blue is CG. The dots are different hyperparameter settings (LR, Nr. CG modes, stopping criteria, etc.) with error bars over 5 runs. In Bond+LJ, CG yields slightly better energies but takes longer, while GNN can converge faster to GD energies. In pure LJ, using CG and GNN can yield significantly better energies.", "description": "This figure compares the performance of three methods (Gradient Descent, Coarse-Graining, and Graph Neural Network) on two synthetic loop folding tasks with 1000 nodes.  The left panel shows results for a system with both bond and Lennard-Jones (LJ) potentials, while the right shows results for a system with only LJ potentials.  The plots show energy achieved versus time taken. The results demonstrate that the GNN method outperforms the other two, particularly in the challenging all-LJ system, achieving better energy and faster convergence.", "section": "Experiments"}, {"figure_path": "FwxOHl0BEl/figures/figures_7_1.jpg", "caption": "Figure 4: Protein folding simulations Figure (a) shows the energy improvement factor (FG energy / GNN energy) in the function of the speedup factor (FG time / GNN time) for the six selected proteins marked with different colors (c). In all cases, the GNN parameterization leads to speed improvement while it converges higher energy. (b) However, the higher energy in some cases, 2JOF and 1UNC proteins, results in a slightly lower RMSD value, which measures how close the final layout is to the PDB layout. The data points are averaged over ten simulations per protein.", "description": "The figure shows the results of protein folding simulations using three different methods: fine-grained (FG), gradient descent (GD), and graph neural network (GNN).  Panel (a) compares the energy improvement factor (ratio of FG energy to GNN energy) to the speedup factor (ratio of FG time to GNN time) across six different proteins. The GNN consistently shows improvement in speed while having slightly higher energy in some cases, as shown in panel (b). Panel (b) shows that this slightly higher energy does not always correspond to a higher root mean square deviation (RMSD), which indicates that the GNN structures are still quite close to the correct structures. Panel (c) shows the final structures for each of the six proteins obtained with the three methods, confirming that GNN is able to produce accurate predictions.", "section": "Experiments"}, {"figure_path": "FwxOHl0BEl/figures/figures_7_2.jpg", "caption": "Figure 5: 2JOF (Trp-Cage) protein folding. Figure (a) shows the RMSD value evolution of the 2JOF protein as it goes from an unfolded to a folded stage. At every step, we calculated the RMSD of the current layout compared to the PDB layout. We ran the OpenMM simulations at 298K temperature with 2fs timestep for 800000 steps, while the GNN and GD simulations were performed for 400000 steps with various hidden dimensions (10, 100, 300, 500). The black curves show the stochastic nature of protein folding using OpenMM. (b) The first figure shows the PDB (red) and unfolded (blue) layout; the second one is the GNN 500 final layout (blue), while the third is one of the OpenMM layouts, corresponding to the black curve.", "description": "This figure shows the results of protein folding simulations for the Trp-Cage protein (2JOF).  Panel (a) presents a graph comparing the root-mean-square deviation (RMSD) from the known folded structure over time for different methods: OpenMM, gradient descent (GD), and graph neural networks (GNN) with varying numbers of hidden units. The plot illustrates the convergence of the different methods towards the folded state, highlighting the faster convergence and lower RMSD achieved by GNN with more hidden units compared to OpenMM and GD. The stochastic nature of protein folding using OpenMM is also demonstrated. Panel (b) shows 3D visualizations of the protein in unfolded, GNN-optimized, and OpenMM-optimized states, offering a visual comparison of the conformations.", "section": "3 Experiments"}, {"figure_path": "FwxOHl0BEl/figures/figures_8_1.jpg", "caption": "Figure 6: Learning rate and initialization in protein folding for pdb 2JOF: We conducted a sweep of the learning rate to see how robust the advantage of GNN over direct GD is. In a and b we show the energy achieved by GD and GNN vs the number of iterations and wallclock time. GNN1 and GNN2 use one and two GCN layers, respectively. We used early stopping which generally stopped the runs after 3-5k steps. The grey star shows the OpenMM results after 5k steps, which has a worse (higher) energy than our GD and GNN runs, but it takes a fraction of the time (it has many efficiency tricks that our code doesn't have). The dashed line shows the energy achieved by OpenMM after 10k steps. As we see, some of our GNN models reach energies close to the 10k steps of openMM in a fraction of the steps. All experiments show the best energy among three runs. c shows the effect of initialization on the GD runs. We do find the protein converges to significantly different conformations based on the init.", "description": "The figure shows the results of experiments on the robustness of the GNN model in protein folding.  Subfigure (a) and (b) compare the potential energy achieved by gradient descent (GD) and the GNN model with varying learning rates and against OpenMM, demonstrating the GNN's faster convergence to lower energies.  Subfigure (c) shows the impact of different initializations on GD convergence, indicating sensitivity to initial conditions.  The results highlight the GNN's advantages in terms of energy minimization and convergence speed.", "section": "Experiments"}, {"figure_path": "FwxOHl0BEl/figures/figures_11_1.jpg", "caption": "Figure 7: Enkephalin (1PLW). a) The peptide chain is built by stacking amino acids on each other using the peptide bond length from the literature, 1.32 \u00c5. b) Van der Waals, hydrogen bond, and hydrophobic interaction matrix, that we use in the energy optimization.", "description": "This figure shows the structure of the Enkephalin (1PLW) peptide and its interaction matrices. Panel (a) presents the 3D structure of the peptide chain, where amino acids are stacked using a bond length of 1.32 \u00c5. Panels (b) display the interaction matrices used in the energy optimization, including Van der Waals (VdW), hydrogen bond, and hydrophobic bond interactions.  These matrices visually represent the strengths of interactions between pairs of atoms in the peptide.", "section": "Additional Figures"}, {"figure_path": "FwxOHl0BEl/figures/figures_12_1.jpg", "caption": "Figure 8: Comparison of performance of CG Hessian versus baseline MD. Point sizes correspond to the number of CG modes used.", "description": "This figure compares the performance of the coarse-graining (CG) method using the Hessian with the baseline molecular dynamics (MD) method.  The x-axis represents the time taken for the simulation to complete, and the y-axis represents the energy reached.  Different colors represent simulations for different proteins (2JOF, 2MGO, 1PLW, 5AWL). Each point shows a single simulation run and its size indicates the number of collective modes used in the CG method. The figure demonstrates that the CG method is able to reach lower energy states in a similar amount of time as compared to the baseline MD method, suggesting that CG is an effective way to accelerate molecular dynamics simulations.", "section": "3 Experiments"}, {"figure_path": "FwxOHl0BEl/figures/figures_13_1.jpg", "caption": "Figure 9: The folded structures of the 2JOF protein by using the CG and baseline method. The numbers in front of the rows are the numbers of eigenvectors used in the CG reparametrization. Dashed frames show the minimum energy embedding in each case, while the thick line frame highlights the absolute minimum layout.", "description": "This figure compares the final folded structures of the 2JOF protein obtained using different methods: standard molecular dynamics (MD) and coarse-graining (CG) with varying numbers of eigenvectors. The dashed boxes highlight the minimum energy conformation for each method, while the solid box highlights the absolute minimum energy conformation.  It illustrates the effectiveness of the CG method in achieving a structure closer to the true minimum energy conformation, showcasing the impact of the number of eigenvectors used.", "section": "3 Experiments"}]