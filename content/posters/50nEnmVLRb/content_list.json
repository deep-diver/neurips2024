[{"type": "text", "text": "Gaussian Process Bandits for Top-k Recommendations ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Mohit Yadav Daniel Sheldon University of Massachusetts Amherst University of Massachusetts Amherst ymohit@cs.umass.edu sheldon@cs.umass.edu ", "page_idx": 0}, {"type": "text", "text": "Cameron Musco University of Massachusetts Amherst cmusco@cs.umass.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Algorithms that utilize bandit feedback to optimize top-k recommendations are vital for online marketplaces, search engines, and content platforms. However, the combinatorial nature of this problem poses a significant challenge, as the possible number of ordered top-k recommendations from $n$ items grows exponentially with $k$ . As a result, previous work often relies on restrictive assumptions about the reward or bandit feedback models, such as assuming that the feedback discloses rewards for each recommended item rather than a single scalar feedback for the entire set of top- $\\cdot\\mathbf{k}$ recommendations. We introduce a novel contextual bandit algorithm for top- $\\cdot\\mathbf{k}$ recommendations, leveraging a Gaussian process with a Kendall kernel to model the reward function. Our algorithm requires only scalar feedback from the top- $\\cdot\\mathbf{k}$ recommendations and does not impose restrictive assumptions on the reward structure. Theoretical analysis confirms that the proposed algorithm achieves sub-linear regret in relation to the number of rounds and arms. Additionally, empirical results using a bandit simulator demonstrate that the proposed algorithm outperforms other baselines across various scenarios. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The top- $k$ recommendation problem involves providing a ranked list of $\\boldsymbol{\\mathrm{k}}$ items, such as news articles or products, from a pool of $n$ items [34, 13]. Online algorithms must adapt to dynamic user preferences, making bandit algorithms suitable due to their use of limited feedback [1]. Developing bandit algorithms is challenging due to limited feedback and the need for computational efficiency in real-time recommendation environments. Recent research on user interfaces for recommendations highlights that the overall layout of the recommendation page is crucial for user appeal, as modern UI designs have evolved from simple dropdown lists to complex, visually engaging layouts [17, 13, 18]. Consequently, bandit algorithms must jointly select and display all top- $k$ items, rather than simply choosing the most relevant $k$ items and ordering them by decreasing user relevance [31]. ", "page_idx": 0}, {"type": "text", "text": "The joint consideration of top- $k$ items makes the number of arms (possible actions for the bandit algorithm) combinatorially large, i.e., $\\Theta(n^{k})$ . Previous research on bandit algorithms often imposes strict assumptions on feedback models [30, 21]. For instance, semi-bandit feedback provides a scalar reward for each of the top $k$ items, thus decomposing the combinatorial feedback into item-level feedback. However, this type of feedback is frequently unavailable [32]. Another common feedback model is cascade browsing [16], which assumes that users examine items in a predetermined order and cease browsing once a desirable item is found, offering item-specific scalar feedback but failing to capture potential non-linear interactions among items [26]. Figure 1 illustrates the limitations of the cascade model in capturing user interactions within modern top- $k$ recommendation interfaces. ", "page_idx": 0}, {"type": "text", "text": "These limitations motivate us to adopt a more general full-bandit feedback setting, where only a single scalar value is provided for the entire top- $k$ set of recommendations [24]. ", "page_idx": 1}, {"type": "image", "img_path": "50nEnmVLRb/tmp/62992906d690d894dc76026e57ba41af7477fdb19ccbaf9b106b735de7d5a920.jpg", "img_caption": ["Figure 1: A snapshot from Etsy showcases Father\u2019s Day shopping recommendations. The lack of an obvious linear search order challenges the assumptions of the cascade model. Additionally, the proximity and arrangement of items are likely to influence clicks, indicating complex interaction patterns and supporting the need for full-bandit feedback without assumptions about user interactions with recommended items. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Table 1: Compute and memory analysis for the proposed GPTopK bandit algorithm. Rows represent different costs: total compute and memory of the GP-TopK algorithm for $\\mathbf{T}$ rounds, time for matrix-vector multiplication $(\\mathbf{mvm})$ with the kernel matrix $K_{X_{t}}$ for $t^{\\mathrm{{th}}}$ round, and time to update $K_{X_{t}}$ . Columns represent different approaches: the kernel approach, which uses full kernel matrices, and our novel feature approach, which performs the same operations through feature expansions and scales more efficiently with respect to $\\mathbf{T}$ . The symbols $c,\\,k$ , and $\\mathbf{T}$ denote the embedding size for contexts, the number of items, and the number of rounds, respectively. ", "page_idx": 1}, {"type": "table", "img_path": "50nEnmVLRb/tmp/17b49e7853d0fbbb6bd6ca71bfd486750f189b8343dd5138f2668bd95bf6e8c3.jpg", "table_caption": [], "table_footnote": [], "page_idx": 1}, {"type": "text", "text": "Beyond feedback assumptions, the reward structure in bandit algorithms must be decomposable into scalar values for individual items to prevent a combinatorial explosion of arms\u2014something that is not always feasible. For example, modern e-commerce platforms value and track metrics such as diversity and fairness [1], which cannot be captured by focusing solely on individual items [15]. This necessitates algorithms for full-bandit feedback settings that operate without specific assumptions about the objectives or reward structures [24]. ", "page_idx": 1}, {"type": "text", "text": "This work introduces a bandit algorithm that uses Gaussian processes (GPs) to model rewards under full-bandit feedback (i.e., a single scalar value). GPs are selected for their flexibility in accommodating feedback across discrete, continuous, and mixed domains, such as continuous contexts and discrete rankings [33]. Additionally, unlike parametric models that require optimization incorporating accumulated feedback from previous rounds, GP updates are computationally efficient, involving only data updates [24]. Although GP inference may face computational limits, we will develop efficient inference methods tailored to our proposed algorithm. A further challenge in designing GP-based bandit algorithms for top- $k$ recommendations is constructing expressive positivedefinite kernels that capture similarities between top- $k$ recommendations [9]. This work mitigates these computational and modeling challenges, as illustrated in the following sections. ", "page_idx": 1}, {"type": "text", "text": "Broadly speaking, GPs have been previously explored for bandit algorithms [27, 19]. Krause et al. [14] employed GPs for contextual bandits in continuous domains; we focus on the discrete domain of top- $k$ recommendations. Vanchinathan et al. [28] used GPs with a position-based feedback model, and Wang et al. [31] used GPs with semi-bandit feedback for recommending top- $k$ items. In contrast, our work does not rely on a specific reward model or feedback assumption, and develops an an efficient GP-based bandit algorithm for top- $k$ recommendations. ", "page_idx": 1}, {"type": "text", "text": "1.1 Contributions ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Our primary contribution is the GP-TopK algorithm, a contextual bandit algorithm for recommending top- $k$ items. This algorithm operates in a full-bandit feedback setting without relying on assumptions on reward, making it broadly applicable compared to prior works. We leverage GPs with variants of the Kendall kernel [12] to model the reward function and optimize the upper confidence bound (UCB) [27] acquisition function for selecting the next arm. Additionally, we introduce a novel weighted convolutional Kendall kernel for top- $k$ recommendations that address pathologies in existing variants of the Kendall kernel when applied to top- $k$ recommendations. ", "page_idx": 1}, {"type": "text", "text": "Our second key contribution is enhancing the scalability of the GP-TopK algorithm for longer time horizons. Initially, the computational cost for top- $k$ recommendations using the GP-TopK algorithm is ${\\mathcal{O}}(T^{4})$ for $T$ rounds. We first reduce this to $\\mathcal{O}(T^{3})$ by leveraging iterative algorithms from numerical linear algebra [25]. Next, we derive sparse feature representations for the novel weighted convolutional Kendall kernel, further reducing the compute requirements from ${\\mathcal{O}}(T^{4})$ to ${\\mathcal{O}}(T^{2})$ and memory requirements from ${\\mathcal{O}}(T^{2})$ to ${\\mathcal{O}}(T)$ . Table 1 summarizes these improvements in time and memory requirement, including their dependence on other parameters. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "We provide a theoretical analysis showing that GP-TopK\u2019s regret is sub-linear in $T$ , benefiting from the feature representations of the Kendall kernels introduced in this work. Specifically, we establish an upper bound on regret that is nearly quadratic in $n$ , significantly improving over the naive $\\Theta(n^{k})$ bound for top- $k$ recommendations without using feature representations [27]. Finally, we empirically validate GP-TopK\u2019s regret on real-world datasets, demonstrating improvement over baseline methods. ", "page_idx": 2}, {"type": "text", "text": "1.2 Organization ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The remainder of this paper is organized as follows: Section 2 introduces Kendall kernels for full and top- $\\cdot k$ rankings, including the novel weighted convolutional Kendall kernel. Section 3 presents faster matrix-vector multiplication algorithms for Kendall kernels, enhancing the efficiency of the proposed bandit algorithm, which is further detailed along with the regret analysis in Section 4. Finally, Sections 5 and 6 present empirical results and concluding discussion, respectively. ", "page_idx": 2}, {"type": "text", "text": "2 Kendall Kernels for Full and Top-k Rankings ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "This section introduces Kendall kernels and their extensions for top- $k$ recommendations, forming the foundation of our approach. We first establish key notations and them present Sections 2.1 and 2.2, which introduce Kendall kernels for full rankings and top- $\\cdot k$ rankings, respectively. ", "page_idx": 2}, {"type": "text", "text": "Notations: Let $[n]\\,=\\,1,2,\\dots,n$ , with $\\pi$ representing a top- $k$ ranking\u2014an ordered tuple of $k$ distinct elements from $[n]$ . For a full ranking $(k=n)$ ), we use $\\sigma$ and denote the set of all possible top- $k$ rankings by $\\Pi^{k}$ , with cardinality $|\\Pi^{k}|\\,=\\,\\Theta(n^{k})$ . To capture ranking positions, the vector $\\mathbf{p}^{\\sigma}\\in\\mathbb{R}^{n}$ corresponds to a full ranking $\\sigma$ with entry $\\mathbf{p}_{i}^{\\sigma}$ gives the rank of item $i$ . For top- $\\mathbf{-k}$ rankings, $\\mathbf{p}^{\\pi}\\in\\mathbb{R}^{n}$ is similarly constructed by arbitrarily assigning distinct ranks to items not in the top $k$ . For relative ranks, indicator functions ${\\mathfrak{p}}_{i<j}^{\\sigma}$ and ${\\mathfrak{p}}_{i>j}^{\\sigma}$ denote whether item $i$ is ranked before or after item $j$ , respectively in $\\sigma$ . Also, ${\\mathfrak{p}}_{i<j}^{\\pi}$ and $\\mathfrak{p}_{i>j}^{\\tilde{\\pi}}$ are similar indicator functions defined for top- $\\cdot\\mathbf{k}$ rankings. ", "page_idx": 2}, {"type": "text", "text": "2.1 Kendall Kernels for Full Rankings ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Jiao et al. [9] showed that the Kendall tau rank correlation coefficient [12] is a positive definite (p.d.) kernel for full rankings, which we refer to as the standard Kendall (SK) kernel. The weighted Kendall (WK) kernel generalizes the SK kernel by differentially weighting item pairs [10]. Specifically, the SK and WK kernels for full rankings $\\sigma_{1},\\sigma_{2}$ are defined as: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{{k^{s k}}(\\sigma_{1},\\sigma_{2}):=\\displaystyle\\frac{1}{{\\binom{n}{2}}}\\sum_{i<j}\\eta_{i,j}(\\sigma_{1},\\sigma_{2})}}\\\\ {{{k^{w k}}(\\sigma_{1},\\sigma_{2}):=\\displaystyle\\frac{1}{{\\binom{n}{2}}}\\sum_{i<j}w(({\\mathbf{p}_{i}^{\\sigma_{1}}},{\\mathbf{p}_{j}^{\\sigma_{1}}}),({\\mathbf{p}_{i}^{\\sigma_{2}}},{\\mathbf{p}_{j}^{\\sigma_{2}}}))\\cdot\\eta_{i,j}(\\sigma_{1},\\sigma_{2}),}}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\eta_{i,j}$ is 1 if the pair $(i,j)$ is concordant (ordered the same in both rankings) and $-1$ otherwise; concretely, $\\begin{array}{r}{\\eta_{i,j}^{\\phantom{\\dagger}}(\\sigma_{1},\\sigma_{2})\\;:=\\;\\mathfrak{p}_{i<j}^{\\sigma_{1}}\\cdot\\mathfrak{p}_{i<j}^{\\sigma_{2}}\\,+\\,\\mathfrak{p}_{i>j}^{\\sigma_{1}}\\cdot\\mathfrak{p}_{i>j}^{\\sigma_{2}}\\,-\\,\\mathfrak{p}_{i<j}^{\\sigma_{1}}\\cdot\\mathfrak{p}_{i>j}^{\\sigma_{2}}\\,-\\,\\mathfrak{p}_{i>j}^{\\sigma_{1}}\\cdot\\mathfrak{p}_{i<j}^{\\sigma_{2}}}\\end{array}$ ; and $w((\\mathbf{p}_{i}^{\\sigma_{1}},\\mathbf{p}_{j}^{\\sigma_{1}}),(\\mathbf{p}_{i}^{\\sigma_{2}},\\mathbf{p}_{j}^{\\sigma_{2}}))$ is the value of a positive definite weighting kernel $w(\\cdot,\\cdot):\\left[n\\right]^{2}\\times\\left[n\\right]^{2}\\mapsto\\mathbb{R}$ that operates on pairs of ranks. The $w_{i,j}$ adds flexibility and can assign varying importance to ranks, similar to the discounted cumulative gain (DCG) metric [7]. Note that both SK and WK kernels are p.d. and right-invariant with respect to $\\Pi^{n}$ [10]. In other words, they compute similarity based only on the relative ranks of pairs, not on the labels of items, as clearly evident from Equations 1 and 2. ", "page_idx": 2}, {"type": "text", "text": "2.2 Kendall Kernels for Top-k Rankings ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Weighted Kendall and Convolutional Kendall (CK) kernels. To adapt the WK kernel from full rankings to top- $k$ rankings, Jiao et al. [10] set the weighting function $w(i,j,\\sigma_{1},\\sigma_{2})$ to zero if either item is not in the top- $\\cdot\\mathbf{k}$ of either ranking. While this approach yields a p.d. kernel, it disregards ", "page_idx": 2}, {"type": "text", "text": "items outside the intersection of top- $k$ rankings. In contrast, the convolutional operation provides an alternative for adapting the standard Kendall kernel to top- $k$ rankings. ", "page_idx": 3}, {"type": "text", "text": "Let $B_{\\pi}$ denote the set of full rankings consistent with the top- $\\cdot\\mathbf{k}$ ranking $\\pi$ (i.e., for every item $i$ in $\\pi$ , $\\forall\\sigma\\in B_{\\pi},\\mathbf{p}_{i}^{\\pi}=\\mathbf{p}_{i}^{\\sigma})$ ). The convolutional Kendall kernel can be defined as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\nk^{c k}(\\pi_{1},\\pi_{2})=\\frac{1}{|{\\cal{B}}_{\\pi_{1}}|\\cdot|{\\cal{B}}_{\\pi_{2}}|}\\sum_{\\sigma_{1}\\in{\\cal{B}}_{\\pi_{1}},\\ \\sigma_{2}\\in{\\cal{B}}_{\\pi_{2}}}k^{s k}(\\sigma_{1},\\sigma_{2}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $k^{s k}$ is the standard Kendall kernel. Since the CK kernel is a convolution of another p.d. kernel, it is also a p.d. kernel [5]. Unlike the WK kernel for top- $\\cdot\\mathbf{k}$ rankings, the CK kernel accounts for items not in both top- $\\cdot\\mathbf{k}$ rankings. However, computing the CK kernel is expensive, requiring exponentially many evaluations of the kernel $k^{s k}$ in the double summation. Therefore, Jiao et al. [9] developed an efficient algorithm to bypass this double summation, reducing compute to ${\\mathcal{O}}(k\\log k)$ time. ", "page_idx": 3}, {"type": "text", "text": "Proposed Weighted Convolutional Kendall (WCK) Kernel. To combine the strengths of the WK and CK kernels for top- $\\cdot\\mathbf{k}$ rankings, we propose the weighted convolutional Kendall kernel for top- $\\cdot\\mathbf{k}$ rankings $\\pi_{1}$ and $\\pi_{2}\\in\\overline{{\\Pi^{k}}}$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\nk^{w c k}(\\pi_{1},\\pi_{2}):=\\frac{1}{|{\\cal B}_{\\pi_{1}}|\\cdot|{\\cal B}_{\\pi_{2}}|}\\sum_{\\sigma_{1}\\in{\\cal B}_{\\pi_{1}},\\sigma_{2}\\in{\\cal B}_{\\pi_{2}}}k^{w k}(\\sigma_{1},\\sigma_{2}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $k^{w k}$ represents the weighted Kendall kernel for full rankings $\\sigma_{1},\\sigma_{2}\\in\\Pi^{n}$ . ", "page_idx": 3}, {"type": "text", "text": "The proposed WCK kernel combines the flexibility of differentially weighting ranks among the top- $k$ items (as in the WK kernel) with the ability to account for items outside the intersection of both top- $k$ rankings (as in the CK kernel). Additionally, as a convolution of a p.d. kernel, it is also a p.d. kernel. However, computing the WCK kernel remains challenging, as it requires exponentially many evaluation of the $k^{w k}$ kernel, as given in the RHS of Equation 4. To address this, we focus on a specific form of rank weights of the $k^{w k}$ kernel, called as product-symmetric rank weights: ", "page_idx": 3}, {"type": "equation", "text": "$$\nw_{p s}((i_{1},j_{1}),(i_{2},j_{2})):=w_{s}(i_{1},j_{1})\\cdot w_{s}(i_{2},j_{2}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where, $w_{s}(i,j):[n]\\times[n]\\mapsto\\mathbb{R}$ is a symmetric function, i.e., $w_{s}(i,j)=w_{s}(j,i)$ . Notably, the WCK kernel can be computed efficiently for the case of these weights (see Claim 1 below). ", "page_idx": 3}, {"type": "text", "text": "The WCK kernel, even with the relatively simple $w_{p s}$ weights, exhibits notable properties, as shown in Table 2. In this table, we use $\\begin{array}{r}{w_{s}(i,j)\\,=\\,\\frac{1}{\\log(i+1)}\\,\\cdot\\,\\frac{1}{\\log(j+1)}}\\end{array}$ , inspired by the DCG metric commonly applied in recommendation systems [7]. Notably, the WK kernel ranks two rankings with no overlap $\\left[\\pi_{0}\\right.$ and $\\pi_{1}$ ) as more similar than two rankings with the same items in reversed order $\\left[\\pi_{0}\\right.$ and $\\pi_{2}.$ ), indicating a clear pathology. Further, the CK kernel fails to distinguish between reversed pairs at different ranks $(k^{c k}(\\pi_{0},\\pi_{3})=k^{c k}\\big(\\dot{\\pi}_{0},\\pi_{4}))$ , presenting another limitation if known variants of Kendall kernels for top- $\\cdot\\mathbf{k}$ rankings. By using product-symmetric ranking weights, the WCK kernel addresses these shortcomings, providing a more nuanced similarity comparison for top- $k$ rankings. ", "page_idx": 3}, {"type": "text", "text": "Table 2: Comparison of Kendall kernel similarities for topk rankings. The table shows kernel values $k(\\pi_{0},\\cdot)$ for the top- $\\cdot\\mathbf{k}$ ranking $\\pi_{0}\\,=\\,[1,2,3]$ with other rankings $(\\pi_{1},\\,\\pi_{2}$ , $\\pi_{3},\\pi_{4})$ for $n=7$ and $k=3$ . Rankings are arranged left to right by increasing similarity to $\\pi_{0}$ . The similarity values provided by the proposed kernel increase from left to right as expected, demonstrating the desirable behavior of the WCK kernel with DCG rank weights, unlike other variants. All kernels are unit-normalized. See text for further details. ", "page_idx": 3}, {"type": "table", "img_path": "50nEnmVLRb/tmp/139a1921ee3df5af6eab5dbada930477cdd2301578c6754de7e4a57ebaf3d5e8.jpg", "table_caption": [], "table_footnote": [], "page_idx": 3}, {"type": "text", "text": "Claim 1. The weighted convolutional Kendall kernel (Equation 4) with product-symmetric rank weights (Equation 5) can be computed in ${\\mathcal{O}}(k^{2})$ time. ", "page_idx": 3}, {"type": "text", "text": "Appendix A provides the proof that leverages the structure of product-symmetric rank weights $w_{p s}$ to establish the existence of a feature representation for the WCK kernel, as formally stated below in ", "page_idx": 3}, {"type": "text", "text": "Claim 3 below. We then demonstrate that the inner product of these features, and hence the WCK kernel, can be computed in ${\\mathcal{O}}(k^{2})$ time (Algorithm 2 in the appendix). Similar to the result of Jiao et al. [9] for the CK kernel, this approach avoid exponentially many evaluations of $k^{w k}$ on the RHS of Equation (4) by enabling a direct computation of the WCK kernel. ", "page_idx": 4}, {"type": "text", "text": "3 Fast Matrix-Vector Multiplication with Kendall Kernel Matrices ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In Gaussian processes, inference can be accelerated by using iterative algorithms that take advantage of fast matrix-vector-multiplications (MVMs) with the kernel matrix [3]. This section introduces fast algorithms for kernel MVMs by exploiting the implicit structure of Kendall kernel matrices. ", "page_idx": 4}, {"type": "text", "text": "Let $\\mathbf{mvm}(K_{X_{t}})$ denote the runtime required to multiply the $t\\,\\times\\,t$ kernel matrix $\\begin{array}{r l}{K_{X_{t}}}&{{}=}\\end{array}$ $(k(x_{i},x_{j}))_{x_{i},x_{j}\\in X_{t}}$ by any admissible vector. In the naive approach, this runtime is $\\mathbf{mvm}(K X_{t})=$ ${\\mathcal{O}}(t^{2})$ . However, if $k(x_{i},x_{j})=\\phi^{a}(x_{i})^{T}\\phi^{b}(x_{j})$ for any arbitrary $x_{i}$ and $x_{j}$ , where the vectors $\\phi^{a}(x_{i})$ and $\\phi^{b}(x_{j})$ are sparse and contain only $z$ non-zero entries, then $\\mathbf{mvm}(K_{X_{t}})$ reduces to $\\mathcal{O}(z\\cdot t)$ , which is a significant improvement over ${\\mathcal{O}}(t^{2})$ when $z\\ll t$ . When $\\phi^{a}=\\phi^{b}$ , we refer to $\\phi^{a}$ as the linear feature vector for the kernel $k$ . Before focusing on top- $\\cdot\\mathbf{k}$ ranking kernels, we provide a linear feature vector for the WK kernel on full rankings (given earlier in Equation 2). ", "page_idx": 4}, {"type": "text", "text": "Claim 2. Let $\\phi^{w k}(\\sigma):\\Pi^{n}\\mapsto\\mathbb{R}^{\\binom{n}{2}}$ be a vector indexed by unique item pairs $(i,j)$ , defined as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\phi_{i,j}^{w k}(\\sigma):=\\frac{1}{\\sqrt{\\binom{n}{2}}}\\cdot w_{s}(\\mathbf{p}_{i}^{\\sigma},\\mathbf{p}_{j}^{\\sigma})\\cdot\\left(\\mathfrak{p}_{i<j}^{\\sigma}-\\mathfrak{p}_{i>j}^{\\sigma}\\right),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $w_{s}$ is the symmetric weighting function in product-symmetric weights. Then, $\\phi^{w k}$ is $a$ linear feature vector for the weighted Kendall kernel with product-symmetric weights $w_{p s}$ . ", "page_idx": 4}, {"type": "text", "text": "Using Claim 2, the linear feature vector for the WK kernel can be extended to the WK top- $k$ ranking kernel by utilizing the structure of product-symmetric weights, which allows weights to be set to zero for items outside of the top- $k$ rankings, as described in Section 2.2. Precisely, such a feature vector for the top- $\\cdot k$ ranking kernel is sparse; specifically, the feature vector $\\phi^{w k}(\\dot{\\pi})$ contains only ${\\mathcal{O}}(k^{2})$ non-zero entries due to the WK kernel\u2019s focus on item pairs within the top- $k$ . Consequently, the runtime for $\\mathbf{mvm}(K_{X_{t}})$ in the WK kernel matrix is reduced to $\\mathcal{O}(k^{2}\\cdot t)$ . ", "page_idx": 4}, {"type": "text", "text": "Moving forward, we focus on deriving a sparse feature vector for the WCK kernel, enabling fast MVMs with the WCK kernel, which includes the CK kernel as a special case. Notably, any convolutional kernel inherits linear features from its constituent kernel. Specifically, $\\sum_{\\underline{{\\sigma}}\\in B_{\\pi}}\\phi_{i,j}^{w k}(\\sigma)$ forms a feature vector for the WCK kernel, which follows from Equation 4 and However, computing this feature vector explicitly is computationally challenging, as it requires summing over all $\\sigma\\in B_{\\pi}$ , which includes an exponential number of terms, i.e., $\\Theta(n^{\\hat{k}})$ . ", "page_idx": 4}, {"type": "text", "text": "In response to this challenge, Claim 3 shows that the summation can be computed analytically and provides explicit linear feature vectors for the WCK and CK kernels. It also shows that $\\phi^{w c k}$ has only $O(k^{2}+2n k)$ nonzero entries among its ${\\mathcal{O}}(n^{2})$ total entries. Consequently, $\\mathbf{mvm}(K_{X_{t}})$ for ", "page_idx": 4}, {"type": "text", "text": "Claim 3. Let $\\phi^{w c k}(\\pi):\\Pi^{k}\\mapsto\\mathbb{R}^{\\binom{n}{2}}$ be a vector indexed by unique item pairs $(i,j)$ given as: $\\phi_{i,j}^{w c k}(\\pi):=$ $\\begin{array}{r}{\\frac{1}{\\sqrt{\\binom{n}{2}}}\\cdot\\mathbf{w}_{i,j}^{w c k}(\\pi)\\cdot\\left(\\mathfrak{p}_{i<j}^{\\pi}-\\mathfrak{p}_{i>j}^{\\pi}\\right)}\\end{array}$ , where $\\mathbf{w}_{i,j}^{w c k}(\\pi)$ is determined as follows:   \n$\\mathbf{w}_{i,j}^{w c k}(\\pi)=\\left\\{\\begin{array}{l l}{w_{s}(\\mathbf{p}_{i}^{\\pi},\\mathbf{p}_{j}^{\\pi})\\;\\;i f\\,\\mathbf{p}_{i}^{\\pi}\\in[k]\\;\\&\\;\\mathbf{p}_{j}^{\\pi}\\in[k]}\\\\ {w_{s}(\\mathbf{p}_{i}^{\\pi},\\cdot)\\;\\;e l s e\\;i f\\,\\mathbf{p}_{i}^{\\pi}\\in[k]\\;\\&\\;\\mathbf{p}_{j}^{\\pi}\\notin[k]\\,,}\\\\ {w_{s}(\\mathbf{p}_{j}^{\\pi},\\cdot)\\;\\;e l s e\\;i f\\,\\mathbf{p}_{i}^{\\pi}\\notin[k]\\;\\&\\;\\mathbf{p}_{j}^{\\pi}\\in[k]\\,,}\\\\ {0\\quad\\quad\\quad\\quad\\quad o t h e r w i s e,}\\end{array}\\right.$ where $w_{s}$ denotes symmetric weights and $w_{s}(\\ell,\\cdot)=$ n1\u2212k jn=k+1 ws(\u2113, j). Then, the vector \u03d5wck is a linear feature vector for the WCK kernel $k^{w c k}$ . By uniformly setting $w_{s}(\\cdot,\\cdot)\\;\\equiv\\;1$ in the definitions above, $\\phi_{i,j}^{w c k}(\\pi)$ specializes to a linear feature vector for the CK kernel. ", "page_idx": 4}, {"type": "text", "text": "the WCK kernel requires ${\\mathcal{O}}((k^{2}+2n k)\\cdot t)$ operations, which improves from ${\\mathcal{O}}(t^{2})$ to linear in $t$ . However, this introduces a dependence on $n$ , the number of items, which poses a serious limitation and is beneficial only when $n\\leq t$ . In the following theorem, we leverage redundancy in $\\phi^{w c k}$ to eliminate this dependence on $n$ , leading to the following main theorem about the $\\mathbf{mvm}(K_{X_{t}})$ . ", "page_idx": 5}, {"type": "text", "text": "Theorem 1. For the WCK kernel with product-symmetric weights $w_{p s}$ , the computational complexity of multiplying the kernel matrix $K_{X_{t}}$ with any admissible vector is $\\mathcal{O}(k^{2}t)$ , i.e., $\\mathbf{mvm}(K_{X_{t}})=\\mathcal{O}(k^{2}t)$ , where $X_{t}$ is any arbitrary set of $t$ top- $\\boldsymbol{\\cdot}\\boldsymbol{k}$ rankings. ", "page_idx": 5}, {"type": "text", "text": "Appendix A provides the proof in two steps. First, we utilize the values of $\\phi^{w c k}$ from Claim 3 and categorize $\\phi^{\\dot{w}c k}(\\pi_{1})^{T}\\phi^{w c\\dot{k}}(\\pi_{2})$ based on item pairs, as summarized in Table 4. Next, we show that only five combinations yield non-zero values, i.e., $\\begin{array}{r}{\\phi^{w c k}(\\pi_{1})^{T}\\phi^{w c k}(\\pi_{2})=\\sum_{i=1}^{5}s_{i}(\\pi_{1},\\pi_{2})}\\end{array}$ . Each term $s_{i}(\\pi_{1},\\pi_{2})$ is a dot product of vectors $\\phi^{a_{i}}(\\pi_{1})^{T}\\phi^{b_{i}}(\\pi_{2})$ , which contains at most ${\\mathcal{O}}(k^{2})$ non-zero entries. Thus, for the WCK and CK kernels, $\\mathbf{mvm}(\\dot{K}_{X_{t}})=\\mathcal{O}(k^{2}t)$ , since these vectors across all five terms include only ${\\mathcal{O}}(k^{2})$ non-zero entries. Consequently, Theorem 1 demonstrates that employing these vector representations for top- $\\cdot\\mathbf{k}$ rankings leads to faster MVMs, i.e., $\\mathbf{mvm}(K_{X_{t}})=$ $O(\\dot{k}^{2}\\dot{t})\\ll O(t^{2})$ . ", "page_idx": 5}, {"type": "text", "text": "4 Proposed GP-TopK Bandit Algorithm ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we begin by formally defining the top- $k$ recommendation problem within a bandit framework and introduce a generic contextual bandit algorithm, detailed in Algorithm 1. We then explain how the components of the algorithm are instantiated using the proposed GP approach, followed by an analysis of its computational complexity and cumulative regret. ", "page_idx": 5}, {"type": "text", "text": "Let $T$ denote the number of rounds. Contexts $\\mathcal{C}$ are represented in a finite $c$ -dimensional space, i.e., $\\mathcal{C}\\subseteq\\mathbb{R}^{c}$ . In the $t^{t h}$ round, we receive a context $\\mathbf{c}_{t}\\,\\in\\,\\mathcal{C}$ and select a top- $k$ ranking $\\pi_{t}\\in\\Pi^{k}$ . Subsequently, a noisy reward $y_{t}=\\hat{f}(\\mathbf{c}_{t},\\pi_{t})+\\epsilon_{t}$ is observed, where $\\hat{f}$ is the true reward function and $\\epsilon_{t}$ is round-independent noise. The regret is defined as $r_{t}:=\\operatorname*{max}\\pi^{'}\\in\\Pi^{k}\\hat{f}(\\mathbf{c}_{t},\\pi^{'})-\\hat{f}(\\mathbf{c}_{t},\\pi_{t})$ , with cumulative regret $\\begin{array}{r}{R_{T}:=\\sum_{t=1}^{T}r_{t}}\\end{array}$ . The accumulated data at the $t^{t h}$ round is $D_{t}=\\left(\\mathbf{c}_{i},\\pi_{i},y_{i}\\right)_{i=1}^{t}$ Below, the Algorithm 1 p rovides provides a generic schematic of the bandit algorithm. ", "page_idx": 5}, {"type": "table", "img_path": "50nEnmVLRb/tmp/d1b8a0fafecd653a596878d1a88a41d0f6f2060775aef11fcfafd2ea7ce265bc.jpg", "table_caption": [], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "We aim to design the components of above Algorithm 1 with the objectives of minimizing cumulative regret and ensuring computational efficiency. It requires two key components: (a) a reward model $\\mathcal{M}_{t}$ that estimates the reward for any context and top- $k$ ranking utilizing the accumulated data $\\mathcal{D}_{t}$ and (b) an acquisition function $A{\\mathcal F}$ for selecting $\\pi_{t}$ given the reward model $\\mathcal{M}_{t}$ and observed context $\\mathbf{c}_{t}$ ", "page_idx": 5}, {"type": "text", "text": "Reward model $\\mathcal{M}$ and acquisition function $A{\\mathcal F}$ . The proposed GP-TopK bandit algorithm leverages GP regression to model the reward function over the domain of contexts and top- $\\cdot\\mathbf{k}$ rankings. Section B.1 briefs GP regression for the completeness. Essentially, the reward model $\\mathcal{M}$ maintains a distribution over functions $f$ , i.e., $f\\sim\\mathcal{N}(0,k(\\cdot,\\cdot))$ , where $k$ is a product kernel function over both contexts and top- $\\cdot\\mathbf{k}$ rankings $(\\mathcal{C}\\otimes\\Pi^{k})$ . Specifically, the kernel function $k$ is defined as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\nk((\\mathbf{c}_{1},\\pi_{1}),(\\mathbf{c}_{2},\\pi_{2})):=k^{c}(\\mathbf{c}_{1},\\mathbf{c}_{2})\\cdot k^{r}(\\pi_{1},\\pi_{2}),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $k^{c}(\\mathbf{c}_{1},\\mathbf{c}_{2})=\\mathbf{c}_{1}^{T}\\mathbf{c}_{2}$ is the dot-product kernel and $k^{r}$ is a kernel for top- $\\cdot\\mathbf{k}$ rankings. We use variants of the Kendall kernel for $k^{r}$ from Section 2. Updating the reward model $\\mathcal{M}_{t}$ at the $t^{\\mathrm{th}}$ round involves adding new data points to our GP regression, which is computationally inexpensive compared to the fine-tuning steps required by parametric models to incorporate the latest feedback. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "We use the UCB function as the acquisition function, balancing exploration and exploitation by selecting actions that maximize the upper confidence bound of the estimated reward [27]. The UCB acquisition function is $\\begin{array}{r}{\\varDelta\\mathcal{F}(\\mathcal{M}_{t}(\\mathbf{c}_{t},\\pi)):=\\mu_{f|\\mathcal{D}}((\\mathbf{c}_{t},\\pi))+\\beta^{\\frac{1}{2}}\\cdot\\sigma_{f|\\mathcal{D}}((\\mathbf{c}_{t},\\pi))}\\end{array}$ , where $\\sigma_{f|\\mathcal{D}}((\\mathbf{c}_{t},\\pi))=$ $\\sqrt{k_{f|\\mathcal{D}}((\\mathbf{c}_{t},\\pi),(\\mathbf{c}_{t},\\pi))}$ and $\\beta$ controls the trade-off between exploration and exploitation. Here, $\\mu_{f\\mid\\mathcal{D}}$ and $k_{f|\\mathcal{D}}$ are the GP posterior mean and covariance functions, as detailed in Section B.1. At the $t^{\\mathrm{th}}$ round, the algorithm selects the top- $\\cdot\\mathbf{k}$ ranking $\\pi\\in\\Pi^{k}$ that maximizes $\\mathcal{A F}(\\mathcal{M}_{t}(\\mathbf{c}_{t},\\pi))$ , which is performed using local search [19], as detailed further in Appendix B. ", "page_idx": 6}, {"type": "text", "text": "Computational complexity. The GP-TopK bandit algorithm does not require compute for model updates. In other words, updating $\\mathcal{M}_{t}$ , i.e., in the Line 5 of the Algorithm 1 requires only updating the list of accumulated feedback data $\\mathcal{D}_{t}$ . The GP-TopK relies on local search to optimize $A{\\mathcal F}$ , so the computational demands stem solely from $A{\\mathcal F}$ evaluations within the local search. As shown in Section B.1, computing the GP variance term for evaluating $A{\\mathcal F}$ , i.e, $\\sigma_{f|\\mathcal{D}}\\big((\\mathbf{c}_{t},\\pi)\\big)$ involves solving $\\left[K_{X_{t}}+\\sigma^{2}I\\right]^{-1}\\mathbf{v}$ for a vector $\\mathbf{v}$ , where $X_{t}\\;=\\;\\left[(\\mathbf{c}_{1},\\pi_{1}),\\cdot\\cdot\\cdot\\mathbf{\\mu},(\\mathbf{c}_{t},\\pi_{t})\\right]$ . Naively, this operation requires $\\mathcal{O}(t^{3})$ time per round, amounting to total ${\\mathcal{O}}(T^{4})$ over $T$ rounds. Iterative algorithms, however, can expedite the process by leveraging fast MVMs with kernel matrices, as discussed in Section 3. Below, Theorem 2 formalizes the computational demands of the GP-TopK algorithm. ", "page_idx": 6}, {"type": "text", "text": "Theorem 2. Assuming a fixed number of iterations required by the iterative algorithms, the total computational time for running the GP-TopK bandit algorithm for $T$ rounds of top- $.k$ recommendations, using the contextual product kernel (Equation $6$ ), is $\\mathcal{O}(k^{2}c\\ell T^{2})$ . This applies to WK, CK, and WCK top- $.k$ ranking kernels, where $\\ell$ is the number of local search evaluations. ", "page_idx": 6}, {"type": "text", "text": "The proof of Theorem 2, provided in Appendix B, demonstrates efficiency gains from combining feature representations with iterative algorithms, reducing computational time from ${\\mathcal{O}}(T^{4})$ to $\\mathcal{O}(T^{2})$ . This is a substantial improvement, as even a single MVM with the matrix $K_{X_{t}}$ using the full kernel matrix at each round would require $\\mathcal{O}(T^{3})$ compute time. Additionally, the theorem shows that the running time of the GP-TopK algorithm does not explicitly depend on the number of items $n$ . ", "page_idx": 6}, {"type": "text", "text": "Regret analysis. The cumulative regret is $\\begin{array}{r}{R_{T}=\\sum_{t=1}^{T}\\operatorname*{max}_{\\pi^{'}\\in\\Pi^{k}}\\hat{f}(\\mathbf{c}_{t},\\pi^{'})\\!-\\!\\hat{f}(\\mathbf{c}_{t},\\pi_{t})}\\end{array}$ , where $\\pi_{t}$ is the ranking chosen at round $t$ . Optimizing cumulative regret for top- $k$ recommendations is challenging, as it requires learning the context-arm relationship and matching the best possible mapping. To bound cumulative regret, regularity assumptions are essential, as noted in prior works [27, 14]. We consider the following two assumptions, either of which suffices. Also, $\\boldsymbol{\\mathcal{X}}:=\\boldsymbol{\\mathcal{C}}\\otimes\\Pi^{k}$ for below assumptions. ", "page_idx": 6}, {"type": "text", "text": "Assumption 1. $\\mathcal{X}$ is finite, meaning that only finite contexts are considered $(|\\mathcal{C}|<\\infty)$ , and the reward function $\\hat{f}$ is sampled from the GP prior with a noise variance of $\\xi^{2}$ . ", "page_idx": 6}, {"type": "text", "text": "Assumption 2. $\\mathcal{X}$ is arbitrary and the reward function $\\hat{f}$ has a bounded RKHS norm for the kernel $k$ , i.e., $\\|f\\|_{k}\\leq B$ . The reward noises $\\epsilon_{t}$ form an arbitrary martingale difference sequence (i.e., reward noise does not systematically depend on its past values) and are uniformly bounded by $\\xi$ . ", "page_idx": 6}, {"type": "text", "text": "The following theorem proves the regret bound for the GP-TopK algorithm under Assumption 1 or 2. ", "page_idx": 6}, {"type": "text", "text": "Theorem 3. If either Assumptions $^{\\,l}$ or 2 hold, setting $\\beta_{t}$ as $2\\log\\left(\\frac{|\\boldsymbol{C}|\\cdot|\\boldsymbol{\\Pi}^{k}|\\cdot t^{2}\\cdot\\pi^{2}}{6\\delta}\\right)$ and $\\mathrm{300}\\gamma_{t}\\ln^{3}\\left(\\frac{t}{\\delta}\\right)$ respectively, the cumulative regret $\\mathcal{R}_{T}$ of the $G P$ -TopK bandit algorithm for top- $k$ recommendations can, with at least $1\\ -\\ \\delta$ probability, be bounded by $\\tilde{\\mathcal{O}}(n\\sqrt{C_{1}T c(\\log|\\mathcal{C}|+k+\\log(T^{2}\\pi^{2}/6\\delta))})$ ) under Assumption $^{\\,l}$ , and $\\tilde{\\mathcal{O}}(n\\sqrt{C_{1}(2B^{2}c+300n^{2}c^{2}\\ln^{3}(T/\\delta))T})$ under Assumption 2. Here, $\\begin{array}{r}{C_{1}=\\frac{8}{\\log(1+\\xi^{-2})}}\\end{array}$ , and $\\tilde{\\mathcal{O}}$ excludes logarithmic factors related to $n,\\,k,$ , and . ", "page_idx": 6}, {"type": "text", "text": "Appendix B.4 provides the proof, leveraging the insight that $\\log\\operatorname*{det}[I+\\xi^{-2}\\cdot K_{X_{T}}|$ for any set $X_{T}$ can be effectively bounded using the finite-dimensional feature vectors introduced in this work. ", "page_idx": 6}, {"type": "text", "text": "Specifically, Proposition 2 utilizes the feature vectors from Section 2. Building on Proposition 2, Theorem 3 establishes that the cumulative regret of the GP-TopK bandit algorithm grows sublinearly in $T$ with high probability for both assumptions. Furthermore, this result also underscore the importance of using top- $\\cdot\\mathbf{k}$ ranking kernels, which improve the asymptotic order in terms of $n$ by factors of $n^{k/2-1}$ and $n^{k-1}$ under Assumptions 1 and 2, respectively, compared to Srinivas et al. [27]. This improvement is substantial even for small values of $k$ , such as $k=6$ , as shown in Table 3. ", "page_idx": 7}, {"type": "table", "img_path": "50nEnmVLRb/tmp/29901ab08f63e9a2d96164d5b3df5e46c4f39fe1097f941b6de819472dd90c6b.jpg", "table_caption": ["Table 3: Comparison with Srinivas et al. (2010) on regret bounds for the bandit algorithm under both assumptions. Definitions of notations are provided in the main text. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "This section empirically evaluates the proposed GP-TopK bandit algorithms for the top- $\\cdot\\mathbf{k}$ recommendations using a simulation based on the MovieLens dataset [4]. The reliance on simulation for evaluating bandit algorithms is prevalent in the literature. It stems from the difficulty of conducting online evaluations in real-world bandit scenarios, mainly when there are combinatorial arms [28]. Next, we provide details of the simulation setup and considered reward settings. Following that, we present results for the empirical regret for small and large numbers of arms below, respectively. ", "page_idx": 7}, {"type": "text", "text": "Simulation setup and reward settings. The bandit simulation setup follows the framework outlined by Jeunen et al. [8], utilizing real-world datasets on user-item interactions. Specifically, we train user and item embeddings using a collaborative flitering approach [6]. The user embeddings are accessed by the bandit algorithms as context embeddings, while the item embeddings remain hidden. In the non-contextual setup, the first user from the dataset is chosen as a fixed context throughout the bandit algorithm run, allowing us to use the same reward functions as the contextual bandit algorithm. ", "page_idx": 7}, {"type": "text", "text": "For setting up the reward functions, we utilize a similarity function $s(\\mathbf{c},\\theta):=\\varsigma(a\\cdot(\\mathbf{c}^{T}\\theta)-b)$ to measure similarity between any user and item embeddings, where $a$ and $b$ are similarity score and shift scalars, respectively. The sigmoid function $\\varsigma$ maps similarity scores to a range between 0 and 1, enhancing the interpretability of the reward signal [31]. We set $a$ and $b$ to 6 and 0.3, respectively, to fully utilize the range of the similarity function, as assessed by evaluating its value for many arms. ", "page_idx": 7}, {"type": "text", "text": "We set up two preliminary reward functions based on the similarity function $s$ . The first is the DCG metric, $\\begin{array}{r}{\\hat{f}_{\\mathrm{dcg}}^{-}(\\mathbf{c},\\pi)=\\stackrel{\\cdot}{\\sum}_{i=1}^{k}\\frac{1}{\\log_{2}(i+1)}s(\\mathbf{c},\\theta_{\\pi_{i}}).}\\end{array}$ , where c and $\\theta_{\\pi_{i}}$ represent the context and item embeddings, respectively. The second is the diversity measure, $\\begin{array}{r}{\\hat{f}_{\\mathrm{div}}(\\pi)=\\frac{1}{k^{2}}\\sum_{i=1}^{k}\\sum_{j=1}^{k}\\theta_{\\pi_{j}}^{T}\\theta_{\\pi_{i}}}\\end{array}$ These metrics quantify the relevance and diversity of top- $\\cdot\\mathbf{k}$ recommendations, respectively. ", "page_idx": 7}, {"type": "text", "text": "We use these functions in two contextual reward settings. The first setting focuses on normalizedDCG (n-DCG), $\\begin{array}{r}{\\hat{f}_{\\mathrm{ndcg}}(\\mathbf{c},\\pi)\\,=\\,\\frac{\\hat{f}_{\\mathrm{dcg}}(\\mathbf{c},\\pi)}{\\operatorname*{max}_{\\pi^{\\prime}}\\,\\hat{f}_{\\mathrm{dcg}}(\\mathbf{c},\\pi^{\\prime})}}\\end{array}$ maxf\u02c6\u03c0d\u2032cg f(\u02c6dcc,g\u03c0(c),\u03c0\u2032) [7]. The second setting combines f\u02c6ndcg and f\u02c6div as $\\hat{f}_{\\mathrm{ndcgdiv}}(\\mathbf{c},\\pi)=\\lambda\\cdot\\hat{f}_{\\mathrm{ndcg}}(\\mathbf{c},\\pi)+(1-\\lambda)\\cdot\\hat{f}_{\\mathrm{div}}(\\pi)$ , evaluating the aggregate effect of relevance and diversity. We set $\\lambda=0.75$ to emphasize relevance over diversity. ", "page_idx": 7}, {"type": "text", "text": "Evaluation for small arm space. This section presents empirical results for the cumulative regret of bandit algorithms with a limited number of arms. Specifically, with $n=20$ and $k=3$ , there are 6, 840 top- $\\cdot\\mathbf{k}$ rankings, allowing for an exhaustive search to optimize the acquisition function. All bandit algorithms run in batch mode, updating every five rounds. We consider both reward settings for contextual and non-contextual scenarios, using a subset of five users for the contextual setting. ", "page_idx": 7}, {"type": "text", "text": "Several baselines are set to assess the benefits of ranking (Kendall) kernels. Section C details the remaining hyper-parameter configurations and details of other baseline bandit algorithms. ", "page_idx": 8}, {"type": "image", "img_path": "50nEnmVLRb/tmp/14ed70a9f98a8cfd72e64368c521aea9eff1b9d21f9c34a93b85d9596f96fc87.jpg", "img_caption": ["Figure 2: Comparative evaluation of bandit algorithms: The cumulative regret $R_{T}$ over $T$ rounds is shown. Lower values indicate better performance. Plots (a) and (b) represent non-contextual settings for nDCG $\\hat{f}_{\\mathrm{ndcg}})$ and $\\mathrm{\\nDCG{\\}+}$ diversity $(\\hat{f}_{\\mathrm{ndcgdiv}})$ rewards, respectively. Plots (c) and (d) show results for contextual settings for five users using the same rewards. The y-axis for (a) and (b) is on the left, and for (c) and (d) on the right. The GP-TopK algorithm with Kendall kernels, especially the weighted convolutional Kendall (WCK) kernel, outperforms others. Details on other algorithms are in the text. Results are averaged over six trials. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "The Random algorithm randomly recommends any $\\boldsymbol{\\mathrm{k}}$ items. The $\\epsilon$ -greedy algorithm alternates between recommending a random top-k ranking with a probability of $\\epsilon$ and choosing the top- $\\cdot\\mathbf{k}$ ranking with the highest observed mean reward. In contextual settings, $\\epsilon$ -greedy differentiates arms for each unique context. Similarly, MAB-UCB conceptualizes each ranking as an independent arm, an equivalent of using a direct delta kernel approach for GPs along with UCB $A{\\mathcal F}$ . In contextual scenarios, MAB-UCB also treats arms distinctly per context. Each variant of the top-k ranking kernel yields one variation of the proposed GP-TopK algorithm, namely, WK, CK, and WCK. Figure 2 presents empirical values of the cumulative regrets for the above baseline and the proposed GP-TopK algorithms. In all cases, across both reward settings and in both contextual and non-contextual setups, the variants of the proposed GP-TopK algorithm outperform baselines that do not use Kendall kernels, highlighting the significance of top-k ranking kernels for full bandit feedback. Specifically, the CK and WCK kernels significantly outperform the WK kernel regarding the converged values of the regret, with the WCK kernel further improving on the CK kernel variant. ", "page_idx": 8}, {"type": "text", "text": "Evaluation for large arm space. We evaluate bandit algorithms in a large arm space scenario with $n=$ 50 and $k=3$ and $k=5$ , resulting in $1.1\\times10^{5}$ and $1.1\\!\\times\\!10^{10}$ possible top-k rankings, respectively. Using local search, we focus on the nDCG reward. The remaining configuration is consistent with the small arm space setup. We use 10 restarts and 5 steps in each search direction for the local search, starting with 1000 initial candidates. ", "page_idx": 8}, {"type": "text", "text": "Figure 3 shows that the regret for the GP-TopK variants remains consistently lower even with a large arm ", "page_idx": 8}, {"type": "image", "img_path": "50nEnmVLRb/tmp/6a79f041d3ca9cb89bdd5848f527e59e4eb3ba476d35dc1cdab2852f8123e9bc.jpg", "img_caption": ["Figure 3: Comparative evaluation of bandit algorithms for large arm spaces, with $>\\dot{1}.1\\times10^{5}$ arms for the left plot and $>1.1\\stackrel{\\smile}{\\times}10^{10}$ arms for the right plot. Cumulative regret with respect to the rounds of the bandit algorithm is depicted. Results are averaged over six trials. In both settings, the WCK approach outperforms other baselines. For more details, see the textual description. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "space, despite the use of local search. The WCK approach significantly outperforms the CK, especially for $k=5$ , as illustrated in the right plot of Figure 3. Additional empirical results on the effectiveness of local search in a large arm space and other rewards are given in Appendix C. ", "page_idx": 8}, {"type": "text", "text": "6 Discussion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "This work develops a contextual bandit algorithm for top- $k$ recommendations using Gaussian processes with Kendall kernels in a full-bandit feedback setting, without restrictive assumptions about feedback or reward models. Gaussian processes provide computationally efficient model updates for accumulated feedback data, although inference can be challenging. We address this by deriving features for Kendall kernels tailored to top- $k$ rankings, resulting in a faster inference algorithm that reduces complexity from ${\\mathcal{O}}(T^{4})$ to ${\\mathcal{O}}(T^{2}{\\bar{)}}$ . While demonstrated here for the product kernel between contexts and top- $k$ rankings, these computational improvements extend naturally to other kernel types, such as additive kernels. Additionally, we address limitations of known variants and propose a more expressive Kendall kernel for top- $k$ recommendations. Finally, we provide both theoretical and empirical results demonstrating the improved performance of the proposed GP-TopK algorithm. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Future Directions and Limitations. This work opens several research avenues. Efficient matrixvector multiplication with Kendall kernel matrices can enable faster bandit algorithms with various acquisition functions, such as Thompson sampling and expected improvement. Exploring other kernels, like Mallow kernels, for top- $\\cdot\\mathbf{k}$ rankings and developing efficient algorithms for them is an intriguing direction, especially since the effectiveness of our algorithm depends on the function space induced by the RKHS of the underlying kernel. Assessing how well these kernels approximate various reward functions for top- $\\cdot\\mathbf{k}$ recommendations would provide valuable insights. ", "page_idx": 9}, {"type": "text", "text": "Exploring other bandit problem settings, such as stochastic item availability or delayed feedback, would enhance the applicability of this work to more complex scenarios. Extending the finitedimensional GP framework to other acquisition functions using local search is another promising direction. One limitation of our regret analysis is that it does not account for approximations in the arm selection step due to local search [20]. This limitation is common in continuous domains, where optimizing acquisition functions often involves non-convex optimization [27]. ", "page_idx": 9}, {"type": "text", "text": "Impact. This research advances bandit algorithms for top-k item recommendations. By improving recommendation efficiency and accuracy, our algorithms can enhance user experiences across platforms, promoting content relevancy and engagement. However, they may reinforce implicit biases in training data, limiting content diversity and entrenching prejudices. Therefore, monitoring over time is essential when deploying these algorithms in real-world environments. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] R\u00f3bert Busa-Fekete, Bal\u00e1zs Sz\u00f6r\u00e9nyi, Paul Weng, and Shie Mannor. Multi-objective bandits: Optimizing the generalized gini index. In Proceedings of the 34th International Conference on Machine Learning (ICML), pages 625\u2013634. PMLR, 2017.   \n[2] Aryan Deshwal, Syrine Belakaria, Janardhan Rao Doppa, and Dae Hyun Kim. Bayesian optimization over permutation spaces. In Proceedings of the 25th Conference on Artificial Intelligence (AAAI), pages 6515\u20136523, 2022.   \n[3] Jacob R. Gardner, Geoff Pleiss, David Bindel, Kilian Q. Weinberger, and Andrew Gordon Wilson. GPyTorch: Blackbox matrix-matrix Gaussian process inference with GPU acceleration. Advances in the 31st Neural Information Processing Systems (NeurIPS), pages 7576\u20137586, 2018.   \n[4] F Maxwell Harper and Joseph A Konstan. The Movielens datasets: History and context. In Transactions on Interactive Intelligent Systems, volume 5, pages 1\u201319. ACM, 2015.   \n[5] David Haussler. Convolution kernels on discrete structures. Technical report, Technical report, Department of Computer Science, University of California Santa Cruz, 1999.   \n[6] Yifan Hu, Yehuda Koren, and Chris Volinsky. Collaborative filtering for implicit feedback datasets. In 8th IEEE International Conference on Data Mining (ICDM), pages 263\u2013272. IEEE, 2008.   \n[7] Kalervo Jarvelin and Jaana Kekalainen. Cumulated gain-based evaluation of ir techniques. In ACM Transactions of Information Systems, volume 20, pages 422\u2013446, 2002.   \n[8] Olivier Jeunen and Bart Goethals. Top-k contextual bandits with equity of exposure. In Proceedings of the 15th ACM Conference on Recommender Systems (RecSys), pages 310\u2013320, 2021.   \n[9] Yunlong Jiao and Jean-Philippe Vert. The Kendall and Mallows kernels for permutations. In Proceedings of the 32nd International Conference on Machine Learning (ICML), pages 1935\u20131944. PMLR, 2015.   \n[10] Yunlong Jiao and Jean-Philippe Vert. The weighted Kendall and high-order kernels for permutations. In Proceedings of the 35th International Conference on Machine Learning (ICML), volume 80, pages 2314\u20132322. PMLR, 2018.   \n[11] Michael N. Katehakis and Arthur F. Veinott. The multi-armed bandit problem: Decomposition and computation. In Mathematics of Operations Research, volume 12, pages 262\u2013268, 1987.   \n[12] Maurice G Kendall. A new measure of rank correlation. Biometrika, 30(1/2):81\u201393, 1938.   \n[13] Bart P. Knijnenburg, Martijn C. Willemsen, Zeno Gantner, Hakan Soncu, and Chris Newell. Explaining the user experience of recommender systems. In User Modeling and User-Adapted Interaction, volume 22, pages 441\u2013504, 2011.   \n[14] Andreas Krause and Cheng Ong. Contextual gaussian process bandit optimization. Advances in the 24th Neural Information Processing Systems (NeurIPS), page 2447\u20132455, 2011.   \n[15] Matevvz Kunaver and Tomavz Povzrl. Diversity in recommender systems - a survey. In Knowledge Based Systems, volume 123, pages 154\u2013162, 2017.   \n[16] Branislav Kveton, Csaba Szepesvari, Zheng Wen, and Azin Ashkan. Cascading bandits: Learning to rank in the cascade model. In Proceedings of the 32nd International Conference on Machine Learning (ICML), pages 767\u2013776. PMLR, 2015.   \n[17] E. Lex, Dominik Kowald, Paul Seitlinger, Thi Ngoc Trang Tran, Alexander Felfernig, and Markus Schedl. Psychology-informed recommender systems. In Foundations and Trends in Information Retrieval, volume 15, pages 134\u2013242, 2021.   \n[18] Zeyang Liu, Yiqun Liu, Ke Zhou, Min Zhang, and Shaoping Ma. Influence of vertical result in web search examination. In Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 193\u2013202, 2015.   \n[19] Changyong Oh, Jakub Tomczak, Efstratios Gavves, and Max Welling. Combinatorial bayesian optimization using the graph Cartesian product. Advances in the 32nd Neural Information Processing Systems (NeurIPS), 2019.   \n[20] My Phan, Yasin Abbasi Yadkori, and Justin Domke. Thompson sampling and approximate inference. Advances in the 32nd Neural Information Processing Systems (NeurIPS), 32, 2019.   \n[21] Lijing Qin, Shouyuan Chen, and Xiaoyan Zhu. Contextual combinatorial bandit and its application on diversified online recommendation. In In Proceedings of the 2014 SIAM International Conference on Data Mining, pages 461\u2013469, 2014.   \n[22] Carl Edward Rasmussen. Evaluation of Gaussian processes and other methods for non-linear regression. PhD thesis, University of Toronto Toronto, Canada, 1997.   \n[23] Carl Edward Rasmussen. Gaussian Processes in Machine Learning. In Advanced Lectures on Machine Learning, pages 63\u201371. Springer, 2004.   \n[24] Idan Rejwan and Yishay Mansour. Top- $k$ combinatorial bandits with full-bandit feedback. In Algorithmic Learning Theory, pages 752\u2013776. PMLR, 2020.   \n[25] Yunus Saat\u00e7i. Scalable inference for structured Gaussian process models. PhD thesis, University of Cambridge, 2012.   \n[26] Dong-Her Shih, Kuan-Chu Lu, and Po-Yuan Shih. Exploring shopper\u2019s browsing behavior and attention level with an eeg biosensor cap. In Brain Sciences, volume 9, page 301, 2019.   \n[27] Niranjan Srinivas, Andreas Krause, Sham M Kakade, and Matthias Seeger. Gaussian process optimization in the bandit setting: No regret and experimental design. In Proceedings of the 27th International Conference on Machine Learning (ICML), pages 1015\u20131022, 2010.   \n[28] Hastagiri P Vanchinathan, Isidor Nikolic, Fabio De Bona, and Andreas Krause. Exploreexploit in top-n recommender systems via Gaussian processes. In Proceedings of the 8th ACM Conference on Recommender Systems (RecSys), pages 225\u2013232, 2014.   \n[29] Richard S. Varga. Ger\u0161gorin and his circles. 2004.   \n[30] Siwei Wang and Wei Chen. Thompson sampling for combinatorial semi-bandits. In Proceedings of the 35th International Conference on Machine Learning (ICML), pages 5114\u20135122, 2018.   \n[31] Yingfei Wang, Hua Ouyang, Chu Wang, Jianhui Chen, Tsvetan Asamov, and Yi Chang. Efficient ordered combinatorial semi-bandits for whole-page recommendation. In Proceedings of the 20th Conference on Artificial Intelligence (AAAI), volume 31, page 2746\u20132753, 2017.   \n[32] Yue Wang, Dawei Yin, Luo Jie, Pengyuan Wang, Makoto Yamada, Yi Chang, and Qiaozhu Mei. Beyond ranking: Optimizing whole-page presentation. Proceedings of the Ninth ACM International Conference on Web Search and Data Mining, pages 103\u2013112, 2016.   \n[33] Christopher K. I. Williams and Carl Edward Rasmussen. Gaussian Processes for Regression. Advances in the 9th Neural Information Processing Systems (NeurIPS), pages 514\u2013520, 1996.   \n[34] Dong Xin, Jiawei Han, and Kevin Chen-Chuan Chang. Progressive and selective merge: computing top-k with ad-hoc ranking functions. In ACM SIGMOD Conference, pages 103\u2013114, 2007. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Kendall Kernels for Full and Top-k Rankings \u2013 Omitted Details ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "This section includes the proofs that were omitted from Section 2, presented in the following order: ", "page_idx": 12}, {"type": "text", "text": "\u2022 In Section A.1, we present proofs for Claims 2 and 3, which concern the feature representations of Kendall kernels.   \n\u2022 In Section A.2, we provide Algorithms 2 and a proof of its correctness for computing the WCK kernel in $\\mathcal{O}(\\bar{k}^{2})$ time, thereby proving Claim 1. Additionally, we extend this proof to cover the proof of correctness for Algorithm 3, which can compute the CK kernel in ${\\mathcal{O}}(k\\log k)$ , initially introduced by Jiao et al. [9]. The original paper presented the algorithm without a formal proof of correctness, a gap we address and fill in this section.   \n\u2022 Section A.3 details the proof for Theorem 1, discussing the matrix-vector multiplications with the Kendall kernel matrix for top- $\\cdot\\mathbf{k}$ rankings. This proof builds on the Algorithm 2 given for computing the WCK kernel for top-k rankings. ", "page_idx": 12}, {"type": "text", "text": "A.1 Feature Representation for Kendall Kernels for Top-k Rankings ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "This section revisits the claims regarding the feature representations of the weighted Kendall kernel and the weighted convolutional Kendall kernel, subsequently providing the proofs for these claims mentioned earlier. ", "page_idx": 12}, {"type": "text", "text": "Claim 2. Let $\\phi^{w k}(\\sigma):\\Pi^{n}\\mapsto\\mathbb{R}^{\\binom{n}{2}}$ be a vector indexed by unique item pairs $(i,j)$ , defined as: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\phi_{i,j}^{w k}(\\sigma):=\\frac{1}{\\sqrt{\\binom{n}{2}}}\\cdot w_{s}(\\mathbf{p}_{i}^{\\sigma},\\mathbf{p}_{j}^{\\sigma})\\cdot\\left(\\mathfrak{p}_{i<j}^{\\sigma}-\\mathfrak{p}_{i>j}^{\\sigma}\\right),\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where $w_{s}$ is the symmetric weighting function in product-symmetric weights. Then, $\\phi^{w k}$ is $a$ linear feature vector for the weighted Kendall kernel with product-symmetric weights $w_{p s}$ . ", "page_idx": 12}, {"type": "text", "text": "Proof. Following the definition of linear feature representation, we need to prove that $k^{w k}(\\sigma_{1},\\sigma_{2})=$ $\\phi(\\sigma_{1})^{T}\\phi(\\sigma_{2})$ for the product-symmetric weight kernel as given in Equation 5. Recalling from Equation 2, we have $k^{w k}(\\sigma_{1},\\sigma_{2})$ as follows: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{k^{w k}(\\sigma_{1},\\sigma_{2})=\\frac{1}{\\binom{n}{2}}\\cdot\\sum_{i<j}w((\\mathbf{p}_{i}^{\\sigma_{1}},\\mathbf{p}_{j}^{\\sigma_{1}}),(\\mathbf{p}_{i}^{\\sigma_{2}},\\mathbf{p}_{j}^{\\sigma_{2}}))\\cdot\\eta_{i,j}(\\sigma_{1},\\sigma_{2}),}}\\\\ &{}&{=\\frac{1}{\\binom{n}{2}}\\cdot\\sum_{i<j}w_{s}(\\mathbf{p}_{i}^{\\sigma_{1}},\\mathbf{p}_{j}^{\\sigma_{1}})\\cdot w_{s}(\\mathbf{p}_{i}^{\\sigma_{2}},\\mathbf{p}_{j}^{\\sigma_{2}})\\cdot\\eta_{i,j}(\\sigma_{1},\\sigma_{2}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where the second line incorporates the use of the product-symmetric weight kernel. Next, our focus shifts to the simplification of $\\eta_{i,j}(\\sigma_{1},\\sigma_{2})$ , which is elaborated as follows: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\eta_{i,j}(\\sigma_{1},\\sigma_{2})=\\mathfrak{p}_{i<j}^{\\sigma_{1}}\\cdot\\mathfrak{p}_{i<j}^{\\sigma_{2}}+\\mathfrak{p}_{i>j}^{\\sigma_{1}}\\cdot\\mathfrak{p}_{i>j}^{\\sigma_{2}}-\\mathfrak{p}_{i<j}^{\\sigma_{1}}\\cdot\\mathfrak{p}_{i>j}^{\\sigma_{2}}-\\mathfrak{p}_{i>j}^{\\sigma_{1}}\\cdot\\mathfrak{p}_{i<j}^{\\sigma_{2}},}\\\\ &{\\qquad\\qquad\\qquad=\\mathfrak{p}_{i<j}^{\\sigma_{1}}\\cdot(\\mathfrak{p}_{i<j}^{\\sigma_{2}}-\\mathfrak{p}_{i>j}^{\\sigma_{2}})+\\mathfrak{p}_{i>j}^{\\sigma_{1}}\\cdot(\\mathfrak{p}_{i>j}^{\\sigma_{2}}-\\mathfrak{p}_{i<j}^{\\sigma_{2}}),}\\\\ &{\\qquad\\qquad=(\\mathfrak{p}_{i<j}^{\\sigma_{1}}-\\mathfrak{p}_{i>j}^{\\sigma_{1}})\\cdot(\\mathfrak{p}_{i<j}^{\\sigma_{2}}-\\mathfrak{p}_{i>j}^{\\sigma_{2}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Combining the above factorization of $\\eta_{i,j}$ with Equation 7, we get: ", "page_idx": 12}, {"type": "equation", "text": "$$\nk^{w k}(\\sigma_{1},\\sigma_{2})=\\frac{1}{{\\binom{n}{2}}}\\cdot\\sum_{i<j}w_{s}(\\mathbf{p}_{i}^{\\sigma_{1}},\\mathbf{p}_{j}^{\\sigma_{1}})\\cdot w_{s}(\\mathbf{p}_{i}^{\\sigma_{2}},\\mathbf{p}_{j}^{\\sigma_{2}})\\cdot(\\mathfrak{p}_{i<j}^{\\sigma_{1}}-\\mathfrak{p}_{i>j}^{\\sigma_{1}})\\cdot(\\mathfrak{p}_{i<j}^{\\sigma_{2}}-\\mathfrak{p}_{i>j}^{\\sigma_{2}})\n$$", "text_format": "latex", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\displaystyle\\frac{1}{\\binom{n}{2}}\\cdot\\sum_{i<j}\\phi_{i,j}^{w k}(\\sigma_{1})\\cdot\\phi_{i,j}^{w k}(\\sigma_{2})}\\\\ &{=\\phi(\\sigma_{1})^{T}\\phi(\\sigma_{2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Claim 3. Let $\\phi^{w c k}(\\pi):\\Pi^{k}\\mapsto\\mathbb{R}^{\\binom{n}{2}}$ be a vector indexed by unique item pairs $(i,j)$ given as: $\\begin{array}{r}{\\phi_{i,j}^{w c k}(\\pi):=\\frac{1}{\\sqrt{\\binom{n}{2}}}\\cdot{\\bf w}_{i,j}^{w c k}(\\pi)\\cdot\\big({\\bf p}_{i<j}^{\\pi}-{\\bf p}_{i>j}^{\\pi}\\big).}\\end{array}$ , where $\\mathbf{w}_{i,j}^{w c k}(\\pi)$ is determined as follows: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbf{w}_{i,j}^{w c k}(\\pi)=\\left\\{\\begin{array}{l l}{w_{s}(\\mathbf{p}_{i}^{\\pi},\\mathbf{p}_{j}^{\\pi})\\;\\;i f\\,\\mathbf{p}_{i}^{\\pi}\\in[k]\\;\\&\\;\\mathbf{p}_{j}^{\\pi}\\in[k]}\\\\ {w_{s}(\\mathbf{p}_{i}^{\\pi},\\cdot)\\;\\;e l s e\\;i f\\,\\mathbf{p}_{i}^{\\pi}\\in[k]\\;\\&\\;\\mathbf{p}_{j}^{\\pi}\\notin[k]\\,,}\\\\ {w_{s}(\\mathbf{p}_{j}^{\\pi},\\cdot)\\;\\;e l s e\\;i f\\,\\mathbf{p}_{i}^{\\pi}\\notin[k]\\;\\&\\;\\mathbf{p}_{j}^{\\pi}\\in[k]\\,,}\\\\ {0\\quad\\quad\\quad\\quad\\quad o t h e r w i s e,}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $w_{s}$ denotes symmetric weights and $\\begin{array}{r}{w_{s}(\\ell,\\cdot)=\\frac{1}{n-k}\\sum_{j=k+1}^{n}w_{s}(\\ell,j)}\\end{array}$ . Then, the vector $\\phi^{w c k}$ is a linear feature vector for the WCK kernel $k^{w c k}$ . By uniformly setting $w_{s}(\\cdot,\\cdot)\\equiv1$ in the definitions above, $\\phi_{i,j}^{w c k}(\\pi)$ specializes to a linear feature vector for the CK kernel. ", "page_idx": 13}, {"type": "text", "text": "Proof. The main idea revolves around leveraging the feature representation of the Weighted Kendall kernel for a full ranking and the linearity of the convolution operation. It is already established that $k^{w k}(\\sigma_{1},\\sigma_{2})=\\phi^{w\\check{k}}(\\sigma_{1})^{T}\\phi^{w k}(\\sigma_{2})$ , as demonstrated in Claim 2. Recall that the WCK kernel requires a double summation over pairs of rankings from $B_{\\pi_{1}}$ and $B_{\\pi_{2}}$ , which represent the sets of full rankings consistent with their respective top- $\\cdot\\mathbf{k}$ rankings, as described in Equation 4. We simplify the WCK kernel as follows: ", "page_idx": 13}, {"type": "equation", "text": "$$\nk^{w c k}(\\pi_{1},\\pi_{2})=\\frac{1}{|B_{\\pi_{1}}|}\\cdot\\sum_{|B_{\\pi_{2}}|}\\cdot\\sum_{\\sigma_{1}\\in B_{\\pi_{1}}}\\sum_{\\sigma_{2}\\in B_{\\pi_{2}}}\\phi^{w k}(\\sigma_{1})^{T}\\phi^{w k}(\\sigma_{2})}\\\\ {=\\left(\\frac{1}{|B_{\\pi_{1}}|}\\cdot\\sum_{\\sigma_{1}\\in B_{\\pi_{1}}}\\phi^{w k}(\\sigma_{1})^{T}\\right)\\cdot\\underbrace{\\left(\\frac{1}{|B_{\\pi_{2}}|}\\cdot\\sum_{\\sigma_{2}\\in B_{\\pi_{2}}}\\phi^{w k}(\\sigma_{2})\\right)}_{:=\\phi^{w c k}(\\pi_{2})}}\\\\ {=\\phi^{w c k}(\\pi_{1})^{T}\\phi^{w c k}(\\pi_{2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "The simplification above reveals that the feature representation, $\\phi^{w c k}$ , for the WCK kernel, is a $\\binom{n}{2}$ dimensional vector and can be indexed by unique pairs of items $(i,j)$ , much like the $\\phi^{w k}$ . However, the double summation is over an exponentially large number of pairs of rankings. Moving forward, we shift our focus to the individual entries of this representation involving this summation, elucidating the analytical values within the summation by exploring four unique cases, each dependent on whether these specific items fall within the top-k rankings. ", "page_idx": 13}, {"type": "text", "text": "In Case 1, we examine the scenario where items $i$ and $j$ are within the top-k ranking $\\pi$ . Here, the focus is on the feature representation of the pair, specifically when both elements are ranked among the top- $\\cdot\\mathbf{k}$ positions. ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\phi_{i,j}^{w c k}(\\pi)=\\displaystyle\\frac{1}{|B_{\\pi}|}\\cdot\\sum_{\\sigma\\in B_{\\pi}}\\frac{1}{\\sqrt{{\\binom{n}{2}}}}\\cdot w_{s}(\\mathbf{p}_{i}^{\\sigma},\\mathbf{p}_{j}^{\\sigma})\\cdot\\bigl(\\mathfrak{p}_{i<j}^{\\sigma}-\\mathfrak{p}_{i>j}^{\\sigma}\\bigr)}}\\\\ {{=\\displaystyle\\frac{1}{|B_{\\pi}|}\\cdot\\frac{1}{\\sqrt{{\\binom{n}{2}}}}\\cdot w_{s}(\\mathbf{p}_{i}^{\\pi},\\mathbf{p}_{j}^{\\pi})\\cdot\\left(\\displaystyle\\sum_{\\sigma\\in B_{\\pi}}\\mathfrak{p}_{i<j}^{\\sigma}-\\displaystyle\\sum_{\\sigma\\in B_{\\pi}}\\mathfrak{p}_{i>j}^{\\sigma}\\right)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{{\\displaystyle=\\frac{1}{|B_{\\pi}|}\\cdot\\frac{1}{\\sqrt{\\binom{n}{2}}}\\cdot w_{s}(\\mathbf{p}_{i}^{\\pi},\\mathbf{p}_{j}^{\\pi})\\cdot\\big(|B_{\\pi}|\\cdot\\mathfrak{p}_{i<j}^{\\pi}-|B_{\\pi}|\\cdot\\mathfrak{p}_{i>j}^{\\pi}\\big)}}\\\\ {{\\displaystyle=\\frac{1}{\\sqrt{\\binom{n}{2}}}\\cdot w_{s}(\\mathbf{p}_{i}^{\\pi},\\mathbf{p}_{j}^{\\pi})\\cdot\\big(\\mathfrak{p}_{i<j}^{\\pi}-\\mathfrak{p}_{i>j}^{\\pi}\\big)\\,.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The simplification in lines 3rd and 4th follows from the fact that any full ranking $\\sigma\\in B_{\\pi}$ , consistent with the top- $\\cdot\\mathbf{k}$ ranking $\\pi$ , the relative ranks and weights of items $i$ and $j$ remains unchanged, given $\\mathbf{p}_{i}^{\\pi}\\in[k]$ and $\\mathbf{p}_{j}^{\\pi}\\in[k]$ . Concretely, this implies ${\\mathfrak{p}}_{i<j}^{\\sigma}={\\mathfrak{p}}_{i<j}^{\\pi}$ for all $\\sigma\\in B_{\\pi}$ and similar with the other term. ", "page_idx": 14}, {"type": "text", "text": "In Case 2, we analyze when item $i$ is in the top- $\\cdot\\mathbf{k}$ ranking while item $j$ is not. ", "page_idx": 14}, {"type": "text", "text": "Case 2: $\\mathbf{p}_{i}^{\\pi}\\in[k]$ and $\\mathbf{p}_{j}^{\\pi}\\notin[k]$ . ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\phi_{i,j}^{w c k}(\\pi)=\\displaystyle\\frac{1}{|B_{\\pi}|}\\cdot\\sum_{\\sigma\\in B_{\\pi}}\\frac{1}{\\sqrt{\\binom{n}{2}}}\\cdot w_{s}(\\mathbf{p}_{i}^{\\sigma},\\mathbf{p}_{j}^{\\sigma})\\cdot\\left(\\mathbf{p}_{i<j}^{\\sigma}-\\mathbf{p}_{i>j}^{\\sigma}\\right)}\\\\ &{\\qquad=\\displaystyle\\frac{1}{|B_{\\pi}|}\\cdot\\frac{1}{\\sqrt{\\binom{n}{2}}}\\cdot\\sum_{\\sigma\\in B_{\\pi}}w_{s}(\\mathbf{p}_{i}^{\\sigma},\\mathbf{p}_{j}^{\\sigma})\\cdot(1-0)\\ \\ (\\mathrm{since}\\ \\mathbf{p}_{i}^{\\pi}\\in[k]\\ \\ \\mathrm{and}\\ \\mathbf{p}_{j}^{\\pi}\\ \\notin[k])}\\\\ &{\\qquad=\\displaystyle\\frac{1}{|B_{\\pi}|}\\cdot\\frac{1}{\\sqrt{\\binom{n}{2}}}\\cdot\\sum_{\\sigma\\in B_{\\pi}}w_{s}(\\mathbf{p}_{i}^{\\pi},\\mathbf{p}_{j}^{\\sigma}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Next, every possible consistent ranking is considered jointly while fixating on a specific rank outside top- $\\cdot\\mathbf{k}$ elements, leading to $(n-k-1)!$ ! different rankings. Given that $|B_{\\pi}|=(n\\,\\bar{-}\\,k)!$ , we can refine the above expression as follows: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\phi_{i,j}^{w A}(\\pi)=\\displaystyle\\frac{1}{|B_{\\pi}|}\\cdot\\frac{1}{\\sqrt{{\\binom{n}{2}}}}\\cdot\\sum_{l=k+1}^{n}w_{s}(\\mathbf{p}_{i}^{\\pi},l)\\cdot(n-k-1)!}\\\\ &{\\phantom{{=}}=\\frac{(n-k-1)!}{|B_{\\pi}|}\\cdot\\frac{1}{\\sqrt{{\\binom{n}{2}}}}\\cdot\\sum_{l=k+1}^{n}w_{s}(\\mathbf{p}_{i}^{\\pi},l)}\\\\ &{\\phantom{{=}}=\\frac{1}{\\sqrt{{\\binom{n}{2}}}}\\cdot\\frac{1}{n-k}\\cdot\\sum_{l=k+1}^{n}w_{s}(\\mathbf{p}_{i}^{\\pi},l)}\\\\ &{\\phantom{{=}}=\\frac{1}{\\sqrt{{\\binom{n}{2}}}}\\cdot w_{s}(\\mathbf{p}_{i}^{\\pi},\\cdot).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "In Case 3, we analyze when item $i$ is not in the top- $\\cdot\\mathbf{k}$ ranking while item $j$ is. ", "page_idx": 14}, {"type": "text", "text": "Case 3: $\\mathbf{p}_{i}^{\\pi}\\notin[k]$ and $\\mathbf{p}_{j}^{\\pi}\\in[k]$ . Similar to case 2, the simplification follows analogously, with the only change being $\\mathbf{1}_{\\mathbf{p}_{i<j}^{\\sigma}}-\\mathbf{1}_{\\mathbf{p}_{i>j}^{\\sigma}}=-1$ instead of 1. Thus, by symmetry between $i$ and $j$ , we have the following: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\phi_{i,j}^{w c k}(\\pi)=\\frac{-1}{\\sqrt{{\\binom{n}{2}}}}\\cdot w_{s}(\\cdot,\\mathbf{p}_{j}^{\\pi})=\\frac{-1}{\\sqrt{{\\binom{n}{2}}}}\\cdot w_{s}(\\mathbf{p}_{j}^{\\pi},\\cdot)\\qquad(\\mathrm{using~symmetry~of}~w_{s}).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Lastly, in Case 4, we analyze when items $i$ and $j$ are not in the top- $\\cdot\\mathbf{k}$ ranking. ", "page_idx": 14}, {"type": "text", "text": "Case 4: $\\mathbf{p}_{i}^{\\sigma}\\notin[k]$ and $\\mathbf{p}_{j}^{\\sigma}\\notin[k]$ . ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\phi_{i,j}^{w c k}(\\pi)=\\frac{1}{|B_{\\pi}|}\\cdot\\sum_{\\sigma\\in B_{\\pi}}\\phi_{i,j}^{w k}(\\sigma)\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The result of zero arises from symmetry. Since $\\mathbf{p}_{i}^{\\sigma}$ and $\\mathbf{p}_{j}^{\\sigma}$ are not in the top- $k$ ranking, they are treated symmetrically in the summation overall rankings in ${\\'}B_{\\pi}$ . For any ranking $\\sigma$ , suppose $\\mathbf{p}_{i}^{\\sigma}=l$ and $\\mathbf{p}_{j}^{\\sigma}=m$ , there exists a corresponding ranking $\\sigma^{\\prime}$ such that only the items $i$ and $j$ are swapped. Therefore, jointly, these two rankings yield $w_{s}(l,m)$ and $-w_{s}(l,m)$ . Since $w_{s}$ is symmetric, the overall contribution from each pair of such rankings is zero. Hence, the entire summation nets to zero. ", "page_idx": 15}, {"type": "text", "text": "Thus, with the explanation provided for each case and combining results from Equations 8, 9, 10 and 11, it\u2019s trivial to validate the Claim 3, i.e., $\\begin{array}{r}{\\phi_{i,j}^{w c k}(\\pi)=\\frac{1}{\\sqrt{\\binom{n}{2}}}\\cdot\\mathbf{w}_{i,j}^{\\bar{w}c k}(\\pi)\\cdot\\left(\\mathfrak{p}_{i<j}^{\\pi}-\\mathbf{\\hat{p}}_{i>j}^{\\pi}\\right)}\\\\ {.}\\end{array}$ for all unique pair of items. From Case 4, we have $O((n-k)^{2})$ entries leaving at max only $O(k^{2}+2n k)$ non-zero entries. \u53e3 ", "page_idx": 15}, {"type": "text", "text": "A.2 Algorithms for Computing Kendall Kernels for top- $\\mathbf{\\nabla}\\cdot\\mathbf{k}$ Rankings ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this section, we provide and delve into the proofs of Algorithms 2 and 3 for the weighted convolutional Kendall kernel and the convolutional Kendall kernel, as previously discussed in Section 2. Section A.2.1 for valid both the correctness and computational complexity of Algorithm 2 as given earlier in Claim 1. Following this, Section A.2.2 revisits Algorithm 3, initially introduced by Jiao et al. [10]. The original publication presented the algorithm without formal proof of its correctness, which we rectify and offer in Section A.2.2. ", "page_idx": 15}, {"type": "text", "text": "A.2.1 Efficiently Computing the Weighted Convolutional Kendall Kernel ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "This section provides a proof to Claim 1 to establish the efficiency and accuracy of Algorithm 2 in computing the weighted convolutional Kendall kernel, as specified in Equation 4, with a focus on its computational complexity. ", "page_idx": 15}, {"type": "text", "text": "Claim 1. The weighted convolutional Kendall kernel (Equation 4) with product-symmetric rank weights (Equation 5) can be computed in ${\\mathcal{O}}(k^{2})$ time. ", "page_idx": 15}, {"type": "text", "text": "Proof. The claim is proven through Algorithm 2, where we establish its correctness and demonstrate its computation requirement is $\\bar{O}(k^{2})$ . The essence of our proof centers on analyzing the feature representation of the WCK kernel, $\\phi^{w c k}$ , as outlined in Claim 3. The feature vectors of $\\phi^{w c k}$ reside in a $\\binom{n}{2}$ dimensional space, indexed by pairs of items. Our approach is to demonstrate that Algorithm 2 accurately computes the right-hand side (RHS) of the equation $k^{w c k}(\\pi_{1},\\pi_{2})=$ $\\phi^{w c k}(\\pi_{1})^{T}\\phi^{w c k}(\\pi_{2})$ . This involves a summation over item pairs, expressed as $k^{w c k}(\\pi_{1},\\pi_{2})\\,=$ $\\begin{array}{r}{\\sum_{l<m}\\phi_{l,m}^{w c k}(\\pi_{1})^{T}\\phi_{l,m}^{w c k}(\\pi_{2})}\\end{array}$ . ", "page_idx": 15}, {"type": "text", "text": "Our proof analyzes various scenarios: cases where pairs of items, namely $l$ and $m$ , fall within the top- $\\cdot\\mathbf{k}$ , scenarios with one item within the top- $\\cdot\\mathbf{k}$ and the other outside, and situations where neither item is within the top- $\\mathbf{\\nabla}\\cdot\\mathbf{k}$ . Each of these cases contributes distinctively to the computation of the overall kernel, resulting in different terms in the algorithmic computation. This is encapsulated in Algorithm 2, where $\\begin{array}{r}{k^{\\bar{w}c k}(\\pi_{1},\\pi_{2})=\\sum_{i=1}^{5}s_{i}(\\pi_{1},\\bar{\\pi_{2}})}\\end{array}$ , and each $s_{i}$ corresponds to the terms given earlier in Algorithm 2 from Section 2 . ", "page_idx": 15}, {"type": "text", "text": "Before proceeding with the cases of this summation as given in Table 4, we recall the notations utilized by Algorithm 3 in Definition 1. Also, remember that we will be proving for product-symmetric weights as given in Equation 5, where, $w_{s}:[n]\\times[n]\\mapsto\\mathbb{R}^{n}$ and its one-dimensional marginals are $\\begin{array}{r}{w_{s}(\\bar{\\ell},\\cdot)=\\frac{\\circ}{n-k}\\sum_{j=k+1}^{n}w_{s}(\\ell,j)}\\end{array}$ Table 4 shows how these cases are organized and relate to different $s_{i}$ terms required for computing the WCK kernel. The key strategy involves breaking down the kernel\u2019s computation into cases based on the positioning of item pairs within the top- $\\cdot\\mathbf{k}$ rankings. In case 1, we consider all the scenarios when both indices are within the set of items in both top- $\\cdot\\mathbf{k}$ rankings, i.e., all items in the set $I_{1}\\cup I_{2}$ . ", "page_idx": 15}, {"type": "table", "img_path": "50nEnmVLRb/tmp/00b3b2c8b40692f1ab71ca28a87be2e314a66fd13cd60912f64cf5baaef4bedc.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "Table 4: Case categorization for the proof of Algorithms 2 and 3 based on item pair ranks, where $I_{1}$ and $I_{2}$ are the sets of items for top- $\\cdot\\mathbf{k}$ rankings $\\pi_{1}$ and $\\pi_{2}$ , respectively. ", "page_idx": 16}, {"type": "text", "text": "Definition 1. Algorithm 2 and 3 and utilize following notations. \u2022 $I_{1}$ and $I_{2}$ are the sets of items in rankings $\\pi_{1}$ and $\\pi_{2}$ , respectively. \u2022 $\\sigma_{1}\\in\\Pi^{|I_{1}|}$ and $\\tau_{1}\\in\\Pi^{|I_{1}\\cap I_{2}|}$ are the full rankings of $I_{1}$ and $I_{1}\\cap I_{2}$ , both consistent with the input top- $k$ ranking $\\pi_{1}$ . I.e., relative ranks of items is same yielding $\\forall l,m\\in$ I1 \u2229I2, pi\u03c0>1j = pi\u03c41>j. \u2022 Analogously, $\\sigma_{2}$ and $\\tau_{2}$ are constructed utilizing the set $I_{2}$ and ranking $\\pi_{2}$ . ", "page_idx": 16}, {"type": "text", "text": "Algorithm 2 Computing Weighted Convolutional Kendall Kernel   \nInput: Two permutations $\\pi_{1},\\pi_{2}\\in\\Pi^{k}$ . Ranking weighting function $w_{s}:[n]\\times[n]\\mapsto\\mathbb{R}^{n}$ and its one dimensional marginals are $\\begin{array}{r}{w_{s}(\\ell,\\cdot)=\\frac{\\bullet}{n-k}\\sum_{j=k+1}^{n}w_{s}(\\ell,j)}\\end{array}$ .   \nOutput: Convolutional Weighted Kendall kernel $k^{w c k}(\\pi_{1},\\pi_{2})$ . \u2212Let $I_{1}$ and $I_{2}$ be the sets of items in rankings $\\pi_{1}$ and $\\pi_{2}$ , respectively.   \n1: if $|I_{1}\\cap I_{2}|\\ge2$ then   \n2: $\\begin{array}{r}{s_{1}^{\\cdot}(\\pi_{1},\\stackrel{\\cdot\\cdot}{\\pi_{2}})=\\frac{1}{\\binom{n}{2}}\\sum_{1\\leq l<m\\leq n|l,m\\in I_{1}\\cap I_{2}}w_{s}(\\mathbf{p}_{l}^{\\pi_{1}},\\mathbf{p}_{m}^{\\pi_{1}})\\cdot w_{s}(\\mathbf{p}_{l}^{\\pi_{2}},\\mathbf{p}_{m}^{\\pi_{2}})\\cdot\\eta_{l,m}(\\pi_{1},\\pi_{2})}\\end{array}$   \n3: end if   \n4: if $\\left|I_{1}\\cap I_{2}\\right|\\geq1$ and $\\left|I_{1}\\right\\backslash I_{2}\\right|\\geq1$ then $\\begin{array}{r}{s_{2}(\\pi_{1},\\dot{\\pi}_{2})=\\frac{1}{\\binom{n}{2}}\\cdot\\sum_{l\\in I_{1}\\cap I_{2}|m\\in I_{1}\\backslash I_{2}}w_{s}\\big(\\mathbf{p}_{l}^{\\pi_{1}},\\mathbf{p}_{m}^{\\pi_{1}}\\big)\\cdot w_{s}\\big(\\mathbf{p}_{l}^{\\pi_{2}},\\cdot\\big)\\left(\\mathfrak{p}_{l<m}^{\\pi_{1}}-\\mathfrak{p}_{l>m}^{\\pi_{1}}\\right)}\\end{array}$ ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "7: if $\\left|I_{1}\\cap I_{2}\\right|\\geq1$ and $\\left|I_{2}\\setminus I_{1}\\right|\\geq1$ then   \n8: $s_{3}(\\pi_{1},\\pi_{2})=\\frac{1}{\\binom{n}{2}}\\cdot\\sum_{l\\in I_{1}\\cap I_{2}|m\\in I_{2}\\backslash I_{1}}w_{s}(\\mathbf{p}_{l}^{\\pi_{1}},\\cdot\\cdot)\\cdot w_{s}(\\mathbf{p}_{l}^{\\pi_{2}},\\mathbf{p}_{m}^{\\pi_{2}})\\cdot\\left(\\mathfrak{p}_{l<m}^{\\pi_{2}}-\\mathfrak{p}_{l>m}^{\\pi_{2}}\\right)$   \n10: if $|I_{1}\\setminus I_{2}|\\ge1$ and $|I_{2}\\setminus I_{1}|\\ge1$ then   \n11: $s_{4}(\\dot{\\pi}_{1},\\dot{\\pi}_{2})=-\\frac{1}{\\binom{n}{2}}\\cdot\\sum_{l\\in I_{1}\\backslash I_{2}|m\\in I_{2}\\backslash I_{1}}w_{s}(\\mathbf{p}_{l}^{\\pi_{1}},\\cdot)\\cdot w_{s}(\\mathbf{p}_{m}^{\\pi_{2}},\\cdot)$   \n12: end if   \n13: if $|I_{1}\\cap I_{2}|\\ge1$ and $|[n]\\setminus(I_{1}\\cup I_{2})|\\geq1$ then   \n14: $\\begin{array}{r}{s_{5}(\\pi_{1},\\pi_{2})=\\frac{1}{\\binom{n}{2}}\\cdot\\left(\\bar{n}-|I_{1}\\cup I_{2}|\\right)\\cdot\\displaystyle\\sum_{l\\in I_{1}\\cap I_{2}}w_{s}(\\mathbf{p}_{l}^{\\pi_{1}},\\cdot)\\cdot w_{s}(\\mathbf{p}_{l}^{\\pi_{2}},\\cdot)}\\end{array}$ ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "equation", "text": "$$\nk^{w c k}(\\pi_{1},\\pi_{2})=s_{1}(\\pi_{1},\\pi_{2})+s_{2}(\\pi_{1},\\pi_{2})+s_{3}(\\pi_{1},\\pi_{2})+s_{4}(\\pi_{1},\\pi_{2})+s_{5}(\\pi_{1},\\pi_{2})\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Case 1: The pair $(l,m)\\in I_{1}\\cup I_{2}$ falls within the top- $\\cdot\\mathbf{k}$ , leading to three distinct cases. Below, we provide $s_{i}$ terms for each case as given in Table 4. ", "page_idx": 16}, {"type": "text", "text": "Case 1-a: Two items in $I_{1}\\cap I_{2}$ , meaning both $l$ and $m$ belong to $I_{1}\\cap I_{2}$ . Using Claim 3 regarding the feature vector $\\phi^{w c k}$ , we simplify $s_{1}$ as follows: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle s_{1}(\\pi_{1},\\pi_{2})=\\sum_{1\\leq l<m\\leq n|l,m\\in I_{1}\\cap I_{2}}\\phi_{l,m}^{w c k}(\\pi_{1})\\cdot\\phi_{l,m}^{w c k}(\\pi_{2})}\\\\ {\\displaystyle=}\\\\ {\\displaystyle=}\\\\ {\\displaystyle\\qquad\\sum_{1\\leq l<m\\leq n|l,m\\in I_{1}\\cap I_{2}\\cap I_{2}}\\frac{1}{\\sqrt{\\binom{m}{2}}}\\cdot w_{s}(\\mathbf{p}_{l}^{\\pi_{1}},\\mathbf{p}_{m}^{\\pi_{1}})\\cdot\\left(\\mathfrak{p}_{l<m}^{\\pi_{1}}-\\mathfrak{p}_{l>m}^{\\pi_{1}}\\right)}\\\\ {\\displaystyle\\cdot\\frac{1}{\\sqrt{\\binom{m}{2}}}\\cdot w_{s}(\\mathbf{p}_{l}^{\\pi_{2}},\\mathbf{p}_{m}^{\\pi_{2}})\\cdot\\left(\\mathfrak{p}_{l<m}^{\\pi_{2}}-\\mathfrak{p}_{l>m}^{\\pi_{2}}\\right)}\\\\ {\\displaystyle=\\frac{1}{\\binom{m}{2}}\\sum_{1\\leq l<m\\leq n|l,m\\in I_{1}\\cap I_{2}}w_{s}(\\mathbf{p}_{l}^{\\pi_{1}},\\mathbf{p}_{m}^{\\pi_{1}})\\cdot w_{s}(\\mathbf{p}_{l}^{\\pi_{2}},\\mathbf{p}_{m}^{\\pi_{2}})\\cdot\\eta_{l,m}(\\pi_{1},\\pi_{2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Case 1-b: When one item is in $I_{1}\\cap I_{2}$ , the other must reside either in $I_{1}\\setminus I_{2}$ or $I_{2}\\setminus I_{1}$ , thus leading to two distinct sub-cases. This is specified in Table 4. Concretely, if the other item is in $I_{1}\\setminus I_{2}$ , it contributes to the $s_{2}$ terms, whereas if it\u2019s in $I_{2}\\setminus I_{1}$ , it contributes to the $s_{3}$ terms. ", "page_idx": 17}, {"type": "text", "text": "Corresponding to Case 1-b-i, when the other item is in $I_{1}\\cap I_{2}$ , i.e., $s_{2}$ is the term corresponding to indices where $l$ is in $I_{1}\\cap I_{2}$ and $m$ in $I_{1}\\setminus I_{2}$ , or the reverse, represented by partial sums $u$ and $v$ . For the partial sum $u$ , with $l$ in $I_{1}\\cap I_{2}$ and $m$ in $I_{1}\\setminus I_{2}$ , we find that $\\mathbf{p}_{l}^{\\pi_{2}}$ is in $[k]$ , while $\\mathbf{p}_{m}^{\\pi_{2}}$ is not. The simplification of $u$ proceeds using Claim 3 as follows: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{u=\\displaystyle\\sum_{1\\leq l<m\\leq n|l\\in I_{1}\\cap I_{2}|m\\in I_{1}\\backslash I_{2}}\\frac{1}{\\sqrt{\\binom{n}{2}}}\\cdot w_{s}(\\mathbf{p}_{l}^{\\pi_{1}},\\mathbf{p}_{m}^{\\pi_{1}})\\cdot\\big(\\mathfrak{p}_{l<m}^{\\pi_{1}}-\\mathfrak{p}_{l>m}^{\\pi_{1}}\\big)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\quad\\cdot\\displaystyle\\frac{1}{\\sqrt{\\binom{n}{2}}}\\cdot w_{s}(\\mathbf{p}_{l}^{\\pi_{2}},\\cdot)\\left(\\mathfrak{p}_{l<m}^{\\pi_{2}}-\\mathfrak{p}_{l>m}^{\\pi_{2}}\\right)}\\\\ &{=\\displaystyle\\frac{1}{\\binom{n}{2}}\\sum_{1\\leq l<m\\leq n|l\\in I_{1}\\cap I_{2}|m\\in I_{1}\\backslash I_{2}}w_{s}(\\mathbf{p}_{l}^{\\pi_{1}},\\mathbf{p}_{m}^{\\pi_{1}})\\cdot w_{s}(\\mathbf{p}_{l}^{\\pi_{2}},\\cdot)\\left(\\mathfrak{p}_{l<m}^{\\pi_{1}}-\\mathfrak{p}_{l>m}^{\\pi_{1}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Similarly, the partial sum $v$ can be simplified as follows: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{v=\\underset{1\\leq l<m\\leq n\\,\\operatorname*{sup}\\in\\{I_{1}\\}\\cap L\\cup L_{l}\\cup L_{l}\\setminus L_{l}}{\\sum}\\frac{1}{\\sqrt{(2)}}\\cdot w_{s}(\\mathbf{p}_{l}^{\\pi_{1}},\\mathbf{p}_{m}^{\\pi_{1}})\\cdot\\left(\\mathbf{p}_{l<m}^{\\pi_{1}}-\\mathbf{p}_{l>m}^{\\pi_{1}}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\cdot\\frac{1}{\\sqrt{(2)}}\\cdot w_{s}(\\mathbf{p}_{l}^{\\pi_{2}},\\cdot)\\cdot\\left(\\mathbf{p}_{l<m}^{\\pi_{2}}-\\mathbf{p}_{l>m}^{\\pi_{2}}\\right)}\\\\ &{=\\frac{-1}{(2)}\\underset{1\\leq l<m\\leq n\\,\\operatorname*{sup}\\in\\{I_{1}\\}\\cap L\\cup L_{l}\\cup L_{l}\\setminus L_{l}}{\\sum}w_{s}(\\mathbf{p}_{l}^{\\pi_{1}},\\mathbf{p}_{m}^{\\pi_{1}})\\cdot w_{s}(\\mathbf{p}_{l}^{\\pi_{2}},\\cdot)\\cdot\\left(\\mathbf{p}_{l<m}^{\\pi_{1}}-\\mathbf{p}_{l>m}^{\\pi_{1}}\\right)}\\\\ &{=\\frac{-1}{(2)}\\underset{1\\leq m\\leq k\\leq n\\,\\operatorname*{sup}\\in\\{I_{1}\\}\\cap L\\cup L_{l}\\setminus L_{l}}{\\sum}w_{s}(\\mathbf{p}_{m}^{\\pi_{1}},\\mathbf{p}_{l}^{\\pi_{1}})\\cdot w_{s}(\\mathbf{p}_{m}^{\\pi_{2}},\\cdot)\\cdot\\left(\\mathbf{p}_{m<l}^{\\pi_{1}}-\\mathbf{p}_{m>l}^{\\pi_{1}}\\right)}\\\\ &{=\\frac{1}{(2)}\\underset{1\\leq m\\leq l\\leq n\\,\\operatorname*{sup}\\in\\{I_{1}\\}\\cap L\\cup L_{l}\\setminus L_{l}}{\\sum}w_{s}(\\mathbf{p}_{l}^{\\pi_{1}},\\mathbf{p}_{m}^{\\pi_{1}})\\cdot w_{s}(\\mathbf{p}_{l}^{\\pi_{2}},\\cdot)\\left(\\mathbf{p}_{l<m}^{\\pi_{1}}-\\mathbf{p}_{l>m}^{\\pi_{1}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "In the above, the first two lines use results from Claim 3 and use similarity of $w_{s}$ . In the following line, $l$ and $m$ are exchanged. Lastly, the negative sign is pushed into the indicator functions to make the summand function of this partial sum $v$ similar to the partial sum $u$ , and the similarity of the $w_{s}$ is utilized. The above partial sums simplify $s_{2}$ as follows: ", "page_idx": 17}, {"type": "equation", "text": "$$\ns_{2}(\\pi_{1},\\pi_{2})=\\frac{1}{\\binom{n}{2}}\\cdot\\sum_{l\\in I_{1}\\cap I_{2}|m\\in I_{1}\\backslash I_{2}}w_{s}\\big(\\mathbf{p}_{l}^{\\pi_{1}},\\mathbf{p}_{m}^{\\pi_{1}}\\big)\\cdot w_{s}\\big(\\mathbf{p}_{l}^{\\pi_{2}},\\cdot\\big)\\left(\\mathfrak{p}_{l<m}^{\\pi_{1}}-\\mathfrak{p}_{l>m}^{\\pi_{1}}\\right).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Analogously, in Case 1-b-ii, we deduce the corresponding term $s_{3}$ for the pair of indices as described in Table 4 through symmetry. Specifically, the term $s_{3}$ can be outlined as follows: ", "page_idx": 18}, {"type": "equation", "text": "$$\ns_{3}(\\pi_{1},\\pi_{2})=\\frac{1}{\\binom{n}{2}}\\cdot\\sum_{l\\in I_{1}\\cap I_{2}|m\\in I_{2}\\backslash I_{1}}w_{s}(\\mathbf{p}_{l}^{\\pi_{1}},\\cdot)\\cdot w_{s}(\\mathbf{p}_{l}^{\\pi_{2}},\\mathbf{p}_{m}^{\\pi_{2}})\\cdot\\left(\\mathfrak{p}_{l<m}^{\\pi_{2}}-\\mathfrak{p}_{l>m}^{\\pi_{2}}\\right).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Case 1-c: Both items are outside $I_{1}\\cap I_{2}$ , specifically, $l\\in I_{1}\\setminus I_{2}$ and $m\\in I_{2}\\setminus I_{1}$ or the reverse. Like Case 1-b-i, we divide $s_{4}$ into partial summations $u$ and $v$ . Now, we calculate $u$ under the condition that $l\\in I_{1}\\setminus I_{2}$ and $m\\in I_{2}\\setminus I_{1}$ . ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{u=\\displaystyle\\sum_{1\\leq l<m\\leq n|l\\in I_{1}\\backslash I_{2}|m\\in I_{2}\\backslash I_{1}}\\frac{1}{\\sqrt{(n_{2}^{n})}}\\cdot w_{s}(\\mathbf{p}_{l}^{\\pi_{1}},\\cdot)\\cdot\\left(\\mathbf{p}_{l<m}^{\\pi_{1}}-\\mathbf{p}_{l>m}^{\\pi_{1}}\\right)}\\\\ &{\\phantom{\\quad\\quad\\quad\\quad}\\cdot\\frac{1}{\\sqrt{\\binom{n}{2}}}\\cdot w_{s}(\\mathbf{p}_{m}^{\\pi_{2}},\\cdot)\\cdot\\left(\\mathbf{p}_{l<m}^{\\pi_{2}}-\\mathbf{p}_{l>m}^{\\pi_{2}}\\right),}\\\\ &{\\displaystyle=\\frac{1}{\\binom{n}{2}}\\cdot\\sum_{1\\leq l<m\\leq n|l\\in I_{1}\\backslash I_{2}|m\\in I_{2}\\backslash I_{1}}w_{s}(\\mathbf{p}_{l}^{\\pi_{1}},\\cdot)\\cdot\\left(1-0\\right)\\cdot w_{s}(\\mathbf{p}_{m}^{\\pi_{2}},\\cdot)\\cdot\\left(0-1\\right),}\\\\ &{\\displaystyle=\\frac{-1}{\\binom{n}{2}}\\cdot\\sum_{1\\leq l<m\\leq n|l\\in I_{1}\\backslash I_{2}|m\\in I_{2}\\backslash I_{1}}w_{s}(\\mathbf{p}_{l}^{\\pi_{1}},\\cdot)\\cdot w_{s}(\\mathbf{p}_{m}^{\\pi_{2}},\\cdot).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Similarly, we can estimate partial sum $v$ for the set $l\\in I_{2}\\setminus I_{1}$ & $m\\in I_{1}\\setminus I_{2}$ . Using calculations similar to Case-1-b-i for summing $u$ and $v$ , we have: ", "page_idx": 18}, {"type": "equation", "text": "$$\ns_{4}(\\pi_{1},\\pi_{2})=\\frac{-1}{\\binom{n}{2}}\\cdot\\sum_{l\\in I_{1}\\backslash I_{2}|m\\in I_{2}\\backslash I_{1}}w_{s}(\\mathbf{p}_{l}^{\\pi_{1}},\\cdot)\\cdot w_{s}(\\mathbf{p}_{m}^{\\pi_{2}},\\cdot).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Case 2: One item exists in $I_{1}\\cap I_{2}$ , the other in $[n]\\setminus(I_{1}\\cap I_{2})$ . It branches into two sub-cases: Case 2-a with one item in $I_{1}\\cup I_{2}$ , and Case $^{2-\\mathrm{b}}$ , where one item outside $I_{1}\\cap I_{2}$ but is in $I_{1}\\cup I_{2}$ . Focusing on Case 2-a, represented by $s_{5}$ , we simplify as follows. This involves two index scenarios, either $l\\in I_{1}\\cap I_{2}$ and $m\\notin I_{1}\\cup I_{2}$ or vice versa, represented by partial sums $u$ and $v$ . We now simplify $u$ below: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{u=\\frac{1}{\\binom{n}{2}}_{1\\leq l<m\\leq n|l\\in I_{1}\\cap I_{2}|m\\notin I_{1}\\cup I_{2}}\\frac{1}{\\sqrt{\\binom{n}{2}}}\\cdot w_{s}({\\mathbf{p}}_{l}^{\\pi_{1}},\\cdot)\\cdot\\big(\\mathfrak{p}_{l<m}^{\\pi_{1}}-\\mathfrak{p}_{l>m}^{\\pi_{1}}\\big)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\cdot\\frac{1}{\\sqrt{\\binom{n}{2}}}\\cdot w_{s}({\\mathbf{p}}_{l}^{\\pi_{2}},\\cdot)\\cdot\\big(\\mathfrak{p}_{l<m}^{\\pi_{2}}-\\mathfrak{p}_{l>m}^{\\pi_{2}}\\big)\\,,}\\\\ &{\\qquad=\\frac{1}{\\binom{n}{2}}_{1\\leq l<m\\leq n|l\\in I_{1}\\cap I_{2}|m\\notin I_{1}\\cup I_{2}}w_{s}({\\mathbf{p}}_{l}^{\\pi_{1}},\\cdot)\\cdot w_{s}({\\mathbf{p}}_{l}^{\\pi_{2}},\\cdot),}\\\\ &{\\qquad=\\frac{1}{\\binom{n}{2}}_{1\\leq l<m\\leq n|l\\in I_{1}\\cap I_{2}\\atop1\\leq l\\leq m\\leq n|l\\in I_{1}\\cap I_{2}}w_{s}({\\mathbf{p}}_{l}^{\\pi_{1}},\\cdot)\\cdot w_{s}({\\mathbf{p}}_{l}^{\\pi_{2}},\\cdot)\\cdot\\big(n-|I_{1}\\cup I_{2}|\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Using steps similar to the previous case, we get the following value for $s_{5}$ : ", "page_idx": 18}, {"type": "equation", "text": "$$\ns_{5}(\\pi_{1},\\pi_{2})=\\frac{1}{\\binom{n}{2}}\\cdot\\left(n-|I_{1}\\cup I_{2}|\\right)\\cdot\\sum_{l\\in I_{1}\\cap I_{2}}w_{s}(\\mathbf{p}_{l}^{\\pi_{1}},\\cdot)\\cdot w_{s}(\\mathbf{p}_{l}^{\\pi_{2}},\\cdot).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "For Case 2-b, $l$ or $m$ are absent from $I_{1}$ or $I_{2}$ , leading to two sub-scenarios. Consequently, either $\\phi_{l,m}^{w c k}(\\pi_{1})$ is zero or $\\phi_{l,m}^{w c k}(\\pi_{2})$ is zero. Therefore, these terms don\u2019t contribute to the overall WCK kernel value. ", "page_idx": 18}, {"type": "text", "text": "Case 3: No item is in the top- $\\cdot\\mathbf{k}$ , i.e., both $l,m\\not\\in I_{1}\\cup I_{2}$ . As both items are absent from the top- $\\cdot\\mathbf{k}$ in either ranking, the value trivially reduces to zero. ", "page_idx": 19}, {"type": "text", "text": "After covering all configurations of $l$ and $m$ , we incorporate results from Equations 12, 13, 14, 15, and 16. This integration yields the expression $\\begin{array}{r}{k^{w c k}(\\bar{\\pi_{1},\\pi_{2}})=\\sum_{i=1}^{5}s_{i}(\\pi_{1},\\bar{\\pi_{2}})}\\end{array}$ , where, each term $s_{i}$ matches precisely with its corresponding expression in Algorit hm 2. The proof for the correctness of Algorithm 2 is complete, as each term $s_{i}$ corresponds to its respective expression in the algorithm. Regarding the time complexity of Algorithm 2, each term $s_{i}$ sums at most $k^{2}$ quantities, and each quantity summed can be computed in $\\mathcal{O}(1)$ time. Therefore, the computation time required for Algorithm 2 is ${\\mathcal{O}}(k^{2})$ . \u53e3 ", "page_idx": 19}, {"type": "text", "text": "A.2.2 Efficiently Computing the Convolutional Kendall Kernel ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "This section provides Algorithm 3 for computing the convolutional Kendall kernel, as specified in Equation 3. Later, its efficiency and accuracy are proved in Claim 4. ", "page_idx": 19}, {"type": "table", "img_path": "50nEnmVLRb/tmp/3229e7a0c12594d58aa9fe6dd41a33147fb169bb9df035edbf5c3b7249ea84f8.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "Claim 4. Algorithm 3 computes the convolutional Kendall kernel (as given in the Equation 3) with a computational complexity of ${\\mathcal{O}}(k^{2})$ . ", "page_idx": 19}, {"type": "text", "text": "Proof. To establish the correctness of Algorithm 3, we will adopt the same proof approach as the one used for Claim 1 concerning Algorithm 2. Specifically, we will adhere to the earlier categorization in Table 4 and notations given in Definition 1. Since the CK kernel can be derived by uniformly setting the weight function $w_{s}(i,j)=1$ , we will insert them in $s_{i}$ terms as given in Algorithm 2. These cases will be revisited and simplified by applying the condition $w_{s}(\\bar{i},j)=1$ . Note that this also implies its one-direction marginal weights to be 1, i.e., $w_{s}(i,\\cdot)=1$ ", "page_idx": 19}, {"type": "text", "text": "Simplifying the $s_{1}$ Term: For the WCK kernel, Case 1-a leads to the expression of $s_{1}$ as stated in Equation 12. In this case, when two items, specifically $l$ and $m$ , are both in the intersection $I_{1}\\cap I_{2}$ , it implies that $\\mathbf{p}_{l}^{\\pi_{1}},\\mathbf{p}_{m}^{\\pi_{1}},\\mathbf{p}_{l}^{\\pi_{2}}$ , and $\\mathbf{p}_{m}^{\\pi_{2}}$ all rank within the top- $\\cdot\\mathbf{k}$ , denoted as $[k]$ . We simplify the $s_{1}$ term for CK kernel as follows: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{s_{1}(\\pi_{1},\\pi_{2})=\\displaystyle\\frac{1}{{\\binom{n}{2}}}\\sum_{1\\leq l<m\\leq n|l,m\\in I_{1}\\cap I_{2}}w_{s}(\\mathbf{p}_{i}^{\\pi_{l}},\\mathbf{p}_{m}^{\\pi_{1}})\\cdot w_{s}(\\mathbf{p}_{l}^{\\pi_{2}},\\mathbf{p}_{m}^{\\pi_{2}})\\cdot\\eta_{l,m}(\\pi_{1},\\pi_{2})}\\\\ &{\\phantom{\\sum{\\scriptstyle\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ }}=\\displaystyle\\frac{1}{{\\binom{n}{2}}}\\sum_{1\\leq l<m\\leq n|l,m\\in I_{1}\\cap I_{2}}\\eta_{l,m}(\\pi_{1},\\pi_{2})}\\\\ &{\\phantom{\\sum{\\scriptstyle\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ }}=\\displaystyle\\frac{1}{{\\binom{n}{2}}}\\sum_{1\\leq l^{\\prime}<m^{\\prime}\\leq|l|\\cap I_{2}}\\eta_{l^{\\prime},m^{\\prime}}(\\tau_{1},\\tau_{2})=\\displaystyle\\frac{{\\binom{|I_{1}\\cap I_{2}|}{2}}\\cdot\\mathbf{p}_{m}^{\\pi_{2}})\\cdot\\eta_{l,m}(\\pi_{1},\\pi_{2})}{{\\binom{n}{2}}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "The simplification process begins by assigning unit rank weights in the first line, i.e., $\\mathbf{w}_{i}\\;=\\;1$ . Following this, by relabeling the items in $I_{1}\\cap I_{2}$ and using $\\tau_{1}$ and $\\tau_{2}$ , which are the rankings of $\\pi_{1}$ and $\\pi_{2}$ limited to the set $I_{1}\\cap I_{2}$ as defined in Definition 1, it is established that $\\eta_{l^{\\prime},m^{\\prime}}(\\tau_{1},\\tau_{2})=$ $\\eta_{l,m}(\\pi_{1},\\pi_{2})$ . This is because the relative order of any pair of items is maintained in $\\tau_{1}$ and $\\tau_{2}$ . Consequently, this leads to the final simplification to a scaled value of the standard Kendall kernel $k^{s k}$ , as given in Equation 1. ", "page_idx": 20}, {"type": "text", "text": "Simplifying the $s_{2}$ and $s_{3}$ Terms: The $s_{2}$ and $s_{3}$ terms are obtained for Case 1-b, which is for case when one item is in $I_{1}\\cap I_{2}$ and the other item is either in $I_{1}\\setminus I_{2}$ or $I_{2}\\setminus I_{1}$ . We divide this into two sub-cases. Case 1-b-i: The other item is in $I_{1}\\setminus I_{2}$ , with $s_{2}$ representing the summation terms derived from the CK\u2019s inner product. Case 1-b-ii: The other item is $I_{2}\\setminus I_{1}$ , where $s_{3}$ denotes the summation terms. We simplify the $s_{2}$ term for the CK kernel as follows: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle s_{2}(\\pi_{1},\\pi_{2})=\\frac{1}{\\binom{n}{2}}\\sum_{l\\in I_{1}\\cap I_{2}|m\\in I_{1}\\backslash I_{2}}w_{s}({\\mathbf p}_{l}^{\\pi_{1}},{\\mathbf p}_{m}^{\\pi_{1}})\\cdot w_{s}({\\mathbf p}_{m}^{\\pi_{2}},\\cdot)\\left(\\mathfrak{p}_{l<m}^{\\pi_{1}}-\\mathfrak{p}_{l>m}^{\\pi_{1}}\\right)}}\\\\ {{\\displaystyle=\\frac{1}{\\binom{n}{2}}\\sum_{l\\in I_{1}\\cap I_{2}|m\\in I_{1}\\backslash I_{2}}\\left(\\mathfrak{p}_{l<m}^{\\pi_{1}}-\\mathfrak{p}_{l>m}^{\\pi_{1}}\\right)}}\\\\ {{\\displaystyle=\\frac{1}{\\binom{n}{2}}\\sum_{l\\in I_{1}\\cap I_{2}|m\\in I_{1}\\backslash I_{2}}\\mathfrak{p}_{l<m}^{\\pi_{1}}-\\frac{1}{\\binom{n}{2}}\\sum_{l\\in I_{1}\\cap I_{2}|m\\in I_{1}\\backslash I_{2}}\\mathfrak{p}_{l>m}^{\\pi_{1}}\\,.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Next, we examine the terms $u$ and $v$ in detail, starting with $u$ . The term $u$ , which corresponds to ${\\mathfrak{p}}_{l<m}^{\\pi_{1}}$ , signifies instances where item $l$ is ranked before item $m$ in the top- $\\cdot\\mathbf{k}$ ranking $\\pi_{1}$ . This can be derived from the observation that $\\sigma_{1}(l)-1$ items are positioned before item $l$ in the set $I_{1}$ . Out of these items, $\\tau_{1}(l)-1$ also belong to the intersection $I_{1}\\cap I_{2}$ . This follows from the definition of the full rankings $\\sigma_{1}$ and $\\tau_{1}$ on the set $I_{1}$ and the intersection $I_{1}\\cap I_{2}$ , respectively. Consequently, it can be concluded that $\\sigma_{1}(l)-\\tau_{1}(l)$ items from the set difference $I_{1}\\setminus I_{2}$ are ranked before item $l$ . The second term, $v$ , corresponds to ${\\mathfrak{p}}_{l>m}^{\\pi_{1}}$ and involves a calculation that takes into account the items ranked after the $l$ -th item in the set $I$ . Specifically, there are $k-\\sigma_{1}(l)$ items following the $l$ -th item. Within the intersection $I_{1}\\cap I_{2}$ , the number of items before $l$ is given by $|I_{1}\\cap I_{2}|-\\tau_{1}(l)$ . Therefore, the expression $(k-\\sigma_{1}(l))-(|I_{1}\\cap I_{2}|-\\tau_{1}(l))$ represents the count of elements that are positioned after $l$ in the set difference $I_{1}\\setminus I_{2}$ . ", "page_idx": 20}, {"type": "text", "text": "Combining the above calculations for both terms $u$ and $v$ , the $s_{2}$ term for the CK kernel can be simplified as follows: ", "page_idx": 20}, {"type": "equation", "text": "$$\ns_{2}(\\pi_{1},\\pi_{2})=\\frac{1}{{\\binom{n}{2}}}\\sum_{l\\in{I_{1}}\\cap{I_{2}}}2\\cdot(\\sigma_{1}(l)-\\tau_{1}(l))-k+|I_{1}\\cap I_{2}|.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Using the symmetry between Case 1-b-i and Case 1-b-ii, we can simplify $s_{3}$ for the CK kernel as follows: ", "page_idx": 20}, {"type": "equation", "text": "$$\ns_{3}(\\pi_{1},\\pi_{2})=\\frac{1}{{\\binom{n}{2}}}\\sum_{l\\in{I_{1}}\\cap{I_{2}}}2\\cdot(\\sigma_{2}(l)-\\tau_{2}(l))-k+|{I_{1}}\\cap{I_{2}}|.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Simplifying the $s_{4}$ and $s_{5}$ Terms: We simplify the $s_{4}$ and $s_{5}$ terms for the CK kernel starting from Equation 15 and Equation 16, respectively, as follows: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle s_{4}(\\pi_{1},\\pi_{2})=\\frac{-1}{{\\binom{n}{2}}}\\cdot\\sum_{l\\in I_{1}\\backslash I_{2}|m\\in I_{2}\\backslash I_{1}}w_{s}({\\mathbf p}_{l}^{\\pi_{1}},\\cdot)\\cdot w_{s}({\\mathbf p}_{m}^{\\pi_{2}},\\cdot)=\\frac{-|I_{1}\\backslash I_{2}|\\cdot|I_{2}\\backslash I_{1}|}{{\\binom{n}{2}}}}\\end{array}\\quad{\\scriptscriptstyle(20\\leq\\frac{1}{{\\binom{n}{2}}},\\cdot)}}\\\\ {{\\displaystyle s_{5}(\\pi_{1},\\pi_{2})=\\frac{1}{{\\binom{n}{2}}}\\cdot(n-|I_{1}\\cup I_{2}|)\\cdot\\sum_{l\\in I_{1}\\cap I_{2}}w_{s}({\\mathbf p}_{l}^{\\pi_{1}},\\cdot)\\cdot w_{s}({\\mathbf p}_{l}^{\\pi_{2}},\\cdot)=\\frac{|I_{1}\\cap I_{2}|\\cdot|[n]\\backslash(I_{1}\\cup I_{2})|}{{\\binom{n}{2}}}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "We have obtained the values of all the simplified $s_{i}$ terms for the CK kernel in Equations 17, 18, 19, 20, and 21. By combining these terms, we get $\\begin{array}{r}{k^{c k}(\\pi_{1},\\pi_{2})=\\sum_{i=1}^{5}s_{i}(\\pi_{1},\\pi_{2})}\\end{array}$ i5=1 si(\u03c01, \u03c02), where each term $s_{i}$ precisely matches its corresponding expression in Algorithm  3. This completes the proof of the correctness of Algorithm 3. Regarding its time complexity, each term $s_{i}$ sums at most $k^{\\bar{2}}$ quantities, and each quantity can be computed in $\\mathcal{O}(1)$ time. Therefore, the time required for Algorithm 3 to compute the CK kernel is $\\mathcal{O}(k^{\\bar{2}})$ . \u53e3 ", "page_idx": 21}, {"type": "text", "text": "A.3 Fast Matrix-Vector Multiplication with Kendall Kernel Matrix on Top-k Rankings ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "This section revisits Theorem 1 about the fact matrix-vector multiplication time for the Kendall kernel matrix for top- $\\cdot\\mathbf{k}$ rankings. Specifically, we aim to eliminate the $\\mathbf{mvm}(K_{X})$ \u2019s dependence on the number of items, i.e., $n$ on and linear dependence in the number of rounds, i.e., $T$ , as claimed in Theorem 1. ", "page_idx": 21}, {"type": "text", "text": "Theorem 1. For the WCK kernel with product-symmetric weights $w_{p s}$ , the computational complexity of multiplying the kernel matrix $K_{X_{t}}$ with any admissible vector is $\\mathcal{O}(k^{2}t)$ , i.e., $\\mathbf{mv\\bar{m}}(K_{X_{t}})=\\mathcal{O}(\\bar{k}^{2}\\dot{t}),$ , where $X_{t}$ is any arbitrary set of $t$ top- $\\boldsymbol{\\cdot}\\boldsymbol{k}$ rankings. ", "page_idx": 21}, {"type": "text", "text": "Proof. The cornerstone of this proof lies in the computation of the WCK kernel, as delineated in Algorithm 2. This algorithm requires only ${\\mathcal{O}}(k^{2})$ computation. For brevity, we write $X$ to represent $X_{T}$ , and the proof follows for any $X_{t}$ , i.e., any value of $t$ , not just $T$ . ", "page_idx": 21}, {"type": "text", "text": "As also suggested previously, we will demonstrate through the equation $K_{X}=(\\Phi_{X}^{a})^{T}\\Phi_{X}^{b}$ , where both matrices $\\Phi_{X}^{a}$ and $\\Phi_{X}^{b}$ have columns with only ${\\mathcal{O}}(k^{2})$ non-zero entries. Consequently, this leads to the computational complexity of matrix-vector multiplication, denoted as $\\mathbf{mvm}(K_{X})$ , being $\\mathcal{O}(k^{2}\\cdot T)$ . ", "page_idx": 21}, {"type": "text", "text": "From Algorithm 2, we know that each entry of the kernel matrix $k(\\pi_{1},\\pi_{2})$ , can be expressed as a sum $\\textstyle\\sum_{i=1}^{5}s_{i}(\\pi_{1},\\pi_{2})$ . Assuming each $s_{i}(\\pi_{1},\\pi_{2})$ equals $\\phi^{a_{i}}(\\pi_{1})^{T}\\phi^{b_{i}}(\\pi_{2})$ , and considering that all vectors $\\phi^{a_{i}}$ and $\\phi^{b_{i}}$ exhibit this property, we can express $K_{X}$ as $(\\Phi_{X}^{a})^{T}\\Phi_{X}^{b}$ . Here, the $i^{t h}$ row of $(\\Phi_{X}^{a})^{T}$ and the $j^{t h}$ column of $\\Phi_{X}^{b}$ are represented by $[\\phi^{a_{1}}(\\pi_{i})^{T},\\cdot\\cdot\\cdot\\ ,\\phi^{a_{5}}(\\pi_{i})^{T}]$ and $[\\phi^{b_{1}}(\\pi_{j}),\\cdot\\cdot\\cdot\\mathbf{\\pi},\\phi^{b_{5}}(\\bar{\\pi_{j}})]$ , respectively. Therefore, the overall mvm complexity can be characterized by the sparsity of the vectors $\\phi^{a_{i}}$ and $\\phi^{b_{i}}$ , as is formalized in the claim presented below. ", "page_idx": 21}, {"type": "text", "text": "Claim 5. Consider a kernel matrix $K_{X}$ corresponding to any set $X$ of cardinality $T$ . Each entry of $K_{X}$ , denoted as $k(x_{1},x_{2})$ , is defined by the sum $\\textstyle\\sum_{i=1}^{5}s_{i}(x_{1},x_{2})$ , where each $s_{i}(x_{1},x_{2})$ is the result of the dot product $\\phi^{a_{i}}(x_{1})^{T}\\phi^{b_{i}}(x_{2})$ , where, $\\phi^{a_{i}}$ and $\\phi^{b_{i}}$ are vectors characterized by having $O(z)$ non-zero entries. Given this structure, the matrix-vector multiplication complexity for $K_{X}$ is $O(n n z\\cdot T).$ , i.e., $\\mathbf{mvm}(K_{X})=O(z\\cdot T)$ . ", "page_idx": 21}, {"type": "text", "text": "Proof. We will demonstrate this in the following discussion by concentrating on the $k^{\\mathrm{th}}$ entry of the output vector, specifically $K_{X}\\mathbf{v}$ , for any arbitrary vector $\\mathbf{v}$ : ", "page_idx": 21}, {"type": "equation", "text": "$$\n(K_{X}\\mathbf{v})_{k}=\\sum_{j}K_{X}(k,j)v_{j}=\\sum_{j}\\left(\\sum_{i=1}s_{i}(\\pi_{k},\\pi_{j})\\right)v_{j}=\\sum_{j}\\left(\\sum_{i=1}^{5}\\phi^{a_{i}}(\\pi_{k})^{T}\\phi^{b_{i}}(\\pi_{j})\\right)v_{j},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "equation", "text": "$$\n=\\sum_{i=1}^{5}\\left(\\sum_{j}\\phi^{a_{i}}(\\pi_{k})^{T}\\phi^{b_{i}}(\\pi_{j})v_{j}\\right)=\\sum_{i=1}^{5}\\phi^{a_{i}}(\\pi_{k})^{T}\\left(\\sum_{j}\\phi^{b_{i}}(\\pi_{j})v_{j}\\right)\\,.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Given that for all $i,\\;\\phi^{b_{i}}$ possesses only $O(z)$ non-zero entries for any $\\pi_{j}$ , the computation of $\\textstyle\\sum_{j}\\phi^{b_{i}}(\\pi_{j})v_{j}$ requires $O(z)$ operations. This implies that the expression $\\dot{\\sum_{j}\\phi^{b_{i}}}(\\pi_{j})v_{j}$ also necessitates $O(z)$ computation. Applying a similar rationale to $\\phi^{a_{i}}$ , it follows that computing $(K_{X}v)_{k}$ demands only $O(z)$ operations. Extending this argument to all entries of the output vector, it is evident that computing $K_{X}\\mathbf{v}$ requires only $\\bar{O}(z\\cdot T)$ computation \u53e3 ", "page_idx": 22}, {"type": "text", "text": "Utilizing Claim 5, it suffices to complete the proof by showcasing that these exist vectors $\\phi^{a_{i}}$ and $\\phi^{b_{i}}$ , each with only ${\\mathcal{O}}(k^{2})$ non-zero elements, corresponding to each $s_{i}$ as specified in Algorithm 2. Additionally, these vectors ensure that $s_{i}(\\pi_{1},\\pi_{2})=\\phi^{a_{i}}(\\pi_{1})^{T}\\phi^{b_{i}}(\\pi_{2})$ . We will next establish such vectors for all $s_{i}$ terms. Starting with the $s_{1}$ term below. ", "page_idx": 22}, {"type": "text", "text": "Showcasing $s_{1}(\\pi_{1},\\pi_{2})\\nonumber\\,=\\phi^{a_{1}}(\\pi_{1})^{T}\\phi^{b_{1}}(\\pi_{2})$ for sparse $\\phi^{a_{1}}(\\pi_{1})$ and $\\phi^{b_{1}}(\\pi_{2})$ vectors. We begin by manipulating $s_{1}$ , as defined in Equation 12. For the sake of brevity, their scalar factors will be omitted in the following explanation. ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{s_{1}(\\overline{{\\pi}}_{1},\\overline{{\\pi}}_{2})=}&{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Both $\\phi^{a_{1}}$ and $\\phi^{b_{1}}$ are sparse by design, taking non-zero values only when $l$ and $m$ appear in the top- $\\cdot\\mathbf{k}$ rankings. This demonstrates the existence of sparse vectors for the $s_{1}$ term. Next, we will establish the same for the $s_{2}$ and $s_{3}$ terms. ", "page_idx": 22}, {"type": "text", "text": "Showcasing sparse vectors for $s_{2}$ and $s_{3}$ . We begin by manipulating $s_{2}$ , as defined in Equation 13, while ignoring its scalar factor. We will exploit symmetry between $s_{2}$ and $s_{3}$ terms. ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{2(\\pi_{1},\\pi_{2})}\\\\ &{=\\displaystyle\\sum_{l\\in I_{1}\\cap I_{2}\\mid m\\in I_{1}\\setminus I_{2}}w_{s}\\big(\\mathbf{p}_{l}^{\\pi_{1}},\\mathbf{p}_{m}^{\\pi_{1}}\\big)\\cdot w_{s}\\big(\\mathbf{p}_{l}^{\\pi_{2}},\\big)\\left(\\mathbf{p}_{l<m}^{\\pi_{1}}-\\mathbf{p}_{l>m}^{\\pi_{1}}\\right),}\\\\ &{=\\displaystyle\\sum_{l\\in I_{1}\\cap I_{2}}w_{s}\\big(\\mathbf{p}_{l}^{\\pi_{2}},\\big)\\sum_{m\\in I_{1}\\setminus I_{2}}w_{s}\\big(\\mathbf{p}_{l}^{\\pi_{1}},\\mathbf{p}_{m}^{\\pi_{1}}\\big)\\left(\\mathbf{p}_{l<m}^{\\pi_{1}}-\\mathbf{p}_{l>m}^{\\pi_{1}}\\right),}\\\\ &{=\\displaystyle\\sum_{l\\in I_{1}\\cap I_{2}}w_{s}\\big(\\mathbf{p}_{l}^{\\pi_{2}},\\big)\\left(\\sum_{m\\in I_{1}}w_{s}\\big(\\mathbf{p}_{l}^{\\pi_{1}},\\mathbf{p}_{m}^{\\pi_{1}}\\big)\\left(\\mathbf{p}_{l<m}^{\\pi_{1}}-\\mathbf{p}_{l>m}^{\\pi_{1}}\\right)-\\displaystyle\\sum_{m\\in I_{1}\\cap I_{2}}w_{s}\\big(\\mathbf{p}_{l}^{\\pi_{1}},\\mathbf{p}_{m}^{\\pi_{1}}\\big)\\left(\\mathbf{p}_{l<m}^{\\pi_{1}}-\\mathbf{p}_{l>m}^{\\pi_{1}}\\right)\\right)}\\\\ &{=\\displaystyle\\sum_{l\\in I_{1}\\cap I_{2}}\\mathbf{p}_{l}^{\\pi_{2}}\\varepsilon\\big[|w_{s}^{\\pi_{2}}\\big(\\mathbf{k}|^{\\pi_{2}},\\big)\\underbrace{\\mathbf{1}_{p}^{\\pi_{1}}|\\mathbf{p}_{m}^{\\pi_{1}}}_{m\\in I_{1}}\\big\\in[K_{1}\\big]\\sum_{m\\in I_{1}}w_{s}\\big(\\mathbf{p}_{l}^{\\pi_{1}},\\mathbf{p}_{m}^{\\pi_{1}}\\big)\\left(\\mathbf{p}_{l<m}^\n$$", "text_format": "latex", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad-\\underset{l,m\\in\\bar{I}_{l}\\backslash\\bar{I}_{l}^{n_{1}}}{\\sum}w_{i\\in[\\bar{I}_{l}^{n_{2}},\\gamma_{m}^{n_{1}}]}(\\mathbf{p}_{l,m}^{\\tau_{1}},\\mathbf{p}_{m}^{\\tau_{1}})\\left(\\mathbf{p}_{l,m}^{\\tau_{1}}-\\mathbf{p}_{l,m}^{\\tau_{1}}\\right),}\\\\ &{=\\phi^{\\mathrm{q\\alpha~2}}(\\tau_{1})T\\phi^{\\mathrm{i}_{1}}(\\pi_{2})-\\underset{l,m\\in\\bar{I}_{l}\\backslash\\bar{I}_{l}^{n_{2}}}{\\sum}w_{i\\in\\{1,1\\}}w_{s}(\\mathbf{p}_{l}^{\\tau_{1}^{\\prime}},\\cdot)w_{s}(\\mathbf{p}_{l}^{\\tau_{1}^{\\prime}},\\mathbf{p}_{m}^{\\tau_{1}^{\\prime}})\\left(\\mathbf{p}_{l,m}^{\\tau_{1}}-\\mathbf{p}_{l,m}^{\\tau_{1}}\\right),}\\\\ &{=\\phi^{\\mathrm{q\\alpha~2}}(\\pi_{1})T\\phi^{\\mathrm{i}_{1}}(\\pi_{2})+\\underset{l,m\\in\\{1\\}}{\\sum}\\underset{l\\in\\bar{I}_{l}\\backslash\\bar{I}_{l}^{n_{2}}}{\\sum}-\\underset{l\\in\\bar{I}_{l}^{n_{2}}}{\\sum}w_{i\\in\\ \\bar{I}_{l}^{n_{2}}}(\\mathbf{p}_{l,m}^{\\tau_{1}^{\\prime}},\\cdot)\\mathbf{p}_{m}^{\\tau_{1}^{\\prime}}\\mathbf{p}_{m}^{\\tau_{2}^{\\prime}}\\in[k]}\\\\ &{\\quad\\quad\\quad\\quad\\cdot\\underset{l\\in\\bar{I}_{l}\\backslash\\bar{I}_{l}^{n_{1}}}{\\underbrace{w_{s}(\\mathbf{p}_{l}^{\\tau_{1}},\\mathbf{p}_{m}^{\\tau_{1}})\\left(\\mathbf{p}_{l,m}^{\\tau_{1}^{\\prime}}-\\mathbf{p}_{l,m}^{\\tau_{1}^{\\prime}}\\right)}}\\mathbf{p}_{l,m}^{\\tau_{1}^{\\prime}}\\mathbf{p}_{m}^{\\tau_{1}^{\\prime}}\\mathbf{\\bar{\\mathbf{p}} \n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Equation 25 demonstrates the existence of vectors $\\phi^{a_{2}}$ and $\\phi^{b_{2}}$ for the $s_{2}$ term. The vectors $\\phi^{a_{21}}$ and $\\phi^{a_{22}}$ , possessing $O(k)$ and ${\\mathcal{O}}(k^{2})$ non-zero entries respectively, are defined in Equations 23 and 24. Consequently, the $\\phi^{a_{2}}$ vector has ${\\mathcal{O}}(k^{2})$ non-zero entries. Similarly, it can be shown that $\\phi^{b_{2}}$ contains ${\\mathcal{O}}(k^{2})$ non-zero entries, thus fulfilling the proof requirements for proving the $s_{2}$ term. For the $s_{3}$ term, we observe a symmetry between $s_{2}$ and $s_{3}$ , namely $s_{3}(\\pi_{1},\\pi_{2}^{-})=s_{2}\\bar{(}\\pi_{2},\\pi_{1})$ . This symmetry makes it trivial to satisfy the requirements, as further highlighted by the following equation: ", "page_idx": 23}, {"type": "equation", "text": "$$\ns_{3}(\\pi_{1},\\pi_{2})=s_{2}(\\pi_{2},\\pi_{1})=\\phi^{a_{2}}(\\pi_{2})^{T}\\phi^{b_{2}}(\\pi_{1})=\\underbrace{\\phi^{b_{2}}(\\pi_{1})^{T}}_{:=\\phi^{a_{3}}(\\pi_{1})}\\underbrace{\\phi^{a_{2}}(\\pi_{2})}_{:=\\phi^{b_{3}}(\\pi_{2})}=\\phi^{a_{3}}(\\pi_{1})^{T}\\phi^{b_{3}}(\\pi_{2}).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Showcasing sparse vectors $s_{4}(\\pi_{1},\\pi_{2})\\,=\\,\\phi^{4a}(\\pi_{1})^{T}\\phi^{4b}(\\pi_{2})$ . We begin by manipulating the $s_{4}$ term without scalar, as defined in Equation 15. ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle s_{4}(\\pi_{1},\\pi_{2})=-\\sum_{l\\in I_{1}\\backslash I_{2}}w_{s}(\\mathbf{p}_{l}^{\\pi_{1}},\\cdot)\\cdot w_{s}(\\mathbf{p}_{m}^{\\pi_{2}},\\cdot)},}\\\\ {{\\displaystyle\\qquad=-\\sum_{l\\in I_{1}\\backslash I_{2}}w_{s}(\\mathbf{p}_{l}^{\\pi_{1}},\\cdot)\\cdot\\left(\\sum_{m\\in I_{2}}w_{s}(\\mathbf{p}_{m}^{\\pi_{2}},\\cdot)-\\sum_{m\\in I_{1}\\cap I_{2}}w_{s}(\\mathbf{p}_{m}^{\\pi_{2}},\\cdot)\\right).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Observing that $\\overline{{w}}:=\\sum_{m\\in I_{2}}w_{s}(\\mathbf{p}_{m}^{\\pi_{2}},\\cdot)$ represents a constant value that does not depend on $I_{2}$ , we can further simplify the above expression for $s_{4}$ as follows: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{s_{4}(\\pi_{1},\\pi_{2})=-\\displaystyle\\sum_{l\\in I_{1}\\backslash I_{2}}w_{s}(\\mathbf{p}_{l}^{\\pi_{1}},\\cdot)\\cdot\\left(\\overline{{w}}-\\displaystyle\\sum_{m\\in I_{1}\\backslash I_{2}}w_{s}(\\mathbf{p}_{m}^{\\pi_{2}},\\cdot)\\right),}\\\\ &{\\qquad\\qquad=-\\left(\\overline{{w}}-\\displaystyle\\sum_{l\\in I_{1}\\cap I_{2}}w_{s}(\\mathbf{p}_{l}^{\\pi_{1}},\\cdot)\\right)\\cdot\\left(\\overline{{w}}-\\displaystyle\\sum_{m\\in I_{1}\\cap I_{2}}w_{s}(\\mathbf{p}_{m}^{\\pi_{2}},\\cdot)\\right),}\\\\ &{\\qquad=-\\overline{{w}}^{2}+\\overline{{w}}\\left(\\displaystyle\\sum_{l\\in I_{1}\\backslash I_{2}}w_{s}(\\mathbf{p}_{l}^{\\pi_{1}},\\cdot)+\\displaystyle\\sum_{m\\in I_{1}\\backslash I_{2}}+w_{s}(\\mathbf{p}_{m}^{\\pi_{2}},\\cdot)\\right)}\\\\ &{\\qquad\\qquad-\\displaystyle\\sum_{l\\in I_{1}\\backslash I_{2}}w_{s}(\\mathbf{p}_{l}^{\\pi_{1}},\\cdot)\\sum_{m\\in I_{1}\\backslash I_{2}}w_{s}(\\mathbf{p}_{m}^{\\pi_{2}},\\cdot).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Next, to simplify the above equation, we first focus on the second term and have the following: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\overline{{w}}\\left(\\sum_{l\\in I_{1}\\cap I_{2}}w_{s}(\\mathbf{p}_{l}^{\\pi_{1}},\\cdot)+\\sum_{m\\in I_{1}\\cap I_{2}}w_{s}(\\mathbf{p}_{m}^{\\pi_{2}},\\cdot)\\right)\n$$", "text_format": "latex", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle=\\sum_{l\\in[n]}\\underbrace{{\\bf1}_{{\\bf p}_{l}^{\\pi_{1}}\\in[k]}{w_{s}}\\bigl({\\bf p}_{l}^{\\pi_{1}},\\cdot\\bigr)}_{:=\\phi_{l}^{4i_{1}}(\\pi_{1})}\\underbrace{{\\bf1}_{{\\bf p}_{l}^{\\pi_{2}}\\in[k]}{\\overline{{{w}}}}}_{:=\\phi_{l}^{4i_{1}}(\\pi_{2})}+\\sum_{m\\in I_{1}\\cap I_{2}}\\overline{{{w}}}\\cdot{w_{s}}({\\bf p}_{m}^{\\pi_{2}},\\cdot),}}\\\\ {{\\displaystyle=\\phi^{4a_{1}}\\bigl(\\pi_{1}\\bigr)^{T}\\phi^{4b_{1}}\\bigl(\\pi_{2}\\bigr)+\\sum_{m\\in[k]}\\underbrace{{\\bf1}_{{\\bf p}_{m}^{\\pi_{1}}\\in[k]}{\\overline{{{w}}}}}_{:=\\phi_{m}^{4a_{2}}(\\pi_{1})}\\underbrace{w_{s}\\bigl({\\bf p}_{m}^{\\pi_{1}},\\cdot\\bigr)}_{:=\\phi_{m}^{4b_{2}}(\\pi_{2})}}}\\\\ {{\\displaystyle=\\phi^{4a_{1}}\\bigl(\\pi_{1}\\bigr)^{T}\\phi^{4b_{1}}\\bigl(\\pi_{2}\\bigr)+\\phi^{4a_{2}}(\\pi_{1})^{T}\\phi^{4b_{2}}(\\pi_{2}).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Next, we simplify the third and last term in the Equation 27 as follows: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{l\\in I_{1}\\cap I_{2}}w_{s}(\\mathbf{p}_{l}^{\\pi_{1}},\\cdot)\\!\\!\\!\\!\\!\\sum_{m\\in I_{1}\\cap I_{2}}w_{s}(\\mathbf{p}_{m}^{\\pi_{2}},\\cdot)=\\!\\!\\!\\!\\sum_{l\\in[n],m\\in[n]}w_{s}(\\mathbf{p}_{l}^{\\pi_{1}},\\cdot)\\mathbf{1}_{\\mathbf{p}_{l}^{\\pi_{1}},\\mathbf{p}_{m}^{\\pi_{1}}\\in[k]}w_{s}(\\mathbf{p}_{m}^{\\pi_{2}},\\cdot)\\mathbf{1}_{\\mathbf{p}_{l}^{\\pi_{2}},\\mathbf{p}_{m}^{\\pi_{2}}\\in[k]},}&{}\\\\ {\\quad=\\!\\!\\!\\phi^{4a_{3}}(\\pi_{1})^{T}\\phi^{4b_{3}}(\\pi_{2}).}&{~(2\\mathbb{S}^{d_{1}a_{1}}+\\mathbb{P}^{b_{2}a_{2}})}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Next, combining the results from Equations 27, 28, and 29, we obtain the following: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{s_{4}(\\pi_{1},\\pi_{2})=\\underbrace{[\\overline{{w}},\\phi^{4a_{1}}(\\pi_{1});\\phi^{4a_{1}}(\\pi_{1});\\phi^{4a_{3}}(\\pi_{1})]^{T}}_{:=\\phi^{4a}(\\pi_{1})^{T}}[-\\overline{{w}};\\phi^{4b_{1}}(\\pi_{2});\\phi^{4b_{2}}(\\pi_{2});-\\phi^{4b_{3}}(\\pi_{2})]_{:=\\phi^{4b}(\\pi_{2})}}\\\\ &{}&{=\\phi^{4a}(\\pi_{1})^{T}\\phi^{4b}(\\pi_{2}).~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~(30\\pi)^{T}\\phi^{2a}(\\pi_{1});}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Equation 30 showcases both $\\phi^{4a}$ and $\\phi^{4b}$ has three components with having only ${\\mathcal{O}}(k^{2})$ non-zero entries, thus fulfilling the requirements for the $s_{4}$ term. Next, we focus on the $s_{5}$ term. ", "page_idx": 24}, {"type": "text", "text": "Showcasing sparse vectors $s_{5}(\\pi_{1},\\pi_{2})=\\phi^{5a}(\\pi_{1})^{T}\\phi^{5b}(\\pi_{2})$ . We begin by examining the $s_{5}$ term, excluding its scalar component, as outlined in Equation 16. ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\varphi_{i}(x,\\tau)}&{=(\\cdots-\\beta_{i},(\\tau)),\\ \\ \\sum_{\\ell=0}^{\\infty}w_{i}(\\tau^{\\prime\\prime}),\\ \\cdots\\infty,(w_{i}^{\\ell})^{n-1},}\\\\ &{=(\\cdots-(\\lambda_{i}-1)\\iota(\\tau)),\\ \\cdots\\sum_{\\ell=1}^{\\infty}w_{i}(\\tau^{\\prime\\prime}),\\ \\cdots\\infty,(w_{i}^{\\ell})^{n-1},}\\\\ &{=(\\cdots\\alpha^{-1})\\cdot\\ \\sum_{\\ell=1}^{\\infty}w_{i}(\\tau^{\\prime\\prime}),\\ \\cdots\\infty,(w_{i}^{\\ell})^{n-1}+|\\lambda_{i}\\cap\\mathcal{H}_{i}|,\\ \\sum_{\\ell=1,1\\atop i\\neq j_{1}}^{\\infty}w_{i}(\\tau^{\\prime\\prime}),\\ \\cdots\\infty,(w_{i}^{\\ell})^{n-1},}\\\\ &{=\\underset{\\mathcal{C}_{i}\\not=0}{\\sum}\\ \\cdots\\sum_{\\ell=1}^{\\infty}w_{i}(\\tau^{\\prime\\prime}),\\ \\cdots\\infty,(w_{i}^{\\ell})^{n-1}\\sqrt{w_{i}(\\tau^{\\prime\\prime})},}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\alpha^{2}}\\\\ &{=\\underset{\\mathcal{C}_{i}\\not=0}{\\sum}\\ \\cdots\\underset{\\mathcal{C}_{i}\\not=0}{\\sum}w_{i}(\\tau^{\\prime\\prime}),\\ \\cdots\\underset{\\mathcal{C}_{i}\\not=0}{\\sum}w_{i}(\\tau^{\\prime\\prime}),\\ \\cdots\\underset{\\mathcal{C}_{i}\\not=0}{\\sum}w_{i}(\\tau^{\\prime\\prime}),}\\\\ &{=(\\cdots)\\underset{\\mathcal{C}_{i}\\not=0}{\\sum}\\cdots\\underset{\\mathcal{C}_{i}\\not=0}{\\sum}w_{i}(\\tau^{\\prime\\prime}),\\ \\cdots\\underset{\\mathcal{C}_{i}\\not=0}{\\sum}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\phi^{5a_{1}}(\\pi_{1})^{T}\\phi^{5b_{1}}(\\pi_{2})+\\phi^{5a_{2}}(\\pi_{1})^{T}\\phi^{5b_{2}}(\\pi_{2}),}\\\\ &{=\\underbrace{[\\phi^{5a_{1}}(\\pi_{1});\\phi^{5a_{2}}(\\pi_{1})]^{T}}_{:=\\phi^{5a}(\\pi_{1})^{T}}\\underbrace{[\\phi^{5b_{1}}(\\pi_{2})+\\phi^{5b_{2}}(\\pi_{2})]}_{:=\\phi^{5b}(\\pi_{2})}=\\phi^{5a}(\\pi_{1})^{T}\\phi^{5b}(\\pi_{2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "The equation shows that $s_{5}(\\pi_{1},\\pi_{2})\\,=\\,\\phi^{5a}(\\pi_{1})^{T}\\phi^{5b}(\\pi_{2})$ , where both $\\phi^{5a}$ and $\\phi^{5b}$ possess components with a maximum number of non-zero entries, as indicated in Equations 31 and 32. This completes the proof requirements for the $s_{5}$ term. ", "page_idx": 25}, {"type": "text", "text": "By combining the results from Equations 22, 25, 26, 30, and 33, we have demonstrated the existence of vectors $\\phi^{a_{i}}$ and $\\phi^{b_{i}}$ , each containing only ${\\mathcal{O}}(k^{2})$ non-zero elements, and have established that $s_{i}(\\pi_{1},\\pi_{2})=\\phi^{a_{i}}(\\pi_{1})^{T}\\phi^{b_{i}}(\\pi_{2})$ for each $i\\in{1,2,3,4,5}$ . In conjunction with Claim 5, this completes the proof. \u53e3 ", "page_idx": 25}, {"type": "text", "text": "B Proposed GP-TopK Bandit Algorithm\u2013 Omitted Details ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "This section includes the proofs that were omitted from Section 4, presented in the following order: ", "page_idx": 25}, {"type": "text", "text": "\u2022 Section B.1 outlines a brief of Gaussian process regression for any domain.   \n\u2022 Section B.2 summarizes the committed details about the local search utilized for optimizing the UCB function.   \n\u2022 Section B.3 provides the removed proof for the Theorem 2 concerning the overall time for the bandit algorithm.   \n\u2022 Section B.4 provides the proof for Theorem 3 concerning regret analysis of the proposed bandit algorithm. ", "page_idx": 25}, {"type": "text", "text": "B.1 Gaussian Process Regression ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "In GP regression [22], the training data are modeled as noisy measurements of a random function $f$ drawn from a GP prior, denoted $f\\sim\\mathcal{N}(0,k(\\cdot,\\cdot))$ , where $k:\\mathcal{X}\\times\\mathcal{X}\\to\\mathbb{R}$ is a kernel function over any domain $\\mathcal{X}$ . The observed training pairs $\\left({{\\bf{x}}_{i}},{y_{i}}\\right)$ are collected as $X=[\\mathbf{x}_{1},\\dots,\\mathbf{x}_{T}]$ and $\\mathbf{y}=[y_{1},\\dots,y_{T}]\\in\\mathbb{R}^{T}$ , where, for an input $\\mathbf{x}_{i}$ , the observed value is modeled as $\\begin{array}{r}{\\mathbf{y}_{i}=f(\\mathbf{x}_{i})+\\boldsymbol{\\epsilon},}\\end{array}$ with $\\epsilon_{i}\\sim\\mathcal{N}(0,\\sigma^{2})$ . The kernel matrix on data is $K_{X}=[k(\\mathbf{x}_{i},\\mathbf{x}_{j})]_{i,j=1}^{T}\\in\\mathbb{R}^{T\\times\\bar{T}}$ . The posterior mean $\\mu_{f\\mid\\mathcal{D}}$ and variance $\\sigma_{f|\\mathcal{D}}$ functions for GPs are: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mu_{f|\\mathcal{D}}(\\mathbf{x}):=\\mathbf{k}_{\\mathbf{x}}^{T}\\mathbf{z}}\\\\ &{\\sigma_{f|\\mathcal{D}}(\\mathbf{x}):=k(\\mathbf{x},\\mathbf{x})-\\mathbf{k}_{\\mathbf{x}}^{T}(K_{X}+\\sigma^{2}I)^{-1}\\mathbf{k}_{\\mathbf{x}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $\\mathbf{k}_{\\mathbf{x}}\\in\\mathbb{R}^{T}$ has as its $i^{t h}$ entry $k(\\mathbf{x},\\mathbf{x}_{i})$ , ${\\mathbf z}=(K_{X}+\\sigma^{2}I)^{-1}{\\mathbf y}$ , and $I$ is an identity matrix. For GP regression on an arbitrary domain $\\mathcal{X}$ , the kernel function must be a p.d. kernel [23]. ", "page_idx": 25}, {"type": "text", "text": "Naive approaches rely on the Cholesky decomposition of the matrix $K_{X}+\\sigma^{2}I$ , which takes $\\Theta(T^{3})$ time [23]. To circumvent the $\\Theta(T^{3})$ runtime, recent works use iterative algorithms such as the conjugate gradient algorithm, which facilitate GP inference by exploiting fast kernel matrix-vector multiplication (MVM) algorithms, i.e., $\\mathbf{v}\\mapsto\\mathbf{K}_{\\mathbf{X}}\\mathbf{v}$ [3]. In practice, these methods yield highly accurate approximations for GP posterior functions with a complexity of $\\Theta(p\\cdot T^{2})$ for $p$ iterations of the conjugate gradient algorithm, as $\\mathbf{mvm}(K_{X})=T^{2}$ , and $\\mathbf{mvm}(M)$ is the operation count for multiplying matrix $M$ by a vector. $p\\ll T$ proves to be efficient in practical application [3]. ", "page_idx": 25}, {"type": "text", "text": "B.2 Contextual GP Reward Model ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Optimizing the $A{\\mathcal F}$ , i.e., UCB function, poses a significant challenge due to its enormous size of $\\Pi^{k}$ . Drawing inspiration from prior research on Bayesian optimization within combinatorial spaces, we employ a breadth-first local search (BFLS) to optimize the UCB acquisition function [2, 19]. The BFLS begins with the selection of several random top- $\\cdot\\mathbf{k}$ rankings. Subsequently, each specific top- $\\cdot\\mathbf{k}$ ranking is compared with the UCB values of its neighboring rankings, proceeding to the one with the highest UCB value. ", "page_idx": 25}, {"type": "text", "text": "The neighbors of a top- $\\cdot\\mathbf{k}$ ranking include all its permutations and the permutations of modified top- $\\cdot\\mathbf{k}$ rankings obtained by swapping one item with any of the remaining items. For any top- $\\cdot\\mathbf{k}$ ranking, there are $(n-k)\\cdot k!+k!$ neighbors, which is often not huge as $k$ is often $\\leq6$ . This search continues until no neighboring top-k ranking with a higher value is discovered. Although BFLS is a local search, the initial random selection and multiple restart points help it evade local minima, a strategy that previous studies have corroborated [19]. ", "page_idx": 26}, {"type": "text", "text": "B.3 Assessing GP-TopK Compute Requirements ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Theorem 2. Assuming a fixed number of iterations required by the iterative algorithms, the total computational time for running the GP-TopK bandit algorithm for $T$ rounds of top- $.k$ recommendations, using the contextual product kernel (Equation 6), is $\\mathcal{O}(k^{2}c\\ell T^{2})$ . This applies to WK, CK, and WCK top-k ranking kernels, where \u2113is the number of local search evaluations for selecting the next arm in every round. ", "page_idx": 26}, {"type": "text", "text": "Proof. The proof can be straightforwardly derived by combining the results presented in Table 1, which succinctly summarizes the time complexities for each step of computing the UCB using both feature and kernel approaches. It is important to emphasize that iterative algorithms enhance results from ${\\mathcal O}(T^{4})$ to $\\mathcal{O}(\\dot{T}^{3})$ in computational complexity. Furthermore, these algorithms can further reduce complexity to $\\mathcal{O}(T^{2})$ when used with the feature approach. ", "page_idx": 26}, {"type": "text", "text": "The results presented in Table 1 can be validated through straightforward observations and by leveraging findings from previous Sections 2. Specifically, Section 2 offers proof for the $\\mathbf{mvm}(K_{X})$ row explicitly. For the compute $K_{X_{t}}$ row, the complexity of kernel approaches is deduced from Algorithms 2 and 3. For feature approaches, the compute $K_{X_{t}}$ row is inferred from the sparsity of the feature representations as stated in Claim 3. Lastly, the memory row is straightforwardly deduced for the kernel approach by counting its entries. For the feature approach, it is derived from the sparsity of the feature representations. \u53e3 ", "page_idx": 26}, {"type": "text", "text": "B.4 Regret Analysis ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "In this section, we revisit Theorem 3 and provide its proof. The proofs build on the work by Krause et al. [14], delivering results for bounding the contextual regret in the context of the top- $\\cdot\\mathbf{k}$ ranking problem. To set the stage for our regret analysis, let\u2019s first define the critical term maximum mutual information, denoted by $\\gamma_{t}$ , is given below: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\gamma_{t}:=\\operatorname*{max}_{X\\subseteq\\mathcal{X}:|X|=t}I(y_{X};f),\\qquad I(y_{X};f)=H(y_{X})-H(y_{X}|f),\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where $I(y_{X};f)$ quantifies the reduction in uncertainty (measured in terms of differential Shannon entropy) about $f$ achieved by revealing $y_{A}$ [27]. In Gaussian observation case, the entropy can be computed in closed form: $\\begin{array}{r}{\\dot{H}(N(\\mu,\\Sigma))\\stackrel{}{=}\\frac{1}{2}\\log\\left|2\\pi e\\Sigma\\right|}\\end{array}$ , so that $I(y_{X};f)\\,=\\,{\\textstyle{\\frac{1}{2}}}\\log|I+\\dot{\\xi}^{-2}K_{X}|$ , where $K_{X}=[k(x,x^{\\prime})]_{x,x^{\\prime}\\in X}$ is the Gram matrix of $k$ evaluated on set $X\\subseteq\\mathcal{X}$ . For the contextual bandit algorithm, $X$ represents contexts and arms considered until round $t$ . ", "page_idx": 26}, {"type": "text", "text": "Before proving Theorem 3, we align the Krause et al. [14] results with our notation for consistency. Furthermore, we modify $\\beta_{t}$ to accommodate embeddings encompassing negative values, aligning with the fact that contextual embeddings may exhibit negative dimensions. ", "page_idx": 26}, {"type": "text", "text": "Proposition 1 (Theorem 1, [14]). Let $\\delta\\in(0,1)$ , and the unknown reward function $\\hat{f}$ be sampled from the known GP prior with known noise variance $\\sigma^{2}$ . Suppose one of the following holds:   \n1. Assumption 1 holds and set $\\beta_{t}=2\\log(|\\mathcal{X}|t^{2}\\pi^{2}/6\\delta)$ .   \n2. Assumption 2 holds and set $\\beta_{t}=2B^{2}+300\\gamma_{t}\\ln^{3}(t/\\delta)$ . Then the cumulative regret $\\mathcal{R}_{T}$ of the contextu\u221aal $G P$ bandit algorithm with the UCB acquisition function is bounded by $\\tilde{\\mathcal{O}}(\\sqrt{C_{1}T\\gamma_{T}\\beta_{T}})$ w.h.p. Precisely, ", "page_idx": 26}, {"type": "text", "text": "$\\operatorname*{Pr}\\left\\{R_{T}\\leq{\\sqrt{C_{1}T\\gamma_{T}\\beta_{t}}}\\right\\}+2\\quad\\forall T\\geq1\\}\\ \\geq\\ 1\\ -\\ \\delta_{t}$ , where, $C_{1}~=~8/\\log(1\\,+\\,\\sigma^{-2})$ and the notation $\\tilde{\\mathcal{O}}$ hides logarithmic factors in $n$ , $\\frac{1}{\\delta}$ and $T$ . ", "page_idx": 27}, {"type": "text", "text": "Proposition 1 shows that the regret $\\mathcal{R}_{T}$ for the contextual GP ban\u221adit algorithm, utilizing the UCB acquisition function is bounded with high probability within $\\tilde{\\mathcal{O}}(\\sqrt{C_{1}T\\gamma_{T}\\beta_{T}})$ , where the notation $\\tilde{\\mathcal{O}}$ hides logarithmic factors in $n$ , $\\frac{1}{\\delta}$ and $T$ . To ascertain the $\\tilde{\\mathcal{O}}$ order for $\\mathcal{R}_{T}$ , it is imperative to first bound the $\\tilde{\\mathcal{O}}$ order of $\\gamma_{T}\\beta_{t}$ . We begin by examining the $\\gamma_{T}$ term in the subsequent proposition. ", "page_idx": 27}, {"type": "text", "text": "Proposition 2. Under the assumptions of Theorem 3, $\\gamma_{T}$ can be succinctly characterized as $\\gamma_{T}=\\mathcal{O}(n^{2}c\\log(n^{2}T)+c\\log T)$ , which also simplifies to $\\tilde{\\mathcal{O}}(n^{2}c)$ , where the $\\tilde{\\mathcal{O}}$ notation omits logarithmic factors in n and $T$ . ", "page_idx": 27}, {"type": "text", "text": "Proof. For the GP bandit algorithm with the UCB acquisition function, $\\gamma_{T}\\mathrm{~\\ensuremath~{~=~}~}C$ \u00b7 $\\log\\left(|I+\\sigma^{-2}K_{X_{T}}|\\right)$ , where $C$ equals $(1/2)\\cdot(1-1/e)^{-1}$ and $K_{X_{T}}$ represents the kernel matrix computed over contexts and arms across $T$ rounds [27, 14]. Precisely, $K_{X_{T}}$ is calculated using the contextual kernel defined in Equation 6. It is applied to contexts and top- $\\cdot\\mathbf{k}$ ratings from the feedback data $\\mathcal{D}_{t}$ , corresponding to Line 6 of the generic contextual bandit Algorithm 1. ", "page_idx": 27}, {"type": "text", "text": "Next, we leverage the characteristic of the contextual kernel being a product kernel. Consequently, the maximum mutual information term for the joint kernel, $\\gamma_{T}$ , can be upper bounded by $c\\cdot\\left(\\gamma_{T}^{\\pi}+\\log T\\right)$ , where $c$ denotes the dimensionality of contexts and $\\gamma_{T}^{\\pi}$ represents the maximum information gain in a non-contextual setting [14]. Specifically, $\\gamma_{T}^{\\pi}$ is computed similarly but is confined to top- $\\cdot\\mathbf{k}$ rankings. That is, $\\gamma_{T}^{\\pi}\\,=\\,C\\cdot\\log\\left(|I+\\sigma^{-2}K_{X^{\\pi}}|\\right)$ , with $K_{X_{T}^{\\pi}}$ being calculated exclusively using the top- $\\cdot\\mathbf{k}$ kernels on the top- $\\cdot\\mathbf{k}$ rankings as selected by the bandit algorithm. $X_{T}^{\\pi}$ represents the top- $\\cdot\\mathbf{k}$ rankings selected by the bandit algorithm, i.e., excluding the contexts from the collected feedback. ", "page_idx": 27}, {"type": "text", "text": "Recalling the formulation for top- $\\cdot\\mathbf{k}$ rankings kernels, we have $K_{X_{T}}\\,=\\,\\Phi_{X_{T}^{\\pi}}^{T}\\Phi_{X_{T}^{\\pi}}$ , where $\\Phi_{X^{\\pi}}\\,\\in$ $\\mathbb{R}^{\\left(n\\right)}\\times T$ comprises feature columns pertinent to the top- $k$ ranking kernels, as elucidated in Section A. Utilizing the Weinstein\u2013Aronszajn identity, $\\gamma_{T}^{\\pi}$ is expressed as ${\\cal C}\\!\\cdot\\!\\log\\left(|I+\\sigma^{-2}\\Phi_{X_{T}^{\\pi}}\\Phi_{X_{T}^{\\pi}}^{T}|\\right)$ . Further, we deduce that $\\begin{array}{r}{\\gamma_{T}^{\\pi}\\leq C\\cdot\\sum_{i=1}^{\\binom{n}{2}}\\log\\left(\\vert1+\\sigma^{-2}\\lambda_{i}\\vert\\right)}\\end{array}$ , where $\\lambda_{i}$ is an eigenvalue of $\\Phi_{X_{T}^{\\pi}}\\Phi_{X_{T}^{\\pi}}^{T}$ . Given the Gershgorin circle theorem, which bounds all eigenvalues of a matrix by the maximum absolute sum of its rows, therefore we can conclude that $\\gamma_{T}^{\\pi}\\ {\\overline{{=}}}\\ O(n^{2}\\log(n^{2}T))$ , as for all the columns of the $\\Phi_{X^{\\pi}}$ have bounded normed as given in Claims 2 and 3, i.e., $||\\phi(\\pi)||_{2}^{2}\\leq1$ [29]. ", "page_idx": 27}, {"type": "text", "text": "By combining $\\gamma_{T}^{\\pi}~=~\\mathcal{O}(n^{2}\\log(n^{2}T))$ with the contextual product kernel, we obtain $\\gamma_{T}=$ $\\mathcal{O}(n^{2}c\\log(n^{2}\\bar{T})+c\\log T)$ , thereby providing the claimed bound in the proposition. \u53e3 ", "page_idx": 27}, {"type": "text", "text": "Next, we build on Propositions 1 and 2 to prove the main theorem regarding the regret of the proposed GP-TopK bandit algorithm for top- $\\cdot\\mathbf{k}$ recommendations. ", "page_idx": 27}, {"type": "text", "text": "Theorem 3. If either Assumptions $^{\\,l}$ or 2 hold, setting $\\beta_{t}$ as $2\\log\\left(\\frac{|\\boldsymbol{c}|\\cdot|\\boldsymbol{\\Pi^{k}}|\\cdot t^{2}\\cdot\\pi^{2}}{6\\delta}\\right)$ and $\\begin{array}{r}{300\\gamma_{t}\\ln^{3}\\left(\\frac{t}{\\delta}\\right)}\\end{array}$ respectively, the cumulative regret $\\mathcal{R}_{T}$ of the $G P$ -TopK bandit algorithm for top- $.k$ recommendations can, with at least $1\\ -\\ \\delta$ probability, be bounded by $\\tilde{\\mathcal{O}}(n\\sqrt{C_{1}T c(\\log|\\mathcal{C}|+k+\\log(T^{2}\\pi^{2}/6\\delta))})$ under Assumption $^{\\,l}$ , and $\\tilde{\\mathcal{O}}(n\\sqrt{C_{1}(2B^{2}c+300n^{2}c^{2}\\ln^{3}(T/\\delta))T})$ under Assumption 2. Here, $\\begin{array}{r}{C_{1}=\\frac{8}{\\log(1+\\xi^{-2})}}\\end{array}$ , and $\\tilde{\\mathcal{O}}$ excludes logarithmic factors related to $n,\\,k,$ , and . ", "page_idx": 27}, {"type": "text", "text": "Proof. We will prove the above theorem for both cases separately. ", "page_idx": 27}, {"type": "text", "text": "For Assumption-1. Given $|{\\mathcal{C}}|$ is finite and $\\beta_{T}=2\\log(|T|T^{2}\\pi^{2}/6\\delta)$ . First, we focus on bounding $\\beta_{T}$ as follows: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\beta_{T}=2\\log(|T|T^{2}\\pi^{2}/6\\delta)\n$$", "text_format": "latex", "page_idx": 27}, {"type": "equation", "text": "$$\n=\\mathcal{O}\\left(\\log\\lvert\\mathcal{C}\\rvert+\\log\\lvert\\Pi^{k}\\rvert+\\log(T^{2}\\pi^{2}/6\\delta)\\right)\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "As ${\\binom{n}{k}}\\leq n^{k}$ and $k!\\leq k^{k}$ , we also have $\\begin{array}{r}{\\log|\\Pi^{k}|=\\log\\left(\\binom{n}{k}k!\\right)\\le\\log\\left(n^{k}k^{k}\\right)=\\mathcal{O}(k\\log(n k))}\\end{array}$ , which implies that $\\beta_{T}=\\mathcal{O}(\\log\\lvert\\mathcal{C}\\rvert+k\\log(n k)+\\log(T^{2}\\pi^{2}/6\\delta))$ . Combining this with Proposition 2, we have following: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{O}(\\gamma_{T}\\beta_{T})=\\mathcal{O}\\left((n^{2}c\\log(n^{2}T)+c\\log T)(\\log|\\mathcal{C}|+k\\log(n k)+\\log(T^{2}\\pi^{2}/6\\delta)\\right)}\\\\ &{\\phantom{\\mathcal{O}\\big(\\gamma_{T}\\beta_{T}\\big)}=\\mathcal{O}\\left(n^{2}c\\log(n^{2}T)(\\log|\\mathcal{C}|+k\\log(n k)+\\log(T^{2}\\pi^{2}/6\\delta)\\big)\\phantom{x x x x x x x x x x x x x}(\\mathrm{Ignoring}\\;c\\log T\\;\\mathrm{term})\\right.}\\\\ &{\\phantom{\\mathcal{O}\\big(\\gamma_{T}\\beta_{T}\\big)}=\\tilde{\\mathcal{O}}\\left(n^{2}c\\left(\\log|\\mathcal{C}|+k+\\log(T^{2}\\pi^{2}/6\\delta)\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Thus, we showcase the asserted bound for the regret $\\mathcal{R}_{T}$ as $\\begin{array}{r l}{\\tilde{\\mathcal{O}}\\left(\\sqrt{C_{1}T\\gamma_{T}\\beta_{T}}\\right)}&{{}=}\\end{array}$ $\\tilde{\\mathcal{O}}\\left(n\\sqrt{C_{1}T c(\\log|\\mathcal{C}|+k+\\log(T^{2}\\pi^{2}/6\\delta))}\\right).$ ", "page_idx": 28}, {"type": "text", "text": "For Assumption-2. Given $\\|f\\|_{k}\\leq B$ and $\\beta_{t}=2B^{2}+300\\gamma_{t}\\ln^{3}(t/\\delta)$ . First, we bound the $\\beta_{T}$ term using Proposition 2 as follows: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\beta_{T}=2B^{2}+300\\cdot\\gamma_{T}\\cdot\\ln^{3}(T/\\delta),}\\\\ &{\\phantom{\\beta_{T}=}=2B^{2}+300\\cdot\\bigl(n^{2}c\\log(n^{2}T)+c\\log T\\bigr)\\cdot\\ln^{3}(T/\\delta).}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Using the above result, we have the following: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{O}(\\sqrt{C_{1}T\\gamma_{T}\\beta_{T}})=\\mathcal{O}\\left(\\sqrt{C_{1}T\\gamma_{T}\\cdot\\left(2B^{2}+300\\cdot\\gamma_{T}\\cdot\\ln^{3}(T/\\delta)\\right)}\\right),}\\\\ &{\\hphantom{\\mathcal{O}(\\sqrt{C_{1}T\\gamma_{T}\\cdot\\left(2B^{2}+300\\cdot\\gamma_{T}\\cdot\\ln^{3}(T/\\delta)\\right)})}=\\mathcal{O}\\left(\\sqrt{C_{1}T n^{2}c\\log(n^{2}T)\\cdot\\left(2B^{2}+300\\cdot n^{2}c\\log(n^{2}T)\\cdot\\ln^{3}(T/\\delta)\\right)}\\right),}\\\\ &{\\hphantom{\\hat{\\mathcal{O}(\\sqrt{C_{1}T\\gamma_{T}\\cdot\\left(2B^{2}+300n^{2}c\\ln^{3}(T/\\delta)\\right)}\\right)}}=\\tilde{\\mathcal{O}}\\left(n\\sqrt{C_{1}T c(2B^{2}+300n^{2}c\\ln^{3}(T/\\delta))}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Comparison with Srinivas et al. (2010). Using the identity kernel for top- $\\cdot\\mathbf{k}$ rankings, we can develop a finite-dimensional feature for the contextual kernel and apply Theorem 5 by Srinivas et al. (2010). Given that $\\gamma_{T}=O(n^{k}c\\log T)$ , the regret bounds are as follows under both assumptions. For instance, the calculations for the $\\bar{\\mathcal{O}}(\\sqrt{C_{1}T\\gamma_{T}\\bar{\\beta}_{T}})$ under the Assumption 2 are as follows: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{O}(\\sqrt{C_{1}T\\gamma_{T}\\beta_{T}})=\\mathcal{O}\\left(\\sqrt{C_{1}T\\gamma_{T}\\cdot\\left(2B^{2}+300\\cdot\\gamma_{T}\\cdot\\ln^{3}(T/\\delta)\\right)}\\right),}\\\\ &{\\qquad\\qquad\\qquad=\\mathcal{O}\\left(\\sqrt{C_{1}T\\left(n^{k}c\\log T\\right)\\cdot\\left(2B^{2}+300\\cdot(n^{k}c\\log T)\\cdot\\ln^{3}(T/\\delta)\\right)}\\right),}\\\\ &{\\qquad\\qquad=\\tilde{\\mathcal{O}}\\left(n^{\\frac{k}{2}}\\sqrt{C_{1}T c(2B^{2}+300n^{k}c\\ln^{3}(T/\\delta))}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Similarly, we can analogously perform the analysis for Assumption 1 and combine it with Proposition 1 to obtain the regret bounds mentioned in the Table 3. ", "page_idx": 28}, {"type": "text", "text": "C Experiments \u2013 Omitted Details ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "This section presents omitted details from the main body of the text. ", "page_idx": 28}, {"type": "text", "text": "C.1 Compute resources ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "We utilized multiple NVIDIA Tesla M40 GPUs with 40 GB RAM on our in-house cluster for our experiments. The experiments in Section 5 required approximately 5 GPU-hours for small arm space and 24 GPU-hours per iteration for large arm space. We conducted about 50 to 100 iterations throughout the project. The results reported in Section C.3 required the same computational resources as the large arm space experiments. ", "page_idx": 28}, {"type": "image", "img_path": "50nEnmVLRb/tmp/a44b39de5b85cf17a5040d7fa7532abe2e4d86b1ad22e4478fa806acdd553b10.jpg", "img_caption": ["Figure 4: Local search results for optimizing combinatorial objectives in $\\Pi^{k}$ for $n=50$ and $k=6$ . For details, see the textual description. Left (a) shows how many times out of 100 trials the local search recovers the exact maximizer, i.e., $\\pi^{'}$ , and right plot (b) shows the average value of the objective for the returned maximizer. These results indicate that the local search utilized in this work is effective. "], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "C.2 Bandit Simulation and Hyper-parameter Configurations \u2013 Omitted Details ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "To set up the simulation, we utilized embeddings trained on the MovieLens dataset using a collaborative filtering approach [6]. We consider a $1M$ variant of the MovieLens dataset, which contains 1 million ratings from 6040 users for 3677 items. Specifically, we train user embeddings $\\mathbf{c}_{u}$ and item embeddings $\\theta_{i}$ such that the user\u2019s attraction to the items are captured by the inner product of the user embedding with the item embeddings, respectively. Both context and item embeddings, i.e., $\\mathbf{c}_{u}$ and $\\theta_{i}$ , are 5-dimensional, optimized by considering the 5-fold performance on this dataset. The reward provided in our experiments is contaminated with zero mean and standard deviation equals 0.05. ", "page_idx": 29}, {"type": "text", "text": "For the $\\epsilon$ -greedy baselines, we considered various values of $\\epsilon$ are considered, specifically $\\epsilon=$ $\\{0.01,0.0\\bar{5},0.1\\}$ . The outcomes are presented for the configuration that demonstrates optimal performance. For MAB-UCB baseline, the algorithm has an upper confidence score $u c b(i)\\;=\\;$ $\\begin{array}{r}{\\overline{{\\mu}}_{i}+\\beta_{m a b}\\sqrt{\\frac{2\\ln(t+1)}{n_{i}}}}\\end{array}$ 2 ln(nti+1)[11]. Here, \u00b5i represents the average reward, n denotes the total number of rounds, and $n_{i}$ signifies the frequency of arm $i$ being played. $\\beta_{m a b}$ is a hyper-parameter. We evaluate $\\beta_{m a b}$ values within the set $\\{0.\\bar{1},0.25,0.5\\}$ and disclose results for the best-performing configuration. For the parameters of proposed GP-TopK bandit algorithms, we set $\\beta_{t}=\\bar{\\beta_{g p}}\\cdot\\log(|\\bar{\\chi^{\\bigr|}}\\cdot t^{2}\\cdot\\bar{\\pi^{2}})$ with $\\beta_{g p}\\in\\left\\lbrace0.05,0.1,0.5\\right\\rbrace$ , reporting results the value that yields the best performance. The choice of $\\beta_{t}$ is informed by prior work in GP bandits [27]. The selection of $\\sigma$ for all variants is determined by optimizing the log-likelihood of the observed after every 10 rounds by considering values in the set $\\{\\bar{0}.01,0.0\\bar{5},0.1\\}$ . ", "page_idx": 29}, {"type": "text", "text": "C.3 Additional results ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Local search results for optimizing combinatorial objectives in $\\Pi^{k}$ for $n=50$ and $k=6$ . Specifically, $\\pi^{\\star}=\\operatorname*{max}_{\\pi}\\phi^{r}(\\pi)^{T}\\phi^{r}(\\pi^{'})$ , where $\\phi^{r}(\\pi^{'}$ represents the feature vector for Kendall kernels on top- $\\cdot\\mathbf{k}$ rankings. Notably, for this optimization problem, it is known that the optimal value is 1 obtained by only $\\pi^{'}$ . Figure 4 shows results for this optimization problem when applied to WK, CK, and WCK kernels. ", "page_idx": 29}, {"type": "text", "text": "Reward results for large arm space for the $\\mathrm{\\nDCG{\\}+}$ diversity reward. Similar to Figure 3, a large setup with $n\\,=\\,50$ for $k\\,=\\,3$ and $k\\,=\\,6$ , is considered. For $k\\,=\\,6$ , the possible arms are over $1.1\\stackrel{-}{\\times}10^{10}$ possible top- $\\cdot\\mathbf{k}$ rankings. Given the vastness of this arm space, computing the optimal arm for the diversity reward is not straightforward. Therefore, we focus on reporting the cumulative reward in Figure 5. We implement this setup using a Local search in batch mode, updating every 5 round and considering a substantial horizon of $T=100$ rounds. Specifically, we use 5 restarts, 5 steps in every search direction, and start with 1000 initial candidates. Figure 5 shows that the WCK approach demonstrates superior performance, continuing to learn effectively even after extensive rounds. ", "page_idx": 29}, {"type": "image", "img_path": "50nEnmVLRb/tmp/1403c0b09f096fc3a375718abf4c7db9d770139ca9494d871e37ea095738d6a3.jpg", "img_caption": ["Figure 5: Comparative evaluation of bandit algorithms for large arm spaces for the $\\mathrm{nDCG}+$ diversity reward, with $>1.1\\times10^{5}$ for the left plot and $>1.1\\stackrel{\\smile}{\\times}10^{10}$ for the right plot, respectively. Cumulative reward with respect to the rounds of the bandit algorithm is depicted. Results are averaged over 6 trials. In both settings, the WCK approach outperforms other baselines. For more details, see the textual description. "], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: Section 1 briefs both contributions and scope of this work. ", "page_idx": 30}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: Section 6 reflects on the limitations of this work. ", "page_idx": 30}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: Proofs of all Claims and Theorems are provided in the Appendix. ", "page_idx": 30}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: Section 5 provides necessary details of bandit simulator and experimental setups considered in this work. ", "page_idx": 30}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: Our code can be accessed using this hyper-link. ", "page_idx": 30}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: Section 5 provides experimental details and a few remaining details are given in the Appendix C. ", "page_idx": 30}, {"type": "text", "text": "7. Experiment Statistical Significance ", "page_idx": 31}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] Justification: All figures reported in this work have errorbars with them. ", "page_idx": 31}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: Section C provides relevant information about compute resources. ", "page_idx": 31}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: Section 6 reflects on the impact of this work. ", "page_idx": 31}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] Justification: this work does not release any such resource or asset. ", "page_idx": 31}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: this work release only code with instructions for its usage. ", "page_idx": 31}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}]