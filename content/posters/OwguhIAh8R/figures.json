[{"figure_path": "OwguhIAh8R/figures/figures_1_1.jpg", "caption": "Figure 1: Motivating example of HGDL study, where each node is a local urban region [1] and edges represent taxi services [2] commuting between regions. Heterogeneous node types indicate disparate land use, including residence (R), service (S), leisure (L), and transit (T), among which R nodes are of our interest. Colored R nodes are with ground truth, which delineate their distributions over multiple point-of-interests (POIs), and each POI is deemed as a class/label. Our HGDL uncolored R nodes, enabling a precise delineation of regional urban functionality.", "description": "This figure shows an example of a heterogeneous graph representing urban regions.  Nodes represent regions with different functionalities (residence, service, leisure, transit), and edges represent taxi services between regions.  The goal is to predict the distribution of points of interest (POIs) within each region (represented by the colors in the 'residence' nodes), given the region's type, features, and connections to other regions.  The colored nodes show the ground truth label distributions, while uncolored nodes represent those needing prediction.", "section": "Introduction"}, {"figure_path": "OwguhIAh8R/figures/figures_3_1.jpg", "caption": "Figure 2: The proposed HGDL framework. Using k meta-paths, the heterogeneous network in is converted to k homogeneous meta-path graphs in \u2461. Topology homogenization in \u2462 proactively aggregates all k meta-path graphs, through learnable weight matrix W \u2208 Rn\u00d7f for each graph, and finally obtain attention \u0398 \u2208 Rn\u00d7k across all graphs. Topology and feature consistency-aware graph transformer in \u2463 harmonizes the local and global consistencies. The objective function in \u2464 unifies loss and regularization terms to guide nodal label distribution learning.", "description": "This figure illustrates the proposed Heterogeneous Graph Label Distribution Learning (HGDL) framework.  It shows the process of converting a heterogeneous graph into multiple homogeneous meta-path graphs, aggregating them using a learnable attention mechanism, and then using a transformer to combine topology and feature information for label distribution prediction. The process is optimized using a joint objective function that balances KL-divergence and an attention regularizer.", "section": "4 HGDL: The Proposed Approach"}, {"figure_path": "OwguhIAh8R/figures/figures_8_1.jpg", "caption": "Figure 3: Comparisons between HGDL vs. results from a single meta-path (CAD and CLD are calculated in natural log for better visualization) for five datasets.", "description": "This figure compares the performance of the proposed HGDL model against using only a single meta-path for five different datasets.  The bar chart displays different evaluation metrics for each method and dataset, offering a visual comparison of their performance.  The use of a single meta-path represents a simplified approach compared to HGDL's more sophisticated method of integrating multiple meta-paths.  The results highlight HGDL's superior performance.", "section": "6 Experiments"}, {"figure_path": "OwguhIAh8R/figures/figures_8_2.jpg", "caption": "Figure 4: KL and CLD tradeoff function example. The estimated probability distribution is [x1, x2, 0.9], and true probability distribution is [0.05, 0.05, 0.9], with x1 + x2 = 0.1. Horizontal axis is the x1 value and vertical axis is the loss for both CLD and KL divergence. Green dashed lines cover the tradeoff region where the CLD loss monotonically increases and KL-divergence decreases.", "description": "This figure shows an example of the tradeoff between KL divergence and Clark distance.  It illustrates how, for a three-class label distribution prediction problem where one class has a high probability (0.9), the KL divergence and Clark distance show an inverse relationship in a certain region.  This highlights a limitation of using Clark distance as a metric when one class dominates, as its ability to reflect changes in distribution is limited in these circumstances.", "section": "6.2 Results"}, {"figure_path": "OwguhIAh8R/figures/figures_16_1.jpg", "caption": "Figure 2: The proposed HGDL framework. Using k meta-paths, the heterogeneous network in is converted to k homogeneous meta-path graphs in \u2461. Topology homogenization in \u2462 proactively aggregates all k meta-path graphs, through learnable weight matrix W \u2208 Rn\u00d7f for each graph, and finally obtain attention \u0398 \u2208 Rn\u00d7k across all graphs. Topology and feature consistency-aware graph transformer in \u2463 harmonizes the local and global consistencies. The objective function in \u2464 unifies loss and regularization terms to guide nodal label distribution learning.", "description": "This figure illustrates the proposed Heterogeneous Graph Label Distribution Learning (HGDL) framework. It consists of three main components: 1) Optimal Graph Topology Homogenization, which converts the heterogeneous graph into multiple homogeneous meta-path graphs and learns the optimal topology using an attention mechanism; 2) Topology and Content Consistency-Aware Graph Transformer, which harmonizes the information from the learned optimal topology and nodal features using a transformer architecture; and 3) End-to-End Optimization Objective, which unifies the learning of optimal topology, feature-topology harmonization, and label distribution prediction into a joint optimization problem. This framework aims to effectively address the challenges of heterogeneity and inconsistency in real-world graphs for label distribution learning.", "section": "4 HGDL: The Proposed Approach"}, {"figure_path": "OwguhIAh8R/figures/figures_16_2.jpg", "caption": "Figure 2: The proposed HGDL framework. Using k meta-paths, the heterogeneous network in is converted to k homogeneous meta-path graphs in \u2461. Topology homogenization in \u2462 proactively aggregates all k meta-path graphs, through learnable weight matrix W \u2208 Rn\u00d7f for each graph, and finally obtain attention \u0398 \u2208 Rn\u00d7k across all graphs. Topology and feature consistency-aware graph transformer in \u2463 harmonizes the local and global consistencies. The objective function in \u2464 unifies loss and regularization terms to guide nodal label distribution learning.", "description": "This figure illustrates the HGDL framework, showing how it addresses heterogeneous graph label distribution learning.  It starts with a heterogeneous graph and generates multiple homogeneous meta-path graphs.  A topology homogenization step learns optimal graph topology through an attention mechanism. A topology and content consistency-aware graph transformer harmonizes nodal features with the learned topology. Finally, a joint optimization objective is used to learn the label distribution for nodes. ", "section": "4 HGDL: The Proposed Approach"}, {"figure_path": "OwguhIAh8R/figures/figures_20_1.jpg", "caption": "Figure 5: HGDL vs. baseline with different edge drop rates on DRUG and ACM dataset", "description": "This figure compares the performance of HGDL against a baseline model with varying edge drop rates on two datasets: DRUG and ACM. The x-axis represents the edge drop rate, and the y-axis represents the KL loss.  The plot shows how the KL loss changes as the edge drop rate increases for both datasets, allowing for a visual comparison of HGDL's performance relative to the baseline under different levels of edge dropout.  This helps illustrate the impact of edge dropout on model performance and the effectiveness of the proposed HGDL approach.", "section": "6 Experiments"}, {"figure_path": "OwguhIAh8R/figures/figures_21_1.jpg", "caption": "Figure 8: Training process KL-divergence validation loss comparisons on the ACM dataset. The x-axis denotes epochs, and the y-axis denotes training validation loss. Early stop is applied to all three methods, so methods terminate at different epochs. A method terminates at the minimum loss, if no smaller loss is found after continuing a patience number of epochs.", "description": "This figure compares the validation KL-divergence loss curves of three different models (HANKL, HGDL, and SeHGNNKL) during training on the ACM dataset.  The plot shows how the loss changes over the number of training epochs.  Early stopping was used for all three models, so the training ended at different points, but always when no improvement in the loss was observed after a set patience period.", "section": "6.2 Results"}, {"figure_path": "OwguhIAh8R/figures/figures_21_2.jpg", "caption": "Figure 8: Training process KL-divergence validation loss comparisons on the ACM dataset. The x-axis denotes epochs, and the y-axis denotes training validation loss. Early stop is applied to all three methods, so methods terminate at different epochs. A method terminates at the minimum loss, if no smaller loss is found after continuing a patience number of epochs.", "description": "This figure compares the training process KL-divergence validation loss for three different methods (HANKL, HGDL, and SeHGNNKL) on the ACM dataset.  It shows how the validation loss changes over epochs. Early stopping is used, meaning training stops when the validation loss stops improving to prevent overfitting. The plot helps illustrate the convergence speed and the final validation loss achieved by each method.", "section": "6.2 Results"}, {"figure_path": "OwguhIAh8R/figures/figures_21_3.jpg", "caption": "Figure 8: Training process KL-divergence validation loss comparisons on the ACM dataset. The x-axis denotes epochs, and the y-axis denotes training validation loss. Early stop is applied to all three methods, so methods terminate at different epochs. A method terminates at the minimum loss, if no smaller loss is found after continuing a patience number of epochs.", "description": "This figure compares the training process KL-divergence validation loss for three different models: HANKL, HGDL, and SeHGNNKL, on the ACM dataset.  The plot shows how the validation loss changes over training epochs.  Early stopping is used, meaning that training stops when the validation loss stops improving for a certain number of epochs.  The figure shows HGDL achieves a lower validation loss compared to the other two models.", "section": "6.2 Results"}, {"figure_path": "OwguhIAh8R/figures/figures_22_1.jpg", "caption": "Figure 9: Sensitive analysis of \u03b3, ranging from 0, 1e-5, 1e-4, 1e-3, 1e-2, and 0.1, converted to the values of log(\u03b3) as -6, -5, -4, -3, -2, and -1 along the X-axis.", "description": "This figure shows the result of a sensitivity analysis performed on the hyperparameter \u03b3.  The analysis explores how different values of \u03b3 affect the model's performance, as measured by KL divergence. The x-axis represents the logarithm of \u03b3, while the y-axis shows the KL divergence achieved on five different datasets (DRUG, URBAN, YELP, ACM, DBLP). The figure helps to determine the optimal value of \u03b3 for each dataset.", "section": "5 Analysis"}]