{"references": [{"fullname_first_author": "Shao", "paper_title": "Character-LLM: A trainable agent for role-playing", "publication_date": "2023-12-06", "reason": "This paper is frequently cited for introducing a trainable agent for persona-driven role-playing and providing a baseline for faithfulness evaluation."}, {"fullname_first_author": "Bowman", "paper_title": "A large annotated corpus for learning natural language inference", "publication_date": "2015-08-05", "reason": "This paper provides a crucial dataset and benchmark for Natural Language Inference (NLI), which is a key component in the proposed faithfulness metric."}, {"fullname_first_author": "Rafailov", "paper_title": "Direct preference optimization: Your language model is secretly a reward model", "publication_date": "2023-12-10", "reason": "This paper introduces the Direct Preference Optimization (DPO) framework, which is directly leveraged in this work for optimizing the faithfulness of the role-playing models."}, {"fullname_first_author": "Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-12-06", "reason": "This foundational paper on large language models (LLMs) is crucial because the proposed method builds upon the capabilities of LLMs for persona-driven role-playing."}, {"fullname_first_author": "Lewis", "paper_title": "Retrieval-augmented generation for knowledge-intensive NLP tasks", "publication_date": "2020-12-06", "reason": "This paper introduces the Retrieval-Augmented Generation (RAG) technique, which is one of the compared methods in this paper and a significant approach in the field of knowledge-intensive NLP."}]}