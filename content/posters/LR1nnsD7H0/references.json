{"references": [{"fullname_first_author": "M. Azabou", "paper_title": "A unified, scalable framework for neural population decoding", "publication_date": "2023-10-26", "reason": "This paper introduces a scalable framework for neural population decoding, addressing challenges similar to those faced in this paper regarding sEEG data integration and generalization across subjects."}, {"fullname_first_author": "A. Dosovitskiy", "paper_title": "An image is worth 16x16 words: Transformers for image recognition at scale", "publication_date": "2020-10-27", "reason": "This paper highlights the success of transformer architectures in achieving excellent performance and generalization when trained on large and diverse datasets, a key aspect that motivated the use of transformers in this paper."}, {"fullname_first_author": "J. L. Fozard", "paper_title": "Age differences and changes in reaction time: The Baltimore longitudinal study of aging", "publication_date": "1994-07-01", "reason": "This paper is relevant to understanding the impact of age-related variability in reaction times on the design and analysis of behavioral decoding experiments."}, {"fullname_first_author": "J. I. Glaser", "paper_title": "Machine learning for neural decoding", "publication_date": "2020-07-01", "reason": "This paper provides a comprehensive overview of machine learning for neural decoding, relevant to the study of neural decoding from sEEG."}, {"fullname_first_author": "A. Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-06-12", "reason": "This paper introduced the transformer architecture, which is the foundation for many modern neural network architectures and was instrumental in this paper's approach to neural decoding."}]}