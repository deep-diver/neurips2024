[{"figure_path": "LR1nnsD7H0/figures/figures_3_1.jpg", "caption": "Figure 1: Outline of our network architecture. The sEEG signals are converted to vector embeddings through temporal convolutions and then processed by sequential self-attention operations in the time and electrode dimensions in an alternating fashion. The latents are compressed and projected through subject-specific task heads to obtain behavioral predictions.", "description": "This figure illustrates the architecture of the Seegnificant model.  The model takes sEEG data as input.  First, a temporal convolutional neural network (CNN) processes the time series data from each electrode independently to create temporal tokens, representing short segments of neural activity. These tokens are then processed through a self-attention mechanism in the time dimension to capture long-range temporal dependencies.  Next, spatial information (3D coordinates) is added to the temporal tokens using positional encoding. These spatially enriched tokens are then further processed using a self-attention mechanism in the electrode dimension to capture spatial relationships between electrodes. Finally, the resulting neural representations are fed into subject-specific regression heads (MLP layers) to predict the behavioral outcome (reaction time in this case).", "section": "3 Methodology"}, {"figure_path": "LR1nnsD7H0/figures/figures_6_1.jpg", "caption": "Figure 2: Overview of behavioral experiment. A. Schematic of the color change detection task. B. Electrode placement projected onto the MNI brain template for four example subjects in our cohort. Red dots show electrodes used for model training; grey dots show electrodes excluded from model training (see section 3.1). C. Electrodes used for model training across all subjects in our cohort projected on an MNI brain template.", "description": "This figure provides a visual overview of the behavioral experiment and the electrode placement used for the study. Panel A shows a schematic of the color change detection task, illustrating the different stages: pre-trial, stimulus, color-change delay, response, and response time. Panels B and C show the electrode placement projected onto the MNI brain template.  Panel B shows electrode locations in four example subjects, distinguishing between electrodes used for model training (red dots) and those excluded (gray dots).  Panel C shows electrodes from all subjects used in model training, providing a summary visualization of the data used in the study.", "section": "4 Experiments"}, {"figure_path": "LR1nnsD7H0/figures/figures_7_1.jpg", "caption": "Figure 3: Comparing decoding performance between the single-subject and multi-subject models for each subject. A. Single-subject vs multi-subject model. B. Single-subject vs finetuned, multi-subject model. Circle size denotes the number of trials (ascending order).", "description": "This figure compares the decoding performance of single-subject models versus multi-subject models (A) and finetuned multi-subject models (B) for each subject. The size of the circles represents the number of trials for each subject, ordered from smallest to largest. It visually demonstrates that using multi-subject models improves decoding performance compared to single-subject models, and further enhancement can be achieved by finetuning the multi-subject model for each individual subject.", "section": "4.3 Training a multi-session, multi-subject model"}, {"figure_path": "LR1nnsD7H0/figures/figures_7_2.jpg", "caption": "Figure 3: Comparing decoding performance between the single-subject and multi-subject models for each subject. A. Single-subject vs multi-subject model. B. Single-subject vs finetuned, multi-subject model. Circle size denotes the number of trials (ascending order).", "description": "This figure compares the decoding performance (R-squared) of single-subject models versus multi-subject models (A) and single-subject models versus finetuned multi-subject models (B) for each subject in the study. The size of the circles in the scatter plots represents the number of trials used for training. It shows that the multi-subject models generally outperform the single-subject models, and finetuning multi-subject models further boosts performance.", "section": "4.3 Training a multi-session, multi-subject model"}, {"figure_path": "LR1nnsD7H0/figures/figures_8_1.jpg", "caption": "Figure 5: Decoding performance (mean \u00b1 sem) for various baselines and our proposed models.", "description": "This figure compares the performance of various baseline models (PCA + Wiener, PCA + Ridge, PCA + XGB, MLP, CNN + MLP, PCA + Lasso) with the proposed Seegnificant model.  The Seegnificant model is tested in several configurations: single-subject, multi-subject, multi-subject with fine-tuning, and single-subject with transfer learning. The results show that the multi-subject and transfer learning variants of Seegnificant outperform all baseline models, highlighting the benefits of the multi-subject training approach.", "section": "4.5 Comparison with baselines"}, {"figure_path": "LR1nnsD7H0/figures/figures_8_2.jpg", "caption": "Figure 6: Summary of ablation results. (A) Decoding performance (mean \u00b1 sem) across different model variants. (B) Head-to-head comparison of the decoding performance of our multi-session, multi-subject model vs its 2D attention variant.", "description": "Figure 6 presents the results of an ablation study evaluating the impact of different components of the proposed model on decoding performance. Panel (A) shows a bar graph comparing the performance of the full model with versions where various components (temporal attention, spatial positional encoding, spatial attention, and subject-specific regression heads) have been removed.  Panel (B) displays a scatter plot comparing the performance of the full multi-session, multi-subject model against a variant using a single 2D attention mechanism instead of separate temporal and spatial attention mechanisms.", "section": "4.6 Ablations"}, {"figure_path": "LR1nnsD7H0/figures/figures_16_1.jpg", "caption": "Figure 7: Per subject decoding performance comparison between the model with and without spatial positional encoding.", "description": "This figure shows a scatter plot comparing the decoding performance (R-squared) of a multi-subject model trained with spatial positional encoding against the same model trained without it, for each of the 21 subjects.  Each point represents a subject, with color indicating the number of electrodes used for that subject. The dashed line represents the line of equality (y = x).  The plot shows that the inclusion of spatial positional encoding has a variable effect on individual subjects, with some showing improvement and others showing no significant change in performance. A statistical test comparing the two sets of results is mentioned in the paper's appendix.", "section": "A.4.2 Contribution of spatial positional encoding"}]