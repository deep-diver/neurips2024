[{"figure_path": "pRSgf5VdD0/figures/figures_2_1.jpg", "caption": "Figure 1: Illustration of the setting for I = 3 different groups clustered in 3, 2, and 3 subgroups respectively. Samples consist of one element of each group, the dashed lines indicate samples (1, 3, 1) and (3, 7, 6).", "description": "This figure illustrates an example of the setting described in the paper, where tokens are grouped into a small number of classes. The figure shows three sets (i=1,2,3) of tokens, each partitioned into subgroups (clusters). The dashed lines represent samples of the interaction function f, demonstrating how the function's output depends only on the class membership of the input tokens. This example helps visualize how the hidden structure of token classes is learned based on observations of their interactions.", "section": "Setting and Motivation"}, {"figure_path": "pRSgf5VdD0/figures/figures_7_1.jpg", "caption": "Figure 2: Simulation of the setting in Theorem 5 with N = 1000, K = I = 3, D = 2, \u03bb = 0, S = 100.000. (left) trajectories of 50 randomly sampled tokens from 6 different classes. (right) Average distance of token embeddings within a class for different classes (colored) and average distance between all pairs of embeddings (black).", "description": "This figure shows the result of a simulation to illustrate Theorem 5. The left panel shows trajectories of 50 randomly chosen tokens' embeddings during gradient descent. Each color represents a different class. The right panel shows the average distance between embeddings within the same class (colored lines) and the average distance between all pairs of embeddings (black line).  It demonstrates how gradient descent recovers the cluster structure of embeddings, which is a key aspect of the paper's analysis.", "section": "Analysis of Gradient Descent Dynamics"}]