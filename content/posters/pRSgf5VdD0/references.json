{"references": [{"fullname_first_author": "Kwangjun Ahn", "paper_title": "Transformers learn to implement preconditioned gradient descent for in-context learning", "publication_date": "2023", "reason": "This paper is highly relevant as it also studies the dynamics of token embeddings and their ability to recover hidden structures, which is the central theme of the current paper."}, {"fullname_first_author": "Francis Bach", "paper_title": "Breaking the curse of dimensionality with convex neural networks", "publication_date": "2017-01-01", "reason": "This paper provides essential background on the theoretical understanding of neural networks, which is relevant to the analysis of the current paper's model."}, {"fullname_first_author": "Arthur Jacot", "paper_title": "Neural tangent kernel: Convergence and generalization in neural networks", "publication_date": "2018", "reason": "This paper offers insights into the theoretical properties of neural networks, which are important for the current paper's analysis."}, {"fullname_first_author": "Raghunandan H. Keshavan", "paper_title": "Matrix completion from a few entries", "publication_date": "2010-06-01", "reason": "This paper provides theoretical foundations for matrix completion, which is relevant to the information-theoretic analysis of learning from sparse interactions in the current paper."}, {"fullname_first_author": "Yuchen Li", "paper_title": "How do transformers learn topic structure: Towards a mechanistic understanding", "publication_date": "2023-07-23", "reason": "This paper investigates the dynamics of word embeddings and transformers, which is closely related to the topic of learning from context and hidden structures in the current paper."}]}